4_noisy_pubmed_10.txtBegin epxeriment: noisy_level: 0.2 cont_weight: 100 epoch: der1:0.3 der2:0 dfr1:0.4 dfr2:0
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=2500, cl_num_layers=2, cl_num_proj_hidden=64, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=100.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.0, drop_feat_rate_1=0.4, drop_feat_rate_2=0.0, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=32, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, noisy_level=0.2, num_hidden=64, num_proj_hidden=64, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=40, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 88648])
add edge: torch.Size([2, 106338])
remove edge: torch.Size([2, 70830])
updated graph: torch.Size([2, 88520])
=== Noisy graph ===
/home/project-robust-contrastive/Robust-Contrastive-Learning/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 1059.309326171875 = 1.083004117012024 + 100.0 * 10.582262992858887
Epoch 0, val loss: 1.0838348865509033
Epoch 10, training loss: 1059.2659912109375 = 1.0797984600067139 + 100.0 * 10.58186149597168
Epoch 10, val loss: 1.0806342363357544
Epoch 20, training loss: 1059.1131591796875 = 1.0763323307037354 + 100.0 * 10.580368995666504
Epoch 20, val loss: 1.0771842002868652
Epoch 30, training loss: 1058.507568359375 = 1.0726327896118164 + 100.0 * 10.574349403381348
Epoch 30, val loss: 1.0735080242156982
Epoch 40, training loss: 1056.173828125 = 1.0686899423599243 + 100.0 * 10.551051139831543
Epoch 40, val loss: 1.0696032047271729
Epoch 50, training loss: 1048.306396484375 = 1.0645337104797363 + 100.0 * 10.472417831420898
Epoch 50, val loss: 1.0655251741409302
Epoch 60, training loss: 1024.54931640625 = 1.0604331493377686 + 100.0 * 10.234888076782227
Epoch 60, val loss: 1.0614970922470093
Epoch 70, training loss: 974.5687255859375 = 1.0559817552566528 + 100.0 * 9.735127449035645
Epoch 70, val loss: 1.0570857524871826
Epoch 80, training loss: 954.8825073242188 = 1.0518662929534912 + 100.0 * 9.53830623626709
Epoch 80, val loss: 1.053184151649475
Epoch 90, training loss: 948.8098754882812 = 1.0489991903305054 + 100.0 * 9.477608680725098
Epoch 90, val loss: 1.0505647659301758
Epoch 100, training loss: 939.8062133789062 = 1.0467119216918945 + 100.0 * 9.387595176696777
Epoch 100, val loss: 1.0484230518341064
Epoch 110, training loss: 926.9706420898438 = 1.0445456504821777 + 100.0 * 9.259261131286621
Epoch 110, val loss: 1.046305775642395
Epoch 120, training loss: 912.3021240234375 = 1.0427604913711548 + 100.0 * 9.112593650817871
Epoch 120, val loss: 1.0445570945739746
Epoch 130, training loss: 901.9644775390625 = 1.041649580001831 + 100.0 * 9.009228706359863
Epoch 130, val loss: 1.0433963537216187
Epoch 140, training loss: 896.2919921875 = 1.0403848886489868 + 100.0 * 8.952515602111816
Epoch 140, val loss: 1.0420796871185303
Epoch 150, training loss: 891.7067260742188 = 1.0388109683990479 + 100.0 * 8.906679153442383
Epoch 150, val loss: 1.0405819416046143
Epoch 160, training loss: 887.8734130859375 = 1.0376521348953247 + 100.0 * 8.86835765838623
Epoch 160, val loss: 1.0395089387893677
Epoch 170, training loss: 885.9075927734375 = 1.0367764234542847 + 100.0 * 8.848708152770996
Epoch 170, val loss: 1.0386509895324707
Epoch 180, training loss: 884.7200927734375 = 1.0356367826461792 + 100.0 * 8.836844444274902
Epoch 180, val loss: 1.0375339984893799
Epoch 190, training loss: 883.47802734375 = 1.0344154834747314 + 100.0 * 8.82443618774414
Epoch 190, val loss: 1.0363892316818237
Epoch 200, training loss: 881.969482421875 = 1.0334454774856567 + 100.0 * 8.80936050415039
Epoch 200, val loss: 1.0354762077331543
Epoch 210, training loss: 879.9237670898438 = 1.0327836275100708 + 100.0 * 8.788909912109375
Epoch 210, val loss: 1.0348564386367798
Epoch 220, training loss: 877.2493286132812 = 1.0324362516403198 + 100.0 * 8.762168884277344
Epoch 220, val loss: 1.0345373153686523
Epoch 230, training loss: 874.12646484375 = 1.0323070287704468 + 100.0 * 8.730941772460938
Epoch 230, val loss: 1.0344350337982178
Epoch 240, training loss: 871.6109619140625 = 1.0322140455245972 + 100.0 * 8.705787658691406
Epoch 240, val loss: 1.0343186855316162
Epoch 250, training loss: 869.378662109375 = 1.031856656074524 + 100.0 * 8.683467864990234
Epoch 250, val loss: 1.0339596271514893
Epoch 260, training loss: 867.1387329101562 = 1.0314154624938965 + 100.0 * 8.661072731018066
Epoch 260, val loss: 1.033547043800354
Epoch 270, training loss: 865.3756713867188 = 1.0310050249099731 + 100.0 * 8.643446922302246
Epoch 270, val loss: 1.033118724822998
Epoch 280, training loss: 863.3899536132812 = 1.0304840803146362 + 100.0 * 8.623595237731934
Epoch 280, val loss: 1.032627820968628
Epoch 290, training loss: 861.7564697265625 = 1.029842495918274 + 100.0 * 8.607266426086426
Epoch 290, val loss: 1.0320225954055786
Epoch 300, training loss: 860.43017578125 = 1.0291321277618408 + 100.0 * 8.594010353088379
Epoch 300, val loss: 1.031358003616333
Epoch 310, training loss: 858.95361328125 = 1.0283225774765015 + 100.0 * 8.579253196716309
Epoch 310, val loss: 1.0306248664855957
Epoch 320, training loss: 857.8513793945312 = 1.0274549722671509 + 100.0 * 8.568239212036133
Epoch 320, val loss: 1.0298393964767456
Epoch 330, training loss: 857.0732421875 = 1.026536464691162 + 100.0 * 8.560466766357422
Epoch 330, val loss: 1.0289958715438843
Epoch 340, training loss: 856.088134765625 = 1.025558352470398 + 100.0 * 8.550625801086426
Epoch 340, val loss: 1.0281070470809937
Epoch 350, training loss: 855.2520141601562 = 1.0246108770370483 + 100.0 * 8.542274475097656
Epoch 350, val loss: 1.0272282361984253
Epoch 360, training loss: 854.5530395507812 = 1.0236895084381104 + 100.0 * 8.535293579101562
Epoch 360, val loss: 1.0263534784317017
Epoch 370, training loss: 853.8488159179688 = 1.0226824283599854 + 100.0 * 8.528261184692383
Epoch 370, val loss: 1.025466799736023
Epoch 380, training loss: 853.093994140625 = 1.0216730833053589 + 100.0 * 8.520723342895508
Epoch 380, val loss: 1.024518370628357
Epoch 390, training loss: 852.4318237304688 = 1.0205801725387573 + 100.0 * 8.51411247253418
Epoch 390, val loss: 1.0235202312469482
Epoch 400, training loss: 851.7510986328125 = 1.0194282531738281 + 100.0 * 8.507316589355469
Epoch 400, val loss: 1.0224405527114868
Epoch 410, training loss: 851.2754516601562 = 1.018227219581604 + 100.0 * 8.502572059631348
Epoch 410, val loss: 1.0213202238082886
Epoch 420, training loss: 850.8212890625 = 1.0169041156768799 + 100.0 * 8.49804401397705
Epoch 420, val loss: 1.020084023475647
Epoch 430, training loss: 850.2531127929688 = 1.0155366659164429 + 100.0 * 8.492375373840332
Epoch 430, val loss: 1.0187931060791016
Epoch 440, training loss: 849.791259765625 = 1.0141305923461914 + 100.0 * 8.487771034240723
Epoch 440, val loss: 1.017500877380371
Epoch 450, training loss: 849.7039794921875 = 1.0126497745513916 + 100.0 * 8.486913681030273
Epoch 450, val loss: 1.0161538124084473
Epoch 460, training loss: 849.11328125 = 1.0110845565795898 + 100.0 * 8.481021881103516
Epoch 460, val loss: 1.0146085023880005
Epoch 470, training loss: 848.7796630859375 = 1.0094537734985352 + 100.0 * 8.477702140808105
Epoch 470, val loss: 1.0130786895751953
Epoch 480, training loss: 848.4761352539062 = 1.0077636241912842 + 100.0 * 8.47468376159668
Epoch 480, val loss: 1.011528491973877
Epoch 490, training loss: 848.2221069335938 = 1.00602388381958 + 100.0 * 8.472160339355469
Epoch 490, val loss: 1.0098797082901
Epoch 500, training loss: 848.0447998046875 = 1.0041358470916748 + 100.0 * 8.470406532287598
Epoch 500, val loss: 1.0081266164779663
Epoch 510, training loss: 847.8882446289062 = 1.0021731853485107 + 100.0 * 8.468860626220703
Epoch 510, val loss: 1.0062915086746216
Epoch 520, training loss: 847.5280151367188 = 1.0002597570419312 + 100.0 * 8.465277671813965
Epoch 520, val loss: 1.0044927597045898
Epoch 530, training loss: 847.3023681640625 = 0.9983336925506592 + 100.0 * 8.463040351867676
Epoch 530, val loss: 1.0026986598968506
Epoch 540, training loss: 847.0784301757812 = 0.9963698983192444 + 100.0 * 8.460820198059082
Epoch 540, val loss: 1.0008726119995117
Epoch 550, training loss: 847.1477661132812 = 0.9943567514419556 + 100.0 * 8.461533546447754
Epoch 550, val loss: 0.9990585446357727
Epoch 560, training loss: 846.772705078125 = 0.9922475814819336 + 100.0 * 8.457804679870605
Epoch 560, val loss: 0.9969977736473083
Epoch 570, training loss: 846.4877319335938 = 0.9901111721992493 + 100.0 * 8.454976081848145
Epoch 570, val loss: 0.9950342774391174
Epoch 580, training loss: 846.284912109375 = 0.9880231618881226 + 100.0 * 8.45296859741211
Epoch 580, val loss: 0.9930903911590576
Epoch 590, training loss: 846.079345703125 = 0.9859119057655334 + 100.0 * 8.450934410095215
Epoch 590, val loss: 0.991121232509613
Epoch 600, training loss: 845.8984985351562 = 0.9837605357170105 + 100.0 * 8.44914722442627
Epoch 600, val loss: 0.9891371130943298
Epoch 610, training loss: 845.7285766601562 = 0.9814964532852173 + 100.0 * 8.447470664978027
Epoch 610, val loss: 0.9870028495788574
Epoch 620, training loss: 845.5731201171875 = 0.9791495203971863 + 100.0 * 8.445940017700195
Epoch 620, val loss: 0.984838604927063
Epoch 630, training loss: 845.3811645507812 = 0.9768757224082947 + 100.0 * 8.444043159484863
Epoch 630, val loss: 0.9827346801757812
Epoch 640, training loss: 845.1785888671875 = 0.9746137261390686 + 100.0 * 8.442039489746094
Epoch 640, val loss: 0.9806308150291443
Epoch 650, training loss: 844.9883422851562 = 0.9723178744316101 + 100.0 * 8.440160751342773
Epoch 650, val loss: 0.9785025119781494
Epoch 660, training loss: 844.8128662109375 = 0.9699819684028625 + 100.0 * 8.43842887878418
Epoch 660, val loss: 0.9763461351394653
Epoch 670, training loss: 844.8763427734375 = 0.9675281643867493 + 100.0 * 8.439087867736816
Epoch 670, val loss: 0.9740782976150513
Epoch 680, training loss: 844.5715942382812 = 0.9649983644485474 + 100.0 * 8.436065673828125
Epoch 680, val loss: 0.9717128276824951
Epoch 690, training loss: 844.320556640625 = 0.9625231623649597 + 100.0 * 8.43358039855957
Epoch 690, val loss: 0.9694317579269409
Epoch 700, training loss: 844.1251220703125 = 0.9600726366043091 + 100.0 * 8.431650161743164
Epoch 700, val loss: 0.9671434760093689
Epoch 710, training loss: 844.3768310546875 = 0.9575960040092468 + 100.0 * 8.434192657470703
Epoch 710, val loss: 0.9647670388221741
Epoch 720, training loss: 843.8832397460938 = 0.9548203349113464 + 100.0 * 8.42928409576416
Epoch 720, val loss: 0.9622480869293213
Epoch 730, training loss: 843.653564453125 = 0.9521436095237732 + 100.0 * 8.427014350891113
Epoch 730, val loss: 0.9597806930541992
Epoch 740, training loss: 843.504638671875 = 0.9495164155960083 + 100.0 * 8.425551414489746
Epoch 740, val loss: 0.9573478102684021
Epoch 750, training loss: 843.8380737304688 = 0.9467936754226685 + 100.0 * 8.428913116455078
Epoch 750, val loss: 0.954878032207489
Epoch 760, training loss: 843.3630981445312 = 0.9439607858657837 + 100.0 * 8.42419147491455
Epoch 760, val loss: 0.9521293044090271
Epoch 770, training loss: 843.1182250976562 = 0.9411171078681946 + 100.0 * 8.421771049499512
Epoch 770, val loss: 0.9495084285736084
Epoch 780, training loss: 842.9984741210938 = 0.9382742047309875 + 100.0 * 8.420601844787598
Epoch 780, val loss: 0.9469016790390015
Epoch 790, training loss: 842.867919921875 = 0.9354258179664612 + 100.0 * 8.41932487487793
Epoch 790, val loss: 0.9442315697669983
Epoch 800, training loss: 842.7582397460938 = 0.932514488697052 + 100.0 * 8.418257713317871
Epoch 800, val loss: 0.9415386915206909
Epoch 810, training loss: 842.6689453125 = 0.9295561909675598 + 100.0 * 8.417393684387207
Epoch 810, val loss: 0.9387837052345276
Epoch 820, training loss: 842.9346923828125 = 0.926512598991394 + 100.0 * 8.420082092285156
Epoch 820, val loss: 0.9359221458435059
Epoch 830, training loss: 842.544189453125 = 0.9233439564704895 + 100.0 * 8.416208267211914
Epoch 830, val loss: 0.9329508543014526
Epoch 840, training loss: 842.4302368164062 = 0.920201301574707 + 100.0 * 8.41510009765625
Epoch 840, val loss: 0.9300758242607117
Epoch 850, training loss: 842.296630859375 = 0.9171095490455627 + 100.0 * 8.413795471191406
Epoch 850, val loss: 0.9272205829620361
Epoch 860, training loss: 842.3277587890625 = 0.9140476584434509 + 100.0 * 8.41413688659668
Epoch 860, val loss: 0.924318253993988
Epoch 870, training loss: 842.1387939453125 = 0.9107376933097839 + 100.0 * 8.412280082702637
Epoch 870, val loss: 0.9213195443153381
Epoch 880, training loss: 842.1610717773438 = 0.9075042009353638 + 100.0 * 8.412535667419434
Epoch 880, val loss: 0.9182597398757935
Epoch 890, training loss: 841.9783325195312 = 0.9043588042259216 + 100.0 * 8.41073989868164
Epoch 890, val loss: 0.915368914604187
Epoch 900, training loss: 841.885986328125 = 0.9012576341629028 + 100.0 * 8.409847259521484
Epoch 900, val loss: 0.9125277400016785
Epoch 910, training loss: 841.917236328125 = 0.8981735110282898 + 100.0 * 8.41019058227539
Epoch 910, val loss: 0.9096422791481018
Epoch 920, training loss: 841.7847290039062 = 0.894914448261261 + 100.0 * 8.40889835357666
Epoch 920, val loss: 0.9066316485404968
Epoch 930, training loss: 841.6885986328125 = 0.8917044401168823 + 100.0 * 8.407968521118164
Epoch 930, val loss: 0.9036493301391602
Epoch 940, training loss: 841.574462890625 = 0.8885971307754517 + 100.0 * 8.406858444213867
Epoch 940, val loss: 0.9007773995399475
Epoch 950, training loss: 841.4806518554688 = 0.885522186756134 + 100.0 * 8.405951499938965
Epoch 950, val loss: 0.8979456424713135
Epoch 960, training loss: 841.4379272460938 = 0.8824627995491028 + 100.0 * 8.40555477142334
Epoch 960, val loss: 0.8950925469398499
Epoch 970, training loss: 841.396728515625 = 0.8792773485183716 + 100.0 * 8.405174255371094
Epoch 970, val loss: 0.8921216726303101
Epoch 980, training loss: 841.3306884765625 = 0.8760643005371094 + 100.0 * 8.404546737670898
Epoch 980, val loss: 0.88913494348526
Epoch 990, training loss: 841.32958984375 = 0.8729726076126099 + 100.0 * 8.404565811157227
Epoch 990, val loss: 0.886232316493988
Epoch 1000, training loss: 841.1139526367188 = 0.8697298169136047 + 100.0 * 8.40244197845459
Epoch 1000, val loss: 0.8832659125328064
Epoch 1010, training loss: 841.0655517578125 = 0.8665791153907776 + 100.0 * 8.401989936828613
Epoch 1010, val loss: 0.880328893661499
Epoch 1020, training loss: 840.974609375 = 0.863536536693573 + 100.0 * 8.401110649108887
Epoch 1020, val loss: 0.877562403678894
Epoch 1030, training loss: 840.8809204101562 = 0.8605563044548035 + 100.0 * 8.400203704833984
Epoch 1030, val loss: 0.8747791051864624
Epoch 1040, training loss: 840.8604736328125 = 0.8575705885887146 + 100.0 * 8.400029182434082
Epoch 1040, val loss: 0.8720099329948425
Epoch 1050, training loss: 840.7846069335938 = 0.8544458150863647 + 100.0 * 8.399301528930664
Epoch 1050, val loss: 0.8690778017044067
Epoch 1060, training loss: 840.68896484375 = 0.851301372051239 + 100.0 * 8.39837646484375
Epoch 1060, val loss: 0.8661881685256958
Epoch 1070, training loss: 840.6148681640625 = 0.8482643365859985 + 100.0 * 8.397665977478027
Epoch 1070, val loss: 0.863402783870697
Epoch 1080, training loss: 840.5654296875 = 0.8452807664871216 + 100.0 * 8.397201538085938
Epoch 1080, val loss: 0.8606486916542053
Epoch 1090, training loss: 840.8223876953125 = 0.8421643376350403 + 100.0 * 8.399802207946777
Epoch 1090, val loss: 0.8577395081520081
Epoch 1100, training loss: 840.4231567382812 = 0.8389958739280701 + 100.0 * 8.395841598510742
Epoch 1100, val loss: 0.854776918888092
Epoch 1110, training loss: 840.3399047851562 = 0.8359578251838684 + 100.0 * 8.395039558410645
Epoch 1110, val loss: 0.8519681692123413
Epoch 1120, training loss: 840.4650268554688 = 0.8329041600227356 + 100.0 * 8.396321296691895
Epoch 1120, val loss: 0.84916090965271
Epoch 1130, training loss: 840.2512817382812 = 0.8298524022102356 + 100.0 * 8.394214630126953
Epoch 1130, val loss: 0.8462628722190857
Epoch 1140, training loss: 840.1275024414062 = 0.8268418908119202 + 100.0 * 8.393006324768066
Epoch 1140, val loss: 0.8434998989105225
Epoch 1150, training loss: 840.0703125 = 0.8238553404808044 + 100.0 * 8.392464637756348
Epoch 1150, val loss: 0.840715765953064
Epoch 1160, training loss: 840.1384887695312 = 0.8208608627319336 + 100.0 * 8.393176078796387
Epoch 1160, val loss: 0.8378919363021851
Epoch 1170, training loss: 840.0493774414062 = 0.8176579475402832 + 100.0 * 8.392316818237305
Epoch 1170, val loss: 0.8349093794822693
Epoch 1180, training loss: 839.9559936523438 = 0.8144708871841431 + 100.0 * 8.3914155960083
Epoch 1180, val loss: 0.8319604396820068
Epoch 1190, training loss: 839.8168334960938 = 0.8114357590675354 + 100.0 * 8.390053749084473
Epoch 1190, val loss: 0.8291400074958801
Epoch 1200, training loss: 839.8696899414062 = 0.8084193468093872 + 100.0 * 8.390612602233887
Epoch 1200, val loss: 0.8263548612594604
Epoch 1210, training loss: 839.76953125 = 0.8051947951316833 + 100.0 * 8.389643669128418
Epoch 1210, val loss: 0.8232583403587341
Epoch 1220, training loss: 839.6741943359375 = 0.80196613073349 + 100.0 * 8.38872241973877
Epoch 1220, val loss: 0.8202801942825317
Epoch 1230, training loss: 839.6244506835938 = 0.7989206910133362 + 100.0 * 8.38825511932373
Epoch 1230, val loss: 0.8174096941947937
Epoch 1240, training loss: 839.5403442382812 = 0.7959276437759399 + 100.0 * 8.387444496154785
Epoch 1240, val loss: 0.8146129250526428
Epoch 1250, training loss: 839.4904174804688 = 0.7929360866546631 + 100.0 * 8.386975288391113
Epoch 1250, val loss: 0.8118082284927368
Epoch 1260, training loss: 839.9662475585938 = 0.78986656665802 + 100.0 * 8.391763687133789
Epoch 1260, val loss: 0.8089113235473633
Epoch 1270, training loss: 839.5018920898438 = 0.7865902185440063 + 100.0 * 8.387153625488281
Epoch 1270, val loss: 0.8058098554611206
Epoch 1280, training loss: 839.3805541992188 = 0.7834543585777283 + 100.0 * 8.385971069335938
Epoch 1280, val loss: 0.8028917908668518
Epoch 1290, training loss: 839.3175048828125 = 0.7804061770439148 + 100.0 * 8.385371208190918
Epoch 1290, val loss: 0.8000196814537048
Epoch 1300, training loss: 839.3038940429688 = 0.777367889881134 + 100.0 * 8.385265350341797
Epoch 1300, val loss: 0.7971558570861816
Epoch 1310, training loss: 839.2059326171875 = 0.7743058204650879 + 100.0 * 8.384316444396973
Epoch 1310, val loss: 0.7942562103271484
Epoch 1320, training loss: 839.1312255859375 = 0.7712896466255188 + 100.0 * 8.383599281311035
Epoch 1320, val loss: 0.791378915309906
Epoch 1330, training loss: 839.4638061523438 = 0.7681811451911926 + 100.0 * 8.386956214904785
Epoch 1330, val loss: 0.7884116768836975
Epoch 1340, training loss: 839.01416015625 = 0.7649536728858948 + 100.0 * 8.382492065429688
Epoch 1340, val loss: 0.7853649258613586
Epoch 1350, training loss: 839.0054931640625 = 0.7618945837020874 + 100.0 * 8.38243579864502
Epoch 1350, val loss: 0.7824869155883789
Epoch 1360, training loss: 838.9055786132812 = 0.7589195370674133 + 100.0 * 8.38146686553955
Epoch 1360, val loss: 0.7796893119812012
Epoch 1370, training loss: 838.9525146484375 = 0.7559836506843567 + 100.0 * 8.381965637207031
Epoch 1370, val loss: 0.7768591642379761
Epoch 1380, training loss: 838.9017944335938 = 0.7527220845222473 + 100.0 * 8.381490707397461
Epoch 1380, val loss: 0.773748517036438
Epoch 1390, training loss: 838.9642333984375 = 0.749427855014801 + 100.0 * 8.382147789001465
Epoch 1390, val loss: 0.7706241011619568
Epoch 1400, training loss: 838.7177734375 = 0.7463818788528442 + 100.0 * 8.379714012145996
Epoch 1400, val loss: 0.7677990794181824
Epoch 1410, training loss: 838.6878662109375 = 0.74346524477005 + 100.0 * 8.379444122314453
Epoch 1410, val loss: 0.7650488615036011
Epoch 1420, training loss: 838.6257934570312 = 0.7405994534492493 + 100.0 * 8.378851890563965
Epoch 1420, val loss: 0.7623128890991211
Epoch 1430, training loss: 838.9744873046875 = 0.7376648187637329 + 100.0 * 8.382368087768555
Epoch 1430, val loss: 0.7595361471176147
Epoch 1440, training loss: 838.7160034179688 = 0.7345238924026489 + 100.0 * 8.379815101623535
Epoch 1440, val loss: 0.756514847278595
Epoch 1450, training loss: 838.5277099609375 = 0.7314987182617188 + 100.0 * 8.377962112426758
Epoch 1450, val loss: 0.7536487579345703
Epoch 1460, training loss: 838.4486083984375 = 0.7285966277122498 + 100.0 * 8.37720012664795
Epoch 1460, val loss: 0.7509018778800964
Epoch 1470, training loss: 838.4518432617188 = 0.7257267832756042 + 100.0 * 8.3772611618042
Epoch 1470, val loss: 0.7481614351272583
Epoch 1480, training loss: 838.5090942382812 = 0.7227641940116882 + 100.0 * 8.377862930297852
Epoch 1480, val loss: 0.7453221678733826
Epoch 1490, training loss: 838.3433837890625 = 0.7197760343551636 + 100.0 * 8.376235961914062
Epoch 1490, val loss: 0.7425099611282349
Epoch 1500, training loss: 838.274169921875 = 0.7169125080108643 + 100.0 * 8.375572204589844
Epoch 1500, val loss: 0.7397671341896057
Epoch 1510, training loss: 838.2191162109375 = 0.7140640616416931 + 100.0 * 8.37505054473877
Epoch 1510, val loss: 0.7370669841766357
Epoch 1520, training loss: 838.2710571289062 = 0.7112310528755188 + 100.0 * 8.375597953796387
Epoch 1520, val loss: 0.7343446016311646
Epoch 1530, training loss: 838.3863525390625 = 0.7081769108772278 + 100.0 * 8.376781463623047
Epoch 1530, val loss: 0.7314024567604065
Epoch 1540, training loss: 838.15625 = 0.7050732970237732 + 100.0 * 8.37451171875
Epoch 1540, val loss: 0.7284680008888245
Epoch 1550, training loss: 838.1124877929688 = 0.7022188901901245 + 100.0 * 8.374102592468262
Epoch 1550, val loss: 0.7258053421974182
Epoch 1560, training loss: 838.0225830078125 = 0.6994588375091553 + 100.0 * 8.373230934143066
Epoch 1560, val loss: 0.72318434715271
Epoch 1570, training loss: 837.9874267578125 = 0.6967505216598511 + 100.0 * 8.372906684875488
Epoch 1570, val loss: 0.7205802202224731
Epoch 1580, training loss: 838.2662963867188 = 0.6940227150917053 + 100.0 * 8.375722885131836
Epoch 1580, val loss: 0.7179543375968933
Epoch 1590, training loss: 838.1526489257812 = 0.6909876465797424 + 100.0 * 8.374616622924805
Epoch 1590, val loss: 0.7151234745979309
Epoch 1600, training loss: 837.8958740234375 = 0.6881116628646851 + 100.0 * 8.372077941894531
Epoch 1600, val loss: 0.7123711109161377
Epoch 1610, training loss: 837.8602905273438 = 0.6853995323181152 + 100.0 * 8.371748924255371
Epoch 1610, val loss: 0.7098068594932556
Epoch 1620, training loss: 837.8031616210938 = 0.6827301383018494 + 100.0 * 8.371204376220703
Epoch 1620, val loss: 0.7072811126708984
Epoch 1630, training loss: 837.8981323242188 = 0.6800769567489624 + 100.0 * 8.372180938720703
Epoch 1630, val loss: 0.7047766447067261
Epoch 1640, training loss: 837.7762451171875 = 0.6772549748420715 + 100.0 * 8.370989799499512
Epoch 1640, val loss: 0.7020406126976013
Epoch 1650, training loss: 838.0452880859375 = 0.674429178237915 + 100.0 * 8.373708724975586
Epoch 1650, val loss: 0.6993539929389954
Epoch 1660, training loss: 837.73681640625 = 0.6715970635414124 + 100.0 * 8.370652198791504
Epoch 1660, val loss: 0.6967006325721741
Epoch 1670, training loss: 837.6240844726562 = 0.668959379196167 + 100.0 * 8.369551658630371
Epoch 1670, val loss: 0.694190502166748
Epoch 1680, training loss: 837.617431640625 = 0.6664149165153503 + 100.0 * 8.36950969696045
Epoch 1680, val loss: 0.6917850375175476
Epoch 1690, training loss: 837.5584716796875 = 0.6639049649238586 + 100.0 * 8.368946075439453
Epoch 1690, val loss: 0.6894137859344482
Epoch 1700, training loss: 837.5433959960938 = 0.6614001393318176 + 100.0 * 8.368820190429688
Epoch 1700, val loss: 0.6870492696762085
Epoch 1710, training loss: 837.7899169921875 = 0.6588571071624756 + 100.0 * 8.371310234069824
Epoch 1710, val loss: 0.6846446990966797
Epoch 1720, training loss: 837.4552001953125 = 0.6561184525489807 + 100.0 * 8.367990493774414
Epoch 1720, val loss: 0.6820256114006042
Epoch 1730, training loss: 837.4318237304688 = 0.6535366773605347 + 100.0 * 8.367782592773438
Epoch 1730, val loss: 0.6795865893363953
Epoch 1740, training loss: 837.4199829101562 = 0.6510615348815918 + 100.0 * 8.36768913269043
Epoch 1740, val loss: 0.6772641539573669
Epoch 1750, training loss: 837.5277099609375 = 0.6486180424690247 + 100.0 * 8.368790626525879
Epoch 1750, val loss: 0.674946665763855
Epoch 1760, training loss: 837.3860473632812 = 0.6460770964622498 + 100.0 * 8.367400169372559
Epoch 1760, val loss: 0.6725427508354187
Epoch 1770, training loss: 837.423095703125 = 0.643609881401062 + 100.0 * 8.36779499053955
Epoch 1770, val loss: 0.6701703667640686
Epoch 1780, training loss: 837.3690185546875 = 0.6410883069038391 + 100.0 * 8.367279052734375
Epoch 1780, val loss: 0.6678059101104736
Epoch 1790, training loss: 837.2841796875 = 0.6386502981185913 + 100.0 * 8.366455078125
Epoch 1790, val loss: 0.665550172328949
Epoch 1800, training loss: 837.2105102539062 = 0.6363251209259033 + 100.0 * 8.365741729736328
Epoch 1800, val loss: 0.6633337140083313
Epoch 1810, training loss: 837.1748046875 = 0.6340238451957703 + 100.0 * 8.365407943725586
Epoch 1810, val loss: 0.6611813306808472
Epoch 1820, training loss: 837.314208984375 = 0.631727933883667 + 100.0 * 8.366825103759766
Epoch 1820, val loss: 0.6590356230735779
Epoch 1830, training loss: 837.203857421875 = 0.6292216777801514 + 100.0 * 8.36574649810791
Epoch 1830, val loss: 0.6566272974014282
Epoch 1840, training loss: 837.0928955078125 = 0.6267693042755127 + 100.0 * 8.36466121673584
Epoch 1840, val loss: 0.6543428897857666
Epoch 1850, training loss: 837.0536499023438 = 0.6244930624961853 + 100.0 * 8.364291191101074
Epoch 1850, val loss: 0.6522077322006226
Epoch 1860, training loss: 837.0201416015625 = 0.6222879886627197 + 100.0 * 8.363978385925293
Epoch 1860, val loss: 0.650149941444397
Epoch 1870, training loss: 837.0639038085938 = 0.6201009154319763 + 100.0 * 8.3644380569458
Epoch 1870, val loss: 0.6481068730354309
Epoch 1880, training loss: 837.181640625 = 0.6178227663040161 + 100.0 * 8.365638732910156
Epoch 1880, val loss: 0.6459628939628601
Epoch 1890, training loss: 837.0501098632812 = 0.6155251264572144 + 100.0 * 8.36434555053711
Epoch 1890, val loss: 0.6437994241714478
Epoch 1900, training loss: 836.9006958007812 = 0.6133098006248474 + 100.0 * 8.362874031066895
Epoch 1900, val loss: 0.6417086124420166
Epoch 1910, training loss: 836.8777465820312 = 0.6111921668052673 + 100.0 * 8.362665176391602
Epoch 1910, val loss: 0.6397307515144348
Epoch 1920, training loss: 836.8792724609375 = 0.6091123819351196 + 100.0 * 8.362701416015625
Epoch 1920, val loss: 0.6377667188644409
Epoch 1930, training loss: 837.0265502929688 = 0.6070066094398499 + 100.0 * 8.364195823669434
Epoch 1930, val loss: 0.635776937007904
Epoch 1940, training loss: 836.9053344726562 = 0.6048069596290588 + 100.0 * 8.363005638122559
Epoch 1940, val loss: 0.6337624788284302
Epoch 1950, training loss: 836.9402465820312 = 0.602584183216095 + 100.0 * 8.36337661743164
Epoch 1950, val loss: 0.6316524744033813
Epoch 1960, training loss: 836.7805786132812 = 0.6004366278648376 + 100.0 * 8.361801147460938
Epoch 1960, val loss: 0.6296956539154053
Epoch 1970, training loss: 836.7256469726562 = 0.598420262336731 + 100.0 * 8.361271858215332
Epoch 1970, val loss: 0.6277880668640137
Epoch 1980, training loss: 836.6934814453125 = 0.596461832523346 + 100.0 * 8.360970497131348
Epoch 1980, val loss: 0.625988245010376
Epoch 1990, training loss: 836.6798706054688 = 0.5945212841033936 + 100.0 * 8.36085319519043
Epoch 1990, val loss: 0.6241638660430908
Epoch 2000, training loss: 836.8983764648438 = 0.5925612449645996 + 100.0 * 8.363058090209961
Epoch 2000, val loss: 0.6223122477531433
Epoch 2010, training loss: 836.6475219726562 = 0.5904496908187866 + 100.0 * 8.360570907592773
Epoch 2010, val loss: 0.6203738451004028
Epoch 2020, training loss: 836.6035766601562 = 0.588436484336853 + 100.0 * 8.360151290893555
Epoch 2020, val loss: 0.6185060739517212
Epoch 2030, training loss: 836.5825805664062 = 0.586523711681366 + 100.0 * 8.359960556030273
Epoch 2030, val loss: 0.6167133450508118
Epoch 2040, training loss: 836.5599365234375 = 0.5846610069274902 + 100.0 * 8.359752655029297
Epoch 2040, val loss: 0.6149967908859253
Epoch 2050, training loss: 836.6285400390625 = 0.58281010389328 + 100.0 * 8.360457420349121
Epoch 2050, val loss: 0.6132532358169556
Epoch 2060, training loss: 836.627685546875 = 0.5808590054512024 + 100.0 * 8.360467910766602
Epoch 2060, val loss: 0.6114657521247864
Epoch 2070, training loss: 836.5421752929688 = 0.5788898468017578 + 100.0 * 8.35963249206543
Epoch 2070, val loss: 0.6096192002296448
Epoch 2080, training loss: 836.533203125 = 0.5770223140716553 + 100.0 * 8.359561920166016
Epoch 2080, val loss: 0.6079205870628357
Epoch 2090, training loss: 836.5228881835938 = 0.5752112865447998 + 100.0 * 8.359477043151855
Epoch 2090, val loss: 0.6062465906143188
Epoch 2100, training loss: 836.5548095703125 = 0.5734047889709473 + 100.0 * 8.359813690185547
Epoch 2100, val loss: 0.6045907735824585
Epoch 2110, training loss: 836.4686889648438 = 0.5716177821159363 + 100.0 * 8.358970642089844
Epoch 2110, val loss: 0.6029164791107178
Epoch 2120, training loss: 836.44140625 = 0.5698544979095459 + 100.0 * 8.358716011047363
Epoch 2120, val loss: 0.6013103723526001
Epoch 2130, training loss: 836.4515380859375 = 0.5681116580963135 + 100.0 * 8.358834266662598
Epoch 2130, val loss: 0.5997136235237122
Epoch 2140, training loss: 836.3923950195312 = 0.5663845539093018 + 100.0 * 8.358260154724121
Epoch 2140, val loss: 0.5980955958366394
Epoch 2150, training loss: 836.5142822265625 = 0.564674973487854 + 100.0 * 8.359496116638184
Epoch 2150, val loss: 0.596515953540802
Epoch 2160, training loss: 836.3285522460938 = 0.5629246830940247 + 100.0 * 8.357656478881836
Epoch 2160, val loss: 0.5949153304100037
Epoch 2170, training loss: 836.376708984375 = 0.5612533688545227 + 100.0 * 8.358154296875
Epoch 2170, val loss: 0.5933589339256287
Epoch 2180, training loss: 836.4706420898438 = 0.5595675110816956 + 100.0 * 8.359110832214355
Epoch 2180, val loss: 0.5917906761169434
Epoch 2190, training loss: 836.3123779296875 = 0.5578790307044983 + 100.0 * 8.357544898986816
Epoch 2190, val loss: 0.5902891755104065
Epoch 2200, training loss: 836.25830078125 = 0.5563088059425354 + 100.0 * 8.357019424438477
Epoch 2200, val loss: 0.5888266563415527
Epoch 2210, training loss: 836.4352416992188 = 0.5547387599945068 + 100.0 * 8.358804702758789
Epoch 2210, val loss: 0.5873785614967346
Epoch 2220, training loss: 836.234375 = 0.5530585646629333 + 100.0 * 8.356813430786133
Epoch 2220, val loss: 0.585851788520813
Epoch 2230, training loss: 836.1681518554688 = 0.551433265209198 + 100.0 * 8.35616683959961
Epoch 2230, val loss: 0.5843648314476013
Epoch 2240, training loss: 836.1511840820312 = 0.5499221086502075 + 100.0 * 8.356012344360352
Epoch 2240, val loss: 0.5830127596855164
Epoch 2250, training loss: 836.1298828125 = 0.5484579205513 + 100.0 * 8.355813980102539
Epoch 2250, val loss: 0.581660807132721
Epoch 2260, training loss: 836.1546020507812 = 0.5470144152641296 + 100.0 * 8.35607624053955
Epoch 2260, val loss: 0.5803735256195068
Epoch 2270, training loss: 836.331787109375 = 0.5455430746078491 + 100.0 * 8.35786247253418
Epoch 2270, val loss: 0.5790232419967651
Epoch 2280, training loss: 836.1822509765625 = 0.5440309643745422 + 100.0 * 8.356382369995117
Epoch 2280, val loss: 0.5776181221008301
Epoch 2290, training loss: 836.11572265625 = 0.5425872206687927 + 100.0 * 8.355731010437012
Epoch 2290, val loss: 0.5763228535652161
Epoch 2300, training loss: 836.0932006835938 = 0.5411741137504578 + 100.0 * 8.355520248413086
Epoch 2300, val loss: 0.5750362873077393
Epoch 2310, training loss: 836.135498046875 = 0.5397724509239197 + 100.0 * 8.35595703125
Epoch 2310, val loss: 0.5737742185592651
Epoch 2320, training loss: 836.08349609375 = 0.5383552312850952 + 100.0 * 8.355451583862305
Epoch 2320, val loss: 0.5725066661834717
Epoch 2330, training loss: 836.1133422851562 = 0.5369680523872375 + 100.0 * 8.35576343536377
Epoch 2330, val loss: 0.5712344646453857
Epoch 2340, training loss: 836.4138793945312 = 0.5355489253997803 + 100.0 * 8.358783721923828
Epoch 2340, val loss: 0.5699241757392883
Epoch 2350, training loss: 836.0816650390625 = 0.5340803861618042 + 100.0 * 8.355476379394531
Epoch 2350, val loss: 0.5686073899269104
Epoch 2360, training loss: 835.9777221679688 = 0.5327282547950745 + 100.0 * 8.354450225830078
Epoch 2360, val loss: 0.5673954486846924
Epoch 2370, training loss: 835.9208374023438 = 0.5314645171165466 + 100.0 * 8.353893280029297
Epoch 2370, val loss: 0.5662854313850403
Epoch 2380, training loss: 835.9043579101562 = 0.5302467942237854 + 100.0 * 8.353740692138672
Epoch 2380, val loss: 0.5651659369468689
Epoch 2390, training loss: 835.907958984375 = 0.5290309190750122 + 100.0 * 8.353789329528809
Epoch 2390, val loss: 0.5640950202941895
Epoch 2400, training loss: 836.3274536132812 = 0.5277925729751587 + 100.0 * 8.357996940612793
Epoch 2400, val loss: 0.5629377961158752
Epoch 2410, training loss: 835.9398803710938 = 0.5264331102371216 + 100.0 * 8.354134559631348
Epoch 2410, val loss: 0.5617870688438416
Epoch 2420, training loss: 835.8641357421875 = 0.5251747369766235 + 100.0 * 8.353389739990234
Epoch 2420, val loss: 0.5606455206871033
Epoch 2430, training loss: 835.830810546875 = 0.5239989757537842 + 100.0 * 8.353068351745605
Epoch 2430, val loss: 0.5596029162406921
Epoch 2440, training loss: 835.8212890625 = 0.5228468179702759 + 100.0 * 8.352984428405762
Epoch 2440, val loss: 0.5585910081863403
Epoch 2450, training loss: 836.1229248046875 = 0.5216865539550781 + 100.0 * 8.356012344360352
Epoch 2450, val loss: 0.5575631260871887
Epoch 2460, training loss: 835.7987670898438 = 0.520441472530365 + 100.0 * 8.352783203125
Epoch 2460, val loss: 0.5564684867858887
Epoch 2470, training loss: 835.7744140625 = 0.5192688703536987 + 100.0 * 8.352551460266113
Epoch 2470, val loss: 0.5554002523422241
Epoch 2480, training loss: 835.7462768554688 = 0.5181600451469421 + 100.0 * 8.352280616760254
Epoch 2480, val loss: 0.5544662475585938
Epoch 2490, training loss: 835.7266235351562 = 0.5170767903327942 + 100.0 * 8.352095603942871
Epoch 2490, val loss: 0.5534908771514893
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7732115677321156
0.818445265521988
=== training gcn model ===
Epoch 0, training loss: 1059.3212890625 = 1.0949701070785522 + 100.0 * 10.582262992858887
Epoch 0, val loss: 1.0950249433517456
Epoch 10, training loss: 1059.2763671875 = 1.0911662578582764 + 100.0 * 10.581851959228516
Epoch 10, val loss: 1.0912153720855713
Epoch 20, training loss: 1059.1036376953125 = 1.0871570110321045 + 100.0 * 10.580164909362793
Epoch 20, val loss: 1.087200403213501
Epoch 30, training loss: 1058.37646484375 = 1.0827901363372803 + 100.0 * 10.57293701171875
Epoch 30, val loss: 1.0828320980072021
Epoch 40, training loss: 1055.6336669921875 = 1.0779649019241333 + 100.0 * 10.54555606842041
Epoch 40, val loss: 1.0780162811279297
Epoch 50, training loss: 1047.5509033203125 = 1.0725653171539307 + 100.0 * 10.464783668518066
Epoch 50, val loss: 1.0726720094680786
Epoch 60, training loss: 1027.8604736328125 = 1.067009687423706 + 100.0 * 10.267934799194336
Epoch 60, val loss: 1.0672566890716553
Epoch 70, training loss: 989.213623046875 = 1.0616945028305054 + 100.0 * 9.881519317626953
Epoch 70, val loss: 1.0620887279510498
Epoch 80, training loss: 959.890869140625 = 1.0567392110824585 + 100.0 * 9.588340759277344
Epoch 80, val loss: 1.0572257041931152
Epoch 90, training loss: 946.2980346679688 = 1.052483320236206 + 100.0 * 9.452455520629883
Epoch 90, val loss: 1.0531033277511597
Epoch 100, training loss: 933.40234375 = 1.0493344068527222 + 100.0 * 9.323530197143555
Epoch 100, val loss: 1.0501909255981445
Epoch 110, training loss: 918.0570678710938 = 1.0474417209625244 + 100.0 * 9.170096397399902
Epoch 110, val loss: 1.048453450202942
Epoch 120, training loss: 905.3715209960938 = 1.046446442604065 + 100.0 * 9.043251037597656
Epoch 120, val loss: 1.0475980043411255
Epoch 130, training loss: 894.7297973632812 = 1.0458756685256958 + 100.0 * 8.93683910369873
Epoch 130, val loss: 1.0471254587173462
Epoch 140, training loss: 888.1121826171875 = 1.0456281900405884 + 100.0 * 8.870665550231934
Epoch 140, val loss: 1.04690682888031
Epoch 150, training loss: 884.726318359375 = 1.0450550317764282 + 100.0 * 8.836812973022461
Epoch 150, val loss: 1.0462766885757446
Epoch 160, training loss: 881.0409545898438 = 1.0442229509353638 + 100.0 * 8.799966812133789
Epoch 160, val loss: 1.0453975200653076
Epoch 170, training loss: 877.8952026367188 = 1.043622612953186 + 100.0 * 8.768515586853027
Epoch 170, val loss: 1.0447845458984375
Epoch 180, training loss: 875.7554931640625 = 1.0432181358337402 + 100.0 * 8.747122764587402
Epoch 180, val loss: 1.0443683862686157
Epoch 190, training loss: 873.7359008789062 = 1.0426973104476929 + 100.0 * 8.72693157196045
Epoch 190, val loss: 1.0438584089279175
Epoch 200, training loss: 871.6736450195312 = 1.0422096252441406 + 100.0 * 8.706314086914062
Epoch 200, val loss: 1.0433995723724365
Epoch 210, training loss: 869.6842651367188 = 1.0417633056640625 + 100.0 * 8.68642520904541
Epoch 210, val loss: 1.0429511070251465
Epoch 220, training loss: 868.135986328125 = 1.041260838508606 + 100.0 * 8.670947074890137
Epoch 220, val loss: 1.0424714088439941
Epoch 230, training loss: 866.5886840820312 = 1.0407248735427856 + 100.0 * 8.655479431152344
Epoch 230, val loss: 1.0419518947601318
Epoch 240, training loss: 865.198486328125 = 1.0401731729507446 + 100.0 * 8.641583442687988
Epoch 240, val loss: 1.0414202213287354
Epoch 250, training loss: 863.9456176757812 = 1.039617657661438 + 100.0 * 8.629059791564941
Epoch 250, val loss: 1.0408916473388672
Epoch 260, training loss: 863.0090942382812 = 1.0390490293502808 + 100.0 * 8.61970043182373
Epoch 260, val loss: 1.0403449535369873
Epoch 270, training loss: 861.91796875 = 1.0384740829467773 + 100.0 * 8.608795166015625
Epoch 270, val loss: 1.0397945642471313
Epoch 280, training loss: 861.05615234375 = 1.037912368774414 + 100.0 * 8.60018253326416
Epoch 280, val loss: 1.0392556190490723
Epoch 290, training loss: 860.4501342773438 = 1.0373796224594116 + 100.0 * 8.594127655029297
Epoch 290, val loss: 1.0387176275253296
Epoch 300, training loss: 859.4793701171875 = 1.036781907081604 + 100.0 * 8.584425926208496
Epoch 300, val loss: 1.0381417274475098
Epoch 310, training loss: 858.6013793945312 = 1.036205768585205 + 100.0 * 8.575652122497559
Epoch 310, val loss: 1.0375980138778687
Epoch 320, training loss: 857.8936767578125 = 1.0356382131576538 + 100.0 * 8.568580627441406
Epoch 320, val loss: 1.0370460748672485
Epoch 330, training loss: 857.1817626953125 = 1.0350643396377563 + 100.0 * 8.561467170715332
Epoch 330, val loss: 1.0364822149276733
Epoch 340, training loss: 856.8027954101562 = 1.0344586372375488 + 100.0 * 8.557682991027832
Epoch 340, val loss: 1.0358920097351074
Epoch 350, training loss: 856.1459350585938 = 1.0337631702423096 + 100.0 * 8.551121711730957
Epoch 350, val loss: 1.0351983308792114
Epoch 360, training loss: 855.432861328125 = 1.0330827236175537 + 100.0 * 8.543997764587402
Epoch 360, val loss: 1.0345351696014404
Epoch 370, training loss: 854.8438720703125 = 1.0323913097381592 + 100.0 * 8.538114547729492
Epoch 370, val loss: 1.033856749534607
Epoch 380, training loss: 854.4283447265625 = 1.0315957069396973 + 100.0 * 8.533967018127441
Epoch 380, val loss: 1.0330862998962402
Epoch 390, training loss: 853.9661865234375 = 1.0307459831237793 + 100.0 * 8.529354095458984
Epoch 390, val loss: 1.0322562456130981
Epoch 400, training loss: 853.412841796875 = 1.0299371480941772 + 100.0 * 8.523829460144043
Epoch 400, val loss: 1.0314565896987915
Epoch 410, training loss: 852.894775390625 = 1.0290982723236084 + 100.0 * 8.518656730651855
Epoch 410, val loss: 1.0306340456008911
Epoch 420, training loss: 852.8116455078125 = 1.0282111167907715 + 100.0 * 8.517834663391113
Epoch 420, val loss: 1.029783010482788
Epoch 430, training loss: 851.9986572265625 = 1.027245283126831 + 100.0 * 8.509714126586914
Epoch 430, val loss: 1.028809666633606
Epoch 440, training loss: 851.5868530273438 = 1.0262831449508667 + 100.0 * 8.505605697631836
Epoch 440, val loss: 1.027877926826477
Epoch 450, training loss: 851.1675415039062 = 1.0253275632858276 + 100.0 * 8.501421928405762
Epoch 450, val loss: 1.0269492864608765
Epoch 460, training loss: 850.7742919921875 = 1.0243507623672485 + 100.0 * 8.497499465942383
Epoch 460, val loss: 1.0259929895401
Epoch 470, training loss: 850.743408203125 = 1.0233005285263062 + 100.0 * 8.497200965881348
Epoch 470, val loss: 1.0249799489974976
Epoch 480, training loss: 850.2531127929688 = 1.022181749343872 + 100.0 * 8.4923095703125
Epoch 480, val loss: 1.023909330368042
Epoch 490, training loss: 849.71630859375 = 1.0210881233215332 + 100.0 * 8.48695182800293
Epoch 490, val loss: 1.0228224992752075
Epoch 500, training loss: 849.4224243164062 = 1.0200138092041016 + 100.0 * 8.484024047851562
Epoch 500, val loss: 1.0217621326446533
Epoch 510, training loss: 849.0509643554688 = 1.0188875198364258 + 100.0 * 8.480320930480957
Epoch 510, val loss: 1.0206753015518188
Epoch 520, training loss: 848.967529296875 = 1.017756700515747 + 100.0 * 8.479497909545898
Epoch 520, val loss: 1.0195589065551758
Epoch 530, training loss: 848.5255737304688 = 1.0165008306503296 + 100.0 * 8.475090980529785
Epoch 530, val loss: 1.0184135437011719
Epoch 540, training loss: 848.1715698242188 = 1.0153369903564453 + 100.0 * 8.471562385559082
Epoch 540, val loss: 1.0172531604766846
Epoch 550, training loss: 847.9754028320312 = 1.0141289234161377 + 100.0 * 8.469613075256348
Epoch 550, val loss: 1.016111135482788
Epoch 560, training loss: 847.7645874023438 = 1.0128134489059448 + 100.0 * 8.467517852783203
Epoch 560, val loss: 1.0148329734802246
Epoch 570, training loss: 847.3692626953125 = 1.0115123987197876 + 100.0 * 8.463577270507812
Epoch 570, val loss: 1.0135964155197144
Epoch 580, training loss: 847.0450439453125 = 1.0102341175079346 + 100.0 * 8.460348129272461
Epoch 580, val loss: 1.0123755931854248
Epoch 590, training loss: 846.7877807617188 = 1.0089010000228882 + 100.0 * 8.457788467407227
Epoch 590, val loss: 1.0111093521118164
Epoch 600, training loss: 846.6849975585938 = 1.0075181722640991 + 100.0 * 8.456774711608887
Epoch 600, val loss: 1.0098307132720947
Epoch 610, training loss: 846.8026123046875 = 1.0060198307037354 + 100.0 * 8.457965850830078
Epoch 610, val loss: 1.0082954168319702
Epoch 620, training loss: 846.2582397460938 = 1.0044668912887573 + 100.0 * 8.452537536621094
Epoch 620, val loss: 1.0068747997283936
Epoch 630, training loss: 845.9774780273438 = 1.0030016899108887 + 100.0 * 8.449745178222656
Epoch 630, val loss: 1.0055053234100342
Epoch 640, training loss: 845.7921142578125 = 1.0014930963516235 + 100.0 * 8.447906494140625
Epoch 640, val loss: 1.0040664672851562
Epoch 650, training loss: 845.6051025390625 = 0.999925434589386 + 100.0 * 8.446051597595215
Epoch 650, val loss: 1.0025805234909058
Epoch 660, training loss: 845.43310546875 = 0.9983303546905518 + 100.0 * 8.444347381591797
Epoch 660, val loss: 1.0010744333267212
Epoch 670, training loss: 845.4047241210938 = 0.9967001080513 + 100.0 * 8.444080352783203
Epoch 670, val loss: 0.9995355010032654
Epoch 680, training loss: 845.4346313476562 = 0.9949135780334473 + 100.0 * 8.44439697265625
Epoch 680, val loss: 0.9978176951408386
Epoch 690, training loss: 845.0029296875 = 0.9931432604789734 + 100.0 * 8.44009780883789
Epoch 690, val loss: 0.9961383938789368
Epoch 700, training loss: 844.8390502929688 = 0.9914193749427795 + 100.0 * 8.4384765625
Epoch 700, val loss: 0.994526743888855
Epoch 710, training loss: 844.7173461914062 = 0.9896572828292847 + 100.0 * 8.437276840209961
Epoch 710, val loss: 0.9928589463233948
Epoch 720, training loss: 844.9039306640625 = 0.9878174066543579 + 100.0 * 8.43916130065918
Epoch 720, val loss: 0.9911373853683472
Epoch 730, training loss: 844.593994140625 = 0.9859023690223694 + 100.0 * 8.436080932617188
Epoch 730, val loss: 0.9892417788505554
Epoch 740, training loss: 844.4264526367188 = 0.9839708805084229 + 100.0 * 8.434425354003906
Epoch 740, val loss: 0.9874844551086426
Epoch 750, training loss: 844.2626953125 = 0.9821047782897949 + 100.0 * 8.432806015014648
Epoch 750, val loss: 0.985668420791626
Epoch 760, training loss: 844.1383056640625 = 0.9801433682441711 + 100.0 * 8.431581497192383
Epoch 760, val loss: 0.9838391542434692
Epoch 770, training loss: 844.5177612304688 = 0.9781404733657837 + 100.0 * 8.435396194458008
Epoch 770, val loss: 0.9819431900978088
Epoch 780, training loss: 843.9683227539062 = 0.976003110408783 + 100.0 * 8.429923057556152
Epoch 780, val loss: 0.9798905253410339
Epoch 790, training loss: 843.8826293945312 = 0.9739601612091064 + 100.0 * 8.429086685180664
Epoch 790, val loss: 0.9779311418533325
Epoch 800, training loss: 843.735107421875 = 0.9718858599662781 + 100.0 * 8.427632331848145
Epoch 800, val loss: 0.9759707450866699
Epoch 810, training loss: 843.868408203125 = 0.9697511792182922 + 100.0 * 8.428986549377441
Epoch 810, val loss: 0.9739827513694763
Epoch 820, training loss: 843.7379760742188 = 0.9674574732780457 + 100.0 * 8.427704811096191
Epoch 820, val loss: 0.9717177152633667
Epoch 830, training loss: 843.4717407226562 = 0.9651994109153748 + 100.0 * 8.425065040588379
Epoch 830, val loss: 0.9696476459503174
Epoch 840, training loss: 843.36572265625 = 0.9630516767501831 + 100.0 * 8.424026489257812
Epoch 840, val loss: 0.9675943851470947
Epoch 850, training loss: 843.2293701171875 = 0.9608562588691711 + 100.0 * 8.422684669494629
Epoch 850, val loss: 0.9654761552810669
Epoch 860, training loss: 843.12548828125 = 0.9586132168769836 + 100.0 * 8.421669006347656
Epoch 860, val loss: 0.9633386731147766
Epoch 870, training loss: 843.0597534179688 = 0.9563422799110413 + 100.0 * 8.42103385925293
Epoch 870, val loss: 0.9611684679985046
Epoch 880, training loss: 843.2122192382812 = 0.9539636969566345 + 100.0 * 8.422582626342773
Epoch 880, val loss: 0.9588474035263062
Epoch 890, training loss: 842.9393310546875 = 0.9514209032058716 + 100.0 * 8.419878959655762
Epoch 890, val loss: 0.9564940333366394
Epoch 900, training loss: 842.7929077148438 = 0.9490835070610046 + 100.0 * 8.418437957763672
Epoch 900, val loss: 0.954254150390625
Epoch 910, training loss: 842.6942138671875 = 0.9467388987541199 + 100.0 * 8.417474746704102
Epoch 910, val loss: 0.9519957900047302
Epoch 920, training loss: 842.588623046875 = 0.9443120360374451 + 100.0 * 8.41644287109375
Epoch 920, val loss: 0.9497005343437195
Epoch 930, training loss: 843.0205078125 = 0.9418297410011292 + 100.0 * 8.42078685760498
Epoch 930, val loss: 0.9472076892852783
Epoch 940, training loss: 842.470458984375 = 0.9390711784362793 + 100.0 * 8.415313720703125
Epoch 940, val loss: 0.9447331428527832
Epoch 950, training loss: 842.3499145507812 = 0.9365652203559875 + 100.0 * 8.414133071899414
Epoch 950, val loss: 0.9423298835754395
Epoch 960, training loss: 842.2416381835938 = 0.9340385794639587 + 100.0 * 8.413076400756836
Epoch 960, val loss: 0.9399135112762451
Epoch 970, training loss: 842.1432495117188 = 0.9314793348312378 + 100.0 * 8.412117958068848
Epoch 970, val loss: 0.9374856352806091
Epoch 980, training loss: 842.2493286132812 = 0.9288859963417053 + 100.0 * 8.413204193115234
Epoch 980, val loss: 0.935001790523529
Epoch 990, training loss: 842.0612182617188 = 0.926108181476593 + 100.0 * 8.411351203918457
Epoch 990, val loss: 0.9323161244392395
Epoch 1000, training loss: 842.1519775390625 = 0.9233976006507874 + 100.0 * 8.412285804748535
Epoch 1000, val loss: 0.9297372698783875
Epoch 1010, training loss: 841.8234252929688 = 0.9205764532089233 + 100.0 * 8.409028053283691
Epoch 1010, val loss: 0.9270569086074829
Epoch 1020, training loss: 841.7605590820312 = 0.9178282618522644 + 100.0 * 8.408427238464355
Epoch 1020, val loss: 0.9244530200958252
Epoch 1030, training loss: 841.6937255859375 = 0.9151190519332886 + 100.0 * 8.40778636932373
Epoch 1030, val loss: 0.9218367338180542
Epoch 1040, training loss: 841.5843505859375 = 0.912328839302063 + 100.0 * 8.406720161437988
Epoch 1040, val loss: 0.9192031621932983
Epoch 1050, training loss: 841.6348876953125 = 0.9095178246498108 + 100.0 * 8.40725326538086
Epoch 1050, val loss: 0.9165106415748596
Epoch 1060, training loss: 841.6002197265625 = 0.9065849184989929 + 100.0 * 8.406936645507812
Epoch 1060, val loss: 0.9136452674865723
Epoch 1070, training loss: 841.5090942382812 = 0.9035504460334778 + 100.0 * 8.406055450439453
Epoch 1070, val loss: 0.910766065120697
Epoch 1080, training loss: 841.3694458007812 = 0.9005764722824097 + 100.0 * 8.404688835144043
Epoch 1080, val loss: 0.9080067873001099
Epoch 1090, training loss: 841.225830078125 = 0.897707998752594 + 100.0 * 8.403281211853027
Epoch 1090, val loss: 0.9052380323410034
Epoch 1100, training loss: 841.193603515625 = 0.8947911858558655 + 100.0 * 8.40298843383789
Epoch 1100, val loss: 0.9024375677108765
Epoch 1110, training loss: 841.4000244140625 = 0.8918291330337524 + 100.0 * 8.405081748962402
Epoch 1110, val loss: 0.8995427489280701
Epoch 1120, training loss: 841.1072387695312 = 0.8886674642562866 + 100.0 * 8.402185440063477
Epoch 1120, val loss: 0.8966073393821716
Epoch 1130, training loss: 840.9453735351562 = 0.8856780529022217 + 100.0 * 8.400596618652344
Epoch 1130, val loss: 0.8937626481056213
Epoch 1140, training loss: 840.9025268554688 = 0.8827332258224487 + 100.0 * 8.400197982788086
Epoch 1140, val loss: 0.8909175992012024
Epoch 1150, training loss: 840.873046875 = 0.8797411918640137 + 100.0 * 8.399932861328125
Epoch 1150, val loss: 0.8880782127380371
Epoch 1160, training loss: 841.021484375 = 0.8766054511070251 + 100.0 * 8.401449203491211
Epoch 1160, val loss: 0.8851062655448914
Epoch 1170, training loss: 840.785888671875 = 0.8734706044197083 + 100.0 * 8.399124145507812
Epoch 1170, val loss: 0.8820767402648926
Epoch 1180, training loss: 840.6412353515625 = 0.8704050183296204 + 100.0 * 8.39770793914795
Epoch 1180, val loss: 0.8791711330413818
Epoch 1190, training loss: 840.7870483398438 = 0.8673030734062195 + 100.0 * 8.399197578430176
Epoch 1190, val loss: 0.8762667775154114
Epoch 1200, training loss: 840.509033203125 = 0.8640133738517761 + 100.0 * 8.39645004272461
Epoch 1200, val loss: 0.8730564117431641
Epoch 1210, training loss: 840.489501953125 = 0.860877513885498 + 100.0 * 8.396286010742188
Epoch 1210, val loss: 0.8701092004776001
Epoch 1220, training loss: 840.420654296875 = 0.8578425049781799 + 100.0 * 8.395627975463867
Epoch 1220, val loss: 0.8671830892562866
Epoch 1230, training loss: 840.3367309570312 = 0.8547718524932861 + 100.0 * 8.394819259643555
Epoch 1230, val loss: 0.8642774224281311
Epoch 1240, training loss: 840.2719116210938 = 0.8517022728919983 + 100.0 * 8.39420223236084
Epoch 1240, val loss: 0.8613542318344116
Epoch 1250, training loss: 840.4117431640625 = 0.848603367805481 + 100.0 * 8.395630836486816
Epoch 1250, val loss: 0.8583675622940063
Epoch 1260, training loss: 840.316650390625 = 0.8452238440513611 + 100.0 * 8.39471435546875
Epoch 1260, val loss: 0.8551991581916809
Epoch 1270, training loss: 840.17578125 = 0.8420442342758179 + 100.0 * 8.39333724975586
Epoch 1270, val loss: 0.8521777391433716
Epoch 1280, training loss: 840.0761108398438 = 0.8389528393745422 + 100.0 * 8.392372131347656
Epoch 1280, val loss: 0.8492398858070374
Epoch 1290, training loss: 840.2119750976562 = 0.8358832597732544 + 100.0 * 8.393760681152344
Epoch 1290, val loss: 0.8462876677513123
Epoch 1300, training loss: 840.1995239257812 = 0.8324106931686401 + 100.0 * 8.393671035766602
Epoch 1300, val loss: 0.8430628180503845
Epoch 1310, training loss: 839.970703125 = 0.8291616439819336 + 100.0 * 8.3914155960083
Epoch 1310, val loss: 0.840008020401001
Epoch 1320, training loss: 839.8944091796875 = 0.8261006474494934 + 100.0 * 8.3906831741333
Epoch 1320, val loss: 0.8370959758758545
Epoch 1330, training loss: 839.78173828125 = 0.8230040669441223 + 100.0 * 8.38958740234375
Epoch 1330, val loss: 0.8341962099075317
Epoch 1340, training loss: 839.7881469726562 = 0.8199548125267029 + 100.0 * 8.389681816101074
Epoch 1340, val loss: 0.8313220143318176
Epoch 1350, training loss: 840.1980590820312 = 0.8167494535446167 + 100.0 * 8.393813133239746
Epoch 1350, val loss: 0.8282042145729065
Epoch 1360, training loss: 839.6747436523438 = 0.813377320766449 + 100.0 * 8.3886137008667
Epoch 1360, val loss: 0.8251439929008484
Epoch 1370, training loss: 839.59326171875 = 0.8102803230285645 + 100.0 * 8.387829780578613
Epoch 1370, val loss: 0.8222509622573853
Epoch 1380, training loss: 839.5382690429688 = 0.8072223663330078 + 100.0 * 8.387310028076172
Epoch 1380, val loss: 0.8193588256835938
Epoch 1390, training loss: 839.4991455078125 = 0.8041971325874329 + 100.0 * 8.38694953918457
Epoch 1390, val loss: 0.8165276646614075
Epoch 1400, training loss: 840.028564453125 = 0.8011229634284973 + 100.0 * 8.392273902893066
Epoch 1400, val loss: 0.8135477304458618
Epoch 1410, training loss: 839.61279296875 = 0.7976458072662354 + 100.0 * 8.388151168823242
Epoch 1410, val loss: 0.8104480504989624
Epoch 1420, training loss: 839.4437255859375 = 0.7946266531944275 + 100.0 * 8.386490821838379
Epoch 1420, val loss: 0.8075595498085022
Epoch 1430, training loss: 839.3060913085938 = 0.7915859222412109 + 100.0 * 8.38514518737793
Epoch 1430, val loss: 0.8047791123390198
Epoch 1440, training loss: 839.236083984375 = 0.7886254191398621 + 100.0 * 8.384474754333496
Epoch 1440, val loss: 0.8020107746124268
Epoch 1450, training loss: 839.202880859375 = 0.785638153553009 + 100.0 * 8.384172439575195
Epoch 1450, val loss: 0.7992480397224426
Epoch 1460, training loss: 839.7000122070312 = 0.7826173901557922 + 100.0 * 8.389174461364746
Epoch 1460, val loss: 0.7964153289794922
Epoch 1470, training loss: 839.3404541015625 = 0.7793018221855164 + 100.0 * 8.385611534118652
Epoch 1470, val loss: 0.793309211730957
Epoch 1480, training loss: 839.0877075195312 = 0.7762765288352966 + 100.0 * 8.383113861083984
Epoch 1480, val loss: 0.7905667424201965
Epoch 1490, training loss: 839.0289916992188 = 0.7733050584793091 + 100.0 * 8.382556915283203
Epoch 1490, val loss: 0.7878679037094116
Epoch 1500, training loss: 838.9608154296875 = 0.7704331278800964 + 100.0 * 8.381903648376465
Epoch 1500, val loss: 0.7851995825767517
Epoch 1510, training loss: 838.95654296875 = 0.7675493955612183 + 100.0 * 8.381889343261719
Epoch 1510, val loss: 0.7825598120689392
Epoch 1520, training loss: 839.40087890625 = 0.7644933462142944 + 100.0 * 8.386363983154297
Epoch 1520, val loss: 0.7796531319618225
Epoch 1530, training loss: 838.909423828125 = 0.7612437009811401 + 100.0 * 8.381482124328613
Epoch 1530, val loss: 0.7767189741134644
Epoch 1540, training loss: 838.8853149414062 = 0.7583124041557312 + 100.0 * 8.381270408630371
Epoch 1540, val loss: 0.7740522623062134
Epoch 1550, training loss: 838.7470703125 = 0.7554426193237305 + 100.0 * 8.379916191101074
Epoch 1550, val loss: 0.7714356184005737
Epoch 1560, training loss: 838.7063598632812 = 0.7526334524154663 + 100.0 * 8.379537582397461
Epoch 1560, val loss: 0.7688883543014526
Epoch 1570, training loss: 838.6604614257812 = 0.749829113483429 + 100.0 * 8.379106521606445
Epoch 1570, val loss: 0.766308069229126
Epoch 1580, training loss: 839.2212524414062 = 0.7469973564147949 + 100.0 * 8.384742736816406
Epoch 1580, val loss: 0.763627290725708
Epoch 1590, training loss: 838.9225463867188 = 0.7435412406921387 + 100.0 * 8.381790161132812
Epoch 1590, val loss: 0.7606089115142822
Epoch 1600, training loss: 838.662841796875 = 0.7407511472702026 + 100.0 * 8.379220962524414
Epoch 1600, val loss: 0.758039653301239
Epoch 1610, training loss: 838.513427734375 = 0.7378957271575928 + 100.0 * 8.377755165100098
Epoch 1610, val loss: 0.7554409503936768
Epoch 1620, training loss: 838.4797973632812 = 0.7351180911064148 + 100.0 * 8.377447128295898
Epoch 1620, val loss: 0.7529698014259338
Epoch 1630, training loss: 838.4275512695312 = 0.7324125170707703 + 100.0 * 8.376951217651367
Epoch 1630, val loss: 0.7505215406417847
Epoch 1640, training loss: 838.3851928710938 = 0.729655385017395 + 100.0 * 8.376555442810059
Epoch 1640, val loss: 0.7480371594429016
Epoch 1650, training loss: 838.370361328125 = 0.726898729801178 + 100.0 * 8.376434326171875
Epoch 1650, val loss: 0.7455577254295349
Epoch 1660, training loss: 839.1183471679688 = 0.7239859700202942 + 100.0 * 8.383943557739258
Epoch 1660, val loss: 0.742926836013794
Epoch 1670, training loss: 838.4945068359375 = 0.7209115028381348 + 100.0 * 8.37773609161377
Epoch 1670, val loss: 0.7401158213615417
Epoch 1680, training loss: 838.2633666992188 = 0.7180650234222412 + 100.0 * 8.375452995300293
Epoch 1680, val loss: 0.7376118302345276
Epoch 1690, training loss: 838.2544555664062 = 0.715347409248352 + 100.0 * 8.375391006469727
Epoch 1690, val loss: 0.735198974609375
Epoch 1700, training loss: 838.1786499023438 = 0.7127441167831421 + 100.0 * 8.374658584594727
Epoch 1700, val loss: 0.7328562140464783
Epoch 1710, training loss: 838.1448364257812 = 0.7101171016693115 + 100.0 * 8.374346733093262
Epoch 1710, val loss: 0.7305106520652771
Epoch 1720, training loss: 838.3418579101562 = 0.7074581980705261 + 100.0 * 8.376343727111816
Epoch 1720, val loss: 0.7281220555305481
Epoch 1730, training loss: 838.1087036132812 = 0.7045516967773438 + 100.0 * 8.374041557312012
Epoch 1730, val loss: 0.7255110144615173
Epoch 1740, training loss: 838.062744140625 = 0.7018795013427734 + 100.0 * 8.373608589172363
Epoch 1740, val loss: 0.7231647372245789
Epoch 1750, training loss: 838.0186767578125 = 0.6992673873901367 + 100.0 * 8.373193740844727
Epoch 1750, val loss: 0.7208456993103027
Epoch 1760, training loss: 838.0153198242188 = 0.696716845035553 + 100.0 * 8.373186111450195
Epoch 1760, val loss: 0.7185924053192139
Epoch 1770, training loss: 838.76123046875 = 0.6940613389015198 + 100.0 * 8.380671501159668
Epoch 1770, val loss: 0.7161497473716736
Epoch 1780, training loss: 838.1781005859375 = 0.6910690665245056 + 100.0 * 8.374870300292969
Epoch 1780, val loss: 0.713598370552063
Epoch 1790, training loss: 837.958984375 = 0.688602089881897 + 100.0 * 8.372703552246094
Epoch 1790, val loss: 0.7114129066467285
Epoch 1800, training loss: 837.8606567382812 = 0.6860552430152893 + 100.0 * 8.371746063232422
Epoch 1800, val loss: 0.7091906070709229
Epoch 1810, training loss: 837.8084106445312 = 0.6836059093475342 + 100.0 * 8.371248245239258
Epoch 1810, val loss: 0.7070497870445251
Epoch 1820, training loss: 837.771484375 = 0.6811434030532837 + 100.0 * 8.370903015136719
Epoch 1820, val loss: 0.7049001455307007
Epoch 1830, training loss: 838.2407836914062 = 0.6785947680473328 + 100.0 * 8.375621795654297
Epoch 1830, val loss: 0.7027433514595032
Epoch 1840, training loss: 837.9974975585938 = 0.6758573055267334 + 100.0 * 8.37321662902832
Epoch 1840, val loss: 0.7001705169677734
Epoch 1850, training loss: 837.673095703125 = 0.6733875274658203 + 100.0 * 8.369997024536133
Epoch 1850, val loss: 0.6980993151664734
Epoch 1860, training loss: 837.671630859375 = 0.6709402203559875 + 100.0 * 8.370006561279297
Epoch 1860, val loss: 0.6959898471832275
Epoch 1870, training loss: 837.6300659179688 = 0.6685830950737 + 100.0 * 8.369614601135254
Epoch 1870, val loss: 0.6939210295677185
Epoch 1880, training loss: 838.336181640625 = 0.6661588549613953 + 100.0 * 8.376700401306152
Epoch 1880, val loss: 0.6917468309402466
Epoch 1890, training loss: 837.81298828125 = 0.6633763909339905 + 100.0 * 8.371496200561523
Epoch 1890, val loss: 0.6894044280052185
Epoch 1900, training loss: 837.6129150390625 = 0.6610656380653381 + 100.0 * 8.369518280029297
Epoch 1900, val loss: 0.6873840093612671
Epoch 1910, training loss: 837.5054321289062 = 0.6586834192276001 + 100.0 * 8.368467330932617
Epoch 1910, val loss: 0.68535315990448
Epoch 1920, training loss: 837.5492553710938 = 0.6564342379570007 + 100.0 * 8.368927955627441
Epoch 1920, val loss: 0.6834220290184021
Epoch 1930, training loss: 837.7499389648438 = 0.6539384126663208 + 100.0 * 8.370960235595703
Epoch 1930, val loss: 0.681231677532196
Epoch 1940, training loss: 837.4816284179688 = 0.6516258716583252 + 100.0 * 8.368300437927246
Epoch 1940, val loss: 0.6792812347412109
Epoch 1950, training loss: 837.3695068359375 = 0.6493473649024963 + 100.0 * 8.367201805114746
Epoch 1950, val loss: 0.6773316860198975
Epoch 1960, training loss: 837.3297729492188 = 0.6471309065818787 + 100.0 * 8.366826057434082
Epoch 1960, val loss: 0.6754693984985352
Epoch 1970, training loss: 837.3028564453125 = 0.6449556946754456 + 100.0 * 8.366579055786133
Epoch 1970, val loss: 0.6736144423484802
Epoch 1980, training loss: 837.2940673828125 = 0.6427835822105408 + 100.0 * 8.3665132522583
Epoch 1980, val loss: 0.6717573404312134
Epoch 1990, training loss: 837.9268798828125 = 0.6405171751976013 + 100.0 * 8.37286376953125
Epoch 1990, val loss: 0.669736385345459
Epoch 2000, training loss: 837.5228271484375 = 0.6380459070205688 + 100.0 * 8.368847846984863
Epoch 2000, val loss: 0.667718231678009
Epoch 2010, training loss: 837.3216552734375 = 0.6358240842819214 + 100.0 * 8.36685848236084
Epoch 2010, val loss: 0.6659224629402161
Epoch 2020, training loss: 837.1827392578125 = 0.6337123513221741 + 100.0 * 8.365489959716797
Epoch 2020, val loss: 0.664140522480011
Epoch 2030, training loss: 837.154541015625 = 0.6316649317741394 + 100.0 * 8.365228652954102
Epoch 2030, val loss: 0.6623973846435547
Epoch 2040, training loss: 837.1412353515625 = 0.6296575665473938 + 100.0 * 8.365116119384766
Epoch 2040, val loss: 0.6607085466384888
Epoch 2050, training loss: 837.4739990234375 = 0.6275752782821655 + 100.0 * 8.368464469909668
Epoch 2050, val loss: 0.6589447259902954
Epoch 2060, training loss: 837.0737915039062 = 0.6253156065940857 + 100.0 * 8.364484786987305
Epoch 2060, val loss: 0.6570985913276672
Epoch 2070, training loss: 837.0623168945312 = 0.6233552694320679 + 100.0 * 8.364389419555664
Epoch 2070, val loss: 0.6554700136184692
Epoch 2080, training loss: 837.053955078125 = 0.621383011341095 + 100.0 * 8.364325523376465
Epoch 2080, val loss: 0.6538302898406982
Epoch 2090, training loss: 837.10986328125 = 0.6194330453872681 + 100.0 * 8.364904403686523
Epoch 2090, val loss: 0.6522336602210999
Epoch 2100, training loss: 837.1810302734375 = 0.6173909902572632 + 100.0 * 8.365636825561523
Epoch 2100, val loss: 0.6505212783813477
Epoch 2110, training loss: 837.0086059570312 = 0.6154457330703735 + 100.0 * 8.363931655883789
Epoch 2110, val loss: 0.648938775062561
Epoch 2120, training loss: 836.9775390625 = 0.6135413646697998 + 100.0 * 8.363639831542969
Epoch 2120, val loss: 0.6473873853683472
Epoch 2130, training loss: 837.1693725585938 = 0.6116529107093811 + 100.0 * 8.365577697753906
Epoch 2130, val loss: 0.6458045840263367
Epoch 2140, training loss: 836.8989868164062 = 0.6096965670585632 + 100.0 * 8.362893104553223
Epoch 2140, val loss: 0.6442648768424988
Epoch 2150, training loss: 836.9192504882812 = 0.6078932285308838 + 100.0 * 8.363113403320312
Epoch 2150, val loss: 0.6428146362304688
Epoch 2160, training loss: 836.8582763671875 = 0.6060952544212341 + 100.0 * 8.36252212524414
Epoch 2160, val loss: 0.6413383483886719
Epoch 2170, training loss: 836.9822998046875 = 0.6042892336845398 + 100.0 * 8.36378002166748
Epoch 2170, val loss: 0.6398854851722717
Epoch 2180, training loss: 836.9696655273438 = 0.6024142503738403 + 100.0 * 8.363672256469727
Epoch 2180, val loss: 0.6383332014083862
Epoch 2190, training loss: 836.8377685546875 = 0.6006649732589722 + 100.0 * 8.362371444702148
Epoch 2190, val loss: 0.6369086503982544
Epoch 2200, training loss: 836.7793579101562 = 0.598924458026886 + 100.0 * 8.361804008483887
Epoch 2200, val loss: 0.6355797648429871
Epoch 2210, training loss: 836.7312622070312 = 0.59726482629776 + 100.0 * 8.361339569091797
Epoch 2210, val loss: 0.6342254877090454
Epoch 2220, training loss: 836.7393798828125 = 0.5956048965454102 + 100.0 * 8.361437797546387
Epoch 2220, val loss: 0.6329286098480225
Epoch 2230, training loss: 837.1105346679688 = 0.5939094424247742 + 100.0 * 8.365165710449219
Epoch 2230, val loss: 0.6316194534301758
Epoch 2240, training loss: 837.0018310546875 = 0.5920037031173706 + 100.0 * 8.36409854888916
Epoch 2240, val loss: 0.6299848556518555
Epoch 2250, training loss: 836.6588745117188 = 0.590285062789917 + 100.0 * 8.360686302185059
Epoch 2250, val loss: 0.6286980509757996
Epoch 2260, training loss: 836.6159057617188 = 0.5887647271156311 + 100.0 * 8.360271453857422
Epoch 2260, val loss: 0.6274868845939636
Epoch 2270, training loss: 836.596923828125 = 0.5872209668159485 + 100.0 * 8.36009693145752
Epoch 2270, val loss: 0.6262965798377991
Epoch 2280, training loss: 836.5714111328125 = 0.5857282876968384 + 100.0 * 8.359856605529785
Epoch 2280, val loss: 0.625156581401825
Epoch 2290, training loss: 836.5643310546875 = 0.5842354893684387 + 100.0 * 8.359801292419434
Epoch 2290, val loss: 0.6239910125732422
Epoch 2300, training loss: 836.8514404296875 = 0.58267742395401 + 100.0 * 8.362687110900879
Epoch 2300, val loss: 0.6228001117706299
Epoch 2310, training loss: 836.5592041015625 = 0.5810617208480835 + 100.0 * 8.359781265258789
Epoch 2310, val loss: 0.6214740872383118
Epoch 2320, training loss: 836.8521728515625 = 0.5795260667800903 + 100.0 * 8.362726211547852
Epoch 2320, val loss: 0.6203377842903137
Epoch 2330, training loss: 836.4889526367188 = 0.5779078006744385 + 100.0 * 8.359110832214355
Epoch 2330, val loss: 0.6190659403800964
Epoch 2340, training loss: 836.4887084960938 = 0.5765066146850586 + 100.0 * 8.359122276306152
Epoch 2340, val loss: 0.618004322052002
Epoch 2350, training loss: 836.4361572265625 = 0.5751082301139832 + 100.0 * 8.358610153198242
Epoch 2350, val loss: 0.616919994354248
Epoch 2360, training loss: 836.4215087890625 = 0.5737564563751221 + 100.0 * 8.358477592468262
Epoch 2360, val loss: 0.6159226298332214
Epoch 2370, training loss: 836.879638671875 = 0.5723803639411926 + 100.0 * 8.363072395324707
Epoch 2370, val loss: 0.6148794889450073
Epoch 2380, training loss: 836.6103515625 = 0.5708126425743103 + 100.0 * 8.360395431518555
Epoch 2380, val loss: 0.6136115789413452
Epoch 2390, training loss: 836.4530029296875 = 0.5694573521614075 + 100.0 * 8.358835220336914
Epoch 2390, val loss: 0.6126583814620972
Epoch 2400, training loss: 836.4042358398438 = 0.5680989623069763 + 100.0 * 8.35836124420166
Epoch 2400, val loss: 0.611621081829071
Epoch 2410, training loss: 836.419677734375 = 0.5667679309844971 + 100.0 * 8.358529090881348
Epoch 2410, val loss: 0.6106264591217041
Epoch 2420, training loss: 836.4356689453125 = 0.5654616951942444 + 100.0 * 8.358701705932617
Epoch 2420, val loss: 0.6096636056900024
Epoch 2430, training loss: 836.3560180664062 = 0.5641413927078247 + 100.0 * 8.357918739318848
Epoch 2430, val loss: 0.6087098717689514
Epoch 2440, training loss: 836.2892456054688 = 0.5628836750984192 + 100.0 * 8.357263565063477
Epoch 2440, val loss: 0.607805609703064
Epoch 2450, training loss: 836.4803466796875 = 0.5616376399993896 + 100.0 * 8.359187126159668
Epoch 2450, val loss: 0.6069632768630981
Epoch 2460, training loss: 836.3873901367188 = 0.5602370500564575 + 100.0 * 8.358271598815918
Epoch 2460, val loss: 0.6058195233345032
Epoch 2470, training loss: 836.2434692382812 = 0.5589616298675537 + 100.0 * 8.356844902038574
Epoch 2470, val loss: 0.6049104332923889
Epoch 2480, training loss: 836.1951293945312 = 0.5577318668365479 + 100.0 * 8.35637378692627
Epoch 2480, val loss: 0.6039733290672302
Epoch 2490, training loss: 836.2210083007812 = 0.5565431714057922 + 100.0 * 8.356644630432129
Epoch 2490, val loss: 0.6031056046485901
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7661085743277524
0.8155473447801204
=== training gcn model ===
Epoch 0, training loss: 1059.309326171875 = 1.0810546875 + 100.0 * 10.582283020019531
Epoch 0, val loss: 1.0820684432983398
Epoch 10, training loss: 1059.27197265625 = 1.0782971382141113 + 100.0 * 10.581937789916992
Epoch 10, val loss: 1.079304814338684
Epoch 20, training loss: 1059.1318359375 = 1.0754104852676392 + 100.0 * 10.58056354522705
Epoch 20, val loss: 1.0764203071594238
Epoch 30, training loss: 1058.514892578125 = 1.0724376440048218 + 100.0 * 10.574424743652344
Epoch 30, val loss: 1.0734529495239258
Epoch 40, training loss: 1055.8701171875 = 1.0692898035049438 + 100.0 * 10.54800796508789
Epoch 40, val loss: 1.0703023672103882
Epoch 50, training loss: 1046.4765625 = 1.0657292604446411 + 100.0 * 10.454109191894531
Epoch 50, val loss: 1.0667574405670166
Epoch 60, training loss: 1018.605224609375 = 1.0618263483047485 + 100.0 * 10.175434112548828
Epoch 60, val loss: 1.0629080533981323
Epoch 70, training loss: 968.6829223632812 = 1.0571938753128052 + 100.0 * 9.676257133483887
Epoch 70, val loss: 1.0582807064056396
Epoch 80, training loss: 953.2565307617188 = 1.0524218082427979 + 100.0 * 9.522041320800781
Epoch 80, val loss: 1.0536234378814697
Epoch 90, training loss: 942.3386840820312 = 1.0485141277313232 + 100.0 * 9.412901878356934
Epoch 90, val loss: 1.049791693687439
Epoch 100, training loss: 931.1832275390625 = 1.0452115535736084 + 100.0 * 9.301380157470703
Epoch 100, val loss: 1.0466121435165405
Epoch 110, training loss: 921.6480102539062 = 1.0427262783050537 + 100.0 * 9.206052780151367
Epoch 110, val loss: 1.0442664623260498
Epoch 120, training loss: 915.651123046875 = 1.0413484573364258 + 100.0 * 9.146098136901855
Epoch 120, val loss: 1.042959213256836
Epoch 130, training loss: 908.8432006835938 = 1.0407018661499023 + 100.0 * 9.078024864196777
Epoch 130, val loss: 1.0423029661178589
Epoch 140, training loss: 898.7525024414062 = 1.0404709577560425 + 100.0 * 8.977120399475098
Epoch 140, val loss: 1.0420873165130615
Epoch 150, training loss: 891.1290283203125 = 1.04059898853302 + 100.0 * 8.900884628295898
Epoch 150, val loss: 1.0421655178070068
Epoch 160, training loss: 884.5221557617188 = 1.0404744148254395 + 100.0 * 8.834816932678223
Epoch 160, val loss: 1.0419927835464478
Epoch 170, training loss: 878.012451171875 = 1.0403372049331665 + 100.0 * 8.769721031188965
Epoch 170, val loss: 1.0418249368667603
Epoch 180, training loss: 874.2076416015625 = 1.0402038097381592 + 100.0 * 8.731674194335938
Epoch 180, val loss: 1.0416499376296997
Epoch 190, training loss: 871.35888671875 = 1.039817452430725 + 100.0 * 8.703190803527832
Epoch 190, val loss: 1.0411951541900635
Epoch 200, training loss: 869.454833984375 = 1.039113163948059 + 100.0 * 8.684157371520996
Epoch 200, val loss: 1.0404506921768188
Epoch 210, training loss: 867.981201171875 = 1.0383175611495972 + 100.0 * 8.669428825378418
Epoch 210, val loss: 1.0396367311477661
Epoch 220, training loss: 866.6177978515625 = 1.0375559329986572 + 100.0 * 8.655802726745605
Epoch 220, val loss: 1.0388767719268799
Epoch 230, training loss: 865.2175903320312 = 1.0368280410766602 + 100.0 * 8.641807556152344
Epoch 230, val loss: 1.0381741523742676
Epoch 240, training loss: 863.7255249023438 = 1.0361305475234985 + 100.0 * 8.626893997192383
Epoch 240, val loss: 1.0374919176101685
Epoch 250, training loss: 862.4146118164062 = 1.0354429483413696 + 100.0 * 8.613791465759277
Epoch 250, val loss: 1.0368175506591797
Epoch 260, training loss: 861.310302734375 = 1.0347332954406738 + 100.0 * 8.602755546569824
Epoch 260, val loss: 1.0360982418060303
Epoch 270, training loss: 860.1270751953125 = 1.033996820449829 + 100.0 * 8.590930938720703
Epoch 270, val loss: 1.035388708114624
Epoch 280, training loss: 858.890625 = 1.0333056449890137 + 100.0 * 8.578573226928711
Epoch 280, val loss: 1.0347216129302979
Epoch 290, training loss: 857.7822875976562 = 1.0326470136642456 + 100.0 * 8.567496299743652
Epoch 290, val loss: 1.034067988395691
Epoch 300, training loss: 856.5614624023438 = 1.0319535732269287 + 100.0 * 8.55529499053955
Epoch 300, val loss: 1.0334097146987915
Epoch 310, training loss: 855.487548828125 = 1.031270980834961 + 100.0 * 8.544563293457031
Epoch 310, val loss: 1.0327461957931519
Epoch 320, training loss: 854.8434448242188 = 1.03055739402771 + 100.0 * 8.538128852844238
Epoch 320, val loss: 1.0320353507995605
Epoch 330, training loss: 853.7359619140625 = 1.0297203063964844 + 100.0 * 8.52706241607666
Epoch 330, val loss: 1.03123939037323
Epoch 340, training loss: 853.083251953125 = 1.0288499593734741 + 100.0 * 8.520544052124023
Epoch 340, val loss: 1.0303905010223389
Epoch 350, training loss: 852.5574340820312 = 1.0279449224472046 + 100.0 * 8.515295028686523
Epoch 350, val loss: 1.0294970273971558
Epoch 360, training loss: 851.9324951171875 = 1.0269495248794556 + 100.0 * 8.509055137634277
Epoch 360, val loss: 1.028552770614624
Epoch 370, training loss: 851.381591796875 = 1.025944471359253 + 100.0 * 8.503556251525879
Epoch 370, val loss: 1.0275813341140747
Epoch 380, training loss: 850.8743286132812 = 1.0249300003051758 + 100.0 * 8.498494148254395
Epoch 380, val loss: 1.0266118049621582
Epoch 390, training loss: 851.4150390625 = 1.023905873298645 + 100.0 * 8.503911018371582
Epoch 390, val loss: 1.025588035583496
Epoch 400, training loss: 850.2575073242188 = 1.0227431058883667 + 100.0 * 8.492347717285156
Epoch 400, val loss: 1.0244746208190918
Epoch 410, training loss: 849.5333862304688 = 1.0216131210327148 + 100.0 * 8.48511791229248
Epoch 410, val loss: 1.0234113931655884
Epoch 420, training loss: 849.1353759765625 = 1.0205011367797852 + 100.0 * 8.481148719787598
Epoch 420, val loss: 1.0223404169082642
Epoch 430, training loss: 848.7127075195312 = 1.019359827041626 + 100.0 * 8.476933479309082
Epoch 430, val loss: 1.0212337970733643
Epoch 440, training loss: 848.3409423828125 = 1.0181739330291748 + 100.0 * 8.473227500915527
Epoch 440, val loss: 1.0200825929641724
Epoch 450, training loss: 848.3677368164062 = 1.0169401168823242 + 100.0 * 8.47350788116455
Epoch 450, val loss: 1.0188935995101929
Epoch 460, training loss: 847.8472290039062 = 1.015608310699463 + 100.0 * 8.468316078186035
Epoch 460, val loss: 1.017572045326233
Epoch 470, training loss: 847.4263916015625 = 1.0142461061477661 + 100.0 * 8.46412181854248
Epoch 470, val loss: 1.0162780284881592
Epoch 480, training loss: 847.08984375 = 1.0128976106643677 + 100.0 * 8.460769653320312
Epoch 480, val loss: 1.0149645805358887
Epoch 490, training loss: 846.8089599609375 = 1.011531114578247 + 100.0 * 8.457974433898926
Epoch 490, val loss: 1.013625979423523
Epoch 500, training loss: 846.801513671875 = 1.0101001262664795 + 100.0 * 8.457914352416992
Epoch 500, val loss: 1.0122123956680298
Epoch 510, training loss: 846.3587036132812 = 1.0085759162902832 + 100.0 * 8.453500747680664
Epoch 510, val loss: 1.0107399225234985
Epoch 520, training loss: 846.0108032226562 = 1.0070956945419312 + 100.0 * 8.450037002563477
Epoch 520, val loss: 1.0093053579330444
Epoch 530, training loss: 845.7127075195312 = 1.0056185722351074 + 100.0 * 8.447071075439453
Epoch 530, val loss: 1.0078686475753784
Epoch 540, training loss: 845.51123046875 = 1.0041100978851318 + 100.0 * 8.44507122039795
Epoch 540, val loss: 1.0064102411270142
Epoch 550, training loss: 845.7792358398438 = 1.002504587173462 + 100.0 * 8.44776725769043
Epoch 550, val loss: 1.004810094833374
Epoch 560, training loss: 845.1016845703125 = 1.0008269548416138 + 100.0 * 8.441008567810059
Epoch 560, val loss: 1.0031710863113403
Epoch 570, training loss: 844.7840576171875 = 0.9991676807403564 + 100.0 * 8.437849044799805
Epoch 570, val loss: 1.0015650987625122
Epoch 580, training loss: 844.5518798828125 = 0.9975177645683289 + 100.0 * 8.43554401397705
Epoch 580, val loss: 0.9999549388885498
Epoch 590, training loss: 844.345703125 = 0.995819091796875 + 100.0 * 8.43349838256836
Epoch 590, val loss: 0.9982910752296448
Epoch 600, training loss: 844.8431396484375 = 0.9940664768218994 + 100.0 * 8.438490867614746
Epoch 600, val loss: 0.9965952634811401
Epoch 610, training loss: 844.080322265625 = 0.9921755790710449 + 100.0 * 8.43088150024414
Epoch 610, val loss: 0.9947002530097961
Epoch 620, training loss: 843.9024047851562 = 0.9902662634849548 + 100.0 * 8.429121017456055
Epoch 620, val loss: 0.9928248524665833
Epoch 630, training loss: 843.6215209960938 = 0.9883561134338379 + 100.0 * 8.426331520080566
Epoch 630, val loss: 0.9909600019454956
Epoch 640, training loss: 843.4484252929688 = 0.9864463210105896 + 100.0 * 8.424619674682617
Epoch 640, val loss: 0.9890910983085632
Epoch 650, training loss: 843.2755737304688 = 0.9844782948493958 + 100.0 * 8.422910690307617
Epoch 650, val loss: 0.9871508479118347
Epoch 660, training loss: 843.394775390625 = 0.9824341535568237 + 100.0 * 8.424123764038086
Epoch 660, val loss: 0.9851222038269043
Epoch 670, training loss: 843.1539306640625 = 0.9802600741386414 + 100.0 * 8.421736717224121
Epoch 670, val loss: 0.9830199480056763
Epoch 680, training loss: 842.922119140625 = 0.9780511260032654 + 100.0 * 8.419441223144531
Epoch 680, val loss: 0.9808509945869446
Epoch 690, training loss: 842.7325439453125 = 0.9758132696151733 + 100.0 * 8.417567253112793
Epoch 690, val loss: 0.9786527156829834
Epoch 700, training loss: 842.6876831054688 = 0.9735299348831177 + 100.0 * 8.417141914367676
Epoch 700, val loss: 0.9764226675033569
Epoch 710, training loss: 842.4772338867188 = 0.9711197018623352 + 100.0 * 8.415060997009277
Epoch 710, val loss: 0.9740388989448547
Epoch 720, training loss: 842.5027465820312 = 0.9686466455459595 + 100.0 * 8.4153413772583
Epoch 720, val loss: 0.9716320037841797
Epoch 730, training loss: 842.2913818359375 = 0.9661980867385864 + 100.0 * 8.413251876831055
Epoch 730, val loss: 0.9692336916923523
Epoch 740, training loss: 842.15380859375 = 0.9637179374694824 + 100.0 * 8.411900520324707
Epoch 740, val loss: 0.9668000340461731
Epoch 750, training loss: 842.0577392578125 = 0.9612201452255249 + 100.0 * 8.410964965820312
Epoch 750, val loss: 0.9643531441688538
Epoch 760, training loss: 842.4790649414062 = 0.9586589336395264 + 100.0 * 8.415204048156738
Epoch 760, val loss: 0.9618088006973267
Epoch 770, training loss: 841.9735717773438 = 0.9559483528137207 + 100.0 * 8.410176277160645
Epoch 770, val loss: 0.95917147397995
Epoch 780, training loss: 841.7809448242188 = 0.9532835483551025 + 100.0 * 8.408276557922363
Epoch 780, val loss: 0.9565759897232056
Epoch 790, training loss: 841.6663208007812 = 0.9506070017814636 + 100.0 * 8.407156944274902
Epoch 790, val loss: 0.9539515972137451
Epoch 800, training loss: 841.7347412109375 = 0.947894275188446 + 100.0 * 8.407868385314941
Epoch 800, val loss: 0.9512819647789001
Epoch 810, training loss: 841.6547241210938 = 0.9449823498725891 + 100.0 * 8.407096862792969
Epoch 810, val loss: 0.9484366178512573
Epoch 820, training loss: 841.5274047851562 = 0.9420672059059143 + 100.0 * 8.405853271484375
Epoch 820, val loss: 0.945573627948761
Epoch 830, training loss: 841.339599609375 = 0.9392127990722656 + 100.0 * 8.404004096984863
Epoch 830, val loss: 0.9427805542945862
Epoch 840, training loss: 841.2268676757812 = 0.9363685846328735 + 100.0 * 8.402905464172363
Epoch 840, val loss: 0.9399925470352173
Epoch 850, training loss: 841.1200561523438 = 0.9335055351257324 + 100.0 * 8.401865005493164
Epoch 850, val loss: 0.9371786713600159
Epoch 860, training loss: 841.0307006835938 = 0.930623471736908 + 100.0 * 8.4010009765625
Epoch 860, val loss: 0.9343504905700684
Epoch 870, training loss: 841.0548706054688 = 0.9277020692825317 + 100.0 * 8.40127182006836
Epoch 870, val loss: 0.9314828515052795
Epoch 880, training loss: 841.1715087890625 = 0.92463219165802 + 100.0 * 8.40246868133545
Epoch 880, val loss: 0.9284589290618896
Epoch 890, training loss: 840.9232177734375 = 0.9215336441993713 + 100.0 * 8.400016784667969
Epoch 890, val loss: 0.9254395365715027
Epoch 900, training loss: 840.7388916015625 = 0.9185339212417603 + 100.0 * 8.39820384979248
Epoch 900, val loss: 0.9225215315818787
Epoch 910, training loss: 840.6005249023438 = 0.9155811667442322 + 100.0 * 8.396849632263184
Epoch 910, val loss: 0.9196276068687439
Epoch 920, training loss: 840.5352783203125 = 0.9126233458518982 + 100.0 * 8.39622688293457
Epoch 920, val loss: 0.9167167544364929
Epoch 930, training loss: 840.79931640625 = 0.9096226692199707 + 100.0 * 8.398897171020508
Epoch 930, val loss: 0.9137427806854248
Epoch 940, training loss: 840.5453491210938 = 0.9064158201217651 + 100.0 * 8.39638900756836
Epoch 940, val loss: 0.9106732606887817
Epoch 950, training loss: 840.3796997070312 = 0.9033092260360718 + 100.0 * 8.394763946533203
Epoch 950, val loss: 0.9076003432273865
Epoch 960, training loss: 840.2322998046875 = 0.9002496004104614 + 100.0 * 8.393320083618164
Epoch 960, val loss: 0.9046275615692139
Epoch 970, training loss: 840.1597900390625 = 0.8972135186195374 + 100.0 * 8.39262580871582
Epoch 970, val loss: 0.9016594886779785
Epoch 980, training loss: 840.1940307617188 = 0.8941581845283508 + 100.0 * 8.392998695373535
Epoch 980, val loss: 0.8986631035804749
Epoch 990, training loss: 840.1019897460938 = 0.8909618258476257 + 100.0 * 8.392109870910645
Epoch 990, val loss: 0.8955180048942566
Epoch 1000, training loss: 840.01904296875 = 0.88774573802948 + 100.0 * 8.391312599182129
Epoch 1000, val loss: 0.8924021124839783
Epoch 1010, training loss: 840.0564575195312 = 0.884555995464325 + 100.0 * 8.391718864440918
Epoch 1010, val loss: 0.8892720341682434
Epoch 1020, training loss: 839.8455200195312 = 0.8813758492469788 + 100.0 * 8.389641761779785
Epoch 1020, val loss: 0.8861839175224304
Epoch 1030, training loss: 839.8290405273438 = 0.8782382011413574 + 100.0 * 8.389508247375488
Epoch 1030, val loss: 0.8831300139427185
Epoch 1040, training loss: 839.881591796875 = 0.8750537633895874 + 100.0 * 8.39006519317627
Epoch 1040, val loss: 0.8800275325775146
Epoch 1050, training loss: 839.6868286132812 = 0.8718193173408508 + 100.0 * 8.388150215148926
Epoch 1050, val loss: 0.8768644332885742
Epoch 1060, training loss: 839.6783447265625 = 0.8686264157295227 + 100.0 * 8.388096809387207
Epoch 1060, val loss: 0.8737473487854004
Epoch 1070, training loss: 839.6642456054688 = 0.8653688430786133 + 100.0 * 8.387989044189453
Epoch 1070, val loss: 0.8705703616142273
Epoch 1080, training loss: 839.627685546875 = 0.8620861768722534 + 100.0 * 8.387656211853027
Epoch 1080, val loss: 0.8673792481422424
Epoch 1090, training loss: 839.78173828125 = 0.8587527275085449 + 100.0 * 8.389229774475098
Epoch 1090, val loss: 0.8641396164894104
Epoch 1100, training loss: 839.4807739257812 = 0.8554303646087646 + 100.0 * 8.386253356933594
Epoch 1100, val loss: 0.8608507513999939
Epoch 1110, training loss: 839.3568725585938 = 0.8521558046340942 + 100.0 * 8.38504695892334
Epoch 1110, val loss: 0.8576918244361877
Epoch 1120, training loss: 839.3182373046875 = 0.8489315509796143 + 100.0 * 8.384693145751953
Epoch 1120, val loss: 0.8545472621917725
Epoch 1130, training loss: 839.3740234375 = 0.8456732630729675 + 100.0 * 8.385283470153809
Epoch 1130, val loss: 0.8513733148574829
Epoch 1140, training loss: 839.4122924804688 = 0.8422825336456299 + 100.0 * 8.385700225830078
Epoch 1140, val loss: 0.8480734825134277
Epoch 1150, training loss: 839.320556640625 = 0.8388780951499939 + 100.0 * 8.384817123413086
Epoch 1150, val loss: 0.8447167277336121
Epoch 1160, training loss: 839.1756591796875 = 0.8354732394218445 + 100.0 * 8.383401870727539
Epoch 1160, val loss: 0.8414449095726013
Epoch 1170, training loss: 839.123291015625 = 0.8321779370307922 + 100.0 * 8.382911682128906
Epoch 1170, val loss: 0.8382542729377747
Epoch 1180, training loss: 839.1289672851562 = 0.8289134502410889 + 100.0 * 8.383000373840332
Epoch 1180, val loss: 0.8351017832756042
Epoch 1190, training loss: 839.2620849609375 = 0.8255639672279358 + 100.0 * 8.38436508178711
Epoch 1190, val loss: 0.8318414688110352
Epoch 1200, training loss: 839.0238647460938 = 0.8221737146377563 + 100.0 * 8.382017135620117
Epoch 1200, val loss: 0.8285155296325684
Epoch 1210, training loss: 838.953125 = 0.818828821182251 + 100.0 * 8.381342887878418
Epoch 1210, val loss: 0.8252758383750916
Epoch 1220, training loss: 838.9248657226562 = 0.8155194520950317 + 100.0 * 8.381093978881836
Epoch 1220, val loss: 0.8221009969711304
Epoch 1230, training loss: 838.9033813476562 = 0.8122252225875854 + 100.0 * 8.380911827087402
Epoch 1230, val loss: 0.8189011812210083
Epoch 1240, training loss: 839.318603515625 = 0.8088101744651794 + 100.0 * 8.38509750366211
Epoch 1240, val loss: 0.8155838251113892
Epoch 1250, training loss: 838.8621215820312 = 0.8053293228149414 + 100.0 * 8.38056755065918
Epoch 1250, val loss: 0.8122212886810303
Epoch 1260, training loss: 838.7504272460938 = 0.801977276802063 + 100.0 * 8.379484176635742
Epoch 1260, val loss: 0.8089985847473145
Epoch 1270, training loss: 838.7301635742188 = 0.7986880540847778 + 100.0 * 8.379314422607422
Epoch 1270, val loss: 0.8058188557624817
Epoch 1280, training loss: 838.7520141601562 = 0.7954204082489014 + 100.0 * 8.379566192626953
Epoch 1280, val loss: 0.8026887774467468
Epoch 1290, training loss: 839.1243896484375 = 0.7919531464576721 + 100.0 * 8.38332462310791
Epoch 1290, val loss: 0.7993317246437073
Epoch 1300, training loss: 838.6331787109375 = 0.7884490489959717 + 100.0 * 8.378447532653809
Epoch 1300, val loss: 0.7959365844726562
Epoch 1310, training loss: 838.6300048828125 = 0.7851387858390808 + 100.0 * 8.378448486328125
Epoch 1310, val loss: 0.7927452921867371
Epoch 1320, training loss: 838.5655517578125 = 0.7818785309791565 + 100.0 * 8.377837181091309
Epoch 1320, val loss: 0.7896375060081482
Epoch 1330, training loss: 838.511474609375 = 0.7786673307418823 + 100.0 * 8.377327919006348
Epoch 1330, val loss: 0.7865423560142517
Epoch 1340, training loss: 838.5270385742188 = 0.7754519581794739 + 100.0 * 8.37751579284668
Epoch 1340, val loss: 0.7834520936012268
Epoch 1350, training loss: 838.76513671875 = 0.7721059322357178 + 100.0 * 8.37993049621582
Epoch 1350, val loss: 0.7802320122718811
Epoch 1360, training loss: 838.5578002929688 = 0.7687147259712219 + 100.0 * 8.377890586853027
Epoch 1360, val loss: 0.7769994735717773
Epoch 1370, training loss: 838.4720458984375 = 0.7653908729553223 + 100.0 * 8.377066612243652
Epoch 1370, val loss: 0.7738308310508728
Epoch 1380, training loss: 838.3634643554688 = 0.7621892094612122 + 100.0 * 8.376012802124023
Epoch 1380, val loss: 0.7707611322402954
Epoch 1390, training loss: 838.3256225585938 = 0.7590348124504089 + 100.0 * 8.375665664672852
Epoch 1390, val loss: 0.7677334547042847
Epoch 1400, training loss: 838.337890625 = 0.7558875679969788 + 100.0 * 8.37582015991211
Epoch 1400, val loss: 0.7647218108177185
Epoch 1410, training loss: 838.9563598632812 = 0.7526267170906067 + 100.0 * 8.382037162780762
Epoch 1410, val loss: 0.761537492275238
Epoch 1420, training loss: 838.3726196289062 = 0.7491164207458496 + 100.0 * 8.376235008239746
Epoch 1420, val loss: 0.7582940459251404
Epoch 1430, training loss: 838.2628173828125 = 0.745900571346283 + 100.0 * 8.375168800354004
Epoch 1430, val loss: 0.7552399039268494
Epoch 1440, training loss: 838.1845092773438 = 0.7428062558174133 + 100.0 * 8.374417304992676
Epoch 1440, val loss: 0.7522965669631958
Epoch 1450, training loss: 838.1343383789062 = 0.7397552132606506 + 100.0 * 8.373946189880371
Epoch 1450, val loss: 0.7494001388549805
Epoch 1460, training loss: 838.1113891601562 = 0.7366734743118286 + 100.0 * 8.373746871948242
Epoch 1460, val loss: 0.746494472026825
Epoch 1470, training loss: 838.4963989257812 = 0.7335912585258484 + 100.0 * 8.377628326416016
Epoch 1470, val loss: 0.7435411214828491
Epoch 1480, training loss: 838.3186645507812 = 0.7303061485290527 + 100.0 * 8.375884056091309
Epoch 1480, val loss: 0.7404344081878662
Epoch 1490, training loss: 838.0665283203125 = 0.7270652055740356 + 100.0 * 8.373394966125488
Epoch 1490, val loss: 0.7374127507209778
Epoch 1500, training loss: 838.0060424804688 = 0.7240421772003174 + 100.0 * 8.372819900512695
Epoch 1500, val loss: 0.7345618605613708
Epoch 1510, training loss: 837.9832153320312 = 0.7210465669631958 + 100.0 * 8.372621536254883
Epoch 1510, val loss: 0.731744647026062
Epoch 1520, training loss: 838.1569213867188 = 0.7181059122085571 + 100.0 * 8.374388694763184
Epoch 1520, val loss: 0.7289780974388123
Epoch 1530, training loss: 837.93017578125 = 0.7150049209594727 + 100.0 * 8.372151374816895
Epoch 1530, val loss: 0.7259985208511353
Epoch 1540, training loss: 837.9279174804688 = 0.7119247317314148 + 100.0 * 8.372159957885742
Epoch 1540, val loss: 0.7231307029724121
Epoch 1550, training loss: 837.8756713867188 = 0.7090005278587341 + 100.0 * 8.37166690826416
Epoch 1550, val loss: 0.7204241752624512
Epoch 1560, training loss: 837.8836669921875 = 0.7061012983322144 + 100.0 * 8.37177562713623
Epoch 1560, val loss: 0.7176585793495178
Epoch 1570, training loss: 837.9725341796875 = 0.703113317489624 + 100.0 * 8.37269401550293
Epoch 1570, val loss: 0.714880108833313
Epoch 1580, training loss: 837.7540893554688 = 0.7001349329948425 + 100.0 * 8.370539665222168
Epoch 1580, val loss: 0.7121244668960571
Epoch 1590, training loss: 837.7213134765625 = 0.6972605586051941 + 100.0 * 8.370240211486816
Epoch 1590, val loss: 0.7094563841819763
Epoch 1600, training loss: 837.740966796875 = 0.6944604516029358 + 100.0 * 8.370465278625488
Epoch 1600, val loss: 0.7068426609039307
Epoch 1610, training loss: 838.0153198242188 = 0.6916142702102661 + 100.0 * 8.373237609863281
Epoch 1610, val loss: 0.7041773200035095
Epoch 1620, training loss: 837.7249755859375 = 0.6886814832687378 + 100.0 * 8.370363235473633
Epoch 1620, val loss: 0.7014766931533813
Epoch 1630, training loss: 837.6331176757812 = 0.6858442425727844 + 100.0 * 8.36947250366211
Epoch 1630, val loss: 0.6988421082496643
Epoch 1640, training loss: 837.574462890625 = 0.6830935478210449 + 100.0 * 8.368913650512695
Epoch 1640, val loss: 0.6963320374488831
Epoch 1650, training loss: 837.7223510742188 = 0.6804203987121582 + 100.0 * 8.3704195022583
Epoch 1650, val loss: 0.6938637495040894
Epoch 1660, training loss: 837.5817260742188 = 0.6776741147041321 + 100.0 * 8.369040489196777
Epoch 1660, val loss: 0.691277801990509
Epoch 1670, training loss: 837.5254516601562 = 0.674833357334137 + 100.0 * 8.36850643157959
Epoch 1670, val loss: 0.6886793375015259
Epoch 1680, training loss: 837.5131225585938 = 0.6721500754356384 + 100.0 * 8.368409156799316
Epoch 1680, val loss: 0.6862484812736511
Epoch 1690, training loss: 837.6854858398438 = 0.6694871783256531 + 100.0 * 8.370160102844238
Epoch 1690, val loss: 0.6837655901908875
Epoch 1700, training loss: 837.4332885742188 = 0.6667206287384033 + 100.0 * 8.36766529083252
Epoch 1700, val loss: 0.6812373399734497
Epoch 1710, training loss: 837.3977661132812 = 0.6640790700912476 + 100.0 * 8.367337226867676
Epoch 1710, val loss: 0.6788306832313538
Epoch 1720, training loss: 837.3667602539062 = 0.6615620851516724 + 100.0 * 8.36705207824707
Epoch 1720, val loss: 0.6765304207801819
Epoch 1730, training loss: 837.3235473632812 = 0.6590291857719421 + 100.0 * 8.366644859313965
Epoch 1730, val loss: 0.6742530465126038
Epoch 1740, training loss: 837.5870361328125 = 0.6564742922782898 + 100.0 * 8.369305610656738
Epoch 1740, val loss: 0.6719688177108765
Epoch 1750, training loss: 837.4829711914062 = 0.6538931727409363 + 100.0 * 8.368290901184082
Epoch 1750, val loss: 0.6694561839103699
Epoch 1760, training loss: 837.4568481445312 = 0.6512532234191895 + 100.0 * 8.368056297302246
Epoch 1760, val loss: 0.6671777963638306
Epoch 1770, training loss: 837.2350463867188 = 0.6487072706222534 + 100.0 * 8.365863800048828
Epoch 1770, val loss: 0.6648589372634888
Epoch 1780, training loss: 837.18505859375 = 0.646253764629364 + 100.0 * 8.365387916564941
Epoch 1780, val loss: 0.6626209020614624
Epoch 1790, training loss: 837.1575927734375 = 0.643868625164032 + 100.0 * 8.365137100219727
Epoch 1790, val loss: 0.6604769229888916
Epoch 1800, training loss: 837.3317260742188 = 0.6415036916732788 + 100.0 * 8.366902351379395
Epoch 1800, val loss: 0.6583022475242615
Epoch 1810, training loss: 837.1989135742188 = 0.6390292644500732 + 100.0 * 8.365598678588867
Epoch 1810, val loss: 0.6561480164527893
Epoch 1820, training loss: 837.15771484375 = 0.636609673500061 + 100.0 * 8.365211486816406
Epoch 1820, val loss: 0.6539183259010315
Epoch 1830, training loss: 837.7824096679688 = 0.6342055797576904 + 100.0 * 8.371481895446777
Epoch 1830, val loss: 0.6517614722251892
Epoch 1840, training loss: 837.2178955078125 = 0.6317369937896729 + 100.0 * 8.365861892700195
Epoch 1840, val loss: 0.6495828628540039
Epoch 1850, training loss: 837.05908203125 = 0.629384458065033 + 100.0 * 8.364296913146973
Epoch 1850, val loss: 0.647486686706543
Epoch 1860, training loss: 836.9888916015625 = 0.6271970272064209 + 100.0 * 8.363616943359375
Epoch 1860, val loss: 0.6455432772636414
Epoch 1870, training loss: 836.9484252929688 = 0.6250444054603577 + 100.0 * 8.36323356628418
Epoch 1870, val loss: 0.6436324715614319
Epoch 1880, training loss: 836.9127807617188 = 0.6229106187820435 + 100.0 * 8.362898826599121
Epoch 1880, val loss: 0.6417384743690491
Epoch 1890, training loss: 836.8966674804688 = 0.6207756996154785 + 100.0 * 8.36275863647461
Epoch 1890, val loss: 0.6398538947105408
Epoch 1900, training loss: 837.1190185546875 = 0.618625283241272 + 100.0 * 8.36500358581543
Epoch 1900, val loss: 0.6378966569900513
Epoch 1910, training loss: 837.0029296875 = 0.6163146495819092 + 100.0 * 8.363865852355957
Epoch 1910, val loss: 0.6358869671821594
Epoch 1920, training loss: 836.9696044921875 = 0.6140339374542236 + 100.0 * 8.363555908203125
Epoch 1920, val loss: 0.6338951587677002
Epoch 1930, training loss: 837.032958984375 = 0.6118887066841125 + 100.0 * 8.364211082458496
Epoch 1930, val loss: 0.6320393085479736
Epoch 1940, training loss: 836.8878173828125 = 0.6097305417060852 + 100.0 * 8.362780570983887
Epoch 1940, val loss: 0.6300861239433289
Epoch 1950, training loss: 836.8038330078125 = 0.6076734066009521 + 100.0 * 8.361961364746094
Epoch 1950, val loss: 0.6283242106437683
Epoch 1960, training loss: 836.781494140625 = 0.6056742668151855 + 100.0 * 8.3617582321167
Epoch 1960, val loss: 0.6266024708747864
Epoch 1970, training loss: 836.7453002929688 = 0.6037038564682007 + 100.0 * 8.36141586303711
Epoch 1970, val loss: 0.6248729228973389
Epoch 1980, training loss: 836.8345336914062 = 0.6017555594444275 + 100.0 * 8.362327575683594
Epoch 1980, val loss: 0.6231740117073059
Epoch 1990, training loss: 837.0003662109375 = 0.599706768989563 + 100.0 * 8.364006042480469
Epoch 1990, val loss: 0.621344268321991
Epoch 2000, training loss: 836.801025390625 = 0.5976342558860779 + 100.0 * 8.36203384399414
Epoch 2000, val loss: 0.6195568442344666
Epoch 2010, training loss: 836.6647338867188 = 0.595719039440155 + 100.0 * 8.360690116882324
Epoch 2010, val loss: 0.6179044246673584
Epoch 2020, training loss: 836.6976928710938 = 0.5938438177108765 + 100.0 * 8.361038208007812
Epoch 2020, val loss: 0.6162470579147339
Epoch 2030, training loss: 836.7427978515625 = 0.591956615447998 + 100.0 * 8.3615083694458
Epoch 2030, val loss: 0.614605188369751
Epoch 2040, training loss: 836.5905151367188 = 0.5900840163230896 + 100.0 * 8.360004425048828
Epoch 2040, val loss: 0.6130874752998352
Epoch 2050, training loss: 836.6226806640625 = 0.5882832407951355 + 100.0 * 8.360343933105469
Epoch 2050, val loss: 0.6115564107894897
Epoch 2060, training loss: 836.773193359375 = 0.5864841938018799 + 100.0 * 8.36186695098877
Epoch 2060, val loss: 0.6099346280097961
Epoch 2070, training loss: 836.5902709960938 = 0.5845850110054016 + 100.0 * 8.36005687713623
Epoch 2070, val loss: 0.6083698868751526
Epoch 2080, training loss: 836.547607421875 = 0.5827661752700806 + 100.0 * 8.359648704528809
Epoch 2080, val loss: 0.6068180203437805
Epoch 2090, training loss: 836.590087890625 = 0.5810279250144958 + 100.0 * 8.360090255737305
Epoch 2090, val loss: 0.6053343415260315
Epoch 2100, training loss: 836.53955078125 = 0.5792738795280457 + 100.0 * 8.359602928161621
Epoch 2100, val loss: 0.6037980914115906
Epoch 2110, training loss: 836.4990844726562 = 0.5775760412216187 + 100.0 * 8.359214782714844
Epoch 2110, val loss: 0.6023639440536499
Epoch 2120, training loss: 837.1360473632812 = 0.5758090019226074 + 100.0 * 8.365602493286133
Epoch 2120, val loss: 0.6008093357086182
Epoch 2130, training loss: 836.5933837890625 = 0.5740019083023071 + 100.0 * 8.360194206237793
Epoch 2130, val loss: 0.5993720293045044
Epoch 2140, training loss: 836.4081420898438 = 0.5723121762275696 + 100.0 * 8.358358383178711
Epoch 2140, val loss: 0.5979431867599487
Epoch 2150, training loss: 836.3568725585938 = 0.5707423686981201 + 100.0 * 8.357861518859863
Epoch 2150, val loss: 0.5966135263442993
Epoch 2160, training loss: 836.3358154296875 = 0.5692172050476074 + 100.0 * 8.357666015625
Epoch 2160, val loss: 0.5953856706619263
Epoch 2170, training loss: 836.3406982421875 = 0.5676943063735962 + 100.0 * 8.3577299118042
Epoch 2170, val loss: 0.5941218733787537
Epoch 2180, training loss: 836.8228149414062 = 0.566116988658905 + 100.0 * 8.362566947937012
Epoch 2180, val loss: 0.5928589105606079
Epoch 2190, training loss: 836.4970703125 = 0.5644567608833313 + 100.0 * 8.359326362609863
Epoch 2190, val loss: 0.5912373661994934
Epoch 2200, training loss: 836.2967529296875 = 0.5628779530525208 + 100.0 * 8.357338905334473
Epoch 2200, val loss: 0.5901027917861938
Epoch 2210, training loss: 836.2825927734375 = 0.5614015460014343 + 100.0 * 8.35721206665039
Epoch 2210, val loss: 0.5888417363166809
Epoch 2220, training loss: 836.4414672851562 = 0.5599496364593506 + 100.0 * 8.35881519317627
Epoch 2220, val loss: 0.5876450538635254
Epoch 2230, training loss: 836.2122802734375 = 0.5584242939949036 + 100.0 * 8.356538772583008
Epoch 2230, val loss: 0.5863651037216187
Epoch 2240, training loss: 836.2294921875 = 0.5569871068000793 + 100.0 * 8.356724739074707
Epoch 2240, val loss: 0.5851933360099792
Epoch 2250, training loss: 836.1807861328125 = 0.5556014776229858 + 100.0 * 8.35625171661377
Epoch 2250, val loss: 0.5840513706207275
Epoch 2260, training loss: 836.8748779296875 = 0.5541890859603882 + 100.0 * 8.36320686340332
Epoch 2260, val loss: 0.5827926397323608
Epoch 2270, training loss: 836.3735961914062 = 0.5526406168937683 + 100.0 * 8.358209609985352
Epoch 2270, val loss: 0.5816500782966614
Epoch 2280, training loss: 836.1425170898438 = 0.5512197017669678 + 100.0 * 8.355913162231445
Epoch 2280, val loss: 0.5804674625396729
Epoch 2290, training loss: 836.0703735351562 = 0.5499078631401062 + 100.0 * 8.355204582214355
Epoch 2290, val loss: 0.5794158577919006
Epoch 2300, training loss: 836.0552978515625 = 0.5486379861831665 + 100.0 * 8.355066299438477
Epoch 2300, val loss: 0.5783875584602356
Epoch 2310, training loss: 836.0764770507812 = 0.5473762154579163 + 100.0 * 8.355291366577148
Epoch 2310, val loss: 0.57735675573349
Epoch 2320, training loss: 836.7853393554688 = 0.54604172706604 + 100.0 * 8.362393379211426
Epoch 2320, val loss: 0.5762425065040588
Epoch 2330, training loss: 836.2910766601562 = 0.5446100234985352 + 100.0 * 8.357464790344238
Epoch 2330, val loss: 0.5752295255661011
Epoch 2340, training loss: 836.0714721679688 = 0.5432692170143127 + 100.0 * 8.355281829833984
Epoch 2340, val loss: 0.5739971399307251
Epoch 2350, training loss: 835.9851684570312 = 0.542064368724823 + 100.0 * 8.35443115234375
Epoch 2350, val loss: 0.573156476020813
Epoch 2360, training loss: 835.9468383789062 = 0.5408896207809448 + 100.0 * 8.354059219360352
Epoch 2360, val loss: 0.5721817016601562
Epoch 2370, training loss: 835.9774169921875 = 0.5397310256958008 + 100.0 * 8.354376792907715
Epoch 2370, val loss: 0.5713070034980774
Epoch 2380, training loss: 836.1885986328125 = 0.5385199785232544 + 100.0 * 8.356500625610352
Epoch 2380, val loss: 0.5703169703483582
Epoch 2390, training loss: 835.9678344726562 = 0.5372673273086548 + 100.0 * 8.354305267333984
Epoch 2390, val loss: 0.5692397952079773
Epoch 2400, training loss: 835.9268798828125 = 0.5361083745956421 + 100.0 * 8.353907585144043
Epoch 2400, val loss: 0.568385899066925
Epoch 2410, training loss: 836.270751953125 = 0.5349613428115845 + 100.0 * 8.3573579788208
Epoch 2410, val loss: 0.5673531889915466
Epoch 2420, training loss: 836.1470336914062 = 0.5337026119232178 + 100.0 * 8.356133460998535
Epoch 2420, val loss: 0.5667099356651306
Epoch 2430, training loss: 835.8960571289062 = 0.5324844717979431 + 100.0 * 8.353635787963867
Epoch 2430, val loss: 0.5654927492141724
Epoch 2440, training loss: 835.8019409179688 = 0.5313870906829834 + 100.0 * 8.352705955505371
Epoch 2440, val loss: 0.564727246761322
Epoch 2450, training loss: 835.7721557617188 = 0.5303424000740051 + 100.0 * 8.352417945861816
Epoch 2450, val loss: 0.5639457702636719
Epoch 2460, training loss: 835.7826538085938 = 0.5293042063713074 + 100.0 * 8.352533340454102
Epoch 2460, val loss: 0.5631620287895203
Epoch 2470, training loss: 836.2467651367188 = 0.5282427072525024 + 100.0 * 8.357185363769531
Epoch 2470, val loss: 0.562376856803894
Epoch 2480, training loss: 835.8388061523438 = 0.5270149111747742 + 100.0 * 8.353117942810059
Epoch 2480, val loss: 0.56132572889328
Epoch 2490, training loss: 835.7280883789062 = 0.5258854031562805 + 100.0 * 8.352022171020508
Epoch 2490, val loss: 0.5605008006095886
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7777777777777777
0.8157646888357604
The final CL Acc:0.77237, 0.00480, The final GNN Acc:0.81659, 0.00132
raw graph: torch.Size([2, 88648])
add edge: torch.Size([2, 110674])
remove edge: torch.Size([2, 66400])
updated graph: torch.Size([2, 88426])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 1059.3291015625 = 1.1063172817230225 + 100.0 * 10.58222770690918
Epoch 0, val loss: 1.1051666736602783
Epoch 10, training loss: 1059.2679443359375 = 1.1019929647445679 + 100.0 * 10.581658363342285
Epoch 10, val loss: 1.1008511781692505
Epoch 20, training loss: 1059.0185546875 = 1.0975403785705566 + 100.0 * 10.57921028137207
Epoch 20, val loss: 1.0963910818099976
Epoch 30, training loss: 1057.9322509765625 = 1.0927189588546753 + 100.0 * 10.568394660949707
Epoch 30, val loss: 1.0915367603302002
Epoch 40, training loss: 1053.9051513671875 = 1.0873130559921265 + 100.0 * 10.528178215026855
Epoch 40, val loss: 1.0861037969589233
Epoch 50, training loss: 1042.65771484375 = 1.0813347101211548 + 100.0 * 10.415763854980469
Epoch 50, val loss: 1.0801405906677246
Epoch 60, training loss: 1016.1132202148438 = 1.0750080347061157 + 100.0 * 10.150382041931152
Epoch 60, val loss: 1.0738542079925537
Epoch 70, training loss: 969.6322021484375 = 1.0678489208221436 + 100.0 * 9.685643196105957
Epoch 70, val loss: 1.0667285919189453
Epoch 80, training loss: 947.5956420898438 = 1.0612356662750244 + 100.0 * 9.465344429016113
Epoch 80, val loss: 1.0603655576705933
Epoch 90, training loss: 934.445068359375 = 1.0570118427276611 + 100.0 * 9.333880424499512
Epoch 90, val loss: 1.0563287734985352
Epoch 100, training loss: 918.4287109375 = 1.054550290107727 + 100.0 * 9.173741340637207
Epoch 100, val loss: 1.0539056062698364
Epoch 110, training loss: 910.8309326171875 = 1.0523775815963745 + 100.0 * 9.097785949707031
Epoch 110, val loss: 1.0517733097076416
Epoch 120, training loss: 904.3189086914062 = 1.0498002767562866 + 100.0 * 9.03269100189209
Epoch 120, val loss: 1.0492459535598755
Epoch 130, training loss: 896.9746704101562 = 1.0470945835113525 + 100.0 * 8.959275245666504
Epoch 130, val loss: 1.0466837882995605
Epoch 140, training loss: 892.2797241210938 = 1.0452322959899902 + 100.0 * 8.912344932556152
Epoch 140, val loss: 1.0449827909469604
Epoch 150, training loss: 888.80322265625 = 1.043976902961731 + 100.0 * 8.877592086791992
Epoch 150, val loss: 1.0437766313552856
Epoch 160, training loss: 884.4754638671875 = 1.042960286140442 + 100.0 * 8.834324836730957
Epoch 160, val loss: 1.0427987575531006
Epoch 170, training loss: 880.883056640625 = 1.042220115661621 + 100.0 * 8.798408508300781
Epoch 170, val loss: 1.0420747995376587
Epoch 180, training loss: 878.5746459960938 = 1.041401743888855 + 100.0 * 8.7753324508667
Epoch 180, val loss: 1.0412291288375854
Epoch 190, training loss: 875.7399291992188 = 1.0403764247894287 + 100.0 * 8.746994972229004
Epoch 190, val loss: 1.0402252674102783
Epoch 200, training loss: 872.5352172851562 = 1.0396838188171387 + 100.0 * 8.71495532989502
Epoch 200, val loss: 1.039610505104065
Epoch 210, training loss: 870.7647705078125 = 1.0391790866851807 + 100.0 * 8.697256088256836
Epoch 210, val loss: 1.0391091108322144
Epoch 220, training loss: 869.1827392578125 = 1.0381520986557007 + 100.0 * 8.681446075439453
Epoch 220, val loss: 1.0380771160125732
Epoch 230, training loss: 867.3124389648438 = 1.0370937585830688 + 100.0 * 8.662753105163574
Epoch 230, val loss: 1.0371108055114746
Epoch 240, training loss: 865.2572021484375 = 1.0363821983337402 + 100.0 * 8.642208099365234
Epoch 240, val loss: 1.03648042678833
Epoch 250, training loss: 863.3447265625 = 1.0358089208602905 + 100.0 * 8.623088836669922
Epoch 250, val loss: 1.035951018333435
Epoch 260, training loss: 861.7352294921875 = 1.0350478887557983 + 100.0 * 8.607002258300781
Epoch 260, val loss: 1.0352078676223755
Epoch 270, training loss: 860.19189453125 = 1.0340722799301147 + 100.0 * 8.591578483581543
Epoch 270, val loss: 1.034258246421814
Epoch 280, training loss: 858.45263671875 = 1.033265471458435 + 100.0 * 8.574193954467773
Epoch 280, val loss: 1.0334805250167847
Epoch 290, training loss: 856.904296875 = 1.0325819253921509 + 100.0 * 8.558716773986816
Epoch 290, val loss: 1.0328197479248047
Epoch 300, training loss: 855.5608520507812 = 1.0317057371139526 + 100.0 * 8.545291900634766
Epoch 300, val loss: 1.031949520111084
Epoch 310, training loss: 854.4389038085938 = 1.030652642250061 + 100.0 * 8.534082412719727
Epoch 310, val loss: 1.0309042930603027
Epoch 320, training loss: 853.48486328125 = 1.0295250415802002 + 100.0 * 8.524553298950195
Epoch 320, val loss: 1.0298255681991577
Epoch 330, training loss: 852.582763671875 = 1.0283747911453247 + 100.0 * 8.515543937683105
Epoch 330, val loss: 1.02870512008667
Epoch 340, training loss: 851.7435913085938 = 1.0272812843322754 + 100.0 * 8.507163047790527
Epoch 340, val loss: 1.0276602506637573
Epoch 350, training loss: 850.9403686523438 = 1.0263010263442993 + 100.0 * 8.499140739440918
Epoch 350, val loss: 1.0267109870910645
Epoch 360, training loss: 850.2577514648438 = 1.0252413749694824 + 100.0 * 8.492324829101562
Epoch 360, val loss: 1.025665521621704
Epoch 370, training loss: 849.57080078125 = 1.0241429805755615 + 100.0 * 8.485466003417969
Epoch 370, val loss: 1.0245933532714844
Epoch 380, training loss: 848.9146728515625 = 1.0230685472488403 + 100.0 * 8.47891616821289
Epoch 380, val loss: 1.023537278175354
Epoch 390, training loss: 848.3482666015625 = 1.0219238996505737 + 100.0 * 8.47326374053955
Epoch 390, val loss: 1.0224088430404663
Epoch 400, training loss: 847.8574829101562 = 1.0206490755081177 + 100.0 * 8.468368530273438
Epoch 400, val loss: 1.021147608757019
Epoch 410, training loss: 847.2888793945312 = 1.0193257331848145 + 100.0 * 8.462695121765137
Epoch 410, val loss: 1.0198417901992798
Epoch 420, training loss: 846.817626953125 = 1.017992615699768 + 100.0 * 8.457996368408203
Epoch 420, val loss: 1.0185273885726929
Epoch 430, training loss: 846.439697265625 = 1.0165989398956299 + 100.0 * 8.454231262207031
Epoch 430, val loss: 1.01717209815979
Epoch 440, training loss: 846.6801147460938 = 1.0151360034942627 + 100.0 * 8.456649780273438
Epoch 440, val loss: 1.01566743850708
Epoch 450, training loss: 845.8209228515625 = 1.013506293296814 + 100.0 * 8.448074340820312
Epoch 450, val loss: 1.0141180753707886
Epoch 460, training loss: 845.3997192382812 = 1.0120269060134888 + 100.0 * 8.443877220153809
Epoch 460, val loss: 1.012651801109314
Epoch 470, training loss: 845.0455322265625 = 1.0105383396148682 + 100.0 * 8.440349578857422
Epoch 470, val loss: 1.0111783742904663
Epoch 480, training loss: 844.7322387695312 = 1.0090035200119019 + 100.0 * 8.43723201751709
Epoch 480, val loss: 1.009660243988037
Epoch 490, training loss: 844.4083251953125 = 1.0074669122695923 + 100.0 * 8.434008598327637
Epoch 490, val loss: 1.0081428289413452
Epoch 500, training loss: 844.174072265625 = 1.0059354305267334 + 100.0 * 8.431681632995605
Epoch 500, val loss: 1.0066306591033936
Epoch 510, training loss: 844.1018676757812 = 1.004275918006897 + 100.0 * 8.430975914001465
Epoch 510, val loss: 1.0049656629562378
Epoch 520, training loss: 843.6630859375 = 1.0025343894958496 + 100.0 * 8.426605224609375
Epoch 520, val loss: 1.0032429695129395
Epoch 530, training loss: 843.318359375 = 1.0008732080459595 + 100.0 * 8.423174858093262
Epoch 530, val loss: 1.0016180276870728
Epoch 540, training loss: 843.0259399414062 = 0.9991798400878906 + 100.0 * 8.420267105102539
Epoch 540, val loss: 0.9999256730079651
Epoch 550, training loss: 842.7725830078125 = 0.9973952174186707 + 100.0 * 8.417752265930176
Epoch 550, val loss: 0.9981513619422913
Epoch 560, training loss: 842.5568237304688 = 0.9955822229385376 + 100.0 * 8.41561222076416
Epoch 560, val loss: 0.9963539242744446
Epoch 570, training loss: 842.6281127929688 = 0.9936363101005554 + 100.0 * 8.41634464263916
Epoch 570, val loss: 0.994392991065979
Epoch 580, training loss: 842.2615356445312 = 0.9915608167648315 + 100.0 * 8.412699699401855
Epoch 580, val loss: 0.9923758506774902
Epoch 590, training loss: 841.9381103515625 = 0.989622950553894 + 100.0 * 8.40948486328125
Epoch 590, val loss: 0.9904582500457764
Epoch 600, training loss: 841.7282104492188 = 0.9875835180282593 + 100.0 * 8.407405853271484
Epoch 600, val loss: 0.9884467720985413
Epoch 610, training loss: 841.5663452148438 = 0.9854608178138733 + 100.0 * 8.405808448791504
Epoch 610, val loss: 0.9863488674163818
Epoch 620, training loss: 841.4730834960938 = 0.9832198619842529 + 100.0 * 8.404898643493652
Epoch 620, val loss: 0.9841121435165405
Epoch 630, training loss: 841.1868896484375 = 0.9809719324111938 + 100.0 * 8.402059555053711
Epoch 630, val loss: 0.981922447681427
Epoch 640, training loss: 841.0232543945312 = 0.9787840247154236 + 100.0 * 8.400444984436035
Epoch 640, val loss: 0.9797654747962952
Epoch 650, training loss: 840.8677368164062 = 0.9764887690544128 + 100.0 * 8.39891242980957
Epoch 650, val loss: 0.9775136709213257
Epoch 660, training loss: 840.8538818359375 = 0.9740564227104187 + 100.0 * 8.398797988891602
Epoch 660, val loss: 0.9751099944114685
Epoch 670, training loss: 840.5107421875 = 0.9716402888298035 + 100.0 * 8.395391464233398
Epoch 670, val loss: 0.9727386236190796
Epoch 680, training loss: 840.3565063476562 = 0.9693047404289246 + 100.0 * 8.393872261047363
Epoch 680, val loss: 0.9704110026359558
Epoch 690, training loss: 840.1995239257812 = 0.9668687582015991 + 100.0 * 8.392326354980469
Epoch 690, val loss: 0.9680201411247253
Epoch 700, training loss: 840.3812866210938 = 0.9642586708068848 + 100.0 * 8.394170761108398
Epoch 700, val loss: 0.9653854966163635
Epoch 710, training loss: 839.9954223632812 = 0.961554765701294 + 100.0 * 8.390338897705078
Epoch 710, val loss: 0.9628133773803711
Epoch 720, training loss: 839.8062133789062 = 0.9590255618095398 + 100.0 * 8.388471603393555
Epoch 720, val loss: 0.9603428840637207
Epoch 730, training loss: 839.6497192382812 = 0.9563835263252258 + 100.0 * 8.386933326721191
Epoch 730, val loss: 0.9577280879020691
Epoch 740, training loss: 839.657958984375 = 0.9536444544792175 + 100.0 * 8.387042999267578
Epoch 740, val loss: 0.955024003982544
Epoch 750, training loss: 839.4517822265625 = 0.9506415724754333 + 100.0 * 8.385011672973633
Epoch 750, val loss: 0.9521210789680481
Epoch 760, training loss: 839.243896484375 = 0.9478611350059509 + 100.0 * 8.382960319519043
Epoch 760, val loss: 0.9494163990020752
Epoch 770, training loss: 839.11376953125 = 0.9450644254684448 + 100.0 * 8.38168716430664
Epoch 770, val loss: 0.9467077255249023
Epoch 780, training loss: 838.9750366210938 = 0.9421983957290649 + 100.0 * 8.380328178405762
Epoch 780, val loss: 0.943919837474823
Epoch 790, training loss: 838.8614501953125 = 0.9392935037612915 + 100.0 * 8.37922191619873
Epoch 790, val loss: 0.941119909286499
Epoch 800, training loss: 838.8377685546875 = 0.9361567497253418 + 100.0 * 8.379015922546387
Epoch 800, val loss: 0.9380937814712524
Epoch 810, training loss: 838.6991577148438 = 0.933104932308197 + 100.0 * 8.377660751342773
Epoch 810, val loss: 0.9351206421852112
Epoch 820, training loss: 838.4979858398438 = 0.9301550984382629 + 100.0 * 8.375678062438965
Epoch 820, val loss: 0.9322431087493896
Epoch 830, training loss: 838.399169921875 = 0.9271483421325684 + 100.0 * 8.374720573425293
Epoch 830, val loss: 0.9293076395988464
Epoch 840, training loss: 838.297119140625 = 0.9240735173225403 + 100.0 * 8.373730659484863
Epoch 840, val loss: 0.9263145327568054
Epoch 850, training loss: 838.1854248046875 = 0.9209436178207397 + 100.0 * 8.372644424438477
Epoch 850, val loss: 0.9233269095420837
Epoch 860, training loss: 837.9623413085938 = 0.9177151322364807 + 100.0 * 8.37044620513916
Epoch 860, val loss: 0.92026686668396
Epoch 870, training loss: 837.851806640625 = 0.9145665764808655 + 100.0 * 8.369372367858887
Epoch 870, val loss: 0.9171484708786011
Epoch 880, training loss: 837.897705078125 = 0.9113907814025879 + 100.0 * 8.369863510131836
Epoch 880, val loss: 0.9141204357147217
Epoch 890, training loss: 837.6705932617188 = 0.9081020355224609 + 100.0 * 8.36762523651123
Epoch 890, val loss: 0.9109720587730408
Epoch 900, training loss: 837.4947509765625 = 0.9048559665679932 + 100.0 * 8.365899085998535
Epoch 900, val loss: 0.90790855884552
Epoch 910, training loss: 837.3842163085938 = 0.9016784429550171 + 100.0 * 8.364825248718262
Epoch 910, val loss: 0.9048342704772949
Epoch 920, training loss: 837.53271484375 = 0.8982805609703064 + 100.0 * 8.366344451904297
Epoch 920, val loss: 0.9016671180725098
Epoch 930, training loss: 837.1610107421875 = 0.8950581550598145 + 100.0 * 8.362659454345703
Epoch 930, val loss: 0.8984484076499939
Epoch 940, training loss: 836.9750366210938 = 0.8917851448059082 + 100.0 * 8.360832214355469
Epoch 940, val loss: 0.8953829407691956
Epoch 950, training loss: 836.925537109375 = 0.888536274433136 + 100.0 * 8.360369682312012
Epoch 950, val loss: 0.8922882080078125
Epoch 960, training loss: 837.0137329101562 = 0.8851474523544312 + 100.0 * 8.361286163330078
Epoch 960, val loss: 0.8889123797416687
Epoch 970, training loss: 836.7435302734375 = 0.8817740082740784 + 100.0 * 8.358617782592773
Epoch 970, val loss: 0.8859302997589111
Epoch 980, training loss: 836.5421752929688 = 0.8786287903785706 + 100.0 * 8.356636047363281
Epoch 980, val loss: 0.8828440308570862
Epoch 990, training loss: 836.4461669921875 = 0.8754335045814514 + 100.0 * 8.355707168579102
Epoch 990, val loss: 0.8797364830970764
Epoch 1000, training loss: 836.6011352539062 = 0.8721728324890137 + 100.0 * 8.35728931427002
Epoch 1000, val loss: 0.876620888710022
Epoch 1010, training loss: 836.3990478515625 = 0.8686764240264893 + 100.0 * 8.355303764343262
Epoch 1010, val loss: 0.8734583854675293
Epoch 1020, training loss: 836.2744140625 = 0.865432620048523 + 100.0 * 8.354089736938477
Epoch 1020, val loss: 0.8702607750892639
Epoch 1030, training loss: 836.0971069335938 = 0.8620864748954773 + 100.0 * 8.352350234985352
Epoch 1030, val loss: 0.8671160936355591
Epoch 1040, training loss: 835.9747924804688 = 0.8588902950286865 + 100.0 * 8.35115909576416
Epoch 1040, val loss: 0.8640632033348083
Epoch 1050, training loss: 835.9722290039062 = 0.8556740880012512 + 100.0 * 8.351165771484375
Epoch 1050, val loss: 0.8609260320663452
Epoch 1060, training loss: 835.975830078125 = 0.8521060943603516 + 100.0 * 8.351237297058105
Epoch 1060, val loss: 0.8575538396835327
Epoch 1070, training loss: 835.7486572265625 = 0.8487799763679504 + 100.0 * 8.3489990234375
Epoch 1070, val loss: 0.8544483780860901
Epoch 1080, training loss: 835.6959228515625 = 0.8456085920333862 + 100.0 * 8.348503112792969
Epoch 1080, val loss: 0.8514434099197388
Epoch 1090, training loss: 835.603515625 = 0.8423556685447693 + 100.0 * 8.347611427307129
Epoch 1090, val loss: 0.8483401536941528
Epoch 1100, training loss: 835.67041015625 = 0.8390745520591736 + 100.0 * 8.348313331604004
Epoch 1100, val loss: 0.8452335000038147
Epoch 1110, training loss: 835.6297607421875 = 0.8357144594192505 + 100.0 * 8.347940444946289
Epoch 1110, val loss: 0.8420061469078064
Epoch 1120, training loss: 835.4478759765625 = 0.8323299884796143 + 100.0 * 8.346155166625977
Epoch 1120, val loss: 0.838882327079773
Epoch 1130, training loss: 835.3488159179688 = 0.8291448354721069 + 100.0 * 8.345196723937988
Epoch 1130, val loss: 0.8357883095741272
Epoch 1140, training loss: 835.3193969726562 = 0.8259149789810181 + 100.0 * 8.344934463500977
Epoch 1140, val loss: 0.8326953649520874
Epoch 1150, training loss: 835.33837890625 = 0.8226373195648193 + 100.0 * 8.345157623291016
Epoch 1150, val loss: 0.8295838236808777
Epoch 1160, training loss: 835.2026977539062 = 0.8192883729934692 + 100.0 * 8.343833923339844
Epoch 1160, val loss: 0.8264780640602112
Epoch 1170, training loss: 835.125 = 0.8160842657089233 + 100.0 * 8.34308910369873
Epoch 1170, val loss: 0.823423445224762
Epoch 1180, training loss: 835.0319213867188 = 0.8129268884658813 + 100.0 * 8.34218978881836
Epoch 1180, val loss: 0.8204123973846436
Epoch 1190, training loss: 835.015869140625 = 0.8097299933433533 + 100.0 * 8.342061042785645
Epoch 1190, val loss: 0.8173325657844543
Epoch 1200, training loss: 835.300537109375 = 0.8062219023704529 + 100.0 * 8.344943046569824
Epoch 1200, val loss: 0.8139767646789551
Epoch 1210, training loss: 834.8610229492188 = 0.8028455376625061 + 100.0 * 8.340581893920898
Epoch 1210, val loss: 0.8108539581298828
Epoch 1220, training loss: 834.8341064453125 = 0.7996993660926819 + 100.0 * 8.340344429016113
Epoch 1220, val loss: 0.8079870939254761
Epoch 1230, training loss: 834.7088623046875 = 0.7965784072875977 + 100.0 * 8.339122772216797
Epoch 1230, val loss: 0.8049535751342773
Epoch 1240, training loss: 834.6535034179688 = 0.7933946847915649 + 100.0 * 8.338601112365723
Epoch 1240, val loss: 0.8019466996192932
Epoch 1250, training loss: 834.7176513671875 = 0.7902339100837708 + 100.0 * 8.339274406433105
Epoch 1250, val loss: 0.7989590167999268
Epoch 1260, training loss: 834.6763916015625 = 0.7868057489395142 + 100.0 * 8.338895797729492
Epoch 1260, val loss: 0.7956922650337219
Epoch 1270, training loss: 834.5546264648438 = 0.7835540771484375 + 100.0 * 8.3377103805542
Epoch 1270, val loss: 0.7927101850509644
Epoch 1280, training loss: 834.4547729492188 = 0.7804416418075562 + 100.0 * 8.336743354797363
Epoch 1280, val loss: 0.7897363901138306
Epoch 1290, training loss: 834.4046020507812 = 0.777311384677887 + 100.0 * 8.336273193359375
Epoch 1290, val loss: 0.7868008613586426
Epoch 1300, training loss: 834.585205078125 = 0.7741295099258423 + 100.0 * 8.33811092376709
Epoch 1300, val loss: 0.7837979197502136
Epoch 1310, training loss: 834.3282470703125 = 0.7706419825553894 + 100.0 * 8.335576057434082
Epoch 1310, val loss: 0.780551016330719
Epoch 1320, training loss: 834.2677612304688 = 0.767625629901886 + 100.0 * 8.335000991821289
Epoch 1320, val loss: 0.7776641845703125
Epoch 1330, training loss: 834.20458984375 = 0.7644466161727905 + 100.0 * 8.33440113067627
Epoch 1330, val loss: 0.7747477293014526
Epoch 1340, training loss: 834.1709594726562 = 0.7613682150840759 + 100.0 * 8.33409595489502
Epoch 1340, val loss: 0.771787166595459
Epoch 1350, training loss: 834.4069213867188 = 0.758144736289978 + 100.0 * 8.336487770080566
Epoch 1350, val loss: 0.7687082886695862
Epoch 1360, training loss: 834.1241455078125 = 0.7548667192459106 + 100.0 * 8.33369255065918
Epoch 1360, val loss: 0.7657434940338135
Epoch 1370, training loss: 834.0123291015625 = 0.751774787902832 + 100.0 * 8.332605361938477
Epoch 1370, val loss: 0.7628281712532043
Epoch 1380, training loss: 833.972412109375 = 0.7486448884010315 + 100.0 * 8.332237243652344
Epoch 1380, val loss: 0.7599778175354004
Epoch 1390, training loss: 834.33154296875 = 0.7454217076301575 + 100.0 * 8.335861206054688
Epoch 1390, val loss: 0.7569741010665894
Epoch 1400, training loss: 833.966064453125 = 0.7420691251754761 + 100.0 * 8.332240104675293
Epoch 1400, val loss: 0.7536849975585938
Epoch 1410, training loss: 833.8502197265625 = 0.7389898300170898 + 100.0 * 8.331111907958984
Epoch 1410, val loss: 0.7509458065032959
Epoch 1420, training loss: 833.784912109375 = 0.7359244227409363 + 100.0 * 8.330490112304688
Epoch 1420, val loss: 0.7479619383811951
Epoch 1430, training loss: 833.7323608398438 = 0.7328211069107056 + 100.0 * 8.329995155334473
Epoch 1430, val loss: 0.7451547980308533
Epoch 1440, training loss: 833.8196411132812 = 0.7297618389129639 + 100.0 * 8.330899238586426
Epoch 1440, val loss: 0.7422654032707214
Epoch 1450, training loss: 833.6834716796875 = 0.7264935970306396 + 100.0 * 8.329569816589355
Epoch 1450, val loss: 0.7392107248306274
Epoch 1460, training loss: 833.8218383789062 = 0.723427951335907 + 100.0 * 8.330984115600586
Epoch 1460, val loss: 0.736318051815033
Epoch 1470, training loss: 833.603271484375 = 0.7200978994369507 + 100.0 * 8.328831672668457
Epoch 1470, val loss: 0.733242928981781
Epoch 1480, training loss: 833.538330078125 = 0.7170693278312683 + 100.0 * 8.32821273803711
Epoch 1480, val loss: 0.7304794788360596
Epoch 1490, training loss: 833.4933471679688 = 0.7140393257141113 + 100.0 * 8.32779312133789
Epoch 1490, val loss: 0.7276166081428528
Epoch 1500, training loss: 833.533203125 = 0.7109798192977905 + 100.0 * 8.328222274780273
Epoch 1500, val loss: 0.7248183488845825
Epoch 1510, training loss: 833.6948852539062 = 0.7076842188835144 + 100.0 * 8.329872131347656
Epoch 1510, val loss: 0.7217360138893127
Epoch 1520, training loss: 833.4754638671875 = 0.704654335975647 + 100.0 * 8.32770824432373
Epoch 1520, val loss: 0.7189028263092041
Epoch 1530, training loss: 833.3689575195312 = 0.7016096711158752 + 100.0 * 8.32667350769043
Epoch 1530, val loss: 0.7161015272140503
Epoch 1540, training loss: 833.31396484375 = 0.6986345052719116 + 100.0 * 8.326153755187988
Epoch 1540, val loss: 0.7133392691612244
Epoch 1550, training loss: 833.26513671875 = 0.6956347823143005 + 100.0 * 8.325695037841797
Epoch 1550, val loss: 0.7105792760848999
Epoch 1560, training loss: 833.2806396484375 = 0.6926394104957581 + 100.0 * 8.32588005065918
Epoch 1560, val loss: 0.7078282833099365
Epoch 1570, training loss: 833.6026000976562 = 0.6894108057022095 + 100.0 * 8.329132080078125
Epoch 1570, val loss: 0.704817533493042
Epoch 1580, training loss: 833.2349853515625 = 0.6862841844558716 + 100.0 * 8.32548713684082
Epoch 1580, val loss: 0.7018901705741882
Epoch 1590, training loss: 833.1456909179688 = 0.6833187937736511 + 100.0 * 8.324624061584473
Epoch 1590, val loss: 0.699122965335846
Epoch 1600, training loss: 833.1381225585938 = 0.680351197719574 + 100.0 * 8.324577331542969
Epoch 1600, val loss: 0.6963711380958557
Epoch 1610, training loss: 833.2318115234375 = 0.6773608922958374 + 100.0 * 8.325544357299805
Epoch 1610, val loss: 0.6935151219367981
Epoch 1620, training loss: 833.0848999023438 = 0.6743502616882324 + 100.0 * 8.324105262756348
Epoch 1620, val loss: 0.6909313201904297
Epoch 1630, training loss: 833.0137939453125 = 0.6714747548103333 + 100.0 * 8.323423385620117
Epoch 1630, val loss: 0.6881405711174011
Epoch 1640, training loss: 833.2069091796875 = 0.6685307621955872 + 100.0 * 8.325384140014648
Epoch 1640, val loss: 0.6854365468025208
Epoch 1650, training loss: 833.1007690429688 = 0.6649892926216125 + 100.0 * 8.324357986450195
Epoch 1650, val loss: 0.6821798086166382
Epoch 1660, training loss: 832.957763671875 = 0.6621794700622559 + 100.0 * 8.322956085205078
Epoch 1660, val loss: 0.6795591115951538
Epoch 1670, training loss: 832.9285278320312 = 0.6592495441436768 + 100.0 * 8.32269287109375
Epoch 1670, val loss: 0.676915168762207
Epoch 1680, training loss: 832.8369140625 = 0.6564798951148987 + 100.0 * 8.32180404663086
Epoch 1680, val loss: 0.6743353605270386
Epoch 1690, training loss: 832.8040771484375 = 0.6536917686462402 + 100.0 * 8.321503639221191
Epoch 1690, val loss: 0.6717491745948792
Epoch 1700, training loss: 832.7688598632812 = 0.6509397029876709 + 100.0 * 8.321179389953613
Epoch 1700, val loss: 0.6692051887512207
Epoch 1710, training loss: 832.7385864257812 = 0.6481420993804932 + 100.0 * 8.320904731750488
Epoch 1710, val loss: 0.6666291952133179
Epoch 1720, training loss: 832.7259521484375 = 0.645363450050354 + 100.0 * 8.320805549621582
Epoch 1720, val loss: 0.6640481948852539
Epoch 1730, training loss: 833.233154296875 = 0.6424290537834167 + 100.0 * 8.325906753540039
Epoch 1730, val loss: 0.6612867712974548
Epoch 1740, training loss: 832.8126220703125 = 0.6393252015113831 + 100.0 * 8.321732521057129
Epoch 1740, val loss: 0.6585293412208557
Epoch 1750, training loss: 832.6326904296875 = 0.6366313099861145 + 100.0 * 8.319960594177246
Epoch 1750, val loss: 0.6559610366821289
Epoch 1760, training loss: 832.59521484375 = 0.6338468194007874 + 100.0 * 8.319613456726074
Epoch 1760, val loss: 0.6534276008605957
Epoch 1770, training loss: 832.5607299804688 = 0.6311861872673035 + 100.0 * 8.319295883178711
Epoch 1770, val loss: 0.6509654521942139
Epoch 1780, training loss: 832.5259399414062 = 0.6285080909729004 + 100.0 * 8.318974494934082
Epoch 1780, val loss: 0.6484821438789368
Epoch 1790, training loss: 832.693359375 = 0.6258246302604675 + 100.0 * 8.320674896240234
Epoch 1790, val loss: 0.6459265947341919
Epoch 1800, training loss: 832.4979248046875 = 0.622506320476532 + 100.0 * 8.318754196166992
Epoch 1800, val loss: 0.642983615398407
Epoch 1810, training loss: 832.5966796875 = 0.6198343634605408 + 100.0 * 8.319768905639648
Epoch 1810, val loss: 0.6404345035552979
Epoch 1820, training loss: 832.4238891601562 = 0.6171525120735168 + 100.0 * 8.31806755065918
Epoch 1820, val loss: 0.6380061507225037
Epoch 1830, training loss: 832.3931884765625 = 0.6145533323287964 + 100.0 * 8.31778621673584
Epoch 1830, val loss: 0.6356671452522278
Epoch 1840, training loss: 832.3458251953125 = 0.6119930148124695 + 100.0 * 8.317337989807129
Epoch 1840, val loss: 0.6332935094833374
Epoch 1850, training loss: 832.4393920898438 = 0.6094413995742798 + 100.0 * 8.318299293518066
Epoch 1850, val loss: 0.6309493184089661
Epoch 1860, training loss: 832.3084106445312 = 0.6065807938575745 + 100.0 * 8.317018508911133
Epoch 1860, val loss: 0.628307044506073
Epoch 1870, training loss: 832.2933959960938 = 0.6040354371070862 + 100.0 * 8.316893577575684
Epoch 1870, val loss: 0.6260295510292053
Epoch 1880, training loss: 832.2593994140625 = 0.60147625207901 + 100.0 * 8.31657886505127
Epoch 1880, val loss: 0.6236433982849121
Epoch 1890, training loss: 832.208740234375 = 0.5989865064620972 + 100.0 * 8.316097259521484
Epoch 1890, val loss: 0.6213895678520203
Epoch 1900, training loss: 832.173583984375 = 0.59649658203125 + 100.0 * 8.315771102905273
Epoch 1900, val loss: 0.6191158890724182
Epoch 1910, training loss: 832.148193359375 = 0.5940378308296204 + 100.0 * 8.31554126739502
Epoch 1910, val loss: 0.6168646216392517
Epoch 1920, training loss: 832.3485107421875 = 0.5915681719779968 + 100.0 * 8.317569732666016
Epoch 1920, val loss: 0.6145520210266113
Epoch 1930, training loss: 832.1973876953125 = 0.5885005593299866 + 100.0 * 8.316088676452637
Epoch 1930, val loss: 0.611851155757904
Epoch 1940, training loss: 832.1888427734375 = 0.5861759185791016 + 100.0 * 8.31602668762207
Epoch 1940, val loss: 0.6096692681312561
Epoch 1950, training loss: 832.0618286132812 = 0.5836502909660339 + 100.0 * 8.314781188964844
Epoch 1950, val loss: 0.6074318885803223
Epoch 1960, training loss: 832.0206909179688 = 0.5813083648681641 + 100.0 * 8.314393997192383
Epoch 1960, val loss: 0.6052599549293518
Epoch 1970, training loss: 831.9888305664062 = 0.5789552927017212 + 100.0 * 8.314098358154297
Epoch 1970, val loss: 0.6031531095504761
Epoch 1980, training loss: 831.961669921875 = 0.5766552090644836 + 100.0 * 8.313850402832031
Epoch 1980, val loss: 0.6010494828224182
Epoch 1990, training loss: 831.9891967773438 = 0.5743024349212646 + 100.0 * 8.314148902893066
Epoch 1990, val loss: 0.5989483594894409
Epoch 2000, training loss: 832.3792724609375 = 0.5716727375984192 + 100.0 * 8.318076133728027
Epoch 2000, val loss: 0.5965564846992493
Epoch 2010, training loss: 831.9293823242188 = 0.5692128539085388 + 100.0 * 8.31360149383545
Epoch 2010, val loss: 0.594279944896698
Epoch 2020, training loss: 831.9255981445312 = 0.5668684840202332 + 100.0 * 8.313587188720703
Epoch 2020, val loss: 0.5921840071678162
Epoch 2030, training loss: 831.8402709960938 = 0.5646333694458008 + 100.0 * 8.312756538391113
Epoch 2030, val loss: 0.5901390910148621
Epoch 2040, training loss: 831.8164672851562 = 0.5624069571495056 + 100.0 * 8.312541007995605
Epoch 2040, val loss: 0.5881178975105286
Epoch 2050, training loss: 831.8179321289062 = 0.5601900815963745 + 100.0 * 8.312577247619629
Epoch 2050, val loss: 0.586094319820404
Epoch 2060, training loss: 832.1741943359375 = 0.5578622817993164 + 100.0 * 8.316163063049316
Epoch 2060, val loss: 0.5839434266090393
Epoch 2070, training loss: 831.904052734375 = 0.5553755760192871 + 100.0 * 8.31348705291748
Epoch 2070, val loss: 0.5818619132041931
Epoch 2080, training loss: 831.7625122070312 = 0.5531885027885437 + 100.0 * 8.312093734741211
Epoch 2080, val loss: 0.5797743201255798
Epoch 2090, training loss: 831.70703125 = 0.5509682297706604 + 100.0 * 8.31156063079834
Epoch 2090, val loss: 0.5777766108512878
Epoch 2100, training loss: 831.6767578125 = 0.5488329529762268 + 100.0 * 8.311279296875
Epoch 2100, val loss: 0.5758877992630005
Epoch 2110, training loss: 831.6605834960938 = 0.5466933250427246 + 100.0 * 8.311139106750488
Epoch 2110, val loss: 0.5739488005638123
Epoch 2120, training loss: 831.8421630859375 = 0.5445383191108704 + 100.0 * 8.312975883483887
Epoch 2120, val loss: 0.5719928741455078
Epoch 2130, training loss: 831.7677612304688 = 0.5421518087387085 + 100.0 * 8.312255859375
Epoch 2130, val loss: 0.5699374675750732
Epoch 2140, training loss: 831.7435913085938 = 0.5398590564727783 + 100.0 * 8.312037467956543
Epoch 2140, val loss: 0.567811131477356
Epoch 2150, training loss: 831.6370849609375 = 0.5377641320228577 + 100.0 * 8.310993194580078
Epoch 2150, val loss: 0.5658840537071228
Epoch 2160, training loss: 831.5481567382812 = 0.5356771349906921 + 100.0 * 8.310124397277832
Epoch 2160, val loss: 0.5640578269958496
Epoch 2170, training loss: 831.5424194335938 = 0.533691942691803 + 100.0 * 8.310087203979492
Epoch 2170, val loss: 0.562294065952301
Epoch 2180, training loss: 831.54638671875 = 0.5316659212112427 + 100.0 * 8.310147285461426
Epoch 2180, val loss: 0.5604896545410156
Epoch 2190, training loss: 831.7250366210938 = 0.5296047329902649 + 100.0 * 8.311954498291016
Epoch 2190, val loss: 0.5586702227592468
Epoch 2200, training loss: 831.4768676757812 = 0.5274569988250732 + 100.0 * 8.309494018554688
Epoch 2200, val loss: 0.5566948652267456
Epoch 2210, training loss: 831.5116577148438 = 0.5254850387573242 + 100.0 * 8.309861183166504
Epoch 2210, val loss: 0.5548731684684753
Epoch 2220, training loss: 831.4241943359375 = 0.523491382598877 + 100.0 * 8.309006690979004
Epoch 2220, val loss: 0.5531498789787292
Epoch 2230, training loss: 831.4356689453125 = 0.5215544104576111 + 100.0 * 8.309141159057617
Epoch 2230, val loss: 0.5514159798622131
Epoch 2240, training loss: 831.5552368164062 = 0.5195940136909485 + 100.0 * 8.310356140136719
Epoch 2240, val loss: 0.5496407747268677
Epoch 2250, training loss: 831.4271240234375 = 0.5175550580024719 + 100.0 * 8.30909538269043
Epoch 2250, val loss: 0.547796905040741
Epoch 2260, training loss: 831.6643676757812 = 0.5155551433563232 + 100.0 * 8.311488151550293
Epoch 2260, val loss: 0.5459286570549011
Epoch 2270, training loss: 831.3687133789062 = 0.513569176197052 + 100.0 * 8.308551788330078
Epoch 2270, val loss: 0.5443297028541565
Epoch 2280, training loss: 831.3395385742188 = 0.511664867401123 + 100.0 * 8.308279037475586
Epoch 2280, val loss: 0.5426316261291504
Epoch 2290, training loss: 831.3375244140625 = 0.5098658800125122 + 100.0 * 8.308276176452637
Epoch 2290, val loss: 0.5409970879554749
Epoch 2300, training loss: 831.4119262695312 = 0.5080136656761169 + 100.0 * 8.309039115905762
Epoch 2300, val loss: 0.5394057631492615
Epoch 2310, training loss: 831.2587280273438 = 0.5061851143836975 + 100.0 * 8.307525634765625
Epoch 2310, val loss: 0.537716805934906
Epoch 2320, training loss: 831.2720947265625 = 0.5043867230415344 + 100.0 * 8.307677268981934
Epoch 2320, val loss: 0.5361090302467346
Epoch 2330, training loss: 831.3840942382812 = 0.5026302933692932 + 100.0 * 8.308815002441406
Epoch 2330, val loss: 0.5345137119293213
Epoch 2340, training loss: 831.5657348632812 = 0.5006703734397888 + 100.0 * 8.310650825500488
Epoch 2340, val loss: 0.5327067971229553
Epoch 2350, training loss: 831.2445678710938 = 0.4988119304180145 + 100.0 * 8.307456970214844
Epoch 2350, val loss: 0.5312656760215759
Epoch 2360, training loss: 831.1903686523438 = 0.4970308840274811 + 100.0 * 8.306933403015137
Epoch 2360, val loss: 0.5296196937561035
Epoch 2370, training loss: 831.1663818359375 = 0.49539071321487427 + 100.0 * 8.306710243225098
Epoch 2370, val loss: 0.5281721353530884
Epoch 2380, training loss: 831.1338500976562 = 0.4937123954296112 + 100.0 * 8.306401252746582
Epoch 2380, val loss: 0.5266832709312439
Epoch 2390, training loss: 831.1295166015625 = 0.4920869469642639 + 100.0 * 8.306374549865723
Epoch 2390, val loss: 0.525250256061554
Epoch 2400, training loss: 831.6885986328125 = 0.4903751015663147 + 100.0 * 8.311982154846191
Epoch 2400, val loss: 0.5236160755157471
Epoch 2410, training loss: 831.3862915039062 = 0.4884485900402069 + 100.0 * 8.308978080749512
Epoch 2410, val loss: 0.5222348570823669
Epoch 2420, training loss: 831.08349609375 = 0.4867754280567169 + 100.0 * 8.305967330932617
Epoch 2420, val loss: 0.5206115245819092
Epoch 2430, training loss: 831.061767578125 = 0.4851793050765991 + 100.0 * 8.305766105651855
Epoch 2430, val loss: 0.5192036628723145
Epoch 2440, training loss: 831.047607421875 = 0.48360827565193176 + 100.0 * 8.30564022064209
Epoch 2440, val loss: 0.5178513526916504
Epoch 2450, training loss: 831.28076171875 = 0.48204341530799866 + 100.0 * 8.307987213134766
Epoch 2450, val loss: 0.5163838267326355
Epoch 2460, training loss: 830.998291015625 = 0.4802440106868744 + 100.0 * 8.305180549621582
Epoch 2460, val loss: 0.514921247959137
Epoch 2470, training loss: 831.0022583007812 = 0.4787465035915375 + 100.0 * 8.305234909057617
Epoch 2470, val loss: 0.5136203169822693
Epoch 2480, training loss: 830.9774169921875 = 0.47719600796699524 + 100.0 * 8.305002212524414
Epoch 2480, val loss: 0.5122480988502502
Epoch 2490, training loss: 830.9400634765625 = 0.4757317304611206 + 100.0 * 8.304643630981445
Epoch 2490, val loss: 0.5110313892364502
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8046676813800101
0.8643773092805912
=== training gcn model ===
Epoch 0, training loss: 1059.3243408203125 = 1.10148024559021 + 100.0 * 10.582229614257812
Epoch 0, val loss: 1.1026067733764648
Epoch 10, training loss: 1059.2613525390625 = 1.0970792770385742 + 100.0 * 10.581643104553223
Epoch 10, val loss: 1.0981768369674683
Epoch 20, training loss: 1059.016357421875 = 1.0921767950057983 + 100.0 * 10.579241752624512
Epoch 20, val loss: 1.0932306051254272
Epoch 30, training loss: 1058.006103515625 = 1.0869053602218628 + 100.0 * 10.569191932678223
Epoch 30, val loss: 1.0879443883895874
Epoch 40, training loss: 1054.162109375 = 1.081419587135315 + 100.0 * 10.530806541442871
Epoch 40, val loss: 1.0824416875839233
Epoch 50, training loss: 1042.4674072265625 = 1.0758804082870483 + 100.0 * 10.413914680480957
Epoch 50, val loss: 1.0768697261810303
Epoch 60, training loss: 1014.6426391601562 = 1.0706090927124023 + 100.0 * 10.135720252990723
Epoch 60, val loss: 1.0714757442474365
Epoch 70, training loss: 967.7623291015625 = 1.0649689435958862 + 100.0 * 9.666974067687988
Epoch 70, val loss: 1.0657423734664917
Epoch 80, training loss: 943.038818359375 = 1.0608470439910889 + 100.0 * 9.419779777526855
Epoch 80, val loss: 1.061610221862793
Epoch 90, training loss: 925.9891357421875 = 1.0567593574523926 + 100.0 * 9.249323844909668
Epoch 90, val loss: 1.0574616193771362
Epoch 100, training loss: 914.4788208007812 = 1.0530959367752075 + 100.0 * 9.134257316589355
Epoch 100, val loss: 1.0538828372955322
Epoch 110, training loss: 907.12646484375 = 1.0500978231430054 + 100.0 * 9.060763359069824
Epoch 110, val loss: 1.0509607791900635
Epoch 120, training loss: 898.90673828125 = 1.047746181488037 + 100.0 * 8.97859001159668
Epoch 120, val loss: 1.0487116575241089
Epoch 130, training loss: 890.6466674804688 = 1.0460690259933472 + 100.0 * 8.896005630493164
Epoch 130, val loss: 1.0471179485321045
Epoch 140, training loss: 884.2168579101562 = 1.0451158285140991 + 100.0 * 8.831717491149902
Epoch 140, val loss: 1.0461920499801636
Epoch 150, training loss: 878.75732421875 = 1.0444585084915161 + 100.0 * 8.777129173278809
Epoch 150, val loss: 1.0455307960510254
Epoch 160, training loss: 874.3961181640625 = 1.044050931930542 + 100.0 * 8.7335205078125
Epoch 160, val loss: 1.0451141595840454
Epoch 170, training loss: 870.293212890625 = 1.0437660217285156 + 100.0 * 8.69249439239502
Epoch 170, val loss: 1.0447975397109985
Epoch 180, training loss: 866.8741455078125 = 1.043285608291626 + 100.0 * 8.658308982849121
Epoch 180, val loss: 1.044268012046814
Epoch 190, training loss: 865.0054321289062 = 1.0426691770553589 + 100.0 * 8.639627456665039
Epoch 190, val loss: 1.043617844581604
Epoch 200, training loss: 863.86865234375 = 1.0418261289596558 + 100.0 * 8.628268241882324
Epoch 200, val loss: 1.042776107788086
Epoch 210, training loss: 862.4364013671875 = 1.0408437252044678 + 100.0 * 8.6139554977417
Epoch 210, val loss: 1.04185950756073
Epoch 220, training loss: 861.138671875 = 1.0400123596191406 + 100.0 * 8.60098648071289
Epoch 220, val loss: 1.041090726852417
Epoch 230, training loss: 860.0546264648438 = 1.0393544435501099 + 100.0 * 8.590152740478516
Epoch 230, val loss: 1.040448546409607
Epoch 240, training loss: 858.989501953125 = 1.0387001037597656 + 100.0 * 8.579507827758789
Epoch 240, val loss: 1.03982675075531
Epoch 250, training loss: 858.133544921875 = 1.0380223989486694 + 100.0 * 8.570955276489258
Epoch 250, val loss: 1.0391738414764404
Epoch 260, training loss: 857.0813598632812 = 1.0373504161834717 + 100.0 * 8.560440063476562
Epoch 260, val loss: 1.0385257005691528
Epoch 270, training loss: 855.9733276367188 = 1.0367639064788818 + 100.0 * 8.549365997314453
Epoch 270, val loss: 1.0379737615585327
Epoch 280, training loss: 854.7808837890625 = 1.036277413368225 + 100.0 * 8.537446022033691
Epoch 280, val loss: 1.0375311374664307
Epoch 290, training loss: 853.7706909179688 = 1.035819172859192 + 100.0 * 8.527348518371582
Epoch 290, val loss: 1.0370395183563232
Epoch 300, training loss: 852.8177490234375 = 1.0352375507354736 + 100.0 * 8.51782512664795
Epoch 300, val loss: 1.0365015268325806
Epoch 310, training loss: 851.7755126953125 = 1.0346739292144775 + 100.0 * 8.507408142089844
Epoch 310, val loss: 1.0359288454055786
Epoch 320, training loss: 850.9671020507812 = 1.034071683883667 + 100.0 * 8.499330520629883
Epoch 320, val loss: 1.0353187322616577
Epoch 330, training loss: 850.1907958984375 = 1.0334501266479492 + 100.0 * 8.491573333740234
Epoch 330, val loss: 1.034701943397522
Epoch 340, training loss: 849.46044921875 = 1.0328346490859985 + 100.0 * 8.484275817871094
Epoch 340, val loss: 1.0340842008590698
Epoch 350, training loss: 848.8413696289062 = 1.0321933031082153 + 100.0 * 8.478092193603516
Epoch 350, val loss: 1.0334502458572388
Epoch 360, training loss: 848.206787109375 = 1.0315003395080566 + 100.0 * 8.471753120422363
Epoch 360, val loss: 1.0327603816986084
Epoch 370, training loss: 847.6585083007812 = 1.0308257341384888 + 100.0 * 8.466277122497559
Epoch 370, val loss: 1.032097578048706
Epoch 380, training loss: 847.1508178710938 = 1.0301315784454346 + 100.0 * 8.461206436157227
Epoch 380, val loss: 1.031412124633789
Epoch 390, training loss: 846.8834228515625 = 1.029412031173706 + 100.0 * 8.458539962768555
Epoch 390, val loss: 1.0306549072265625
Epoch 400, training loss: 846.2879638671875 = 1.028560757637024 + 100.0 * 8.452593803405762
Epoch 400, val loss: 1.0298616886138916
Epoch 410, training loss: 845.8856201171875 = 1.0277090072631836 + 100.0 * 8.448578834533691
Epoch 410, val loss: 1.0290160179138184
Epoch 420, training loss: 845.520263671875 = 1.0268234014511108 + 100.0 * 8.444934844970703
Epoch 420, val loss: 1.0281381607055664
Epoch 430, training loss: 845.159912109375 = 1.0258890390396118 + 100.0 * 8.441340446472168
Epoch 430, val loss: 1.0272116661071777
Epoch 440, training loss: 844.7935180664062 = 1.02492356300354 + 100.0 * 8.4376859664917
Epoch 440, val loss: 1.026279330253601
Epoch 450, training loss: 844.8079833984375 = 1.023928165435791 + 100.0 * 8.437840461730957
Epoch 450, val loss: 1.02533757686615
Epoch 460, training loss: 844.1968383789062 = 1.022911787033081 + 100.0 * 8.431739807128906
Epoch 460, val loss: 1.0243178606033325
Epoch 470, training loss: 843.7989501953125 = 1.021926760673523 + 100.0 * 8.427770614624023
Epoch 470, val loss: 1.0233370065689087
Epoch 480, training loss: 843.4518432617188 = 1.0209393501281738 + 100.0 * 8.424308776855469
Epoch 480, val loss: 1.0223515033721924
Epoch 490, training loss: 843.7570190429688 = 1.0199029445648193 + 100.0 * 8.42737102508545
Epoch 490, val loss: 1.0213425159454346
Epoch 500, training loss: 843.03076171875 = 1.0187444686889648 + 100.0 * 8.420120239257812
Epoch 500, val loss: 1.0201917886734009
Epoch 510, training loss: 842.5435180664062 = 1.0176715850830078 + 100.0 * 8.415258407592773
Epoch 510, val loss: 1.0191117525100708
Epoch 520, training loss: 842.2377319335938 = 1.0166370868682861 + 100.0 * 8.412210464477539
Epoch 520, val loss: 1.0180833339691162
Epoch 530, training loss: 841.9257202148438 = 1.0155830383300781 + 100.0 * 8.409101486206055
Epoch 530, val loss: 1.0170470476150513
Epoch 540, training loss: 842.2904663085938 = 1.0144145488739014 + 100.0 * 8.412760734558105
Epoch 540, val loss: 1.0159261226654053
Epoch 550, training loss: 841.4609375 = 1.013148546218872 + 100.0 * 8.404478073120117
Epoch 550, val loss: 1.0146371126174927
Epoch 560, training loss: 841.1058349609375 = 1.0119770765304565 + 100.0 * 8.400938987731934
Epoch 560, val loss: 1.0134913921356201
Epoch 570, training loss: 840.79150390625 = 1.0107779502868652 + 100.0 * 8.397807121276855
Epoch 570, val loss: 1.0123307704925537
Epoch 580, training loss: 840.524658203125 = 1.0095075368881226 + 100.0 * 8.395151138305664
Epoch 580, val loss: 1.011083722114563
Epoch 590, training loss: 840.27197265625 = 1.0081727504730225 + 100.0 * 8.392638206481934
Epoch 590, val loss: 1.009774923324585
Epoch 600, training loss: 840.5350341796875 = 1.006769061088562 + 100.0 * 8.395282745361328
Epoch 600, val loss: 1.0083619356155396
Epoch 610, training loss: 839.9299926757812 = 1.0051904916763306 + 100.0 * 8.38924789428711
Epoch 610, val loss: 1.0068522691726685
Epoch 620, training loss: 839.647216796875 = 1.0036513805389404 + 100.0 * 8.386435508728027
Epoch 620, val loss: 1.0053752660751343
Epoch 630, training loss: 839.4556274414062 = 1.0020884275436401 + 100.0 * 8.384535789489746
Epoch 630, val loss: 1.0038447380065918
Epoch 640, training loss: 839.5254516601562 = 1.0004323720932007 + 100.0 * 8.385250091552734
Epoch 640, val loss: 1.0022591352462769
Epoch 650, training loss: 839.2224731445312 = 0.9986816048622131 + 100.0 * 8.382238388061523
Epoch 650, val loss: 1.000455379486084
Epoch 660, training loss: 838.9444580078125 = 0.9968870282173157 + 100.0 * 8.379475593566895
Epoch 660, val loss: 0.9987288117408752
Epoch 670, training loss: 838.7415771484375 = 0.9950822591781616 + 100.0 * 8.37746524810791
Epoch 670, val loss: 0.9969859719276428
Epoch 680, training loss: 838.5771484375 = 0.9932255744934082 + 100.0 * 8.375839233398438
Epoch 680, val loss: 0.9951745867729187
Epoch 690, training loss: 838.5010375976562 = 0.9913160800933838 + 100.0 * 8.375097274780273
Epoch 690, val loss: 0.9933055639266968
Epoch 700, training loss: 838.4651489257812 = 0.9891999959945679 + 100.0 * 8.374759674072266
Epoch 700, val loss: 0.9912158846855164
Epoch 710, training loss: 838.1985473632812 = 0.9870834946632385 + 100.0 * 8.372115135192871
Epoch 710, val loss: 0.9891677498817444
Epoch 720, training loss: 838.0161743164062 = 0.9849973320960999 + 100.0 * 8.370311737060547
Epoch 720, val loss: 0.9871354103088379
Epoch 730, training loss: 837.8864135742188 = 0.9828482270240784 + 100.0 * 8.369035720825195
Epoch 730, val loss: 0.9850086569786072
Epoch 740, training loss: 837.9943237304688 = 0.9805658459663391 + 100.0 * 8.370137214660645
Epoch 740, val loss: 0.9827297329902649
Epoch 750, training loss: 837.6717529296875 = 0.9781226515769958 + 100.0 * 8.366935729980469
Epoch 750, val loss: 0.9804047346115112
Epoch 760, training loss: 837.5493774414062 = 0.9757442474365234 + 100.0 * 8.36573600769043
Epoch 760, val loss: 0.978102445602417
Epoch 770, training loss: 837.4286499023438 = 0.9733290076255798 + 100.0 * 8.364553451538086
Epoch 770, val loss: 0.975726306438446
Epoch 780, training loss: 837.3311767578125 = 0.9708610773086548 + 100.0 * 8.363602638244629
Epoch 780, val loss: 0.9733086824417114
Epoch 790, training loss: 837.5198974609375 = 0.9683396220207214 + 100.0 * 8.36551570892334
Epoch 790, val loss: 0.9707921743392944
Epoch 800, training loss: 837.3239135742188 = 0.9655594825744629 + 100.0 * 8.3635835647583
Epoch 800, val loss: 0.9680854082107544
Epoch 810, training loss: 837.135986328125 = 0.9628283977508545 + 100.0 * 8.36173152923584
Epoch 810, val loss: 0.9654467105865479
Epoch 820, training loss: 836.9947509765625 = 0.9601508975028992 + 100.0 * 8.360345840454102
Epoch 820, val loss: 0.9628119468688965
Epoch 830, training loss: 836.905029296875 = 0.9574229121208191 + 100.0 * 8.359476089477539
Epoch 830, val loss: 0.9601130485534668
Epoch 840, training loss: 836.9236450195312 = 0.9546146392822266 + 100.0 * 8.35969066619873
Epoch 840, val loss: 0.9573389887809753
Epoch 850, training loss: 836.7315063476562 = 0.9516399502754211 + 100.0 * 8.35779857635498
Epoch 850, val loss: 0.9544510841369629
Epoch 860, training loss: 836.653076171875 = 0.9487166404724121 + 100.0 * 8.357043266296387
Epoch 860, val loss: 0.9515945315361023
Epoch 870, training loss: 836.5838623046875 = 0.945817232131958 + 100.0 * 8.356380462646484
Epoch 870, val loss: 0.9487308859825134
Epoch 880, training loss: 836.6752319335938 = 0.9428979158401489 + 100.0 * 8.35732364654541
Epoch 880, val loss: 0.9457811713218689
Epoch 890, training loss: 836.652587890625 = 0.9396018385887146 + 100.0 * 8.35713005065918
Epoch 890, val loss: 0.9427173733711243
Epoch 900, training loss: 836.374267578125 = 0.9365018010139465 + 100.0 * 8.354377746582031
Epoch 900, val loss: 0.9396803975105286
Epoch 910, training loss: 836.2551879882812 = 0.9334909915924072 + 100.0 * 8.353217124938965
Epoch 910, val loss: 0.9366580843925476
Epoch 920, training loss: 836.17431640625 = 0.9304037094116211 + 100.0 * 8.352438926696777
Epoch 920, val loss: 0.9336053729057312
Epoch 930, training loss: 836.272216796875 = 0.9272449612617493 + 100.0 * 8.353449821472168
Epoch 930, val loss: 0.9304973483085632
Epoch 940, training loss: 836.3314819335938 = 0.923795223236084 + 100.0 * 8.354077339172363
Epoch 940, val loss: 0.9271526336669922
Epoch 950, training loss: 836.0185546875 = 0.9204648733139038 + 100.0 * 8.350980758666992
Epoch 950, val loss: 0.9238665103912354
Epoch 960, training loss: 835.8815307617188 = 0.9172161221504211 + 100.0 * 8.349642753601074
Epoch 960, val loss: 0.9206987023353577
Epoch 970, training loss: 835.7843017578125 = 0.9139562249183655 + 100.0 * 8.348703384399414
Epoch 970, val loss: 0.9175060391426086
Epoch 980, training loss: 835.7225952148438 = 0.9106745719909668 + 100.0 * 8.348119735717773
Epoch 980, val loss: 0.9142722487449646
Epoch 990, training loss: 835.842041015625 = 0.907240629196167 + 100.0 * 8.349348068237305
Epoch 990, val loss: 0.9109002947807312
Epoch 1000, training loss: 835.60595703125 = 0.9037164449691772 + 100.0 * 8.347023010253906
Epoch 1000, val loss: 0.9074629545211792
Epoch 1010, training loss: 835.4539794921875 = 0.9003208875656128 + 100.0 * 8.345536231994629
Epoch 1010, val loss: 0.9040873646736145
Epoch 1020, training loss: 835.3993530273438 = 0.8968943357467651 + 100.0 * 8.345024108886719
Epoch 1020, val loss: 0.9006971120834351
Epoch 1030, training loss: 835.6282958984375 = 0.893380343914032 + 100.0 * 8.347349166870117
Epoch 1030, val loss: 0.8971565365791321
Epoch 1040, training loss: 835.3397216796875 = 0.8895929455757141 + 100.0 * 8.344501495361328
Epoch 1040, val loss: 0.8936120271682739
Epoch 1050, training loss: 835.19482421875 = 0.8860871195793152 + 100.0 * 8.343087196350098
Epoch 1050, val loss: 0.8900616765022278
Epoch 1060, training loss: 835.1032104492188 = 0.8824735879898071 + 100.0 * 8.342207908630371
Epoch 1060, val loss: 0.8865597248077393
Epoch 1070, training loss: 835.0272216796875 = 0.8788798451423645 + 100.0 * 8.341483116149902
Epoch 1070, val loss: 0.8829649090766907
Epoch 1080, training loss: 835.39697265625 = 0.8751726150512695 + 100.0 * 8.34521770477295
Epoch 1080, val loss: 0.8792452812194824
Epoch 1090, training loss: 835.0758056640625 = 0.8710362911224365 + 100.0 * 8.342047691345215
Epoch 1090, val loss: 0.8752593994140625
Epoch 1100, training loss: 834.8984375 = 0.8672431707382202 + 100.0 * 8.340312004089355
Epoch 1100, val loss: 0.8715507388114929
Epoch 1110, training loss: 834.7584228515625 = 0.8635110855102539 + 100.0 * 8.338949203491211
Epoch 1110, val loss: 0.8678433299064636
Epoch 1120, training loss: 834.6983642578125 = 0.859744131565094 + 100.0 * 8.338386535644531
Epoch 1120, val loss: 0.8641193509101868
Epoch 1130, training loss: 834.7034912109375 = 0.8559266924858093 + 100.0 * 8.338475227355957
Epoch 1130, val loss: 0.8603691458702087
Epoch 1140, training loss: 834.5946044921875 = 0.8518273830413818 + 100.0 * 8.337428092956543
Epoch 1140, val loss: 0.8563233017921448
Epoch 1150, training loss: 834.7217407226562 = 0.847756564617157 + 100.0 * 8.338739395141602
Epoch 1150, val loss: 0.8523430824279785
Epoch 1160, training loss: 834.4743041992188 = 0.8437502980232239 + 100.0 * 8.336305618286133
Epoch 1160, val loss: 0.8483024835586548
Epoch 1170, training loss: 834.4227294921875 = 0.839827299118042 + 100.0 * 8.33582878112793
Epoch 1170, val loss: 0.844458281993866
Epoch 1180, training loss: 834.3519897460938 = 0.835910439491272 + 100.0 * 8.335160255432129
Epoch 1180, val loss: 0.8406344652175903
Epoch 1190, training loss: 834.2843017578125 = 0.8319767117500305 + 100.0 * 8.33452320098877
Epoch 1190, val loss: 0.8367332220077515
Epoch 1200, training loss: 834.2589721679688 = 0.8279709219932556 + 100.0 * 8.334310531616211
Epoch 1200, val loss: 0.8327814936637878
Epoch 1210, training loss: 834.3926391601562 = 0.8237766623497009 + 100.0 * 8.335688591003418
Epoch 1210, val loss: 0.8287102580070496
Epoch 1220, training loss: 834.2235107421875 = 0.8195477724075317 + 100.0 * 8.334039688110352
Epoch 1220, val loss: 0.8245400786399841
Epoch 1230, training loss: 834.128662109375 = 0.8154814839363098 + 100.0 * 8.333131790161133
Epoch 1230, val loss: 0.8204150795936584
Epoch 1240, training loss: 834.0075073242188 = 0.8113222718238831 + 100.0 * 8.331961631774902
Epoch 1240, val loss: 0.8164308667182922
Epoch 1250, training loss: 833.9522094726562 = 0.8072353601455688 + 100.0 * 8.331449508666992
Epoch 1250, val loss: 0.812445878982544
Epoch 1260, training loss: 833.973388671875 = 0.8030804991722107 + 100.0 * 8.331703186035156
Epoch 1260, val loss: 0.8083940744400024
Epoch 1270, training loss: 833.9505615234375 = 0.7986729145050049 + 100.0 * 8.33151912689209
Epoch 1270, val loss: 0.8040708899497986
Epoch 1280, training loss: 833.8118896484375 = 0.7944251298904419 + 100.0 * 8.330174446105957
Epoch 1280, val loss: 0.7998146414756775
Epoch 1290, training loss: 833.754638671875 = 0.7902268767356873 + 100.0 * 8.329644203186035
Epoch 1290, val loss: 0.795687198638916
Epoch 1300, training loss: 833.8284301757812 = 0.7860214114189148 + 100.0 * 8.330424308776855
Epoch 1300, val loss: 0.7915460467338562
Epoch 1310, training loss: 833.6843872070312 = 0.7815465331077576 + 100.0 * 8.329028129577637
Epoch 1310, val loss: 0.7872330546379089
Epoch 1320, training loss: 833.6376953125 = 0.7772414684295654 + 100.0 * 8.328604698181152
Epoch 1320, val loss: 0.782956063747406
Epoch 1330, training loss: 833.582763671875 = 0.7729828357696533 + 100.0 * 8.328097343444824
Epoch 1330, val loss: 0.7788379788398743
Epoch 1340, training loss: 833.57861328125 = 0.7687041759490967 + 100.0 * 8.328099250793457
Epoch 1340, val loss: 0.7746326923370361
Epoch 1350, training loss: 833.6690063476562 = 0.7642422914505005 + 100.0 * 8.329048156738281
Epoch 1350, val loss: 0.7703600525856018
Epoch 1360, training loss: 833.5150756835938 = 0.7599961161613464 + 100.0 * 8.327550888061523
Epoch 1360, val loss: 0.7659428119659424
Epoch 1370, training loss: 833.623291015625 = 0.7555721998214722 + 100.0 * 8.3286771774292
Epoch 1370, val loss: 0.7616547346115112
Epoch 1380, training loss: 833.4063110351562 = 0.7510830163955688 + 100.0 * 8.326552391052246
Epoch 1380, val loss: 0.7574157118797302
Epoch 1390, training loss: 833.341552734375 = 0.7468729615211487 + 100.0 * 8.325946807861328
Epoch 1390, val loss: 0.753170907497406
Epoch 1400, training loss: 833.281494140625 = 0.7425969243049622 + 100.0 * 8.32538890838623
Epoch 1400, val loss: 0.7490248084068298
Epoch 1410, training loss: 833.24462890625 = 0.7382791042327881 + 100.0 * 8.325063705444336
Epoch 1410, val loss: 0.7448242902755737
Epoch 1420, training loss: 833.3250122070312 = 0.7339468598365784 + 100.0 * 8.325910568237305
Epoch 1420, val loss: 0.7406460642814636
Epoch 1430, training loss: 833.2198486328125 = 0.7294105887413025 + 100.0 * 8.324904441833496
Epoch 1430, val loss: 0.7361480593681335
Epoch 1440, training loss: 833.178955078125 = 0.725006639957428 + 100.0 * 8.324539184570312
Epoch 1440, val loss: 0.7319410443305969
Epoch 1450, training loss: 833.212158203125 = 0.7206158638000488 + 100.0 * 8.324914932250977
Epoch 1450, val loss: 0.7274914383888245
Epoch 1460, training loss: 833.2462768554688 = 0.7162277102470398 + 100.0 * 8.325300216674805
Epoch 1460, val loss: 0.7231151461601257
Epoch 1470, training loss: 833.0415649414062 = 0.711770236492157 + 100.0 * 8.323297500610352
Epoch 1470, val loss: 0.7190167307853699
Epoch 1480, training loss: 832.976318359375 = 0.7075870633125305 + 100.0 * 8.322687149047852
Epoch 1480, val loss: 0.7148685455322266
Epoch 1490, training loss: 832.9445190429688 = 0.7033731937408447 + 100.0 * 8.32241153717041
Epoch 1490, val loss: 0.7107117772102356
Epoch 1500, training loss: 832.9116821289062 = 0.6991710662841797 + 100.0 * 8.322125434875488
Epoch 1500, val loss: 0.706658124923706
Epoch 1510, training loss: 833.1652221679688 = 0.6948924660682678 + 100.0 * 8.324703216552734
Epoch 1510, val loss: 0.7024795413017273
Epoch 1520, training loss: 832.9210815429688 = 0.6903331875801086 + 100.0 * 8.322307586669922
Epoch 1520, val loss: 0.6980130076408386
Epoch 1530, training loss: 832.8507690429688 = 0.6861112713813782 + 100.0 * 8.321646690368652
Epoch 1530, val loss: 0.6939746141433716
Epoch 1540, training loss: 833.0836181640625 = 0.68185955286026 + 100.0 * 8.324017524719238
Epoch 1540, val loss: 0.6899194121360779
Epoch 1550, training loss: 832.8138427734375 = 0.677478015422821 + 100.0 * 8.32136344909668
Epoch 1550, val loss: 0.685420572757721
Epoch 1560, training loss: 832.76416015625 = 0.6732255816459656 + 100.0 * 8.32090950012207
Epoch 1560, val loss: 0.6814560890197754
Epoch 1570, training loss: 832.6541748046875 = 0.66927570104599 + 100.0 * 8.319849014282227
Epoch 1570, val loss: 0.6775727272033691
Epoch 1580, training loss: 832.6087036132812 = 0.6652544140815735 + 100.0 * 8.31943416595459
Epoch 1580, val loss: 0.673686683177948
Epoch 1590, training loss: 832.6451416015625 = 0.6611922979354858 + 100.0 * 8.319839477539062
Epoch 1590, val loss: 0.6699185371398926
Epoch 1600, training loss: 832.5888671875 = 0.6569806337356567 + 100.0 * 8.319318771362305
Epoch 1600, val loss: 0.6656721830368042
Epoch 1610, training loss: 832.5560913085938 = 0.652851402759552 + 100.0 * 8.319032669067383
Epoch 1610, val loss: 0.6618111729621887
Epoch 1620, training loss: 832.4837036132812 = 0.6490151882171631 + 100.0 * 8.318346977233887
Epoch 1620, val loss: 0.6579729914665222
Epoch 1630, training loss: 832.4210815429688 = 0.6451723575592041 + 100.0 * 8.317758560180664
Epoch 1630, val loss: 0.6542674899101257
Epoch 1640, training loss: 832.3977661132812 = 0.6413158774375916 + 100.0 * 8.317564964294434
Epoch 1640, val loss: 0.6506111025810242
Epoch 1650, training loss: 832.8366088867188 = 0.6373835802078247 + 100.0 * 8.321991920471191
Epoch 1650, val loss: 0.6468105912208557
Epoch 1660, training loss: 832.5326538085938 = 0.6331656575202942 + 100.0 * 8.318994522094727
Epoch 1660, val loss: 0.6427566409111023
Epoch 1670, training loss: 832.288330078125 = 0.6294053196907043 + 100.0 * 8.31658935546875
Epoch 1670, val loss: 0.6391251683235168
Epoch 1680, training loss: 832.2554931640625 = 0.6257164478302002 + 100.0 * 8.31629753112793
Epoch 1680, val loss: 0.6355581879615784
Epoch 1690, training loss: 832.2173461914062 = 0.6220224499702454 + 100.0 * 8.315953254699707
Epoch 1690, val loss: 0.6320726871490479
Epoch 1700, training loss: 832.2774658203125 = 0.6183573007583618 + 100.0 * 8.316591262817383
Epoch 1700, val loss: 0.6285409927368164
Epoch 1710, training loss: 832.1952514648438 = 0.6144422888755798 + 100.0 * 8.315808296203613
Epoch 1710, val loss: 0.6247434616088867
Epoch 1720, training loss: 832.1250610351562 = 0.6107004880905151 + 100.0 * 8.315143585205078
Epoch 1720, val loss: 0.6211912035942078
Epoch 1730, training loss: 832.1072998046875 = 0.6071814894676208 + 100.0 * 8.315001487731934
Epoch 1730, val loss: 0.6177661418914795
Epoch 1740, training loss: 832.1072387695312 = 0.6036425232887268 + 100.0 * 8.315035820007324
Epoch 1740, val loss: 0.6143766641616821
Epoch 1750, training loss: 832.075927734375 = 0.600080668926239 + 100.0 * 8.31475830078125
Epoch 1750, val loss: 0.6109514832496643
Epoch 1760, training loss: 831.9711303710938 = 0.5965302586555481 + 100.0 * 8.313745498657227
Epoch 1760, val loss: 0.6076965928077698
Epoch 1770, training loss: 831.9906005859375 = 0.5930971503257751 + 100.0 * 8.31397533416748
Epoch 1770, val loss: 0.6044217348098755
Epoch 1780, training loss: 832.0861206054688 = 0.5896609425544739 + 100.0 * 8.314964294433594
Epoch 1780, val loss: 0.6009663939476013
Epoch 1790, training loss: 831.9625854492188 = 0.5861287713050842 + 100.0 * 8.313764572143555
Epoch 1790, val loss: 0.5976863503456116
Epoch 1800, training loss: 831.8710327148438 = 0.5827009677886963 + 100.0 * 8.312883377075195
Epoch 1800, val loss: 0.5945079326629639
Epoch 1810, training loss: 831.9168701171875 = 0.5793228149414062 + 100.0 * 8.313375473022461
Epoch 1810, val loss: 0.5913345813751221
Epoch 1820, training loss: 831.8261108398438 = 0.5759795308113098 + 100.0 * 8.312500953674316
Epoch 1820, val loss: 0.5880173444747925
Epoch 1830, training loss: 831.8480834960938 = 0.5726789832115173 + 100.0 * 8.312753677368164
Epoch 1830, val loss: 0.5847934484481812
Epoch 1840, training loss: 831.7258911132812 = 0.5693387985229492 + 100.0 * 8.311565399169922
Epoch 1840, val loss: 0.5818347334861755
Epoch 1850, training loss: 831.6908569335938 = 0.5661344528198242 + 100.0 * 8.311246871948242
Epoch 1850, val loss: 0.578807532787323
Epoch 1860, training loss: 831.72119140625 = 0.5630443096160889 + 100.0 * 8.3115816116333
Epoch 1860, val loss: 0.5758054852485657
Epoch 1870, training loss: 831.8292846679688 = 0.5597818493843079 + 100.0 * 8.312695503234863
Epoch 1870, val loss: 0.5726416110992432
Epoch 1880, training loss: 831.7520141601562 = 0.556465208530426 + 100.0 * 8.311955451965332
Epoch 1880, val loss: 0.5696098804473877
Epoch 1890, training loss: 831.5947875976562 = 0.5533384680747986 + 100.0 * 8.31041431427002
Epoch 1890, val loss: 0.5666087865829468
Epoch 1900, training loss: 831.5536499023438 = 0.5502908825874329 + 100.0 * 8.310033798217773
Epoch 1900, val loss: 0.5638620257377625
Epoch 1910, training loss: 831.5631713867188 = 0.5473166704177856 + 100.0 * 8.310158729553223
Epoch 1910, val loss: 0.5610640645027161
Epoch 1920, training loss: 831.6226806640625 = 0.5443218946456909 + 100.0 * 8.310783386230469
Epoch 1920, val loss: 0.5582515597343445
Epoch 1930, training loss: 831.526123046875 = 0.5414210557937622 + 100.0 * 8.309846878051758
Epoch 1930, val loss: 0.5553780794143677
Epoch 1940, training loss: 831.4618530273438 = 0.5385094285011292 + 100.0 * 8.309233665466309
Epoch 1940, val loss: 0.5527574419975281
Epoch 1950, training loss: 831.591796875 = 0.5355654954910278 + 100.0 * 8.310562133789062
Epoch 1950, val loss: 0.5501778721809387
Epoch 1960, training loss: 831.402587890625 = 0.5328330993652344 + 100.0 * 8.308697700500488
Epoch 1960, val loss: 0.5472507476806641
Epoch 1970, training loss: 831.3535766601562 = 0.5300637483596802 + 100.0 * 8.308235168457031
Epoch 1970, val loss: 0.5448837876319885
Epoch 1980, training loss: 831.4160766601562 = 0.5273590087890625 + 100.0 * 8.308887481689453
Epoch 1980, val loss: 0.5423039197921753
Epoch 1990, training loss: 831.3864135742188 = 0.5245341658592224 + 100.0 * 8.308618545532227
Epoch 1990, val loss: 0.5396532416343689
Epoch 2000, training loss: 831.2972412109375 = 0.5218960642814636 + 100.0 * 8.307753562927246
Epoch 2000, val loss: 0.5371861457824707
Epoch 2010, training loss: 831.2506103515625 = 0.519286572933197 + 100.0 * 8.307312965393066
Epoch 2010, val loss: 0.5348103046417236
Epoch 2020, training loss: 831.280029296875 = 0.516758918762207 + 100.0 * 8.307632446289062
Epoch 2020, val loss: 0.5324181318283081
Epoch 2030, training loss: 831.4871215820312 = 0.5140818357467651 + 100.0 * 8.309730529785156
Epoch 2030, val loss: 0.5298595428466797
Epoch 2040, training loss: 831.2113037109375 = 0.5113301277160645 + 100.0 * 8.306999206542969
Epoch 2040, val loss: 0.5274029970169067
Epoch 2050, training loss: 831.1541137695312 = 0.5088878273963928 + 100.0 * 8.306451797485352
Epoch 2050, val loss: 0.5252726674079895
Epoch 2060, training loss: 831.1268310546875 = 0.5065099000930786 + 100.0 * 8.30620288848877
Epoch 2060, val loss: 0.5229611396789551
Epoch 2070, training loss: 831.09765625 = 0.5041279196739197 + 100.0 * 8.30593490600586
Epoch 2070, val loss: 0.5208995342254639
Epoch 2080, training loss: 831.2407836914062 = 0.501771092414856 + 100.0 * 8.307390213012695
Epoch 2080, val loss: 0.5188066959381104
Epoch 2090, training loss: 831.083740234375 = 0.4991757869720459 + 100.0 * 8.305846214294434
Epoch 2090, val loss: 0.5163010954856873
Epoch 2100, training loss: 831.1163330078125 = 0.4968050718307495 + 100.0 * 8.306195259094238
Epoch 2100, val loss: 0.5142174363136292
Epoch 2110, training loss: 831.027099609375 = 0.49459701776504517 + 100.0 * 8.305325508117676
Epoch 2110, val loss: 0.5121134519577026
Epoch 2120, training loss: 831.0428466796875 = 0.4923560619354248 + 100.0 * 8.30550479888916
Epoch 2120, val loss: 0.5101879835128784
Epoch 2130, training loss: 831.1504516601562 = 0.49000975489616394 + 100.0 * 8.306604385375977
Epoch 2130, val loss: 0.5080901384353638
Epoch 2140, training loss: 831.0371704101562 = 0.4878598153591156 + 100.0 * 8.305493354797363
Epoch 2140, val loss: 0.5059664249420166
Epoch 2150, training loss: 831.1114501953125 = 0.4856424033641815 + 100.0 * 8.306258201599121
Epoch 2150, val loss: 0.5039666295051575
Epoch 2160, training loss: 830.9642944335938 = 0.4834365248680115 + 100.0 * 8.304808616638184
Epoch 2160, val loss: 0.5020241737365723
Epoch 2170, training loss: 830.8963012695312 = 0.4814223349094391 + 100.0 * 8.30414867401123
Epoch 2170, val loss: 0.5001888871192932
Epoch 2180, training loss: 830.8881225585938 = 0.479404479265213 + 100.0 * 8.304086685180664
Epoch 2180, val loss: 0.49839818477630615
Epoch 2190, training loss: 830.8568115234375 = 0.477414608001709 + 100.0 * 8.303793907165527
Epoch 2190, val loss: 0.4966079592704773
Epoch 2200, training loss: 830.843017578125 = 0.4754621088504791 + 100.0 * 8.303675651550293
Epoch 2200, val loss: 0.49484407901763916
Epoch 2210, training loss: 831.0621948242188 = 0.47354161739349365 + 100.0 * 8.305886268615723
Epoch 2210, val loss: 0.4930095374584198
Epoch 2220, training loss: 830.8709106445312 = 0.47120946645736694 + 100.0 * 8.303997039794922
Epoch 2220, val loss: 0.4911492168903351
Epoch 2230, training loss: 830.8851318359375 = 0.469300776720047 + 100.0 * 8.304158210754395
Epoch 2230, val loss: 0.48926830291748047
Epoch 2240, training loss: 830.8215942382812 = 0.4673961102962494 + 100.0 * 8.303542137145996
Epoch 2240, val loss: 0.4876592755317688
Epoch 2250, training loss: 830.8085327148438 = 0.46559950709342957 + 100.0 * 8.30342960357666
Epoch 2250, val loss: 0.48601871728897095
Epoch 2260, training loss: 830.8284912109375 = 0.46380123496055603 + 100.0 * 8.3036470413208
Epoch 2260, val loss: 0.4844803810119629
Epoch 2270, training loss: 830.80322265625 = 0.46198198199272156 + 100.0 * 8.303412437438965
Epoch 2270, val loss: 0.4829411804676056
Epoch 2280, training loss: 830.765869140625 = 0.46018677949905396 + 100.0 * 8.303056716918945
Epoch 2280, val loss: 0.4813501536846161
Epoch 2290, training loss: 830.7112426757812 = 0.45847928524017334 + 100.0 * 8.30252742767334
Epoch 2290, val loss: 0.479743093252182
Epoch 2300, training loss: 830.7142333984375 = 0.45678117871284485 + 100.0 * 8.302574157714844
Epoch 2300, val loss: 0.4782537817955017
Epoch 2310, training loss: 830.7630615234375 = 0.4550657272338867 + 100.0 * 8.303079605102539
Epoch 2310, val loss: 0.47678235173225403
Epoch 2320, training loss: 830.7551879882812 = 0.45327720046043396 + 100.0 * 8.303019523620605
Epoch 2320, val loss: 0.4752989411354065
Epoch 2330, training loss: 830.7025146484375 = 0.4516419768333435 + 100.0 * 8.302508354187012
Epoch 2330, val loss: 0.47387388348579407
Epoch 2340, training loss: 830.6567993164062 = 0.45002272725105286 + 100.0 * 8.302067756652832
Epoch 2340, val loss: 0.47239646315574646
Epoch 2350, training loss: 830.7135620117188 = 0.4484151005744934 + 100.0 * 8.302651405334473
Epoch 2350, val loss: 0.47101032733917236
Epoch 2360, training loss: 830.6475830078125 = 0.44674447178840637 + 100.0 * 8.302008628845215
Epoch 2360, val loss: 0.4696057140827179
Epoch 2370, training loss: 830.6425170898438 = 0.4451761245727539 + 100.0 * 8.301973342895508
Epoch 2370, val loss: 0.46820303797721863
Epoch 2380, training loss: 830.6046752929688 = 0.44368159770965576 + 100.0 * 8.301609992980957
Epoch 2380, val loss: 0.4668898284435272
Epoch 2390, training loss: 830.6403198242188 = 0.44217750430107117 + 100.0 * 8.301980972290039
Epoch 2390, val loss: 0.4655798077583313
Epoch 2400, training loss: 830.5716552734375 = 0.4406558573246002 + 100.0 * 8.301309585571289
Epoch 2400, val loss: 0.46428537368774414
Epoch 2410, training loss: 830.5289916992188 = 0.4391762614250183 + 100.0 * 8.300898551940918
Epoch 2410, val loss: 0.4631723463535309
Epoch 2420, training loss: 830.653076171875 = 0.43772605061531067 + 100.0 * 8.302153587341309
Epoch 2420, val loss: 0.46204960346221924
Epoch 2430, training loss: 830.577392578125 = 0.43623086810112 + 100.0 * 8.301411628723145
Epoch 2430, val loss: 0.46056780219078064
Epoch 2440, training loss: 830.5047607421875 = 0.43477940559387207 + 100.0 * 8.300700187683105
Epoch 2440, val loss: 0.4593840539455414
Epoch 2450, training loss: 830.6201782226562 = 0.4333639442920685 + 100.0 * 8.301868438720703
Epoch 2450, val loss: 0.4582645297050476
Epoch 2460, training loss: 830.4772338867188 = 0.43198132514953613 + 100.0 * 8.30045223236084
Epoch 2460, val loss: 0.45704180002212524
Epoch 2470, training loss: 830.4384155273438 = 0.43063488602638245 + 100.0 * 8.300077438354492
Epoch 2470, val loss: 0.45605382323265076
Epoch 2480, training loss: 830.4146118164062 = 0.4293546974658966 + 100.0 * 8.29985237121582
Epoch 2480, val loss: 0.4549173414707184
Epoch 2490, training loss: 830.48291015625 = 0.4280434548854828 + 100.0 * 8.300548553466797
Epoch 2490, val loss: 0.4539570212364197
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.830542871638762
0.8632181409838442
=== training gcn model ===
Epoch 0, training loss: 1059.3106689453125 = 1.0846977233886719 + 100.0 * 10.582259178161621
Epoch 0, val loss: 1.0844695568084717
Epoch 10, training loss: 1059.2613525390625 = 1.08133864402771 + 100.0 * 10.58180046081543
Epoch 10, val loss: 1.0811090469360352
Epoch 20, training loss: 1059.056396484375 = 1.0777100324630737 + 100.0 * 10.57978630065918
Epoch 20, val loss: 1.0774941444396973
Epoch 30, training loss: 1058.153564453125 = 1.0737144947052002 + 100.0 * 10.570798873901367
Epoch 30, val loss: 1.0734999179840088
Epoch 40, training loss: 1054.466552734375 = 1.069281816482544 + 100.0 * 10.53397274017334
Epoch 40, val loss: 1.0690864324569702
Epoch 50, training loss: 1041.7884521484375 = 1.0645387172698975 + 100.0 * 10.407238960266113
Epoch 50, val loss: 1.064380168914795
Epoch 60, training loss: 1005.4038696289062 = 1.0595576763153076 + 100.0 * 10.043442726135254
Epoch 60, val loss: 1.0594029426574707
Epoch 70, training loss: 960.6572265625 = 1.0540884733200073 + 100.0 * 9.596031188964844
Epoch 70, val loss: 1.054000973701477
Epoch 80, training loss: 953.1270751953125 = 1.0499953031539917 + 100.0 * 9.520771026611328
Epoch 80, val loss: 1.0500767230987549
Epoch 90, training loss: 943.588134765625 = 1.0478479862213135 + 100.0 * 9.425402641296387
Epoch 90, val loss: 1.0480223894119263
Epoch 100, training loss: 931.950927734375 = 1.0467511415481567 + 100.0 * 9.309041976928711
Epoch 100, val loss: 1.0469461679458618
Epoch 110, training loss: 920.6654663085938 = 1.0456215143203735 + 100.0 * 9.196198463439941
Epoch 110, val loss: 1.0458580255508423
Epoch 120, training loss: 912.7172241210938 = 1.0443992614746094 + 100.0 * 9.116728782653809
Epoch 120, val loss: 1.0447096824645996
Epoch 130, training loss: 905.9959106445312 = 1.0432599782943726 + 100.0 * 9.04952621459961
Epoch 130, val loss: 1.0436288118362427
Epoch 140, training loss: 896.3216552734375 = 1.0425704717636108 + 100.0 * 8.952791213989258
Epoch 140, val loss: 1.0430430173873901
Epoch 150, training loss: 888.1636962890625 = 1.0424972772598267 + 100.0 * 8.871212005615234
Epoch 150, val loss: 1.0430819988250732
Epoch 160, training loss: 883.842041015625 = 1.0424059629440308 + 100.0 * 8.827996253967285
Epoch 160, val loss: 1.0429646968841553
Epoch 170, training loss: 881.5433959960938 = 1.0414729118347168 + 100.0 * 8.80501937866211
Epoch 170, val loss: 1.0419338941574097
Epoch 180, training loss: 880.1475830078125 = 1.0399131774902344 + 100.0 * 8.79107666015625
Epoch 180, val loss: 1.04033625125885
Epoch 190, training loss: 878.4322509765625 = 1.0385186672210693 + 100.0 * 8.773937225341797
Epoch 190, val loss: 1.039045810699463
Epoch 200, training loss: 876.3366088867188 = 1.0378167629241943 + 100.0 * 8.7529878616333
Epoch 200, val loss: 1.0384478569030762
Epoch 210, training loss: 873.44580078125 = 1.0375028848648071 + 100.0 * 8.724082946777344
Epoch 210, val loss: 1.0381901264190674
Epoch 220, training loss: 869.6937255859375 = 1.0373756885528564 + 100.0 * 8.686563491821289
Epoch 220, val loss: 1.038107991218567
Epoch 230, training loss: 866.3735961914062 = 1.0372645854949951 + 100.0 * 8.653363227844238
Epoch 230, val loss: 1.038030743598938
Epoch 240, training loss: 863.5866088867188 = 1.0369346141815186 + 100.0 * 8.625496864318848
Epoch 240, val loss: 1.0377346277236938
Epoch 250, training loss: 861.2571411132812 = 1.036455512046814 + 100.0 * 8.60220718383789
Epoch 250, val loss: 1.0372878313064575
Epoch 260, training loss: 859.4619750976562 = 1.0359787940979004 + 100.0 * 8.584259986877441
Epoch 260, val loss: 1.0368741750717163
Epoch 270, training loss: 857.3255615234375 = 1.0354218482971191 + 100.0 * 8.562901496887207
Epoch 270, val loss: 1.0362986326217651
Epoch 280, training loss: 855.5821533203125 = 1.0348436832427979 + 100.0 * 8.545473098754883
Epoch 280, val loss: 1.0357226133346558
Epoch 290, training loss: 854.5532836914062 = 1.0342297554016113 + 100.0 * 8.53519058227539
Epoch 290, val loss: 1.0351207256317139
Epoch 300, training loss: 852.8136596679688 = 1.0335142612457275 + 100.0 * 8.517801284790039
Epoch 300, val loss: 1.034446358680725
Epoch 310, training loss: 851.7963256835938 = 1.0328351259231567 + 100.0 * 8.507635116577148
Epoch 310, val loss: 1.0338469743728638
Epoch 320, training loss: 850.6115112304688 = 1.0321491956710815 + 100.0 * 8.495793342590332
Epoch 320, val loss: 1.0331705808639526
Epoch 330, training loss: 849.6839599609375 = 1.0314275026321411 + 100.0 * 8.486525535583496
Epoch 330, val loss: 1.0324939489364624
Epoch 340, training loss: 848.79638671875 = 1.0306665897369385 + 100.0 * 8.477657318115234
Epoch 340, val loss: 1.0317740440368652
Epoch 350, training loss: 848.2249755859375 = 1.0298479795455933 + 100.0 * 8.471951484680176
Epoch 350, val loss: 1.0309867858886719
Epoch 360, training loss: 847.5036010742188 = 1.0289020538330078 + 100.0 * 8.464746475219727
Epoch 360, val loss: 1.030071496963501
Epoch 370, training loss: 846.709716796875 = 1.0279399156570435 + 100.0 * 8.456817626953125
Epoch 370, val loss: 1.0291509628295898
Epoch 380, training loss: 846.1530151367188 = 1.0269641876220703 + 100.0 * 8.451260566711426
Epoch 380, val loss: 1.0282301902770996
Epoch 390, training loss: 845.6370239257812 = 1.0259361267089844 + 100.0 * 8.446110725402832
Epoch 390, val loss: 1.0272588729858398
Epoch 400, training loss: 845.15625 = 1.024789810180664 + 100.0 * 8.441314697265625
Epoch 400, val loss: 1.0261486768722534
Epoch 410, training loss: 844.677001953125 = 1.0236183404922485 + 100.0 * 8.43653392791748
Epoch 410, val loss: 1.0250385999679565
Epoch 420, training loss: 844.287109375 = 1.0224592685699463 + 100.0 * 8.432646751403809
Epoch 420, val loss: 1.0239101648330688
Epoch 430, training loss: 843.9885864257812 = 1.021195411682129 + 100.0 * 8.42967414855957
Epoch 430, val loss: 1.0226902961730957
Epoch 440, training loss: 843.5238647460938 = 1.0198445320129395 + 100.0 * 8.425040245056152
Epoch 440, val loss: 1.0214210748672485
Epoch 450, training loss: 843.1683959960938 = 1.018539547920227 + 100.0 * 8.42149829864502
Epoch 450, val loss: 1.0201754570007324
Epoch 460, training loss: 842.8425903320312 = 1.017199158668518 + 100.0 * 8.418253898620605
Epoch 460, val loss: 1.0188732147216797
Epoch 470, training loss: 842.677978515625 = 1.0157644748687744 + 100.0 * 8.416622161865234
Epoch 470, val loss: 1.0174751281738281
Epoch 480, training loss: 842.3156127929688 = 1.0142730474472046 + 100.0 * 8.413013458251953
Epoch 480, val loss: 1.0160516500473022
Epoch 490, training loss: 841.9246826171875 = 1.012816309928894 + 100.0 * 8.40911865234375
Epoch 490, val loss: 1.0146585702896118
Epoch 500, training loss: 841.800048828125 = 1.0112886428833008 + 100.0 * 8.40788745880127
Epoch 500, val loss: 1.0131888389587402
Epoch 510, training loss: 841.52783203125 = 1.0096367597579956 + 100.0 * 8.405181884765625
Epoch 510, val loss: 1.011609435081482
Epoch 520, training loss: 841.33056640625 = 1.0079814195632935 + 100.0 * 8.403225898742676
Epoch 520, val loss: 1.009987711906433
Epoch 530, training loss: 840.9361572265625 = 1.0062564611434937 + 100.0 * 8.399298667907715
Epoch 530, val loss: 1.0083540678024292
Epoch 540, training loss: 840.7538452148438 = 1.0045220851898193 + 100.0 * 8.397493362426758
Epoch 540, val loss: 1.0066803693771362
Epoch 550, training loss: 840.5938110351562 = 1.0027011632919312 + 100.0 * 8.39591121673584
Epoch 550, val loss: 1.0049355030059814
Epoch 560, training loss: 840.4613037109375 = 1.0007576942443848 + 100.0 * 8.39460563659668
Epoch 560, val loss: 1.0030568838119507
Epoch 570, training loss: 840.2561645507812 = 0.9988145232200623 + 100.0 * 8.392573356628418
Epoch 570, val loss: 1.0011647939682007
Epoch 580, training loss: 840.0615234375 = 0.996855616569519 + 100.0 * 8.390646934509277
Epoch 580, val loss: 0.9992864727973938
Epoch 590, training loss: 840.0599975585938 = 0.9948131442070007 + 100.0 * 8.39065170288086
Epoch 590, val loss: 0.9972916841506958
Epoch 600, training loss: 839.9989624023438 = 0.9925914406776428 + 100.0 * 8.390063285827637
Epoch 600, val loss: 0.9951667189598083
Epoch 610, training loss: 839.6856689453125 = 0.9904338717460632 + 100.0 * 8.38695240020752
Epoch 610, val loss: 0.993070662021637
Epoch 620, training loss: 839.5089111328125 = 0.988241970539093 + 100.0 * 8.385207176208496
Epoch 620, val loss: 0.9909635782241821
Epoch 630, training loss: 839.3795776367188 = 0.9859763979911804 + 100.0 * 8.383935928344727
Epoch 630, val loss: 0.9887591004371643
Epoch 640, training loss: 839.874267578125 = 0.9836472868919373 + 100.0 * 8.388906478881836
Epoch 640, val loss: 0.9864040613174438
Epoch 650, training loss: 839.223876953125 = 0.980961799621582 + 100.0 * 8.382429122924805
Epoch 650, val loss: 0.9839211702346802
Epoch 660, training loss: 839.032958984375 = 0.9785078763961792 + 100.0 * 8.380544662475586
Epoch 660, val loss: 0.9815519452095032
Epoch 670, training loss: 838.8959350585938 = 0.976008951663971 + 100.0 * 8.379199028015137
Epoch 670, val loss: 0.9791407585144043
Epoch 680, training loss: 838.7730712890625 = 0.9734262824058533 + 100.0 * 8.377996444702148
Epoch 680, val loss: 0.9766490459442139
Epoch 690, training loss: 838.6491088867188 = 0.970784604549408 + 100.0 * 8.37678337097168
Epoch 690, val loss: 0.9740955233573914
Epoch 700, training loss: 838.6364135742188 = 0.968093991279602 + 100.0 * 8.376683235168457
Epoch 700, val loss: 0.971505343914032
Epoch 710, training loss: 838.5979614257812 = 0.9652158617973328 + 100.0 * 8.376327514648438
Epoch 710, val loss: 0.9686421751976013
Epoch 720, training loss: 838.3270263671875 = 0.9622968435287476 + 100.0 * 8.373647689819336
Epoch 720, val loss: 0.9659029245376587
Epoch 730, training loss: 838.2514038085938 = 0.9594760537147522 + 100.0 * 8.372919082641602
Epoch 730, val loss: 0.9632053971290588
Epoch 740, training loss: 838.1943969726562 = 0.9565259218215942 + 100.0 * 8.3723783493042
Epoch 740, val loss: 0.9603710770606995
Epoch 750, training loss: 838.0433349609375 = 0.9534880518913269 + 100.0 * 8.370898246765137
Epoch 750, val loss: 0.9574244022369385
Epoch 760, training loss: 837.9165649414062 = 0.9505167603492737 + 100.0 * 8.369660377502441
Epoch 760, val loss: 0.9544973373413086
Epoch 770, training loss: 837.8980712890625 = 0.9474144577980042 + 100.0 * 8.3695068359375
Epoch 770, val loss: 0.9514867663383484
Epoch 780, training loss: 837.8416137695312 = 0.9441447854042053 + 100.0 * 8.368974685668945
Epoch 780, val loss: 0.948424756526947
Epoch 790, training loss: 837.6301879882812 = 0.9409132599830627 + 100.0 * 8.36689281463623
Epoch 790, val loss: 0.9452757835388184
Epoch 800, training loss: 837.463134765625 = 0.9376809597015381 + 100.0 * 8.365254402160645
Epoch 800, val loss: 0.9421777725219727
Epoch 810, training loss: 837.4014892578125 = 0.9343829154968262 + 100.0 * 8.364670753479004
Epoch 810, val loss: 0.9390673041343689
Epoch 820, training loss: 837.5047607421875 = 0.9309597611427307 + 100.0 * 8.365737915039062
Epoch 820, val loss: 0.9357292056083679
Epoch 830, training loss: 837.2478637695312 = 0.9274914860725403 + 100.0 * 8.363204002380371
Epoch 830, val loss: 0.9324326515197754
Epoch 840, training loss: 837.1677856445312 = 0.9241021275520325 + 100.0 * 8.36243724822998
Epoch 840, val loss: 0.9291738271713257
Epoch 850, training loss: 836.9671630859375 = 0.9205516576766968 + 100.0 * 8.360466003417969
Epoch 850, val loss: 0.9257364273071289
Epoch 860, training loss: 836.8692016601562 = 0.9171040058135986 + 100.0 * 8.35952091217041
Epoch 860, val loss: 0.9224144220352173
Epoch 870, training loss: 836.8143920898438 = 0.9136160016059875 + 100.0 * 8.359007835388184
Epoch 870, val loss: 0.919022798538208
Epoch 880, training loss: 836.8480224609375 = 0.9099545478820801 + 100.0 * 8.359380722045898
Epoch 880, val loss: 0.915523111820221
Epoch 890, training loss: 836.5960083007812 = 0.9061602354049683 + 100.0 * 8.356898307800293
Epoch 890, val loss: 0.9119994044303894
Epoch 900, training loss: 836.5089721679688 = 0.9025667309761047 + 100.0 * 8.356063842773438
Epoch 900, val loss: 0.908508837223053
Epoch 910, training loss: 836.4570922851562 = 0.8987720608711243 + 100.0 * 8.355583190917969
Epoch 910, val loss: 0.9049426317214966
Epoch 920, training loss: 836.3197021484375 = 0.8950080275535583 + 100.0 * 8.354247093200684
Epoch 920, val loss: 0.9012799859046936
Epoch 930, training loss: 836.3436889648438 = 0.8912599682807922 + 100.0 * 8.354524612426758
Epoch 930, val loss: 0.8976466655731201
Epoch 940, training loss: 836.1093139648438 = 0.8874289393424988 + 100.0 * 8.352218627929688
Epoch 940, val loss: 0.8940417170524597
Epoch 950, training loss: 836.0097045898438 = 0.8836681842803955 + 100.0 * 8.3512601852417
Epoch 950, val loss: 0.8904111981391907
Epoch 960, training loss: 835.9658203125 = 0.8798584342002869 + 100.0 * 8.350859642028809
Epoch 960, val loss: 0.8867304921150208
Epoch 970, training loss: 835.947265625 = 0.8757784366607666 + 100.0 * 8.350714683532715
Epoch 970, val loss: 0.8828034400939941
Epoch 980, training loss: 835.8269653320312 = 0.8718006014823914 + 100.0 * 8.3495512008667
Epoch 980, val loss: 0.87909334897995
Epoch 990, training loss: 835.6881713867188 = 0.8679836392402649 + 100.0 * 8.348201751708984
Epoch 990, val loss: 0.8754785656929016
Epoch 1000, training loss: 835.5726318359375 = 0.8641228079795837 + 100.0 * 8.347084999084473
Epoch 1000, val loss: 0.871796190738678
Epoch 1010, training loss: 835.6116333007812 = 0.8602833151817322 + 100.0 * 8.347513198852539
Epoch 1010, val loss: 0.8681399822235107
Epoch 1020, training loss: 835.4630737304688 = 0.8560202121734619 + 100.0 * 8.346070289611816
Epoch 1020, val loss: 0.8640276193618774
Epoch 1030, training loss: 835.5328979492188 = 0.8519977331161499 + 100.0 * 8.346809387207031
Epoch 1030, val loss: 0.8602204918861389
Epoch 1040, training loss: 835.34814453125 = 0.8481681942939758 + 100.0 * 8.345000267028809
Epoch 1040, val loss: 0.8565829396247864
Epoch 1050, training loss: 835.2457885742188 = 0.844297468662262 + 100.0 * 8.344015121459961
Epoch 1050, val loss: 0.8528770804405212
Epoch 1060, training loss: 835.5435180664062 = 0.8404251933097839 + 100.0 * 8.347030639648438
Epoch 1060, val loss: 0.8491237163543701
Epoch 1070, training loss: 835.226806640625 = 0.8362616896629333 + 100.0 * 8.343905448913574
Epoch 1070, val loss: 0.8452339768409729
Epoch 1080, training loss: 835.0792236328125 = 0.8324092626571655 + 100.0 * 8.34246826171875
Epoch 1080, val loss: 0.8415926098823547
Epoch 1090, training loss: 834.9899291992188 = 0.8285242915153503 + 100.0 * 8.34161376953125
Epoch 1090, val loss: 0.8379061222076416
Epoch 1100, training loss: 835.0951538085938 = 0.824579119682312 + 100.0 * 8.342705726623535
Epoch 1100, val loss: 0.8342476487159729
Epoch 1110, training loss: 834.9015502929688 = 0.8203925490379333 + 100.0 * 8.340811729431152
Epoch 1110, val loss: 0.8301060199737549
Epoch 1120, training loss: 834.8916625976562 = 0.8164375424385071 + 100.0 * 8.340751647949219
Epoch 1120, val loss: 0.8264597058296204
Epoch 1130, training loss: 834.7705078125 = 0.8126257061958313 + 100.0 * 8.339578628540039
Epoch 1130, val loss: 0.822721540927887
Epoch 1140, training loss: 834.6844482421875 = 0.8086881637573242 + 100.0 * 8.338757514953613
Epoch 1140, val loss: 0.819042980670929
Epoch 1150, training loss: 834.6312255859375 = 0.8048559427261353 + 100.0 * 8.338263511657715
Epoch 1150, val loss: 0.8153764009475708
Epoch 1160, training loss: 835.1753540039062 = 0.8009982109069824 + 100.0 * 8.343743324279785
Epoch 1160, val loss: 0.8115516304969788
Epoch 1170, training loss: 834.7996215820312 = 0.7964653372764587 + 100.0 * 8.340031623840332
Epoch 1170, val loss: 0.8074809908866882
Epoch 1180, training loss: 834.5907592773438 = 0.7926703691482544 + 100.0 * 8.337981224060059
Epoch 1180, val loss: 0.8038787841796875
Epoch 1190, training loss: 834.4318237304688 = 0.788898766040802 + 100.0 * 8.336429595947266
Epoch 1190, val loss: 0.8002858757972717
Epoch 1200, training loss: 834.37548828125 = 0.7850842475891113 + 100.0 * 8.335904121398926
Epoch 1200, val loss: 0.7966502904891968
Epoch 1210, training loss: 834.335205078125 = 0.7813097238540649 + 100.0 * 8.335538864135742
Epoch 1210, val loss: 0.7930803894996643
Epoch 1220, training loss: 834.5296020507812 = 0.7775190472602844 + 100.0 * 8.337520599365234
Epoch 1220, val loss: 0.7894459962844849
Epoch 1230, training loss: 834.3391723632812 = 0.7732226848602295 + 100.0 * 8.335659980773926
Epoch 1230, val loss: 0.7853962779045105
Epoch 1240, training loss: 834.3446655273438 = 0.7694000005722046 + 100.0 * 8.335752487182617
Epoch 1240, val loss: 0.7817864418029785
Epoch 1250, training loss: 834.1595458984375 = 0.7655445337295532 + 100.0 * 8.333939552307129
Epoch 1250, val loss: 0.7781721353530884
Epoch 1260, training loss: 834.1126098632812 = 0.7617851495742798 + 100.0 * 8.333508491516113
Epoch 1260, val loss: 0.7746378779411316
Epoch 1270, training loss: 834.2676391601562 = 0.7580919861793518 + 100.0 * 8.335095405578613
Epoch 1270, val loss: 0.7710696458816528
Epoch 1280, training loss: 834.0213623046875 = 0.7539684772491455 + 100.0 * 8.332674026489258
Epoch 1280, val loss: 0.7672343254089355
Epoch 1290, training loss: 833.9923706054688 = 0.7502469420433044 + 100.0 * 8.33242130279541
Epoch 1290, val loss: 0.7637724280357361
Epoch 1300, training loss: 833.9547729492188 = 0.7465034127235413 + 100.0 * 8.332082748413086
Epoch 1300, val loss: 0.7602066993713379
Epoch 1310, training loss: 833.9852905273438 = 0.742821216583252 + 100.0 * 8.332425117492676
Epoch 1310, val loss: 0.7567793130874634
Epoch 1320, training loss: 833.9651489257812 = 0.7389525175094604 + 100.0 * 8.33226203918457
Epoch 1320, val loss: 0.7531397938728333
Epoch 1330, training loss: 833.836669921875 = 0.7353087663650513 + 100.0 * 8.331013679504395
Epoch 1330, val loss: 0.7496780157089233
Epoch 1340, training loss: 833.7844848632812 = 0.7316567897796631 + 100.0 * 8.330528259277344
Epoch 1340, val loss: 0.7462251782417297
Epoch 1350, training loss: 833.7650756835938 = 0.7280734181404114 + 100.0 * 8.33036994934082
Epoch 1350, val loss: 0.7428320646286011
Epoch 1360, training loss: 834.0543823242188 = 0.7244859933853149 + 100.0 * 8.333298683166504
Epoch 1360, val loss: 0.7392992377281189
Epoch 1370, training loss: 833.7431030273438 = 0.7204981446266174 + 100.0 * 8.330225944519043
Epoch 1370, val loss: 0.7358665466308594
Epoch 1380, training loss: 833.6740112304688 = 0.7170201539993286 + 100.0 * 8.329569816589355
Epoch 1380, val loss: 0.7324758768081665
Epoch 1390, training loss: 833.60595703125 = 0.7134227156639099 + 100.0 * 8.328925132751465
Epoch 1390, val loss: 0.7291684150695801
Epoch 1400, training loss: 833.5593872070312 = 0.7099436521530151 + 100.0 * 8.32849407196045
Epoch 1400, val loss: 0.7258658409118652
Epoch 1410, training loss: 833.8621826171875 = 0.7063561081886292 + 100.0 * 8.331558227539062
Epoch 1410, val loss: 0.72251296043396
Epoch 1420, training loss: 833.6622314453125 = 0.702572226524353 + 100.0 * 8.329596519470215
Epoch 1420, val loss: 0.7190101742744446
Epoch 1430, training loss: 833.647216796875 = 0.6991051435470581 + 100.0 * 8.32948112487793
Epoch 1430, val loss: 0.7156765460968018
Epoch 1440, training loss: 833.4329833984375 = 0.6955376267433167 + 100.0 * 8.327374458312988
Epoch 1440, val loss: 0.7123425602912903
Epoch 1450, training loss: 833.4117431640625 = 0.6921461224555969 + 100.0 * 8.32719612121582
Epoch 1450, val loss: 0.7092748284339905
Epoch 1460, training loss: 833.3887329101562 = 0.6887784600257874 + 100.0 * 8.32699966430664
Epoch 1460, val loss: 0.7060697078704834
Epoch 1470, training loss: 833.4483642578125 = 0.6853745579719543 + 100.0 * 8.327630043029785
Epoch 1470, val loss: 0.7028983235359192
Epoch 1480, training loss: 833.2843627929688 = 0.6819568276405334 + 100.0 * 8.326024055480957
Epoch 1480, val loss: 0.6997102499008179
Epoch 1490, training loss: 833.349365234375 = 0.6786476373672485 + 100.0 * 8.326706886291504
Epoch 1490, val loss: 0.6966282725334167
Epoch 1500, training loss: 833.4754638671875 = 0.6750378012657166 + 100.0 * 8.328003883361816
Epoch 1500, val loss: 0.6932991743087769
Epoch 1510, training loss: 833.2223510742188 = 0.6717543005943298 + 100.0 * 8.325506210327148
Epoch 1510, val loss: 0.6901003122329712
Epoch 1520, training loss: 833.1531372070312 = 0.6684936285018921 + 100.0 * 8.324846267700195
Epoch 1520, val loss: 0.687085747718811
Epoch 1530, training loss: 833.1114501953125 = 0.665277898311615 + 100.0 * 8.324461936950684
Epoch 1530, val loss: 0.684101939201355
Epoch 1540, training loss: 833.1143188476562 = 0.6620951294898987 + 100.0 * 8.324522018432617
Epoch 1540, val loss: 0.6810991168022156
Epoch 1550, training loss: 833.4249267578125 = 0.658877432346344 + 100.0 * 8.32766056060791
Epoch 1550, val loss: 0.678004801273346
Epoch 1560, training loss: 833.166015625 = 0.6553320288658142 + 100.0 * 8.325106620788574
Epoch 1560, val loss: 0.6748324632644653
Epoch 1570, training loss: 833.0531005859375 = 0.65220707654953 + 100.0 * 8.32400894165039
Epoch 1570, val loss: 0.6719224452972412
Epoch 1580, training loss: 832.9674072265625 = 0.649010181427002 + 100.0 * 8.3231840133667
Epoch 1580, val loss: 0.6689733266830444
Epoch 1590, training loss: 832.921142578125 = 0.6459475159645081 + 100.0 * 8.322751998901367
Epoch 1590, val loss: 0.6661111116409302
Epoch 1600, training loss: 833.061279296875 = 0.6428271532058716 + 100.0 * 8.32418441772461
Epoch 1600, val loss: 0.6632481813430786
Epoch 1610, training loss: 832.8948974609375 = 0.6395306587219238 + 100.0 * 8.322553634643555
Epoch 1610, val loss: 0.6600965261459351
Epoch 1620, training loss: 832.97216796875 = 0.6364734172821045 + 100.0 * 8.323356628417969
Epoch 1620, val loss: 0.657254159450531
Epoch 1630, training loss: 833.0321044921875 = 0.6333004832267761 + 100.0 * 8.32398796081543
Epoch 1630, val loss: 0.6544474363327026
Epoch 1640, training loss: 832.815673828125 = 0.6302875280380249 + 100.0 * 8.321853637695312
Epoch 1640, val loss: 0.6514835953712463
Epoch 1650, training loss: 832.748779296875 = 0.6273995637893677 + 100.0 * 8.321213722229004
Epoch 1650, val loss: 0.6488757729530334
Epoch 1660, training loss: 832.6921997070312 = 0.6245143413543701 + 100.0 * 8.320676803588867
Epoch 1660, val loss: 0.6461805701255798
Epoch 1670, training loss: 832.685546875 = 0.6216745972633362 + 100.0 * 8.320638656616211
Epoch 1670, val loss: 0.643531858921051
Epoch 1680, training loss: 833.0452880859375 = 0.6186941266059875 + 100.0 * 8.324265480041504
Epoch 1680, val loss: 0.640778124332428
Epoch 1690, training loss: 832.7399291992188 = 0.6157174110412598 + 100.0 * 8.321242332458496
Epoch 1690, val loss: 0.6380547881126404
Epoch 1700, training loss: 832.6902465820312 = 0.6128537058830261 + 100.0 * 8.32077407836914
Epoch 1700, val loss: 0.6353884935379028
Epoch 1710, training loss: 832.6216430664062 = 0.6100279688835144 + 100.0 * 8.32011604309082
Epoch 1710, val loss: 0.6328057646751404
Epoch 1720, training loss: 832.526611328125 = 0.6072711944580078 + 100.0 * 8.319192886352539
Epoch 1720, val loss: 0.6302248239517212
Epoch 1730, training loss: 832.5033569335938 = 0.6045751571655273 + 100.0 * 8.318987846374512
Epoch 1730, val loss: 0.6277401447296143
Epoch 1740, training loss: 832.7540283203125 = 0.6017435193061829 + 100.0 * 8.32152271270752
Epoch 1740, val loss: 0.6251343488693237
Epoch 1750, training loss: 832.4157104492188 = 0.5989461541175842 + 100.0 * 8.318167686462402
Epoch 1750, val loss: 0.6225387454032898
Epoch 1760, training loss: 832.39990234375 = 0.5963441133499146 + 100.0 * 8.318035125732422
Epoch 1760, val loss: 0.6201472878456116
Epoch 1770, training loss: 832.5665893554688 = 0.5936198830604553 + 100.0 * 8.319729804992676
Epoch 1770, val loss: 0.6177283525466919
Epoch 1780, training loss: 832.5734252929688 = 0.5908262729644775 + 100.0 * 8.319826126098633
Epoch 1780, val loss: 0.614899754524231
Epoch 1790, training loss: 832.4008178710938 = 0.5881206393241882 + 100.0 * 8.318126678466797
Epoch 1790, val loss: 0.612647294998169
Epoch 1800, training loss: 832.291748046875 = 0.5856660604476929 + 100.0 * 8.317060470581055
Epoch 1800, val loss: 0.6102478504180908
Epoch 1810, training loss: 832.2266845703125 = 0.5831482410430908 + 100.0 * 8.316435813903809
Epoch 1810, val loss: 0.6079836487770081
Epoch 1820, training loss: 832.207275390625 = 0.5806877613067627 + 100.0 * 8.316266059875488
Epoch 1820, val loss: 0.6057331562042236
Epoch 1830, training loss: 832.4689331054688 = 0.5782079100608826 + 100.0 * 8.318907737731934
Epoch 1830, val loss: 0.6033685803413391
Epoch 1840, training loss: 832.1887817382812 = 0.5754573941230774 + 100.0 * 8.316133499145508
Epoch 1840, val loss: 0.6009617447853088
Epoch 1850, training loss: 832.1508178710938 = 0.57305908203125 + 100.0 * 8.315777778625488
Epoch 1850, val loss: 0.5986894369125366
Epoch 1860, training loss: 832.108642578125 = 0.5706099271774292 + 100.0 * 8.315380096435547
Epoch 1860, val loss: 0.5965111255645752
Epoch 1870, training loss: 832.0745239257812 = 0.5682682394981384 + 100.0 * 8.315062522888184
Epoch 1870, val loss: 0.5943199992179871
Epoch 1880, training loss: 832.2238159179688 = 0.5659089088439941 + 100.0 * 8.31657886505127
Epoch 1880, val loss: 0.5921559929847717
Epoch 1890, training loss: 832.0471801757812 = 0.5633790493011475 + 100.0 * 8.314838409423828
Epoch 1890, val loss: 0.5898654460906982
Epoch 1900, training loss: 832.5745239257812 = 0.5610513687133789 + 100.0 * 8.320135116577148
Epoch 1900, val loss: 0.5876510739326477
Epoch 1910, training loss: 832.2008666992188 = 0.5581942200660706 + 100.0 * 8.316427230834961
Epoch 1910, val loss: 0.5851296186447144
Epoch 1920, training loss: 832.0084228515625 = 0.5561766028404236 + 100.0 * 8.314522743225098
Epoch 1920, val loss: 0.5832581520080566
Epoch 1930, training loss: 831.9276733398438 = 0.5538976788520813 + 100.0 * 8.313737869262695
Epoch 1930, val loss: 0.581202507019043
Epoch 1940, training loss: 831.887939453125 = 0.5517923831939697 + 100.0 * 8.313361167907715
Epoch 1940, val loss: 0.579277753829956
Epoch 1950, training loss: 831.853515625 = 0.5496387481689453 + 100.0 * 8.31303882598877
Epoch 1950, val loss: 0.5773041844367981
Epoch 1960, training loss: 831.8525390625 = 0.5474746227264404 + 100.0 * 8.313050270080566
Epoch 1960, val loss: 0.5753232836723328
Epoch 1970, training loss: 832.2969970703125 = 0.5451846718788147 + 100.0 * 8.31751823425293
Epoch 1970, val loss: 0.5731483101844788
Epoch 1980, training loss: 831.9346313476562 = 0.5429028868675232 + 100.0 * 8.31391716003418
Epoch 1980, val loss: 0.5712308287620544
Epoch 1990, training loss: 831.7871704101562 = 0.5407904386520386 + 100.0 * 8.312463760375977
Epoch 1990, val loss: 0.5692430734634399
Epoch 2000, training loss: 831.7483520507812 = 0.5386883616447449 + 100.0 * 8.31209659576416
Epoch 2000, val loss: 0.567371129989624
Epoch 2010, training loss: 831.7498779296875 = 0.5366225838661194 + 100.0 * 8.312132835388184
Epoch 2010, val loss: 0.5654881596565247
Epoch 2020, training loss: 832.34521484375 = 0.5344031453132629 + 100.0 * 8.318107604980469
Epoch 2020, val loss: 0.5635073184967041
Epoch 2030, training loss: 831.8884887695312 = 0.532239556312561 + 100.0 * 8.313562393188477
Epoch 2030, val loss: 0.5614914894104004
Epoch 2040, training loss: 831.7321166992188 = 0.5302255749702454 + 100.0 * 8.312019348144531
Epoch 2040, val loss: 0.5596792101860046
Epoch 2050, training loss: 831.6570434570312 = 0.5282729864120483 + 100.0 * 8.311287879943848
Epoch 2050, val loss: 0.5579248666763306
Epoch 2060, training loss: 831.6190185546875 = 0.5263393521308899 + 100.0 * 8.31092643737793
Epoch 2060, val loss: 0.5561468601226807
Epoch 2070, training loss: 831.679443359375 = 0.5244252681732178 + 100.0 * 8.31155014038086
Epoch 2070, val loss: 0.5543630719184875
Epoch 2080, training loss: 831.976806640625 = 0.522242546081543 + 100.0 * 8.314545631408691
Epoch 2080, val loss: 0.5524480938911438
Epoch 2090, training loss: 831.6111450195312 = 0.5202073454856873 + 100.0 * 8.310909271240234
Epoch 2090, val loss: 0.5505401492118835
Epoch 2100, training loss: 831.5711669921875 = 0.5183662176132202 + 100.0 * 8.310527801513672
Epoch 2100, val loss: 0.549008309841156
Epoch 2110, training loss: 831.5081176757812 = 0.5165069699287415 + 100.0 * 8.309916496276855
Epoch 2110, val loss: 0.5472474694252014
Epoch 2120, training loss: 831.4845581054688 = 0.5147427320480347 + 100.0 * 8.309698104858398
Epoch 2120, val loss: 0.5456759333610535
Epoch 2130, training loss: 831.4609985351562 = 0.5129398703575134 + 100.0 * 8.309480667114258
Epoch 2130, val loss: 0.544059157371521
Epoch 2140, training loss: 831.4656982421875 = 0.5111780166625977 + 100.0 * 8.309545516967773
Epoch 2140, val loss: 0.5424478054046631
Epoch 2150, training loss: 832.0650634765625 = 0.5093607306480408 + 100.0 * 8.315557479858398
Epoch 2150, val loss: 0.540747344493866
Epoch 2160, training loss: 831.604248046875 = 0.5072024464607239 + 100.0 * 8.310970306396484
Epoch 2160, val loss: 0.5389196276664734
Epoch 2170, training loss: 831.4329223632812 = 0.5055702328681946 + 100.0 * 8.309273719787598
Epoch 2170, val loss: 0.5373566150665283
Epoch 2180, training loss: 831.3560180664062 = 0.503786027431488 + 100.0 * 8.30852222442627
Epoch 2180, val loss: 0.5358222723007202
Epoch 2190, training loss: 831.3389282226562 = 0.5021188259124756 + 100.0 * 8.308367729187012
Epoch 2190, val loss: 0.534335196018219
Epoch 2200, training loss: 831.439697265625 = 0.5004597306251526 + 100.0 * 8.309391975402832
Epoch 2200, val loss: 0.5328930616378784
Epoch 2210, training loss: 831.5344848632812 = 0.49845612049102783 + 100.0 * 8.310359954833984
Epoch 2210, val loss: 0.5309314131736755
Epoch 2220, training loss: 831.3367309570312 = 0.4966834485530853 + 100.0 * 8.30840015411377
Epoch 2220, val loss: 0.529497504234314
Epoch 2230, training loss: 831.328369140625 = 0.4950600564479828 + 100.0 * 8.308333396911621
Epoch 2230, val loss: 0.52796870470047
Epoch 2240, training loss: 831.2442016601562 = 0.4934505522251129 + 100.0 * 8.307507514953613
Epoch 2240, val loss: 0.5265384912490845
Epoch 2250, training loss: 831.2213745117188 = 0.49187901616096497 + 100.0 * 8.307294845581055
Epoch 2250, val loss: 0.5251243710517883
Epoch 2260, training loss: 831.4227294921875 = 0.49030134081840515 + 100.0 * 8.309324264526367
Epoch 2260, val loss: 0.5236414074897766
Epoch 2270, training loss: 831.2372436523438 = 0.48849400877952576 + 100.0 * 8.307487487792969
Epoch 2270, val loss: 0.5221723318099976
Epoch 2280, training loss: 831.19287109375 = 0.486935555934906 + 100.0 * 8.307059288024902
Epoch 2280, val loss: 0.5207082033157349
Epoch 2290, training loss: 831.163330078125 = 0.48538270592689514 + 100.0 * 8.306778907775879
Epoch 2290, val loss: 0.519397497177124
Epoch 2300, training loss: 831.1411743164062 = 0.48389580845832825 + 100.0 * 8.306572914123535
Epoch 2300, val loss: 0.5180320143699646
Epoch 2310, training loss: 831.35302734375 = 0.4824296236038208 + 100.0 * 8.308706283569336
Epoch 2310, val loss: 0.516808032989502
Epoch 2320, training loss: 831.1046752929688 = 0.48067814111709595 + 100.0 * 8.30624008178711
Epoch 2320, val loss: 0.5151247382164001
Epoch 2330, training loss: 831.092529296875 = 0.47920918464660645 + 100.0 * 8.306133270263672
Epoch 2330, val loss: 0.5139657258987427
Epoch 2340, training loss: 831.0636596679688 = 0.4777775704860687 + 100.0 * 8.305858612060547
Epoch 2340, val loss: 0.5126198530197144
Epoch 2350, training loss: 831.0285034179688 = 0.47633296251296997 + 100.0 * 8.305521965026855
Epoch 2350, val loss: 0.5113819241523743
Epoch 2360, training loss: 831.010498046875 = 0.4749307930469513 + 100.0 * 8.3053560256958
Epoch 2360, val loss: 0.5101422071456909
Epoch 2370, training loss: 831.029052734375 = 0.47353190183639526 + 100.0 * 8.30555534362793
Epoch 2370, val loss: 0.5088491439819336
Epoch 2380, training loss: 831.3919677734375 = 0.4720507562160492 + 100.0 * 8.309199333190918
Epoch 2380, val loss: 0.5074471831321716
Epoch 2390, training loss: 831.1397094726562 = 0.4705732762813568 + 100.0 * 8.30669116973877
Epoch 2390, val loss: 0.5064161419868469
Epoch 2400, training loss: 831.0722045898438 = 0.4691736698150635 + 100.0 * 8.3060302734375
Epoch 2400, val loss: 0.5050154328346252
Epoch 2410, training loss: 831.0592651367188 = 0.4677814841270447 + 100.0 * 8.305914878845215
Epoch 2410, val loss: 0.5039325952529907
Epoch 2420, training loss: 830.9010620117188 = 0.4663979709148407 + 100.0 * 8.304347038269043
Epoch 2420, val loss: 0.5026260018348694
Epoch 2430, training loss: 830.9034423828125 = 0.46508461236953735 + 100.0 * 8.304383277893066
Epoch 2430, val loss: 0.5014809966087341
Epoch 2440, training loss: 831.010986328125 = 0.46374955773353577 + 100.0 * 8.305472373962402
Epoch 2440, val loss: 0.5002903342247009
Epoch 2450, training loss: 830.9818115234375 = 0.4623642563819885 + 100.0 * 8.305194854736328
Epoch 2450, val loss: 0.4991263449192047
Epoch 2460, training loss: 831.198974609375 = 0.461119681596756 + 100.0 * 8.307378768920898
Epoch 2460, val loss: 0.4980633556842804
Epoch 2470, training loss: 830.8748779296875 = 0.45965972542762756 + 100.0 * 8.304152488708496
Epoch 2470, val loss: 0.4966992437839508
Epoch 2480, training loss: 830.8390502929688 = 0.45846596360206604 + 100.0 * 8.303805351257324
Epoch 2480, val loss: 0.49578234553337097
Epoch 2490, training loss: 830.7911376953125 = 0.4572370648384094 + 100.0 * 8.303339004516602
Epoch 2490, val loss: 0.49466270208358765
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8280060882800608
0.8643048612620445
The final CL Acc:0.82107, 0.01165, The final GNN Acc:0.86397, 0.00053
Begin epxeriment: noisy_level: 0.2 cont_weight: 100 epoch: der1:0.3 der2:0 dfr1:0.4 dfr2:0.4
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=2500, cl_num_layers=2, cl_num_proj_hidden=64, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=100.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.0, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=32, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, noisy_level=0.2, num_hidden=64, num_proj_hidden=64, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=40, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 88648])
add edge: torch.Size([2, 106318])
remove edge: torch.Size([2, 70852])
updated graph: torch.Size([2, 88522])
=== Noisy graph ===
/home/project-robust-contrastive/Robust-Contrastive-Learning/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 1059.344482421875 = 1.1152063608169556 + 100.0 * 10.582292556762695
Epoch 0, val loss: 1.1154061555862427
Epoch 10, training loss: 1059.302490234375 = 1.1101322174072266 + 100.0 * 10.581923484802246
Epoch 10, val loss: 1.1103371381759644
Epoch 20, training loss: 1059.1094970703125 = 1.1045154333114624 + 100.0 * 10.580049514770508
Epoch 20, val loss: 1.104736566543579
Epoch 30, training loss: 1058.21484375 = 1.0981937646865845 + 100.0 * 10.5711669921875
Epoch 30, val loss: 1.0984392166137695
Epoch 40, training loss: 1054.955810546875 = 1.0911630392074585 + 100.0 * 10.538646697998047
Epoch 40, val loss: 1.0914485454559326
Epoch 50, training loss: 1047.3165283203125 = 1.083545446395874 + 100.0 * 10.46233081817627
Epoch 50, val loss: 1.0839654207229614
Epoch 60, training loss: 1031.55810546875 = 1.0768963098526 + 100.0 * 10.304811477661133
Epoch 60, val loss: 1.0774972438812256
Epoch 70, training loss: 1004.2965698242188 = 1.0699806213378906 + 100.0 * 10.032265663146973
Epoch 70, val loss: 1.0705349445343018
Epoch 80, training loss: 977.2039794921875 = 1.0631139278411865 + 100.0 * 9.761408805847168
Epoch 80, val loss: 1.0637905597686768
Epoch 90, training loss: 962.6209716796875 = 1.0582412481307983 + 100.0 * 9.61562728881836
Epoch 90, val loss: 1.0590091943740845
Epoch 100, training loss: 957.174560546875 = 1.0542393922805786 + 100.0 * 9.561203002929688
Epoch 100, val loss: 1.0550516843795776
Epoch 110, training loss: 950.1845092773438 = 1.0506725311279297 + 100.0 * 9.491338729858398
Epoch 110, val loss: 1.0515141487121582
Epoch 120, training loss: 938.1422729492188 = 1.0473157167434692 + 100.0 * 9.370949745178223
Epoch 120, val loss: 1.0481865406036377
Epoch 130, training loss: 926.2328491210938 = 1.0436315536499023 + 100.0 * 9.25189208984375
Epoch 130, val loss: 1.0445104837417603
Epoch 140, training loss: 921.0589599609375 = 1.0397413969039917 + 100.0 * 9.20019245147705
Epoch 140, val loss: 1.0406036376953125
Epoch 150, training loss: 916.7967529296875 = 1.0359774827957153 + 100.0 * 9.157608032226562
Epoch 150, val loss: 1.0368709564208984
Epoch 160, training loss: 909.781494140625 = 1.0336172580718994 + 100.0 * 9.087478637695312
Epoch 160, val loss: 1.0346007347106934
Epoch 170, training loss: 902.8958740234375 = 1.0321006774902344 + 100.0 * 9.018637657165527
Epoch 170, val loss: 1.0329900979995728
Epoch 180, training loss: 900.01416015625 = 1.029009222984314 + 100.0 * 8.989851951599121
Epoch 180, val loss: 1.0298223495483398
Epoch 190, training loss: 896.718994140625 = 1.0252022743225098 + 100.0 * 8.956937789916992
Epoch 190, val loss: 1.0261791944503784
Epoch 200, training loss: 892.683349609375 = 1.0220717191696167 + 100.0 * 8.91661262512207
Epoch 200, val loss: 1.0232410430908203
Epoch 210, training loss: 888.7892456054688 = 1.0193663835525513 + 100.0 * 8.87769889831543
Epoch 210, val loss: 1.0206657648086548
Epoch 220, training loss: 886.0104370117188 = 1.0164481401443481 + 100.0 * 8.849940299987793
Epoch 220, val loss: 1.0178110599517822
Epoch 230, training loss: 883.54736328125 = 1.0130361318588257 + 100.0 * 8.825343132019043
Epoch 230, val loss: 1.0145047903060913
Epoch 240, training loss: 880.7156372070312 = 1.0095024108886719 + 100.0 * 8.7970609664917
Epoch 240, val loss: 1.0111430883407593
Epoch 250, training loss: 877.3778686523438 = 1.0059336423873901 + 100.0 * 8.76371955871582
Epoch 250, val loss: 1.007789969444275
Epoch 260, training loss: 874.6627807617188 = 1.0021687746047974 + 100.0 * 8.736605644226074
Epoch 260, val loss: 1.0041738748550415
Epoch 270, training loss: 872.9391479492188 = 0.9978369474411011 + 100.0 * 8.719412803649902
Epoch 270, val loss: 0.9999930262565613
Epoch 280, training loss: 871.7277221679688 = 0.9928761124610901 + 100.0 * 8.707348823547363
Epoch 280, val loss: 0.9952350854873657
Epoch 290, training loss: 870.7908325195312 = 0.9874815940856934 + 100.0 * 8.698033332824707
Epoch 290, val loss: 0.9900955557823181
Epoch 300, training loss: 869.9705200195312 = 0.98176109790802 + 100.0 * 8.689888000488281
Epoch 300, val loss: 0.9846773147583008
Epoch 310, training loss: 869.3453369140625 = 0.9758217930793762 + 100.0 * 8.683694839477539
Epoch 310, val loss: 0.979047417640686
Epoch 320, training loss: 868.3483276367188 = 0.9696288108825684 + 100.0 * 8.673787117004395
Epoch 320, val loss: 0.9732004404067993
Epoch 330, training loss: 867.4492797851562 = 0.963212251663208 + 100.0 * 8.664860725402832
Epoch 330, val loss: 0.9671801328659058
Epoch 340, training loss: 866.759765625 = 0.9565814733505249 + 100.0 * 8.658031463623047
Epoch 340, val loss: 0.9609543681144714
Epoch 350, training loss: 865.4676513671875 = 0.9497339129447937 + 100.0 * 8.645179748535156
Epoch 350, val loss: 0.9545539021492004
Epoch 360, training loss: 864.1890869140625 = 0.9427640438079834 + 100.0 * 8.632463455200195
Epoch 360, val loss: 0.9480674862861633
Epoch 370, training loss: 862.8903198242188 = 0.9356276988983154 + 100.0 * 8.619546890258789
Epoch 370, val loss: 0.9414406418800354
Epoch 380, training loss: 861.8472900390625 = 0.9282458424568176 + 100.0 * 8.609190940856934
Epoch 380, val loss: 0.9345664381980896
Epoch 390, training loss: 860.7595825195312 = 0.9205332398414612 + 100.0 * 8.598390579223633
Epoch 390, val loss: 0.9275014996528625
Epoch 400, training loss: 859.6516723632812 = 0.9126679301261902 + 100.0 * 8.587389945983887
Epoch 400, val loss: 0.9202499985694885
Epoch 410, training loss: 858.7638549804688 = 0.9045815467834473 + 100.0 * 8.578592300415039
Epoch 410, val loss: 0.9128481149673462
Epoch 420, training loss: 858.10107421875 = 0.8962437510490417 + 100.0 * 8.57204818725586
Epoch 420, val loss: 0.9052218198776245
Epoch 430, training loss: 857.3328247070312 = 0.8878146409988403 + 100.0 * 8.56445026397705
Epoch 430, val loss: 0.8975012302398682
Epoch 440, training loss: 856.64404296875 = 0.8794063329696655 + 100.0 * 8.557646751403809
Epoch 440, val loss: 0.8898633122444153
Epoch 450, training loss: 856.10205078125 = 0.8709621429443359 + 100.0 * 8.552310943603516
Epoch 450, val loss: 0.882212221622467
Epoch 460, training loss: 855.5974731445312 = 0.8625141978263855 + 100.0 * 8.54734992980957
Epoch 460, val loss: 0.8745067119598389
Epoch 470, training loss: 855.07666015625 = 0.8540451526641846 + 100.0 * 8.54222583770752
Epoch 470, val loss: 0.8668663501739502
Epoch 480, training loss: 854.641845703125 = 0.8457436561584473 + 100.0 * 8.53796100616455
Epoch 480, val loss: 0.8593631982803345
Epoch 490, training loss: 854.1561279296875 = 0.8374704718589783 + 100.0 * 8.533186912536621
Epoch 490, val loss: 0.8518919944763184
Epoch 500, training loss: 853.82080078125 = 0.8293277621269226 + 100.0 * 8.529914855957031
Epoch 500, val loss: 0.8445623517036438
Epoch 510, training loss: 853.4125366210938 = 0.8212718963623047 + 100.0 * 8.525912284851074
Epoch 510, val loss: 0.8372700214385986
Epoch 520, training loss: 852.9766235351562 = 0.8133018016815186 + 100.0 * 8.52163314819336
Epoch 520, val loss: 0.8301051259040833
Epoch 530, training loss: 852.90380859375 = 0.8054099082946777 + 100.0 * 8.520983695983887
Epoch 530, val loss: 0.8229665756225586
Epoch 540, training loss: 852.2940673828125 = 0.7975624203681946 + 100.0 * 8.514965057373047
Epoch 540, val loss: 0.8159045577049255
Epoch 550, training loss: 851.9147338867188 = 0.7898787260055542 + 100.0 * 8.511248588562012
Epoch 550, val loss: 0.8090182542800903
Epoch 560, training loss: 851.547607421875 = 0.7823367714881897 + 100.0 * 8.507652282714844
Epoch 560, val loss: 0.802212655544281
Epoch 570, training loss: 851.3013916015625 = 0.7748451232910156 + 100.0 * 8.505265235900879
Epoch 570, val loss: 0.7954819798469543
Epoch 580, training loss: 851.0932006835938 = 0.7673877477645874 + 100.0 * 8.503257751464844
Epoch 580, val loss: 0.7888046503067017
Epoch 590, training loss: 850.6778564453125 = 0.7600604295730591 + 100.0 * 8.499177932739258
Epoch 590, val loss: 0.782233476638794
Epoch 600, training loss: 850.8128662109375 = 0.752800703048706 + 100.0 * 8.500600814819336
Epoch 600, val loss: 0.7757132053375244
Epoch 610, training loss: 850.1871948242188 = 0.7456554770469666 + 100.0 * 8.494415283203125
Epoch 610, val loss: 0.7692914605140686
Epoch 620, training loss: 849.86181640625 = 0.7386365532875061 + 100.0 * 8.491231918334961
Epoch 620, val loss: 0.7629951238632202
Epoch 630, training loss: 849.6378173828125 = 0.7317089438438416 + 100.0 * 8.48906135559082
Epoch 630, val loss: 0.7568204998970032
Epoch 640, training loss: 849.3882446289062 = 0.724886953830719 + 100.0 * 8.48663330078125
Epoch 640, val loss: 0.7506911158561707
Epoch 650, training loss: 849.1893920898438 = 0.718198835849762 + 100.0 * 8.484711647033691
Epoch 650, val loss: 0.7447023391723633
Epoch 660, training loss: 849.1050415039062 = 0.7114788293838501 + 100.0 * 8.483935356140137
Epoch 660, val loss: 0.7386226654052734
Epoch 670, training loss: 848.8117065429688 = 0.7048597931861877 + 100.0 * 8.48106861114502
Epoch 670, val loss: 0.7327218055725098
Epoch 680, training loss: 848.5911254882812 = 0.6985442638397217 + 100.0 * 8.478925704956055
Epoch 680, val loss: 0.7270995378494263
Epoch 690, training loss: 848.4182739257812 = 0.6923393607139587 + 100.0 * 8.477259635925293
Epoch 690, val loss: 0.7215521335601807
Epoch 700, training loss: 848.2391357421875 = 0.6862894296646118 + 100.0 * 8.475528717041016
Epoch 700, val loss: 0.7161691784858704
Epoch 710, training loss: 848.0700073242188 = 0.6804078221321106 + 100.0 * 8.473896026611328
Epoch 710, val loss: 0.7109687328338623
Epoch 720, training loss: 847.9578857421875 = 0.6746541857719421 + 100.0 * 8.472831726074219
Epoch 720, val loss: 0.7058933973312378
Epoch 730, training loss: 847.8737182617188 = 0.6690244674682617 + 100.0 * 8.472046852111816
Epoch 730, val loss: 0.7008333206176758
Epoch 740, training loss: 847.6432495117188 = 0.6636120676994324 + 100.0 * 8.469796180725098
Epoch 740, val loss: 0.6961145401000977
Epoch 750, training loss: 847.4896850585938 = 0.6584240198135376 + 100.0 * 8.46831226348877
Epoch 750, val loss: 0.6915838718414307
Epoch 760, training loss: 847.3560180664062 = 0.6534172296524048 + 100.0 * 8.467025756835938
Epoch 760, val loss: 0.6872105598449707
Epoch 770, training loss: 847.4711303710938 = 0.6485838890075684 + 100.0 * 8.468225479125977
Epoch 770, val loss: 0.682984471321106
Epoch 780, training loss: 847.1843872070312 = 0.64388108253479 + 100.0 * 8.465405464172363
Epoch 780, val loss: 0.6790286898612976
Epoch 790, training loss: 847.0390625 = 0.6394068002700806 + 100.0 * 8.463996887207031
Epoch 790, val loss: 0.6751695275306702
Epoch 800, training loss: 846.8992309570312 = 0.6351408362388611 + 100.0 * 8.462640762329102
Epoch 800, val loss: 0.6715667843818665
Epoch 810, training loss: 846.7767333984375 = 0.6309992074966431 + 100.0 * 8.461457252502441
Epoch 810, val loss: 0.6680882573127747
Epoch 820, training loss: 846.6744995117188 = 0.6270524859428406 + 100.0 * 8.460474014282227
Epoch 820, val loss: 0.6648076176643372
Epoch 830, training loss: 846.7411499023438 = 0.6232738494873047 + 100.0 * 8.46117877960205
Epoch 830, val loss: 0.6617395281791687
Epoch 840, training loss: 846.7100830078125 = 0.6195940971374512 + 100.0 * 8.460905075073242
Epoch 840, val loss: 0.6586170196533203
Epoch 850, training loss: 846.3916015625 = 0.6160977482795715 + 100.0 * 8.457755088806152
Epoch 850, val loss: 0.6558303236961365
Epoch 860, training loss: 846.31982421875 = 0.6127907037734985 + 100.0 * 8.457070350646973
Epoch 860, val loss: 0.6532089114189148
Epoch 870, training loss: 846.2097778320312 = 0.6096593737602234 + 100.0 * 8.456001281738281
Epoch 870, val loss: 0.6507209539413452
Epoch 880, training loss: 846.1258544921875 = 0.6066787838935852 + 100.0 * 8.455191612243652
Epoch 880, val loss: 0.6484308242797852
Epoch 890, training loss: 846.1146850585938 = 0.6038148999214172 + 100.0 * 8.455108642578125
Epoch 890, val loss: 0.646239161491394
Epoch 900, training loss: 845.9931640625 = 0.6010186672210693 + 100.0 * 8.4539213180542
Epoch 900, val loss: 0.6441029906272888
Epoch 910, training loss: 846.3424682617188 = 0.598339319229126 + 100.0 * 8.457441329956055
Epoch 910, val loss: 0.6420742273330688
Epoch 920, training loss: 845.7785034179688 = 0.5957986116409302 + 100.0 * 8.451827049255371
Epoch 920, val loss: 0.6402731537818909
Epoch 930, training loss: 845.704833984375 = 0.5934174060821533 + 100.0 * 8.4511137008667
Epoch 930, val loss: 0.6386364698410034
Epoch 940, training loss: 845.6048583984375 = 0.5911280512809753 + 100.0 * 8.4501371383667
Epoch 940, val loss: 0.6369479894638062
Epoch 950, training loss: 845.4850463867188 = 0.588964581489563 + 100.0 * 8.448960304260254
Epoch 950, val loss: 0.6355300545692444
Epoch 960, training loss: 845.3767700195312 = 0.5868724584579468 + 100.0 * 8.447898864746094
Epoch 960, val loss: 0.6340993642807007
Epoch 970, training loss: 845.2770385742188 = 0.5848727226257324 + 100.0 * 8.446921348571777
Epoch 970, val loss: 0.6327760815620422
Epoch 980, training loss: 845.3275146484375 = 0.5829452872276306 + 100.0 * 8.4474458694458
Epoch 980, val loss: 0.6314615607261658
Epoch 990, training loss: 845.322265625 = 0.5809929966926575 + 100.0 * 8.447412490844727
Epoch 990, val loss: 0.6303848028182983
Epoch 1000, training loss: 845.1502075195312 = 0.5791696310043335 + 100.0 * 8.445710182189941
Epoch 1000, val loss: 0.6291429996490479
Epoch 1010, training loss: 844.8861083984375 = 0.5774617791175842 + 100.0 * 8.443086624145508
Epoch 1010, val loss: 0.6281335353851318
Epoch 1020, training loss: 844.7810668945312 = 0.5758532285690308 + 100.0 * 8.442051887512207
Epoch 1020, val loss: 0.6272010207176208
Epoch 1030, training loss: 844.665771484375 = 0.5742843747138977 + 100.0 * 8.44091510772705
Epoch 1030, val loss: 0.6263160705566406
Epoch 1040, training loss: 844.5665283203125 = 0.5727661848068237 + 100.0 * 8.439937591552734
Epoch 1040, val loss: 0.6254643797874451
Epoch 1050, training loss: 845.3939819335938 = 0.5712795257568359 + 100.0 * 8.448226928710938
Epoch 1050, val loss: 0.6246777176856995
Epoch 1060, training loss: 844.4364624023438 = 0.5697757005691528 + 100.0 * 8.438667297363281
Epoch 1060, val loss: 0.6238842010498047
Epoch 1070, training loss: 844.3239135742188 = 0.5683547258377075 + 100.0 * 8.437555313110352
Epoch 1070, val loss: 0.6231186389923096
Epoch 1080, training loss: 844.2061767578125 = 0.5670078992843628 + 100.0 * 8.436391830444336
Epoch 1080, val loss: 0.6224310398101807
Epoch 1090, training loss: 844.1017456054688 = 0.5657007098197937 + 100.0 * 8.4353609085083
Epoch 1090, val loss: 0.6218128800392151
Epoch 1100, training loss: 844.0311279296875 = 0.5644215941429138 + 100.0 * 8.434667587280273
Epoch 1100, val loss: 0.6212026476860046
Epoch 1110, training loss: 844.717529296875 = 0.5631689429283142 + 100.0 * 8.441543579101562
Epoch 1110, val loss: 0.6206716299057007
Epoch 1120, training loss: 843.9398803710938 = 0.5618183016777039 + 100.0 * 8.433780670166016
Epoch 1120, val loss: 0.6198554635047913
Epoch 1130, training loss: 843.8035278320312 = 0.5605958104133606 + 100.0 * 8.432429313659668
Epoch 1130, val loss: 0.6193318367004395
Epoch 1140, training loss: 843.7056884765625 = 0.5594362020492554 + 100.0 * 8.431462287902832
Epoch 1140, val loss: 0.6188057065010071
Epoch 1150, training loss: 843.6016235351562 = 0.5582729578018188 + 100.0 * 8.43043327331543
Epoch 1150, val loss: 0.6183119416236877
Epoch 1160, training loss: 843.535888671875 = 0.5571528077125549 + 100.0 * 8.429787635803223
Epoch 1160, val loss: 0.6178362965583801
Epoch 1170, training loss: 843.8577880859375 = 0.5560301542282104 + 100.0 * 8.43301773071289
Epoch 1170, val loss: 0.6172820329666138
Epoch 1180, training loss: 843.7140502929688 = 0.55489182472229 + 100.0 * 8.431591987609863
Epoch 1180, val loss: 0.6168375015258789
Epoch 1190, training loss: 843.3804931640625 = 0.5537963509559631 + 100.0 * 8.428267478942871
Epoch 1190, val loss: 0.6164460778236389
Epoch 1200, training loss: 843.268310546875 = 0.5527486801147461 + 100.0 * 8.427155494689941
Epoch 1200, val loss: 0.6159940958023071
Epoch 1210, training loss: 843.2091674804688 = 0.5517251491546631 + 100.0 * 8.42657470703125
Epoch 1210, val loss: 0.6155597567558289
Epoch 1220, training loss: 843.4052124023438 = 0.5507081151008606 + 100.0 * 8.428544998168945
Epoch 1220, val loss: 0.6150632500648499
Epoch 1230, training loss: 843.0682983398438 = 0.5496752262115479 + 100.0 * 8.425186157226562
Epoch 1230, val loss: 0.614787220954895
Epoch 1240, training loss: 843.0201416015625 = 0.5486835241317749 + 100.0 * 8.424714088439941
Epoch 1240, val loss: 0.6143574118614197
Epoch 1250, training loss: 842.9742431640625 = 0.5477049350738525 + 100.0 * 8.424264907836914
Epoch 1250, val loss: 0.6139554977416992
Epoch 1260, training loss: 842.9127807617188 = 0.5467527508735657 + 100.0 * 8.423660278320312
Epoch 1260, val loss: 0.6136454939842224
Epoch 1270, training loss: 842.947265625 = 0.5458037853240967 + 100.0 * 8.424015045166016
Epoch 1270, val loss: 0.6132779121398926
Epoch 1280, training loss: 843.1527709960938 = 0.5448533296585083 + 100.0 * 8.426078796386719
Epoch 1280, val loss: 0.6127675175666809
Epoch 1290, training loss: 842.7605590820312 = 0.543910026550293 + 100.0 * 8.42216682434082
Epoch 1290, val loss: 0.6125717163085938
Epoch 1300, training loss: 842.6787719726562 = 0.5429830551147461 + 100.0 * 8.421358108520508
Epoch 1300, val loss: 0.6121630072593689
Epoch 1310, training loss: 842.6405029296875 = 0.5420678853988647 + 100.0 * 8.420984268188477
Epoch 1310, val loss: 0.6118375658988953
Epoch 1320, training loss: 842.56884765625 = 0.5411832928657532 + 100.0 * 8.420276641845703
Epoch 1320, val loss: 0.6115255951881409
Epoch 1330, training loss: 842.6144409179688 = 0.5403202176094055 + 100.0 * 8.420741081237793
Epoch 1330, val loss: 0.611301064491272
Epoch 1340, training loss: 842.7019653320312 = 0.539402186870575 + 100.0 * 8.421625137329102
Epoch 1340, val loss: 0.6108607053756714
Epoch 1350, training loss: 842.5833129882812 = 0.5384917855262756 + 100.0 * 8.420448303222656
Epoch 1350, val loss: 0.6105216145515442
Epoch 1360, training loss: 842.4013061523438 = 0.5376035571098328 + 100.0 * 8.4186372756958
Epoch 1360, val loss: 0.6101542711257935
Epoch 1370, training loss: 842.342041015625 = 0.5367480516433716 + 100.0 * 8.418052673339844
Epoch 1370, val loss: 0.6098592877388
Epoch 1380, training loss: 842.3923950195312 = 0.5358911156654358 + 100.0 * 8.418564796447754
Epoch 1380, val loss: 0.6094741225242615
Epoch 1390, training loss: 842.4983520507812 = 0.5350193381309509 + 100.0 * 8.419632911682129
Epoch 1390, val loss: 0.6091374754905701
Epoch 1400, training loss: 842.2420043945312 = 0.5341585874557495 + 100.0 * 8.417078018188477
Epoch 1400, val loss: 0.6089004278182983
Epoch 1410, training loss: 842.1483154296875 = 0.5333115458488464 + 100.0 * 8.416150093078613
Epoch 1410, val loss: 0.6085405945777893
Epoch 1420, training loss: 842.1094360351562 = 0.5324870944023132 + 100.0 * 8.415769577026367
Epoch 1420, val loss: 0.6082341074943542
Epoch 1430, training loss: 842.0750732421875 = 0.5316711068153381 + 100.0 * 8.415433883666992
Epoch 1430, val loss: 0.6079153418540955
Epoch 1440, training loss: 842.3091430664062 = 0.5308565497398376 + 100.0 * 8.4177827835083
Epoch 1440, val loss: 0.6074979305267334
Epoch 1450, training loss: 842.0596923828125 = 0.5300021171569824 + 100.0 * 8.41529655456543
Epoch 1450, val loss: 0.6073057651519775
Epoch 1460, training loss: 842.0126953125 = 0.5291870832443237 + 100.0 * 8.414834976196289
Epoch 1460, val loss: 0.6069583892822266
Epoch 1470, training loss: 841.9244995117188 = 0.5283748507499695 + 100.0 * 8.413961410522461
Epoch 1470, val loss: 0.6066974401473999
Epoch 1480, training loss: 841.8726196289062 = 0.5275818705558777 + 100.0 * 8.413450241088867
Epoch 1480, val loss: 0.6063094139099121
Epoch 1490, training loss: 841.8528442382812 = 0.5267964601516724 + 100.0 * 8.413260459899902
Epoch 1490, val loss: 0.6060194373130798
Epoch 1500, training loss: 842.3115234375 = 0.5259982347488403 + 100.0 * 8.417855262756348
Epoch 1500, val loss: 0.6055371761322021
Epoch 1510, training loss: 841.8702392578125 = 0.525189995765686 + 100.0 * 8.413450241088867
Epoch 1510, val loss: 0.6054781079292297
Epoch 1520, training loss: 841.707763671875 = 0.5243882536888123 + 100.0 * 8.411833763122559
Epoch 1520, val loss: 0.6050509214401245
Epoch 1530, training loss: 841.6847534179688 = 0.5236124396324158 + 100.0 * 8.411611557006836
Epoch 1530, val loss: 0.6047455072402954
Epoch 1540, training loss: 842.0094604492188 = 0.5228301286697388 + 100.0 * 8.41486644744873
Epoch 1540, val loss: 0.6044046878814697
Epoch 1550, training loss: 841.685546875 = 0.5220443606376648 + 100.0 * 8.411635398864746
Epoch 1550, val loss: 0.6040890216827393
Epoch 1560, training loss: 841.617919921875 = 0.5212479829788208 + 100.0 * 8.410966873168945
Epoch 1560, val loss: 0.6038088798522949
Epoch 1570, training loss: 841.54150390625 = 0.5204700827598572 + 100.0 * 8.410210609436035
Epoch 1570, val loss: 0.6034192442893982
Epoch 1580, training loss: 841.5131225585938 = 0.519692063331604 + 100.0 * 8.409934043884277
Epoch 1580, val loss: 0.6030874848365784
Epoch 1590, training loss: 841.6181640625 = 0.5189211964607239 + 100.0 * 8.410992622375488
Epoch 1590, val loss: 0.6027631759643555
Epoch 1600, training loss: 841.4992065429688 = 0.5181127190589905 + 100.0 * 8.409811019897461
Epoch 1600, val loss: 0.6025163531303406
Epoch 1610, training loss: 841.6077270507812 = 0.5173377990722656 + 100.0 * 8.410903930664062
Epoch 1610, val loss: 0.6021721959114075
Epoch 1620, training loss: 841.4108276367188 = 0.5165238380432129 + 100.0 * 8.408943176269531
Epoch 1620, val loss: 0.6017799973487854
Epoch 1630, training loss: 841.3724975585938 = 0.5157269835472107 + 100.0 * 8.408567428588867
Epoch 1630, val loss: 0.6013027429580688
Epoch 1640, training loss: 841.3165893554688 = 0.5149616003036499 + 100.0 * 8.408016204833984
Epoch 1640, val loss: 0.6010409593582153
Epoch 1650, training loss: 841.2655029296875 = 0.5141910314559937 + 100.0 * 8.407512664794922
Epoch 1650, val loss: 0.6006614565849304
Epoch 1660, training loss: 841.3988037109375 = 0.5134185552597046 + 100.0 * 8.408853530883789
Epoch 1660, val loss: 0.6002843976020813
Epoch 1670, training loss: 841.3237915039062 = 0.5126227736473083 + 100.0 * 8.408111572265625
Epoch 1670, val loss: 0.599976122379303
Epoch 1680, training loss: 841.25048828125 = 0.5118429064750671 + 100.0 * 8.407386779785156
Epoch 1680, val loss: 0.5997310280799866
Epoch 1690, training loss: 841.1668701171875 = 0.511070191860199 + 100.0 * 8.4065580368042
Epoch 1690, val loss: 0.599297285079956
Epoch 1700, training loss: 841.1700439453125 = 0.5103213787078857 + 100.0 * 8.406597137451172
Epoch 1700, val loss: 0.59897381067276
Epoch 1710, training loss: 841.13623046875 = 0.5095420479774475 + 100.0 * 8.406267166137695
Epoch 1710, val loss: 0.5986427068710327
Epoch 1720, training loss: 841.0243530273438 = 0.50873863697052 + 100.0 * 8.405156135559082
Epoch 1720, val loss: 0.5982746481895447
Epoch 1730, training loss: 840.9920654296875 = 0.5079630017280579 + 100.0 * 8.404841423034668
Epoch 1730, val loss: 0.5979183912277222
Epoch 1740, training loss: 840.9594116210938 = 0.5071913003921509 + 100.0 * 8.404521942138672
Epoch 1740, val loss: 0.5975470542907715
Epoch 1750, training loss: 841.2077026367188 = 0.5064125657081604 + 100.0 * 8.407012939453125
Epoch 1750, val loss: 0.5972012281417847
Epoch 1760, training loss: 841.3768920898438 = 0.5055766105651855 + 100.0 * 8.408713340759277
Epoch 1760, val loss: 0.596575915813446
Epoch 1770, training loss: 840.885986328125 = 0.5047456622123718 + 100.0 * 8.403812408447266
Epoch 1770, val loss: 0.5963207483291626
Epoch 1780, training loss: 840.9085083007812 = 0.5039469003677368 + 100.0 * 8.404045104980469
Epoch 1780, val loss: 0.5959988832473755
Epoch 1790, training loss: 840.82421875 = 0.5031622052192688 + 100.0 * 8.403210639953613
Epoch 1790, val loss: 0.595580518245697
Epoch 1800, training loss: 840.7887573242188 = 0.5023679733276367 + 100.0 * 8.402863502502441
Epoch 1800, val loss: 0.5951656699180603
Epoch 1810, training loss: 840.8768310546875 = 0.5015720725059509 + 100.0 * 8.403752326965332
Epoch 1810, val loss: 0.594680905342102
Epoch 1820, training loss: 841.13232421875 = 0.5007331371307373 + 100.0 * 8.406315803527832
Epoch 1820, val loss: 0.5942251086235046
Epoch 1830, training loss: 840.8125610351562 = 0.4998921751976013 + 100.0 * 8.40312671661377
Epoch 1830, val loss: 0.593972384929657
Epoch 1840, training loss: 840.705322265625 = 0.4990622103214264 + 100.0 * 8.40206241607666
Epoch 1840, val loss: 0.5934886932373047
Epoch 1850, training loss: 840.6633911132812 = 0.49825960397720337 + 100.0 * 8.401651382446289
Epoch 1850, val loss: 0.5931313633918762
Epoch 1860, training loss: 840.6397094726562 = 0.49745070934295654 + 100.0 * 8.401422500610352
Epoch 1860, val loss: 0.592701256275177
Epoch 1870, training loss: 840.60400390625 = 0.49663758277893066 + 100.0 * 8.401073455810547
Epoch 1870, val loss: 0.5923397541046143
Epoch 1880, training loss: 840.6351928710938 = 0.4958151876926422 + 100.0 * 8.40139389038086
Epoch 1880, val loss: 0.5919573307037354
Epoch 1890, training loss: 841.0543823242188 = 0.49497178196907043 + 100.0 * 8.405593872070312
Epoch 1890, val loss: 0.5915318131446838
Epoch 1900, training loss: 840.6463623046875 = 0.4940792918205261 + 100.0 * 8.401522636413574
Epoch 1900, val loss: 0.59100741147995
Epoch 1910, training loss: 840.5181884765625 = 0.49321508407592773 + 100.0 * 8.400249481201172
Epoch 1910, val loss: 0.5905761122703552
Epoch 1920, training loss: 840.5234985351562 = 0.4923633933067322 + 100.0 * 8.400311470031738
Epoch 1920, val loss: 0.5901448726654053
Epoch 1930, training loss: 840.947998046875 = 0.49150070548057556 + 100.0 * 8.40456485748291
Epoch 1930, val loss: 0.589555025100708
Epoch 1940, training loss: 840.4912109375 = 0.4905931353569031 + 100.0 * 8.400006294250488
Epoch 1940, val loss: 0.5894331336021423
Epoch 1950, training loss: 840.4129638671875 = 0.4897116422653198 + 100.0 * 8.399232864379883
Epoch 1950, val loss: 0.5888457298278809
Epoch 1960, training loss: 840.3907470703125 = 0.4888347089290619 + 100.0 * 8.399019241333008
Epoch 1960, val loss: 0.5884768962860107
Epoch 1970, training loss: 840.3989868164062 = 0.4879629611968994 + 100.0 * 8.399109840393066
Epoch 1970, val loss: 0.588093101978302
Epoch 1980, training loss: 840.7416381835938 = 0.48709726333618164 + 100.0 * 8.402544975280762
Epoch 1980, val loss: 0.5876971483230591
Epoch 1990, training loss: 840.4129638671875 = 0.4861319661140442 + 100.0 * 8.39926815032959
Epoch 1990, val loss: 0.5871580839157104
Epoch 2000, training loss: 840.3568115234375 = 0.48523494601249695 + 100.0 * 8.39871597290039
Epoch 2000, val loss: 0.586642324924469
Epoch 2010, training loss: 840.28515625 = 0.48431387543678284 + 100.0 * 8.398008346557617
Epoch 2010, val loss: 0.5861880779266357
Epoch 2020, training loss: 840.2930297851562 = 0.4834176301956177 + 100.0 * 8.398096084594727
Epoch 2020, val loss: 0.5857158303260803
Epoch 2030, training loss: 840.660888671875 = 0.4824865758419037 + 100.0 * 8.40178394317627
Epoch 2030, val loss: 0.5851019024848938
Epoch 2040, training loss: 840.3369140625 = 0.4815271496772766 + 100.0 * 8.398553848266602
Epoch 2040, val loss: 0.5847802758216858
Epoch 2050, training loss: 840.25146484375 = 0.48057854175567627 + 100.0 * 8.397708892822266
Epoch 2050, val loss: 0.5843005776405334
Epoch 2060, training loss: 840.1499633789062 = 0.47963976860046387 + 100.0 * 8.396703720092773
Epoch 2060, val loss: 0.583828330039978
Epoch 2070, training loss: 840.1165771484375 = 0.4787062108516693 + 100.0 * 8.396378517150879
Epoch 2070, val loss: 0.5833272933959961
Epoch 2080, training loss: 840.1847534179688 = 0.47776302695274353 + 100.0 * 8.397069931030273
Epoch 2080, val loss: 0.5827421545982361
Epoch 2090, training loss: 840.6023559570312 = 0.4767906069755554 + 100.0 * 8.40125560760498
Epoch 2090, val loss: 0.5821898579597473
Epoch 2100, training loss: 840.22998046875 = 0.4757925868034363 + 100.0 * 8.397541999816895
Epoch 2100, val loss: 0.5819376111030579
Epoch 2110, training loss: 840.0791015625 = 0.47480130195617676 + 100.0 * 8.396042823791504
Epoch 2110, val loss: 0.5811814665794373
Epoch 2120, training loss: 839.9901733398438 = 0.473832905292511 + 100.0 * 8.395163536071777
Epoch 2120, val loss: 0.5808278918266296
Epoch 2130, training loss: 839.9973754882812 = 0.47285592555999756 + 100.0 * 8.395245552062988
Epoch 2130, val loss: 0.5803238153457642
Epoch 2140, training loss: 840.4905395507812 = 0.4718640148639679 + 100.0 * 8.400186538696289
Epoch 2140, val loss: 0.5798051953315735
Epoch 2150, training loss: 840.013671875 = 0.4708139896392822 + 100.0 * 8.395428657531738
Epoch 2150, val loss: 0.5791450142860413
Epoch 2160, training loss: 839.8944091796875 = 0.469791978597641 + 100.0 * 8.394246101379395
Epoch 2160, val loss: 0.5786266922950745
Epoch 2170, training loss: 839.9692993164062 = 0.4687681198120117 + 100.0 * 8.395005226135254
Epoch 2170, val loss: 0.5782052874565125
Epoch 2180, training loss: 840.2352905273438 = 0.46772581338882446 + 100.0 * 8.397675514221191
Epoch 2180, val loss: 0.5777108669281006
Epoch 2190, training loss: 839.8956298828125 = 0.46663573384284973 + 100.0 * 8.39428997039795
Epoch 2190, val loss: 0.576962411403656
Epoch 2200, training loss: 839.8469848632812 = 0.4655783176422119 + 100.0 * 8.393814086914062
Epoch 2200, val loss: 0.5763225555419922
Epoch 2210, training loss: 839.768798828125 = 0.46452435851097107 + 100.0 * 8.39304256439209
Epoch 2210, val loss: 0.5758525729179382
Epoch 2220, training loss: 839.8107299804688 = 0.463467538356781 + 100.0 * 8.393472671508789
Epoch 2220, val loss: 0.5752426981925964
Epoch 2230, training loss: 839.9718017578125 = 0.46238553524017334 + 100.0 * 8.39509391784668
Epoch 2230, val loss: 0.5746201872825623
Epoch 2240, training loss: 839.9671020507812 = 0.46125754714012146 + 100.0 * 8.395058631896973
Epoch 2240, val loss: 0.5741470456123352
Epoch 2250, training loss: 839.6694946289062 = 0.4601611793041229 + 100.0 * 8.392093658447266
Epoch 2250, val loss: 0.5736931562423706
Epoch 2260, training loss: 839.6456909179688 = 0.45906153321266174 + 100.0 * 8.391866683959961
Epoch 2260, val loss: 0.5731834769248962
Epoch 2270, training loss: 839.63623046875 = 0.4579654932022095 + 100.0 * 8.391782760620117
Epoch 2270, val loss: 0.572639524936676
Epoch 2280, training loss: 839.7888793945312 = 0.4568531811237335 + 100.0 * 8.393320083618164
Epoch 2280, val loss: 0.5722135901451111
Epoch 2290, training loss: 839.6231079101562 = 0.45570075511932373 + 100.0 * 8.391674041748047
Epoch 2290, val loss: 0.5715359449386597
Epoch 2300, training loss: 839.58056640625 = 0.45454108715057373 + 100.0 * 8.391260147094727
Epoch 2300, val loss: 0.5708425045013428
Epoch 2310, training loss: 839.4920654296875 = 0.4534035623073578 + 100.0 * 8.390386581420898
Epoch 2310, val loss: 0.5703578591346741
Epoch 2320, training loss: 839.539306640625 = 0.4522640109062195 + 100.0 * 8.390870094299316
Epoch 2320, val loss: 0.5698957443237305
Epoch 2330, training loss: 839.5619506835938 = 0.45109304785728455 + 100.0 * 8.391108512878418
Epoch 2330, val loss: 0.5692852735519409
Epoch 2340, training loss: 839.6685791015625 = 0.44989898800849915 + 100.0 * 8.392187118530273
Epoch 2340, val loss: 0.568641722202301
Epoch 2350, training loss: 839.3732299804688 = 0.44870632886886597 + 100.0 * 8.38924503326416
Epoch 2350, val loss: 0.5681973099708557
Epoch 2360, training loss: 839.3746948242188 = 0.44752219319343567 + 100.0 * 8.38927173614502
Epoch 2360, val loss: 0.5677506923675537
Epoch 2370, training loss: 839.369140625 = 0.44632792472839355 + 100.0 * 8.389227867126465
Epoch 2370, val loss: 0.5671113729476929
Epoch 2380, training loss: 840.0924072265625 = 0.44512826204299927 + 100.0 * 8.396472930908203
Epoch 2380, val loss: 0.5664003491401672
Epoch 2390, training loss: 839.5162353515625 = 0.443848580121994 + 100.0 * 8.390724182128906
Epoch 2390, val loss: 0.566156268119812
Epoch 2400, training loss: 839.3152465820312 = 0.4426071345806122 + 100.0 * 8.388726234436035
Epoch 2400, val loss: 0.5654288530349731
Epoch 2410, training loss: 839.2221069335938 = 0.44138452410697937 + 100.0 * 8.38780689239502
Epoch 2410, val loss: 0.5649943947792053
Epoch 2420, training loss: 839.1841430664062 = 0.44016239047050476 + 100.0 * 8.387439727783203
Epoch 2420, val loss: 0.5644164085388184
Epoch 2430, training loss: 839.2194213867188 = 0.4389316141605377 + 100.0 * 8.387804985046387
Epoch 2430, val loss: 0.5638914704322815
Epoch 2440, training loss: 839.6712036132812 = 0.43768760561943054 + 100.0 * 8.392334938049316
Epoch 2440, val loss: 0.5632092952728271
Epoch 2450, training loss: 839.2796630859375 = 0.4363762140274048 + 100.0 * 8.388432502746582
Epoch 2450, val loss: 0.5631020069122314
Epoch 2460, training loss: 839.1453247070312 = 0.4351152777671814 + 100.0 * 8.387102127075195
Epoch 2460, val loss: 0.5623881220817566
Epoch 2470, training loss: 839.074462890625 = 0.43384650349617004 + 100.0 * 8.386405944824219
Epoch 2470, val loss: 0.5620015263557434
Epoch 2480, training loss: 839.0677490234375 = 0.4325868487358093 + 100.0 * 8.386351585388184
Epoch 2480, val loss: 0.5614793300628662
Epoch 2490, training loss: 839.2687377929688 = 0.4313124120235443 + 100.0 * 8.388374328613281
Epoch 2490, val loss: 0.5610515475273132
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7813292744799594
0.8164167210026807
=== training gcn model ===
Epoch 0, training loss: 1059.3336181640625 = 1.1027977466583252 + 100.0 * 10.582308769226074
Epoch 0, val loss: 1.1015968322753906
Epoch 10, training loss: 1059.3040771484375 = 1.0984134674072266 + 100.0 * 10.582056999206543
Epoch 10, val loss: 1.0972387790679932
Epoch 20, training loss: 1059.1793212890625 = 1.0936555862426758 + 100.0 * 10.580856323242188
Epoch 20, val loss: 1.092513084411621
Epoch 30, training loss: 1058.585205078125 = 1.0884053707122803 + 100.0 * 10.574968338012695
Epoch 30, val loss: 1.0872876644134521
Epoch 40, training loss: 1055.961669921875 = 1.082478642463684 + 100.0 * 10.54879093170166
Epoch 40, val loss: 1.0813746452331543
Epoch 50, training loss: 1046.6026611328125 = 1.0758991241455078 + 100.0 * 10.455266952514648
Epoch 50, val loss: 1.0748636722564697
Epoch 60, training loss: 1016.7723999023438 = 1.069483995437622 + 100.0 * 10.157029151916504
Epoch 60, val loss: 1.0685604810714722
Epoch 70, training loss: 973.4199829101562 = 1.0632730722427368 + 100.0 * 9.723567008972168
Epoch 70, val loss: 1.0625511407852173
Epoch 80, training loss: 960.39111328125 = 1.0591423511505127 + 100.0 * 9.5933198928833
Epoch 80, val loss: 1.058606505393982
Epoch 90, training loss: 947.4507446289062 = 1.0555987358093262 + 100.0 * 9.463951110839844
Epoch 90, val loss: 1.0552102327346802
Epoch 100, training loss: 931.5353393554688 = 1.0527092218399048 + 100.0 * 9.304825782775879
Epoch 100, val loss: 1.0524742603302002
Epoch 110, training loss: 920.8002319335938 = 1.0503973960876465 + 100.0 * 9.197498321533203
Epoch 110, val loss: 1.050229787826538
Epoch 120, training loss: 915.0806274414062 = 1.0479012727737427 + 100.0 * 9.140327453613281
Epoch 120, val loss: 1.0477595329284668
Epoch 130, training loss: 907.434326171875 = 1.0452609062194824 + 100.0 * 9.06389045715332
Epoch 130, val loss: 1.0451611280441284
Epoch 140, training loss: 900.6168212890625 = 1.0426740646362305 + 100.0 * 8.995741844177246
Epoch 140, val loss: 1.0426234006881714
Epoch 150, training loss: 895.7330322265625 = 1.0401482582092285 + 100.0 * 8.946928977966309
Epoch 150, val loss: 1.0401359796524048
Epoch 160, training loss: 889.9619750976562 = 1.037763237953186 + 100.0 * 8.889242172241211
Epoch 160, val loss: 1.037805438041687
Epoch 170, training loss: 884.5756225585938 = 1.0356026887893677 + 100.0 * 8.835400581359863
Epoch 170, val loss: 1.035715937614441
Epoch 180, training loss: 881.1071166992188 = 1.0334736108779907 + 100.0 * 8.800736427307129
Epoch 180, val loss: 1.033639669418335
Epoch 190, training loss: 879.376220703125 = 1.0309165716171265 + 100.0 * 8.783452987670898
Epoch 190, val loss: 1.0311511754989624
Epoch 200, training loss: 877.5150756835938 = 1.0278799533843994 + 100.0 * 8.764871597290039
Epoch 200, val loss: 1.0281931161880493
Epoch 210, training loss: 875.9628295898438 = 1.0245842933654785 + 100.0 * 8.749382019042969
Epoch 210, val loss: 1.0250046253204346
Epoch 220, training loss: 874.4210205078125 = 1.0211843252182007 + 100.0 * 8.73399829864502
Epoch 220, val loss: 1.021728277206421
Epoch 230, training loss: 872.8507080078125 = 1.0177080631256104 + 100.0 * 8.718330383300781
Epoch 230, val loss: 1.0183871984481812
Epoch 240, training loss: 871.3517456054688 = 1.0141069889068604 + 100.0 * 8.703376770019531
Epoch 240, val loss: 1.01491379737854
Epoch 250, training loss: 870.5134887695312 = 1.010258436203003 + 100.0 * 8.695032119750977
Epoch 250, val loss: 1.0112135410308838
Epoch 260, training loss: 868.9165649414062 = 1.0060871839523315 + 100.0 * 8.679104804992676
Epoch 260, val loss: 1.0071772336959839
Epoch 270, training loss: 867.8521118164062 = 1.001632571220398 + 100.0 * 8.66850471496582
Epoch 270, val loss: 1.0028923749923706
Epoch 280, training loss: 866.6368408203125 = 0.9968960285186768 + 100.0 * 8.656399726867676
Epoch 280, val loss: 0.9983465671539307
Epoch 290, training loss: 865.4501342773438 = 0.9918767213821411 + 100.0 * 8.644582748413086
Epoch 290, val loss: 0.9935308694839478
Epoch 300, training loss: 864.3719482421875 = 0.9865690469741821 + 100.0 * 8.633853912353516
Epoch 300, val loss: 0.98844975233078
Epoch 310, training loss: 863.650634765625 = 0.980893075466156 + 100.0 * 8.626697540283203
Epoch 310, val loss: 0.9829765558242798
Epoch 320, training loss: 862.307373046875 = 0.9748287200927734 + 100.0 * 8.613325119018555
Epoch 320, val loss: 0.9771844744682312
Epoch 330, training loss: 861.4805908203125 = 0.9684470891952515 + 100.0 * 8.605121612548828
Epoch 330, val loss: 0.9710934162139893
Epoch 340, training loss: 860.6548461914062 = 0.9617353081703186 + 100.0 * 8.596931457519531
Epoch 340, val loss: 0.9646762609481812
Epoch 350, training loss: 859.8938598632812 = 0.9547041058540344 + 100.0 * 8.589391708374023
Epoch 350, val loss: 0.9579724669456482
Epoch 360, training loss: 859.6453247070312 = 0.9473453164100647 + 100.0 * 8.586979866027832
Epoch 360, val loss: 0.9509822130203247
Epoch 370, training loss: 858.7694702148438 = 0.939654529094696 + 100.0 * 8.578298568725586
Epoch 370, val loss: 0.9436520934104919
Epoch 380, training loss: 858.1612548828125 = 0.9317354559898376 + 100.0 * 8.572295188903809
Epoch 380, val loss: 0.9361294507980347
Epoch 390, training loss: 857.6433715820312 = 0.9236085414886475 + 100.0 * 8.567197799682617
Epoch 390, val loss: 0.9284193515777588
Epoch 400, training loss: 857.1866455078125 = 0.9153041243553162 + 100.0 * 8.562713623046875
Epoch 400, val loss: 0.9205479621887207
Epoch 410, training loss: 856.7800903320312 = 0.9068247079849243 + 100.0 * 8.558732986450195
Epoch 410, val loss: 0.9125257134437561
Epoch 420, training loss: 856.7550659179688 = 0.8981948494911194 + 100.0 * 8.558568954467773
Epoch 420, val loss: 0.904367983341217
Epoch 430, training loss: 856.1782836914062 = 0.889407217502594 + 100.0 * 8.552888870239258
Epoch 430, val loss: 0.89608234167099
Epoch 440, training loss: 855.7495727539062 = 0.8806004524230957 + 100.0 * 8.548689842224121
Epoch 440, val loss: 0.887809693813324
Epoch 450, training loss: 855.3388671875 = 0.871805727481842 + 100.0 * 8.544670104980469
Epoch 450, val loss: 0.879568874835968
Epoch 460, training loss: 855.4964599609375 = 0.8630087375640869 + 100.0 * 8.546334266662598
Epoch 460, val loss: 0.8713663220405579
Epoch 470, training loss: 854.5979614257812 = 0.8541960716247559 + 100.0 * 8.537437438964844
Epoch 470, val loss: 0.8630881309509277
Epoch 480, training loss: 854.2149658203125 = 0.845447838306427 + 100.0 * 8.533695220947266
Epoch 480, val loss: 0.8549261689186096
Epoch 490, training loss: 853.8561401367188 = 0.8367713689804077 + 100.0 * 8.530193328857422
Epoch 490, val loss: 0.8468348979949951
Epoch 500, training loss: 853.9522705078125 = 0.8281159996986389 + 100.0 * 8.531241416931152
Epoch 500, val loss: 0.8387673497200012
Epoch 510, training loss: 853.17529296875 = 0.8194584250450134 + 100.0 * 8.523558616638184
Epoch 510, val loss: 0.8307326436042786
Epoch 520, training loss: 852.7321166992188 = 0.8108688592910767 + 100.0 * 8.51921272277832
Epoch 520, val loss: 0.8227654099464417
Epoch 530, training loss: 852.3660278320312 = 0.8023375272750854 + 100.0 * 8.515637397766113
Epoch 530, val loss: 0.8148192167282104
Epoch 540, training loss: 852.016845703125 = 0.7938404083251953 + 100.0 * 8.512229919433594
Epoch 540, val loss: 0.806944727897644
Epoch 550, training loss: 851.9774780273438 = 0.7853863835334778 + 100.0 * 8.511920928955078
Epoch 550, val loss: 0.7990686893463135
Epoch 560, training loss: 851.6400756835938 = 0.7768928408622742 + 100.0 * 8.508631706237793
Epoch 560, val loss: 0.7912401556968689
Epoch 570, training loss: 851.1250610351562 = 0.768505871295929 + 100.0 * 8.503565788269043
Epoch 570, val loss: 0.783489465713501
Epoch 580, training loss: 850.830322265625 = 0.7602617144584656 + 100.0 * 8.500700950622559
Epoch 580, val loss: 0.7758936285972595
Epoch 590, training loss: 850.7614135742188 = 0.7521183490753174 + 100.0 * 8.500092506408691
Epoch 590, val loss: 0.7683766484260559
Epoch 600, training loss: 850.517822265625 = 0.7440394759178162 + 100.0 * 8.497737884521484
Epoch 600, val loss: 0.7609499096870422
Epoch 610, training loss: 850.08740234375 = 0.7360838055610657 + 100.0 * 8.493513107299805
Epoch 610, val loss: 0.7536742091178894
Epoch 620, training loss: 849.8027954101562 = 0.728308916091919 + 100.0 * 8.490744590759277
Epoch 620, val loss: 0.7465537786483765
Epoch 630, training loss: 849.7543334960938 = 0.7206836342811584 + 100.0 * 8.490336418151855
Epoch 630, val loss: 0.7395601868629456
Epoch 640, training loss: 849.3465576171875 = 0.7132115960121155 + 100.0 * 8.486333847045898
Epoch 640, val loss: 0.7327628135681152
Epoch 650, training loss: 849.0747680664062 = 0.7059482932090759 + 100.0 * 8.483688354492188
Epoch 650, val loss: 0.726190447807312
Epoch 660, training loss: 849.1011962890625 = 0.6988869905471802 + 100.0 * 8.484023094177246
Epoch 660, val loss: 0.7197499871253967
Epoch 670, training loss: 848.605712890625 = 0.6919819116592407 + 100.0 * 8.479137420654297
Epoch 670, val loss: 0.7136253714561462
Epoch 680, training loss: 848.4384155273438 = 0.6853477358818054 + 100.0 * 8.477530479431152
Epoch 680, val loss: 0.7076999545097351
Epoch 690, training loss: 848.1583862304688 = 0.6789442300796509 + 100.0 * 8.474794387817383
Epoch 690, val loss: 0.7019778490066528
Epoch 700, training loss: 848.0350341796875 = 0.6727699041366577 + 100.0 * 8.47362232208252
Epoch 700, val loss: 0.6965285539627075
Epoch 710, training loss: 847.7620849609375 = 0.666814923286438 + 100.0 * 8.470952987670898
Epoch 710, val loss: 0.6913294792175293
Epoch 720, training loss: 847.632568359375 = 0.6611030697822571 + 100.0 * 8.469714164733887
Epoch 720, val loss: 0.6863617300987244
Epoch 730, training loss: 847.3578491210938 = 0.6556687951087952 + 100.0 * 8.467021942138672
Epoch 730, val loss: 0.6816617250442505
Epoch 740, training loss: 847.18994140625 = 0.6504831314086914 + 100.0 * 8.465394020080566
Epoch 740, val loss: 0.6772410869598389
Epoch 750, training loss: 847.136474609375 = 0.6455330848693848 + 100.0 * 8.464909553527832
Epoch 750, val loss: 0.6730795502662659
Epoch 760, training loss: 847.0953369140625 = 0.6407596468925476 + 100.0 * 8.464546203613281
Epoch 760, val loss: 0.66912442445755
Epoch 770, training loss: 846.896484375 = 0.6362308859825134 + 100.0 * 8.462602615356445
Epoch 770, val loss: 0.6652787923812866
Epoch 780, training loss: 846.6610717773438 = 0.631954550743103 + 100.0 * 8.460290908813477
Epoch 780, val loss: 0.661851704120636
Epoch 790, training loss: 846.4771728515625 = 0.6279269456863403 + 100.0 * 8.458492279052734
Epoch 790, val loss: 0.6585978865623474
Epoch 800, training loss: 846.353271484375 = 0.6241003274917603 + 100.0 * 8.457291603088379
Epoch 800, val loss: 0.6555622220039368
Epoch 810, training loss: 846.6121215820312 = 0.6204684376716614 + 100.0 * 8.459916114807129
Epoch 810, val loss: 0.6527102589607239
Epoch 820, training loss: 846.60205078125 = 0.6169657707214355 + 100.0 * 8.459851264953613
Epoch 820, val loss: 0.6499967575073242
Epoch 830, training loss: 846.18994140625 = 0.6136529445648193 + 100.0 * 8.45576286315918
Epoch 830, val loss: 0.6475129723548889
Epoch 840, training loss: 845.9478149414062 = 0.610580325126648 + 100.0 * 8.45337200164795
Epoch 840, val loss: 0.6452223658561707
Epoch 850, training loss: 845.7948608398438 = 0.6076860427856445 + 100.0 * 8.451871871948242
Epoch 850, val loss: 0.6430932879447937
Epoch 860, training loss: 845.6610717773438 = 0.6049430966377258 + 100.0 * 8.4505615234375
Epoch 860, val loss: 0.6411517858505249
Epoch 870, training loss: 846.2335815429688 = 0.6023382544517517 + 100.0 * 8.45631217956543
Epoch 870, val loss: 0.6394697427749634
Epoch 880, training loss: 845.6559448242188 = 0.5997986197471619 + 100.0 * 8.4505615234375
Epoch 880, val loss: 0.6376457810401917
Epoch 890, training loss: 845.41845703125 = 0.5974374413490295 + 100.0 * 8.448210716247559
Epoch 890, val loss: 0.6359683871269226
Epoch 900, training loss: 845.250732421875 = 0.5952327847480774 + 100.0 * 8.446555137634277
Epoch 900, val loss: 0.6345447301864624
Epoch 910, training loss: 845.1448974609375 = 0.5931552648544312 + 100.0 * 8.445517539978027
Epoch 910, val loss: 0.6331998109817505
Epoch 920, training loss: 845.2052001953125 = 0.5911787152290344 + 100.0 * 8.44614028930664
Epoch 920, val loss: 0.6319500207901001
Epoch 930, training loss: 844.9253540039062 = 0.5892495512962341 + 100.0 * 8.443361282348633
Epoch 930, val loss: 0.6307002305984497
Epoch 940, training loss: 844.8848266601562 = 0.5874247550964355 + 100.0 * 8.442974090576172
Epoch 940, val loss: 0.6296169757843018
Epoch 950, training loss: 844.8287353515625 = 0.5856940746307373 + 100.0 * 8.44243049621582
Epoch 950, val loss: 0.628568172454834
Epoch 960, training loss: 844.643798828125 = 0.58404940366745 + 100.0 * 8.440597534179688
Epoch 960, val loss: 0.6276432871818542
Epoch 970, training loss: 844.5160522460938 = 0.582486093044281 + 100.0 * 8.439335823059082
Epoch 970, val loss: 0.6267567873001099
Epoch 980, training loss: 844.8756103515625 = 0.5809974670410156 + 100.0 * 8.442946434020996
Epoch 980, val loss: 0.6258814930915833
Epoch 990, training loss: 844.4464721679688 = 0.5794930458068848 + 100.0 * 8.43867015838623
Epoch 990, val loss: 0.6251541376113892
Epoch 1000, training loss: 844.2351684570312 = 0.5781092047691345 + 100.0 * 8.436570167541504
Epoch 1000, val loss: 0.6243950724601746
Epoch 1010, training loss: 844.1687622070312 = 0.5767792463302612 + 100.0 * 8.435919761657715
Epoch 1010, val loss: 0.6237257719039917
Epoch 1020, training loss: 844.3990478515625 = 0.5754832029342651 + 100.0 * 8.43823528289795
Epoch 1020, val loss: 0.623023271560669
Epoch 1030, training loss: 844.066650390625 = 0.5741831064224243 + 100.0 * 8.434925079345703
Epoch 1030, val loss: 0.6224595904350281
Epoch 1040, training loss: 843.8488159179688 = 0.5729849934577942 + 100.0 * 8.432758331298828
Epoch 1040, val loss: 0.6217730045318604
Epoch 1050, training loss: 843.748291015625 = 0.5718323588371277 + 100.0 * 8.431764602661133
Epoch 1050, val loss: 0.6212382912635803
Epoch 1060, training loss: 843.6847534179688 = 0.5707178115844727 + 100.0 * 8.431139945983887
Epoch 1060, val loss: 0.6207544803619385
Epoch 1070, training loss: 844.1683959960938 = 0.569606602191925 + 100.0 * 8.435988426208496
Epoch 1070, val loss: 0.6202118396759033
Epoch 1080, training loss: 843.6238403320312 = 0.5685073137283325 + 100.0 * 8.430553436279297
Epoch 1080, val loss: 0.6196710467338562
Epoch 1090, training loss: 843.5767211914062 = 0.567442774772644 + 100.0 * 8.430092811584473
Epoch 1090, val loss: 0.6192441582679749
Epoch 1100, training loss: 843.325439453125 = 0.5664061903953552 + 100.0 * 8.427590370178223
Epoch 1100, val loss: 0.6187843680381775
Epoch 1110, training loss: 843.2352905273438 = 0.5654112696647644 + 100.0 * 8.426698684692383
Epoch 1110, val loss: 0.6183485388755798
Epoch 1120, training loss: 843.3070068359375 = 0.56443852186203 + 100.0 * 8.427425384521484
Epoch 1120, val loss: 0.6179761290550232
Epoch 1130, training loss: 843.128662109375 = 0.563446581363678 + 100.0 * 8.425651550292969
Epoch 1130, val loss: 0.6174655556678772
Epoch 1140, training loss: 843.136474609375 = 0.5624632239341736 + 100.0 * 8.425740242004395
Epoch 1140, val loss: 0.617115318775177
Epoch 1150, training loss: 843.0046997070312 = 0.5615441203117371 + 100.0 * 8.424431800842285
Epoch 1150, val loss: 0.6166689991950989
Epoch 1160, training loss: 842.8894653320312 = 0.5606332421302795 + 100.0 * 8.423288345336914
Epoch 1160, val loss: 0.6163113117218018
Epoch 1170, training loss: 842.9337158203125 = 0.5597419738769531 + 100.0 * 8.423739433288574
Epoch 1170, val loss: 0.6159927845001221
Epoch 1180, training loss: 842.8446655273438 = 0.5588432550430298 + 100.0 * 8.422858238220215
Epoch 1180, val loss: 0.6155052185058594
Epoch 1190, training loss: 843.2817993164062 = 0.5579327940940857 + 100.0 * 8.427238464355469
Epoch 1190, val loss: 0.6151207685470581
Epoch 1200, training loss: 842.8527221679688 = 0.5570381283760071 + 100.0 * 8.422956466674805
Epoch 1200, val loss: 0.6148529052734375
Epoch 1210, training loss: 842.6533203125 = 0.5562015175819397 + 100.0 * 8.420970916748047
Epoch 1210, val loss: 0.6144105792045593
Epoch 1220, training loss: 842.5595703125 = 0.5553889870643616 + 100.0 * 8.420042037963867
Epoch 1220, val loss: 0.6141740679740906
Epoch 1230, training loss: 842.4796142578125 = 0.5545907020568848 + 100.0 * 8.41925048828125
Epoch 1230, val loss: 0.613823413848877
Epoch 1240, training loss: 842.4630737304688 = 0.5538040995597839 + 100.0 * 8.419092178344727
Epoch 1240, val loss: 0.6135096549987793
Epoch 1250, training loss: 843.3751831054688 = 0.5530270934104919 + 100.0 * 8.428221702575684
Epoch 1250, val loss: 0.6132388710975647
Epoch 1260, training loss: 842.4014892578125 = 0.5521565675735474 + 100.0 * 8.418493270874023
Epoch 1260, val loss: 0.6128018498420715
Epoch 1270, training loss: 842.3432006835938 = 0.551368236541748 + 100.0 * 8.41791820526123
Epoch 1270, val loss: 0.6124833822250366
Epoch 1280, training loss: 842.2804565429688 = 0.5506260395050049 + 100.0 * 8.417298316955566
Epoch 1280, val loss: 0.6122603416442871
Epoch 1290, training loss: 842.1948852539062 = 0.5498983860015869 + 100.0 * 8.416449546813965
Epoch 1290, val loss: 0.6119495630264282
Epoch 1300, training loss: 842.1502685546875 = 0.5491822957992554 + 100.0 * 8.416010856628418
Epoch 1300, val loss: 0.6116895079612732
Epoch 1310, training loss: 842.3155517578125 = 0.5484697818756104 + 100.0 * 8.417671203613281
Epoch 1310, val loss: 0.6114435195922852
Epoch 1320, training loss: 842.061767578125 = 0.5476948022842407 + 100.0 * 8.415141105651855
Epoch 1320, val loss: 0.6110341548919678
Epoch 1330, training loss: 842.2161254882812 = 0.5469515919685364 + 100.0 * 8.416691780090332
Epoch 1330, val loss: 0.6108076572418213
Epoch 1340, training loss: 842.0007934570312 = 0.5462337136268616 + 100.0 * 8.414546012878418
Epoch 1340, val loss: 0.6104701161384583
Epoch 1350, training loss: 841.94775390625 = 0.5455535650253296 + 100.0 * 8.414022445678711
Epoch 1350, val loss: 0.6101956367492676
Epoch 1360, training loss: 842.5485229492188 = 0.5448870062828064 + 100.0 * 8.420036315917969
Epoch 1360, val loss: 0.6098513007164001
Epoch 1370, training loss: 842.1192016601562 = 0.5441418290138245 + 100.0 * 8.415750503540039
Epoch 1370, val loss: 0.6097800135612488
Epoch 1380, training loss: 841.86865234375 = 0.5434643030166626 + 100.0 * 8.413251876831055
Epoch 1380, val loss: 0.6094165444374084
Epoch 1390, training loss: 841.785400390625 = 0.5428014993667603 + 100.0 * 8.412425994873047
Epoch 1390, val loss: 0.6092187762260437
Epoch 1400, training loss: 841.7423095703125 = 0.5421556830406189 + 100.0 * 8.412001609802246
Epoch 1400, val loss: 0.6090199947357178
Epoch 1410, training loss: 841.7888793945312 = 0.541511595249176 + 100.0 * 8.412473678588867
Epoch 1410, val loss: 0.6088243722915649
Epoch 1420, training loss: 841.8643188476562 = 0.5408489108085632 + 100.0 * 8.41323471069336
Epoch 1420, val loss: 0.6085798144340515
Epoch 1430, training loss: 841.6514282226562 = 0.5401667952537537 + 100.0 * 8.411112785339355
Epoch 1430, val loss: 0.6083604097366333
Epoch 1440, training loss: 841.6141967773438 = 0.5395271182060242 + 100.0 * 8.410746574401855
Epoch 1440, val loss: 0.6080685257911682
Epoch 1450, training loss: 841.6046752929688 = 0.5389022827148438 + 100.0 * 8.41065788269043
Epoch 1450, val loss: 0.6079174280166626
Epoch 1460, training loss: 841.72021484375 = 0.5382727384567261 + 100.0 * 8.411819458007812
Epoch 1460, val loss: 0.6076616048812866
Epoch 1470, training loss: 841.5656127929688 = 0.5376467108726501 + 100.0 * 8.410279273986816
Epoch 1470, val loss: 0.6073368191719055
Epoch 1480, training loss: 841.5973510742188 = 0.5370171070098877 + 100.0 * 8.410603523254395
Epoch 1480, val loss: 0.6071575284004211
Epoch 1490, training loss: 841.8483276367188 = 0.5363622307777405 + 100.0 * 8.413119316101074
Epoch 1490, val loss: 0.6069130897521973
Epoch 1500, training loss: 841.4105834960938 = 0.5357198715209961 + 100.0 * 8.408748626708984
Epoch 1500, val loss: 0.6066648364067078
Epoch 1510, training loss: 841.3683471679688 = 0.5351088643074036 + 100.0 * 8.408332824707031
Epoch 1510, val loss: 0.6064915060997009
Epoch 1520, training loss: 841.3087768554688 = 0.5345181822776794 + 100.0 * 8.407742500305176
Epoch 1520, val loss: 0.6062505841255188
Epoch 1530, training loss: 841.2516479492188 = 0.533938467502594 + 100.0 * 8.407176971435547
Epoch 1530, val loss: 0.606077253818512
Epoch 1540, training loss: 841.2569580078125 = 0.533352792263031 + 100.0 * 8.407236099243164
Epoch 1540, val loss: 0.605917751789093
Epoch 1550, training loss: 841.9127197265625 = 0.5327412486076355 + 100.0 * 8.413800239562988
Epoch 1550, val loss: 0.605696976184845
Epoch 1560, training loss: 841.3717651367188 = 0.5321364998817444 + 100.0 * 8.408395767211914
Epoch 1560, val loss: 0.6054368019104004
Epoch 1570, training loss: 841.1641235351562 = 0.5315183997154236 + 100.0 * 8.406326293945312
Epoch 1570, val loss: 0.6052783131599426
Epoch 1580, training loss: 841.0828857421875 = 0.5309534072875977 + 100.0 * 8.405519485473633
Epoch 1580, val loss: 0.6050559282302856
Epoch 1590, training loss: 841.1038818359375 = 0.5303820967674255 + 100.0 * 8.40573501586914
Epoch 1590, val loss: 0.6049337387084961
Epoch 1600, training loss: 841.4963989257812 = 0.5297970175743103 + 100.0 * 8.409666061401367
Epoch 1600, val loss: 0.6046851277351379
Epoch 1610, training loss: 841.0387573242188 = 0.5291892290115356 + 100.0 * 8.405096054077148
Epoch 1610, val loss: 0.6044927835464478
Epoch 1620, training loss: 840.9575805664062 = 0.5286116003990173 + 100.0 * 8.404289245605469
Epoch 1620, val loss: 0.6042912006378174
Epoch 1630, training loss: 841.1421508789062 = 0.5280406475067139 + 100.0 * 8.40614128112793
Epoch 1630, val loss: 0.6041608452796936
Epoch 1640, training loss: 840.8574829101562 = 0.5274385213851929 + 100.0 * 8.403300285339355
Epoch 1640, val loss: 0.6038119196891785
Epoch 1650, training loss: 840.8090209960938 = 0.526851236820221 + 100.0 * 8.40282154083252
Epoch 1650, val loss: 0.6036365032196045
Epoch 1660, training loss: 840.7869262695312 = 0.5262853503227234 + 100.0 * 8.402606010437012
Epoch 1660, val loss: 0.6034385561943054
Epoch 1670, training loss: 840.8975830078125 = 0.5257207751274109 + 100.0 * 8.403718948364258
Epoch 1670, val loss: 0.6032454967498779
Epoch 1680, training loss: 840.833984375 = 0.5251215696334839 + 100.0 * 8.403088569641113
Epoch 1680, val loss: 0.603049635887146
Epoch 1690, training loss: 840.6741943359375 = 0.52452552318573 + 100.0 * 8.401496887207031
Epoch 1690, val loss: 0.6028245687484741
Epoch 1700, training loss: 840.6904907226562 = 0.5239483714103699 + 100.0 * 8.401665687561035
Epoch 1700, val loss: 0.6025928258895874
Epoch 1710, training loss: 841.0135498046875 = 0.5233677625656128 + 100.0 * 8.404901504516602
Epoch 1710, val loss: 0.6024543642997742
Epoch 1720, training loss: 840.6134033203125 = 0.5227848291397095 + 100.0 * 8.400906562805176
Epoch 1720, val loss: 0.6021642684936523
Epoch 1730, training loss: 840.5431518554688 = 0.5221995711326599 + 100.0 * 8.400209426879883
Epoch 1730, val loss: 0.6020215153694153
Epoch 1740, training loss: 840.54638671875 = 0.5216253399848938 + 100.0 * 8.400247573852539
Epoch 1740, val loss: 0.6018398404121399
Epoch 1750, training loss: 840.84716796875 = 0.5210584998130798 + 100.0 * 8.403261184692383
Epoch 1750, val loss: 0.6016482710838318
Epoch 1760, training loss: 840.505615234375 = 0.5204296708106995 + 100.0 * 8.39985179901123
Epoch 1760, val loss: 0.6013067960739136
Epoch 1770, training loss: 840.4940795898438 = 0.519824206829071 + 100.0 * 8.399742126464844
Epoch 1770, val loss: 0.601127564907074
Epoch 1780, training loss: 840.4365844726562 = 0.5192371606826782 + 100.0 * 8.399173736572266
Epoch 1780, val loss: 0.6009039282798767
Epoch 1790, training loss: 840.4175415039062 = 0.5186553001403809 + 100.0 * 8.398988723754883
Epoch 1790, val loss: 0.6006922721862793
Epoch 1800, training loss: 840.9397583007812 = 0.5180537104606628 + 100.0 * 8.404216766357422
Epoch 1800, val loss: 0.6004115343093872
Epoch 1810, training loss: 840.506591796875 = 0.5174251794815063 + 100.0 * 8.39989185333252
Epoch 1810, val loss: 0.6003221869468689
Epoch 1820, training loss: 840.3442993164062 = 0.5168092250823975 + 100.0 * 8.398275375366211
Epoch 1820, val loss: 0.599921464920044
Epoch 1830, training loss: 840.2664184570312 = 0.5162163972854614 + 100.0 * 8.397501945495605
Epoch 1830, val loss: 0.5997692346572876
Epoch 1840, training loss: 840.2642822265625 = 0.5156299471855164 + 100.0 * 8.397486686706543
Epoch 1840, val loss: 0.5994682908058167
Epoch 1850, training loss: 840.7046508789062 = 0.5150396823883057 + 100.0 * 8.401896476745605
Epoch 1850, val loss: 0.5991702079772949
Epoch 1860, training loss: 840.3704833984375 = 0.5143854022026062 + 100.0 * 8.398560523986816
Epoch 1860, val loss: 0.5990554094314575
Epoch 1870, training loss: 840.4371948242188 = 0.5137656927108765 + 100.0 * 8.3992338180542
Epoch 1870, val loss: 0.5987368226051331
Epoch 1880, training loss: 840.255615234375 = 0.5131296515464783 + 100.0 * 8.397424697875977
Epoch 1880, val loss: 0.5984495878219604
Epoch 1890, training loss: 840.192626953125 = 0.5125128626823425 + 100.0 * 8.396800994873047
Epoch 1890, val loss: 0.5982163548469543
Epoch 1900, training loss: 840.1083374023438 = 0.5119029879570007 + 100.0 * 8.395964622497559
Epoch 1900, val loss: 0.5979235172271729
Epoch 1910, training loss: 840.0420532226562 = 0.5113030076026917 + 100.0 * 8.395307540893555
Epoch 1910, val loss: 0.5976825952529907
Epoch 1920, training loss: 840.0234985351562 = 0.510701060295105 + 100.0 * 8.39512825012207
Epoch 1920, val loss: 0.5974306464195251
Epoch 1930, training loss: 840.2046508789062 = 0.5100913047790527 + 100.0 * 8.39694595336914
Epoch 1930, val loss: 0.5971429944038391
Epoch 1940, training loss: 840.1497802734375 = 0.5094316005706787 + 100.0 * 8.396403312683105
Epoch 1940, val loss: 0.5968286395072937
Epoch 1950, training loss: 840.1187133789062 = 0.5087679028511047 + 100.0 * 8.396099090576172
Epoch 1950, val loss: 0.5964916944503784
Epoch 1960, training loss: 840.016357421875 = 0.5081227421760559 + 100.0 * 8.395082473754883
Epoch 1960, val loss: 0.596200704574585
Epoch 1970, training loss: 839.8984375 = 0.5074942708015442 + 100.0 * 8.393909454345703
Epoch 1970, val loss: 0.5959868431091309
Epoch 1980, training loss: 839.920654296875 = 0.5068764686584473 + 100.0 * 8.394137382507324
Epoch 1980, val loss: 0.5956903696060181
Epoch 1990, training loss: 840.11328125 = 0.5062493085861206 + 100.0 * 8.39607048034668
Epoch 1990, val loss: 0.5954599380493164
Epoch 2000, training loss: 839.8619384765625 = 0.5055883526802063 + 100.0 * 8.393563270568848
Epoch 2000, val loss: 0.5951170921325684
Epoch 2010, training loss: 839.9176635742188 = 0.5049344301223755 + 100.0 * 8.394126892089844
Epoch 2010, val loss: 0.5948778390884399
Epoch 2020, training loss: 840.166259765625 = 0.5042831897735596 + 100.0 * 8.39661979675293
Epoch 2020, val loss: 0.5946958661079407
Epoch 2030, training loss: 840.0098266601562 = 0.5035972595214844 + 100.0 * 8.395062446594238
Epoch 2030, val loss: 0.5940890312194824
Epoch 2040, training loss: 839.8184204101562 = 0.5029224157333374 + 100.0 * 8.393155097961426
Epoch 2040, val loss: 0.593849241733551
Epoch 2050, training loss: 839.71142578125 = 0.502273678779602 + 100.0 * 8.392091751098633
Epoch 2050, val loss: 0.5935397744178772
Epoch 2060, training loss: 839.6763305664062 = 0.5016235709190369 + 100.0 * 8.391746520996094
Epoch 2060, val loss: 0.5932086706161499
Epoch 2070, training loss: 839.68994140625 = 0.5009679794311523 + 100.0 * 8.391889572143555
Epoch 2070, val loss: 0.5929335951805115
Epoch 2080, training loss: 840.13623046875 = 0.5002903342247009 + 100.0 * 8.39635944366455
Epoch 2080, val loss: 0.592655599117279
Epoch 2090, training loss: 840.1080322265625 = 0.4995749592781067 + 100.0 * 8.396084785461426
Epoch 2090, val loss: 0.5922449231147766
Epoch 2100, training loss: 839.7607421875 = 0.498809814453125 + 100.0 * 8.392619132995605
Epoch 2100, val loss: 0.5918084979057312
Epoch 2110, training loss: 839.55908203125 = 0.49808767437934875 + 100.0 * 8.390609741210938
Epoch 2110, val loss: 0.5914769172668457
Epoch 2120, training loss: 839.5514526367188 = 0.4973980486392975 + 100.0 * 8.390541076660156
Epoch 2120, val loss: 0.5911041498184204
Epoch 2130, training loss: 839.5145874023438 = 0.49671024084091187 + 100.0 * 8.390178680419922
Epoch 2130, val loss: 0.5907871723175049
Epoch 2140, training loss: 839.4843139648438 = 0.4960183799266815 + 100.0 * 8.389883041381836
Epoch 2140, val loss: 0.5904434323310852
Epoch 2150, training loss: 839.4530639648438 = 0.49531519412994385 + 100.0 * 8.389577865600586
Epoch 2150, val loss: 0.5901027917861938
Epoch 2160, training loss: 839.470458984375 = 0.49459517002105713 + 100.0 * 8.389759063720703
Epoch 2160, val loss: 0.5897654891014099
Epoch 2170, training loss: 840.3460693359375 = 0.49383744597435 + 100.0 * 8.39852237701416
Epoch 2170, val loss: 0.5893507599830627
Epoch 2180, training loss: 839.6630859375 = 0.49304547905921936 + 100.0 * 8.391700744628906
Epoch 2180, val loss: 0.5887763500213623
Epoch 2190, training loss: 839.4219970703125 = 0.49225524067878723 + 100.0 * 8.389297485351562
Epoch 2190, val loss: 0.5885195136070251
Epoch 2200, training loss: 839.3678588867188 = 0.491497278213501 + 100.0 * 8.388763427734375
Epoch 2200, val loss: 0.5881540179252625
Epoch 2210, training loss: 839.3201293945312 = 0.4907543659210205 + 100.0 * 8.388293266296387
Epoch 2210, val loss: 0.5877258777618408
Epoch 2220, training loss: 839.30615234375 = 0.4900042712688446 + 100.0 * 8.388161659240723
Epoch 2220, val loss: 0.5873734354972839
Epoch 2230, training loss: 839.7828369140625 = 0.4892406165599823 + 100.0 * 8.392935752868652
Epoch 2230, val loss: 0.5868365168571472
Epoch 2240, training loss: 839.6597290039062 = 0.48839008808135986 + 100.0 * 8.39171314239502
Epoch 2240, val loss: 0.586769700050354
Epoch 2250, training loss: 839.448974609375 = 0.4875488579273224 + 100.0 * 8.38961410522461
Epoch 2250, val loss: 0.5861045718193054
Epoch 2260, training loss: 839.2208862304688 = 0.48674044013023376 + 100.0 * 8.387341499328613
Epoch 2260, val loss: 0.5856937766075134
Epoch 2270, training loss: 839.2161865234375 = 0.4859526753425598 + 100.0 * 8.38730239868164
Epoch 2270, val loss: 0.5853820443153381
Epoch 2280, training loss: 839.2913208007812 = 0.4851587116718292 + 100.0 * 8.3880615234375
Epoch 2280, val loss: 0.5849118232727051
Epoch 2290, training loss: 839.5457763671875 = 0.4843249022960663 + 100.0 * 8.39061450958252
Epoch 2290, val loss: 0.5844579935073853
Epoch 2300, training loss: 839.272705078125 = 0.4834778606891632 + 100.0 * 8.387892723083496
Epoch 2300, val loss: 0.5841494202613831
Epoch 2310, training loss: 839.123291015625 = 0.4826348125934601 + 100.0 * 8.386406898498535
Epoch 2310, val loss: 0.5836276412010193
Epoch 2320, training loss: 839.0784301757812 = 0.48180273175239563 + 100.0 * 8.385966300964355
Epoch 2320, val loss: 0.5832430720329285
Epoch 2330, training loss: 839.0946655273438 = 0.4809683859348297 + 100.0 * 8.386137008666992
Epoch 2330, val loss: 0.5828275084495544
Epoch 2340, training loss: 839.6314697265625 = 0.4801133871078491 + 100.0 * 8.39151382446289
Epoch 2340, val loss: 0.5824130773544312
Epoch 2350, training loss: 839.173095703125 = 0.4791991114616394 + 100.0 * 8.38693904876709
Epoch 2350, val loss: 0.5819084644317627
Epoch 2360, training loss: 839.0634155273438 = 0.47831493616104126 + 100.0 * 8.38585090637207
Epoch 2360, val loss: 0.5814917683601379
Epoch 2370, training loss: 839.0654907226562 = 0.47743555903434753 + 100.0 * 8.385880470275879
Epoch 2370, val loss: 0.5810444355010986
Epoch 2380, training loss: 839.2871704101562 = 0.4765508472919464 + 100.0 * 8.388106346130371
Epoch 2380, val loss: 0.580624520778656
Epoch 2390, training loss: 838.9381713867188 = 0.4756436049938202 + 100.0 * 8.384625434875488
Epoch 2390, val loss: 0.5802350640296936
Epoch 2400, training loss: 838.97216796875 = 0.4747525155544281 + 100.0 * 8.384974479675293
Epoch 2400, val loss: 0.5798627734184265
Epoch 2410, training loss: 838.9065551757812 = 0.4738558828830719 + 100.0 * 8.384326934814453
Epoch 2410, val loss: 0.5794329047203064
Epoch 2420, training loss: 839.0250854492188 = 0.472953200340271 + 100.0 * 8.385520935058594
Epoch 2420, val loss: 0.5790374279022217
Epoch 2430, training loss: 839.18359375 = 0.47201719880104065 + 100.0 * 8.387115478515625
Epoch 2430, val loss: 0.5784329175949097
Epoch 2440, training loss: 838.875244140625 = 0.4710589349269867 + 100.0 * 8.384041786193848
Epoch 2440, val loss: 0.5780426859855652
Epoch 2450, training loss: 838.8734130859375 = 0.47010961174964905 + 100.0 * 8.384033203125
Epoch 2450, val loss: 0.5776024460792542
Epoch 2460, training loss: 838.957763671875 = 0.469175785779953 + 100.0 * 8.384885787963867
Epoch 2460, val loss: 0.577190101146698
Epoch 2470, training loss: 839.0249633789062 = 0.4682216942310333 + 100.0 * 8.385567665100098
Epoch 2470, val loss: 0.5766575932502747
Epoch 2480, training loss: 838.8109130859375 = 0.4672505855560303 + 100.0 * 8.383437156677246
Epoch 2480, val loss: 0.5762838125228882
Epoch 2490, training loss: 838.7684326171875 = 0.4662967622280121 + 100.0 * 8.383021354675293
Epoch 2490, val loss: 0.5757888555526733
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7579908675799086
0.8172136492066943
=== training gcn model ===
Epoch 0, training loss: 1059.3392333984375 = 1.109229564666748 + 100.0 * 10.582300186157227
Epoch 0, val loss: 1.1082096099853516
Epoch 10, training loss: 1059.30029296875 = 1.1043473482131958 + 100.0 * 10.58195972442627
Epoch 10, val loss: 1.1033222675323486
Epoch 20, training loss: 1059.14208984375 = 1.0986865758895874 + 100.0 * 10.580434799194336
Epoch 20, val loss: 1.0976572036743164
Epoch 30, training loss: 1058.445068359375 = 1.0921993255615234 + 100.0 * 10.573529243469238
Epoch 30, val loss: 1.0911785364151
Epoch 40, training loss: 1055.49462890625 = 1.0850290060043335 + 100.0 * 10.544095993041992
Epoch 40, val loss: 1.084027886390686
Epoch 50, training loss: 1044.477783203125 = 1.077134370803833 + 100.0 * 10.434005737304688
Epoch 50, val loss: 1.076181173324585
Epoch 60, training loss: 1008.682861328125 = 1.068820595741272 + 100.0 * 10.076140403747559
Epoch 60, val loss: 1.067907691001892
Epoch 70, training loss: 956.92529296875 = 1.0598419904708862 + 100.0 * 9.55865478515625
Epoch 70, val loss: 1.0591202974319458
Epoch 80, training loss: 943.2213134765625 = 1.053645133972168 + 100.0 * 9.421676635742188
Epoch 80, val loss: 1.0533599853515625
Epoch 90, training loss: 930.848876953125 = 1.0496761798858643 + 100.0 * 9.297991752624512
Epoch 90, val loss: 1.0495645999908447
Epoch 100, training loss: 926.6239013671875 = 1.0461251735687256 + 100.0 * 9.255777359008789
Epoch 100, val loss: 1.046037197113037
Epoch 110, training loss: 925.0904541015625 = 1.042298436164856 + 100.0 * 9.24048137664795
Epoch 110, val loss: 1.0422202348709106
Epoch 120, training loss: 923.3450927734375 = 1.0385493040084839 + 100.0 * 9.223065376281738
Epoch 120, val loss: 1.0385401248931885
Epoch 130, training loss: 921.5999755859375 = 1.0353145599365234 + 100.0 * 9.205646514892578
Epoch 130, val loss: 1.0353981256484985
Epoch 140, training loss: 919.658447265625 = 1.0325175523757935 + 100.0 * 9.186259269714355
Epoch 140, val loss: 1.0326440334320068
Epoch 150, training loss: 917.0265502929688 = 1.0298874378204346 + 100.0 * 9.159966468811035
Epoch 150, val loss: 1.0300668478012085
Epoch 160, training loss: 913.0033569335938 = 1.0277587175369263 + 100.0 * 9.119755744934082
Epoch 160, val loss: 1.0280394554138184
Epoch 170, training loss: 906.3685913085938 = 1.0263794660568237 + 100.0 * 9.053421974182129
Epoch 170, val loss: 1.0267974138259888
Epoch 180, training loss: 896.5942993164062 = 1.0259382724761963 + 100.0 * 8.955683708190918
Epoch 180, val loss: 1.0264666080474854
Epoch 190, training loss: 887.9346313476562 = 1.02582585811615 + 100.0 * 8.869088172912598
Epoch 190, val loss: 1.026360034942627
Epoch 200, training loss: 883.1118774414062 = 1.0246731042861938 + 100.0 * 8.82087230682373
Epoch 200, val loss: 1.025170922279358
Epoch 210, training loss: 879.0869140625 = 1.0224473476409912 + 100.0 * 8.780644416809082
Epoch 210, val loss: 1.0229599475860596
Epoch 220, training loss: 875.5803833007812 = 1.019689679145813 + 100.0 * 8.745606422424316
Epoch 220, val loss: 1.020235538482666
Epoch 230, training loss: 873.2919311523438 = 1.0166659355163574 + 100.0 * 8.722752571105957
Epoch 230, val loss: 1.0172169208526611
Epoch 240, training loss: 871.5540161132812 = 1.0133296251296997 + 100.0 * 8.70540714263916
Epoch 240, val loss: 1.0138903856277466
Epoch 250, training loss: 870.420654296875 = 1.009748101234436 + 100.0 * 8.694108963012695
Epoch 250, val loss: 1.0103275775909424
Epoch 260, training loss: 869.3654174804688 = 1.0060497522354126 + 100.0 * 8.68359375
Epoch 260, val loss: 1.0066776275634766
Epoch 270, training loss: 868.2896118164062 = 1.0022612810134888 + 100.0 * 8.672873497009277
Epoch 270, val loss: 1.002945065498352
Epoch 280, training loss: 867.1109008789062 = 0.9983190894126892 + 100.0 * 8.661126136779785
Epoch 280, val loss: 0.9990732669830322
Epoch 290, training loss: 866.4303588867188 = 0.9942178726196289 + 100.0 * 8.654361724853516
Epoch 290, val loss: 0.9950206279754639
Epoch 300, training loss: 864.7904052734375 = 0.9898183345794678 + 100.0 * 8.638006210327148
Epoch 300, val loss: 0.9907034039497375
Epoch 310, training loss: 863.4794921875 = 0.9852837920188904 + 100.0 * 8.6249418258667
Epoch 310, val loss: 0.9862396717071533
Epoch 320, training loss: 862.3172607421875 = 0.9804761409759521 + 100.0 * 8.613368034362793
Epoch 320, val loss: 0.9814971089363098
Epoch 330, training loss: 861.6679077148438 = 0.9752343893051147 + 100.0 * 8.606926918029785
Epoch 330, val loss: 0.9763116836547852
Epoch 340, training loss: 860.4043579101562 = 0.9694727659225464 + 100.0 * 8.594348907470703
Epoch 340, val loss: 0.9706310629844666
Epoch 350, training loss: 859.6505126953125 = 0.9633992314338684 + 100.0 * 8.586871147155762
Epoch 350, val loss: 0.9646704196929932
Epoch 360, training loss: 858.890869140625 = 0.9570255279541016 + 100.0 * 8.579338073730469
Epoch 360, val loss: 0.9584258794784546
Epoch 370, training loss: 858.260986328125 = 0.9503399133682251 + 100.0 * 8.57310676574707
Epoch 370, val loss: 0.9518831372261047
Epoch 380, training loss: 857.7237548828125 = 0.9433208107948303 + 100.0 * 8.567804336547852
Epoch 380, val loss: 0.9449958205223083
Epoch 390, training loss: 857.0545654296875 = 0.9359951019287109 + 100.0 * 8.561185836791992
Epoch 390, val loss: 0.9378606081008911
Epoch 400, training loss: 856.50146484375 = 0.928429365158081 + 100.0 * 8.555730819702148
Epoch 400, val loss: 0.9305018186569214
Epoch 410, training loss: 856.1928100585938 = 0.920620322227478 + 100.0 * 8.552721977233887
Epoch 410, val loss: 0.9228964447975159
Epoch 420, training loss: 855.4732666015625 = 0.9125251173973083 + 100.0 * 8.545607566833496
Epoch 420, val loss: 0.9150697588920593
Epoch 430, training loss: 854.9846801757812 = 0.9042930603027344 + 100.0 * 8.540803909301758
Epoch 430, val loss: 0.907113254070282
Epoch 440, training loss: 854.5280151367188 = 0.8959420323371887 + 100.0 * 8.536320686340332
Epoch 440, val loss: 0.8990508913993835
Epoch 450, training loss: 854.0880126953125 = 0.8873939514160156 + 100.0 * 8.53200626373291
Epoch 450, val loss: 0.890803873538971
Epoch 460, training loss: 853.4991455078125 = 0.878718376159668 + 100.0 * 8.526204109191895
Epoch 460, val loss: 0.8824726343154907
Epoch 470, training loss: 853.146240234375 = 0.8699784278869629 + 100.0 * 8.522762298583984
Epoch 470, val loss: 0.8741085529327393
Epoch 480, training loss: 852.7942504882812 = 0.8610774874687195 + 100.0 * 8.519331932067871
Epoch 480, val loss: 0.8655345439910889
Epoch 490, training loss: 852.2789916992188 = 0.8521164655685425 + 100.0 * 8.51426887512207
Epoch 490, val loss: 0.8569996356964111
Epoch 500, training loss: 851.9816284179688 = 0.8431426882743835 + 100.0 * 8.511384963989258
Epoch 500, val loss: 0.848465621471405
Epoch 510, training loss: 851.6118774414062 = 0.8340528011322021 + 100.0 * 8.50777816772461
Epoch 510, val loss: 0.8397988080978394
Epoch 520, training loss: 851.2859497070312 = 0.8250254392623901 + 100.0 * 8.504609107971191
Epoch 520, val loss: 0.8312707543373108
Epoch 530, training loss: 851.0040893554688 = 0.8160852789878845 + 100.0 * 8.501879692077637
Epoch 530, val loss: 0.8228217959403992
Epoch 540, training loss: 850.766357421875 = 0.8072122931480408 + 100.0 * 8.499591827392578
Epoch 540, val loss: 0.8144660592079163
Epoch 550, training loss: 850.720947265625 = 0.798438549041748 + 100.0 * 8.499224662780762
Epoch 550, val loss: 0.8062312602996826
Epoch 560, training loss: 850.2401123046875 = 0.7897165417671204 + 100.0 * 8.49450397491455
Epoch 560, val loss: 0.7980590462684631
Epoch 570, training loss: 849.9512939453125 = 0.7812057733535767 + 100.0 * 8.491701126098633
Epoch 570, val loss: 0.7901360988616943
Epoch 580, training loss: 849.7132568359375 = 0.7728655338287354 + 100.0 * 8.48940372467041
Epoch 580, val loss: 0.7823935747146606
Epoch 590, training loss: 849.8070678710938 = 0.7646599411964417 + 100.0 * 8.490424156188965
Epoch 590, val loss: 0.7747634053230286
Epoch 600, training loss: 849.3262329101562 = 0.7566389441490173 + 100.0 * 8.485695838928223
Epoch 600, val loss: 0.7674022316932678
Epoch 610, training loss: 849.029052734375 = 0.7489003539085388 + 100.0 * 8.48280143737793
Epoch 610, val loss: 0.7603498697280884
Epoch 620, training loss: 848.8826904296875 = 0.7413861751556396 + 100.0 * 8.481412887573242
Epoch 620, val loss: 0.7535410523414612
Epoch 630, training loss: 848.6533813476562 = 0.7340289354324341 + 100.0 * 8.479193687438965
Epoch 630, val loss: 0.7468256950378418
Epoch 640, training loss: 848.4854736328125 = 0.726990282535553 + 100.0 * 8.477584838867188
Epoch 640, val loss: 0.740524411201477
Epoch 650, training loss: 848.2149047851562 = 0.7202008962631226 + 100.0 * 8.474946975708008
Epoch 650, val loss: 0.7344460487365723
Epoch 660, training loss: 848.2469482421875 = 0.7136403322219849 + 100.0 * 8.475333213806152
Epoch 660, val loss: 0.7286252379417419
Epoch 670, training loss: 848.039794921875 = 0.7073321342468262 + 100.0 * 8.4733247756958
Epoch 670, val loss: 0.723053514957428
Epoch 680, training loss: 847.6802368164062 = 0.7013069987297058 + 100.0 * 8.469789505004883
Epoch 680, val loss: 0.7177677750587463
Epoch 690, training loss: 847.44140625 = 0.6955538392066956 + 100.0 * 8.467458724975586
Epoch 690, val loss: 0.7127906084060669
Epoch 700, training loss: 847.270751953125 = 0.6900725960731506 + 100.0 * 8.46580696105957
Epoch 700, val loss: 0.7081075310707092
Epoch 710, training loss: 847.9763793945312 = 0.6848074197769165 + 100.0 * 8.472915649414062
Epoch 710, val loss: 0.7036080956459045
Epoch 720, training loss: 847.13916015625 = 0.6797000765800476 + 100.0 * 8.464594841003418
Epoch 720, val loss: 0.6993238925933838
Epoch 730, training loss: 846.7958374023438 = 0.6749510765075684 + 100.0 * 8.461209297180176
Epoch 730, val loss: 0.6953306198120117
Epoch 740, training loss: 846.5870361328125 = 0.6704159379005432 + 100.0 * 8.459166526794434
Epoch 740, val loss: 0.6915342211723328
Epoch 750, training loss: 846.8667602539062 = 0.6660890579223633 + 100.0 * 8.462006568908691
Epoch 750, val loss: 0.6879326701164246
Epoch 760, training loss: 846.2474365234375 = 0.6619555950164795 + 100.0 * 8.455855369567871
Epoch 760, val loss: 0.684729278087616
Epoch 770, training loss: 846.1190795898438 = 0.6580520272254944 + 100.0 * 8.454609870910645
Epoch 770, val loss: 0.6816533207893372
Epoch 780, training loss: 845.92822265625 = 0.6543476581573486 + 100.0 * 8.452738761901855
Epoch 780, val loss: 0.6787075400352478
Epoch 790, training loss: 845.7686767578125 = 0.6508324146270752 + 100.0 * 8.451178550720215
Epoch 790, val loss: 0.6760167479515076
Epoch 800, training loss: 845.6337890625 = 0.6474682092666626 + 100.0 * 8.44986343383789
Epoch 800, val loss: 0.6734129786491394
Epoch 810, training loss: 846.1048583984375 = 0.6442333459854126 + 100.0 * 8.454606056213379
Epoch 810, val loss: 0.6709311604499817
Epoch 820, training loss: 845.4439697265625 = 0.6410983204841614 + 100.0 * 8.448028564453125
Epoch 820, val loss: 0.6687079668045044
Epoch 830, training loss: 845.2608642578125 = 0.6381597518920898 + 100.0 * 8.446227073669434
Epoch 830, val loss: 0.6665140390396118
Epoch 840, training loss: 845.1196899414062 = 0.6353622078895569 + 100.0 * 8.444843292236328
Epoch 840, val loss: 0.6644989252090454
Epoch 850, training loss: 845.2465209960938 = 0.6326854228973389 + 100.0 * 8.446138381958008
Epoch 850, val loss: 0.6625737547874451
Epoch 860, training loss: 844.990234375 = 0.6300856471061707 + 100.0 * 8.443601608276367
Epoch 860, val loss: 0.6607310771942139
Epoch 870, training loss: 844.8261108398438 = 0.6276084780693054 + 100.0 * 8.441985130310059
Epoch 870, val loss: 0.6590304970741272
Epoch 880, training loss: 844.7174072265625 = 0.6252361536026001 + 100.0 * 8.440921783447266
Epoch 880, val loss: 0.657374918460846
Epoch 890, training loss: 844.7257690429688 = 0.6229473948478699 + 100.0 * 8.441028594970703
Epoch 890, val loss: 0.6558189988136292
Epoch 900, training loss: 844.4727783203125 = 0.6207532286643982 + 100.0 * 8.438520431518555
Epoch 900, val loss: 0.6545365452766418
Epoch 910, training loss: 844.4064331054688 = 0.618647575378418 + 100.0 * 8.437877655029297
Epoch 910, val loss: 0.6531754732131958
Epoch 920, training loss: 844.3910522460938 = 0.6166232824325562 + 100.0 * 8.437744140625
Epoch 920, val loss: 0.6518133282661438
Epoch 930, training loss: 844.1763916015625 = 0.6146217584609985 + 100.0 * 8.435617446899414
Epoch 930, val loss: 0.6505020260810852
Epoch 940, training loss: 844.1072387695312 = 0.6127332448959351 + 100.0 * 8.434945106506348
Epoch 940, val loss: 0.6493216156959534
Epoch 950, training loss: 844.0196533203125 = 0.6109247803688049 + 100.0 * 8.434087753295898
Epoch 950, val loss: 0.6482254862785339
Epoch 960, training loss: 844.3079833984375 = 0.6091729402542114 + 100.0 * 8.43698787689209
Epoch 960, val loss: 0.6470112204551697
Epoch 970, training loss: 844.005126953125 = 0.6074486970901489 + 100.0 * 8.433977127075195
Epoch 970, val loss: 0.6461119651794434
Epoch 980, training loss: 843.7142333984375 = 0.6058223247528076 + 100.0 * 8.431083679199219
Epoch 980, val loss: 0.6451904773712158
Epoch 990, training loss: 843.6403198242188 = 0.60425865650177 + 100.0 * 8.430360794067383
Epoch 990, val loss: 0.6441798806190491
Epoch 1000, training loss: 843.5490112304688 = 0.6027629375457764 + 100.0 * 8.429462432861328
Epoch 1000, val loss: 0.6433494687080383
Epoch 1010, training loss: 843.727783203125 = 0.6012924909591675 + 100.0 * 8.431264877319336
Epoch 1010, val loss: 0.6423568725585938
Epoch 1020, training loss: 843.392578125 = 0.5998435020446777 + 100.0 * 8.427927017211914
Epoch 1020, val loss: 0.6418402194976807
Epoch 1030, training loss: 843.2762451171875 = 0.598463237285614 + 100.0 * 8.426777839660645
Epoch 1030, val loss: 0.6408864259719849
Epoch 1040, training loss: 843.194091796875 = 0.5971388220787048 + 100.0 * 8.425969123840332
Epoch 1040, val loss: 0.6402059197425842
Epoch 1050, training loss: 843.2943115234375 = 0.5958552956581116 + 100.0 * 8.426984786987305
Epoch 1050, val loss: 0.6395074725151062
Epoch 1060, training loss: 843.1483154296875 = 0.5945824980735779 + 100.0 * 8.425537109375
Epoch 1060, val loss: 0.6388704776763916
Epoch 1070, training loss: 843.3448486328125 = 0.5933475494384766 + 100.0 * 8.427515029907227
Epoch 1070, val loss: 0.6381130814552307
Epoch 1080, training loss: 843.0020141601562 = 0.5921187996864319 + 100.0 * 8.42409896850586
Epoch 1080, val loss: 0.637522280216217
Epoch 1090, training loss: 842.8416137695312 = 0.5909749865531921 + 100.0 * 8.422506332397461
Epoch 1090, val loss: 0.6369042992591858
Epoch 1100, training loss: 842.7606811523438 = 0.5898569822311401 + 100.0 * 8.421708106994629
Epoch 1100, val loss: 0.6363934874534607
Epoch 1110, training loss: 842.6973876953125 = 0.5887698531150818 + 100.0 * 8.421086311340332
Epoch 1110, val loss: 0.6358547806739807
Epoch 1120, training loss: 843.521484375 = 0.587698757648468 + 100.0 * 8.429337501525879
Epoch 1120, val loss: 0.6354514956474304
Epoch 1130, training loss: 842.5794677734375 = 0.5865635871887207 + 100.0 * 8.419929504394531
Epoch 1130, val loss: 0.6347364187240601
Epoch 1140, training loss: 842.6041259765625 = 0.5855259895324707 + 100.0 * 8.420186042785645
Epoch 1140, val loss: 0.634026288986206
Epoch 1150, training loss: 842.447021484375 = 0.5845491886138916 + 100.0 * 8.418624877929688
Epoch 1150, val loss: 0.6336145401000977
Epoch 1160, training loss: 842.40478515625 = 0.5835888981819153 + 100.0 * 8.418211936950684
Epoch 1160, val loss: 0.6331964731216431
Epoch 1170, training loss: 842.3396606445312 = 0.5826401114463806 + 100.0 * 8.417570114135742
Epoch 1170, val loss: 0.6327176690101624
Epoch 1180, training loss: 842.31298828125 = 0.5817062258720398 + 100.0 * 8.417312622070312
Epoch 1180, val loss: 0.6322958469390869
Epoch 1190, training loss: 842.883056640625 = 0.5807563662528992 + 100.0 * 8.423023223876953
Epoch 1190, val loss: 0.631732702255249
Epoch 1200, training loss: 842.41650390625 = 0.5797664523124695 + 100.0 * 8.418367385864258
Epoch 1200, val loss: 0.6314294338226318
Epoch 1210, training loss: 842.185302734375 = 0.5788655877113342 + 100.0 * 8.416064262390137
Epoch 1210, val loss: 0.6308029890060425
Epoch 1220, training loss: 842.0992431640625 = 0.5780150890350342 + 100.0 * 8.415212631225586
Epoch 1220, val loss: 0.6304683089256287
Epoch 1230, training loss: 842.05712890625 = 0.577174961566925 + 100.0 * 8.414799690246582
Epoch 1230, val loss: 0.6300700902938843
Epoch 1240, training loss: 842.0156860351562 = 0.5763440132141113 + 100.0 * 8.414393424987793
Epoch 1240, val loss: 0.6296884417533875
Epoch 1250, training loss: 842.1021118164062 = 0.5755147933959961 + 100.0 * 8.415266036987305
Epoch 1250, val loss: 0.629395067691803
Epoch 1260, training loss: 841.927978515625 = 0.5746639370918274 + 100.0 * 8.413533210754395
Epoch 1260, val loss: 0.6288874745368958
Epoch 1270, training loss: 841.8860473632812 = 0.573844850063324 + 100.0 * 8.413122177124023
Epoch 1270, val loss: 0.6284874081611633
Epoch 1280, training loss: 841.8423461914062 = 0.5730611681938171 + 100.0 * 8.41269302368164
Epoch 1280, val loss: 0.6280936598777771
Epoch 1290, training loss: 841.793701171875 = 0.5722907781600952 + 100.0 * 8.412214279174805
Epoch 1290, val loss: 0.6277838349342346
Epoch 1300, training loss: 841.7781982421875 = 0.5715315341949463 + 100.0 * 8.412066459655762
Epoch 1300, val loss: 0.6274679899215698
Epoch 1310, training loss: 842.2179565429688 = 0.5707683563232422 + 100.0 * 8.416472434997559
Epoch 1310, val loss: 0.6272091865539551
Epoch 1320, training loss: 841.7763061523438 = 0.569943368434906 + 100.0 * 8.412063598632812
Epoch 1320, val loss: 0.6266078352928162
Epoch 1330, training loss: 841.6680297851562 = 0.5691801905632019 + 100.0 * 8.410988807678223
Epoch 1330, val loss: 0.6262719035148621
Epoch 1340, training loss: 841.6217041015625 = 0.5684602856636047 + 100.0 * 8.410531997680664
Epoch 1340, val loss: 0.6258432269096375
Epoch 1350, training loss: 841.5643920898438 = 0.5677478909492493 + 100.0 * 8.409966468811035
Epoch 1350, val loss: 0.6255818009376526
Epoch 1360, training loss: 841.5158081054688 = 0.5670427680015564 + 100.0 * 8.4094877243042
Epoch 1360, val loss: 0.6252192258834839
Epoch 1370, training loss: 841.4752807617188 = 0.5663378238677979 + 100.0 * 8.409089088439941
Epoch 1370, val loss: 0.6249004006385803
Epoch 1380, training loss: 841.4443969726562 = 0.5656373500823975 + 100.0 * 8.408787727355957
Epoch 1380, val loss: 0.6245458722114563
Epoch 1390, training loss: 841.9547119140625 = 0.5649502277374268 + 100.0 * 8.413897514343262
Epoch 1390, val loss: 0.6240615844726562
Epoch 1400, training loss: 841.822265625 = 0.5641770958900452 + 100.0 * 8.412581443786621
Epoch 1400, val loss: 0.6240880489349365
Epoch 1410, training loss: 841.436767578125 = 0.5634336471557617 + 100.0 * 8.408733367919922
Epoch 1410, val loss: 0.6234384775161743
Epoch 1420, training loss: 841.322509765625 = 0.5627536773681641 + 100.0 * 8.407597541809082
Epoch 1420, val loss: 0.623134970664978
Epoch 1430, training loss: 841.2671508789062 = 0.5621033310890198 + 100.0 * 8.407050132751465
Epoch 1430, val loss: 0.6229045391082764
Epoch 1440, training loss: 841.2295532226562 = 0.5614514946937561 + 100.0 * 8.406681060791016
Epoch 1440, val loss: 0.622599720954895
Epoch 1450, training loss: 841.2293701171875 = 0.5607925057411194 + 100.0 * 8.406685829162598
Epoch 1450, val loss: 0.6223282814025879
Epoch 1460, training loss: 841.5553588867188 = 0.5601212978363037 + 100.0 * 8.409952163696289
Epoch 1460, val loss: 0.6220621466636658
Epoch 1470, training loss: 841.2440795898438 = 0.5594176650047302 + 100.0 * 8.40684700012207
Epoch 1470, val loss: 0.6215461492538452
Epoch 1480, training loss: 841.119140625 = 0.5587574243545532 + 100.0 * 8.405603408813477
Epoch 1480, val loss: 0.6212977170944214
Epoch 1490, training loss: 841.09765625 = 0.5581066608428955 + 100.0 * 8.4053955078125
Epoch 1490, val loss: 0.6209734678268433
Epoch 1500, training loss: 841.1561279296875 = 0.5574495792388916 + 100.0 * 8.405986785888672
Epoch 1500, val loss: 0.6206821203231812
Epoch 1510, training loss: 841.0093383789062 = 0.556791365146637 + 100.0 * 8.404525756835938
Epoch 1510, val loss: 0.620378851890564
Epoch 1520, training loss: 840.9527587890625 = 0.5561434030532837 + 100.0 * 8.403965950012207
Epoch 1520, val loss: 0.6200172901153564
Epoch 1530, training loss: 840.9315795898438 = 0.5554983019828796 + 100.0 * 8.40376091003418
Epoch 1530, val loss: 0.6197372674942017
Epoch 1540, training loss: 841.0879516601562 = 0.554844319820404 + 100.0 * 8.405330657958984
Epoch 1540, val loss: 0.6194961667060852
Epoch 1550, training loss: 841.111083984375 = 0.5541762113571167 + 100.0 * 8.405569076538086
Epoch 1550, val loss: 0.619295597076416
Epoch 1560, training loss: 840.875 = 0.5534831285476685 + 100.0 * 8.403215408325195
Epoch 1560, val loss: 0.6187689900398254
Epoch 1570, training loss: 840.8050537109375 = 0.5528278946876526 + 100.0 * 8.402522087097168
Epoch 1570, val loss: 0.6184104084968567
Epoch 1580, training loss: 840.7147216796875 = 0.5521955490112305 + 100.0 * 8.401625633239746
Epoch 1580, val loss: 0.6181928515434265
Epoch 1590, training loss: 840.6738891601562 = 0.5515751838684082 + 100.0 * 8.401223182678223
Epoch 1590, val loss: 0.6178842186927795
Epoch 1600, training loss: 840.660400390625 = 0.5509474277496338 + 100.0 * 8.401094436645508
Epoch 1600, val loss: 0.6176301836967468
Epoch 1610, training loss: 841.3970947265625 = 0.5502933263778687 + 100.0 * 8.408468246459961
Epoch 1610, val loss: 0.617199182510376
Epoch 1620, training loss: 840.6312866210938 = 0.5496099591255188 + 100.0 * 8.400816917419434
Epoch 1620, val loss: 0.617024302482605
Epoch 1630, training loss: 840.5886840820312 = 0.5489647388458252 + 100.0 * 8.400397300720215
Epoch 1630, val loss: 0.6166801452636719
Epoch 1640, training loss: 840.510986328125 = 0.5483459234237671 + 100.0 * 8.399626731872559
Epoch 1640, val loss: 0.6164554357528687
Epoch 1650, training loss: 840.47607421875 = 0.5477333664894104 + 100.0 * 8.399283409118652
Epoch 1650, val loss: 0.6162024736404419
Epoch 1660, training loss: 840.8558959960938 = 0.547113835811615 + 100.0 * 8.403087615966797
Epoch 1660, val loss: 0.6161772012710571
Epoch 1670, training loss: 840.4967041015625 = 0.5464401245117188 + 100.0 * 8.399502754211426
Epoch 1670, val loss: 0.6154268383979797
Epoch 1680, training loss: 840.4207153320312 = 0.5457910895347595 + 100.0 * 8.398749351501465
Epoch 1680, val loss: 0.6153290867805481
Epoch 1690, training loss: 840.3534545898438 = 0.5451743006706238 + 100.0 * 8.398082733154297
Epoch 1690, val loss: 0.6149159669876099
Epoch 1700, training loss: 840.3008422851562 = 0.544558584690094 + 100.0 * 8.397562980651855
Epoch 1700, val loss: 0.6147512197494507
Epoch 1710, training loss: 840.2667846679688 = 0.5439432263374329 + 100.0 * 8.397228240966797
Epoch 1710, val loss: 0.6144346594810486
Epoch 1720, training loss: 841.0496215820312 = 0.5433123111724854 + 100.0 * 8.405062675476074
Epoch 1720, val loss: 0.6140508651733398
Epoch 1730, training loss: 840.4497680664062 = 0.5426250696182251 + 100.0 * 8.39907169342041
Epoch 1730, val loss: 0.6138932108879089
Epoch 1740, training loss: 840.2914428710938 = 0.5419607162475586 + 100.0 * 8.39749526977539
Epoch 1740, val loss: 0.6134467124938965
Epoch 1750, training loss: 840.1508178710938 = 0.5413376688957214 + 100.0 * 8.396095275878906
Epoch 1750, val loss: 0.6132500767707825
Epoch 1760, training loss: 840.1149291992188 = 0.5407216548919678 + 100.0 * 8.395742416381836
Epoch 1760, val loss: 0.6128829717636108
Epoch 1770, training loss: 840.0911865234375 = 0.5401033759117126 + 100.0 * 8.39551067352295
Epoch 1770, val loss: 0.6126506924629211
Epoch 1780, training loss: 840.5032958984375 = 0.5394719839096069 + 100.0 * 8.399638175964355
Epoch 1780, val loss: 0.6122898459434509
Epoch 1790, training loss: 840.2889404296875 = 0.538791298866272 + 100.0 * 8.397500991821289
Epoch 1790, val loss: 0.6119317412376404
Epoch 1800, training loss: 840.038330078125 = 0.5381082892417908 + 100.0 * 8.395002365112305
Epoch 1800, val loss: 0.6116844415664673
Epoch 1810, training loss: 839.9993896484375 = 0.5374640226364136 + 100.0 * 8.39461898803711
Epoch 1810, val loss: 0.6113523840904236
Epoch 1820, training loss: 839.9568481445312 = 0.5368292927742004 + 100.0 * 8.394200325012207
Epoch 1820, val loss: 0.6110502481460571
Epoch 1830, training loss: 839.9591674804688 = 0.5361911654472351 + 100.0 * 8.394229888916016
Epoch 1830, val loss: 0.6106892228126526
Epoch 1840, training loss: 840.2673950195312 = 0.5355381369590759 + 100.0 * 8.397318840026855
Epoch 1840, val loss: 0.6102409958839417
Epoch 1850, training loss: 839.9199829101562 = 0.5348415374755859 + 100.0 * 8.393851280212402
Epoch 1850, val loss: 0.6102734208106995
Epoch 1860, training loss: 839.8553466796875 = 0.5341759324073792 + 100.0 * 8.393211364746094
Epoch 1860, val loss: 0.6097680330276489
Epoch 1870, training loss: 839.8501586914062 = 0.533521831035614 + 100.0 * 8.393166542053223
Epoch 1870, val loss: 0.6094278693199158
Epoch 1880, training loss: 840.386474609375 = 0.5328649282455444 + 100.0 * 8.398536682128906
Epoch 1880, val loss: 0.6090468764305115
Epoch 1890, training loss: 839.9601440429688 = 0.532132089138031 + 100.0 * 8.394280433654785
Epoch 1890, val loss: 0.6088917255401611
Epoch 1900, training loss: 839.8143310546875 = 0.5314450860023499 + 100.0 * 8.392828941345215
Epoch 1900, val loss: 0.6083994507789612
Epoch 1910, training loss: 839.7476806640625 = 0.5307669043540955 + 100.0 * 8.392168998718262
Epoch 1910, val loss: 0.6080319881439209
Epoch 1920, training loss: 839.7092895507812 = 0.5300899147987366 + 100.0 * 8.391792297363281
Epoch 1920, val loss: 0.6077495217323303
Epoch 1930, training loss: 839.6828002929688 = 0.5294061303138733 + 100.0 * 8.391533851623535
Epoch 1930, val loss: 0.6073737740516663
Epoch 1940, training loss: 839.8430786132812 = 0.5287150740623474 + 100.0 * 8.393143653869629
Epoch 1940, val loss: 0.6069524884223938
Epoch 1950, training loss: 839.94091796875 = 0.5279727578163147 + 100.0 * 8.394129753112793
Epoch 1950, val loss: 0.6063841581344604
Epoch 1960, training loss: 839.8228149414062 = 0.5271962881088257 + 100.0 * 8.392955780029297
Epoch 1960, val loss: 0.6061476469039917
Epoch 1970, training loss: 839.6226806640625 = 0.526477575302124 + 100.0 * 8.390961647033691
Epoch 1970, val loss: 0.6057488918304443
Epoch 1980, training loss: 839.5802612304688 = 0.5257793664932251 + 100.0 * 8.390544891357422
Epoch 1980, val loss: 0.6053481698036194
Epoch 1990, training loss: 839.5419311523438 = 0.5250792503356934 + 100.0 * 8.390168190002441
Epoch 1990, val loss: 0.6049571633338928
Epoch 2000, training loss: 839.517822265625 = 0.5243688821792603 + 100.0 * 8.389934539794922
Epoch 2000, val loss: 0.6046086549758911
Epoch 2010, training loss: 839.4935913085938 = 0.5236504077911377 + 100.0 * 8.389699935913086
Epoch 2010, val loss: 0.6042349934577942
Epoch 2020, training loss: 839.5015258789062 = 0.5229254364967346 + 100.0 * 8.389785766601562
Epoch 2020, val loss: 0.6038524508476257
Epoch 2030, training loss: 840.0799560546875 = 0.5221894383430481 + 100.0 * 8.395577430725098
Epoch 2030, val loss: 0.6033790707588196
Epoch 2040, training loss: 839.6895141601562 = 0.5213853120803833 + 100.0 * 8.391681671142578
Epoch 2040, val loss: 0.6029654145240784
Epoch 2050, training loss: 839.6239624023438 = 0.5206270217895508 + 100.0 * 8.391033172607422
Epoch 2050, val loss: 0.6024903655052185
Epoch 2060, training loss: 839.4012451171875 = 0.5198501944541931 + 100.0 * 8.388813972473145
Epoch 2060, val loss: 0.6021429300308228
Epoch 2070, training loss: 839.3761596679688 = 0.5190975069999695 + 100.0 * 8.388570785522461
Epoch 2070, val loss: 0.6018885970115662
Epoch 2080, training loss: 839.41015625 = 0.5183454155921936 + 100.0 * 8.388917922973633
Epoch 2080, val loss: 0.6015397310256958
Epoch 2090, training loss: 839.4922485351562 = 0.5175729393959045 + 100.0 * 8.38974666595459
Epoch 2090, val loss: 0.6011753678321838
Epoch 2100, training loss: 839.2965087890625 = 0.5167809724807739 + 100.0 * 8.387797355651855
Epoch 2100, val loss: 0.6005452275276184
Epoch 2110, training loss: 839.363525390625 = 0.5160031318664551 + 100.0 * 8.38847541809082
Epoch 2110, val loss: 0.600001335144043
Epoch 2120, training loss: 839.3037109375 = 0.5152051448822021 + 100.0 * 8.387885093688965
Epoch 2120, val loss: 0.5997728705406189
Epoch 2130, training loss: 839.4219970703125 = 0.5144023895263672 + 100.0 * 8.389076232910156
Epoch 2130, val loss: 0.5994551777839661
Epoch 2140, training loss: 839.3489379882812 = 0.5135878920555115 + 100.0 * 8.38835334777832
Epoch 2140, val loss: 0.5988438129425049
Epoch 2150, training loss: 839.2769165039062 = 0.512765109539032 + 100.0 * 8.387641906738281
Epoch 2150, val loss: 0.598523736000061
Epoch 2160, training loss: 839.195556640625 = 0.5119523406028748 + 100.0 * 8.386836051940918
Epoch 2160, val loss: 0.5979195833206177
Epoch 2170, training loss: 839.2448120117188 = 0.5111448764801025 + 100.0 * 8.387336730957031
Epoch 2170, val loss: 0.59752357006073
Epoch 2180, training loss: 839.222900390625 = 0.5103206038475037 + 100.0 * 8.387125968933105
Epoch 2180, val loss: 0.597115159034729
Epoch 2190, training loss: 839.1121215820312 = 0.5094864368438721 + 100.0 * 8.386026382446289
Epoch 2190, val loss: 0.5968533158302307
Epoch 2200, training loss: 839.208984375 = 0.5086637139320374 + 100.0 * 8.387002944946289
Epoch 2200, val loss: 0.5964966416358948
Epoch 2210, training loss: 839.35546875 = 0.5078035593032837 + 100.0 * 8.388476371765137
Epoch 2210, val loss: 0.5959855914115906
Epoch 2220, training loss: 839.0633544921875 = 0.5069283843040466 + 100.0 * 8.385563850402832
Epoch 2220, val loss: 0.5953942537307739
Epoch 2230, training loss: 838.9942016601562 = 0.5060864686965942 + 100.0 * 8.384881019592285
Epoch 2230, val loss: 0.5951977372169495
Epoch 2240, training loss: 838.9716186523438 = 0.5052577257156372 + 100.0 * 8.384663581848145
Epoch 2240, val loss: 0.594688892364502
Epoch 2250, training loss: 838.9747314453125 = 0.5044263601303101 + 100.0 * 8.384702682495117
Epoch 2250, val loss: 0.5942829847335815
Epoch 2260, training loss: 839.5755615234375 = 0.5035700798034668 + 100.0 * 8.39072036743164
Epoch 2260, val loss: 0.5940340161323547
Epoch 2270, training loss: 839.1375732421875 = 0.5026677846908569 + 100.0 * 8.386348724365234
Epoch 2270, val loss: 0.5934227108955383
Epoch 2280, training loss: 838.9334106445312 = 0.5017814040184021 + 100.0 * 8.384316444396973
Epoch 2280, val loss: 0.592988908290863
Epoch 2290, training loss: 838.8685913085938 = 0.500916063785553 + 100.0 * 8.383676528930664
Epoch 2290, val loss: 0.5924864411354065
Epoch 2300, training loss: 838.8793334960938 = 0.5000542998313904 + 100.0 * 8.383792877197266
Epoch 2300, val loss: 0.5920403599739075
Epoch 2310, training loss: 839.1672973632812 = 0.4991811513900757 + 100.0 * 8.386680603027344
Epoch 2310, val loss: 0.5914750695228577
Epoch 2320, training loss: 838.8657836914062 = 0.49825021624565125 + 100.0 * 8.383675575256348
Epoch 2320, val loss: 0.5914424061775208
Epoch 2330, training loss: 838.7815551757812 = 0.4973553419113159 + 100.0 * 8.382842063903809
Epoch 2330, val loss: 0.5908582806587219
Epoch 2340, training loss: 838.7822875976562 = 0.4964618980884552 + 100.0 * 8.382858276367188
Epoch 2340, val loss: 0.590387761592865
Epoch 2350, training loss: 839.2447509765625 = 0.4955592155456543 + 100.0 * 8.387492179870605
Epoch 2350, val loss: 0.5898624658584595
Epoch 2360, training loss: 838.8501586914062 = 0.4946138858795166 + 100.0 * 8.38355541229248
Epoch 2360, val loss: 0.5896600484848022
Epoch 2370, training loss: 838.7651977539062 = 0.4936768710613251 + 100.0 * 8.382715225219727
Epoch 2370, val loss: 0.589130699634552
Epoch 2380, training loss: 838.697265625 = 0.49276769161224365 + 100.0 * 8.382044792175293
Epoch 2380, val loss: 0.5886784791946411
Epoch 2390, training loss: 838.6700439453125 = 0.4918590486049652 + 100.0 * 8.381781578063965
Epoch 2390, val loss: 0.5882743000984192
Epoch 2400, training loss: 838.8163452148438 = 0.49094516038894653 + 100.0 * 8.383254051208496
Epoch 2400, val loss: 0.587827205657959
Epoch 2410, training loss: 838.7910766601562 = 0.48998793959617615 + 100.0 * 8.383010864257812
Epoch 2410, val loss: 0.5875974893569946
Epoch 2420, training loss: 838.7852172851562 = 0.4890133738517761 + 100.0 * 8.382962226867676
Epoch 2420, val loss: 0.5871748924255371
Epoch 2430, training loss: 838.6294555664062 = 0.4880483150482178 + 100.0 * 8.381414413452148
Epoch 2430, val loss: 0.5864800214767456
Epoch 2440, training loss: 838.5803833007812 = 0.4871021807193756 + 100.0 * 8.380932807922363
Epoch 2440, val loss: 0.5860456228256226
Epoch 2450, training loss: 838.5325317382812 = 0.4861510396003723 + 100.0 * 8.380463600158691
Epoch 2450, val loss: 0.5856848955154419
Epoch 2460, training loss: 838.5660400390625 = 0.48519596457481384 + 100.0 * 8.38080883026123
Epoch 2460, val loss: 0.5852642059326172
Epoch 2470, training loss: 839.0798950195312 = 0.48421818017959595 + 100.0 * 8.385956764221191
Epoch 2470, val loss: 0.5846907496452332
Epoch 2480, training loss: 838.9134521484375 = 0.48319941759109497 + 100.0 * 8.384302139282227
Epoch 2480, val loss: 0.5840207934379578
Epoch 2490, training loss: 838.5213623046875 = 0.482158362865448 + 100.0 * 8.380392074584961
Epoch 2490, val loss: 0.5839589238166809
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7559614408929477
0.8172136492066943
The final CL Acc:0.76509, 0.01151, The final GNN Acc:0.81695, 0.00038
raw graph: torch.Size([2, 88648])
add edge: torch.Size([2, 111036])
remove edge: torch.Size([2, 66422])
updated graph: torch.Size([2, 88810])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 1059.3262939453125 = 1.095129370689392 + 100.0 * 10.582311630249023
Epoch 0, val loss: 1.093775987625122
Epoch 10, training loss: 1059.2952880859375 = 1.0910671949386597 + 100.0 * 10.58204174041748
Epoch 10, val loss: 1.0897040367126465
Epoch 20, training loss: 1059.1751708984375 = 1.086680293083191 + 100.0 * 10.58088493347168
Epoch 20, val loss: 1.0853326320648193
Epoch 30, training loss: 1058.6568603515625 = 1.0819973945617676 + 100.0 * 10.575748443603516
Epoch 30, val loss: 1.0806645154953003
Epoch 40, training loss: 1056.298095703125 = 1.0768171548843384 + 100.0 * 10.552212715148926
Epoch 40, val loss: 1.075487494468689
Epoch 50, training loss: 1045.8973388671875 = 1.0705904960632324 + 100.0 * 10.448267936706543
Epoch 50, val loss: 1.0692533254623413
Epoch 60, training loss: 1005.8495483398438 = 1.0628843307495117 + 100.0 * 10.047866821289062
Epoch 60, val loss: 1.0615389347076416
Epoch 70, training loss: 964.6512451171875 = 1.0541646480560303 + 100.0 * 9.635971069335938
Epoch 70, val loss: 1.053113341331482
Epoch 80, training loss: 958.3612670898438 = 1.0474432706832886 + 100.0 * 9.573138236999512
Epoch 80, val loss: 1.0468205213546753
Epoch 90, training loss: 947.12353515625 = 1.0428708791732788 + 100.0 * 9.460806846618652
Epoch 90, val loss: 1.042604684829712
Epoch 100, training loss: 930.184814453125 = 1.0393610000610352 + 100.0 * 9.291454315185547
Epoch 100, val loss: 1.039318323135376
Epoch 110, training loss: 914.6509399414062 = 1.0358073711395264 + 100.0 * 9.136151313781738
Epoch 110, val loss: 1.0359258651733398
Epoch 120, training loss: 906.1444091796875 = 1.0317631959915161 + 100.0 * 9.051126480102539
Epoch 120, val loss: 1.0320446491241455
Epoch 130, training loss: 900.0756225585938 = 1.0274711847305298 + 100.0 * 8.99048137664795
Epoch 130, val loss: 1.027936577796936
Epoch 140, training loss: 897.4908447265625 = 1.0231125354766846 + 100.0 * 8.964676856994629
Epoch 140, val loss: 1.0237607955932617
Epoch 150, training loss: 895.1886596679688 = 1.018729567527771 + 100.0 * 8.941699028015137
Epoch 150, val loss: 1.0195817947387695
Epoch 160, training loss: 892.3656005859375 = 1.0144078731536865 + 100.0 * 8.913512229919434
Epoch 160, val loss: 1.015518307685852
Epoch 170, training loss: 888.443115234375 = 1.0104587078094482 + 100.0 * 8.874326705932617
Epoch 170, val loss: 1.0118869543075562
Epoch 180, training loss: 883.3096923828125 = 1.0071626901626587 + 100.0 * 8.823025703430176
Epoch 180, val loss: 1.0089226961135864
Epoch 190, training loss: 879.2598876953125 = 1.0042279958724976 + 100.0 * 8.782556533813477
Epoch 190, val loss: 1.0062172412872314
Epoch 200, training loss: 876.13720703125 = 1.0005234479904175 + 100.0 * 8.75136661529541
Epoch 200, val loss: 1.0026988983154297
Epoch 210, training loss: 872.8013916015625 = 0.9964174032211304 + 100.0 * 8.718050003051758
Epoch 210, val loss: 0.9989041686058044
Epoch 220, training loss: 869.5526733398438 = 0.9926180839538574 + 100.0 * 8.685600280761719
Epoch 220, val loss: 0.9954195022583008
Epoch 230, training loss: 866.9935302734375 = 0.9886021614074707 + 100.0 * 8.660049438476562
Epoch 230, val loss: 0.9916622042655945
Epoch 240, training loss: 864.6826782226562 = 0.9839058518409729 + 100.0 * 8.636987686157227
Epoch 240, val loss: 0.9872323870658875
Epoch 250, training loss: 862.7590942382812 = 0.9786459803581238 + 100.0 * 8.617804527282715
Epoch 250, val loss: 0.9822684526443481
Epoch 260, training loss: 861.3458251953125 = 0.9728990793228149 + 100.0 * 8.603729248046875
Epoch 260, val loss: 0.9768623113632202
Epoch 270, training loss: 860.184814453125 = 0.9666745066642761 + 100.0 * 8.592181205749512
Epoch 270, val loss: 0.9709712266921997
Epoch 280, training loss: 858.9137573242188 = 0.9601476192474365 + 100.0 * 8.579536437988281
Epoch 280, val loss: 0.9648320078849792
Epoch 290, training loss: 857.8265991210938 = 0.9533953666687012 + 100.0 * 8.568732261657715
Epoch 290, val loss: 0.9584814310073853
Epoch 300, training loss: 856.9125366210938 = 0.9463594555854797 + 100.0 * 8.559661865234375
Epoch 300, val loss: 0.9518912434577942
Epoch 310, training loss: 855.9296875 = 0.9388989806175232 + 100.0 * 8.549907684326172
Epoch 310, val loss: 0.94486403465271
Epoch 320, training loss: 854.9149780273438 = 0.9310984015464783 + 100.0 * 8.539838790893555
Epoch 320, val loss: 0.9375466108322144
Epoch 330, training loss: 854.1133422851562 = 0.9228771924972534 + 100.0 * 8.531905174255371
Epoch 330, val loss: 0.9298296570777893
Epoch 340, training loss: 853.5278930664062 = 0.9142456650733948 + 100.0 * 8.52613639831543
Epoch 340, val loss: 0.9217353463172913
Epoch 350, training loss: 853.0054931640625 = 0.9051893949508667 + 100.0 * 8.521002769470215
Epoch 350, val loss: 0.9131971001625061
Epoch 360, training loss: 852.2836303710938 = 0.8958263397216797 + 100.0 * 8.513877868652344
Epoch 360, val loss: 0.904456377029419
Epoch 370, training loss: 851.6990356445312 = 0.8863186836242676 + 100.0 * 8.508127212524414
Epoch 370, val loss: 0.8955726623535156
Epoch 380, training loss: 851.1492919921875 = 0.8767076730728149 + 100.0 * 8.502725601196289
Epoch 380, val loss: 0.8866220116615295
Epoch 390, training loss: 850.7767944335938 = 0.8670241236686707 + 100.0 * 8.49909782409668
Epoch 390, val loss: 0.8776108026504517
Epoch 400, training loss: 850.3634643554688 = 0.857309877872467 + 100.0 * 8.495061874389648
Epoch 400, val loss: 0.8685709834098816
Epoch 410, training loss: 849.62548828125 = 0.8475706577301025 + 100.0 * 8.487778663635254
Epoch 410, val loss: 0.859591543674469
Epoch 420, training loss: 849.08837890625 = 0.8378919959068298 + 100.0 * 8.482504844665527
Epoch 420, val loss: 0.8506240844726562
Epoch 430, training loss: 848.7855834960938 = 0.8282064199447632 + 100.0 * 8.479574203491211
Epoch 430, val loss: 0.841658353805542
Epoch 440, training loss: 848.38330078125 = 0.81843501329422 + 100.0 * 8.475648880004883
Epoch 440, val loss: 0.8326601982116699
Epoch 450, training loss: 847.8809814453125 = 0.8087826371192932 + 100.0 * 8.470722198486328
Epoch 450, val loss: 0.8237495422363281
Epoch 460, training loss: 847.4768676757812 = 0.7992017269134521 + 100.0 * 8.466776847839355
Epoch 460, val loss: 0.8149255514144897
Epoch 470, training loss: 847.1466064453125 = 0.7897098064422607 + 100.0 * 8.463568687438965
Epoch 470, val loss: 0.8062073588371277
Epoch 480, training loss: 847.2037963867188 = 0.780331552028656 + 100.0 * 8.464234352111816
Epoch 480, val loss: 0.7976315021514893
Epoch 490, training loss: 846.6442260742188 = 0.7710576057434082 + 100.0 * 8.458731651306152
Epoch 490, val loss: 0.7890834212303162
Epoch 500, training loss: 846.3587646484375 = 0.7619777321815491 + 100.0 * 8.455967903137207
Epoch 500, val loss: 0.7807867527008057
Epoch 510, training loss: 846.1742553710938 = 0.7531214356422424 + 100.0 * 8.454211235046387
Epoch 510, val loss: 0.7726974487304688
Epoch 520, training loss: 846.004150390625 = 0.7443756461143494 + 100.0 * 8.452597618103027
Epoch 520, val loss: 0.7647033333778381
Epoch 530, training loss: 845.7243041992188 = 0.7358243465423584 + 100.0 * 8.449884414672852
Epoch 530, val loss: 0.7569491863250732
Epoch 540, training loss: 845.55859375 = 0.7275108098983765 + 100.0 * 8.448310852050781
Epoch 540, val loss: 0.7493810057640076
Epoch 550, training loss: 845.4411010742188 = 0.7193198204040527 + 100.0 * 8.44721794128418
Epoch 550, val loss: 0.7419629693031311
Epoch 560, training loss: 845.1978149414062 = 0.7113187909126282 + 100.0 * 8.444865226745605
Epoch 560, val loss: 0.7347187399864197
Epoch 570, training loss: 844.9329833984375 = 0.7035565376281738 + 100.0 * 8.442294120788574
Epoch 570, val loss: 0.7277221083641052
Epoch 580, training loss: 844.7565307617188 = 0.6959916949272156 + 100.0 * 8.440605163574219
Epoch 580, val loss: 0.7209174036979675
Epoch 590, training loss: 844.9553833007812 = 0.6885889768600464 + 100.0 * 8.442667961120605
Epoch 590, val loss: 0.7142689228057861
Epoch 600, training loss: 844.33984375 = 0.68137127161026 + 100.0 * 8.43658447265625
Epoch 600, val loss: 0.7077917456626892
Epoch 610, training loss: 844.1841430664062 = 0.6743934154510498 + 100.0 * 8.435097694396973
Epoch 610, val loss: 0.7015517950057983
Epoch 620, training loss: 843.9735717773438 = 0.667597234249115 + 100.0 * 8.433059692382812
Epoch 620, val loss: 0.6955072283744812
Epoch 630, training loss: 843.851318359375 = 0.6610040664672852 + 100.0 * 8.431902885437012
Epoch 630, val loss: 0.6896438002586365
Epoch 640, training loss: 843.8348999023438 = 0.6545228362083435 + 100.0 * 8.431803703308105
Epoch 640, val loss: 0.683859646320343
Epoch 650, training loss: 843.4867553710938 = 0.6482346057891846 + 100.0 * 8.428384780883789
Epoch 650, val loss: 0.678310215473175
Epoch 660, training loss: 843.2784423828125 = 0.6421642303466797 + 100.0 * 8.426362991333008
Epoch 660, val loss: 0.6729559302330017
Epoch 670, training loss: 843.2242431640625 = 0.6362680196762085 + 100.0 * 8.42587947845459
Epoch 670, val loss: 0.6677746772766113
Epoch 680, training loss: 843.1204833984375 = 0.6304838061332703 + 100.0 * 8.42490005493164
Epoch 680, val loss: 0.6626557111740112
Epoch 690, training loss: 842.814697265625 = 0.6248947381973267 + 100.0 * 8.421897888183594
Epoch 690, val loss: 0.6577802896499634
Epoch 700, training loss: 842.6659545898438 = 0.6195095181465149 + 100.0 * 8.420464515686035
Epoch 700, val loss: 0.653100311756134
Epoch 710, training loss: 842.5191650390625 = 0.6143043637275696 + 100.0 * 8.419048309326172
Epoch 710, val loss: 0.6485885977745056
Epoch 720, training loss: 842.9769897460938 = 0.6092246770858765 + 100.0 * 8.423677444458008
Epoch 720, val loss: 0.6441920399665833
Epoch 730, training loss: 842.2592163085938 = 0.6042777895927429 + 100.0 * 8.416549682617188
Epoch 730, val loss: 0.6399112939834595
Epoch 740, training loss: 842.1349487304688 = 0.59955894947052 + 100.0 * 8.415353775024414
Epoch 740, val loss: 0.6358727812767029
Epoch 750, training loss: 841.9848022460938 = 0.5950090289115906 + 100.0 * 8.413897514343262
Epoch 750, val loss: 0.6319953799247742
Epoch 760, training loss: 842.2777709960938 = 0.5906050801277161 + 100.0 * 8.416872024536133
Epoch 760, val loss: 0.6282272934913635
Epoch 770, training loss: 841.8062133789062 = 0.5863127112388611 + 100.0 * 8.412199020385742
Epoch 770, val loss: 0.6245928406715393
Epoch 780, training loss: 841.5975341796875 = 0.5822234749794006 + 100.0 * 8.41015338897705
Epoch 780, val loss: 0.6211519241333008
Epoch 790, training loss: 841.4557495117188 = 0.5782942771911621 + 100.0 * 8.408774375915527
Epoch 790, val loss: 0.6178410649299622
Epoch 800, training loss: 841.3272094726562 = 0.5745201110839844 + 100.0 * 8.407526969909668
Epoch 800, val loss: 0.6147083044052124
Epoch 810, training loss: 841.3536987304688 = 0.5708727836608887 + 100.0 * 8.407828330993652
Epoch 810, val loss: 0.6116847991943359
Epoch 820, training loss: 841.460693359375 = 0.5672784447669983 + 100.0 * 8.408934593200684
Epoch 820, val loss: 0.6086542010307312
Epoch 830, training loss: 841.1283569335938 = 0.5638353824615479 + 100.0 * 8.405645370483398
Epoch 830, val loss: 0.6058266162872314
Epoch 840, training loss: 840.8964233398438 = 0.5606013536453247 + 100.0 * 8.403358459472656
Epoch 840, val loss: 0.6031737327575684
Epoch 850, training loss: 840.7630004882812 = 0.557522714138031 + 100.0 * 8.402054786682129
Epoch 850, val loss: 0.6006652116775513
Epoch 860, training loss: 840.63818359375 = 0.5545435547828674 + 100.0 * 8.400835990905762
Epoch 860, val loss: 0.5982497334480286
Epoch 870, training loss: 840.5305786132812 = 0.5516796708106995 + 100.0 * 8.399788856506348
Epoch 870, val loss: 0.5959356427192688
Epoch 880, training loss: 840.7361450195312 = 0.5489119291305542 + 100.0 * 8.401872634887695
Epoch 880, val loss: 0.5936959385871887
Epoch 890, training loss: 840.7686157226562 = 0.5461311936378479 + 100.0 * 8.40222454071045
Epoch 890, val loss: 0.5914637446403503
Epoch 900, training loss: 840.3823852539062 = 0.5435199737548828 + 100.0 * 8.398388862609863
Epoch 900, val loss: 0.5893810987472534
Epoch 910, training loss: 840.1952514648438 = 0.54105544090271 + 100.0 * 8.396541595458984
Epoch 910, val loss: 0.5874273180961609
Epoch 920, training loss: 840.0875854492188 = 0.5386894345283508 + 100.0 * 8.395488739013672
Epoch 920, val loss: 0.5855569839477539
Epoch 930, training loss: 840.4014892578125 = 0.536396324634552 + 100.0 * 8.398651123046875
Epoch 930, val loss: 0.5837368369102478
Epoch 940, training loss: 840.0729370117188 = 0.5340878367424011 + 100.0 * 8.39538860321045
Epoch 940, val loss: 0.5819387435913086
Epoch 950, training loss: 839.9080810546875 = 0.5319584608078003 + 100.0 * 8.393760681152344
Epoch 950, val loss: 0.5802765488624573
Epoch 960, training loss: 839.7634887695312 = 0.5298969745635986 + 100.0 * 8.392335891723633
Epoch 960, val loss: 0.5786935091018677
Epoch 970, training loss: 839.6936645507812 = 0.527919590473175 + 100.0 * 8.391657829284668
Epoch 970, val loss: 0.5771757364273071
Epoch 980, training loss: 840.275634765625 = 0.5259884595870972 + 100.0 * 8.397496223449707
Epoch 980, val loss: 0.5756906270980835
Epoch 990, training loss: 839.771728515625 = 0.5240270495414734 + 100.0 * 8.392477035522461
Epoch 990, val loss: 0.5741758942604065
Epoch 1000, training loss: 839.5182495117188 = 0.5222207307815552 + 100.0 * 8.389960289001465
Epoch 1000, val loss: 0.5728004574775696
Epoch 1010, training loss: 839.4114379882812 = 0.5205049514770508 + 100.0 * 8.388909339904785
Epoch 1010, val loss: 0.5715113878250122
Epoch 1020, training loss: 839.3458251953125 = 0.5188472867012024 + 100.0 * 8.388269424438477
Epoch 1020, val loss: 0.5702865719795227
Epoch 1030, training loss: 839.6138305664062 = 0.5172162055969238 + 100.0 * 8.390966415405273
Epoch 1030, val loss: 0.569022536277771
Epoch 1040, training loss: 839.5114135742188 = 0.5155596137046814 + 100.0 * 8.389958381652832
Epoch 1040, val loss: 0.567816436290741
Epoch 1050, training loss: 839.2011108398438 = 0.5140058398246765 + 100.0 * 8.386871337890625
Epoch 1050, val loss: 0.566670298576355
Epoch 1060, training loss: 839.112548828125 = 0.5125225186347961 + 100.0 * 8.386000633239746
Epoch 1060, val loss: 0.5655626058578491
Epoch 1070, training loss: 839.0847778320312 = 0.5110799670219421 + 100.0 * 8.385736465454102
Epoch 1070, val loss: 0.5645025968551636
Epoch 1080, training loss: 839.318115234375 = 0.5096430778503418 + 100.0 * 8.388084411621094
Epoch 1080, val loss: 0.5634328126907349
Epoch 1090, training loss: 839.0079956054688 = 0.5082380175590515 + 100.0 * 8.384997367858887
Epoch 1090, val loss: 0.5624016523361206
Epoch 1100, training loss: 838.88525390625 = 0.5068928003311157 + 100.0 * 8.383783340454102
Epoch 1100, val loss: 0.5614301562309265
Epoch 1110, training loss: 838.8115844726562 = 0.5055927634239197 + 100.0 * 8.38305950164795
Epoch 1110, val loss: 0.5604823231697083
Epoch 1120, training loss: 839.0776977539062 = 0.5043127536773682 + 100.0 * 8.385733604431152
Epoch 1120, val loss: 0.5595529675483704
Epoch 1130, training loss: 838.776611328125 = 0.5029972791671753 + 100.0 * 8.382736206054688
Epoch 1130, val loss: 0.5585827231407166
Epoch 1140, training loss: 838.7075805664062 = 0.5017615556716919 + 100.0 * 8.382058143615723
Epoch 1140, val loss: 0.5577110648155212
Epoch 1150, training loss: 838.5658569335938 = 0.5005839467048645 + 100.0 * 8.38065242767334
Epoch 1150, val loss: 0.5568689703941345
Epoch 1160, training loss: 838.5166015625 = 0.4994335174560547 + 100.0 * 8.380171775817871
Epoch 1160, val loss: 0.5560578107833862
Epoch 1170, training loss: 838.920654296875 = 0.49828577041625977 + 100.0 * 8.384223937988281
Epoch 1170, val loss: 0.5552310347557068
Epoch 1180, training loss: 838.5938110351562 = 0.49709343910217285 + 100.0 * 8.380967140197754
Epoch 1180, val loss: 0.5543670654296875
Epoch 1190, training loss: 838.399169921875 = 0.49598538875579834 + 100.0 * 8.379032135009766
Epoch 1190, val loss: 0.553626298904419
Epoch 1200, training loss: 838.2808227539062 = 0.49490901827812195 + 100.0 * 8.377859115600586
Epoch 1200, val loss: 0.5528785586357117
Epoch 1210, training loss: 838.2462158203125 = 0.493867963552475 + 100.0 * 8.377523422241211
Epoch 1210, val loss: 0.5521728992462158
Epoch 1220, training loss: 838.5092163085938 = 0.4928148090839386 + 100.0 * 8.38016414642334
Epoch 1220, val loss: 0.5514613389968872
Epoch 1230, training loss: 838.4510498046875 = 0.49174654483795166 + 100.0 * 8.379592895507812
Epoch 1230, val loss: 0.5506861209869385
Epoch 1240, training loss: 838.1544799804688 = 0.49070724844932556 + 100.0 * 8.37663745880127
Epoch 1240, val loss: 0.5499742031097412
Epoch 1250, training loss: 837.97216796875 = 0.48972031474113464 + 100.0 * 8.374824523925781
Epoch 1250, val loss: 0.5493055582046509
Epoch 1260, training loss: 837.9718017578125 = 0.4887661933898926 + 100.0 * 8.37483024597168
Epoch 1260, val loss: 0.5486796498298645
Epoch 1270, training loss: 838.426025390625 = 0.48778626322746277 + 100.0 * 8.379382133483887
Epoch 1270, val loss: 0.5479925274848938
Epoch 1280, training loss: 837.9869995117188 = 0.4867929220199585 + 100.0 * 8.375001907348633
Epoch 1280, val loss: 0.5473286509513855
Epoch 1290, training loss: 837.8270263671875 = 0.48584941029548645 + 100.0 * 8.373412132263184
Epoch 1290, val loss: 0.5467175841331482
Epoch 1300, training loss: 837.7265014648438 = 0.4849529564380646 + 100.0 * 8.372415542602539
Epoch 1300, val loss: 0.5461094379425049
Epoch 1310, training loss: 837.8132934570312 = 0.484054833650589 + 100.0 * 8.373291969299316
Epoch 1310, val loss: 0.5455313324928284
Epoch 1320, training loss: 837.6686401367188 = 0.4830944240093231 + 100.0 * 8.371855735778809
Epoch 1320, val loss: 0.544857919216156
Epoch 1330, training loss: 837.6188354492188 = 0.4821801483631134 + 100.0 * 8.371366500854492
Epoch 1330, val loss: 0.5442756414413452
Epoch 1340, training loss: 837.616943359375 = 0.48130056262016296 + 100.0 * 8.371356010437012
Epoch 1340, val loss: 0.5437058806419373
Epoch 1350, training loss: 837.6666259765625 = 0.48042798042297363 + 100.0 * 8.371862411499023
Epoch 1350, val loss: 0.5431244373321533
Epoch 1360, training loss: 837.4078979492188 = 0.47956955432891846 + 100.0 * 8.369283676147461
Epoch 1360, val loss: 0.5425719618797302
Epoch 1370, training loss: 837.3873901367188 = 0.47873204946517944 + 100.0 * 8.369086265563965
Epoch 1370, val loss: 0.542033314704895
Epoch 1380, training loss: 837.5540771484375 = 0.47789278626441956 + 100.0 * 8.37076187133789
Epoch 1380, val loss: 0.5415047407150269
Epoch 1390, training loss: 837.4586791992188 = 0.47700682282447815 + 100.0 * 8.369816780090332
Epoch 1390, val loss: 0.5409195423126221
Epoch 1400, training loss: 837.5204467773438 = 0.476162850856781 + 100.0 * 8.370443344116211
Epoch 1400, val loss: 0.5403658747673035
Epoch 1410, training loss: 837.240478515625 = 0.475269615650177 + 100.0 * 8.36765193939209
Epoch 1410, val loss: 0.5397394895553589
Epoch 1420, training loss: 837.1375122070312 = 0.4744493365287781 + 100.0 * 8.366630554199219
Epoch 1420, val loss: 0.5392459034919739
Epoch 1430, training loss: 837.0546875 = 0.4736347198486328 + 100.0 * 8.36581039428711
Epoch 1430, val loss: 0.5387146472930908
Epoch 1440, training loss: 837.02001953125 = 0.47283869981765747 + 100.0 * 8.365471839904785
Epoch 1440, val loss: 0.5382511019706726
Epoch 1450, training loss: 837.2791137695312 = 0.4720209240913391 + 100.0 * 8.368070602416992
Epoch 1450, val loss: 0.5377370119094849
Epoch 1460, training loss: 837.1166381835938 = 0.4711558520793915 + 100.0 * 8.366455078125
Epoch 1460, val loss: 0.5371336340904236
Epoch 1470, training loss: 836.9493408203125 = 0.4703297019004822 + 100.0 * 8.364789962768555
Epoch 1470, val loss: 0.536658763885498
Epoch 1480, training loss: 836.8683471679688 = 0.469542920589447 + 100.0 * 8.363987922668457
Epoch 1480, val loss: 0.5361630916595459
Epoch 1490, training loss: 837.0780029296875 = 0.46873748302459717 + 100.0 * 8.366092681884766
Epoch 1490, val loss: 0.5356433987617493
Epoch 1500, training loss: 836.8301391601562 = 0.46791815757751465 + 100.0 * 8.363622665405273
Epoch 1500, val loss: 0.5351616740226746
Epoch 1510, training loss: 836.7008666992188 = 0.467133492231369 + 100.0 * 8.362337112426758
Epoch 1510, val loss: 0.5347043871879578
Epoch 1520, training loss: 836.6807250976562 = 0.4663618803024292 + 100.0 * 8.362143516540527
Epoch 1520, val loss: 0.5342350006103516
Epoch 1530, training loss: 837.3119506835938 = 0.46556156873703003 + 100.0 * 8.368463516235352
Epoch 1530, val loss: 0.5337380766868591
Epoch 1540, training loss: 836.7892456054688 = 0.46471959352493286 + 100.0 * 8.363245010375977
Epoch 1540, val loss: 0.533262312412262
Epoch 1550, training loss: 836.599609375 = 0.4639336168766022 + 100.0 * 8.361356735229492
Epoch 1550, val loss: 0.5328001379966736
Epoch 1560, training loss: 836.54443359375 = 0.4631747007369995 + 100.0 * 8.360812187194824
Epoch 1560, val loss: 0.5323859453201294
Epoch 1570, training loss: 836.6328125 = 0.46240153908729553 + 100.0 * 8.361703872680664
Epoch 1570, val loss: 0.5319507718086243
Epoch 1580, training loss: 836.4270629882812 = 0.4615989625453949 + 100.0 * 8.359654426574707
Epoch 1580, val loss: 0.5314925909042358
Epoch 1590, training loss: 836.3779296875 = 0.46083253622055054 + 100.0 * 8.359170913696289
Epoch 1590, val loss: 0.5310883522033691
Epoch 1600, training loss: 836.3655395507812 = 0.46006110310554504 + 100.0 * 8.359054565429688
Epoch 1600, val loss: 0.5306726694107056
Epoch 1610, training loss: 836.7847290039062 = 0.45926225185394287 + 100.0 * 8.36325454711914
Epoch 1610, val loss: 0.5302415490150452
Epoch 1620, training loss: 836.4324340820312 = 0.4584384560585022 + 100.0 * 8.359740257263184
Epoch 1620, val loss: 0.5297379493713379
Epoch 1630, training loss: 836.3102416992188 = 0.4576523005962372 + 100.0 * 8.358526229858398
Epoch 1630, val loss: 0.5293492078781128
Epoch 1640, training loss: 836.3858032226562 = 0.45688381791114807 + 100.0 * 8.359289169311523
Epoch 1640, val loss: 0.5289288759231567
Epoch 1650, training loss: 836.4393310546875 = 0.45607638359069824 + 100.0 * 8.359832763671875
Epoch 1650, val loss: 0.5284774899482727
Epoch 1660, training loss: 836.1672973632812 = 0.45523741841316223 + 100.0 * 8.357120513916016
Epoch 1660, val loss: 0.5279721021652222
Epoch 1670, training loss: 836.1834106445312 = 0.4544266164302826 + 100.0 * 8.357290267944336
Epoch 1670, val loss: 0.5275489687919617
Epoch 1680, training loss: 836.0493774414062 = 0.4536677598953247 + 100.0 * 8.35595703125
Epoch 1680, val loss: 0.5271464586257935
Epoch 1690, training loss: 835.99853515625 = 0.45290201902389526 + 100.0 * 8.355456352233887
Epoch 1690, val loss: 0.5267472863197327
Epoch 1700, training loss: 835.9654541015625 = 0.45214030146598816 + 100.0 * 8.355133056640625
Epoch 1700, val loss: 0.5263857841491699
Epoch 1710, training loss: 835.9437866210938 = 0.4513664245605469 + 100.0 * 8.354924201965332
Epoch 1710, val loss: 0.5260025858879089
Epoch 1720, training loss: 836.5469360351562 = 0.45058760046958923 + 100.0 * 8.360963821411133
Epoch 1720, val loss: 0.5256383419036865
Epoch 1730, training loss: 836.3062133789062 = 0.4496786892414093 + 100.0 * 8.358565330505371
Epoch 1730, val loss: 0.5249927639961243
Epoch 1740, training loss: 835.8864135742188 = 0.44884198904037476 + 100.0 * 8.354375839233398
Epoch 1740, val loss: 0.5246086716651917
Epoch 1750, training loss: 835.8760375976562 = 0.4480801224708557 + 100.0 * 8.354279518127441
Epoch 1750, val loss: 0.5242481827735901
Epoch 1760, training loss: 835.7987060546875 = 0.44730693101882935 + 100.0 * 8.353513717651367
Epoch 1760, val loss: 0.52388995885849
Epoch 1770, training loss: 835.7639770507812 = 0.44652923941612244 + 100.0 * 8.353174209594727
Epoch 1770, val loss: 0.5234675407409668
Epoch 1780, training loss: 835.7437744140625 = 0.4457437992095947 + 100.0 * 8.352980613708496
Epoch 1780, val loss: 0.5230956077575684
Epoch 1790, training loss: 836.1181640625 = 0.44494566321372986 + 100.0 * 8.356732368469238
Epoch 1790, val loss: 0.5226881504058838
Epoch 1800, training loss: 835.8639526367188 = 0.44405996799468994 + 100.0 * 8.354199409484863
Epoch 1800, val loss: 0.5221718549728394
Epoch 1810, training loss: 835.9132690429688 = 0.44323721528053284 + 100.0 * 8.354700088500977
Epoch 1810, val loss: 0.5217636823654175
Epoch 1820, training loss: 835.6695556640625 = 0.44240331649780273 + 100.0 * 8.352272033691406
Epoch 1820, val loss: 0.521337628364563
Epoch 1830, training loss: 835.6044311523438 = 0.44160476326942444 + 100.0 * 8.351628303527832
Epoch 1830, val loss: 0.5209569931030273
Epoch 1840, training loss: 835.6723022460938 = 0.4408033788204193 + 100.0 * 8.352314949035645
Epoch 1840, val loss: 0.5205659866333008
Epoch 1850, training loss: 835.9834594726562 = 0.43995627760887146 + 100.0 * 8.355435371398926
Epoch 1850, val loss: 0.5201098322868347
Epoch 1860, training loss: 835.6626586914062 = 0.4391000270843506 + 100.0 * 8.352235794067383
Epoch 1860, val loss: 0.519700288772583
Epoch 1870, training loss: 835.5601196289062 = 0.4382726848125458 + 100.0 * 8.351218223571777
Epoch 1870, val loss: 0.5192790627479553
Epoch 1880, training loss: 835.667236328125 = 0.4374490976333618 + 100.0 * 8.35229778289795
Epoch 1880, val loss: 0.5188934206962585
Epoch 1890, training loss: 835.5123291015625 = 0.43659400939941406 + 100.0 * 8.350757598876953
Epoch 1890, val loss: 0.5184856057167053
Epoch 1900, training loss: 835.4288940429688 = 0.43576765060424805 + 100.0 * 8.349930763244629
Epoch 1900, val loss: 0.5181189179420471
Epoch 1910, training loss: 835.4945678710938 = 0.43493223190307617 + 100.0 * 8.35059642791748
Epoch 1910, val loss: 0.5177081227302551
Epoch 1920, training loss: 835.6432495117188 = 0.434067964553833 + 100.0 * 8.352091789245605
Epoch 1920, val loss: 0.5172977447509766
Epoch 1930, training loss: 835.6414794921875 = 0.4331859350204468 + 100.0 * 8.352083206176758
Epoch 1930, val loss: 0.5168229341506958
Epoch 1940, training loss: 835.4409790039062 = 0.43227726221084595 + 100.0 * 8.35008716583252
Epoch 1940, val loss: 0.5163546204566956
Epoch 1950, training loss: 835.3429565429688 = 0.4313977360725403 + 100.0 * 8.349115371704102
Epoch 1950, val loss: 0.5159602761268616
Epoch 1960, training loss: 835.3043212890625 = 0.4305453598499298 + 100.0 * 8.348737716674805
Epoch 1960, val loss: 0.5155506134033203
Epoch 1970, training loss: 835.2799072265625 = 0.4296925365924835 + 100.0 * 8.348502159118652
Epoch 1970, val loss: 0.5151938199996948
Epoch 1980, training loss: 835.4657592773438 = 0.4288160800933838 + 100.0 * 8.350369453430176
Epoch 1980, val loss: 0.5147671699523926
Epoch 1990, training loss: 835.307861328125 = 0.42790302634239197 + 100.0 * 8.348799705505371
Epoch 1990, val loss: 0.5143090486526489
Epoch 2000, training loss: 835.1943359375 = 0.42698270082473755 + 100.0 * 8.347673416137695
Epoch 2000, val loss: 0.5138459801673889
Epoch 2010, training loss: 835.174560546875 = 0.42609894275665283 + 100.0 * 8.347484588623047
Epoch 2010, val loss: 0.5134603977203369
Epoch 2020, training loss: 835.341064453125 = 0.42521703243255615 + 100.0 * 8.34915828704834
Epoch 2020, val loss: 0.5130432844161987
Epoch 2030, training loss: 835.2222900390625 = 0.42425093054771423 + 100.0 * 8.347980499267578
Epoch 2030, val loss: 0.5124917030334473
Epoch 2040, training loss: 835.11962890625 = 0.42331093549728394 + 100.0 * 8.346962928771973
Epoch 2040, val loss: 0.5120753049850464
Epoch 2050, training loss: 835.1279907226562 = 0.4223930835723877 + 100.0 * 8.34705638885498
Epoch 2050, val loss: 0.5115529894828796
Epoch 2060, training loss: 835.3681640625 = 0.4214633107185364 + 100.0 * 8.349467277526855
Epoch 2060, val loss: 0.5111140012741089
Epoch 2070, training loss: 835.0283203125 = 0.4205058515071869 + 100.0 * 8.346077919006348
Epoch 2070, val loss: 0.5106934905052185
Epoch 2080, training loss: 835.0004272460938 = 0.4195708930492401 + 100.0 * 8.345808982849121
Epoch 2080, val loss: 0.5102140307426453
Epoch 2090, training loss: 834.9808349609375 = 0.41864317655563354 + 100.0 * 8.345622062683105
Epoch 2090, val loss: 0.5097974538803101
Epoch 2100, training loss: 835.0838623046875 = 0.4176980257034302 + 100.0 * 8.346661567687988
Epoch 2100, val loss: 0.5093194842338562
Epoch 2110, training loss: 835.1597900390625 = 0.41671833395957947 + 100.0 * 8.347430229187012
Epoch 2110, val loss: 0.5088685750961304
Epoch 2120, training loss: 835.0432739257812 = 0.4157143235206604 + 100.0 * 8.346275329589844
Epoch 2120, val loss: 0.5083680748939514
Epoch 2130, training loss: 835.06640625 = 0.41471067070961 + 100.0 * 8.346516609191895
Epoch 2130, val loss: 0.507843017578125
Epoch 2140, training loss: 834.8770751953125 = 0.41371965408325195 + 100.0 * 8.344634056091309
Epoch 2140, val loss: 0.507388174533844
Epoch 2150, training loss: 834.841796875 = 0.41274163126945496 + 100.0 * 8.344290733337402
Epoch 2150, val loss: 0.5069034695625305
Epoch 2160, training loss: 834.7850341796875 = 0.4117538034915924 + 100.0 * 8.343732833862305
Epoch 2160, val loss: 0.5064742565155029
Epoch 2170, training loss: 834.76611328125 = 0.4107552170753479 + 100.0 * 8.34355354309082
Epoch 2170, val loss: 0.5060003995895386
Epoch 2180, training loss: 834.91015625 = 0.40973934531211853 + 100.0 * 8.345004081726074
Epoch 2180, val loss: 0.5054721832275391
Epoch 2190, training loss: 835.0628051757812 = 0.4086828827857971 + 100.0 * 8.346541404724121
Epoch 2190, val loss: 0.5050023794174194
Epoch 2200, training loss: 834.8278198242188 = 0.407578706741333 + 100.0 * 8.344202041625977
Epoch 2200, val loss: 0.5044154524803162
Epoch 2210, training loss: 834.7393188476562 = 0.4065572917461395 + 100.0 * 8.343327522277832
Epoch 2210, val loss: 0.5039800405502319
Epoch 2220, training loss: 834.6599731445312 = 0.4055313766002655 + 100.0 * 8.342544555664062
Epoch 2220, val loss: 0.5035049319267273
Epoch 2230, training loss: 834.6485595703125 = 0.40449658036231995 + 100.0 * 8.342440605163574
Epoch 2230, val loss: 0.5030272006988525
Epoch 2240, training loss: 834.9581909179688 = 0.40345582365989685 + 100.0 * 8.345547676086426
Epoch 2240, val loss: 0.5025840997695923
Epoch 2250, training loss: 834.9921875 = 0.4023212492465973 + 100.0 * 8.345898628234863
Epoch 2250, val loss: 0.5019964575767517
Epoch 2260, training loss: 834.7047729492188 = 0.4011961817741394 + 100.0 * 8.343035697937012
Epoch 2260, val loss: 0.5014179348945618
Epoch 2270, training loss: 834.6090087890625 = 0.40011611580848694 + 100.0 * 8.34208869934082
Epoch 2270, val loss: 0.5009536743164062
Epoch 2280, training loss: 834.5372314453125 = 0.39905375242233276 + 100.0 * 8.341382026672363
Epoch 2280, val loss: 0.5004134774208069
Epoch 2290, training loss: 834.5232543945312 = 0.39798638224601746 + 100.0 * 8.341252326965332
Epoch 2290, val loss: 0.4999884068965912
Epoch 2300, training loss: 834.6427001953125 = 0.3968885838985443 + 100.0 * 8.34245777130127
Epoch 2300, val loss: 0.4994303584098816
Epoch 2310, training loss: 834.7593994140625 = 0.39574968814849854 + 100.0 * 8.343636512756348
Epoch 2310, val loss: 0.4988994896411896
Epoch 2320, training loss: 834.6011962890625 = 0.39459773898124695 + 100.0 * 8.342065811157227
Epoch 2320, val loss: 0.4983447790145874
Epoch 2330, training loss: 834.5325317382812 = 0.39347681403160095 + 100.0 * 8.341390609741211
Epoch 2330, val loss: 0.49783894419670105
Epoch 2340, training loss: 834.7479858398438 = 0.3923320770263672 + 100.0 * 8.34355640411377
Epoch 2340, val loss: 0.4972984790802002
Epoch 2350, training loss: 834.4649047851562 = 0.39116933941841125 + 100.0 * 8.340737342834473
Epoch 2350, val loss: 0.4966922700405121
Epoch 2360, training loss: 834.36181640625 = 0.3900231420993805 + 100.0 * 8.339717864990234
Epoch 2360, val loss: 0.4961923062801361
Epoch 2370, training loss: 834.371337890625 = 0.3888763189315796 + 100.0 * 8.339824676513672
Epoch 2370, val loss: 0.49565717577934265
Epoch 2380, training loss: 834.3624267578125 = 0.387716144323349 + 100.0 * 8.339747428894043
Epoch 2380, val loss: 0.49511581659317017
Epoch 2390, training loss: 834.953125 = 0.38653305172920227 + 100.0 * 8.34566593170166
Epoch 2390, val loss: 0.4945487678050995
Epoch 2400, training loss: 834.5048217773438 = 0.3852861821651459 + 100.0 * 8.341195106506348
Epoch 2400, val loss: 0.49394506216049194
Epoch 2410, training loss: 834.443603515625 = 0.38407567143440247 + 100.0 * 8.340595245361328
Epoch 2410, val loss: 0.49338048696517944
Epoch 2420, training loss: 834.4193115234375 = 0.3828640282154083 + 100.0 * 8.340364456176758
Epoch 2420, val loss: 0.49282872676849365
Epoch 2430, training loss: 834.2808837890625 = 0.38164567947387695 + 100.0 * 8.33899211883545
Epoch 2430, val loss: 0.49227285385131836
Epoch 2440, training loss: 834.3763427734375 = 0.3804227411746979 + 100.0 * 8.339959144592285
Epoch 2440, val loss: 0.491670161485672
Epoch 2450, training loss: 834.3274536132812 = 0.3791601061820984 + 100.0 * 8.339483261108398
Epoch 2450, val loss: 0.4911361336708069
Epoch 2460, training loss: 834.2605590820312 = 0.37789833545684814 + 100.0 * 8.338827133178711
Epoch 2460, val loss: 0.49055323004722595
Epoch 2470, training loss: 834.2036743164062 = 0.37665367126464844 + 100.0 * 8.33827018737793
Epoch 2470, val loss: 0.48996856808662415
Epoch 2480, training loss: 834.5169677734375 = 0.37540754675865173 + 100.0 * 8.341415405273438
Epoch 2480, val loss: 0.4894185960292816
Epoch 2490, training loss: 834.1452026367188 = 0.3740827739238739 + 100.0 * 8.337711334228516
Epoch 2490, val loss: 0.48876628279685974
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.808726534753932
0.8637977251322178
=== training gcn model ===
Epoch 0, training loss: 1059.34814453125 = 1.1208926439285278 + 100.0 * 10.58227252960205
Epoch 0, val loss: 1.1217302083969116
Epoch 10, training loss: 1059.302734375 = 1.1159648895263672 + 100.0 * 10.581868171691895
Epoch 10, val loss: 1.1168112754821777
Epoch 20, training loss: 1059.1168212890625 = 1.110609769821167 + 100.0 * 10.580061912536621
Epoch 20, val loss: 1.1114734411239624
Epoch 30, training loss: 1058.2933349609375 = 1.104830265045166 + 100.0 * 10.571884155273438
Epoch 30, val loss: 1.1057103872299194
Epoch 40, training loss: 1055.1790771484375 = 1.0985430479049683 + 100.0 * 10.54080581665039
Epoch 40, val loss: 1.0994136333465576
Epoch 50, training loss: 1046.7740478515625 = 1.0913273096084595 + 100.0 * 10.456827163696289
Epoch 50, val loss: 1.092151403427124
Epoch 60, training loss: 1029.266357421875 = 1.0836424827575684 + 100.0 * 10.281826972961426
Epoch 60, val loss: 1.084510087966919
Epoch 70, training loss: 999.5423583984375 = 1.0755685567855835 + 100.0 * 9.984667778015137
Epoch 70, val loss: 1.0763216018676758
Epoch 80, training loss: 949.6994018554688 = 1.067320704460144 + 100.0 * 9.486320495605469
Epoch 80, val loss: 1.0681520700454712
Epoch 90, training loss: 933.6058959960938 = 1.0605558156967163 + 100.0 * 9.325453758239746
Epoch 90, val loss: 1.0614641904830933
Epoch 100, training loss: 922.5886840820312 = 1.0543442964553833 + 100.0 * 9.215343475341797
Epoch 100, val loss: 1.0553467273712158
Epoch 110, training loss: 917.9481201171875 = 1.0481938123703003 + 100.0 * 9.168998718261719
Epoch 110, val loss: 1.049328088760376
Epoch 120, training loss: 914.5775756835938 = 1.0425258874893188 + 100.0 * 9.135350227355957
Epoch 120, val loss: 1.0437922477722168
Epoch 130, training loss: 910.8966674804688 = 1.0374321937561035 + 100.0 * 9.098592758178711
Epoch 130, val loss: 1.0388011932373047
Epoch 140, training loss: 906.725341796875 = 1.032826542854309 + 100.0 * 9.056924819946289
Epoch 140, val loss: 1.034265398979187
Epoch 150, training loss: 902.0516357421875 = 1.028653860092163 + 100.0 * 9.01023006439209
Epoch 150, val loss: 1.0301322937011719
Epoch 160, training loss: 897.8851928710938 = 1.0247634649276733 + 100.0 * 8.96860408782959
Epoch 160, val loss: 1.0262070894241333
Epoch 170, training loss: 895.24462890625 = 1.0208499431610107 + 100.0 * 8.942237854003906
Epoch 170, val loss: 1.0222351551055908
Epoch 180, training loss: 892.9383544921875 = 1.0168384313583374 + 100.0 * 8.919215202331543
Epoch 180, val loss: 1.0181832313537598
Epoch 190, training loss: 889.885009765625 = 1.012938141822815 + 100.0 * 8.888720512390137
Epoch 190, val loss: 1.014319896697998
Epoch 200, training loss: 886.465576171875 = 1.0095322132110596 + 100.0 * 8.854560852050781
Epoch 200, val loss: 1.0110160112380981
Epoch 210, training loss: 883.7548828125 = 1.0066286325454712 + 100.0 * 8.827482223510742
Epoch 210, val loss: 1.0081782341003418
Epoch 220, training loss: 882.223388671875 = 1.0035332441329956 + 100.0 * 8.812198638916016
Epoch 220, val loss: 1.0050745010375977
Epoch 230, training loss: 880.9957885742188 = 0.9993463754653931 + 100.0 * 8.799964904785156
Epoch 230, val loss: 1.000874400138855
Epoch 240, training loss: 879.4812622070312 = 0.9944291710853577 + 100.0 * 8.784868240356445
Epoch 240, val loss: 0.9960854053497314
Epoch 250, training loss: 877.5763549804688 = 0.9899401664733887 + 100.0 * 8.765864372253418
Epoch 250, val loss: 0.9917904138565063
Epoch 260, training loss: 875.2507934570312 = 0.9860674738883972 + 100.0 * 8.742647171020508
Epoch 260, val loss: 0.9880725741386414
Epoch 270, training loss: 873.1256713867188 = 0.9824731945991516 + 100.0 * 8.721431732177734
Epoch 270, val loss: 0.9845550060272217
Epoch 280, training loss: 872.1621704101562 = 0.9782410264015198 + 100.0 * 8.711838722229004
Epoch 280, val loss: 0.9803591966629028
Epoch 290, training loss: 870.9965209960938 = 0.9729463458061218 + 100.0 * 8.700235366821289
Epoch 290, val loss: 0.9749695658683777
Epoch 300, training loss: 869.8279418945312 = 0.9675960540771484 + 100.0 * 8.688603401184082
Epoch 300, val loss: 0.9697603583335876
Epoch 310, training loss: 868.4530639648438 = 0.9625748991966248 + 100.0 * 8.674904823303223
Epoch 310, val loss: 0.9649139642715454
Epoch 320, training loss: 866.7744140625 = 0.9578889012336731 + 100.0 * 8.658164978027344
Epoch 320, val loss: 0.9604005813598633
Epoch 330, training loss: 864.8073120117188 = 0.953514575958252 + 100.0 * 8.638538360595703
Epoch 330, val loss: 0.9561337232589722
Epoch 340, training loss: 862.9651489257812 = 0.948961615562439 + 100.0 * 8.620162010192871
Epoch 340, val loss: 0.9516323804855347
Epoch 350, training loss: 861.2136840820312 = 0.943950891494751 + 100.0 * 8.602697372436523
Epoch 350, val loss: 0.9467217922210693
Epoch 360, training loss: 859.6563720703125 = 0.9384514689445496 + 100.0 * 8.587179183959961
Epoch 360, val loss: 0.941275417804718
Epoch 370, training loss: 858.5352783203125 = 0.9325990080833435 + 100.0 * 8.576026916503906
Epoch 370, val loss: 0.9354391098022461
Epoch 380, training loss: 857.114990234375 = 0.9261441230773926 + 100.0 * 8.561888694763184
Epoch 380, val loss: 0.9291492104530334
Epoch 390, training loss: 856.1372680664062 = 0.9193151593208313 + 100.0 * 8.552179336547852
Epoch 390, val loss: 0.9224619269371033
Epoch 400, training loss: 855.4661254882812 = 0.9119053483009338 + 100.0 * 8.545541763305664
Epoch 400, val loss: 0.9152073860168457
Epoch 410, training loss: 854.5469970703125 = 0.9041755795478821 + 100.0 * 8.536428451538086
Epoch 410, val loss: 0.9076799750328064
Epoch 420, training loss: 853.8998413085938 = 0.8961674571037292 + 100.0 * 8.530036926269531
Epoch 420, val loss: 0.8998892903327942
Epoch 430, training loss: 853.1412963867188 = 0.8879651427268982 + 100.0 * 8.522533416748047
Epoch 430, val loss: 0.8919182419776917
Epoch 440, training loss: 852.4170532226562 = 0.8796180486679077 + 100.0 * 8.515374183654785
Epoch 440, val loss: 0.883841335773468
Epoch 450, training loss: 851.9013061523438 = 0.871010959148407 + 100.0 * 8.510302543640137
Epoch 450, val loss: 0.8755142092704773
Epoch 460, training loss: 851.3131713867188 = 0.8621662259101868 + 100.0 * 8.504509925842285
Epoch 460, val loss: 0.8669397234916687
Epoch 470, training loss: 850.8394165039062 = 0.8531651496887207 + 100.0 * 8.499862670898438
Epoch 470, val loss: 0.8582831025123596
Epoch 480, training loss: 850.39453125 = 0.8439896702766418 + 100.0 * 8.495505332946777
Epoch 480, val loss: 0.8494617938995361
Epoch 490, training loss: 850.0398559570312 = 0.8347013592720032 + 100.0 * 8.492051124572754
Epoch 490, val loss: 0.840535044670105
Epoch 500, training loss: 849.6915893554688 = 0.8253100514411926 + 100.0 * 8.488662719726562
Epoch 500, val loss: 0.8315507769584656
Epoch 510, training loss: 849.5626831054688 = 0.8158969283103943 + 100.0 * 8.487467765808105
Epoch 510, val loss: 0.8225738406181335
Epoch 520, training loss: 849.0693359375 = 0.806484043598175 + 100.0 * 8.48262882232666
Epoch 520, val loss: 0.8135485649108887
Epoch 530, training loss: 848.7151489257812 = 0.7971832752227783 + 100.0 * 8.479179382324219
Epoch 530, val loss: 0.8046813011169434
Epoch 540, training loss: 848.5186767578125 = 0.7879440784454346 + 100.0 * 8.477307319641113
Epoch 540, val loss: 0.7959045171737671
Epoch 550, training loss: 847.9829711914062 = 0.7788615226745605 + 100.0 * 8.472041130065918
Epoch 550, val loss: 0.7872809767723083
Epoch 560, training loss: 847.6253051757812 = 0.7699650526046753 + 100.0 * 8.46855354309082
Epoch 560, val loss: 0.7788431644439697
Epoch 570, training loss: 847.292236328125 = 0.7612096071243286 + 100.0 * 8.465310096740723
Epoch 570, val loss: 0.7705875635147095
Epoch 580, training loss: 847.1258544921875 = 0.7524847984313965 + 100.0 * 8.463733673095703
Epoch 580, val loss: 0.7623175978660583
Epoch 590, training loss: 846.56494140625 = 0.7440006136894226 + 100.0 * 8.458209037780762
Epoch 590, val loss: 0.7543760538101196
Epoch 600, training loss: 846.1572875976562 = 0.7357644438743591 + 100.0 * 8.454215049743652
Epoch 600, val loss: 0.7466539144515991
Epoch 610, training loss: 845.8351440429688 = 0.7276787161827087 + 100.0 * 8.451074600219727
Epoch 610, val loss: 0.739104151725769
Epoch 620, training loss: 845.6532592773438 = 0.7196676731109619 + 100.0 * 8.449336051940918
Epoch 620, val loss: 0.7316251397132874
Epoch 630, training loss: 845.2669067382812 = 0.7118058800697327 + 100.0 * 8.445550918579102
Epoch 630, val loss: 0.7243897318840027
Epoch 640, training loss: 845.0458374023438 = 0.7042663097381592 + 100.0 * 8.443415641784668
Epoch 640, val loss: 0.7174212336540222
Epoch 650, training loss: 844.9606323242188 = 0.6969426870346069 + 100.0 * 8.442636489868164
Epoch 650, val loss: 0.7106955647468567
Epoch 660, training loss: 844.6844482421875 = 0.6897278428077698 + 100.0 * 8.439947128295898
Epoch 660, val loss: 0.7040039300918579
Epoch 670, training loss: 844.2733764648438 = 0.6828132271766663 + 100.0 * 8.435905456542969
Epoch 670, val loss: 0.6976982951164246
Epoch 680, training loss: 843.983642578125 = 0.6761381030082703 + 100.0 * 8.433074951171875
Epoch 680, val loss: 0.6916297674179077
Epoch 690, training loss: 843.7471313476562 = 0.6697267293930054 + 100.0 * 8.430773735046387
Epoch 690, val loss: 0.6858242750167847
Epoch 700, training loss: 843.51123046875 = 0.6635493040084839 + 100.0 * 8.428476333618164
Epoch 700, val loss: 0.6802005171775818
Epoch 710, training loss: 843.4744873046875 = 0.6575689911842346 + 100.0 * 8.428169250488281
Epoch 710, val loss: 0.6747828125953674
Epoch 720, training loss: 843.1311645507812 = 0.6517205238342285 + 100.0 * 8.42479419708252
Epoch 720, val loss: 0.6695317625999451
Epoch 730, training loss: 842.8869018554688 = 0.6461488008499146 + 100.0 * 8.422407150268555
Epoch 730, val loss: 0.6645649075508118
Epoch 740, training loss: 842.6392822265625 = 0.6408589482307434 + 100.0 * 8.419983863830566
Epoch 740, val loss: 0.6598840951919556
Epoch 750, training loss: 842.4164428710938 = 0.6357886791229248 + 100.0 * 8.417806625366211
Epoch 750, val loss: 0.6553844213485718
Epoch 760, training loss: 842.2465209960938 = 0.6308901309967041 + 100.0 * 8.416155815124512
Epoch 760, val loss: 0.6510729193687439
Epoch 770, training loss: 842.437744140625 = 0.6261104345321655 + 100.0 * 8.418116569519043
Epoch 770, val loss: 0.6468611359596252
Epoch 780, training loss: 841.9359741210938 = 0.6214638948440552 + 100.0 * 8.413145065307617
Epoch 780, val loss: 0.6429085731506348
Epoch 790, training loss: 841.814453125 = 0.617053747177124 + 100.0 * 8.41197395324707
Epoch 790, val loss: 0.6391303539276123
Epoch 800, training loss: 841.6107788085938 = 0.6128128170967102 + 100.0 * 8.409979820251465
Epoch 800, val loss: 0.6354920864105225
Epoch 810, training loss: 841.450927734375 = 0.6087522506713867 + 100.0 * 8.408421516418457
Epoch 810, val loss: 0.632104754447937
Epoch 820, training loss: 841.3463134765625 = 0.604820191860199 + 100.0 * 8.407415390014648
Epoch 820, val loss: 0.6287864446640015
Epoch 830, training loss: 841.1453247070312 = 0.6010021567344666 + 100.0 * 8.40544319152832
Epoch 830, val loss: 0.6256214380264282
Epoch 840, training loss: 841.0706176757812 = 0.5973549485206604 + 100.0 * 8.404732704162598
Epoch 840, val loss: 0.622657835483551
Epoch 850, training loss: 840.9077758789062 = 0.5938723683357239 + 100.0 * 8.403139114379883
Epoch 850, val loss: 0.6198248267173767
Epoch 860, training loss: 840.9505004882812 = 0.5905133485794067 + 100.0 * 8.403599739074707
Epoch 860, val loss: 0.6171017289161682
Epoch 870, training loss: 840.6839599609375 = 0.5872204303741455 + 100.0 * 8.400967597961426
Epoch 870, val loss: 0.6145792007446289
Epoch 880, training loss: 840.5200805664062 = 0.5841058492660522 + 100.0 * 8.399359703063965
Epoch 880, val loss: 0.6121318340301514
Epoch 890, training loss: 840.3703002929688 = 0.5811213254928589 + 100.0 * 8.397891998291016
Epoch 890, val loss: 0.6098628640174866
Epoch 900, training loss: 840.4845581054688 = 0.5782322287559509 + 100.0 * 8.399063110351562
Epoch 900, val loss: 0.6076459288597107
Epoch 910, training loss: 840.3987426757812 = 0.5753304958343506 + 100.0 * 8.398234367370605
Epoch 910, val loss: 0.6055156588554382
Epoch 920, training loss: 840.0756225585938 = 0.5726157426834106 + 100.0 * 8.39503002166748
Epoch 920, val loss: 0.6035512089729309
Epoch 930, training loss: 839.942138671875 = 0.5700017213821411 + 100.0 * 8.393721580505371
Epoch 930, val loss: 0.6016353964805603
Epoch 940, training loss: 839.8798828125 = 0.5674983263015747 + 100.0 * 8.393123626708984
Epoch 940, val loss: 0.5998849272727966
Epoch 950, training loss: 839.9835815429688 = 0.5650127530097961 + 100.0 * 8.394186019897461
Epoch 950, val loss: 0.5980963706970215
Epoch 960, training loss: 839.659912109375 = 0.5625972747802734 + 100.0 * 8.390973091125488
Epoch 960, val loss: 0.5964034795761108
Epoch 970, training loss: 839.577392578125 = 0.5603084564208984 + 100.0 * 8.39017105102539
Epoch 970, val loss: 0.5948104858398438
Epoch 980, training loss: 839.4901123046875 = 0.5580970048904419 + 100.0 * 8.389320373535156
Epoch 980, val loss: 0.5933532118797302
Epoch 990, training loss: 839.396728515625 = 0.5559588670730591 + 100.0 * 8.388407707214355
Epoch 990, val loss: 0.5919068455696106
Epoch 1000, training loss: 839.6356811523438 = 0.5538771748542786 + 100.0 * 8.390817642211914
Epoch 1000, val loss: 0.5905337333679199
Epoch 1010, training loss: 839.5092163085938 = 0.5517177581787109 + 100.0 * 8.389575004577637
Epoch 1010, val loss: 0.589108407497406
Epoch 1020, training loss: 839.2462158203125 = 0.5497171878814697 + 100.0 * 8.386964797973633
Epoch 1020, val loss: 0.587868869304657
Epoch 1030, training loss: 839.1427001953125 = 0.5478113889694214 + 100.0 * 8.38594913482666
Epoch 1030, val loss: 0.5865839123725891
Epoch 1040, training loss: 839.0523071289062 = 0.5459654927253723 + 100.0 * 8.385063171386719
Epoch 1040, val loss: 0.5854349136352539
Epoch 1050, training loss: 839.1294555664062 = 0.5441783666610718 + 100.0 * 8.385852813720703
Epoch 1050, val loss: 0.5844018459320068
Epoch 1060, training loss: 838.9766235351562 = 0.5423160195350647 + 100.0 * 8.384343147277832
Epoch 1060, val loss: 0.5831008553504944
Epoch 1070, training loss: 838.9208374023438 = 0.5405758619308472 + 100.0 * 8.38380241394043
Epoch 1070, val loss: 0.582131028175354
Epoch 1080, training loss: 838.80419921875 = 0.5389313697814941 + 100.0 * 8.382652282714844
Epoch 1080, val loss: 0.5810679793357849
Epoch 1090, training loss: 838.7169189453125 = 0.5373386144638062 + 100.0 * 8.381795883178711
Epoch 1090, val loss: 0.580121636390686
Epoch 1100, training loss: 838.6489868164062 = 0.5357916355133057 + 100.0 * 8.381132125854492
Epoch 1100, val loss: 0.5792558193206787
Epoch 1110, training loss: 838.5880126953125 = 0.5342807769775391 + 100.0 * 8.380537033081055
Epoch 1110, val loss: 0.5783479809761047
Epoch 1120, training loss: 838.8800048828125 = 0.5327918529510498 + 100.0 * 8.383472442626953
Epoch 1120, val loss: 0.5775833129882812
Epoch 1130, training loss: 838.6663208007812 = 0.5312564373016357 + 100.0 * 8.38135051727295
Epoch 1130, val loss: 0.5765911340713501
Epoch 1140, training loss: 838.3911743164062 = 0.5298232436180115 + 100.0 * 8.378613471984863
Epoch 1140, val loss: 0.5757614970207214
Epoch 1150, training loss: 838.3478393554688 = 0.5284544229507446 + 100.0 * 8.378193855285645
Epoch 1150, val loss: 0.5749998688697815
Epoch 1160, training loss: 838.2510986328125 = 0.527134120464325 + 100.0 * 8.377239227294922
Epoch 1160, val loss: 0.5742692351341248
Epoch 1170, training loss: 838.2534790039062 = 0.5258373618125916 + 100.0 * 8.377276420593262
Epoch 1170, val loss: 0.5734870433807373
Epoch 1180, training loss: 838.3244018554688 = 0.5244778990745544 + 100.0 * 8.377999305725098
Epoch 1180, val loss: 0.5727442502975464
Epoch 1190, training loss: 838.1983032226562 = 0.5231361389160156 + 100.0 * 8.376751899719238
Epoch 1190, val loss: 0.57196444272995
Epoch 1200, training loss: 838.0346069335938 = 0.5219040513038635 + 100.0 * 8.375126838684082
Epoch 1200, val loss: 0.5712829232215881
Epoch 1210, training loss: 837.9293212890625 = 0.5207299590110779 + 100.0 * 8.374085426330566
Epoch 1210, val loss: 0.5706707835197449
Epoch 1220, training loss: 837.8705444335938 = 0.5195855498313904 + 100.0 * 8.373509407043457
Epoch 1220, val loss: 0.5700857639312744
Epoch 1230, training loss: 837.8899536132812 = 0.5184581875801086 + 100.0 * 8.3737154006958
Epoch 1230, val loss: 0.5694722533226013
Epoch 1240, training loss: 837.7678833007812 = 0.5172618627548218 + 100.0 * 8.372506141662598
Epoch 1240, val loss: 0.5687177777290344
Epoch 1250, training loss: 837.8211669921875 = 0.516110897064209 + 100.0 * 8.373050689697266
Epoch 1250, val loss: 0.5681405663490295
Epoch 1260, training loss: 837.6378173828125 = 0.5150060057640076 + 100.0 * 8.371228218078613
Epoch 1260, val loss: 0.5675392150878906
Epoch 1270, training loss: 837.5661010742188 = 0.5139522552490234 + 100.0 * 8.370521545410156
Epoch 1270, val loss: 0.5669680833816528
Epoch 1280, training loss: 837.4970703125 = 0.5129123330116272 + 100.0 * 8.369841575622559
Epoch 1280, val loss: 0.5663798451423645
Epoch 1290, training loss: 837.46826171875 = 0.5118710994720459 + 100.0 * 8.369564056396484
Epoch 1290, val loss: 0.5657986998558044
Epoch 1300, training loss: 837.7703857421875 = 0.5107869505882263 + 100.0 * 8.37259578704834
Epoch 1300, val loss: 0.5651030540466309
Epoch 1310, training loss: 837.4576416015625 = 0.5096800923347473 + 100.0 * 8.369479179382324
Epoch 1310, val loss: 0.5645970702171326
Epoch 1320, training loss: 837.2855834960938 = 0.5086612701416016 + 100.0 * 8.367769241333008
Epoch 1320, val loss: 0.5639612078666687
Epoch 1330, training loss: 837.2227172851562 = 0.5076696276664734 + 100.0 * 8.36715030670166
Epoch 1330, val loss: 0.5634332895278931
Epoch 1340, training loss: 837.1788330078125 = 0.5066965222358704 + 100.0 * 8.366721153259277
Epoch 1340, val loss: 0.5628724098205566
Epoch 1350, training loss: 837.3370971679688 = 0.5057157874107361 + 100.0 * 8.368313789367676
Epoch 1350, val loss: 0.5622019171714783
Epoch 1360, training loss: 837.0499877929688 = 0.5046708583831787 + 100.0 * 8.365452766418457
Epoch 1360, val loss: 0.5617515444755554
Epoch 1370, training loss: 836.9942016601562 = 0.5036815404891968 + 100.0 * 8.36490535736084
Epoch 1370, val loss: 0.5611273050308228
Epoch 1380, training loss: 836.947509765625 = 0.502730667591095 + 100.0 * 8.364447593688965
Epoch 1380, val loss: 0.5605969429016113
Epoch 1390, training loss: 836.9065551757812 = 0.501801609992981 + 100.0 * 8.364047050476074
Epoch 1390, val loss: 0.5600956082344055
Epoch 1400, training loss: 836.9261474609375 = 0.5008743405342102 + 100.0 * 8.364253044128418
Epoch 1400, val loss: 0.5595217347145081
Epoch 1410, training loss: 836.8966674804688 = 0.4998798370361328 + 100.0 * 8.363967895507812
Epoch 1410, val loss: 0.5588588118553162
Epoch 1420, training loss: 836.8583984375 = 0.4989055395126343 + 100.0 * 8.363595008850098
Epoch 1420, val loss: 0.5583839416503906
Epoch 1430, training loss: 836.7113647460938 = 0.49798837304115295 + 100.0 * 8.362133979797363
Epoch 1430, val loss: 0.5578001737594604
Epoch 1440, training loss: 836.6461791992188 = 0.4970932602882385 + 100.0 * 8.361491203308105
Epoch 1440, val loss: 0.5572739243507385
Epoch 1450, training loss: 836.6156616210938 = 0.4962148070335388 + 100.0 * 8.361194610595703
Epoch 1450, val loss: 0.5567261576652527
Epoch 1460, training loss: 837.1660766601562 = 0.495311439037323 + 100.0 * 8.366707801818848
Epoch 1460, val loss: 0.5560085773468018
Epoch 1470, training loss: 836.7422485351562 = 0.494329571723938 + 100.0 * 8.362479209899902
Epoch 1470, val loss: 0.5557084679603577
Epoch 1480, training loss: 836.5255126953125 = 0.49341627955436707 + 100.0 * 8.360321044921875
Epoch 1480, val loss: 0.5549675226211548
Epoch 1490, training loss: 836.4293823242188 = 0.4925377368927002 + 100.0 * 8.359368324279785
Epoch 1490, val loss: 0.5544970631599426
Epoch 1500, training loss: 836.4381713867188 = 0.4916742444038391 + 100.0 * 8.359464645385742
Epoch 1500, val loss: 0.5539750456809998
Epoch 1510, training loss: 836.5110473632812 = 0.4907713234424591 + 100.0 * 8.36020278930664
Epoch 1510, val loss: 0.5533804297447205
Epoch 1520, training loss: 836.347900390625 = 0.48986607789993286 + 100.0 * 8.358580589294434
Epoch 1520, val loss: 0.5528337955474854
Epoch 1530, training loss: 836.286865234375 = 0.48899751901626587 + 100.0 * 8.357978820800781
Epoch 1530, val loss: 0.5522351264953613
Epoch 1540, training loss: 836.2396850585938 = 0.4881390333175659 + 100.0 * 8.357515335083008
Epoch 1540, val loss: 0.551719605922699
Epoch 1550, training loss: 836.208984375 = 0.4872889518737793 + 100.0 * 8.357216835021973
Epoch 1550, val loss: 0.5511907935142517
Epoch 1560, training loss: 836.3709106445312 = 0.48643097281455994 + 100.0 * 8.358844757080078
Epoch 1560, val loss: 0.5506417751312256
Epoch 1570, training loss: 836.1482543945312 = 0.4855167269706726 + 100.0 * 8.356627464294434
Epoch 1570, val loss: 0.550050675868988
Epoch 1580, training loss: 836.1259155273438 = 0.4846475422382355 + 100.0 * 8.356412887573242
Epoch 1580, val loss: 0.5494634509086609
Epoch 1590, training loss: 836.1730346679688 = 0.4837886393070221 + 100.0 * 8.356892585754395
Epoch 1590, val loss: 0.5488464832305908
Epoch 1600, training loss: 836.2476196289062 = 0.48288291692733765 + 100.0 * 8.357646942138672
Epoch 1600, val loss: 0.5482922196388245
Epoch 1610, training loss: 836.00732421875 = 0.4819760322570801 + 100.0 * 8.355253219604492
Epoch 1610, val loss: 0.5477212071418762
Epoch 1620, training loss: 835.9927978515625 = 0.4811244010925293 + 100.0 * 8.355116844177246
Epoch 1620, val loss: 0.5472628474235535
Epoch 1630, training loss: 835.9509887695312 = 0.4802999198436737 + 100.0 * 8.354706764221191
Epoch 1630, val loss: 0.5466737151145935
Epoch 1640, training loss: 835.9072265625 = 0.47948092222213745 + 100.0 * 8.354277610778809
Epoch 1640, val loss: 0.5461774468421936
Epoch 1650, training loss: 835.88818359375 = 0.47865793108940125 + 100.0 * 8.354095458984375
Epoch 1650, val loss: 0.5456095933914185
Epoch 1660, training loss: 836.7616577148438 = 0.4777897000312805 + 100.0 * 8.362838745117188
Epoch 1660, val loss: 0.5448887348175049
Epoch 1670, training loss: 836.0531616210938 = 0.47682955861091614 + 100.0 * 8.35576343536377
Epoch 1670, val loss: 0.5444871783256531
Epoch 1680, training loss: 835.8330078125 = 0.47599107027053833 + 100.0 * 8.353569984436035
Epoch 1680, val loss: 0.5438717007637024
Epoch 1690, training loss: 835.757568359375 = 0.4751848876476288 + 100.0 * 8.352824211120605
Epoch 1690, val loss: 0.5432952046394348
Epoch 1700, training loss: 835.7139892578125 = 0.4743911325931549 + 100.0 * 8.352396011352539
Epoch 1700, val loss: 0.5428102016448975
Epoch 1710, training loss: 835.6776733398438 = 0.47360092401504517 + 100.0 * 8.352041244506836
Epoch 1710, val loss: 0.5422701239585876
Epoch 1720, training loss: 835.6448364257812 = 0.4728001654148102 + 100.0 * 8.351720809936523
Epoch 1720, val loss: 0.5417510271072388
Epoch 1730, training loss: 835.6130981445312 = 0.47199395298957825 + 100.0 * 8.351410865783691
Epoch 1730, val loss: 0.5412178039550781
Epoch 1740, training loss: 835.7567749023438 = 0.47118812799453735 + 100.0 * 8.352855682373047
Epoch 1740, val loss: 0.5407806038856506
Epoch 1750, training loss: 835.6974487304688 = 0.4702899158000946 + 100.0 * 8.352272033691406
Epoch 1750, val loss: 0.540075421333313
Epoch 1760, training loss: 835.6881103515625 = 0.4694156050682068 + 100.0 * 8.352187156677246
Epoch 1760, val loss: 0.5394688844680786
Epoch 1770, training loss: 835.4837646484375 = 0.46861037611961365 + 100.0 * 8.350151062011719
Epoch 1770, val loss: 0.5389498472213745
Epoch 1780, training loss: 835.4713745117188 = 0.46782299876213074 + 100.0 * 8.350035667419434
Epoch 1780, val loss: 0.5384231209754944
Epoch 1790, training loss: 835.4203491210938 = 0.46704787015914917 + 100.0 * 8.349533081054688
Epoch 1790, val loss: 0.5379364490509033
Epoch 1800, training loss: 835.5344848632812 = 0.4662664830684662 + 100.0 * 8.350682258605957
Epoch 1800, val loss: 0.5374858379364014
Epoch 1810, training loss: 835.3709106445312 = 0.46543869376182556 + 100.0 * 8.349054336547852
Epoch 1810, val loss: 0.5367604494094849
Epoch 1820, training loss: 835.6522827148438 = 0.46463072299957275 + 100.0 * 8.351876258850098
Epoch 1820, val loss: 0.5364183783531189
Epoch 1830, training loss: 835.377197265625 = 0.463753879070282 + 100.0 * 8.34913444519043
Epoch 1830, val loss: 0.53548663854599
Epoch 1840, training loss: 835.3106079101562 = 0.4629433751106262 + 100.0 * 8.34847640991211
Epoch 1840, val loss: 0.5350611209869385
Epoch 1850, training loss: 835.245361328125 = 0.46216586232185364 + 100.0 * 8.347831726074219
Epoch 1850, val loss: 0.5345139503479004
Epoch 1860, training loss: 835.177001953125 = 0.4613950252532959 + 100.0 * 8.347156524658203
Epoch 1860, val loss: 0.5339574813842773
Epoch 1870, training loss: 835.1220092773438 = 0.46061187982559204 + 100.0 * 8.346613883972168
Epoch 1870, val loss: 0.5334107875823975
Epoch 1880, training loss: 835.0816650390625 = 0.45981550216674805 + 100.0 * 8.34621810913086
Epoch 1880, val loss: 0.5328353047370911
Epoch 1890, training loss: 835.0848999023438 = 0.45900633931159973 + 100.0 * 8.346259117126465
Epoch 1890, val loss: 0.532255232334137
Epoch 1900, training loss: 835.5425415039062 = 0.45817646384239197 + 100.0 * 8.35084342956543
Epoch 1900, val loss: 0.5318513512611389
Epoch 1910, training loss: 835.2081298828125 = 0.45726245641708374 + 100.0 * 8.347508430480957
Epoch 1910, val loss: 0.5308682322502136
Epoch 1920, training loss: 834.9540405273438 = 0.456417053937912 + 100.0 * 8.344976425170898
Epoch 1920, val loss: 0.5303335189819336
Epoch 1930, training loss: 834.9296264648438 = 0.45560115575790405 + 100.0 * 8.34473991394043
Epoch 1930, val loss: 0.5297815799713135
Epoch 1940, training loss: 834.8870239257812 = 0.4547967314720154 + 100.0 * 8.344322204589844
Epoch 1940, val loss: 0.5291703939437866
Epoch 1950, training loss: 834.9700317382812 = 0.45398712158203125 + 100.0 * 8.345160484313965
Epoch 1950, val loss: 0.528537392616272
Epoch 1960, training loss: 834.95361328125 = 0.45311930775642395 + 100.0 * 8.34500503540039
Epoch 1960, val loss: 0.5279805064201355
Epoch 1970, training loss: 834.8529052734375 = 0.4522658586502075 + 100.0 * 8.344006538391113
Epoch 1970, val loss: 0.5274437069892883
Epoch 1980, training loss: 834.7948608398438 = 0.4514304995536804 + 100.0 * 8.34343433380127
Epoch 1980, val loss: 0.526802659034729
Epoch 1990, training loss: 834.8732299804688 = 0.45060810446739197 + 100.0 * 8.344225883483887
Epoch 1990, val loss: 0.5263621211051941
Epoch 2000, training loss: 834.7974243164062 = 0.44973835349082947 + 100.0 * 8.343476295471191
Epoch 2000, val loss: 0.5256608724594116
Epoch 2010, training loss: 834.7302856445312 = 0.44888579845428467 + 100.0 * 8.342813491821289
Epoch 2010, val loss: 0.5249971151351929
Epoch 2020, training loss: 834.6663818359375 = 0.44805315136909485 + 100.0 * 8.342183113098145
Epoch 2020, val loss: 0.524465799331665
Epoch 2030, training loss: 834.6385498046875 = 0.4472288489341736 + 100.0 * 8.341913223266602
Epoch 2030, val loss: 0.5238810777664185
Epoch 2040, training loss: 834.8372192382812 = 0.4463966488838196 + 100.0 * 8.343908309936523
Epoch 2040, val loss: 0.5232391357421875
Epoch 2050, training loss: 834.6264038085938 = 0.44551408290863037 + 100.0 * 8.341809272766113
Epoch 2050, val loss: 0.5227057337760925
Epoch 2060, training loss: 834.5742797851562 = 0.44465911388397217 + 100.0 * 8.341296195983887
Epoch 2060, val loss: 0.5220245718955994
Epoch 2070, training loss: 834.626708984375 = 0.4438138008117676 + 100.0 * 8.341829299926758
Epoch 2070, val loss: 0.5214648246765137
Epoch 2080, training loss: 834.6652221679688 = 0.44295403361320496 + 100.0 * 8.342223167419434
Epoch 2080, val loss: 0.5208112001419067
Epoch 2090, training loss: 834.5322875976562 = 0.44208797812461853 + 100.0 * 8.340902328491211
Epoch 2090, val loss: 0.520372748374939
Epoch 2100, training loss: 834.5515747070312 = 0.44123637676239014 + 100.0 * 8.341103553771973
Epoch 2100, val loss: 0.519726574420929
Epoch 2110, training loss: 834.5706787109375 = 0.44036930799484253 + 100.0 * 8.341302871704102
Epoch 2110, val loss: 0.5190832018852234
Epoch 2120, training loss: 834.4937133789062 = 0.4394921660423279 + 100.0 * 8.34054183959961
Epoch 2120, val loss: 0.5185470581054688
Epoch 2130, training loss: 834.3937377929688 = 0.4386346936225891 + 100.0 * 8.339550971984863
Epoch 2130, val loss: 0.5179927945137024
Epoch 2140, training loss: 834.3721923828125 = 0.4377904534339905 + 100.0 * 8.339344024658203
Epoch 2140, val loss: 0.5173917412757874
Epoch 2150, training loss: 834.38134765625 = 0.43694207072257996 + 100.0 * 8.339444160461426
Epoch 2150, val loss: 0.51680588722229
Epoch 2160, training loss: 834.9033203125 = 0.43606072664260864 + 100.0 * 8.344673156738281
Epoch 2160, val loss: 0.5160574913024902
Epoch 2170, training loss: 834.6449584960938 = 0.4351186752319336 + 100.0 * 8.342098236083984
Epoch 2170, val loss: 0.5155305862426758
Epoch 2180, training loss: 834.2939453125 = 0.43420594930648804 + 100.0 * 8.338597297668457
Epoch 2180, val loss: 0.5149273872375488
Epoch 2190, training loss: 834.276611328125 = 0.433343768119812 + 100.0 * 8.338432312011719
Epoch 2190, val loss: 0.5144229531288147
Epoch 2200, training loss: 834.2415161132812 = 0.4324958324432373 + 100.0 * 8.338089942932129
Epoch 2200, val loss: 0.5138502717018127
Epoch 2210, training loss: 834.2200317382812 = 0.4316472113132477 + 100.0 * 8.337883949279785
Epoch 2210, val loss: 0.5132865905761719
Epoch 2220, training loss: 834.3994140625 = 0.4307870864868164 + 100.0 * 8.339686393737793
Epoch 2220, val loss: 0.5128474831581116
Epoch 2230, training loss: 834.2340087890625 = 0.4298383891582489 + 100.0 * 8.338042259216309
Epoch 2230, val loss: 0.5120416879653931
Epoch 2240, training loss: 834.1520385742188 = 0.42891427874565125 + 100.0 * 8.337231636047363
Epoch 2240, val loss: 0.5114076733589172
Epoch 2250, training loss: 834.129150390625 = 0.42803001403808594 + 100.0 * 8.337011337280273
Epoch 2250, val loss: 0.5107831358909607
Epoch 2260, training loss: 834.110107421875 = 0.4271606504917145 + 100.0 * 8.33682918548584
Epoch 2260, val loss: 0.5102299451828003
Epoch 2270, training loss: 834.1541748046875 = 0.4262857437133789 + 100.0 * 8.337279319763184
Epoch 2270, val loss: 0.5096360445022583
Epoch 2280, training loss: 834.2384643554688 = 0.42537498474121094 + 100.0 * 8.338130950927734
Epoch 2280, val loss: 0.5089689493179321
Epoch 2290, training loss: 834.137939453125 = 0.4244430363178253 + 100.0 * 8.337135314941406
Epoch 2290, val loss: 0.508414089679718
Epoch 2300, training loss: 834.2592163085938 = 0.42353466153144836 + 100.0 * 8.338356971740723
Epoch 2300, val loss: 0.5077304840087891
Epoch 2310, training loss: 834.0571899414062 = 0.4225611388683319 + 100.0 * 8.336346626281738
Epoch 2310, val loss: 0.5071612000465393
Epoch 2320, training loss: 834.0355834960938 = 0.4216313660144806 + 100.0 * 8.336139678955078
Epoch 2320, val loss: 0.5065626502037048
Epoch 2330, training loss: 833.9859619140625 = 0.4207167327404022 + 100.0 * 8.335652351379395
Epoch 2330, val loss: 0.5059276223182678
Epoch 2340, training loss: 833.9461059570312 = 0.4198148846626282 + 100.0 * 8.3352632522583
Epoch 2340, val loss: 0.5053462982177734
Epoch 2350, training loss: 833.94384765625 = 0.4189131557941437 + 100.0 * 8.335249900817871
Epoch 2350, val loss: 0.5046588778495789
Epoch 2360, training loss: 834.3466796875 = 0.4179922342300415 + 100.0 * 8.339286804199219
Epoch 2360, val loss: 0.5038638114929199
Epoch 2370, training loss: 834.0145263671875 = 0.41696569323539734 + 100.0 * 8.335975646972656
Epoch 2370, val loss: 0.50345379114151
Epoch 2380, training loss: 833.9193725585938 = 0.4160015285015106 + 100.0 * 8.335033416748047
Epoch 2380, val loss: 0.5026876926422119
Epoch 2390, training loss: 833.8742065429688 = 0.4150705337524414 + 100.0 * 8.334590911865234
Epoch 2390, val loss: 0.5020851492881775
Epoch 2400, training loss: 833.8513793945312 = 0.41414740681648254 + 100.0 * 8.334372520446777
Epoch 2400, val loss: 0.5014915466308594
Epoch 2410, training loss: 834.023193359375 = 0.41321036219596863 + 100.0 * 8.336099624633789
Epoch 2410, val loss: 0.5009027123451233
Epoch 2420, training loss: 833.8321533203125 = 0.4122174084186554 + 100.0 * 8.334198951721191
Epoch 2420, val loss: 0.50017249584198
Epoch 2430, training loss: 833.8284912109375 = 0.41124409437179565 + 100.0 * 8.334172248840332
Epoch 2430, val loss: 0.4994521141052246
Epoch 2440, training loss: 833.7702026367188 = 0.4102807641029358 + 100.0 * 8.333599090576172
Epoch 2440, val loss: 0.4988887906074524
Epoch 2450, training loss: 833.7420043945312 = 0.4093345105648041 + 100.0 * 8.33332633972168
Epoch 2450, val loss: 0.4982283413410187
Epoch 2460, training loss: 833.80126953125 = 0.4083844721317291 + 100.0 * 8.333929061889648
Epoch 2460, val loss: 0.4976558983325958
Epoch 2470, training loss: 833.9534912109375 = 0.4073904752731323 + 100.0 * 8.335460662841797
Epoch 2470, val loss: 0.49703285098075867
Epoch 2480, training loss: 833.9746704101562 = 0.40637657046318054 + 100.0 * 8.33568286895752
Epoch 2480, val loss: 0.49632641673088074
Epoch 2490, training loss: 833.7118530273438 = 0.40535664558410645 + 100.0 * 8.333065032958984
Epoch 2490, val loss: 0.4955916404724121
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.798072044647387
0.8640150691878578
=== training gcn model ===
Epoch 0, training loss: 1059.3291015625 = 1.1004997491836548 + 100.0 * 10.582286834716797
Epoch 0, val loss: 1.1016709804534912
Epoch 10, training loss: 1059.2781982421875 = 1.0959588289260864 + 100.0 * 10.581822395324707
Epoch 10, val loss: 1.0970996618270874
Epoch 20, training loss: 1059.03515625 = 1.090902328491211 + 100.0 * 10.579442024230957
Epoch 20, val loss: 1.0919862985610962
Epoch 30, training loss: 1057.93603515625 = 1.0852903127670288 + 100.0 * 10.568507194519043
Epoch 30, val loss: 1.0862982273101807
Epoch 40, training loss: 1054.0162353515625 = 1.0789554119110107 + 100.0 * 10.529373168945312
Epoch 40, val loss: 1.0798310041427612
Epoch 50, training loss: 1044.6036376953125 = 1.0718817710876465 + 100.0 * 10.435317993164062
Epoch 50, val loss: 1.0726121664047241
Epoch 60, training loss: 1029.0089111328125 = 1.0650297403335571 + 100.0 * 10.279438972473145
Epoch 60, val loss: 1.0655875205993652
Epoch 70, training loss: 1015.1701049804688 = 1.0578244924545288 + 100.0 * 10.141122817993164
Epoch 70, val loss: 1.0580811500549316
Epoch 80, training loss: 998.1984252929688 = 1.050133228302002 + 100.0 * 9.97148323059082
Epoch 80, val loss: 1.0502068996429443
Epoch 90, training loss: 968.6801147460938 = 1.0435408353805542 + 100.0 * 9.676365852355957
Epoch 90, val loss: 1.0437742471694946
Epoch 100, training loss: 942.17138671875 = 1.0387994050979614 + 100.0 * 9.411325454711914
Epoch 100, val loss: 1.0393515825271606
Epoch 110, training loss: 927.1228637695312 = 1.0351158380508423 + 100.0 * 9.26087760925293
Epoch 110, val loss: 1.0358480215072632
Epoch 120, training loss: 921.9100341796875 = 1.0312843322753906 + 100.0 * 9.208786964416504
Epoch 120, val loss: 1.032091498374939
Epoch 130, training loss: 919.0601196289062 = 1.0269945859909058 + 100.0 * 9.180331230163574
Epoch 130, val loss: 1.0276906490325928
Epoch 140, training loss: 915.3638305664062 = 1.022091269493103 + 100.0 * 9.143417358398438
Epoch 140, val loss: 1.0227088928222656
Epoch 150, training loss: 909.8746948242188 = 1.0173149108886719 + 100.0 * 9.088573455810547
Epoch 150, val loss: 1.0179386138916016
Epoch 160, training loss: 900.9122314453125 = 1.0136027336120605 + 100.0 * 8.99898624420166
Epoch 160, val loss: 1.0143436193466187
Epoch 170, training loss: 890.9330444335938 = 1.011160969734192 + 100.0 * 8.899218559265137
Epoch 170, val loss: 1.0119333267211914
Epoch 180, training loss: 884.3081665039062 = 1.0082249641418457 + 100.0 * 8.832999229431152
Epoch 180, val loss: 1.0088225603103638
Epoch 190, training loss: 879.8853759765625 = 1.0040218830108643 + 100.0 * 8.788813591003418
Epoch 190, val loss: 1.0045015811920166
Epoch 200, training loss: 876.8795776367188 = 0.9989408850669861 + 100.0 * 8.758806228637695
Epoch 200, val loss: 0.999447762966156
Epoch 210, training loss: 874.9201049804688 = 0.9934829473495483 + 100.0 * 8.739266395568848
Epoch 210, val loss: 0.9940707087516785
Epoch 220, training loss: 873.5115966796875 = 0.9875792264938354 + 100.0 * 8.725240707397461
Epoch 220, val loss: 0.9882321953773499
Epoch 230, training loss: 872.1715087890625 = 0.9812928438186646 + 100.0 * 8.711901664733887
Epoch 230, val loss: 0.9820350408554077
Epoch 240, training loss: 870.71142578125 = 0.9748812913894653 + 100.0 * 8.697365760803223
Epoch 240, val loss: 0.9757868051528931
Epoch 250, training loss: 869.5119018554688 = 0.9685816168785095 + 100.0 * 8.685433387756348
Epoch 250, val loss: 0.9696087837219238
Epoch 260, training loss: 867.2067260742188 = 0.9620892405509949 + 100.0 * 8.662446022033691
Epoch 260, val loss: 0.9633072018623352
Epoch 270, training loss: 864.9567260742188 = 0.9555802345275879 + 100.0 * 8.64001178741455
Epoch 270, val loss: 0.9569482207298279
Epoch 280, training loss: 862.7484741210938 = 0.9490260481834412 + 100.0 * 8.61799430847168
Epoch 280, val loss: 0.9505022764205933
Epoch 290, training loss: 860.9947509765625 = 0.9417381882667542 + 100.0 * 8.600530624389648
Epoch 290, val loss: 0.9433692097663879
Epoch 300, training loss: 859.4292602539062 = 0.9336853623390198 + 100.0 * 8.584955215454102
Epoch 300, val loss: 0.9353968501091003
Epoch 310, training loss: 858.1300048828125 = 0.9249165058135986 + 100.0 * 8.572051048278809
Epoch 310, val loss: 0.9268110990524292
Epoch 320, training loss: 857.0403442382812 = 0.9155933260917664 + 100.0 * 8.561247825622559
Epoch 320, val loss: 0.9177026152610779
Epoch 330, training loss: 856.5220336914062 = 0.905733048915863 + 100.0 * 8.55616283416748
Epoch 330, val loss: 0.9081048965454102
Epoch 340, training loss: 855.5743408203125 = 0.8953004479408264 + 100.0 * 8.54679012298584
Epoch 340, val loss: 0.8979306221008301
Epoch 350, training loss: 854.9398193359375 = 0.8844176530838013 + 100.0 * 8.54055404663086
Epoch 350, val loss: 0.88736492395401
Epoch 360, training loss: 854.460693359375 = 0.8731869459152222 + 100.0 * 8.53587532043457
Epoch 360, val loss: 0.8764879107475281
Epoch 370, training loss: 854.0421142578125 = 0.8616486191749573 + 100.0 * 8.531805038452148
Epoch 370, val loss: 0.8653557896614075
Epoch 380, training loss: 853.6798095703125 = 0.849861741065979 + 100.0 * 8.528299331665039
Epoch 380, val loss: 0.8539971709251404
Epoch 390, training loss: 853.2965087890625 = 0.8379562497138977 + 100.0 * 8.524585723876953
Epoch 390, val loss: 0.8425450325012207
Epoch 400, training loss: 852.9109497070312 = 0.826025664806366 + 100.0 * 8.520849227905273
Epoch 400, val loss: 0.8310548067092896
Epoch 410, training loss: 852.5436401367188 = 0.814015805721283 + 100.0 * 8.517295837402344
Epoch 410, val loss: 0.8195358514785767
Epoch 420, training loss: 852.1710815429688 = 0.8020312786102295 + 100.0 * 8.513690948486328
Epoch 420, val loss: 0.8080364465713501
Epoch 430, training loss: 851.9456176757812 = 0.7900397181510925 + 100.0 * 8.511555671691895
Epoch 430, val loss: 0.7967009544372559
Epoch 440, training loss: 851.4531860351562 = 0.778205931186676 + 100.0 * 8.506750106811523
Epoch 440, val loss: 0.7853399515151978
Epoch 450, training loss: 851.1087036132812 = 0.7665952444076538 + 100.0 * 8.50342082977295
Epoch 450, val loss: 0.7742260098457336
Epoch 460, training loss: 850.5362548828125 = 0.7550989389419556 + 100.0 * 8.497811317443848
Epoch 460, val loss: 0.7633679509162903
Epoch 470, training loss: 850.1039428710938 = 0.7438481450080872 + 100.0 * 8.493600845336914
Epoch 470, val loss: 0.7527299523353577
Epoch 480, training loss: 849.6971435546875 = 0.732822597026825 + 100.0 * 8.489643096923828
Epoch 480, val loss: 0.7423052787780762
Epoch 490, training loss: 849.2076416015625 = 0.7220096588134766 + 100.0 * 8.484856605529785
Epoch 490, val loss: 0.7321236729621887
Epoch 500, training loss: 849.1419067382812 = 0.7114779949188232 + 100.0 * 8.484304428100586
Epoch 500, val loss: 0.722227156162262
Epoch 510, training loss: 848.4489135742188 = 0.7012040615081787 + 100.0 * 8.477477073669434
Epoch 510, val loss: 0.7125099897384644
Epoch 520, training loss: 847.9200439453125 = 0.6912094354629517 + 100.0 * 8.472288131713867
Epoch 520, val loss: 0.7031988501548767
Epoch 530, training loss: 847.5162963867188 = 0.6814612746238708 + 100.0 * 8.468348503112793
Epoch 530, val loss: 0.6941342353820801
Epoch 540, training loss: 847.427978515625 = 0.6720181703567505 + 100.0 * 8.467559814453125
Epoch 540, val loss: 0.6853229999542236
Epoch 550, training loss: 846.750244140625 = 0.6627299189567566 + 100.0 * 8.460875511169434
Epoch 550, val loss: 0.6767374277114868
Epoch 560, training loss: 846.421630859375 = 0.6537652611732483 + 100.0 * 8.45767879486084
Epoch 560, val loss: 0.668455183506012
Epoch 570, training loss: 846.0430297851562 = 0.6450974941253662 + 100.0 * 8.4539794921875
Epoch 570, val loss: 0.6604793071746826
Epoch 580, training loss: 845.7112426757812 = 0.6366785764694214 + 100.0 * 8.450745582580566
Epoch 580, val loss: 0.6527652144432068
Epoch 590, training loss: 845.4373168945312 = 0.62846440076828 + 100.0 * 8.448088645935059
Epoch 590, val loss: 0.6452271342277527
Epoch 600, training loss: 845.0081787109375 = 0.6205106973648071 + 100.0 * 8.443877220153809
Epoch 600, val loss: 0.6380012035369873
Epoch 610, training loss: 844.7181396484375 = 0.6128173470497131 + 100.0 * 8.44105339050293
Epoch 610, val loss: 0.6309676170349121
Epoch 620, training loss: 844.4368896484375 = 0.6053593158721924 + 100.0 * 8.438315391540527
Epoch 620, val loss: 0.6242305040359497
Epoch 630, training loss: 844.2019653320312 = 0.5980821251869202 + 100.0 * 8.436038970947266
Epoch 630, val loss: 0.6176409125328064
Epoch 640, training loss: 843.948486328125 = 0.5909968018531799 + 100.0 * 8.433574676513672
Epoch 640, val loss: 0.6112847924232483
Epoch 650, training loss: 843.7034912109375 = 0.5842347741127014 + 100.0 * 8.431192398071289
Epoch 650, val loss: 0.6052209138870239
Epoch 660, training loss: 843.5028076171875 = 0.5777482986450195 + 100.0 * 8.429250717163086
Epoch 660, val loss: 0.5994147658348083
Epoch 670, training loss: 843.3472290039062 = 0.5714844465255737 + 100.0 * 8.427757263183594
Epoch 670, val loss: 0.5938172936439514
Epoch 680, training loss: 843.4265747070312 = 0.5653952956199646 + 100.0 * 8.428611755371094
Epoch 680, val loss: 0.5883895754814148
Epoch 690, training loss: 843.1796264648438 = 0.5595160722732544 + 100.0 * 8.426200866699219
Epoch 690, val loss: 0.5832015872001648
Epoch 700, training loss: 842.8851928710938 = 0.5539155602455139 + 100.0 * 8.42331314086914
Epoch 700, val loss: 0.5782735347747803
Epoch 710, training loss: 842.7279052734375 = 0.548518180847168 + 100.0 * 8.421793937683105
Epoch 710, val loss: 0.5735377669334412
Epoch 720, training loss: 842.588134765625 = 0.5433400273323059 + 100.0 * 8.420448303222656
Epoch 720, val loss: 0.5689949989318848
Epoch 730, training loss: 842.4632568359375 = 0.5383449792861938 + 100.0 * 8.419249534606934
Epoch 730, val loss: 0.564635157585144
Epoch 740, training loss: 842.6881103515625 = 0.5335252285003662 + 100.0 * 8.42154598236084
Epoch 740, val loss: 0.5604040026664734
Epoch 750, training loss: 842.353515625 = 0.528845489025116 + 100.0 * 8.418246269226074
Epoch 750, val loss: 0.5564156174659729
Epoch 760, training loss: 842.1480712890625 = 0.5243884325027466 + 100.0 * 8.416236877441406
Epoch 760, val loss: 0.5526037812232971
Epoch 770, training loss: 842.0109252929688 = 0.5201050043106079 + 100.0 * 8.414908409118652
Epoch 770, val loss: 0.5489352941513062
Epoch 780, training loss: 841.8976440429688 = 0.5159780383110046 + 100.0 * 8.413816452026367
Epoch 780, val loss: 0.5454320311546326
Epoch 790, training loss: 841.7908325195312 = 0.5120142698287964 + 100.0 * 8.412788391113281
Epoch 790, val loss: 0.5420965552330017
Epoch 800, training loss: 842.8496704101562 = 0.5081725716590881 + 100.0 * 8.423415184020996
Epoch 800, val loss: 0.5389432311058044
Epoch 810, training loss: 841.8740234375 = 0.5043994188308716 + 100.0 * 8.4136962890625
Epoch 810, val loss: 0.5357858538627625
Epoch 820, training loss: 841.56396484375 = 0.5008577108383179 + 100.0 * 8.41063117980957
Epoch 820, val loss: 0.5328582525253296
Epoch 830, training loss: 841.3828125 = 0.49746206402778625 + 100.0 * 8.408853530883789
Epoch 830, val loss: 0.530093789100647
Epoch 840, training loss: 841.2781982421875 = 0.4942041337490082 + 100.0 * 8.40783977508545
Epoch 840, val loss: 0.5274671912193298
Epoch 850, training loss: 841.1776123046875 = 0.49106818437576294 + 100.0 * 8.406865119934082
Epoch 850, val loss: 0.524979829788208
Epoch 860, training loss: 841.1572875976562 = 0.4880407452583313 + 100.0 * 8.406692504882812
Epoch 860, val loss: 0.5226140022277832
Epoch 870, training loss: 840.9950561523438 = 0.4850682020187378 + 100.0 * 8.405099868774414
Epoch 870, val loss: 0.5203043222427368
Epoch 880, training loss: 841.0269165039062 = 0.48224687576293945 + 100.0 * 8.405447006225586
Epoch 880, val loss: 0.5181111097335815
Epoch 890, training loss: 840.8082275390625 = 0.4795331358909607 + 100.0 * 8.403286933898926
Epoch 890, val loss: 0.5160665512084961
Epoch 900, training loss: 840.7124633789062 = 0.4769168496131897 + 100.0 * 8.402355194091797
Epoch 900, val loss: 0.5140936970710754
Epoch 910, training loss: 840.6068115234375 = 0.47439542412757874 + 100.0 * 8.401324272155762
Epoch 910, val loss: 0.5122280120849609
Epoch 920, training loss: 840.5592041015625 = 0.47195443511009216 + 100.0 * 8.400872230529785
Epoch 920, val loss: 0.5104513168334961
Epoch 930, training loss: 840.5364379882812 = 0.4695472717285156 + 100.0 * 8.40066909790039
Epoch 930, val loss: 0.5086917877197266
Epoch 940, training loss: 840.3972778320312 = 0.4672447144985199 + 100.0 * 8.399300575256348
Epoch 940, val loss: 0.5070912837982178
Epoch 950, training loss: 840.2926635742188 = 0.46502551436424255 + 100.0 * 8.398276329040527
Epoch 950, val loss: 0.5055369138717651
Epoch 960, training loss: 840.1909790039062 = 0.46288105845451355 + 100.0 * 8.3972806930542
Epoch 960, val loss: 0.5040579438209534
Epoch 970, training loss: 840.131591796875 = 0.46081864833831787 + 100.0 * 8.396707534790039
Epoch 970, val loss: 0.5026289820671082
Epoch 980, training loss: 840.2754516601562 = 0.4587928354740143 + 100.0 * 8.39816665649414
Epoch 980, val loss: 0.5012832880020142
Epoch 990, training loss: 839.9583129882812 = 0.45682716369628906 + 100.0 * 8.395014762878418
Epoch 990, val loss: 0.5000471472740173
Epoch 1000, training loss: 839.8575439453125 = 0.4549534022808075 + 100.0 * 8.394025802612305
Epoch 1000, val loss: 0.4988352060317993
Epoch 1010, training loss: 839.7908325195312 = 0.4531257152557373 + 100.0 * 8.393377304077148
Epoch 1010, val loss: 0.4976530075073242
Epoch 1020, training loss: 840.0438842773438 = 0.4513537585735321 + 100.0 * 8.395925521850586
Epoch 1020, val loss: 0.4965572655200958
Epoch 1030, training loss: 839.835693359375 = 0.449584424495697 + 100.0 * 8.393860816955566
Epoch 1030, val loss: 0.4954175651073456
Epoch 1040, training loss: 839.6209716796875 = 0.4479142725467682 + 100.0 * 8.391730308532715
Epoch 1040, val loss: 0.4944024980068207
Epoch 1050, training loss: 839.5430908203125 = 0.4462931752204895 + 100.0 * 8.390968322753906
Epoch 1050, val loss: 0.49345409870147705
Epoch 1060, training loss: 839.53564453125 = 0.44472140073776245 + 100.0 * 8.390909194946289
Epoch 1060, val loss: 0.49249881505966187
Epoch 1070, training loss: 839.3578491210938 = 0.44319188594818115 + 100.0 * 8.38914680480957
Epoch 1070, val loss: 0.4915383756160736
Epoch 1080, training loss: 839.3338012695312 = 0.44170668721199036 + 100.0 * 8.388920783996582
Epoch 1080, val loss: 0.49068763852119446
Epoch 1090, training loss: 839.3426513671875 = 0.4402524530887604 + 100.0 * 8.389023780822754
Epoch 1090, val loss: 0.48988229036331177
Epoch 1100, training loss: 839.14990234375 = 0.43883463740348816 + 100.0 * 8.387110710144043
Epoch 1100, val loss: 0.4890962541103363
Epoch 1110, training loss: 839.1893920898438 = 0.437458872795105 + 100.0 * 8.387519836425781
Epoch 1110, val loss: 0.48839932680130005
Epoch 1120, training loss: 839.2061157226562 = 0.436094731092453 + 100.0 * 8.387700080871582
Epoch 1120, val loss: 0.4876154959201813
Epoch 1130, training loss: 838.9767456054688 = 0.4347488284111023 + 100.0 * 8.385419845581055
Epoch 1130, val loss: 0.48688364028930664
Epoch 1140, training loss: 838.899169921875 = 0.43346139788627625 + 100.0 * 8.38465690612793
Epoch 1140, val loss: 0.4862067699432373
Epoch 1150, training loss: 838.8378295898438 = 0.43220680952072144 + 100.0 * 8.384056091308594
Epoch 1150, val loss: 0.4855504631996155
Epoch 1160, training loss: 839.0327758789062 = 0.43097397685050964 + 100.0 * 8.386017799377441
Epoch 1160, val loss: 0.4848996102809906
Epoch 1170, training loss: 839.091796875 = 0.4297199845314026 + 100.0 * 8.38662052154541
Epoch 1170, val loss: 0.48429784178733826
Epoch 1180, training loss: 838.7338256835938 = 0.42851030826568604 + 100.0 * 8.383052825927734
Epoch 1180, val loss: 0.48372575640678406
Epoch 1190, training loss: 838.607421875 = 0.4273458421230316 + 100.0 * 8.381800651550293
Epoch 1190, val loss: 0.48314040899276733
Epoch 1200, training loss: 838.5421752929688 = 0.4262029826641083 + 100.0 * 8.381159782409668
Epoch 1200, val loss: 0.48256629705429077
Epoch 1210, training loss: 838.4717407226562 = 0.42508557438850403 + 100.0 * 8.38046646118164
Epoch 1210, val loss: 0.48207464814186096
Epoch 1220, training loss: 838.47705078125 = 0.42398351430892944 + 100.0 * 8.38053035736084
Epoch 1220, val loss: 0.48156630992889404
Epoch 1230, training loss: 838.7066040039062 = 0.422870010137558 + 100.0 * 8.382837295532227
Epoch 1230, val loss: 0.48105400800704956
Epoch 1240, training loss: 838.320556640625 = 0.42176827788352966 + 100.0 * 8.378988265991211
Epoch 1240, val loss: 0.48051542043685913
Epoch 1250, training loss: 838.2872924804688 = 0.42070749402046204 + 100.0 * 8.378665924072266
Epoch 1250, val loss: 0.4800148904323578
Epoch 1260, training loss: 838.2227783203125 = 0.41967472434043884 + 100.0 * 8.378030776977539
Epoch 1260, val loss: 0.47956299781799316
Epoch 1270, training loss: 838.2411499023438 = 0.41865822672843933 + 100.0 * 8.378225326538086
Epoch 1270, val loss: 0.4791408181190491
Epoch 1280, training loss: 838.2879028320312 = 0.4176349639892578 + 100.0 * 8.378702163696289
Epoch 1280, val loss: 0.47867682576179504
Epoch 1290, training loss: 838.1090087890625 = 0.41662514209747314 + 100.0 * 8.376923561096191
Epoch 1290, val loss: 0.4781695604324341
Epoch 1300, training loss: 838.0164794921875 = 0.41564449667930603 + 100.0 * 8.376008033752441
Epoch 1300, val loss: 0.47779810428619385
Epoch 1310, training loss: 838.0056762695312 = 0.4146758019924164 + 100.0 * 8.375909805297852
Epoch 1310, val loss: 0.47728171944618225
Epoch 1320, training loss: 838.0189819335938 = 0.4137101173400879 + 100.0 * 8.376052856445312
Epoch 1320, val loss: 0.4769378900527954
Epoch 1330, training loss: 837.8395385742188 = 0.4127545654773712 + 100.0 * 8.374267578125
Epoch 1330, val loss: 0.47651591897010803
Epoch 1340, training loss: 837.7965087890625 = 0.4118172228336334 + 100.0 * 8.373847007751465
Epoch 1340, val loss: 0.4761298894882202
Epoch 1350, training loss: 838.0205688476562 = 0.4108833074569702 + 100.0 * 8.376096725463867
Epoch 1350, val loss: 0.47568216919898987
Epoch 1360, training loss: 837.6968994140625 = 0.40995165705680847 + 100.0 * 8.372869491577148
Epoch 1360, val loss: 0.4753619432449341
Epoch 1370, training loss: 837.6571655273438 = 0.4090384244918823 + 100.0 * 8.372481346130371
Epoch 1370, val loss: 0.4749603867530823
Epoch 1380, training loss: 837.6604614257812 = 0.4081408381462097 + 100.0 * 8.372523307800293
Epoch 1380, val loss: 0.47457319498062134
Epoch 1390, training loss: 837.6182861328125 = 0.4072440266609192 + 100.0 * 8.372110366821289
Epoch 1390, val loss: 0.4742100238800049
Epoch 1400, training loss: 837.5252075195312 = 0.4063560962677002 + 100.0 * 8.371188163757324
Epoch 1400, val loss: 0.4738883078098297
Epoch 1410, training loss: 837.4183959960938 = 0.405476838350296 + 100.0 * 8.370129585266113
Epoch 1410, val loss: 0.47346898913383484
Epoch 1420, training loss: 837.4168090820312 = 0.4046139121055603 + 100.0 * 8.370121955871582
Epoch 1420, val loss: 0.473175972700119
Epoch 1430, training loss: 837.5763549804688 = 0.40374237298965454 + 100.0 * 8.371726036071777
Epoch 1430, val loss: 0.4728274345397949
Epoch 1440, training loss: 837.4593505859375 = 0.40286174416542053 + 100.0 * 8.370565414428711
Epoch 1440, val loss: 0.4724157154560089
Epoch 1450, training loss: 837.2638549804688 = 0.40199366211891174 + 100.0 * 8.368618965148926
Epoch 1450, val loss: 0.4720430374145508
Epoch 1460, training loss: 837.1549682617188 = 0.4011419713497162 + 100.0 * 8.367538452148438
Epoch 1460, val loss: 0.4717261493206024
Epoch 1470, training loss: 837.1603393554688 = 0.40029895305633545 + 100.0 * 8.367600440979004
Epoch 1470, val loss: 0.47135868668556213
Epoch 1480, training loss: 837.2684326171875 = 0.3994433581829071 + 100.0 * 8.368690490722656
Epoch 1480, val loss: 0.47102412581443787
Epoch 1490, training loss: 837.0359497070312 = 0.39859819412231445 + 100.0 * 8.366373062133789
Epoch 1490, val loss: 0.47068676352500916
Epoch 1500, training loss: 836.9375 = 0.3977587819099426 + 100.0 * 8.365397453308105
Epoch 1500, val loss: 0.4703376591205597
Epoch 1510, training loss: 837.1898803710938 = 0.3969288468360901 + 100.0 * 8.367929458618164
Epoch 1510, val loss: 0.46991127729415894
Epoch 1520, training loss: 837.0258178710938 = 0.3960772156715393 + 100.0 * 8.366297721862793
Epoch 1520, val loss: 0.4696671962738037
Epoch 1530, training loss: 836.821044921875 = 0.39523544907569885 + 100.0 * 8.3642578125
Epoch 1530, val loss: 0.46930786967277527
Epoch 1540, training loss: 836.7575073242188 = 0.3944163918495178 + 100.0 * 8.363631248474121
Epoch 1540, val loss: 0.469014436006546
Epoch 1550, training loss: 836.7364501953125 = 0.39360639452934265 + 100.0 * 8.363428115844727
Epoch 1550, val loss: 0.4686974883079529
Epoch 1560, training loss: 837.0771484375 = 0.39279547333717346 + 100.0 * 8.366843223571777
Epoch 1560, val loss: 0.46840038895606995
Epoch 1570, training loss: 836.6907348632812 = 0.39195477962493896 + 100.0 * 8.362987518310547
Epoch 1570, val loss: 0.4679208993911743
Epoch 1580, training loss: 836.5541381835938 = 0.3911498188972473 + 100.0 * 8.361629486083984
Epoch 1580, val loss: 0.4676579236984253
Epoch 1590, training loss: 836.6140747070312 = 0.39035335183143616 + 100.0 * 8.362236976623535
Epoch 1590, val loss: 0.46727630496025085
Epoch 1600, training loss: 836.6343994140625 = 0.38952895998954773 + 100.0 * 8.362448692321777
Epoch 1600, val loss: 0.4669426679611206
Epoch 1610, training loss: 836.4691162109375 = 0.3887142837047577 + 100.0 * 8.360803604125977
Epoch 1610, val loss: 0.46658632159233093
Epoch 1620, training loss: 836.4261474609375 = 0.38791701197624207 + 100.0 * 8.360382080078125
Epoch 1620, val loss: 0.4662804901599884
Epoch 1630, training loss: 836.3456420898438 = 0.3871212303638458 + 100.0 * 8.35958480834961
Epoch 1630, val loss: 0.4659258723258972
Epoch 1640, training loss: 836.346923828125 = 0.3863341808319092 + 100.0 * 8.35960578918457
Epoch 1640, val loss: 0.46564534306526184
Epoch 1650, training loss: 837.0259399414062 = 0.38553598523139954 + 100.0 * 8.366403579711914
Epoch 1650, val loss: 0.4653477370738983
Epoch 1660, training loss: 836.3551025390625 = 0.38469648361206055 + 100.0 * 8.35970401763916
Epoch 1660, val loss: 0.4649181067943573
Epoch 1670, training loss: 836.1990356445312 = 0.3839013874530792 + 100.0 * 8.35815143585205
Epoch 1670, val loss: 0.4645988941192627
Epoch 1680, training loss: 836.17138671875 = 0.3831231892108917 + 100.0 * 8.357882499694824
Epoch 1680, val loss: 0.4642569422721863
Epoch 1690, training loss: 836.1195678710938 = 0.38234835863113403 + 100.0 * 8.357372283935547
Epoch 1690, val loss: 0.4639371335506439
Epoch 1700, training loss: 836.140869140625 = 0.3815750181674957 + 100.0 * 8.357592582702637
Epoch 1700, val loss: 0.463661253452301
Epoch 1710, training loss: 836.3678588867188 = 0.3807796835899353 + 100.0 * 8.359870910644531
Epoch 1710, val loss: 0.4633406102657318
Epoch 1720, training loss: 836.0518188476562 = 0.37997695803642273 + 100.0 * 8.356718063354492
Epoch 1720, val loss: 0.4629672169685364
Epoch 1730, training loss: 836.0189819335938 = 0.3792007863521576 + 100.0 * 8.35639762878418
Epoch 1730, val loss: 0.46260330080986023
Epoch 1740, training loss: 835.9337158203125 = 0.3784406781196594 + 100.0 * 8.355552673339844
Epoch 1740, val loss: 0.46230348944664
Epoch 1750, training loss: 835.9486694335938 = 0.3776824474334717 + 100.0 * 8.35571002960205
Epoch 1750, val loss: 0.4619624614715576
Epoch 1760, training loss: 836.3103637695312 = 0.376915842294693 + 100.0 * 8.359334945678711
Epoch 1760, val loss: 0.46165090799331665
Epoch 1770, training loss: 835.8867797851562 = 0.37614136934280396 + 100.0 * 8.355106353759766
Epoch 1770, val loss: 0.4613005816936493
Epoch 1780, training loss: 835.8287963867188 = 0.3753878176212311 + 100.0 * 8.354534149169922
Epoch 1780, val loss: 0.46110275387763977
Epoch 1790, training loss: 835.7840576171875 = 0.37464573979377747 + 100.0 * 8.354094505310059
Epoch 1790, val loss: 0.4607415497303009
Epoch 1800, training loss: 835.80126953125 = 0.3739064633846283 + 100.0 * 8.354273796081543
Epoch 1800, val loss: 0.46047070622444153
Epoch 1810, training loss: 836.1014404296875 = 0.37315669655799866 + 100.0 * 8.357282638549805
Epoch 1810, val loss: 0.460199236869812
Epoch 1820, training loss: 835.7864990234375 = 0.3723936378955841 + 100.0 * 8.354141235351562
Epoch 1820, val loss: 0.45977097749710083
Epoch 1830, training loss: 835.658935546875 = 0.3716537356376648 + 100.0 * 8.352872848510742
Epoch 1830, val loss: 0.4594784379005432
Epoch 1840, training loss: 835.6737670898438 = 0.37091532349586487 + 100.0 * 8.353028297424316
Epoch 1840, val loss: 0.4591655433177948
Epoch 1850, training loss: 835.732421875 = 0.3701708912849426 + 100.0 * 8.353622436523438
Epoch 1850, val loss: 0.45881974697113037
Epoch 1860, training loss: 835.66552734375 = 0.36942100524902344 + 100.0 * 8.352960586547852
Epoch 1860, val loss: 0.45857957005500793
Epoch 1870, training loss: 835.6732177734375 = 0.368672251701355 + 100.0 * 8.353045463562012
Epoch 1870, val loss: 0.4582582414150238
Epoch 1880, training loss: 835.5076904296875 = 0.3679194450378418 + 100.0 * 8.351397514343262
Epoch 1880, val loss: 0.4579237699508667
Epoch 1890, training loss: 835.4721069335938 = 0.3671758770942688 + 100.0 * 8.351049423217773
Epoch 1890, val loss: 0.45759546756744385
Epoch 1900, training loss: 835.5380859375 = 0.3664339780807495 + 100.0 * 8.351716041564941
Epoch 1900, val loss: 0.4572241008281708
Epoch 1910, training loss: 835.5037231445312 = 0.3656843602657318 + 100.0 * 8.351380348205566
Epoch 1910, val loss: 0.45695510506629944
Epoch 1920, training loss: 835.6961059570312 = 0.3649446368217468 + 100.0 * 8.353311538696289
Epoch 1920, val loss: 0.45678699016571045
Epoch 1930, training loss: 835.3802490234375 = 0.36416736245155334 + 100.0 * 8.350160598754883
Epoch 1930, val loss: 0.4562513530254364
Epoch 1940, training loss: 835.3240356445312 = 0.36341917514801025 + 100.0 * 8.34960651397705
Epoch 1940, val loss: 0.4561353027820587
Epoch 1950, training loss: 835.2957763671875 = 0.3626812696456909 + 100.0 * 8.34933090209961
Epoch 1950, val loss: 0.45567426085472107
Epoch 1960, training loss: 835.2555541992188 = 0.3619547486305237 + 100.0 * 8.348936080932617
Epoch 1960, val loss: 0.45546936988830566
Epoch 1970, training loss: 835.2254638671875 = 0.36122992634773254 + 100.0 * 8.348642349243164
Epoch 1970, val loss: 0.4551710784435272
Epoch 1980, training loss: 835.4661865234375 = 0.3604999780654907 + 100.0 * 8.351057052612305
Epoch 1980, val loss: 0.45482131838798523
Epoch 1990, training loss: 835.2633666992188 = 0.35974183678627014 + 100.0 * 8.34903621673584
Epoch 1990, val loss: 0.45459961891174316
Epoch 2000, training loss: 835.268798828125 = 0.3589952290058136 + 100.0 * 8.349098205566406
Epoch 2000, val loss: 0.45424047112464905
Epoch 2010, training loss: 835.2578735351562 = 0.3582606017589569 + 100.0 * 8.34899616241455
Epoch 2010, val loss: 0.4539250135421753
Epoch 2020, training loss: 835.1788330078125 = 0.35752394795417786 + 100.0 * 8.348213195800781
Epoch 2020, val loss: 0.45368239283561707
Epoch 2030, training loss: 835.1395874023438 = 0.35679319500923157 + 100.0 * 8.347827911376953
Epoch 2030, val loss: 0.45334091782569885
Epoch 2040, training loss: 835.0957641601562 = 0.3560692071914673 + 100.0 * 8.347396850585938
Epoch 2040, val loss: 0.45308512449264526
Epoch 2050, training loss: 835.4417114257812 = 0.35534602403640747 + 100.0 * 8.350863456726074
Epoch 2050, val loss: 0.45287564396858215
Epoch 2060, training loss: 835.1126098632812 = 0.35459381341934204 + 100.0 * 8.347579956054688
Epoch 2060, val loss: 0.45247960090637207
Epoch 2070, training loss: 835.0108642578125 = 0.353859543800354 + 100.0 * 8.346570014953613
Epoch 2070, val loss: 0.4522243142127991
Epoch 2080, training loss: 834.9666748046875 = 0.3531416952610016 + 100.0 * 8.346135139465332
Epoch 2080, val loss: 0.451907217502594
Epoch 2090, training loss: 834.9154052734375 = 0.3524239659309387 + 100.0 * 8.345629692077637
Epoch 2090, val loss: 0.4516329765319824
Epoch 2100, training loss: 834.9238891601562 = 0.3517119288444519 + 100.0 * 8.345722198486328
Epoch 2100, val loss: 0.45140036940574646
Epoch 2110, training loss: 835.7470092773438 = 0.35099583864212036 + 100.0 * 8.353960037231445
Epoch 2110, val loss: 0.45122405886650085
Epoch 2120, training loss: 835.001953125 = 0.35023045539855957 + 100.0 * 8.346517562866211
Epoch 2120, val loss: 0.45075157284736633
Epoch 2130, training loss: 834.9252319335938 = 0.34949368238449097 + 100.0 * 8.345757484436035
Epoch 2130, val loss: 0.4503885805606842
Epoch 2140, training loss: 834.8063354492188 = 0.34877538681030273 + 100.0 * 8.344575881958008
Epoch 2140, val loss: 0.4501579701900482
Epoch 2150, training loss: 834.8275146484375 = 0.3480605185031891 + 100.0 * 8.344794273376465
Epoch 2150, val loss: 0.4498665928840637
Epoch 2160, training loss: 835.300537109375 = 0.34733760356903076 + 100.0 * 8.349532127380371
Epoch 2160, val loss: 0.4495406448841095
Epoch 2170, training loss: 834.8274536132812 = 0.34659379720687866 + 100.0 * 8.344808578491211
Epoch 2170, val loss: 0.44938144087791443
Epoch 2180, training loss: 834.718017578125 = 0.3458581268787384 + 100.0 * 8.343721389770508
Epoch 2180, val loss: 0.4490196704864502
Epoch 2190, training loss: 834.6697998046875 = 0.3451351821422577 + 100.0 * 8.343246459960938
Epoch 2190, val loss: 0.4488599896430969
Epoch 2200, training loss: 834.6494140625 = 0.34441864490509033 + 100.0 * 8.343050003051758
Epoch 2200, val loss: 0.4485349953174591
Epoch 2210, training loss: 834.6344604492188 = 0.3437003195285797 + 100.0 * 8.342907905578613
Epoch 2210, val loss: 0.4483122229576111
Epoch 2220, training loss: 835.3388671875 = 0.34297531843185425 + 100.0 * 8.349959373474121
Epoch 2220, val loss: 0.4480767250061035
Epoch 2230, training loss: 834.754150390625 = 0.34223219752311707 + 100.0 * 8.34411907196045
Epoch 2230, val loss: 0.4477280378341675
Epoch 2240, training loss: 834.5623779296875 = 0.3415025770664215 + 100.0 * 8.342208862304688
Epoch 2240, val loss: 0.44752293825149536
Epoch 2250, training loss: 834.5578002929688 = 0.3407853841781616 + 100.0 * 8.342170715332031
Epoch 2250, val loss: 0.44730761647224426
Epoch 2260, training loss: 834.5905151367188 = 0.34007078409194946 + 100.0 * 8.342504501342773
Epoch 2260, val loss: 0.4470628798007965
Epoch 2270, training loss: 834.7106323242188 = 0.33934351801872253 + 100.0 * 8.34371280670166
Epoch 2270, val loss: 0.4467586874961853
Epoch 2280, training loss: 834.9931640625 = 0.33860427141189575 + 100.0 * 8.346545219421387
Epoch 2280, val loss: 0.44638726115226746
Epoch 2290, training loss: 834.5253295898438 = 0.3378579020500183 + 100.0 * 8.341875076293945
Epoch 2290, val loss: 0.44628778100013733
Epoch 2300, training loss: 834.4086303710938 = 0.33711302280426025 + 100.0 * 8.340715408325195
Epoch 2300, val loss: 0.4459519684314728
Epoch 2310, training loss: 834.37548828125 = 0.33638104796409607 + 100.0 * 8.340391159057617
Epoch 2310, val loss: 0.4457530975341797
Epoch 2320, training loss: 834.332763671875 = 0.33564871549606323 + 100.0 * 8.339971542358398
Epoch 2320, val loss: 0.4454808831214905
Epoch 2330, training loss: 834.3053588867188 = 0.3349144160747528 + 100.0 * 8.339704513549805
Epoch 2330, val loss: 0.44523799419403076
Epoch 2340, training loss: 834.3875732421875 = 0.33417758345603943 + 100.0 * 8.340534210205078
Epoch 2340, val loss: 0.4450427293777466
Epoch 2350, training loss: 834.6989135742188 = 0.3334231674671173 + 100.0 * 8.34365463256836
Epoch 2350, val loss: 0.4447977840900421
Epoch 2360, training loss: 834.3624267578125 = 0.33264657855033875 + 100.0 * 8.34029769897461
Epoch 2360, val loss: 0.44450539350509644
Epoch 2370, training loss: 834.2862548828125 = 0.33189070224761963 + 100.0 * 8.339543342590332
Epoch 2370, val loss: 0.44417309761047363
Epoch 2380, training loss: 834.18994140625 = 0.33114832639694214 + 100.0 * 8.338587760925293
Epoch 2380, val loss: 0.44392767548561096
Epoch 2390, training loss: 834.1842651367188 = 0.33040767908096313 + 100.0 * 8.338539123535156
Epoch 2390, val loss: 0.443722128868103
Epoch 2400, training loss: 834.4891357421875 = 0.32966217398643494 + 100.0 * 8.341594696044922
Epoch 2400, val loss: 0.44338348507881165
Epoch 2410, training loss: 834.1232299804688 = 0.32889604568481445 + 100.0 * 8.337943077087402
Epoch 2410, val loss: 0.44326111674308777
Epoch 2420, training loss: 834.102294921875 = 0.3281398415565491 + 100.0 * 8.33774185180664
Epoch 2420, val loss: 0.44302046298980713
Epoch 2430, training loss: 834.1463012695312 = 0.32738634943962097 + 100.0 * 8.338189125061035
Epoch 2430, val loss: 0.4427753984928131
Epoch 2440, training loss: 834.1652221679688 = 0.3266259431838989 + 100.0 * 8.338386535644531
Epoch 2440, val loss: 0.4425593316555023
Epoch 2450, training loss: 834.0399780273438 = 0.32586604356765747 + 100.0 * 8.337141036987305
Epoch 2450, val loss: 0.4424354135990143
Epoch 2460, training loss: 834.0243530273438 = 0.32510656118392944 + 100.0 * 8.336992263793945
Epoch 2460, val loss: 0.44216594099998474
Epoch 2470, training loss: 834.6732788085938 = 0.324344277381897 + 100.0 * 8.343489646911621
Epoch 2470, val loss: 0.44193100929260254
Epoch 2480, training loss: 834.142578125 = 0.3235669732093811 + 100.0 * 8.338190078735352
Epoch 2480, val loss: 0.44181564450263977
Epoch 2490, training loss: 833.9775390625 = 0.3227832019329071 + 100.0 * 8.3365478515625
Epoch 2490, val loss: 0.44148021936416626
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8183663115169963
0.8643773092805912
The final CL Acc:0.80839, 0.00829, The final GNN Acc:0.86406, 0.00024
Begin epxeriment: noisy_level: 0.2 cont_weight: 100 epoch: der1:0.3 der2:0 dfr1:0.4 dfr2:0.3
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=2500, cl_num_layers=2, cl_num_proj_hidden=64, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=100.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.0, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=32, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, noisy_level=0.2, num_hidden=64, num_proj_hidden=64, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=40, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 88648])
add edge: torch.Size([2, 106640])
remove edge: torch.Size([2, 70770])
updated graph: torch.Size([2, 88762])
=== Noisy graph ===
/home/project-robust-contrastive/Robust-Contrastive-Learning/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 1059.322509765625 = 1.092212200164795 + 100.0 * 10.582304000854492
Epoch 0, val loss: 1.0933119058609009
Epoch 10, training loss: 1059.29443359375 = 1.0882312059402466 + 100.0 * 10.582061767578125
Epoch 10, val loss: 1.0892575979232788
Epoch 20, training loss: 1059.1842041015625 = 1.0838946104049683 + 100.0 * 10.581003189086914
Epoch 20, val loss: 1.0848362445831299
Epoch 30, training loss: 1058.7017822265625 = 1.0790557861328125 + 100.0 * 10.576226234436035
Epoch 30, val loss: 1.079888105392456
Epoch 40, training loss: 1056.78564453125 = 1.0737874507904053 + 100.0 * 10.557119369506836
Epoch 40, val loss: 1.0745067596435547
Epoch 50, training loss: 1051.11962890625 = 1.0680197477340698 + 100.0 * 10.500516891479492
Epoch 50, val loss: 1.0685547590255737
Epoch 60, training loss: 1038.4466552734375 = 1.062201738357544 + 100.0 * 10.373844146728516
Epoch 60, val loss: 1.0626370906829834
Epoch 70, training loss: 1015.0099487304688 = 1.0569722652435303 + 100.0 * 10.139530181884766
Epoch 70, val loss: 1.0571564435958862
Epoch 80, training loss: 991.3307495117188 = 1.0517460107803345 + 100.0 * 9.902790069580078
Epoch 80, val loss: 1.051756739616394
Epoch 90, training loss: 960.2196655273438 = 1.0471389293670654 + 100.0 * 9.59172534942627
Epoch 90, val loss: 1.0472302436828613
Epoch 100, training loss: 935.8864135742188 = 1.0428582429885864 + 100.0 * 9.348435401916504
Epoch 100, val loss: 1.0432225465774536
Epoch 110, training loss: 926.4290771484375 = 1.0393092632293701 + 100.0 * 9.253897666931152
Epoch 110, val loss: 1.0399168729782104
Epoch 120, training loss: 923.3109130859375 = 1.0362664461135864 + 100.0 * 9.222746849060059
Epoch 120, val loss: 1.0369738340377808
Epoch 130, training loss: 919.9606323242188 = 1.0331131219863892 + 100.0 * 9.189274787902832
Epoch 130, val loss: 1.0338406562805176
Epoch 140, training loss: 915.1017456054688 = 1.0299700498580933 + 100.0 * 9.140717506408691
Epoch 140, val loss: 1.0307425260543823
Epoch 150, training loss: 907.7650146484375 = 1.0274370908737183 + 100.0 * 9.067375183105469
Epoch 150, val loss: 1.0283586978912354
Epoch 160, training loss: 899.7562255859375 = 1.0258831977844238 + 100.0 * 8.987303733825684
Epoch 160, val loss: 1.026896357536316
Epoch 170, training loss: 894.6078491210938 = 1.02448308467865 + 100.0 * 8.935833930969238
Epoch 170, val loss: 1.0254472494125366
Epoch 180, training loss: 890.5068359375 = 1.0223398208618164 + 100.0 * 8.894845008850098
Epoch 180, val loss: 1.0231355428695679
Epoch 190, training loss: 888.4705810546875 = 1.0195344686508179 + 100.0 * 8.874510765075684
Epoch 190, val loss: 1.0201703310012817
Epoch 200, training loss: 886.94287109375 = 1.0161795616149902 + 100.0 * 8.859267234802246
Epoch 200, val loss: 1.0168050527572632
Epoch 210, training loss: 885.4697875976562 = 1.0127370357513428 + 100.0 * 8.84457015991211
Epoch 210, val loss: 1.0134479999542236
Epoch 220, training loss: 883.85546875 = 1.009404182434082 + 100.0 * 8.828460693359375
Epoch 220, val loss: 1.0102343559265137
Epoch 230, training loss: 881.7423706054688 = 1.0062952041625977 + 100.0 * 8.807360649108887
Epoch 230, val loss: 1.0072439908981323
Epoch 240, training loss: 879.044189453125 = 1.0034575462341309 + 100.0 * 8.780406951904297
Epoch 240, val loss: 1.0045143365859985
Epoch 250, training loss: 875.674072265625 = 1.0009517669677734 + 100.0 * 8.74673080444336
Epoch 250, val loss: 1.0021026134490967
Epoch 260, training loss: 872.2647705078125 = 0.9987059831619263 + 100.0 * 8.712660789489746
Epoch 260, val loss: 0.999891459941864
Epoch 270, training loss: 869.7501220703125 = 0.996030330657959 + 100.0 * 8.687541007995605
Epoch 270, val loss: 0.9971476793289185
Epoch 280, training loss: 867.75732421875 = 0.9924730658531189 + 100.0 * 8.667648315429688
Epoch 280, val loss: 0.9935770034790039
Epoch 290, training loss: 866.174072265625 = 0.9885255694389343 + 100.0 * 8.65185546875
Epoch 290, val loss: 0.9896478652954102
Epoch 300, training loss: 864.9441528320312 = 0.9842310547828674 + 100.0 * 8.639598846435547
Epoch 300, val loss: 0.985397219657898
Epoch 310, training loss: 863.9004516601562 = 0.9796203970909119 + 100.0 * 8.6292085647583
Epoch 310, val loss: 0.9808571934700012
Epoch 320, training loss: 863.16748046875 = 0.974712610244751 + 100.0 * 8.621927261352539
Epoch 320, val loss: 0.976044774055481
Epoch 330, training loss: 862.224609375 = 0.9695985913276672 + 100.0 * 8.612549781799316
Epoch 330, val loss: 0.9710091352462769
Epoch 340, training loss: 861.2861328125 = 0.9643601775169373 + 100.0 * 8.603218078613281
Epoch 340, val loss: 0.9658836126327515
Epoch 350, training loss: 860.4637451171875 = 0.959036111831665 + 100.0 * 8.595046997070312
Epoch 350, val loss: 0.9606804251670837
Epoch 360, training loss: 859.6982421875 = 0.9535589814186096 + 100.0 * 8.587447166442871
Epoch 360, val loss: 0.9552826285362244
Epoch 370, training loss: 859.0416259765625 = 0.9478017091751099 + 100.0 * 8.580938339233398
Epoch 370, val loss: 0.9495970010757446
Epoch 380, training loss: 858.4583740234375 = 0.9417901039123535 + 100.0 * 8.575165748596191
Epoch 380, val loss: 0.9436641931533813
Epoch 390, training loss: 858.2007446289062 = 0.9355370998382568 + 100.0 * 8.572651863098145
Epoch 390, val loss: 0.9375057816505432
Epoch 400, training loss: 857.5206909179688 = 0.9290274381637573 + 100.0 * 8.565917015075684
Epoch 400, val loss: 0.9310887455940247
Epoch 410, training loss: 857.113037109375 = 0.9223694205284119 + 100.0 * 8.561906814575195
Epoch 410, val loss: 0.9245067238807678
Epoch 420, training loss: 856.7314453125 = 0.9155539274215698 + 100.0 * 8.558158874511719
Epoch 420, val loss: 0.9177750945091248
Epoch 430, training loss: 856.3536376953125 = 0.9086088538169861 + 100.0 * 8.554450035095215
Epoch 430, val loss: 0.9109178781509399
Epoch 440, training loss: 856.3252563476562 = 0.9015843272209167 + 100.0 * 8.55423641204834
Epoch 440, val loss: 0.9039701223373413
Epoch 450, training loss: 855.624755859375 = 0.8943994045257568 + 100.0 * 8.547303199768066
Epoch 450, val loss: 0.8968609571456909
Epoch 460, training loss: 855.2135009765625 = 0.8872689008712769 + 100.0 * 8.543262481689453
Epoch 460, val loss: 0.8897957801818848
Epoch 470, training loss: 854.8113403320312 = 0.8801514506340027 + 100.0 * 8.539312362670898
Epoch 470, val loss: 0.8827400207519531
Epoch 480, training loss: 854.4038696289062 = 0.8730117678642273 + 100.0 * 8.535308837890625
Epoch 480, val loss: 0.8756614923477173
Epoch 490, training loss: 853.9810180664062 = 0.8658655285835266 + 100.0 * 8.53115177154541
Epoch 490, val loss: 0.8685605525970459
Epoch 500, training loss: 853.6258544921875 = 0.8586671948432922 + 100.0 * 8.527671813964844
Epoch 500, val loss: 0.8614048361778259
Epoch 510, training loss: 853.2088012695312 = 0.8514130711555481 + 100.0 * 8.523573875427246
Epoch 510, val loss: 0.8542150259017944
Epoch 520, training loss: 852.8552856445312 = 0.8441947102546692 + 100.0 * 8.520111083984375
Epoch 520, val loss: 0.8470380306243896
Epoch 530, training loss: 852.4915161132812 = 0.8369711637496948 + 100.0 * 8.516545295715332
Epoch 530, val loss: 0.8398432731628418
Epoch 540, training loss: 852.1388549804688 = 0.8296408653259277 + 100.0 * 8.513092041015625
Epoch 540, val loss: 0.8325145840644836
Epoch 550, training loss: 851.9002685546875 = 0.8222672343254089 + 100.0 * 8.510780334472656
Epoch 550, val loss: 0.8251615762710571
Epoch 560, training loss: 851.4856567382812 = 0.8149930238723755 + 100.0 * 8.506706237792969
Epoch 560, val loss: 0.8179328441619873
Epoch 570, training loss: 851.10009765625 = 0.8077811002731323 + 100.0 * 8.502923011779785
Epoch 570, val loss: 0.81075119972229
Epoch 580, training loss: 850.761962890625 = 0.8005824685096741 + 100.0 * 8.499613761901855
Epoch 580, val loss: 0.8035795092582703
Epoch 590, training loss: 850.451416015625 = 0.7933681011199951 + 100.0 * 8.496580123901367
Epoch 590, val loss: 0.7963965535163879
Epoch 600, training loss: 850.16064453125 = 0.7861631512641907 + 100.0 * 8.493744850158691
Epoch 600, val loss: 0.7892207503318787
Epoch 610, training loss: 850.0895385742188 = 0.7789505124092102 + 100.0 * 8.4931058883667
Epoch 610, val loss: 0.7820415496826172
Epoch 620, training loss: 849.704345703125 = 0.771724283695221 + 100.0 * 8.489326477050781
Epoch 620, val loss: 0.7748837471008301
Epoch 630, training loss: 849.43896484375 = 0.7645727396011353 + 100.0 * 8.486743927001953
Epoch 630, val loss: 0.7677813768386841
Epoch 640, training loss: 850.0269165039062 = 0.7574578523635864 + 100.0 * 8.492694854736328
Epoch 640, val loss: 0.7607014179229736
Epoch 650, training loss: 849.1528930664062 = 0.7503224015235901 + 100.0 * 8.484025955200195
Epoch 650, val loss: 0.7536376714706421
Epoch 660, training loss: 848.9412841796875 = 0.7433520555496216 + 100.0 * 8.481979370117188
Epoch 660, val loss: 0.7467563152313232
Epoch 670, training loss: 848.701904296875 = 0.7365527749061584 + 100.0 * 8.479653358459473
Epoch 670, val loss: 0.7400580644607544
Epoch 680, training loss: 848.5020141601562 = 0.7298907041549683 + 100.0 * 8.477721214294434
Epoch 680, val loss: 0.733508288860321
Epoch 690, training loss: 848.3121337890625 = 0.7233684659004211 + 100.0 * 8.475887298583984
Epoch 690, val loss: 0.7271039485931396
Epoch 700, training loss: 848.1236572265625 = 0.7169860601425171 + 100.0 * 8.474066734313965
Epoch 700, val loss: 0.7208378911018372
Epoch 710, training loss: 848.35107421875 = 0.7107435464859009 + 100.0 * 8.47640323638916
Epoch 710, val loss: 0.7146929502487183
Epoch 720, training loss: 847.8448486328125 = 0.7045413851737976 + 100.0 * 8.471403121948242
Epoch 720, val loss: 0.7086625099182129
Epoch 730, training loss: 847.6530151367188 = 0.6985679864883423 + 100.0 * 8.469544410705566
Epoch 730, val loss: 0.7028480172157288
Epoch 740, training loss: 847.4490356445312 = 0.6928752064704895 + 100.0 * 8.467561721801758
Epoch 740, val loss: 0.6973018646240234
Epoch 750, training loss: 847.25390625 = 0.6873351335525513 + 100.0 * 8.465665817260742
Epoch 750, val loss: 0.691909670829773
Epoch 760, training loss: 847.0922241210938 = 0.6819528341293335 + 100.0 * 8.464102745056152
Epoch 760, val loss: 0.6866939663887024
Epoch 770, training loss: 847.6705932617188 = 0.6766680479049683 + 100.0 * 8.469939231872559
Epoch 770, val loss: 0.6815779209136963
Epoch 780, training loss: 846.8117065429688 = 0.6715176701545715 + 100.0 * 8.46140193939209
Epoch 780, val loss: 0.6766125559806824
Epoch 790, training loss: 846.6632080078125 = 0.6665869951248169 + 100.0 * 8.459966659545898
Epoch 790, val loss: 0.6718959808349609
Epoch 800, training loss: 846.4642944335938 = 0.6618444919586182 + 100.0 * 8.458024024963379
Epoch 800, val loss: 0.6673334240913391
Epoch 810, training loss: 846.3079833984375 = 0.6572675704956055 + 100.0 * 8.456506729125977
Epoch 810, val loss: 0.6629780530929565
Epoch 820, training loss: 846.1520385742188 = 0.6528275609016418 + 100.0 * 8.454992294311523
Epoch 820, val loss: 0.658726155757904
Epoch 830, training loss: 846.0354614257812 = 0.648507297039032 + 100.0 * 8.453869819641113
Epoch 830, val loss: 0.6546146869659424
Epoch 840, training loss: 846.352783203125 = 0.6442684531211853 + 100.0 * 8.457084655761719
Epoch 840, val loss: 0.6506085395812988
Epoch 850, training loss: 845.8685913085938 = 0.6401709318161011 + 100.0 * 8.45228385925293
Epoch 850, val loss: 0.646704912185669
Epoch 860, training loss: 845.6245727539062 = 0.636218249797821 + 100.0 * 8.449883460998535
Epoch 860, val loss: 0.6430187225341797
Epoch 870, training loss: 845.4765014648438 = 0.6324726939201355 + 100.0 * 8.448440551757812
Epoch 870, val loss: 0.6395185589790344
Epoch 880, training loss: 845.3452758789062 = 0.6288390159606934 + 100.0 * 8.447164535522461
Epoch 880, val loss: 0.6361270546913147
Epoch 890, training loss: 845.4810180664062 = 0.6253200769424438 + 100.0 * 8.448556900024414
Epoch 890, val loss: 0.6328390836715698
Epoch 900, training loss: 845.2037353515625 = 0.6217859387397766 + 100.0 * 8.445819854736328
Epoch 900, val loss: 0.6295047998428345
Epoch 910, training loss: 845.048095703125 = 0.6184290051460266 + 100.0 * 8.444296836853027
Epoch 910, val loss: 0.6264107823371887
Epoch 920, training loss: 844.850341796875 = 0.6152163743972778 + 100.0 * 8.442351341247559
Epoch 920, val loss: 0.6234209537506104
Epoch 930, training loss: 844.7532958984375 = 0.6121019124984741 + 100.0 * 8.441411972045898
Epoch 930, val loss: 0.620537281036377
Epoch 940, training loss: 844.6632080078125 = 0.6090723276138306 + 100.0 * 8.44054126739502
Epoch 940, val loss: 0.6177260279655457
Epoch 950, training loss: 844.6174926757812 = 0.6061023473739624 + 100.0 * 8.44011402130127
Epoch 950, val loss: 0.6150170564651489
Epoch 960, training loss: 844.7177734375 = 0.6032213568687439 + 100.0 * 8.441145896911621
Epoch 960, val loss: 0.6124013662338257
Epoch 970, training loss: 844.592529296875 = 0.6002435684204102 + 100.0 * 8.439923286437988
Epoch 970, val loss: 0.609595775604248
Epoch 980, training loss: 844.2981567382812 = 0.597494900226593 + 100.0 * 8.437006950378418
Epoch 980, val loss: 0.6071175932884216
Epoch 990, training loss: 844.166015625 = 0.594865083694458 + 100.0 * 8.435711860656738
Epoch 990, val loss: 0.6047562956809998
Epoch 1000, training loss: 844.069091796875 = 0.5923266410827637 + 100.0 * 8.434767723083496
Epoch 1000, val loss: 0.6024455428123474
Epoch 1010, training loss: 843.9719848632812 = 0.5898400545120239 + 100.0 * 8.433821678161621
Epoch 1010, val loss: 0.6002107262611389
Epoch 1020, training loss: 843.8871459960938 = 0.5874098539352417 + 100.0 * 8.432997703552246
Epoch 1020, val loss: 0.5980077385902405
Epoch 1030, training loss: 843.82080078125 = 0.5850421190261841 + 100.0 * 8.432357788085938
Epoch 1030, val loss: 0.5958756804466248
Epoch 1040, training loss: 844.3452758789062 = 0.5827364921569824 + 100.0 * 8.43762493133545
Epoch 1040, val loss: 0.593813955783844
Epoch 1050, training loss: 843.752197265625 = 0.5803272128105164 + 100.0 * 8.431718826293945
Epoch 1050, val loss: 0.5916444659233093
Epoch 1060, training loss: 843.6397705078125 = 0.5781322717666626 + 100.0 * 8.43061637878418
Epoch 1060, val loss: 0.589704155921936
Epoch 1070, training loss: 843.518798828125 = 0.5759992599487305 + 100.0 * 8.429428100585938
Epoch 1070, val loss: 0.5878124833106995
Epoch 1080, training loss: 843.450927734375 = 0.5739272236824036 + 100.0 * 8.428770065307617
Epoch 1080, val loss: 0.5859441757202148
Epoch 1090, training loss: 843.3953857421875 = 0.5718958973884583 + 100.0 * 8.428235054016113
Epoch 1090, val loss: 0.5841512680053711
Epoch 1100, training loss: 844.1275024414062 = 0.5698903203010559 + 100.0 * 8.435576438903809
Epoch 1100, val loss: 0.5822774171829224
Epoch 1110, training loss: 843.4780883789062 = 0.5678842067718506 + 100.0 * 8.429101943969727
Epoch 1110, val loss: 0.5806378126144409
Epoch 1120, training loss: 843.2451171875 = 0.5659372210502625 + 100.0 * 8.42679214477539
Epoch 1120, val loss: 0.5789521932601929
Epoch 1130, training loss: 843.1717529296875 = 0.5641202926635742 + 100.0 * 8.42607593536377
Epoch 1130, val loss: 0.5773565769195557
Epoch 1140, training loss: 843.0853271484375 = 0.5623391270637512 + 100.0 * 8.425230026245117
Epoch 1140, val loss: 0.5758019089698792
Epoch 1150, training loss: 843.0260009765625 = 0.5605906844139099 + 100.0 * 8.424654006958008
Epoch 1150, val loss: 0.5743345618247986
Epoch 1160, training loss: 842.9722900390625 = 0.5588682889938354 + 100.0 * 8.424134254455566
Epoch 1160, val loss: 0.5728311538696289
Epoch 1170, training loss: 843.2831420898438 = 0.5571441054344177 + 100.0 * 8.427260398864746
Epoch 1170, val loss: 0.5713432431221008
Epoch 1180, training loss: 843.0105590820312 = 0.555460512638092 + 100.0 * 8.424551010131836
Epoch 1180, val loss: 0.5699096322059631
Epoch 1190, training loss: 842.815185546875 = 0.5537713766098022 + 100.0 * 8.422614097595215
Epoch 1190, val loss: 0.5684027075767517
Epoch 1200, training loss: 842.7593383789062 = 0.5521912574768066 + 100.0 * 8.42207145690918
Epoch 1200, val loss: 0.5671535730361938
Epoch 1210, training loss: 842.6738891601562 = 0.5506408214569092 + 100.0 * 8.421232223510742
Epoch 1210, val loss: 0.5658431649208069
Epoch 1220, training loss: 842.6243896484375 = 0.5491144061088562 + 100.0 * 8.42075252532959
Epoch 1220, val loss: 0.5645668506622314
Epoch 1230, training loss: 842.74853515625 = 0.5476138591766357 + 100.0 * 8.422009468078613
Epoch 1230, val loss: 0.5633636713027954
Epoch 1240, training loss: 842.6229858398438 = 0.5461071133613586 + 100.0 * 8.420768737792969
Epoch 1240, val loss: 0.562140703201294
Epoch 1250, training loss: 842.5709228515625 = 0.5446640849113464 + 100.0 * 8.420262336730957
Epoch 1250, val loss: 0.5609020590782166
Epoch 1260, training loss: 842.596435546875 = 0.5432178974151611 + 100.0 * 8.4205322265625
Epoch 1260, val loss: 0.5597664713859558
Epoch 1270, training loss: 842.4237060546875 = 0.5418581962585449 + 100.0 * 8.418818473815918
Epoch 1270, val loss: 0.5587476491928101
Epoch 1280, training loss: 842.2880249023438 = 0.5405168533325195 + 100.0 * 8.417474746704102
Epoch 1280, val loss: 0.557632565498352
Epoch 1290, training loss: 842.2125244140625 = 0.5392200946807861 + 100.0 * 8.416732788085938
Epoch 1290, val loss: 0.5566280484199524
Epoch 1300, training loss: 842.1640625 = 0.5379467010498047 + 100.0 * 8.416260719299316
Epoch 1300, val loss: 0.555643618106842
Epoch 1310, training loss: 842.4314575195312 = 0.5366514921188354 + 100.0 * 8.41894817352295
Epoch 1310, val loss: 0.5546349287033081
Epoch 1320, training loss: 842.2591552734375 = 0.5353754758834839 + 100.0 * 8.417237281799316
Epoch 1320, val loss: 0.5536004304885864
Epoch 1330, training loss: 842.056640625 = 0.5341347455978394 + 100.0 * 8.4152250289917
Epoch 1330, val loss: 0.552767276763916
Epoch 1340, training loss: 841.9343872070312 = 0.5329283475875854 + 100.0 * 8.41401481628418
Epoch 1340, val loss: 0.551857054233551
Epoch 1350, training loss: 841.8721923828125 = 0.5317341089248657 + 100.0 * 8.41340446472168
Epoch 1350, val loss: 0.5509377121925354
Epoch 1360, training loss: 841.9078369140625 = 0.5305607318878174 + 100.0 * 8.413772583007812
Epoch 1360, val loss: 0.5500431656837463
Epoch 1370, training loss: 842.139404296875 = 0.5293478965759277 + 100.0 * 8.41610050201416
Epoch 1370, val loss: 0.5491217374801636
Epoch 1380, training loss: 841.7601928710938 = 0.5281697511672974 + 100.0 * 8.412320137023926
Epoch 1380, val loss: 0.548304557800293
Epoch 1390, training loss: 841.65625 = 0.5270244479179382 + 100.0 * 8.41129207611084
Epoch 1390, val loss: 0.5475114583969116
Epoch 1400, training loss: 841.6051635742188 = 0.5259253978729248 + 100.0 * 8.410792350769043
Epoch 1400, val loss: 0.5467262268066406
Epoch 1410, training loss: 841.6160888671875 = 0.5248388051986694 + 100.0 * 8.41091251373291
Epoch 1410, val loss: 0.5460366606712341
Epoch 1420, training loss: 841.5911254882812 = 0.5237335562705994 + 100.0 * 8.410674095153809
Epoch 1420, val loss: 0.5452315807342529
Epoch 1430, training loss: 841.4595947265625 = 0.5226048827171326 + 100.0 * 8.409370422363281
Epoch 1430, val loss: 0.5444133877754211
Epoch 1440, training loss: 841.4188232421875 = 0.5215343832969666 + 100.0 * 8.40897274017334
Epoch 1440, val loss: 0.5436388254165649
Epoch 1450, training loss: 841.4371337890625 = 0.5205092430114746 + 100.0 * 8.40916633605957
Epoch 1450, val loss: 0.542928159236908
Epoch 1460, training loss: 841.3029174804688 = 0.5194395780563354 + 100.0 * 8.407835006713867
Epoch 1460, val loss: 0.5422698259353638
Epoch 1470, training loss: 841.3977661132812 = 0.5184196829795837 + 100.0 * 8.408793449401855
Epoch 1470, val loss: 0.5416083931922913
Epoch 1480, training loss: 841.5499877929688 = 0.5172994136810303 + 100.0 * 8.410326957702637
Epoch 1480, val loss: 0.5410089492797852
Epoch 1490, training loss: 841.2522583007812 = 0.5162206292152405 + 100.0 * 8.407360076904297
Epoch 1490, val loss: 0.5402258038520813
Epoch 1500, training loss: 841.1210327148438 = 0.515213668346405 + 100.0 * 8.406058311462402
Epoch 1500, val loss: 0.5395413637161255
Epoch 1510, training loss: 841.0525512695312 = 0.5142495632171631 + 100.0 * 8.405383110046387
Epoch 1510, val loss: 0.5389688014984131
Epoch 1520, training loss: 840.9921875 = 0.513292670249939 + 100.0 * 8.404788970947266
Epoch 1520, val loss: 0.5383561849594116
Epoch 1530, training loss: 840.93017578125 = 0.5123505592346191 + 100.0 * 8.404178619384766
Epoch 1530, val loss: 0.537801206111908
Epoch 1540, training loss: 840.8773193359375 = 0.5114215612411499 + 100.0 * 8.403658866882324
Epoch 1540, val loss: 0.5372646450996399
Epoch 1550, training loss: 840.8549194335938 = 0.5105037093162537 + 100.0 * 8.403444290161133
Epoch 1550, val loss: 0.5367408990859985
Epoch 1560, training loss: 841.5933227539062 = 0.5095788836479187 + 100.0 * 8.410837173461914
Epoch 1560, val loss: 0.5362730026245117
Epoch 1570, training loss: 840.9869384765625 = 0.5085262060165405 + 100.0 * 8.404784202575684
Epoch 1570, val loss: 0.5353532433509827
Epoch 1580, training loss: 840.7232055664062 = 0.5076166987419128 + 100.0 * 8.402155876159668
Epoch 1580, val loss: 0.5348744988441467
Epoch 1590, training loss: 840.6259765625 = 0.5067253112792969 + 100.0 * 8.401192665100098
Epoch 1590, val loss: 0.5342817902565002
Epoch 1600, training loss: 840.5601806640625 = 0.5058529376983643 + 100.0 * 8.400543212890625
Epoch 1600, val loss: 0.533746063709259
Epoch 1610, training loss: 840.53662109375 = 0.5049813985824585 + 100.0 * 8.40031623840332
Epoch 1610, val loss: 0.5331669449806213
Epoch 1620, training loss: 840.7514038085938 = 0.504070520401001 + 100.0 * 8.402473449707031
Epoch 1620, val loss: 0.5325700044631958
Epoch 1630, training loss: 840.4189453125 = 0.503157377243042 + 100.0 * 8.399157524108887
Epoch 1630, val loss: 0.5319827795028687
Epoch 1640, training loss: 840.3953247070312 = 0.5022647380828857 + 100.0 * 8.398930549621582
Epoch 1640, val loss: 0.5315101742744446
Epoch 1650, training loss: 840.3871459960938 = 0.5013723969459534 + 100.0 * 8.398858070373535
Epoch 1650, val loss: 0.5308678150177002
Epoch 1660, training loss: 840.7327270507812 = 0.5004487037658691 + 100.0 * 8.402322769165039
Epoch 1660, val loss: 0.5302490592002869
Epoch 1670, training loss: 840.282958984375 = 0.49953171610832214 + 100.0 * 8.397834777832031
Epoch 1670, val loss: 0.5298148393630981
Epoch 1680, training loss: 840.1875 = 0.49864840507507324 + 100.0 * 8.396888732910156
Epoch 1680, val loss: 0.5292479395866394
Epoch 1690, training loss: 840.1430053710938 = 0.4977925419807434 + 100.0 * 8.396451950073242
Epoch 1690, val loss: 0.5287449359893799
Epoch 1700, training loss: 840.087890625 = 0.4969383478164673 + 100.0 * 8.395909309387207
Epoch 1700, val loss: 0.5282500982284546
Epoch 1710, training loss: 840.0426025390625 = 0.49609076976776123 + 100.0 * 8.395464897155762
Epoch 1710, val loss: 0.5277597308158875
Epoch 1720, training loss: 840.0400390625 = 0.49523842334747314 + 100.0 * 8.395447731018066
Epoch 1720, val loss: 0.5273177623748779
Epoch 1730, training loss: 840.6802978515625 = 0.49437370896339417 + 100.0 * 8.401859283447266
Epoch 1730, val loss: 0.5268791317939758
Epoch 1740, training loss: 840.07958984375 = 0.49337220191955566 + 100.0 * 8.395862579345703
Epoch 1740, val loss: 0.5261322855949402
Epoch 1750, training loss: 839.9164428710938 = 0.49249452352523804 + 100.0 * 8.39423942565918
Epoch 1750, val loss: 0.5257022976875305
Epoch 1760, training loss: 839.8681030273438 = 0.4916587471961975 + 100.0 * 8.39376449584961
Epoch 1760, val loss: 0.5251172780990601
Epoch 1770, training loss: 839.8264770507812 = 0.4908432960510254 + 100.0 * 8.393356323242188
Epoch 1770, val loss: 0.5246699452400208
Epoch 1780, training loss: 839.7788696289062 = 0.4900268018245697 + 100.0 * 8.392888069152832
Epoch 1780, val loss: 0.5242066979408264
Epoch 1790, training loss: 839.7382202148438 = 0.4892024099826813 + 100.0 * 8.39249038696289
Epoch 1790, val loss: 0.5237489342689514
Epoch 1800, training loss: 839.7063598632812 = 0.48838236927986145 + 100.0 * 8.392179489135742
Epoch 1800, val loss: 0.5232986807823181
Epoch 1810, training loss: 839.863037109375 = 0.4875514805316925 + 100.0 * 8.393754959106445
Epoch 1810, val loss: 0.5228316783905029
Epoch 1820, training loss: 839.8344116210938 = 0.4866783618927002 + 100.0 * 8.393477439880371
Epoch 1820, val loss: 0.5221372842788696
Epoch 1830, training loss: 839.7684936523438 = 0.48578891158103943 + 100.0 * 8.392827033996582
Epoch 1830, val loss: 0.5219961404800415
Epoch 1840, training loss: 839.5798950195312 = 0.48495879769325256 + 100.0 * 8.390949249267578
Epoch 1840, val loss: 0.5214271545410156
Epoch 1850, training loss: 839.5441284179688 = 0.48415476083755493 + 100.0 * 8.390600204467773
Epoch 1850, val loss: 0.5209513902664185
Epoch 1860, training loss: 839.5059814453125 = 0.48334944248199463 + 100.0 * 8.390226364135742
Epoch 1860, val loss: 0.5205479264259338
Epoch 1870, training loss: 839.4795532226562 = 0.4825472831726074 + 100.0 * 8.389969825744629
Epoch 1870, val loss: 0.5202064514160156
Epoch 1880, training loss: 839.6959838867188 = 0.48174118995666504 + 100.0 * 8.392142295837402
Epoch 1880, val loss: 0.5198678374290466
Epoch 1890, training loss: 839.453369140625 = 0.4808728098869324 + 100.0 * 8.389724731445312
Epoch 1890, val loss: 0.5192830562591553
Epoch 1900, training loss: 839.4078979492188 = 0.48001331090927124 + 100.0 * 8.389278411865234
Epoch 1900, val loss: 0.5188010931015015
Epoch 1910, training loss: 839.3570556640625 = 0.47920453548431396 + 100.0 * 8.388778686523438
Epoch 1910, val loss: 0.5184589624404907
Epoch 1920, training loss: 839.3206787109375 = 0.4784022569656372 + 100.0 * 8.388422966003418
Epoch 1920, val loss: 0.5180598497390747
Epoch 1930, training loss: 839.3016967773438 = 0.47760313749313354 + 100.0 * 8.388240814208984
Epoch 1930, val loss: 0.517707884311676
Epoch 1940, training loss: 839.589111328125 = 0.4768019914627075 + 100.0 * 8.391122817993164
Epoch 1940, val loss: 0.5174645185470581
Epoch 1950, training loss: 839.2669067382812 = 0.4759153723716736 + 100.0 * 8.387909889221191
Epoch 1950, val loss: 0.5167675018310547
Epoch 1960, training loss: 839.2471923828125 = 0.47507378458976746 + 100.0 * 8.387721061706543
Epoch 1960, val loss: 0.5164178609848022
Epoch 1970, training loss: 839.1993408203125 = 0.47426968812942505 + 100.0 * 8.387250900268555
Epoch 1970, val loss: 0.5160085558891296
Epoch 1980, training loss: 839.138916015625 = 0.4734659492969513 + 100.0 * 8.3866548538208
Epoch 1980, val loss: 0.5156491994857788
Epoch 1990, training loss: 839.3303833007812 = 0.47266191244125366 + 100.0 * 8.388577461242676
Epoch 1990, val loss: 0.5151830315589905
Epoch 2000, training loss: 839.2185668945312 = 0.4718223512172699 + 100.0 * 8.387467384338379
Epoch 2000, val loss: 0.5148069858551025
Epoch 2010, training loss: 839.0889282226562 = 0.4709925353527069 + 100.0 * 8.386178970336914
Epoch 2010, val loss: 0.5144521594047546
Epoch 2020, training loss: 839.0311889648438 = 0.4701928198337555 + 100.0 * 8.38560962677002
Epoch 2020, val loss: 0.5140847563743591
Epoch 2030, training loss: 839.0789184570312 = 0.46940600872039795 + 100.0 * 8.38609504699707
Epoch 2030, val loss: 0.5138078927993774
Epoch 2040, training loss: 839.4358520507812 = 0.4686051607131958 + 100.0 * 8.38967227935791
Epoch 2040, val loss: 0.5134868621826172
Epoch 2050, training loss: 839.0979614257812 = 0.4677433967590332 + 100.0 * 8.38630199432373
Epoch 2050, val loss: 0.5128844976425171
Epoch 2060, training loss: 838.944091796875 = 0.4669651687145233 + 100.0 * 8.384771347045898
Epoch 2060, val loss: 0.5126343369483948
Epoch 2070, training loss: 838.8744506835938 = 0.46618831157684326 + 100.0 * 8.384082794189453
Epoch 2070, val loss: 0.5122894048690796
Epoch 2080, training loss: 838.97607421875 = 0.4654270112514496 + 100.0 * 8.385106086730957
Epoch 2080, val loss: 0.5119606256484985
Epoch 2090, training loss: 838.9175415039062 = 0.46463602781295776 + 100.0 * 8.384529113769531
Epoch 2090, val loss: 0.5116297602653503
Epoch 2100, training loss: 838.8382568359375 = 0.46382802724838257 + 100.0 * 8.383744239807129
Epoch 2100, val loss: 0.5112999081611633
Epoch 2110, training loss: 838.8675537109375 = 0.4630352556705475 + 100.0 * 8.384045600891113
Epoch 2110, val loss: 0.5108413100242615
Epoch 2120, training loss: 838.8487548828125 = 0.4622427523136139 + 100.0 * 8.383865356445312
Epoch 2120, val loss: 0.510493814945221
Epoch 2130, training loss: 838.8435668945312 = 0.46143394708633423 + 100.0 * 8.383821487426758
Epoch 2130, val loss: 0.5103515982627869
Epoch 2140, training loss: 838.9022827148438 = 0.46062347292900085 + 100.0 * 8.384416580200195
Epoch 2140, val loss: 0.5098553895950317
Epoch 2150, training loss: 838.665771484375 = 0.45981383323669434 + 100.0 * 8.382059097290039
Epoch 2150, val loss: 0.5096628069877625
Epoch 2160, training loss: 838.6817626953125 = 0.4590330421924591 + 100.0 * 8.382226943969727
Epoch 2160, val loss: 0.5094452500343323
Epoch 2170, training loss: 838.7091064453125 = 0.45824548602104187 + 100.0 * 8.382508277893066
Epoch 2170, val loss: 0.5091321468353271
Epoch 2180, training loss: 838.7708740234375 = 0.457447350025177 + 100.0 * 8.383133888244629
Epoch 2180, val loss: 0.5089544653892517
Epoch 2190, training loss: 838.6085205078125 = 0.4566246271133423 + 100.0 * 8.381519317626953
Epoch 2190, val loss: 0.5086042284965515
Epoch 2200, training loss: 838.574951171875 = 0.4558248519897461 + 100.0 * 8.38119125366211
Epoch 2200, val loss: 0.5081936717033386
Epoch 2210, training loss: 838.73779296875 = 0.4550386965274811 + 100.0 * 8.382827758789062
Epoch 2210, val loss: 0.5080010890960693
Epoch 2220, training loss: 838.6590576171875 = 0.4542240798473358 + 100.0 * 8.382048606872559
Epoch 2220, val loss: 0.5075713992118835
Epoch 2230, training loss: 838.5596923828125 = 0.45341673493385315 + 100.0 * 8.381062507629395
Epoch 2230, val loss: 0.5073243379592896
Epoch 2240, training loss: 838.483642578125 = 0.4526386559009552 + 100.0 * 8.38031005859375
Epoch 2240, val loss: 0.507043719291687
Epoch 2250, training loss: 838.4486083984375 = 0.45186689496040344 + 100.0 * 8.37996768951416
Epoch 2250, val loss: 0.5067759156227112
Epoch 2260, training loss: 838.8191528320312 = 0.45109087228775024 + 100.0 * 8.38368034362793
Epoch 2260, val loss: 0.5063171982765198
Epoch 2270, training loss: 838.3944702148438 = 0.4502548277378082 + 100.0 * 8.37944221496582
Epoch 2270, val loss: 0.5063578486442566
Epoch 2280, training loss: 838.3764038085938 = 0.4494413435459137 + 100.0 * 8.37926959991455
Epoch 2280, val loss: 0.5058534741401672
Epoch 2290, training loss: 838.3516235351562 = 0.4486708343029022 + 100.0 * 8.379029273986816
Epoch 2290, val loss: 0.5057817101478577
Epoch 2300, training loss: 838.3194580078125 = 0.4478999674320221 + 100.0 * 8.378715515136719
Epoch 2300, val loss: 0.5054891109466553
Epoch 2310, training loss: 838.297119140625 = 0.44713571667671204 + 100.0 * 8.378499984741211
Epoch 2310, val loss: 0.5052948594093323
Epoch 2320, training loss: 838.2822265625 = 0.44637104868888855 + 100.0 * 8.378358840942383
Epoch 2320, val loss: 0.5051130056381226
Epoch 2330, training loss: 838.412353515625 = 0.44560614228248596 + 100.0 * 8.379667282104492
Epoch 2330, val loss: 0.5049871206283569
Epoch 2340, training loss: 838.3976440429688 = 0.4448027014732361 + 100.0 * 8.379528045654297
Epoch 2340, val loss: 0.5047540068626404
Epoch 2350, training loss: 838.40576171875 = 0.44398143887519836 + 100.0 * 8.379617691040039
Epoch 2350, val loss: 0.5042793154716492
Epoch 2360, training loss: 838.1959228515625 = 0.4432056248188019 + 100.0 * 8.377527236938477
Epoch 2360, val loss: 0.5041316151618958
Epoch 2370, training loss: 838.21044921875 = 0.4424542784690857 + 100.0 * 8.377679824829102
Epoch 2370, val loss: 0.5039503574371338
Epoch 2380, training loss: 838.2584228515625 = 0.4416978061199188 + 100.0 * 8.378167152404785
Epoch 2380, val loss: 0.503731906414032
Epoch 2390, training loss: 838.4058227539062 = 0.4409314692020416 + 100.0 * 8.37964916229248
Epoch 2390, val loss: 0.5036531686782837
Epoch 2400, training loss: 838.134033203125 = 0.44014477729797363 + 100.0 * 8.376938819885254
Epoch 2400, val loss: 0.5033464431762695
Epoch 2410, training loss: 838.1343994140625 = 0.4393883943557739 + 100.0 * 8.37695026397705
Epoch 2410, val loss: 0.5029728412628174
Epoch 2420, training loss: 838.0973510742188 = 0.4386414885520935 + 100.0 * 8.3765869140625
Epoch 2420, val loss: 0.5027835965156555
Epoch 2430, training loss: 838.0726318359375 = 0.4378964602947235 + 100.0 * 8.376347541809082
Epoch 2430, val loss: 0.5026060938835144
Epoch 2440, training loss: 838.168212890625 = 0.43715134263038635 + 100.0 * 8.377310752868652
Epoch 2440, val loss: 0.5023613572120667
Epoch 2450, training loss: 838.48779296875 = 0.4363882839679718 + 100.0 * 8.380514144897461
Epoch 2450, val loss: 0.5022234320640564
Epoch 2460, training loss: 838.0895385742188 = 0.43561244010925293 + 100.0 * 8.37653923034668
Epoch 2460, val loss: 0.5019700527191162
Epoch 2470, training loss: 838.0047607421875 = 0.43486469984054565 + 100.0 * 8.375699043273926
Epoch 2470, val loss: 0.5019267797470093
Epoch 2480, training loss: 837.9512939453125 = 0.43413931131362915 + 100.0 * 8.375171661376953
Epoch 2480, val loss: 0.5016148090362549
Epoch 2490, training loss: 837.9228515625 = 0.43341612815856934 + 100.0 * 8.374894142150879
Epoch 2490, val loss: 0.5014954805374146
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7950279046169457
0.8097515032963849
=== training gcn model ===
Epoch 0, training loss: 1059.3377685546875 = 1.107919454574585 + 100.0 * 10.582298278808594
Epoch 0, val loss: 1.1085373163223267
Epoch 10, training loss: 1059.3023681640625 = 1.1028642654418945 + 100.0 * 10.58199405670166
Epoch 10, val loss: 1.1034241914749146
Epoch 20, training loss: 1059.16650390625 = 1.097423791885376 + 100.0 * 10.58069133758545
Epoch 20, val loss: 1.0979198217391968
Epoch 30, training loss: 1058.5615234375 = 1.091554880142212 + 100.0 * 10.574699401855469
Epoch 30, val loss: 1.0919630527496338
Epoch 40, training loss: 1056.0274658203125 = 1.085024356842041 + 100.0 * 10.549424171447754
Epoch 40, val loss: 1.085288405418396
Epoch 50, training loss: 1047.66259765625 = 1.077575445175171 + 100.0 * 10.465850830078125
Epoch 50, val loss: 1.0776915550231934
Epoch 60, training loss: 1024.88330078125 = 1.0704381465911865 + 100.0 * 10.238128662109375
Epoch 60, val loss: 1.0705071687698364
Epoch 70, training loss: 978.770751953125 = 1.0635457038879395 + 100.0 * 9.777071952819824
Epoch 70, val loss: 1.0635290145874023
Epoch 80, training loss: 957.1702270507812 = 1.0580888986587524 + 100.0 * 9.561120986938477
Epoch 80, val loss: 1.058159589767456
Epoch 90, training loss: 947.2161865234375 = 1.053286075592041 + 100.0 * 9.461628913879395
Epoch 90, val loss: 1.0534772872924805
Epoch 100, training loss: 934.1686401367188 = 1.0491595268249512 + 100.0 * 9.331194877624512
Epoch 100, val loss: 1.0495100021362305
Epoch 110, training loss: 921.2725830078125 = 1.0459911823272705 + 100.0 * 9.202265739440918
Epoch 110, val loss: 1.0464856624603271
Epoch 120, training loss: 915.0917358398438 = 1.0432584285736084 + 100.0 * 9.140484809875488
Epoch 120, val loss: 1.0438460111618042
Epoch 130, training loss: 907.0587158203125 = 1.0405235290527344 + 100.0 * 9.060181617736816
Epoch 130, val loss: 1.0412061214447021
Epoch 140, training loss: 896.761474609375 = 1.0383260250091553 + 100.0 * 8.957231521606445
Epoch 140, val loss: 1.039126992225647
Epoch 150, training loss: 889.8993530273438 = 1.036749243736267 + 100.0 * 8.888626098632812
Epoch 150, val loss: 1.037569284439087
Epoch 160, training loss: 886.9503784179688 = 1.0348985195159912 + 100.0 * 8.85915470123291
Epoch 160, val loss: 1.0356231927871704
Epoch 170, training loss: 884.496826171875 = 1.0324831008911133 + 100.0 * 8.834643363952637
Epoch 170, val loss: 1.0330761671066284
Epoch 180, training loss: 881.7200927734375 = 1.0299211740493774 + 100.0 * 8.806901931762695
Epoch 180, val loss: 1.0304678678512573
Epoch 190, training loss: 878.8507690429688 = 1.0276120901107788 + 100.0 * 8.778231620788574
Epoch 190, val loss: 1.0281803607940674
Epoch 200, training loss: 876.1473388671875 = 1.0256528854370117 + 100.0 * 8.751216888427734
Epoch 200, val loss: 1.0262495279312134
Epoch 210, training loss: 873.8917846679688 = 1.0238127708435059 + 100.0 * 8.728679656982422
Epoch 210, val loss: 1.0244100093841553
Epoch 220, training loss: 871.8873901367188 = 1.0218358039855957 + 100.0 * 8.70865535736084
Epoch 220, val loss: 1.0224156379699707
Epoch 230, training loss: 870.3154907226562 = 1.01960289478302 + 100.0 * 8.69295883178711
Epoch 230, val loss: 1.0201611518859863
Epoch 240, training loss: 868.8810424804688 = 1.017006278038025 + 100.0 * 8.678640365600586
Epoch 240, val loss: 1.0176118612289429
Epoch 250, training loss: 867.5653686523438 = 1.0141514539718628 + 100.0 * 8.665512084960938
Epoch 250, val loss: 1.014793038368225
Epoch 260, training loss: 866.1583251953125 = 1.0112122297286987 + 100.0 * 8.651471138000488
Epoch 260, val loss: 1.0119218826293945
Epoch 270, training loss: 865.0538330078125 = 1.0082645416259766 + 100.0 * 8.640456199645996
Epoch 270, val loss: 1.0090385675430298
Epoch 280, training loss: 863.4420166015625 = 1.005255937576294 + 100.0 * 8.624367713928223
Epoch 280, val loss: 1.0060551166534424
Epoch 290, training loss: 862.1129150390625 = 1.0021374225616455 + 100.0 * 8.61110782623291
Epoch 290, val loss: 1.0029659271240234
Epoch 300, training loss: 860.9096069335938 = 0.9988691210746765 + 100.0 * 8.59910774230957
Epoch 300, val loss: 0.999745786190033
Epoch 310, training loss: 859.9512939453125 = 0.9953953623771667 + 100.0 * 8.589558601379395
Epoch 310, val loss: 0.9963023662567139
Epoch 320, training loss: 858.8955078125 = 0.9916583299636841 + 100.0 * 8.579038619995117
Epoch 320, val loss: 0.9926212430000305
Epoch 330, training loss: 857.9849853515625 = 0.9876799583435059 + 100.0 * 8.56997299194336
Epoch 330, val loss: 0.9886911511421204
Epoch 340, training loss: 857.2012939453125 = 0.9834659695625305 + 100.0 * 8.562178611755371
Epoch 340, val loss: 0.9845291972160339
Epoch 350, training loss: 856.9432983398438 = 0.9789295792579651 + 100.0 * 8.559643745422363
Epoch 350, val loss: 0.9800959229469299
Epoch 360, training loss: 855.8466796875 = 0.9741694331169128 + 100.0 * 8.548725128173828
Epoch 360, val loss: 0.9753701686859131
Epoch 370, training loss: 855.2160034179688 = 0.9692031145095825 + 100.0 * 8.542468070983887
Epoch 370, val loss: 0.9704991579055786
Epoch 380, training loss: 854.7014770507812 = 0.9640458226203918 + 100.0 * 8.537374496459961
Epoch 380, val loss: 0.9654186964035034
Epoch 390, training loss: 854.2220458984375 = 0.9586776494979858 + 100.0 * 8.532633781433105
Epoch 390, val loss: 0.9601245522499084
Epoch 400, training loss: 854.1533813476562 = 0.953057050704956 + 100.0 * 8.532003402709961
Epoch 400, val loss: 0.954605221748352
Epoch 410, training loss: 853.483642578125 = 0.9471733570098877 + 100.0 * 8.525364875793457
Epoch 410, val loss: 0.9487721920013428
Epoch 420, training loss: 853.1210327148438 = 0.9410877227783203 + 100.0 * 8.521799087524414
Epoch 420, val loss: 0.9427624344825745
Epoch 430, training loss: 852.7677001953125 = 0.9348356127738953 + 100.0 * 8.518328666687012
Epoch 430, val loss: 0.9365820288658142
Epoch 440, training loss: 852.5559692382812 = 0.9283875226974487 + 100.0 * 8.516275405883789
Epoch 440, val loss: 0.9301929473876953
Epoch 450, training loss: 852.2503662109375 = 0.9217139482498169 + 100.0 * 8.513286590576172
Epoch 450, val loss: 0.9235876798629761
Epoch 460, training loss: 852.3239135742188 = 0.9148781895637512 + 100.0 * 8.514090538024902
Epoch 460, val loss: 0.9167882800102234
Epoch 470, training loss: 851.6868286132812 = 0.9078899621963501 + 100.0 * 8.507789611816406
Epoch 470, val loss: 0.9098652601242065
Epoch 480, training loss: 851.4036865234375 = 0.9008411169052124 + 100.0 * 8.50502872467041
Epoch 480, val loss: 0.9028895497322083
Epoch 490, training loss: 851.1466674804688 = 0.893747866153717 + 100.0 * 8.50252914428711
Epoch 490, val loss: 0.8958414793014526
Epoch 500, training loss: 850.8867797851562 = 0.886617124080658 + 100.0 * 8.500001907348633
Epoch 500, val loss: 0.8887402415275574
Epoch 510, training loss: 850.689697265625 = 0.8794616460800171 + 100.0 * 8.498102188110352
Epoch 510, val loss: 0.8816260099411011
Epoch 520, training loss: 850.666259765625 = 0.8722431063652039 + 100.0 * 8.497940063476562
Epoch 520, val loss: 0.8744195699691772
Epoch 530, training loss: 850.2850341796875 = 0.865013599395752 + 100.0 * 8.494200706481934
Epoch 530, val loss: 0.8672264814376831
Epoch 540, training loss: 849.8914794921875 = 0.857876181602478 + 100.0 * 8.490336418151855
Epoch 540, val loss: 0.8600941896438599
Epoch 550, training loss: 849.614501953125 = 0.8508225679397583 + 100.0 * 8.48763656616211
Epoch 550, val loss: 0.8530281782150269
Epoch 560, training loss: 849.3351440429688 = 0.8438483476638794 + 100.0 * 8.484912872314453
Epoch 560, val loss: 0.8460652232170105
Epoch 570, training loss: 849.1143798828125 = 0.8369629383087158 + 100.0 * 8.482773780822754
Epoch 570, val loss: 0.8391875624656677
Epoch 580, training loss: 849.0377807617188 = 0.8301151990890503 + 100.0 * 8.482076644897461
Epoch 580, val loss: 0.8323017954826355
Epoch 590, training loss: 848.6963500976562 = 0.8233304619789124 + 100.0 * 8.478730201721191
Epoch 590, val loss: 0.8255171179771423
Epoch 600, training loss: 848.7909545898438 = 0.8166637420654297 + 100.0 * 8.479743003845215
Epoch 600, val loss: 0.8188729882240295
Epoch 610, training loss: 848.2537841796875 = 0.8101094961166382 + 100.0 * 8.47443675994873
Epoch 610, val loss: 0.8122652769088745
Epoch 620, training loss: 847.9481201171875 = 0.8036988377571106 + 100.0 * 8.471444129943848
Epoch 620, val loss: 0.8058732151985168
Epoch 630, training loss: 847.7510375976562 = 0.7974507212638855 + 100.0 * 8.469535827636719
Epoch 630, val loss: 0.7996187210083008
Epoch 640, training loss: 847.5675048828125 = 0.7913481593132019 + 100.0 * 8.467761993408203
Epoch 640, val loss: 0.7935311198234558
Epoch 650, training loss: 847.5997314453125 = 0.7853827476501465 + 100.0 * 8.468143463134766
Epoch 650, val loss: 0.7875813841819763
Epoch 660, training loss: 847.50732421875 = 0.7795151472091675 + 100.0 * 8.467277526855469
Epoch 660, val loss: 0.7817526459693909
Epoch 670, training loss: 847.1339111328125 = 0.7738217115402222 + 100.0 * 8.463601112365723
Epoch 670, val loss: 0.7760676145553589
Epoch 680, training loss: 846.881591796875 = 0.7683764696121216 + 100.0 * 8.461132049560547
Epoch 680, val loss: 0.7706829309463501
Epoch 690, training loss: 846.751708984375 = 0.7631007432937622 + 100.0 * 8.459885597229004
Epoch 690, val loss: 0.7654674649238586
Epoch 700, training loss: 846.9306030273438 = 0.7580069303512573 + 100.0 * 8.461726188659668
Epoch 700, val loss: 0.7604497671127319
Epoch 710, training loss: 846.5643310546875 = 0.7530531883239746 + 100.0 * 8.458112716674805
Epoch 710, val loss: 0.7555122971534729
Epoch 720, training loss: 846.3508911132812 = 0.7483194470405579 + 100.0 * 8.456026077270508
Epoch 720, val loss: 0.7509129643440247
Epoch 730, training loss: 846.1873779296875 = 0.7437777519226074 + 100.0 * 8.454436302185059
Epoch 730, val loss: 0.7464421987533569
Epoch 740, training loss: 846.0986938476562 = 0.739421546459198 + 100.0 * 8.453592300415039
Epoch 740, val loss: 0.7421642541885376
Epoch 750, training loss: 846.2138671875 = 0.7352373600006104 + 100.0 * 8.45478630065918
Epoch 750, val loss: 0.7380572557449341
Epoch 760, training loss: 845.862060546875 = 0.7311458587646484 + 100.0 * 8.451309204101562
Epoch 760, val loss: 0.7341757416725159
Epoch 770, training loss: 845.741943359375 = 0.7273043394088745 + 100.0 * 8.450146675109863
Epoch 770, val loss: 0.7304161787033081
Epoch 780, training loss: 845.5795288085938 = 0.7236445546150208 + 100.0 * 8.448558807373047
Epoch 780, val loss: 0.7269354462623596
Epoch 790, training loss: 845.493896484375 = 0.7201711535453796 + 100.0 * 8.447737693786621
Epoch 790, val loss: 0.7236142754554749
Epoch 800, training loss: 845.8875732421875 = 0.7168265581130981 + 100.0 * 8.45170783996582
Epoch 800, val loss: 0.7203453183174133
Epoch 810, training loss: 845.3992309570312 = 0.7135663628578186 + 100.0 * 8.446856498718262
Epoch 810, val loss: 0.7173811793327332
Epoch 820, training loss: 845.0903930664062 = 0.7105206251144409 + 100.0 * 8.443799018859863
Epoch 820, val loss: 0.7144858837127686
Epoch 830, training loss: 844.9679565429688 = 0.7076588273048401 + 100.0 * 8.44260311126709
Epoch 830, val loss: 0.7117827534675598
Epoch 840, training loss: 844.83935546875 = 0.7049283981323242 + 100.0 * 8.441344261169434
Epoch 840, val loss: 0.7092539668083191
Epoch 850, training loss: 844.7767944335938 = 0.7023244500160217 + 100.0 * 8.440744400024414
Epoch 850, val loss: 0.706885814666748
Epoch 860, training loss: 844.6588745117188 = 0.6998047232627869 + 100.0 * 8.439590454101562
Epoch 860, val loss: 0.7045578956604004
Epoch 870, training loss: 844.6375122070312 = 0.6973540186882019 + 100.0 * 8.439401626586914
Epoch 870, val loss: 0.7023641467094421
Epoch 880, training loss: 844.4031372070312 = 0.6950244307518005 + 100.0 * 8.437081336975098
Epoch 880, val loss: 0.7002413272857666
Epoch 890, training loss: 844.2709350585938 = 0.6928282380104065 + 100.0 * 8.435781478881836
Epoch 890, val loss: 0.698246955871582
Epoch 900, training loss: 844.568603515625 = 0.6907125115394592 + 100.0 * 8.4387788772583
Epoch 900, val loss: 0.6962985396385193
Epoch 910, training loss: 844.3233032226562 = 0.6885942220687866 + 100.0 * 8.436347007751465
Epoch 910, val loss: 0.6945505738258362
Epoch 920, training loss: 844.04541015625 = 0.6865584850311279 + 100.0 * 8.433588027954102
Epoch 920, val loss: 0.6927030682563782
Epoch 930, training loss: 843.88037109375 = 0.6846526861190796 + 100.0 * 8.431957244873047
Epoch 930, val loss: 0.6910618543624878
Epoch 940, training loss: 843.8014526367188 = 0.6828272342681885 + 100.0 * 8.43118667602539
Epoch 940, val loss: 0.6895166635513306
Epoch 950, training loss: 843.70849609375 = 0.6810724139213562 + 100.0 * 8.43027400970459
Epoch 950, val loss: 0.6880131959915161
Epoch 960, training loss: 843.640380859375 = 0.6793668270111084 + 100.0 * 8.429610252380371
Epoch 960, val loss: 0.6865921020507812
Epoch 970, training loss: 843.6580810546875 = 0.6776910424232483 + 100.0 * 8.429803848266602
Epoch 970, val loss: 0.6851809024810791
Epoch 980, training loss: 843.5573120117188 = 0.6760316491127014 + 100.0 * 8.428812980651855
Epoch 980, val loss: 0.6837999224662781
Epoch 990, training loss: 843.5023193359375 = 0.6744410395622253 + 100.0 * 8.428278923034668
Epoch 990, val loss: 0.6824872493743896
Epoch 1000, training loss: 843.6070556640625 = 0.6728881001472473 + 100.0 * 8.429341316223145
Epoch 1000, val loss: 0.6812408566474915
Epoch 1010, training loss: 843.3580322265625 = 0.6713714003562927 + 100.0 * 8.42686653137207
Epoch 1010, val loss: 0.680124819278717
Epoch 1020, training loss: 843.230224609375 = 0.6699246764183044 + 100.0 * 8.425602912902832
Epoch 1020, val loss: 0.6789328455924988
Epoch 1030, training loss: 843.1681518554688 = 0.6685309410095215 + 100.0 * 8.424996376037598
Epoch 1030, val loss: 0.6778913736343384
Epoch 1040, training loss: 843.6572875976562 = 0.6671670079231262 + 100.0 * 8.429901123046875
Epoch 1040, val loss: 0.6768413186073303
Epoch 1050, training loss: 843.1128540039062 = 0.6657757759094238 + 100.0 * 8.424470901489258
Epoch 1050, val loss: 0.6756698489189148
Epoch 1060, training loss: 842.9373168945312 = 0.6644785404205322 + 100.0 * 8.422728538513184
Epoch 1060, val loss: 0.6747369170188904
Epoch 1070, training loss: 842.8760375976562 = 0.6632475852966309 + 100.0 * 8.422127723693848
Epoch 1070, val loss: 0.6737877726554871
Epoch 1080, training loss: 842.7978515625 = 0.6620562076568604 + 100.0 * 8.421358108520508
Epoch 1080, val loss: 0.672903299331665
Epoch 1090, training loss: 842.9392700195312 = 0.6608936190605164 + 100.0 * 8.422783851623535
Epoch 1090, val loss: 0.6720582246780396
Epoch 1100, training loss: 843.122802734375 = 0.6596982479095459 + 100.0 * 8.424631118774414
Epoch 1100, val loss: 0.6712900996208191
Epoch 1110, training loss: 842.6489868164062 = 0.6584791541099548 + 100.0 * 8.419904708862305
Epoch 1110, val loss: 0.6702383756637573
Epoch 1120, training loss: 842.5603637695312 = 0.6573405861854553 + 100.0 * 8.41903018951416
Epoch 1120, val loss: 0.6694784760475159
Epoch 1130, training loss: 842.4375 = 0.6562626957893372 + 100.0 * 8.41781234741211
Epoch 1130, val loss: 0.6686932444572449
Epoch 1140, training loss: 842.3953247070312 = 0.6552209854125977 + 100.0 * 8.417401313781738
Epoch 1140, val loss: 0.6679559946060181
Epoch 1150, training loss: 842.32080078125 = 0.6541936993598938 + 100.0 * 8.416666030883789
Epoch 1150, val loss: 0.6672379374504089
Epoch 1160, training loss: 842.5643310546875 = 0.6531648635864258 + 100.0 * 8.419112205505371
Epoch 1160, val loss: 0.6664637923240662
Epoch 1170, training loss: 842.1881713867188 = 0.6520830988883972 + 100.0 * 8.415360450744629
Epoch 1170, val loss: 0.6658260226249695
Epoch 1180, training loss: 842.1542358398438 = 0.6510375738143921 + 100.0 * 8.415031433105469
Epoch 1180, val loss: 0.6650364995002747
Epoch 1190, training loss: 842.0729370117188 = 0.6500632166862488 + 100.0 * 8.414228439331055
Epoch 1190, val loss: 0.6644529104232788
Epoch 1200, training loss: 841.9826049804688 = 0.6491163372993469 + 100.0 * 8.413334846496582
Epoch 1200, val loss: 0.6637811660766602
Epoch 1210, training loss: 841.9505615234375 = 0.6481914520263672 + 100.0 * 8.413023948669434
Epoch 1210, val loss: 0.6631587743759155
Epoch 1220, training loss: 842.0413818359375 = 0.6472243070602417 + 100.0 * 8.413941383361816
Epoch 1220, val loss: 0.662570059299469
Epoch 1230, training loss: 841.9677734375 = 0.6462318301200867 + 100.0 * 8.413215637207031
Epoch 1230, val loss: 0.6618578433990479
Epoch 1240, training loss: 841.80224609375 = 0.6452898383140564 + 100.0 * 8.411569595336914
Epoch 1240, val loss: 0.661254346370697
Epoch 1250, training loss: 841.7514038085938 = 0.6443926692008972 + 100.0 * 8.411069869995117
Epoch 1250, val loss: 0.660670280456543
Epoch 1260, training loss: 842.1497192382812 = 0.6434794664382935 + 100.0 * 8.415061950683594
Epoch 1260, val loss: 0.6600188612937927
Epoch 1270, training loss: 841.7196655273438 = 0.6425231695175171 + 100.0 * 8.410771369934082
Epoch 1270, val loss: 0.6595291495323181
Epoch 1280, training loss: 841.5771484375 = 0.6416192054748535 + 100.0 * 8.409355163574219
Epoch 1280, val loss: 0.6589065194129944
Epoch 1290, training loss: 841.5145874023438 = 0.6407446265220642 + 100.0 * 8.408738136291504
Epoch 1290, val loss: 0.6583521366119385
Epoch 1300, training loss: 841.44287109375 = 0.6398875117301941 + 100.0 * 8.408029556274414
Epoch 1300, val loss: 0.6578419208526611
Epoch 1310, training loss: 841.470703125 = 0.6390324831008911 + 100.0 * 8.408316612243652
Epoch 1310, val loss: 0.6572747230529785
Epoch 1320, training loss: 841.7068481445312 = 0.6381279826164246 + 100.0 * 8.410687446594238
Epoch 1320, val loss: 0.656671404838562
Epoch 1330, training loss: 841.4183349609375 = 0.6372004747390747 + 100.0 * 8.407811164855957
Epoch 1330, val loss: 0.6561518907546997
Epoch 1340, training loss: 841.29833984375 = 0.6363298892974854 + 100.0 * 8.406620025634766
Epoch 1340, val loss: 0.655635416507721
Epoch 1350, training loss: 841.2042236328125 = 0.6355018615722656 + 100.0 * 8.40568733215332
Epoch 1350, val loss: 0.6551709175109863
Epoch 1360, training loss: 841.1530151367188 = 0.6346824765205383 + 100.0 * 8.405182838439941
Epoch 1360, val loss: 0.6546983122825623
Epoch 1370, training loss: 841.2652587890625 = 0.6338642239570618 + 100.0 * 8.4063138961792
Epoch 1370, val loss: 0.6542841792106628
Epoch 1380, training loss: 841.2994995117188 = 0.6329677700996399 + 100.0 * 8.406664848327637
Epoch 1380, val loss: 0.6536617875099182
Epoch 1390, training loss: 841.1488647460938 = 0.6320496797561646 + 100.0 * 8.405167579650879
Epoch 1390, val loss: 0.653084397315979
Epoch 1400, training loss: 840.9927368164062 = 0.6312056183815002 + 100.0 * 8.40361499786377
Epoch 1400, val loss: 0.6526266932487488
Epoch 1410, training loss: 840.9443359375 = 0.6304095983505249 + 100.0 * 8.403139114379883
Epoch 1410, val loss: 0.6521942019462585
Epoch 1420, training loss: 840.956787109375 = 0.6296156048774719 + 100.0 * 8.403271675109863
Epoch 1420, val loss: 0.6517478227615356
Epoch 1430, training loss: 841.2614135742188 = 0.6287814974784851 + 100.0 * 8.406326293945312
Epoch 1430, val loss: 0.6513277888298035
Epoch 1440, training loss: 840.8687744140625 = 0.6279067397117615 + 100.0 * 8.402408599853516
Epoch 1440, val loss: 0.6506854891777039
Epoch 1450, training loss: 840.7517700195312 = 0.6270908713340759 + 100.0 * 8.401247024536133
Epoch 1450, val loss: 0.6502861380577087
Epoch 1460, training loss: 840.7498168945312 = 0.6263015866279602 + 100.0 * 8.401235580444336
Epoch 1460, val loss: 0.649890124797821
Epoch 1470, training loss: 840.68798828125 = 0.6255112886428833 + 100.0 * 8.400625228881836
Epoch 1470, val loss: 0.6494591236114502
Epoch 1480, training loss: 840.7083129882812 = 0.6247177720069885 + 100.0 * 8.400835990905762
Epoch 1480, val loss: 0.6490456461906433
Epoch 1490, training loss: 840.67333984375 = 0.6238995790481567 + 100.0 * 8.400494575500488
Epoch 1490, val loss: 0.6485581398010254
Epoch 1500, training loss: 840.9808349609375 = 0.6230660080909729 + 100.0 * 8.40357780456543
Epoch 1500, val loss: 0.6481243371963501
Epoch 1510, training loss: 840.681396484375 = 0.6221449971199036 + 100.0 * 8.400592803955078
Epoch 1510, val loss: 0.6474576592445374
Epoch 1520, training loss: 840.510009765625 = 0.6213126182556152 + 100.0 * 8.398886680603027
Epoch 1520, val loss: 0.6470233798027039
Epoch 1530, training loss: 840.4261474609375 = 0.6205283999443054 + 100.0 * 8.398056030273438
Epoch 1530, val loss: 0.6465610861778259
Epoch 1540, training loss: 840.40283203125 = 0.6197547316551208 + 100.0 * 8.397830963134766
Epoch 1540, val loss: 0.6461312174797058
Epoch 1550, training loss: 840.3533935546875 = 0.6189785599708557 + 100.0 * 8.397344589233398
Epoch 1550, val loss: 0.6457239985466003
Epoch 1560, training loss: 840.3181762695312 = 0.6181901097297668 + 100.0 * 8.397000312805176
Epoch 1560, val loss: 0.6453003883361816
Epoch 1570, training loss: 840.4364013671875 = 0.6173919439315796 + 100.0 * 8.39819049835205
Epoch 1570, val loss: 0.6449196934700012
Epoch 1580, training loss: 840.5313110351562 = 0.6164858341217041 + 100.0 * 8.399147987365723
Epoch 1580, val loss: 0.6443045139312744
Epoch 1590, training loss: 840.3290405273438 = 0.6155560612678528 + 100.0 * 8.397134780883789
Epoch 1590, val loss: 0.6437812447547913
Epoch 1600, training loss: 840.2633056640625 = 0.6147223711013794 + 100.0 * 8.396485328674316
Epoch 1600, val loss: 0.6432207226753235
Epoch 1610, training loss: 840.1474609375 = 0.6139230728149414 + 100.0 * 8.39533519744873
Epoch 1610, val loss: 0.6428061127662659
Epoch 1620, training loss: 840.1209106445312 = 0.6131349205970764 + 100.0 * 8.3950777053833
Epoch 1620, val loss: 0.6424036026000977
Epoch 1630, training loss: 840.0847778320312 = 0.6123279333114624 + 100.0 * 8.39472484588623
Epoch 1630, val loss: 0.6419281959533691
Epoch 1640, training loss: 840.0492553710938 = 0.6115100979804993 + 100.0 * 8.394377708435059
Epoch 1640, val loss: 0.6414843201637268
Epoch 1650, training loss: 840.0281372070312 = 0.6106863021850586 + 100.0 * 8.394174575805664
Epoch 1650, val loss: 0.6410049796104431
Epoch 1660, training loss: 840.7454223632812 = 0.6098347306251526 + 100.0 * 8.401355743408203
Epoch 1660, val loss: 0.6404578685760498
Epoch 1670, training loss: 840.0560913085938 = 0.6088932752609253 + 100.0 * 8.394472122192383
Epoch 1670, val loss: 0.6398811936378479
Epoch 1680, training loss: 840.0926513671875 = 0.6079872846603394 + 100.0 * 8.39484691619873
Epoch 1680, val loss: 0.639354944229126
Epoch 1690, training loss: 839.9347534179688 = 0.6071188449859619 + 100.0 * 8.39327621459961
Epoch 1690, val loss: 0.638870894908905
Epoch 1700, training loss: 839.9006958007812 = 0.6062842011451721 + 100.0 * 8.3929443359375
Epoch 1700, val loss: 0.638414740562439
Epoch 1710, training loss: 840.0628051757812 = 0.6054366230964661 + 100.0 * 8.394574165344238
Epoch 1710, val loss: 0.6379779577255249
Epoch 1720, training loss: 839.927490234375 = 0.6045278906822205 + 100.0 * 8.393229484558105
Epoch 1720, val loss: 0.6373811364173889
Epoch 1730, training loss: 839.900146484375 = 0.6036283373832703 + 100.0 * 8.392965316772461
Epoch 1730, val loss: 0.636773943901062
Epoch 1740, training loss: 839.7894287109375 = 0.6027607321739197 + 100.0 * 8.391866683959961
Epoch 1740, val loss: 0.6363219618797302
Epoch 1750, training loss: 839.7632446289062 = 0.6018974184989929 + 100.0 * 8.391613960266113
Epoch 1750, val loss: 0.6357877850532532
Epoch 1760, training loss: 839.875732421875 = 0.6010217666625977 + 100.0 * 8.392746925354004
Epoch 1760, val loss: 0.6353099942207336
Epoch 1770, training loss: 839.7577514648438 = 0.6000937819480896 + 100.0 * 8.391576766967773
Epoch 1770, val loss: 0.634663462638855
Epoch 1780, training loss: 839.7439575195312 = 0.5991773009300232 + 100.0 * 8.391448020935059
Epoch 1780, val loss: 0.6341429352760315
Epoch 1790, training loss: 839.659423828125 = 0.5982716679573059 + 100.0 * 8.39061164855957
Epoch 1790, val loss: 0.6335901618003845
Epoch 1800, training loss: 839.7572021484375 = 0.5973788499832153 + 100.0 * 8.39159870147705
Epoch 1800, val loss: 0.6331123113632202
Epoch 1810, training loss: 839.9136962890625 = 0.5963811278343201 + 100.0 * 8.393173217773438
Epoch 1810, val loss: 0.6324644684791565
Epoch 1820, training loss: 839.6455078125 = 0.5953574180603027 + 100.0 * 8.390501976013184
Epoch 1820, val loss: 0.6317822933197021
Epoch 1830, training loss: 839.63134765625 = 0.5944346785545349 + 100.0 * 8.390369415283203
Epoch 1830, val loss: 0.6311517953872681
Epoch 1840, training loss: 839.5433349609375 = 0.5935552716255188 + 100.0 * 8.389497756958008
Epoch 1840, val loss: 0.6306607723236084
Epoch 1850, training loss: 839.5232543945312 = 0.5926818251609802 + 100.0 * 8.38930606842041
Epoch 1850, val loss: 0.6301100254058838
Epoch 1860, training loss: 839.4903564453125 = 0.591800332069397 + 100.0 * 8.388985633850098
Epoch 1860, val loss: 0.6296047568321228
Epoch 1870, training loss: 839.46435546875 = 0.5909031629562378 + 100.0 * 8.388734817504883
Epoch 1870, val loss: 0.6290696859359741
Epoch 1880, training loss: 839.4420776367188 = 0.5899954438209534 + 100.0 * 8.388521194458008
Epoch 1880, val loss: 0.6285221576690674
Epoch 1890, training loss: 839.445068359375 = 0.5890780687332153 + 100.0 * 8.38856029510498
Epoch 1890, val loss: 0.6279510259628296
Epoch 1900, training loss: 839.9713745117188 = 0.5881296396255493 + 100.0 * 8.393832206726074
Epoch 1900, val loss: 0.627299427986145
Epoch 1910, training loss: 839.4451293945312 = 0.5871045589447021 + 100.0 * 8.388580322265625
Epoch 1910, val loss: 0.6267118453979492
Epoch 1920, training loss: 839.5175170898438 = 0.58614182472229 + 100.0 * 8.389313697814941
Epoch 1920, val loss: 0.6261162757873535
Epoch 1930, training loss: 839.4732666015625 = 0.5851814150810242 + 100.0 * 8.388880729675293
Epoch 1930, val loss: 0.6254932880401611
Epoch 1940, training loss: 839.3264770507812 = 0.584223210811615 + 100.0 * 8.387422561645508
Epoch 1940, val loss: 0.6249353289604187
Epoch 1950, training loss: 839.2955322265625 = 0.5832897424697876 + 100.0 * 8.38712215423584
Epoch 1950, val loss: 0.6244215965270996
Epoch 1960, training loss: 839.4278564453125 = 0.582351565361023 + 100.0 * 8.388455390930176
Epoch 1960, val loss: 0.6239027380943298
Epoch 1970, training loss: 839.517578125 = 0.5813314318656921 + 100.0 * 8.389362335205078
Epoch 1970, val loss: 0.6232393980026245
Epoch 1980, training loss: 839.3617553710938 = 0.5803002119064331 + 100.0 * 8.38781452178955
Epoch 1980, val loss: 0.6224830746650696
Epoch 1990, training loss: 839.2549438476562 = 0.5793229937553406 + 100.0 * 8.38675594329834
Epoch 1990, val loss: 0.6219907402992249
Epoch 2000, training loss: 839.18408203125 = 0.5783606767654419 + 100.0 * 8.386056900024414
Epoch 2000, val loss: 0.6213486194610596
Epoch 2010, training loss: 839.1727905273438 = 0.5773963928222656 + 100.0 * 8.385953903198242
Epoch 2010, val loss: 0.6207820177078247
Epoch 2020, training loss: 839.34716796875 = 0.5764153599739075 + 100.0 * 8.387707710266113
Epoch 2020, val loss: 0.6202665567398071
Epoch 2030, training loss: 839.1820068359375 = 0.5753667950630188 + 100.0 * 8.386066436767578
Epoch 2030, val loss: 0.6195483803749084
Epoch 2040, training loss: 839.1953735351562 = 0.5743280053138733 + 100.0 * 8.386210441589355
Epoch 2040, val loss: 0.6189209222793579
Epoch 2050, training loss: 839.4249267578125 = 0.5733014345169067 + 100.0 * 8.388516426086426
Epoch 2050, val loss: 0.6182223558425903
Epoch 2060, training loss: 839.0494384765625 = 0.5722328424453735 + 100.0 * 8.384772300720215
Epoch 2060, val loss: 0.617667555809021
Epoch 2070, training loss: 839.0930786132812 = 0.5712080001831055 + 100.0 * 8.385218620300293
Epoch 2070, val loss: 0.6170729994773865
Epoch 2080, training loss: 839.004150390625 = 0.5702002644538879 + 100.0 * 8.384339332580566
Epoch 2080, val loss: 0.6164491176605225
Epoch 2090, training loss: 838.9886474609375 = 0.5691903233528137 + 100.0 * 8.384194374084473
Epoch 2090, val loss: 0.6158320903778076
Epoch 2100, training loss: 839.1760864257812 = 0.5681674480438232 + 100.0 * 8.386078834533691
Epoch 2100, val loss: 0.6151498556137085
Epoch 2110, training loss: 839.0884399414062 = 0.567049503326416 + 100.0 * 8.385213851928711
Epoch 2110, val loss: 0.6145014762878418
Epoch 2120, training loss: 839.0105590820312 = 0.565940797328949 + 100.0 * 8.384446144104004
Epoch 2120, val loss: 0.6139413118362427
Epoch 2130, training loss: 838.9363403320312 = 0.5648748278617859 + 100.0 * 8.38371467590332
Epoch 2130, val loss: 0.6132400035858154
Epoch 2140, training loss: 838.877685546875 = 0.563822329044342 + 100.0 * 8.383138656616211
Epoch 2140, val loss: 0.6126586198806763
Epoch 2150, training loss: 838.9223022460938 = 0.5627700686454773 + 100.0 * 8.38359546661377
Epoch 2150, val loss: 0.612046480178833
Epoch 2160, training loss: 839.2394409179688 = 0.5616504549980164 + 100.0 * 8.386777877807617
Epoch 2160, val loss: 0.6112881898880005
Epoch 2170, training loss: 838.841064453125 = 0.5604841113090515 + 100.0 * 8.382805824279785
Epoch 2170, val loss: 0.6106380224227905
Epoch 2180, training loss: 838.8182373046875 = 0.5593749284744263 + 100.0 * 8.382588386535645
Epoch 2180, val loss: 0.609958291053772
Epoch 2190, training loss: 838.7980346679688 = 0.5582943558692932 + 100.0 * 8.382397651672363
Epoch 2190, val loss: 0.6093183159828186
Epoch 2200, training loss: 838.9379272460938 = 0.5571949481964111 + 100.0 * 8.383807182312012
Epoch 2200, val loss: 0.6086863279342651
Epoch 2210, training loss: 838.91845703125 = 0.5560255646705627 + 100.0 * 8.383624076843262
Epoch 2210, val loss: 0.6079201102256775
Epoch 2220, training loss: 838.8107299804688 = 0.5548579692840576 + 100.0 * 8.382558822631836
Epoch 2220, val loss: 0.6071140766143799
Epoch 2230, training loss: 838.6901245117188 = 0.5537124276161194 + 100.0 * 8.381363868713379
Epoch 2230, val loss: 0.6064542531967163
Epoch 2240, training loss: 838.6806640625 = 0.5525801181793213 + 100.0 * 8.381280899047852
Epoch 2240, val loss: 0.6057454943656921
Epoch 2250, training loss: 838.6522827148438 = 0.5514316558837891 + 100.0 * 8.38100814819336
Epoch 2250, val loss: 0.6050637364387512
Epoch 2260, training loss: 838.87353515625 = 0.5502694249153137 + 100.0 * 8.383232116699219
Epoch 2260, val loss: 0.6044366359710693
Epoch 2270, training loss: 838.7550048828125 = 0.549039363861084 + 100.0 * 8.382060050964355
Epoch 2270, val loss: 0.6034483909606934
Epoch 2280, training loss: 838.6893920898438 = 0.5478119254112244 + 100.0 * 8.381416320800781
Epoch 2280, val loss: 0.6028280258178711
Epoch 2290, training loss: 838.61083984375 = 0.5466341972351074 + 100.0 * 8.38064193725586
Epoch 2290, val loss: 0.6019641757011414
Epoch 2300, training loss: 838.5585327148438 = 0.5454748272895813 + 100.0 * 8.380130767822266
Epoch 2300, val loss: 0.6013047695159912
Epoch 2310, training loss: 838.6305541992188 = 0.5443158149719238 + 100.0 * 8.38086223602295
Epoch 2310, val loss: 0.6005784869194031
Epoch 2320, training loss: 839.1729736328125 = 0.5430794358253479 + 100.0 * 8.386299133300781
Epoch 2320, val loss: 0.5998511910438538
Epoch 2330, training loss: 838.7081298828125 = 0.5417892336845398 + 100.0 * 8.38166332244873
Epoch 2330, val loss: 0.5989160537719727
Epoch 2340, training loss: 838.5370483398438 = 0.5405713319778442 + 100.0 * 8.379964828491211
Epoch 2340, val loss: 0.5982328057289124
Epoch 2350, training loss: 838.4776000976562 = 0.5393930077552795 + 100.0 * 8.379382133483887
Epoch 2350, val loss: 0.5975014567375183
Epoch 2360, training loss: 838.444091796875 = 0.5382140278816223 + 100.0 * 8.379058837890625
Epoch 2360, val loss: 0.596840500831604
Epoch 2370, training loss: 838.4103393554688 = 0.5370193123817444 + 100.0 * 8.378732681274414
Epoch 2370, val loss: 0.596126914024353
Epoch 2380, training loss: 838.5441284179688 = 0.5358131527900696 + 100.0 * 8.380083084106445
Epoch 2380, val loss: 0.5953571200370789
Epoch 2390, training loss: 838.4461669921875 = 0.5345287322998047 + 100.0 * 8.37911605834961
Epoch 2390, val loss: 0.5946587920188904
Epoch 2400, training loss: 838.5282592773438 = 0.5332156419754028 + 100.0 * 8.379950523376465
Epoch 2400, val loss: 0.593750536441803
Epoch 2410, training loss: 838.356201171875 = 0.5319696664810181 + 100.0 * 8.378242492675781
Epoch 2410, val loss: 0.593043863773346
Epoch 2420, training loss: 838.3352661132812 = 0.5307663679122925 + 100.0 * 8.378045082092285
Epoch 2420, val loss: 0.5923001766204834
Epoch 2430, training loss: 838.4501342773438 = 0.5295572280883789 + 100.0 * 8.379205703735352
Epoch 2430, val loss: 0.5915771126747131
Epoch 2440, training loss: 838.6549072265625 = 0.5282723307609558 + 100.0 * 8.381266593933105
Epoch 2440, val loss: 0.590738832950592
Epoch 2450, training loss: 838.37744140625 = 0.5269596576690674 + 100.0 * 8.378504753112793
Epoch 2450, val loss: 0.5900640487670898
Epoch 2460, training loss: 838.2852783203125 = 0.5257006883621216 + 100.0 * 8.377595901489258
Epoch 2460, val loss: 0.5892795324325562
Epoch 2470, training loss: 838.2446899414062 = 0.5244768261909485 + 100.0 * 8.377202033996582
Epoch 2470, val loss: 0.5885649919509888
Epoch 2480, training loss: 838.2251586914062 = 0.5232474207878113 + 100.0 * 8.377018928527832
Epoch 2480, val loss: 0.5878766179084778
Epoch 2490, training loss: 838.3312377929688 = 0.5220145583152771 + 100.0 * 8.378091812133789
Epoch 2490, val loss: 0.5870937705039978
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7640791476407914
0.8107657755560386
=== training gcn model ===
Epoch 0, training loss: 1059.3341064453125 = 1.1032919883728027 + 100.0 * 10.582308769226074
Epoch 0, val loss: 1.1041531562805176
Epoch 10, training loss: 1059.307373046875 = 1.0984864234924316 + 100.0 * 10.582088470458984
Epoch 10, val loss: 1.099222183227539
Epoch 20, training loss: 1059.2017822265625 = 1.093121886253357 + 100.0 * 10.581086158752441
Epoch 20, val loss: 1.0937076807022095
Epoch 30, training loss: 1058.7357177734375 = 1.0872081518173218 + 100.0 * 10.576485633850098
Epoch 30, val loss: 1.0876224040985107
Epoch 40, training loss: 1056.8658447265625 = 1.0806000232696533 + 100.0 * 10.557852745056152
Epoch 40, val loss: 1.0808128118515015
Epoch 50, training loss: 1051.154541015625 = 1.0733526945114136 + 100.0 * 10.500811576843262
Epoch 50, val loss: 1.0733685493469238
Epoch 60, training loss: 1037.267822265625 = 1.066465973854065 + 100.0 * 10.362013816833496
Epoch 60, val loss: 1.0665287971496582
Epoch 70, training loss: 1006.7925415039062 = 1.0606945753097534 + 100.0 * 10.057318687438965
Epoch 70, val loss: 1.0606608390808105
Epoch 80, training loss: 968.4904174804688 = 1.054553747177124 + 100.0 * 9.674358367919922
Epoch 80, val loss: 1.0543885231018066
Epoch 90, training loss: 961.1372680664062 = 1.0488373041152954 + 100.0 * 9.600884437561035
Epoch 90, val loss: 1.0486717224121094
Epoch 100, training loss: 958.3597412109375 = 1.0432718992233276 + 100.0 * 9.573164939880371
Epoch 100, val loss: 1.0431071519851685
Epoch 110, training loss: 955.021484375 = 1.0382670164108276 + 100.0 * 9.53983211517334
Epoch 110, val loss: 1.0381661653518677
Epoch 120, training loss: 950.6748046875 = 1.0335960388183594 + 100.0 * 9.49641227722168
Epoch 120, val loss: 1.0336174964904785
Epoch 130, training loss: 944.37353515625 = 1.0289536714553833 + 100.0 * 9.433445930480957
Epoch 130, val loss: 1.0291844606399536
Epoch 140, training loss: 935.83251953125 = 1.0248723030090332 + 100.0 * 9.348075866699219
Epoch 140, val loss: 1.0254218578338623
Epoch 150, training loss: 927.2725219726562 = 1.0221529006958008 + 100.0 * 9.262503623962402
Epoch 150, val loss: 1.0229716300964355
Epoch 160, training loss: 922.4611206054688 = 1.0205265283584595 + 100.0 * 9.21440601348877
Epoch 160, val loss: 1.0213545560836792
Epoch 170, training loss: 920.255859375 = 1.017775535583496 + 100.0 * 9.192380905151367
Epoch 170, val loss: 1.0183396339416504
Epoch 180, training loss: 916.5657348632812 = 1.0140795707702637 + 100.0 * 9.155516624450684
Epoch 180, val loss: 1.0145829916000366
Epoch 190, training loss: 910.505859375 = 1.0114920139312744 + 100.0 * 9.09494400024414
Epoch 190, val loss: 1.0121419429779053
Epoch 200, training loss: 902.4966430664062 = 1.0101572275161743 + 100.0 * 9.014864921569824
Epoch 200, val loss: 1.0110317468643188
Epoch 210, training loss: 897.1118774414062 = 1.0093265771865845 + 100.0 * 8.96102523803711
Epoch 210, val loss: 1.0102342367172241
Epoch 220, training loss: 893.1217651367188 = 1.006622552871704 + 100.0 * 8.921151161193848
Epoch 220, val loss: 1.007502555847168
Epoch 230, training loss: 888.56591796875 = 1.0033683776855469 + 100.0 * 8.875625610351562
Epoch 230, val loss: 1.0043525695800781
Epoch 240, training loss: 885.6050415039062 = 1.0003291368484497 + 100.0 * 8.846047401428223
Epoch 240, val loss: 1.001393437385559
Epoch 250, training loss: 883.4019165039062 = 0.9966721534729004 + 100.0 * 8.824052810668945
Epoch 250, val loss: 0.9978271722793579
Epoch 260, training loss: 881.0728759765625 = 0.9925257563591003 + 100.0 * 8.800803184509277
Epoch 260, val loss: 0.9938281178474426
Epoch 270, training loss: 878.281982421875 = 0.9882562160491943 + 100.0 * 8.772936820983887
Epoch 270, val loss: 0.9896827936172485
Epoch 280, training loss: 875.554931640625 = 0.9840980172157288 + 100.0 * 8.745708465576172
Epoch 280, val loss: 0.9856468439102173
Epoch 290, training loss: 872.8909301757812 = 0.9796260595321655 + 100.0 * 8.71911334991455
Epoch 290, val loss: 0.9812965989112854
Epoch 300, training loss: 870.73291015625 = 0.9747007489204407 + 100.0 * 8.697582244873047
Epoch 300, val loss: 0.9764778017997742
Epoch 310, training loss: 869.0040283203125 = 0.9692997932434082 + 100.0 * 8.680347442626953
Epoch 310, val loss: 0.9711998105049133
Epoch 320, training loss: 867.6655883789062 = 0.9634293913841248 + 100.0 * 8.667021751403809
Epoch 320, val loss: 0.9654710292816162
Epoch 330, training loss: 866.664794921875 = 0.9572120308876038 + 100.0 * 8.657075881958008
Epoch 330, val loss: 0.9594594836235046
Epoch 340, training loss: 865.407470703125 = 0.9507266879081726 + 100.0 * 8.644567489624023
Epoch 340, val loss: 0.953101396560669
Epoch 350, training loss: 864.5493774414062 = 0.943926215171814 + 100.0 * 8.636054992675781
Epoch 350, val loss: 0.9464707970619202
Epoch 360, training loss: 863.789794921875 = 0.9367983937263489 + 100.0 * 8.62852954864502
Epoch 360, val loss: 0.9395108819007874
Epoch 370, training loss: 863.1521606445312 = 0.9293609857559204 + 100.0 * 8.622227668762207
Epoch 370, val loss: 0.9322400689125061
Epoch 380, training loss: 862.4247436523438 = 0.9216538667678833 + 100.0 * 8.615031242370605
Epoch 380, val loss: 0.9248010516166687
Epoch 390, training loss: 861.6974487304688 = 0.9138737916946411 + 100.0 * 8.60783576965332
Epoch 390, val loss: 0.9172360897064209
Epoch 400, training loss: 861.0938720703125 = 0.906043291091919 + 100.0 * 8.60187816619873
Epoch 400, val loss: 0.9095732569694519
Epoch 410, training loss: 860.6365966796875 = 0.8979558944702148 + 100.0 * 8.597386360168457
Epoch 410, val loss: 0.9017915725708008
Epoch 420, training loss: 860.0011596679688 = 0.8898186683654785 + 100.0 * 8.591113090515137
Epoch 420, val loss: 0.8937863111495972
Epoch 430, training loss: 859.4974365234375 = 0.8815833330154419 + 100.0 * 8.586158752441406
Epoch 430, val loss: 0.8858155012130737
Epoch 440, training loss: 859.0440063476562 = 0.8732811212539673 + 100.0 * 8.581707000732422
Epoch 440, val loss: 0.8777638673782349
Epoch 450, training loss: 858.5833740234375 = 0.8649781346321106 + 100.0 * 8.577183723449707
Epoch 450, val loss: 0.869681715965271
Epoch 460, training loss: 858.564697265625 = 0.8566953539848328 + 100.0 * 8.577079772949219
Epoch 460, val loss: 0.861627995967865
Epoch 470, training loss: 857.6398315429688 = 0.8484458327293396 + 100.0 * 8.567914009094238
Epoch 470, val loss: 0.853585422039032
Epoch 480, training loss: 857.1531372070312 = 0.8404045701026917 + 100.0 * 8.563127517700195
Epoch 480, val loss: 0.8457297086715698
Epoch 490, training loss: 856.6163940429688 = 0.8324854373931885 + 100.0 * 8.557839393615723
Epoch 490, val loss: 0.8379678726196289
Epoch 500, training loss: 856.1237182617188 = 0.8246632814407349 + 100.0 * 8.552990913391113
Epoch 500, val loss: 0.8302773833274841
Epoch 510, training loss: 855.9886474609375 = 0.8168649673461914 + 100.0 * 8.551717758178711
Epoch 510, val loss: 0.8225686550140381
Epoch 520, training loss: 855.2424926757812 = 0.8090485334396362 + 100.0 * 8.544334411621094
Epoch 520, val loss: 0.8149401545524597
Epoch 530, training loss: 854.82763671875 = 0.8014074563980103 + 100.0 * 8.540262222290039
Epoch 530, val loss: 0.8075224757194519
Epoch 540, training loss: 854.385498046875 = 0.7939159274101257 + 100.0 * 8.53591537475586
Epoch 540, val loss: 0.800111711025238
Epoch 550, training loss: 853.9508666992188 = 0.7865471839904785 + 100.0 * 8.53164291381836
Epoch 550, val loss: 0.7929258942604065
Epoch 560, training loss: 853.8222045898438 = 0.7793146967887878 + 100.0 * 8.530428886413574
Epoch 560, val loss: 0.7858574390411377
Epoch 570, training loss: 853.3169555664062 = 0.7721831798553467 + 100.0 * 8.525447845458984
Epoch 570, val loss: 0.7787709832191467
Epoch 580, training loss: 852.7974243164062 = 0.7652668356895447 + 100.0 * 8.5203218460083
Epoch 580, val loss: 0.7719821929931641
Epoch 590, training loss: 852.4639282226562 = 0.7585535645484924 + 100.0 * 8.517053604125977
Epoch 590, val loss: 0.7653464674949646
Epoch 600, training loss: 852.1639404296875 = 0.7519933581352234 + 100.0 * 8.514119148254395
Epoch 600, val loss: 0.7588879466056824
Epoch 610, training loss: 852.1063232421875 = 0.7455653548240662 + 100.0 * 8.5136079788208
Epoch 610, val loss: 0.7526686191558838
Epoch 620, training loss: 851.9534912109375 = 0.7392191290855408 + 100.0 * 8.5121431350708
Epoch 620, val loss: 0.7462354898452759
Epoch 630, training loss: 851.5558471679688 = 0.7330703139305115 + 100.0 * 8.508227348327637
Epoch 630, val loss: 0.7402337789535522
Epoch 640, training loss: 851.3539428710938 = 0.7271721363067627 + 100.0 * 8.506267547607422
Epoch 640, val loss: 0.734520673751831
Epoch 650, training loss: 851.2022705078125 = 0.721476137638092 + 100.0 * 8.504807472229004
Epoch 650, val loss: 0.7289689779281616
Epoch 660, training loss: 851.043701171875 = 0.7159630060195923 + 100.0 * 8.503277778625488
Epoch 660, val loss: 0.7235273122787476
Epoch 670, training loss: 851.1953125 = 0.7106534838676453 + 100.0 * 8.504846572875977
Epoch 670, val loss: 0.718305766582489
Epoch 680, training loss: 850.7310180664062 = 0.7054620385169983 + 100.0 * 8.500255584716797
Epoch 680, val loss: 0.7133011817932129
Epoch 690, training loss: 850.6033325195312 = 0.7005203366279602 + 100.0 * 8.499028205871582
Epoch 690, val loss: 0.7085232734680176
Epoch 700, training loss: 850.443359375 = 0.6958062052726746 + 100.0 * 8.497475624084473
Epoch 700, val loss: 0.7039209604263306
Epoch 710, training loss: 850.4378051757812 = 0.6912575960159302 + 100.0 * 8.497465133666992
Epoch 710, val loss: 0.699495255947113
Epoch 720, training loss: 850.0983276367188 = 0.6868536472320557 + 100.0 * 8.494114875793457
Epoch 720, val loss: 0.6953021287918091
Epoch 730, training loss: 849.9336547851562 = 0.682699978351593 + 100.0 * 8.492509841918945
Epoch 730, val loss: 0.6913796067237854
Epoch 740, training loss: 849.7213745117188 = 0.6787546277046204 + 100.0 * 8.490426063537598
Epoch 740, val loss: 0.6876247525215149
Epoch 750, training loss: 849.9298095703125 = 0.6749351024627686 + 100.0 * 8.492548942565918
Epoch 750, val loss: 0.6841639280319214
Epoch 760, training loss: 849.3692016601562 = 0.6712449789047241 + 100.0 * 8.486979484558105
Epoch 760, val loss: 0.6805819272994995
Epoch 770, training loss: 849.1498413085938 = 0.6677342057228088 + 100.0 * 8.484821319580078
Epoch 770, val loss: 0.6772797703742981
Epoch 780, training loss: 848.9669189453125 = 0.6643873453140259 + 100.0 * 8.483025550842285
Epoch 780, val loss: 0.6741822361946106
Epoch 790, training loss: 849.180908203125 = 0.6611741781234741 + 100.0 * 8.485197067260742
Epoch 790, val loss: 0.671328067779541
Epoch 800, training loss: 848.6854858398438 = 0.6581088304519653 + 100.0 * 8.480274200439453
Epoch 800, val loss: 0.6683943271636963
Epoch 810, training loss: 848.4157104492188 = 0.6552237868309021 + 100.0 * 8.477604866027832
Epoch 810, val loss: 0.6658313274383545
Epoch 820, training loss: 848.21728515625 = 0.6524702906608582 + 100.0 * 8.475647926330566
Epoch 820, val loss: 0.6633671522140503
Epoch 830, training loss: 848.028076171875 = 0.6498436331748962 + 100.0 * 8.473782539367676
Epoch 830, val loss: 0.6610304713249207
Epoch 840, training loss: 848.1583251953125 = 0.6473132371902466 + 100.0 * 8.475110054016113
Epoch 840, val loss: 0.6587322950363159
Epoch 850, training loss: 847.8665161132812 = 0.6448357701301575 + 100.0 * 8.472216606140137
Epoch 850, val loss: 0.6566572189331055
Epoch 860, training loss: 847.5985107421875 = 0.6425037980079651 + 100.0 * 8.469559669494629
Epoch 860, val loss: 0.6546333432197571
Epoch 870, training loss: 847.3344116210938 = 0.6402983665466309 + 100.0 * 8.466940879821777
Epoch 870, val loss: 0.6528288125991821
Epoch 880, training loss: 847.1773071289062 = 0.638208270072937 + 100.0 * 8.465391159057617
Epoch 880, val loss: 0.6511020064353943
Epoch 890, training loss: 847.1435546875 = 0.6361940503120422 + 100.0 * 8.465073585510254
Epoch 890, val loss: 0.6495169401168823
Epoch 900, training loss: 846.9287109375 = 0.6342329382896423 + 100.0 * 8.462944984436035
Epoch 900, val loss: 0.6477488279342651
Epoch 910, training loss: 846.7493286132812 = 0.6323591470718384 + 100.0 * 8.461169242858887
Epoch 910, val loss: 0.6463107466697693
Epoch 920, training loss: 846.6104125976562 = 0.6305769681930542 + 100.0 * 8.459798812866211
Epoch 920, val loss: 0.6448348760604858
Epoch 930, training loss: 846.8903198242188 = 0.6288419365882874 + 100.0 * 8.462615013122559
Epoch 930, val loss: 0.6436609029769897
Epoch 940, training loss: 846.6464233398438 = 0.6271188855171204 + 100.0 * 8.460192680358887
Epoch 940, val loss: 0.6419999599456787
Epoch 950, training loss: 846.2638549804688 = 0.6254810690879822 + 100.0 * 8.45638370513916
Epoch 950, val loss: 0.6408659219741821
Epoch 960, training loss: 846.1853637695312 = 0.6239407062530518 + 100.0 * 8.45561408996582
Epoch 960, val loss: 0.6397788524627686
Epoch 970, training loss: 846.068359375 = 0.6224554181098938 + 100.0 * 8.454459190368652
Epoch 970, val loss: 0.6386300921440125
Epoch 980, training loss: 846.0352172851562 = 0.6209990978240967 + 100.0 * 8.454142570495605
Epoch 980, val loss: 0.6376094818115234
Epoch 990, training loss: 845.944580078125 = 0.6195604801177979 + 100.0 * 8.45324993133545
Epoch 990, val loss: 0.6364930272102356
Epoch 1000, training loss: 845.8922119140625 = 0.6181715130805969 + 100.0 * 8.452740669250488
Epoch 1000, val loss: 0.6356433629989624
Epoch 1010, training loss: 845.7749633789062 = 0.6168582439422607 + 100.0 * 8.451581001281738
Epoch 1010, val loss: 0.6346370577812195
Epoch 1020, training loss: 845.669677734375 = 0.6155722141265869 + 100.0 * 8.450540542602539
Epoch 1020, val loss: 0.6337653398513794
Epoch 1030, training loss: 845.5884399414062 = 0.6143289804458618 + 100.0 * 8.44974136352539
Epoch 1030, val loss: 0.6329152584075928
Epoch 1040, training loss: 845.5986938476562 = 0.6131227016448975 + 100.0 * 8.44985580444336
Epoch 1040, val loss: 0.6321841478347778
Epoch 1050, training loss: 845.4332885742188 = 0.6119272112846375 + 100.0 * 8.448213577270508
Epoch 1050, val loss: 0.6313368082046509
Epoch 1060, training loss: 845.3336181640625 = 0.6107810139656067 + 100.0 * 8.44722843170166
Epoch 1060, val loss: 0.6305041313171387
Epoch 1070, training loss: 845.2085571289062 = 0.6096828579902649 + 100.0 * 8.445988655090332
Epoch 1070, val loss: 0.6299219727516174
Epoch 1080, training loss: 845.1407470703125 = 0.6086211204528809 + 100.0 * 8.445321083068848
Epoch 1080, val loss: 0.6292445063591003
Epoch 1090, training loss: 845.4407348632812 = 0.6075736284255981 + 100.0 * 8.448331832885742
Epoch 1090, val loss: 0.628521203994751
Epoch 1100, training loss: 845.0189208984375 = 0.606498658657074 + 100.0 * 8.444124221801758
Epoch 1100, val loss: 0.6278720498085022
Epoch 1110, training loss: 844.8049926757812 = 0.6055100560188293 + 100.0 * 8.441994667053223
Epoch 1110, val loss: 0.6273179650306702
Epoch 1120, training loss: 844.7061767578125 = 0.6045560240745544 + 100.0 * 8.44101619720459
Epoch 1120, val loss: 0.6268059611320496
Epoch 1130, training loss: 844.6131591796875 = 0.6036198735237122 + 100.0 * 8.440094947814941
Epoch 1130, val loss: 0.6263132691383362
Epoch 1140, training loss: 845.1497802734375 = 0.6026957631111145 + 100.0 * 8.445470809936523
Epoch 1140, val loss: 0.6259356141090393
Epoch 1150, training loss: 844.5548095703125 = 0.6017308831214905 + 100.0 * 8.439530372619629
Epoch 1150, val loss: 0.6250954270362854
Epoch 1160, training loss: 844.2991333007812 = 0.6008186340332031 + 100.0 * 8.436983108520508
Epoch 1160, val loss: 0.6246238946914673
Epoch 1170, training loss: 844.2189331054688 = 0.5999454259872437 + 100.0 * 8.436189651489258
Epoch 1170, val loss: 0.6242258548736572
Epoch 1180, training loss: 844.12158203125 = 0.5990971326828003 + 100.0 * 8.435224533081055
Epoch 1180, val loss: 0.6237154006958008
Epoch 1190, training loss: 844.0625610351562 = 0.5982525944709778 + 100.0 * 8.434642791748047
Epoch 1190, val loss: 0.6233142614364624
Epoch 1200, training loss: 844.4281005859375 = 0.5973670482635498 + 100.0 * 8.438307762145996
Epoch 1200, val loss: 0.6227461099624634
Epoch 1210, training loss: 844.0040893554688 = 0.5964608788490295 + 100.0 * 8.434076309204102
Epoch 1210, val loss: 0.6223646998405457
Epoch 1220, training loss: 843.9119262695312 = 0.5956457853317261 + 100.0 * 8.433162689208984
Epoch 1220, val loss: 0.6218125820159912
Epoch 1230, training loss: 843.7328491210938 = 0.5948576331138611 + 100.0 * 8.431380271911621
Epoch 1230, val loss: 0.6214470863342285
Epoch 1240, training loss: 843.6403198242188 = 0.5940881371498108 + 100.0 * 8.430461883544922
Epoch 1240, val loss: 0.621064305305481
Epoch 1250, training loss: 843.5676879882812 = 0.5933292508125305 + 100.0 * 8.429743766784668
Epoch 1250, val loss: 0.6206355690956116
Epoch 1260, training loss: 843.6875 = 0.5925730466842651 + 100.0 * 8.430949211120605
Epoch 1260, val loss: 0.6201626658439636
Epoch 1270, training loss: 843.4041748046875 = 0.5917679071426392 + 100.0 * 8.428123474121094
Epoch 1270, val loss: 0.6198647618293762
Epoch 1280, training loss: 843.3839721679688 = 0.5910078883171082 + 100.0 * 8.427929878234863
Epoch 1280, val loss: 0.6193630695343018
Epoch 1290, training loss: 843.29931640625 = 0.5902782082557678 + 100.0 * 8.427090644836426
Epoch 1290, val loss: 0.6191005706787109
Epoch 1300, training loss: 843.1961669921875 = 0.5895735025405884 + 100.0 * 8.426065444946289
Epoch 1300, val loss: 0.6186697483062744
Epoch 1310, training loss: 843.1846313476562 = 0.5888724327087402 + 100.0 * 8.425957679748535
Epoch 1310, val loss: 0.6184496283531189
Epoch 1320, training loss: 843.0379028320312 = 0.5881496071815491 + 100.0 * 8.424497604370117
Epoch 1320, val loss: 0.6179054379463196
Epoch 1330, training loss: 843.0253295898438 = 0.5874276757240295 + 100.0 * 8.424379348754883
Epoch 1330, val loss: 0.6176770329475403
Epoch 1340, training loss: 842.9081420898438 = 0.58675616979599 + 100.0 * 8.423213958740234
Epoch 1340, val loss: 0.6172318458557129
Epoch 1350, training loss: 842.8569946289062 = 0.5860894918441772 + 100.0 * 8.422709465026855
Epoch 1350, val loss: 0.6169705986976624
Epoch 1360, training loss: 843.3394775390625 = 0.585396409034729 + 100.0 * 8.42754077911377
Epoch 1360, val loss: 0.6166876554489136
Epoch 1370, training loss: 842.6918334960938 = 0.584684431552887 + 100.0 * 8.421072006225586
Epoch 1370, val loss: 0.6161954998970032
Epoch 1380, training loss: 842.630615234375 = 0.5840257406234741 + 100.0 * 8.420465469360352
Epoch 1380, val loss: 0.6158242225646973
Epoch 1390, training loss: 842.5591430664062 = 0.5833876132965088 + 100.0 * 8.419757843017578
Epoch 1390, val loss: 0.6155857443809509
Epoch 1400, training loss: 842.4713134765625 = 0.5827633738517761 + 100.0 * 8.418885231018066
Epoch 1400, val loss: 0.6152502298355103
Epoch 1410, training loss: 842.3949584960938 = 0.5821361541748047 + 100.0 * 8.41812801361084
Epoch 1410, val loss: 0.6149694323539734
Epoch 1420, training loss: 842.4796142578125 = 0.5815157890319824 + 100.0 * 8.418980598449707
Epoch 1420, val loss: 0.6145390868186951
Epoch 1430, training loss: 842.531982421875 = 0.580846905708313 + 100.0 * 8.419510841369629
Epoch 1430, val loss: 0.6143441796302795
Epoch 1440, training loss: 842.218505859375 = 0.5801734328269958 + 100.0 * 8.416382789611816
Epoch 1440, val loss: 0.6140263676643372
Epoch 1450, training loss: 842.14306640625 = 0.5795503258705139 + 100.0 * 8.415635108947754
Epoch 1450, val loss: 0.613620400428772
Epoch 1460, training loss: 842.1010131835938 = 0.5789457559585571 + 100.0 * 8.415221214294434
Epoch 1460, val loss: 0.6133393049240112
Epoch 1470, training loss: 842.4352416992188 = 0.5783379077911377 + 100.0 * 8.418569564819336
Epoch 1470, val loss: 0.6131278872489929
Epoch 1480, training loss: 842.1151733398438 = 0.577668309211731 + 100.0 * 8.415374755859375
Epoch 1480, val loss: 0.6126236319541931
Epoch 1490, training loss: 842.0480346679688 = 0.5770261883735657 + 100.0 * 8.41471004486084
Epoch 1490, val loss: 0.6123781204223633
Epoch 1500, training loss: 841.8809814453125 = 0.5764201283454895 + 100.0 * 8.413045883178711
Epoch 1500, val loss: 0.6120069622993469
Epoch 1510, training loss: 841.8096313476562 = 0.5758297443389893 + 100.0 * 8.412338256835938
Epoch 1510, val loss: 0.6117512583732605
Epoch 1520, training loss: 842.05322265625 = 0.5752461552619934 + 100.0 * 8.414779663085938
Epoch 1520, val loss: 0.6113141179084778
Epoch 1530, training loss: 841.7762451171875 = 0.57459956407547 + 100.0 * 8.412016868591309
Epoch 1530, val loss: 0.6112836599349976
Epoch 1540, training loss: 841.6870727539062 = 0.5739932060241699 + 100.0 * 8.411130905151367
Epoch 1540, val loss: 0.6107863187789917
Epoch 1550, training loss: 841.6060791015625 = 0.5733915567398071 + 100.0 * 8.410326957702637
Epoch 1550, val loss: 0.6105930805206299
Epoch 1560, training loss: 841.5403442382812 = 0.5728024840354919 + 100.0 * 8.409675598144531
Epoch 1560, val loss: 0.6102421879768372
Epoch 1570, training loss: 841.5576171875 = 0.572212815284729 + 100.0 * 8.4098539352417
Epoch 1570, val loss: 0.6099715232849121
Epoch 1580, training loss: 841.7440185546875 = 0.5715841054916382 + 100.0 * 8.411724090576172
Epoch 1580, val loss: 0.6095491647720337
Epoch 1590, training loss: 841.5227661132812 = 0.5709306597709656 + 100.0 * 8.409518241882324
Epoch 1590, val loss: 0.6093167662620544
Epoch 1600, training loss: 841.3704223632812 = 0.5703054070472717 + 100.0 * 8.408000946044922
Epoch 1600, val loss: 0.6089306473731995
Epoch 1610, training loss: 841.3195190429688 = 0.5697108507156372 + 100.0 * 8.407498359680176
Epoch 1610, val loss: 0.6085850596427917
Epoch 1620, training loss: 841.267578125 = 0.5691248774528503 + 100.0 * 8.406984329223633
Epoch 1620, val loss: 0.6082783937454224
Epoch 1630, training loss: 841.260009765625 = 0.5685381889343262 + 100.0 * 8.406914710998535
Epoch 1630, val loss: 0.6079354286193848
Epoch 1640, training loss: 841.6755981445312 = 0.5679328441619873 + 100.0 * 8.411076545715332
Epoch 1640, val loss: 0.6075001955032349
Epoch 1650, training loss: 841.3641357421875 = 0.5672764182090759 + 100.0 * 8.407968521118164
Epoch 1650, val loss: 0.607470691204071
Epoch 1660, training loss: 841.2109985351562 = 0.5666518211364746 + 100.0 * 8.40644359588623
Epoch 1660, val loss: 0.6068767309188843
Epoch 1670, training loss: 841.0748901367188 = 0.5660459399223328 + 100.0 * 8.405088424682617
Epoch 1670, val loss: 0.6066668629646301
Epoch 1680, training loss: 841.0057983398438 = 0.5654539465904236 + 100.0 * 8.404403686523438
Epoch 1680, val loss: 0.6062484383583069
Epoch 1690, training loss: 840.9942626953125 = 0.5648592710494995 + 100.0 * 8.40429401397705
Epoch 1690, val loss: 0.6059964895248413
Epoch 1700, training loss: 841.5121459960938 = 0.5642356872558594 + 100.0 * 8.409479141235352
Epoch 1700, val loss: 0.6056143045425415
Epoch 1710, training loss: 840.8942260742188 = 0.5635760426521301 + 100.0 * 8.403306007385254
Epoch 1710, val loss: 0.6052916049957275
Epoch 1720, training loss: 840.9107666015625 = 0.562956690788269 + 100.0 * 8.403478622436523
Epoch 1720, val loss: 0.604772686958313
Epoch 1730, training loss: 840.8164672851562 = 0.5623521208763123 + 100.0 * 8.402541160583496
Epoch 1730, val loss: 0.6044905185699463
Epoch 1740, training loss: 840.7859497070312 = 0.5617538690567017 + 100.0 * 8.402241706848145
Epoch 1740, val loss: 0.6041492819786072
Epoch 1750, training loss: 840.8758544921875 = 0.5611520409584045 + 100.0 * 8.403146743774414
Epoch 1750, val loss: 0.6037309765815735
Epoch 1760, training loss: 840.9157104492188 = 0.5605089664459229 + 100.0 * 8.403552055358887
Epoch 1760, val loss: 0.60334712266922
Epoch 1770, training loss: 840.7450561523438 = 0.5598583817481995 + 100.0 * 8.401851654052734
Epoch 1770, val loss: 0.6030664443969727
Epoch 1780, training loss: 840.6608276367188 = 0.5592319369316101 + 100.0 * 8.401016235351562
Epoch 1780, val loss: 0.60263592004776
Epoch 1790, training loss: 840.611572265625 = 0.5586176514625549 + 100.0 * 8.400529861450195
Epoch 1790, val loss: 0.6022827625274658
Epoch 1800, training loss: 840.5950317382812 = 0.5580027103424072 + 100.0 * 8.400370597839355
Epoch 1800, val loss: 0.6019464731216431
Epoch 1810, training loss: 841.0191040039062 = 0.5573737025260925 + 100.0 * 8.404617309570312
Epoch 1810, val loss: 0.601560652256012
Epoch 1820, training loss: 840.7015991210938 = 0.5567033290863037 + 100.0 * 8.401449203491211
Epoch 1820, val loss: 0.6011335849761963
Epoch 1830, training loss: 840.54248046875 = 0.5560458898544312 + 100.0 * 8.399864196777344
Epoch 1830, val loss: 0.6006113290786743
Epoch 1840, training loss: 840.4616088867188 = 0.5554124712944031 + 100.0 * 8.399062156677246
Epoch 1840, val loss: 0.6002495884895325
Epoch 1850, training loss: 840.431396484375 = 0.5547825694084167 + 100.0 * 8.398765563964844
Epoch 1850, val loss: 0.5998594760894775
Epoch 1860, training loss: 840.4051513671875 = 0.5541517734527588 + 100.0 * 8.398509979248047
Epoch 1860, val loss: 0.5994905829429626
Epoch 1870, training loss: 840.6591796875 = 0.5535150170326233 + 100.0 * 8.401056289672852
Epoch 1870, val loss: 0.599253237247467
Epoch 1880, training loss: 840.8241577148438 = 0.5528085231781006 + 100.0 * 8.402713775634766
Epoch 1880, val loss: 0.5984609723091125
Epoch 1890, training loss: 840.4031372070312 = 0.5520799160003662 + 100.0 * 8.398510932922363
Epoch 1890, val loss: 0.5980184078216553
Epoch 1900, training loss: 840.3192138671875 = 0.5514100193977356 + 100.0 * 8.39767837524414
Epoch 1900, val loss: 0.5975854396820068
Epoch 1910, training loss: 840.2941284179688 = 0.5507615804672241 + 100.0 * 8.397433280944824
Epoch 1910, val loss: 0.5971606969833374
Epoch 1920, training loss: 840.2471923828125 = 0.5501156449317932 + 100.0 * 8.396970748901367
Epoch 1920, val loss: 0.596734344959259
Epoch 1930, training loss: 840.2305908203125 = 0.5494634509086609 + 100.0 * 8.396811485290527
Epoch 1930, val loss: 0.5962657928466797
Epoch 1940, training loss: 840.3526000976562 = 0.5487978458404541 + 100.0 * 8.398037910461426
Epoch 1940, val loss: 0.5957778692245483
Epoch 1950, training loss: 840.3443603515625 = 0.5480899214744568 + 100.0 * 8.39796257019043
Epoch 1950, val loss: 0.5952942967414856
Epoch 1960, training loss: 840.2495727539062 = 0.5473828911781311 + 100.0 * 8.397022247314453
Epoch 1960, val loss: 0.594845175743103
Epoch 1970, training loss: 840.1663208007812 = 0.5466927886009216 + 100.0 * 8.396196365356445
Epoch 1970, val loss: 0.5943301320075989
Epoch 1980, training loss: 840.0928955078125 = 0.5460132956504822 + 100.0 * 8.395468711853027
Epoch 1980, val loss: 0.5938735604286194
Epoch 1990, training loss: 840.0796508789062 = 0.5453329682350159 + 100.0 * 8.395342826843262
Epoch 1990, val loss: 0.5934068560600281
Epoch 2000, training loss: 840.0579223632812 = 0.5446453094482422 + 100.0 * 8.395133018493652
Epoch 2000, val loss: 0.592910647392273
Epoch 2010, training loss: 840.2409057617188 = 0.5439478754997253 + 100.0 * 8.39696979522705
Epoch 2010, val loss: 0.5924562811851501
Epoch 2020, training loss: 840.0432739257812 = 0.5432005524635315 + 100.0 * 8.395000457763672
Epoch 2020, val loss: 0.5918580889701843
Epoch 2030, training loss: 840.1290283203125 = 0.542452871799469 + 100.0 * 8.395865440368652
Epoch 2030, val loss: 0.5912976264953613
Epoch 2040, training loss: 839.99755859375 = 0.5417218208312988 + 100.0 * 8.39455795288086
Epoch 2040, val loss: 0.5907908082008362
Epoch 2050, training loss: 839.9558715820312 = 0.540999174118042 + 100.0 * 8.394148826599121
Epoch 2050, val loss: 0.5902811884880066
Epoch 2060, training loss: 840.0358276367188 = 0.5402876138687134 + 100.0 * 8.3949556350708
Epoch 2060, val loss: 0.589891254901886
Epoch 2070, training loss: 840.0311889648438 = 0.5395352840423584 + 100.0 * 8.394916534423828
Epoch 2070, val loss: 0.5892535448074341
Epoch 2080, training loss: 839.8982543945312 = 0.5387723445892334 + 100.0 * 8.393594741821289
Epoch 2080, val loss: 0.5887131094932556
Epoch 2090, training loss: 839.8545532226562 = 0.5380355715751648 + 100.0 * 8.393165588378906
Epoch 2090, val loss: 0.5880482196807861
Epoch 2100, training loss: 839.82666015625 = 0.5373091697692871 + 100.0 * 8.39289379119873
Epoch 2100, val loss: 0.5875716805458069
Epoch 2110, training loss: 839.8748779296875 = 0.5365807414054871 + 100.0 * 8.393383026123047
Epoch 2110, val loss: 0.5869762897491455
Epoch 2120, training loss: 839.93212890625 = 0.5358303785324097 + 100.0 * 8.393962860107422
Epoch 2120, val loss: 0.5864073038101196
Epoch 2130, training loss: 839.7311401367188 = 0.5350693464279175 + 100.0 * 8.391960144042969
Epoch 2130, val loss: 0.5859522223472595
Epoch 2140, training loss: 839.745361328125 = 0.5343224406242371 + 100.0 * 8.392110824584961
Epoch 2140, val loss: 0.5853946208953857
Epoch 2150, training loss: 839.7096557617188 = 0.5335716009140015 + 100.0 * 8.39176082611084
Epoch 2150, val loss: 0.5848439931869507
Epoch 2160, training loss: 840.062744140625 = 0.5328141450881958 + 100.0 * 8.395298957824707
Epoch 2160, val loss: 0.5842660665512085
Epoch 2170, training loss: 839.7504272460938 = 0.5320063233375549 + 100.0 * 8.392184257507324
Epoch 2170, val loss: 0.5835596323013306
Epoch 2180, training loss: 839.65576171875 = 0.5312302112579346 + 100.0 * 8.391244888305664
Epoch 2180, val loss: 0.5830206274986267
Epoch 2190, training loss: 839.721435546875 = 0.5304557085037231 + 100.0 * 8.3919095993042
Epoch 2190, val loss: 0.5823562741279602
Epoch 2200, training loss: 839.760009765625 = 0.5296529531478882 + 100.0 * 8.392303466796875
Epoch 2200, val loss: 0.5817843079566956
Epoch 2210, training loss: 839.6188354492188 = 0.5288414359092712 + 100.0 * 8.390899658203125
Epoch 2210, val loss: 0.5812972784042358
Epoch 2220, training loss: 839.5087890625 = 0.5280424356460571 + 100.0 * 8.38980770111084
Epoch 2220, val loss: 0.5806165337562561
Epoch 2230, training loss: 839.46533203125 = 0.5272529125213623 + 100.0 * 8.38938045501709
Epoch 2230, val loss: 0.5800543427467346
Epoch 2240, training loss: 839.953125 = 0.5264665484428406 + 100.0 * 8.394266128540039
Epoch 2240, val loss: 0.579677939414978
Epoch 2250, training loss: 839.8576049804688 = 0.5255911946296692 + 100.0 * 8.393320083618164
Epoch 2250, val loss: 0.5786430239677429
Epoch 2260, training loss: 839.3981323242188 = 0.5247256755828857 + 100.0 * 8.388733863830566
Epoch 2260, val loss: 0.5780722498893738
Epoch 2270, training loss: 839.404296875 = 0.5239034295082092 + 100.0 * 8.388803482055664
Epoch 2270, val loss: 0.577549934387207
Epoch 2280, training loss: 839.3375244140625 = 0.5230932831764221 + 100.0 * 8.388144493103027
Epoch 2280, val loss: 0.5769181847572327
Epoch 2290, training loss: 839.3375854492188 = 0.5222800970077515 + 100.0 * 8.388153076171875
Epoch 2290, val loss: 0.5762532353401184
Epoch 2300, training loss: 839.7656860351562 = 0.5214511156082153 + 100.0 * 8.39244270324707
Epoch 2300, val loss: 0.5755671262741089
Epoch 2310, training loss: 839.2930297851562 = 0.5205628275871277 + 100.0 * 8.387724876403809
Epoch 2310, val loss: 0.5750290155410767
Epoch 2320, training loss: 839.2283935546875 = 0.5196966528892517 + 100.0 * 8.387086868286133
Epoch 2320, val loss: 0.5742722749710083
Epoch 2330, training loss: 839.2010498046875 = 0.5188484787940979 + 100.0 * 8.386821746826172
Epoch 2330, val loss: 0.5736604928970337
Epoch 2340, training loss: 839.18359375 = 0.5180050134658813 + 100.0 * 8.386655807495117
Epoch 2340, val loss: 0.5730656981468201
Epoch 2350, training loss: 839.1439819335938 = 0.5171553492546082 + 100.0 * 8.386268615722656
Epoch 2350, val loss: 0.5724095106124878
Epoch 2360, training loss: 839.1681518554688 = 0.5162967443466187 + 100.0 * 8.386518478393555
Epoch 2360, val loss: 0.5718335509300232
Epoch 2370, training loss: 839.2313842773438 = 0.5154182314872742 + 100.0 * 8.38715934753418
Epoch 2370, val loss: 0.5711735486984253
Epoch 2380, training loss: 839.4066162109375 = 0.5145171880722046 + 100.0 * 8.388920783996582
Epoch 2380, val loss: 0.5702874064445496
Epoch 2390, training loss: 839.3401489257812 = 0.5136063694953918 + 100.0 * 8.388265609741211
Epoch 2390, val loss: 0.5697542428970337
Epoch 2400, training loss: 839.3712768554688 = 0.512686014175415 + 100.0 * 8.388586044311523
Epoch 2400, val loss: 0.5689750909805298
Epoch 2410, training loss: 839.0790405273438 = 0.5117619633674622 + 100.0 * 8.385672569274902
Epoch 2410, val loss: 0.5683888792991638
Epoch 2420, training loss: 838.9750366210938 = 0.5108712911605835 + 100.0 * 8.384641647338867
Epoch 2420, val loss: 0.5676266551017761
Epoch 2430, training loss: 838.9502563476562 = 0.5099893808364868 + 100.0 * 8.38440227508545
Epoch 2430, val loss: 0.5669808983802795
Epoch 2440, training loss: 838.9634399414062 = 0.5091046690940857 + 100.0 * 8.384543418884277
Epoch 2440, val loss: 0.5663654804229736
Epoch 2450, training loss: 839.204345703125 = 0.5082067847251892 + 100.0 * 8.386961936950684
Epoch 2450, val loss: 0.5656704902648926
Epoch 2460, training loss: 838.8697509765625 = 0.507269024848938 + 100.0 * 8.383625030517578
Epoch 2460, val loss: 0.5649445056915283
Epoch 2470, training loss: 838.91845703125 = 0.5063461661338806 + 100.0 * 8.38412094116211
Epoch 2470, val loss: 0.5642611980438232
Epoch 2480, training loss: 838.8349609375 = 0.5054320096969604 + 100.0 * 8.383295059204102
Epoch 2480, val loss: 0.5635458827018738
Epoch 2490, training loss: 839.0690307617188 = 0.5045242309570312 + 100.0 * 8.385644912719727
Epoch 2490, val loss: 0.5627487897872925
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.776255707762557
0.8112004636673188
The final CL Acc:0.77845, 0.01273, The final GNN Acc:0.81057, 0.00061
raw graph: torch.Size([2, 88648])
add edge: torch.Size([2, 110820])
remove edge: torch.Size([2, 66218])
updated graph: torch.Size([2, 88390])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 1059.345458984375 = 1.1156673431396484 + 100.0 * 10.582297325134277
Epoch 0, val loss: 1.114112138748169
Epoch 10, training loss: 1059.302734375 = 1.1110159158706665 + 100.0 * 10.581917762756348
Epoch 10, val loss: 1.1094545125961304
Epoch 20, training loss: 1059.1187744140625 = 1.1059844493865967 + 100.0 * 10.580127716064453
Epoch 20, val loss: 1.1044032573699951
Epoch 30, training loss: 1058.267578125 = 1.1003530025482178 + 100.0 * 10.571672439575195
Epoch 30, val loss: 1.0987293720245361
Epoch 40, training loss: 1054.816650390625 = 1.0937862396240234 + 100.0 * 10.537229537963867
Epoch 40, val loss: 1.0920968055725098
Epoch 50, training loss: 1044.412841796875 = 1.0861858129501343 + 100.0 * 10.433266639709473
Epoch 50, val loss: 1.084459662437439
Epoch 60, training loss: 1019.544189453125 = 1.0783603191375732 + 100.0 * 10.18465805053711
Epoch 60, val loss: 1.0767077207565308
Epoch 70, training loss: 978.8919677734375 = 1.0699458122253418 + 100.0 * 9.778220176696777
Epoch 70, val loss: 1.0682988166809082
Epoch 80, training loss: 956.0387573242188 = 1.062217354774475 + 100.0 * 9.549765586853027
Epoch 80, val loss: 1.0609310865402222
Epoch 90, training loss: 950.2705078125 = 1.0573406219482422 + 100.0 * 9.492132186889648
Epoch 90, val loss: 1.0564496517181396
Epoch 100, training loss: 944.3948974609375 = 1.05336594581604 + 100.0 * 9.433415412902832
Epoch 100, val loss: 1.0527266263961792
Epoch 110, training loss: 938.0096435546875 = 1.049126148223877 + 100.0 * 9.36960506439209
Epoch 110, val loss: 1.0486059188842773
Epoch 120, training loss: 930.475341796875 = 1.0444761514663696 + 100.0 * 9.29430866241455
Epoch 120, val loss: 1.044101357460022
Epoch 130, training loss: 922.77978515625 = 1.0400737524032593 + 100.0 * 9.21739673614502
Epoch 130, val loss: 1.039965033531189
Epoch 140, training loss: 918.0906982421875 = 1.0364229679107666 + 100.0 * 9.17054271697998
Epoch 140, val loss: 1.036463975906372
Epoch 150, training loss: 915.098876953125 = 1.0326250791549683 + 100.0 * 9.14066219329834
Epoch 150, val loss: 1.032753348350525
Epoch 160, training loss: 910.43505859375 = 1.0290789604187012 + 100.0 * 9.094059944152832
Epoch 160, val loss: 1.029458999633789
Epoch 170, training loss: 903.5001831054688 = 1.0267333984375 + 100.0 * 9.024734497070312
Epoch 170, val loss: 1.0273727178573608
Epoch 180, training loss: 894.7435302734375 = 1.0251985788345337 + 100.0 * 8.937183380126953
Epoch 180, val loss: 1.0259873867034912
Epoch 190, training loss: 888.989990234375 = 1.02325439453125 + 100.0 * 8.879667282104492
Epoch 190, val loss: 1.0239945650100708
Epoch 200, training loss: 885.3136596679688 = 1.0193579196929932 + 100.0 * 8.84294319152832
Epoch 200, val loss: 1.0200626850128174
Epoch 210, training loss: 883.34423828125 = 1.0142993927001953 + 100.0 * 8.823299407958984
Epoch 210, val loss: 1.0151499509811401
Epoch 220, training loss: 881.5557250976562 = 1.0094361305236816 + 100.0 * 8.805462837219238
Epoch 220, val loss: 1.0105382204055786
Epoch 230, training loss: 879.6603393554688 = 1.0052987337112427 + 100.0 * 8.786550521850586
Epoch 230, val loss: 1.0066548585891724
Epoch 240, training loss: 877.1683349609375 = 1.001355528831482 + 100.0 * 8.761670112609863
Epoch 240, val loss: 1.0030161142349243
Epoch 250, training loss: 874.5062255859375 = 0.9975437521934509 + 100.0 * 8.735086441040039
Epoch 250, val loss: 0.9994586706161499
Epoch 260, training loss: 872.1998901367188 = 0.9934121370315552 + 100.0 * 8.712064743041992
Epoch 260, val loss: 0.9955246448516846
Epoch 270, training loss: 870.2889404296875 = 0.9887576699256897 + 100.0 * 8.693001747131348
Epoch 270, val loss: 0.9911010265350342
Epoch 280, training loss: 868.461181640625 = 0.9838576912879944 + 100.0 * 8.674773216247559
Epoch 280, val loss: 0.9864928126335144
Epoch 290, training loss: 866.7341918945312 = 0.9789682626724243 + 100.0 * 8.657552719116211
Epoch 290, val loss: 0.9819133877754211
Epoch 300, training loss: 865.180419921875 = 0.9739474654197693 + 100.0 * 8.642065048217773
Epoch 300, val loss: 0.9771875739097595
Epoch 310, training loss: 863.9912719726562 = 0.968574047088623 + 100.0 * 8.630227088928223
Epoch 310, val loss: 0.9720970988273621
Epoch 320, training loss: 862.7966918945312 = 0.9628078937530518 + 100.0 * 8.618338584899902
Epoch 320, val loss: 0.9666478037834167
Epoch 330, training loss: 861.5667724609375 = 0.9569807648658752 + 100.0 * 8.606098175048828
Epoch 330, val loss: 0.9612094163894653
Epoch 340, training loss: 860.0825805664062 = 0.9513617157936096 + 100.0 * 8.591312408447266
Epoch 340, val loss: 0.9560075402259827
Epoch 350, training loss: 858.53564453125 = 0.9457219839096069 + 100.0 * 8.575899124145508
Epoch 350, val loss: 0.9507055878639221
Epoch 360, training loss: 856.9851684570312 = 0.9397873282432556 + 100.0 * 8.560454368591309
Epoch 360, val loss: 0.9452401399612427
Epoch 370, training loss: 855.7244873046875 = 0.9335356950759888 + 100.0 * 8.5479097366333
Epoch 370, val loss: 0.9393678903579712
Epoch 380, training loss: 854.7669067382812 = 0.9266561269760132 + 100.0 * 8.538402557373047
Epoch 380, val loss: 0.9329476356506348
Epoch 390, training loss: 853.7581176757812 = 0.9193004369735718 + 100.0 * 8.528388023376465
Epoch 390, val loss: 0.9261383414268494
Epoch 400, training loss: 852.8827514648438 = 0.9118443727493286 + 100.0 * 8.519708633422852
Epoch 400, val loss: 0.9192023277282715
Epoch 410, training loss: 852.1187744140625 = 0.9043232798576355 + 100.0 * 8.512145042419434
Epoch 410, val loss: 0.9121788740158081
Epoch 420, training loss: 851.327880859375 = 0.8965849876403809 + 100.0 * 8.504312515258789
Epoch 420, val loss: 0.9050573706626892
Epoch 430, training loss: 850.69140625 = 0.8888880014419556 + 100.0 * 8.498024940490723
Epoch 430, val loss: 0.8979077339172363
Epoch 440, training loss: 850.136962890625 = 0.881203293800354 + 100.0 * 8.492557525634766
Epoch 440, val loss: 0.890852153301239
Epoch 450, training loss: 849.7318115234375 = 0.8734567761421204 + 100.0 * 8.4885835647583
Epoch 450, val loss: 0.8837801218032837
Epoch 460, training loss: 849.0008544921875 = 0.8658634424209595 + 100.0 * 8.48134994506836
Epoch 460, val loss: 0.8768264055252075
Epoch 470, training loss: 848.5670776367188 = 0.8584257960319519 + 100.0 * 8.477087020874023
Epoch 470, val loss: 0.8700373768806458
Epoch 480, training loss: 848.1017456054688 = 0.8510698080062866 + 100.0 * 8.472506523132324
Epoch 480, val loss: 0.8633675575256348
Epoch 490, training loss: 847.781494140625 = 0.8438153862953186 + 100.0 * 8.469376564025879
Epoch 490, val loss: 0.856848955154419
Epoch 500, training loss: 847.4822998046875 = 0.8366677761077881 + 100.0 * 8.466456413269043
Epoch 500, val loss: 0.850311815738678
Epoch 510, training loss: 846.94970703125 = 0.8296905755996704 + 100.0 * 8.461199760437012
Epoch 510, val loss: 0.8440532088279724
Epoch 520, training loss: 846.5929565429688 = 0.822875440120697 + 100.0 * 8.457700729370117
Epoch 520, val loss: 0.8379629254341125
Epoch 530, training loss: 846.2650146484375 = 0.8161828517913818 + 100.0 * 8.454488754272461
Epoch 530, val loss: 0.8319410681724548
Epoch 540, training loss: 845.908203125 = 0.8097002506256104 + 100.0 * 8.450984954833984
Epoch 540, val loss: 0.8261297941207886
Epoch 550, training loss: 845.592041015625 = 0.8033784627914429 + 100.0 * 8.44788646697998
Epoch 550, val loss: 0.8204648494720459
Epoch 560, training loss: 845.4879150390625 = 0.7971401214599609 + 100.0 * 8.446907997131348
Epoch 560, val loss: 0.8149052262306213
Epoch 570, training loss: 845.0762329101562 = 0.7910272479057312 + 100.0 * 8.442852020263672
Epoch 570, val loss: 0.8094300627708435
Epoch 580, training loss: 844.7125244140625 = 0.7850589156150818 + 100.0 * 8.439274787902832
Epoch 580, val loss: 0.8041201829910278
Epoch 590, training loss: 844.4132080078125 = 0.7791820168495178 + 100.0 * 8.43634033203125
Epoch 590, val loss: 0.7989090085029602
Epoch 600, training loss: 844.8690795898438 = 0.7733755111694336 + 100.0 * 8.440957069396973
Epoch 600, val loss: 0.7937306761741638
Epoch 610, training loss: 844.0972290039062 = 0.7675275802612305 + 100.0 * 8.433297157287598
Epoch 610, val loss: 0.7885430455207825
Epoch 620, training loss: 843.6666870117188 = 0.7618804574012756 + 100.0 * 8.429047584533691
Epoch 620, val loss: 0.7835260033607483
Epoch 630, training loss: 843.431884765625 = 0.7562562823295593 + 100.0 * 8.426755905151367
Epoch 630, val loss: 0.7785038948059082
Epoch 640, training loss: 843.3011474609375 = 0.7506747841835022 + 100.0 * 8.425504684448242
Epoch 640, val loss: 0.7735161781311035
Epoch 650, training loss: 843.034912109375 = 0.7450525164604187 + 100.0 * 8.422898292541504
Epoch 650, val loss: 0.7685561180114746
Epoch 660, training loss: 842.8749389648438 = 0.7394890785217285 + 100.0 * 8.421354293823242
Epoch 660, val loss: 0.7635558247566223
Epoch 670, training loss: 842.6533203125 = 0.7339009642601013 + 100.0 * 8.419194221496582
Epoch 670, val loss: 0.758571982383728
Epoch 680, training loss: 842.4732055664062 = 0.7283191680908203 + 100.0 * 8.417448997497559
Epoch 680, val loss: 0.7535800933837891
Epoch 690, training loss: 842.47314453125 = 0.722734272480011 + 100.0 * 8.41750431060791
Epoch 690, val loss: 0.7485738396644592
Epoch 700, training loss: 842.2999877929688 = 0.7170382142066956 + 100.0 * 8.4158296585083
Epoch 700, val loss: 0.7434532642364502
Epoch 710, training loss: 842.0391235351562 = 0.7113708257675171 + 100.0 * 8.413277626037598
Epoch 710, val loss: 0.7384291887283325
Epoch 720, training loss: 841.85888671875 = 0.705738365650177 + 100.0 * 8.411531448364258
Epoch 720, val loss: 0.7333429455757141
Epoch 730, training loss: 841.8475952148438 = 0.7000799775123596 + 100.0 * 8.41147518157959
Epoch 730, val loss: 0.7282621264457703
Epoch 740, training loss: 841.6878662109375 = 0.6943413019180298 + 100.0 * 8.409934997558594
Epoch 740, val loss: 0.7230432629585266
Epoch 750, training loss: 841.4653930664062 = 0.6886521577835083 + 100.0 * 8.407767295837402
Epoch 750, val loss: 0.7179373502731323
Epoch 760, training loss: 841.3601684570312 = 0.6829814314842224 + 100.0 * 8.406771659851074
Epoch 760, val loss: 0.7127299904823303
Epoch 770, training loss: 841.4207763671875 = 0.6772542595863342 + 100.0 * 8.407435417175293
Epoch 770, val loss: 0.7074926495552063
Epoch 780, training loss: 841.0941162109375 = 0.6714791059494019 + 100.0 * 8.404226303100586
Epoch 780, val loss: 0.7023628950119019
Epoch 790, training loss: 840.9730834960938 = 0.6658030152320862 + 100.0 * 8.403072357177734
Epoch 790, val loss: 0.6971600651741028
Epoch 800, training loss: 841.080322265625 = 0.6600813865661621 + 100.0 * 8.404202461242676
Epoch 800, val loss: 0.6918670535087585
Epoch 810, training loss: 840.7753295898438 = 0.654370903968811 + 100.0 * 8.401209831237793
Epoch 810, val loss: 0.6867393255233765
Epoch 820, training loss: 840.6177978515625 = 0.6487745642662048 + 100.0 * 8.399689674377441
Epoch 820, val loss: 0.6815678477287292
Epoch 830, training loss: 840.5067749023438 = 0.6431986689567566 + 100.0 * 8.398635864257812
Epoch 830, val loss: 0.6764094233512878
Epoch 840, training loss: 840.3948364257812 = 0.6376727819442749 + 100.0 * 8.397571563720703
Epoch 840, val loss: 0.6713539958000183
Epoch 850, training loss: 840.3069458007812 = 0.6322135329246521 + 100.0 * 8.396747589111328
Epoch 850, val loss: 0.6663013696670532
Epoch 860, training loss: 840.578369140625 = 0.6267796158790588 + 100.0 * 8.399516105651855
Epoch 860, val loss: 0.6612465977668762
Epoch 870, training loss: 840.0890502929688 = 0.6213779449462891 + 100.0 * 8.394676208496094
Epoch 870, val loss: 0.6563278436660767
Epoch 880, training loss: 840.019287109375 = 0.616131067276001 + 100.0 * 8.394031524658203
Epoch 880, val loss: 0.6516054272651672
Epoch 890, training loss: 840.3434448242188 = 0.6109686493873596 + 100.0 * 8.397324562072754
Epoch 890, val loss: 0.6468461155891418
Epoch 900, training loss: 839.8121337890625 = 0.6059246063232422 + 100.0 * 8.392062187194824
Epoch 900, val loss: 0.6420761346817017
Epoch 910, training loss: 839.718994140625 = 0.6010217666625977 + 100.0 * 8.391180038452148
Epoch 910, val loss: 0.6375610828399658
Epoch 920, training loss: 839.5867919921875 = 0.5962164402008057 + 100.0 * 8.38990592956543
Epoch 920, val loss: 0.6331725716590881
Epoch 930, training loss: 839.4860229492188 = 0.5915671586990356 + 100.0 * 8.388944625854492
Epoch 930, val loss: 0.6289007663726807
Epoch 940, training loss: 839.4891967773438 = 0.5870444774627686 + 100.0 * 8.389021873474121
Epoch 940, val loss: 0.6247702240943909
Epoch 950, training loss: 839.4949951171875 = 0.5826000571250916 + 100.0 * 8.389123916625977
Epoch 950, val loss: 0.620549738407135
Epoch 960, training loss: 839.22021484375 = 0.5782959461212158 + 100.0 * 8.386419296264648
Epoch 960, val loss: 0.6166617274284363
Epoch 970, training loss: 839.1197509765625 = 0.5741746425628662 + 100.0 * 8.385456085205078
Epoch 970, val loss: 0.6128824949264526
Epoch 980, training loss: 839.0406494140625 = 0.5701892971992493 + 100.0 * 8.38470458984375
Epoch 980, val loss: 0.6092730164527893
Epoch 990, training loss: 838.9710083007812 = 0.5663562417030334 + 100.0 * 8.38404655456543
Epoch 990, val loss: 0.605798602104187
Epoch 1000, training loss: 839.5862426757812 = 0.5625772476196289 + 100.0 * 8.390236854553223
Epoch 1000, val loss: 0.6024278998374939
Epoch 1010, training loss: 838.8306274414062 = 0.5589187145233154 + 100.0 * 8.38271713256836
Epoch 1010, val loss: 0.5990004539489746
Epoch 1020, training loss: 838.7550659179688 = 0.55545973777771 + 100.0 * 8.381996154785156
Epoch 1020, val loss: 0.5958214402198792
Epoch 1030, training loss: 838.6406860351562 = 0.5520908832550049 + 100.0 * 8.38088607788086
Epoch 1030, val loss: 0.592803418636322
Epoch 1040, training loss: 838.5573120117188 = 0.5488640666007996 + 100.0 * 8.380084037780762
Epoch 1040, val loss: 0.5899797081947327
Epoch 1050, training loss: 838.5028686523438 = 0.5457692742347717 + 100.0 * 8.379570960998535
Epoch 1050, val loss: 0.5872030854225159
Epoch 1060, training loss: 838.5219116210938 = 0.54268479347229 + 100.0 * 8.379792213439941
Epoch 1060, val loss: 0.5843585729598999
Epoch 1070, training loss: 838.4943237304688 = 0.5397053360939026 + 100.0 * 8.379546165466309
Epoch 1070, val loss: 0.5817813277244568
Epoch 1080, training loss: 838.3057861328125 = 0.5368971824645996 + 100.0 * 8.37768840789795
Epoch 1080, val loss: 0.57929527759552
Epoch 1090, training loss: 838.1704711914062 = 0.534183144569397 + 100.0 * 8.376362800598145
Epoch 1090, val loss: 0.5768396854400635
Epoch 1100, training loss: 838.069580078125 = 0.5315653681755066 + 100.0 * 8.375380516052246
Epoch 1100, val loss: 0.5745528936386108
Epoch 1110, training loss: 837.9981689453125 = 0.5290331840515137 + 100.0 * 8.374691009521484
Epoch 1110, val loss: 0.5723240971565247
Epoch 1120, training loss: 838.4998779296875 = 0.5265643000602722 + 100.0 * 8.379733085632324
Epoch 1120, val loss: 0.5701557397842407
Epoch 1130, training loss: 837.8622436523438 = 0.52408367395401 + 100.0 * 8.373381614685059
Epoch 1130, val loss: 0.5680027008056641
Epoch 1140, training loss: 837.7722778320312 = 0.5217615962028503 + 100.0 * 8.372505187988281
Epoch 1140, val loss: 0.5659515857696533
Epoch 1150, training loss: 837.6863403320312 = 0.519497811794281 + 100.0 * 8.371668815612793
Epoch 1150, val loss: 0.563951313495636
Epoch 1160, training loss: 837.723876953125 = 0.5173038244247437 + 100.0 * 8.372065544128418
Epoch 1160, val loss: 0.5620304942131042
Epoch 1170, training loss: 837.5128173828125 = 0.5151188969612122 + 100.0 * 8.369976997375488
Epoch 1170, val loss: 0.5601974725723267
Epoch 1180, training loss: 837.4326171875 = 0.5130157470703125 + 100.0 * 8.369195938110352
Epoch 1180, val loss: 0.5584035515785217
Epoch 1190, training loss: 837.3987426757812 = 0.5109906792640686 + 100.0 * 8.368877410888672
Epoch 1190, val loss: 0.556631326675415
Epoch 1200, training loss: 837.5037231445312 = 0.5089913010597229 + 100.0 * 8.36994743347168
Epoch 1200, val loss: 0.5549012422561646
Epoch 1210, training loss: 837.2040405273438 = 0.5070074796676636 + 100.0 * 8.36697006225586
Epoch 1210, val loss: 0.553305447101593
Epoch 1220, training loss: 837.1511840820312 = 0.505122721195221 + 100.0 * 8.366460800170898
Epoch 1220, val loss: 0.551696240901947
Epoch 1230, training loss: 837.0772705078125 = 0.503288745880127 + 100.0 * 8.365739822387695
Epoch 1230, val loss: 0.5501092672348022
Epoch 1240, training loss: 837.0476684570312 = 0.5014832019805908 + 100.0 * 8.365462303161621
Epoch 1240, val loss: 0.5486092567443848
Epoch 1250, training loss: 837.3414916992188 = 0.4996959865093231 + 100.0 * 8.368417739868164
Epoch 1250, val loss: 0.5470455884933472
Epoch 1260, training loss: 836.9307861328125 = 0.49786999821662903 + 100.0 * 8.36432933807373
Epoch 1260, val loss: 0.5456514954566956
Epoch 1270, training loss: 836.856689453125 = 0.49614182114601135 + 100.0 * 8.363605499267578
Epoch 1270, val loss: 0.5442565679550171
Epoch 1280, training loss: 836.8146362304688 = 0.494462251663208 + 100.0 * 8.363202095031738
Epoch 1280, val loss: 0.5427737236022949
Epoch 1290, training loss: 836.8138427734375 = 0.4927844703197479 + 100.0 * 8.363210678100586
Epoch 1290, val loss: 0.5414843559265137
Epoch 1300, training loss: 836.6476440429688 = 0.4911547601222992 + 100.0 * 8.361564636230469
Epoch 1300, val loss: 0.540080726146698
Epoch 1310, training loss: 836.6055908203125 = 0.48954951763153076 + 100.0 * 8.361160278320312
Epoch 1310, val loss: 0.5387552976608276
Epoch 1320, training loss: 836.6801147460938 = 0.4879762530326843 + 100.0 * 8.361921310424805
Epoch 1320, val loss: 0.5374168157577515
Epoch 1330, training loss: 836.5891723632812 = 0.486388623714447 + 100.0 * 8.361027717590332
Epoch 1330, val loss: 0.5361856818199158
Epoch 1340, training loss: 836.4696044921875 = 0.4847980737686157 + 100.0 * 8.359848022460938
Epoch 1340, val loss: 0.5349065065383911
Epoch 1350, training loss: 836.3812255859375 = 0.48326221108436584 + 100.0 * 8.358979225158691
Epoch 1350, val loss: 0.5337079167366028
Epoch 1360, training loss: 836.3247680664062 = 0.48177817463874817 + 100.0 * 8.358429908752441
Epoch 1360, val loss: 0.5324759483337402
Epoch 1370, training loss: 836.2738647460938 = 0.48031628131866455 + 100.0 * 8.357934951782227
Epoch 1370, val loss: 0.5313356518745422
Epoch 1380, training loss: 836.5438232421875 = 0.47888296842575073 + 100.0 * 8.360649108886719
Epoch 1380, val loss: 0.5301202535629272
Epoch 1390, training loss: 836.4847412109375 = 0.4773913323879242 + 100.0 * 8.36007308959961
Epoch 1390, val loss: 0.5290305018424988
Epoch 1400, training loss: 836.1452026367188 = 0.47595569491386414 + 100.0 * 8.35669231414795
Epoch 1400, val loss: 0.5279076099395752
Epoch 1410, training loss: 836.0695190429688 = 0.474577397108078 + 100.0 * 8.355949401855469
Epoch 1410, val loss: 0.526827335357666
Epoch 1420, training loss: 836.0270385742188 = 0.4732213318347931 + 100.0 * 8.355538368225098
Epoch 1420, val loss: 0.5257918238639832
Epoch 1430, training loss: 835.9868774414062 = 0.4718939960002899 + 100.0 * 8.355149269104004
Epoch 1430, val loss: 0.524741530418396
Epoch 1440, training loss: 836.582763671875 = 0.470563143491745 + 100.0 * 8.361122131347656
Epoch 1440, val loss: 0.5236936807632446
Epoch 1450, training loss: 835.9036865234375 = 0.469169557094574 + 100.0 * 8.354345321655273
Epoch 1450, val loss: 0.5226911306381226
Epoch 1460, training loss: 835.8643798828125 = 0.46786779165267944 + 100.0 * 8.353964805603027
Epoch 1460, val loss: 0.5216702222824097
Epoch 1470, training loss: 835.7874755859375 = 0.4665886461734772 + 100.0 * 8.353208541870117
Epoch 1470, val loss: 0.5207207798957825
Epoch 1480, training loss: 835.7250366210938 = 0.4653199017047882 + 100.0 * 8.3525972366333
Epoch 1480, val loss: 0.5197767019271851
Epoch 1490, training loss: 835.8642578125 = 0.46405351161956787 + 100.0 * 8.354001998901367
Epoch 1490, val loss: 0.5188780426979065
Epoch 1500, training loss: 835.6380004882812 = 0.46276429295539856 + 100.0 * 8.351752281188965
Epoch 1500, val loss: 0.5178431272506714
Epoch 1510, training loss: 835.672119140625 = 0.4614987075328827 + 100.0 * 8.352106094360352
Epoch 1510, val loss: 0.5169323086738586
Epoch 1520, training loss: 835.7628784179688 = 0.46025002002716064 + 100.0 * 8.353026390075684
Epoch 1520, val loss: 0.5159782767295837
Epoch 1530, training loss: 835.5489501953125 = 0.4590022563934326 + 100.0 * 8.350899696350098
Epoch 1530, val loss: 0.5150282382965088
Epoch 1540, training loss: 835.4536743164062 = 0.45779499411582947 + 100.0 * 8.349958419799805
Epoch 1540, val loss: 0.5141631364822388
Epoch 1550, training loss: 835.4285278320312 = 0.4565936028957367 + 100.0 * 8.349719047546387
Epoch 1550, val loss: 0.5133065581321716
Epoch 1560, training loss: 835.4589233398438 = 0.4553980231285095 + 100.0 * 8.350035667419434
Epoch 1560, val loss: 0.5124398469924927
Epoch 1570, training loss: 835.49560546875 = 0.4541880190372467 + 100.0 * 8.350414276123047
Epoch 1570, val loss: 0.5115517973899841
Epoch 1580, training loss: 835.346435546875 = 0.45298904180526733 + 100.0 * 8.348934173583984
Epoch 1580, val loss: 0.5106338858604431
Epoch 1590, training loss: 835.2809448242188 = 0.4518090784549713 + 100.0 * 8.348291397094727
Epoch 1590, val loss: 0.5097320675849915
Epoch 1600, training loss: 835.5415649414062 = 0.45064830780029297 + 100.0 * 8.350909233093262
Epoch 1600, val loss: 0.5087522864341736
Epoch 1610, training loss: 835.304931640625 = 0.449421226978302 + 100.0 * 8.348555564880371
Epoch 1610, val loss: 0.5081679224967957
Epoch 1620, training loss: 835.200927734375 = 0.4482589662075043 + 100.0 * 8.347526550292969
Epoch 1620, val loss: 0.5072339773178101
Epoch 1630, training loss: 835.4110717773438 = 0.4471028745174408 + 100.0 * 8.349639892578125
Epoch 1630, val loss: 0.5063581466674805
Epoch 1640, training loss: 835.1427612304688 = 0.4459034204483032 + 100.0 * 8.346968650817871
Epoch 1640, val loss: 0.5056337714195251
Epoch 1650, training loss: 835.094970703125 = 0.44476354122161865 + 100.0 * 8.346502304077148
Epoch 1650, val loss: 0.5047810077667236
Epoch 1660, training loss: 835.0507202148438 = 0.443615585565567 + 100.0 * 8.346071243286133
Epoch 1660, val loss: 0.5040092468261719
Epoch 1670, training loss: 835.0159912109375 = 0.4424867331981659 + 100.0 * 8.345734596252441
Epoch 1670, val loss: 0.5032668709754944
Epoch 1680, training loss: 835.2667846679688 = 0.44134947657585144 + 100.0 * 8.348254203796387
Epoch 1680, val loss: 0.5025699138641357
Epoch 1690, training loss: 835.0564575195312 = 0.44018396735191345 + 100.0 * 8.346162796020508
Epoch 1690, val loss: 0.5015583038330078
Epoch 1700, training loss: 835.0279541015625 = 0.43902280926704407 + 100.0 * 8.3458890914917
Epoch 1700, val loss: 0.5008821487426758
Epoch 1710, training loss: 834.910888671875 = 0.437895268201828 + 100.0 * 8.344730377197266
Epoch 1710, val loss: 0.50008225440979
Epoch 1720, training loss: 834.885009765625 = 0.4367873966693878 + 100.0 * 8.344482421875
Epoch 1720, val loss: 0.49931952357292175
Epoch 1730, training loss: 834.879638671875 = 0.4356788694858551 + 100.0 * 8.344439506530762
Epoch 1730, val loss: 0.49861443042755127
Epoch 1740, training loss: 835.2951049804688 = 0.43454286456108093 + 100.0 * 8.348605155944824
Epoch 1740, val loss: 0.4978988468647003
Epoch 1750, training loss: 834.8330078125 = 0.4334176778793335 + 100.0 * 8.343996047973633
Epoch 1750, val loss: 0.4971092939376831
Epoch 1760, training loss: 834.7868041992188 = 0.43230870366096497 + 100.0 * 8.343544960021973
Epoch 1760, val loss: 0.49635857343673706
Epoch 1770, training loss: 834.7531127929688 = 0.4312095642089844 + 100.0 * 8.343218803405762
Epoch 1770, val loss: 0.49568188190460205
Epoch 1780, training loss: 834.7274169921875 = 0.43013790249824524 + 100.0 * 8.342972755432129
Epoch 1780, val loss: 0.49495401978492737
Epoch 1790, training loss: 835.0162963867188 = 0.42905938625335693 + 100.0 * 8.345871925354004
Epoch 1790, val loss: 0.4942125082015991
Epoch 1800, training loss: 834.7689819335938 = 0.4279380440711975 + 100.0 * 8.34341049194336
Epoch 1800, val loss: 0.4936670958995819
Epoch 1810, training loss: 834.6921997070312 = 0.42686235904693604 + 100.0 * 8.342653274536133
Epoch 1810, val loss: 0.4928654432296753
Epoch 1820, training loss: 834.626708984375 = 0.4257863461971283 + 100.0 * 8.342009544372559
Epoch 1820, val loss: 0.4922492802143097
Epoch 1830, training loss: 834.60595703125 = 0.4247281551361084 + 100.0 * 8.341812133789062
Epoch 1830, val loss: 0.49153995513916016
Epoch 1840, training loss: 835.1004028320312 = 0.42366889119148254 + 100.0 * 8.34676742553711
Epoch 1840, val loss: 0.4908072054386139
Epoch 1850, training loss: 834.7296752929688 = 0.42255935072898865 + 100.0 * 8.343070983886719
Epoch 1850, val loss: 0.4902973473072052
Epoch 1860, training loss: 834.5725708007812 = 0.4214983582496643 + 100.0 * 8.341510772705078
Epoch 1860, val loss: 0.4895329475402832
Epoch 1870, training loss: 834.4934692382812 = 0.42045190930366516 + 100.0 * 8.340729713439941
Epoch 1870, val loss: 0.4889288544654846
Epoch 1880, training loss: 834.515869140625 = 0.41941389441490173 + 100.0 * 8.340964317321777
Epoch 1880, val loss: 0.488299697637558
Epoch 1890, training loss: 834.7279052734375 = 0.4183577597141266 + 100.0 * 8.343095779418945
Epoch 1890, val loss: 0.48765361309051514
Epoch 1900, training loss: 834.4542846679688 = 0.4173012971878052 + 100.0 * 8.340370178222656
Epoch 1900, val loss: 0.48697808384895325
Epoch 1910, training loss: 834.3831787109375 = 0.416258305311203 + 100.0 * 8.339669227600098
Epoch 1910, val loss: 0.4863782227039337
Epoch 1920, training loss: 834.3534545898438 = 0.415227472782135 + 100.0 * 8.33938217163086
Epoch 1920, val loss: 0.4857412874698639
Epoch 1930, training loss: 834.3268432617188 = 0.41420087218284607 + 100.0 * 8.339126586914062
Epoch 1930, val loss: 0.4851539433002472
Epoch 1940, training loss: 834.6210327148438 = 0.4131603538990021 + 100.0 * 8.342079162597656
Epoch 1940, val loss: 0.48459506034851074
Epoch 1950, training loss: 834.4142456054688 = 0.41209936141967773 + 100.0 * 8.340021133422852
Epoch 1950, val loss: 0.4838528335094452
Epoch 1960, training loss: 834.3482666015625 = 0.4110403060913086 + 100.0 * 8.339372634887695
Epoch 1960, val loss: 0.48320451378822327
Epoch 1970, training loss: 834.22705078125 = 0.4100179374217987 + 100.0 * 8.338170051574707
Epoch 1970, val loss: 0.48255282640457153
Epoch 1980, training loss: 834.2296142578125 = 0.4090001583099365 + 100.0 * 8.33820629119873
Epoch 1980, val loss: 0.4819241762161255
Epoch 1990, training loss: 834.5247802734375 = 0.407982736825943 + 100.0 * 8.341168403625488
Epoch 1990, val loss: 0.48121634125709534
Epoch 2000, training loss: 834.1768188476562 = 0.40692609548568726 + 100.0 * 8.337698936462402
Epoch 2000, val loss: 0.4807755649089813
Epoch 2010, training loss: 834.117919921875 = 0.40590643882751465 + 100.0 * 8.337120056152344
Epoch 2010, val loss: 0.480050653219223
Epoch 2020, training loss: 834.1153564453125 = 0.40489518642425537 + 100.0 * 8.337104797363281
Epoch 2020, val loss: 0.4794889986515045
Epoch 2030, training loss: 834.4517822265625 = 0.40388789772987366 + 100.0 * 8.340478897094727
Epoch 2030, val loss: 0.47889482975006104
Epoch 2040, training loss: 834.1318359375 = 0.40284547209739685 + 100.0 * 8.337289810180664
Epoch 2040, val loss: 0.47827231884002686
Epoch 2050, training loss: 834.0861206054688 = 0.40182942152023315 + 100.0 * 8.33684253692627
Epoch 2050, val loss: 0.47771596908569336
Epoch 2060, training loss: 834.022216796875 = 0.40083128213882446 + 100.0 * 8.336214065551758
Epoch 2060, val loss: 0.4770981967449188
Epoch 2070, training loss: 833.9735717773438 = 0.39983975887298584 + 100.0 * 8.335737228393555
Epoch 2070, val loss: 0.47656166553497314
Epoch 2080, training loss: 833.940185546875 = 0.3988528549671173 + 100.0 * 8.335412979125977
Epoch 2080, val loss: 0.4759962856769562
Epoch 2090, training loss: 833.9649047851562 = 0.39785972237586975 + 100.0 * 8.335670471191406
Epoch 2090, val loss: 0.47546371817588806
Epoch 2100, training loss: 834.33837890625 = 0.39684608578681946 + 100.0 * 8.339415550231934
Epoch 2100, val loss: 0.4747004210948944
Epoch 2110, training loss: 834.0919799804688 = 0.39579740166664124 + 100.0 * 8.33696174621582
Epoch 2110, val loss: 0.4742913842201233
Epoch 2120, training loss: 833.8649291992188 = 0.39478981494903564 + 100.0 * 8.334701538085938
Epoch 2120, val loss: 0.47364526987075806
Epoch 2130, training loss: 833.8466186523438 = 0.3938019275665283 + 100.0 * 8.334527969360352
Epoch 2130, val loss: 0.4730050563812256
Epoch 2140, training loss: 833.8021240234375 = 0.3928241729736328 + 100.0 * 8.33409309387207
Epoch 2140, val loss: 0.4724523425102234
Epoch 2150, training loss: 833.844482421875 = 0.3918395936489105 + 100.0 * 8.334526062011719
Epoch 2150, val loss: 0.4719735085964203
Epoch 2160, training loss: 834.13037109375 = 0.39083269238471985 + 100.0 * 8.337395668029785
Epoch 2160, val loss: 0.4714502692222595
Epoch 2170, training loss: 833.84130859375 = 0.38983526825904846 + 100.0 * 8.334514617919922
Epoch 2170, val loss: 0.4706675112247467
Epoch 2180, training loss: 833.7323608398438 = 0.388836532831192 + 100.0 * 8.33343505859375
Epoch 2180, val loss: 0.47021007537841797
Epoch 2190, training loss: 833.6849365234375 = 0.3878636360168457 + 100.0 * 8.33297061920166
Epoch 2190, val loss: 0.46956831216812134
Epoch 2200, training loss: 833.7356567382812 = 0.386886864900589 + 100.0 * 8.333487510681152
Epoch 2200, val loss: 0.46904486417770386
Epoch 2210, training loss: 833.8291015625 = 0.3858846426010132 + 100.0 * 8.334432601928711
Epoch 2210, val loss: 0.4684482514858246
Epoch 2220, training loss: 833.6088256835938 = 0.3848852813243866 + 100.0 * 8.332239151000977
Epoch 2220, val loss: 0.46785932779312134
Epoch 2230, training loss: 833.6253662109375 = 0.3839016854763031 + 100.0 * 8.332414627075195
Epoch 2230, val loss: 0.4672919809818268
Epoch 2240, training loss: 833.5767822265625 = 0.3829270303249359 + 100.0 * 8.331938743591309
Epoch 2240, val loss: 0.46674156188964844
Epoch 2250, training loss: 834.1303100585938 = 0.38194602727890015 + 100.0 * 8.337483406066895
Epoch 2250, val loss: 0.4662313759326935
Epoch 2260, training loss: 833.8799438476562 = 0.3809131383895874 + 100.0 * 8.334990501403809
Epoch 2260, val loss: 0.46568015217781067
Epoch 2270, training loss: 833.5079345703125 = 0.3799167573451996 + 100.0 * 8.331279754638672
Epoch 2270, val loss: 0.4650241732597351
Epoch 2280, training loss: 833.50439453125 = 0.37893587350845337 + 100.0 * 8.331254959106445
Epoch 2280, val loss: 0.46451497077941895
Epoch 2290, training loss: 833.4495849609375 = 0.37797030806541443 + 100.0 * 8.330716133117676
Epoch 2290, val loss: 0.4640297591686249
Epoch 2300, training loss: 833.43212890625 = 0.3770073354244232 + 100.0 * 8.330551147460938
Epoch 2300, val loss: 0.4634794294834137
Epoch 2310, training loss: 833.9797973632812 = 0.3760468661785126 + 100.0 * 8.336037635803223
Epoch 2310, val loss: 0.4628007113933563
Epoch 2320, training loss: 833.6030883789062 = 0.37501877546310425 + 100.0 * 8.332281112670898
Epoch 2320, val loss: 0.46255189180374146
Epoch 2330, training loss: 833.4513549804688 = 0.3740423321723938 + 100.0 * 8.33077335357666
Epoch 2330, val loss: 0.46185383200645447
Epoch 2340, training loss: 833.353271484375 = 0.3730732798576355 + 100.0 * 8.329802513122559
Epoch 2340, val loss: 0.4613533914089203
Epoch 2350, training loss: 833.3132934570312 = 0.37211480736732483 + 100.0 * 8.329411506652832
Epoch 2350, val loss: 0.46089327335357666
Epoch 2360, training loss: 833.2862548828125 = 0.3711602985858917 + 100.0 * 8.329151153564453
Epoch 2360, val loss: 0.46038758754730225
Epoch 2370, training loss: 833.2706298828125 = 0.37020641565322876 + 100.0 * 8.329004287719727
Epoch 2370, val loss: 0.4599028527736664
Epoch 2380, training loss: 834.2993774414062 = 0.3692585527896881 + 100.0 * 8.339301109313965
Epoch 2380, val loss: 0.4592919647693634
Epoch 2390, training loss: 833.2518920898438 = 0.36821606755256653 + 100.0 * 8.328836441040039
Epoch 2390, val loss: 0.4588715434074402
Epoch 2400, training loss: 833.28076171875 = 0.3672468066215515 + 100.0 * 8.329134941101074
Epoch 2400, val loss: 0.45833390951156616
Epoch 2410, training loss: 833.2008666992188 = 0.36629518866539 + 100.0 * 8.328346252441406
Epoch 2410, val loss: 0.45794379711151123
Epoch 2420, training loss: 833.1461791992188 = 0.36535900831222534 + 100.0 * 8.327808380126953
Epoch 2420, val loss: 0.4575285017490387
Epoch 2430, training loss: 833.1234130859375 = 0.36442694067955017 + 100.0 * 8.327589988708496
Epoch 2430, val loss: 0.4570518732070923
Epoch 2440, training loss: 833.0969848632812 = 0.36348968744277954 + 100.0 * 8.327335357666016
Epoch 2440, val loss: 0.45657479763031006
Epoch 2450, training loss: 833.0899047851562 = 0.3625522553920746 + 100.0 * 8.32727336883545
Epoch 2450, val loss: 0.45614442229270935
Epoch 2460, training loss: 833.7225952148438 = 0.36161378026008606 + 100.0 * 8.333609580993652
Epoch 2460, val loss: 0.45560333132743835
Epoch 2470, training loss: 833.503662109375 = 0.3606222867965698 + 100.0 * 8.331430435180664
Epoch 2470, val loss: 0.4552323818206787
Epoch 2480, training loss: 833.1055297851562 = 0.3596605360507965 + 100.0 * 8.327458381652832
Epoch 2480, val loss: 0.45474866032600403
Epoch 2490, training loss: 833.1135864257812 = 0.3587203025817871 + 100.0 * 8.32754898071289
Epoch 2490, val loss: 0.454127699136734
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8310502283105022
0.8648844454104181
=== training gcn model ===
Epoch 0, training loss: 1059.3587646484375 = 1.1322588920593262 + 100.0 * 10.582265853881836
Epoch 0, val loss: 1.132469892501831
Epoch 10, training loss: 1059.301513671875 = 1.1262896060943604 + 100.0 * 10.581751823425293
Epoch 10, val loss: 1.1264419555664062
Epoch 20, training loss: 1059.0584716796875 = 1.1197508573532104 + 100.0 * 10.579387664794922
Epoch 20, val loss: 1.119813084602356
Epoch 30, training loss: 1057.984130859375 = 1.1123155355453491 + 100.0 * 10.568717956542969
Epoch 30, val loss: 1.1122639179229736
Epoch 40, training loss: 1053.962646484375 = 1.1039063930511475 + 100.0 * 10.528587341308594
Epoch 40, val loss: 1.1037529706954956
Epoch 50, training loss: 1043.306396484375 = 1.0948474407196045 + 100.0 * 10.422115325927734
Epoch 50, val loss: 1.0946959257125854
Epoch 60, training loss: 1021.9235229492188 = 1.0870776176452637 + 100.0 * 10.208364486694336
Epoch 60, val loss: 1.0870821475982666
Epoch 70, training loss: 985.96044921875 = 1.0793685913085938 + 100.0 * 9.848811149597168
Epoch 70, val loss: 1.0792632102966309
Epoch 80, training loss: 958.52392578125 = 1.0738232135772705 + 100.0 * 9.574501037597656
Epoch 80, val loss: 1.0741477012634277
Epoch 90, training loss: 947.2349853515625 = 1.0694129467010498 + 100.0 * 9.461655616760254
Epoch 90, val loss: 1.0699741840362549
Epoch 100, training loss: 939.7633666992188 = 1.0652121305465698 + 100.0 * 9.386981964111328
Epoch 100, val loss: 1.0659055709838867
Epoch 110, training loss: 930.6321411132812 = 1.0607342720031738 + 100.0 * 9.295714378356934
Epoch 110, val loss: 1.0615196228027344
Epoch 120, training loss: 921.0748291015625 = 1.0557775497436523 + 100.0 * 9.200190544128418
Epoch 120, val loss: 1.056654691696167
Epoch 130, training loss: 914.5831909179688 = 1.0509819984436035 + 100.0 * 9.135322570800781
Epoch 130, val loss: 1.0519824028015137
Epoch 140, training loss: 910.0755615234375 = 1.0467768907546997 + 100.0 * 9.090288162231445
Epoch 140, val loss: 1.047891616821289
Epoch 150, training loss: 903.1537475585938 = 1.0433241128921509 + 100.0 * 9.021103858947754
Epoch 150, val loss: 1.0445597171783447
Epoch 160, training loss: 893.1988525390625 = 1.0409144163131714 + 100.0 * 8.921579360961914
Epoch 160, val loss: 1.0423307418823242
Epoch 170, training loss: 884.544189453125 = 1.0394097566604614 + 100.0 * 8.835047721862793
Epoch 170, val loss: 1.0408965349197388
Epoch 180, training loss: 880.6182250976562 = 1.03736412525177 + 100.0 * 8.795808792114258
Epoch 180, val loss: 1.038730502128601
Epoch 190, training loss: 877.4282836914062 = 1.0341931581497192 + 100.0 * 8.763940811157227
Epoch 190, val loss: 1.0354636907577515
Epoch 200, training loss: 874.7463989257812 = 1.0309855937957764 + 100.0 * 8.737154006958008
Epoch 200, val loss: 1.03230881690979
Epoch 210, training loss: 873.2235717773438 = 1.0280345678329468 + 100.0 * 8.721955299377441
Epoch 210, val loss: 1.0294842720031738
Epoch 220, training loss: 871.6246337890625 = 1.0249178409576416 + 100.0 * 8.705997467041016
Epoch 220, val loss: 1.0264523029327393
Epoch 230, training loss: 870.0819091796875 = 1.0215792655944824 + 100.0 * 8.690603256225586
Epoch 230, val loss: 1.0232654809951782
Epoch 240, training loss: 868.2216796875 = 1.0182331800460815 + 100.0 * 8.67203426361084
Epoch 240, val loss: 1.020076036453247
Epoch 250, training loss: 866.1134033203125 = 1.0149860382080078 + 100.0 * 8.650983810424805
Epoch 250, val loss: 1.0169869661331177
Epoch 260, training loss: 864.2581787109375 = 1.0116384029388428 + 100.0 * 8.632465362548828
Epoch 260, val loss: 1.013800024986267
Epoch 270, training loss: 862.49951171875 = 1.007958173751831 + 100.0 * 8.61491584777832
Epoch 270, val loss: 1.0102707147598267
Epoch 280, training loss: 861.0408935546875 = 1.0040361881256104 + 100.0 * 8.60036849975586
Epoch 280, val loss: 1.006504774093628
Epoch 290, training loss: 859.8477172851562 = 0.9998330473899841 + 100.0 * 8.588479042053223
Epoch 290, val loss: 1.002461314201355
Epoch 300, training loss: 858.531982421875 = 0.9953134655952454 + 100.0 * 8.575366973876953
Epoch 300, val loss: 0.9981173872947693
Epoch 310, training loss: 857.4334716796875 = 0.9904893040657043 + 100.0 * 8.564430236816406
Epoch 310, val loss: 0.9934903979301453
Epoch 320, training loss: 856.4276733398438 = 0.985365629196167 + 100.0 * 8.554423332214355
Epoch 320, val loss: 0.9885973334312439
Epoch 330, training loss: 855.7168579101562 = 0.9800039529800415 + 100.0 * 8.547369003295898
Epoch 330, val loss: 0.9834755659103394
Epoch 340, training loss: 854.8035888671875 = 0.974378228187561 + 100.0 * 8.538291931152344
Epoch 340, val loss: 0.9781255125999451
Epoch 350, training loss: 853.9157104492188 = 0.9685683846473694 + 100.0 * 8.529471397399902
Epoch 350, val loss: 0.972615122795105
Epoch 360, training loss: 853.175537109375 = 0.9625344276428223 + 100.0 * 8.522130012512207
Epoch 360, val loss: 0.9668972492218018
Epoch 370, training loss: 852.445556640625 = 0.956135630607605 + 100.0 * 8.514894485473633
Epoch 370, val loss: 0.9607927799224854
Epoch 380, training loss: 851.7105712890625 = 0.9495211839675903 + 100.0 * 8.507610321044922
Epoch 380, val loss: 0.9545711874961853
Epoch 390, training loss: 851.0020141601562 = 0.9426624178886414 + 100.0 * 8.500593185424805
Epoch 390, val loss: 0.9480561017990112
Epoch 400, training loss: 850.4027709960938 = 0.9355097413063049 + 100.0 * 8.494672775268555
Epoch 400, val loss: 0.9412654638290405
Epoch 410, training loss: 850.0519409179688 = 0.9280832409858704 + 100.0 * 8.491238594055176
Epoch 410, val loss: 0.934267520904541
Epoch 420, training loss: 849.4376831054688 = 0.9203341603279114 + 100.0 * 8.485173225402832
Epoch 420, val loss: 0.9269379377365112
Epoch 430, training loss: 848.9691772460938 = 0.9124937653541565 + 100.0 * 8.48056697845459
Epoch 430, val loss: 0.9195359349250793
Epoch 440, training loss: 848.5313720703125 = 0.9044842720031738 + 100.0 * 8.476268768310547
Epoch 440, val loss: 0.9119987487792969
Epoch 450, training loss: 848.131103515625 = 0.8963062763214111 + 100.0 * 8.4723482131958
Epoch 450, val loss: 0.9043096303939819
Epoch 460, training loss: 848.1534423828125 = 0.8879675269126892 + 100.0 * 8.472655296325684
Epoch 460, val loss: 0.8964749574661255
Epoch 470, training loss: 847.5278930664062 = 0.8794161677360535 + 100.0 * 8.466485023498535
Epoch 470, val loss: 0.8884304165840149
Epoch 480, training loss: 847.2373657226562 = 0.8707639575004578 + 100.0 * 8.463665962219238
Epoch 480, val loss: 0.8802997469902039
Epoch 490, training loss: 846.923828125 = 0.86191725730896 + 100.0 * 8.46061897277832
Epoch 490, val loss: 0.8720036745071411
Epoch 500, training loss: 846.6890258789062 = 0.8529955148696899 + 100.0 * 8.45836067199707
Epoch 500, val loss: 0.8636581301689148
Epoch 510, training loss: 846.5196533203125 = 0.8439958691596985 + 100.0 * 8.456756591796875
Epoch 510, val loss: 0.8551962971687317
Epoch 520, training loss: 846.35693359375 = 0.8348540663719177 + 100.0 * 8.455221176147461
Epoch 520, val loss: 0.8466383814811707
Epoch 530, training loss: 845.9766845703125 = 0.8257712721824646 + 100.0 * 8.451509475708008
Epoch 530, val loss: 0.8381630182266235
Epoch 540, training loss: 845.6669921875 = 0.8166932463645935 + 100.0 * 8.448502540588379
Epoch 540, val loss: 0.829666793346405
Epoch 550, training loss: 845.508056640625 = 0.8076171875 + 100.0 * 8.447004318237305
Epoch 550, val loss: 0.8211771845817566
Epoch 560, training loss: 845.3508911132812 = 0.7984579801559448 + 100.0 * 8.445524215698242
Epoch 560, val loss: 0.8126201033592224
Epoch 570, training loss: 844.8388061523438 = 0.7894054651260376 + 100.0 * 8.4404935836792
Epoch 570, val loss: 0.8041917681694031
Epoch 580, training loss: 844.5693969726562 = 0.7804237604141235 + 100.0 * 8.43789005279541
Epoch 580, val loss: 0.7958352565765381
Epoch 590, training loss: 844.44384765625 = 0.7714766263961792 + 100.0 * 8.436723709106445
Epoch 590, val loss: 0.7875180840492249
Epoch 600, training loss: 844.092041015625 = 0.7625177502632141 + 100.0 * 8.433295249938965
Epoch 600, val loss: 0.779128909111023
Epoch 610, training loss: 843.7251586914062 = 0.7536390423774719 + 100.0 * 8.429715156555176
Epoch 610, val loss: 0.7708889842033386
Epoch 620, training loss: 843.7220458984375 = 0.7448163628578186 + 100.0 * 8.42977237701416
Epoch 620, val loss: 0.7626324892044067
Epoch 630, training loss: 843.2340698242188 = 0.7359520792961121 + 100.0 * 8.424981117248535
Epoch 630, val loss: 0.7543924450874329
Epoch 640, training loss: 842.90771484375 = 0.7272650003433228 + 100.0 * 8.421804428100586
Epoch 640, val loss: 0.7463201284408569
Epoch 650, training loss: 842.832763671875 = 0.7186487317085266 + 100.0 * 8.421141624450684
Epoch 650, val loss: 0.7382245659828186
Epoch 660, training loss: 842.5796508789062 = 0.7100176215171814 + 100.0 * 8.418696403503418
Epoch 660, val loss: 0.7302318811416626
Epoch 670, training loss: 842.2471923828125 = 0.7015880942344666 + 100.0 * 8.41545581817627
Epoch 670, val loss: 0.7223817706108093
Epoch 680, training loss: 842.12060546875 = 0.6933047771453857 + 100.0 * 8.414273262023926
Epoch 680, val loss: 0.7146317958831787
Epoch 690, training loss: 842.0783081054688 = 0.6850963234901428 + 100.0 * 8.413931846618652
Epoch 690, val loss: 0.7069598436355591
Epoch 700, training loss: 841.765625 = 0.6771051287651062 + 100.0 * 8.410884857177734
Epoch 700, val loss: 0.6995605230331421
Epoch 710, training loss: 841.5524291992188 = 0.6694005727767944 + 100.0 * 8.408830642700195
Epoch 710, val loss: 0.6923712491989136
Epoch 720, training loss: 841.6608276367188 = 0.6618320345878601 + 100.0 * 8.409990310668945
Epoch 720, val loss: 0.6853287816047668
Epoch 730, training loss: 841.4155883789062 = 0.6543958783149719 + 100.0 * 8.407611846923828
Epoch 730, val loss: 0.6783766150474548
Epoch 740, training loss: 841.0568237304688 = 0.6473283767700195 + 100.0 * 8.404094696044922
Epoch 740, val loss: 0.6718506217002869
Epoch 750, training loss: 840.9229736328125 = 0.6404816508293152 + 100.0 * 8.402824401855469
Epoch 750, val loss: 0.6654942631721497
Epoch 760, training loss: 841.1546020507812 = 0.6338309049606323 + 100.0 * 8.405207633972168
Epoch 760, val loss: 0.6592844128608704
Epoch 770, training loss: 840.8668212890625 = 0.627342700958252 + 100.0 * 8.402395248413086
Epoch 770, val loss: 0.6532924175262451
Epoch 780, training loss: 840.553466796875 = 0.6212210655212402 + 100.0 * 8.399322509765625
Epoch 780, val loss: 0.6477294564247131
Epoch 790, training loss: 840.3096313476562 = 0.6153237223625183 + 100.0 * 8.396943092346191
Epoch 790, val loss: 0.6422684788703918
Epoch 800, training loss: 840.1629028320312 = 0.6096395254135132 + 100.0 * 8.395532608032227
Epoch 800, val loss: 0.6370317339897156
Epoch 810, training loss: 840.0465087890625 = 0.6042119860649109 + 100.0 * 8.394423484802246
Epoch 810, val loss: 0.6320777535438538
Epoch 820, training loss: 840.0648193359375 = 0.5989211797714233 + 100.0 * 8.394659042358398
Epoch 820, val loss: 0.6272580623626709
Epoch 830, training loss: 839.8534545898438 = 0.5938279032707214 + 100.0 * 8.392596244812012
Epoch 830, val loss: 0.6226480603218079
Epoch 840, training loss: 839.65087890625 = 0.5890528559684753 + 100.0 * 8.390618324279785
Epoch 840, val loss: 0.6183740496635437
Epoch 850, training loss: 839.5110473632812 = 0.5844694375991821 + 100.0 * 8.389266014099121
Epoch 850, val loss: 0.6142210364341736
Epoch 860, training loss: 839.397216796875 = 0.5800782442092896 + 100.0 * 8.388171195983887
Epoch 860, val loss: 0.6102915406227112
Epoch 870, training loss: 839.8446044921875 = 0.5758270025253296 + 100.0 * 8.392687797546387
Epoch 870, val loss: 0.6064807176589966
Epoch 880, training loss: 839.267822265625 = 0.5716755390167236 + 100.0 * 8.386961936950684
Epoch 880, val loss: 0.6028254628181458
Epoch 890, training loss: 839.049072265625 = 0.5678393840789795 + 100.0 * 8.384812355041504
Epoch 890, val loss: 0.5995054244995117
Epoch 900, training loss: 838.96630859375 = 0.5641639232635498 + 100.0 * 8.384021759033203
Epoch 900, val loss: 0.5962392091751099
Epoch 910, training loss: 839.1107788085938 = 0.5606132745742798 + 100.0 * 8.385501861572266
Epoch 910, val loss: 0.5931479930877686
Epoch 920, training loss: 838.7805786132812 = 0.557190477848053 + 100.0 * 8.382233619689941
Epoch 920, val loss: 0.5902007818222046
Epoch 930, training loss: 838.6439819335938 = 0.5539593696594238 + 100.0 * 8.380900382995605
Epoch 930, val loss: 0.5874620676040649
Epoch 940, training loss: 838.6372680664062 = 0.550862729549408 + 100.0 * 8.380864143371582
Epoch 940, val loss: 0.5848151445388794
Epoch 950, training loss: 838.4759521484375 = 0.5478276014328003 + 100.0 * 8.379281044006348
Epoch 950, val loss: 0.5821954607963562
Epoch 960, training loss: 838.4381713867188 = 0.5449734926223755 + 100.0 * 8.378931999206543
Epoch 960, val loss: 0.5799144506454468
Epoch 970, training loss: 838.3240356445312 = 0.5422616004943848 + 100.0 * 8.37781810760498
Epoch 970, val loss: 0.5776110887527466
Epoch 980, training loss: 838.3822021484375 = 0.5396570563316345 + 100.0 * 8.378425598144531
Epoch 980, val loss: 0.5754713416099548
Epoch 990, training loss: 838.2432250976562 = 0.5371177792549133 + 100.0 * 8.377060890197754
Epoch 990, val loss: 0.5734333395957947
Epoch 1000, training loss: 838.24365234375 = 0.5347285270690918 + 100.0 * 8.377089500427246
Epoch 1000, val loss: 0.571496307849884
Epoch 1010, training loss: 838.0160522460938 = 0.5323723554611206 + 100.0 * 8.374836921691895
Epoch 1010, val loss: 0.5695827007293701
Epoch 1020, training loss: 837.9298095703125 = 0.5301568508148193 + 100.0 * 8.37399673461914
Epoch 1020, val loss: 0.5678747296333313
Epoch 1030, training loss: 837.8596801757812 = 0.5280203819274902 + 100.0 * 8.373316764831543
Epoch 1030, val loss: 0.5662002563476562
Epoch 1040, training loss: 837.767333984375 = 0.5259711742401123 + 100.0 * 8.372413635253906
Epoch 1040, val loss: 0.5646102428436279
Epoch 1050, training loss: 837.852783203125 = 0.5239974856376648 + 100.0 * 8.37328815460205
Epoch 1050, val loss: 0.5630947351455688
Epoch 1060, training loss: 837.6497192382812 = 0.5220272541046143 + 100.0 * 8.37127685546875
Epoch 1060, val loss: 0.5615935325622559
Epoch 1070, training loss: 837.7021484375 = 0.5201976299285889 + 100.0 * 8.371819496154785
Epoch 1070, val loss: 0.5602895021438599
Epoch 1080, training loss: 837.6223754882812 = 0.5184244513511658 + 100.0 * 8.371039390563965
Epoch 1080, val loss: 0.5589035749435425
Epoch 1090, training loss: 837.61181640625 = 0.5167160630226135 + 100.0 * 8.370950698852539
Epoch 1090, val loss: 0.5576156973838806
Epoch 1100, training loss: 837.4190063476562 = 0.5150623917579651 + 100.0 * 8.369039535522461
Epoch 1100, val loss: 0.5564587116241455
Epoch 1110, training loss: 837.3446044921875 = 0.5135049223899841 + 100.0 * 8.368310928344727
Epoch 1110, val loss: 0.5553471446037292
Epoch 1120, training loss: 837.2649536132812 = 0.5119884610176086 + 100.0 * 8.36752986907959
Epoch 1120, val loss: 0.5542716383934021
Epoch 1130, training loss: 837.311767578125 = 0.5105138421058655 + 100.0 * 8.368012428283691
Epoch 1130, val loss: 0.5532416105270386
Epoch 1140, training loss: 837.3721923828125 = 0.509056031703949 + 100.0 * 8.368631362915039
Epoch 1140, val loss: 0.5521395206451416
Epoch 1150, training loss: 837.1024780273438 = 0.5076373815536499 + 100.0 * 8.365948677062988
Epoch 1150, val loss: 0.5512856245040894
Epoch 1160, training loss: 837.0133666992188 = 0.5063250064849854 + 100.0 * 8.365070343017578
Epoch 1160, val loss: 0.550324559211731
Epoch 1170, training loss: 836.926513671875 = 0.5050199627876282 + 100.0 * 8.364214897155762
Epoch 1170, val loss: 0.5494336485862732
Epoch 1180, training loss: 837.1151123046875 = 0.5037497878074646 + 100.0 * 8.366113662719727
Epoch 1180, val loss: 0.548597514629364
Epoch 1190, training loss: 836.815185546875 = 0.5024663805961609 + 100.0 * 8.363127708435059
Epoch 1190, val loss: 0.5477434992790222
Epoch 1200, training loss: 836.7503662109375 = 0.5012627243995667 + 100.0 * 8.3624906539917
Epoch 1200, val loss: 0.5469503998756409
Epoch 1210, training loss: 836.847900390625 = 0.5000836253166199 + 100.0 * 8.363478660583496
Epoch 1210, val loss: 0.54613196849823
Epoch 1220, training loss: 836.7898559570312 = 0.49889296293258667 + 100.0 * 8.362909317016602
Epoch 1220, val loss: 0.5453367233276367
Epoch 1230, training loss: 836.6197509765625 = 0.4977603852748871 + 100.0 * 8.361220359802246
Epoch 1230, val loss: 0.5446673631668091
Epoch 1240, training loss: 836.5045166015625 = 0.4966588616371155 + 100.0 * 8.360078811645508
Epoch 1240, val loss: 0.543929398059845
Epoch 1250, training loss: 836.4691772460938 = 0.49558040499687195 + 100.0 * 8.359735488891602
Epoch 1250, val loss: 0.5432605147361755
Epoch 1260, training loss: 836.6576538085938 = 0.49451944231987 + 100.0 * 8.361631393432617
Epoch 1260, val loss: 0.5426256060600281
Epoch 1270, training loss: 836.37451171875 = 0.4934481382369995 + 100.0 * 8.358810424804688
Epoch 1270, val loss: 0.5418944358825684
Epoch 1280, training loss: 836.4175415039062 = 0.49244260787963867 + 100.0 * 8.359251022338867
Epoch 1280, val loss: 0.5413171648979187
Epoch 1290, training loss: 836.3424072265625 = 0.49142026901245117 + 100.0 * 8.35851001739502
Epoch 1290, val loss: 0.5406544208526611
Epoch 1300, training loss: 836.3650512695312 = 0.49043694138526917 + 100.0 * 8.358746528625488
Epoch 1300, val loss: 0.5400592684745789
Epoch 1310, training loss: 836.1863403320312 = 0.4894568920135498 + 100.0 * 8.356968879699707
Epoch 1310, val loss: 0.53941410779953
Epoch 1320, training loss: 836.1253051757812 = 0.4885278642177582 + 100.0 * 8.356368064880371
Epoch 1320, val loss: 0.5388925671577454
Epoch 1330, training loss: 836.073974609375 = 0.4876040816307068 + 100.0 * 8.355863571166992
Epoch 1330, val loss: 0.5382847785949707
Epoch 1340, training loss: 836.1453247070312 = 0.48669666051864624 + 100.0 * 8.356586456298828
Epoch 1340, val loss: 0.5377267599105835
Epoch 1350, training loss: 836.0349731445312 = 0.4857783317565918 + 100.0 * 8.355491638183594
Epoch 1350, val loss: 0.5372342467308044
Epoch 1360, training loss: 836.1807861328125 = 0.4848828911781311 + 100.0 * 8.356959342956543
Epoch 1360, val loss: 0.5367386341094971
Epoch 1370, training loss: 836.0245971679688 = 0.48397961258888245 + 100.0 * 8.355405807495117
Epoch 1370, val loss: 0.5361297726631165
Epoch 1380, training loss: 835.896240234375 = 0.48309826850891113 + 100.0 * 8.354131698608398
Epoch 1380, val loss: 0.5356395840644836
Epoch 1390, training loss: 835.8421020507812 = 0.4822678864002228 + 100.0 * 8.353598594665527
Epoch 1390, val loss: 0.5351508855819702
Epoch 1400, training loss: 835.7711181640625 = 0.48145249485969543 + 100.0 * 8.352896690368652
Epoch 1400, val loss: 0.5346701741218567
Epoch 1410, training loss: 835.7424926757812 = 0.48065733909606934 + 100.0 * 8.352618217468262
Epoch 1410, val loss: 0.5342370271682739
Epoch 1420, training loss: 836.056884765625 = 0.47985684871673584 + 100.0 * 8.355770111083984
Epoch 1420, val loss: 0.5337043404579163
Epoch 1430, training loss: 835.7764282226562 = 0.4790359139442444 + 100.0 * 8.352973937988281
Epoch 1430, val loss: 0.5332754254341125
Epoch 1440, training loss: 835.633544921875 = 0.4782751798629761 + 100.0 * 8.351552963256836
Epoch 1440, val loss: 0.5328396558761597
Epoch 1450, training loss: 835.8182373046875 = 0.47751161456108093 + 100.0 * 8.35340690612793
Epoch 1450, val loss: 0.532357394695282
Epoch 1460, training loss: 835.5745849609375 = 0.47669005393981934 + 100.0 * 8.35097885131836
Epoch 1460, val loss: 0.5319501757621765
Epoch 1470, training loss: 835.626708984375 = 0.47591787576675415 + 100.0 * 8.351508140563965
Epoch 1470, val loss: 0.531500518321991
Epoch 1480, training loss: 835.4201049804688 = 0.475188672542572 + 100.0 * 8.349449157714844
Epoch 1480, val loss: 0.5310025811195374
Epoch 1490, training loss: 835.3687133789062 = 0.47445616126060486 + 100.0 * 8.348942756652832
Epoch 1490, val loss: 0.530684769153595
Epoch 1500, training loss: 835.4779052734375 = 0.4737374782562256 + 100.0 * 8.350041389465332
Epoch 1500, val loss: 0.5302261114120483
Epoch 1510, training loss: 835.3550415039062 = 0.47296658158302307 + 100.0 * 8.348820686340332
Epoch 1510, val loss: 0.5297204852104187
Epoch 1520, training loss: 835.3721313476562 = 0.4722321033477783 + 100.0 * 8.3489990234375
Epoch 1520, val loss: 0.5294377207756042
Epoch 1530, training loss: 835.1995239257812 = 0.4715324342250824 + 100.0 * 8.34727954864502
Epoch 1530, val loss: 0.5289303064346313
Epoch 1540, training loss: 835.1751098632812 = 0.4708314836025238 + 100.0 * 8.34704303741455
Epoch 1540, val loss: 0.5285952091217041
Epoch 1550, training loss: 835.399169921875 = 0.47013846039772034 + 100.0 * 8.349289894104004
Epoch 1550, val loss: 0.528255045413971
Epoch 1560, training loss: 835.1431884765625 = 0.46941134333610535 + 100.0 * 8.3467378616333
Epoch 1560, val loss: 0.5276802778244019
Epoch 1570, training loss: 835.0776977539062 = 0.46872153878211975 + 100.0 * 8.346089363098145
Epoch 1570, val loss: 0.5273551344871521
Epoch 1580, training loss: 835.0880126953125 = 0.4680401086807251 + 100.0 * 8.346199989318848
Epoch 1580, val loss: 0.5268521308898926
Epoch 1590, training loss: 835.0279541015625 = 0.46735015511512756 + 100.0 * 8.345605850219727
Epoch 1590, val loss: 0.5265220403671265
Epoch 1600, training loss: 834.9242553710938 = 0.4666782021522522 + 100.0 * 8.344575881958008
Epoch 1600, val loss: 0.5260659456253052
Epoch 1610, training loss: 835.4374389648438 = 0.46600404381752014 + 100.0 * 8.349714279174805
Epoch 1610, val loss: 0.5256192088127136
Epoch 1620, training loss: 834.9935302734375 = 0.4652971029281616 + 100.0 * 8.345282554626465
Epoch 1620, val loss: 0.5252557992935181
Epoch 1630, training loss: 834.8111572265625 = 0.46464186906814575 + 100.0 * 8.343464851379395
Epoch 1630, val loss: 0.5247634649276733
Epoch 1640, training loss: 834.818359375 = 0.4639955163002014 + 100.0 * 8.343544006347656
Epoch 1640, val loss: 0.5244046449661255
Epoch 1650, training loss: 835.2412719726562 = 0.46332234144210815 + 100.0 * 8.347779273986816
Epoch 1650, val loss: 0.5239501595497131
Epoch 1660, training loss: 834.83935546875 = 0.46263831853866577 + 100.0 * 8.343767166137695
Epoch 1660, val loss: 0.5235903263092041
Epoch 1670, training loss: 834.6808471679688 = 0.4620152711868286 + 100.0 * 8.342187881469727
Epoch 1670, val loss: 0.5231088399887085
Epoch 1680, training loss: 834.6157836914062 = 0.4613788425922394 + 100.0 * 8.341544151306152
Epoch 1680, val loss: 0.522739589214325
Epoch 1690, training loss: 834.5791015625 = 0.4607560634613037 + 100.0 * 8.34118366241455
Epoch 1690, val loss: 0.5223249197006226
Epoch 1700, training loss: 834.9802856445312 = 0.4601183235645294 + 100.0 * 8.34520149230957
Epoch 1700, val loss: 0.5218276977539062
Epoch 1710, training loss: 834.977294921875 = 0.4594259560108185 + 100.0 * 8.345178604125977
Epoch 1710, val loss: 0.5214608907699585
Epoch 1720, training loss: 834.5162353515625 = 0.458760142326355 + 100.0 * 8.340575218200684
Epoch 1720, val loss: 0.5210915207862854
Epoch 1730, training loss: 834.4920654296875 = 0.4581311047077179 + 100.0 * 8.340339660644531
Epoch 1730, val loss: 0.5205428600311279
Epoch 1740, training loss: 834.39208984375 = 0.4575117826461792 + 100.0 * 8.339345932006836
Epoch 1740, val loss: 0.5202061533927917
Epoch 1750, training loss: 834.3612670898438 = 0.4569065570831299 + 100.0 * 8.339043617248535
Epoch 1750, val loss: 0.5197982788085938
Epoch 1760, training loss: 834.33154296875 = 0.4562927186489105 + 100.0 * 8.338752746582031
Epoch 1760, val loss: 0.5193997025489807
Epoch 1770, training loss: 834.7462768554688 = 0.4556788504123688 + 100.0 * 8.34290599822998
Epoch 1770, val loss: 0.51900315284729
Epoch 1780, training loss: 834.3951416015625 = 0.4549872875213623 + 100.0 * 8.339401245117188
Epoch 1780, val loss: 0.5185728669166565
Epoch 1790, training loss: 834.53857421875 = 0.4543609917163849 + 100.0 * 8.340842247009277
Epoch 1790, val loss: 0.518150806427002
Epoch 1800, training loss: 834.2332763671875 = 0.4537115693092346 + 100.0 * 8.33779525756836
Epoch 1800, val loss: 0.5176135301589966
Epoch 1810, training loss: 834.1936645507812 = 0.45309776067733765 + 100.0 * 8.33740520477295
Epoch 1810, val loss: 0.517266571521759
Epoch 1820, training loss: 834.156494140625 = 0.452493816614151 + 100.0 * 8.337039947509766
Epoch 1820, val loss: 0.5168298482894897
Epoch 1830, training loss: 834.1675415039062 = 0.45188817381858826 + 100.0 * 8.337156295776367
Epoch 1830, val loss: 0.516461193561554
Epoch 1840, training loss: 834.5309448242188 = 0.45126327872276306 + 100.0 * 8.340797424316406
Epoch 1840, val loss: 0.5159258246421814
Epoch 1850, training loss: 834.378662109375 = 0.4506150782108307 + 100.0 * 8.339280128479004
Epoch 1850, val loss: 0.5156139135360718
Epoch 1860, training loss: 834.1449584960938 = 0.4499896764755249 + 100.0 * 8.336949348449707
Epoch 1860, val loss: 0.5151505470275879
Epoch 1870, training loss: 834.0370483398438 = 0.44938546419143677 + 100.0 * 8.33587646484375
Epoch 1870, val loss: 0.514778196811676
Epoch 1880, training loss: 834.042724609375 = 0.4487815499305725 + 100.0 * 8.335939407348633
Epoch 1880, val loss: 0.5143373608589172
Epoch 1890, training loss: 834.1748046875 = 0.44817236065864563 + 100.0 * 8.337265968322754
Epoch 1890, val loss: 0.5139233469963074
Epoch 1900, training loss: 834.0282592773438 = 0.44754546880722046 + 100.0 * 8.335806846618652
Epoch 1900, val loss: 0.5135030746459961
Epoch 1910, training loss: 833.9359741210938 = 0.44693511724472046 + 100.0 * 8.334890365600586
Epoch 1910, val loss: 0.5130324363708496
Epoch 1920, training loss: 834.0033569335938 = 0.44632676243782043 + 100.0 * 8.335570335388184
Epoch 1920, val loss: 0.5125982165336609
Epoch 1930, training loss: 833.8821411132812 = 0.4457080364227295 + 100.0 * 8.334364891052246
Epoch 1930, val loss: 0.5122670531272888
Epoch 1940, training loss: 833.7928466796875 = 0.4451078176498413 + 100.0 * 8.333477020263672
Epoch 1940, val loss: 0.5118274688720703
Epoch 1950, training loss: 833.8207397460938 = 0.4445047378540039 + 100.0 * 8.333762168884277
Epoch 1950, val loss: 0.5114371180534363
Epoch 1960, training loss: 834.0909423828125 = 0.44388753175735474 + 100.0 * 8.336470603942871
Epoch 1960, val loss: 0.5110432505607605
Epoch 1970, training loss: 833.9703369140625 = 0.44322288036346436 + 100.0 * 8.335270881652832
Epoch 1970, val loss: 0.5104210376739502
Epoch 1980, training loss: 833.7884521484375 = 0.4425983428955078 + 100.0 * 8.333457946777344
Epoch 1980, val loss: 0.5100883841514587
Epoch 1990, training loss: 833.7020874023438 = 0.4419848620891571 + 100.0 * 8.332601547241211
Epoch 1990, val loss: 0.5095788836479187
Epoch 2000, training loss: 833.7310180664062 = 0.4413796365261078 + 100.0 * 8.33289623260498
Epoch 2000, val loss: 0.5092061758041382
Epoch 2010, training loss: 833.7575073242188 = 0.44076022505760193 + 100.0 * 8.333168029785156
Epoch 2010, val loss: 0.5087131857872009
Epoch 2020, training loss: 833.7194213867188 = 0.4401426613330841 + 100.0 * 8.332793235778809
Epoch 2020, val loss: 0.5083330273628235
Epoch 2030, training loss: 833.6038208007812 = 0.43951714038848877 + 100.0 * 8.331643104553223
Epoch 2030, val loss: 0.5078856945037842
Epoch 2040, training loss: 833.6597900390625 = 0.4388900697231293 + 100.0 * 8.332208633422852
Epoch 2040, val loss: 0.5074319243431091
Epoch 2050, training loss: 833.4908447265625 = 0.4382709264755249 + 100.0 * 8.330525398254395
Epoch 2050, val loss: 0.5070041418075562
Epoch 2060, training loss: 833.5016479492188 = 0.4376557469367981 + 100.0 * 8.330639839172363
Epoch 2060, val loss: 0.5065088868141174
Epoch 2070, training loss: 833.6220092773438 = 0.43702948093414307 + 100.0 * 8.331850051879883
Epoch 2070, val loss: 0.5061269998550415
Epoch 2080, training loss: 833.5851440429688 = 0.43637898564338684 + 100.0 * 8.331487655639648
Epoch 2080, val loss: 0.5056710839271545
Epoch 2090, training loss: 833.5917358398438 = 0.43573081493377686 + 100.0 * 8.331560134887695
Epoch 2090, val loss: 0.5051051378250122
Epoch 2100, training loss: 833.4410400390625 = 0.43509092926979065 + 100.0 * 8.330059051513672
Epoch 2100, val loss: 0.504723310470581
Epoch 2110, training loss: 833.3372802734375 = 0.4344657361507416 + 100.0 * 8.329028129577637
Epoch 2110, val loss: 0.5042754411697388
Epoch 2120, training loss: 833.3385620117188 = 0.433838814496994 + 100.0 * 8.329047203063965
Epoch 2120, val loss: 0.5038946270942688
Epoch 2130, training loss: 833.6556396484375 = 0.4331973195075989 + 100.0 * 8.332223892211914
Epoch 2130, val loss: 0.5034362077713013
Epoch 2140, training loss: 833.4310913085938 = 0.43253442645072937 + 100.0 * 8.329985618591309
Epoch 2140, val loss: 0.5029349327087402
Epoch 2150, training loss: 833.6005859375 = 0.43187564611434937 + 100.0 * 8.331686973571777
Epoch 2150, val loss: 0.5024672746658325
Epoch 2160, training loss: 833.2918090820312 = 0.4312068223953247 + 100.0 * 8.328605651855469
Epoch 2160, val loss: 0.5020541548728943
Epoch 2170, training loss: 833.2467651367188 = 0.4305630326271057 + 100.0 * 8.32816219329834
Epoch 2170, val loss: 0.5015510320663452
Epoch 2180, training loss: 833.248046875 = 0.42991793155670166 + 100.0 * 8.328181266784668
Epoch 2180, val loss: 0.5011134743690491
Epoch 2190, training loss: 833.2649536132812 = 0.4292663335800171 + 100.0 * 8.328356742858887
Epoch 2190, val loss: 0.5006732940673828
Epoch 2200, training loss: 833.136962890625 = 0.4286109507083893 + 100.0 * 8.327083587646484
Epoch 2200, val loss: 0.5001236200332642
Epoch 2210, training loss: 833.2828979492188 = 0.4279575049877167 + 100.0 * 8.3285493850708
Epoch 2210, val loss: 0.499575674533844
Epoch 2220, training loss: 833.099609375 = 0.42728307843208313 + 100.0 * 8.326723098754883
Epoch 2220, val loss: 0.4991973340511322
Epoch 2230, training loss: 833.662353515625 = 0.4266192317008972 + 100.0 * 8.332357406616211
Epoch 2230, val loss: 0.4986664354801178
Epoch 2240, training loss: 833.2304077148438 = 0.4259091317653656 + 100.0 * 8.328044891357422
Epoch 2240, val loss: 0.49824440479278564
Epoch 2250, training loss: 833.0973510742188 = 0.42523735761642456 + 100.0 * 8.32672119140625
Epoch 2250, val loss: 0.4976778030395508
Epoch 2260, training loss: 833.0029296875 = 0.4245823919773102 + 100.0 * 8.325783729553223
Epoch 2260, val loss: 0.49724850058555603
Epoch 2270, training loss: 832.9638671875 = 0.42392969131469727 + 100.0 * 8.325399398803711
Epoch 2270, val loss: 0.4968137741088867
Epoch 2280, training loss: 833.0070190429688 = 0.4232802987098694 + 100.0 * 8.325837135314941
Epoch 2280, val loss: 0.4963430166244507
Epoch 2290, training loss: 833.6116333007812 = 0.42257922887802124 + 100.0 * 8.331890106201172
Epoch 2290, val loss: 0.4958288371562958
Epoch 2300, training loss: 832.95361328125 = 0.42185840010643005 + 100.0 * 8.3253173828125
Epoch 2300, val loss: 0.4953913390636444
Epoch 2310, training loss: 832.9066162109375 = 0.42118510603904724 + 100.0 * 8.324853897094727
Epoch 2310, val loss: 0.4949164390563965
Epoch 2320, training loss: 832.8936157226562 = 0.42051541805267334 + 100.0 * 8.32473087310791
Epoch 2320, val loss: 0.4944431781768799
Epoch 2330, training loss: 832.8551025390625 = 0.4198538661003113 + 100.0 * 8.324352264404297
Epoch 2330, val loss: 0.494003564119339
Epoch 2340, training loss: 832.8329467773438 = 0.4191877841949463 + 100.0 * 8.324137687683105
Epoch 2340, val loss: 0.49352502822875977
Epoch 2350, training loss: 833.1155395507812 = 0.41850847005844116 + 100.0 * 8.326970100402832
Epoch 2350, val loss: 0.49299168586730957
Epoch 2360, training loss: 832.8693237304688 = 0.4177684187889099 + 100.0 * 8.324515342712402
Epoch 2360, val loss: 0.4926692247390747
Epoch 2370, training loss: 832.9236450195312 = 0.4170611798763275 + 100.0 * 8.325065612792969
Epoch 2370, val loss: 0.49201062321662903
Epoch 2380, training loss: 832.815185546875 = 0.41637399792671204 + 100.0 * 8.32398796081543
Epoch 2380, val loss: 0.49169307947158813
Epoch 2390, training loss: 832.880126953125 = 0.41568735241889954 + 100.0 * 8.324644088745117
Epoch 2390, val loss: 0.4912174642086029
Epoch 2400, training loss: 832.8583984375 = 0.41498425602912903 + 100.0 * 8.324434280395508
Epoch 2400, val loss: 0.4907337427139282
Epoch 2410, training loss: 832.7652587890625 = 0.4142906963825226 + 100.0 * 8.323509216308594
Epoch 2410, val loss: 0.49023473262786865
Epoch 2420, training loss: 832.781494140625 = 0.4135936498641968 + 100.0 * 8.323678970336914
Epoch 2420, val loss: 0.489852637052536
Epoch 2430, training loss: 833.16455078125 = 0.41288161277770996 + 100.0 * 8.327516555786133
Epoch 2430, val loss: 0.4894227981567383
Epoch 2440, training loss: 832.8469848632812 = 0.4121440351009369 + 100.0 * 8.324348449707031
Epoch 2440, val loss: 0.4888257384300232
Epoch 2450, training loss: 832.7205200195312 = 0.41143324971199036 + 100.0 * 8.323090553283691
Epoch 2450, val loss: 0.4884371757507324
Epoch 2460, training loss: 832.6604614257812 = 0.41072696447372437 + 100.0 * 8.322497367858887
Epoch 2460, val loss: 0.4879375994205475
Epoch 2470, training loss: 832.7223510742188 = 0.410020112991333 + 100.0 * 8.32312297821045
Epoch 2470, val loss: 0.4875486195087433
Epoch 2480, training loss: 832.9033813476562 = 0.40928536653518677 + 100.0 * 8.32494068145752
Epoch 2480, val loss: 0.4870566129684448
Epoch 2490, training loss: 832.7212524414062 = 0.40855225920677185 + 100.0 * 8.323126792907715
Epoch 2490, val loss: 0.4865720272064209
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8026382546930492
0.8624212127798305
=== training gcn model ===
Epoch 0, training loss: 1059.318603515625 = 1.0913034677505493 + 100.0 * 10.58227252960205
Epoch 0, val loss: 1.091594934463501
Epoch 10, training loss: 1059.264404296875 = 1.087587833404541 + 100.0 * 10.581767082214355
Epoch 10, val loss: 1.087846279144287
Epoch 20, training loss: 1059.012939453125 = 1.0836774110794067 + 100.0 * 10.579293251037598
Epoch 20, val loss: 1.083927035331726
Epoch 30, training loss: 1057.86572265625 = 1.079628348350525 + 100.0 * 10.567861557006836
Epoch 30, val loss: 1.0798739194869995
Epoch 40, training loss: 1053.5521240234375 = 1.0751804113388062 + 100.0 * 10.52476978302002
Epoch 40, val loss: 1.0753740072250366
Epoch 50, training loss: 1041.739013671875 = 1.0701647996902466 + 100.0 * 10.40668773651123
Epoch 50, val loss: 1.0703129768371582
Epoch 60, training loss: 1019.1849365234375 = 1.0649389028549194 + 100.0 * 10.18120002746582
Epoch 60, val loss: 1.0650876760482788
Epoch 70, training loss: 988.1141357421875 = 1.0593912601470947 + 100.0 * 9.8705472946167
Epoch 70, val loss: 1.0594162940979004
Epoch 80, training loss: 964.8292846679688 = 1.0530986785888672 + 100.0 * 9.637762069702148
Epoch 80, val loss: 1.052972435951233
Epoch 90, training loss: 947.912109375 = 1.046621561050415 + 100.0 * 9.46865463256836
Epoch 90, val loss: 1.0465196371078491
Epoch 100, training loss: 930.0415649414062 = 1.0413761138916016 + 100.0 * 9.29000186920166
Epoch 100, val loss: 1.041440486907959
Epoch 110, training loss: 913.0772094726562 = 1.0377423763275146 + 100.0 * 9.120394706726074
Epoch 110, val loss: 1.0379964113235474
Epoch 120, training loss: 903.9049682617188 = 1.0349597930908203 + 100.0 * 9.02869987487793
Epoch 120, val loss: 1.0353367328643799
Epoch 130, training loss: 899.1721801757812 = 1.032533049583435 + 100.0 * 8.981396675109863
Epoch 130, val loss: 1.032909631729126
Epoch 140, training loss: 896.9451293945312 = 1.0296003818511963 + 100.0 * 8.959155082702637
Epoch 140, val loss: 1.029842495918274
Epoch 150, training loss: 894.3203735351562 = 1.0262213945388794 + 100.0 * 8.932941436767578
Epoch 150, val loss: 1.0263702869415283
Epoch 160, training loss: 890.9453125 = 1.022873044013977 + 100.0 * 8.899224281311035
Epoch 160, val loss: 1.0230116844177246
Epoch 170, training loss: 886.4349365234375 = 1.019951581954956 + 100.0 * 8.85414981842041
Epoch 170, val loss: 1.0201441049575806
Epoch 180, training loss: 881.7743530273438 = 1.0175349712371826 + 100.0 * 8.807568550109863
Epoch 180, val loss: 1.017771601676941
Epoch 190, training loss: 878.5794677734375 = 1.0150082111358643 + 100.0 * 8.775644302368164
Epoch 190, val loss: 1.0152119398117065
Epoch 200, training loss: 875.6192016601562 = 1.0119285583496094 + 100.0 * 8.746072769165039
Epoch 200, val loss: 1.0120923519134521
Epoch 210, training loss: 872.7657470703125 = 1.008612036705017 + 100.0 * 8.717571258544922
Epoch 210, val loss: 1.008803129196167
Epoch 220, training loss: 870.6008911132812 = 1.0052064657211304 + 100.0 * 8.69595718383789
Epoch 220, val loss: 1.0054055452346802
Epoch 230, training loss: 868.8147583007812 = 1.0015442371368408 + 100.0 * 8.678132057189941
Epoch 230, val loss: 1.0017169713974
Epoch 240, training loss: 867.4364624023438 = 0.9975218176841736 + 100.0 * 8.664389610290527
Epoch 240, val loss: 0.9976789951324463
Epoch 250, training loss: 866.3009033203125 = 0.9930800795555115 + 100.0 * 8.653078079223633
Epoch 250, val loss: 0.9932593703269958
Epoch 260, training loss: 865.4308471679688 = 0.9882511496543884 + 100.0 * 8.644425392150879
Epoch 260, val loss: 0.9884598255157471
Epoch 270, training loss: 864.6156005859375 = 0.9830621480941772 + 100.0 * 8.63632583618164
Epoch 270, val loss: 0.9832959175109863
Epoch 280, training loss: 863.7216796875 = 0.9776642322540283 + 100.0 * 8.627440452575684
Epoch 280, val loss: 0.9779806733131409
Epoch 290, training loss: 862.687744140625 = 0.9721804261207581 + 100.0 * 8.617156028747559
Epoch 290, val loss: 0.9725923538208008
Epoch 300, training loss: 861.5882568359375 = 0.9665851593017578 + 100.0 * 8.606216430664062
Epoch 300, val loss: 0.9671265482902527
Epoch 310, training loss: 860.5164794921875 = 0.9607259631156921 + 100.0 * 8.59555721282959
Epoch 310, val loss: 0.9613367319107056
Epoch 320, training loss: 859.1663818359375 = 0.9546126127243042 + 100.0 * 8.582118034362793
Epoch 320, val loss: 0.9553298354148865
Epoch 330, training loss: 857.9583129882812 = 0.9482292532920837 + 100.0 * 8.570100784301758
Epoch 330, val loss: 0.9490796327590942
Epoch 340, training loss: 856.9386596679688 = 0.9414414763450623 + 100.0 * 8.559971809387207
Epoch 340, val loss: 0.9424447417259216
Epoch 350, training loss: 856.0889282226562 = 0.9341543316841125 + 100.0 * 8.55154800415039
Epoch 350, val loss: 0.9352602362632751
Epoch 360, training loss: 854.9522705078125 = 0.926471471786499 + 100.0 * 8.540258407592773
Epoch 360, val loss: 0.9278399348258972
Epoch 370, training loss: 853.9630737304688 = 0.9186344742774963 + 100.0 * 8.530444145202637
Epoch 370, val loss: 0.9202247858047485
Epoch 380, training loss: 853.2353515625 = 0.9105872511863708 + 100.0 * 8.523247718811035
Epoch 380, val loss: 0.912362277507782
Epoch 390, training loss: 852.1972045898438 = 0.9022344946861267 + 100.0 * 8.51294994354248
Epoch 390, val loss: 0.9042838215827942
Epoch 400, training loss: 851.29150390625 = 0.8937637805938721 + 100.0 * 8.50397777557373
Epoch 400, val loss: 0.8961121439933777
Epoch 410, training loss: 850.467529296875 = 0.8851547241210938 + 100.0 * 8.495823860168457
Epoch 410, val loss: 0.887793779373169
Epoch 420, training loss: 850.1165161132812 = 0.8762968182563782 + 100.0 * 8.492402076721191
Epoch 420, val loss: 0.8792566061019897
Epoch 430, training loss: 849.1576538085938 = 0.8672031164169312 + 100.0 * 8.482904434204102
Epoch 430, val loss: 0.8704411387443542
Epoch 440, training loss: 848.5675048828125 = 0.8580164313316345 + 100.0 * 8.477094650268555
Epoch 440, val loss: 0.8615785241127014
Epoch 450, training loss: 848.0619506835938 = 0.8486785292625427 + 100.0 * 8.472132682800293
Epoch 450, val loss: 0.8526201844215393
Epoch 460, training loss: 847.6310424804688 = 0.8392021059989929 + 100.0 * 8.467918395996094
Epoch 460, val loss: 0.843537449836731
Epoch 470, training loss: 847.4794311523438 = 0.829535722732544 + 100.0 * 8.466499328613281
Epoch 470, val loss: 0.834327220916748
Epoch 480, training loss: 846.9396362304688 = 0.8197615742683411 + 100.0 * 8.461198806762695
Epoch 480, val loss: 0.8250318765640259
Epoch 490, training loss: 846.5936889648438 = 0.8100642561912537 + 100.0 * 8.457836151123047
Epoch 490, val loss: 0.8158361911773682
Epoch 500, training loss: 846.4889526367188 = 0.8003846406936646 + 100.0 * 8.45688533782959
Epoch 500, val loss: 0.8066776394844055
Epoch 510, training loss: 845.9759521484375 = 0.7907881736755371 + 100.0 * 8.451851844787598
Epoch 510, val loss: 0.7975655198097229
Epoch 520, training loss: 845.6611328125 = 0.7813137173652649 + 100.0 * 8.448798179626465
Epoch 520, val loss: 0.7886436581611633
Epoch 530, training loss: 845.378662109375 = 0.7719756960868835 + 100.0 * 8.446066856384277
Epoch 530, val loss: 0.7798805236816406
Epoch 540, training loss: 845.1814575195312 = 0.7627702951431274 + 100.0 * 8.44418716430664
Epoch 540, val loss: 0.7712881565093994
Epoch 550, training loss: 845.0663452148438 = 0.75367671251297 + 100.0 * 8.443126678466797
Epoch 550, val loss: 0.7627410292625427
Epoch 560, training loss: 844.7476806640625 = 0.7447081804275513 + 100.0 * 8.440030097961426
Epoch 560, val loss: 0.7544321417808533
Epoch 570, training loss: 844.4683227539062 = 0.7360898852348328 + 100.0 * 8.437322616577148
Epoch 570, val loss: 0.7464219331741333
Epoch 580, training loss: 844.2308349609375 = 0.7276955246925354 + 100.0 * 8.435030937194824
Epoch 580, val loss: 0.7386527061462402
Epoch 590, training loss: 844.0090942382812 = 0.7195355892181396 + 100.0 * 8.43289566040039
Epoch 590, val loss: 0.731113076210022
Epoch 600, training loss: 843.8433227539062 = 0.7116353511810303 + 100.0 * 8.431317329406738
Epoch 600, val loss: 0.7238438725471497
Epoch 610, training loss: 843.7889404296875 = 0.7038499712944031 + 100.0 * 8.430850982666016
Epoch 610, val loss: 0.7167143821716309
Epoch 620, training loss: 843.43994140625 = 0.6963679790496826 + 100.0 * 8.427435874938965
Epoch 620, val loss: 0.7098730206489563
Epoch 630, training loss: 843.1539916992188 = 0.6892575621604919 + 100.0 * 8.424647331237793
Epoch 630, val loss: 0.7033975720405579
Epoch 640, training loss: 842.9318237304688 = 0.6824216246604919 + 100.0 * 8.422493934631348
Epoch 640, val loss: 0.6971716284751892
Epoch 650, training loss: 842.9928588867188 = 0.6757511496543884 + 100.0 * 8.423171043395996
Epoch 650, val loss: 0.6910948157310486
Epoch 660, training loss: 842.5787353515625 = 0.6692315340042114 + 100.0 * 8.419095039367676
Epoch 660, val loss: 0.6852601766586304
Epoch 670, training loss: 842.3023071289062 = 0.6630720496177673 + 100.0 * 8.41639232635498
Epoch 670, val loss: 0.67970210313797
Epoch 680, training loss: 842.06787109375 = 0.6571417450904846 + 100.0 * 8.414107322692871
Epoch 680, val loss: 0.6743989586830139
Epoch 690, training loss: 841.9871826171875 = 0.6514055728912354 + 100.0 * 8.413357734680176
Epoch 690, val loss: 0.6693098545074463
Epoch 700, training loss: 841.8821411132812 = 0.6457701921463013 + 100.0 * 8.41236400604248
Epoch 700, val loss: 0.6642463207244873
Epoch 710, training loss: 841.5956420898438 = 0.6403056383132935 + 100.0 * 8.409553527832031
Epoch 710, val loss: 0.6594217419624329
Epoch 720, training loss: 841.3748779296875 = 0.6351475715637207 + 100.0 * 8.407397270202637
Epoch 720, val loss: 0.6548393964767456
Epoch 730, training loss: 841.19140625 = 0.6301727294921875 + 100.0 * 8.405611991882324
Epoch 730, val loss: 0.6504650115966797
Epoch 740, training loss: 841.02197265625 = 0.625357449054718 + 100.0 * 8.403965950012207
Epoch 740, val loss: 0.6462417244911194
Epoch 750, training loss: 840.955810546875 = 0.6207155585289001 + 100.0 * 8.403350830078125
Epoch 750, val loss: 0.6421836614608765
Epoch 760, training loss: 841.289306640625 = 0.616142213344574 + 100.0 * 8.406731605529785
Epoch 760, val loss: 0.6382274031639099
Epoch 770, training loss: 840.7401123046875 = 0.6116795539855957 + 100.0 * 8.401284217834473
Epoch 770, val loss: 0.6342950463294983
Epoch 780, training loss: 840.4843139648438 = 0.6074956655502319 + 100.0 * 8.398768424987793
Epoch 780, val loss: 0.6307304501533508
Epoch 790, training loss: 840.3267211914062 = 0.6035312414169312 + 100.0 * 8.397232055664062
Epoch 790, val loss: 0.6273355484008789
Epoch 800, training loss: 840.1790161132812 = 0.5997064113616943 + 100.0 * 8.395792961120605
Epoch 800, val loss: 0.6240785717964172
Epoch 810, training loss: 840.033935546875 = 0.5960425138473511 + 100.0 * 8.394378662109375
Epoch 810, val loss: 0.6209895014762878
Epoch 820, training loss: 839.9451293945312 = 0.5925034284591675 + 100.0 * 8.393526077270508
Epoch 820, val loss: 0.618026852607727
Epoch 830, training loss: 840.1470336914062 = 0.5889948606491089 + 100.0 * 8.395580291748047
Epoch 830, val loss: 0.6150983572006226
Epoch 840, training loss: 839.676513671875 = 0.5855453610420227 + 100.0 * 8.390909194946289
Epoch 840, val loss: 0.6122090220451355
Epoch 850, training loss: 839.5528564453125 = 0.5824059247970581 + 100.0 * 8.389704704284668
Epoch 850, val loss: 0.6096141934394836
Epoch 860, training loss: 839.4364013671875 = 0.5794172286987305 + 100.0 * 8.388569831848145
Epoch 860, val loss: 0.6071773767471313
Epoch 870, training loss: 839.2999267578125 = 0.5765168070793152 + 100.0 * 8.38723373413086
Epoch 870, val loss: 0.6048315167427063
Epoch 880, training loss: 839.1798706054688 = 0.5737170577049255 + 100.0 * 8.386061668395996
Epoch 880, val loss: 0.6025972366333008
Epoch 890, training loss: 839.857666015625 = 0.5710011720657349 + 100.0 * 8.392867088317871
Epoch 890, val loss: 0.600415825843811
Epoch 900, training loss: 839.0030517578125 = 0.5682364702224731 + 100.0 * 8.384347915649414
Epoch 900, val loss: 0.5982908606529236
Epoch 910, training loss: 838.8555908203125 = 0.5656949877738953 + 100.0 * 8.382899284362793
Epoch 910, val loss: 0.5962923765182495
Epoch 920, training loss: 838.7313232421875 = 0.563289999961853 + 100.0 * 8.381680488586426
Epoch 920, val loss: 0.5944266319274902
Epoch 930, training loss: 838.6248168945312 = 0.5609585046768188 + 100.0 * 8.380638122558594
Epoch 930, val loss: 0.5926424264907837
Epoch 940, training loss: 839.2970581054688 = 0.5586883425712585 + 100.0 * 8.387383460998535
Epoch 940, val loss: 0.5908467769622803
Epoch 950, training loss: 838.630859375 = 0.5562927722930908 + 100.0 * 8.380745887756348
Epoch 950, val loss: 0.589140772819519
Epoch 960, training loss: 838.3905639648438 = 0.5541207790374756 + 100.0 * 8.378364562988281
Epoch 960, val loss: 0.5874909162521362
Epoch 970, training loss: 838.2417602539062 = 0.5520810484886169 + 100.0 * 8.376896858215332
Epoch 970, val loss: 0.5859859585762024
Epoch 980, training loss: 838.1201782226562 = 0.5500865578651428 + 100.0 * 8.375700950622559
Epoch 980, val loss: 0.5845761895179749
Epoch 990, training loss: 838.0263061523438 = 0.5481424927711487 + 100.0 * 8.374781608581543
Epoch 990, val loss: 0.5831794142723083
Epoch 1000, training loss: 837.9320068359375 = 0.5462446212768555 + 100.0 * 8.373857498168945
Epoch 1000, val loss: 0.5818384289741516
Epoch 1010, training loss: 837.8655395507812 = 0.5443874001502991 + 100.0 * 8.373211860656738
Epoch 1010, val loss: 0.580532968044281
Epoch 1020, training loss: 838.515869140625 = 0.5425541996955872 + 100.0 * 8.379733085632324
Epoch 1020, val loss: 0.5791917443275452
Epoch 1030, training loss: 837.8409423828125 = 0.540644109249115 + 100.0 * 8.373003005981445
Epoch 1030, val loss: 0.5779348015785217
Epoch 1040, training loss: 837.6510620117188 = 0.5389237403869629 + 100.0 * 8.371121406555176
Epoch 1040, val loss: 0.5767824053764343
Epoch 1050, training loss: 837.5546875 = 0.5372528433799744 + 100.0 * 8.370174407958984
Epoch 1050, val loss: 0.5756511688232422
Epoch 1060, training loss: 837.4652709960938 = 0.5356164574623108 + 100.0 * 8.369296073913574
Epoch 1060, val loss: 0.5745781064033508
Epoch 1070, training loss: 837.4146728515625 = 0.5340163707733154 + 100.0 * 8.368806838989258
Epoch 1070, val loss: 0.5735543966293335
Epoch 1080, training loss: 837.9832763671875 = 0.5323963761329651 + 100.0 * 8.37450885772705
Epoch 1080, val loss: 0.5725151896476746
Epoch 1090, training loss: 837.4959716796875 = 0.5307119488716125 + 100.0 * 8.36965274810791
Epoch 1090, val loss: 0.5714146494865417
Epoch 1100, training loss: 837.3060913085938 = 0.5291711091995239 + 100.0 * 8.367769241333008
Epoch 1100, val loss: 0.5703902840614319
Epoch 1110, training loss: 837.1765747070312 = 0.5276986360549927 + 100.0 * 8.366488456726074
Epoch 1110, val loss: 0.5694708824157715
Epoch 1120, training loss: 837.1193237304688 = 0.5262472033500671 + 100.0 * 8.365930557250977
Epoch 1120, val loss: 0.5686010718345642
Epoch 1130, training loss: 837.66064453125 = 0.5247905850410461 + 100.0 * 8.371358871459961
Epoch 1130, val loss: 0.567739725112915
Epoch 1140, training loss: 837.258544921875 = 0.5232943892478943 + 100.0 * 8.367352485656738
Epoch 1140, val loss: 0.5667338371276855
Epoch 1150, training loss: 837.035400390625 = 0.5218742489814758 + 100.0 * 8.365135192871094
Epoch 1150, val loss: 0.5659016966819763
Epoch 1160, training loss: 836.9089965820312 = 0.5205249190330505 + 100.0 * 8.363884925842285
Epoch 1160, val loss: 0.5650919079780579
Epoch 1170, training loss: 836.8492431640625 = 0.5192010998725891 + 100.0 * 8.363300323486328
Epoch 1170, val loss: 0.5643418431282043
Epoch 1180, training loss: 836.8062133789062 = 0.5179038643836975 + 100.0 * 8.362883567810059
Epoch 1180, val loss: 0.5635828971862793
Epoch 1190, training loss: 837.1421508789062 = 0.5165991187095642 + 100.0 * 8.366255760192871
Epoch 1190, val loss: 0.5628857016563416
Epoch 1200, training loss: 836.8402099609375 = 0.5152544379234314 + 100.0 * 8.363249778747559
Epoch 1200, val loss: 0.5620294809341431
Epoch 1210, training loss: 836.682373046875 = 0.5139744281768799 + 100.0 * 8.36168384552002
Epoch 1210, val loss: 0.5612966418266296
Epoch 1220, training loss: 836.6240844726562 = 0.512739896774292 + 100.0 * 8.361113548278809
Epoch 1220, val loss: 0.5606257319450378
Epoch 1230, training loss: 836.9420776367188 = 0.5115212202072144 + 100.0 * 8.36430549621582
Epoch 1230, val loss: 0.5598885416984558
Epoch 1240, training loss: 836.654541015625 = 0.5102394223213196 + 100.0 * 8.361442565917969
Epoch 1240, val loss: 0.559242844581604
Epoch 1250, training loss: 836.5536499023438 = 0.5090379118919373 + 100.0 * 8.360445976257324
Epoch 1250, val loss: 0.5585188865661621
Epoch 1260, training loss: 836.4229736328125 = 0.5078728795051575 + 100.0 * 8.359150886535645
Epoch 1260, val loss: 0.5579083561897278
Epoch 1270, training loss: 836.372314453125 = 0.5067351460456848 + 100.0 * 8.35865592956543
Epoch 1270, val loss: 0.5572903752326965
Epoch 1280, training loss: 836.6062622070312 = 0.5056106448173523 + 100.0 * 8.361006736755371
Epoch 1280, val loss: 0.5566834807395935
Epoch 1290, training loss: 836.3134155273438 = 0.5043957233428955 + 100.0 * 8.3580904006958
Epoch 1290, val loss: 0.5559871196746826
Epoch 1300, training loss: 836.2527465820312 = 0.5032539367675781 + 100.0 * 8.357495307922363
Epoch 1300, val loss: 0.5554184913635254
Epoch 1310, training loss: 836.1875610351562 = 0.5021486878395081 + 100.0 * 8.356854438781738
Epoch 1310, val loss: 0.5547885298728943
Epoch 1320, training loss: 836.1365356445312 = 0.501061737537384 + 100.0 * 8.356354713439941
Epoch 1320, val loss: 0.5542558431625366
Epoch 1330, training loss: 836.0726928710938 = 0.4999801516532898 + 100.0 * 8.355727195739746
Epoch 1330, val loss: 0.553658664226532
Epoch 1340, training loss: 836.56982421875 = 0.4988991618156433 + 100.0 * 8.360709190368652
Epoch 1340, val loss: 0.5531702041625977
Epoch 1350, training loss: 836.2745971679688 = 0.4976854920387268 + 100.0 * 8.357769012451172
Epoch 1350, val loss: 0.552358090877533
Epoch 1360, training loss: 836.0758056640625 = 0.49654895067214966 + 100.0 * 8.355792999267578
Epoch 1360, val loss: 0.5516858696937561
Epoch 1370, training loss: 835.9421997070312 = 0.4954967200756073 + 100.0 * 8.354467391967773
Epoch 1370, val loss: 0.5511435866355896
Epoch 1380, training loss: 835.8826293945312 = 0.49445512890815735 + 100.0 * 8.3538818359375
Epoch 1380, val loss: 0.5505942702293396
Epoch 1390, training loss: 835.9192504882812 = 0.4934106171131134 + 100.0 * 8.35425853729248
Epoch 1390, val loss: 0.5500376224517822
Epoch 1400, training loss: 835.9158935546875 = 0.4923350214958191 + 100.0 * 8.354235649108887
Epoch 1400, val loss: 0.549450695514679
Epoch 1410, training loss: 835.770751953125 = 0.4912729263305664 + 100.0 * 8.352794647216797
Epoch 1410, val loss: 0.5488239526748657
Epoch 1420, training loss: 835.6995239257812 = 0.4902367889881134 + 100.0 * 8.352092742919922
Epoch 1420, val loss: 0.5482271909713745
Epoch 1430, training loss: 835.6950073242188 = 0.48922476172447205 + 100.0 * 8.352058410644531
Epoch 1430, val loss: 0.5476872324943542
Epoch 1440, training loss: 836.1754150390625 = 0.4881579577922821 + 100.0 * 8.35687255859375
Epoch 1440, val loss: 0.547051727771759
Epoch 1450, training loss: 835.6813354492188 = 0.4870506227016449 + 100.0 * 8.351943016052246
Epoch 1450, val loss: 0.5464779138565063
Epoch 1460, training loss: 835.5086669921875 = 0.48603519797325134 + 100.0 * 8.350226402282715
Epoch 1460, val loss: 0.5458828806877136
Epoch 1470, training loss: 835.4556274414062 = 0.4850327670574188 + 100.0 * 8.349705696105957
Epoch 1470, val loss: 0.5453514456748962
Epoch 1480, training loss: 835.6212768554688 = 0.48404690623283386 + 100.0 * 8.351371765136719
Epoch 1480, val loss: 0.5448833703994751
Epoch 1490, training loss: 835.4747314453125 = 0.48298829793930054 + 100.0 * 8.3499174118042
Epoch 1490, val loss: 0.544128954410553
Epoch 1500, training loss: 835.3900756835938 = 0.48197177052497864 + 100.0 * 8.349081039428711
Epoch 1500, val loss: 0.5436307787895203
Epoch 1510, training loss: 835.3530883789062 = 0.4809717535972595 + 100.0 * 8.348721504211426
Epoch 1510, val loss: 0.5429786443710327
Epoch 1520, training loss: 835.3265991210938 = 0.479972779750824 + 100.0 * 8.348465919494629
Epoch 1520, val loss: 0.5424633622169495
Epoch 1530, training loss: 835.3426513671875 = 0.4789775311946869 + 100.0 * 8.348636627197266
Epoch 1530, val loss: 0.5419011116027832
Epoch 1540, training loss: 835.1704711914062 = 0.4779761731624603 + 100.0 * 8.346924781799316
Epoch 1540, val loss: 0.5413102507591248
Epoch 1550, training loss: 835.3421630859375 = 0.4769958555698395 + 100.0 * 8.348651885986328
Epoch 1550, val loss: 0.5407829880714417
Epoch 1560, training loss: 835.132080078125 = 0.47594767808914185 + 100.0 * 8.346561431884766
Epoch 1560, val loss: 0.5400612354278564
Epoch 1570, training loss: 835.0469970703125 = 0.47496211528778076 + 100.0 * 8.345720291137695
Epoch 1570, val loss: 0.5395162105560303
Epoch 1580, training loss: 834.9677124023438 = 0.4739972949028015 + 100.0 * 8.344937324523926
Epoch 1580, val loss: 0.5389009714126587
Epoch 1590, training loss: 834.9334716796875 = 0.4730439782142639 + 100.0 * 8.3446044921875
Epoch 1590, val loss: 0.5383561849594116
Epoch 1600, training loss: 834.9823608398438 = 0.47208163142204285 + 100.0 * 8.345102310180664
Epoch 1600, val loss: 0.5377535223960876
Epoch 1610, training loss: 835.2265625 = 0.471059650182724 + 100.0 * 8.347555160522461
Epoch 1610, val loss: 0.5371929407119751
Epoch 1620, training loss: 834.8662109375 = 0.47002723813056946 + 100.0 * 8.343961715698242
Epoch 1620, val loss: 0.5364561676979065
Epoch 1630, training loss: 834.7737426757812 = 0.46905556321144104 + 100.0 * 8.343047142028809
Epoch 1630, val loss: 0.5358977913856506
Epoch 1640, training loss: 834.723388671875 = 0.4681040048599243 + 100.0 * 8.34255313873291
Epoch 1640, val loss: 0.5353007912635803
Epoch 1650, training loss: 834.7555541992188 = 0.467142790555954 + 100.0 * 8.342884063720703
Epoch 1650, val loss: 0.5346781015396118
Epoch 1660, training loss: 834.9970703125 = 0.4661429226398468 + 100.0 * 8.345309257507324
Epoch 1660, val loss: 0.5340344905853271
Epoch 1670, training loss: 834.7373657226562 = 0.46509718894958496 + 100.0 * 8.34272289276123
Epoch 1670, val loss: 0.5333613157272339
Epoch 1680, training loss: 834.6466674804688 = 0.46411842107772827 + 100.0 * 8.341825485229492
Epoch 1680, val loss: 0.5327444672584534
Epoch 1690, training loss: 834.6248779296875 = 0.46315765380859375 + 100.0 * 8.341617584228516
Epoch 1690, val loss: 0.5321361422538757
Epoch 1700, training loss: 834.5474243164062 = 0.4621972143650055 + 100.0 * 8.340851783752441
Epoch 1700, val loss: 0.5315237045288086
Epoch 1710, training loss: 834.9276733398438 = 0.46122562885284424 + 100.0 * 8.344664573669434
Epoch 1710, val loss: 0.5308529734611511
Epoch 1720, training loss: 834.4935302734375 = 0.4601849913597107 + 100.0 * 8.340332984924316
Epoch 1720, val loss: 0.5302420258522034
Epoch 1730, training loss: 834.4415893554688 = 0.4591994285583496 + 100.0 * 8.339823722839355
Epoch 1730, val loss: 0.5295554995536804
Epoch 1740, training loss: 834.3844604492188 = 0.45825061202049255 + 100.0 * 8.339262008666992
Epoch 1740, val loss: 0.5289709568023682
Epoch 1750, training loss: 834.360595703125 = 0.45729130506515503 + 100.0 * 8.339033126831055
Epoch 1750, val loss: 0.5283477902412415
Epoch 1760, training loss: 834.4580078125 = 0.45631319284439087 + 100.0 * 8.340017318725586
Epoch 1760, val loss: 0.5276688933372498
Epoch 1770, training loss: 834.368896484375 = 0.4552750587463379 + 100.0 * 8.339136123657227
Epoch 1770, val loss: 0.5270053744316101
Epoch 1780, training loss: 834.5050048828125 = 0.45423585176467896 + 100.0 * 8.340507507324219
Epoch 1780, val loss: 0.5262867212295532
Epoch 1790, training loss: 834.2744750976562 = 0.45323437452316284 + 100.0 * 8.338212013244629
Epoch 1790, val loss: 0.5256276726722717
Epoch 1800, training loss: 834.2555541992188 = 0.4522620439529419 + 100.0 * 8.338032722473145
Epoch 1800, val loss: 0.5250004529953003
Epoch 1810, training loss: 834.2195434570312 = 0.4512806236743927 + 100.0 * 8.337682723999023
Epoch 1810, val loss: 0.5243270993232727
Epoch 1820, training loss: 834.2750244140625 = 0.4502948522567749 + 100.0 * 8.338247299194336
Epoch 1820, val loss: 0.5236363410949707
Epoch 1830, training loss: 834.3363037109375 = 0.449268639087677 + 100.0 * 8.33887004852295
Epoch 1830, val loss: 0.5229285955429077
Epoch 1840, training loss: 834.1422119140625 = 0.44824835658073425 + 100.0 * 8.336939811706543
Epoch 1840, val loss: 0.5223115086555481
Epoch 1850, training loss: 834.213623046875 = 0.4472479224205017 + 100.0 * 8.337663650512695
Epoch 1850, val loss: 0.5216378569602966
Epoch 1860, training loss: 834.2177124023438 = 0.446211040019989 + 100.0 * 8.337715148925781
Epoch 1860, val loss: 0.5208132266998291
Epoch 1870, training loss: 834.067138671875 = 0.4451776444911957 + 100.0 * 8.336219787597656
Epoch 1870, val loss: 0.5201213955879211
Epoch 1880, training loss: 834.1004638671875 = 0.4441682994365692 + 100.0 * 8.336563110351562
Epoch 1880, val loss: 0.5194583535194397
Epoch 1890, training loss: 834.1640014648438 = 0.4431324303150177 + 100.0 * 8.33720874786377
Epoch 1890, val loss: 0.5186462998390198
Epoch 1900, training loss: 834.38916015625 = 0.442055881023407 + 100.0 * 8.339470863342285
Epoch 1900, val loss: 0.5178782939910889
Epoch 1910, training loss: 833.9945678710938 = 0.44100242853164673 + 100.0 * 8.335536003112793
Epoch 1910, val loss: 0.5171420574188232
Epoch 1920, training loss: 833.8959350585938 = 0.4399816393852234 + 100.0 * 8.334559440612793
Epoch 1920, val loss: 0.5164433717727661
Epoch 1930, training loss: 833.8759155273438 = 0.438976526260376 + 100.0 * 8.334369659423828
Epoch 1930, val loss: 0.5157299637794495
Epoch 1940, training loss: 833.8772583007812 = 0.4379785656929016 + 100.0 * 8.334392547607422
Epoch 1940, val loss: 0.5150771141052246
Epoch 1950, training loss: 834.1539306640625 = 0.43695271015167236 + 100.0 * 8.337169647216797
Epoch 1950, val loss: 0.5143836140632629
Epoch 1960, training loss: 833.8148193359375 = 0.43585431575775146 + 100.0 * 8.333789825439453
Epoch 1960, val loss: 0.5135079026222229
Epoch 1970, training loss: 833.8336181640625 = 0.4348019063472748 + 100.0 * 8.333988189697266
Epoch 1970, val loss: 0.5127267837524414
Epoch 1980, training loss: 833.86865234375 = 0.4337543547153473 + 100.0 * 8.334348678588867
Epoch 1980, val loss: 0.5119698643684387
Epoch 1990, training loss: 833.9323120117188 = 0.43268945813179016 + 100.0 * 8.334996223449707
Epoch 1990, val loss: 0.5112069845199585
Epoch 2000, training loss: 833.7750244140625 = 0.4316282272338867 + 100.0 * 8.333434104919434
Epoch 2000, val loss: 0.510511577129364
Epoch 2010, training loss: 833.699462890625 = 0.4305630028247833 + 100.0 * 8.33268928527832
Epoch 2010, val loss: 0.5097090005874634
Epoch 2020, training loss: 833.7390747070312 = 0.4295057952404022 + 100.0 * 8.33309555053711
Epoch 2020, val loss: 0.5088947415351868
Epoch 2030, training loss: 833.8391723632812 = 0.4284241795539856 + 100.0 * 8.334107398986816
Epoch 2030, val loss: 0.5081659555435181
Epoch 2040, training loss: 833.7666015625 = 0.42733681201934814 + 100.0 * 8.333393096923828
Epoch 2040, val loss: 0.5074870586395264
Epoch 2050, training loss: 834.2532958984375 = 0.42621222138404846 + 100.0 * 8.338271141052246
Epoch 2050, val loss: 0.5066286325454712
Epoch 2060, training loss: 833.7159423828125 = 0.4250650703907013 + 100.0 * 8.332908630371094
Epoch 2060, val loss: 0.5057263374328613
Epoch 2070, training loss: 833.6138305664062 = 0.4239782989025116 + 100.0 * 8.33189868927002
Epoch 2070, val loss: 0.5050198435783386
Epoch 2080, training loss: 833.5450439453125 = 0.4229193925857544 + 100.0 * 8.331221580505371
Epoch 2080, val loss: 0.504264771938324
Epoch 2090, training loss: 833.5153198242188 = 0.4218611717224121 + 100.0 * 8.330934524536133
Epoch 2090, val loss: 0.5035219788551331
Epoch 2100, training loss: 833.5035400390625 = 0.4207874536514282 + 100.0 * 8.330827713012695
Epoch 2100, val loss: 0.5027477145195007
Epoch 2110, training loss: 833.8619995117188 = 0.41970109939575195 + 100.0 * 8.334423065185547
Epoch 2110, val loss: 0.5020385384559631
Epoch 2120, training loss: 833.5945434570312 = 0.4185263216495514 + 100.0 * 8.33176040649414
Epoch 2120, val loss: 0.5010825395584106
Epoch 2130, training loss: 833.5340576171875 = 0.4173978567123413 + 100.0 * 8.33116626739502
Epoch 2130, val loss: 0.5003396272659302
Epoch 2140, training loss: 833.4242553710938 = 0.4163166582584381 + 100.0 * 8.330079078674316
Epoch 2140, val loss: 0.4995657503604889
Epoch 2150, training loss: 833.4442749023438 = 0.41523197293281555 + 100.0 * 8.330290794372559
Epoch 2150, val loss: 0.4987607002258301
Epoch 2160, training loss: 833.9169921875 = 0.4140923321247101 + 100.0 * 8.335029602050781
Epoch 2160, val loss: 0.4979467988014221
Epoch 2170, training loss: 833.3728637695312 = 0.41289710998535156 + 100.0 * 8.329599380493164
Epoch 2170, val loss: 0.4971234202384949
Epoch 2180, training loss: 833.3943481445312 = 0.41178441047668457 + 100.0 * 8.329825401306152
Epoch 2180, val loss: 0.49640437960624695
Epoch 2190, training loss: 833.3350830078125 = 0.4106915593147278 + 100.0 * 8.329243659973145
Epoch 2190, val loss: 0.49562105536460876
Epoch 2200, training loss: 833.3013305664062 = 0.40961357951164246 + 100.0 * 8.328917503356934
Epoch 2200, val loss: 0.4949074387550354
Epoch 2210, training loss: 833.556884765625 = 0.4085198640823364 + 100.0 * 8.331483840942383
Epoch 2210, val loss: 0.4941803812980652
Epoch 2220, training loss: 833.2738037109375 = 0.4073500335216522 + 100.0 * 8.328664779663086
Epoch 2220, val loss: 0.4933357834815979
Epoch 2230, training loss: 833.272705078125 = 0.40622714161872864 + 100.0 * 8.328664779663086
Epoch 2230, val loss: 0.492573082447052
Epoch 2240, training loss: 833.2451171875 = 0.40512239933013916 + 100.0 * 8.328399658203125
Epoch 2240, val loss: 0.4918389320373535
Epoch 2250, training loss: 833.2017822265625 = 0.40402448177337646 + 100.0 * 8.327977180480957
Epoch 2250, val loss: 0.491078644990921
Epoch 2260, training loss: 833.41845703125 = 0.4029194116592407 + 100.0 * 8.330155372619629
Epoch 2260, val loss: 0.4903075695037842
Epoch 2270, training loss: 833.165283203125 = 0.4017600119113922 + 100.0 * 8.327635765075684
Epoch 2270, val loss: 0.48965513706207275
Epoch 2280, training loss: 833.1249389648438 = 0.40063485503196716 + 100.0 * 8.327242851257324
Epoch 2280, val loss: 0.48890700936317444
Epoch 2290, training loss: 833.1190795898438 = 0.3995223641395569 + 100.0 * 8.327195167541504
Epoch 2290, val loss: 0.48814597725868225
Epoch 2300, training loss: 833.1695556640625 = 0.3984149694442749 + 100.0 * 8.32771110534668
Epoch 2300, val loss: 0.48746129870414734
Epoch 2310, training loss: 833.6104125976562 = 0.3972644805908203 + 100.0 * 8.332131385803223
Epoch 2310, val loss: 0.4867292642593384
Epoch 2320, training loss: 833.2230224609375 = 0.3960961699485779 + 100.0 * 8.328269004821777
Epoch 2320, val loss: 0.4859964847564697
Epoch 2330, training loss: 833.0841674804688 = 0.3949601352214813 + 100.0 * 8.326891899108887
Epoch 2330, val loss: 0.48518508672714233
Epoch 2340, training loss: 833.0225830078125 = 0.39386290311813354 + 100.0 * 8.326287269592285
Epoch 2340, val loss: 0.48457562923431396
Epoch 2350, training loss: 832.9996337890625 = 0.3927578032016754 + 100.0 * 8.326068878173828
Epoch 2350, val loss: 0.4838266372680664
Epoch 2360, training loss: 833.1463623046875 = 0.39164429903030396 + 100.0 * 8.327547073364258
Epoch 2360, val loss: 0.48312026262283325
Epoch 2370, training loss: 832.9943237304688 = 0.3904828429222107 + 100.0 * 8.326038360595703
Epoch 2370, val loss: 0.48241180181503296
Epoch 2380, training loss: 833.0719604492188 = 0.3893280029296875 + 100.0 * 8.326826095581055
Epoch 2380, val loss: 0.48162075877189636
Epoch 2390, training loss: 833.0655517578125 = 0.3881920576095581 + 100.0 * 8.326773643493652
Epoch 2390, val loss: 0.4809059202671051
Epoch 2400, training loss: 833.29345703125 = 0.38701048493385315 + 100.0 * 8.32906436920166
Epoch 2400, val loss: 0.48012182116508484
Epoch 2410, training loss: 832.9594116210938 = 0.38582682609558105 + 100.0 * 8.325736045837402
Epoch 2410, val loss: 0.47946470975875854
Epoch 2420, training loss: 832.8449096679688 = 0.3847092092037201 + 100.0 * 8.324602127075195
Epoch 2420, val loss: 0.478783518075943
Epoch 2430, training loss: 832.8303833007812 = 0.3836041986942291 + 100.0 * 8.324467658996582
Epoch 2430, val loss: 0.4781171679496765
Epoch 2440, training loss: 832.8033447265625 = 0.382495254278183 + 100.0 * 8.32420825958252
Epoch 2440, val loss: 0.47746342420578003
Epoch 2450, training loss: 832.9353637695312 = 0.3813837468624115 + 100.0 * 8.325539588928223
Epoch 2450, val loss: 0.4768853485584259
Epoch 2460, training loss: 832.8587036132812 = 0.3801892399787903 + 100.0 * 8.324785232543945
Epoch 2460, val loss: 0.4760097861289978
Epoch 2470, training loss: 832.9091186523438 = 0.3790130317211151 + 100.0 * 8.325301170349121
Epoch 2470, val loss: 0.47526267170906067
Epoch 2480, training loss: 832.8038330078125 = 0.37785035371780396 + 100.0 * 8.324259757995605
Epoch 2480, val loss: 0.47454071044921875
Epoch 2490, training loss: 832.697021484375 = 0.3767200708389282 + 100.0 * 8.323203086853027
Epoch 2490, val loss: 0.47392162680625916
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8112633181126332
0.8639426211693111
The final CL Acc:0.81498, 0.01189, The final GNN Acc:0.86375, 0.00101
Begin epxeriment: noisy_level: 0.2 cont_weight: 100 epoch: der1:0.3 der2:0 dfr1:0.4 dfr2:0.5
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=2500, cl_num_layers=2, cl_num_proj_hidden=64, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=100.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.0, drop_feat_rate_1=0.4, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=32, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, noisy_level=0.2, num_hidden=64, num_proj_hidden=64, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=40, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 88648])
add edge: torch.Size([2, 106484])
remove edge: torch.Size([2, 70990])
updated graph: torch.Size([2, 88826])
=== Noisy graph ===
/home/project-robust-contrastive/Robust-Contrastive-Learning/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 1059.3221435546875 = 1.091617465019226 + 100.0 * 10.582304954528809
Epoch 0, val loss: 1.0912305116653442
Epoch 10, training loss: 1059.286376953125 = 1.087370753288269 + 100.0 * 10.581990242004395
Epoch 10, val loss: 1.086975336074829
Epoch 20, training loss: 1059.1387939453125 = 1.082830548286438 + 100.0 * 10.580558776855469
Epoch 20, val loss: 1.082430362701416
Epoch 30, training loss: 1058.4791259765625 = 1.0781259536743164 + 100.0 * 10.574009895324707
Epoch 30, val loss: 1.0777413845062256
Epoch 40, training loss: 1055.86376953125 = 1.0730972290039062 + 100.0 * 10.547905921936035
Epoch 40, val loss: 1.0726966857910156
Epoch 50, training loss: 1048.2066650390625 = 1.0677968263626099 + 100.0 * 10.471389770507812
Epoch 50, val loss: 1.0674748420715332
Epoch 60, training loss: 1031.0318603515625 = 1.0633224248886108 + 100.0 * 10.29968547821045
Epoch 60, val loss: 1.0631701946258545
Epoch 70, training loss: 1004.5910034179688 = 1.059242844581604 + 100.0 * 10.035317420959473
Epoch 70, val loss: 1.0592468976974487
Epoch 80, training loss: 982.0897827148438 = 1.0549397468566895 + 100.0 * 9.810348510742188
Epoch 80, val loss: 1.0551378726959229
Epoch 90, training loss: 965.4813842773438 = 1.050371766090393 + 100.0 * 9.644309997558594
Epoch 90, val loss: 1.050692081451416
Epoch 100, training loss: 954.9202880859375 = 1.0458019971847534 + 100.0 * 9.538744926452637
Epoch 100, val loss: 1.0463106632232666
Epoch 110, training loss: 943.259033203125 = 1.0420379638671875 + 100.0 * 9.42216968536377
Epoch 110, val loss: 1.042801022529602
Epoch 120, training loss: 930.6409912109375 = 1.0394132137298584 + 100.0 * 9.296015739440918
Epoch 120, val loss: 1.0403679609298706
Epoch 130, training loss: 926.5746459960938 = 1.0363504886627197 + 100.0 * 9.255382537841797
Epoch 130, val loss: 1.037368655204773
Epoch 140, training loss: 924.7293090820312 = 1.0317556858062744 + 100.0 * 9.23697566986084
Epoch 140, val loss: 1.0329951047897339
Epoch 150, training loss: 922.6546630859375 = 1.0272372961044312 + 100.0 * 9.21627426147461
Epoch 150, val loss: 1.0288059711456299
Epoch 160, training loss: 920.3482666015625 = 1.0231953859329224 + 100.0 * 9.19325065612793
Epoch 160, val loss: 1.0249913930892944
Epoch 170, training loss: 916.9315185546875 = 1.0189175605773926 + 100.0 * 9.159126281738281
Epoch 170, val loss: 1.020945429801941
Epoch 180, training loss: 911.2890014648438 = 1.0146852731704712 + 100.0 * 9.102743148803711
Epoch 180, val loss: 1.0170270204544067
Epoch 190, training loss: 904.07373046875 = 1.0108305215835571 + 100.0 * 9.03062915802002
Epoch 190, val loss: 1.0134470462799072
Epoch 200, training loss: 899.8218383789062 = 1.0066336393356323 + 100.0 * 8.988151550292969
Epoch 200, val loss: 1.0094170570373535
Epoch 210, training loss: 896.3614501953125 = 1.0012816190719604 + 100.0 * 8.953601837158203
Epoch 210, val loss: 1.004286289215088
Epoch 220, training loss: 892.183349609375 = 0.9956560134887695 + 100.0 * 8.911876678466797
Epoch 220, val loss: 0.9989229440689087
Epoch 230, training loss: 888.0883178710938 = 0.9904577136039734 + 100.0 * 8.870978355407715
Epoch 230, val loss: 0.9939136505126953
Epoch 240, training loss: 883.1773681640625 = 0.9848458170890808 + 100.0 * 8.821925163269043
Epoch 240, val loss: 0.9885000586509705
Epoch 250, training loss: 880.1498413085938 = 0.9784250855445862 + 100.0 * 8.79171371459961
Epoch 250, val loss: 0.9822949171066284
Epoch 260, training loss: 877.8074340820312 = 0.9711194634437561 + 100.0 * 8.768362998962402
Epoch 260, val loss: 0.9752650260925293
Epoch 270, training loss: 875.77685546875 = 0.9631140828132629 + 100.0 * 8.748137474060059
Epoch 270, val loss: 0.9676072597503662
Epoch 280, training loss: 873.9794311523438 = 0.9545943140983582 + 100.0 * 8.73024845123291
Epoch 280, val loss: 0.9595018625259399
Epoch 290, training loss: 872.408203125 = 0.9455693960189819 + 100.0 * 8.71462631225586
Epoch 290, val loss: 0.9509210586547852
Epoch 300, training loss: 870.857421875 = 0.9360814094543457 + 100.0 * 8.699213027954102
Epoch 300, val loss: 0.9418811798095703
Epoch 310, training loss: 869.4052124023438 = 0.9260979294776917 + 100.0 * 8.684791564941406
Epoch 310, val loss: 0.9324147701263428
Epoch 320, training loss: 868.1460571289062 = 0.9156800508499146 + 100.0 * 8.672303199768066
Epoch 320, val loss: 0.9225389957427979
Epoch 330, training loss: 866.9148559570312 = 0.9048227667808533 + 100.0 * 8.660099983215332
Epoch 330, val loss: 0.9122403264045715
Epoch 340, training loss: 865.6971435546875 = 0.8936240077018738 + 100.0 * 8.648035049438477
Epoch 340, val loss: 0.9016205072402954
Epoch 350, training loss: 864.5852661132812 = 0.8821200728416443 + 100.0 * 8.637031555175781
Epoch 350, val loss: 0.8907135128974915
Epoch 360, training loss: 863.5874633789062 = 0.8702991604804993 + 100.0 * 8.627171516418457
Epoch 360, val loss: 0.8795397281646729
Epoch 370, training loss: 862.4950561523438 = 0.8582057952880859 + 100.0 * 8.616368293762207
Epoch 370, val loss: 0.8681336045265198
Epoch 380, training loss: 861.5864868164062 = 0.8459382057189941 + 100.0 * 8.607405662536621
Epoch 380, val loss: 0.8565734624862671
Epoch 390, training loss: 860.9705200195312 = 0.8335565328598022 + 100.0 * 8.601369857788086
Epoch 390, val loss: 0.8448913097381592
Epoch 400, training loss: 859.968994140625 = 0.8210055232048035 + 100.0 * 8.591480255126953
Epoch 400, val loss: 0.8331161737442017
Epoch 410, training loss: 859.3531494140625 = 0.8085096478462219 + 100.0 * 8.58544635772705
Epoch 410, val loss: 0.8213679194450378
Epoch 420, training loss: 858.6671142578125 = 0.7960858345031738 + 100.0 * 8.578710556030273
Epoch 420, val loss: 0.8097444772720337
Epoch 430, training loss: 858.0931396484375 = 0.7837932705879211 + 100.0 * 8.57309341430664
Epoch 430, val loss: 0.7982607483863831
Epoch 440, training loss: 857.7214965820312 = 0.7716243267059326 + 100.0 * 8.569499015808105
Epoch 440, val loss: 0.7869178652763367
Epoch 450, training loss: 857.2391967773438 = 0.7595659494400024 + 100.0 * 8.564796447753906
Epoch 450, val loss: 0.7757620215415955
Epoch 460, training loss: 856.7960815429688 = 0.747780978679657 + 100.0 * 8.5604829788208
Epoch 460, val loss: 0.7648279070854187
Epoch 470, training loss: 856.4374389648438 = 0.7362253069877625 + 100.0 * 8.557012557983398
Epoch 470, val loss: 0.7541728019714355
Epoch 480, training loss: 856.2904052734375 = 0.7249473929405212 + 100.0 * 8.555654525756836
Epoch 480, val loss: 0.7437912225723267
Epoch 490, training loss: 855.9109497070312 = 0.7139909863471985 + 100.0 * 8.551969528198242
Epoch 490, val loss: 0.733729362487793
Epoch 500, training loss: 855.5651245117188 = 0.7033812403678894 + 100.0 * 8.548617362976074
Epoch 500, val loss: 0.724054217338562
Epoch 510, training loss: 855.3767700195312 = 0.6931610107421875 + 100.0 * 8.546835899353027
Epoch 510, val loss: 0.7147670984268188
Epoch 520, training loss: 855.2174072265625 = 0.6833301186561584 + 100.0 * 8.545340538024902
Epoch 520, val loss: 0.7058591246604919
Epoch 530, training loss: 854.9174194335938 = 0.6739460825920105 + 100.0 * 8.542434692382812
Epoch 530, val loss: 0.6973660588264465
Epoch 540, training loss: 854.6344604492188 = 0.6649954915046692 + 100.0 * 8.539694786071777
Epoch 540, val loss: 0.6893301010131836
Epoch 550, training loss: 854.7483520507812 = 0.6564562320709229 + 100.0 * 8.540919303894043
Epoch 550, val loss: 0.6817238926887512
Epoch 560, training loss: 854.1674194335938 = 0.648351788520813 + 100.0 * 8.53519058227539
Epoch 560, val loss: 0.6745262145996094
Epoch 570, training loss: 853.9483642578125 = 0.6407338380813599 + 100.0 * 8.533076286315918
Epoch 570, val loss: 0.667797863483429
Epoch 580, training loss: 853.7132568359375 = 0.6335424184799194 + 100.0 * 8.530797004699707
Epoch 580, val loss: 0.661518394947052
Epoch 590, training loss: 853.443603515625 = 0.6266809105873108 + 100.0 * 8.528168678283691
Epoch 590, val loss: 0.6555875539779663
Epoch 600, training loss: 853.1990356445312 = 0.6202086210250854 + 100.0 * 8.525788307189941
Epoch 600, val loss: 0.6499826312065125
Epoch 610, training loss: 852.9497680664062 = 0.6141980290412903 + 100.0 * 8.523355484008789
Epoch 610, val loss: 0.6448235511779785
Epoch 620, training loss: 852.6692504882812 = 0.6085562109947205 + 100.0 * 8.520606994628906
Epoch 620, val loss: 0.6400827169418335
Epoch 630, training loss: 852.396728515625 = 0.6032580733299255 + 100.0 * 8.517934799194336
Epoch 630, val loss: 0.635642409324646
Epoch 640, training loss: 852.1246948242188 = 0.5982918739318848 + 100.0 * 8.515264511108398
Epoch 640, val loss: 0.6315328478813171
Epoch 650, training loss: 852.2137451171875 = 0.5936418771743774 + 100.0 * 8.51620101928711
Epoch 650, val loss: 0.6277491450309753
Epoch 660, training loss: 851.6085205078125 = 0.58921879529953 + 100.0 * 8.51019287109375
Epoch 660, val loss: 0.6242114901542664
Epoch 670, training loss: 851.4209594726562 = 0.5850867629051208 + 100.0 * 8.5083589553833
Epoch 670, val loss: 0.6209065914154053
Epoch 680, training loss: 851.1445922851562 = 0.5812212824821472 + 100.0 * 8.505633354187012
Epoch 680, val loss: 0.6178913712501526
Epoch 690, training loss: 850.883056640625 = 0.5775845646858215 + 100.0 * 8.50305461883545
Epoch 690, val loss: 0.6151020526885986
Epoch 700, training loss: 851.60205078125 = 0.5741530656814575 + 100.0 * 8.510278701782227
Epoch 700, val loss: 0.6124972105026245
Epoch 710, training loss: 850.58447265625 = 0.5708563923835754 + 100.0 * 8.500136375427246
Epoch 710, val loss: 0.6100234985351562
Epoch 720, training loss: 850.2649536132812 = 0.567785918712616 + 100.0 * 8.496971130371094
Epoch 720, val loss: 0.607769787311554
Epoch 730, training loss: 850.0339965820312 = 0.5648876428604126 + 100.0 * 8.494690895080566
Epoch 730, val loss: 0.605741560459137
Epoch 740, training loss: 849.8530883789062 = 0.562142550945282 + 100.0 * 8.49290943145752
Epoch 740, val loss: 0.6038299798965454
Epoch 750, training loss: 850.0005493164062 = 0.5595276355743408 + 100.0 * 8.494410514831543
Epoch 750, val loss: 0.6020126342773438
Epoch 760, training loss: 850.0095825195312 = 0.5569884777069092 + 100.0 * 8.494525909423828
Epoch 760, val loss: 0.6003239750862122
Epoch 770, training loss: 849.44140625 = 0.5545785427093506 + 100.0 * 8.488868713378906
Epoch 770, val loss: 0.5986591577529907
Epoch 780, training loss: 849.1881713867188 = 0.5523010492324829 + 100.0 * 8.486358642578125
Epoch 780, val loss: 0.5972219109535217
Epoch 790, training loss: 848.9659423828125 = 0.5501360893249512 + 100.0 * 8.484158515930176
Epoch 790, val loss: 0.5958516597747803
Epoch 800, training loss: 848.8016357421875 = 0.5480753779411316 + 100.0 * 8.482535362243652
Epoch 800, val loss: 0.5945456624031067
Epoch 810, training loss: 848.63232421875 = 0.5461188554763794 + 100.0 * 8.48086166381836
Epoch 810, val loss: 0.5933650732040405
Epoch 820, training loss: 848.4896850585938 = 0.5442485809326172 + 100.0 * 8.479454040527344
Epoch 820, val loss: 0.5922374129295349
Epoch 830, training loss: 849.1671142578125 = 0.5424529910087585 + 100.0 * 8.486246109008789
Epoch 830, val loss: 0.5911706686019897
Epoch 840, training loss: 848.2252807617188 = 0.540688157081604 + 100.0 * 8.476845741271973
Epoch 840, val loss: 0.5901690125465393
Epoch 850, training loss: 848.1300659179688 = 0.5390399098396301 + 100.0 * 8.475910186767578
Epoch 850, val loss: 0.5892395377159119
Epoch 860, training loss: 847.8973388671875 = 0.5374568104743958 + 100.0 * 8.47359848022461
Epoch 860, val loss: 0.5883787870407104
Epoch 870, training loss: 847.7814331054688 = 0.5359311699867249 + 100.0 * 8.472455024719238
Epoch 870, val loss: 0.5875514149665833
Epoch 880, training loss: 847.733642578125 = 0.5344498753547668 + 100.0 * 8.471992492675781
Epoch 880, val loss: 0.5867722630500793
Epoch 890, training loss: 847.7461547851562 = 0.5329751968383789 + 100.0 * 8.472131729125977
Epoch 890, val loss: 0.5860980153083801
Epoch 900, training loss: 847.4793090820312 = 0.531548023223877 + 100.0 * 8.469477653503418
Epoch 900, val loss: 0.5853028297424316
Epoch 910, training loss: 847.3516235351562 = 0.5301823616027832 + 100.0 * 8.46821403503418
Epoch 910, val loss: 0.5846736431121826
Epoch 920, training loss: 847.2395629882812 = 0.5288757681846619 + 100.0 * 8.467106819152832
Epoch 920, val loss: 0.5840485095977783
Epoch 930, training loss: 847.1455078125 = 0.5276064276695251 + 100.0 * 8.466178894042969
Epoch 930, val loss: 0.5834876298904419
Epoch 940, training loss: 847.3947143554688 = 0.5263763666152954 + 100.0 * 8.468683242797852
Epoch 940, val loss: 0.582958996295929
Epoch 950, training loss: 847.1771850585938 = 0.5251245498657227 + 100.0 * 8.466520309448242
Epoch 950, val loss: 0.5824033617973328
Epoch 960, training loss: 846.9461059570312 = 0.523951530456543 + 100.0 * 8.464221954345703
Epoch 960, val loss: 0.5819560289382935
Epoch 970, training loss: 846.7702026367188 = 0.522807776927948 + 100.0 * 8.46247386932373
Epoch 970, val loss: 0.5814855694770813
Epoch 980, training loss: 846.688232421875 = 0.5217040777206421 + 100.0 * 8.461665153503418
Epoch 980, val loss: 0.5810394287109375
Epoch 990, training loss: 846.904296875 = 0.5206316709518433 + 100.0 * 8.463836669921875
Epoch 990, val loss: 0.5806090235710144
Epoch 1000, training loss: 846.6161499023438 = 0.5195422768592834 + 100.0 * 8.460966110229492
Epoch 1000, val loss: 0.5802767276763916
Epoch 1010, training loss: 846.5233764648438 = 0.5184915065765381 + 100.0 * 8.46004867553711
Epoch 1010, val loss: 0.5798242688179016
Epoch 1020, training loss: 846.36669921875 = 0.5174776315689087 + 100.0 * 8.458492279052734
Epoch 1020, val loss: 0.5794644355773926
Epoch 1030, training loss: 846.27685546875 = 0.5164967775344849 + 100.0 * 8.457603454589844
Epoch 1030, val loss: 0.5791427493095398
Epoch 1040, training loss: 846.1943969726562 = 0.5155346393585205 + 100.0 * 8.456788063049316
Epoch 1040, val loss: 0.5788096785545349
Epoch 1050, training loss: 846.1796875 = 0.5145914554595947 + 100.0 * 8.456650733947754
Epoch 1050, val loss: 0.5785170197486877
Epoch 1060, training loss: 846.3997802734375 = 0.5136476755142212 + 100.0 * 8.458861351013184
Epoch 1060, val loss: 0.5781821012496948
Epoch 1070, training loss: 845.9957275390625 = 0.5127096176147461 + 100.0 * 8.454830169677734
Epoch 1070, val loss: 0.5778828263282776
Epoch 1080, training loss: 845.909423828125 = 0.5118160247802734 + 100.0 * 8.453975677490234
Epoch 1080, val loss: 0.577566921710968
Epoch 1090, training loss: 845.8092041015625 = 0.5109549164772034 + 100.0 * 8.452982902526855
Epoch 1090, val loss: 0.5773338079452515
Epoch 1100, training loss: 845.906982421875 = 0.5101011991500854 + 100.0 * 8.45396900177002
Epoch 1100, val loss: 0.5770364999771118
Epoch 1110, training loss: 845.6809692382812 = 0.5092232823371887 + 100.0 * 8.451717376708984
Epoch 1110, val loss: 0.5768565535545349
Epoch 1120, training loss: 845.7349243164062 = 0.5083751678466797 + 100.0 * 8.452265739440918
Epoch 1120, val loss: 0.5765184760093689
Epoch 1130, training loss: 845.4868774414062 = 0.5075533390045166 + 100.0 * 8.449792861938477
Epoch 1130, val loss: 0.5763407945632935
Epoch 1140, training loss: 845.4608764648438 = 0.5067543983459473 + 100.0 * 8.449541091918945
Epoch 1140, val loss: 0.5761388540267944
Epoch 1150, training loss: 845.7283325195312 = 0.5059681534767151 + 100.0 * 8.452223777770996
Epoch 1150, val loss: 0.575921893119812
Epoch 1160, training loss: 845.3616333007812 = 0.5051712393760681 + 100.0 * 8.448564529418945
Epoch 1160, val loss: 0.5756948590278625
Epoch 1170, training loss: 845.24658203125 = 0.5043993592262268 + 100.0 * 8.44742202758789
Epoch 1170, val loss: 0.5754972100257874
Epoch 1180, training loss: 845.1284790039062 = 0.5036472678184509 + 100.0 * 8.446248054504395
Epoch 1180, val loss: 0.5752873420715332
Epoch 1190, training loss: 845.0661010742188 = 0.5029054880142212 + 100.0 * 8.445631980895996
Epoch 1190, val loss: 0.575128972530365
Epoch 1200, training loss: 845.0772094726562 = 0.5021751523017883 + 100.0 * 8.44575023651123
Epoch 1200, val loss: 0.5749755501747131
Epoch 1210, training loss: 845.7662963867188 = 0.501435399055481 + 100.0 * 8.452648162841797
Epoch 1210, val loss: 0.5746590495109558
Epoch 1220, training loss: 845.18603515625 = 0.5006633996963501 + 100.0 * 8.446853637695312
Epoch 1220, val loss: 0.574518084526062
Epoch 1230, training loss: 844.8748779296875 = 0.4999430477619171 + 100.0 * 8.44374942779541
Epoch 1230, val loss: 0.5744000673294067
Epoch 1240, training loss: 844.7643432617188 = 0.4992375075817108 + 100.0 * 8.44265079498291
Epoch 1240, val loss: 0.5741748809814453
Epoch 1250, training loss: 844.710205078125 = 0.49855053424835205 + 100.0 * 8.442116737365723
Epoch 1250, val loss: 0.5739954113960266
Epoch 1260, training loss: 844.6351318359375 = 0.4978681206703186 + 100.0 * 8.441372871398926
Epoch 1260, val loss: 0.573847234249115
Epoch 1270, training loss: 844.5845947265625 = 0.4971850514411926 + 100.0 * 8.440874099731445
Epoch 1270, val loss: 0.5737143158912659
Epoch 1280, training loss: 844.7000122070312 = 0.49649983644485474 + 100.0 * 8.442034721374512
Epoch 1280, val loss: 0.5735563039779663
Epoch 1290, training loss: 844.6063842773438 = 0.4958034157752991 + 100.0 * 8.441105842590332
Epoch 1290, val loss: 0.5734479427337646
Epoch 1300, training loss: 844.4902954101562 = 0.495109498500824 + 100.0 * 8.43995189666748
Epoch 1300, val loss: 0.5732002258300781
Epoch 1310, training loss: 844.4149780273438 = 0.4944496154785156 + 100.0 * 8.439205169677734
Epoch 1310, val loss: 0.5730392932891846
Epoch 1320, training loss: 844.353515625 = 0.4938015043735504 + 100.0 * 8.438597679138184
Epoch 1320, val loss: 0.5729256272315979
Epoch 1330, training loss: 844.39404296875 = 0.4931570589542389 + 100.0 * 8.439008712768555
Epoch 1330, val loss: 0.5727140307426453
Epoch 1340, training loss: 844.3317260742188 = 0.49249881505966187 + 100.0 * 8.438392639160156
Epoch 1340, val loss: 0.5725961327552795
Epoch 1350, training loss: 844.2145385742188 = 0.49184131622314453 + 100.0 * 8.437227249145508
Epoch 1350, val loss: 0.5724084377288818
Epoch 1360, training loss: 844.1913452148438 = 0.49120286107063293 + 100.0 * 8.43700122833252
Epoch 1360, val loss: 0.5722744464874268
Epoch 1370, training loss: 844.1218872070312 = 0.4905796945095062 + 100.0 * 8.436312675476074
Epoch 1370, val loss: 0.5721321105957031
Epoch 1380, training loss: 844.0676879882812 = 0.4899551570415497 + 100.0 * 8.43577766418457
Epoch 1380, val loss: 0.5720155239105225
Epoch 1390, training loss: 844.0452270507812 = 0.48933809995651245 + 100.0 * 8.435559272766113
Epoch 1390, val loss: 0.5719094276428223
Epoch 1400, training loss: 844.5082397460938 = 0.4887191653251648 + 100.0 * 8.440195083618164
Epoch 1400, val loss: 0.5718601942062378
Epoch 1410, training loss: 844.1251220703125 = 0.48806536197662354 + 100.0 * 8.436370849609375
Epoch 1410, val loss: 0.5715665221214294
Epoch 1420, training loss: 844.8077392578125 = 0.48743051290512085 + 100.0 * 8.44320297241211
Epoch 1420, val loss: 0.5715091228485107
Epoch 1430, training loss: 844.08349609375 = 0.48680707812309265 + 100.0 * 8.435966491699219
Epoch 1430, val loss: 0.5712935328483582
Epoch 1440, training loss: 843.8233642578125 = 0.4861890375614166 + 100.0 * 8.433371543884277
Epoch 1440, val loss: 0.5712677836418152
Epoch 1450, training loss: 843.776123046875 = 0.4855898916721344 + 100.0 * 8.432905197143555
Epoch 1450, val loss: 0.5710945129394531
Epoch 1460, training loss: 843.7224731445312 = 0.4849984049797058 + 100.0 * 8.432374954223633
Epoch 1460, val loss: 0.5709933638572693
Epoch 1470, training loss: 843.6796264648438 = 0.48440346121788025 + 100.0 * 8.431952476501465
Epoch 1470, val loss: 0.5708751082420349
Epoch 1480, training loss: 843.6424560546875 = 0.48380419611930847 + 100.0 * 8.431586265563965
Epoch 1480, val loss: 0.5707744359970093
Epoch 1490, training loss: 843.6223754882812 = 0.48320645093917847 + 100.0 * 8.431391716003418
Epoch 1490, val loss: 0.5706545114517212
Epoch 1500, training loss: 844.0829467773438 = 0.48260924220085144 + 100.0 * 8.436003684997559
Epoch 1500, val loss: 0.5705116987228394
Epoch 1510, training loss: 843.9993286132812 = 0.48199015855789185 + 100.0 * 8.435173034667969
Epoch 1510, val loss: 0.5703709721565247
Epoch 1520, training loss: 843.5006103515625 = 0.48136380314826965 + 100.0 * 8.430191993713379
Epoch 1520, val loss: 0.5702576637268066
Epoch 1530, training loss: 843.5274658203125 = 0.48076310753822327 + 100.0 * 8.430466651916504
Epoch 1530, val loss: 0.5701844096183777
Epoch 1540, training loss: 843.4253540039062 = 0.4801769256591797 + 100.0 * 8.429451942443848
Epoch 1540, val loss: 0.5700050592422485
Epoch 1550, training loss: 843.4161987304688 = 0.47959521412849426 + 100.0 * 8.429366111755371
Epoch 1550, val loss: 0.5698895454406738
Epoch 1560, training loss: 843.5848999023438 = 0.4790077805519104 + 100.0 * 8.431058883666992
Epoch 1560, val loss: 0.569757342338562
Epoch 1570, training loss: 843.3539428710938 = 0.47840049862861633 + 100.0 * 8.428755760192871
Epoch 1570, val loss: 0.5696571469306946
Epoch 1580, training loss: 843.3965454101562 = 0.4777996838092804 + 100.0 * 8.429187774658203
Epoch 1580, val loss: 0.5695883631706238
Epoch 1590, training loss: 843.249755859375 = 0.47720208764076233 + 100.0 * 8.427725791931152
Epoch 1590, val loss: 0.5694158673286438
Epoch 1600, training loss: 843.4030151367188 = 0.4766102731227875 + 100.0 * 8.429264068603516
Epoch 1600, val loss: 0.5693234205245972
Epoch 1610, training loss: 843.2517700195312 = 0.4759944677352905 + 100.0 * 8.427757263183594
Epoch 1610, val loss: 0.5691839456558228
Epoch 1620, training loss: 843.218505859375 = 0.4753771424293518 + 100.0 * 8.427431106567383
Epoch 1620, val loss: 0.5690809488296509
Epoch 1630, training loss: 843.1018676757812 = 0.4747810959815979 + 100.0 * 8.426270484924316
Epoch 1630, val loss: 0.5689916014671326
Epoch 1640, training loss: 843.431884765625 = 0.47417885065078735 + 100.0 * 8.429576873779297
Epoch 1640, val loss: 0.5689821839332581
Epoch 1650, training loss: 843.1641845703125 = 0.47353896498680115 + 100.0 * 8.42690658569336
Epoch 1650, val loss: 0.5686454176902771
Epoch 1660, training loss: 843.0576782226562 = 0.47293707728385925 + 100.0 * 8.425847053527832
Epoch 1660, val loss: 0.5686463713645935
Epoch 1670, training loss: 842.9612426757812 = 0.4723246097564697 + 100.0 * 8.424888610839844
Epoch 1670, val loss: 0.5684746503829956
Epoch 1680, training loss: 843.2622680664062 = 0.4717278778553009 + 100.0 * 8.427905082702637
Epoch 1680, val loss: 0.5684232115745544
Epoch 1690, training loss: 843.1035766601562 = 0.4710955321788788 + 100.0 * 8.426324844360352
Epoch 1690, val loss: 0.5681803226470947
Epoch 1700, training loss: 842.8897705078125 = 0.47046101093292236 + 100.0 * 8.424193382263184
Epoch 1700, val loss: 0.5681145787239075
Epoch 1710, training loss: 842.80517578125 = 0.4698522686958313 + 100.0 * 8.42335319519043
Epoch 1710, val loss: 0.5680450201034546
Epoch 1720, training loss: 842.763427734375 = 0.4692397713661194 + 100.0 * 8.422942161560059
Epoch 1720, val loss: 0.567894458770752
Epoch 1730, training loss: 842.7379760742188 = 0.4686298370361328 + 100.0 * 8.422693252563477
Epoch 1730, val loss: 0.5678436160087585
Epoch 1740, training loss: 843.7173461914062 = 0.46800634264945984 + 100.0 * 8.432493209838867
Epoch 1740, val loss: 0.5677960515022278
Epoch 1750, training loss: 843.0183715820312 = 0.46735963225364685 + 100.0 * 8.42551040649414
Epoch 1750, val loss: 0.5675168633460999
Epoch 1760, training loss: 842.6386108398438 = 0.46671998500823975 + 100.0 * 8.42171859741211
Epoch 1760, val loss: 0.5674931406974792
Epoch 1770, training loss: 842.5859375 = 0.4660990238189697 + 100.0 * 8.421197891235352
Epoch 1770, val loss: 0.5673677921295166
Epoch 1780, training loss: 842.5499267578125 = 0.46548622846603394 + 100.0 * 8.420844078063965
Epoch 1780, val loss: 0.5672633647918701
Epoch 1790, training loss: 842.507568359375 = 0.46486642956733704 + 100.0 * 8.420427322387695
Epoch 1790, val loss: 0.5671840906143188
Epoch 1800, training loss: 842.5994873046875 = 0.46424031257629395 + 100.0 * 8.42135238647461
Epoch 1800, val loss: 0.5670499205589294
Epoch 1810, training loss: 842.6915893554688 = 0.4635864794254303 + 100.0 * 8.422280311584473
Epoch 1810, val loss: 0.5670195817947388
Epoch 1820, training loss: 842.4843139648438 = 0.46292537450790405 + 100.0 * 8.42021369934082
Epoch 1820, val loss: 0.5667481422424316
Epoch 1830, training loss: 842.4549560546875 = 0.4622873365879059 + 100.0 * 8.419926643371582
Epoch 1830, val loss: 0.5667662024497986
Epoch 1840, training loss: 842.338623046875 = 0.46164843440055847 + 100.0 * 8.418769836425781
Epoch 1840, val loss: 0.5666716694831848
Epoch 1850, training loss: 842.3297729492188 = 0.4610074460506439 + 100.0 * 8.41868782043457
Epoch 1850, val loss: 0.5665571689605713
Epoch 1860, training loss: 842.8467407226562 = 0.46036165952682495 + 100.0 * 8.423863410949707
Epoch 1860, val loss: 0.5664557218551636
Epoch 1870, training loss: 842.4049072265625 = 0.45968008041381836 + 100.0 * 8.419452667236328
Epoch 1870, val loss: 0.5663828253746033
Epoch 1880, training loss: 842.2899780273438 = 0.45900627970695496 + 100.0 * 8.418310165405273
Epoch 1880, val loss: 0.5662736892700195
Epoch 1890, training loss: 842.2071533203125 = 0.4583442211151123 + 100.0 * 8.417488098144531
Epoch 1890, val loss: 0.566193163394928
Epoch 1900, training loss: 842.20849609375 = 0.4576725363731384 + 100.0 * 8.417508125305176
Epoch 1900, val loss: 0.5660801529884338
Epoch 1910, training loss: 842.55712890625 = 0.45698803663253784 + 100.0 * 8.421001434326172
Epoch 1910, val loss: 0.5660039782524109
Epoch 1920, training loss: 842.1082153320312 = 0.4562890827655792 + 100.0 * 8.416519165039062
Epoch 1920, val loss: 0.565779983997345
Epoch 1930, training loss: 842.0485229492188 = 0.4555946886539459 + 100.0 * 8.415929794311523
Epoch 1930, val loss: 0.5657248497009277
Epoch 1940, training loss: 842.0108642578125 = 0.4549047648906708 + 100.0 * 8.415559768676758
Epoch 1940, val loss: 0.5656094551086426
Epoch 1950, training loss: 842.00927734375 = 0.45421478152275085 + 100.0 * 8.415550231933594
Epoch 1950, val loss: 0.5655160546302795
Epoch 1960, training loss: 842.2821044921875 = 0.4535119831562042 + 100.0 * 8.418286323547363
Epoch 1960, val loss: 0.5653837323188782
Epoch 1970, training loss: 841.9721069335938 = 0.452792227268219 + 100.0 * 8.415192604064941
Epoch 1970, val loss: 0.5652645230293274
Epoch 1980, training loss: 841.972900390625 = 0.45208171010017395 + 100.0 * 8.415207862854004
Epoch 1980, val loss: 0.5651301741600037
Epoch 1990, training loss: 842.033447265625 = 0.45135778188705444 + 100.0 * 8.415821075439453
Epoch 1990, val loss: 0.5649836659431458
Epoch 2000, training loss: 841.839111328125 = 0.4506426453590393 + 100.0 * 8.413885116577148
Epoch 2000, val loss: 0.5649669170379639
Epoch 2010, training loss: 842.1983642578125 = 0.44992291927337646 + 100.0 * 8.417484283447266
Epoch 2010, val loss: 0.5647571086883545
Epoch 2020, training loss: 841.7686157226562 = 0.44915810227394104 + 100.0 * 8.41319465637207
Epoch 2020, val loss: 0.5647163987159729
Epoch 2030, training loss: 841.7059936523438 = 0.4484143555164337 + 100.0 * 8.412575721740723
Epoch 2030, val loss: 0.5646188259124756
Epoch 2040, training loss: 841.6608276367188 = 0.4476741552352905 + 100.0 * 8.412131309509277
Epoch 2040, val loss: 0.5645108819007874
Epoch 2050, training loss: 841.6380615234375 = 0.44693484902381897 + 100.0 * 8.411911010742188
Epoch 2050, val loss: 0.5644547343254089
Epoch 2060, training loss: 841.5879516601562 = 0.4461914598941803 + 100.0 * 8.411417961120605
Epoch 2060, val loss: 0.5643479228019714
Epoch 2070, training loss: 841.6209106445312 = 0.4454406499862671 + 100.0 * 8.411754608154297
Epoch 2070, val loss: 0.5642278790473938
Epoch 2080, training loss: 842.6145629882812 = 0.44467586278915405 + 100.0 * 8.421698570251465
Epoch 2080, val loss: 0.56412273645401
Epoch 2090, training loss: 841.7198486328125 = 0.4438183605670929 + 100.0 * 8.412759780883789
Epoch 2090, val loss: 0.5638929605484009
Epoch 2100, training loss: 841.5245971679688 = 0.4430285692214966 + 100.0 * 8.410815238952637
Epoch 2100, val loss: 0.5638613104820251
Epoch 2110, training loss: 841.5010986328125 = 0.4422428607940674 + 100.0 * 8.410588264465332
Epoch 2110, val loss: 0.5637302994728088
Epoch 2120, training loss: 841.4082641601562 = 0.44146478176116943 + 100.0 * 8.40966796875
Epoch 2120, val loss: 0.563651978969574
Epoch 2130, training loss: 841.3759765625 = 0.4406757354736328 + 100.0 * 8.409353256225586
Epoch 2130, val loss: 0.5635387301445007
Epoch 2140, training loss: 841.3430786132812 = 0.43987908959388733 + 100.0 * 8.409031867980957
Epoch 2140, val loss: 0.5634216666221619
Epoch 2150, training loss: 841.4100952148438 = 0.4390731453895569 + 100.0 * 8.409709930419922
Epoch 2150, val loss: 0.5632689595222473
Epoch 2160, training loss: 841.7894287109375 = 0.4382427930831909 + 100.0 * 8.413512229919434
Epoch 2160, val loss: 0.5631435513496399
Epoch 2170, training loss: 841.2825927734375 = 0.43737560510635376 + 100.0 * 8.408452033996582
Epoch 2170, val loss: 0.5630509853363037
Epoch 2180, training loss: 841.271728515625 = 0.4365300238132477 + 100.0 * 8.40835189819336
Epoch 2180, val loss: 0.5629822015762329
Epoch 2190, training loss: 841.2554931640625 = 0.4356944262981415 + 100.0 * 8.408198356628418
Epoch 2190, val loss: 0.5627702474594116
Epoch 2200, training loss: 841.3123168945312 = 0.43485748767852783 + 100.0 * 8.408774375915527
Epoch 2200, val loss: 0.5627412796020508
Epoch 2210, training loss: 841.510009765625 = 0.4339999854564667 + 100.0 * 8.410759925842285
Epoch 2210, val loss: 0.5625935792922974
Epoch 2220, training loss: 841.1614379882812 = 0.43313801288604736 + 100.0 * 8.407282829284668
Epoch 2220, val loss: 0.5624533295631409
Epoch 2230, training loss: 841.1181640625 = 0.4322742819786072 + 100.0 * 8.406859397888184
Epoch 2230, val loss: 0.5623229146003723
Epoch 2240, training loss: 841.0582885742188 = 0.4314071834087372 + 100.0 * 8.406269073486328
Epoch 2240, val loss: 0.5622256398200989
Epoch 2250, training loss: 841.1099853515625 = 0.4305327236652374 + 100.0 * 8.406794548034668
Epoch 2250, val loss: 0.562086820602417
Epoch 2260, training loss: 841.5155639648438 = 0.42963889241218567 + 100.0 * 8.410859107971191
Epoch 2260, val loss: 0.5619786381721497
Epoch 2270, training loss: 841.0120239257812 = 0.42872354388237 + 100.0 * 8.40583324432373
Epoch 2270, val loss: 0.5618262887001038
Epoch 2280, training loss: 840.98583984375 = 0.42781105637550354 + 100.0 * 8.405580520629883
Epoch 2280, val loss: 0.5616809725761414
Epoch 2290, training loss: 841.1514282226562 = 0.42689886689186096 + 100.0 * 8.407245635986328
Epoch 2290, val loss: 0.5615628957748413
Epoch 2300, training loss: 840.9528198242188 = 0.42597970366477966 + 100.0 * 8.405268669128418
Epoch 2300, val loss: 0.5614134073257446
Epoch 2310, training loss: 840.9067993164062 = 0.425051212310791 + 100.0 * 8.404817581176758
Epoch 2310, val loss: 0.5613114237785339
Epoch 2320, training loss: 840.9879760742188 = 0.42412009835243225 + 100.0 * 8.405638694763184
Epoch 2320, val loss: 0.5612335801124573
Epoch 2330, training loss: 841.1327514648438 = 0.42315730452537537 + 100.0 * 8.407095909118652
Epoch 2330, val loss: 0.561026930809021
Epoch 2340, training loss: 840.9454956054688 = 0.42218056321144104 + 100.0 * 8.405233383178711
Epoch 2340, val loss: 0.5608483552932739
Epoch 2350, training loss: 840.8251342773438 = 0.42121562361717224 + 100.0 * 8.40403938293457
Epoch 2350, val loss: 0.5607856512069702
Epoch 2360, training loss: 840.8003540039062 = 0.4202428460121155 + 100.0 * 8.403800964355469
Epoch 2360, val loss: 0.5606169104576111
Epoch 2370, training loss: 840.9445190429688 = 0.4192521870136261 + 100.0 * 8.405252456665039
Epoch 2370, val loss: 0.5604867339134216
Epoch 2380, training loss: 840.9788208007812 = 0.41823750734329224 + 100.0 * 8.405606269836426
Epoch 2380, val loss: 0.5604283213615417
Epoch 2390, training loss: 840.7662963867188 = 0.4172009229660034 + 100.0 * 8.403491020202637
Epoch 2390, val loss: 0.5602062344551086
Epoch 2400, training loss: 840.7234497070312 = 0.4161812365055084 + 100.0 * 8.403072357177734
Epoch 2400, val loss: 0.5601378679275513
Epoch 2410, training loss: 840.6401977539062 = 0.41514644026756287 + 100.0 * 8.402250289916992
Epoch 2410, val loss: 0.5600122809410095
Epoch 2420, training loss: 840.5985107421875 = 0.4141145348548889 + 100.0 * 8.401844024658203
Epoch 2420, val loss: 0.5598924160003662
Epoch 2430, training loss: 840.642578125 = 0.41307327151298523 + 100.0 * 8.402295112609863
Epoch 2430, val loss: 0.559798538684845
Epoch 2440, training loss: 841.118408203125 = 0.4120136797428131 + 100.0 * 8.407064437866211
Epoch 2440, val loss: 0.5597389936447144
Epoch 2450, training loss: 840.6574096679688 = 0.41092124581336975 + 100.0 * 8.402464866638184
Epoch 2450, val loss: 0.5595185160636902
Epoch 2460, training loss: 840.5719604492188 = 0.40983447432518005 + 100.0 * 8.401620864868164
Epoch 2460, val loss: 0.5594308972358704
Epoch 2470, training loss: 840.55322265625 = 0.40874749422073364 + 100.0 * 8.401444435119629
Epoch 2470, val loss: 0.5593592524528503
Epoch 2480, training loss: 840.658935546875 = 0.407647043466568 + 100.0 * 8.402512550354004
Epoch 2480, val loss: 0.5593035221099854
Epoch 2490, training loss: 840.4520263671875 = 0.40651512145996094 + 100.0 * 8.400455474853516
Epoch 2490, val loss: 0.5590823292732239
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.786910197869102
0.8154024487430269
=== training gcn model ===
Epoch 0, training loss: 1059.34326171875 = 1.1108615398406982 + 100.0 * 10.582324028015137
Epoch 0, val loss: 1.1124945878982544
Epoch 10, training loss: 1059.3179931640625 = 1.1061677932739258 + 100.0 * 10.582118034362793
Epoch 10, val loss: 1.1077940464019775
Epoch 20, training loss: 1059.216552734375 = 1.1011509895324707 + 100.0 * 10.581153869628906
Epoch 20, val loss: 1.1027523279190063
Epoch 30, training loss: 1058.7552490234375 = 1.095612645149231 + 100.0 * 10.5765962600708
Epoch 30, val loss: 1.097124457359314
Epoch 40, training loss: 1056.8182373046875 = 1.089285135269165 + 100.0 * 10.557290077209473
Epoch 40, val loss: 1.0906450748443604
Epoch 50, training loss: 1050.3817138671875 = 1.081895112991333 + 100.0 * 10.492998123168945
Epoch 50, val loss: 1.0830392837524414
Epoch 60, training loss: 1032.151123046875 = 1.0739176273345947 + 100.0 * 10.310770988464355
Epoch 60, val loss: 1.0748928785324097
Epoch 70, training loss: 989.131591796875 = 1.06550931930542 + 100.0 * 9.880661010742188
Epoch 70, val loss: 1.0662221908569336
Epoch 80, training loss: 970.7267456054688 = 1.0578306913375854 + 100.0 * 9.69668960571289
Epoch 80, val loss: 1.0586544275283813
Epoch 90, training loss: 963.7766723632812 = 1.0521717071533203 + 100.0 * 9.62724494934082
Epoch 90, val loss: 1.0530318021774292
Epoch 100, training loss: 959.5109252929688 = 1.0474369525909424 + 100.0 * 9.584634780883789
Epoch 100, val loss: 1.0483478307724
Epoch 110, training loss: 953.0531616210938 = 1.0437343120574951 + 100.0 * 9.52009391784668
Epoch 110, val loss: 1.044686198234558
Epoch 120, training loss: 941.3885498046875 = 1.040421485900879 + 100.0 * 9.403481483459473
Epoch 120, val loss: 1.0413912534713745
Epoch 130, training loss: 927.3768920898438 = 1.037628412246704 + 100.0 * 9.263392448425293
Epoch 130, val loss: 1.038719892501831
Epoch 140, training loss: 919.8907470703125 = 1.034958004951477 + 100.0 * 9.188557624816895
Epoch 140, val loss: 1.0360397100448608
Epoch 150, training loss: 913.1436157226562 = 1.0319465398788452 + 100.0 * 9.121116638183594
Epoch 150, val loss: 1.033089280128479
Epoch 160, training loss: 906.606689453125 = 1.0295805931091309 + 100.0 * 9.055770874023438
Epoch 160, val loss: 1.0308315753936768
Epoch 170, training loss: 903.3380737304688 = 1.0268982648849487 + 100.0 * 9.023111343383789
Epoch 170, val loss: 1.0280284881591797
Epoch 180, training loss: 900.4871215820312 = 1.0235686302185059 + 100.0 * 8.994635581970215
Epoch 180, val loss: 1.0247571468353271
Epoch 190, training loss: 897.3328247070312 = 1.0205659866333008 + 100.0 * 8.963122367858887
Epoch 190, val loss: 1.0218122005462646
Epoch 200, training loss: 893.558349609375 = 1.0176048278808594 + 100.0 * 8.925407409667969
Epoch 200, val loss: 1.0189249515533447
Epoch 210, training loss: 889.6570434570312 = 1.0146377086639404 + 100.0 * 8.88642406463623
Epoch 210, val loss: 1.0160045623779297
Epoch 220, training loss: 886.4990844726562 = 1.0114048719406128 + 100.0 * 8.854876518249512
Epoch 220, val loss: 1.0129196643829346
Epoch 230, training loss: 883.1095581054688 = 1.0079591274261475 + 100.0 * 8.821016311645508
Epoch 230, val loss: 1.0096502304077148
Epoch 240, training loss: 879.8051147460938 = 1.0042996406555176 + 100.0 * 8.788008689880371
Epoch 240, val loss: 1.0061458349227905
Epoch 250, training loss: 877.3735961914062 = 1.000207543373108 + 100.0 * 8.763733863830566
Epoch 250, val loss: 1.002204179763794
Epoch 260, training loss: 875.1895751953125 = 0.9955706000328064 + 100.0 * 8.74194049835205
Epoch 260, val loss: 0.9978036284446716
Epoch 270, training loss: 873.4429321289062 = 0.9906196594238281 + 100.0 * 8.724523544311523
Epoch 270, val loss: 0.9930360913276672
Epoch 280, training loss: 871.9266357421875 = 0.9854665994644165 + 100.0 * 8.70941162109375
Epoch 280, val loss: 0.9881275296211243
Epoch 290, training loss: 870.6034545898438 = 0.9800771474838257 + 100.0 * 8.696233749389648
Epoch 290, val loss: 0.9830067157745361
Epoch 300, training loss: 869.483642578125 = 0.9743418097496033 + 100.0 * 8.68509292602539
Epoch 300, val loss: 0.9775424003601074
Epoch 310, training loss: 868.3593139648438 = 0.968201756477356 + 100.0 * 8.673911094665527
Epoch 310, val loss: 0.9717333912849426
Epoch 320, training loss: 867.3644409179688 = 0.9617355465888977 + 100.0 * 8.664027214050293
Epoch 320, val loss: 0.9655994772911072
Epoch 330, training loss: 866.6454467773438 = 0.9548622965812683 + 100.0 * 8.656906127929688
Epoch 330, val loss: 0.9590743184089661
Epoch 340, training loss: 865.7978515625 = 0.9475181698799133 + 100.0 * 8.648503303527832
Epoch 340, val loss: 0.9521481990814209
Epoch 350, training loss: 865.0409545898438 = 0.939773678779602 + 100.0 * 8.641012191772461
Epoch 350, val loss: 0.9447930455207825
Epoch 360, training loss: 864.3734130859375 = 0.9316426515579224 + 100.0 * 8.634417533874512
Epoch 360, val loss: 0.9370973110198975
Epoch 370, training loss: 863.6322021484375 = 0.9231276512145996 + 100.0 * 8.627090454101562
Epoch 370, val loss: 0.9290986657142639
Epoch 380, training loss: 862.9336547851562 = 0.9143050312995911 + 100.0 * 8.620193481445312
Epoch 380, val loss: 0.9207947850227356
Epoch 390, training loss: 862.2557983398438 = 0.9051311612129211 + 100.0 * 8.613506317138672
Epoch 390, val loss: 0.9121829271316528
Epoch 400, training loss: 861.349609375 = 0.8957908749580383 + 100.0 * 8.604537963867188
Epoch 400, val loss: 0.9033725261688232
Epoch 410, training loss: 860.5256958007812 = 0.8861792087554932 + 100.0 * 8.596395492553711
Epoch 410, val loss: 0.894380509853363
Epoch 420, training loss: 859.7509765625 = 0.8763189911842346 + 100.0 * 8.588746070861816
Epoch 420, val loss: 0.8851619362831116
Epoch 430, training loss: 859.3665771484375 = 0.8662300705909729 + 100.0 * 8.585003852844238
Epoch 430, val loss: 0.875715970993042
Epoch 440, training loss: 858.6586303710938 = 0.8558289408683777 + 100.0 * 8.578027725219727
Epoch 440, val loss: 0.8660094141960144
Epoch 450, training loss: 857.8167114257812 = 0.845268726348877 + 100.0 * 8.569714546203613
Epoch 450, val loss: 0.8561923503875732
Epoch 460, training loss: 857.1646728515625 = 0.834632396697998 + 100.0 * 8.563300132751465
Epoch 460, val loss: 0.8463143706321716
Epoch 470, training loss: 856.91064453125 = 0.8239263892173767 + 100.0 * 8.560867309570312
Epoch 470, val loss: 0.8363754749298096
Epoch 480, training loss: 856.051025390625 = 0.8131446242332458 + 100.0 * 8.55237865447998
Epoch 480, val loss: 0.8264601826667786
Epoch 490, training loss: 855.5665283203125 = 0.8024231791496277 + 100.0 * 8.547640800476074
Epoch 490, val loss: 0.8166082501411438
Epoch 500, training loss: 855.1767578125 = 0.7917755246162415 + 100.0 * 8.54384994506836
Epoch 500, val loss: 0.8068817257881165
Epoch 510, training loss: 854.6973266601562 = 0.7812336683273315 + 100.0 * 8.53916072845459
Epoch 510, val loss: 0.7972158193588257
Epoch 520, training loss: 854.3003540039062 = 0.770858645439148 + 100.0 * 8.535294532775879
Epoch 520, val loss: 0.7878135442733765
Epoch 530, training loss: 853.8731079101562 = 0.7607088088989258 + 100.0 * 8.531124114990234
Epoch 530, val loss: 0.7786086201667786
Epoch 540, training loss: 853.5474243164062 = 0.7507837414741516 + 100.0 * 8.527966499328613
Epoch 540, val loss: 0.7696763873100281
Epoch 550, training loss: 853.3681640625 = 0.741131603717804 + 100.0 * 8.526269912719727
Epoch 550, val loss: 0.7610288262367249
Epoch 560, training loss: 853.0830078125 = 0.7317428588867188 + 100.0 * 8.523512840270996
Epoch 560, val loss: 0.7526202201843262
Epoch 570, training loss: 852.7427368164062 = 0.722682535648346 + 100.0 * 8.520200729370117
Epoch 570, val loss: 0.7446141839027405
Epoch 580, training loss: 852.4718017578125 = 0.71397465467453 + 100.0 * 8.517578125
Epoch 580, val loss: 0.7369450330734253
Epoch 590, training loss: 852.1929931640625 = 0.7056409120559692 + 100.0 * 8.514873504638672
Epoch 590, val loss: 0.7296289801597595
Epoch 600, training loss: 851.9878540039062 = 0.6976652145385742 + 100.0 * 8.512901306152344
Epoch 600, val loss: 0.7226797938346863
Epoch 610, training loss: 852.52099609375 = 0.6900506615638733 + 100.0 * 8.518309593200684
Epoch 610, val loss: 0.716026246547699
Epoch 620, training loss: 851.6195678710938 = 0.6826997995376587 + 100.0 * 8.509368896484375
Epoch 620, val loss: 0.7097715735435486
Epoch 630, training loss: 851.4960327148438 = 0.6757742762565613 + 100.0 * 8.50820255279541
Epoch 630, val loss: 0.7038918137550354
Epoch 640, training loss: 851.3409423828125 = 0.6692101955413818 + 100.0 * 8.506717681884766
Epoch 640, val loss: 0.6983596682548523
Epoch 650, training loss: 851.1766357421875 = 0.6629840135574341 + 100.0 * 8.505136489868164
Epoch 650, val loss: 0.6931583285331726
Epoch 660, training loss: 851.0595703125 = 0.6570754051208496 + 100.0 * 8.504024505615234
Epoch 660, val loss: 0.6882527470588684
Epoch 670, training loss: 851.2940673828125 = 0.6514609456062317 + 100.0 * 8.506425857543945
Epoch 670, val loss: 0.6836239099502563
Epoch 680, training loss: 850.9740600585938 = 0.646125316619873 + 100.0 * 8.503279685974121
Epoch 680, val loss: 0.6793249845504761
Epoch 690, training loss: 850.7728271484375 = 0.6411051750183105 + 100.0 * 8.501317024230957
Epoch 690, val loss: 0.6753260493278503
Epoch 700, training loss: 850.6380004882812 = 0.636359691619873 + 100.0 * 8.500016212463379
Epoch 700, val loss: 0.6715880632400513
Epoch 710, training loss: 850.5740356445312 = 0.6318646669387817 + 100.0 * 8.499422073364258
Epoch 710, val loss: 0.6680953502655029
Epoch 720, training loss: 850.5740356445312 = 0.627598762512207 + 100.0 * 8.49946403503418
Epoch 720, val loss: 0.6648305058479309
Epoch 730, training loss: 850.3682250976562 = 0.6235698461532593 + 100.0 * 8.497446060180664
Epoch 730, val loss: 0.6617956161499023
Epoch 740, training loss: 850.2455444335938 = 0.6197528839111328 + 100.0 * 8.496257781982422
Epoch 740, val loss: 0.6589698791503906
Epoch 750, training loss: 850.1484985351562 = 0.6161436438560486 + 100.0 * 8.495323181152344
Epoch 750, val loss: 0.6563349366188049
Epoch 760, training loss: 850.4752197265625 = 0.6127217411994934 + 100.0 * 8.498624801635742
Epoch 760, val loss: 0.6538363695144653
Epoch 770, training loss: 850.1323852539062 = 0.6094362735748291 + 100.0 * 8.495229721069336
Epoch 770, val loss: 0.651569664478302
Epoch 780, training loss: 849.899658203125 = 0.6063500642776489 + 100.0 * 8.49293327331543
Epoch 780, val loss: 0.6494462490081787
Epoch 790, training loss: 849.7960815429688 = 0.6034401655197144 + 100.0 * 8.491926193237305
Epoch 790, val loss: 0.6474966406822205
Epoch 800, training loss: 849.6685791015625 = 0.600682258605957 + 100.0 * 8.490678787231445
Epoch 800, val loss: 0.645696222782135
Epoch 810, training loss: 849.6161499023438 = 0.5980586409568787 + 100.0 * 8.490180969238281
Epoch 810, val loss: 0.6440157890319824
Epoch 820, training loss: 849.57958984375 = 0.5955249071121216 + 100.0 * 8.489840507507324
Epoch 820, val loss: 0.6424076557159424
Epoch 830, training loss: 849.5008544921875 = 0.5931087732315063 + 100.0 * 8.4890775680542
Epoch 830, val loss: 0.6409342885017395
Epoch 840, training loss: 849.2683715820312 = 0.5908350944519043 + 100.0 * 8.486775398254395
Epoch 840, val loss: 0.6395688056945801
Epoch 850, training loss: 849.4276733398438 = 0.5886667966842651 + 100.0 * 8.48838996887207
Epoch 850, val loss: 0.6382983922958374
Epoch 860, training loss: 849.1068725585938 = 0.5865836143493652 + 100.0 * 8.48520278930664
Epoch 860, val loss: 0.6372227072715759
Epoch 870, training loss: 848.9197387695312 = 0.5845997333526611 + 100.0 * 8.483351707458496
Epoch 870, val loss: 0.636104166507721
Epoch 880, training loss: 848.7778930664062 = 0.5827082991600037 + 100.0 * 8.481951713562012
Epoch 880, val loss: 0.6351522207260132
Epoch 890, training loss: 848.6406860351562 = 0.5808932781219482 + 100.0 * 8.480598449707031
Epoch 890, val loss: 0.6342540383338928
Epoch 900, training loss: 849.1041870117188 = 0.5791563987731934 + 100.0 * 8.485250473022461
Epoch 900, val loss: 0.6333703398704529
Epoch 910, training loss: 848.5703125 = 0.5774206519126892 + 100.0 * 8.479928970336914
Epoch 910, val loss: 0.6325807571411133
Epoch 920, training loss: 848.3529663085938 = 0.5757930278778076 + 100.0 * 8.477771759033203
Epoch 920, val loss: 0.6318851709365845
Epoch 930, training loss: 848.1309814453125 = 0.5742571353912354 + 100.0 * 8.475566864013672
Epoch 930, val loss: 0.6312297582626343
Epoch 940, training loss: 847.9577026367188 = 0.5727836489677429 + 100.0 * 8.473849296569824
Epoch 940, val loss: 0.6306368708610535
Epoch 950, training loss: 848.5721435546875 = 0.5713592767715454 + 100.0 * 8.480008125305176
Epoch 950, val loss: 0.6299945116043091
Epoch 960, training loss: 847.8281860351562 = 0.5699098706245422 + 100.0 * 8.472582817077637
Epoch 960, val loss: 0.6295627355575562
Epoch 970, training loss: 847.7763671875 = 0.5685474276542664 + 100.0 * 8.472078323364258
Epoch 970, val loss: 0.6290794610977173
Epoch 980, training loss: 847.4031372070312 = 0.5672451853752136 + 100.0 * 8.468358993530273
Epoch 980, val loss: 0.6286301016807556
Epoch 990, training loss: 847.3511352539062 = 0.5660025477409363 + 100.0 * 8.467851638793945
Epoch 990, val loss: 0.6282402873039246
Epoch 1000, training loss: 847.2310791015625 = 0.5647884011268616 + 100.0 * 8.466663360595703
Epoch 1000, val loss: 0.6278742551803589
Epoch 1010, training loss: 846.9874267578125 = 0.5636057257652283 + 100.0 * 8.464238166809082
Epoch 1010, val loss: 0.6275310516357422
Epoch 1020, training loss: 846.855224609375 = 0.5624588131904602 + 100.0 * 8.46292781829834
Epoch 1020, val loss: 0.627220630645752
Epoch 1030, training loss: 847.61962890625 = 0.5613393187522888 + 100.0 * 8.470582962036133
Epoch 1030, val loss: 0.6269401907920837
Epoch 1040, training loss: 846.9234619140625 = 0.5601852536201477 + 100.0 * 8.463632583618164
Epoch 1040, val loss: 0.6266149878501892
Epoch 1050, training loss: 846.603271484375 = 0.5591012239456177 + 100.0 * 8.460441589355469
Epoch 1050, val loss: 0.6263509392738342
Epoch 1060, training loss: 846.391845703125 = 0.5580569505691528 + 100.0 * 8.458337783813477
Epoch 1060, val loss: 0.6260936856269836
Epoch 1070, training loss: 846.314208984375 = 0.5570312142372131 + 100.0 * 8.457571983337402
Epoch 1070, val loss: 0.6258552074432373
Epoch 1080, training loss: 846.5958251953125 = 0.5560177564620972 + 100.0 * 8.460397720336914
Epoch 1080, val loss: 0.6255534887313843
Epoch 1090, training loss: 846.1270751953125 = 0.5549981594085693 + 100.0 * 8.455720901489258
Epoch 1090, val loss: 0.625412106513977
Epoch 1100, training loss: 846.0309448242188 = 0.5540213584899902 + 100.0 * 8.454769134521484
Epoch 1100, val loss: 0.6251698732376099
Epoch 1110, training loss: 845.901123046875 = 0.5530649423599243 + 100.0 * 8.45348072052002
Epoch 1110, val loss: 0.6250258088111877
Epoch 1120, training loss: 846.2470703125 = 0.5521181225776672 + 100.0 * 8.456949234008789
Epoch 1120, val loss: 0.6249188184738159
Epoch 1130, training loss: 845.9202270507812 = 0.5511548519134521 + 100.0 * 8.453690528869629
Epoch 1130, val loss: 0.6245470643043518
Epoch 1140, training loss: 845.6282348632812 = 0.5502293109893799 + 100.0 * 8.450779914855957
Epoch 1140, val loss: 0.624383807182312
Epoch 1150, training loss: 845.5579833984375 = 0.549328625202179 + 100.0 * 8.45008659362793
Epoch 1150, val loss: 0.6242702007293701
Epoch 1160, training loss: 845.4679565429688 = 0.5484501123428345 + 100.0 * 8.44919490814209
Epoch 1160, val loss: 0.6241040229797363
Epoch 1170, training loss: 845.6995239257812 = 0.5475792288780212 + 100.0 * 8.451519012451172
Epoch 1170, val loss: 0.6238932609558105
Epoch 1180, training loss: 845.8590087890625 = 0.5466868877410889 + 100.0 * 8.453123092651367
Epoch 1180, val loss: 0.6237729787826538
Epoch 1190, training loss: 845.324951171875 = 0.5457997918128967 + 100.0 * 8.447792053222656
Epoch 1190, val loss: 0.6236897706985474
Epoch 1200, training loss: 845.1832275390625 = 0.5449587106704712 + 100.0 * 8.446382522583008
Epoch 1200, val loss: 0.6234761476516724
Epoch 1210, training loss: 845.0399780273438 = 0.5441306829452515 + 100.0 * 8.444958686828613
Epoch 1210, val loss: 0.6234018802642822
Epoch 1220, training loss: 845.046630859375 = 0.5433177947998047 + 100.0 * 8.445033073425293
Epoch 1220, val loss: 0.6233522295951843
Epoch 1230, training loss: 845.1103515625 = 0.5424915552139282 + 100.0 * 8.4456787109375
Epoch 1230, val loss: 0.6231753826141357
Epoch 1240, training loss: 844.837646484375 = 0.5416669845581055 + 100.0 * 8.442959785461426
Epoch 1240, val loss: 0.6230049729347229
Epoch 1250, training loss: 844.7741088867188 = 0.5408677458763123 + 100.0 * 8.44233226776123
Epoch 1250, val loss: 0.6228529214859009
Epoch 1260, training loss: 844.7232666015625 = 0.540086030960083 + 100.0 * 8.441831588745117
Epoch 1260, val loss: 0.6227906942367554
Epoch 1270, training loss: 845.3572387695312 = 0.5393019318580627 + 100.0 * 8.448179244995117
Epoch 1270, val loss: 0.6225992441177368
Epoch 1280, training loss: 844.6553955078125 = 0.5384764671325684 + 100.0 * 8.441169738769531
Epoch 1280, val loss: 0.6225312948226929
Epoch 1290, training loss: 844.483154296875 = 0.5376982092857361 + 100.0 * 8.439454078674316
Epoch 1290, val loss: 0.6224290132522583
Epoch 1300, training loss: 844.4205322265625 = 0.5369353890419006 + 100.0 * 8.438836097717285
Epoch 1300, val loss: 0.622319221496582
Epoch 1310, training loss: 844.5467529296875 = 0.536178469657898 + 100.0 * 8.440105438232422
Epoch 1310, val loss: 0.6222694516181946
Epoch 1320, training loss: 844.2703247070312 = 0.5353983044624329 + 100.0 * 8.437349319458008
Epoch 1320, val loss: 0.6221238970756531
Epoch 1330, training loss: 844.230224609375 = 0.5346367955207825 + 100.0 * 8.436956405639648
Epoch 1330, val loss: 0.6219973564147949
Epoch 1340, training loss: 844.162353515625 = 0.5338882803916931 + 100.0 * 8.436285018920898
Epoch 1340, val loss: 0.6219339966773987
Epoch 1350, training loss: 844.078369140625 = 0.5331515669822693 + 100.0 * 8.435452461242676
Epoch 1350, val loss: 0.6218149065971375
Epoch 1360, training loss: 844.0508422851562 = 0.5324171781539917 + 100.0 * 8.435184478759766
Epoch 1360, val loss: 0.6216913461685181
Epoch 1370, training loss: 844.652587890625 = 0.5316851735115051 + 100.0 * 8.441208839416504
Epoch 1370, val loss: 0.6214422583580017
Epoch 1380, training loss: 844.3870239257812 = 0.5308954119682312 + 100.0 * 8.43856143951416
Epoch 1380, val loss: 0.62149977684021
Epoch 1390, training loss: 843.8951416015625 = 0.5301415920257568 + 100.0 * 8.433650016784668
Epoch 1390, val loss: 0.6213792562484741
Epoch 1400, training loss: 843.8413696289062 = 0.5294166803359985 + 100.0 * 8.433119773864746
Epoch 1400, val loss: 0.6212037205696106
Epoch 1410, training loss: 843.7273559570312 = 0.5287060737609863 + 100.0 * 8.431986808776855
Epoch 1410, val loss: 0.6211407780647278
Epoch 1420, training loss: 843.67431640625 = 0.5280004143714905 + 100.0 * 8.431463241577148
Epoch 1420, val loss: 0.6210383772850037
Epoch 1430, training loss: 843.7422485351562 = 0.5272940397262573 + 100.0 * 8.432149887084961
Epoch 1430, val loss: 0.6209315657615662
Epoch 1440, training loss: 843.9281616210938 = 0.5265617966651917 + 100.0 * 8.434016227722168
Epoch 1440, val loss: 0.6209404468536377
Epoch 1450, training loss: 843.7843627929688 = 0.5258398652076721 + 100.0 * 8.432585716247559
Epoch 1450, val loss: 0.6205876469612122
Epoch 1460, training loss: 843.634033203125 = 0.5251092910766602 + 100.0 * 8.431089401245117
Epoch 1460, val loss: 0.6205397248268127
Epoch 1470, training loss: 843.4700927734375 = 0.5243961215019226 + 100.0 * 8.42945671081543
Epoch 1470, val loss: 0.6204032301902771
Epoch 1480, training loss: 843.4317626953125 = 0.5236955285072327 + 100.0 * 8.429080963134766
Epoch 1480, val loss: 0.6203160285949707
Epoch 1490, training loss: 843.4820556640625 = 0.5229940414428711 + 100.0 * 8.429590225219727
Epoch 1490, val loss: 0.6201536655426025
Epoch 1500, training loss: 843.3536987304688 = 0.5222851634025574 + 100.0 * 8.428314208984375
Epoch 1500, val loss: 0.6199977993965149
Epoch 1510, training loss: 843.9310913085938 = 0.5215756297111511 + 100.0 * 8.43409538269043
Epoch 1510, val loss: 0.6199074983596802
Epoch 1520, training loss: 843.3831176757812 = 0.5208388566970825 + 100.0 * 8.42862319946289
Epoch 1520, val loss: 0.6196187734603882
Epoch 1530, training loss: 843.17578125 = 0.5201299786567688 + 100.0 * 8.426556587219238
Epoch 1530, val loss: 0.6195176839828491
Epoch 1540, training loss: 843.0968017578125 = 0.5194365382194519 + 100.0 * 8.425773620605469
Epoch 1540, val loss: 0.6193897724151611
Epoch 1550, training loss: 843.0382080078125 = 0.5187519192695618 + 100.0 * 8.42519474029541
Epoch 1550, val loss: 0.6192498207092285
Epoch 1560, training loss: 843.0062866210938 = 0.518061637878418 + 100.0 * 8.424881935119629
Epoch 1560, val loss: 0.619122326374054
Epoch 1570, training loss: 843.3460693359375 = 0.5173749923706055 + 100.0 * 8.4282865524292
Epoch 1570, val loss: 0.6189075708389282
Epoch 1580, training loss: 843.1180419921875 = 0.5166420936584473 + 100.0 * 8.426013946533203
Epoch 1580, val loss: 0.6188764572143555
Epoch 1590, training loss: 842.9940185546875 = 0.5159307718276978 + 100.0 * 8.42478084564209
Epoch 1590, val loss: 0.6187184453010559
Epoch 1600, training loss: 842.8470458984375 = 0.5152409076690674 + 100.0 * 8.423317909240723
Epoch 1600, val loss: 0.6186023354530334
Epoch 1610, training loss: 842.8197631835938 = 0.5145621299743652 + 100.0 * 8.423051834106445
Epoch 1610, val loss: 0.6184231638908386
Epoch 1620, training loss: 843.4715576171875 = 0.5138723254203796 + 100.0 * 8.429576873779297
Epoch 1620, val loss: 0.6182547807693481
Epoch 1630, training loss: 842.8855590820312 = 0.5131424069404602 + 100.0 * 8.423724174499512
Epoch 1630, val loss: 0.6181831955909729
Epoch 1640, training loss: 842.6658325195312 = 0.5124492049217224 + 100.0 * 8.421533584594727
Epoch 1640, val loss: 0.6179709434509277
Epoch 1650, training loss: 842.6043701171875 = 0.5117677450180054 + 100.0 * 8.420926094055176
Epoch 1650, val loss: 0.6178078055381775
Epoch 1660, training loss: 842.56396484375 = 0.5110882520675659 + 100.0 * 8.420528411865234
Epoch 1660, val loss: 0.6176535487174988
Epoch 1670, training loss: 842.60400390625 = 0.5104061961174011 + 100.0 * 8.420936584472656
Epoch 1670, val loss: 0.6174886226654053
Epoch 1680, training loss: 842.9515380859375 = 0.5097049474716187 + 100.0 * 8.424418449401855
Epoch 1680, val loss: 0.6172106266021729
Epoch 1690, training loss: 842.6218872070312 = 0.5089675188064575 + 100.0 * 8.42112922668457
Epoch 1690, val loss: 0.6170236468315125
Epoch 1700, training loss: 842.4654541015625 = 0.5082569122314453 + 100.0 * 8.419571876525879
Epoch 1700, val loss: 0.6168368458747864
Epoch 1710, training loss: 842.3895263671875 = 0.5075527429580688 + 100.0 * 8.418819427490234
Epoch 1710, val loss: 0.6166877746582031
Epoch 1720, training loss: 842.3321533203125 = 0.5068625211715698 + 100.0 * 8.418252944946289
Epoch 1720, val loss: 0.616503119468689
Epoch 1730, training loss: 842.2870483398438 = 0.506168782711029 + 100.0 * 8.417808532714844
Epoch 1730, val loss: 0.616353452205658
Epoch 1740, training loss: 842.43017578125 = 0.5054707527160645 + 100.0 * 8.419246673583984
Epoch 1740, val loss: 0.6162556409835815
Epoch 1750, training loss: 842.6734008789062 = 0.5047271847724915 + 100.0 * 8.421687126159668
Epoch 1750, val loss: 0.6160836815834045
Epoch 1760, training loss: 842.3078002929688 = 0.5039680600166321 + 100.0 * 8.418038368225098
Epoch 1760, val loss: 0.6157439351081848
Epoch 1770, training loss: 842.2315673828125 = 0.5032511353492737 + 100.0 * 8.417283058166504
Epoch 1770, val loss: 0.6155366897583008
Epoch 1780, training loss: 842.1105346679688 = 0.5025460720062256 + 100.0 * 8.4160795211792
Epoch 1780, val loss: 0.6153388023376465
Epoch 1790, training loss: 842.0734252929688 = 0.5018464922904968 + 100.0 * 8.415716171264648
Epoch 1790, val loss: 0.615210235118866
Epoch 1800, training loss: 842.0316772460938 = 0.501143217086792 + 100.0 * 8.415305137634277
Epoch 1800, val loss: 0.6149970889091492
Epoch 1810, training loss: 842.0374145507812 = 0.5004311800003052 + 100.0 * 8.415369987487793
Epoch 1810, val loss: 0.6148064732551575
Epoch 1820, training loss: 842.6622924804688 = 0.4997037649154663 + 100.0 * 8.421626091003418
Epoch 1820, val loss: 0.6146352291107178
Epoch 1830, training loss: 842.1454467773438 = 0.4989364743232727 + 100.0 * 8.416464805603027
Epoch 1830, val loss: 0.6143319606781006
Epoch 1840, training loss: 841.951416015625 = 0.49820080399513245 + 100.0 * 8.414531707763672
Epoch 1840, val loss: 0.6141332983970642
Epoch 1850, training loss: 841.8834228515625 = 0.49747171998023987 + 100.0 * 8.413859367370605
Epoch 1850, val loss: 0.6139057278633118
Epoch 1860, training loss: 841.8624877929688 = 0.49674245715141296 + 100.0 * 8.413657188415527
Epoch 1860, val loss: 0.6137060523033142
Epoch 1870, training loss: 842.1651000976562 = 0.49600571393966675 + 100.0 * 8.416690826416016
Epoch 1870, val loss: 0.6134111881256104
Epoch 1880, training loss: 841.8670654296875 = 0.49523746967315674 + 100.0 * 8.413718223571777
Epoch 1880, val loss: 0.613182544708252
Epoch 1890, training loss: 841.795654296875 = 0.49448537826538086 + 100.0 * 8.41301155090332
Epoch 1890, val loss: 0.6128538846969604
Epoch 1900, training loss: 841.9113159179688 = 0.49373579025268555 + 100.0 * 8.414175987243652
Epoch 1900, val loss: 0.6125581860542297
Epoch 1910, training loss: 841.8504638671875 = 0.4929494261741638 + 100.0 * 8.413575172424316
Epoch 1910, val loss: 0.6123722791671753
Epoch 1920, training loss: 841.675048828125 = 0.49214646220207214 + 100.0 * 8.411828994750977
Epoch 1920, val loss: 0.612019419670105
Epoch 1930, training loss: 841.6785888671875 = 0.4913676381111145 + 100.0 * 8.411871910095215
Epoch 1930, val loss: 0.611861526966095
Epoch 1940, training loss: 841.630859375 = 0.4906063377857208 + 100.0 * 8.411402702331543
Epoch 1940, val loss: 0.6114975810050964
Epoch 1950, training loss: 841.59375 = 0.4898322820663452 + 100.0 * 8.411039352416992
Epoch 1950, val loss: 0.6112782955169678
Epoch 1960, training loss: 841.63623046875 = 0.4890565872192383 + 100.0 * 8.411471366882324
Epoch 1960, val loss: 0.610980749130249
Epoch 1970, training loss: 841.95654296875 = 0.4882562458515167 + 100.0 * 8.414682388305664
Epoch 1970, val loss: 0.610660970211029
Epoch 1980, training loss: 841.5599975585938 = 0.4874233603477478 + 100.0 * 8.410725593566895
Epoch 1980, val loss: 0.6103900074958801
Epoch 1990, training loss: 841.5372314453125 = 0.4866190254688263 + 100.0 * 8.410506248474121
Epoch 1990, val loss: 0.6100160479545593
Epoch 2000, training loss: 841.490234375 = 0.4858149290084839 + 100.0 * 8.410043716430664
Epoch 2000, val loss: 0.6097891926765442
Epoch 2010, training loss: 841.4711303710938 = 0.48502248525619507 + 100.0 * 8.409860610961914
Epoch 2010, val loss: 0.6094602942466736
Epoch 2020, training loss: 841.624267578125 = 0.4842199981212616 + 100.0 * 8.41140079498291
Epoch 2020, val loss: 0.6091493964195251
Epoch 2030, training loss: 841.5366821289062 = 0.4833831489086151 + 100.0 * 8.41053295135498
Epoch 2030, val loss: 0.6088626384735107
Epoch 2040, training loss: 841.5036010742188 = 0.48253747820854187 + 100.0 * 8.410210609436035
Epoch 2040, val loss: 0.6086329221725464
Epoch 2050, training loss: 841.7230224609375 = 0.4817051887512207 + 100.0 * 8.412413597106934
Epoch 2050, val loss: 0.6082744002342224
Epoch 2060, training loss: 841.4152221679688 = 0.48084360361099243 + 100.0 * 8.409343719482422
Epoch 2060, val loss: 0.6080198884010315
Epoch 2070, training loss: 841.3737182617188 = 0.47999677062034607 + 100.0 * 8.408937454223633
Epoch 2070, val loss: 0.607700526714325
Epoch 2080, training loss: 841.3140258789062 = 0.47915366291999817 + 100.0 * 8.40834903717041
Epoch 2080, val loss: 0.6074241995811462
Epoch 2090, training loss: 841.2745971679688 = 0.4783095121383667 + 100.0 * 8.407962799072266
Epoch 2090, val loss: 0.6071527004241943
Epoch 2100, training loss: 841.4225463867188 = 0.4774636924266815 + 100.0 * 8.40945053100586
Epoch 2100, val loss: 0.6067850589752197
Epoch 2110, training loss: 841.433349609375 = 0.47657859325408936 + 100.0 * 8.409567832946777
Epoch 2110, val loss: 0.6064950823783875
Epoch 2120, training loss: 841.2940673828125 = 0.47567346692085266 + 100.0 * 8.408184051513672
Epoch 2120, val loss: 0.606255054473877
Epoch 2130, training loss: 841.2500610351562 = 0.47479602694511414 + 100.0 * 8.407752990722656
Epoch 2130, val loss: 0.6058857440948486
Epoch 2140, training loss: 841.236572265625 = 0.4739135205745697 + 100.0 * 8.407626152038574
Epoch 2140, val loss: 0.605552613735199
Epoch 2150, training loss: 841.284912109375 = 0.4730265736579895 + 100.0 * 8.408119201660156
Epoch 2150, val loss: 0.6052753329277039
Epoch 2160, training loss: 841.1625366210938 = 0.4721246361732483 + 100.0 * 8.406904220581055
Epoch 2160, val loss: 0.6049108505249023
Epoch 2170, training loss: 841.1678466796875 = 0.4712255895137787 + 100.0 * 8.406966209411621
Epoch 2170, val loss: 0.6045011281967163
Epoch 2180, training loss: 841.322998046875 = 0.47031480073928833 + 100.0 * 8.408526420593262
Epoch 2180, val loss: 0.604157567024231
Epoch 2190, training loss: 841.1278076171875 = 0.46938419342041016 + 100.0 * 8.406584739685059
Epoch 2190, val loss: 0.6037915945053101
Epoch 2200, training loss: 841.0629272460938 = 0.4684527516365051 + 100.0 * 8.40594482421875
Epoch 2200, val loss: 0.6034666299819946
Epoch 2210, training loss: 841.3487548828125 = 0.4675312638282776 + 100.0 * 8.408812522888184
Epoch 2210, val loss: 0.6030182838439941
Epoch 2220, training loss: 841.2756958007812 = 0.4665714204311371 + 100.0 * 8.40809154510498
Epoch 2220, val loss: 0.6026493906974792
Epoch 2230, training loss: 841.0443115234375 = 0.46559879183769226 + 100.0 * 8.405787467956543
Epoch 2230, val loss: 0.6023487448692322
Epoch 2240, training loss: 840.958251953125 = 0.4646580219268799 + 100.0 * 8.404935836791992
Epoch 2240, val loss: 0.6019222736358643
Epoch 2250, training loss: 840.9242553710938 = 0.46371349692344666 + 100.0 * 8.404605865478516
Epoch 2250, val loss: 0.6015881896018982
Epoch 2260, training loss: 840.9052124023438 = 0.46276798844337463 + 100.0 * 8.404424667358398
Epoch 2260, val loss: 0.6012054681777954
Epoch 2270, training loss: 840.9572143554688 = 0.4618086516857147 + 100.0 * 8.404953956604004
Epoch 2270, val loss: 0.6008745431900024
Epoch 2280, training loss: 841.1746826171875 = 0.460831880569458 + 100.0 * 8.40713882446289
Epoch 2280, val loss: 0.6003957986831665
Epoch 2290, training loss: 841.308349609375 = 0.4598350524902344 + 100.0 * 8.408485412597656
Epoch 2290, val loss: 0.5998732447624207
Epoch 2300, training loss: 840.8601684570312 = 0.4587963819503784 + 100.0 * 8.404013633728027
Epoch 2300, val loss: 0.5997074842453003
Epoch 2310, training loss: 840.8173217773438 = 0.45780518651008606 + 100.0 * 8.403594970703125
Epoch 2310, val loss: 0.5992704629898071
Epoch 2320, training loss: 840.794677734375 = 0.45682138204574585 + 100.0 * 8.4033784866333
Epoch 2320, val loss: 0.5988317131996155
Epoch 2330, training loss: 840.8826293945312 = 0.45582279562950134 + 100.0 * 8.404268264770508
Epoch 2330, val loss: 0.5985516309738159
Epoch 2340, training loss: 841.0899047851562 = 0.45479243993759155 + 100.0 * 8.406351089477539
Epoch 2340, val loss: 0.5982303023338318
Epoch 2350, training loss: 840.7842407226562 = 0.4537394344806671 + 100.0 * 8.403305053710938
Epoch 2350, val loss: 0.597693920135498
Epoch 2360, training loss: 840.6795654296875 = 0.4527057111263275 + 100.0 * 8.402268409729004
Epoch 2360, val loss: 0.5973888039588928
Epoch 2370, training loss: 840.6521606445312 = 0.451680451631546 + 100.0 * 8.402005195617676
Epoch 2370, val loss: 0.597037672996521
Epoch 2380, training loss: 840.6685791015625 = 0.45065632462501526 + 100.0 * 8.402178764343262
Epoch 2380, val loss: 0.5966948866844177
Epoch 2390, training loss: 841.2858276367188 = 0.44961464405059814 + 100.0 * 8.40836238861084
Epoch 2390, val loss: 0.5962808728218079
Epoch 2400, training loss: 840.7327880859375 = 0.44852131605148315 + 100.0 * 8.40284252166748
Epoch 2400, val loss: 0.5959508419036865
Epoch 2410, training loss: 840.5524291992188 = 0.44745001196861267 + 100.0 * 8.401049613952637
Epoch 2410, val loss: 0.5955783128738403
Epoch 2420, training loss: 840.5371704101562 = 0.4463930130004883 + 100.0 * 8.400907516479492
Epoch 2420, val loss: 0.595257043838501
Epoch 2430, training loss: 840.5165405273438 = 0.445333868265152 + 100.0 * 8.400712013244629
Epoch 2430, val loss: 0.594947099685669
Epoch 2440, training loss: 840.9246826171875 = 0.44426438212394714 + 100.0 * 8.404804229736328
Epoch 2440, val loss: 0.5948688983917236
Epoch 2450, training loss: 840.5690307617188 = 0.4431450068950653 + 100.0 * 8.401259422302246
Epoch 2450, val loss: 0.5939184427261353
Epoch 2460, training loss: 840.5989379882812 = 0.44200244545936584 + 100.0 * 8.401569366455078
Epoch 2460, val loss: 0.5939480662345886
Epoch 2470, training loss: 840.4532470703125 = 0.44090694189071655 + 100.0 * 8.400123596191406
Epoch 2470, val loss: 0.5933742523193359
Epoch 2480, training loss: 840.3978881835938 = 0.4398064613342285 + 100.0 * 8.399580955505371
Epoch 2480, val loss: 0.5930593013763428
Epoch 2490, training loss: 840.375244140625 = 0.438707172870636 + 100.0 * 8.399365425109863
Epoch 2490, val loss: 0.5927562117576599
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7584982242516489
0.8138085923349997
=== training gcn model ===
Epoch 0, training loss: 1059.34326171875 = 1.112789273262024 + 100.0 * 10.582304954528809
Epoch 0, val loss: 1.1141383647918701
Epoch 10, training loss: 1059.3111572265625 = 1.1078485250473022 + 100.0 * 10.582033157348633
Epoch 10, val loss: 1.1091163158416748
Epoch 20, training loss: 1059.1861572265625 = 1.1022752523422241 + 100.0 * 10.580838203430176
Epoch 20, val loss: 1.1034547090530396
Epoch 30, training loss: 1058.6322021484375 = 1.0959298610687256 + 100.0 * 10.575362205505371
Epoch 30, val loss: 1.0969977378845215
Epoch 40, training loss: 1056.3482666015625 = 1.0888172388076782 + 100.0 * 10.552594184875488
Epoch 40, val loss: 1.0897520780563354
Epoch 50, training loss: 1048.710205078125 = 1.0808180570602417 + 100.0 * 10.47629451751709
Epoch 50, val loss: 1.0816062688827515
Epoch 60, training loss: 1026.5283203125 = 1.0730234384536743 + 100.0 * 10.254552841186523
Epoch 60, val loss: 1.073791265487671
Epoch 70, training loss: 978.4828491210938 = 1.0647250413894653 + 100.0 * 9.774181365966797
Epoch 70, val loss: 1.0653448104858398
Epoch 80, training loss: 960.8302612304688 = 1.0583432912826538 + 100.0 * 9.597719192504883
Epoch 80, val loss: 1.0591689348220825
Epoch 90, training loss: 954.5228881835938 = 1.0534133911132812 + 100.0 * 9.53469467163086
Epoch 90, val loss: 1.0542105436325073
Epoch 100, training loss: 944.426513671875 = 1.048936128616333 + 100.0 * 9.433775901794434
Epoch 100, val loss: 1.049739956855774
Epoch 110, training loss: 930.72509765625 = 1.044905662536621 + 100.0 * 9.296801567077637
Epoch 110, val loss: 1.0457847118377686
Epoch 120, training loss: 924.3522338867188 = 1.0410232543945312 + 100.0 * 9.233112335205078
Epoch 120, val loss: 1.0420035123825073
Epoch 130, training loss: 922.2413330078125 = 1.037245273590088 + 100.0 * 9.212040901184082
Epoch 130, val loss: 1.03834867477417
Epoch 140, training loss: 919.1124877929688 = 1.0340712070465088 + 100.0 * 9.180784225463867
Epoch 140, val loss: 1.035317301750183
Epoch 150, training loss: 914.922119140625 = 1.0313425064086914 + 100.0 * 9.138907432556152
Epoch 150, val loss: 1.0326650142669678
Epoch 160, training loss: 908.35986328125 = 1.0289644002914429 + 100.0 * 9.073308944702148
Epoch 160, val loss: 1.0304023027420044
Epoch 170, training loss: 898.9286499023438 = 1.027520775794983 + 100.0 * 8.979011535644531
Epoch 170, val loss: 1.028991460800171
Epoch 180, training loss: 891.4511108398438 = 1.0262774229049683 + 100.0 * 8.904248237609863
Epoch 180, val loss: 1.0276286602020264
Epoch 190, training loss: 887.5269165039062 = 1.0238155126571655 + 100.0 * 8.865031242370605
Epoch 190, val loss: 1.0250288248062134
Epoch 200, training loss: 884.8524780273438 = 1.0205620527267456 + 100.0 * 8.838318824768066
Epoch 200, val loss: 1.0218086242675781
Epoch 210, training loss: 881.84423828125 = 1.0173646211624146 + 100.0 * 8.808268547058105
Epoch 210, val loss: 1.0187064409255981
Epoch 220, training loss: 878.3134765625 = 1.0143113136291504 + 100.0 * 8.772992134094238
Epoch 220, val loss: 1.0157502889633179
Epoch 230, training loss: 875.0498657226562 = 1.011289358139038 + 100.0 * 8.740386009216309
Epoch 230, val loss: 1.0128082036972046
Epoch 240, training loss: 872.41259765625 = 1.0079606771469116 + 100.0 * 8.714046478271484
Epoch 240, val loss: 1.009567379951477
Epoch 250, training loss: 870.7184448242188 = 1.004206895828247 + 100.0 * 8.697142601013184
Epoch 250, val loss: 1.005898118019104
Epoch 260, training loss: 869.5773315429688 = 1.0000832080841064 + 100.0 * 8.685772895812988
Epoch 260, val loss: 1.001874566078186
Epoch 270, training loss: 868.1738891601562 = 0.9956644177436829 + 100.0 * 8.671782493591309
Epoch 270, val loss: 0.9975965023040771
Epoch 280, training loss: 866.9791870117188 = 0.9910528659820557 + 100.0 * 8.659881591796875
Epoch 280, val loss: 0.9931304454803467
Epoch 290, training loss: 865.8169555664062 = 0.9862610101699829 + 100.0 * 8.648306846618652
Epoch 290, val loss: 0.9885019659996033
Epoch 300, training loss: 865.5784912109375 = 0.9812248349189758 + 100.0 * 8.645973205566406
Epoch 300, val loss: 0.9836390018463135
Epoch 310, training loss: 864.0247192382812 = 0.9757679104804993 + 100.0 * 8.630489349365234
Epoch 310, val loss: 0.9783687591552734
Epoch 320, training loss: 862.98388671875 = 0.969981849193573 + 100.0 * 8.620139122009277
Epoch 320, val loss: 0.972798228263855
Epoch 330, training loss: 862.0494995117188 = 0.963873028755188 + 100.0 * 8.610856056213379
Epoch 330, val loss: 0.9669341444969177
Epoch 340, training loss: 861.2850952148438 = 0.9574258327484131 + 100.0 * 8.603277206420898
Epoch 340, val loss: 0.9607556462287903
Epoch 350, training loss: 860.4570922851562 = 0.9505439400672913 + 100.0 * 8.595065116882324
Epoch 350, val loss: 0.954146683216095
Epoch 360, training loss: 859.8126831054688 = 0.9432680010795593 + 100.0 * 8.588693618774414
Epoch 360, val loss: 0.9471942186355591
Epoch 370, training loss: 859.1798095703125 = 0.9356391429901123 + 100.0 * 8.582441329956055
Epoch 370, val loss: 0.9399063587188721
Epoch 380, training loss: 858.5728759765625 = 0.9276513457298279 + 100.0 * 8.576452255249023
Epoch 380, val loss: 0.9322860836982727
Epoch 390, training loss: 858.065185546875 = 0.919281542301178 + 100.0 * 8.57145881652832
Epoch 390, val loss: 0.9243173599243164
Epoch 400, training loss: 857.6973266601562 = 0.9105038642883301 + 100.0 * 8.56786823272705
Epoch 400, val loss: 0.9159839749336243
Epoch 410, training loss: 857.0900268554688 = 0.9014502763748169 + 100.0 * 8.561885833740234
Epoch 410, val loss: 0.9074115753173828
Epoch 420, training loss: 856.64111328125 = 0.8921138644218445 + 100.0 * 8.557490348815918
Epoch 420, val loss: 0.8985967040061951
Epoch 430, training loss: 856.2052612304688 = 0.8824888467788696 + 100.0 * 8.553227424621582
Epoch 430, val loss: 0.8895432949066162
Epoch 440, training loss: 855.8082885742188 = 0.8726451396942139 + 100.0 * 8.549356460571289
Epoch 440, val loss: 0.8803122043609619
Epoch 450, training loss: 855.5408325195312 = 0.8626295924186707 + 100.0 * 8.546782493591309
Epoch 450, val loss: 0.8709400296211243
Epoch 460, training loss: 855.217529296875 = 0.8525301814079285 + 100.0 * 8.543649673461914
Epoch 460, val loss: 0.8615391254425049
Epoch 470, training loss: 854.8150634765625 = 0.8423773050308228 + 100.0 * 8.539726257324219
Epoch 470, val loss: 0.8521075248718262
Epoch 480, training loss: 854.4786987304688 = 0.832207977771759 + 100.0 * 8.53646469116211
Epoch 480, val loss: 0.8427087068557739
Epoch 490, training loss: 854.1423950195312 = 0.822111964225769 + 100.0 * 8.533203125
Epoch 490, val loss: 0.8334046602249146
Epoch 500, training loss: 853.8671264648438 = 0.8121404051780701 + 100.0 * 8.530550003051758
Epoch 500, val loss: 0.8242641687393188
Epoch 510, training loss: 853.6015625 = 0.8022380471229553 + 100.0 * 8.527993202209473
Epoch 510, val loss: 0.8152366876602173
Epoch 520, training loss: 853.5877685546875 = 0.7925040125846863 + 100.0 * 8.527953147888184
Epoch 520, val loss: 0.8064028024673462
Epoch 530, training loss: 853.2257080078125 = 0.7829404473304749 + 100.0 * 8.52442741394043
Epoch 530, val loss: 0.797720730304718
Epoch 540, training loss: 852.8922119140625 = 0.7736455202102661 + 100.0 * 8.521185874938965
Epoch 540, val loss: 0.7893653512001038
Epoch 550, training loss: 852.5638427734375 = 0.764692485332489 + 100.0 * 8.517991065979004
Epoch 550, val loss: 0.7813469171524048
Epoch 560, training loss: 852.3339233398438 = 0.755999743938446 + 100.0 * 8.515779495239258
Epoch 560, val loss: 0.7736344933509827
Epoch 570, training loss: 852.2329711914062 = 0.7475919127464294 + 100.0 * 8.514853477478027
Epoch 570, val loss: 0.7662005424499512
Epoch 580, training loss: 851.93505859375 = 0.7394725680351257 + 100.0 * 8.511955261230469
Epoch 580, val loss: 0.7590950727462769
Epoch 590, training loss: 851.7064208984375 = 0.7317023873329163 + 100.0 * 8.509747505187988
Epoch 590, val loss: 0.7523098587989807
Epoch 600, training loss: 851.498046875 = 0.7242346405982971 + 100.0 * 8.50773811340332
Epoch 600, val loss: 0.7458624839782715
Epoch 610, training loss: 851.2726440429688 = 0.7171048521995544 + 100.0 * 8.505555152893066
Epoch 610, val loss: 0.7397181391716003
Epoch 620, training loss: 851.1444702148438 = 0.7102957963943481 + 100.0 * 8.504342079162598
Epoch 620, val loss: 0.7339170575141907
Epoch 630, training loss: 851.056640625 = 0.7037707567214966 + 100.0 * 8.503528594970703
Epoch 630, val loss: 0.7284306287765503
Epoch 640, training loss: 850.797119140625 = 0.6975812911987305 + 100.0 * 8.500995635986328
Epoch 640, val loss: 0.7232542634010315
Epoch 650, training loss: 850.6135864257812 = 0.6917035579681396 + 100.0 * 8.499218940734863
Epoch 650, val loss: 0.7183989882469177
Epoch 660, training loss: 850.472900390625 = 0.6861567497253418 + 100.0 * 8.497867584228516
Epoch 660, val loss: 0.7138519883155823
Epoch 670, training loss: 850.3363647460938 = 0.6808487772941589 + 100.0 * 8.49655532836914
Epoch 670, val loss: 0.7095030546188354
Epoch 680, training loss: 850.0712280273438 = 0.6758248805999756 + 100.0 * 8.493953704833984
Epoch 680, val loss: 0.7055039405822754
Epoch 690, training loss: 849.8942260742188 = 0.6711022257804871 + 100.0 * 8.492231369018555
Epoch 690, val loss: 0.7017563581466675
Epoch 700, training loss: 849.6958618164062 = 0.6666538715362549 + 100.0 * 8.4902925491333
Epoch 700, val loss: 0.6982958912849426
Epoch 710, training loss: 849.5494995117188 = 0.6624406576156616 + 100.0 * 8.488870620727539
Epoch 710, val loss: 0.6950642466545105
Epoch 720, training loss: 849.3362426757812 = 0.6583997011184692 + 100.0 * 8.486778259277344
Epoch 720, val loss: 0.691952109336853
Epoch 730, training loss: 849.2293701171875 = 0.6545628905296326 + 100.0 * 8.485748291015625
Epoch 730, val loss: 0.6891227960586548
Epoch 740, training loss: 848.8936767578125 = 0.6509654521942139 + 100.0 * 8.482427597045898
Epoch 740, val loss: 0.6864668726921082
Epoch 750, training loss: 848.9118041992188 = 0.6475537419319153 + 100.0 * 8.48264217376709
Epoch 750, val loss: 0.6839748024940491
Epoch 760, training loss: 848.86865234375 = 0.6442364454269409 + 100.0 * 8.482244491577148
Epoch 760, val loss: 0.6816259026527405
Epoch 770, training loss: 848.3931884765625 = 0.6411119699478149 + 100.0 * 8.477520942687988
Epoch 770, val loss: 0.6794741749763489
Epoch 780, training loss: 848.104736328125 = 0.6381675601005554 + 100.0 * 8.474665641784668
Epoch 780, val loss: 0.6774596571922302
Epoch 790, training loss: 847.8681030273438 = 0.6353888511657715 + 100.0 * 8.47232723236084
Epoch 790, val loss: 0.6756429672241211
Epoch 800, training loss: 848.12451171875 = 0.6327402591705322 + 100.0 * 8.4749174118042
Epoch 800, val loss: 0.6739524006843567
Epoch 810, training loss: 847.712890625 = 0.6300779581069946 + 100.0 * 8.47082805633545
Epoch 810, val loss: 0.6721794605255127
Epoch 820, training loss: 847.4133911132812 = 0.6275771260261536 + 100.0 * 8.46785831451416
Epoch 820, val loss: 0.6706377863883972
Epoch 830, training loss: 847.1577758789062 = 0.6252219676971436 + 100.0 * 8.465325355529785
Epoch 830, val loss: 0.6691298484802246
Epoch 840, training loss: 846.9277954101562 = 0.6229861974716187 + 100.0 * 8.463047981262207
Epoch 840, val loss: 0.6678304076194763
Epoch 850, training loss: 846.7554321289062 = 0.6208508014678955 + 100.0 * 8.461345672607422
Epoch 850, val loss: 0.6665762662887573
Epoch 860, training loss: 846.59423828125 = 0.6187923550605774 + 100.0 * 8.459754943847656
Epoch 860, val loss: 0.6653732657432556
Epoch 870, training loss: 847.083740234375 = 0.6167965531349182 + 100.0 * 8.464669227600098
Epoch 870, val loss: 0.6642994284629822
Epoch 880, training loss: 846.2970581054688 = 0.6148224472999573 + 100.0 * 8.456822395324707
Epoch 880, val loss: 0.6631320118904114
Epoch 890, training loss: 846.2085571289062 = 0.6129527688026428 + 100.0 * 8.455955505371094
Epoch 890, val loss: 0.6620731949806213
Epoch 900, training loss: 846.027099609375 = 0.6111602783203125 + 100.0 * 8.4541597366333
Epoch 900, val loss: 0.6611140966415405
Epoch 910, training loss: 846.7720336914062 = 0.6094322204589844 + 100.0 * 8.461626052856445
Epoch 910, val loss: 0.6601582169532776
Epoch 920, training loss: 845.8341674804688 = 0.6076762080192566 + 100.0 * 8.452264785766602
Epoch 920, val loss: 0.6592748761177063
Epoch 930, training loss: 845.716064453125 = 0.6060293316841125 + 100.0 * 8.45110034942627
Epoch 930, val loss: 0.6584771275520325
Epoch 940, training loss: 845.5164794921875 = 0.6044570207595825 + 100.0 * 8.44912052154541
Epoch 940, val loss: 0.6576899290084839
Epoch 950, training loss: 845.3951416015625 = 0.60294508934021 + 100.0 * 8.447921752929688
Epoch 950, val loss: 0.6569746732711792
Epoch 960, training loss: 845.3010864257812 = 0.6014754772186279 + 100.0 * 8.446995735168457
Epoch 960, val loss: 0.6562809944152832
Epoch 970, training loss: 845.432861328125 = 0.6000197529792786 + 100.0 * 8.448328018188477
Epoch 970, val loss: 0.655576765537262
Epoch 980, training loss: 845.195068359375 = 0.5985829830169678 + 100.0 * 8.445964813232422
Epoch 980, val loss: 0.6549380421638489
Epoch 990, training loss: 844.9179077148438 = 0.5972111821174622 + 100.0 * 8.443206787109375
Epoch 990, val loss: 0.654293954372406
Epoch 1000, training loss: 844.8257446289062 = 0.5959010720252991 + 100.0 * 8.442298889160156
Epoch 1000, val loss: 0.6537623405456543
Epoch 1010, training loss: 844.686279296875 = 0.5946317911148071 + 100.0 * 8.440917015075684
Epoch 1010, val loss: 0.6532164216041565
Epoch 1020, training loss: 844.7048950195312 = 0.5933902263641357 + 100.0 * 8.441115379333496
Epoch 1020, val loss: 0.6526275277137756
Epoch 1030, training loss: 844.8214111328125 = 0.5921142101287842 + 100.0 * 8.442293167114258
Epoch 1030, val loss: 0.652282178401947
Epoch 1040, training loss: 844.4878540039062 = 0.5908296704292297 + 100.0 * 8.438970565795898
Epoch 1040, val loss: 0.6516772508621216
Epoch 1050, training loss: 844.4000854492188 = 0.5896098017692566 + 100.0 * 8.438104629516602
Epoch 1050, val loss: 0.6511793732643127
Epoch 1060, training loss: 844.2409057617188 = 0.5884670615196228 + 100.0 * 8.436524391174316
Epoch 1060, val loss: 0.6506674289703369
Epoch 1070, training loss: 844.0947265625 = 0.587365448474884 + 100.0 * 8.435073852539062
Epoch 1070, val loss: 0.650273859500885
Epoch 1080, training loss: 843.979736328125 = 0.5862884521484375 + 100.0 * 8.433934211730957
Epoch 1080, val loss: 0.6498412489891052
Epoch 1090, training loss: 843.8903198242188 = 0.5852251648902893 + 100.0 * 8.433051109313965
Epoch 1090, val loss: 0.6494631767272949
Epoch 1100, training loss: 843.9266357421875 = 0.5841735601425171 + 100.0 * 8.433424949645996
Epoch 1100, val loss: 0.6490369439125061
Epoch 1110, training loss: 843.7978515625 = 0.5830937027931213 + 100.0 * 8.432147979736328
Epoch 1110, val loss: 0.6486499309539795
Epoch 1120, training loss: 843.6214599609375 = 0.5820254683494568 + 100.0 * 8.430394172668457
Epoch 1120, val loss: 0.6482740044593811
Epoch 1130, training loss: 843.5404052734375 = 0.5810065269470215 + 100.0 * 8.429594039916992
Epoch 1130, val loss: 0.6479072570800781
Epoch 1140, training loss: 843.45556640625 = 0.5800222754478455 + 100.0 * 8.428755760192871
Epoch 1140, val loss: 0.647602915763855
Epoch 1150, training loss: 843.5385131835938 = 0.5790522694587708 + 100.0 * 8.429594993591309
Epoch 1150, val loss: 0.6472638249397278
Epoch 1160, training loss: 843.4967651367188 = 0.5780603289604187 + 100.0 * 8.429186820983887
Epoch 1160, val loss: 0.6469244360923767
Epoch 1170, training loss: 843.3323974609375 = 0.5770841240882874 + 100.0 * 8.427553176879883
Epoch 1170, val loss: 0.6466057896614075
Epoch 1180, training loss: 843.2833862304688 = 0.5761227607727051 + 100.0 * 8.427072525024414
Epoch 1180, val loss: 0.646268367767334
Epoch 1190, training loss: 843.1675415039062 = 0.5751702785491943 + 100.0 * 8.425923347473145
Epoch 1190, val loss: 0.6459814310073853
Epoch 1200, training loss: 843.0558471679688 = 0.5742399096488953 + 100.0 * 8.424816131591797
Epoch 1200, val loss: 0.6456855535507202
Epoch 1210, training loss: 843.01953125 = 0.5733222961425781 + 100.0 * 8.42446231842041
Epoch 1210, val loss: 0.6454448699951172
Epoch 1220, training loss: 843.5458984375 = 0.5723857283592224 + 100.0 * 8.42973518371582
Epoch 1220, val loss: 0.6451447606086731
Epoch 1230, training loss: 842.944091796875 = 0.5714287161827087 + 100.0 * 8.423727035522461
Epoch 1230, val loss: 0.6448053121566772
Epoch 1240, training loss: 842.8232421875 = 0.5705052614212036 + 100.0 * 8.422527313232422
Epoch 1240, val loss: 0.6444913744926453
Epoch 1250, training loss: 842.739501953125 = 0.5696132183074951 + 100.0 * 8.421698570251465
Epoch 1250, val loss: 0.6443201303482056
Epoch 1260, training loss: 842.67041015625 = 0.5687338709831238 + 100.0 * 8.421016693115234
Epoch 1260, val loss: 0.644001305103302
Epoch 1270, training loss: 842.7007446289062 = 0.5678597092628479 + 100.0 * 8.4213285446167
Epoch 1270, val loss: 0.6437866687774658
Epoch 1280, training loss: 842.6616821289062 = 0.566954493522644 + 100.0 * 8.420947074890137
Epoch 1280, val loss: 0.6434459090232849
Epoch 1290, training loss: 842.6250610351562 = 0.5660378336906433 + 100.0 * 8.4205904006958
Epoch 1290, val loss: 0.6432613730430603
Epoch 1300, training loss: 842.4869995117188 = 0.5651572942733765 + 100.0 * 8.419218063354492
Epoch 1300, val loss: 0.6429129838943481
Epoch 1310, training loss: 842.43408203125 = 0.5643025636672974 + 100.0 * 8.418697357177734
Epoch 1310, val loss: 0.6426839828491211
Epoch 1320, training loss: 842.847412109375 = 0.5634474754333496 + 100.0 * 8.422839164733887
Epoch 1320, val loss: 0.6424288749694824
Epoch 1330, training loss: 842.4723510742188 = 0.5625594258308411 + 100.0 * 8.419097900390625
Epoch 1330, val loss: 0.642249584197998
Epoch 1340, training loss: 842.3700561523438 = 0.5616909265518188 + 100.0 * 8.418083190917969
Epoch 1340, val loss: 0.6418477892875671
Epoch 1350, training loss: 842.2593994140625 = 0.5608580112457275 + 100.0 * 8.416985511779785
Epoch 1350, val loss: 0.641705334186554
Epoch 1360, training loss: 842.2216186523438 = 0.5600363612174988 + 100.0 * 8.41661548614502
Epoch 1360, val loss: 0.6414213180541992
Epoch 1370, training loss: 842.6370239257812 = 0.5592159032821655 + 100.0 * 8.420778274536133
Epoch 1370, val loss: 0.6411643624305725
Epoch 1380, training loss: 842.2284545898438 = 0.5583471655845642 + 100.0 * 8.416701316833496
Epoch 1380, val loss: 0.6410911083221436
Epoch 1390, training loss: 842.0719604492188 = 0.5575237274169922 + 100.0 * 8.415144920349121
Epoch 1390, val loss: 0.6407585740089417
Epoch 1400, training loss: 841.9976806640625 = 0.5567180514335632 + 100.0 * 8.414409637451172
Epoch 1400, val loss: 0.6406261324882507
Epoch 1410, training loss: 841.9415283203125 = 0.5559206008911133 + 100.0 * 8.413856506347656
Epoch 1410, val loss: 0.6403999328613281
Epoch 1420, training loss: 842.0014038085938 = 0.5551204681396484 + 100.0 * 8.41446304321289
Epoch 1420, val loss: 0.6401679515838623
Epoch 1430, training loss: 841.95849609375 = 0.5542826652526855 + 100.0 * 8.414042472839355
Epoch 1430, val loss: 0.6399366855621338
Epoch 1440, training loss: 841.9737548828125 = 0.5534318685531616 + 100.0 * 8.414203643798828
Epoch 1440, val loss: 0.6396931409835815
Epoch 1450, training loss: 841.7838745117188 = 0.5526174306869507 + 100.0 * 8.412312507629395
Epoch 1450, val loss: 0.6394274830818176
Epoch 1460, training loss: 841.7294921875 = 0.5518258213996887 + 100.0 * 8.411776542663574
Epoch 1460, val loss: 0.6392461061477661
Epoch 1470, training loss: 842.2002563476562 = 0.5510241985321045 + 100.0 * 8.416492462158203
Epoch 1470, val loss: 0.6390549540519714
Epoch 1480, training loss: 841.737548828125 = 0.5501899123191833 + 100.0 * 8.411873817443848
Epoch 1480, val loss: 0.6387752294540405
Epoch 1490, training loss: 841.6060180664062 = 0.5493622422218323 + 100.0 * 8.410566329956055
Epoch 1490, val loss: 0.6385005116462708
Epoch 1500, training loss: 841.5160522460938 = 0.5485658049583435 + 100.0 * 8.409674644470215
Epoch 1500, val loss: 0.6383140683174133
Epoch 1510, training loss: 841.485107421875 = 0.5477713346481323 + 100.0 * 8.40937328338623
Epoch 1510, val loss: 0.6380531787872314
Epoch 1520, training loss: 841.9839477539062 = 0.5469670295715332 + 100.0 * 8.414369583129883
Epoch 1520, val loss: 0.637859046459198
Epoch 1530, training loss: 841.6146240234375 = 0.5461165904998779 + 100.0 * 8.410684585571289
Epoch 1530, val loss: 0.6376344561576843
Epoch 1540, training loss: 841.447509765625 = 0.5452839732170105 + 100.0 * 8.409022331237793
Epoch 1540, val loss: 0.6373714208602905
Epoch 1550, training loss: 841.2923583984375 = 0.544464111328125 + 100.0 * 8.407479286193848
Epoch 1550, val loss: 0.6370989680290222
Epoch 1560, training loss: 841.2476806640625 = 0.5436607003211975 + 100.0 * 8.4070405960083
Epoch 1560, val loss: 0.6368111371994019
Epoch 1570, training loss: 841.3427734375 = 0.5428582429885864 + 100.0 * 8.407999038696289
Epoch 1570, val loss: 0.636538028717041
Epoch 1580, training loss: 841.3258666992188 = 0.5420249700546265 + 100.0 * 8.407837867736816
Epoch 1580, val loss: 0.6363995671272278
Epoch 1590, training loss: 841.28466796875 = 0.5411604642868042 + 100.0 * 8.407435417175293
Epoch 1590, val loss: 0.6360606551170349
Epoch 1600, training loss: 841.1859130859375 = 0.5403081774711609 + 100.0 * 8.406455993652344
Epoch 1600, val loss: 0.6358780860900879
Epoch 1610, training loss: 841.0781860351562 = 0.5394806861877441 + 100.0 * 8.405386924743652
Epoch 1610, val loss: 0.635539174079895
Epoch 1620, training loss: 840.9725952148438 = 0.5386726260185242 + 100.0 * 8.404338836669922
Epoch 1620, val loss: 0.6353588104248047
Epoch 1630, training loss: 840.9411010742188 = 0.5378645658493042 + 100.0 * 8.404032707214355
Epoch 1630, val loss: 0.6351197957992554
Epoch 1640, training loss: 840.9867553710938 = 0.537048876285553 + 100.0 * 8.404497146606445
Epoch 1640, val loss: 0.6348890066146851
Epoch 1650, training loss: 841.265625 = 0.5362045168876648 + 100.0 * 8.407294273376465
Epoch 1650, val loss: 0.6345784664154053
Epoch 1660, training loss: 840.9618530273438 = 0.5353244543075562 + 100.0 * 8.404265403747559
Epoch 1660, val loss: 0.6344248652458191
Epoch 1670, training loss: 840.8163452148438 = 0.5344828963279724 + 100.0 * 8.40281867980957
Epoch 1670, val loss: 0.6340537071228027
Epoch 1680, training loss: 840.8097534179688 = 0.5336408615112305 + 100.0 * 8.402761459350586
Epoch 1680, val loss: 0.6338714957237244
Epoch 1690, training loss: 840.8930053710938 = 0.5327824354171753 + 100.0 * 8.403602600097656
Epoch 1690, val loss: 0.6336225867271423
Epoch 1700, training loss: 840.783447265625 = 0.5319157838821411 + 100.0 * 8.402515411376953
Epoch 1700, val loss: 0.6333526372909546
Epoch 1710, training loss: 840.7939453125 = 0.5310416221618652 + 100.0 * 8.402628898620605
Epoch 1710, val loss: 0.6330726146697998
Epoch 1720, training loss: 840.6632690429688 = 0.53016197681427 + 100.0 * 8.401330947875977
Epoch 1720, val loss: 0.6328644752502441
Epoch 1730, training loss: 840.713623046875 = 0.5292930006980896 + 100.0 * 8.401843070983887
Epoch 1730, val loss: 0.6325821876525879
Epoch 1740, training loss: 840.6506958007812 = 0.528403103351593 + 100.0 * 8.401223182678223
Epoch 1740, val loss: 0.6322911977767944
Epoch 1750, training loss: 840.5475463867188 = 0.5275101661682129 + 100.0 * 8.400199890136719
Epoch 1750, val loss: 0.6320074796676636
Epoch 1760, training loss: 840.47802734375 = 0.5266355276107788 + 100.0 * 8.399514198303223
Epoch 1760, val loss: 0.6317406892776489
Epoch 1770, training loss: 840.4966430664062 = 0.525757908821106 + 100.0 * 8.39970874786377
Epoch 1770, val loss: 0.6314330101013184
Epoch 1780, training loss: 840.8349609375 = 0.5248550772666931 + 100.0 * 8.403100967407227
Epoch 1780, val loss: 0.6311666965484619
Epoch 1790, training loss: 840.4920654296875 = 0.5239163637161255 + 100.0 * 8.399681091308594
Epoch 1790, val loss: 0.6309927701950073
Epoch 1800, training loss: 840.3363647460938 = 0.52301025390625 + 100.0 * 8.398133277893066
Epoch 1800, val loss: 0.630659282207489
Epoch 1810, training loss: 840.2802124023438 = 0.5221118927001953 + 100.0 * 8.397581100463867
Epoch 1810, val loss: 0.6304150819778442
Epoch 1820, training loss: 840.3443603515625 = 0.5212147831916809 + 100.0 * 8.398231506347656
Epoch 1820, val loss: 0.6301120519638062
Epoch 1830, training loss: 840.5830078125 = 0.5202804803848267 + 100.0 * 8.400627136230469
Epoch 1830, val loss: 0.6298137903213501
Epoch 1840, training loss: 840.2923583984375 = 0.5193200707435608 + 100.0 * 8.397729873657227
Epoch 1840, val loss: 0.6295095682144165
Epoch 1850, training loss: 840.1805419921875 = 0.5183748006820679 + 100.0 * 8.396621704101562
Epoch 1850, val loss: 0.629238486289978
Epoch 1860, training loss: 840.129638671875 = 0.5174477696418762 + 100.0 * 8.396121978759766
Epoch 1860, val loss: 0.6288896799087524
Epoch 1870, training loss: 840.1008911132812 = 0.5165222883224487 + 100.0 * 8.395843505859375
Epoch 1870, val loss: 0.6286280751228333
Epoch 1880, training loss: 840.298828125 = 0.5155864953994751 + 100.0 * 8.397832870483398
Epoch 1880, val loss: 0.6282771229743958
Epoch 1890, training loss: 840.1901245117188 = 0.5146114826202393 + 100.0 * 8.39675521850586
Epoch 1890, val loss: 0.6280428767204285
Epoch 1900, training loss: 840.0993041992188 = 0.5136155486106873 + 100.0 * 8.395856857299805
Epoch 1900, val loss: 0.6276876926422119
Epoch 1910, training loss: 839.9952392578125 = 0.5126280784606934 + 100.0 * 8.39482593536377
Epoch 1910, val loss: 0.6274252533912659
Epoch 1920, training loss: 839.9293212890625 = 0.5116620659828186 + 100.0 * 8.394176483154297
Epoch 1920, val loss: 0.6271095871925354
Epoch 1930, training loss: 839.881103515625 = 0.5106949210166931 + 100.0 * 8.393704414367676
Epoch 1930, val loss: 0.6268110275268555
Epoch 1940, training loss: 839.8931274414062 = 0.5097208023071289 + 100.0 * 8.393834114074707
Epoch 1940, val loss: 0.6265637278556824
Epoch 1950, training loss: 840.3831787109375 = 0.5087246894836426 + 100.0 * 8.398744583129883
Epoch 1950, val loss: 0.6263384222984314
Epoch 1960, training loss: 840.3480224609375 = 0.5076767206192017 + 100.0 * 8.39840316772461
Epoch 1960, val loss: 0.6258260011672974
Epoch 1970, training loss: 839.7929077148438 = 0.5066326260566711 + 100.0 * 8.392862319946289
Epoch 1970, val loss: 0.6254644989967346
Epoch 1980, training loss: 839.78076171875 = 0.5056160688400269 + 100.0 * 8.392751693725586
Epoch 1980, val loss: 0.6252136826515198
Epoch 1990, training loss: 839.7343139648438 = 0.5046138167381287 + 100.0 * 8.39229679107666
Epoch 1990, val loss: 0.6248923540115356
Epoch 2000, training loss: 839.7083740234375 = 0.503612756729126 + 100.0 * 8.392047882080078
Epoch 2000, val loss: 0.6246203780174255
Epoch 2010, training loss: 839.7859497070312 = 0.5025936961174011 + 100.0 * 8.392833709716797
Epoch 2010, val loss: 0.6243353486061096
Epoch 2020, training loss: 839.9070434570312 = 0.5015411972999573 + 100.0 * 8.394055366516113
Epoch 2020, val loss: 0.6239948868751526
Epoch 2030, training loss: 839.715576171875 = 0.5004802942276001 + 100.0 * 8.39215087890625
Epoch 2030, val loss: 0.6235379576683044
Epoch 2040, training loss: 839.9702758789062 = 0.49941307306289673 + 100.0 * 8.394708633422852
Epoch 2040, val loss: 0.6232803463935852
Epoch 2050, training loss: 839.66748046875 = 0.49831610918045044 + 100.0 * 8.391692161560059
Epoch 2050, val loss: 0.6229531764984131
Epoch 2060, training loss: 839.5690307617188 = 0.4972388744354248 + 100.0 * 8.390717506408691
Epoch 2060, val loss: 0.6226469278335571
Epoch 2070, training loss: 839.5125732421875 = 0.4961637556552887 + 100.0 * 8.390164375305176
Epoch 2070, val loss: 0.6223722696304321
Epoch 2080, training loss: 839.4963989257812 = 0.4950869679450989 + 100.0 * 8.390012741088867
Epoch 2080, val loss: 0.6220488548278809
Epoch 2090, training loss: 839.866943359375 = 0.49400123953819275 + 100.0 * 8.393729209899902
Epoch 2090, val loss: 0.6217857003211975
Epoch 2100, training loss: 839.5607299804688 = 0.4928530752658844 + 100.0 * 8.390678405761719
Epoch 2100, val loss: 0.6213942170143127
Epoch 2110, training loss: 839.458740234375 = 0.4917154908180237 + 100.0 * 8.389670372009277
Epoch 2110, val loss: 0.6210984587669373
Epoch 2120, training loss: 839.407958984375 = 0.49059751629829407 + 100.0 * 8.38917350769043
Epoch 2120, val loss: 0.6207038164138794
Epoch 2130, training loss: 839.3595581054688 = 0.48948678374290466 + 100.0 * 8.388700485229492
Epoch 2130, val loss: 0.6204464435577393
Epoch 2140, training loss: 839.399169921875 = 0.48837336897850037 + 100.0 * 8.389107704162598
Epoch 2140, val loss: 0.6200436949729919
Epoch 2150, training loss: 839.84814453125 = 0.4872317612171173 + 100.0 * 8.393609046936035
Epoch 2150, val loss: 0.6196463704109192
Epoch 2160, training loss: 839.4442749023438 = 0.48602285981178284 + 100.0 * 8.389582633972168
Epoch 2160, val loss: 0.6194999814033508
Epoch 2170, training loss: 839.2862548828125 = 0.4848453104496002 + 100.0 * 8.38801383972168
Epoch 2170, val loss: 0.619076669216156
Epoch 2180, training loss: 839.2498168945312 = 0.4836842119693756 + 100.0 * 8.38766098022461
Epoch 2180, val loss: 0.6186985969543457
Epoch 2190, training loss: 839.2352905273438 = 0.48252174258232117 + 100.0 * 8.387527465820312
Epoch 2190, val loss: 0.6184530258178711
Epoch 2200, training loss: 839.4730834960938 = 0.4813486933708191 + 100.0 * 8.389917373657227
Epoch 2200, val loss: 0.6180607080459595
Epoch 2210, training loss: 839.2341918945312 = 0.4801272451877594 + 100.0 * 8.387540817260742
Epoch 2210, val loss: 0.6177434921264648
Epoch 2220, training loss: 839.7361450195312 = 0.47889986634254456 + 100.0 * 8.392572402954102
Epoch 2220, val loss: 0.6173704266548157
Epoch 2230, training loss: 839.2689208984375 = 0.4776456654071808 + 100.0 * 8.38791275024414
Epoch 2230, val loss: 0.6170409917831421
Epoch 2240, training loss: 839.15966796875 = 0.47641894221305847 + 100.0 * 8.386832237243652
Epoch 2240, val loss: 0.6166281700134277
Epoch 2250, training loss: 839.1015014648438 = 0.475210964679718 + 100.0 * 8.386262893676758
Epoch 2250, val loss: 0.6163905262947083
Epoch 2260, training loss: 839.0680541992188 = 0.47400394082069397 + 100.0 * 8.385940551757812
Epoch 2260, val loss: 0.616083025932312
Epoch 2270, training loss: 839.03955078125 = 0.47279053926467896 + 100.0 * 8.38566780090332
Epoch 2270, val loss: 0.6157606840133667
Epoch 2280, training loss: 839.0280151367188 = 0.47156497836112976 + 100.0 * 8.385564804077148
Epoch 2280, val loss: 0.6154859066009521
Epoch 2290, training loss: 839.2683715820312 = 0.4703249931335449 + 100.0 * 8.387980461120605
Epoch 2290, val loss: 0.6152163147926331
Epoch 2300, training loss: 839.291259765625 = 0.4690214991569519 + 100.0 * 8.388222694396973
Epoch 2300, val loss: 0.6150230765342712
Epoch 2310, training loss: 839.0520629882812 = 0.46768826246261597 + 100.0 * 8.385843276977539
Epoch 2310, val loss: 0.6143736243247986
Epoch 2320, training loss: 839.039306640625 = 0.4664038121700287 + 100.0 * 8.38572883605957
Epoch 2320, val loss: 0.6142128705978394
Epoch 2330, training loss: 838.9386596679688 = 0.4651417136192322 + 100.0 * 8.384735107421875
Epoch 2330, val loss: 0.6138304471969604
Epoch 2340, training loss: 838.8948364257812 = 0.4638802111148834 + 100.0 * 8.384309768676758
Epoch 2340, val loss: 0.6135686039924622
Epoch 2350, training loss: 838.8721923828125 = 0.46260592341423035 + 100.0 * 8.384096145629883
Epoch 2350, val loss: 0.6132750511169434
Epoch 2360, training loss: 838.8572998046875 = 0.46131694316864014 + 100.0 * 8.383959770202637
Epoch 2360, val loss: 0.6129845380783081
Epoch 2370, training loss: 839.0191650390625 = 0.46001720428466797 + 100.0 * 8.385591506958008
Epoch 2370, val loss: 0.6126111745834351
Epoch 2380, training loss: 839.0834350585938 = 0.4586750864982605 + 100.0 * 8.386247634887695
Epoch 2380, val loss: 0.6123237013816833
Epoch 2390, training loss: 839.0076293945312 = 0.45729437470436096 + 100.0 * 8.385503768920898
Epoch 2390, val loss: 0.6120994687080383
Epoch 2400, training loss: 838.8565673828125 = 0.45594194531440735 + 100.0 * 8.38400650024414
Epoch 2400, val loss: 0.6118419766426086
Epoch 2410, training loss: 838.7620239257812 = 0.45462116599082947 + 100.0 * 8.383073806762695
Epoch 2410, val loss: 0.6115357875823975
Epoch 2420, training loss: 838.7438354492188 = 0.4533108174800873 + 100.0 * 8.382905006408691
Epoch 2420, val loss: 0.6112151741981506
Epoch 2430, training loss: 838.7507934570312 = 0.4519926607608795 + 100.0 * 8.382987976074219
Epoch 2430, val loss: 0.6109636425971985
Epoch 2440, training loss: 839.1830444335938 = 0.4506652057170868 + 100.0 * 8.387323379516602
Epoch 2440, val loss: 0.6106592416763306
Epoch 2450, training loss: 838.82568359375 = 0.4492654800415039 + 100.0 * 8.383764266967773
Epoch 2450, val loss: 0.610421895980835
Epoch 2460, training loss: 838.7843627929688 = 0.4478801190853119 + 100.0 * 8.3833646774292
Epoch 2460, val loss: 0.6101344227790833
Epoch 2470, training loss: 838.6953735351562 = 0.44650664925575256 + 100.0 * 8.382488250732422
Epoch 2470, val loss: 0.6099288463592529
Epoch 2480, training loss: 838.6473999023438 = 0.4451543986797333 + 100.0 * 8.382022857666016
Epoch 2480, val loss: 0.609615683555603
Epoch 2490, training loss: 838.5969848632812 = 0.44379961490631104 + 100.0 * 8.381531715393066
Epoch 2490, val loss: 0.6094107031822205
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7732115677321156
0.8161269289284939
The final CL Acc:0.77287, 0.01160, The final GNN Acc:0.81511, 0.00097
raw graph: torch.Size([2, 88648])
add edge: torch.Size([2, 110626])
remove edge: torch.Size([2, 66690])
updated graph: torch.Size([2, 88668])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 1059.334228515625 = 1.1027421951293945 + 100.0 * 10.582314491271973
Epoch 0, val loss: 1.1020033359527588
Epoch 10, training loss: 1059.3004150390625 = 1.0983887910842896 + 100.0 * 10.582019805908203
Epoch 10, val loss: 1.0976893901824951
Epoch 20, training loss: 1059.1475830078125 = 1.0937421321868896 + 100.0 * 10.580537796020508
Epoch 20, val loss: 1.0931071043014526
Epoch 30, training loss: 1058.4229736328125 = 1.0888702869415283 + 100.0 * 10.573341369628906
Epoch 30, val loss: 1.0883033275604248
Epoch 40, training loss: 1055.466796875 = 1.0834933519363403 + 100.0 * 10.543832778930664
Epoch 40, val loss: 1.0829532146453857
Epoch 50, training loss: 1046.33251953125 = 1.0770010948181152 + 100.0 * 10.452554702758789
Epoch 50, val loss: 1.0764636993408203
Epoch 60, training loss: 1027.235107421875 = 1.0693345069885254 + 100.0 * 10.26165771484375
Epoch 60, val loss: 1.0688802003860474
Epoch 70, training loss: 1002.183349609375 = 1.061002254486084 + 100.0 * 10.011223793029785
Epoch 70, val loss: 1.060611605644226
Epoch 80, training loss: 988.787841796875 = 1.0526520013809204 + 100.0 * 9.877351760864258
Epoch 80, val loss: 1.0526968240737915
Epoch 90, training loss: 976.8480224609375 = 1.0455766916275024 + 100.0 * 9.758024215698242
Epoch 90, val loss: 1.0460411310195923
Epoch 100, training loss: 963.1073608398438 = 1.0400680303573608 + 100.0 * 9.620673179626465
Epoch 100, val loss: 1.0409914255142212
Epoch 110, training loss: 950.9880981445312 = 1.0357623100280762 + 100.0 * 9.499523162841797
Epoch 110, val loss: 1.0369616746902466
Epoch 120, training loss: 937.5272216796875 = 1.0310710668563843 + 100.0 * 9.364961624145508
Epoch 120, val loss: 1.0322517156600952
Epoch 130, training loss: 924.5914306640625 = 1.0252139568328857 + 100.0 * 9.235662460327148
Epoch 130, val loss: 1.0263956785202026
Epoch 140, training loss: 918.1764526367188 = 1.0189546346664429 + 100.0 * 9.171574592590332
Epoch 140, val loss: 1.02023446559906
Epoch 150, training loss: 912.3969116210938 = 1.013331651687622 + 100.0 * 9.113836288452148
Epoch 150, val loss: 1.014927625656128
Epoch 160, training loss: 902.5222778320312 = 1.0092822313308716 + 100.0 * 9.015130043029785
Epoch 160, val loss: 1.0113343000411987
Epoch 170, training loss: 892.6494140625 = 1.006326675415039 + 100.0 * 8.916430473327637
Epoch 170, val loss: 1.0084083080291748
Epoch 180, training loss: 887.440185546875 = 1.0014795064926147 + 100.0 * 8.864387512207031
Epoch 180, val loss: 1.003589153289795
Epoch 190, training loss: 883.9871826171875 = 0.9951097369194031 + 100.0 * 8.829920768737793
Epoch 190, val loss: 0.9974329471588135
Epoch 200, training loss: 881.31005859375 = 0.988156259059906 + 100.0 * 8.803218841552734
Epoch 200, val loss: 0.9907439947128296
Epoch 210, training loss: 878.928466796875 = 0.9809990525245667 + 100.0 * 8.779474258422852
Epoch 210, val loss: 0.9838938117027283
Epoch 220, training loss: 876.8858642578125 = 0.9735842347145081 + 100.0 * 8.759122848510742
Epoch 220, val loss: 0.9767765998840332
Epoch 230, training loss: 875.4276123046875 = 0.9656743407249451 + 100.0 * 8.744619369506836
Epoch 230, val loss: 0.9691480398178101
Epoch 240, training loss: 874.308837890625 = 0.9571548700332642 + 100.0 * 8.733516693115234
Epoch 240, val loss: 0.9609450101852417
Epoch 250, training loss: 873.25634765625 = 0.9481295347213745 + 100.0 * 8.723082542419434
Epoch 250, val loss: 0.952275812625885
Epoch 260, training loss: 872.1672973632812 = 0.9387536644935608 + 100.0 * 8.712285041809082
Epoch 260, val loss: 0.9433289170265198
Epoch 270, training loss: 870.9264526367188 = 0.9291344881057739 + 100.0 * 8.699973106384277
Epoch 270, val loss: 0.9341317415237427
Epoch 280, training loss: 870.0322265625 = 0.9192265272140503 + 100.0 * 8.691129684448242
Epoch 280, val loss: 0.9246809482574463
Epoch 290, training loss: 867.9703369140625 = 0.908962070941925 + 100.0 * 8.670614242553711
Epoch 290, val loss: 0.9148783087730408
Epoch 300, training loss: 865.8248291015625 = 0.8985376358032227 + 100.0 * 8.649262428283691
Epoch 300, val loss: 0.9049193263053894
Epoch 310, training loss: 863.5848388671875 = 0.8878994584083557 + 100.0 * 8.626969337463379
Epoch 310, val loss: 0.8948127627372742
Epoch 320, training loss: 861.6234130859375 = 0.8768582940101624 + 100.0 * 8.607465744018555
Epoch 320, val loss: 0.8841956257820129
Epoch 330, training loss: 859.4733276367188 = 0.8653156757354736 + 100.0 * 8.586080551147461
Epoch 330, val loss: 0.8731884360313416
Epoch 340, training loss: 857.9229736328125 = 0.8532337546348572 + 100.0 * 8.570697784423828
Epoch 340, val loss: 0.8616701364517212
Epoch 350, training loss: 857.1630859375 = 0.8404940962791443 + 100.0 * 8.563225746154785
Epoch 350, val loss: 0.849445641040802
Epoch 360, training loss: 855.7061767578125 = 0.8270784020423889 + 100.0 * 8.54879093170166
Epoch 360, val loss: 0.8367130756378174
Epoch 370, training loss: 854.8936767578125 = 0.8133153915405273 + 100.0 * 8.540803909301758
Epoch 370, val loss: 0.8236296772956848
Epoch 380, training loss: 854.2765502929688 = 0.7991830706596375 + 100.0 * 8.534773826599121
Epoch 380, val loss: 0.8101809024810791
Epoch 390, training loss: 853.5720825195312 = 0.7847880721092224 + 100.0 * 8.527873039245605
Epoch 390, val loss: 0.7965551018714905
Epoch 400, training loss: 852.8477172851562 = 0.770313024520874 + 100.0 * 8.520773887634277
Epoch 400, val loss: 0.7828378677368164
Epoch 410, training loss: 852.417236328125 = 0.7557656168937683 + 100.0 * 8.51661491394043
Epoch 410, val loss: 0.769092321395874
Epoch 420, training loss: 851.8942260742188 = 0.7411760687828064 + 100.0 * 8.511530876159668
Epoch 420, val loss: 0.7553443908691406
Epoch 430, training loss: 851.3369140625 = 0.7267317771911621 + 100.0 * 8.506101608276367
Epoch 430, val loss: 0.7417516112327576
Epoch 440, training loss: 850.8678588867188 = 0.7123996615409851 + 100.0 * 8.501554489135742
Epoch 440, val loss: 0.7283035516738892
Epoch 450, training loss: 850.4378662109375 = 0.698237419128418 + 100.0 * 8.497396469116211
Epoch 450, val loss: 0.7150835394859314
Epoch 460, training loss: 850.0928955078125 = 0.6843989491462708 + 100.0 * 8.494085311889648
Epoch 460, val loss: 0.7021494507789612
Epoch 470, training loss: 849.6697998046875 = 0.6708934307098389 + 100.0 * 8.489989280700684
Epoch 470, val loss: 0.6895709037780762
Epoch 480, training loss: 849.32275390625 = 0.6577147245407104 + 100.0 * 8.486650466918945
Epoch 480, val loss: 0.6773554086685181
Epoch 490, training loss: 849.0567016601562 = 0.6449642181396484 + 100.0 * 8.48411750793457
Epoch 490, val loss: 0.6655657887458801
Epoch 500, training loss: 848.9943237304688 = 0.6325715184211731 + 100.0 * 8.483617782592773
Epoch 500, val loss: 0.6541098952293396
Epoch 510, training loss: 848.7146606445312 = 0.6206423044204712 + 100.0 * 8.480939865112305
Epoch 510, val loss: 0.6431715488433838
Epoch 520, training loss: 848.30078125 = 0.609224796295166 + 100.0 * 8.47691535949707
Epoch 520, val loss: 0.632713258266449
Epoch 530, training loss: 848.0848388671875 = 0.5982414484024048 + 100.0 * 8.474865913391113
Epoch 530, val loss: 0.6226975321769714
Epoch 540, training loss: 847.9249267578125 = 0.5877454280853271 + 100.0 * 8.473371505737305
Epoch 540, val loss: 0.6131299138069153
Epoch 550, training loss: 847.7317504882812 = 0.577663242816925 + 100.0 * 8.471541404724121
Epoch 550, val loss: 0.6040804386138916
Epoch 560, training loss: 847.413330078125 = 0.5681109428405762 + 100.0 * 8.468452453613281
Epoch 560, val loss: 0.595469057559967
Epoch 570, training loss: 847.204833984375 = 0.5589995980262756 + 100.0 * 8.466458320617676
Epoch 570, val loss: 0.5873057842254639
Epoch 580, training loss: 847.0496215820312 = 0.5503296256065369 + 100.0 * 8.46499252319336
Epoch 580, val loss: 0.5795602798461914
Epoch 590, training loss: 847.05810546875 = 0.5420258641242981 + 100.0 * 8.465160369873047
Epoch 590, val loss: 0.5723075270652771
Epoch 600, training loss: 846.6154174804688 = 0.5342172384262085 + 100.0 * 8.460811614990234
Epoch 600, val loss: 0.5653943419456482
Epoch 610, training loss: 846.3233032226562 = 0.5268070697784424 + 100.0 * 8.457964897155762
Epoch 610, val loss: 0.558906078338623
Epoch 620, training loss: 846.038818359375 = 0.5197927355766296 + 100.0 * 8.455190658569336
Epoch 620, val loss: 0.5527828335762024
Epoch 630, training loss: 845.7824096679688 = 0.5131469964981079 + 100.0 * 8.452692985534668
Epoch 630, val loss: 0.5470210313796997
Epoch 640, training loss: 845.9968872070312 = 0.5068312883377075 + 100.0 * 8.454900741577148
Epoch 640, val loss: 0.5416343212127686
Epoch 650, training loss: 845.4588012695312 = 0.5008568167686462 + 100.0 * 8.449579238891602
Epoch 650, val loss: 0.5365192890167236
Epoch 660, training loss: 845.122802734375 = 0.495238333940506 + 100.0 * 8.44627571105957
Epoch 660, val loss: 0.5317754745483398
Epoch 670, training loss: 845.3212890625 = 0.4899388253688812 + 100.0 * 8.44831371307373
Epoch 670, val loss: 0.5272835493087769
Epoch 680, training loss: 844.8040161132812 = 0.48488396406173706 + 100.0 * 8.443191528320312
Epoch 680, val loss: 0.523239254951477
Epoch 690, training loss: 844.5280151367188 = 0.48017916083335876 + 100.0 * 8.440478324890137
Epoch 690, val loss: 0.5193809270858765
Epoch 700, training loss: 844.2659301757812 = 0.4757443964481354 + 100.0 * 8.437901496887207
Epoch 700, val loss: 0.515863835811615
Epoch 710, training loss: 844.04833984375 = 0.4715733826160431 + 100.0 * 8.435768127441406
Epoch 710, val loss: 0.5125249624252319
Epoch 720, training loss: 843.9949951171875 = 0.4676298201084137 + 100.0 * 8.435273170471191
Epoch 720, val loss: 0.5094186067581177
Epoch 730, training loss: 843.7435913085938 = 0.4638887941837311 + 100.0 * 8.4327974319458
Epoch 730, val loss: 0.5066521763801575
Epoch 740, training loss: 843.7864990234375 = 0.46038171648979187 + 100.0 * 8.433260917663574
Epoch 740, val loss: 0.5039466619491577
Epoch 750, training loss: 843.4782104492188 = 0.4570278525352478 + 100.0 * 8.430212020874023
Epoch 750, val loss: 0.5016090273857117
Epoch 760, training loss: 843.3015747070312 = 0.4538842439651489 + 100.0 * 8.42847728729248
Epoch 760, val loss: 0.49927425384521484
Epoch 770, training loss: 843.125 = 0.45089226961135864 + 100.0 * 8.426741600036621
Epoch 770, val loss: 0.4972372353076935
Epoch 780, training loss: 842.9568481445312 = 0.44805464148521423 + 100.0 * 8.425087928771973
Epoch 780, val loss: 0.4952602982521057
Epoch 790, training loss: 842.8456420898438 = 0.44535255432128906 + 100.0 * 8.424002647399902
Epoch 790, val loss: 0.49345043301582336
Epoch 800, training loss: 843.00048828125 = 0.4427763819694519 + 100.0 * 8.425577163696289
Epoch 800, val loss: 0.49172043800354004
Epoch 810, training loss: 842.6893920898438 = 0.44029635190963745 + 100.0 * 8.422491073608398
Epoch 810, val loss: 0.4902866780757904
Epoch 820, training loss: 842.5330200195312 = 0.4379538595676422 + 100.0 * 8.420950889587402
Epoch 820, val loss: 0.48878374695777893
Epoch 830, training loss: 842.3970947265625 = 0.4357149004936218 + 100.0 * 8.4196138381958
Epoch 830, val loss: 0.48747891187667847
Epoch 840, training loss: 842.3208618164062 = 0.43357160687446594 + 100.0 * 8.418872833251953
Epoch 840, val loss: 0.4862183928489685
Epoch 850, training loss: 842.6722412109375 = 0.43150967359542847 + 100.0 * 8.422407150268555
Epoch 850, val loss: 0.4850079119205475
Epoch 860, training loss: 842.180908203125 = 0.42951807379722595 + 100.0 * 8.417513847351074
Epoch 860, val loss: 0.4839239716529846
Epoch 870, training loss: 842.0272827148438 = 0.4276229441165924 + 100.0 * 8.415996551513672
Epoch 870, val loss: 0.4829365909099579
Epoch 880, training loss: 842.290283203125 = 0.42579805850982666 + 100.0 * 8.418644905090332
Epoch 880, val loss: 0.4819187521934509
Epoch 890, training loss: 841.8804321289062 = 0.42402297258377075 + 100.0 * 8.41456413269043
Epoch 890, val loss: 0.48118433356285095
Epoch 900, training loss: 841.7596435546875 = 0.4223352372646332 + 100.0 * 8.413372993469238
Epoch 900, val loss: 0.48027944564819336
Epoch 910, training loss: 841.6237182617188 = 0.42069876194000244 + 100.0 * 8.412030220031738
Epoch 910, val loss: 0.47959545254707336
Epoch 920, training loss: 841.5363159179688 = 0.4191248118877411 + 100.0 * 8.411171913146973
Epoch 920, val loss: 0.4788524806499481
Epoch 930, training loss: 841.7247924804688 = 0.4175930917263031 + 100.0 * 8.413071632385254
Epoch 930, val loss: 0.4782116711139679
Epoch 940, training loss: 841.4093627929688 = 0.4161062240600586 + 100.0 * 8.409933090209961
Epoch 940, val loss: 0.47752583026885986
Epoch 950, training loss: 841.2203979492188 = 0.4146699607372284 + 100.0 * 8.40805721282959
Epoch 950, val loss: 0.47694504261016846
Epoch 960, training loss: 841.81884765625 = 0.4132798910140991 + 100.0 * 8.414055824279785
Epoch 960, val loss: 0.47628575563430786
Epoch 970, training loss: 841.2860107421875 = 0.4119022488594055 + 100.0 * 8.408740997314453
Epoch 970, val loss: 0.475954532623291
Epoch 980, training loss: 840.92431640625 = 0.4105895459651947 + 100.0 * 8.405137062072754
Epoch 980, val loss: 0.47539016604423523
Epoch 990, training loss: 840.804443359375 = 0.40931087732315063 + 100.0 * 8.403951644897461
Epoch 990, val loss: 0.4749307632446289
Epoch 1000, training loss: 840.752685546875 = 0.40806546807289124 + 100.0 * 8.403446197509766
Epoch 1000, val loss: 0.4744742512702942
Epoch 1010, training loss: 840.980712890625 = 0.4068404734134674 + 100.0 * 8.405738830566406
Epoch 1010, val loss: 0.47404900193214417
Epoch 1020, training loss: 840.649169921875 = 0.4056383967399597 + 100.0 * 8.402435302734375
Epoch 1020, val loss: 0.47375741600990295
Epoch 1030, training loss: 840.5447387695312 = 0.4044731557369232 + 100.0 * 8.401402473449707
Epoch 1030, val loss: 0.47332748770713806
Epoch 1040, training loss: 840.3111572265625 = 0.4033353328704834 + 100.0 * 8.399078369140625
Epoch 1040, val loss: 0.47298386693000793
Epoch 1050, training loss: 840.2427368164062 = 0.40222764015197754 + 100.0 * 8.398405075073242
Epoch 1050, val loss: 0.47263598442077637
Epoch 1060, training loss: 840.1217651367188 = 0.4011397361755371 + 100.0 * 8.39720630645752
Epoch 1060, val loss: 0.47233694791793823
Epoch 1070, training loss: 840.4801635742188 = 0.4000707268714905 + 100.0 * 8.400800704956055
Epoch 1070, val loss: 0.4721251428127289
Epoch 1080, training loss: 840.15673828125 = 0.3990132510662079 + 100.0 * 8.397577285766602
Epoch 1080, val loss: 0.47178927063941956
Epoch 1090, training loss: 840.2864379882812 = 0.39796945452690125 + 100.0 * 8.398884773254395
Epoch 1090, val loss: 0.47154027223587036
Epoch 1100, training loss: 839.8604125976562 = 0.39695677161216736 + 100.0 * 8.394634246826172
Epoch 1100, val loss: 0.4711877703666687
Epoch 1110, training loss: 839.7086181640625 = 0.3959748148918152 + 100.0 * 8.393126487731934
Epoch 1110, val loss: 0.470926433801651
Epoch 1120, training loss: 839.6175537109375 = 0.39501670002937317 + 100.0 * 8.39222526550293
Epoch 1120, val loss: 0.47070202231407166
Epoch 1130, training loss: 839.54931640625 = 0.3940770924091339 + 100.0 * 8.391551971435547
Epoch 1130, val loss: 0.47048068046569824
Epoch 1140, training loss: 839.4625854492188 = 0.39315277338027954 + 100.0 * 8.390694618225098
Epoch 1140, val loss: 0.470278263092041
Epoch 1150, training loss: 839.5506591796875 = 0.39224231243133545 + 100.0 * 8.391584396362305
Epoch 1150, val loss: 0.4701211452484131
Epoch 1160, training loss: 839.4046630859375 = 0.3913302719593048 + 100.0 * 8.390132904052734
Epoch 1160, val loss: 0.4698502719402313
Epoch 1170, training loss: 839.2777709960938 = 0.3904366195201874 + 100.0 * 8.388873100280762
Epoch 1170, val loss: 0.46962207555770874
Epoch 1180, training loss: 839.2421875 = 0.38957226276397705 + 100.0 * 8.38852596282959
Epoch 1180, val loss: 0.46936121582984924
Epoch 1190, training loss: 839.1455688476562 = 0.3887210190296173 + 100.0 * 8.387568473815918
Epoch 1190, val loss: 0.4691964089870453
Epoch 1200, training loss: 839.2806396484375 = 0.38787996768951416 + 100.0 * 8.388927459716797
Epoch 1200, val loss: 0.46899381279945374
Epoch 1210, training loss: 839.1549072265625 = 0.38703370094299316 + 100.0 * 8.387679100036621
Epoch 1210, val loss: 0.46878695487976074
Epoch 1220, training loss: 839.0103149414062 = 0.3862023651599884 + 100.0 * 8.38624095916748
Epoch 1220, val loss: 0.46856260299682617
Epoch 1230, training loss: 838.8969116210938 = 0.38539376854896545 + 100.0 * 8.385115623474121
Epoch 1230, val loss: 0.4683721959590912
Epoch 1240, training loss: 838.853515625 = 0.38459792733192444 + 100.0 * 8.384689331054688
Epoch 1240, val loss: 0.46813297271728516
Epoch 1250, training loss: 839.1111450195312 = 0.38381022214889526 + 100.0 * 8.387273788452148
Epoch 1250, val loss: 0.46791914105415344
Epoch 1260, training loss: 838.8736572265625 = 0.3830074965953827 + 100.0 * 8.384906768798828
Epoch 1260, val loss: 0.46782079339027405
Epoch 1270, training loss: 839.25 = 0.38223376870155334 + 100.0 * 8.388677597045898
Epoch 1270, val loss: 0.46752506494522095
Epoch 1280, training loss: 838.7859497070312 = 0.3814430832862854 + 100.0 * 8.384044647216797
Epoch 1280, val loss: 0.4674387276172638
Epoch 1290, training loss: 838.5892333984375 = 0.3806837797164917 + 100.0 * 8.382085800170898
Epoch 1290, val loss: 0.46724316477775574
Epoch 1300, training loss: 838.4846801757812 = 0.3799300789833069 + 100.0 * 8.381047248840332
Epoch 1300, val loss: 0.46702995896339417
Epoch 1310, training loss: 838.468017578125 = 0.3791870176792145 + 100.0 * 8.380887985229492
Epoch 1310, val loss: 0.46686166524887085
Epoch 1320, training loss: 838.6001586914062 = 0.3784477710723877 + 100.0 * 8.382217407226562
Epoch 1320, val loss: 0.46663928031921387
Epoch 1330, training loss: 838.7319946289062 = 0.377704918384552 + 100.0 * 8.383543014526367
Epoch 1330, val loss: 0.4664565324783325
Epoch 1340, training loss: 838.4017333984375 = 0.3769608736038208 + 100.0 * 8.380248069763184
Epoch 1340, val loss: 0.4663504362106323
Epoch 1350, training loss: 838.2532348632812 = 0.37623530626296997 + 100.0 * 8.378769874572754
Epoch 1350, val loss: 0.4661860466003418
Epoch 1360, training loss: 838.2063598632812 = 0.3755198121070862 + 100.0 * 8.378308296203613
Epoch 1360, val loss: 0.46598684787750244
Epoch 1370, training loss: 838.2849731445312 = 0.3748050630092621 + 100.0 * 8.379101753234863
Epoch 1370, val loss: 0.46589022874832153
Epoch 1380, training loss: 838.1029663085938 = 0.37409037351608276 + 100.0 * 8.377288818359375
Epoch 1380, val loss: 0.46565836668014526
Epoch 1390, training loss: 838.6159057617188 = 0.37337857484817505 + 100.0 * 8.382425308227539
Epoch 1390, val loss: 0.4656318426132202
Epoch 1400, training loss: 838.0980224609375 = 0.37266847491264343 + 100.0 * 8.377253532409668
Epoch 1400, val loss: 0.46517422795295715
Epoch 1410, training loss: 837.9811401367188 = 0.3719640076160431 + 100.0 * 8.376091957092285
Epoch 1410, val loss: 0.4651678502559662
Epoch 1420, training loss: 837.8770141601562 = 0.37128281593322754 + 100.0 * 8.375057220458984
Epoch 1420, val loss: 0.46491751074790955
Epoch 1430, training loss: 837.8072509765625 = 0.370606392621994 + 100.0 * 8.374366760253906
Epoch 1430, val loss: 0.4647718071937561
Epoch 1440, training loss: 837.7667236328125 = 0.36993417143821716 + 100.0 * 8.373968124389648
Epoch 1440, val loss: 0.46462005376815796
Epoch 1450, training loss: 837.8041381835938 = 0.3692631423473358 + 100.0 * 8.374348640441895
Epoch 1450, val loss: 0.4644475281238556
Epoch 1460, training loss: 838.071044921875 = 0.36859118938446045 + 100.0 * 8.37702465057373
Epoch 1460, val loss: 0.4642361104488373
Epoch 1470, training loss: 837.6797485351562 = 0.3679004907608032 + 100.0 * 8.37311840057373
Epoch 1470, val loss: 0.4641491770744324
Epoch 1480, training loss: 837.6502685546875 = 0.36722904443740845 + 100.0 * 8.372830390930176
Epoch 1480, val loss: 0.4639635682106018
Epoch 1490, training loss: 837.5626220703125 = 0.36657464504241943 + 100.0 * 8.371960639953613
Epoch 1490, val loss: 0.4638291001319885
Epoch 1500, training loss: 837.6781005859375 = 0.3659217357635498 + 100.0 * 8.373122215270996
Epoch 1500, val loss: 0.46376940608024597
Epoch 1510, training loss: 837.4635009765625 = 0.36526745557785034 + 100.0 * 8.37098217010498
Epoch 1510, val loss: 0.4635302424430847
Epoch 1520, training loss: 837.413330078125 = 0.364615261554718 + 100.0 * 8.370487213134766
Epoch 1520, val loss: 0.4634101986885071
Epoch 1530, training loss: 837.37158203125 = 0.363971084356308 + 100.0 * 8.370076179504395
Epoch 1530, val loss: 0.4632209837436676
Epoch 1540, training loss: 837.3934936523438 = 0.36332985758781433 + 100.0 * 8.370301246643066
Epoch 1540, val loss: 0.46305137872695923
Epoch 1550, training loss: 837.920166015625 = 0.3626890778541565 + 100.0 * 8.375575065612793
Epoch 1550, val loss: 0.4628593325614929
Epoch 1560, training loss: 837.4263305664062 = 0.36202484369277954 + 100.0 * 8.370643615722656
Epoch 1560, val loss: 0.4628238379955292
Epoch 1570, training loss: 837.2622680664062 = 0.3613906800746918 + 100.0 * 8.369009017944336
Epoch 1570, val loss: 0.46258825063705444
Epoch 1580, training loss: 837.1766967773438 = 0.3607581555843353 + 100.0 * 8.368159294128418
Epoch 1580, val loss: 0.4624466300010681
Epoch 1590, training loss: 837.1302490234375 = 0.36013221740722656 + 100.0 * 8.367701530456543
Epoch 1590, val loss: 0.46230536699295044
Epoch 1600, training loss: 837.1229858398438 = 0.3595100939273834 + 100.0 * 8.367634773254395
Epoch 1600, val loss: 0.4621153771877289
Epoch 1610, training loss: 837.792236328125 = 0.35888901352882385 + 100.0 * 8.374333381652832
Epoch 1610, val loss: 0.4619186818599701
Epoch 1620, training loss: 837.3737182617188 = 0.3582395315170288 + 100.0 * 8.370155334472656
Epoch 1620, val loss: 0.46184849739074707
Epoch 1630, training loss: 837.0604248046875 = 0.35760998725891113 + 100.0 * 8.36702823638916
Epoch 1630, val loss: 0.4616774916648865
Epoch 1640, training loss: 836.9943237304688 = 0.35699495673179626 + 100.0 * 8.366373062133789
Epoch 1640, val loss: 0.4614829421043396
Epoch 1650, training loss: 836.9241943359375 = 0.3563830256462097 + 100.0 * 8.365677833557129
Epoch 1650, val loss: 0.46135902404785156
Epoch 1660, training loss: 836.9048461914062 = 0.35577479004859924 + 100.0 * 8.365490913391113
Epoch 1660, val loss: 0.461210161447525
Epoch 1670, training loss: 837.357177734375 = 0.3551667332649231 + 100.0 * 8.370019912719727
Epoch 1670, val loss: 0.4610227644443512
Epoch 1680, training loss: 837.1695556640625 = 0.3545389771461487 + 100.0 * 8.368149757385254
Epoch 1680, val loss: 0.46081727743148804
Epoch 1690, training loss: 836.9822387695312 = 0.3539070785045624 + 100.0 * 8.366283416748047
Epoch 1690, val loss: 0.46077975630760193
Epoch 1700, training loss: 836.78564453125 = 0.3532974421977997 + 100.0 * 8.364323616027832
Epoch 1700, val loss: 0.46058785915374756
Epoch 1710, training loss: 836.744384765625 = 0.3526895344257355 + 100.0 * 8.363917350769043
Epoch 1710, val loss: 0.4604392945766449
Epoch 1720, training loss: 836.7221069335938 = 0.3520817756652832 + 100.0 * 8.363699913024902
Epoch 1720, val loss: 0.460305780172348
Epoch 1730, training loss: 836.8589477539062 = 0.35147562623023987 + 100.0 * 8.365074157714844
Epoch 1730, val loss: 0.46009209752082825
Epoch 1740, training loss: 836.8378295898438 = 0.3508524000644684 + 100.0 * 8.364870071411133
Epoch 1740, val loss: 0.4600062668323517
Epoch 1750, training loss: 836.6439208984375 = 0.3502241373062134 + 100.0 * 8.362936973571777
Epoch 1750, val loss: 0.45987123250961304
Epoch 1760, training loss: 836.6385498046875 = 0.3496081531047821 + 100.0 * 8.362889289855957
Epoch 1760, val loss: 0.4597720205783844
Epoch 1770, training loss: 836.63134765625 = 0.34899255633354187 + 100.0 * 8.362823486328125
Epoch 1770, val loss: 0.4596099257469177
Epoch 1780, training loss: 836.8192138671875 = 0.3483704626560211 + 100.0 * 8.364707946777344
Epoch 1780, val loss: 0.4595155417919159
Epoch 1790, training loss: 836.5870361328125 = 0.3477393388748169 + 100.0 * 8.362393379211426
Epoch 1790, val loss: 0.45930734276771545
Epoch 1800, training loss: 836.5863647460938 = 0.34711623191833496 + 100.0 * 8.36239242553711
Epoch 1800, val loss: 0.45908212661743164
Epoch 1810, training loss: 836.53759765625 = 0.3464886248111725 + 100.0 * 8.361910820007324
Epoch 1810, val loss: 0.4589560031890869
Epoch 1820, training loss: 836.6445922851562 = 0.3458556830883026 + 100.0 * 8.362987518310547
Epoch 1820, val loss: 0.4588494598865509
Epoch 1830, training loss: 836.5841064453125 = 0.3452145755290985 + 100.0 * 8.362388610839844
Epoch 1830, val loss: 0.4586608111858368
Epoch 1840, training loss: 836.4825439453125 = 0.3445746600627899 + 100.0 * 8.361379623413086
Epoch 1840, val loss: 0.45843908190727234
Epoch 1850, training loss: 836.39794921875 = 0.3439360558986664 + 100.0 * 8.360540390014648
Epoch 1850, val loss: 0.4582430422306061
Epoch 1860, training loss: 836.3908081054688 = 0.3432978093624115 + 100.0 * 8.360474586486816
Epoch 1860, val loss: 0.4580727815628052
Epoch 1870, training loss: 836.886474609375 = 0.3426569104194641 + 100.0 * 8.365438461303711
Epoch 1870, val loss: 0.45782187581062317
Epoch 1880, training loss: 836.47314453125 = 0.34199151396751404 + 100.0 * 8.361311912536621
Epoch 1880, val loss: 0.45778363943099976
Epoch 1890, training loss: 836.3399047851562 = 0.3413391709327698 + 100.0 * 8.3599853515625
Epoch 1890, val loss: 0.45755815505981445
Epoch 1900, training loss: 836.2962036132812 = 0.34069523215293884 + 100.0 * 8.3595552444458
Epoch 1900, val loss: 0.45745977759361267
Epoch 1910, training loss: 836.4768676757812 = 0.3400498926639557 + 100.0 * 8.361368179321289
Epoch 1910, val loss: 0.45737025141716003
Epoch 1920, training loss: 836.2523803710938 = 0.339398056268692 + 100.0 * 8.359129905700684
Epoch 1920, val loss: 0.4570736885070801
Epoch 1930, training loss: 836.2207641601562 = 0.3387487530708313 + 100.0 * 8.358819961547852
Epoch 1930, val loss: 0.4569356441497803
Epoch 1940, training loss: 836.2281494140625 = 0.33809974789619446 + 100.0 * 8.358901023864746
Epoch 1940, val loss: 0.4567730128765106
Epoch 1950, training loss: 836.3580322265625 = 0.3374529480934143 + 100.0 * 8.36020565032959
Epoch 1950, val loss: 0.45658567547798157
Epoch 1960, training loss: 836.4680786132812 = 0.33679643273353577 + 100.0 * 8.361312866210938
Epoch 1960, val loss: 0.4564380645751953
Epoch 1970, training loss: 836.2026977539062 = 0.33613163232803345 + 100.0 * 8.358665466308594
Epoch 1970, val loss: 0.4563532769680023
Epoch 1980, training loss: 836.0759887695312 = 0.3354819715023041 + 100.0 * 8.357404708862305
Epoch 1980, val loss: 0.4561927020549774
Epoch 1990, training loss: 836.0497436523438 = 0.33483532071113586 + 100.0 * 8.357149124145508
Epoch 1990, val loss: 0.4560249447822571
Epoch 2000, training loss: 836.0809326171875 = 0.3341870903968811 + 100.0 * 8.357467651367188
Epoch 2000, val loss: 0.45588240027427673
Epoch 2010, training loss: 836.328857421875 = 0.33353176712989807 + 100.0 * 8.359952926635742
Epoch 2010, val loss: 0.4557461440563202
Epoch 2020, training loss: 836.2711181640625 = 0.332863986492157 + 100.0 * 8.359382629394531
Epoch 2020, val loss: 0.4555746018886566
Epoch 2030, training loss: 836.137451171875 = 0.3321981430053711 + 100.0 * 8.358052253723145
Epoch 2030, val loss: 0.45540910959243774
Epoch 2040, training loss: 835.9400024414062 = 0.33152666687965393 + 100.0 * 8.356084823608398
Epoch 2040, val loss: 0.4553651511669159
Epoch 2050, training loss: 835.9356079101562 = 0.3308654725551605 + 100.0 * 8.356047630310059
Epoch 2050, val loss: 0.4552718997001648
Epoch 2060, training loss: 835.9111938476562 = 0.3302062749862671 + 100.0 * 8.355810165405273
Epoch 2060, val loss: 0.45514294505119324
Epoch 2070, training loss: 836.059814453125 = 0.32954132556915283 + 100.0 * 8.35730266571045
Epoch 2070, val loss: 0.45505598187446594
Epoch 2080, training loss: 835.9872436523438 = 0.3288663625717163 + 100.0 * 8.356583595275879
Epoch 2080, val loss: 0.4548426866531372
Epoch 2090, training loss: 835.9888305664062 = 0.3281960189342499 + 100.0 * 8.356606483459473
Epoch 2090, val loss: 0.4546014964580536
Epoch 2100, training loss: 835.9069213867188 = 0.3275134265422821 + 100.0 * 8.355793952941895
Epoch 2100, val loss: 0.4545109272003174
Epoch 2110, training loss: 835.7753295898438 = 0.32682812213897705 + 100.0 * 8.354484558105469
Epoch 2110, val loss: 0.45443230867385864
Epoch 2120, training loss: 835.7896118164062 = 0.32614749670028687 + 100.0 * 8.354634284973145
Epoch 2120, val loss: 0.4542939364910126
Epoch 2130, training loss: 835.8250122070312 = 0.32546427845954895 + 100.0 * 8.354995727539062
Epoch 2130, val loss: 0.45417433977127075
Epoch 2140, training loss: 835.912353515625 = 0.32477861642837524 + 100.0 * 8.355875968933105
Epoch 2140, val loss: 0.4540400803089142
Epoch 2150, training loss: 835.7182006835938 = 0.3240858018398285 + 100.0 * 8.353940963745117
Epoch 2150, val loss: 0.4538664221763611
Epoch 2160, training loss: 835.7056274414062 = 0.3233984410762787 + 100.0 * 8.353821754455566
Epoch 2160, val loss: 0.45375972986221313
Epoch 2170, training loss: 835.6362915039062 = 0.3227142095565796 + 100.0 * 8.35313606262207
Epoch 2170, val loss: 0.4536404609680176
Epoch 2180, training loss: 836.03857421875 = 0.322028785943985 + 100.0 * 8.357165336608887
Epoch 2180, val loss: 0.4536612033843994
Epoch 2190, training loss: 835.7649536132812 = 0.3213287591934204 + 100.0 * 8.354435920715332
Epoch 2190, val loss: 0.45331934094429016
Epoch 2200, training loss: 835.691650390625 = 0.3206274211406708 + 100.0 * 8.353710174560547
Epoch 2200, val loss: 0.453258752822876
Epoch 2210, training loss: 835.5226440429688 = 0.31993919610977173 + 100.0 * 8.35202693939209
Epoch 2210, val loss: 0.45310136675834656
Epoch 2220, training loss: 835.4835205078125 = 0.31925028562545776 + 100.0 * 8.351642608642578
Epoch 2220, val loss: 0.4529868960380554
Epoch 2230, training loss: 835.4674682617188 = 0.3185597360134125 + 100.0 * 8.351489067077637
Epoch 2230, val loss: 0.4529106914997101
Epoch 2240, training loss: 835.6907958984375 = 0.3178667426109314 + 100.0 * 8.353729248046875
Epoch 2240, val loss: 0.45283734798431396
Epoch 2250, training loss: 835.6649780273438 = 0.3171581029891968 + 100.0 * 8.35347843170166
Epoch 2250, val loss: 0.4526916444301605
Epoch 2260, training loss: 835.5203857421875 = 0.3164442479610443 + 100.0 * 8.352039337158203
Epoch 2260, val loss: 0.45259201526641846
Epoch 2270, training loss: 835.4457397460938 = 0.3157432973384857 + 100.0 * 8.351300239562988
Epoch 2270, val loss: 0.45244917273521423
Epoch 2280, training loss: 835.3626708984375 = 0.31503891944885254 + 100.0 * 8.350476264953613
Epoch 2280, val loss: 0.4523613154888153
Epoch 2290, training loss: 835.3745727539062 = 0.3143388628959656 + 100.0 * 8.350602149963379
Epoch 2290, val loss: 0.45229291915893555
Epoch 2300, training loss: 835.606201171875 = 0.3136383295059204 + 100.0 * 8.352925300598145
Epoch 2300, val loss: 0.4521516263484955
Epoch 2310, training loss: 835.640625 = 0.3129173815250397 + 100.0 * 8.353277206420898
Epoch 2310, val loss: 0.45210468769073486
Epoch 2320, training loss: 835.3755493164062 = 0.3121941387653351 + 100.0 * 8.35063362121582
Epoch 2320, val loss: 0.45210084319114685
Epoch 2330, training loss: 835.2614135742188 = 0.3114781081676483 + 100.0 * 8.349499702453613
Epoch 2330, val loss: 0.45195290446281433
Epoch 2340, training loss: 835.206298828125 = 0.31076639890670776 + 100.0 * 8.348955154418945
Epoch 2340, val loss: 0.45189347863197327
Epoch 2350, training loss: 835.23388671875 = 0.3100528120994568 + 100.0 * 8.349238395690918
Epoch 2350, val loss: 0.45181524753570557
Epoch 2360, training loss: 835.938720703125 = 0.3093308210372925 + 100.0 * 8.356293678283691
Epoch 2360, val loss: 0.45181068778038025
Epoch 2370, training loss: 835.2990112304688 = 0.30858761072158813 + 100.0 * 8.34990406036377
Epoch 2370, val loss: 0.45163625478744507
Epoch 2380, training loss: 835.150146484375 = 0.30785176157951355 + 100.0 * 8.34842300415039
Epoch 2380, val loss: 0.4515353739261627
Epoch 2390, training loss: 835.1116943359375 = 0.307119220495224 + 100.0 * 8.348045349121094
Epoch 2390, val loss: 0.4514743685722351
Epoch 2400, training loss: 835.0843505859375 = 0.30638545751571655 + 100.0 * 8.347779273986816
Epoch 2400, val loss: 0.4514029026031494
Epoch 2410, training loss: 835.3539428710938 = 0.30565378069877625 + 100.0 * 8.350482940673828
Epoch 2410, val loss: 0.451235294342041
Epoch 2420, training loss: 835.1812133789062 = 0.30489346385002136 + 100.0 * 8.348763465881348
Epoch 2420, val loss: 0.4512644112110138
Epoch 2430, training loss: 835.0877075195312 = 0.3041382431983948 + 100.0 * 8.347835540771484
Epoch 2430, val loss: 0.4511077105998993
Epoch 2440, training loss: 835.006591796875 = 0.3033846914768219 + 100.0 * 8.347031593322754
Epoch 2440, val loss: 0.4511148929595947
Epoch 2450, training loss: 834.967041015625 = 0.302636981010437 + 100.0 * 8.346644401550293
Epoch 2450, val loss: 0.4510134756565094
Epoch 2460, training loss: 835.1160278320312 = 0.3018878400325775 + 100.0 * 8.34814167022705
Epoch 2460, val loss: 0.45104527473449707
Epoch 2470, training loss: 835.2575073242188 = 0.3011198043823242 + 100.0 * 8.349563598632812
Epoch 2470, val loss: 0.45094144344329834
Epoch 2480, training loss: 834.9686889648438 = 0.30034199357032776 + 100.0 * 8.346683502197266
Epoch 2480, val loss: 0.4507814049720764
Epoch 2490, training loss: 834.8797607421875 = 0.29956409335136414 + 100.0 * 8.345802307128906
Epoch 2490, val loss: 0.45075640082359314
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8112633181126332
0.8652466855031515
=== training gcn model ===
Epoch 0, training loss: 1059.3446044921875 = 1.1149170398712158 + 100.0 * 10.582297325134277
Epoch 0, val loss: 1.113219141960144
Epoch 10, training loss: 1059.2996826171875 = 1.1102389097213745 + 100.0 * 10.581894874572754
Epoch 10, val loss: 1.1085495948791504
Epoch 20, training loss: 1059.1162109375 = 1.1050631999969482 + 100.0 * 10.580110549926758
Epoch 20, val loss: 1.1034071445465088
Epoch 30, training loss: 1058.331787109375 = 1.0993103981018066 + 100.0 * 10.572324752807617
Epoch 30, val loss: 1.0976923704147339
Epoch 40, training loss: 1055.365478515625 = 1.0927220582962036 + 100.0 * 10.542726516723633
Epoch 40, val loss: 1.091115117073059
Epoch 50, training loss: 1046.7955322265625 = 1.0850497484207153 + 100.0 * 10.457104682922363
Epoch 50, val loss: 1.083477258682251
Epoch 60, training loss: 1028.2012939453125 = 1.0764905214309692 + 100.0 * 10.271247863769531
Epoch 60, val loss: 1.0750505924224854
Epoch 70, training loss: 997.859619140625 = 1.0666441917419434 + 100.0 * 9.96792984008789
Epoch 70, val loss: 1.0652506351470947
Epoch 80, training loss: 962.1004028320312 = 1.05520761013031 + 100.0 * 9.610451698303223
Epoch 80, val loss: 1.0539394617080688
Epoch 90, training loss: 944.95751953125 = 1.0445780754089355 + 100.0 * 9.439129829406738
Epoch 90, val loss: 1.0436688661575317
Epoch 100, training loss: 930.9797973632812 = 1.036453366279602 + 100.0 * 9.299433708190918
Epoch 100, val loss: 1.0358946323394775
Epoch 110, training loss: 922.4727172851562 = 1.030078411102295 + 100.0 * 9.214426040649414
Epoch 110, val loss: 1.0297226905822754
Epoch 120, training loss: 919.037841796875 = 1.0236918926239014 + 100.0 * 9.18014144897461
Epoch 120, val loss: 1.0235038995742798
Epoch 130, training loss: 915.3380126953125 = 1.016765832901001 + 100.0 * 9.14321231842041
Epoch 130, val loss: 1.0168291330337524
Epoch 140, training loss: 910.3550415039062 = 1.0104624032974243 + 100.0 * 9.093445777893066
Epoch 140, val loss: 1.010844111442566
Epoch 150, training loss: 904.49169921875 = 1.0055937767028809 + 100.0 * 9.034860610961914
Epoch 150, val loss: 1.006213903427124
Epoch 160, training loss: 900.8865966796875 = 1.0006991624832153 + 100.0 * 8.998859405517578
Epoch 160, val loss: 1.0012820959091187
Epoch 170, training loss: 899.4976806640625 = 0.9935256242752075 + 100.0 * 8.985041618347168
Epoch 170, val loss: 0.9940928220748901
Epoch 180, training loss: 898.0164184570312 = 0.9854434132575989 + 100.0 * 8.970309257507324
Epoch 180, val loss: 0.986261248588562
Epoch 190, training loss: 896.5897216796875 = 0.9783535003662109 + 100.0 * 8.956113815307617
Epoch 190, val loss: 0.9794092178344727
Epoch 200, training loss: 894.868408203125 = 0.9717844724655151 + 100.0 * 8.938965797424316
Epoch 200, val loss: 0.9730499982833862
Epoch 210, training loss: 892.3692016601562 = 0.9654713273048401 + 100.0 * 8.914037704467773
Epoch 210, val loss: 0.9670121669769287
Epoch 220, training loss: 888.6260375976562 = 0.960075318813324 + 100.0 * 8.876659393310547
Epoch 220, val loss: 0.9619458317756653
Epoch 230, training loss: 885.715087890625 = 0.9554105997085571 + 100.0 * 8.847597122192383
Epoch 230, val loss: 0.9575363397598267
Epoch 240, training loss: 883.300537109375 = 0.9485618472099304 + 100.0 * 8.823519706726074
Epoch 240, val loss: 0.9505887627601624
Epoch 250, training loss: 881.3887329101562 = 0.9401376843452454 + 100.0 * 8.804486274719238
Epoch 250, val loss: 0.9422950744628906
Epoch 260, training loss: 879.4776611328125 = 0.9316736459732056 + 100.0 * 8.785459518432617
Epoch 260, val loss: 0.9341551065444946
Epoch 270, training loss: 877.2526245117188 = 0.9237803220748901 + 100.0 * 8.763288497924805
Epoch 270, val loss: 0.9266381859779358
Epoch 280, training loss: 875.1900634765625 = 0.9159901142120361 + 100.0 * 8.742740631103516
Epoch 280, val loss: 0.9191578030586243
Epoch 290, training loss: 873.8031005859375 = 0.9074533581733704 + 100.0 * 8.72895622253418
Epoch 290, val loss: 0.9108728766441345
Epoch 300, training loss: 872.7099609375 = 0.8979105353355408 + 100.0 * 8.718120574951172
Epoch 300, val loss: 0.9016754627227783
Epoch 310, training loss: 871.6192626953125 = 0.8881577849388123 + 100.0 * 8.707310676574707
Epoch 310, val loss: 0.8923103213310242
Epoch 320, training loss: 870.2101440429688 = 0.878686785697937 + 100.0 * 8.693314552307129
Epoch 320, val loss: 0.88333660364151
Epoch 330, training loss: 868.6423950195312 = 0.8695016503334045 + 100.0 * 8.677728652954102
Epoch 330, val loss: 0.8745933175086975
Epoch 340, training loss: 867.283203125 = 0.8599825501441956 + 100.0 * 8.66423225402832
Epoch 340, val loss: 0.865546703338623
Epoch 350, training loss: 865.9625244140625 = 0.8493592143058777 + 100.0 * 8.651131629943848
Epoch 350, val loss: 0.8552550673484802
Epoch 360, training loss: 864.8883056640625 = 0.8383854031562805 + 100.0 * 8.640499114990234
Epoch 360, val loss: 0.8447280526161194
Epoch 370, training loss: 863.8685302734375 = 0.8280753493309021 + 100.0 * 8.630404472351074
Epoch 370, val loss: 0.8348978757858276
Epoch 380, training loss: 862.6817016601562 = 0.8183668255805969 + 100.0 * 8.618633270263672
Epoch 380, val loss: 0.8257019519805908
Epoch 390, training loss: 861.3975830078125 = 0.8090044856071472 + 100.0 * 8.60588550567627
Epoch 390, val loss: 0.8169090747833252
Epoch 400, training loss: 860.1748046875 = 0.7996456623077393 + 100.0 * 8.593751907348633
Epoch 400, val loss: 0.8080012798309326
Epoch 410, training loss: 859.04736328125 = 0.7901428937911987 + 100.0 * 8.582571983337402
Epoch 410, val loss: 0.7989919781684875
Epoch 420, training loss: 858.2083129882812 = 0.7804305553436279 + 100.0 * 8.574278831481934
Epoch 420, val loss: 0.789826512336731
Epoch 430, training loss: 857.5781860351562 = 0.7704746127128601 + 100.0 * 8.568077087402344
Epoch 430, val loss: 0.7803652286529541
Epoch 440, training loss: 857.206787109375 = 0.7603273391723633 + 100.0 * 8.564464569091797
Epoch 440, val loss: 0.7707088589668274
Epoch 450, training loss: 856.6768798828125 = 0.7500394582748413 + 100.0 * 8.5592679977417
Epoch 450, val loss: 0.7610742449760437
Epoch 460, training loss: 856.1083984375 = 0.7400442361831665 + 100.0 * 8.553683280944824
Epoch 460, val loss: 0.7516152262687683
Epoch 470, training loss: 855.6151123046875 = 0.7302655577659607 + 100.0 * 8.548848152160645
Epoch 470, val loss: 0.742368757724762
Epoch 480, training loss: 855.104736328125 = 0.7207082509994507 + 100.0 * 8.543840408325195
Epoch 480, val loss: 0.7334030270576477
Epoch 490, training loss: 854.7095336914062 = 0.7113474011421204 + 100.0 * 8.539981842041016
Epoch 490, val loss: 0.7246224284172058
Epoch 500, training loss: 854.0699462890625 = 0.7022262811660767 + 100.0 * 8.533677101135254
Epoch 500, val loss: 0.7159584164619446
Epoch 510, training loss: 853.5239868164062 = 0.6932810544967651 + 100.0 * 8.52830696105957
Epoch 510, val loss: 0.7075985670089722
Epoch 520, training loss: 852.9673461914062 = 0.6844194531440735 + 100.0 * 8.522829055786133
Epoch 520, val loss: 0.6992244124412537
Epoch 530, training loss: 852.4650268554688 = 0.67568439245224 + 100.0 * 8.51789379119873
Epoch 530, val loss: 0.6910552978515625
Epoch 540, training loss: 852.212890625 = 0.6670828461647034 + 100.0 * 8.515458106994629
Epoch 540, val loss: 0.6829425692558289
Epoch 550, training loss: 851.5650634765625 = 0.6585581302642822 + 100.0 * 8.509064674377441
Epoch 550, val loss: 0.6749659180641174
Epoch 560, training loss: 851.0892944335938 = 0.6502177715301514 + 100.0 * 8.504390716552734
Epoch 560, val loss: 0.6671503782272339
Epoch 570, training loss: 850.512451171875 = 0.6420407295227051 + 100.0 * 8.498703956604004
Epoch 570, val loss: 0.6595633029937744
Epoch 580, training loss: 850.0686645507812 = 0.6339887976646423 + 100.0 * 8.494346618652344
Epoch 580, val loss: 0.6520693302154541
Epoch 590, training loss: 849.5755004882812 = 0.6261545419692993 + 100.0 * 8.489493370056152
Epoch 590, val loss: 0.64480060338974
Epoch 600, training loss: 849.0960083007812 = 0.6184754967689514 + 100.0 * 8.48477554321289
Epoch 600, val loss: 0.6377255916595459
Epoch 610, training loss: 848.9361572265625 = 0.6109247803688049 + 100.0 * 8.48325252532959
Epoch 610, val loss: 0.6307989954948425
Epoch 620, training loss: 848.3195190429688 = 0.6035475134849548 + 100.0 * 8.47715950012207
Epoch 620, val loss: 0.6240834593772888
Epoch 630, training loss: 847.9016723632812 = 0.5965208411216736 + 100.0 * 8.473052024841309
Epoch 630, val loss: 0.6176936626434326
Epoch 640, training loss: 847.5929565429688 = 0.5896459221839905 + 100.0 * 8.470032691955566
Epoch 640, val loss: 0.6114624738693237
Epoch 650, training loss: 847.2866821289062 = 0.5828815698623657 + 100.0 * 8.46703815460205
Epoch 650, val loss: 0.6052972674369812
Epoch 660, training loss: 847.10009765625 = 0.5764216780662537 + 100.0 * 8.46523666381836
Epoch 660, val loss: 0.5995019674301147
Epoch 670, training loss: 846.7847290039062 = 0.5703026652336121 + 100.0 * 8.462143898010254
Epoch 670, val loss: 0.5940223336219788
Epoch 680, training loss: 846.4801025390625 = 0.564375102519989 + 100.0 * 8.45915699005127
Epoch 680, val loss: 0.5886977910995483
Epoch 690, training loss: 846.2423706054688 = 0.5587201118469238 + 100.0 * 8.456836700439453
Epoch 690, val loss: 0.5836369395256042
Epoch 700, training loss: 846.05517578125 = 0.5533146262168884 + 100.0 * 8.455018043518066
Epoch 700, val loss: 0.5789036154747009
Epoch 710, training loss: 846.0558471679688 = 0.5480374097824097 + 100.0 * 8.455078125
Epoch 710, val loss: 0.5741347074508667
Epoch 720, training loss: 845.6702880859375 = 0.5431015491485596 + 100.0 * 8.451272010803223
Epoch 720, val loss: 0.5698366165161133
Epoch 730, training loss: 845.3758544921875 = 0.5384142994880676 + 100.0 * 8.44837474822998
Epoch 730, val loss: 0.5657601356506348
Epoch 740, training loss: 845.1748657226562 = 0.5338975787162781 + 100.0 * 8.446410179138184
Epoch 740, val loss: 0.5618458390235901
Epoch 750, training loss: 845.264892578125 = 0.5295262932777405 + 100.0 * 8.44735336303711
Epoch 750, val loss: 0.5580573678016663
Epoch 760, training loss: 844.9263916015625 = 0.5254019498825073 + 100.0 * 8.444009780883789
Epoch 760, val loss: 0.5545288324356079
Epoch 770, training loss: 844.584228515625 = 0.5215734839439392 + 100.0 * 8.440627098083496
Epoch 770, val loss: 0.5512810945510864
Epoch 780, training loss: 844.3937377929688 = 0.5178765058517456 + 100.0 * 8.438758850097656
Epoch 780, val loss: 0.5481290817260742
Epoch 790, training loss: 844.2031860351562 = 0.514357328414917 + 100.0 * 8.436888694763184
Epoch 790, val loss: 0.5451878309249878
Epoch 800, training loss: 844.4315795898438 = 0.5110039114952087 + 100.0 * 8.43920612335205
Epoch 800, val loss: 0.5425450205802917
Epoch 810, training loss: 843.869384765625 = 0.5077180862426758 + 100.0 * 8.433616638183594
Epoch 810, val loss: 0.5398012399673462
Epoch 820, training loss: 843.6898193359375 = 0.5046728849411011 + 100.0 * 8.431851387023926
Epoch 820, val loss: 0.5372740030288696
Epoch 830, training loss: 843.5244750976562 = 0.5017539858818054 + 100.0 * 8.430227279663086
Epoch 830, val loss: 0.5349259972572327
Epoch 840, training loss: 843.449951171875 = 0.49896910786628723 + 100.0 * 8.429510116577148
Epoch 840, val loss: 0.5326941013336182
Epoch 850, training loss: 843.2408447265625 = 0.49624699354171753 + 100.0 * 8.427445411682129
Epoch 850, val loss: 0.5305227041244507
Epoch 860, training loss: 843.1671752929688 = 0.49372753500938416 + 100.0 * 8.426734924316406
Epoch 860, val loss: 0.5285467505455017
Epoch 870, training loss: 842.9179077148438 = 0.4913134276866913 + 100.0 * 8.42426586151123
Epoch 870, val loss: 0.5266285538673401
Epoch 880, training loss: 842.7687377929688 = 0.48898476362228394 + 100.0 * 8.422797203063965
Epoch 880, val loss: 0.5248432755470276
Epoch 890, training loss: 842.6365966796875 = 0.48676058650016785 + 100.0 * 8.42149829864502
Epoch 890, val loss: 0.5231541991233826
Epoch 900, training loss: 842.9612426757812 = 0.48458215594291687 + 100.0 * 8.424766540527344
Epoch 900, val loss: 0.5214516520500183
Epoch 910, training loss: 842.572021484375 = 0.482463002204895 + 100.0 * 8.42089557647705
Epoch 910, val loss: 0.5198915004730225
Epoch 920, training loss: 842.2952880859375 = 0.48047301173210144 + 100.0 * 8.418148040771484
Epoch 920, val loss: 0.5184828042984009
Epoch 930, training loss: 842.1715698242188 = 0.47857600450515747 + 100.0 * 8.416930198669434
Epoch 930, val loss: 0.5171334743499756
Epoch 940, training loss: 842.2169799804688 = 0.47673898935317993 + 100.0 * 8.417402267456055
Epoch 940, val loss: 0.5158180594444275
Epoch 950, training loss: 841.8956298828125 = 0.4749716818332672 + 100.0 * 8.414206504821777
Epoch 950, val loss: 0.5145432949066162
Epoch 960, training loss: 841.8161010742188 = 0.47328439354896545 + 100.0 * 8.41342830657959
Epoch 960, val loss: 0.5133588314056396
Epoch 970, training loss: 841.7199096679688 = 0.47164857387542725 + 100.0 * 8.412483215332031
Epoch 970, val loss: 0.5122402310371399
Epoch 980, training loss: 841.54638671875 = 0.4700652062892914 + 100.0 * 8.410762786865234
Epoch 980, val loss: 0.5111713409423828
Epoch 990, training loss: 841.845458984375 = 0.4685467779636383 + 100.0 * 8.413768768310547
Epoch 990, val loss: 0.5101613998413086
Epoch 1000, training loss: 841.463134765625 = 0.4670092463493347 + 100.0 * 8.409961700439453
Epoch 1000, val loss: 0.5092028975486755
Epoch 1010, training loss: 841.2454833984375 = 0.46560391783714294 + 100.0 * 8.407798767089844
Epoch 1010, val loss: 0.5082021355628967
Epoch 1020, training loss: 841.0824584960938 = 0.4642419219017029 + 100.0 * 8.406182289123535
Epoch 1020, val loss: 0.5073788166046143
Epoch 1030, training loss: 840.9358520507812 = 0.4629131853580475 + 100.0 * 8.404729843139648
Epoch 1030, val loss: 0.5065084099769592
Epoch 1040, training loss: 840.8176879882812 = 0.46163418889045715 + 100.0 * 8.403560638427734
Epoch 1040, val loss: 0.5056940317153931
Epoch 1050, training loss: 840.73779296875 = 0.4603845179080963 + 100.0 * 8.4027738571167
Epoch 1050, val loss: 0.5049054622650146
Epoch 1060, training loss: 841.1076049804688 = 0.45914196968078613 + 100.0 * 8.406484603881836
Epoch 1060, val loss: 0.504203200340271
Epoch 1070, training loss: 840.647705078125 = 0.45791006088256836 + 100.0 * 8.401898384094238
Epoch 1070, val loss: 0.5032557249069214
Epoch 1080, training loss: 840.7274169921875 = 0.45673277974128723 + 100.0 * 8.40270709991455
Epoch 1080, val loss: 0.5025080442428589
Epoch 1090, training loss: 840.4024047851562 = 0.45557206869125366 + 100.0 * 8.399468421936035
Epoch 1090, val loss: 0.5018702149391174
Epoch 1100, training loss: 840.221923828125 = 0.4544735550880432 + 100.0 * 8.397674560546875
Epoch 1100, val loss: 0.5011204481124878
Epoch 1110, training loss: 840.1026611328125 = 0.4533982276916504 + 100.0 * 8.396492958068848
Epoch 1110, val loss: 0.5004990100860596
Epoch 1120, training loss: 840.0027465820312 = 0.452351838350296 + 100.0 * 8.395503997802734
Epoch 1120, val loss: 0.4998309910297394
Epoch 1130, training loss: 840.173583984375 = 0.4513271450996399 + 100.0 * 8.397222518920898
Epoch 1130, val loss: 0.49926629662513733
Epoch 1140, training loss: 839.964599609375 = 0.45026394724845886 + 100.0 * 8.395143508911133
Epoch 1140, val loss: 0.4985097348690033
Epoch 1150, training loss: 839.8038940429688 = 0.44929540157318115 + 100.0 * 8.393546104431152
Epoch 1150, val loss: 0.4979623854160309
Epoch 1160, training loss: 839.917236328125 = 0.4483318030834198 + 100.0 * 8.394689559936523
Epoch 1160, val loss: 0.4974862039089203
Epoch 1170, training loss: 839.5797729492188 = 0.4474073052406311 + 100.0 * 8.391324043273926
Epoch 1170, val loss: 0.4969365894794464
Epoch 1180, training loss: 839.4500122070312 = 0.44651633501052856 + 100.0 * 8.390034675598145
Epoch 1180, val loss: 0.49634143710136414
Epoch 1190, training loss: 839.326904296875 = 0.4456372857093811 + 100.0 * 8.388813018798828
Epoch 1190, val loss: 0.4959130585193634
Epoch 1200, training loss: 839.2970581054688 = 0.4447725713253021 + 100.0 * 8.38852310180664
Epoch 1200, val loss: 0.49541887640953064
Epoch 1210, training loss: 839.4234619140625 = 0.4438907504081726 + 100.0 * 8.389795303344727
Epoch 1210, val loss: 0.49492162466049194
Epoch 1220, training loss: 839.070556640625 = 0.44305190443992615 + 100.0 * 8.386275291442871
Epoch 1220, val loss: 0.49448147416114807
Epoch 1230, training loss: 838.9622192382812 = 0.44223839044570923 + 100.0 * 8.385199546813965
Epoch 1230, val loss: 0.4941086173057556
Epoch 1240, training loss: 838.8740234375 = 0.44143426418304443 + 100.0 * 8.384325981140137
Epoch 1240, val loss: 0.4936515688896179
Epoch 1250, training loss: 838.8209228515625 = 0.4406450390815735 + 100.0 * 8.38380241394043
Epoch 1250, val loss: 0.4932210147380829
Epoch 1260, training loss: 839.1006469726562 = 0.43984153866767883 + 100.0 * 8.386608123779297
Epoch 1260, val loss: 0.4928343892097473
Epoch 1270, training loss: 838.6995239257812 = 0.43905970454216003 + 100.0 * 8.382604598999023
Epoch 1270, val loss: 0.4924185872077942
Epoch 1280, training loss: 838.5791625976562 = 0.43830397725105286 + 100.0 * 8.38140869140625
Epoch 1280, val loss: 0.4920703172683716
Epoch 1290, training loss: 838.4839477539062 = 0.43755292892456055 + 100.0 * 8.380463600158691
Epoch 1290, val loss: 0.4916839897632599
Epoch 1300, training loss: 838.4808959960938 = 0.4368184804916382 + 100.0 * 8.380440711975098
Epoch 1300, val loss: 0.49133288860321045
Epoch 1310, training loss: 838.699462890625 = 0.436065137386322 + 100.0 * 8.382634162902832
Epoch 1310, val loss: 0.49102291464805603
Epoch 1320, training loss: 838.3287963867188 = 0.43532562255859375 + 100.0 * 8.378934860229492
Epoch 1320, val loss: 0.4905700087547302
Epoch 1330, training loss: 838.2242431640625 = 0.43461355566978455 + 100.0 * 8.377896308898926
Epoch 1330, val loss: 0.4903143346309662
Epoch 1340, training loss: 838.1766357421875 = 0.4339040219783783 + 100.0 * 8.377427101135254
Epoch 1340, val loss: 0.48992064595222473
Epoch 1350, training loss: 838.364501953125 = 0.4332009553909302 + 100.0 * 8.379312515258789
Epoch 1350, val loss: 0.4895187020301819
Epoch 1360, training loss: 838.1647338867188 = 0.4324987232685089 + 100.0 * 8.37732219696045
Epoch 1360, val loss: 0.48938071727752686
Epoch 1370, training loss: 838.067138671875 = 0.4318036437034607 + 100.0 * 8.37635326385498
Epoch 1370, val loss: 0.4889758825302124
Epoch 1380, training loss: 838.00244140625 = 0.4311158061027527 + 100.0 * 8.375713348388672
Epoch 1380, val loss: 0.48865213990211487
Epoch 1390, training loss: 838.1783447265625 = 0.4304366707801819 + 100.0 * 8.377479553222656
Epoch 1390, val loss: 0.4883674085140228
Epoch 1400, training loss: 837.8515625 = 0.42975711822509766 + 100.0 * 8.374217987060547
Epoch 1400, val loss: 0.48803889751434326
Epoch 1410, training loss: 837.8212890625 = 0.42910173535346985 + 100.0 * 8.373922348022461
Epoch 1410, val loss: 0.48777055740356445
Epoch 1420, training loss: 837.7792358398438 = 0.42844516038894653 + 100.0 * 8.373507499694824
Epoch 1420, val loss: 0.48745283484458923
Epoch 1430, training loss: 837.932373046875 = 0.42779356241226196 + 100.0 * 8.375045776367188
Epoch 1430, val loss: 0.4872245788574219
Epoch 1440, training loss: 837.7075805664062 = 0.4271441400051117 + 100.0 * 8.372804641723633
Epoch 1440, val loss: 0.4869014322757721
Epoch 1450, training loss: 837.6905517578125 = 0.4265073835849762 + 100.0 * 8.372640609741211
Epoch 1450, val loss: 0.48659688234329224
Epoch 1460, training loss: 837.9197387695312 = 0.42585739493370056 + 100.0 * 8.37493896484375
Epoch 1460, val loss: 0.4863723814487457
Epoch 1470, training loss: 837.5746459960938 = 0.42520907521247864 + 100.0 * 8.37149429321289
Epoch 1470, val loss: 0.4859614670276642
Epoch 1480, training loss: 837.4403686523438 = 0.4245959520339966 + 100.0 * 8.370157241821289
Epoch 1480, val loss: 0.4858413338661194
Epoch 1490, training loss: 837.38134765625 = 0.4239874482154846 + 100.0 * 8.369573593139648
Epoch 1490, val loss: 0.4854641556739807
Epoch 1500, training loss: 837.3338623046875 = 0.4233870208263397 + 100.0 * 8.369104385375977
Epoch 1500, val loss: 0.48525798320770264
Epoch 1510, training loss: 837.2853393554688 = 0.42279180884361267 + 100.0 * 8.36862564086914
Epoch 1510, val loss: 0.48499608039855957
Epoch 1520, training loss: 837.7238159179688 = 0.4221929609775543 + 100.0 * 8.373016357421875
Epoch 1520, val loss: 0.4846649765968323
Epoch 1530, training loss: 837.4680786132812 = 0.4215765595436096 + 100.0 * 8.370465278625488
Epoch 1530, val loss: 0.4845283329486847
Epoch 1540, training loss: 837.142578125 = 0.42098698019981384 + 100.0 * 8.367216110229492
Epoch 1540, val loss: 0.48419877886772156
Epoch 1550, training loss: 837.1376342773438 = 0.4204038083553314 + 100.0 * 8.367172241210938
Epoch 1550, val loss: 0.4839853048324585
Epoch 1560, training loss: 837.0387573242188 = 0.4198373258113861 + 100.0 * 8.366189002990723
Epoch 1560, val loss: 0.48372623324394226
Epoch 1570, training loss: 837.0115356445312 = 0.4192765951156616 + 100.0 * 8.365922927856445
Epoch 1570, val loss: 0.48353976011276245
Epoch 1580, training loss: 837.4492797851562 = 0.41871345043182373 + 100.0 * 8.370306015014648
Epoch 1580, val loss: 0.48334747552871704
Epoch 1590, training loss: 837.140869140625 = 0.418109267950058 + 100.0 * 8.367227554321289
Epoch 1590, val loss: 0.48301631212234497
Epoch 1600, training loss: 836.957275390625 = 0.4175497591495514 + 100.0 * 8.365397453308105
Epoch 1600, val loss: 0.4828989803791046
Epoch 1610, training loss: 836.9629516601562 = 0.4169822931289673 + 100.0 * 8.365459442138672
Epoch 1610, val loss: 0.48264962434768677
Epoch 1620, training loss: 836.8062744140625 = 0.4164213538169861 + 100.0 * 8.363898277282715
Epoch 1620, val loss: 0.48243263363838196
Epoch 1630, training loss: 836.7530517578125 = 0.4158734679222107 + 100.0 * 8.363371849060059
Epoch 1630, val loss: 0.48220115900039673
Epoch 1640, training loss: 836.8346557617188 = 0.41532519459724426 + 100.0 * 8.364192962646484
Epoch 1640, val loss: 0.4819919168949127
Epoch 1650, training loss: 836.7349243164062 = 0.414758563041687 + 100.0 * 8.363202095031738
Epoch 1650, val loss: 0.48179858922958374
Epoch 1660, training loss: 836.6570434570312 = 0.41419753432273865 + 100.0 * 8.362428665161133
Epoch 1660, val loss: 0.4816237688064575
Epoch 1670, training loss: 836.5938720703125 = 0.4136420786380768 + 100.0 * 8.361802101135254
Epoch 1670, val loss: 0.4813733398914337
Epoch 1680, training loss: 836.5904541015625 = 0.41309210658073425 + 100.0 * 8.361773490905762
Epoch 1680, val loss: 0.4811360538005829
Epoch 1690, training loss: 836.6024780273438 = 0.4125380516052246 + 100.0 * 8.361899375915527
Epoch 1690, val loss: 0.48095905780792236
Epoch 1700, training loss: 836.6748657226562 = 0.4119885265827179 + 100.0 * 8.362628936767578
Epoch 1700, val loss: 0.480871319770813
Epoch 1710, training loss: 836.4522094726562 = 0.41143083572387695 + 100.0 * 8.360407829284668
Epoch 1710, val loss: 0.48061010241508484
Epoch 1720, training loss: 836.414794921875 = 0.4108961522579193 + 100.0 * 8.360038757324219
Epoch 1720, val loss: 0.4804168939590454
Epoch 1730, training loss: 836.3980102539062 = 0.410362184047699 + 100.0 * 8.35987663269043
Epoch 1730, val loss: 0.48028743267059326
Epoch 1740, training loss: 836.5000610351562 = 0.40982532501220703 + 100.0 * 8.360901832580566
Epoch 1740, val loss: 0.48009493947029114
Epoch 1750, training loss: 836.375244140625 = 0.40928876399993896 + 100.0 * 8.359659194946289
Epoch 1750, val loss: 0.47984519600868225
Epoch 1760, training loss: 836.3487548828125 = 0.4087521433830261 + 100.0 * 8.359399795532227
Epoch 1760, val loss: 0.47972604632377625
Epoch 1770, training loss: 836.2503662109375 = 0.4082196056842804 + 100.0 * 8.358421325683594
Epoch 1770, val loss: 0.4795403778553009
Epoch 1780, training loss: 836.6641845703125 = 0.40767592191696167 + 100.0 * 8.362565040588379
Epoch 1780, val loss: 0.47941458225250244
Epoch 1790, training loss: 836.2647705078125 = 0.40713822841644287 + 100.0 * 8.358575820922852
Epoch 1790, val loss: 0.4791090488433838
Epoch 1800, training loss: 836.1340942382812 = 0.4066062569618225 + 100.0 * 8.357275009155273
Epoch 1800, val loss: 0.47904014587402344
Epoch 1810, training loss: 836.0587768554688 = 0.40607935190200806 + 100.0 * 8.356527328491211
Epoch 1810, val loss: 0.4788144528865814
Epoch 1820, training loss: 836.0311279296875 = 0.40555301308631897 + 100.0 * 8.356255531311035
Epoch 1820, val loss: 0.47865861654281616
Epoch 1830, training loss: 836.2865600585938 = 0.4050231873989105 + 100.0 * 8.35881519317627
Epoch 1830, val loss: 0.478435218334198
Epoch 1840, training loss: 836.042236328125 = 0.40449172258377075 + 100.0 * 8.356377601623535
Epoch 1840, val loss: 0.4783569872379303
Epoch 1850, training loss: 836.3276977539062 = 0.4039575159549713 + 100.0 * 8.359237670898438
Epoch 1850, val loss: 0.47818654775619507
Epoch 1860, training loss: 835.929931640625 = 0.40341198444366455 + 100.0 * 8.355264663696289
Epoch 1860, val loss: 0.47794288396835327
Epoch 1870, training loss: 835.8853759765625 = 0.40288645029067993 + 100.0 * 8.354825019836426
Epoch 1870, val loss: 0.4778822362422943
Epoch 1880, training loss: 835.8605346679688 = 0.402366578578949 + 100.0 * 8.354581832885742
Epoch 1880, val loss: 0.4776625633239746
Epoch 1890, training loss: 835.809326171875 = 0.4018436074256897 + 100.0 * 8.354074478149414
Epoch 1890, val loss: 0.47753289341926575
Epoch 1900, training loss: 835.8079833984375 = 0.4013175368309021 + 100.0 * 8.354066848754883
Epoch 1900, val loss: 0.477344274520874
Epoch 1910, training loss: 836.1014404296875 = 0.4007885754108429 + 100.0 * 8.357006072998047
Epoch 1910, val loss: 0.47718873620033264
Epoch 1920, training loss: 835.9317016601562 = 0.40024134516716003 + 100.0 * 8.355314254760742
Epoch 1920, val loss: 0.4770558178424835
Epoch 1930, training loss: 835.750244140625 = 0.3997013568878174 + 100.0 * 8.35350513458252
Epoch 1930, val loss: 0.4768519103527069
Epoch 1940, training loss: 835.6871948242188 = 0.3991679847240448 + 100.0 * 8.352880477905273
Epoch 1940, val loss: 0.47670304775238037
Epoch 1950, training loss: 835.6974487304688 = 0.3986364006996155 + 100.0 * 8.352988243103027
Epoch 1950, val loss: 0.47652074694633484
Epoch 1960, training loss: 836.0463256835938 = 0.3980940878391266 + 100.0 * 8.35648250579834
Epoch 1960, val loss: 0.4763181805610657
Epoch 1970, training loss: 835.6480712890625 = 0.3975507616996765 + 100.0 * 8.352505683898926
Epoch 1970, val loss: 0.4761669337749481
Epoch 1980, training loss: 835.5618896484375 = 0.39700910449028015 + 100.0 * 8.351648330688477
Epoch 1980, val loss: 0.47596275806427
Epoch 1990, training loss: 835.55810546875 = 0.39647573232650757 + 100.0 * 8.351615905761719
Epoch 1990, val loss: 0.4758147895336151
Epoch 2000, training loss: 835.5309448242188 = 0.39594605565071106 + 100.0 * 8.351349830627441
Epoch 2000, val loss: 0.47563716769218445
Epoch 2010, training loss: 835.9541015625 = 0.3954065442085266 + 100.0 * 8.355587005615234
Epoch 2010, val loss: 0.4754011929035187
Epoch 2020, training loss: 835.6337890625 = 0.3948577642440796 + 100.0 * 8.352389335632324
Epoch 2020, val loss: 0.4752522110939026
Epoch 2030, training loss: 835.474609375 = 0.39431196451187134 + 100.0 * 8.35080337524414
Epoch 2030, val loss: 0.4751454293727875
Epoch 2040, training loss: 835.455810546875 = 0.39377522468566895 + 100.0 * 8.35062026977539
Epoch 2040, val loss: 0.474970281124115
Epoch 2050, training loss: 835.8699340820312 = 0.3932357728481293 + 100.0 * 8.354766845703125
Epoch 2050, val loss: 0.4747997522354126
Epoch 2060, training loss: 835.4464721679688 = 0.39266806840896606 + 100.0 * 8.35053825378418
Epoch 2060, val loss: 0.474559485912323
Epoch 2070, training loss: 835.34375 = 0.39212530851364136 + 100.0 * 8.349515914916992
Epoch 2070, val loss: 0.4744487702846527
Epoch 2080, training loss: 835.3126831054688 = 0.3915814757347107 + 100.0 * 8.349210739135742
Epoch 2080, val loss: 0.47430887818336487
Epoch 2090, training loss: 835.298583984375 = 0.39104220271110535 + 100.0 * 8.349075317382812
Epoch 2090, val loss: 0.47413668036460876
Epoch 2100, training loss: 835.2659912109375 = 0.3905003070831299 + 100.0 * 8.3487548828125
Epoch 2100, val loss: 0.47400134801864624
Epoch 2110, training loss: 835.2955932617188 = 0.3899536430835724 + 100.0 * 8.349056243896484
Epoch 2110, val loss: 0.4738912582397461
Epoch 2120, training loss: 835.6570434570312 = 0.3893889784812927 + 100.0 * 8.352676391601562
Epoch 2120, val loss: 0.473751962184906
Epoch 2130, training loss: 835.4169921875 = 0.388808935880661 + 100.0 * 8.350281715393066
Epoch 2130, val loss: 0.47355958819389343
Epoch 2140, training loss: 835.181640625 = 0.38825106620788574 + 100.0 * 8.347933769226074
Epoch 2140, val loss: 0.4733690321445465
Epoch 2150, training loss: 835.1558227539062 = 0.3876979649066925 + 100.0 * 8.347681045532227
Epoch 2150, val loss: 0.473203182220459
Epoch 2160, training loss: 835.1251831054688 = 0.3871460556983948 + 100.0 * 8.347380638122559
Epoch 2160, val loss: 0.47299924492836
Epoch 2170, training loss: 835.0989379882812 = 0.3865884244441986 + 100.0 * 8.347123146057129
Epoch 2170, val loss: 0.47292032837867737
Epoch 2180, training loss: 836.1087646484375 = 0.3860182762145996 + 100.0 * 8.357227325439453
Epoch 2180, val loss: 0.472756952047348
Epoch 2190, training loss: 835.4562377929688 = 0.38542404770851135 + 100.0 * 8.3507080078125
Epoch 2190, val loss: 0.4724746346473694
Epoch 2200, training loss: 835.0352172851562 = 0.38484954833984375 + 100.0 * 8.346504211425781
Epoch 2200, val loss: 0.47245004773139954
Epoch 2210, training loss: 835.0502319335938 = 0.3842840790748596 + 100.0 * 8.346659660339355
Epoch 2210, val loss: 0.4722250998020172
Epoch 2220, training loss: 834.9713745117188 = 0.38372260332107544 + 100.0 * 8.345876693725586
Epoch 2220, val loss: 0.47216975688934326
Epoch 2230, training loss: 834.9475708007812 = 0.3831603527069092 + 100.0 * 8.345643997192383
Epoch 2230, val loss: 0.4719708561897278
Epoch 2240, training loss: 834.9453735351562 = 0.3825903832912445 + 100.0 * 8.345627784729004
Epoch 2240, val loss: 0.4718724191188812
Epoch 2250, training loss: 835.5655517578125 = 0.3820088505744934 + 100.0 * 8.351835250854492
Epoch 2250, val loss: 0.47176286578178406
Epoch 2260, training loss: 834.9454956054688 = 0.38140249252319336 + 100.0 * 8.345641136169434
Epoch 2260, val loss: 0.471480131149292
Epoch 2270, training loss: 834.8887329101562 = 0.3808091878890991 + 100.0 * 8.34507942199707
Epoch 2270, val loss: 0.4714062809944153
Epoch 2280, training loss: 834.8357543945312 = 0.38022780418395996 + 100.0 * 8.344554901123047
Epoch 2280, val loss: 0.47124212980270386
Epoch 2290, training loss: 834.8009033203125 = 0.3796411156654358 + 100.0 * 8.344212532043457
Epoch 2290, val loss: 0.471107542514801
Epoch 2300, training loss: 835.02490234375 = 0.379054993391037 + 100.0 * 8.346458435058594
Epoch 2300, val loss: 0.47106900811195374
Epoch 2310, training loss: 834.8734741210938 = 0.3784238398075104 + 100.0 * 8.344950675964355
Epoch 2310, val loss: 0.4706948399543762
Epoch 2320, training loss: 834.84521484375 = 0.3778138756752014 + 100.0 * 8.344674110412598
Epoch 2320, val loss: 0.47074222564697266
Epoch 2330, training loss: 834.7099609375 = 0.37720125913619995 + 100.0 * 8.343327522277832
Epoch 2330, val loss: 0.4704485237598419
Epoch 2340, training loss: 834.6880493164062 = 0.3765943646430969 + 100.0 * 8.343114852905273
Epoch 2340, val loss: 0.47036781907081604
Epoch 2350, training loss: 834.6668701171875 = 0.37598323822021484 + 100.0 * 8.34290885925293
Epoch 2350, val loss: 0.4702194035053253
Epoch 2360, training loss: 834.8914184570312 = 0.37537068128585815 + 100.0 * 8.345160484313965
Epoch 2360, val loss: 0.47008034586906433
Epoch 2370, training loss: 834.6138916015625 = 0.3747238516807556 + 100.0 * 8.342391967773438
Epoch 2370, val loss: 0.4699200987815857
Epoch 2380, training loss: 834.5975341796875 = 0.3740887939929962 + 100.0 * 8.34223461151123
Epoch 2380, val loss: 0.4698435962200165
Epoch 2390, training loss: 834.5667114257812 = 0.3734549880027771 + 100.0 * 8.34193229675293
Epoch 2390, val loss: 0.46962717175483704
Epoch 2400, training loss: 834.5414428710938 = 0.3728216886520386 + 100.0 * 8.341686248779297
Epoch 2400, val loss: 0.4695630967617035
Epoch 2410, training loss: 834.7310791015625 = 0.37218424677848816 + 100.0 * 8.343588829040527
Epoch 2410, val loss: 0.46938270330429077
Epoch 2420, training loss: 834.5197143554688 = 0.3715256154537201 + 100.0 * 8.341482162475586
Epoch 2420, val loss: 0.4692387878894806
Epoch 2430, training loss: 834.48486328125 = 0.37088072299957275 + 100.0 * 8.341139793395996
Epoch 2430, val loss: 0.4691159427165985
Epoch 2440, training loss: 834.4354858398438 = 0.37022414803504944 + 100.0 * 8.340652465820312
Epoch 2440, val loss: 0.46887776255607605
Epoch 2450, training loss: 834.3858032226562 = 0.36957478523254395 + 100.0 * 8.34016227722168
Epoch 2450, val loss: 0.46882063150405884
Epoch 2460, training loss: 834.6555786132812 = 0.3689175844192505 + 100.0 * 8.342866897583008
Epoch 2460, val loss: 0.4686169922351837
Epoch 2470, training loss: 834.6438598632812 = 0.3682287037372589 + 100.0 * 8.342756271362305
Epoch 2470, val loss: 0.46852293610572815
Epoch 2480, training loss: 834.4716796875 = 0.36753782629966736 + 100.0 * 8.341041564941406
Epoch 2480, val loss: 0.4683131277561188
Epoch 2490, training loss: 834.2864990234375 = 0.3668532073497772 + 100.0 * 8.33919620513916
Epoch 2490, val loss: 0.46815162897109985
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8178589548452562
0.8638701731507644
=== training gcn model ===
Epoch 0, training loss: 1059.3587646484375 = 1.1270928382873535 + 100.0 * 10.582316398620605
Epoch 0, val loss: 1.1250981092453003
Epoch 10, training loss: 1059.323974609375 = 1.1214596033096313 + 100.0 * 10.582024574279785
Epoch 10, val loss: 1.1194556951522827
Epoch 20, training loss: 1059.1842041015625 = 1.1153266429901123 + 100.0 * 10.5806884765625
Epoch 20, val loss: 1.113294005393982
Epoch 30, training loss: 1058.5496826171875 = 1.1084342002868652 + 100.0 * 10.574413299560547
Epoch 30, val loss: 1.1063790321350098
Epoch 40, training loss: 1055.8780517578125 = 1.1005767583847046 + 100.0 * 10.547774314880371
Epoch 40, val loss: 1.0985199213027954
Epoch 50, training loss: 1047.28515625 = 1.091848373413086 + 100.0 * 10.461934089660645
Epoch 50, val loss: 1.0898417234420776
Epoch 60, training loss: 1026.12060546875 = 1.082848072052002 + 100.0 * 10.250377655029297
Epoch 60, val loss: 1.0810060501098633
Epoch 70, training loss: 989.2545166015625 = 1.072924017906189 + 100.0 * 9.881815910339355
Epoch 70, val loss: 1.0710511207580566
Epoch 80, training loss: 963.6652221679688 = 1.0641367435455322 + 100.0 * 9.62601089477539
Epoch 80, val loss: 1.0628899335861206
Epoch 90, training loss: 952.0595703125 = 1.058030605316162 + 100.0 * 9.510015487670898
Epoch 90, val loss: 1.0570441484451294
Epoch 100, training loss: 940.7078857421875 = 1.0525060892105103 + 100.0 * 9.396553993225098
Epoch 100, val loss: 1.051722764968872
Epoch 110, training loss: 927.0256958007812 = 1.047294020652771 + 100.0 * 9.259783744812012
Epoch 110, val loss: 1.0467615127563477
Epoch 120, training loss: 916.3349609375 = 1.042128086090088 + 100.0 * 9.152928352355957
Epoch 120, val loss: 1.0418002605438232
Epoch 130, training loss: 909.8037109375 = 1.0368666648864746 + 100.0 * 9.087668418884277
Epoch 130, val loss: 1.0367637872695923
Epoch 140, training loss: 903.6734008789062 = 1.031782627105713 + 100.0 * 9.026415824890137
Epoch 140, val loss: 1.0318809747695923
Epoch 150, training loss: 897.984130859375 = 1.0267504453659058 + 100.0 * 8.969573974609375
Epoch 150, val loss: 1.0270646810531616
Epoch 160, training loss: 893.9339599609375 = 1.0217634439468384 + 100.0 * 8.929121971130371
Epoch 160, val loss: 1.0223337411880493
Epoch 170, training loss: 890.239990234375 = 1.016951322555542 + 100.0 * 8.892230033874512
Epoch 170, val loss: 1.0178163051605225
Epoch 180, training loss: 886.67822265625 = 1.0123199224472046 + 100.0 * 8.856658935546875
Epoch 180, val loss: 1.01345956325531
Epoch 190, training loss: 884.7056884765625 = 1.0074193477630615 + 100.0 * 8.836982727050781
Epoch 190, val loss: 1.0086982250213623
Epoch 200, training loss: 883.2157592773438 = 1.0017573833465576 + 100.0 * 8.822139739990234
Epoch 200, val loss: 1.003144383430481
Epoch 210, training loss: 881.564697265625 = 0.9959692358970642 + 100.0 * 8.805686950683594
Epoch 210, val loss: 0.9975680708885193
Epoch 220, training loss: 879.6755981445312 = 0.9904870390892029 + 100.0 * 8.786850929260254
Epoch 220, val loss: 0.9923381805419922
Epoch 230, training loss: 877.3594970703125 = 0.9852148294448853 + 100.0 * 8.763742446899414
Epoch 230, val loss: 0.9873386025428772
Epoch 240, training loss: 874.8150634765625 = 0.9800928235054016 + 100.0 * 8.738349914550781
Epoch 240, val loss: 0.9824565052986145
Epoch 250, training loss: 872.7200317382812 = 0.9746138453483582 + 100.0 * 8.717453956604004
Epoch 250, val loss: 0.9771808981895447
Epoch 260, training loss: 870.9554443359375 = 0.9685260057449341 + 100.0 * 8.699869155883789
Epoch 260, val loss: 0.9713927507400513
Epoch 270, training loss: 869.187255859375 = 0.9620455503463745 + 100.0 * 8.682251930236816
Epoch 270, val loss: 0.965232789516449
Epoch 280, training loss: 867.4517822265625 = 0.9553662538528442 + 100.0 * 8.664963722229004
Epoch 280, val loss: 0.958920955657959
Epoch 290, training loss: 865.9418334960938 = 0.9484313130378723 + 100.0 * 8.649933815002441
Epoch 290, val loss: 0.9523290991783142
Epoch 300, training loss: 864.7584228515625 = 0.9409409761428833 + 100.0 * 8.638175010681152
Epoch 300, val loss: 0.9452351927757263
Epoch 310, training loss: 863.5554809570312 = 0.9328915476799011 + 100.0 * 8.626226425170898
Epoch 310, val loss: 0.9376083016395569
Epoch 320, training loss: 862.4918212890625 = 0.9244560599327087 + 100.0 * 8.615674018859863
Epoch 320, val loss: 0.92966628074646
Epoch 330, training loss: 861.5411376953125 = 0.9156110286712646 + 100.0 * 8.606255531311035
Epoch 330, val loss: 0.9213615655899048
Epoch 340, training loss: 860.944580078125 = 0.9063454866409302 + 100.0 * 8.600381851196289
Epoch 340, val loss: 0.9125939011573792
Epoch 350, training loss: 860.08935546875 = 0.8964776992797852 + 100.0 * 8.591928482055664
Epoch 350, val loss: 0.9033730626106262
Epoch 360, training loss: 859.3756103515625 = 0.8863387703895569 + 100.0 * 8.584892272949219
Epoch 360, val loss: 0.893926203250885
Epoch 370, training loss: 858.703369140625 = 0.8759742975234985 + 100.0 * 8.57827377319336
Epoch 370, val loss: 0.8842716813087463
Epoch 380, training loss: 857.9823608398438 = 0.8655205368995667 + 100.0 * 8.571167945861816
Epoch 380, val loss: 0.874550998210907
Epoch 390, training loss: 857.2921752929688 = 0.8550183773040771 + 100.0 * 8.564371109008789
Epoch 390, val loss: 0.8648040294647217
Epoch 400, training loss: 856.637451171875 = 0.844252347946167 + 100.0 * 8.557931900024414
Epoch 400, val loss: 0.8548358678817749
Epoch 410, training loss: 856.0651245117188 = 0.8334128260612488 + 100.0 * 8.552316665649414
Epoch 410, val loss: 0.8448817133903503
Epoch 420, training loss: 855.5174560546875 = 0.8224654197692871 + 100.0 * 8.546950340270996
Epoch 420, val loss: 0.8348065614700317
Epoch 430, training loss: 854.9595336914062 = 0.8114768862724304 + 100.0 * 8.541481018066406
Epoch 430, val loss: 0.8247566819190979
Epoch 440, training loss: 854.4105224609375 = 0.80048668384552 + 100.0 * 8.536100387573242
Epoch 440, val loss: 0.8146981596946716
Epoch 450, training loss: 854.0341796875 = 0.7894709706306458 + 100.0 * 8.53244686126709
Epoch 450, val loss: 0.8046286702156067
Epoch 460, training loss: 853.392822265625 = 0.7784474492073059 + 100.0 * 8.526144027709961
Epoch 460, val loss: 0.7946237921714783
Epoch 470, training loss: 852.6754150390625 = 0.7675763964653015 + 100.0 * 8.519078254699707
Epoch 470, val loss: 0.7847546339035034
Epoch 480, training loss: 852.0107421875 = 0.7567541599273682 + 100.0 * 8.512539863586426
Epoch 480, val loss: 0.7749671936035156
Epoch 490, training loss: 851.4856567382812 = 0.7459807991981506 + 100.0 * 8.507396697998047
Epoch 490, val loss: 0.7652373313903809
Epoch 500, training loss: 850.9819946289062 = 0.7351429462432861 + 100.0 * 8.50246810913086
Epoch 500, val loss: 0.7555317878723145
Epoch 510, training loss: 850.4246215820312 = 0.7243560552597046 + 100.0 * 8.497002601623535
Epoch 510, val loss: 0.7457895278930664
Epoch 520, training loss: 850.031494140625 = 0.7135694622993469 + 100.0 * 8.493179321289062
Epoch 520, val loss: 0.736065149307251
Epoch 530, training loss: 849.6421508789062 = 0.7027686238288879 + 100.0 * 8.489394187927246
Epoch 530, val loss: 0.7263919115066528
Epoch 540, training loss: 849.5911254882812 = 0.6920575499534607 + 100.0 * 8.488990783691406
Epoch 540, val loss: 0.7167519330978394
Epoch 550, training loss: 849.0321044921875 = 0.6813952922821045 + 100.0 * 8.48350715637207
Epoch 550, val loss: 0.707197368144989
Epoch 560, training loss: 848.6360473632812 = 0.6709091067314148 + 100.0 * 8.47965145111084
Epoch 560, val loss: 0.697817862033844
Epoch 570, training loss: 848.4984741210938 = 0.660616397857666 + 100.0 * 8.478378295898438
Epoch 570, val loss: 0.6885991096496582
Epoch 580, training loss: 848.2757568359375 = 0.6504076719284058 + 100.0 * 8.476253509521484
Epoch 580, val loss: 0.6795716285705566
Epoch 590, training loss: 847.6196899414062 = 0.6405812501907349 + 100.0 * 8.469791412353516
Epoch 590, val loss: 0.670820415019989
Epoch 600, training loss: 847.2457885742188 = 0.6310409307479858 + 100.0 * 8.466147422790527
Epoch 600, val loss: 0.6623592972755432
Epoch 610, training loss: 846.9028930664062 = 0.6217626929283142 + 100.0 * 8.462811470031738
Epoch 610, val loss: 0.6541896462440491
Epoch 620, training loss: 846.5808715820312 = 0.612774133682251 + 100.0 * 8.459680557250977
Epoch 620, val loss: 0.6462875008583069
Epoch 630, training loss: 846.2853393554688 = 0.6040554046630859 + 100.0 * 8.456812858581543
Epoch 630, val loss: 0.6386284232139587
Epoch 640, training loss: 846.3170776367188 = 0.5955387949943542 + 100.0 * 8.457215309143066
Epoch 640, val loss: 0.6311464309692383
Epoch 650, training loss: 845.8380737304688 = 0.5873480439186096 + 100.0 * 8.452507019042969
Epoch 650, val loss: 0.6240251660346985
Epoch 660, training loss: 845.4843139648438 = 0.5795274376869202 + 100.0 * 8.449048042297363
Epoch 660, val loss: 0.6172204613685608
Epoch 670, training loss: 845.2257080078125 = 0.5720258355140686 + 100.0 * 8.446537017822266
Epoch 670, val loss: 0.6107335686683655
Epoch 680, training loss: 844.9727783203125 = 0.5648403763771057 + 100.0 * 8.444079399108887
Epoch 680, val loss: 0.6045511364936829
Epoch 690, training loss: 845.67919921875 = 0.5579587817192078 + 100.0 * 8.451211929321289
Epoch 690, val loss: 0.5986427664756775
Epoch 700, training loss: 844.7418212890625 = 0.5512374043464661 + 100.0 * 8.441905975341797
Epoch 700, val loss: 0.5928508043289185
Epoch 710, training loss: 844.4273681640625 = 0.5449475049972534 + 100.0 * 8.438824653625488
Epoch 710, val loss: 0.5875434279441833
Epoch 720, training loss: 844.1834106445312 = 0.5389766097068787 + 100.0 * 8.436444282531738
Epoch 720, val loss: 0.5825181007385254
Epoch 730, training loss: 843.9788208007812 = 0.533284068107605 + 100.0 * 8.434455871582031
Epoch 730, val loss: 0.5777195692062378
Epoch 740, training loss: 843.8019409179688 = 0.5278796553611755 + 100.0 * 8.432740211486816
Epoch 740, val loss: 0.5732018947601318
Epoch 750, training loss: 843.93798828125 = 0.5227208733558655 + 100.0 * 8.434152603149414
Epoch 750, val loss: 0.5688878893852234
Epoch 760, training loss: 843.6657104492188 = 0.517756462097168 + 100.0 * 8.431479454040527
Epoch 760, val loss: 0.5648666024208069
Epoch 770, training loss: 843.33642578125 = 0.5131010413169861 + 100.0 * 8.42823314666748
Epoch 770, val loss: 0.5610129237174988
Epoch 780, training loss: 843.1052856445312 = 0.508696973323822 + 100.0 * 8.425966262817383
Epoch 780, val loss: 0.5574443936347961
Epoch 790, training loss: 842.9422607421875 = 0.5044988989830017 + 100.0 * 8.42437744140625
Epoch 790, val loss: 0.5540909171104431
Epoch 800, training loss: 842.8223266601562 = 0.500520646572113 + 100.0 * 8.4232177734375
Epoch 800, val loss: 0.5509206056594849
Epoch 810, training loss: 842.7468872070312 = 0.4967174530029297 + 100.0 * 8.422501564025879
Epoch 810, val loss: 0.5478985905647278
Epoch 820, training loss: 842.6292114257812 = 0.49309465289115906 + 100.0 * 8.421360969543457
Epoch 820, val loss: 0.545021653175354
Epoch 830, training loss: 842.3853759765625 = 0.4896497428417206 + 100.0 * 8.418957710266113
Epoch 830, val loss: 0.5423506498336792
Epoch 840, training loss: 842.2614135742188 = 0.4863912761211395 + 100.0 * 8.417750358581543
Epoch 840, val loss: 0.5398731231689453
Epoch 850, training loss: 842.0953979492188 = 0.48327893018722534 + 100.0 * 8.416121482849121
Epoch 850, val loss: 0.5374734997749329
Epoch 860, training loss: 841.9881591796875 = 0.48031798005104065 + 100.0 * 8.415078163146973
Epoch 860, val loss: 0.5352227091789246
Epoch 870, training loss: 842.0278930664062 = 0.4774779677391052 + 100.0 * 8.415504455566406
Epoch 870, val loss: 0.5331209301948547
Epoch 880, training loss: 841.7828369140625 = 0.4747455418109894 + 100.0 * 8.413081169128418
Epoch 880, val loss: 0.531024694442749
Epoch 890, training loss: 841.6171875 = 0.4721646010875702 + 100.0 * 8.411450386047363
Epoch 890, val loss: 0.5291050672531128
Epoch 900, training loss: 841.4073486328125 = 0.4696931540966034 + 100.0 * 8.409377098083496
Epoch 900, val loss: 0.5272977948188782
Epoch 910, training loss: 841.2832641601562 = 0.4673399329185486 + 100.0 * 8.408159255981445
Epoch 910, val loss: 0.5255330204963684
Epoch 920, training loss: 841.3186645507812 = 0.46509021520614624 + 100.0 * 8.408535957336426
Epoch 920, val loss: 0.5238106846809387
Epoch 930, training loss: 841.21337890625 = 0.46289098262786865 + 100.0 * 8.40750503540039
Epoch 930, val loss: 0.5223745703697205
Epoch 940, training loss: 841.001220703125 = 0.4608057737350464 + 100.0 * 8.405404090881348
Epoch 940, val loss: 0.5207735300064087
Epoch 950, training loss: 840.9559326171875 = 0.45880892872810364 + 100.0 * 8.4049711227417
Epoch 950, val loss: 0.5193489789962769
Epoch 960, training loss: 840.836669921875 = 0.45687389373779297 + 100.0 * 8.40379810333252
Epoch 960, val loss: 0.5180116295814514
Epoch 970, training loss: 840.6703491210938 = 0.45501551032066345 + 100.0 * 8.402153015136719
Epoch 970, val loss: 0.5166522264480591
Epoch 980, training loss: 840.50244140625 = 0.45323890447616577 + 100.0 * 8.400491714477539
Epoch 980, val loss: 0.5153985619544983
Epoch 990, training loss: 840.4480590820312 = 0.45152920484542847 + 100.0 * 8.399965286254883
Epoch 990, val loss: 0.5142033100128174
Epoch 1000, training loss: 840.7909545898438 = 0.4498734474182129 + 100.0 * 8.403410911560059
Epoch 1000, val loss: 0.51308274269104
Epoch 1010, training loss: 840.3828735351562 = 0.44822630286216736 + 100.0 * 8.399346351623535
Epoch 1010, val loss: 0.511817455291748
Epoch 1020, training loss: 840.1981811523438 = 0.4466845691204071 + 100.0 * 8.397515296936035
Epoch 1020, val loss: 0.5107958912849426
Epoch 1030, training loss: 840.066650390625 = 0.44517767429351807 + 100.0 * 8.396214485168457
Epoch 1030, val loss: 0.5097336769104004
Epoch 1040, training loss: 840.1766357421875 = 0.44372808933258057 + 100.0 * 8.397329330444336
Epoch 1040, val loss: 0.5087342262268066
Epoch 1050, training loss: 839.9867553710938 = 0.44228923320770264 + 100.0 * 8.395444869995117
Epoch 1050, val loss: 0.5077489018440247
Epoch 1060, training loss: 839.8758544921875 = 0.44090738892555237 + 100.0 * 8.394349098205566
Epoch 1060, val loss: 0.506786048412323
Epoch 1070, training loss: 839.700439453125 = 0.439574658870697 + 100.0 * 8.392608642578125
Epoch 1070, val loss: 0.5058861374855042
Epoch 1080, training loss: 839.66357421875 = 0.43828722834587097 + 100.0 * 8.392252922058105
Epoch 1080, val loss: 0.505025327205658
Epoch 1090, training loss: 840.17919921875 = 0.4370163083076477 + 100.0 * 8.397421836853027
Epoch 1090, val loss: 0.5041682720184326
Epoch 1100, training loss: 839.5198974609375 = 0.4357302486896515 + 100.0 * 8.390841484069824
Epoch 1100, val loss: 0.503234326839447
Epoch 1110, training loss: 839.4483032226562 = 0.434520959854126 + 100.0 * 8.390137672424316
Epoch 1110, val loss: 0.5023268461227417
Epoch 1120, training loss: 839.3322143554688 = 0.4333432614803314 + 100.0 * 8.388988494873047
Epoch 1120, val loss: 0.5015637278556824
Epoch 1130, training loss: 839.2600708007812 = 0.4321932792663574 + 100.0 * 8.38827896118164
Epoch 1130, val loss: 0.5007801055908203
Epoch 1140, training loss: 839.1920166015625 = 0.4310661554336548 + 100.0 * 8.387609481811523
Epoch 1140, val loss: 0.4999954402446747
Epoch 1150, training loss: 839.4678344726562 = 0.4299532175064087 + 100.0 * 8.390378952026367
Epoch 1150, val loss: 0.4992477297782898
Epoch 1160, training loss: 839.1157836914062 = 0.4288345277309418 + 100.0 * 8.386869430541992
Epoch 1160, val loss: 0.4984627366065979
Epoch 1170, training loss: 839.0845336914062 = 0.42776212096214294 + 100.0 * 8.386568069458008
Epoch 1170, val loss: 0.49765607714653015
Epoch 1180, training loss: 838.9515991210938 = 0.42670729756355286 + 100.0 * 8.385249137878418
Epoch 1180, val loss: 0.49687838554382324
Epoch 1190, training loss: 838.8676147460938 = 0.42568206787109375 + 100.0 * 8.384419441223145
Epoch 1190, val loss: 0.49615633487701416
Epoch 1200, training loss: 838.9536743164062 = 0.42467206716537476 + 100.0 * 8.385290145874023
Epoch 1200, val loss: 0.4954451024532318
Epoch 1210, training loss: 838.8756713867188 = 0.4236642122268677 + 100.0 * 8.384520530700684
Epoch 1210, val loss: 0.49472999572753906
Epoch 1220, training loss: 838.7308959960938 = 0.42267751693725586 + 100.0 * 8.383082389831543
Epoch 1220, val loss: 0.4940322935581207
Epoch 1230, training loss: 838.8458862304688 = 0.4217122197151184 + 100.0 * 8.384242057800293
Epoch 1230, val loss: 0.49332571029663086
Epoch 1240, training loss: 838.5986328125 = 0.4207635521888733 + 100.0 * 8.381778717041016
Epoch 1240, val loss: 0.4926528036594391
Epoch 1250, training loss: 838.5685424804688 = 0.4198468029499054 + 100.0 * 8.381486892700195
Epoch 1250, val loss: 0.49202820658683777
Epoch 1260, training loss: 838.4540405273438 = 0.4189344346523285 + 100.0 * 8.380351066589355
Epoch 1260, val loss: 0.49134474992752075
Epoch 1270, training loss: 838.4325561523438 = 0.4180447459220886 + 100.0 * 8.380145072937012
Epoch 1270, val loss: 0.4906761944293976
Epoch 1280, training loss: 838.7447509765625 = 0.41715967655181885 + 100.0 * 8.383275985717773
Epoch 1280, val loss: 0.48999661207199097
Epoch 1290, training loss: 838.3764038085938 = 0.41628503799438477 + 100.0 * 8.37960147857666
Epoch 1290, val loss: 0.48942914605140686
Epoch 1300, training loss: 838.2565307617188 = 0.4154217541217804 + 100.0 * 8.378411293029785
Epoch 1300, val loss: 0.488766074180603
Epoch 1310, training loss: 838.290283203125 = 0.4145772159099579 + 100.0 * 8.37875747680664
Epoch 1310, val loss: 0.4881328046321869
Epoch 1320, training loss: 838.2841796875 = 0.41374078392982483 + 100.0 * 8.378704071044922
Epoch 1320, val loss: 0.48751333355903625
Epoch 1330, training loss: 838.1631469726562 = 0.41292452812194824 + 100.0 * 8.37750244140625
Epoch 1330, val loss: 0.48693209886550903
Epoch 1340, training loss: 838.10009765625 = 0.4121147096157074 + 100.0 * 8.376879692077637
Epoch 1340, val loss: 0.48632335662841797
Epoch 1350, training loss: 838.0547485351562 = 0.41132184863090515 + 100.0 * 8.376434326171875
Epoch 1350, val loss: 0.4857499599456787
Epoch 1360, training loss: 838.0553588867188 = 0.4105384945869446 + 100.0 * 8.376448631286621
Epoch 1360, val loss: 0.48515745997428894
Epoch 1370, training loss: 838.00341796875 = 0.40976443886756897 + 100.0 * 8.375936508178711
Epoch 1370, val loss: 0.48464617133140564
Epoch 1380, training loss: 838.4859619140625 = 0.40898752212524414 + 100.0 * 8.380769729614258
Epoch 1380, val loss: 0.4840705096721649
Epoch 1390, training loss: 837.9249877929688 = 0.40818795561790466 + 100.0 * 8.375167846679688
Epoch 1390, val loss: 0.48345237970352173
Epoch 1400, training loss: 837.8721313476562 = 0.40743693709373474 + 100.0 * 8.37464714050293
Epoch 1400, val loss: 0.48282700777053833
Epoch 1410, training loss: 837.7622680664062 = 0.4067000150680542 + 100.0 * 8.373556137084961
Epoch 1410, val loss: 0.48228588700294495
Epoch 1420, training loss: 837.7265014648438 = 0.4059795141220093 + 100.0 * 8.373205184936523
Epoch 1420, val loss: 0.48179322481155396
Epoch 1430, training loss: 837.682373046875 = 0.40526247024536133 + 100.0 * 8.372771263122559
Epoch 1430, val loss: 0.48120859265327454
Epoch 1440, training loss: 837.6710815429688 = 0.40455570816993713 + 100.0 * 8.372665405273438
Epoch 1440, val loss: 0.48071709275245667
Epoch 1450, training loss: 837.9702758789062 = 0.4038553535938263 + 100.0 * 8.375663757324219
Epoch 1450, val loss: 0.4801638424396515
Epoch 1460, training loss: 837.6699829101562 = 0.40311720967292786 + 100.0 * 8.372668266296387
Epoch 1460, val loss: 0.4796171486377716
Epoch 1470, training loss: 837.666015625 = 0.40240904688835144 + 100.0 * 8.372635841369629
Epoch 1470, val loss: 0.47898972034454346
Epoch 1480, training loss: 837.6132202148438 = 0.40172794461250305 + 100.0 * 8.372115135192871
Epoch 1480, val loss: 0.4785131514072418
Epoch 1490, training loss: 837.4735107421875 = 0.4010416865348816 + 100.0 * 8.37072467803955
Epoch 1490, val loss: 0.4779645502567291
Epoch 1500, training loss: 837.4312744140625 = 0.4003746807575226 + 100.0 * 8.370308876037598
Epoch 1500, val loss: 0.4774261713027954
Epoch 1510, training loss: 837.589111328125 = 0.3997143805027008 + 100.0 * 8.371893882751465
Epoch 1510, val loss: 0.47688931226730347
Epoch 1520, training loss: 837.3587036132812 = 0.39903804659843445 + 100.0 * 8.369596481323242
Epoch 1520, val loss: 0.4764159619808197
Epoch 1530, training loss: 837.3206176757812 = 0.3983767032623291 + 100.0 * 8.369222640991211
Epoch 1530, val loss: 0.4758521020412445
Epoch 1540, training loss: 837.4462280273438 = 0.39772194623947144 + 100.0 * 8.370485305786133
Epoch 1540, val loss: 0.4753343462944031
Epoch 1550, training loss: 837.4306640625 = 0.39706528186798096 + 100.0 * 8.370335578918457
Epoch 1550, val loss: 0.4748268723487854
Epoch 1560, training loss: 837.3112182617188 = 0.39642274379730225 + 100.0 * 8.369148254394531
Epoch 1560, val loss: 0.47435450553894043
Epoch 1570, training loss: 837.2140502929688 = 0.3957836627960205 + 100.0 * 8.368182182312012
Epoch 1570, val loss: 0.47379931807518005
Epoch 1580, training loss: 837.1498413085938 = 0.3951624631881714 + 100.0 * 8.367547035217285
Epoch 1580, val loss: 0.47335654497146606
Epoch 1590, training loss: 837.0956420898438 = 0.3945448696613312 + 100.0 * 8.367011070251465
Epoch 1590, val loss: 0.47282692790031433
Epoch 1600, training loss: 837.0809326171875 = 0.39392900466918945 + 100.0 * 8.366869926452637
Epoch 1600, val loss: 0.4723401665687561
Epoch 1610, training loss: 837.4375610351562 = 0.3933119475841522 + 100.0 * 8.370442390441895
Epoch 1610, val loss: 0.4718311131000519
Epoch 1620, training loss: 837.36181640625 = 0.3926803767681122 + 100.0 * 8.369690895080566
Epoch 1620, val loss: 0.4714314043521881
Epoch 1630, training loss: 837.0266723632812 = 0.3920319080352783 + 100.0 * 8.36634635925293
Epoch 1630, val loss: 0.47083863615989685
Epoch 1640, training loss: 836.9522094726562 = 0.39141982793807983 + 100.0 * 8.365608215332031
Epoch 1640, val loss: 0.47034889459609985
Epoch 1650, training loss: 836.89306640625 = 0.3908171057701111 + 100.0 * 8.365022659301758
Epoch 1650, val loss: 0.46988388895988464
Epoch 1660, training loss: 836.9486083984375 = 0.3902178704738617 + 100.0 * 8.365584373474121
Epoch 1660, val loss: 0.46937888860702515
Epoch 1670, training loss: 837.1113891601562 = 0.3896063268184662 + 100.0 * 8.367218017578125
Epoch 1670, val loss: 0.46888482570648193
Epoch 1680, training loss: 836.8103637695312 = 0.389003723859787 + 100.0 * 8.364213943481445
Epoch 1680, val loss: 0.468441367149353
Epoch 1690, training loss: 836.7616577148438 = 0.3884046971797943 + 100.0 * 8.36373233795166
Epoch 1690, val loss: 0.4680035412311554
Epoch 1700, training loss: 836.7147216796875 = 0.3878205418586731 + 100.0 * 8.363268852233887
Epoch 1700, val loss: 0.4675631821155548
Epoch 1710, training loss: 836.7466430664062 = 0.38724276423454285 + 100.0 * 8.363594055175781
Epoch 1710, val loss: 0.4671446681022644
Epoch 1720, training loss: 837.0357666015625 = 0.386658251285553 + 100.0 * 8.366491317749023
Epoch 1720, val loss: 0.46665632724761963
Epoch 1730, training loss: 836.764892578125 = 0.38604405522346497 + 100.0 * 8.363788604736328
Epoch 1730, val loss: 0.46608105301856995
Epoch 1740, training loss: 836.5804443359375 = 0.3854615092277527 + 100.0 * 8.361949920654297
Epoch 1740, val loss: 0.4657389521598816
Epoch 1750, training loss: 836.5048217773438 = 0.38488391041755676 + 100.0 * 8.361199378967285
Epoch 1750, val loss: 0.4652785062789917
Epoch 1760, training loss: 836.4957885742188 = 0.3843133747577667 + 100.0 * 8.361114501953125
Epoch 1760, val loss: 0.46481558680534363
Epoch 1770, training loss: 836.722900390625 = 0.3837384283542633 + 100.0 * 8.363391876220703
Epoch 1770, val loss: 0.46445024013519287
Epoch 1780, training loss: 836.7261962890625 = 0.38313642144203186 + 100.0 * 8.363430976867676
Epoch 1780, val loss: 0.46381595730781555
Epoch 1790, training loss: 836.49609375 = 0.3825403153896332 + 100.0 * 8.361135482788086
Epoch 1790, val loss: 0.46347206830978394
Epoch 1800, training loss: 836.3507080078125 = 0.3819683790206909 + 100.0 * 8.359687805175781
Epoch 1800, val loss: 0.4629732668399811
Epoch 1810, training loss: 836.3074340820312 = 0.38140490651130676 + 100.0 * 8.359260559082031
Epoch 1810, val loss: 0.4625908136367798
Epoch 1820, training loss: 836.2513427734375 = 0.38084331154823303 + 100.0 * 8.358704566955566
Epoch 1820, val loss: 0.46214932203292847
Epoch 1830, training loss: 836.2158813476562 = 0.3802824318408966 + 100.0 * 8.358355522155762
Epoch 1830, val loss: 0.4617020785808563
Epoch 1840, training loss: 836.2041015625 = 0.3797188997268677 + 100.0 * 8.358243942260742
Epoch 1840, val loss: 0.46125754714012146
Epoch 1850, training loss: 837.7489013671875 = 0.3791329860687256 + 100.0 * 8.373697280883789
Epoch 1850, val loss: 0.46076804399490356
Epoch 1860, training loss: 836.1856689453125 = 0.37851789593696594 + 100.0 * 8.358071327209473
Epoch 1860, val loss: 0.46021249890327454
Epoch 1870, training loss: 836.1597900390625 = 0.3779281675815582 + 100.0 * 8.357818603515625
Epoch 1870, val loss: 0.45990535616874695
Epoch 1880, training loss: 836.1004028320312 = 0.37735414505004883 + 100.0 * 8.357230186462402
Epoch 1880, val loss: 0.45945650339126587
Epoch 1890, training loss: 836.0257568359375 = 0.37679392099380493 + 100.0 * 8.356490135192871
Epoch 1890, val loss: 0.45902714133262634
Epoch 1900, training loss: 835.9887084960938 = 0.37623336911201477 + 100.0 * 8.356124877929688
Epoch 1900, val loss: 0.45860427618026733
Epoch 1910, training loss: 835.9586791992188 = 0.37566515803337097 + 100.0 * 8.355830192565918
Epoch 1910, val loss: 0.45816725492477417
Epoch 1920, training loss: 836.409912109375 = 0.37509578466415405 + 100.0 * 8.360347747802734
Epoch 1920, val loss: 0.45780760049819946
Epoch 1930, training loss: 836.130126953125 = 0.3744831681251526 + 100.0 * 8.357556343078613
Epoch 1930, val loss: 0.4572073519229889
Epoch 1940, training loss: 835.9165649414062 = 0.3738909363746643 + 100.0 * 8.355426788330078
Epoch 1940, val loss: 0.4568811058998108
Epoch 1950, training loss: 835.860107421875 = 0.3733048439025879 + 100.0 * 8.354867935180664
Epoch 1950, val loss: 0.45636993646621704
Epoch 1960, training loss: 835.8120727539062 = 0.37273362278938293 + 100.0 * 8.354393005371094
Epoch 1960, val loss: 0.45601505041122437
Epoch 1970, training loss: 835.7801513671875 = 0.3721604645252228 + 100.0 * 8.354080200195312
Epoch 1970, val loss: 0.45557525753974915
Epoch 1980, training loss: 835.952392578125 = 0.3715890944004059 + 100.0 * 8.35580825805664
Epoch 1980, val loss: 0.45511138439178467
Epoch 1990, training loss: 835.848388671875 = 0.37099212408065796 + 100.0 * 8.354774475097656
Epoch 1990, val loss: 0.45480331778526306
Epoch 2000, training loss: 835.8220825195312 = 0.37038683891296387 + 100.0 * 8.354516983032227
Epoch 2000, val loss: 0.45425140857696533
Epoch 2010, training loss: 835.6944580078125 = 0.36980825662612915 + 100.0 * 8.353246688842773
Epoch 2010, val loss: 0.4539220631122589
Epoch 2020, training loss: 835.6465454101562 = 0.36923545598983765 + 100.0 * 8.35277271270752
Epoch 2020, val loss: 0.4535374343395233
Epoch 2030, training loss: 835.61328125 = 0.368663489818573 + 100.0 * 8.352446556091309
Epoch 2030, val loss: 0.45313403010368347
Epoch 2040, training loss: 835.6716918945312 = 0.36809220910072327 + 100.0 * 8.353035926818848
Epoch 2040, val loss: 0.45277369022369385
Epoch 2050, training loss: 836.1561889648438 = 0.36751213669776917 + 100.0 * 8.357887268066406
Epoch 2050, val loss: 0.45241665840148926
Epoch 2060, training loss: 835.7420043945312 = 0.36687782406806946 + 100.0 * 8.353751182556152
Epoch 2060, val loss: 0.45189380645751953
Epoch 2070, training loss: 835.57470703125 = 0.36629775166511536 + 100.0 * 8.352084159851074
Epoch 2070, val loss: 0.4515256881713867
Epoch 2080, training loss: 835.4996337890625 = 0.36570554971694946 + 100.0 * 8.351339340209961
Epoch 2080, val loss: 0.4511658847332001
Epoch 2090, training loss: 835.481201171875 = 0.3651253283023834 + 100.0 * 8.351161003112793
Epoch 2090, val loss: 0.45074111223220825
Epoch 2100, training loss: 835.9063720703125 = 0.3645332455635071 + 100.0 * 8.35541820526123
Epoch 2100, val loss: 0.45030537247657776
Epoch 2110, training loss: 835.5196533203125 = 0.3639165163040161 + 100.0 * 8.351557731628418
Epoch 2110, val loss: 0.44999316334724426
Epoch 2120, training loss: 835.4736328125 = 0.363311231136322 + 100.0 * 8.351103782653809
Epoch 2120, val loss: 0.4496288597583771
Epoch 2130, training loss: 835.366943359375 = 0.36271557211875916 + 100.0 * 8.350042343139648
Epoch 2130, val loss: 0.4492141902446747
Epoch 2140, training loss: 835.34033203125 = 0.36212924122810364 + 100.0 * 8.34978199005127
Epoch 2140, val loss: 0.4488063454627991
Epoch 2150, training loss: 835.3275756835938 = 0.3615427017211914 + 100.0 * 8.34965991973877
Epoch 2150, val loss: 0.44844117760658264
Epoch 2160, training loss: 835.9363403320312 = 0.36094769835472107 + 100.0 * 8.355753898620605
Epoch 2160, val loss: 0.4480370283126831
Epoch 2170, training loss: 835.4417114257812 = 0.3603449761867523 + 100.0 * 8.350813865661621
Epoch 2170, val loss: 0.44772857427597046
Epoch 2180, training loss: 835.3724365234375 = 0.359720915555954 + 100.0 * 8.350127220153809
Epoch 2180, val loss: 0.4473005533218384
Epoch 2190, training loss: 835.2744140625 = 0.3591282069683075 + 100.0 * 8.349152565002441
Epoch 2190, val loss: 0.4469603896141052
Epoch 2200, training loss: 835.2175903320312 = 0.358531653881073 + 100.0 * 8.348590850830078
Epoch 2200, val loss: 0.446585088968277
Epoch 2210, training loss: 835.1814575195312 = 0.3579409718513489 + 100.0 * 8.348235130310059
Epoch 2210, val loss: 0.44623327255249023
Epoch 2220, training loss: 835.2281494140625 = 0.3573332726955414 + 100.0 * 8.348708152770996
Epoch 2220, val loss: 0.44584858417510986
Epoch 2230, training loss: 835.5439453125 = 0.3567100763320923 + 100.0 * 8.351872444152832
Epoch 2230, val loss: 0.44547462463378906
Epoch 2240, training loss: 835.216796875 = 0.35608094930648804 + 100.0 * 8.348607063293457
Epoch 2240, val loss: 0.4451029300689697
Epoch 2250, training loss: 835.0903930664062 = 0.3554491102695465 + 100.0 * 8.347349166870117
Epoch 2250, val loss: 0.4447230398654938
Epoch 2260, training loss: 835.0388793945312 = 0.35482725501060486 + 100.0 * 8.346840858459473
Epoch 2260, val loss: 0.44446250796318054
Epoch 2270, training loss: 835.063232421875 = 0.3542003929615021 + 100.0 * 8.347090721130371
Epoch 2270, val loss: 0.44421613216400146
Epoch 2280, training loss: 835.16748046875 = 0.3535608649253845 + 100.0 * 8.348138809204102
Epoch 2280, val loss: 0.44388121366500854
Epoch 2290, training loss: 834.99560546875 = 0.35289430618286133 + 100.0 * 8.346426963806152
Epoch 2290, val loss: 0.4434768855571747
Epoch 2300, training loss: 834.9042358398438 = 0.3522416353225708 + 100.0 * 8.34552001953125
Epoch 2300, val loss: 0.4431375563144684
Epoch 2310, training loss: 834.8690185546875 = 0.35159769654273987 + 100.0 * 8.345173835754395
Epoch 2310, val loss: 0.4428431987762451
Epoch 2320, training loss: 834.9112548828125 = 0.3509550094604492 + 100.0 * 8.345602989196777
Epoch 2320, val loss: 0.4424944221973419
Epoch 2330, training loss: 835.2788696289062 = 0.3503114581108093 + 100.0 * 8.349285125732422
Epoch 2330, val loss: 0.4421843886375427
Epoch 2340, training loss: 834.8851928710938 = 0.34961560368537903 + 100.0 * 8.345355987548828
Epoch 2340, val loss: 0.4417043626308441
Epoch 2350, training loss: 834.8032836914062 = 0.34894540905952454 + 100.0 * 8.34454345703125
Epoch 2350, val loss: 0.44139590859413147
Epoch 2360, training loss: 834.7600708007812 = 0.34828388690948486 + 100.0 * 8.344118118286133
Epoch 2360, val loss: 0.441019207239151
Epoch 2370, training loss: 834.7094116210938 = 0.34761670231819153 + 100.0 * 8.343618392944336
Epoch 2370, val loss: 0.44066497683525085
Epoch 2380, training loss: 834.6829223632812 = 0.34694769978523254 + 100.0 * 8.34335994720459
Epoch 2380, val loss: 0.44029852747917175
Epoch 2390, training loss: 834.7531127929688 = 0.34627845883369446 + 100.0 * 8.34406852722168
Epoch 2390, val loss: 0.43997231125831604
Epoch 2400, training loss: 834.9403076171875 = 0.34560221433639526 + 100.0 * 8.345947265625
Epoch 2400, val loss: 0.43964335322380066
Epoch 2410, training loss: 834.9282836914062 = 0.34490516781806946 + 100.0 * 8.345833778381348
Epoch 2410, val loss: 0.439238965511322
Epoch 2420, training loss: 834.6104736328125 = 0.3442023694515228 + 100.0 * 8.342662811279297
Epoch 2420, val loss: 0.4388551115989685
Epoch 2430, training loss: 834.6149291992188 = 0.34351786971092224 + 100.0 * 8.342714309692383
Epoch 2430, val loss: 0.43849095702171326
Epoch 2440, training loss: 834.588623046875 = 0.3428367078304291 + 100.0 * 8.34245777130127
Epoch 2440, val loss: 0.4381420612335205
Epoch 2450, training loss: 834.6140747070312 = 0.34215444326400757 + 100.0 * 8.342719078063965
Epoch 2450, val loss: 0.4377678632736206
Epoch 2460, training loss: 834.8275756835938 = 0.3414658010005951 + 100.0 * 8.344861030578613
Epoch 2460, val loss: 0.4374004602432251
Epoch 2470, training loss: 834.5031127929688 = 0.3407650291919708 + 100.0 * 8.341623306274414
Epoch 2470, val loss: 0.43706583976745605
Epoch 2480, training loss: 834.5589599609375 = 0.3400748670101166 + 100.0 * 8.342188835144043
Epoch 2480, val loss: 0.43675971031188965
Epoch 2490, training loss: 834.4795532226562 = 0.339377760887146 + 100.0 * 8.341402053833008
Epoch 2490, val loss: 0.4364287853240967
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8163368848300354
0.8648119973918714
The final CL Acc:0.81515, 0.00282, The final GNN Acc:0.86464, 0.00057
Begin epxeriment: noisy_level: 0.2 cont_weight: 100 epoch: der1:0.3 der2:0.2 dfr1:0.4 dfr2:0
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=2500, cl_num_layers=2, cl_num_proj_hidden=64, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=100.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.2, drop_feat_rate_1=0.4, drop_feat_rate_2=0.0, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=32, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, noisy_level=0.2, num_hidden=64, num_proj_hidden=64, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=40, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 88648])
add edge: torch.Size([2, 106292])
remove edge: torch.Size([2, 71024])
updated graph: torch.Size([2, 88668])
=== Noisy graph ===
/home/project-robust-contrastive/Robust-Contrastive-Learning/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 1059.3104248046875 = 1.0883733034133911 + 100.0 * 10.582220077514648
Epoch 0, val loss: 1.0886543989181519
Epoch 10, training loss: 1059.2586669921875 = 1.084975004196167 + 100.0 * 10.581737518310547
Epoch 10, val loss: 1.0852810144424438
Epoch 20, training loss: 1059.0699462890625 = 1.0814517736434937 + 100.0 * 10.579885482788086
Epoch 20, val loss: 1.081788182258606
Epoch 30, training loss: 1058.308349609375 = 1.0776634216308594 + 100.0 * 10.572307586669922
Epoch 30, val loss: 1.0780401229858398
Epoch 40, training loss: 1055.4986572265625 = 1.0734857320785522 + 100.0 * 10.544251441955566
Epoch 40, val loss: 1.0738883018493652
Epoch 50, training loss: 1046.9017333984375 = 1.0689033269882202 + 100.0 * 10.458329200744629
Epoch 50, val loss: 1.069393277168274
Epoch 60, training loss: 1022.24951171875 = 1.0643603801727295 + 100.0 * 10.211852073669434
Epoch 60, val loss: 1.0649532079696655
Epoch 70, training loss: 980.560302734375 = 1.059287190437317 + 100.0 * 9.795010566711426
Epoch 70, val loss: 1.0599939823150635
Epoch 80, training loss: 962.7050170898438 = 1.0543659925460815 + 100.0 * 9.616506576538086
Epoch 80, val loss: 1.055187463760376
Epoch 90, training loss: 949.33935546875 = 1.0502145290374756 + 100.0 * 9.482891082763672
Epoch 90, val loss: 1.0512332916259766
Epoch 100, training loss: 934.2589721679688 = 1.047229528427124 + 100.0 * 9.332117080688477
Epoch 100, val loss: 1.0484280586242676
Epoch 110, training loss: 927.369873046875 = 1.0450700521469116 + 100.0 * 9.263248443603516
Epoch 110, val loss: 1.0463182926177979
Epoch 120, training loss: 923.3507690429688 = 1.043092131614685 + 100.0 * 9.223076820373535
Epoch 120, val loss: 1.0445154905319214
Epoch 130, training loss: 917.126220703125 = 1.0419548749923706 + 100.0 * 9.160842895507812
Epoch 130, val loss: 1.0435645580291748
Epoch 140, training loss: 908.0833129882812 = 1.0418881177902222 + 100.0 * 9.070414543151855
Epoch 140, val loss: 1.04359769821167
Epoch 150, training loss: 897.3095092773438 = 1.0427687168121338 + 100.0 * 8.962667465209961
Epoch 150, val loss: 1.0444316864013672
Epoch 160, training loss: 891.8350830078125 = 1.0432368516921997 + 100.0 * 8.907918930053711
Epoch 160, val loss: 1.044690489768982
Epoch 170, training loss: 888.6365966796875 = 1.0425065755844116 + 100.0 * 8.875941276550293
Epoch 170, val loss: 1.0439002513885498
Epoch 180, training loss: 884.4085083007812 = 1.041831135749817 + 100.0 * 8.833666801452637
Epoch 180, val loss: 1.0433145761489868
Epoch 190, training loss: 880.076171875 = 1.0417581796646118 + 100.0 * 8.79034423828125
Epoch 190, val loss: 1.0432151556015015
Epoch 200, training loss: 876.0361938476562 = 1.0417591333389282 + 100.0 * 8.749944686889648
Epoch 200, val loss: 1.0431699752807617
Epoch 210, training loss: 872.8920288085938 = 1.0416496992111206 + 100.0 * 8.718503952026367
Epoch 210, val loss: 1.0430351495742798
Epoch 220, training loss: 870.51904296875 = 1.0413897037506104 + 100.0 * 8.69477653503418
Epoch 220, val loss: 1.0427604913711548
Epoch 230, training loss: 868.9248657226562 = 1.041018009185791 + 100.0 * 8.678838729858398
Epoch 230, val loss: 1.0423434972763062
Epoch 240, training loss: 867.4052124023438 = 1.0405067205429077 + 100.0 * 8.663646697998047
Epoch 240, val loss: 1.041817307472229
Epoch 250, training loss: 866.2766723632812 = 1.0399723052978516 + 100.0 * 8.652366638183594
Epoch 250, val loss: 1.041273593902588
Epoch 260, training loss: 865.2269287109375 = 1.0394154787063599 + 100.0 * 8.641875267028809
Epoch 260, val loss: 1.0407121181488037
Epoch 270, training loss: 864.330810546875 = 1.0388498306274414 + 100.0 * 8.632919311523438
Epoch 270, val loss: 1.0401432514190674
Epoch 280, training loss: 863.4409790039062 = 1.0382589101791382 + 100.0 * 8.624027252197266
Epoch 280, val loss: 1.0395896434783936
Epoch 290, training loss: 862.5269775390625 = 1.0376795530319214 + 100.0 * 8.614892959594727
Epoch 290, val loss: 1.0390077829360962
Epoch 300, training loss: 861.656982421875 = 1.03707754611969 + 100.0 * 8.606199264526367
Epoch 300, val loss: 1.0384211540222168
Epoch 310, training loss: 860.7077026367188 = 1.0364917516708374 + 100.0 * 8.596712112426758
Epoch 310, val loss: 1.0378563404083252
Epoch 320, training loss: 859.8145141601562 = 1.0359221696853638 + 100.0 * 8.587785720825195
Epoch 320, val loss: 1.0373157262802124
Epoch 330, training loss: 859.1692504882812 = 1.0353500843048096 + 100.0 * 8.581338882446289
Epoch 330, val loss: 1.0367486476898193
Epoch 340, training loss: 858.3877563476562 = 1.0347188711166382 + 100.0 * 8.573530197143555
Epoch 340, val loss: 1.0361316204071045
Epoch 350, training loss: 857.7952270507812 = 1.0340399742126465 + 100.0 * 8.567611694335938
Epoch 350, val loss: 1.03546941280365
Epoch 360, training loss: 857.463134765625 = 1.0332854986190796 + 100.0 * 8.564298629760742
Epoch 360, val loss: 1.0347157716751099
Epoch 370, training loss: 856.9141235351562 = 1.0324357748031616 + 100.0 * 8.558816909790039
Epoch 370, val loss: 1.0339226722717285
Epoch 380, training loss: 856.4779052734375 = 1.031584620475769 + 100.0 * 8.554463386535645
Epoch 380, val loss: 1.033106803894043
Epoch 390, training loss: 856.1170043945312 = 1.030706524848938 + 100.0 * 8.550863265991211
Epoch 390, val loss: 1.032252550125122
Epoch 400, training loss: 855.9785766601562 = 1.0297926664352417 + 100.0 * 8.549488067626953
Epoch 400, val loss: 1.0313626527786255
Epoch 410, training loss: 855.479248046875 = 1.0288386344909668 + 100.0 * 8.544504165649414
Epoch 410, val loss: 1.0304508209228516
Epoch 420, training loss: 855.1201782226562 = 1.027890682220459 + 100.0 * 8.540923118591309
Epoch 420, val loss: 1.0295253992080688
Epoch 430, training loss: 854.7833251953125 = 1.0269286632537842 + 100.0 * 8.537564277648926
Epoch 430, val loss: 1.0285720825195312
Epoch 440, training loss: 854.5740966796875 = 1.0259380340576172 + 100.0 * 8.535481452941895
Epoch 440, val loss: 1.0275911092758179
Epoch 450, training loss: 854.3365478515625 = 1.0248724222183228 + 100.0 * 8.533116340637207
Epoch 450, val loss: 1.0265746116638184
Epoch 460, training loss: 854.0910034179688 = 1.0237995386123657 + 100.0 * 8.530672073364258
Epoch 460, val loss: 1.025518774986267
Epoch 470, training loss: 853.7508544921875 = 1.0226848125457764 + 100.0 * 8.527281761169434
Epoch 470, val loss: 1.024418592453003
Epoch 480, training loss: 853.4612426757812 = 1.0215569734573364 + 100.0 * 8.524396896362305
Epoch 480, val loss: 1.0233216285705566
Epoch 490, training loss: 853.2208251953125 = 1.0204159021377563 + 100.0 * 8.522004127502441
Epoch 490, val loss: 1.022200107574463
Epoch 500, training loss: 852.9969482421875 = 1.0192152261734009 + 100.0 * 8.519777297973633
Epoch 500, val loss: 1.0210273265838623
Epoch 510, training loss: 852.6942749023438 = 1.018005609512329 + 100.0 * 8.516762733459473
Epoch 510, val loss: 1.0198501348495483
Epoch 520, training loss: 852.4482421875 = 1.016800045967102 + 100.0 * 8.514314651489258
Epoch 520, val loss: 1.0186803340911865
Epoch 530, training loss: 852.7366333007812 = 1.0155616998672485 + 100.0 * 8.517210960388184
Epoch 530, val loss: 1.0174717903137207
Epoch 540, training loss: 852.1130981445312 = 1.0142648220062256 + 100.0 * 8.510988235473633
Epoch 540, val loss: 1.0162166357040405
Epoch 550, training loss: 851.7623901367188 = 1.0130090713500977 + 100.0 * 8.50749397277832
Epoch 550, val loss: 1.014998435974121
Epoch 560, training loss: 851.5052490234375 = 1.0117480754852295 + 100.0 * 8.504935264587402
Epoch 560, val loss: 1.0137693881988525
Epoch 570, training loss: 851.263671875 = 1.0104682445526123 + 100.0 * 8.502532005310059
Epoch 570, val loss: 1.0125255584716797
Epoch 580, training loss: 851.1101684570312 = 1.009161353111267 + 100.0 * 8.501009941101074
Epoch 580, val loss: 1.0112683773040771
Epoch 590, training loss: 851.104248046875 = 1.0077656507492065 + 100.0 * 8.500965118408203
Epoch 590, val loss: 1.0098482370376587
Epoch 600, training loss: 850.6620483398438 = 1.0063512325286865 + 100.0 * 8.496557235717773
Epoch 600, val loss: 1.0085065364837646
Epoch 610, training loss: 850.4347534179688 = 1.0049597024917603 + 100.0 * 8.494297981262207
Epoch 610, val loss: 1.0071734189987183
Epoch 620, training loss: 850.4234008789062 = 1.0035370588302612 + 100.0 * 8.4941987991333
Epoch 620, val loss: 1.005812644958496
Epoch 630, training loss: 850.2032470703125 = 1.0020740032196045 + 100.0 * 8.492012023925781
Epoch 630, val loss: 1.004321575164795
Epoch 640, training loss: 849.909912109375 = 1.0005733966827393 + 100.0 * 8.489093780517578
Epoch 640, val loss: 1.0028842687606812
Epoch 650, training loss: 849.7283325195312 = 0.9990854859352112 + 100.0 * 8.487292289733887
Epoch 650, val loss: 1.001434564590454
Epoch 660, training loss: 849.7919311523438 = 0.9975483417510986 + 100.0 * 8.487943649291992
Epoch 660, val loss: 0.9999059438705444
Epoch 670, training loss: 849.4282836914062 = 0.9959294199943542 + 100.0 * 8.484323501586914
Epoch 670, val loss: 0.9983298182487488
Epoch 680, training loss: 849.2357788085938 = 0.9943458437919617 + 100.0 * 8.482414245605469
Epoch 680, val loss: 0.9967690110206604
Epoch 690, training loss: 849.0755004882812 = 0.9927485585212708 + 100.0 * 8.480827331542969
Epoch 690, val loss: 0.9952201247215271
Epoch 700, training loss: 848.9992065429688 = 0.9911283850669861 + 100.0 * 8.480080604553223
Epoch 700, val loss: 0.9936375617980957
Epoch 710, training loss: 848.822021484375 = 0.9894140362739563 + 100.0 * 8.478325843811035
Epoch 710, val loss: 0.9919437170028687
Epoch 720, training loss: 848.6586303710938 = 0.9876893162727356 + 100.0 * 8.476709365844727
Epoch 720, val loss: 0.9902689456939697
Epoch 730, training loss: 848.5337524414062 = 0.9859700202941895 + 100.0 * 8.475478172302246
Epoch 730, val loss: 0.9885751008987427
Epoch 740, training loss: 848.6578979492188 = 0.9841874241828918 + 100.0 * 8.476737022399902
Epoch 740, val loss: 0.9868215322494507
Epoch 750, training loss: 848.2997436523438 = 0.9823322892189026 + 100.0 * 8.473174095153809
Epoch 750, val loss: 0.9850123524665833
Epoch 760, training loss: 848.1761474609375 = 0.9805068969726562 + 100.0 * 8.471956253051758
Epoch 760, val loss: 0.9832395315170288
Epoch 770, training loss: 848.073974609375 = 0.9786706566810608 + 100.0 * 8.470952987670898
Epoch 770, val loss: 0.9814524054527283
Epoch 780, training loss: 848.2461547851562 = 0.976751446723938 + 100.0 * 8.472694396972656
Epoch 780, val loss: 0.9795836210250854
Epoch 790, training loss: 847.8749389648438 = 0.9747943878173828 + 100.0 * 8.469001770019531
Epoch 790, val loss: 0.9776384234428406
Epoch 800, training loss: 847.7220458984375 = 0.9728437066078186 + 100.0 * 8.46749210357666
Epoch 800, val loss: 0.9757667779922485
Epoch 810, training loss: 847.7271728515625 = 0.9708918333053589 + 100.0 * 8.467562675476074
Epoch 810, val loss: 0.9738646745681763
Epoch 820, training loss: 847.525634765625 = 0.9688707590103149 + 100.0 * 8.465567588806152
Epoch 820, val loss: 0.9718450307846069
Epoch 830, training loss: 847.4075317382812 = 0.9668419361114502 + 100.0 * 8.464406967163086
Epoch 830, val loss: 0.9698982834815979
Epoch 840, training loss: 847.3150634765625 = 0.9648061990737915 + 100.0 * 8.463502883911133
Epoch 840, val loss: 0.9679027199745178
Epoch 850, training loss: 847.4325561523438 = 0.9627273678779602 + 100.0 * 8.464698791503906
Epoch 850, val loss: 0.9658804535865784
Epoch 860, training loss: 847.1477661132812 = 0.9605574607849121 + 100.0 * 8.461872100830078
Epoch 860, val loss: 0.9637528657913208
Epoch 870, training loss: 847.0818481445312 = 0.9584159255027771 + 100.0 * 8.461234092712402
Epoch 870, val loss: 0.9616798162460327
Epoch 880, training loss: 846.9821166992188 = 0.9562167525291443 + 100.0 * 8.460258483886719
Epoch 880, val loss: 0.9595229625701904
Epoch 890, training loss: 846.850830078125 = 0.9540458917617798 + 100.0 * 8.458968162536621
Epoch 890, val loss: 0.9574128985404968
Epoch 900, training loss: 846.728515625 = 0.951850950717926 + 100.0 * 8.45776653289795
Epoch 900, val loss: 0.9552856087684631
Epoch 910, training loss: 846.6561889648438 = 0.9496450424194336 + 100.0 * 8.45706558227539
Epoch 910, val loss: 0.9531230330467224
Epoch 920, training loss: 846.8082885742188 = 0.9472971558570862 + 100.0 * 8.458609580993652
Epoch 920, val loss: 0.9508371949195862
Epoch 930, training loss: 846.5961303710938 = 0.9448862075805664 + 100.0 * 8.456512451171875
Epoch 930, val loss: 0.9485048651695251
Epoch 940, training loss: 846.3870239257812 = 0.9425632953643799 + 100.0 * 8.454444885253906
Epoch 940, val loss: 0.9462781548500061
Epoch 950, training loss: 846.2788696289062 = 0.9402667880058289 + 100.0 * 8.453386306762695
Epoch 950, val loss: 0.9440802931785583
Epoch 960, training loss: 846.277587890625 = 0.9379469156265259 + 100.0 * 8.453396797180176
Epoch 960, val loss: 0.9418488144874573
Epoch 970, training loss: 846.3999633789062 = 0.9354172348976135 + 100.0 * 8.454645156860352
Epoch 970, val loss: 0.9394188523292542
Epoch 980, training loss: 846.1549682617188 = 0.9328474998474121 + 100.0 * 8.452220916748047
Epoch 980, val loss: 0.9369332790374756
Epoch 990, training loss: 846.0157470703125 = 0.9304054379463196 + 100.0 * 8.45085334777832
Epoch 990, val loss: 0.9345945119857788
Epoch 1000, training loss: 845.9115600585938 = 0.9280067086219788 + 100.0 * 8.449835777282715
Epoch 1000, val loss: 0.9322752952575684
Epoch 1010, training loss: 845.82421875 = 0.9255871772766113 + 100.0 * 8.448986053466797
Epoch 1010, val loss: 0.929942786693573
Epoch 1020, training loss: 845.745361328125 = 0.9231278300285339 + 100.0 * 8.448222160339355
Epoch 1020, val loss: 0.9275761842727661
Epoch 1030, training loss: 845.7036743164062 = 0.9206335544586182 + 100.0 * 8.447830200195312
Epoch 1030, val loss: 0.9251881241798401
Epoch 1040, training loss: 845.9988403320312 = 0.9179553985595703 + 100.0 * 8.45080852508545
Epoch 1040, val loss: 0.9225798845291138
Epoch 1050, training loss: 845.74365234375 = 0.9151688814163208 + 100.0 * 8.448285102844238
Epoch 1050, val loss: 0.9199534058570862
Epoch 1060, training loss: 845.5347900390625 = 0.912554919719696 + 100.0 * 8.446222305297852
Epoch 1060, val loss: 0.9174370765686035
Epoch 1070, training loss: 845.431884765625 = 0.9100005030632019 + 100.0 * 8.445219039916992
Epoch 1070, val loss: 0.9149940609931946
Epoch 1080, training loss: 845.3462524414062 = 0.9074259400367737 + 100.0 * 8.444388389587402
Epoch 1080, val loss: 0.9125338792800903
Epoch 1090, training loss: 845.2842407226562 = 0.9048218727111816 + 100.0 * 8.443794250488281
Epoch 1090, val loss: 0.910042941570282
Epoch 1100, training loss: 845.2573852539062 = 0.9021778106689453 + 100.0 * 8.443552017211914
Epoch 1100, val loss: 0.9075110554695129
Epoch 1110, training loss: 845.5952758789062 = 0.8993374109268188 + 100.0 * 8.446959495544434
Epoch 1110, val loss: 0.904758095741272
Epoch 1120, training loss: 845.2242431640625 = 0.8963934183120728 + 100.0 * 8.443278312683105
Epoch 1120, val loss: 0.9019824862480164
Epoch 1130, training loss: 845.0728759765625 = 0.8936324119567871 + 100.0 * 8.441792488098145
Epoch 1130, val loss: 0.8993433117866516
Epoch 1140, training loss: 845.002197265625 = 0.8909473419189453 + 100.0 * 8.441112518310547
Epoch 1140, val loss: 0.8968220353126526
Epoch 1150, training loss: 844.9343872070312 = 0.8882476091384888 + 100.0 * 8.440461158752441
Epoch 1150, val loss: 0.8942618370056152
Epoch 1160, training loss: 845.0000610351562 = 0.8854994177818298 + 100.0 * 8.441145896911621
Epoch 1160, val loss: 0.8916640877723694
Epoch 1170, training loss: 844.810791015625 = 0.8826056122779846 + 100.0 * 8.439281463623047
Epoch 1170, val loss: 0.8888587355613708
Epoch 1180, training loss: 844.7576293945312 = 0.8797850608825684 + 100.0 * 8.4387788772583
Epoch 1180, val loss: 0.8862161636352539
Epoch 1190, training loss: 844.69482421875 = 0.8769786953926086 + 100.0 * 8.438179016113281
Epoch 1190, val loss: 0.8835380673408508
Epoch 1200, training loss: 844.6404418945312 = 0.8741822838783264 + 100.0 * 8.437662124633789
Epoch 1200, val loss: 0.8808921575546265
Epoch 1210, training loss: 844.5993041992188 = 0.8713469505310059 + 100.0 * 8.43727970123291
Epoch 1210, val loss: 0.8782321810722351
Epoch 1220, training loss: 845.1893310546875 = 0.8683154582977295 + 100.0 * 8.44321060180664
Epoch 1220, val loss: 0.8753489851951599
Epoch 1230, training loss: 844.6961669921875 = 0.8651577234268188 + 100.0 * 8.438309669494629
Epoch 1230, val loss: 0.8723668456077576
Epoch 1240, training loss: 844.5093994140625 = 0.8621947169303894 + 100.0 * 8.436471939086914
Epoch 1240, val loss: 0.8695685267448425
Epoch 1250, training loss: 844.3898315429688 = 0.8593682646751404 + 100.0 * 8.435304641723633
Epoch 1250, val loss: 0.8669058680534363
Epoch 1260, training loss: 844.3286743164062 = 0.8565206527709961 + 100.0 * 8.434721946716309
Epoch 1260, val loss: 0.864224374294281
Epoch 1270, training loss: 844.3759155273438 = 0.8536387085914612 + 100.0 * 8.435222625732422
Epoch 1270, val loss: 0.8615222573280334
Epoch 1280, training loss: 844.30908203125 = 0.8505392670631409 + 100.0 * 8.434585571289062
Epoch 1280, val loss: 0.8585337400436401
Epoch 1290, training loss: 844.17626953125 = 0.8475119471549988 + 100.0 * 8.433287620544434
Epoch 1290, val loss: 0.8557324409484863
Epoch 1300, training loss: 844.1182861328125 = 0.84455806016922 + 100.0 * 8.432737350463867
Epoch 1300, val loss: 0.8529229164123535
Epoch 1310, training loss: 844.0654907226562 = 0.8416320085525513 + 100.0 * 8.432238578796387
Epoch 1310, val loss: 0.8501954078674316
Epoch 1320, training loss: 844.03955078125 = 0.8386902809143066 + 100.0 * 8.432008743286133
Epoch 1320, val loss: 0.8474096059799194
Epoch 1330, training loss: 844.4545288085938 = 0.8356210589408875 + 100.0 * 8.436188697814941
Epoch 1330, val loss: 0.844455897808075
Epoch 1340, training loss: 844.1476440429688 = 0.8323853611946106 + 100.0 * 8.433152198791504
Epoch 1340, val loss: 0.8415005207061768
Epoch 1350, training loss: 843.9413452148438 = 0.8292673826217651 + 100.0 * 8.431120872497559
Epoch 1350, val loss: 0.8386147022247314
Epoch 1360, training loss: 843.8574829101562 = 0.8262661099433899 + 100.0 * 8.430312156677246
Epoch 1360, val loss: 0.8357532024383545
Epoch 1370, training loss: 843.7969970703125 = 0.8233137726783752 + 100.0 * 8.429737091064453
Epoch 1370, val loss: 0.8330022096633911
Epoch 1380, training loss: 843.755859375 = 0.82033771276474 + 100.0 * 8.42935562133789
Epoch 1380, val loss: 0.8302124738693237
Epoch 1390, training loss: 844.1549072265625 = 0.8172755241394043 + 100.0 * 8.43337631225586
Epoch 1390, val loss: 0.8272915482521057
Epoch 1400, training loss: 843.8488159179688 = 0.8139923214912415 + 100.0 * 8.43034839630127
Epoch 1400, val loss: 0.8242866396903992
Epoch 1410, training loss: 843.6940307617188 = 0.8108912706375122 + 100.0 * 8.428831100463867
Epoch 1410, val loss: 0.8213624358177185
Epoch 1420, training loss: 843.5742797851562 = 0.8079012632369995 + 100.0 * 8.427663803100586
Epoch 1420, val loss: 0.8185991048812866
Epoch 1430, training loss: 843.52392578125 = 0.8049081563949585 + 100.0 * 8.427189826965332
Epoch 1430, val loss: 0.815800130367279
Epoch 1440, training loss: 843.8128662109375 = 0.8018633723258972 + 100.0 * 8.430109977722168
Epoch 1440, val loss: 0.8129633665084839
Epoch 1450, training loss: 843.7261962890625 = 0.798636257648468 + 100.0 * 8.429275512695312
Epoch 1450, val loss: 0.8098900318145752
Epoch 1460, training loss: 843.4271850585938 = 0.7954699397087097 + 100.0 * 8.42631721496582
Epoch 1460, val loss: 0.8070134520530701
Epoch 1470, training loss: 843.3717651367188 = 0.7924636602401733 + 100.0 * 8.425792694091797
Epoch 1470, val loss: 0.8041921854019165
Epoch 1480, training loss: 843.3703002929688 = 0.7894974946975708 + 100.0 * 8.42580795288086
Epoch 1480, val loss: 0.8014464974403381
Epoch 1490, training loss: 843.4926147460938 = 0.7864501476287842 + 100.0 * 8.427062034606934
Epoch 1490, val loss: 0.7985879778862
Epoch 1500, training loss: 843.2461547851562 = 0.7833319902420044 + 100.0 * 8.424628257751465
Epoch 1500, val loss: 0.7956857681274414
Epoch 1510, training loss: 843.2125244140625 = 0.7803170084953308 + 100.0 * 8.424322128295898
Epoch 1510, val loss: 0.7929092049598694
Epoch 1520, training loss: 843.1782836914062 = 0.7773532867431641 + 100.0 * 8.424009323120117
Epoch 1520, val loss: 0.7901467084884644
Epoch 1530, training loss: 843.4537963867188 = 0.7742974162101746 + 100.0 * 8.42679500579834
Epoch 1530, val loss: 0.7872779965400696
Epoch 1540, training loss: 843.1835327148438 = 0.7711841464042664 + 100.0 * 8.424123764038086
Epoch 1540, val loss: 0.7843712568283081
Epoch 1550, training loss: 843.0569458007812 = 0.7681649923324585 + 100.0 * 8.422887802124023
Epoch 1550, val loss: 0.7816160321235657
Epoch 1560, training loss: 843.2969970703125 = 0.7651638984680176 + 100.0 * 8.425318717956543
Epoch 1560, val loss: 0.7788134813308716
Epoch 1570, training loss: 843.06103515625 = 0.762000322341919 + 100.0 * 8.422989845275879
Epoch 1570, val loss: 0.7758044004440308
Epoch 1580, training loss: 842.9611206054688 = 0.7589206695556641 + 100.0 * 8.422021865844727
Epoch 1580, val loss: 0.7730478048324585
Epoch 1590, training loss: 842.8936767578125 = 0.756020724773407 + 100.0 * 8.42137622833252
Epoch 1590, val loss: 0.7703768610954285
Epoch 1600, training loss: 842.8434448242188 = 0.7531264424324036 + 100.0 * 8.420903205871582
Epoch 1600, val loss: 0.7677008509635925
Epoch 1610, training loss: 842.8912353515625 = 0.7502228617668152 + 100.0 * 8.421409606933594
Epoch 1610, val loss: 0.7650622725486755
Epoch 1620, training loss: 842.9971923828125 = 0.7471422553062439 + 100.0 * 8.422500610351562
Epoch 1620, val loss: 0.762081503868103
Epoch 1630, training loss: 842.7853393554688 = 0.744016170501709 + 100.0 * 8.42041301727295
Epoch 1630, val loss: 0.759240448474884
Epoch 1640, training loss: 842.7425537109375 = 0.7410718202590942 + 100.0 * 8.420014381408691
Epoch 1640, val loss: 0.7564899921417236
Epoch 1650, training loss: 842.6593627929688 = 0.7382298111915588 + 100.0 * 8.419211387634277
Epoch 1650, val loss: 0.75392746925354
Epoch 1660, training loss: 842.6097412109375 = 0.7353954911231995 + 100.0 * 8.418743133544922
Epoch 1660, val loss: 0.7512733340263367
Epoch 1670, training loss: 842.5719604492188 = 0.7325627207756042 + 100.0 * 8.418394088745117
Epoch 1670, val loss: 0.7486855983734131
Epoch 1680, training loss: 842.8987426757812 = 0.7297050356864929 + 100.0 * 8.421690940856934
Epoch 1680, val loss: 0.7459970116615295
Epoch 1690, training loss: 842.8711547851562 = 0.7265270948410034 + 100.0 * 8.421446800231934
Epoch 1690, val loss: 0.7431355714797974
Epoch 1700, training loss: 842.474853515625 = 0.7235523462295532 + 100.0 * 8.417512893676758
Epoch 1700, val loss: 0.7403512597084045
Epoch 1710, training loss: 842.4598388671875 = 0.7207604646682739 + 100.0 * 8.417390823364258
Epoch 1710, val loss: 0.7377784252166748
Epoch 1720, training loss: 842.4041748046875 = 0.718004584312439 + 100.0 * 8.416861534118652
Epoch 1720, val loss: 0.7352502346038818
Epoch 1730, training loss: 842.8267211914062 = 0.7152010202407837 + 100.0 * 8.421114921569824
Epoch 1730, val loss: 0.7326169610023499
Epoch 1740, training loss: 842.5250244140625 = 0.7121965885162354 + 100.0 * 8.41812801361084
Epoch 1740, val loss: 0.7298938632011414
Epoch 1750, training loss: 842.3513793945312 = 0.7093846797943115 + 100.0 * 8.416419982910156
Epoch 1750, val loss: 0.7272757291793823
Epoch 1760, training loss: 842.2599487304688 = 0.7066769599914551 + 100.0 * 8.415533065795898
Epoch 1760, val loss: 0.7248125076293945
Epoch 1770, training loss: 842.2781982421875 = 0.7039765119552612 + 100.0 * 8.415741920471191
Epoch 1770, val loss: 0.722295343875885
Epoch 1780, training loss: 842.6449584960938 = 0.7011082172393799 + 100.0 * 8.419438362121582
Epoch 1780, val loss: 0.7195898294448853
Epoch 1790, training loss: 842.3038940429688 = 0.6982848644256592 + 100.0 * 8.416055679321289
Epoch 1790, val loss: 0.7170810103416443
Epoch 1800, training loss: 842.16845703125 = 0.6955541372299194 + 100.0 * 8.414729118347168
Epoch 1800, val loss: 0.7145510315895081
Epoch 1810, training loss: 842.1123657226562 = 0.6929478645324707 + 100.0 * 8.414194107055664
Epoch 1810, val loss: 0.7121707201004028
Epoch 1820, training loss: 842.0746459960938 = 0.6903325319290161 + 100.0 * 8.413843154907227
Epoch 1820, val loss: 0.7097563147544861
Epoch 1830, training loss: 842.0353393554688 = 0.6877216696739197 + 100.0 * 8.41347599029541
Epoch 1830, val loss: 0.7073548436164856
Epoch 1840, training loss: 842.1033935546875 = 0.6851118803024292 + 100.0 * 8.414182662963867
Epoch 1840, val loss: 0.7048513293266296
Epoch 1850, training loss: 841.9994506835938 = 0.6822511553764343 + 100.0 * 8.413171768188477
Epoch 1850, val loss: 0.7023425698280334
Epoch 1860, training loss: 842.0565185546875 = 0.6795366406440735 + 100.0 * 8.413769721984863
Epoch 1860, val loss: 0.6997105479240417
Epoch 1870, training loss: 841.9332885742188 = 0.6769536733627319 + 100.0 * 8.41256332397461
Epoch 1870, val loss: 0.6974561810493469
Epoch 1880, training loss: 841.9053955078125 = 0.6744319200515747 + 100.0 * 8.412309646606445
Epoch 1880, val loss: 0.6951838135719299
Epoch 1890, training loss: 842.0490112304688 = 0.6719233393669128 + 100.0 * 8.41377067565918
Epoch 1890, val loss: 0.6927934288978577
Epoch 1900, training loss: 842.0118408203125 = 0.6691966652870178 + 100.0 * 8.413426399230957
Epoch 1900, val loss: 0.6904066205024719
Epoch 1910, training loss: 841.936767578125 = 0.6665028929710388 + 100.0 * 8.412702560424805
Epoch 1910, val loss: 0.6878102421760559
Epoch 1920, training loss: 841.8284912109375 = 0.6639996767044067 + 100.0 * 8.41164493560791
Epoch 1920, val loss: 0.6856136322021484
Epoch 1930, training loss: 841.7701416015625 = 0.6616008281707764 + 100.0 * 8.41108512878418
Epoch 1930, val loss: 0.6833952069282532
Epoch 1940, training loss: 841.73828125 = 0.6592206954956055 + 100.0 * 8.41079044342041
Epoch 1940, val loss: 0.6811935305595398
Epoch 1950, training loss: 841.731201171875 = 0.6568463444709778 + 100.0 * 8.410743713378906
Epoch 1950, val loss: 0.6790460348129272
Epoch 1960, training loss: 842.1837768554688 = 0.6543940305709839 + 100.0 * 8.41529369354248
Epoch 1960, val loss: 0.6766961812973022
Epoch 1970, training loss: 841.8406982421875 = 0.651814341545105 + 100.0 * 8.41188907623291
Epoch 1970, val loss: 0.674534797668457
Epoch 1980, training loss: 841.721923828125 = 0.6493566632270813 + 100.0 * 8.410725593566895
Epoch 1980, val loss: 0.6721785068511963
Epoch 1990, training loss: 841.650146484375 = 0.6470916867256165 + 100.0 * 8.410030364990234
Epoch 1990, val loss: 0.6701931357383728
Epoch 2000, training loss: 841.6175537109375 = 0.6448342800140381 + 100.0 * 8.409727096557617
Epoch 2000, val loss: 0.6680970788002014
Epoch 2010, training loss: 841.75634765625 = 0.6425811648368835 + 100.0 * 8.411137580871582
Epoch 2010, val loss: 0.6660597920417786
Epoch 2020, training loss: 841.5520629882812 = 0.640201985836029 + 100.0 * 8.40911865234375
Epoch 2020, val loss: 0.6638990640640259
Epoch 2030, training loss: 841.6317138671875 = 0.6379587054252625 + 100.0 * 8.409937858581543
Epoch 2030, val loss: 0.661938488483429
Epoch 2040, training loss: 841.5972900390625 = 0.6356199383735657 + 100.0 * 8.409616470336914
Epoch 2040, val loss: 0.659720242023468
Epoch 2050, training loss: 841.5176391601562 = 0.633383572101593 + 100.0 * 8.408843040466309
Epoch 2050, val loss: 0.6577768325805664
Epoch 2060, training loss: 841.4640502929688 = 0.6312482953071594 + 100.0 * 8.40832805633545
Epoch 2060, val loss: 0.6558007001876831
Epoch 2070, training loss: 841.4260864257812 = 0.6291367411613464 + 100.0 * 8.40796947479248
Epoch 2070, val loss: 0.6539144515991211
Epoch 2080, training loss: 841.40771484375 = 0.6270504593849182 + 100.0 * 8.407806396484375
Epoch 2080, val loss: 0.652065634727478
Epoch 2090, training loss: 841.4293212890625 = 0.6249661445617676 + 100.0 * 8.40804386138916
Epoch 2090, val loss: 0.650211751461029
Epoch 2100, training loss: 841.9639282226562 = 0.6227790117263794 + 100.0 * 8.413411140441895
Epoch 2100, val loss: 0.6482415795326233
Epoch 2110, training loss: 841.4512939453125 = 0.6204649806022644 + 100.0 * 8.408308029174805
Epoch 2110, val loss: 0.6460973024368286
Epoch 2120, training loss: 841.3456420898438 = 0.6184127330780029 + 100.0 * 8.407272338867188
Epoch 2120, val loss: 0.6442112326622009
Epoch 2130, training loss: 841.319580078125 = 0.6164448857307434 + 100.0 * 8.407031059265137
Epoch 2130, val loss: 0.6425434350967407
Epoch 2140, training loss: 841.2785034179688 = 0.614531934261322 + 100.0 * 8.40664005279541
Epoch 2140, val loss: 0.6407831907272339
Epoch 2150, training loss: 841.269287109375 = 0.61262047290802 + 100.0 * 8.406566619873047
Epoch 2150, val loss: 0.6390683650970459
Epoch 2160, training loss: 842.0028686523438 = 0.6106513142585754 + 100.0 * 8.413922309875488
Epoch 2160, val loss: 0.6370883584022522
Epoch 2170, training loss: 841.3594360351562 = 0.6084850430488586 + 100.0 * 8.407509803771973
Epoch 2170, val loss: 0.6353949904441833
Epoch 2180, training loss: 841.2183227539062 = 0.6065416932106018 + 100.0 * 8.40611743927002
Epoch 2180, val loss: 0.6337364912033081
Epoch 2190, training loss: 841.1939697265625 = 0.6047354936599731 + 100.0 * 8.405892372131348
Epoch 2190, val loss: 0.6320164203643799
Epoch 2200, training loss: 841.1572875976562 = 0.6029586791992188 + 100.0 * 8.405543327331543
Epoch 2200, val loss: 0.6304373741149902
Epoch 2210, training loss: 841.4619750976562 = 0.6011592745780945 + 100.0 * 8.408608436584473
Epoch 2210, val loss: 0.6287752985954285
Epoch 2220, training loss: 841.187255859375 = 0.5991915464401245 + 100.0 * 8.40588092803955
Epoch 2220, val loss: 0.6271817088127136
Epoch 2230, training loss: 841.157958984375 = 0.5973803400993347 + 100.0 * 8.405606269836426
Epoch 2230, val loss: 0.6254834532737732
Epoch 2240, training loss: 841.0962524414062 = 0.5956721305847168 + 100.0 * 8.405006408691406
Epoch 2240, val loss: 0.6240755319595337
Epoch 2250, training loss: 841.0582885742188 = 0.5939900279045105 + 100.0 * 8.404643058776855
Epoch 2250, val loss: 0.6225807666778564
Epoch 2260, training loss: 841.0853881835938 = 0.5923275351524353 + 100.0 * 8.404930114746094
Epoch 2260, val loss: 0.6211575269699097
Epoch 2270, training loss: 841.62548828125 = 0.5905368328094482 + 100.0 * 8.41034984588623
Epoch 2270, val loss: 0.6196064949035645
Epoch 2280, training loss: 841.066162109375 = 0.5886562466621399 + 100.0 * 8.40477466583252
Epoch 2280, val loss: 0.6178267598152161
Epoch 2290, training loss: 841.0148315429688 = 0.5869849920272827 + 100.0 * 8.404278755187988
Epoch 2290, val loss: 0.616439163684845
Epoch 2300, training loss: 840.9760131835938 = 0.5854610800743103 + 100.0 * 8.403905868530273
Epoch 2300, val loss: 0.615119993686676
Epoch 2310, training loss: 840.940673828125 = 0.5839019417762756 + 100.0 * 8.40356731414795
Epoch 2310, val loss: 0.6137712001800537
Epoch 2320, training loss: 840.9219360351562 = 0.5823672413825989 + 100.0 * 8.403395652770996
Epoch 2320, val loss: 0.6124644875526428
Epoch 2330, training loss: 840.9678344726562 = 0.5808467864990234 + 100.0 * 8.40386962890625
Epoch 2330, val loss: 0.611250102519989
Epoch 2340, training loss: 841.0927734375 = 0.5791531801223755 + 100.0 * 8.405136108398438
Epoch 2340, val loss: 0.6096424460411072
Epoch 2350, training loss: 840.8946533203125 = 0.5775313973426819 + 100.0 * 8.40317153930664
Epoch 2350, val loss: 0.6083846092224121
Epoch 2360, training loss: 840.9007568359375 = 0.5760132670402527 + 100.0 * 8.403247833251953
Epoch 2360, val loss: 0.6068540215492249
Epoch 2370, training loss: 840.8447265625 = 0.574574887752533 + 100.0 * 8.402701377868652
Epoch 2370, val loss: 0.6057385206222534
Epoch 2380, training loss: 841.0218505859375 = 0.5731439590454102 + 100.0 * 8.404487609863281
Epoch 2380, val loss: 0.6045809984207153
Epoch 2390, training loss: 840.8217163085938 = 0.5716003179550171 + 100.0 * 8.402501106262207
Epoch 2390, val loss: 0.6031786203384399
Epoch 2400, training loss: 840.7894287109375 = 0.5701504945755005 + 100.0 * 8.402193069458008
Epoch 2400, val loss: 0.6019312143325806
Epoch 2410, training loss: 840.7692260742188 = 0.5687758326530457 + 100.0 * 8.40200424194336
Epoch 2410, val loss: 0.6007935404777527
Epoch 2420, training loss: 840.7747802734375 = 0.5674127340316772 + 100.0 * 8.402073860168457
Epoch 2420, val loss: 0.5996114611625671
Epoch 2430, training loss: 841.0740356445312 = 0.566003680229187 + 100.0 * 8.405080795288086
Epoch 2430, val loss: 0.5983797311782837
Epoch 2440, training loss: 840.7506103515625 = 0.564556896686554 + 100.0 * 8.401860237121582
Epoch 2440, val loss: 0.5971508622169495
Epoch 2450, training loss: 840.7080078125 = 0.5631780624389648 + 100.0 * 8.401448249816895
Epoch 2450, val loss: 0.5959957838058472
Epoch 2460, training loss: 840.7106323242188 = 0.5618871450424194 + 100.0 * 8.401487350463867
Epoch 2460, val loss: 0.5948886275291443
Epoch 2470, training loss: 840.8633422851562 = 0.5606006383895874 + 100.0 * 8.403027534484863
Epoch 2470, val loss: 0.5936561822891235
Epoch 2480, training loss: 840.7281494140625 = 0.5592038631439209 + 100.0 * 8.401689529418945
Epoch 2480, val loss: 0.5926953554153442
Epoch 2490, training loss: 840.709228515625 = 0.5579206943511963 + 100.0 * 8.40151309967041
Epoch 2490, val loss: 0.5917143225669861
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7777777777777777
0.8183003694848947
=== training gcn model ===
Epoch 0, training loss: 1059.3472900390625 = 1.119803786277771 + 100.0 * 10.582275390625
Epoch 0, val loss: 1.118952989578247
Epoch 10, training loss: 1059.308349609375 = 1.1148744821548462 + 100.0 * 10.581934928894043
Epoch 10, val loss: 1.113981008529663
Epoch 20, training loss: 1059.178466796875 = 1.1094385385513306 + 100.0 * 10.580689430236816
Epoch 20, val loss: 1.1084778308868408
Epoch 30, training loss: 1058.6856689453125 = 1.1031996011734009 + 100.0 * 10.575824737548828
Epoch 30, val loss: 1.102150321006775
Epoch 40, training loss: 1056.778076171875 = 1.0958703756332397 + 100.0 * 10.556822776794434
Epoch 40, val loss: 1.0947390794754028
Epoch 50, training loss: 1050.3514404296875 = 1.0875358581542969 + 100.0 * 10.492639541625977
Epoch 50, val loss: 1.0864307880401611
Epoch 60, training loss: 1031.5374755859375 = 1.0790218114852905 + 100.0 * 10.304584503173828
Epoch 60, val loss: 1.0781372785568237
Epoch 70, training loss: 989.396240234375 = 1.0698989629745483 + 100.0 * 9.88326358795166
Epoch 70, val loss: 1.0692676305770874
Epoch 80, training loss: 963.803466796875 = 1.0628856420516968 + 100.0 * 9.627406120300293
Epoch 80, val loss: 1.0629971027374268
Epoch 90, training loss: 953.7199096679688 = 1.0590870380401611 + 100.0 * 9.52660846710205
Epoch 90, val loss: 1.0596120357513428
Epoch 100, training loss: 937.4346923828125 = 1.0572611093521118 + 100.0 * 9.363774299621582
Epoch 100, val loss: 1.0579767227172852
Epoch 110, training loss: 919.0977172851562 = 1.0558831691741943 + 100.0 * 9.180418014526367
Epoch 110, val loss: 1.0565400123596191
Epoch 120, training loss: 909.2677612304688 = 1.0539308786392212 + 100.0 * 9.082138061523438
Epoch 120, val loss: 1.0545611381530762
Epoch 130, training loss: 903.054443359375 = 1.0519517660140991 + 100.0 * 9.020025253295898
Epoch 130, val loss: 1.0525871515274048
Epoch 140, training loss: 896.8115844726562 = 1.0504522323608398 + 100.0 * 8.957611083984375
Epoch 140, val loss: 1.0511467456817627
Epoch 150, training loss: 890.7330932617188 = 1.0495814085006714 + 100.0 * 8.896835327148438
Epoch 150, val loss: 1.0502758026123047
Epoch 160, training loss: 886.1791381835938 = 1.0486900806427002 + 100.0 * 8.851304054260254
Epoch 160, val loss: 1.0492817163467407
Epoch 170, training loss: 881.679443359375 = 1.047404170036316 + 100.0 * 8.806320190429688
Epoch 170, val loss: 1.0480210781097412
Epoch 180, training loss: 877.1514282226562 = 1.0465697050094604 + 100.0 * 8.761048316955566
Epoch 180, val loss: 1.0472534894943237
Epoch 190, training loss: 873.7196044921875 = 1.0459105968475342 + 100.0 * 8.726737022399902
Epoch 190, val loss: 1.0466123819351196
Epoch 200, training loss: 871.6251831054688 = 1.0451456308364868 + 100.0 * 8.70580005645752
Epoch 200, val loss: 1.0458711385726929
Epoch 210, training loss: 869.8912353515625 = 1.0443254709243774 + 100.0 * 8.688468933105469
Epoch 210, val loss: 1.045088768005371
Epoch 220, training loss: 868.624267578125 = 1.0434454679489136 + 100.0 * 8.67580795288086
Epoch 220, val loss: 1.044252634048462
Epoch 230, training loss: 867.4995727539062 = 1.042608618736267 + 100.0 * 8.664569854736328
Epoch 230, val loss: 1.0434720516204834
Epoch 240, training loss: 866.5100708007812 = 1.0418459177017212 + 100.0 * 8.654682159423828
Epoch 240, val loss: 1.042755365371704
Epoch 250, training loss: 865.59912109375 = 1.0410789251327515 + 100.0 * 8.645580291748047
Epoch 250, val loss: 1.042043685913086
Epoch 260, training loss: 864.7506713867188 = 1.0403367280960083 + 100.0 * 8.637103080749512
Epoch 260, val loss: 1.0413568019866943
Epoch 270, training loss: 864.0100708007812 = 1.0395898818969727 + 100.0 * 8.629704475402832
Epoch 270, val loss: 1.0406439304351807
Epoch 280, training loss: 863.1925659179688 = 1.038840413093567 + 100.0 * 8.621537208557129
Epoch 280, val loss: 1.0399419069290161
Epoch 290, training loss: 862.3856811523438 = 1.03810453414917 + 100.0 * 8.613475799560547
Epoch 290, val loss: 1.0392438173294067
Epoch 300, training loss: 861.6055297851562 = 1.0373886823654175 + 100.0 * 8.605681419372559
Epoch 300, val loss: 1.0385669469833374
Epoch 310, training loss: 860.9689331054688 = 1.0366605520248413 + 100.0 * 8.599322319030762
Epoch 310, val loss: 1.0378891229629517
Epoch 320, training loss: 860.2498779296875 = 1.0358717441558838 + 100.0 * 8.592140197753906
Epoch 320, val loss: 1.0371266603469849
Epoch 330, training loss: 859.7305297851562 = 1.0350717306137085 + 100.0 * 8.586954116821289
Epoch 330, val loss: 1.0363658666610718
Epoch 340, training loss: 859.1985473632812 = 1.034248948097229 + 100.0 * 8.581643104553223
Epoch 340, val loss: 1.0355663299560547
Epoch 350, training loss: 858.7739868164062 = 1.0333893299102783 + 100.0 * 8.57740592956543
Epoch 350, val loss: 1.0347458124160767
Epoch 360, training loss: 858.3963623046875 = 1.0324833393096924 + 100.0 * 8.573638916015625
Epoch 360, val loss: 1.0338826179504395
Epoch 370, training loss: 857.9715576171875 = 1.031562089920044 + 100.0 * 8.5693998336792
Epoch 370, val loss: 1.033010721206665
Epoch 380, training loss: 857.5806884765625 = 1.0306519269943237 + 100.0 * 8.565500259399414
Epoch 380, val loss: 1.0321515798568726
Epoch 390, training loss: 857.22119140625 = 1.0297057628631592 + 100.0 * 8.561914443969727
Epoch 390, val loss: 1.0312471389770508
Epoch 400, training loss: 856.7833251953125 = 1.0287469625473022 + 100.0 * 8.55754566192627
Epoch 400, val loss: 1.0303231477737427
Epoch 410, training loss: 856.5941772460938 = 1.0277581214904785 + 100.0 * 8.5556640625
Epoch 410, val loss: 1.0293538570404053
Epoch 420, training loss: 856.0054931640625 = 1.0267045497894287 + 100.0 * 8.549787521362305
Epoch 420, val loss: 1.0283626317977905
Epoch 430, training loss: 855.6216430664062 = 1.0256708860397339 + 100.0 * 8.54595947265625
Epoch 430, val loss: 1.0273796319961548
Epoch 440, training loss: 855.2598876953125 = 1.0246154069900513 + 100.0 * 8.542352676391602
Epoch 440, val loss: 1.0263725519180298
Epoch 450, training loss: 854.9262084960938 = 1.0235155820846558 + 100.0 * 8.539027214050293
Epoch 450, val loss: 1.0253190994262695
Epoch 460, training loss: 854.6968994140625 = 1.0223957300186157 + 100.0 * 8.536745071411133
Epoch 460, val loss: 1.024244785308838
Epoch 470, training loss: 854.2747802734375 = 1.0212465524673462 + 100.0 * 8.532535552978516
Epoch 470, val loss: 1.0231561660766602
Epoch 480, training loss: 853.9741821289062 = 1.0200997591018677 + 100.0 * 8.529541015625
Epoch 480, val loss: 1.022074580192566
Epoch 490, training loss: 853.7893676757812 = 1.0189323425292969 + 100.0 * 8.527704238891602
Epoch 490, val loss: 1.020967721939087
Epoch 500, training loss: 853.494140625 = 1.017737865447998 + 100.0 * 8.524764060974121
Epoch 500, val loss: 1.0197879076004028
Epoch 510, training loss: 853.147705078125 = 1.0165196657180786 + 100.0 * 8.52131175994873
Epoch 510, val loss: 1.018652081489563
Epoch 520, training loss: 852.8700561523438 = 1.0153179168701172 + 100.0 * 8.518547058105469
Epoch 520, val loss: 1.0174765586853027
Epoch 530, training loss: 852.7115478515625 = 1.0140725374221802 + 100.0 * 8.516974449157715
Epoch 530, val loss: 1.0162965059280396
Epoch 540, training loss: 852.4537963867188 = 1.0128023624420166 + 100.0 * 8.514410018920898
Epoch 540, val loss: 1.015076756477356
Epoch 550, training loss: 852.0379638671875 = 1.0115301609039307 + 100.0 * 8.51026439666748
Epoch 550, val loss: 1.0138803720474243
Epoch 560, training loss: 851.8175659179688 = 1.0102533102035522 + 100.0 * 8.508072853088379
Epoch 560, val loss: 1.012671709060669
Epoch 570, training loss: 851.5046997070312 = 1.0089292526245117 + 100.0 * 8.504958152770996
Epoch 570, val loss: 1.0114169120788574
Epoch 580, training loss: 851.2369995117188 = 1.0076147317886353 + 100.0 * 8.502293586730957
Epoch 580, val loss: 1.0101470947265625
Epoch 590, training loss: 851.1585693359375 = 1.0062706470489502 + 100.0 * 8.5015230178833
Epoch 590, val loss: 1.008821725845337
Epoch 600, training loss: 850.8084716796875 = 1.0048264265060425 + 100.0 * 8.49803638458252
Epoch 600, val loss: 1.0074759721755981
Epoch 610, training loss: 850.5339965820312 = 1.0033942461013794 + 100.0 * 8.495306015014648
Epoch 610, val loss: 1.006146788597107
Epoch 620, training loss: 850.37890625 = 1.0019490718841553 + 100.0 * 8.493769645690918
Epoch 620, val loss: 1.0047825574874878
Epoch 630, training loss: 850.1743774414062 = 1.000435709953308 + 100.0 * 8.491739273071289
Epoch 630, val loss: 1.0033326148986816
Epoch 640, training loss: 850.017578125 = 0.9988927245140076 + 100.0 * 8.49018669128418
Epoch 640, val loss: 1.001853108406067
Epoch 650, training loss: 849.9537963867188 = 0.997287392616272 + 100.0 * 8.489564895629883
Epoch 650, val loss: 1.0003515481948853
Epoch 660, training loss: 849.7074584960938 = 0.9956799149513245 + 100.0 * 8.487117767333984
Epoch 660, val loss: 0.9987772107124329
Epoch 670, training loss: 849.5872802734375 = 0.994020938873291 + 100.0 * 8.485932350158691
Epoch 670, val loss: 0.9972184300422668
Epoch 680, training loss: 849.5652465820312 = 0.9922882914543152 + 100.0 * 8.485729217529297
Epoch 680, val loss: 0.9955667853355408
Epoch 690, training loss: 849.3732299804688 = 0.9905278086662292 + 100.0 * 8.483826637268066
Epoch 690, val loss: 0.9938893914222717
Epoch 700, training loss: 849.256103515625 = 0.9887491464614868 + 100.0 * 8.482673645019531
Epoch 700, val loss: 0.9922063946723938
Epoch 710, training loss: 849.1936645507812 = 0.9869003295898438 + 100.0 * 8.482068061828613
Epoch 710, val loss: 0.9904224276542664
Epoch 720, training loss: 849.237548828125 = 0.9850208759307861 + 100.0 * 8.482524871826172
Epoch 720, val loss: 0.9886155724525452
Epoch 730, training loss: 848.9718017578125 = 0.983084499835968 + 100.0 * 8.479887008666992
Epoch 730, val loss: 0.9868072271347046
Epoch 740, training loss: 848.8438110351562 = 0.9811749458312988 + 100.0 * 8.478626251220703
Epoch 740, val loss: 0.9849870204925537
Epoch 750, training loss: 848.8338012695312 = 0.9792172312736511 + 100.0 * 8.478546142578125
Epoch 750, val loss: 0.9831482768058777
Epoch 760, training loss: 848.6586303710938 = 0.9772113561630249 + 100.0 * 8.476814270019531
Epoch 760, val loss: 0.9812042713165283
Epoch 770, training loss: 848.5715942382812 = 0.975190281867981 + 100.0 * 8.475963592529297
Epoch 770, val loss: 0.979296088218689
Epoch 780, training loss: 848.4759521484375 = 0.9731661677360535 + 100.0 * 8.475028038024902
Epoch 780, val loss: 0.9773604273796082
Epoch 790, training loss: 848.3856201171875 = 0.9711122512817383 + 100.0 * 8.47414493560791
Epoch 790, val loss: 0.9754018783569336
Epoch 800, training loss: 848.69140625 = 0.9690249562263489 + 100.0 * 8.47722339630127
Epoch 800, val loss: 0.9733443260192871
Epoch 810, training loss: 848.3159790039062 = 0.9667091965675354 + 100.0 * 8.473492622375488
Epoch 810, val loss: 0.9712342619895935
Epoch 820, training loss: 848.3157348632812 = 0.9645102024078369 + 100.0 * 8.473511695861816
Epoch 820, val loss: 0.9691545367240906
Epoch 830, training loss: 848.1104736328125 = 0.9623286724090576 + 100.0 * 8.471481323242188
Epoch 830, val loss: 0.9670630693435669
Epoch 840, training loss: 848.0360717773438 = 0.9601525068283081 + 100.0 * 8.470759391784668
Epoch 840, val loss: 0.9649751782417297
Epoch 850, training loss: 847.9428100585938 = 0.9579179286956787 + 100.0 * 8.4698486328125
Epoch 850, val loss: 0.9628463983535767
Epoch 860, training loss: 847.9015502929688 = 0.9556690454483032 + 100.0 * 8.46945858001709
Epoch 860, val loss: 0.9606897234916687
Epoch 870, training loss: 847.9439086914062 = 0.9533191323280334 + 100.0 * 8.469905853271484
Epoch 870, val loss: 0.9584305286407471
Epoch 880, training loss: 847.7174682617188 = 0.9509149789810181 + 100.0 * 8.467665672302246
Epoch 880, val loss: 0.9561716914176941
Epoch 890, training loss: 847.6725463867188 = 0.9485984444618225 + 100.0 * 8.467239379882812
Epoch 890, val loss: 0.9539917707443237
Epoch 900, training loss: 847.6173706054688 = 0.9462843537330627 + 100.0 * 8.466711044311523
Epoch 900, val loss: 0.9517694711685181
Epoch 910, training loss: 847.6620483398438 = 0.9438688158988953 + 100.0 * 8.467182159423828
Epoch 910, val loss: 0.9494787454605103
Epoch 920, training loss: 847.5453491210938 = 0.9414048194885254 + 100.0 * 8.466039657592773
Epoch 920, val loss: 0.9471251368522644
Epoch 930, training loss: 847.3438720703125 = 0.9389477968215942 + 100.0 * 8.464049339294434
Epoch 930, val loss: 0.9448200464248657
Epoch 940, training loss: 847.2727661132812 = 0.9365596771240234 + 100.0 * 8.463361740112305
Epoch 940, val loss: 0.9425312280654907
Epoch 950, training loss: 847.1975708007812 = 0.9341235160827637 + 100.0 * 8.462634086608887
Epoch 950, val loss: 0.9402397871017456
Epoch 960, training loss: 847.3081665039062 = 0.9316422939300537 + 100.0 * 8.463765144348145
Epoch 960, val loss: 0.9378535151481628
Epoch 970, training loss: 847.0823974609375 = 0.9290421605110168 + 100.0 * 8.461533546447754
Epoch 970, val loss: 0.9354434013366699
Epoch 980, training loss: 846.9828491210938 = 0.9265796542167664 + 100.0 * 8.460562705993652
Epoch 980, val loss: 0.9330692291259766
Epoch 990, training loss: 847.0028686523438 = 0.9240906834602356 + 100.0 * 8.460787773132324
Epoch 990, val loss: 0.9307043552398682
Epoch 1000, training loss: 846.8231201171875 = 0.9215326905250549 + 100.0 * 8.459015846252441
Epoch 1000, val loss: 0.9283275008201599
Epoch 1010, training loss: 846.7152099609375 = 0.9190183877944946 + 100.0 * 8.457962036132812
Epoch 1010, val loss: 0.9259260296821594
Epoch 1020, training loss: 846.6710815429688 = 0.9164471626281738 + 100.0 * 8.45754623413086
Epoch 1020, val loss: 0.9234839677810669
Epoch 1030, training loss: 846.53125 = 0.9138736128807068 + 100.0 * 8.45617389678955
Epoch 1030, val loss: 0.9210692644119263
Epoch 1040, training loss: 846.4786376953125 = 0.9113038778305054 + 100.0 * 8.455673217773438
Epoch 1040, val loss: 0.9186580181121826
Epoch 1050, training loss: 846.3890380859375 = 0.908683717250824 + 100.0 * 8.454803466796875
Epoch 1050, val loss: 0.9161449670791626
Epoch 1060, training loss: 846.3041381835938 = 0.9060421586036682 + 100.0 * 8.453980445861816
Epoch 1060, val loss: 0.9136561155319214
Epoch 1070, training loss: 846.163330078125 = 0.9034160375595093 + 100.0 * 8.452598571777344
Epoch 1070, val loss: 0.9111958146095276
Epoch 1080, training loss: 846.0247192382812 = 0.9007626175880432 + 100.0 * 8.451239585876465
Epoch 1080, val loss: 0.9087002873420715
Epoch 1090, training loss: 845.9712524414062 = 0.8981126546859741 + 100.0 * 8.45073127746582
Epoch 1090, val loss: 0.9062585234642029
Epoch 1100, training loss: 846.06884765625 = 0.8953661322593689 + 100.0 * 8.45173454284668
Epoch 1100, val loss: 0.9036548137664795
Epoch 1110, training loss: 845.8665771484375 = 0.892620325088501 + 100.0 * 8.449739456176758
Epoch 1110, val loss: 0.9010015726089478
Epoch 1120, training loss: 845.8046264648438 = 0.8898177146911621 + 100.0 * 8.449148178100586
Epoch 1120, val loss: 0.8983576893806458
Epoch 1130, training loss: 845.7162475585938 = 0.8870755434036255 + 100.0 * 8.448291778564453
Epoch 1130, val loss: 0.8957785964012146
Epoch 1140, training loss: 845.7442626953125 = 0.8842804431915283 + 100.0 * 8.448599815368652
Epoch 1140, val loss: 0.8931525349617004
Epoch 1150, training loss: 845.5621337890625 = 0.8814525008201599 + 100.0 * 8.446806907653809
Epoch 1150, val loss: 0.8904399871826172
Epoch 1160, training loss: 845.4835815429688 = 0.8786093592643738 + 100.0 * 8.446049690246582
Epoch 1160, val loss: 0.8878067135810852
Epoch 1170, training loss: 845.4904174804688 = 0.8757840991020203 + 100.0 * 8.446146011352539
Epoch 1170, val loss: 0.8850885629653931
Epoch 1180, training loss: 845.3584594726562 = 0.8728684782981873 + 100.0 * 8.444855690002441
Epoch 1180, val loss: 0.8823981881141663
Epoch 1190, training loss: 845.3837890625 = 0.8699776530265808 + 100.0 * 8.445137977600098
Epoch 1190, val loss: 0.8796290755271912
Epoch 1200, training loss: 845.392333984375 = 0.8670207858085632 + 100.0 * 8.445253372192383
Epoch 1200, val loss: 0.8768528699874878
Epoch 1210, training loss: 845.2429809570312 = 0.8639851808547974 + 100.0 * 8.4437894821167
Epoch 1210, val loss: 0.8739187717437744
Epoch 1220, training loss: 845.1492309570312 = 0.8610599637031555 + 100.0 * 8.44288158416748
Epoch 1220, val loss: 0.8712464570999146
Epoch 1230, training loss: 845.0545043945312 = 0.8581938743591309 + 100.0 * 8.441963195800781
Epoch 1230, val loss: 0.8685305118560791
Epoch 1240, training loss: 844.98583984375 = 0.855326771736145 + 100.0 * 8.441305160522461
Epoch 1240, val loss: 0.8658204078674316
Epoch 1250, training loss: 845.0169067382812 = 0.8524221181869507 + 100.0 * 8.441644668579102
Epoch 1250, val loss: 0.8631190657615662
Epoch 1260, training loss: 844.9074096679688 = 0.8493174314498901 + 100.0 * 8.440581321716309
Epoch 1260, val loss: 0.8601431846618652
Epoch 1270, training loss: 844.9719848632812 = 0.8462878465652466 + 100.0 * 8.441256523132324
Epoch 1270, val loss: 0.8573367595672607
Epoch 1280, training loss: 844.7837524414062 = 0.8433483242988586 + 100.0 * 8.439404487609863
Epoch 1280, val loss: 0.8545565605163574
Epoch 1290, training loss: 844.737060546875 = 0.8404667973518372 + 100.0 * 8.438965797424316
Epoch 1290, val loss: 0.8518550395965576
Epoch 1300, training loss: 844.8596801757812 = 0.8375173807144165 + 100.0 * 8.440221786499023
Epoch 1300, val loss: 0.8491114974021912
Epoch 1310, training loss: 844.763427734375 = 0.834561288356781 + 100.0 * 8.439289093017578
Epoch 1310, val loss: 0.8463120460510254
Epoch 1320, training loss: 844.7437744140625 = 0.8315111994743347 + 100.0 * 8.439123153686523
Epoch 1320, val loss: 0.8435074687004089
Epoch 1330, training loss: 844.5390625 = 0.8285828828811646 + 100.0 * 8.437104225158691
Epoch 1330, val loss: 0.8406811356544495
Epoch 1340, training loss: 844.4468383789062 = 0.8256797194480896 + 100.0 * 8.436211585998535
Epoch 1340, val loss: 0.8379855751991272
Epoch 1350, training loss: 844.391357421875 = 0.822803795337677 + 100.0 * 8.435685157775879
Epoch 1350, val loss: 0.8353398442268372
Epoch 1360, training loss: 844.6724243164062 = 0.8198714852333069 + 100.0 * 8.438525199890137
Epoch 1360, val loss: 0.8326298594474792
Epoch 1370, training loss: 844.5172119140625 = 0.8167911171913147 + 100.0 * 8.437004089355469
Epoch 1370, val loss: 0.8295911550521851
Epoch 1380, training loss: 844.2449951171875 = 0.8137893080711365 + 100.0 * 8.434311866760254
Epoch 1380, val loss: 0.8269127011299133
Epoch 1390, training loss: 844.1713256835938 = 0.8109637498855591 + 100.0 * 8.433603286743164
Epoch 1390, val loss: 0.8243008852005005
Epoch 1400, training loss: 844.1331176757812 = 0.8081448674201965 + 100.0 * 8.433249473571777
Epoch 1400, val loss: 0.8216372728347778
Epoch 1410, training loss: 844.3286743164062 = 0.8052439093589783 + 100.0 * 8.435234069824219
Epoch 1410, val loss: 0.8189660310745239
Epoch 1420, training loss: 844.1849975585938 = 0.8021852970123291 + 100.0 * 8.433828353881836
Epoch 1420, val loss: 0.8160632848739624
Epoch 1430, training loss: 844.0654907226562 = 0.7992197275161743 + 100.0 * 8.432662963867188
Epoch 1430, val loss: 0.8133598566055298
Epoch 1440, training loss: 843.9182739257812 = 0.796367883682251 + 100.0 * 8.431219100952148
Epoch 1440, val loss: 0.8106971979141235
Epoch 1450, training loss: 843.9146118164062 = 0.7935672402381897 + 100.0 * 8.4312105178833
Epoch 1450, val loss: 0.8080568313598633
Epoch 1460, training loss: 843.9489135742188 = 0.7906413078308105 + 100.0 * 8.4315824508667
Epoch 1460, val loss: 0.8054240345954895
Epoch 1470, training loss: 844.0819702148438 = 0.7876601815223694 + 100.0 * 8.432943344116211
Epoch 1470, val loss: 0.8026015758514404
Epoch 1480, training loss: 843.8116455078125 = 0.7847601175308228 + 100.0 * 8.430268287658691
Epoch 1480, val loss: 0.7998979091644287
Epoch 1490, training loss: 843.6851196289062 = 0.7819555401802063 + 100.0 * 8.429031372070312
Epoch 1490, val loss: 0.7973438501358032
Epoch 1500, training loss: 843.611328125 = 0.7791984677314758 + 100.0 * 8.428321838378906
Epoch 1500, val loss: 0.794786274433136
Epoch 1510, training loss: 843.6144409179688 = 0.776420533657074 + 100.0 * 8.428380012512207
Epoch 1510, val loss: 0.7922484278678894
Epoch 1520, training loss: 843.5950317382812 = 0.7734336853027344 + 100.0 * 8.428215980529785
Epoch 1520, val loss: 0.7894037365913391
Epoch 1530, training loss: 843.58642578125 = 0.7705706357955933 + 100.0 * 8.4281587600708
Epoch 1530, val loss: 0.7868556976318359
Epoch 1540, training loss: 843.4627685546875 = 0.7678285837173462 + 100.0 * 8.426949501037598
Epoch 1540, val loss: 0.7842552065849304
Epoch 1550, training loss: 843.4141235351562 = 0.765088677406311 + 100.0 * 8.426490783691406
Epoch 1550, val loss: 0.7817214727401733
Epoch 1560, training loss: 843.3744506835938 = 0.7623756527900696 + 100.0 * 8.42612075805664
Epoch 1560, val loss: 0.7792477607727051
Epoch 1570, training loss: 843.4137573242188 = 0.7596240043640137 + 100.0 * 8.426541328430176
Epoch 1570, val loss: 0.7767243385314941
Epoch 1580, training loss: 843.4387817382812 = 0.7568702101707458 + 100.0 * 8.42681884765625
Epoch 1580, val loss: 0.7741290926933289
Epoch 1590, training loss: 843.3294677734375 = 0.7540519833564758 + 100.0 * 8.42575454711914
Epoch 1590, val loss: 0.7716434001922607
Epoch 1600, training loss: 843.1722412109375 = 0.7514174580574036 + 100.0 * 8.424208641052246
Epoch 1600, val loss: 0.7691093683242798
Epoch 1610, training loss: 843.1405639648438 = 0.748798668384552 + 100.0 * 8.423917770385742
Epoch 1610, val loss: 0.7667613625526428
Epoch 1620, training loss: 843.4786376953125 = 0.7460675239562988 + 100.0 * 8.427325248718262
Epoch 1620, val loss: 0.7642143368721008
Epoch 1630, training loss: 843.0838623046875 = 0.743302047252655 + 100.0 * 8.423405647277832
Epoch 1630, val loss: 0.7616523504257202
Epoch 1640, training loss: 842.9915161132812 = 0.7406854629516602 + 100.0 * 8.422508239746094
Epoch 1640, val loss: 0.7592611908912659
Epoch 1650, training loss: 843.049560546875 = 0.7381281852722168 + 100.0 * 8.423114776611328
Epoch 1650, val loss: 0.7568939924240112
Epoch 1660, training loss: 842.949462890625 = 0.7354715466499329 + 100.0 * 8.422140121459961
Epoch 1660, val loss: 0.7544498443603516
Epoch 1670, training loss: 842.9298706054688 = 0.7329370975494385 + 100.0 * 8.421969413757324
Epoch 1670, val loss: 0.752087414264679
Epoch 1680, training loss: 843.1206665039062 = 0.7303191423416138 + 100.0 * 8.423903465270996
Epoch 1680, val loss: 0.7496432662010193
Epoch 1690, training loss: 842.8765258789062 = 0.7276642322540283 + 100.0 * 8.421488761901855
Epoch 1690, val loss: 0.7472963929176331
Epoch 1700, training loss: 842.7734985351562 = 0.7252187728881836 + 100.0 * 8.420482635498047
Epoch 1700, val loss: 0.7450242638587952
Epoch 1710, training loss: 842.712890625 = 0.722764253616333 + 100.0 * 8.419900894165039
Epoch 1710, val loss: 0.7427769303321838
Epoch 1720, training loss: 842.6937255859375 = 0.7203196287155151 + 100.0 * 8.419734001159668
Epoch 1720, val loss: 0.740546703338623
Epoch 1730, training loss: 842.8870239257812 = 0.717856228351593 + 100.0 * 8.42169189453125
Epoch 1730, val loss: 0.7381851673126221
Epoch 1740, training loss: 842.8292846679688 = 0.7151890993118286 + 100.0 * 8.421140670776367
Epoch 1740, val loss: 0.7357779145240784
Epoch 1750, training loss: 842.6867065429688 = 0.712638258934021 + 100.0 * 8.419740676879883
Epoch 1750, val loss: 0.7334762215614319
Epoch 1760, training loss: 842.5451049804688 = 0.7102855443954468 + 100.0 * 8.41834831237793
Epoch 1760, val loss: 0.73128741979599
Epoch 1770, training loss: 842.5153198242188 = 0.7079432606697083 + 100.0 * 8.418073654174805
Epoch 1770, val loss: 0.7291959524154663
Epoch 1780, training loss: 842.6058959960938 = 0.705610454082489 + 100.0 * 8.419002532958984
Epoch 1780, val loss: 0.72702556848526
Epoch 1790, training loss: 842.5176391601562 = 0.7031283378601074 + 100.0 * 8.418145179748535
Epoch 1790, val loss: 0.7247191071510315
Epoch 1800, training loss: 842.4484252929688 = 0.7007324695587158 + 100.0 * 8.417476654052734
Epoch 1800, val loss: 0.7225315570831299
Epoch 1810, training loss: 842.3978271484375 = 0.6984459161758423 + 100.0 * 8.416994094848633
Epoch 1810, val loss: 0.72046959400177
Epoch 1820, training loss: 842.4945678710938 = 0.6961449384689331 + 100.0 * 8.417984008789062
Epoch 1820, val loss: 0.7183507084846497
Epoch 1830, training loss: 842.43310546875 = 0.6937037110328674 + 100.0 * 8.417393684387207
Epoch 1830, val loss: 0.7159931659698486
Epoch 1840, training loss: 842.394775390625 = 0.6912965178489685 + 100.0 * 8.417035102844238
Epoch 1840, val loss: 0.7139392495155334
Epoch 1850, training loss: 842.2734985351562 = 0.6890660524368286 + 100.0 * 8.415843963623047
Epoch 1850, val loss: 0.7118536829948425
Epoch 1860, training loss: 842.2019653320312 = 0.6869090795516968 + 100.0 * 8.41515064239502
Epoch 1860, val loss: 0.7098577618598938
Epoch 1870, training loss: 842.174560546875 = 0.6847384572029114 + 100.0 * 8.414897918701172
Epoch 1870, val loss: 0.7079041004180908
Epoch 1880, training loss: 842.1502075195312 = 0.6825958490371704 + 100.0 * 8.41467571258545
Epoch 1880, val loss: 0.7059286236763
Epoch 1890, training loss: 842.5135498046875 = 0.6803812980651855 + 100.0 * 8.41833209991455
Epoch 1890, val loss: 0.7040051817893982
Epoch 1900, training loss: 842.4807739257812 = 0.6778929233551025 + 100.0 * 8.418028831481934
Epoch 1900, val loss: 0.7015529870986938
Epoch 1910, training loss: 842.0774536132812 = 0.6756815910339355 + 100.0 * 8.414017677307129
Epoch 1910, val loss: 0.699520468711853
Epoch 1920, training loss: 842.0794677734375 = 0.6735625267028809 + 100.0 * 8.414058685302734
Epoch 1920, val loss: 0.6977407932281494
Epoch 1930, training loss: 842.1843872070312 = 0.6715004444122314 + 100.0 * 8.415128707885742
Epoch 1930, val loss: 0.6958035826683044
Epoch 1940, training loss: 841.984619140625 = 0.669231116771698 + 100.0 * 8.413153648376465
Epoch 1940, val loss: 0.6937105655670166
Epoch 1950, training loss: 841.9393310546875 = 0.6671581268310547 + 100.0 * 8.412721633911133
Epoch 1950, val loss: 0.691786527633667
Epoch 1960, training loss: 841.90625 = 0.6651381254196167 + 100.0 * 8.412410736083984
Epoch 1960, val loss: 0.6899975538253784
Epoch 1970, training loss: 841.8833618164062 = 0.6631561517715454 + 100.0 * 8.412201881408691
Epoch 1970, val loss: 0.6881933212280273
Epoch 1980, training loss: 841.8578491210938 = 0.6611579656600952 + 100.0 * 8.411967277526855
Epoch 1980, val loss: 0.6863604784011841
Epoch 1990, training loss: 842.0580444335938 = 0.6591402888298035 + 100.0 * 8.413989067077637
Epoch 1990, val loss: 0.684419572353363
Epoch 2000, training loss: 841.9656372070312 = 0.6568591594696045 + 100.0 * 8.413087844848633
Epoch 2000, val loss: 0.6825333833694458
Epoch 2010, training loss: 841.8585815429688 = 0.6547913551330566 + 100.0 * 8.41203784942627
Epoch 2010, val loss: 0.6804998517036438
Epoch 2020, training loss: 841.7680053710938 = 0.6528446078300476 + 100.0 * 8.411151885986328
Epoch 2020, val loss: 0.6787801384925842
Epoch 2030, training loss: 841.72509765625 = 0.6509390473365784 + 100.0 * 8.410741806030273
Epoch 2030, val loss: 0.6771097779273987
Epoch 2040, training loss: 841.70751953125 = 0.6490257382392883 + 100.0 * 8.410584449768066
Epoch 2040, val loss: 0.6753548383712769
Epoch 2050, training loss: 842.0819702148438 = 0.6470550298690796 + 100.0 * 8.414349555969238
Epoch 2050, val loss: 0.6734122037887573
Epoch 2060, training loss: 841.849609375 = 0.6449218988418579 + 100.0 * 8.412047386169434
Epoch 2060, val loss: 0.6717036962509155
Epoch 2070, training loss: 841.6998901367188 = 0.6429435610771179 + 100.0 * 8.410569190979004
Epoch 2070, val loss: 0.6697884202003479
Epoch 2080, training loss: 841.6053466796875 = 0.6410647034645081 + 100.0 * 8.409643173217773
Epoch 2080, val loss: 0.668207049369812
Epoch 2090, training loss: 841.591064453125 = 0.6392175555229187 + 100.0 * 8.409518241882324
Epoch 2090, val loss: 0.6665371060371399
Epoch 2100, training loss: 841.729736328125 = 0.6373198628425598 + 100.0 * 8.410923957824707
Epoch 2100, val loss: 0.664879560470581
Epoch 2110, training loss: 841.5813598632812 = 0.6353817582130432 + 100.0 * 8.409460067749023
Epoch 2110, val loss: 0.6630265712738037
Epoch 2120, training loss: 841.6972045898438 = 0.6334607601165771 + 100.0 * 8.410636901855469
Epoch 2120, val loss: 0.6612489819526672
Epoch 2130, training loss: 841.4940795898438 = 0.6315451860427856 + 100.0 * 8.408625602722168
Epoch 2130, val loss: 0.6596100330352783
Epoch 2140, training loss: 841.4700317382812 = 0.6297697424888611 + 100.0 * 8.408402442932129
Epoch 2140, val loss: 0.6580236554145813
Epoch 2150, training loss: 841.4415283203125 = 0.6280295848846436 + 100.0 * 8.408134460449219
Epoch 2150, val loss: 0.656449019908905
Epoch 2160, training loss: 841.4387817382812 = 0.6262894868850708 + 100.0 * 8.408124923706055
Epoch 2160, val loss: 0.654869556427002
Epoch 2170, training loss: 841.5353393554688 = 0.624514639377594 + 100.0 * 8.40910816192627
Epoch 2170, val loss: 0.6532906889915466
Epoch 2180, training loss: 841.4541625976562 = 0.6226387619972229 + 100.0 * 8.408315658569336
Epoch 2180, val loss: 0.6515966057777405
Epoch 2190, training loss: 841.564453125 = 0.620857298374176 + 100.0 * 8.409436225891113
Epoch 2190, val loss: 0.6500351428985596
Epoch 2200, training loss: 841.3963012695312 = 0.6190352439880371 + 100.0 * 8.4077730178833
Epoch 2200, val loss: 0.6483776569366455
Epoch 2210, training loss: 841.3366088867188 = 0.6173030138015747 + 100.0 * 8.407193183898926
Epoch 2210, val loss: 0.646834671497345
Epoch 2220, training loss: 841.3217163085938 = 0.6156325936317444 + 100.0 * 8.407060623168945
Epoch 2220, val loss: 0.6453731060028076
Epoch 2230, training loss: 841.6130981445312 = 0.6138923764228821 + 100.0 * 8.409992218017578
Epoch 2230, val loss: 0.6437699198722839
Epoch 2240, training loss: 841.364990234375 = 0.6121402978897095 + 100.0 * 8.4075288772583
Epoch 2240, val loss: 0.6421772241592407
Epoch 2250, training loss: 841.2610473632812 = 0.6104846596717834 + 100.0 * 8.406505584716797
Epoch 2250, val loss: 0.6407576203346252
Epoch 2260, training loss: 841.2223510742188 = 0.6088696122169495 + 100.0 * 8.406134605407715
Epoch 2260, val loss: 0.6392881274223328
Epoch 2270, training loss: 841.3214721679688 = 0.6072321534156799 + 100.0 * 8.407142639160156
Epoch 2270, val loss: 0.6377836465835571
Epoch 2280, training loss: 841.23046875 = 0.605465829372406 + 100.0 * 8.40625
Epoch 2280, val loss: 0.6362003684043884
Epoch 2290, training loss: 841.1786499023438 = 0.6038200855255127 + 100.0 * 8.40574836730957
Epoch 2290, val loss: 0.6347585320472717
Epoch 2300, training loss: 841.2133178710938 = 0.6022338271141052 + 100.0 * 8.406110763549805
Epoch 2300, val loss: 0.6333489418029785
Epoch 2310, training loss: 841.2533569335938 = 0.6006036996841431 + 100.0 * 8.406527519226074
Epoch 2310, val loss: 0.6318932175636292
Epoch 2320, training loss: 841.2166137695312 = 0.5989477634429932 + 100.0 * 8.406176567077637
Epoch 2320, val loss: 0.6305117011070251
Epoch 2330, training loss: 841.0787963867188 = 0.5973960161209106 + 100.0 * 8.404813766479492
Epoch 2330, val loss: 0.6290577054023743
Epoch 2340, training loss: 841.0711669921875 = 0.5958824753761292 + 100.0 * 8.404752731323242
Epoch 2340, val loss: 0.6277087926864624
Epoch 2350, training loss: 841.0830688476562 = 0.5943688154220581 + 100.0 * 8.404887199401855
Epoch 2350, val loss: 0.6263640522956848
Epoch 2360, training loss: 841.2252197265625 = 0.5927969217300415 + 100.0 * 8.40632438659668
Epoch 2360, val loss: 0.6249873042106628
Epoch 2370, training loss: 841.2677001953125 = 0.5912097692489624 + 100.0 * 8.40676498413086
Epoch 2370, val loss: 0.6237534880638123
Epoch 2380, training loss: 841.0357055664062 = 0.5896262526512146 + 100.0 * 8.404460906982422
Epoch 2380, val loss: 0.6221204996109009
Epoch 2390, training loss: 840.997314453125 = 0.5881568789482117 + 100.0 * 8.404091835021973
Epoch 2390, val loss: 0.6209830641746521
Epoch 2400, training loss: 840.954345703125 = 0.5867429971694946 + 100.0 * 8.40367603302002
Epoch 2400, val loss: 0.6196894645690918
Epoch 2410, training loss: 840.9356689453125 = 0.5853235721588135 + 100.0 * 8.40350341796875
Epoch 2410, val loss: 0.6185042262077332
Epoch 2420, training loss: 841.1632080078125 = 0.5838756561279297 + 100.0 * 8.405793190002441
Epoch 2420, val loss: 0.6172942519187927
Epoch 2430, training loss: 840.9822387695312 = 0.5823072195053101 + 100.0 * 8.403999328613281
Epoch 2430, val loss: 0.6156829595565796
Epoch 2440, training loss: 840.9314575195312 = 0.5808334946632385 + 100.0 * 8.4035062789917
Epoch 2440, val loss: 0.6145631074905396
Epoch 2450, training loss: 840.9161987304688 = 0.5794762969017029 + 100.0 * 8.403367042541504
Epoch 2450, val loss: 0.6133122444152832
Epoch 2460, training loss: 841.0035400390625 = 0.5780733823776245 + 100.0 * 8.404254913330078
Epoch 2460, val loss: 0.6121567487716675
Epoch 2470, training loss: 840.8845825195312 = 0.5766318440437317 + 100.0 * 8.40307903289795
Epoch 2470, val loss: 0.6107690930366516
Epoch 2480, training loss: 840.8196411132812 = 0.5752658247947693 + 100.0 * 8.402443885803223
Epoch 2480, val loss: 0.6096304655075073
Epoch 2490, training loss: 840.8359375 = 0.5739337801933289 + 100.0 * 8.402620315551758
Epoch 2490, val loss: 0.608526885509491
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7747336377473364
0.8162718249655873
=== training gcn model ===
Epoch 0, training loss: 1059.315673828125 = 1.0907360315322876 + 100.0 * 10.582249641418457
Epoch 0, val loss: 1.0914033651351929
Epoch 10, training loss: 1059.273681640625 = 1.0869293212890625 + 100.0 * 10.581868171691895
Epoch 10, val loss: 1.0875612497329712
Epoch 20, training loss: 1059.128173828125 = 1.0826612710952759 + 100.0 * 10.58045482635498
Epoch 20, val loss: 1.0832701921463013
Epoch 30, training loss: 1058.537353515625 = 1.077851414680481 + 100.0 * 10.574594497680664
Epoch 30, val loss: 1.0784207582473755
Epoch 40, training loss: 1056.2110595703125 = 1.0724622011184692 + 100.0 * 10.551385879516602
Epoch 40, val loss: 1.0729889869689941
Epoch 50, training loss: 1048.7850341796875 = 1.066542387008667 + 100.0 * 10.477185249328613
Epoch 50, val loss: 1.0670753717422485
Epoch 60, training loss: 1027.977783203125 = 1.060620903968811 + 100.0 * 10.269171714782715
Epoch 60, val loss: 1.0612208843231201
Epoch 70, training loss: 983.0521240234375 = 1.054463505744934 + 100.0 * 9.819976806640625
Epoch 70, val loss: 1.0551486015319824
Epoch 80, training loss: 946.49072265625 = 1.0495543479919434 + 100.0 * 9.454411506652832
Epoch 80, val loss: 1.0505433082580566
Epoch 90, training loss: 933.0081787109375 = 1.0465433597564697 + 100.0 * 9.319616317749023
Epoch 90, val loss: 1.0477794408798218
Epoch 100, training loss: 927.478271484375 = 1.0445014238357544 + 100.0 * 9.264337539672852
Epoch 100, val loss: 1.0459091663360596
Epoch 110, training loss: 923.4307861328125 = 1.0429353713989258 + 100.0 * 9.223878860473633
Epoch 110, val loss: 1.044437050819397
Epoch 120, training loss: 917.8367309570312 = 1.0417413711547852 + 100.0 * 9.167949676513672
Epoch 120, val loss: 1.0432393550872803
Epoch 130, training loss: 909.9134521484375 = 1.0410455465316772 + 100.0 * 9.088724136352539
Epoch 130, val loss: 1.0425513982772827
Epoch 140, training loss: 900.206787109375 = 1.0409715175628662 + 100.0 * 8.991658210754395
Epoch 140, val loss: 1.042441725730896
Epoch 150, training loss: 893.859130859375 = 1.0408813953399658 + 100.0 * 8.928182601928711
Epoch 150, val loss: 1.0422192811965942
Epoch 160, training loss: 891.1021728515625 = 1.039893627166748 + 100.0 * 8.900622367858887
Epoch 160, val loss: 1.041081428527832
Epoch 170, training loss: 888.5185546875 = 1.0384098291397095 + 100.0 * 8.874801635742188
Epoch 170, val loss: 1.039576530456543
Epoch 180, training loss: 885.6558227539062 = 1.0373499393463135 + 100.0 * 8.846184730529785
Epoch 180, val loss: 1.0385618209838867
Epoch 190, training loss: 882.0382080078125 = 1.0367624759674072 + 100.0 * 8.810014724731445
Epoch 190, val loss: 1.0380507707595825
Epoch 200, training loss: 878.2037963867188 = 1.036531925201416 + 100.0 * 8.771672248840332
Epoch 200, val loss: 1.0378408432006836
Epoch 210, training loss: 875.05712890625 = 1.0362472534179688 + 100.0 * 8.740208625793457
Epoch 210, val loss: 1.0375337600708008
Epoch 220, training loss: 872.8641967773438 = 1.0356389284133911 + 100.0 * 8.71828556060791
Epoch 220, val loss: 1.0369068384170532
Epoch 230, training loss: 871.1390991210938 = 1.0348249673843384 + 100.0 * 8.701042175292969
Epoch 230, val loss: 1.0360924005508423
Epoch 240, training loss: 869.5030517578125 = 1.0340176820755005 + 100.0 * 8.684690475463867
Epoch 240, val loss: 1.035301923751831
Epoch 250, training loss: 868.0460205078125 = 1.033294677734375 + 100.0 * 8.670126914978027
Epoch 250, val loss: 1.0345934629440308
Epoch 260, training loss: 866.4029541015625 = 1.032568335533142 + 100.0 * 8.653703689575195
Epoch 260, val loss: 1.033909559249878
Epoch 270, training loss: 864.9863891601562 = 1.0318595170974731 + 100.0 * 8.639545440673828
Epoch 270, val loss: 1.033214807510376
Epoch 280, training loss: 863.6830444335938 = 1.0311673879623413 + 100.0 * 8.626518249511719
Epoch 280, val loss: 1.0325380563735962
Epoch 290, training loss: 862.4758911132812 = 1.0304670333862305 + 100.0 * 8.61445426940918
Epoch 290, val loss: 1.0318678617477417
Epoch 300, training loss: 861.434814453125 = 1.029740333557129 + 100.0 * 8.604050636291504
Epoch 300, val loss: 1.0311521291732788
Epoch 310, training loss: 860.52001953125 = 1.0289455652236938 + 100.0 * 8.594910621643066
Epoch 310, val loss: 1.0303813219070435
Epoch 320, training loss: 859.6895141601562 = 1.0281556844711304 + 100.0 * 8.586613655090332
Epoch 320, val loss: 1.0296127796173096
Epoch 330, training loss: 859.0082397460938 = 1.0273407697677612 + 100.0 * 8.579809188842773
Epoch 330, val loss: 1.0288265943527222
Epoch 340, training loss: 858.4111328125 = 1.026428461074829 + 100.0 * 8.573846817016602
Epoch 340, val loss: 1.027940034866333
Epoch 350, training loss: 857.7947387695312 = 1.025445580482483 + 100.0 * 8.567692756652832
Epoch 350, val loss: 1.026987075805664
Epoch 360, training loss: 857.2587890625 = 1.0244256258010864 + 100.0 * 8.56234359741211
Epoch 360, val loss: 1.0260024070739746
Epoch 370, training loss: 856.9495239257812 = 1.0233787298202515 + 100.0 * 8.559261322021484
Epoch 370, val loss: 1.0249677896499634
Epoch 380, training loss: 856.3678588867188 = 1.0222145318984985 + 100.0 * 8.55345630645752
Epoch 380, val loss: 1.0238653421401978
Epoch 390, training loss: 855.9146728515625 = 1.0210390090942383 + 100.0 * 8.548935890197754
Epoch 390, val loss: 1.022727370262146
Epoch 400, training loss: 855.4790649414062 = 1.0198801755905151 + 100.0 * 8.544591903686523
Epoch 400, val loss: 1.0216073989868164
Epoch 410, training loss: 855.1345825195312 = 1.018729567527771 + 100.0 * 8.541158676147461
Epoch 410, val loss: 1.0204758644104004
Epoch 420, training loss: 854.8675537109375 = 1.0174590349197388 + 100.0 * 8.538500785827637
Epoch 420, val loss: 1.0192532539367676
Epoch 430, training loss: 854.4263305664062 = 1.0161751508712769 + 100.0 * 8.534101486206055
Epoch 430, val loss: 1.0179877281188965
Epoch 440, training loss: 854.0460205078125 = 1.0149074792861938 + 100.0 * 8.530311584472656
Epoch 440, val loss: 1.0167516469955444
Epoch 450, training loss: 853.845458984375 = 1.0136150121688843 + 100.0 * 8.528318405151367
Epoch 450, val loss: 1.0155035257339478
Epoch 460, training loss: 853.5562744140625 = 1.0123038291931152 + 100.0 * 8.525439262390137
Epoch 460, val loss: 1.0141645669937134
Epoch 470, training loss: 853.0941162109375 = 1.010945200920105 + 100.0 * 8.520832061767578
Epoch 470, val loss: 1.0128662586212158
Epoch 480, training loss: 852.7222290039062 = 1.0096287727355957 + 100.0 * 8.517126083374023
Epoch 480, val loss: 1.0115573406219482
Epoch 490, training loss: 852.3822631835938 = 1.0082930326461792 + 100.0 * 8.513739585876465
Epoch 490, val loss: 1.010256290435791
Epoch 500, training loss: 852.1571044921875 = 1.0069324970245361 + 100.0 * 8.51150131225586
Epoch 500, val loss: 1.008938193321228
Epoch 510, training loss: 852.334716796875 = 1.0055348873138428 + 100.0 * 8.513291358947754
Epoch 510, val loss: 1.0074775218963623
Epoch 520, training loss: 851.5599365234375 = 1.003974437713623 + 100.0 * 8.505559921264648
Epoch 520, val loss: 1.0059789419174194
Epoch 530, training loss: 851.2994995117188 = 1.0025091171264648 + 100.0 * 8.502969741821289
Epoch 530, val loss: 1.0045541524887085
Epoch 540, training loss: 851.0430297851562 = 1.0010294914245605 + 100.0 * 8.500419616699219
Epoch 540, val loss: 1.003105640411377
Epoch 550, training loss: 850.8136596679688 = 0.9995000958442688 + 100.0 * 8.498141288757324
Epoch 550, val loss: 1.0016065835952759
Epoch 560, training loss: 850.6013793945312 = 0.9979070425033569 + 100.0 * 8.496034622192383
Epoch 560, val loss: 1.0000452995300293
Epoch 570, training loss: 850.836669921875 = 0.9962348341941833 + 100.0 * 8.498404502868652
Epoch 570, val loss: 0.9984381794929504
Epoch 580, training loss: 850.32421875 = 0.9944545030593872 + 100.0 * 8.493297576904297
Epoch 580, val loss: 0.9966737627983093
Epoch 590, training loss: 850.1533813476562 = 0.9927195906639099 + 100.0 * 8.491606712341309
Epoch 590, val loss: 0.9949383735656738
Epoch 600, training loss: 849.9852905273438 = 0.99094557762146 + 100.0 * 8.489943504333496
Epoch 600, val loss: 0.9931749701499939
Epoch 610, training loss: 849.7788696289062 = 0.9890891909599304 + 100.0 * 8.487897872924805
Epoch 610, val loss: 0.9913613200187683
Epoch 620, training loss: 849.634033203125 = 0.9872007966041565 + 100.0 * 8.486468315124512
Epoch 620, val loss: 0.9895139336585999
Epoch 630, training loss: 849.8326416015625 = 0.9852433800697327 + 100.0 * 8.488473892211914
Epoch 630, val loss: 0.9876018166542053
Epoch 640, training loss: 849.4916381835938 = 0.9832028150558472 + 100.0 * 8.485084533691406
Epoch 640, val loss: 0.9855641722679138
Epoch 650, training loss: 849.297119140625 = 0.9811848998069763 + 100.0 * 8.483159065246582
Epoch 650, val loss: 0.9835991263389587
Epoch 660, training loss: 849.1464233398438 = 0.9791628122329712 + 100.0 * 8.481672286987305
Epoch 660, val loss: 0.9816291928291321
Epoch 670, training loss: 849.1922607421875 = 0.9770665764808655 + 100.0 * 8.482151985168457
Epoch 670, val loss: 0.9796141386032104
Epoch 680, training loss: 849.0274658203125 = 0.974876344203949 + 100.0 * 8.480525970458984
Epoch 680, val loss: 0.9773815870285034
Epoch 690, training loss: 848.8612670898438 = 0.9726897478103638 + 100.0 * 8.478885650634766
Epoch 690, val loss: 0.9752761125564575
Epoch 700, training loss: 848.695556640625 = 0.9705165028572083 + 100.0 * 8.477250099182129
Epoch 700, val loss: 0.9731491208076477
Epoch 710, training loss: 848.5939331054688 = 0.9683048129081726 + 100.0 * 8.476256370544434
Epoch 710, val loss: 0.9709834456443787
Epoch 720, training loss: 848.7931518554688 = 0.9660177826881409 + 100.0 * 8.478271484375
Epoch 720, val loss: 0.9687199592590332
Epoch 730, training loss: 848.4556274414062 = 0.9636421799659729 + 100.0 * 8.474920272827148
Epoch 730, val loss: 0.9663958549499512
Epoch 740, training loss: 848.2802124023438 = 0.9613499045372009 + 100.0 * 8.473188400268555
Epoch 740, val loss: 0.9641551375389099
Epoch 750, training loss: 848.1546630859375 = 0.9590256214141846 + 100.0 * 8.471956253051758
Epoch 750, val loss: 0.9618926048278809
Epoch 760, training loss: 848.171142578125 = 0.9566865563392639 + 100.0 * 8.472145080566406
Epoch 760, val loss: 0.9595673680305481
Epoch 770, training loss: 848.0285034179688 = 0.9541334509849548 + 100.0 * 8.470743179321289
Epoch 770, val loss: 0.9571498036384583
Epoch 780, training loss: 847.888916015625 = 0.9517384767532349 + 100.0 * 8.469371795654297
Epoch 780, val loss: 0.9547416567802429
Epoch 790, training loss: 847.7637939453125 = 0.9493145942687988 + 100.0 * 8.468144416809082
Epoch 790, val loss: 0.9523687362670898
Epoch 800, training loss: 847.8023071289062 = 0.9468543529510498 + 100.0 * 8.468554496765137
Epoch 800, val loss: 0.9499785900115967
Epoch 810, training loss: 847.6183471679688 = 0.9442324638366699 + 100.0 * 8.466741561889648
Epoch 810, val loss: 0.9474076628684998
Epoch 820, training loss: 847.4944458007812 = 0.9417069554328918 + 100.0 * 8.465527534484863
Epoch 820, val loss: 0.9449403882026672
Epoch 830, training loss: 847.3858032226562 = 0.9391864538192749 + 100.0 * 8.464466094970703
Epoch 830, val loss: 0.9424593448638916
Epoch 840, training loss: 847.294189453125 = 0.9366133809089661 + 100.0 * 8.463576316833496
Epoch 840, val loss: 0.9399747252464294
Epoch 850, training loss: 847.55859375 = 0.9340283870697021 + 100.0 * 8.466245651245117
Epoch 850, val loss: 0.9374144077301025
Epoch 860, training loss: 847.2033081054688 = 0.9312071800231934 + 100.0 * 8.46272087097168
Epoch 860, val loss: 0.9346956014633179
Epoch 870, training loss: 847.0628662109375 = 0.9285906553268433 + 100.0 * 8.461342811584473
Epoch 870, val loss: 0.9321139454841614
Epoch 880, training loss: 846.940673828125 = 0.9259307980537415 + 100.0 * 8.460147857666016
Epoch 880, val loss: 0.9295290112495422
Epoch 890, training loss: 847.1629638671875 = 0.9232341051101685 + 100.0 * 8.462397575378418
Epoch 890, val loss: 0.9268781542778015
Epoch 900, training loss: 846.9078979492188 = 0.9203990697860718 + 100.0 * 8.459875106811523
Epoch 900, val loss: 0.9241361021995544
Epoch 910, training loss: 846.7239990234375 = 0.9176438450813293 + 100.0 * 8.458063125610352
Epoch 910, val loss: 0.9214639067649841
Epoch 920, training loss: 846.6124877929688 = 0.9149138331413269 + 100.0 * 8.456975936889648
Epoch 920, val loss: 0.9188120365142822
Epoch 930, training loss: 846.6182861328125 = 0.9121186137199402 + 100.0 * 8.457061767578125
Epoch 930, val loss: 0.9161349534988403
Epoch 940, training loss: 846.5115356445312 = 0.9091460108757019 + 100.0 * 8.456024169921875
Epoch 940, val loss: 0.9131693243980408
Epoch 950, training loss: 846.4379272460938 = 0.90621018409729 + 100.0 * 8.455317497253418
Epoch 950, val loss: 0.9104011058807373
Epoch 960, training loss: 846.3191528320312 = 0.9033628106117249 + 100.0 * 8.454157829284668
Epoch 960, val loss: 0.9076161980628967
Epoch 970, training loss: 846.2481079101562 = 0.9005191922187805 + 100.0 * 8.453475952148438
Epoch 970, val loss: 0.9048546552658081
Epoch 980, training loss: 846.1728515625 = 0.8976454734802246 + 100.0 * 8.452752113342285
Epoch 980, val loss: 0.9020682573318481
Epoch 990, training loss: 846.6453247070312 = 0.8947308659553528 + 100.0 * 8.45750617980957
Epoch 990, val loss: 0.8991164565086365
Epoch 1000, training loss: 846.0682983398438 = 0.891437828540802 + 100.0 * 8.45176887512207
Epoch 1000, val loss: 0.8960721492767334
Epoch 1010, training loss: 845.9925537109375 = 0.8884397745132446 + 100.0 * 8.451041221618652
Epoch 1010, val loss: 0.893212080001831
Epoch 1020, training loss: 845.9237670898438 = 0.8854614496231079 + 100.0 * 8.450383186340332
Epoch 1020, val loss: 0.8903520107269287
Epoch 1030, training loss: 845.8740234375 = 0.8825014233589172 + 100.0 * 8.449914932250977
Epoch 1030, val loss: 0.8875064849853516
Epoch 1040, training loss: 846.2229614257812 = 0.8794025182723999 + 100.0 * 8.453435897827148
Epoch 1040, val loss: 0.8845256567001343
Epoch 1050, training loss: 845.8116455078125 = 0.8761865496635437 + 100.0 * 8.449355125427246
Epoch 1050, val loss: 0.8813976645469666
Epoch 1060, training loss: 845.6620483398438 = 0.8731290102005005 + 100.0 * 8.44788932800293
Epoch 1060, val loss: 0.878478467464447
Epoch 1070, training loss: 845.6106567382812 = 0.8700801134109497 + 100.0 * 8.447405815124512
Epoch 1070, val loss: 0.8755882978439331
Epoch 1080, training loss: 845.556884765625 = 0.8670506477355957 + 100.0 * 8.446898460388184
Epoch 1080, val loss: 0.872670590877533
Epoch 1090, training loss: 845.9451293945312 = 0.8639013767242432 + 100.0 * 8.450812339782715
Epoch 1090, val loss: 0.8697000741958618
Epoch 1100, training loss: 845.5988159179688 = 0.8605768084526062 + 100.0 * 8.447381973266602
Epoch 1100, val loss: 0.8664290308952332
Epoch 1110, training loss: 845.3934326171875 = 0.8574165105819702 + 100.0 * 8.44536018371582
Epoch 1110, val loss: 0.8634281754493713
Epoch 1120, training loss: 845.3451538085938 = 0.8542861342430115 + 100.0 * 8.444908142089844
Epoch 1120, val loss: 0.860481321811676
Epoch 1130, training loss: 845.340576171875 = 0.8511818051338196 + 100.0 * 8.444893836975098
Epoch 1130, val loss: 0.8575029373168945
Epoch 1140, training loss: 845.343017578125 = 0.8478819131851196 + 100.0 * 8.444951057434082
Epoch 1140, val loss: 0.8543177843093872
Epoch 1150, training loss: 845.19091796875 = 0.8446498513221741 + 100.0 * 8.443462371826172
Epoch 1150, val loss: 0.8513243198394775
Epoch 1160, training loss: 845.114013671875 = 0.8415166139602661 + 100.0 * 8.44272518157959
Epoch 1160, val loss: 0.8483405709266663
Epoch 1170, training loss: 845.0645751953125 = 0.8383939862251282 + 100.0 * 8.442261695861816
Epoch 1170, val loss: 0.8453554511070251
Epoch 1180, training loss: 845.1566772460938 = 0.8352918028831482 + 100.0 * 8.443214416503906
Epoch 1180, val loss: 0.8424044847488403
Epoch 1190, training loss: 845.0684814453125 = 0.8318196535110474 + 100.0 * 8.442366600036621
Epoch 1190, val loss: 0.8391709923744202
Epoch 1200, training loss: 844.956787109375 = 0.828657329082489 + 100.0 * 8.44128131866455
Epoch 1200, val loss: 0.836120069026947
Epoch 1210, training loss: 844.8676147460938 = 0.8254981637001038 + 100.0 * 8.440421104431152
Epoch 1210, val loss: 0.8332312703132629
Epoch 1220, training loss: 844.8092651367188 = 0.8224113583564758 + 100.0 * 8.439868927001953
Epoch 1220, val loss: 0.8302493095397949
Epoch 1230, training loss: 844.9473876953125 = 0.8192631602287292 + 100.0 * 8.44128131866455
Epoch 1230, val loss: 0.827288031578064
Epoch 1240, training loss: 844.7138061523438 = 0.815904974937439 + 100.0 * 8.438979148864746
Epoch 1240, val loss: 0.8241426944732666
Epoch 1250, training loss: 844.6514282226562 = 0.8127519488334656 + 100.0 * 8.438386917114258
Epoch 1250, val loss: 0.8212082982063293
Epoch 1260, training loss: 844.5949096679688 = 0.8096633553504944 + 100.0 * 8.437851905822754
Epoch 1260, val loss: 0.8182891607284546
Epoch 1270, training loss: 844.7591552734375 = 0.8065670132637024 + 100.0 * 8.439525604248047
Epoch 1270, val loss: 0.8154186010360718
Epoch 1280, training loss: 844.56591796875 = 0.8032102584838867 + 100.0 * 8.437626838684082
Epoch 1280, val loss: 0.8122783899307251
Epoch 1290, training loss: 844.5036010742188 = 0.800133228302002 + 100.0 * 8.437034606933594
Epoch 1290, val loss: 0.8093469738960266
Epoch 1300, training loss: 844.3900146484375 = 0.7970364689826965 + 100.0 * 8.435929298400879
Epoch 1300, val loss: 0.8065057396888733
Epoch 1310, training loss: 844.3363647460938 = 0.7940379977226257 + 100.0 * 8.435422897338867
Epoch 1310, val loss: 0.8037145137786865
Epoch 1320, training loss: 844.5811157226562 = 0.7909591794013977 + 100.0 * 8.437901496887207
Epoch 1320, val loss: 0.8008107542991638
Epoch 1330, training loss: 844.4021606445312 = 0.787803590297699 + 100.0 * 8.43614387512207
Epoch 1330, val loss: 0.797793447971344
Epoch 1340, training loss: 844.3019409179688 = 0.7846373915672302 + 100.0 * 8.435173034667969
Epoch 1340, val loss: 0.7949354648590088
Epoch 1350, training loss: 844.1593627929688 = 0.7815201282501221 + 100.0 * 8.433778762817383
Epoch 1350, val loss: 0.7920464277267456
Epoch 1360, training loss: 844.1065063476562 = 0.7786169648170471 + 100.0 * 8.433279037475586
Epoch 1360, val loss: 0.7893455624580383
Epoch 1370, training loss: 844.0276489257812 = 0.7756495475769043 + 100.0 * 8.432519912719727
Epoch 1370, val loss: 0.7865960001945496
Epoch 1380, training loss: 843.996826171875 = 0.7727075219154358 + 100.0 * 8.432241439819336
Epoch 1380, val loss: 0.7839043736457825
Epoch 1390, training loss: 844.4968872070312 = 0.7695639133453369 + 100.0 * 8.437273025512695
Epoch 1390, val loss: 0.7810530066490173
Epoch 1400, training loss: 843.9523315429688 = 0.7664304971694946 + 100.0 * 8.431859016418457
Epoch 1400, val loss: 0.7780019044876099
Epoch 1410, training loss: 843.8477172851562 = 0.7635277509689331 + 100.0 * 8.430841445922852
Epoch 1410, val loss: 0.7753210663795471
Epoch 1420, training loss: 843.8054809570312 = 0.7606133818626404 + 100.0 * 8.430448532104492
Epoch 1420, val loss: 0.7727030515670776
Epoch 1430, training loss: 843.925048828125 = 0.7577763795852661 + 100.0 * 8.431673049926758
Epoch 1430, val loss: 0.7700532078742981
Epoch 1440, training loss: 843.7105102539062 = 0.754676342010498 + 100.0 * 8.429557800292969
Epoch 1440, val loss: 0.7671623229980469
Epoch 1450, training loss: 843.6956787109375 = 0.7517316937446594 + 100.0 * 8.429439544677734
Epoch 1450, val loss: 0.7645061612129211
Epoch 1460, training loss: 843.6309814453125 = 0.7488943934440613 + 100.0 * 8.428820610046387
Epoch 1460, val loss: 0.7618533372879028
Epoch 1470, training loss: 843.582763671875 = 0.7460635304450989 + 100.0 * 8.428366661071777
Epoch 1470, val loss: 0.7592796087265015
Epoch 1480, training loss: 843.6305541992188 = 0.7432105541229248 + 100.0 * 8.428873062133789
Epoch 1480, val loss: 0.7566253542900085
Epoch 1490, training loss: 843.471923828125 = 0.7403292059898376 + 100.0 * 8.427315711975098
Epoch 1490, val loss: 0.7539923191070557
Epoch 1500, training loss: 843.4602661132812 = 0.7375773787498474 + 100.0 * 8.427227020263672
Epoch 1500, val loss: 0.7514634728431702
Epoch 1510, training loss: 843.4208984375 = 0.7347636222839355 + 100.0 * 8.426861763000488
Epoch 1510, val loss: 0.7488874197006226
Epoch 1520, training loss: 843.6220703125 = 0.7319275736808777 + 100.0 * 8.428901672363281
Epoch 1520, val loss: 0.7462305426597595
Epoch 1530, training loss: 843.5781860351562 = 0.7289698123931885 + 100.0 * 8.428492546081543
Epoch 1530, val loss: 0.7435400485992432
Epoch 1540, training loss: 843.3507690429688 = 0.7259060740470886 + 100.0 * 8.426248550415039
Epoch 1540, val loss: 0.7407272458076477
Epoch 1550, training loss: 843.3309326171875 = 0.7231439352035522 + 100.0 * 8.426077842712402
Epoch 1550, val loss: 0.7382477521896362
Epoch 1560, training loss: 843.1969604492188 = 0.7204972505569458 + 100.0 * 8.424764633178711
Epoch 1560, val loss: 0.7357774972915649
Epoch 1570, training loss: 843.1630859375 = 0.7178791761398315 + 100.0 * 8.42445182800293
Epoch 1570, val loss: 0.7333968877792358
Epoch 1580, training loss: 843.1867065429688 = 0.7152552604675293 + 100.0 * 8.424714088439941
Epoch 1580, val loss: 0.7309411764144897
Epoch 1590, training loss: 843.2236328125 = 0.7123650908470154 + 100.0 * 8.4251127243042
Epoch 1590, val loss: 0.7282965183258057
Epoch 1600, training loss: 843.0877685546875 = 0.7095065116882324 + 100.0 * 8.423782348632812
Epoch 1600, val loss: 0.7256742119789124
Epoch 1610, training loss: 843.046142578125 = 0.7069063782691956 + 100.0 * 8.423392295837402
Epoch 1610, val loss: 0.7234022617340088
Epoch 1620, training loss: 842.9845581054688 = 0.7043430209159851 + 100.0 * 8.422801971435547
Epoch 1620, val loss: 0.7210261821746826
Epoch 1630, training loss: 842.950439453125 = 0.7017937898635864 + 100.0 * 8.422486305236816
Epoch 1630, val loss: 0.7187157869338989
Epoch 1640, training loss: 843.0173950195312 = 0.6992031335830688 + 100.0 * 8.423181533813477
Epoch 1640, val loss: 0.7164500951766968
Epoch 1650, training loss: 843.3082275390625 = 0.6963801980018616 + 100.0 * 8.426118850708008
Epoch 1650, val loss: 0.7137227654457092
Epoch 1660, training loss: 843.1165161132812 = 0.6933355927467346 + 100.0 * 8.42423152923584
Epoch 1660, val loss: 0.710961639881134
Epoch 1670, training loss: 842.9154663085938 = 0.6908853650093079 + 100.0 * 8.422245979309082
Epoch 1670, val loss: 0.7088094353675842
Epoch 1680, training loss: 842.8076171875 = 0.6884834170341492 + 100.0 * 8.421191215515137
Epoch 1680, val loss: 0.7065865993499756
Epoch 1690, training loss: 842.7657470703125 = 0.6861063241958618 + 100.0 * 8.420796394348145
Epoch 1690, val loss: 0.7044036388397217
Epoch 1700, training loss: 842.733154296875 = 0.6837124824523926 + 100.0 * 8.420494079589844
Epoch 1700, val loss: 0.702194094657898
Epoch 1710, training loss: 842.699951171875 = 0.6813085675239563 + 100.0 * 8.420186042785645
Epoch 1710, val loss: 0.7000420689582825
Epoch 1720, training loss: 842.6787109375 = 0.6788975596427917 + 100.0 * 8.419998168945312
Epoch 1720, val loss: 0.6978533267974854
Epoch 1730, training loss: 842.9545288085938 = 0.6764779090881348 + 100.0 * 8.422780990600586
Epoch 1730, val loss: 0.6956236362457275
Epoch 1740, training loss: 842.7755737304688 = 0.673678457736969 + 100.0 * 8.421018600463867
Epoch 1740, val loss: 0.692984402179718
Epoch 1750, training loss: 842.680419921875 = 0.6712481379508972 + 100.0 * 8.42009162902832
Epoch 1750, val loss: 0.6909402012825012
Epoch 1760, training loss: 842.5736083984375 = 0.668921709060669 + 100.0 * 8.419046401977539
Epoch 1760, val loss: 0.6888043880462646
Epoch 1770, training loss: 842.5282592773438 = 0.6666303277015686 + 100.0 * 8.41861629486084
Epoch 1770, val loss: 0.6866952776908875
Epoch 1780, training loss: 842.5004272460938 = 0.6643772721290588 + 100.0 * 8.418360710144043
Epoch 1780, val loss: 0.684708297252655
Epoch 1790, training loss: 842.5382080078125 = 0.662113606929779 + 100.0 * 8.418761253356934
Epoch 1790, val loss: 0.6826894879341125
Epoch 1800, training loss: 842.84423828125 = 0.6595887541770935 + 100.0 * 8.421846389770508
Epoch 1800, val loss: 0.6803299188613892
Epoch 1810, training loss: 842.432861328125 = 0.6570468544960022 + 100.0 * 8.417757987976074
Epoch 1810, val loss: 0.6780640482902527
Epoch 1820, training loss: 842.464599609375 = 0.654831051826477 + 100.0 * 8.418097496032715
Epoch 1820, val loss: 0.6759704947471619
Epoch 1830, training loss: 842.3634643554688 = 0.6526990532875061 + 100.0 * 8.417107582092285
Epoch 1830, val loss: 0.6740932464599609
Epoch 1840, training loss: 842.3339233398438 = 0.6506175994873047 + 100.0 * 8.41683292388916
Epoch 1840, val loss: 0.6722354292869568
Epoch 1850, training loss: 842.3062133789062 = 0.6485013961791992 + 100.0 * 8.416577339172363
Epoch 1850, val loss: 0.670303225517273
Epoch 1860, training loss: 842.2816772460938 = 0.6463931202888489 + 100.0 * 8.416352272033691
Epoch 1860, val loss: 0.6684370636940002
Epoch 1870, training loss: 842.79833984375 = 0.6442463397979736 + 100.0 * 8.421541213989258
Epoch 1870, val loss: 0.6665576696395874
Epoch 1880, training loss: 842.53125 = 0.6417588591575623 + 100.0 * 8.41889476776123
Epoch 1880, val loss: 0.6641271114349365
Epoch 1890, training loss: 842.2333374023438 = 0.639604389667511 + 100.0 * 8.415937423706055
Epoch 1890, val loss: 0.66226726770401
Epoch 1900, training loss: 842.1901245117188 = 0.6375926733016968 + 100.0 * 8.415525436401367
Epoch 1900, val loss: 0.6604794263839722
Epoch 1910, training loss: 842.1739501953125 = 0.6356412768363953 + 100.0 * 8.415383338928223
Epoch 1910, val loss: 0.658750593662262
Epoch 1920, training loss: 842.5763549804688 = 0.633645236492157 + 100.0 * 8.419426918029785
Epoch 1920, val loss: 0.6568945050239563
Epoch 1930, training loss: 842.2494506835938 = 0.631454586982727 + 100.0 * 8.416179656982422
Epoch 1930, val loss: 0.6548941731452942
Epoch 1940, training loss: 842.0962524414062 = 0.6294766664505005 + 100.0 * 8.414668083190918
Epoch 1940, val loss: 0.6531894207000732
Epoch 1950, training loss: 842.0515747070312 = 0.6275734305381775 + 100.0 * 8.414239883422852
Epoch 1950, val loss: 0.6514624953269958
Epoch 1960, training loss: 842.0619506835938 = 0.6257162094116211 + 100.0 * 8.414361953735352
Epoch 1960, val loss: 0.6497415900230408
Epoch 1970, training loss: 842.2644653320312 = 0.6237645745277405 + 100.0 * 8.416406631469727
Epoch 1970, val loss: 0.6479218602180481
Epoch 1980, training loss: 842.062255859375 = 0.6217268705368042 + 100.0 * 8.414405822753906
Epoch 1980, val loss: 0.6462714076042175
Epoch 1990, training loss: 841.9608154296875 = 0.6198585033416748 + 100.0 * 8.413409233093262
Epoch 1990, val loss: 0.6445783376693726
Epoch 2000, training loss: 841.9625244140625 = 0.6180428266525269 + 100.0 * 8.413444519042969
Epoch 2000, val loss: 0.6429186463356018
Epoch 2010, training loss: 842.265869140625 = 0.6161872744560242 + 100.0 * 8.416496276855469
Epoch 2010, val loss: 0.6412104964256287
Epoch 2020, training loss: 841.9952392578125 = 0.6142691969871521 + 100.0 * 8.413809776306152
Epoch 2020, val loss: 0.6396237015724182
Epoch 2030, training loss: 841.9078369140625 = 0.6124128699302673 + 100.0 * 8.412954330444336
Epoch 2030, val loss: 0.6379202008247375
Epoch 2040, training loss: 841.8342895507812 = 0.6107591390609741 + 100.0 * 8.412235260009766
Epoch 2040, val loss: 0.6365152597427368
Epoch 2050, training loss: 841.7948608398438 = 0.6090606451034546 + 100.0 * 8.411857604980469
Epoch 2050, val loss: 0.6349968314170837
Epoch 2060, training loss: 841.8311767578125 = 0.6073824167251587 + 100.0 * 8.412238121032715
Epoch 2060, val loss: 0.633595883846283
Epoch 2070, training loss: 841.9822998046875 = 0.6055501699447632 + 100.0 * 8.41376781463623
Epoch 2070, val loss: 0.6319516897201538
Epoch 2080, training loss: 841.7996215820312 = 0.6037325859069824 + 100.0 * 8.411958694458008
Epoch 2080, val loss: 0.6303728818893433
Epoch 2090, training loss: 841.8363647460938 = 0.6020284295082092 + 100.0 * 8.41234302520752
Epoch 2090, val loss: 0.6287289261817932
Epoch 2100, training loss: 841.7244873046875 = 0.6003606915473938 + 100.0 * 8.41124153137207
Epoch 2100, val loss: 0.627388596534729
Epoch 2110, training loss: 841.6610107421875 = 0.598770022392273 + 100.0 * 8.410622596740723
Epoch 2110, val loss: 0.6258518695831299
Epoch 2120, training loss: 841.719970703125 = 0.5971882343292236 + 100.0 * 8.41122817993164
Epoch 2120, val loss: 0.6244060397148132
Epoch 2130, training loss: 841.8042602539062 = 0.5955175757408142 + 100.0 * 8.412087440490723
Epoch 2130, val loss: 0.6229755878448486
Epoch 2140, training loss: 841.709716796875 = 0.5938663482666016 + 100.0 * 8.411158561706543
Epoch 2140, val loss: 0.621669352054596
Epoch 2150, training loss: 841.6724853515625 = 0.592282235622406 + 100.0 * 8.410801887512207
Epoch 2150, val loss: 0.6202134490013123
Epoch 2160, training loss: 841.5220336914062 = 0.5906990766525269 + 100.0 * 8.409313201904297
Epoch 2160, val loss: 0.6188067197799683
Epoch 2170, training loss: 841.5372314453125 = 0.5892156362533569 + 100.0 * 8.409480094909668
Epoch 2170, val loss: 0.6175380945205688
Epoch 2180, training loss: 841.5640258789062 = 0.5877406597137451 + 100.0 * 8.409762382507324
Epoch 2180, val loss: 0.616184413433075
Epoch 2190, training loss: 841.5672607421875 = 0.5861995816230774 + 100.0 * 8.409811019897461
Epoch 2190, val loss: 0.6148195266723633
Epoch 2200, training loss: 841.789794921875 = 0.5846534967422485 + 100.0 * 8.4120512008667
Epoch 2200, val loss: 0.6134792566299438
Epoch 2210, training loss: 841.5712280273438 = 0.5830886363983154 + 100.0 * 8.409881591796875
Epoch 2210, val loss: 0.6123284697532654
Epoch 2220, training loss: 841.56787109375 = 0.5816165208816528 + 100.0 * 8.409862518310547
Epoch 2220, val loss: 0.6108693480491638
Epoch 2230, training loss: 841.3892822265625 = 0.5801383256912231 + 100.0 * 8.40809154510498
Epoch 2230, val loss: 0.609665036201477
Epoch 2240, training loss: 841.373291015625 = 0.578731894493103 + 100.0 * 8.40794563293457
Epoch 2240, val loss: 0.6084514260292053
Epoch 2250, training loss: 841.3497314453125 = 0.5774115324020386 + 100.0 * 8.407723426818848
Epoch 2250, val loss: 0.6072038412094116
Epoch 2260, training loss: 841.437255859375 = 0.5760448575019836 + 100.0 * 8.408612251281738
Epoch 2260, val loss: 0.6060326099395752
Epoch 2270, training loss: 841.4354248046875 = 0.5745875239372253 + 100.0 * 8.408608436584473
Epoch 2270, val loss: 0.604923665523529
Epoch 2280, training loss: 841.3765869140625 = 0.5732317566871643 + 100.0 * 8.40803337097168
Epoch 2280, val loss: 0.6037815809249878
Epoch 2290, training loss: 841.39404296875 = 0.5718161463737488 + 100.0 * 8.408222198486328
Epoch 2290, val loss: 0.6024978160858154
Epoch 2300, training loss: 841.3128051757812 = 0.5704478025436401 + 100.0 * 8.407423973083496
Epoch 2300, val loss: 0.6013639569282532
Epoch 2310, training loss: 841.2244873046875 = 0.5691832304000854 + 100.0 * 8.406553268432617
Epoch 2310, val loss: 0.6001572012901306
Epoch 2320, training loss: 841.2151489257812 = 0.5679132342338562 + 100.0 * 8.406472206115723
Epoch 2320, val loss: 0.5991373658180237
Epoch 2330, training loss: 841.2715454101562 = 0.5666501522064209 + 100.0 * 8.407049179077148
Epoch 2330, val loss: 0.5979918241500854
Epoch 2340, training loss: 841.2488403320312 = 0.5653356909751892 + 100.0 * 8.406835556030273
Epoch 2340, val loss: 0.5968158841133118
Epoch 2350, training loss: 841.340087890625 = 0.5640339255332947 + 100.0 * 8.407760620117188
Epoch 2350, val loss: 0.5958790183067322
Epoch 2360, training loss: 841.0883178710938 = 0.5626919269561768 + 100.0 * 8.405256271362305
Epoch 2360, val loss: 0.5946584939956665
Epoch 2370, training loss: 841.0792236328125 = 0.561495840549469 + 100.0 * 8.405177116394043
Epoch 2370, val loss: 0.5936959981918335
Epoch 2380, training loss: 841.06494140625 = 0.5603234767913818 + 100.0 * 8.405046463012695
Epoch 2380, val loss: 0.5927350521087646
Epoch 2390, training loss: 841.0296020507812 = 0.559185802936554 + 100.0 * 8.404704093933105
Epoch 2390, val loss: 0.5917345881462097
Epoch 2400, training loss: 841.17724609375 = 0.5580250024795532 + 100.0 * 8.4061918258667
Epoch 2400, val loss: 0.5906981229782104
Epoch 2410, training loss: 841.1515502929688 = 0.5566853284835815 + 100.0 * 8.405948638916016
Epoch 2410, val loss: 0.5895504951477051
Epoch 2420, training loss: 841.1717529296875 = 0.5553982257843018 + 100.0 * 8.406163215637207
Epoch 2420, val loss: 0.5883823037147522
Epoch 2430, training loss: 841.0255737304688 = 0.554212749004364 + 100.0 * 8.40471363067627
Epoch 2430, val loss: 0.5875195264816284
Epoch 2440, training loss: 840.9436645507812 = 0.55311119556427 + 100.0 * 8.403905868530273
Epoch 2440, val loss: 0.5866003632545471
Epoch 2450, training loss: 840.9051513671875 = 0.5520501136779785 + 100.0 * 8.403531074523926
Epoch 2450, val loss: 0.5857341885566711
Epoch 2460, training loss: 840.9124755859375 = 0.5509733557701111 + 100.0 * 8.40361499786377
Epoch 2460, val loss: 0.5848706960678101
Epoch 2470, training loss: 841.3872680664062 = 0.5498590469360352 + 100.0 * 8.408373832702637
Epoch 2470, val loss: 0.5841651558876038
Epoch 2480, training loss: 841.0755615234375 = 0.5485826134681702 + 100.0 * 8.405269622802734
Epoch 2480, val loss: 0.5826066136360168
Epoch 2490, training loss: 841.0283813476562 = 0.5474624037742615 + 100.0 * 8.40480899810791
Epoch 2490, val loss: 0.5819542407989502
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7691527143581938
0.8168514091139608
The final CL Acc:0.77389, 0.00357, The final GNN Acc:0.81714, 0.00085
raw graph: torch.Size([2, 88648])
add edge: torch.Size([2, 110824])
remove edge: torch.Size([2, 66766])
updated graph: torch.Size([2, 88942])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 1059.3365478515625 = 1.1092644929885864 + 100.0 * 10.58227252960205
Epoch 0, val loss: 1.1074211597442627
Epoch 10, training loss: 1059.2933349609375 = 1.104247808456421 + 100.0 * 10.581891059875488
Epoch 10, val loss: 1.1024129390716553
Epoch 20, training loss: 1059.132568359375 = 1.0987133979797363 + 100.0 * 10.580338478088379
Epoch 20, val loss: 1.0968915224075317
Epoch 30, training loss: 1058.4378662109375 = 1.0925276279449463 + 100.0 * 10.573453903198242
Epoch 30, val loss: 1.0907248258590698
Epoch 40, training loss: 1055.6044921875 = 1.0855045318603516 + 100.0 * 10.54518985748291
Epoch 40, val loss: 1.0837422609329224
Epoch 50, training loss: 1045.9644775390625 = 1.0778238773345947 + 100.0 * 10.44886589050293
Epoch 50, val loss: 1.0762125253677368
Epoch 60, training loss: 1015.3597412109375 = 1.0706908702850342 + 100.0 * 10.142890930175781
Epoch 60, val loss: 1.069329023361206
Epoch 70, training loss: 961.570556640625 = 1.0635095834732056 + 100.0 * 9.605070114135742
Epoch 70, val loss: 1.0621919631958008
Epoch 80, training loss: 944.4434814453125 = 1.0578891038894653 + 100.0 * 9.433856010437012
Epoch 80, val loss: 1.0570107698440552
Epoch 90, training loss: 929.6654052734375 = 1.053462028503418 + 100.0 * 9.28611946105957
Epoch 90, val loss: 1.052781105041504
Epoch 100, training loss: 921.78955078125 = 1.0494942665100098 + 100.0 * 9.20740032196045
Epoch 100, val loss: 1.049019455909729
Epoch 110, training loss: 917.81689453125 = 1.0461698770523071 + 100.0 * 9.167707443237305
Epoch 110, val loss: 1.0459173917770386
Epoch 120, training loss: 912.6672973632812 = 1.043657660484314 + 100.0 * 9.116236686706543
Epoch 120, val loss: 1.0436495542526245
Epoch 130, training loss: 906.0463256835938 = 1.0421336889266968 + 100.0 * 9.050042152404785
Epoch 130, val loss: 1.042327642440796
Epoch 140, training loss: 899.4616088867188 = 1.041258692741394 + 100.0 * 8.984203338623047
Epoch 140, val loss: 1.0415221452713013
Epoch 150, training loss: 894.3632202148438 = 1.0403306484222412 + 100.0 * 8.933228492736816
Epoch 150, val loss: 1.0406067371368408
Epoch 160, training loss: 889.416259765625 = 1.039207100868225 + 100.0 * 8.883770942687988
Epoch 160, val loss: 1.0395265817642212
Epoch 170, training loss: 883.4131469726562 = 1.0382187366485596 + 100.0 * 8.823749542236328
Epoch 170, val loss: 1.0385972261428833
Epoch 180, training loss: 878.0301513671875 = 1.0375382900238037 + 100.0 * 8.769926071166992
Epoch 180, val loss: 1.0379782915115356
Epoch 190, training loss: 873.7767333984375 = 1.0371226072311401 + 100.0 * 8.727396011352539
Epoch 190, val loss: 1.037636637687683
Epoch 200, training loss: 870.441162109375 = 1.0367432832717896 + 100.0 * 8.69404411315918
Epoch 200, val loss: 1.037307858467102
Epoch 210, training loss: 867.8123779296875 = 1.0362226963043213 + 100.0 * 8.66776180267334
Epoch 210, val loss: 1.0368142127990723
Epoch 220, training loss: 865.2398071289062 = 1.0355826616287231 + 100.0 * 8.64204216003418
Epoch 220, val loss: 1.036200761795044
Epoch 230, training loss: 863.3687133789062 = 1.0349342823028564 + 100.0 * 8.623337745666504
Epoch 230, val loss: 1.035575032234192
Epoch 240, training loss: 862.0193481445312 = 1.0342061519622803 + 100.0 * 8.609851837158203
Epoch 240, val loss: 1.034852385520935
Epoch 250, training loss: 860.8951416015625 = 1.0333725214004517 + 100.0 * 8.598617553710938
Epoch 250, val loss: 1.0340341329574585
Epoch 260, training loss: 859.94189453125 = 1.0324891805648804 + 100.0 * 8.589094161987305
Epoch 260, val loss: 1.0331642627716064
Epoch 270, training loss: 859.0859985351562 = 1.0315836668014526 + 100.0 * 8.580544471740723
Epoch 270, val loss: 1.032281517982483
Epoch 280, training loss: 858.321044921875 = 1.030684471130371 + 100.0 * 8.572903633117676
Epoch 280, val loss: 1.0314098596572876
Epoch 290, training loss: 857.6030883789062 = 1.0297861099243164 + 100.0 * 8.565732955932617
Epoch 290, val loss: 1.0305424928665161
Epoch 300, training loss: 857.2257690429688 = 1.0288809537887573 + 100.0 * 8.561968803405762
Epoch 300, val loss: 1.0296653509140015
Epoch 310, training loss: 856.1796264648438 = 1.0279349088668823 + 100.0 * 8.55151653289795
Epoch 310, val loss: 1.0287415981292725
Epoch 320, training loss: 855.4550170898438 = 1.027016520500183 + 100.0 * 8.544280052185059
Epoch 320, val loss: 1.0278655290603638
Epoch 330, training loss: 854.8359985351562 = 1.026058316230774 + 100.0 * 8.53809928894043
Epoch 330, val loss: 1.0269365310668945
Epoch 340, training loss: 854.2991943359375 = 1.025033950805664 + 100.0 * 8.53274154663086
Epoch 340, val loss: 1.0259368419647217
Epoch 350, training loss: 853.8627319335938 = 1.0239399671554565 + 100.0 * 8.528388023376465
Epoch 350, val loss: 1.024885654449463
Epoch 360, training loss: 853.3056030273438 = 1.02285897731781 + 100.0 * 8.5228271484375
Epoch 360, val loss: 1.0238728523254395
Epoch 370, training loss: 852.9312133789062 = 1.0218102931976318 + 100.0 * 8.519094467163086
Epoch 370, val loss: 1.0229157209396362
Epoch 380, training loss: 852.2821655273438 = 1.020754098892212 + 100.0 * 8.512614250183105
Epoch 380, val loss: 1.021903395652771
Epoch 390, training loss: 851.7217407226562 = 1.0197290182113647 + 100.0 * 8.507019996643066
Epoch 390, val loss: 1.0209389925003052
Epoch 400, training loss: 851.135986328125 = 1.018702745437622 + 100.0 * 8.50117301940918
Epoch 400, val loss: 1.0199675559997559
Epoch 410, training loss: 850.7636108398438 = 1.017626404762268 + 100.0 * 8.497459411621094
Epoch 410, val loss: 1.0189299583435059
Epoch 420, training loss: 850.1911010742188 = 1.0164422988891602 + 100.0 * 8.49174690246582
Epoch 420, val loss: 1.0178048610687256
Epoch 430, training loss: 849.6975708007812 = 1.0152587890625 + 100.0 * 8.486823081970215
Epoch 430, val loss: 1.0166761875152588
Epoch 440, training loss: 849.3206176757812 = 1.014004111289978 + 100.0 * 8.48306655883789
Epoch 440, val loss: 1.0154823064804077
Epoch 450, training loss: 848.9596557617188 = 1.0126359462738037 + 100.0 * 8.479470252990723
Epoch 450, val loss: 1.0141525268554688
Epoch 460, training loss: 848.5731811523438 = 1.0112173557281494 + 100.0 * 8.475619316101074
Epoch 460, val loss: 1.012805700302124
Epoch 470, training loss: 848.2595825195312 = 1.0097798109054565 + 100.0 * 8.472497940063477
Epoch 470, val loss: 1.0114381313323975
Epoch 480, training loss: 847.9365844726562 = 1.0082762241363525 + 100.0 * 8.469283103942871
Epoch 480, val loss: 1.0100047588348389
Epoch 490, training loss: 847.6687622070312 = 1.006690502166748 + 100.0 * 8.466620445251465
Epoch 490, val loss: 1.0084575414657593
Epoch 500, training loss: 847.3748168945312 = 1.0050522089004517 + 100.0 * 8.46369743347168
Epoch 500, val loss: 1.006913185119629
Epoch 510, training loss: 847.0913696289062 = 1.0034117698669434 + 100.0 * 8.4608793258667
Epoch 510, val loss: 1.0053566694259644
Epoch 520, training loss: 846.8321533203125 = 1.0017355680465698 + 100.0 * 8.458304405212402
Epoch 520, val loss: 1.00375497341156
Epoch 530, training loss: 846.7019653320312 = 0.999980092048645 + 100.0 * 8.457019805908203
Epoch 530, val loss: 1.0020700693130493
Epoch 540, training loss: 846.3977661132812 = 0.9981616735458374 + 100.0 * 8.453995704650879
Epoch 540, val loss: 1.000360131263733
Epoch 550, training loss: 846.1995239257812 = 0.9963599443435669 + 100.0 * 8.452032089233398
Epoch 550, val loss: 0.9986271262168884
Epoch 560, training loss: 845.9288940429688 = 0.9944688081741333 + 100.0 * 8.449344635009766
Epoch 560, val loss: 0.9968195557594299
Epoch 570, training loss: 845.7421875 = 0.9925510287284851 + 100.0 * 8.44749641418457
Epoch 570, val loss: 0.9950017333030701
Epoch 580, training loss: 845.5692749023438 = 0.9905910491943359 + 100.0 * 8.445786476135254
Epoch 580, val loss: 0.9931474924087524
Epoch 590, training loss: 845.38232421875 = 0.9885215163230896 + 100.0 * 8.443938255310059
Epoch 590, val loss: 0.9911771416664124
Epoch 600, training loss: 845.2181396484375 = 0.9864236116409302 + 100.0 * 8.442317008972168
Epoch 600, val loss: 0.9891781806945801
Epoch 610, training loss: 845.0023193359375 = 0.9843416810035706 + 100.0 * 8.440179824829102
Epoch 610, val loss: 0.9871892929077148
Epoch 620, training loss: 845.00634765625 = 0.9821955561637878 + 100.0 * 8.440241813659668
Epoch 620, val loss: 0.9851332306861877
Epoch 630, training loss: 844.6752319335938 = 0.9799548387527466 + 100.0 * 8.436952590942383
Epoch 630, val loss: 0.9830071926116943
Epoch 640, training loss: 844.445556640625 = 0.9777501821517944 + 100.0 * 8.434678077697754
Epoch 640, val loss: 0.9809180498123169
Epoch 650, training loss: 844.3977661132812 = 0.9755010604858398 + 100.0 * 8.434222221374512
Epoch 650, val loss: 0.978754460811615
Epoch 660, training loss: 844.1893920898438 = 0.973106861114502 + 100.0 * 8.43216323852539
Epoch 660, val loss: 0.9764633774757385
Epoch 670, training loss: 843.9080200195312 = 0.9707432389259338 + 100.0 * 8.429372787475586
Epoch 670, val loss: 0.9742460250854492
Epoch 680, training loss: 843.7141723632812 = 0.9683797955513 + 100.0 * 8.427457809448242
Epoch 680, val loss: 0.97197425365448
Epoch 690, training loss: 843.9412841796875 = 0.9658939242362976 + 100.0 * 8.429754257202148
Epoch 690, val loss: 0.9696178436279297
Epoch 700, training loss: 843.5694580078125 = 0.9631618857383728 + 100.0 * 8.426063537597656
Epoch 700, val loss: 0.9670007824897766
Epoch 710, training loss: 843.3031005859375 = 0.9606142044067383 + 100.0 * 8.42342472076416
Epoch 710, val loss: 0.9645547270774841
Epoch 720, training loss: 843.0936889648438 = 0.9580928683280945 + 100.0 * 8.421356201171875
Epoch 720, val loss: 0.9621469378471375
Epoch 730, training loss: 842.933349609375 = 0.9555473327636719 + 100.0 * 8.419777870178223
Epoch 730, val loss: 0.9597151875495911
Epoch 740, training loss: 842.7780151367188 = 0.952947735786438 + 100.0 * 8.418251037597656
Epoch 740, val loss: 0.9572286605834961
Epoch 750, training loss: 842.6943969726562 = 0.9502674341201782 + 100.0 * 8.417441368103027
Epoch 750, val loss: 0.9546933174133301
Epoch 760, training loss: 842.6973266601562 = 0.9474598169326782 + 100.0 * 8.417498588562012
Epoch 760, val loss: 0.9519084095954895
Epoch 770, training loss: 842.35546875 = 0.9446266889572144 + 100.0 * 8.414108276367188
Epoch 770, val loss: 0.9492629766464233
Epoch 780, training loss: 842.1760864257812 = 0.9419301152229309 + 100.0 * 8.412341117858887
Epoch 780, val loss: 0.9466755390167236
Epoch 790, training loss: 842.0303955078125 = 0.9392045140266418 + 100.0 * 8.410911560058594
Epoch 790, val loss: 0.9440667033195496
Epoch 800, training loss: 842.0960693359375 = 0.9364397525787354 + 100.0 * 8.411596298217773
Epoch 800, val loss: 0.9413672685623169
Epoch 810, training loss: 842.0977783203125 = 0.9333704113960266 + 100.0 * 8.411643981933594
Epoch 810, val loss: 0.9384779334068298
Epoch 820, training loss: 841.6839599609375 = 0.930419921875 + 100.0 * 8.407535552978516
Epoch 820, val loss: 0.9356564879417419
Epoch 830, training loss: 841.4977416992188 = 0.9275549054145813 + 100.0 * 8.405701637268066
Epoch 830, val loss: 0.9328866004943848
Epoch 840, training loss: 841.3688354492188 = 0.92464280128479 + 100.0 * 8.404441833496094
Epoch 840, val loss: 0.9301049113273621
Epoch 850, training loss: 841.2799682617188 = 0.9216768741607666 + 100.0 * 8.403582572937012
Epoch 850, val loss: 0.9272434115409851
Epoch 860, training loss: 841.3495483398438 = 0.9185369610786438 + 100.0 * 8.40431022644043
Epoch 860, val loss: 0.9242095351219177
Epoch 870, training loss: 841.0818481445312 = 0.915351390838623 + 100.0 * 8.401664733886719
Epoch 870, val loss: 0.9211406707763672
Epoch 880, training loss: 840.9328002929688 = 0.9122142791748047 + 100.0 * 8.400205612182617
Epoch 880, val loss: 0.9181330800056458
Epoch 890, training loss: 840.8138427734375 = 0.9090375304222107 + 100.0 * 8.3990478515625
Epoch 890, val loss: 0.9150633215904236
Epoch 900, training loss: 840.9982299804688 = 0.9057883620262146 + 100.0 * 8.400924682617188
Epoch 900, val loss: 0.9118689298629761
Epoch 910, training loss: 840.6611328125 = 0.9022908806800842 + 100.0 * 8.397588729858398
Epoch 910, val loss: 0.9085814952850342
Epoch 920, training loss: 840.542236328125 = 0.8989166021347046 + 100.0 * 8.396432876586914
Epoch 920, val loss: 0.9053432941436768
Epoch 930, training loss: 840.4381713867188 = 0.8955469727516174 + 100.0 * 8.395425796508789
Epoch 930, val loss: 0.9020828008651733
Epoch 940, training loss: 840.593505859375 = 0.8920591473579407 + 100.0 * 8.397014617919922
Epoch 940, val loss: 0.8987044095993042
Epoch 950, training loss: 840.3394775390625 = 0.8884454369544983 + 100.0 * 8.394510269165039
Epoch 950, val loss: 0.8952500820159912
Epoch 960, training loss: 840.1795043945312 = 0.8848934769630432 + 100.0 * 8.392946243286133
Epoch 960, val loss: 0.8918461203575134
Epoch 970, training loss: 840.0791625976562 = 0.881365180015564 + 100.0 * 8.39197826385498
Epoch 970, val loss: 0.8884419202804565
Epoch 980, training loss: 840.2315063476562 = 0.8777190446853638 + 100.0 * 8.393537521362305
Epoch 980, val loss: 0.884945273399353
Epoch 990, training loss: 839.9524536132812 = 0.8739935159683228 + 100.0 * 8.39078426361084
Epoch 990, val loss: 0.8813033103942871
Epoch 1000, training loss: 839.83984375 = 0.8703176975250244 + 100.0 * 8.389695167541504
Epoch 1000, val loss: 0.8778398633003235
Epoch 1010, training loss: 839.785888671875 = 0.8666810989379883 + 100.0 * 8.389191627502441
Epoch 1010, val loss: 0.8742966651916504
Epoch 1020, training loss: 839.73876953125 = 0.8629133105278015 + 100.0 * 8.388758659362793
Epoch 1020, val loss: 0.8707120418548584
Epoch 1030, training loss: 839.6181640625 = 0.8591209053993225 + 100.0 * 8.387590408325195
Epoch 1030, val loss: 0.867112934589386
Epoch 1040, training loss: 839.5653076171875 = 0.8552858829498291 + 100.0 * 8.387100219726562
Epoch 1040, val loss: 0.86346036195755
Epoch 1050, training loss: 839.5929565429688 = 0.8514707088470459 + 100.0 * 8.387414932250977
Epoch 1050, val loss: 0.859758198261261
Epoch 1060, training loss: 839.3640747070312 = 0.8474586009979248 + 100.0 * 8.38516616821289
Epoch 1060, val loss: 0.8559386730194092
Epoch 1070, training loss: 839.277099609375 = 0.843604326248169 + 100.0 * 8.384334564208984
Epoch 1070, val loss: 0.8522632122039795
Epoch 1080, training loss: 839.1964721679688 = 0.8397942781448364 + 100.0 * 8.383566856384277
Epoch 1080, val loss: 0.8486025333404541
Epoch 1090, training loss: 839.2141723632812 = 0.8359121680259705 + 100.0 * 8.383782386779785
Epoch 1090, val loss: 0.84493088722229
Epoch 1100, training loss: 839.0782470703125 = 0.8318831920623779 + 100.0 * 8.382463455200195
Epoch 1100, val loss: 0.8410009145736694
Epoch 1110, training loss: 839.0093994140625 = 0.8278974890708923 + 100.0 * 8.381814956665039
Epoch 1110, val loss: 0.8372393250465393
Epoch 1120, training loss: 839.0557861328125 = 0.8240227699279785 + 100.0 * 8.382317543029785
Epoch 1120, val loss: 0.8334335088729858
Epoch 1130, training loss: 838.8532104492188 = 0.8198684453964233 + 100.0 * 8.380332946777344
Epoch 1130, val loss: 0.8295571208000183
Epoch 1140, training loss: 838.7906494140625 = 0.815888524055481 + 100.0 * 8.37974739074707
Epoch 1140, val loss: 0.825748860836029
Epoch 1150, training loss: 838.73291015625 = 0.8119611740112305 + 100.0 * 8.379209518432617
Epoch 1150, val loss: 0.822008490562439
Epoch 1160, training loss: 838.6629028320312 = 0.8080218434333801 + 100.0 * 8.378548622131348
Epoch 1160, val loss: 0.8182628154754639
Epoch 1170, training loss: 839.1298217773438 = 0.8040132522583008 + 100.0 * 8.383257865905762
Epoch 1170, val loss: 0.8143653869628906
Epoch 1180, training loss: 838.6939086914062 = 0.7995524406433105 + 100.0 * 8.37894344329834
Epoch 1180, val loss: 0.810196578502655
Epoch 1190, training loss: 838.5419311523438 = 0.7954985499382019 + 100.0 * 8.377464294433594
Epoch 1190, val loss: 0.8063744902610779
Epoch 1200, training loss: 838.405029296875 = 0.7915481925010681 + 100.0 * 8.376134872436523
Epoch 1200, val loss: 0.8026082515716553
Epoch 1210, training loss: 838.3563842773438 = 0.7875838279724121 + 100.0 * 8.375687599182129
Epoch 1210, val loss: 0.7988269329071045
Epoch 1220, training loss: 838.287841796875 = 0.7835665345191956 + 100.0 * 8.375042915344238
Epoch 1220, val loss: 0.7950240969657898
Epoch 1230, training loss: 838.2445678710938 = 0.7795397043228149 + 100.0 * 8.374650001525879
Epoch 1230, val loss: 0.791197657585144
Epoch 1240, training loss: 838.673583984375 = 0.7754482626914978 + 100.0 * 8.378981590270996
Epoch 1240, val loss: 0.7871749997138977
Epoch 1250, training loss: 838.4012451171875 = 0.7709972262382507 + 100.0 * 8.376302719116211
Epoch 1250, val loss: 0.7830625176429749
Epoch 1260, training loss: 838.1702270507812 = 0.7667357921600342 + 100.0 * 8.374034881591797
Epoch 1260, val loss: 0.779143750667572
Epoch 1270, training loss: 838.0211791992188 = 0.7627628445625305 + 100.0 * 8.372584342956543
Epoch 1270, val loss: 0.7753410339355469
Epoch 1280, training loss: 837.9891967773438 = 0.7587791681289673 + 100.0 * 8.37230396270752
Epoch 1280, val loss: 0.771569550037384
Epoch 1290, training loss: 837.9368286132812 = 0.7547575831413269 + 100.0 * 8.371820449829102
Epoch 1290, val loss: 0.7678021192550659
Epoch 1300, training loss: 838.0704956054688 = 0.7506560683250427 + 100.0 * 8.373198509216309
Epoch 1300, val loss: 0.763968825340271
Epoch 1310, training loss: 837.892822265625 = 0.7464667558670044 + 100.0 * 8.371463775634766
Epoch 1310, val loss: 0.7599238157272339
Epoch 1320, training loss: 837.7911376953125 = 0.7423062324523926 + 100.0 * 8.370488166809082
Epoch 1320, val loss: 0.7560935020446777
Epoch 1330, training loss: 837.7398071289062 = 0.7382980585098267 + 100.0 * 8.370015144348145
Epoch 1330, val loss: 0.7523207664489746
Epoch 1340, training loss: 837.7024536132812 = 0.7343003749847412 + 100.0 * 8.369681358337402
Epoch 1340, val loss: 0.7485950589179993
Epoch 1350, training loss: 837.9930419921875 = 0.7302207350730896 + 100.0 * 8.372628211975098
Epoch 1350, val loss: 0.7448020577430725
Epoch 1360, training loss: 837.7205810546875 = 0.7260185480117798 + 100.0 * 8.369945526123047
Epoch 1360, val loss: 0.7407479882240295
Epoch 1370, training loss: 837.6170654296875 = 0.7219699621200562 + 100.0 * 8.368950843811035
Epoch 1370, val loss: 0.7370437979698181
Epoch 1380, training loss: 837.5416259765625 = 0.7180715799331665 + 100.0 * 8.36823558807373
Epoch 1380, val loss: 0.7333503365516663
Epoch 1390, training loss: 837.6375122070312 = 0.7141368389129639 + 100.0 * 8.369234085083008
Epoch 1390, val loss: 0.7296756505966187
Epoch 1400, training loss: 837.4605102539062 = 0.7100343108177185 + 100.0 * 8.367505073547363
Epoch 1400, val loss: 0.7258309125900269
Epoch 1410, training loss: 837.4070434570312 = 0.7060623168945312 + 100.0 * 8.367010116577148
Epoch 1410, val loss: 0.7221779227256775
Epoch 1420, training loss: 837.3692016601562 = 0.7022261619567871 + 100.0 * 8.366669654846191
Epoch 1420, val loss: 0.7185736298561096
Epoch 1430, training loss: 837.3162841796875 = 0.6984087228775024 + 100.0 * 8.366178512573242
Epoch 1430, val loss: 0.7150296568870544
Epoch 1440, training loss: 837.2819213867188 = 0.6945801973342896 + 100.0 * 8.365873336791992
Epoch 1440, val loss: 0.7114658355712891
Epoch 1450, training loss: 837.3093872070312 = 0.6907667517662048 + 100.0 * 8.366186141967773
Epoch 1450, val loss: 0.7078943252563477
Epoch 1460, training loss: 837.7440185546875 = 0.6866887211799622 + 100.0 * 8.370573043823242
Epoch 1460, val loss: 0.7040953636169434
Epoch 1470, training loss: 837.3636474609375 = 0.682532012462616 + 100.0 * 8.36681079864502
Epoch 1470, val loss: 0.7002018690109253
Epoch 1480, training loss: 837.149169921875 = 0.6788078546524048 + 100.0 * 8.364703178405762
Epoch 1480, val loss: 0.6967629790306091
Epoch 1490, training loss: 837.0859985351562 = 0.6752002835273743 + 100.0 * 8.364108085632324
Epoch 1490, val loss: 0.693450391292572
Epoch 1500, training loss: 837.0427856445312 = 0.6716094613075256 + 100.0 * 8.3637113571167
Epoch 1500, val loss: 0.690142810344696
Epoch 1510, training loss: 837.000732421875 = 0.6680015325546265 + 100.0 * 8.363327026367188
Epoch 1510, val loss: 0.686809241771698
Epoch 1520, training loss: 836.9609985351562 = 0.6644096970558167 + 100.0 * 8.36296558380127
Epoch 1520, val loss: 0.6834878921508789
Epoch 1530, training loss: 836.9581298828125 = 0.6607977747917175 + 100.0 * 8.3629732131958
Epoch 1530, val loss: 0.6801773309707642
Epoch 1540, training loss: 837.4852294921875 = 0.6569216847419739 + 100.0 * 8.36828327178955
Epoch 1540, val loss: 0.6765260100364685
Epoch 1550, training loss: 836.94970703125 = 0.6529356241226196 + 100.0 * 8.362967491149902
Epoch 1550, val loss: 0.6729282140731812
Epoch 1560, training loss: 836.8360595703125 = 0.649493396282196 + 100.0 * 8.361865997314453
Epoch 1560, val loss: 0.6697061657905579
Epoch 1570, training loss: 836.7987060546875 = 0.6460820436477661 + 100.0 * 8.361526489257812
Epoch 1570, val loss: 0.6665401458740234
Epoch 1580, training loss: 836.7412109375 = 0.6426759362220764 + 100.0 * 8.360984802246094
Epoch 1580, val loss: 0.6634236574172974
Epoch 1590, training loss: 836.70361328125 = 0.6392819285392761 + 100.0 * 8.36064338684082
Epoch 1590, val loss: 0.6603221893310547
Epoch 1600, training loss: 836.66845703125 = 0.6358870267868042 + 100.0 * 8.360325813293457
Epoch 1600, val loss: 0.6572151780128479
Epoch 1610, training loss: 837.1401977539062 = 0.6323912143707275 + 100.0 * 8.36507797241211
Epoch 1610, val loss: 0.6541222929954529
Epoch 1620, training loss: 836.7947998046875 = 0.6287485957145691 + 100.0 * 8.361660957336426
Epoch 1620, val loss: 0.6505417227745056
Epoch 1630, training loss: 836.6712646484375 = 0.625316858291626 + 100.0 * 8.360459327697754
Epoch 1630, val loss: 0.6474192142486572
Epoch 1640, training loss: 836.5563354492188 = 0.6220823526382446 + 100.0 * 8.359342575073242
Epoch 1640, val loss: 0.6444981098175049
Epoch 1650, training loss: 836.4911499023438 = 0.6188832521438599 + 100.0 * 8.358722686767578
Epoch 1650, val loss: 0.6415824890136719
Epoch 1660, training loss: 836.45556640625 = 0.6157012581825256 + 100.0 * 8.3583984375
Epoch 1660, val loss: 0.6386940479278564
Epoch 1670, training loss: 836.4226684570312 = 0.6125181317329407 + 100.0 * 8.358101844787598
Epoch 1670, val loss: 0.6357964277267456
Epoch 1680, training loss: 836.4918212890625 = 0.6093468070030212 + 100.0 * 8.358824729919434
Epoch 1680, val loss: 0.6328660845756531
Epoch 1690, training loss: 836.4590454101562 = 0.605957567691803 + 100.0 * 8.35853099822998
Epoch 1690, val loss: 0.6297760605812073
Epoch 1700, training loss: 836.3355712890625 = 0.6026655435562134 + 100.0 * 8.357329368591309
Epoch 1700, val loss: 0.6267807483673096
Epoch 1710, training loss: 836.3142700195312 = 0.5995737314224243 + 100.0 * 8.357147216796875
Epoch 1710, val loss: 0.6239930987358093
Epoch 1720, training loss: 836.2882080078125 = 0.5965539813041687 + 100.0 * 8.356916427612305
Epoch 1720, val loss: 0.6212266087532043
Epoch 1730, training loss: 836.4613647460938 = 0.5934901833534241 + 100.0 * 8.358678817749023
Epoch 1730, val loss: 0.6183972358703613
Epoch 1740, training loss: 836.1871337890625 = 0.5902999639511108 + 100.0 * 8.355968475341797
Epoch 1740, val loss: 0.6155411005020142
Epoch 1750, training loss: 836.1679077148438 = 0.5872939229011536 + 100.0 * 8.355806350708008
Epoch 1750, val loss: 0.6128059029579163
Epoch 1760, training loss: 836.1441040039062 = 0.5843647122383118 + 100.0 * 8.355597496032715
Epoch 1760, val loss: 0.6101587414741516
Epoch 1770, training loss: 836.0931396484375 = 0.5814784169197083 + 100.0 * 8.355116844177246
Epoch 1770, val loss: 0.6075466275215149
Epoch 1780, training loss: 836.1727905273438 = 0.578606367111206 + 100.0 * 8.355941772460938
Epoch 1780, val loss: 0.6048625111579895
Epoch 1790, training loss: 836.1189575195312 = 0.5754146575927734 + 100.0 * 8.355435371398926
Epoch 1790, val loss: 0.6019716858863831
Epoch 1800, training loss: 836.0693969726562 = 0.5723782777786255 + 100.0 * 8.35496997833252
Epoch 1800, val loss: 0.599261462688446
Epoch 1810, training loss: 836.0184326171875 = 0.5695504546165466 + 100.0 * 8.354488372802734
Epoch 1810, val loss: 0.5967546701431274
Epoch 1820, training loss: 835.9425048828125 = 0.5668420195579529 + 100.0 * 8.35375690460205
Epoch 1820, val loss: 0.5942991375923157
Epoch 1830, training loss: 835.9230346679688 = 0.5641182661056519 + 100.0 * 8.353589057922363
Epoch 1830, val loss: 0.5918657183647156
Epoch 1840, training loss: 836.2407836914062 = 0.561367392539978 + 100.0 * 8.356794357299805
Epoch 1840, val loss: 0.5894010066986084
Epoch 1850, training loss: 835.9130859375 = 0.5583153367042542 + 100.0 * 8.353548049926758
Epoch 1850, val loss: 0.5866039991378784
Epoch 1860, training loss: 835.8851318359375 = 0.5556316375732422 + 100.0 * 8.35329532623291
Epoch 1860, val loss: 0.5842432379722595
Epoch 1870, training loss: 835.8270874023438 = 0.5530042052268982 + 100.0 * 8.352741241455078
Epoch 1870, val loss: 0.5818860530853271
Epoch 1880, training loss: 835.7783203125 = 0.5504456758499146 + 100.0 * 8.352278709411621
Epoch 1880, val loss: 0.5796089768409729
Epoch 1890, training loss: 835.7545776367188 = 0.5478783249855042 + 100.0 * 8.352066993713379
Epoch 1890, val loss: 0.5773217678070068
Epoch 1900, training loss: 836.2823486328125 = 0.5452448129653931 + 100.0 * 8.35737133026123
Epoch 1900, val loss: 0.5750191807746887
Epoch 1910, training loss: 835.8685302734375 = 0.5424355268478394 + 100.0 * 8.35326099395752
Epoch 1910, val loss: 0.572399377822876
Epoch 1920, training loss: 835.6666870117188 = 0.5398991107940674 + 100.0 * 8.35126781463623
Epoch 1920, val loss: 0.5701815485954285
Epoch 1930, training loss: 835.64794921875 = 0.5374508500099182 + 100.0 * 8.351104736328125
Epoch 1930, val loss: 0.5680252909660339
Epoch 1940, training loss: 835.606201171875 = 0.5350514054298401 + 100.0 * 8.350711822509766
Epoch 1940, val loss: 0.565886378288269
Epoch 1950, training loss: 835.5746459960938 = 0.53263920545578 + 100.0 * 8.350419998168945
Epoch 1950, val loss: 0.5637550950050354
Epoch 1960, training loss: 835.5634155273438 = 0.5302383303642273 + 100.0 * 8.350332260131836
Epoch 1960, val loss: 0.5616410374641418
Epoch 1970, training loss: 836.497802734375 = 0.5277310609817505 + 100.0 * 8.359701156616211
Epoch 1970, val loss: 0.5594539642333984
Epoch 1980, training loss: 835.5650634765625 = 0.5248753428459167 + 100.0 * 8.350401878356934
Epoch 1980, val loss: 0.5568472743034363
Epoch 1990, training loss: 835.5604858398438 = 0.5225520730018616 + 100.0 * 8.350379943847656
Epoch 1990, val loss: 0.554752767086029
Epoch 2000, training loss: 835.478759765625 = 0.520302951335907 + 100.0 * 8.349584579467773
Epoch 2000, val loss: 0.5527710914611816
Epoch 2010, training loss: 835.4261474609375 = 0.5181119441986084 + 100.0 * 8.349080085754395
Epoch 2010, val loss: 0.5508543252944946
Epoch 2020, training loss: 835.4004516601562 = 0.5159045457839966 + 100.0 * 8.348845481872559
Epoch 2020, val loss: 0.5489214062690735
Epoch 2030, training loss: 835.3726196289062 = 0.5137032270431519 + 100.0 * 8.348588943481445
Epoch 2030, val loss: 0.5469837784767151
Epoch 2040, training loss: 835.4066772460938 = 0.5114818215370178 + 100.0 * 8.348952293395996
Epoch 2040, val loss: 0.5450481176376343
Epoch 2050, training loss: 835.5401000976562 = 0.5091123580932617 + 100.0 * 8.350310325622559
Epoch 2050, val loss: 0.542897641658783
Epoch 2060, training loss: 835.4287109375 = 0.5067704319953918 + 100.0 * 8.34921932220459
Epoch 2060, val loss: 0.5408064723014832
Epoch 2070, training loss: 835.2869873046875 = 0.5046882033348083 + 100.0 * 8.347823143005371
Epoch 2070, val loss: 0.5389963984489441
Epoch 2080, training loss: 835.2698974609375 = 0.5026386976242065 + 100.0 * 8.347672462463379
Epoch 2080, val loss: 0.5372119545936584
Epoch 2090, training loss: 835.2740478515625 = 0.5005982518196106 + 100.0 * 8.347734451293945
Epoch 2090, val loss: 0.5354622006416321
Epoch 2100, training loss: 835.6258544921875 = 0.4984702169895172 + 100.0 * 8.351273536682129
Epoch 2100, val loss: 0.5336293578147888
Epoch 2110, training loss: 835.3097534179688 = 0.4963013529777527 + 100.0 * 8.348134994506836
Epoch 2110, val loss: 0.5315524935722351
Epoch 2120, training loss: 835.2024536132812 = 0.49430978298187256 + 100.0 * 8.347081184387207
Epoch 2120, val loss: 0.5299596786499023
Epoch 2130, training loss: 835.1513061523438 = 0.49237582087516785 + 100.0 * 8.346589088439941
Epoch 2130, val loss: 0.5281561017036438
Epoch 2140, training loss: 835.1112670898438 = 0.49046072363853455 + 100.0 * 8.346207618713379
Epoch 2140, val loss: 0.5265572667121887
Epoch 2150, training loss: 835.1096801757812 = 0.48855265974998474 + 100.0 * 8.346211433410645
Epoch 2150, val loss: 0.5249035358428955
Epoch 2160, training loss: 835.7495727539062 = 0.48652952909469604 + 100.0 * 8.352630615234375
Epoch 2160, val loss: 0.5231429934501648
Epoch 2170, training loss: 835.1589965820312 = 0.48443326354026794 + 100.0 * 8.346745491027832
Epoch 2170, val loss: 0.5212036967277527
Epoch 2180, training loss: 835.038330078125 = 0.48255690932273865 + 100.0 * 8.345558166503906
Epoch 2180, val loss: 0.5195723176002502
Epoch 2190, training loss: 835.0193481445312 = 0.48077091574668884 + 100.0 * 8.345385551452637
Epoch 2190, val loss: 0.5180670022964478
Epoch 2200, training loss: 835.0069580078125 = 0.4790216386318207 + 100.0 * 8.345279693603516
Epoch 2200, val loss: 0.516531229019165
Epoch 2210, training loss: 835.384521484375 = 0.4771950840950012 + 100.0 * 8.34907341003418
Epoch 2210, val loss: 0.5149676203727722
Epoch 2220, training loss: 835.0250244140625 = 0.4752452075481415 + 100.0 * 8.345498085021973
Epoch 2220, val loss: 0.5132123231887817
Epoch 2230, training loss: 834.9329833984375 = 0.47354593873023987 + 100.0 * 8.34459400177002
Epoch 2230, val loss: 0.5117608904838562
Epoch 2240, training loss: 834.9022827148438 = 0.4718342423439026 + 100.0 * 8.344304084777832
Epoch 2240, val loss: 0.5103011727333069
Epoch 2250, training loss: 834.8655395507812 = 0.4701891541481018 + 100.0 * 8.343953132629395
Epoch 2250, val loss: 0.5088559985160828
Epoch 2260, training loss: 834.8401489257812 = 0.4685265123844147 + 100.0 * 8.343716621398926
Epoch 2260, val loss: 0.5073916912078857
Epoch 2270, training loss: 834.8535766601562 = 0.4668775498867035 + 100.0 * 8.343867301940918
Epoch 2270, val loss: 0.505908191204071
Epoch 2280, training loss: 835.3572998046875 = 0.4651661813259125 + 100.0 * 8.348921775817871
Epoch 2280, val loss: 0.5042418241500854
Epoch 2290, training loss: 834.9573364257812 = 0.4632996916770935 + 100.0 * 8.344940185546875
Epoch 2290, val loss: 0.5029644966125488
Epoch 2300, training loss: 834.8038940429688 = 0.4617057144641876 + 100.0 * 8.343421936035156
Epoch 2300, val loss: 0.5014468431472778
Epoch 2310, training loss: 834.7510375976562 = 0.4601401388645172 + 100.0 * 8.34290885925293
Epoch 2310, val loss: 0.5001457333564758
Epoch 2320, training loss: 834.9035034179688 = 0.45863547921180725 + 100.0 * 8.344449043273926
Epoch 2320, val loss: 0.49881595373153687
Epoch 2330, training loss: 834.7192993164062 = 0.45692533254623413 + 100.0 * 8.342623710632324
Epoch 2330, val loss: 0.4973171651363373
Epoch 2340, training loss: 834.6864624023438 = 0.45536327362060547 + 100.0 * 8.342310905456543
Epoch 2340, val loss: 0.4960346519947052
Epoch 2350, training loss: 834.6660766601562 = 0.45388540625572205 + 100.0 * 8.342122077941895
Epoch 2350, val loss: 0.49471282958984375
Epoch 2360, training loss: 834.6458740234375 = 0.4524388313293457 + 100.0 * 8.341934204101562
Epoch 2360, val loss: 0.49350205063819885
Epoch 2370, training loss: 834.7789916992188 = 0.45098525285720825 + 100.0 * 8.343279838562012
Epoch 2370, val loss: 0.492184042930603
Epoch 2380, training loss: 834.7264404296875 = 0.4494342505931854 + 100.0 * 8.342769622802734
Epoch 2380, val loss: 0.4908314645290375
Epoch 2390, training loss: 834.6419677734375 = 0.4479040801525116 + 100.0 * 8.341940879821777
Epoch 2390, val loss: 0.48963120579719543
Epoch 2400, training loss: 834.5930786132812 = 0.4464816451072693 + 100.0 * 8.341465950012207
Epoch 2400, val loss: 0.4883647859096527
Epoch 2410, training loss: 834.5515747070312 = 0.44514355063438416 + 100.0 * 8.341064453125
Epoch 2410, val loss: 0.48725560307502747
Epoch 2420, training loss: 834.51611328125 = 0.44381096959114075 + 100.0 * 8.340723037719727
Epoch 2420, val loss: 0.48608535528182983
Epoch 2430, training loss: 834.4930419921875 = 0.4424901604652405 + 100.0 * 8.340505599975586
Epoch 2430, val loss: 0.4849769175052643
Epoch 2440, training loss: 834.5182495117188 = 0.44118788838386536 + 100.0 * 8.340770721435547
Epoch 2440, val loss: 0.48381009697914124
Epoch 2450, training loss: 834.9256591796875 = 0.43981853127479553 + 100.0 * 8.344858169555664
Epoch 2450, val loss: 0.4825228154659271
Epoch 2460, training loss: 834.5352783203125 = 0.438275009393692 + 100.0 * 8.340970039367676
Epoch 2460, val loss: 0.4814737141132355
Epoch 2470, training loss: 834.5509643554688 = 0.4369869530200958 + 100.0 * 8.341139793395996
Epoch 2470, val loss: 0.48040521144866943
Epoch 2480, training loss: 834.580078125 = 0.43560266494750977 + 100.0 * 8.341444969177246
Epoch 2480, val loss: 0.47912395000457764
Epoch 2490, training loss: 834.4453735351562 = 0.4342855215072632 + 100.0 * 8.340110778808594
Epoch 2490, val loss: 0.47801071405410767
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8274987316083207
0.8636528290951243
=== training gcn model ===
Epoch 0, training loss: 1059.3214111328125 = 1.0946613550186157 + 100.0 * 10.582267761230469
Epoch 0, val loss: 1.0947062969207764
Epoch 10, training loss: 1059.2763671875 = 1.090903401374817 + 100.0 * 10.581854820251465
Epoch 10, val loss: 1.0909481048583984
Epoch 20, training loss: 1059.1162109375 = 1.0868418216705322 + 100.0 * 10.580293655395508
Epoch 20, val loss: 1.0868935585021973
Epoch 30, training loss: 1058.46826171875 = 1.0823897123336792 + 100.0 * 10.573859214782715
Epoch 30, val loss: 1.0824424028396606
Epoch 40, training loss: 1055.7623291015625 = 1.0772738456726074 + 100.0 * 10.546850204467773
Epoch 40, val loss: 1.077286958694458
Epoch 50, training loss: 1044.794677734375 = 1.0711538791656494 + 100.0 * 10.437234878540039
Epoch 50, val loss: 1.0711053609848022
Epoch 60, training loss: 1011.3660278320312 = 1.063868761062622 + 100.0 * 10.103021621704102
Epoch 60, val loss: 1.063766598701477
Epoch 70, training loss: 971.503662109375 = 1.056044340133667 + 100.0 * 9.704476356506348
Epoch 70, val loss: 1.0561496019363403
Epoch 80, training loss: 953.3672485351562 = 1.0504096746444702 + 100.0 * 9.523168563842773
Epoch 80, val loss: 1.0507844686508179
Epoch 90, training loss: 936.4342041015625 = 1.0467500686645508 + 100.0 * 9.353874206542969
Epoch 90, val loss: 1.0472320318222046
Epoch 100, training loss: 926.0104370117188 = 1.0441603660583496 + 100.0 * 9.249662399291992
Epoch 100, val loss: 1.0446537733078003
Epoch 110, training loss: 923.239013671875 = 1.0414842367172241 + 100.0 * 9.221975326538086
Epoch 110, val loss: 1.0419570207595825
Epoch 120, training loss: 919.5958251953125 = 1.038598895072937 + 100.0 * 9.185572624206543
Epoch 120, val loss: 1.039113163948059
Epoch 130, training loss: 914.1325073242188 = 1.0363773107528687 + 100.0 * 9.130961418151855
Epoch 130, val loss: 1.0369926691055298
Epoch 140, training loss: 906.35205078125 = 1.0351710319519043 + 100.0 * 9.053169250488281
Epoch 140, val loss: 1.035904049873352
Epoch 150, training loss: 898.9987182617188 = 1.0346999168395996 + 100.0 * 8.979640007019043
Epoch 150, val loss: 1.0355013608932495
Epoch 160, training loss: 894.6458740234375 = 1.0342304706573486 + 100.0 * 8.936116218566895
Epoch 160, val loss: 1.0350066423416138
Epoch 170, training loss: 890.2146606445312 = 1.0335049629211426 + 100.0 * 8.89181137084961
Epoch 170, val loss: 1.0342814922332764
Epoch 180, training loss: 884.6344604492188 = 1.0331933498382568 + 100.0 * 8.836012840270996
Epoch 180, val loss: 1.0340458154678345
Epoch 190, training loss: 879.0797119140625 = 1.0336898565292358 + 100.0 * 8.780460357666016
Epoch 190, val loss: 1.0345942974090576
Epoch 200, training loss: 875.4346923828125 = 1.0340502262115479 + 100.0 * 8.744006156921387
Epoch 200, val loss: 1.0349104404449463
Epoch 210, training loss: 873.0275268554688 = 1.0336912870407104 + 100.0 * 8.719938278198242
Epoch 210, val loss: 1.0344784259796143
Epoch 220, training loss: 870.948974609375 = 1.0329104661941528 + 100.0 * 8.6991605758667
Epoch 220, val loss: 1.0336939096450806
Epoch 230, training loss: 868.4437255859375 = 1.0322935581207275 + 100.0 * 8.674114227294922
Epoch 230, val loss: 1.0331425666809082
Epoch 240, training loss: 865.7173461914062 = 1.0321738719940186 + 100.0 * 8.646851539611816
Epoch 240, val loss: 1.0330967903137207
Epoch 250, training loss: 862.8992309570312 = 1.0323095321655273 + 100.0 * 8.618669509887695
Epoch 250, val loss: 1.0332181453704834
Epoch 260, training loss: 860.8614501953125 = 1.0322896242141724 + 100.0 * 8.598291397094727
Epoch 260, val loss: 1.0331645011901855
Epoch 270, training loss: 859.4160766601562 = 1.0319048166275024 + 100.0 * 8.583841323852539
Epoch 270, val loss: 1.032728910446167
Epoch 280, training loss: 858.424560546875 = 1.0311989784240723 + 100.0 * 8.573933601379395
Epoch 280, val loss: 1.0320053100585938
Epoch 290, training loss: 857.5995483398438 = 1.030359148979187 + 100.0 * 8.565691947937012
Epoch 290, val loss: 1.031177282333374
Epoch 300, training loss: 856.812744140625 = 1.0295430421829224 + 100.0 * 8.557831764221191
Epoch 300, val loss: 1.0303562879562378
Epoch 310, training loss: 855.9783325195312 = 1.0287467241287231 + 100.0 * 8.549495697021484
Epoch 310, val loss: 1.0295801162719727
Epoch 320, training loss: 855.1404418945312 = 1.0279967784881592 + 100.0 * 8.54112434387207
Epoch 320, val loss: 1.028835654258728
Epoch 330, training loss: 854.39501953125 = 1.027268409729004 + 100.0 * 8.533677101135254
Epoch 330, val loss: 1.0280934572219849
Epoch 340, training loss: 853.62255859375 = 1.0264625549316406 + 100.0 * 8.525960922241211
Epoch 340, val loss: 1.0273267030715942
Epoch 350, training loss: 852.9171142578125 = 1.025602102279663 + 100.0 * 8.518915176391602
Epoch 350, val loss: 1.0264873504638672
Epoch 360, training loss: 852.3466186523438 = 1.0246788263320923 + 100.0 * 8.513219833374023
Epoch 360, val loss: 1.0255799293518066
Epoch 370, training loss: 851.8617553710938 = 1.023658037185669 + 100.0 * 8.508380889892578
Epoch 370, val loss: 1.0245871543884277
Epoch 380, training loss: 851.4871826171875 = 1.022595763206482 + 100.0 * 8.504646301269531
Epoch 380, val loss: 1.0235451459884644
Epoch 390, training loss: 851.10986328125 = 1.021514892578125 + 100.0 * 8.500883102416992
Epoch 390, val loss: 1.0224990844726562
Epoch 400, training loss: 850.8931884765625 = 1.02040433883667 + 100.0 * 8.498727798461914
Epoch 400, val loss: 1.0214192867279053
Epoch 410, training loss: 850.4465942382812 = 1.019274353981018 + 100.0 * 8.49427318572998
Epoch 410, val loss: 1.0203341245651245
Epoch 420, training loss: 850.0531616210938 = 1.0181922912597656 + 100.0 * 8.490349769592285
Epoch 420, val loss: 1.0192806720733643
Epoch 430, training loss: 849.5881958007812 = 1.017136573791504 + 100.0 * 8.485710144042969
Epoch 430, val loss: 1.0182379484176636
Epoch 440, training loss: 849.2542114257812 = 1.0161030292510986 + 100.0 * 8.482380867004395
Epoch 440, val loss: 1.0172165632247925
Epoch 450, training loss: 848.9427490234375 = 1.0149906873703003 + 100.0 * 8.479277610778809
Epoch 450, val loss: 1.0161443948745728
Epoch 460, training loss: 848.4246215820312 = 1.0138905048370361 + 100.0 * 8.474106788635254
Epoch 460, val loss: 1.015076994895935
Epoch 470, training loss: 847.9926147460938 = 1.0128439664840698 + 100.0 * 8.46979808807373
Epoch 470, val loss: 1.0140568017959595
Epoch 480, training loss: 847.60888671875 = 1.0117976665496826 + 100.0 * 8.465970993041992
Epoch 480, val loss: 1.0130342245101929
Epoch 490, training loss: 847.3610229492188 = 1.0107299089431763 + 100.0 * 8.463502883911133
Epoch 490, val loss: 1.011960506439209
Epoch 500, training loss: 846.9379272460938 = 1.0095075368881226 + 100.0 * 8.459283828735352
Epoch 500, val loss: 1.0107923746109009
Epoch 510, training loss: 846.6267700195312 = 1.0083435773849487 + 100.0 * 8.456184387207031
Epoch 510, val loss: 1.009657621383667
Epoch 520, training loss: 846.2462768554688 = 1.0071947574615479 + 100.0 * 8.452390670776367
Epoch 520, val loss: 1.0085382461547852
Epoch 530, training loss: 846.0056762695312 = 1.0059938430786133 + 100.0 * 8.449996948242188
Epoch 530, val loss: 1.0073509216308594
Epoch 540, training loss: 845.7378540039062 = 1.004678726196289 + 100.0 * 8.447331428527832
Epoch 540, val loss: 1.0060805082321167
Epoch 550, training loss: 845.5859985351562 = 1.00333833694458 + 100.0 * 8.445826530456543
Epoch 550, val loss: 1.0047459602355957
Epoch 560, training loss: 845.2202758789062 = 1.001923680305481 + 100.0 * 8.442183494567871
Epoch 560, val loss: 1.0033479928970337
Epoch 570, training loss: 844.95703125 = 1.000478982925415 + 100.0 * 8.439565658569336
Epoch 570, val loss: 1.0019469261169434
Epoch 580, training loss: 844.7119140625 = 0.9990091919898987 + 100.0 * 8.437129020690918
Epoch 580, val loss: 1.0005043745040894
Epoch 590, training loss: 844.7928466796875 = 0.9974448680877686 + 100.0 * 8.43795394897461
Epoch 590, val loss: 0.9989752769470215
Epoch 600, training loss: 844.3311157226562 = 0.9956948161125183 + 100.0 * 8.433354377746582
Epoch 600, val loss: 0.9972696304321289
Epoch 610, training loss: 844.1239624023438 = 0.9939553737640381 + 100.0 * 8.431300163269043
Epoch 610, val loss: 0.9955547451972961
Epoch 620, training loss: 843.94189453125 = 0.9921247363090515 + 100.0 * 8.429497718811035
Epoch 620, val loss: 0.9937695860862732
Epoch 630, training loss: 843.7726440429688 = 0.9902093410491943 + 100.0 * 8.427824020385742
Epoch 630, val loss: 0.9918959140777588
Epoch 640, training loss: 843.8248901367188 = 0.9881976246833801 + 100.0 * 8.428366661071777
Epoch 640, val loss: 0.9899535179138184
Epoch 650, training loss: 843.590087890625 = 0.9860270023345947 + 100.0 * 8.426040649414062
Epoch 650, val loss: 0.9878100752830505
Epoch 660, training loss: 843.3383178710938 = 0.9838476777076721 + 100.0 * 8.423544883728027
Epoch 660, val loss: 0.9856933951377869
Epoch 670, training loss: 843.1820678710938 = 0.9816429018974304 + 100.0 * 8.422004699707031
Epoch 670, val loss: 0.9835547208786011
Epoch 680, training loss: 843.2244873046875 = 0.9793579578399658 + 100.0 * 8.42245101928711
Epoch 680, val loss: 0.9812951683998108
Epoch 690, training loss: 842.9730224609375 = 0.9769327640533447 + 100.0 * 8.419960975646973
Epoch 690, val loss: 0.9789924621582031
Epoch 700, training loss: 842.806640625 = 0.9745425581932068 + 100.0 * 8.418320655822754
Epoch 700, val loss: 0.976645290851593
Epoch 710, training loss: 842.6696166992188 = 0.9720785617828369 + 100.0 * 8.416975021362305
Epoch 710, val loss: 0.9742734432220459
Epoch 720, training loss: 842.9059448242188 = 0.9695560932159424 + 100.0 * 8.419363975524902
Epoch 720, val loss: 0.9717979431152344
Epoch 730, training loss: 842.4231567382812 = 0.9668532609939575 + 100.0 * 8.414563179016113
Epoch 730, val loss: 0.9691885709762573
Epoch 740, training loss: 842.3128051757812 = 0.9642225503921509 + 100.0 * 8.413485527038574
Epoch 740, val loss: 0.9666485786437988
Epoch 750, training loss: 842.195068359375 = 0.9615825414657593 + 100.0 * 8.412334442138672
Epoch 750, val loss: 0.9640772938728333
Epoch 760, training loss: 842.1080932617188 = 0.9589004516601562 + 100.0 * 8.411491394042969
Epoch 760, val loss: 0.9614549875259399
Epoch 770, training loss: 842.0529174804688 = 0.9560456871986389 + 100.0 * 8.410968780517578
Epoch 770, val loss: 0.9586721062660217
Epoch 780, training loss: 841.8679809570312 = 0.9531661868095398 + 100.0 * 8.409148216247559
Epoch 780, val loss: 0.9558714628219604
Epoch 790, training loss: 841.7080078125 = 0.9503555297851562 + 100.0 * 8.407576560974121
Epoch 790, val loss: 0.9531397223472595
Epoch 800, training loss: 841.581298828125 = 0.9475377798080444 + 100.0 * 8.40633773803711
Epoch 800, val loss: 0.9503914713859558
Epoch 810, training loss: 841.5936279296875 = 0.9446757435798645 + 100.0 * 8.406489372253418
Epoch 810, val loss: 0.9475496411323547
Epoch 820, training loss: 841.5798950195312 = 0.9415441155433655 + 100.0 * 8.406383514404297
Epoch 820, val loss: 0.9445810914039612
Epoch 830, training loss: 841.25341796875 = 0.9385255575180054 + 100.0 * 8.403148651123047
Epoch 830, val loss: 0.9416319131851196
Epoch 840, training loss: 841.1121215820312 = 0.9355899095535278 + 100.0 * 8.401764869689941
Epoch 840, val loss: 0.9387408494949341
Epoch 850, training loss: 841.0084838867188 = 0.9326066374778748 + 100.0 * 8.400758743286133
Epoch 850, val loss: 0.9358172416687012
Epoch 860, training loss: 841.330322265625 = 0.9295045137405396 + 100.0 * 8.404007911682129
Epoch 860, val loss: 0.932780921459198
Epoch 870, training loss: 840.813720703125 = 0.9261968731880188 + 100.0 * 8.39887523651123
Epoch 870, val loss: 0.9295365810394287
Epoch 880, training loss: 840.7363891601562 = 0.9230164885520935 + 100.0 * 8.398133277893066
Epoch 880, val loss: 0.9264326095581055
Epoch 890, training loss: 840.64306640625 = 0.9198638796806335 + 100.0 * 8.397232055664062
Epoch 890, val loss: 0.9233399033546448
Epoch 900, training loss: 840.589111328125 = 0.9166333079338074 + 100.0 * 8.396724700927734
Epoch 900, val loss: 0.9201584458351135
Epoch 910, training loss: 840.5053100585938 = 0.9132198691368103 + 100.0 * 8.395920753479004
Epoch 910, val loss: 0.9168150424957275
Epoch 920, training loss: 840.3810424804688 = 0.9097949266433716 + 100.0 * 8.394712448120117
Epoch 920, val loss: 0.9134809374809265
Epoch 930, training loss: 840.3169555664062 = 0.9064391255378723 + 100.0 * 8.394104957580566
Epoch 930, val loss: 0.9102309346199036
Epoch 940, training loss: 840.2266235351562 = 0.9030787348747253 + 100.0 * 8.393235206604004
Epoch 940, val loss: 0.9069088697433472
Epoch 950, training loss: 840.140380859375 = 0.899631679058075 + 100.0 * 8.392407417297363
Epoch 950, val loss: 0.9035617709159851
Epoch 960, training loss: 840.47998046875 = 0.8961355090141296 + 100.0 * 8.395838737487793
Epoch 960, val loss: 0.9001674056053162
Epoch 970, training loss: 840.0301513671875 = 0.8923434615135193 + 100.0 * 8.391378402709961
Epoch 970, val loss: 0.8964236974716187
Epoch 980, training loss: 839.9197998046875 = 0.888769805431366 + 100.0 * 8.390310287475586
Epoch 980, val loss: 0.8929439187049866
Epoch 990, training loss: 839.836669921875 = 0.8852721452713013 + 100.0 * 8.389513969421387
Epoch 990, val loss: 0.8895265460014343
Epoch 1000, training loss: 839.756103515625 = 0.8817511200904846 + 100.0 * 8.38874340057373
Epoch 1000, val loss: 0.886090874671936
Epoch 1010, training loss: 839.947265625 = 0.878150463104248 + 100.0 * 8.390690803527832
Epoch 1010, val loss: 0.8826386332511902
Epoch 1020, training loss: 839.8679809570312 = 0.8741607069969177 + 100.0 * 8.389938354492188
Epoch 1020, val loss: 0.8786088824272156
Epoch 1030, training loss: 839.6097412109375 = 0.870405375957489 + 100.0 * 8.3873929977417
Epoch 1030, val loss: 0.8749973177909851
Epoch 1040, training loss: 839.4760131835938 = 0.8668227195739746 + 100.0 * 8.386092185974121
Epoch 1040, val loss: 0.8715197443962097
Epoch 1050, training loss: 839.3709716796875 = 0.8632186651229858 + 100.0 * 8.385077476501465
Epoch 1050, val loss: 0.8679887652397156
Epoch 1060, training loss: 839.3090209960938 = 0.8595805764198303 + 100.0 * 8.38449478149414
Epoch 1060, val loss: 0.8644391298294067
Epoch 1070, training loss: 839.4109497070312 = 0.855753481388092 + 100.0 * 8.385551452636719
Epoch 1070, val loss: 0.8606894612312317
Epoch 1080, training loss: 839.1697387695312 = 0.8518661260604858 + 100.0 * 8.3831787109375
Epoch 1080, val loss: 0.8569222092628479
Epoch 1090, training loss: 839.077392578125 = 0.8481588959693909 + 100.0 * 8.382292747497559
Epoch 1090, val loss: 0.8533036708831787
Epoch 1100, training loss: 838.98486328125 = 0.8445109724998474 + 100.0 * 8.381403923034668
Epoch 1100, val loss: 0.8497469425201416
Epoch 1110, training loss: 839.2348022460938 = 0.8407641053199768 + 100.0 * 8.383940696716309
Epoch 1110, val loss: 0.8461449146270752
Epoch 1120, training loss: 839.0206909179688 = 0.8367930054664612 + 100.0 * 8.38183879852295
Epoch 1120, val loss: 0.842123806476593
Epoch 1130, training loss: 838.7698974609375 = 0.8329598307609558 + 100.0 * 8.379369735717773
Epoch 1130, val loss: 0.8384587168693542
Epoch 1140, training loss: 838.69873046875 = 0.8292626142501831 + 100.0 * 8.378694534301758
Epoch 1140, val loss: 0.8348670601844788
Epoch 1150, training loss: 838.6244506835938 = 0.8255564570426941 + 100.0 * 8.377988815307617
Epoch 1150, val loss: 0.8312172889709473
Epoch 1160, training loss: 838.5486450195312 = 0.8217778205871582 + 100.0 * 8.37726879119873
Epoch 1160, val loss: 0.8275493383407593
Epoch 1170, training loss: 838.4922485351562 = 0.8179892301559448 + 100.0 * 8.376742362976074
Epoch 1170, val loss: 0.8238320350646973
Epoch 1180, training loss: 838.8880615234375 = 0.8140205144882202 + 100.0 * 8.38074016571045
Epoch 1180, val loss: 0.8198658227920532
Epoch 1190, training loss: 838.450927734375 = 0.8097943067550659 + 100.0 * 8.376411437988281
Epoch 1190, val loss: 0.8158503174781799
Epoch 1200, training loss: 838.29345703125 = 0.8058766722679138 + 100.0 * 8.374876022338867
Epoch 1200, val loss: 0.8120930194854736
Epoch 1210, training loss: 838.2339477539062 = 0.8020650744438171 + 100.0 * 8.374319076538086
Epoch 1210, val loss: 0.8083692193031311
Epoch 1220, training loss: 838.1761474609375 = 0.7982426285743713 + 100.0 * 8.373779296875
Epoch 1220, val loss: 0.8046568036079407
Epoch 1230, training loss: 838.2943115234375 = 0.7943334579467773 + 100.0 * 8.375
Epoch 1230, val loss: 0.8008302450180054
Epoch 1240, training loss: 838.1036376953125 = 0.7901908159255981 + 100.0 * 8.37313461303711
Epoch 1240, val loss: 0.7968288064002991
Epoch 1250, training loss: 838.0484008789062 = 0.7862371802330017 + 100.0 * 8.372621536254883
Epoch 1250, val loss: 0.7929960489273071
Epoch 1260, training loss: 838.0107421875 = 0.7823667526245117 + 100.0 * 8.372283935546875
Epoch 1260, val loss: 0.7892168164253235
Epoch 1270, training loss: 837.9658203125 = 0.7782931327819824 + 100.0 * 8.371874809265137
Epoch 1270, val loss: 0.7852359414100647
Epoch 1280, training loss: 837.8712158203125 = 0.7742295861244202 + 100.0 * 8.370969772338867
Epoch 1280, val loss: 0.7813649773597717
Epoch 1290, training loss: 837.7943115234375 = 0.7702898979187012 + 100.0 * 8.370240211486816
Epoch 1290, val loss: 0.7775722146034241
Epoch 1300, training loss: 837.7325439453125 = 0.7664169073104858 + 100.0 * 8.369661331176758
Epoch 1300, val loss: 0.7738227248191833
Epoch 1310, training loss: 837.6921997070312 = 0.7625112533569336 + 100.0 * 8.36929702758789
Epoch 1310, val loss: 0.7700374126434326
Epoch 1320, training loss: 837.8726196289062 = 0.7585315704345703 + 100.0 * 8.371140480041504
Epoch 1320, val loss: 0.766158401966095
Epoch 1330, training loss: 837.9298095703125 = 0.7543244361877441 + 100.0 * 8.37175464630127
Epoch 1330, val loss: 0.7622624635696411
Epoch 1340, training loss: 837.5989379882812 = 0.7501804828643799 + 100.0 * 8.368487358093262
Epoch 1340, val loss: 0.7581030130386353
Epoch 1350, training loss: 837.5109252929688 = 0.746229887008667 + 100.0 * 8.367647171020508
Epoch 1350, val loss: 0.7544083595275879
Epoch 1360, training loss: 837.4419555664062 = 0.7423892617225647 + 100.0 * 8.366995811462402
Epoch 1360, val loss: 0.7506944537162781
Epoch 1370, training loss: 837.3842163085938 = 0.7385250329971313 + 100.0 * 8.366456985473633
Epoch 1370, val loss: 0.74700528383255
Epoch 1380, training loss: 837.335205078125 = 0.7346445918083191 + 100.0 * 8.366005897521973
Epoch 1380, val loss: 0.7432841062545776
Epoch 1390, training loss: 837.3098754882812 = 0.7307505011558533 + 100.0 * 8.365791320800781
Epoch 1390, val loss: 0.739565908908844
Epoch 1400, training loss: 837.5870971679688 = 0.7267208695411682 + 100.0 * 8.368603706359863
Epoch 1400, val loss: 0.7357263565063477
Epoch 1410, training loss: 837.2944946289062 = 0.7225649952888489 + 100.0 * 8.365718841552734
Epoch 1410, val loss: 0.7317124009132385
Epoch 1420, training loss: 837.16748046875 = 0.7186919450759888 + 100.0 * 8.364487648010254
Epoch 1420, val loss: 0.7280145883560181
Epoch 1430, training loss: 837.2250366210938 = 0.7148570418357849 + 100.0 * 8.36510181427002
Epoch 1430, val loss: 0.7243379950523376
Epoch 1440, training loss: 837.1571655273438 = 0.7108632326126099 + 100.0 * 8.364462852478027
Epoch 1440, val loss: 0.7205292582511902
Epoch 1450, training loss: 837.0609741210938 = 0.7069186568260193 + 100.0 * 8.363540649414062
Epoch 1450, val loss: 0.7168590426445007
Epoch 1460, training loss: 837.0217895507812 = 0.7031793594360352 + 100.0 * 8.36318588256836
Epoch 1460, val loss: 0.7132892608642578
Epoch 1470, training loss: 837.1213989257812 = 0.6994187831878662 + 100.0 * 8.364219665527344
Epoch 1470, val loss: 0.7098020315170288
Epoch 1480, training loss: 836.9384765625 = 0.695491373538971 + 100.0 * 8.36242961883545
Epoch 1480, val loss: 0.7059175968170166
Epoch 1490, training loss: 836.894775390625 = 0.6916785836219788 + 100.0 * 8.362030982971191
Epoch 1490, val loss: 0.702421247959137
Epoch 1500, training loss: 836.8401489257812 = 0.6880636215209961 + 100.0 * 8.361520767211914
Epoch 1500, val loss: 0.6989302635192871
Epoch 1510, training loss: 836.8013305664062 = 0.6844269633293152 + 100.0 * 8.36116886138916
Epoch 1510, val loss: 0.6955268383026123
Epoch 1520, training loss: 836.9555053710938 = 0.6807576417922974 + 100.0 * 8.362747192382812
Epoch 1520, val loss: 0.692041277885437
Epoch 1530, training loss: 836.7973022460938 = 0.6769733428955078 + 100.0 * 8.36120319366455
Epoch 1530, val loss: 0.6884595155715942
Epoch 1540, training loss: 836.72998046875 = 0.673334002494812 + 100.0 * 8.360566139221191
Epoch 1540, val loss: 0.6850318908691406
Epoch 1550, training loss: 836.65966796875 = 0.6697733402252197 + 100.0 * 8.359898567199707
Epoch 1550, val loss: 0.681656002998352
Epoch 1560, training loss: 836.6110229492188 = 0.6662540435791016 + 100.0 * 8.359447479248047
Epoch 1560, val loss: 0.678301990032196
Epoch 1570, training loss: 836.7633666992188 = 0.6627370715141296 + 100.0 * 8.361006736755371
Epoch 1570, val loss: 0.6748812198638916
Epoch 1580, training loss: 836.6266479492188 = 0.6589156985282898 + 100.0 * 8.3596773147583
Epoch 1580, val loss: 0.6713998317718506
Epoch 1590, training loss: 836.514892578125 = 0.655360996723175 + 100.0 * 8.358595848083496
Epoch 1590, val loss: 0.6680321097373962
Epoch 1600, training loss: 836.4326782226562 = 0.6520014405250549 + 100.0 * 8.357807159423828
Epoch 1600, val loss: 0.6649294495582581
Epoch 1610, training loss: 836.3820190429688 = 0.6486741304397583 + 100.0 * 8.357333183288574
Epoch 1610, val loss: 0.6617516875267029
Epoch 1620, training loss: 836.3543701171875 = 0.6453623175621033 + 100.0 * 8.35708999633789
Epoch 1620, val loss: 0.6586587429046631
Epoch 1630, training loss: 836.8065185546875 = 0.6419571042060852 + 100.0 * 8.361645698547363
Epoch 1630, val loss: 0.6553605198860168
Epoch 1640, training loss: 836.4317626953125 = 0.6382508277893066 + 100.0 * 8.357934951782227
Epoch 1640, val loss: 0.6519898772239685
Epoch 1650, training loss: 836.231201171875 = 0.6349456310272217 + 100.0 * 8.355962753295898
Epoch 1650, val loss: 0.6488847136497498
Epoch 1660, training loss: 836.2054443359375 = 0.6317674517631531 + 100.0 * 8.35573673248291
Epoch 1660, val loss: 0.6459406614303589
Epoch 1670, training loss: 836.1663208007812 = 0.6285925507545471 + 100.0 * 8.355377197265625
Epoch 1670, val loss: 0.6429736018180847
Epoch 1680, training loss: 836.5255126953125 = 0.6253350973129272 + 100.0 * 8.359002113342285
Epoch 1680, val loss: 0.6400177478790283
Epoch 1690, training loss: 836.2713012695312 = 0.6218389868736267 + 100.0 * 8.356494903564453
Epoch 1690, val loss: 0.6365979909896851
Epoch 1700, training loss: 836.065185546875 = 0.6186482906341553 + 100.0 * 8.35446548461914
Epoch 1700, val loss: 0.6336547136306763
Epoch 1710, training loss: 836.0216064453125 = 0.6156240105628967 + 100.0 * 8.354060173034668
Epoch 1710, val loss: 0.6308676600456238
Epoch 1720, training loss: 835.974609375 = 0.6126455664634705 + 100.0 * 8.353619575500488
Epoch 1720, val loss: 0.6280879378318787
Epoch 1730, training loss: 835.93798828125 = 0.609643816947937 + 100.0 * 8.353283882141113
Epoch 1730, val loss: 0.6253137588500977
Epoch 1740, training loss: 836.0297241210938 = 0.6066179871559143 + 100.0 * 8.354230880737305
Epoch 1740, val loss: 0.6225995421409607
Epoch 1750, training loss: 835.8849487304688 = 0.6033848524093628 + 100.0 * 8.352815628051758
Epoch 1750, val loss: 0.6194247603416443
Epoch 1760, training loss: 835.8883666992188 = 0.6003145575523376 + 100.0 * 8.352880477905273
Epoch 1760, val loss: 0.6166542172431946
Epoch 1770, training loss: 835.980224609375 = 0.5973126292228699 + 100.0 * 8.353829383850098
Epoch 1770, val loss: 0.6137715578079224
Epoch 1780, training loss: 835.8231201171875 = 0.5943773984909058 + 100.0 * 8.352287292480469
Epoch 1780, val loss: 0.6110637784004211
Epoch 1790, training loss: 835.7471313476562 = 0.5915625095367432 + 100.0 * 8.351555824279785
Epoch 1790, val loss: 0.608490526676178
Epoch 1800, training loss: 835.6984252929688 = 0.5888095498085022 + 100.0 * 8.351096153259277
Epoch 1800, val loss: 0.605922520160675
Epoch 1810, training loss: 835.676025390625 = 0.5860389471054077 + 100.0 * 8.350899696350098
Epoch 1810, val loss: 0.6033326983451843
Epoch 1820, training loss: 836.06201171875 = 0.5832392573356628 + 100.0 * 8.354787826538086
Epoch 1820, val loss: 0.6005359292030334
Epoch 1830, training loss: 835.7581176757812 = 0.5801122784614563 + 100.0 * 8.35177993774414
Epoch 1830, val loss: 0.5980224013328552
Epoch 1840, training loss: 835.569580078125 = 0.5773826241493225 + 100.0 * 8.349922180175781
Epoch 1840, val loss: 0.5954701900482178
Epoch 1850, training loss: 835.5610961914062 = 0.5747725367546082 + 100.0 * 8.349863052368164
Epoch 1850, val loss: 0.592969536781311
Epoch 1860, training loss: 835.6450805664062 = 0.5721773505210876 + 100.0 * 8.350728988647461
Epoch 1860, val loss: 0.5906201004981995
Epoch 1870, training loss: 835.484375 = 0.5693451166152954 + 100.0 * 8.349150657653809
Epoch 1870, val loss: 0.5880876779556274
Epoch 1880, training loss: 835.4425659179688 = 0.5667063593864441 + 100.0 * 8.348758697509766
Epoch 1880, val loss: 0.5856420397758484
Epoch 1890, training loss: 835.4129028320312 = 0.5642174482345581 + 100.0 * 8.34848690032959
Epoch 1890, val loss: 0.5833615064620972
Epoch 1900, training loss: 835.3782348632812 = 0.561781644821167 + 100.0 * 8.348164558410645
Epoch 1900, val loss: 0.5811845064163208
Epoch 1910, training loss: 835.5070190429688 = 0.5593084692955017 + 100.0 * 8.34947681427002
Epoch 1910, val loss: 0.5790367722511292
Epoch 1920, training loss: 835.3457641601562 = 0.5565599203109741 + 100.0 * 8.347891807556152
Epoch 1920, val loss: 0.5762830972671509
Epoch 1930, training loss: 835.4064331054688 = 0.5539518594741821 + 100.0 * 8.348525047302246
Epoch 1930, val loss: 0.5741398930549622
Epoch 1940, training loss: 835.2593383789062 = 0.5516683459281921 + 100.0 * 8.347076416015625
Epoch 1940, val loss: 0.5719402432441711
Epoch 1950, training loss: 835.2322998046875 = 0.5494334101676941 + 100.0 * 8.34682846069336
Epoch 1950, val loss: 0.5699043869972229
Epoch 1960, training loss: 835.2237548828125 = 0.5471779108047485 + 100.0 * 8.346765518188477
Epoch 1960, val loss: 0.5679107308387756
Epoch 1970, training loss: 835.460205078125 = 0.544845700263977 + 100.0 * 8.349153518676758
Epoch 1970, val loss: 0.5657116770744324
Epoch 1980, training loss: 835.22314453125 = 0.5424215793609619 + 100.0 * 8.346807479858398
Epoch 1980, val loss: 0.5636618733406067
Epoch 1990, training loss: 835.1166381835938 = 0.5402287840843201 + 100.0 * 8.34576416015625
Epoch 1990, val loss: 0.5615761876106262
Epoch 2000, training loss: 835.0703735351562 = 0.5380935668945312 + 100.0 * 8.345322608947754
Epoch 2000, val loss: 0.559731662273407
Epoch 2010, training loss: 835.05615234375 = 0.5359818339347839 + 100.0 * 8.34520149230957
Epoch 2010, val loss: 0.5577524900436401
Epoch 2020, training loss: 835.5255737304688 = 0.5337511301040649 + 100.0 * 8.349918365478516
Epoch 2020, val loss: 0.5556567907333374
Epoch 2030, training loss: 835.0804443359375 = 0.5313714742660522 + 100.0 * 8.345490455627441
Epoch 2030, val loss: 0.5535665154457092
Epoch 2040, training loss: 834.9978637695312 = 0.5292660593986511 + 100.0 * 8.344686508178711
Epoch 2040, val loss: 0.5516719222068787
Epoch 2050, training loss: 834.9374389648438 = 0.5272344946861267 + 100.0 * 8.344101905822754
Epoch 2050, val loss: 0.5498655438423157
Epoch 2060, training loss: 834.8736572265625 = 0.5252580642700195 + 100.0 * 8.343483924865723
Epoch 2060, val loss: 0.5481427907943726
Epoch 2070, training loss: 834.842529296875 = 0.5233092308044434 + 100.0 * 8.343192100524902
Epoch 2070, val loss: 0.5464206337928772
Epoch 2080, training loss: 834.9365844726562 = 0.521333634853363 + 100.0 * 8.344152450561523
Epoch 2080, val loss: 0.5447947382926941
Epoch 2090, training loss: 834.8433837890625 = 0.519239604473114 + 100.0 * 8.343241691589355
Epoch 2090, val loss: 0.542633056640625
Epoch 2100, training loss: 834.916748046875 = 0.5171582102775574 + 100.0 * 8.343996047973633
Epoch 2100, val loss: 0.5409337878227234
Epoch 2110, training loss: 834.818359375 = 0.5152494311332703 + 100.0 * 8.34303092956543
Epoch 2110, val loss: 0.539085328578949
Epoch 2120, training loss: 834.7593383789062 = 0.5133554339408875 + 100.0 * 8.342459678649902
Epoch 2120, val loss: 0.5373280644416809
Epoch 2130, training loss: 834.6643676757812 = 0.5114989280700684 + 100.0 * 8.34152889251709
Epoch 2130, val loss: 0.5358323454856873
Epoch 2140, training loss: 834.634765625 = 0.5097347497940063 + 100.0 * 8.3412504196167
Epoch 2140, val loss: 0.5342839956283569
Epoch 2150, training loss: 834.652099609375 = 0.5079748034477234 + 100.0 * 8.34144115447998
Epoch 2150, val loss: 0.5327540636062622
Epoch 2160, training loss: 834.8397827148438 = 0.5060874819755554 + 100.0 * 8.343337059020996
Epoch 2160, val loss: 0.5311828255653381
Epoch 2170, training loss: 834.7687377929688 = 0.5041289925575256 + 100.0 * 8.342645645141602
Epoch 2170, val loss: 0.5293465256690979
Epoch 2180, training loss: 834.5696411132812 = 0.5023555159568787 + 100.0 * 8.340672492980957
Epoch 2180, val loss: 0.5277340412139893
Epoch 2190, training loss: 834.4872436523438 = 0.5006745457649231 + 100.0 * 8.339865684509277
Epoch 2190, val loss: 0.5262404680252075
Epoch 2200, training loss: 834.4722290039062 = 0.4990259110927582 + 100.0 * 8.33973217010498
Epoch 2200, val loss: 0.5248871445655823
Epoch 2210, training loss: 834.623046875 = 0.49734216928482056 + 100.0 * 8.341257095336914
Epoch 2210, val loss: 0.5234687328338623
Epoch 2220, training loss: 834.4229736328125 = 0.49551424384117126 + 100.0 * 8.339274406433105
Epoch 2220, val loss: 0.5216693878173828
Epoch 2230, training loss: 834.3841552734375 = 0.4938098192214966 + 100.0 * 8.338903427124023
Epoch 2230, val loss: 0.5202233195304871
Epoch 2240, training loss: 834.3574829101562 = 0.49222445487976074 + 100.0 * 8.338652610778809
Epoch 2240, val loss: 0.5188667178153992
Epoch 2250, training loss: 834.3506469726562 = 0.49066054821014404 + 100.0 * 8.338600158691406
Epoch 2250, val loss: 0.5174466371536255
Epoch 2260, training loss: 834.5975341796875 = 0.4890563189983368 + 100.0 * 8.341084480285645
Epoch 2260, val loss: 0.5159207582473755
Epoch 2270, training loss: 834.4505004882812 = 0.4873291850090027 + 100.0 * 8.339632034301758
Epoch 2270, val loss: 0.5145624279975891
Epoch 2280, training loss: 834.3124389648438 = 0.4856949746608734 + 100.0 * 8.33826732635498
Epoch 2280, val loss: 0.5130828022956848
Epoch 2290, training loss: 834.2556762695312 = 0.4841791093349457 + 100.0 * 8.337715148925781
Epoch 2290, val loss: 0.511962890625
Epoch 2300, training loss: 834.2293701171875 = 0.48269781470298767 + 100.0 * 8.3374662399292
Epoch 2300, val loss: 0.510658860206604
Epoch 2310, training loss: 834.2301025390625 = 0.48121392726898193 + 100.0 * 8.337489128112793
Epoch 2310, val loss: 0.5094484090805054
Epoch 2320, training loss: 834.2916259765625 = 0.47968101501464844 + 100.0 * 8.338119506835938
Epoch 2320, val loss: 0.5081594586372375
Epoch 2330, training loss: 834.1328125 = 0.4781676232814789 + 100.0 * 8.336546897888184
Epoch 2330, val loss: 0.506679892539978
Epoch 2340, training loss: 834.1493530273438 = 0.4767453670501709 + 100.0 * 8.336726188659668
Epoch 2340, val loss: 0.505369246006012
Epoch 2350, training loss: 834.2987670898438 = 0.4752841591835022 + 100.0 * 8.338234901428223
Epoch 2350, val loss: 0.5040391087532043
Epoch 2360, training loss: 834.1441040039062 = 0.473600834608078 + 100.0 * 8.336705207824707
Epoch 2360, val loss: 0.5028906464576721
Epoch 2370, training loss: 834.1021728515625 = 0.4721342623233795 + 100.0 * 8.33630084991455
Epoch 2370, val loss: 0.501349687576294
Epoch 2380, training loss: 834.0565185546875 = 0.47075116634368896 + 100.0 * 8.335857391357422
Epoch 2380, val loss: 0.5004108548164368
Epoch 2390, training loss: 834.018798828125 = 0.4694157838821411 + 100.0 * 8.335494041442871
Epoch 2390, val loss: 0.49915772676467896
Epoch 2400, training loss: 834.0450439453125 = 0.4680744707584381 + 100.0 * 8.335769653320312
Epoch 2400, val loss: 0.49802929162979126
Epoch 2410, training loss: 834.1948852539062 = 0.4666486978530884 + 100.0 * 8.337282180786133
Epoch 2410, val loss: 0.49671512842178345
Epoch 2420, training loss: 833.9866943359375 = 0.46517249941825867 + 100.0 * 8.33521556854248
Epoch 2420, val loss: 0.49558505415916443
Epoch 2430, training loss: 833.9494018554688 = 0.4638681709766388 + 100.0 * 8.334855079650879
Epoch 2430, val loss: 0.4945546090602875
Epoch 2440, training loss: 834.0479125976562 = 0.46258071064949036 + 100.0 * 8.335853576660156
Epoch 2440, val loss: 0.49339568614959717
Epoch 2450, training loss: 833.89990234375 = 0.4612181782722473 + 100.0 * 8.334386825561523
Epoch 2450, val loss: 0.4922123849391937
Epoch 2460, training loss: 833.9284057617188 = 0.4599522650241852 + 100.0 * 8.334684371948242
Epoch 2460, val loss: 0.4910351037979126
Epoch 2470, training loss: 833.99462890625 = 0.45868605375289917 + 100.0 * 8.335359573364258
Epoch 2470, val loss: 0.48995763063430786
Epoch 2480, training loss: 833.83642578125 = 0.4573414921760559 + 100.0 * 8.33379077911377
Epoch 2480, val loss: 0.488943487405777
Epoch 2490, training loss: 833.843994140625 = 0.45610693097114563 + 100.0 * 8.333878517150879
Epoch 2490, val loss: 0.4880514442920685
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8224251648909182
0.8642324132434979
=== training gcn model ===
Epoch 0, training loss: 1059.3271484375 = 1.1054271459579468 + 100.0 * 10.582216262817383
Epoch 0, val loss: 1.1048731803894043
Epoch 10, training loss: 1059.261474609375 = 1.1008691787719727 + 100.0 * 10.5816068649292
Epoch 10, val loss: 1.1002542972564697
Epoch 20, training loss: 1059.0137939453125 = 1.0957775115966797 + 100.0 * 10.579179763793945
Epoch 20, val loss: 1.0950850248336792
Epoch 30, training loss: 1058.0140380859375 = 1.090031623840332 + 100.0 * 10.569239616394043
Epoch 30, val loss: 1.0892280340194702
Epoch 40, training loss: 1054.2781982421875 = 1.0835912227630615 + 100.0 * 10.531946182250977
Epoch 40, val loss: 1.0826729536056519
Epoch 50, training loss: 1042.1456298828125 = 1.0764204263687134 + 100.0 * 10.41069221496582
Epoch 50, val loss: 1.0753904581069946
Epoch 60, training loss: 1007.1974487304688 = 1.0688302516937256 + 100.0 * 10.061285972595215
Epoch 60, val loss: 1.0676965713500977
Epoch 70, training loss: 955.5784912109375 = 1.0600550174713135 + 100.0 * 9.545184135437012
Epoch 70, val loss: 1.0588237047195435
Epoch 80, training loss: 938.3465576171875 = 1.052546501159668 + 100.0 * 9.372940063476562
Epoch 80, val loss: 1.051777958869934
Epoch 90, training loss: 927.6544189453125 = 1.0479780435562134 + 100.0 * 9.266064643859863
Epoch 90, val loss: 1.0475788116455078
Epoch 100, training loss: 923.4948120117188 = 1.0442675352096558 + 100.0 * 9.224505424499512
Epoch 100, val loss: 1.0439237356185913
Epoch 110, training loss: 920.5033569335938 = 1.039991855621338 + 100.0 * 9.194633483886719
Epoch 110, val loss: 1.0396937131881714
Epoch 120, training loss: 916.7747192382812 = 1.0364505052566528 + 100.0 * 9.15738296508789
Epoch 120, val loss: 1.0363221168518066
Epoch 130, training loss: 911.24267578125 = 1.0343444347381592 + 100.0 * 9.102083206176758
Epoch 130, val loss: 1.0343401432037354
Epoch 140, training loss: 903.4058227539062 = 1.0334511995315552 + 100.0 * 9.023723602294922
Epoch 140, val loss: 1.0335054397583008
Epoch 150, training loss: 896.0713500976562 = 1.0333706140518188 + 100.0 * 8.950379371643066
Epoch 150, val loss: 1.0334256887435913
Epoch 160, training loss: 890.5982055664062 = 1.0335036516189575 + 100.0 * 8.895647048950195
Epoch 160, val loss: 1.0335508584976196
Epoch 170, training loss: 885.0157470703125 = 1.0336735248565674 + 100.0 * 8.839820861816406
Epoch 170, val loss: 1.0337653160095215
Epoch 180, training loss: 881.5534057617188 = 1.0339391231536865 + 100.0 * 8.805194854736328
Epoch 180, val loss: 1.0340349674224854
Epoch 190, training loss: 878.5418701171875 = 1.0337083339691162 + 100.0 * 8.775081634521484
Epoch 190, val loss: 1.0337809324264526
Epoch 200, training loss: 874.9130249023438 = 1.033480167388916 + 100.0 * 8.738795280456543
Epoch 200, val loss: 1.0335793495178223
Epoch 210, training loss: 871.8616943359375 = 1.0335626602172852 + 100.0 * 8.708281517028809
Epoch 210, val loss: 1.0337005853652954
Epoch 220, training loss: 869.1580200195312 = 1.0334393978118896 + 100.0 * 8.681245803833008
Epoch 220, val loss: 1.0336109399795532
Epoch 230, training loss: 866.5524291992188 = 1.0330913066864014 + 100.0 * 8.655193328857422
Epoch 230, val loss: 1.0333232879638672
Epoch 240, training loss: 864.5070190429688 = 1.0327539443969727 + 100.0 * 8.634742736816406
Epoch 240, val loss: 1.0330250263214111
Epoch 250, training loss: 862.7655029296875 = 1.0321685075759888 + 100.0 * 8.61733341217041
Epoch 250, val loss: 1.0324336290359497
Epoch 260, training loss: 861.305419921875 = 1.0313626527786255 + 100.0 * 8.602740287780762
Epoch 260, val loss: 1.0316388607025146
Epoch 270, training loss: 860.2747192382812 = 1.0306154489517212 + 100.0 * 8.592440605163574
Epoch 270, val loss: 1.0308809280395508
Epoch 280, training loss: 859.0579833984375 = 1.0299460887908936 + 100.0 * 8.580280303955078
Epoch 280, val loss: 1.0302186012268066
Epoch 290, training loss: 857.7973022460938 = 1.0293488502502441 + 100.0 * 8.567679405212402
Epoch 290, val loss: 1.029675841331482
Epoch 300, training loss: 856.5211181640625 = 1.0288351774215698 + 100.0 * 8.554923057556152
Epoch 300, val loss: 1.0291926860809326
Epoch 310, training loss: 855.385498046875 = 1.0283198356628418 + 100.0 * 8.543571472167969
Epoch 310, val loss: 1.0287055969238281
Epoch 320, training loss: 854.3395385742188 = 1.0277297496795654 + 100.0 * 8.53311824798584
Epoch 320, val loss: 1.028132438659668
Epoch 330, training loss: 853.455078125 = 1.0269389152526855 + 100.0 * 8.52428150177002
Epoch 330, val loss: 1.0273606777191162
Epoch 340, training loss: 852.7296142578125 = 1.0260465145111084 + 100.0 * 8.517035484313965
Epoch 340, val loss: 1.0264984369277954
Epoch 350, training loss: 852.0911254882812 = 1.0250864028930664 + 100.0 * 8.510660171508789
Epoch 350, val loss: 1.025575876235962
Epoch 360, training loss: 851.478515625 = 1.0240943431854248 + 100.0 * 8.504544258117676
Epoch 360, val loss: 1.0246236324310303
Epoch 370, training loss: 851.470458984375 = 1.0230847597122192 + 100.0 * 8.504473686218262
Epoch 370, val loss: 1.0236222743988037
Epoch 380, training loss: 850.4412231445312 = 1.0218957662582397 + 100.0 * 8.494193077087402
Epoch 380, val loss: 1.0224705934524536
Epoch 390, training loss: 849.93310546875 = 1.020769715309143 + 100.0 * 8.489123344421387
Epoch 390, val loss: 1.0214216709136963
Epoch 400, training loss: 849.4199829101562 = 1.0196768045425415 + 100.0 * 8.484003067016602
Epoch 400, val loss: 1.0203728675842285
Epoch 410, training loss: 848.8993530273438 = 1.018588662147522 + 100.0 * 8.47880744934082
Epoch 410, val loss: 1.0193380117416382
Epoch 420, training loss: 848.5029296875 = 1.0174299478530884 + 100.0 * 8.474854469299316
Epoch 420, val loss: 1.018223762512207
Epoch 430, training loss: 848.0574340820312 = 1.016266107559204 + 100.0 * 8.47041130065918
Epoch 430, val loss: 1.0170897245407104
Epoch 440, training loss: 847.4700927734375 = 1.015081763267517 + 100.0 * 8.464550018310547
Epoch 440, val loss: 1.015997290611267
Epoch 450, training loss: 847.0164794921875 = 1.013941764831543 + 100.0 * 8.460025787353516
Epoch 450, val loss: 1.0148612260818481
Epoch 460, training loss: 846.578857421875 = 1.0127246379852295 + 100.0 * 8.45566177368164
Epoch 460, val loss: 1.0137059688568115
Epoch 470, training loss: 846.333984375 = 1.0114494562149048 + 100.0 * 8.453225135803223
Epoch 470, val loss: 1.0124999284744263
Epoch 480, training loss: 846.041748046875 = 1.0100383758544922 + 100.0 * 8.4503173828125
Epoch 480, val loss: 1.0110723972320557
Epoch 490, training loss: 845.5775146484375 = 1.0086112022399902 + 100.0 * 8.44568920135498
Epoch 490, val loss: 1.0097191333770752
Epoch 500, training loss: 845.2422485351562 = 1.0072065591812134 + 100.0 * 8.442350387573242
Epoch 500, val loss: 1.0083551406860352
Epoch 510, training loss: 844.9926147460938 = 1.0057694911956787 + 100.0 * 8.439867973327637
Epoch 510, val loss: 1.0069472789764404
Epoch 520, training loss: 844.928466796875 = 1.0041751861572266 + 100.0 * 8.43924331665039
Epoch 520, val loss: 1.0053900480270386
Epoch 530, training loss: 844.40576171875 = 1.0026150941848755 + 100.0 * 8.43403148651123
Epoch 530, val loss: 1.003877878189087
Epoch 540, training loss: 844.0810546875 = 1.0011084079742432 + 100.0 * 8.43079948425293
Epoch 540, val loss: 1.0023925304412842
Epoch 550, training loss: 843.8037109375 = 0.9995470643043518 + 100.0 * 8.428041458129883
Epoch 550, val loss: 1.0008774995803833
Epoch 560, training loss: 843.5635986328125 = 0.9979739189147949 + 100.0 * 8.42565631866455
Epoch 560, val loss: 0.9993340969085693
Epoch 570, training loss: 843.3438720703125 = 0.9962469339370728 + 100.0 * 8.423476219177246
Epoch 570, val loss: 0.9976552724838257
Epoch 580, training loss: 843.1119995117188 = 0.9944987893104553 + 100.0 * 8.421175003051758
Epoch 580, val loss: 0.9959436058998108
Epoch 590, training loss: 842.90869140625 = 0.9927816390991211 + 100.0 * 8.419158935546875
Epoch 590, val loss: 0.9942653179168701
Epoch 600, training loss: 842.7109375 = 0.9910300970077515 + 100.0 * 8.41719913482666
Epoch 600, val loss: 0.992551863193512
Epoch 610, training loss: 842.6885375976562 = 0.9891661405563354 + 100.0 * 8.416994094848633
Epoch 610, val loss: 0.990711510181427
Epoch 620, training loss: 842.3364868164062 = 0.9872390627861023 + 100.0 * 8.413492202758789
Epoch 620, val loss: 0.9888389706611633
Epoch 630, training loss: 842.1652221679688 = 0.9853548407554626 + 100.0 * 8.411798477172852
Epoch 630, val loss: 0.9869706034660339
Epoch 640, training loss: 842.0122680664062 = 0.9833994507789612 + 100.0 * 8.41028881072998
Epoch 640, val loss: 0.9850818514823914
Epoch 650, training loss: 841.9182739257812 = 0.9813458919525146 + 100.0 * 8.409369468688965
Epoch 650, val loss: 0.9830057621002197
Epoch 660, training loss: 841.6561279296875 = 0.979265570640564 + 100.0 * 8.406768798828125
Epoch 660, val loss: 0.9810001254081726
Epoch 670, training loss: 841.4862060546875 = 0.9771906137466431 + 100.0 * 8.40509033203125
Epoch 670, val loss: 0.9789875149726868
Epoch 680, training loss: 841.321533203125 = 0.9750900268554688 + 100.0 * 8.403464317321777
Epoch 680, val loss: 0.9769414067268372
Epoch 690, training loss: 841.2203979492188 = 0.9729680418968201 + 100.0 * 8.402474403381348
Epoch 690, val loss: 0.9748693704605103
Epoch 700, training loss: 841.5673217773438 = 0.9706602096557617 + 100.0 * 8.405966758728027
Epoch 700, val loss: 0.9726210236549377
Epoch 710, training loss: 840.947265625 = 0.9682872295379639 + 100.0 * 8.399789810180664
Epoch 710, val loss: 0.9703150987625122
Epoch 720, training loss: 840.7928466796875 = 0.9660448431968689 + 100.0 * 8.39826774597168
Epoch 720, val loss: 0.9681159853935242
Epoch 730, training loss: 840.607421875 = 0.9637685418128967 + 100.0 * 8.39643669128418
Epoch 730, val loss: 0.965889036655426
Epoch 740, training loss: 840.4546508789062 = 0.961477518081665 + 100.0 * 8.39493179321289
Epoch 740, val loss: 0.9636551141738892
Epoch 750, training loss: 840.3464965820312 = 0.9591513872146606 + 100.0 * 8.39387321472168
Epoch 750, val loss: 0.9613651633262634
Epoch 760, training loss: 840.2972412109375 = 0.9565223455429077 + 100.0 * 8.393406867980957
Epoch 760, val loss: 0.9587843418121338
Epoch 770, training loss: 840.18896484375 = 0.9539263844490051 + 100.0 * 8.392350196838379
Epoch 770, val loss: 0.9562626481056213
Epoch 780, training loss: 840.009765625 = 0.9514501094818115 + 100.0 * 8.390583038330078
Epoch 780, val loss: 0.953829824924469
Epoch 790, training loss: 839.83056640625 = 0.9489567875862122 + 100.0 * 8.388815879821777
Epoch 790, val loss: 0.9514114260673523
Epoch 800, training loss: 839.7062377929688 = 0.9464158415794373 + 100.0 * 8.387598037719727
Epoch 800, val loss: 0.9489296078681946
Epoch 810, training loss: 839.5890502929688 = 0.9438003897666931 + 100.0 * 8.386452674865723
Epoch 810, val loss: 0.9463773369789124
Epoch 820, training loss: 839.4788818359375 = 0.9411414861679077 + 100.0 * 8.385376930236816
Epoch 820, val loss: 0.9437822103500366
Epoch 830, training loss: 840.6503295898438 = 0.9382847547531128 + 100.0 * 8.397120475769043
Epoch 830, val loss: 0.9409416317939758
Epoch 840, training loss: 839.4174194335938 = 0.9350930452346802 + 100.0 * 8.384822845458984
Epoch 840, val loss: 0.9378604292869568
Epoch 850, training loss: 839.2567749023438 = 0.9322947859764099 + 100.0 * 8.383244514465332
Epoch 850, val loss: 0.9351241588592529
Epoch 860, training loss: 839.0940551757812 = 0.9294455051422119 + 100.0 * 8.381646156311035
Epoch 860, val loss: 0.9323574900627136
Epoch 870, training loss: 838.982421875 = 0.9265416860580444 + 100.0 * 8.380558967590332
Epoch 870, val loss: 0.9295334815979004
Epoch 880, training loss: 838.8876342773438 = 0.9236240386962891 + 100.0 * 8.379639625549316
Epoch 880, val loss: 0.9266788363456726
Epoch 890, training loss: 838.7906494140625 = 0.9206499457359314 + 100.0 * 8.378700256347656
Epoch 890, val loss: 0.9237735867500305
Epoch 900, training loss: 838.6990356445312 = 0.9176220893859863 + 100.0 * 8.377814292907715
Epoch 900, val loss: 0.9208241105079651
Epoch 910, training loss: 838.7882690429688 = 0.9145363569259644 + 100.0 * 8.378737449645996
Epoch 910, val loss: 0.9178179502487183
Epoch 920, training loss: 838.8934936523438 = 0.9111307263374329 + 100.0 * 8.379823684692383
Epoch 920, val loss: 0.9144682884216309
Epoch 930, training loss: 838.5409545898438 = 0.9078722596168518 + 100.0 * 8.376330375671387
Epoch 930, val loss: 0.9113409519195557
Epoch 940, training loss: 838.3886108398438 = 0.9046971201896667 + 100.0 * 8.374838829040527
Epoch 940, val loss: 0.9082523584365845
Epoch 950, training loss: 838.290771484375 = 0.9014931321144104 + 100.0 * 8.373892784118652
Epoch 950, val loss: 0.9051446318626404
Epoch 960, training loss: 838.2100219726562 = 0.8982732892036438 + 100.0 * 8.373117446899414
Epoch 960, val loss: 0.9020129442214966
Epoch 970, training loss: 838.15771484375 = 0.8949969410896301 + 100.0 * 8.372627258300781
Epoch 970, val loss: 0.8988295793533325
Epoch 980, training loss: 838.3120727539062 = 0.8915154933929443 + 100.0 * 8.374205589294434
Epoch 980, val loss: 0.8954405188560486
Epoch 990, training loss: 838.125732421875 = 0.887971818447113 + 100.0 * 8.372377395629883
Epoch 990, val loss: 0.892013669013977
Epoch 1000, training loss: 837.9381713867188 = 0.8845657110214233 + 100.0 * 8.370535850524902
Epoch 1000, val loss: 0.8887149095535278
Epoch 1010, training loss: 837.8560791015625 = 0.8811564445495605 + 100.0 * 8.369749069213867
Epoch 1010, val loss: 0.8854159712791443
Epoch 1020, training loss: 837.7802734375 = 0.8777056336402893 + 100.0 * 8.369026184082031
Epoch 1020, val loss: 0.8820715546607971
Epoch 1030, training loss: 837.7068481445312 = 0.8742222189903259 + 100.0 * 8.368326187133789
Epoch 1030, val loss: 0.8787001371383667
Epoch 1040, training loss: 837.679931640625 = 0.870693564414978 + 100.0 * 8.36809253692627
Epoch 1040, val loss: 0.8752947449684143
Epoch 1050, training loss: 837.6522827148438 = 0.866855800151825 + 100.0 * 8.367854118347168
Epoch 1050, val loss: 0.8715102672576904
Epoch 1060, training loss: 837.6526489257812 = 0.8630469441413879 + 100.0 * 8.36789608001709
Epoch 1060, val loss: 0.8678759932518005
Epoch 1070, training loss: 837.5126953125 = 0.8594078421592712 + 100.0 * 8.366532325744629
Epoch 1070, val loss: 0.8643650412559509
Epoch 1080, training loss: 837.40869140625 = 0.8557755351066589 + 100.0 * 8.36552906036377
Epoch 1080, val loss: 0.8608478903770447
Epoch 1090, training loss: 837.328125 = 0.8521429300308228 + 100.0 * 8.36475944519043
Epoch 1090, val loss: 0.8573278188705444
Epoch 1100, training loss: 837.2654418945312 = 0.8484712839126587 + 100.0 * 8.36417007446289
Epoch 1100, val loss: 0.8537750244140625
Epoch 1110, training loss: 837.2058715820312 = 0.8447445631027222 + 100.0 * 8.363611221313477
Epoch 1110, val loss: 0.8501668572425842
Epoch 1120, training loss: 837.147705078125 = 0.8409594893455505 + 100.0 * 8.363067626953125
Epoch 1120, val loss: 0.8465126156806946
Epoch 1130, training loss: 837.2726440429688 = 0.8371516466140747 + 100.0 * 8.364355087280273
Epoch 1130, val loss: 0.8428369760513306
Epoch 1140, training loss: 837.2984619140625 = 0.8329118490219116 + 100.0 * 8.364655494689941
Epoch 1140, val loss: 0.8386762142181396
Epoch 1150, training loss: 837.0444946289062 = 0.8289113640785217 + 100.0 * 8.36215591430664
Epoch 1150, val loss: 0.834856390953064
Epoch 1160, training loss: 836.938720703125 = 0.8250781297683716 + 100.0 * 8.361136436462402
Epoch 1160, val loss: 0.8311620354652405
Epoch 1170, training loss: 836.8796997070312 = 0.8211971521377563 + 100.0 * 8.36058521270752
Epoch 1170, val loss: 0.8274068832397461
Epoch 1180, training loss: 836.8255004882812 = 0.8173268437385559 + 100.0 * 8.360081672668457
Epoch 1180, val loss: 0.82368403673172
Epoch 1190, training loss: 836.775634765625 = 0.8134305477142334 + 100.0 * 8.35962200164795
Epoch 1190, val loss: 0.8199294209480286
Epoch 1200, training loss: 836.9140625 = 0.8094651103019714 + 100.0 * 8.361045837402344
Epoch 1200, val loss: 0.8160962462425232
Epoch 1210, training loss: 836.87451171875 = 0.8050606846809387 + 100.0 * 8.360694885253906
Epoch 1210, val loss: 0.8118069171905518
Epoch 1220, training loss: 836.6689453125 = 0.8009182214736938 + 100.0 * 8.358680725097656
Epoch 1220, val loss: 0.8078376650810242
Epoch 1230, training loss: 836.6025390625 = 0.7969304323196411 + 100.0 * 8.35805606842041
Epoch 1230, val loss: 0.8040299415588379
Epoch 1240, training loss: 836.5511474609375 = 0.7929491400718689 + 100.0 * 8.357582092285156
Epoch 1240, val loss: 0.8001952767372131
Epoch 1250, training loss: 836.4907836914062 = 0.7889761924743652 + 100.0 * 8.357017517089844
Epoch 1250, val loss: 0.7963628768920898
Epoch 1260, training loss: 836.4644165039062 = 0.7849898934364319 + 100.0 * 8.356794357299805
Epoch 1260, val loss: 0.7925134301185608
Epoch 1270, training loss: 836.8479614257812 = 0.7807833552360535 + 100.0 * 8.360671997070312
Epoch 1270, val loss: 0.788425624370575
Epoch 1280, training loss: 836.4278564453125 = 0.7765107750892639 + 100.0 * 8.356513977050781
Epoch 1280, val loss: 0.7843785881996155
Epoch 1290, training loss: 836.3277587890625 = 0.7724606394767761 + 100.0 * 8.355552673339844
Epoch 1290, val loss: 0.7804836630821228
Epoch 1300, training loss: 836.2904052734375 = 0.7684550881385803 + 100.0 * 8.355219841003418
Epoch 1300, val loss: 0.7766333818435669
Epoch 1310, training loss: 836.233642578125 = 0.7644314169883728 + 100.0 * 8.354692459106445
Epoch 1310, val loss: 0.7727814316749573
Epoch 1320, training loss: 836.1903686523438 = 0.7604097723960876 + 100.0 * 8.354299545288086
Epoch 1320, val loss: 0.768927276134491
Epoch 1330, training loss: 836.1480712890625 = 0.756369411945343 + 100.0 * 8.353917121887207
Epoch 1330, val loss: 0.7650597095489502
Epoch 1340, training loss: 836.1190185546875 = 0.7523020505905151 + 100.0 * 8.353667259216309
Epoch 1340, val loss: 0.7611713409423828
Epoch 1350, training loss: 836.782958984375 = 0.7480981349945068 + 100.0 * 8.36034870147705
Epoch 1350, val loss: 0.7571297287940979
Epoch 1360, training loss: 836.0999755859375 = 0.7436872720718384 + 100.0 * 8.353562355041504
Epoch 1360, val loss: 0.7529540061950684
Epoch 1370, training loss: 836.026123046875 = 0.7396253943443298 + 100.0 * 8.352865219116211
Epoch 1370, val loss: 0.7490348815917969
Epoch 1380, training loss: 835.9725952148438 = 0.7356045842170715 + 100.0 * 8.352370262145996
Epoch 1380, val loss: 0.7452016472816467
Epoch 1390, training loss: 835.9208984375 = 0.7316335439682007 + 100.0 * 8.351892471313477
Epoch 1390, val loss: 0.7414105534553528
Epoch 1400, training loss: 835.9186401367188 = 0.7276502847671509 + 100.0 * 8.351909637451172
Epoch 1400, val loss: 0.7376117706298828
Epoch 1410, training loss: 836.224853515625 = 0.723463237285614 + 100.0 * 8.355013847351074
Epoch 1410, val loss: 0.7335959672927856
Epoch 1420, training loss: 835.86865234375 = 0.7193605303764343 + 100.0 * 8.351492881774902
Epoch 1420, val loss: 0.7297054529190063
Epoch 1430, training loss: 835.7701416015625 = 0.7153587341308594 + 100.0 * 8.350547790527344
Epoch 1430, val loss: 0.7258961200714111
Epoch 1440, training loss: 835.7351684570312 = 0.7113935947418213 + 100.0 * 8.350237846374512
Epoch 1440, val loss: 0.7221274971961975
Epoch 1450, training loss: 835.6952514648438 = 0.7074909806251526 + 100.0 * 8.34987735748291
Epoch 1450, val loss: 0.7184222340583801
Epoch 1460, training loss: 835.6551513671875 = 0.7035479545593262 + 100.0 * 8.349515914916992
Epoch 1460, val loss: 0.714674174785614
Epoch 1470, training loss: 835.6218872070312 = 0.6996055245399475 + 100.0 * 8.349223136901855
Epoch 1470, val loss: 0.7109371423721313
Epoch 1480, training loss: 835.66357421875 = 0.6956601142883301 + 100.0 * 8.349678993225098
Epoch 1480, val loss: 0.707196056842804
Epoch 1490, training loss: 835.682373046875 = 0.6913965344429016 + 100.0 * 8.349909782409668
Epoch 1490, val loss: 0.7030969262123108
Epoch 1500, training loss: 835.6547241210938 = 0.6872942447662354 + 100.0 * 8.349674224853516
Epoch 1500, val loss: 0.6992600560188293
Epoch 1510, training loss: 835.4984130859375 = 0.6833990216255188 + 100.0 * 8.348150253295898
Epoch 1510, val loss: 0.6955732107162476
Epoch 1520, training loss: 835.4672241210938 = 0.6795907020568848 + 100.0 * 8.34787654876709
Epoch 1520, val loss: 0.6919525265693665
Epoch 1530, training loss: 835.4238891601562 = 0.6758154034614563 + 100.0 * 8.347480773925781
Epoch 1530, val loss: 0.6883981227874756
Epoch 1540, training loss: 835.4176635742188 = 0.672024667263031 + 100.0 * 8.347456932067871
Epoch 1540, val loss: 0.6848133206367493
Epoch 1550, training loss: 835.7796020507812 = 0.6681066751480103 + 100.0 * 8.351115226745605
Epoch 1550, val loss: 0.6810776591300964
Epoch 1560, training loss: 835.4660034179688 = 0.6641397476196289 + 100.0 * 8.348018646240234
Epoch 1560, val loss: 0.6773727536201477
Epoch 1570, training loss: 835.3456420898438 = 0.6603600978851318 + 100.0 * 8.346853256225586
Epoch 1570, val loss: 0.6738085746765137
Epoch 1580, training loss: 835.2811889648438 = 0.6566729545593262 + 100.0 * 8.346244812011719
Epoch 1580, val loss: 0.6703374981880188
Epoch 1590, training loss: 835.2384643554688 = 0.6530135869979858 + 100.0 * 8.345854759216309
Epoch 1590, val loss: 0.6668928265571594
Epoch 1600, training loss: 835.2451171875 = 0.6493708491325378 + 100.0 * 8.34595775604248
Epoch 1600, val loss: 0.6634613275527954
Epoch 1610, training loss: 835.4052124023438 = 0.6455333828926086 + 100.0 * 8.347597122192383
Epoch 1610, val loss: 0.6598267555236816
Epoch 1620, training loss: 835.2155151367188 = 0.641788899898529 + 100.0 * 8.34573745727539
Epoch 1620, val loss: 0.6563370823860168
Epoch 1630, training loss: 835.1475830078125 = 0.6381353735923767 + 100.0 * 8.345094680786133
Epoch 1630, val loss: 0.6529194116592407
Epoch 1640, training loss: 835.0997924804688 = 0.6346129179000854 + 100.0 * 8.34465217590332
Epoch 1640, val loss: 0.6495871543884277
Epoch 1650, training loss: 835.08837890625 = 0.6310879588127136 + 100.0 * 8.344573020935059
Epoch 1650, val loss: 0.6462860703468323
Epoch 1660, training loss: 835.24951171875 = 0.6275211572647095 + 100.0 * 8.346220016479492
Epoch 1660, val loss: 0.642917811870575
Epoch 1670, training loss: 835.0419311523438 = 0.6239228844642639 + 100.0 * 8.3441801071167
Epoch 1670, val loss: 0.6395977139472961
Epoch 1680, training loss: 834.9791870117188 = 0.6204598546028137 + 100.0 * 8.343586921691895
Epoch 1680, val loss: 0.6363410353660583
Epoch 1690, training loss: 834.9535522460938 = 0.6170260310173035 + 100.0 * 8.343365669250488
Epoch 1690, val loss: 0.6331482529640198
Epoch 1700, training loss: 834.929931640625 = 0.6136684417724609 + 100.0 * 8.343162536621094
Epoch 1700, val loss: 0.6300244927406311
Epoch 1710, training loss: 835.0588989257812 = 0.6102861166000366 + 100.0 * 8.344486236572266
Epoch 1710, val loss: 0.6268786191940308
Epoch 1720, training loss: 835.0828857421875 = 0.6066221594810486 + 100.0 * 8.344762802124023
Epoch 1720, val loss: 0.6234217286109924
Epoch 1730, training loss: 834.9512939453125 = 0.6031864881515503 + 100.0 * 8.343481063842773
Epoch 1730, val loss: 0.6202623844146729
Epoch 1740, training loss: 834.8593139648438 = 0.5999005436897278 + 100.0 * 8.342594146728516
Epoch 1740, val loss: 0.6171776056289673
Epoch 1750, training loss: 834.8004760742188 = 0.5967299938201904 + 100.0 * 8.342037200927734
Epoch 1750, val loss: 0.6142460107803345
Epoch 1760, training loss: 834.7645874023438 = 0.5935778617858887 + 100.0 * 8.341710090637207
Epoch 1760, val loss: 0.6113227605819702
Epoch 1770, training loss: 834.7360229492188 = 0.590461015701294 + 100.0 * 8.341455459594727
Epoch 1770, val loss: 0.6084339022636414
Epoch 1780, training loss: 834.7161865234375 = 0.5873348116874695 + 100.0 * 8.341288566589355
Epoch 1780, val loss: 0.6055471301078796
Epoch 1790, training loss: 835.0296630859375 = 0.5841915607452393 + 100.0 * 8.344454765319824
Epoch 1790, val loss: 0.6025955080986023
Epoch 1800, training loss: 834.79150390625 = 0.580818772315979 + 100.0 * 8.342106819152832
Epoch 1800, val loss: 0.5995606184005737
Epoch 1810, training loss: 834.7308349609375 = 0.5777367949485779 + 100.0 * 8.341530799865723
Epoch 1810, val loss: 0.5966646671295166
Epoch 1820, training loss: 834.6165161132812 = 0.574724018573761 + 100.0 * 8.340417861938477
Epoch 1820, val loss: 0.5939100384712219
Epoch 1830, training loss: 834.5880737304688 = 0.5717971920967102 + 100.0 * 8.340163230895996
Epoch 1830, val loss: 0.5912277698516846
Epoch 1840, training loss: 834.5634155273438 = 0.5688853859901428 + 100.0 * 8.339944839477539
Epoch 1840, val loss: 0.5885401964187622
Epoch 1850, training loss: 834.5435180664062 = 0.5659942626953125 + 100.0 * 8.339775085449219
Epoch 1850, val loss: 0.5859009027481079
Epoch 1860, training loss: 835.0926513671875 = 0.5630469918251038 + 100.0 * 8.345295906066895
Epoch 1860, val loss: 0.5831732153892517
Epoch 1870, training loss: 834.7274169921875 = 0.5599101781845093 + 100.0 * 8.3416748046875
Epoch 1870, val loss: 0.5802833437919617
Epoch 1880, training loss: 834.5051879882812 = 0.5570725798606873 + 100.0 * 8.339481353759766
Epoch 1880, val loss: 0.5777210593223572
Epoch 1890, training loss: 834.4546508789062 = 0.5542967319488525 + 100.0 * 8.339003562927246
Epoch 1890, val loss: 0.5751781463623047
Epoch 1900, training loss: 834.426025390625 = 0.5516082048416138 + 100.0 * 8.338744163513184
Epoch 1900, val loss: 0.5727255940437317
Epoch 1910, training loss: 834.4002685546875 = 0.5489492416381836 + 100.0 * 8.338513374328613
Epoch 1910, val loss: 0.5703175067901611
Epoch 1920, training loss: 834.734375 = 0.546266496181488 + 100.0 * 8.341880798339844
Epoch 1920, val loss: 0.5679053068161011
Epoch 1930, training loss: 834.5292358398438 = 0.5433074831962585 + 100.0 * 8.339859008789062
Epoch 1930, val loss: 0.5651452541351318
Epoch 1940, training loss: 834.3668823242188 = 0.5407310724258423 + 100.0 * 8.338261604309082
Epoch 1940, val loss: 0.5628264546394348
Epoch 1950, training loss: 834.3116455078125 = 0.5381278991699219 + 100.0 * 8.337735176086426
Epoch 1950, val loss: 0.5604968667030334
Epoch 1960, training loss: 834.2855834960938 = 0.5356458425521851 + 100.0 * 8.337499618530273
Epoch 1960, val loss: 0.5582265853881836
Epoch 1970, training loss: 834.2557983398438 = 0.5331671237945557 + 100.0 * 8.337226867675781
Epoch 1970, val loss: 0.5559970736503601
Epoch 1980, training loss: 834.2803955078125 = 0.5307095050811768 + 100.0 * 8.337496757507324
Epoch 1980, val loss: 0.553803563117981
Epoch 1990, training loss: 834.5816040039062 = 0.5281058549880981 + 100.0 * 8.340535163879395
Epoch 1990, val loss: 0.5514199733734131
Epoch 2000, training loss: 834.2581176757812 = 0.5256090760231018 + 100.0 * 8.337325096130371
Epoch 2000, val loss: 0.5491488575935364
Epoch 2010, training loss: 834.1812744140625 = 0.5232340097427368 + 100.0 * 8.336580276489258
Epoch 2010, val loss: 0.5470299124717712
Epoch 2020, training loss: 834.1491088867188 = 0.5209071040153503 + 100.0 * 8.336281776428223
Epoch 2020, val loss: 0.5449331998825073
Epoch 2030, training loss: 834.1212158203125 = 0.5186648964881897 + 100.0 * 8.33602523803711
Epoch 2030, val loss: 0.5429339408874512
Epoch 2040, training loss: 834.1152954101562 = 0.5164161920547485 + 100.0 * 8.335988998413086
Epoch 2040, val loss: 0.5409176349639893
Epoch 2050, training loss: 834.430419921875 = 0.5141705274581909 + 100.0 * 8.339162826538086
Epoch 2050, val loss: 0.5389090776443481
Epoch 2060, training loss: 834.1727905273438 = 0.5117253661155701 + 100.0 * 8.336610794067383
Epoch 2060, val loss: 0.5367050766944885
Epoch 2070, training loss: 834.133056640625 = 0.5095460414886475 + 100.0 * 8.336235046386719
Epoch 2070, val loss: 0.5347620844841003
Epoch 2080, training loss: 834.120361328125 = 0.5072895884513855 + 100.0 * 8.33613109588623
Epoch 2080, val loss: 0.5327655673027039
Epoch 2090, training loss: 833.988037109375 = 0.5051448941230774 + 100.0 * 8.334829330444336
Epoch 2090, val loss: 0.530831515789032
Epoch 2100, training loss: 833.9896240234375 = 0.5030326843261719 + 100.0 * 8.33486557006836
Epoch 2100, val loss: 0.5289704203605652
Epoch 2110, training loss: 833.9559936523438 = 0.5009572505950928 + 100.0 * 8.334549903869629
Epoch 2110, val loss: 0.5271279811859131
Epoch 2120, training loss: 833.9924926757812 = 0.4988890290260315 + 100.0 * 8.334936141967773
Epoch 2120, val loss: 0.5253075957298279
Epoch 2130, training loss: 834.17919921875 = 0.49673861265182495 + 100.0 * 8.336824417114258
Epoch 2130, val loss: 0.5234435796737671
Epoch 2140, training loss: 833.9711303710938 = 0.4945945739746094 + 100.0 * 8.334765434265137
Epoch 2140, val loss: 0.5215147137641907
Epoch 2150, training loss: 833.8844604492188 = 0.4926146864891052 + 100.0 * 8.333918571472168
Epoch 2150, val loss: 0.5197516679763794
Epoch 2160, training loss: 833.8410034179688 = 0.49068164825439453 + 100.0 * 8.333503723144531
Epoch 2160, val loss: 0.5180844068527222
Epoch 2170, training loss: 833.8110961914062 = 0.4888228178024292 + 100.0 * 8.333222389221191
Epoch 2170, val loss: 0.5164410471916199
Epoch 2180, training loss: 833.7970581054688 = 0.4869649410247803 + 100.0 * 8.333101272583008
Epoch 2180, val loss: 0.5148101449012756
Epoch 2190, training loss: 833.9364013671875 = 0.48510071635246277 + 100.0 * 8.334512710571289
Epoch 2190, val loss: 0.5131751298904419
Epoch 2200, training loss: 833.7736206054688 = 0.4831359386444092 + 100.0 * 8.332904815673828
Epoch 2200, val loss: 0.5114876627922058
Epoch 2210, training loss: 833.8063354492188 = 0.4813264310359955 + 100.0 * 8.333250045776367
Epoch 2210, val loss: 0.5099049806594849
Epoch 2220, training loss: 833.87841796875 = 0.4794437289237976 + 100.0 * 8.333990097045898
Epoch 2220, val loss: 0.5082864761352539
Epoch 2230, training loss: 833.7971801757812 = 0.4776202142238617 + 100.0 * 8.333195686340332
Epoch 2230, val loss: 0.5065900087356567
Epoch 2240, training loss: 833.6903076171875 = 0.47585606575012207 + 100.0 * 8.332144737243652
Epoch 2240, val loss: 0.5050732493400574
Epoch 2250, training loss: 833.6497192382812 = 0.4741682708263397 + 100.0 * 8.331755638122559
Epoch 2250, val loss: 0.5036177635192871
Epoch 2260, training loss: 833.63037109375 = 0.47252461314201355 + 100.0 * 8.331578254699707
Epoch 2260, val loss: 0.502220869064331
Epoch 2270, training loss: 833.6502685546875 = 0.47088006138801575 + 100.0 * 8.331793785095215
Epoch 2270, val loss: 0.5008177161216736
Epoch 2280, training loss: 833.7612915039062 = 0.46919623017311096 + 100.0 * 8.332921028137207
Epoch 2280, val loss: 0.49939823150634766
Epoch 2290, training loss: 833.713623046875 = 0.4674297273159027 + 100.0 * 8.332462310791016
Epoch 2290, val loss: 0.4977758824825287
Epoch 2300, training loss: 833.6737060546875 = 0.4658307135105133 + 100.0 * 8.33207893371582
Epoch 2300, val loss: 0.4963803291320801
Epoch 2310, training loss: 833.6150512695312 = 0.46419528126716614 + 100.0 * 8.33150863647461
Epoch 2310, val loss: 0.49501127004623413
Epoch 2320, training loss: 833.51708984375 = 0.46267256140708923 + 100.0 * 8.330544471740723
Epoch 2320, val loss: 0.4936659336090088
Epoch 2330, training loss: 833.481201171875 = 0.4611665606498718 + 100.0 * 8.3302001953125
Epoch 2330, val loss: 0.49239033460617065
Epoch 2340, training loss: 833.4767456054688 = 0.4596836566925049 + 100.0 * 8.330170631408691
Epoch 2340, val loss: 0.4911276400089264
Epoch 2350, training loss: 833.9521484375 = 0.45812544226646423 + 100.0 * 8.334939956665039
Epoch 2350, val loss: 0.4897323548793793
Epoch 2360, training loss: 833.6373901367188 = 0.4564959406852722 + 100.0 * 8.331809043884277
Epoch 2360, val loss: 0.4883999228477478
Epoch 2370, training loss: 833.4763793945312 = 0.4550377428531647 + 100.0 * 8.33021354675293
Epoch 2370, val loss: 0.4871217608451843
Epoch 2380, training loss: 833.3956909179688 = 0.4536234140396118 + 100.0 * 8.329421043395996
Epoch 2380, val loss: 0.48592516779899597
Epoch 2390, training loss: 833.373291015625 = 0.4522477984428406 + 100.0 * 8.32921028137207
Epoch 2390, val loss: 0.48474642634391785
Epoch 2400, training loss: 833.49951171875 = 0.45085033774375916 + 100.0 * 8.330486297607422
Epoch 2400, val loss: 0.48352038860321045
Epoch 2410, training loss: 833.4847412109375 = 0.44932228326797485 + 100.0 * 8.330353736877441
Epoch 2410, val loss: 0.4821821451187134
Epoch 2420, training loss: 833.3087158203125 = 0.44793501496315 + 100.0 * 8.328607559204102
Epoch 2420, val loss: 0.4811009168624878
Epoch 2430, training loss: 833.3026733398438 = 0.446612149477005 + 100.0 * 8.328560829162598
Epoch 2430, val loss: 0.4799942076206207
Epoch 2440, training loss: 833.2686157226562 = 0.44528552889823914 + 100.0 * 8.328232765197754
Epoch 2440, val loss: 0.47883614897727966
Epoch 2450, training loss: 833.25634765625 = 0.44401150941848755 + 100.0 * 8.328123092651367
Epoch 2450, val loss: 0.4777546226978302
Epoch 2460, training loss: 833.276123046875 = 0.4427241384983063 + 100.0 * 8.328333854675293
Epoch 2460, val loss: 0.4766257703304291
Epoch 2470, training loss: 833.4840087890625 = 0.44139182567596436 + 100.0 * 8.330426216125488
Epoch 2470, val loss: 0.4754236936569214
Epoch 2480, training loss: 833.2609252929688 = 0.4401133954524994 + 100.0 * 8.328207969665527
Epoch 2480, val loss: 0.4745413064956665
Epoch 2490, training loss: 833.2225341796875 = 0.43883082270622253 + 100.0 * 8.327836990356445
Epoch 2490, val loss: 0.47337958216667175
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8249619482496194
0.8642324132434979
The final CL Acc:0.82496, 0.00207, The final GNN Acc:0.86404, 0.00027
Begin epxeriment: noisy_level: 0.2 cont_weight: 100 epoch: der1:0.3 der2:0.2 dfr1:0.4 dfr2:0.4
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=2500, cl_num_layers=2, cl_num_proj_hidden=64, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=100.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.2, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=32, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, noisy_level=0.2, num_hidden=64, num_proj_hidden=64, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=40, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 88648])
add edge: torch.Size([2, 106386])
remove edge: torch.Size([2, 71128])
updated graph: torch.Size([2, 88866])
=== Noisy graph ===
/home/project-robust-contrastive/Robust-Contrastive-Learning/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 1059.3065185546875 = 1.0764142274856567 + 100.0 * 10.582301139831543
Epoch 0, val loss: 1.0767464637756348
Epoch 10, training loss: 1059.2783203125 = 1.073607325553894 + 100.0 * 10.582047462463379
Epoch 10, val loss: 1.0739527940750122
Epoch 20, training loss: 1059.1746826171875 = 1.0706168413162231 + 100.0 * 10.58104133605957
Epoch 20, val loss: 1.070975661277771
Epoch 30, training loss: 1058.7296142578125 = 1.067427158355713 + 100.0 * 10.576622009277344
Epoch 30, val loss: 1.0678023099899292
Epoch 40, training loss: 1056.96728515625 = 1.0640240907669067 + 100.0 * 10.559033393859863
Epoch 40, val loss: 1.0644292831420898
Epoch 50, training loss: 1051.6949462890625 = 1.06028151512146 + 100.0 * 10.506346702575684
Epoch 50, val loss: 1.0607483386993408
Epoch 60, training loss: 1039.368896484375 = 1.056610107421875 + 100.0 * 10.383122444152832
Epoch 60, val loss: 1.057244062423706
Epoch 70, training loss: 1015.9503784179688 = 1.053091049194336 + 100.0 * 10.148972511291504
Epoch 70, val loss: 1.0538275241851807
Epoch 80, training loss: 989.0344848632812 = 1.0489648580551147 + 100.0 * 9.879855155944824
Epoch 80, val loss: 1.049826741218567
Epoch 90, training loss: 971.4974975585938 = 1.0445102453231812 + 100.0 * 9.704529762268066
Epoch 90, val loss: 1.0455677509307861
Epoch 100, training loss: 957.8214721679688 = 1.039992094039917 + 100.0 * 9.567814826965332
Epoch 100, val loss: 1.0412561893463135
Epoch 110, training loss: 941.4859008789062 = 1.036272644996643 + 100.0 * 9.404496192932129
Epoch 110, val loss: 1.0377174615859985
Epoch 120, training loss: 932.2471923828125 = 1.0329736471176147 + 100.0 * 9.312142372131348
Epoch 120, val loss: 1.034349799156189
Epoch 130, training loss: 929.4402465820312 = 1.028907060623169 + 100.0 * 9.284112930297852
Epoch 130, val loss: 1.0302960872650146
Epoch 140, training loss: 926.385009765625 = 1.0248074531555176 + 100.0 * 9.253602027893066
Epoch 140, val loss: 1.0263662338256836
Epoch 150, training loss: 921.859375 = 1.0212868452072144 + 100.0 * 9.208380699157715
Epoch 150, val loss: 1.023022174835205
Epoch 160, training loss: 914.842041015625 = 1.018416404724121 + 100.0 * 9.138236045837402
Epoch 160, val loss: 1.0203537940979004
Epoch 170, training loss: 904.442138671875 = 1.0172103643417358 + 100.0 * 9.034249305725098
Epoch 170, val loss: 1.0193005800247192
Epoch 180, training loss: 897.7106323242188 = 1.0157030820846558 + 100.0 * 8.966949462890625
Epoch 180, val loss: 1.0174798965454102
Epoch 190, training loss: 892.756103515625 = 1.012121319770813 + 100.0 * 8.917439460754395
Epoch 190, val loss: 1.0139591693878174
Epoch 200, training loss: 889.4102172851562 = 1.008030652999878 + 100.0 * 8.884021759033203
Epoch 200, val loss: 1.0099750757217407
Epoch 210, training loss: 886.4906005859375 = 1.0037689208984375 + 100.0 * 8.854867935180664
Epoch 210, val loss: 1.0058454275131226
Epoch 220, training loss: 883.76318359375 = 0.9994024634361267 + 100.0 * 8.827637672424316
Epoch 220, val loss: 1.0016157627105713
Epoch 230, training loss: 881.516845703125 = 0.9948155283927917 + 100.0 * 8.805220603942871
Epoch 230, val loss: 0.9971330165863037
Epoch 240, training loss: 879.447509765625 = 0.9898434281349182 + 100.0 * 8.784576416015625
Epoch 240, val loss: 0.9922806024551392
Epoch 250, training loss: 877.284912109375 = 0.984679102897644 + 100.0 * 8.763002395629883
Epoch 250, val loss: 0.9872698187828064
Epoch 260, training loss: 875.4415283203125 = 0.9793657064437866 + 100.0 * 8.744621276855469
Epoch 260, val loss: 0.9821442365646362
Epoch 270, training loss: 873.3609619140625 = 0.9737913608551025 + 100.0 * 8.723871231079102
Epoch 270, val loss: 0.9766773581504822
Epoch 280, training loss: 871.7329711914062 = 0.9677972197532654 + 100.0 * 8.70765209197998
Epoch 280, val loss: 0.9708321690559387
Epoch 290, training loss: 870.4651489257812 = 0.9613397121429443 + 100.0 * 8.695037841796875
Epoch 290, val loss: 0.964568555355072
Epoch 300, training loss: 868.9502563476562 = 0.9543853998184204 + 100.0 * 8.67995834350586
Epoch 300, val loss: 0.9577580094337463
Epoch 310, training loss: 867.7666625976562 = 0.9469931721687317 + 100.0 * 8.668196678161621
Epoch 310, val loss: 0.9505779147148132
Epoch 320, training loss: 866.6681518554688 = 0.9392321705818176 + 100.0 * 8.657289505004883
Epoch 320, val loss: 0.9430693984031677
Epoch 330, training loss: 865.7174072265625 = 0.9311456680297852 + 100.0 * 8.647862434387207
Epoch 330, val loss: 0.9352365732192993
Epoch 340, training loss: 864.89697265625 = 0.9227920174598694 + 100.0 * 8.639741897583008
Epoch 340, val loss: 0.9271966814994812
Epoch 350, training loss: 864.046142578125 = 0.9142647981643677 + 100.0 * 8.631319046020508
Epoch 350, val loss: 0.9190292358398438
Epoch 360, training loss: 863.3074951171875 = 0.9056122899055481 + 100.0 * 8.624018669128418
Epoch 360, val loss: 0.9107090830802917
Epoch 370, training loss: 862.7969970703125 = 0.8967928290367126 + 100.0 * 8.619002342224121
Epoch 370, val loss: 0.9022476673126221
Epoch 380, training loss: 862.0244140625 = 0.8877940773963928 + 100.0 * 8.611366271972656
Epoch 380, val loss: 0.8936892151832581
Epoch 390, training loss: 861.4810791015625 = 0.8787261247634888 + 100.0 * 8.606023788452148
Epoch 390, val loss: 0.8850678205490112
Epoch 400, training loss: 860.9953002929688 = 0.8695920705795288 + 100.0 * 8.60125732421875
Epoch 400, val loss: 0.8763650059700012
Epoch 410, training loss: 860.6012573242188 = 0.8604053854942322 + 100.0 * 8.597408294677734
Epoch 410, val loss: 0.8676308393478394
Epoch 420, training loss: 860.3844604492188 = 0.8511624336242676 + 100.0 * 8.595333099365234
Epoch 420, val loss: 0.8589009642601013
Epoch 430, training loss: 859.897705078125 = 0.8419279456138611 + 100.0 * 8.590558052062988
Epoch 430, val loss: 0.8501878380775452
Epoch 440, training loss: 859.4966430664062 = 0.8327973484992981 + 100.0 * 8.586638450622559
Epoch 440, val loss: 0.8415982723236084
Epoch 450, training loss: 859.1434936523438 = 0.8237494826316833 + 100.0 * 8.583197593688965
Epoch 450, val loss: 0.833117663860321
Epoch 460, training loss: 858.979248046875 = 0.8148275017738342 + 100.0 * 8.581644058227539
Epoch 460, val loss: 0.8247418999671936
Epoch 470, training loss: 858.588134765625 = 0.8059532046318054 + 100.0 * 8.577821731567383
Epoch 470, val loss: 0.8165134191513062
Epoch 480, training loss: 858.2459106445312 = 0.7972607612609863 + 100.0 * 8.57448673248291
Epoch 480, val loss: 0.8084036707878113
Epoch 490, training loss: 857.937255859375 = 0.7887099385261536 + 100.0 * 8.57148551940918
Epoch 490, val loss: 0.8004670739173889
Epoch 500, training loss: 857.6787109375 = 0.7803152203559875 + 100.0 * 8.568984031677246
Epoch 500, val loss: 0.7926986217498779
Epoch 510, training loss: 857.5802001953125 = 0.7720411419868469 + 100.0 * 8.568081855773926
Epoch 510, val loss: 0.7850561141967773
Epoch 520, training loss: 857.199951171875 = 0.7639397382736206 + 100.0 * 8.564360618591309
Epoch 520, val loss: 0.7775924205780029
Epoch 530, training loss: 856.9617919921875 = 0.7560625672340393 + 100.0 * 8.562057495117188
Epoch 530, val loss: 0.7703346014022827
Epoch 540, training loss: 856.7464599609375 = 0.7484169006347656 + 100.0 * 8.559980392456055
Epoch 540, val loss: 0.7633745670318604
Epoch 550, training loss: 856.4907836914062 = 0.7409693598747253 + 100.0 * 8.55749797821045
Epoch 550, val loss: 0.7565693259239197
Epoch 560, training loss: 856.2437133789062 = 0.7337612509727478 + 100.0 * 8.555099487304688
Epoch 560, val loss: 0.7500438094139099
Epoch 570, training loss: 856.0089111328125 = 0.726810872554779 + 100.0 * 8.552821159362793
Epoch 570, val loss: 0.7437891960144043
Epoch 580, training loss: 856.1284790039062 = 0.720085620880127 + 100.0 * 8.554083824157715
Epoch 580, val loss: 0.7377957701683044
Epoch 590, training loss: 855.5055541992188 = 0.7135868072509766 + 100.0 * 8.547920227050781
Epoch 590, val loss: 0.7319231033325195
Epoch 600, training loss: 855.3019409179688 = 0.7073908448219299 + 100.0 * 8.545945167541504
Epoch 600, val loss: 0.7263849377632141
Epoch 610, training loss: 855.0384521484375 = 0.7014379501342773 + 100.0 * 8.543370246887207
Epoch 610, val loss: 0.7211358547210693
Epoch 620, training loss: 855.0768432617188 = 0.6957387328147888 + 100.0 * 8.543810844421387
Epoch 620, val loss: 0.7161257863044739
Epoch 630, training loss: 854.6461181640625 = 0.6902000308036804 + 100.0 * 8.539559364318848
Epoch 630, val loss: 0.7113177180290222
Epoch 640, training loss: 854.427734375 = 0.6849235892295837 + 100.0 * 8.53742790222168
Epoch 640, val loss: 0.7067393660545349
Epoch 650, training loss: 854.2111206054688 = 0.6798812747001648 + 100.0 * 8.53531265258789
Epoch 650, val loss: 0.7024112939834595
Epoch 660, training loss: 854.0467529296875 = 0.6750432848930359 + 100.0 * 8.533717155456543
Epoch 660, val loss: 0.6983106732368469
Epoch 670, training loss: 854.4758911132812 = 0.6703605055809021 + 100.0 * 8.538055419921875
Epoch 670, val loss: 0.6943690776824951
Epoch 680, training loss: 853.822021484375 = 0.6658232808113098 + 100.0 * 8.531561851501465
Epoch 680, val loss: 0.6904909014701843
Epoch 690, training loss: 853.68115234375 = 0.661576509475708 + 100.0 * 8.530196189880371
Epoch 690, val loss: 0.686939537525177
Epoch 700, training loss: 853.4734497070312 = 0.6575303077697754 + 100.0 * 8.528159141540527
Epoch 700, val loss: 0.6836491227149963
Epoch 710, training loss: 853.3489379882812 = 0.6536887288093567 + 100.0 * 8.526952743530273
Epoch 710, val loss: 0.6805181503295898
Epoch 720, training loss: 853.2105712890625 = 0.6500262022018433 + 100.0 * 8.525605201721191
Epoch 720, val loss: 0.677565336227417
Epoch 730, training loss: 853.0786743164062 = 0.6465381979942322 + 100.0 * 8.524321556091309
Epoch 730, val loss: 0.674790620803833
Epoch 740, training loss: 852.95849609375 = 0.6432190537452698 + 100.0 * 8.523152351379395
Epoch 740, val loss: 0.6722033023834229
Epoch 750, training loss: 853.1857299804688 = 0.6400176882743835 + 100.0 * 8.525457382202148
Epoch 750, val loss: 0.6697100400924683
Epoch 760, training loss: 852.784912109375 = 0.6369331479072571 + 100.0 * 8.521479606628418
Epoch 760, val loss: 0.6673181653022766
Epoch 770, training loss: 852.5543212890625 = 0.6340550184249878 + 100.0 * 8.519203186035156
Epoch 770, val loss: 0.6651148200035095
Epoch 780, training loss: 852.4193115234375 = 0.6313474178314209 + 100.0 * 8.517879486083984
Epoch 780, val loss: 0.6631289124488831
Epoch 790, training loss: 852.3036499023438 = 0.6287758946418762 + 100.0 * 8.516748428344727
Epoch 790, val loss: 0.6612726449966431
Epoch 800, training loss: 852.2096557617188 = 0.626296877861023 + 100.0 * 8.515833854675293
Epoch 800, val loss: 0.6595073938369751
Epoch 810, training loss: 852.0546264648438 = 0.623930811882019 + 100.0 * 8.514307022094727
Epoch 810, val loss: 0.6578195691108704
Epoch 820, training loss: 851.8447875976562 = 0.6216959357261658 + 100.0 * 8.51223087310791
Epoch 820, val loss: 0.6562936902046204
Epoch 830, training loss: 851.7208862304688 = 0.6195710897445679 + 100.0 * 8.51101303100586
Epoch 830, val loss: 0.6548449993133545
Epoch 840, training loss: 851.7031860351562 = 0.6175077557563782 + 100.0 * 8.510856628417969
Epoch 840, val loss: 0.6534534096717834
Epoch 850, training loss: 851.3518676757812 = 0.6155261397361755 + 100.0 * 8.507363319396973
Epoch 850, val loss: 0.6522447466850281
Epoch 860, training loss: 851.181640625 = 0.613670825958252 + 100.0 * 8.505680084228516
Epoch 860, val loss: 0.6510923504829407
Epoch 870, training loss: 851.060546875 = 0.6119087934494019 + 100.0 * 8.504486083984375
Epoch 870, val loss: 0.650009274482727
Epoch 880, training loss: 850.9252319335938 = 0.6101864576339722 + 100.0 * 8.503150939941406
Epoch 880, val loss: 0.6489607691764832
Epoch 890, training loss: 850.7310791015625 = 0.6085351705551147 + 100.0 * 8.501225471496582
Epoch 890, val loss: 0.6479477286338806
Epoch 900, training loss: 850.5369262695312 = 0.6069489121437073 + 100.0 * 8.499300003051758
Epoch 900, val loss: 0.6470152139663696
Epoch 910, training loss: 851.0940551757812 = 0.6054194569587708 + 100.0 * 8.504886627197266
Epoch 910, val loss: 0.6459963321685791
Epoch 920, training loss: 850.3681640625 = 0.603823184967041 + 100.0 * 8.49764347076416
Epoch 920, val loss: 0.6453025937080383
Epoch 930, training loss: 850.1383056640625 = 0.6023473739624023 + 100.0 * 8.495359420776367
Epoch 930, val loss: 0.644432544708252
Epoch 940, training loss: 849.930419921875 = 0.6009447574615479 + 100.0 * 8.493294715881348
Epoch 940, val loss: 0.6436347961425781
Epoch 950, training loss: 849.796875 = 0.5995995998382568 + 100.0 * 8.491972923278809
Epoch 950, val loss: 0.6429529786109924
Epoch 960, training loss: 849.647705078125 = 0.5982997417449951 + 100.0 * 8.490493774414062
Epoch 960, val loss: 0.6422761678695679
Epoch 970, training loss: 849.572509765625 = 0.5970250964164734 + 100.0 * 8.489754676818848
Epoch 970, val loss: 0.6416307687759399
Epoch 980, training loss: 849.5037231445312 = 0.5957274436950684 + 100.0 * 8.489080429077148
Epoch 980, val loss: 0.6410415768623352
Epoch 990, training loss: 849.2940673828125 = 0.5944628119468689 + 100.0 * 8.486995697021484
Epoch 990, val loss: 0.6403101086616516
Epoch 1000, training loss: 849.1937255859375 = 0.593267560005188 + 100.0 * 8.486004829406738
Epoch 1000, val loss: 0.6396763920783997
Epoch 1010, training loss: 849.09228515625 = 0.5921158194541931 + 100.0 * 8.485001564025879
Epoch 1010, val loss: 0.6391096115112305
Epoch 1020, training loss: 849.6395263671875 = 0.590984582901001 + 100.0 * 8.490485191345215
Epoch 1020, val loss: 0.6384196877479553
Epoch 1030, training loss: 848.9754028320312 = 0.5897822380065918 + 100.0 * 8.483856201171875
Epoch 1030, val loss: 0.6379648447036743
Epoch 1040, training loss: 848.8729248046875 = 0.5886644721031189 + 100.0 * 8.482842445373535
Epoch 1040, val loss: 0.6374024748802185
Epoch 1050, training loss: 848.71533203125 = 0.5875867605209351 + 100.0 * 8.481277465820312
Epoch 1050, val loss: 0.6368237733840942
Epoch 1060, training loss: 848.6591796875 = 0.586533784866333 + 100.0 * 8.48072624206543
Epoch 1060, val loss: 0.6362569332122803
Epoch 1070, training loss: 848.9910278320312 = 0.5854793190956116 + 100.0 * 8.484055519104004
Epoch 1070, val loss: 0.6355926990509033
Epoch 1080, training loss: 848.6287841796875 = 0.5843956470489502 + 100.0 * 8.480443954467773
Epoch 1080, val loss: 0.635281503200531
Epoch 1090, training loss: 848.4281005859375 = 0.5833568572998047 + 100.0 * 8.478446960449219
Epoch 1090, val loss: 0.6346292495727539
Epoch 1100, training loss: 848.3640747070312 = 0.5823496580123901 + 100.0 * 8.47781753540039
Epoch 1100, val loss: 0.6340994834899902
Epoch 1110, training loss: 848.3020629882812 = 0.581361711025238 + 100.0 * 8.47720718383789
Epoch 1110, val loss: 0.63362717628479
Epoch 1120, training loss: 848.418212890625 = 0.5803801417350769 + 100.0 * 8.478378295898438
Epoch 1120, val loss: 0.6330456137657166
Epoch 1130, training loss: 848.1959228515625 = 0.5793600082397461 + 100.0 * 8.476165771484375
Epoch 1130, val loss: 0.6325588226318359
Epoch 1140, training loss: 848.1612548828125 = 0.5783751606941223 + 100.0 * 8.475829124450684
Epoch 1140, val loss: 0.6320282220840454
Epoch 1150, training loss: 848.0704345703125 = 0.5774232745170593 + 100.0 * 8.474929809570312
Epoch 1150, val loss: 0.6315256357192993
Epoch 1160, training loss: 847.9995727539062 = 0.5764942169189453 + 100.0 * 8.474230766296387
Epoch 1160, val loss: 0.6310686469078064
Epoch 1170, training loss: 847.9388427734375 = 0.5755710601806641 + 100.0 * 8.4736328125
Epoch 1170, val loss: 0.6305912137031555
Epoch 1180, training loss: 847.9984130859375 = 0.5746524333953857 + 100.0 * 8.474237442016602
Epoch 1180, val loss: 0.630177915096283
Epoch 1190, training loss: 847.8618774414062 = 0.5736975073814392 + 100.0 * 8.472882270812988
Epoch 1190, val loss: 0.6295877695083618
Epoch 1200, training loss: 847.898681640625 = 0.5727619528770447 + 100.0 * 8.473258972167969
Epoch 1200, val loss: 0.6291646957397461
Epoch 1210, training loss: 847.7496948242188 = 0.5718478560447693 + 100.0 * 8.471778869628906
Epoch 1210, val loss: 0.6285690069198608
Epoch 1220, training loss: 847.6571044921875 = 0.5709639191627502 + 100.0 * 8.470861434936523
Epoch 1220, val loss: 0.6281106472015381
Epoch 1230, training loss: 847.6519165039062 = 0.5700923204421997 + 100.0 * 8.470818519592285
Epoch 1230, val loss: 0.62769615650177
Epoch 1240, training loss: 848.0057373046875 = 0.5691918730735779 + 100.0 * 8.474365234375
Epoch 1240, val loss: 0.6272565722465515
Epoch 1250, training loss: 847.5422973632812 = 0.5682699084281921 + 100.0 * 8.46973991394043
Epoch 1250, val loss: 0.626675546169281
Epoch 1260, training loss: 847.5031127929688 = 0.5673971176147461 + 100.0 * 8.46935749053955
Epoch 1260, val loss: 0.6260634064674377
Epoch 1270, training loss: 847.4142456054688 = 0.5665520429611206 + 100.0 * 8.468477249145508
Epoch 1270, val loss: 0.6256967186927795
Epoch 1280, training loss: 847.353759765625 = 0.5657216310501099 + 100.0 * 8.467880249023438
Epoch 1280, val loss: 0.6251916289329529
Epoch 1290, training loss: 847.4248046875 = 0.5648884177207947 + 100.0 * 8.468599319458008
Epoch 1290, val loss: 0.6246756911277771
Epoch 1300, training loss: 847.3131713867188 = 0.5640236139297485 + 100.0 * 8.467491149902344
Epoch 1300, val loss: 0.6242678761482239
Epoch 1310, training loss: 847.2403564453125 = 0.5631561875343323 + 100.0 * 8.466772079467773
Epoch 1310, val loss: 0.6237521767616272
Epoch 1320, training loss: 847.1549072265625 = 0.56231689453125 + 100.0 * 8.465926170349121
Epoch 1320, val loss: 0.6233037114143372
Epoch 1330, training loss: 847.0997314453125 = 0.561479389667511 + 100.0 * 8.46538257598877
Epoch 1330, val loss: 0.6228286027908325
Epoch 1340, training loss: 847.1478271484375 = 0.5606594681739807 + 100.0 * 8.465871810913086
Epoch 1340, val loss: 0.6224089860916138
Epoch 1350, training loss: 847.1098022460938 = 0.5598089694976807 + 100.0 * 8.465499877929688
Epoch 1350, val loss: 0.6219310760498047
Epoch 1360, training loss: 846.9833374023438 = 0.5589515566825867 + 100.0 * 8.46424388885498
Epoch 1360, val loss: 0.6214115619659424
Epoch 1370, training loss: 846.9346313476562 = 0.5581173896789551 + 100.0 * 8.463765144348145
Epoch 1370, val loss: 0.6208726763725281
Epoch 1380, training loss: 846.8694458007812 = 0.5573029518127441 + 100.0 * 8.46312141418457
Epoch 1380, val loss: 0.6203873157501221
Epoch 1390, training loss: 846.9945068359375 = 0.5564849376678467 + 100.0 * 8.464380264282227
Epoch 1390, val loss: 0.6198264956474304
Epoch 1400, training loss: 846.8467407226562 = 0.5556234121322632 + 100.0 * 8.462911605834961
Epoch 1400, val loss: 0.6194924116134644
Epoch 1410, training loss: 846.7387084960938 = 0.5547885894775391 + 100.0 * 8.461838722229004
Epoch 1410, val loss: 0.6189931631088257
Epoch 1420, training loss: 846.6879272460938 = 0.5539730787277222 + 100.0 * 8.461339950561523
Epoch 1420, val loss: 0.6185351610183716
Epoch 1430, training loss: 846.6492309570312 = 0.5531600117683411 + 100.0 * 8.460960388183594
Epoch 1430, val loss: 0.6180860996246338
Epoch 1440, training loss: 846.8804931640625 = 0.5523359775543213 + 100.0 * 8.463281631469727
Epoch 1440, val loss: 0.617570698261261
Epoch 1450, training loss: 846.6851806640625 = 0.5514806509017944 + 100.0 * 8.461337089538574
Epoch 1450, val loss: 0.6171849370002747
Epoch 1460, training loss: 846.7363891601562 = 0.550627589225769 + 100.0 * 8.461857795715332
Epoch 1460, val loss: 0.6166515350341797
Epoch 1470, training loss: 846.4877319335938 = 0.5497789978981018 + 100.0 * 8.459379196166992
Epoch 1470, val loss: 0.6160895824432373
Epoch 1480, training loss: 846.4609375 = 0.5489547848701477 + 100.0 * 8.45911979675293
Epoch 1480, val loss: 0.6155684590339661
Epoch 1490, training loss: 846.4374389648438 = 0.5481324791908264 + 100.0 * 8.458892822265625
Epoch 1490, val loss: 0.6150850653648376
Epoch 1500, training loss: 846.4474487304688 = 0.5472995638847351 + 100.0 * 8.459001541137695
Epoch 1500, val loss: 0.6145824790000916
Epoch 1510, training loss: 846.4972534179688 = 0.5464465022087097 + 100.0 * 8.459507942199707
Epoch 1510, val loss: 0.614084780216217
Epoch 1520, training loss: 846.3641967773438 = 0.5455851554870605 + 100.0 * 8.458186149597168
Epoch 1520, val loss: 0.6137958765029907
Epoch 1530, training loss: 846.311279296875 = 0.5447364449501038 + 100.0 * 8.45766544342041
Epoch 1530, val loss: 0.6132094860076904
Epoch 1540, training loss: 846.3555297851562 = 0.5438836216926575 + 100.0 * 8.45811653137207
Epoch 1540, val loss: 0.612705409526825
Epoch 1550, training loss: 846.2032470703125 = 0.5430274605751038 + 100.0 * 8.456602096557617
Epoch 1550, val loss: 0.6122825741767883
Epoch 1560, training loss: 846.1649169921875 = 0.5421939492225647 + 100.0 * 8.45622730255127
Epoch 1560, val loss: 0.6118332743644714
Epoch 1570, training loss: 846.1576538085938 = 0.5413587689399719 + 100.0 * 8.456162452697754
Epoch 1570, val loss: 0.6114420890808105
Epoch 1580, training loss: 846.2570190429688 = 0.5405020117759705 + 100.0 * 8.457164764404297
Epoch 1580, val loss: 0.6109573245048523
Epoch 1590, training loss: 846.1367797851562 = 0.539624810218811 + 100.0 * 8.455971717834473
Epoch 1590, val loss: 0.6103112101554871
Epoch 1600, training loss: 846.1619873046875 = 0.5387600660324097 + 100.0 * 8.456232070922852
Epoch 1600, val loss: 0.6099993586540222
Epoch 1610, training loss: 846.036376953125 = 0.5378766655921936 + 100.0 * 8.454984664916992
Epoch 1610, val loss: 0.6092990040779114
Epoch 1620, training loss: 845.9591674804688 = 0.5370148420333862 + 100.0 * 8.454221725463867
Epoch 1620, val loss: 0.6088867783546448
Epoch 1630, training loss: 845.89599609375 = 0.5361650586128235 + 100.0 * 8.453598022460938
Epoch 1630, val loss: 0.6083649396896362
Epoch 1640, training loss: 845.95361328125 = 0.5353103280067444 + 100.0 * 8.454182624816895
Epoch 1640, val loss: 0.6078758239746094
Epoch 1650, training loss: 846.0122680664062 = 0.5344184637069702 + 100.0 * 8.454778671264648
Epoch 1650, val loss: 0.6073940396308899
Epoch 1660, training loss: 845.8406372070312 = 0.5335104465484619 + 100.0 * 8.453071594238281
Epoch 1660, val loss: 0.6069551706314087
Epoch 1670, training loss: 845.77001953125 = 0.532633364200592 + 100.0 * 8.452373504638672
Epoch 1670, val loss: 0.6065170764923096
Epoch 1680, training loss: 845.8606567382812 = 0.5317567586898804 + 100.0 * 8.453289031982422
Epoch 1680, val loss: 0.6060938239097595
Epoch 1690, training loss: 845.6986083984375 = 0.5308606624603271 + 100.0 * 8.451677322387695
Epoch 1690, val loss: 0.605536937713623
Epoch 1700, training loss: 845.686279296875 = 0.5299788117408752 + 100.0 * 8.451562881469727
Epoch 1700, val loss: 0.6050996780395508
Epoch 1710, training loss: 845.8923950195312 = 0.5290829539299011 + 100.0 * 8.453633308410645
Epoch 1710, val loss: 0.6047210693359375
Epoch 1720, training loss: 845.65869140625 = 0.5281398296356201 + 100.0 * 8.451305389404297
Epoch 1720, val loss: 0.603931725025177
Epoch 1730, training loss: 845.5910034179688 = 0.5272330045700073 + 100.0 * 8.450637817382812
Epoch 1730, val loss: 0.6034966707229614
Epoch 1740, training loss: 845.5032348632812 = 0.5263316631317139 + 100.0 * 8.449769020080566
Epoch 1740, val loss: 0.602993905544281
Epoch 1750, training loss: 845.4921264648438 = 0.5254382491111755 + 100.0 * 8.449666976928711
Epoch 1750, val loss: 0.6025869846343994
Epoch 1760, training loss: 845.6803588867188 = 0.5245261192321777 + 100.0 * 8.451558113098145
Epoch 1760, val loss: 0.6020975112915039
Epoch 1770, training loss: 845.8153686523438 = 0.5235783457756042 + 100.0 * 8.45291805267334
Epoch 1770, val loss: 0.601318359375
Epoch 1780, training loss: 845.4454956054688 = 0.5225951075553894 + 100.0 * 8.44922924041748
Epoch 1780, val loss: 0.6010136008262634
Epoch 1790, training loss: 845.3211669921875 = 0.5216507911682129 + 100.0 * 8.44799518585205
Epoch 1790, val loss: 0.6004853248596191
Epoch 1800, training loss: 845.2993774414062 = 0.5207258462905884 + 100.0 * 8.447786331176758
Epoch 1800, val loss: 0.5999979972839355
Epoch 1810, training loss: 845.2888793945312 = 0.5198025703430176 + 100.0 * 8.447690963745117
Epoch 1810, val loss: 0.5996175408363342
Epoch 1820, training loss: 845.55224609375 = 0.5188595652580261 + 100.0 * 8.450333595275879
Epoch 1820, val loss: 0.5991957187652588
Epoch 1830, training loss: 845.2777099609375 = 0.5178713202476501 + 100.0 * 8.447598457336426
Epoch 1830, val loss: 0.5983970761299133
Epoch 1840, training loss: 845.2548217773438 = 0.5168989300727844 + 100.0 * 8.447379112243652
Epoch 1840, val loss: 0.5980054140090942
Epoch 1850, training loss: 845.176513671875 = 0.5159360766410828 + 100.0 * 8.446605682373047
Epoch 1850, val loss: 0.5974006056785583
Epoch 1860, training loss: 845.1592407226562 = 0.5149732232093811 + 100.0 * 8.446442604064941
Epoch 1860, val loss: 0.5970458984375
Epoch 1870, training loss: 845.410400390625 = 0.5139865875244141 + 100.0 * 8.44896411895752
Epoch 1870, val loss: 0.596528947353363
Epoch 1880, training loss: 845.1386108398438 = 0.5129666924476624 + 100.0 * 8.446256637573242
Epoch 1880, val loss: 0.5957770943641663
Epoch 1890, training loss: 845.022216796875 = 0.5119829773902893 + 100.0 * 8.44510269165039
Epoch 1890, val loss: 0.5953974723815918
Epoch 1900, training loss: 844.9776611328125 = 0.5110031962394714 + 100.0 * 8.444666862487793
Epoch 1900, val loss: 0.5948696136474609
Epoch 1910, training loss: 845.1182250976562 = 0.5100181698799133 + 100.0 * 8.44608211517334
Epoch 1910, val loss: 0.5943020582199097
Epoch 1920, training loss: 844.9422607421875 = 0.5089799165725708 + 100.0 * 8.44433307647705
Epoch 1920, val loss: 0.5937702655792236
Epoch 1930, training loss: 844.896240234375 = 0.5079484581947327 + 100.0 * 8.443882942199707
Epoch 1930, val loss: 0.5932401418685913
Epoch 1940, training loss: 844.8823852539062 = 0.5069271326065063 + 100.0 * 8.443755149841309
Epoch 1940, val loss: 0.5926640033721924
Epoch 1950, training loss: 844.8073120117188 = 0.5059046745300293 + 100.0 * 8.443014144897461
Epoch 1950, val loss: 0.5921392440795898
Epoch 1960, training loss: 844.8688354492188 = 0.5048868656158447 + 100.0 * 8.443639755249023
Epoch 1960, val loss: 0.5915457606315613
Epoch 1970, training loss: 845.0005493164062 = 0.5038161277770996 + 100.0 * 8.444967269897461
Epoch 1970, val loss: 0.5910251140594482
Epoch 1980, training loss: 844.7710571289062 = 0.5027253031730652 + 100.0 * 8.442683219909668
Epoch 1980, val loss: 0.5905561447143555
Epoch 1990, training loss: 844.6945190429688 = 0.50167316198349 + 100.0 * 8.44192886352539
Epoch 1990, val loss: 0.5899844765663147
Epoch 2000, training loss: 844.6427612304688 = 0.5006213784217834 + 100.0 * 8.441421508789062
Epoch 2000, val loss: 0.5895236134529114
Epoch 2010, training loss: 844.6165161132812 = 0.4995688199996948 + 100.0 * 8.441169738769531
Epoch 2010, val loss: 0.5889808535575867
Epoch 2020, training loss: 844.919921875 = 0.49850890040397644 + 100.0 * 8.4442138671875
Epoch 2020, val loss: 0.5885958075523376
Epoch 2030, training loss: 844.841796875 = 0.49737563729286194 + 100.0 * 8.44344425201416
Epoch 2030, val loss: 0.5877876877784729
Epoch 2040, training loss: 844.5921020507812 = 0.49625152349472046 + 100.0 * 8.440958023071289
Epoch 2040, val loss: 0.5873240828514099
Epoch 2050, training loss: 844.529541015625 = 0.4951533377170563 + 100.0 * 8.440343856811523
Epoch 2050, val loss: 0.5866928100585938
Epoch 2060, training loss: 844.478759765625 = 0.4940584599971771 + 100.0 * 8.439846992492676
Epoch 2060, val loss: 0.5861198306083679
Epoch 2070, training loss: 844.44287109375 = 0.4929616451263428 + 100.0 * 8.439498901367188
Epoch 2070, val loss: 0.5856426358222961
Epoch 2080, training loss: 844.7199096679688 = 0.49185097217559814 + 100.0 * 8.442280769348145
Epoch 2080, val loss: 0.5852294564247131
Epoch 2090, training loss: 844.676513671875 = 0.49064454436302185 + 100.0 * 8.441858291625977
Epoch 2090, val loss: 0.5845381617546082
Epoch 2100, training loss: 844.4788208007812 = 0.48945242166519165 + 100.0 * 8.43989372253418
Epoch 2100, val loss: 0.5837430357933044
Epoch 2110, training loss: 844.3665771484375 = 0.4883011281490326 + 100.0 * 8.438782691955566
Epoch 2110, val loss: 0.5833513736724854
Epoch 2120, training loss: 844.3218994140625 = 0.4871680736541748 + 100.0 * 8.438346862792969
Epoch 2120, val loss: 0.5827813148498535
Epoch 2130, training loss: 844.2926025390625 = 0.4860328137874603 + 100.0 * 8.438065528869629
Epoch 2130, val loss: 0.5822193622589111
Epoch 2140, training loss: 844.2669067382812 = 0.48488757014274597 + 100.0 * 8.437820434570312
Epoch 2140, val loss: 0.5816901326179504
Epoch 2150, training loss: 844.321533203125 = 0.48373398184776306 + 100.0 * 8.43837833404541
Epoch 2150, val loss: 0.5810585618019104
Epoch 2160, training loss: 844.602783203125 = 0.4825352728366852 + 100.0 * 8.441202163696289
Epoch 2160, val loss: 0.5804494023323059
Epoch 2170, training loss: 844.3236083984375 = 0.48129767179489136 + 100.0 * 8.438423156738281
Epoch 2170, val loss: 0.5801237225532532
Epoch 2180, training loss: 844.2286987304688 = 0.4801073372364044 + 100.0 * 8.437485694885254
Epoch 2180, val loss: 0.5794946551322937
Epoch 2190, training loss: 844.5086669921875 = 0.4789208471775055 + 100.0 * 8.44029712677002
Epoch 2190, val loss: 0.579055905342102
Epoch 2200, training loss: 844.2542114257812 = 0.47770437598228455 + 100.0 * 8.437765121459961
Epoch 2200, val loss: 0.5784894824028015
Epoch 2210, training loss: 844.14306640625 = 0.47649717330932617 + 100.0 * 8.436665534973145
Epoch 2210, val loss: 0.578016996383667
Epoch 2220, training loss: 844.0935668945312 = 0.47530293464660645 + 100.0 * 8.436182975769043
Epoch 2220, val loss: 0.5774986743927002
Epoch 2230, training loss: 844.0772705078125 = 0.47409865260124207 + 100.0 * 8.436031341552734
Epoch 2230, val loss: 0.5769773721694946
Epoch 2240, training loss: 844.5238647460938 = 0.4728822410106659 + 100.0 * 8.440509796142578
Epoch 2240, val loss: 0.5763075351715088
Epoch 2250, training loss: 844.2210083007812 = 0.4715850055217743 + 100.0 * 8.437494277954102
Epoch 2250, val loss: 0.5760259032249451
Epoch 2260, training loss: 844.06689453125 = 0.47032660245895386 + 100.0 * 8.435965538024902
Epoch 2260, val loss: 0.5754319429397583
Epoch 2270, training loss: 843.9815673828125 = 0.4690912365913391 + 100.0 * 8.435124397277832
Epoch 2270, val loss: 0.5749503374099731
Epoch 2280, training loss: 843.9679565429688 = 0.4678613543510437 + 100.0 * 8.435001373291016
Epoch 2280, val loss: 0.5744993090629578
Epoch 2290, training loss: 843.9744873046875 = 0.4666229784488678 + 100.0 * 8.435078620910645
Epoch 2290, val loss: 0.5739755630493164
Epoch 2300, training loss: 844.3909912109375 = 0.46536973118782043 + 100.0 * 8.439255714416504
Epoch 2300, val loss: 0.5733771920204163
Epoch 2310, training loss: 843.98583984375 = 0.46404197812080383 + 100.0 * 8.43521785736084
Epoch 2310, val loss: 0.5730863213539124
Epoch 2320, training loss: 843.91162109375 = 0.46275949478149414 + 100.0 * 8.434488296508789
Epoch 2320, val loss: 0.5725883841514587
Epoch 2330, training loss: 843.873779296875 = 0.46149200201034546 + 100.0 * 8.434123039245605
Epoch 2330, val loss: 0.5721322894096375
Epoch 2340, training loss: 843.8514404296875 = 0.46022605895996094 + 100.0 * 8.43391227722168
Epoch 2340, val loss: 0.5717374682426453
Epoch 2350, training loss: 843.8666381835938 = 0.4589502811431885 + 100.0 * 8.434077262878418
Epoch 2350, val loss: 0.5713621973991394
Epoch 2360, training loss: 844.30126953125 = 0.45765185356140137 + 100.0 * 8.438436508178711
Epoch 2360, val loss: 0.5710920095443726
Epoch 2370, training loss: 843.933837890625 = 0.4562962055206299 + 100.0 * 8.434775352478027
Epoch 2370, val loss: 0.5701936483383179
Epoch 2380, training loss: 843.8276977539062 = 0.45497119426727295 + 100.0 * 8.433727264404297
Epoch 2380, val loss: 0.5698348879814148
Epoch 2390, training loss: 843.8155517578125 = 0.4536620080471039 + 100.0 * 8.433618545532227
Epoch 2390, val loss: 0.5694304704666138
Epoch 2400, training loss: 843.7718505859375 = 0.45234647393226624 + 100.0 * 8.433195114135742
Epoch 2400, val loss: 0.5689943432807922
Epoch 2410, training loss: 843.745361328125 = 0.45102670788764954 + 100.0 * 8.432943344116211
Epoch 2410, val loss: 0.5687251687049866
Epoch 2420, training loss: 844.0797119140625 = 0.44970616698265076 + 100.0 * 8.436300277709961
Epoch 2420, val loss: 0.5685011148452759
Epoch 2430, training loss: 843.7035522460938 = 0.44830793142318726 + 100.0 * 8.432552337646484
Epoch 2430, val loss: 0.5677385330200195
Epoch 2440, training loss: 843.6712036132812 = 0.4469508230686188 + 100.0 * 8.432242393493652
Epoch 2440, val loss: 0.5674356818199158
Epoch 2450, training loss: 843.6500244140625 = 0.4456193745136261 + 100.0 * 8.43204402923584
Epoch 2450, val loss: 0.5670093297958374
Epoch 2460, training loss: 843.639892578125 = 0.4442943334579468 + 100.0 * 8.43195629119873
Epoch 2460, val loss: 0.5666368007659912
Epoch 2470, training loss: 843.6226196289062 = 0.4429640769958496 + 100.0 * 8.431796073913574
Epoch 2470, val loss: 0.5663504600524902
Epoch 2480, training loss: 843.9437255859375 = 0.44162896275520325 + 100.0 * 8.435020446777344
Epoch 2480, val loss: 0.5658397078514099
Epoch 2490, training loss: 843.831298828125 = 0.4402117431163788 + 100.0 * 8.433911323547363
Epoch 2490, val loss: 0.5656179189682007
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7615423642820902
0.8135188002608129
=== training gcn model ===
Epoch 0, training loss: 1059.3203125 = 1.0914804935455322 + 100.0 * 10.58228874206543
Epoch 0, val loss: 1.093244194984436
Epoch 10, training loss: 1059.280029296875 = 1.0875989198684692 + 100.0 * 10.581923484802246
Epoch 10, val loss: 1.089297890663147
Epoch 20, training loss: 1059.0987548828125 = 1.0834524631500244 + 100.0 * 10.58015251159668
Epoch 20, val loss: 1.0850598812103271
Epoch 30, training loss: 1058.264404296875 = 1.0788848400115967 + 100.0 * 10.571855545043945
Epoch 30, val loss: 1.0803755521774292
Epoch 40, training loss: 1055.1419677734375 = 1.0737335681915283 + 100.0 * 10.540681838989258
Epoch 40, val loss: 1.0750563144683838
Epoch 50, training loss: 1046.7611083984375 = 1.0676196813583374 + 100.0 * 10.456934928894043
Epoch 50, val loss: 1.0687692165374756
Epoch 60, training loss: 1028.872314453125 = 1.0614404678344727 + 100.0 * 10.278108596801758
Epoch 60, val loss: 1.0625247955322266
Epoch 70, training loss: 1013.198974609375 = 1.0558409690856934 + 100.0 * 10.121431350708008
Epoch 70, val loss: 1.057020902633667
Epoch 80, training loss: 996.3060913085938 = 1.0518767833709717 + 100.0 * 9.952542304992676
Epoch 80, val loss: 1.0531624555587769
Epoch 90, training loss: 961.951416015625 = 1.0481332540512085 + 100.0 * 9.60903263092041
Epoch 90, val loss: 1.049534559249878
Epoch 100, training loss: 935.0790405273438 = 1.0442941188812256 + 100.0 * 9.340347290039062
Epoch 100, val loss: 1.0459365844726562
Epoch 110, training loss: 923.7637329101562 = 1.0410974025726318 + 100.0 * 9.227226257324219
Epoch 110, val loss: 1.042860507965088
Epoch 120, training loss: 914.9950561523438 = 1.0378262996673584 + 100.0 * 9.139572143554688
Epoch 120, val loss: 1.0396499633789062
Epoch 130, training loss: 909.58984375 = 1.0352281332015991 + 100.0 * 9.085546493530273
Epoch 130, val loss: 1.0370334386825562
Epoch 140, training loss: 906.671875 = 1.0324232578277588 + 100.0 * 9.056394577026367
Epoch 140, val loss: 1.0342113971710205
Epoch 150, training loss: 902.6259765625 = 1.0293546915054321 + 100.0 * 9.015966415405273
Epoch 150, val loss: 1.0313389301300049
Epoch 160, training loss: 897.9296264648438 = 1.0266635417938232 + 100.0 * 8.969029426574707
Epoch 160, val loss: 1.0288954973220825
Epoch 170, training loss: 893.9841918945312 = 1.0242342948913574 + 100.0 * 8.92959976196289
Epoch 170, val loss: 1.0266348123550415
Epoch 180, training loss: 891.4965209960938 = 1.0212640762329102 + 100.0 * 8.904752731323242
Epoch 180, val loss: 1.023768424987793
Epoch 190, training loss: 888.708251953125 = 1.0176429748535156 + 100.0 * 8.876906394958496
Epoch 190, val loss: 1.0202878713607788
Epoch 200, training loss: 885.2772827148438 = 1.0140513181686401 + 100.0 * 8.842632293701172
Epoch 200, val loss: 1.0170069932937622
Epoch 210, training loss: 881.9315185546875 = 1.0107685327529907 + 100.0 * 8.809207916259766
Epoch 210, val loss: 1.013962984085083
Epoch 220, training loss: 878.6024169921875 = 1.007099986076355 + 100.0 * 8.77595329284668
Epoch 220, val loss: 1.0105955600738525
Epoch 230, training loss: 876.2130126953125 = 1.0027897357940674 + 100.0 * 8.75210189819336
Epoch 230, val loss: 1.006577730178833
Epoch 240, training loss: 874.4705810546875 = 0.9977864027023315 + 100.0 * 8.73472785949707
Epoch 240, val loss: 1.0019605159759521
Epoch 250, training loss: 872.8341674804688 = 0.9922583699226379 + 100.0 * 8.718419075012207
Epoch 250, val loss: 0.9968016743659973
Epoch 260, training loss: 871.56884765625 = 0.9862584471702576 + 100.0 * 8.705825805664062
Epoch 260, val loss: 0.9912571310997009
Epoch 270, training loss: 870.3600463867188 = 0.9798394441604614 + 100.0 * 8.693801879882812
Epoch 270, val loss: 0.9853709936141968
Epoch 280, training loss: 869.2681274414062 = 0.9729652404785156 + 100.0 * 8.682951927185059
Epoch 280, val loss: 0.9790780544281006
Epoch 290, training loss: 868.3208618164062 = 0.9656434059143066 + 100.0 * 8.673552513122559
Epoch 290, val loss: 0.9724364280700684
Epoch 300, training loss: 867.3947143554688 = 0.9579542279243469 + 100.0 * 8.66436767578125
Epoch 300, val loss: 0.9654443264007568
Epoch 310, training loss: 866.6242065429688 = 0.9498806595802307 + 100.0 * 8.656743049621582
Epoch 310, val loss: 0.9581176042556763
Epoch 320, training loss: 865.8809204101562 = 0.9413995146751404 + 100.0 * 8.649394989013672
Epoch 320, val loss: 0.9504621624946594
Epoch 330, training loss: 865.1181030273438 = 0.9326592087745667 + 100.0 * 8.641854286193848
Epoch 330, val loss: 0.9425991177558899
Epoch 340, training loss: 864.66552734375 = 0.9236142635345459 + 100.0 * 8.637419700622559
Epoch 340, val loss: 0.9344612956047058
Epoch 350, training loss: 863.7070922851562 = 0.9143291115760803 + 100.0 * 8.627927780151367
Epoch 350, val loss: 0.926155149936676
Epoch 360, training loss: 862.9847412109375 = 0.9049146175384521 + 100.0 * 8.620798110961914
Epoch 360, val loss: 0.9177632927894592
Epoch 370, training loss: 862.861328125 = 0.8953695297241211 + 100.0 * 8.619659423828125
Epoch 370, val loss: 0.9092972874641418
Epoch 380, training loss: 862.0714721679688 = 0.8857670426368713 + 100.0 * 8.611857414245605
Epoch 380, val loss: 0.900708794593811
Epoch 390, training loss: 861.616943359375 = 0.8761317133903503 + 100.0 * 8.607407569885254
Epoch 390, val loss: 0.8921879529953003
Epoch 400, training loss: 861.2908325195312 = 0.8664884567260742 + 100.0 * 8.604243278503418
Epoch 400, val loss: 0.8836684226989746
Epoch 410, training loss: 860.9741821289062 = 0.8568856716156006 + 100.0 * 8.601173400878906
Epoch 410, val loss: 0.8752081990242004
Epoch 420, training loss: 860.6972045898438 = 0.8473613262176514 + 100.0 * 8.598498344421387
Epoch 420, val loss: 0.8668290972709656
Epoch 430, training loss: 860.4498291015625 = 0.8379450440406799 + 100.0 * 8.596118927001953
Epoch 430, val loss: 0.8585509657859802
Epoch 440, training loss: 860.2653198242188 = 0.8286265134811401 + 100.0 * 8.594367027282715
Epoch 440, val loss: 0.850354790687561
Epoch 450, training loss: 859.9876708984375 = 0.8194283246994019 + 100.0 * 8.591682434082031
Epoch 450, val loss: 0.8422917127609253
Epoch 460, training loss: 859.6908569335938 = 0.8103821873664856 + 100.0 * 8.588805198669434
Epoch 460, val loss: 0.8343669772148132
Epoch 470, training loss: 859.3980102539062 = 0.8014850616455078 + 100.0 * 8.585965156555176
Epoch 470, val loss: 0.8265364170074463
Epoch 480, training loss: 859.1548461914062 = 0.7927321195602417 + 100.0 * 8.58362102508545
Epoch 480, val loss: 0.8187803626060486
Epoch 490, training loss: 859.0928955078125 = 0.7839906811714172 + 100.0 * 8.583088874816895
Epoch 490, val loss: 0.8111803531646729
Epoch 500, training loss: 858.547119140625 = 0.7754819393157959 + 100.0 * 8.577716827392578
Epoch 500, val loss: 0.8035715818405151
Epoch 510, training loss: 858.2379760742188 = 0.7671035528182983 + 100.0 * 8.574708938598633
Epoch 510, val loss: 0.7961001992225647
Epoch 520, training loss: 857.884521484375 = 0.758810818195343 + 100.0 * 8.571257591247559
Epoch 520, val loss: 0.7887336015701294
Epoch 530, training loss: 857.53076171875 = 0.7506397366523743 + 100.0 * 8.567801475524902
Epoch 530, val loss: 0.7814357280731201
Epoch 540, training loss: 857.2360229492188 = 0.7425325512886047 + 100.0 * 8.564934730529785
Epoch 540, val loss: 0.774200975894928
Epoch 550, training loss: 857.2662963867188 = 0.7344674468040466 + 100.0 * 8.56531810760498
Epoch 550, val loss: 0.766895592212677
Epoch 560, training loss: 856.5632934570312 = 0.7265040874481201 + 100.0 * 8.558367729187012
Epoch 560, val loss: 0.7597140669822693
Epoch 570, training loss: 856.2109375 = 0.7186357975006104 + 100.0 * 8.554923057556152
Epoch 570, val loss: 0.7526388764381409
Epoch 580, training loss: 855.948974609375 = 0.7108328342437744 + 100.0 * 8.55238151550293
Epoch 580, val loss: 0.7455969452857971
Epoch 590, training loss: 855.9141235351562 = 0.7031391263008118 + 100.0 * 8.552109718322754
Epoch 590, val loss: 0.7385040521621704
Epoch 600, training loss: 855.491943359375 = 0.6954154968261719 + 100.0 * 8.547965049743652
Epoch 600, val loss: 0.7316511869430542
Epoch 610, training loss: 855.1076049804688 = 0.6879432201385498 + 100.0 * 8.544197082519531
Epoch 610, val loss: 0.724872350692749
Epoch 620, training loss: 854.8474731445312 = 0.6806089282035828 + 100.0 * 8.541668891906738
Epoch 620, val loss: 0.7182220816612244
Epoch 630, training loss: 854.5827026367188 = 0.6734145879745483 + 100.0 * 8.539093017578125
Epoch 630, val loss: 0.7117522358894348
Epoch 640, training loss: 854.328857421875 = 0.6664015054702759 + 100.0 * 8.536624908447266
Epoch 640, val loss: 0.70545494556427
Epoch 650, training loss: 854.21875 = 0.6595473289489746 + 100.0 * 8.535592079162598
Epoch 650, val loss: 0.6993697285652161
Epoch 660, training loss: 854.1574096679688 = 0.6528427600860596 + 100.0 * 8.535045623779297
Epoch 660, val loss: 0.6932435035705566
Epoch 670, training loss: 853.7413940429688 = 0.6463462114334106 + 100.0 * 8.530950546264648
Epoch 670, val loss: 0.6875240802764893
Epoch 680, training loss: 853.4613037109375 = 0.6401153206825256 + 100.0 * 8.52821159362793
Epoch 680, val loss: 0.6820109486579895
Epoch 690, training loss: 853.2337036132812 = 0.6340811848640442 + 100.0 * 8.525996208190918
Epoch 690, val loss: 0.6767556667327881
Epoch 700, training loss: 853.0455932617188 = 0.62830650806427 + 100.0 * 8.52417278289795
Epoch 700, val loss: 0.6717274188995361
Epoch 710, training loss: 853.0639038085938 = 0.6226760745048523 + 100.0 * 8.524412155151367
Epoch 710, val loss: 0.6668872237205505
Epoch 720, training loss: 852.7368774414062 = 0.6173118352890015 + 100.0 * 8.521195411682129
Epoch 720, val loss: 0.6622071266174316
Epoch 730, training loss: 852.5209350585938 = 0.6121854782104492 + 100.0 * 8.519087791442871
Epoch 730, val loss: 0.6578637957572937
Epoch 740, training loss: 852.3535766601562 = 0.6072847247123718 + 100.0 * 8.517462730407715
Epoch 740, val loss: 0.653704047203064
Epoch 750, training loss: 852.208984375 = 0.6026269197463989 + 100.0 * 8.516063690185547
Epoch 750, val loss: 0.6498103141784668
Epoch 760, training loss: 852.5692749023438 = 0.5982003211975098 + 100.0 * 8.519710540771484
Epoch 760, val loss: 0.6460585594177246
Epoch 770, training loss: 852.0005493164062 = 0.5938807725906372 + 100.0 * 8.514066696166992
Epoch 770, val loss: 0.6425871253013611
Epoch 780, training loss: 851.8714599609375 = 0.5898378491401672 + 100.0 * 8.512816429138184
Epoch 780, val loss: 0.639319896697998
Epoch 790, training loss: 851.6672973632812 = 0.5860126614570618 + 100.0 * 8.510812759399414
Epoch 790, val loss: 0.6361947655677795
Epoch 800, training loss: 851.5655517578125 = 0.5823953747749329 + 100.0 * 8.509831428527832
Epoch 800, val loss: 0.6332732439041138
Epoch 810, training loss: 852.0253295898438 = 0.5789663195610046 + 100.0 * 8.514463424682617
Epoch 810, val loss: 0.6304513216018677
Epoch 820, training loss: 851.4171142578125 = 0.5755789875984192 + 100.0 * 8.508415222167969
Epoch 820, val loss: 0.6278988718986511
Epoch 830, training loss: 851.2371826171875 = 0.5724570751190186 + 100.0 * 8.506647109985352
Epoch 830, val loss: 0.6254798173904419
Epoch 840, training loss: 851.1014404296875 = 0.5694838166236877 + 100.0 * 8.505319595336914
Epoch 840, val loss: 0.6232013702392578
Epoch 850, training loss: 850.988037109375 = 0.5666812062263489 + 100.0 * 8.504213333129883
Epoch 850, val loss: 0.6210317015647888
Epoch 860, training loss: 850.8795166015625 = 0.5639933943748474 + 100.0 * 8.503155708312988
Epoch 860, val loss: 0.6190121173858643
Epoch 870, training loss: 850.8135375976562 = 0.5614203810691833 + 100.0 * 8.502521514892578
Epoch 870, val loss: 0.6171121597290039
Epoch 880, training loss: 851.1012573242188 = 0.5589224696159363 + 100.0 * 8.505423545837402
Epoch 880, val loss: 0.6152705550193787
Epoch 890, training loss: 850.7335205078125 = 0.5565330982208252 + 100.0 * 8.50177001953125
Epoch 890, val loss: 0.6134552955627441
Epoch 900, training loss: 850.513427734375 = 0.5542919039726257 + 100.0 * 8.499590873718262
Epoch 900, val loss: 0.6117845177650452
Epoch 910, training loss: 850.410888671875 = 0.5521481037139893 + 100.0 * 8.498587608337402
Epoch 910, val loss: 0.6102139353752136
Epoch 920, training loss: 850.3108520507812 = 0.5500921607017517 + 100.0 * 8.497607231140137
Epoch 920, val loss: 0.6087204813957214
Epoch 930, training loss: 850.7249145507812 = 0.5480841994285583 + 100.0 * 8.501768112182617
Epoch 930, val loss: 0.6073104739189148
Epoch 940, training loss: 850.2708129882812 = 0.5461572408676147 + 100.0 * 8.497246742248535
Epoch 940, val loss: 0.6058546900749207
Epoch 950, training loss: 850.0638427734375 = 0.544296145439148 + 100.0 * 8.495195388793945
Epoch 950, val loss: 0.6045723557472229
Epoch 960, training loss: 849.9579467773438 = 0.5424976944923401 + 100.0 * 8.494154930114746
Epoch 960, val loss: 0.6033456325531006
Epoch 970, training loss: 849.9052734375 = 0.540782630443573 + 100.0 * 8.493644714355469
Epoch 970, val loss: 0.6021822094917297
Epoch 980, training loss: 850.1373291015625 = 0.5390954613685608 + 100.0 * 8.49598217010498
Epoch 980, val loss: 0.601051390171051
Epoch 990, training loss: 849.7550659179688 = 0.5374637246131897 + 100.0 * 8.492176055908203
Epoch 990, val loss: 0.5997819304466248
Epoch 1000, training loss: 849.599365234375 = 0.5358690023422241 + 100.0 * 8.49063491821289
Epoch 1000, val loss: 0.5987181067466736
Epoch 1010, training loss: 849.5220336914062 = 0.5343484282493591 + 100.0 * 8.489876747131348
Epoch 1010, val loss: 0.5976787209510803
Epoch 1020, training loss: 849.4330444335938 = 0.5328670144081116 + 100.0 * 8.489002227783203
Epoch 1020, val loss: 0.596668004989624
Epoch 1030, training loss: 849.8632202148438 = 0.5314136743545532 + 100.0 * 8.493317604064941
Epoch 1030, val loss: 0.5957298278808594
Epoch 1040, training loss: 849.7366333007812 = 0.5299702286720276 + 100.0 * 8.492066383361816
Epoch 1040, val loss: 0.5945465564727783
Epoch 1050, training loss: 849.2898559570312 = 0.5285384654998779 + 100.0 * 8.4876127243042
Epoch 1050, val loss: 0.5937247276306152
Epoch 1060, training loss: 849.1204833984375 = 0.5271962285041809 + 100.0 * 8.485932350158691
Epoch 1060, val loss: 0.5927608013153076
Epoch 1070, training loss: 849.0360717773438 = 0.525900661945343 + 100.0 * 8.485101699829102
Epoch 1070, val loss: 0.5919124484062195
Epoch 1080, training loss: 848.9474487304688 = 0.5246378779411316 + 100.0 * 8.484228134155273
Epoch 1080, val loss: 0.591079831123352
Epoch 1090, training loss: 848.9202880859375 = 0.5234071016311646 + 100.0 * 8.483968734741211
Epoch 1090, val loss: 0.5902886390686035
Epoch 1100, training loss: 849.0113525390625 = 0.5221799612045288 + 100.0 * 8.484891891479492
Epoch 1100, val loss: 0.5894824862480164
Epoch 1110, training loss: 848.7452392578125 = 0.5209866762161255 + 100.0 * 8.482242584228516
Epoch 1110, val loss: 0.5886071920394897
Epoch 1120, training loss: 848.7926635742188 = 0.5198195576667786 + 100.0 * 8.482728004455566
Epoch 1120, val loss: 0.5878161787986755
Epoch 1130, training loss: 848.5936889648438 = 0.5186440944671631 + 100.0 * 8.480751037597656
Epoch 1130, val loss: 0.5870466232299805
Epoch 1140, training loss: 848.55078125 = 0.5175155401229858 + 100.0 * 8.480332374572754
Epoch 1140, val loss: 0.5863104462623596
Epoch 1150, training loss: 849.1056518554688 = 0.5163998603820801 + 100.0 * 8.485892295837402
Epoch 1150, val loss: 0.5856854319572449
Epoch 1160, training loss: 848.5494384765625 = 0.5152748823165894 + 100.0 * 8.480341911315918
Epoch 1160, val loss: 0.5847000479698181
Epoch 1170, training loss: 848.3392333984375 = 0.5141832232475281 + 100.0 * 8.478250503540039
Epoch 1170, val loss: 0.5840554237365723
Epoch 1180, training loss: 848.25537109375 = 0.5131271481513977 + 100.0 * 8.477422714233398
Epoch 1180, val loss: 0.5833743214607239
Epoch 1190, training loss: 848.2035522460938 = 0.5120988488197327 + 100.0 * 8.476914405822754
Epoch 1190, val loss: 0.5827213525772095
Epoch 1200, training loss: 848.5801391601562 = 0.5110623836517334 + 100.0 * 8.480690956115723
Epoch 1200, val loss: 0.5821067094802856
Epoch 1210, training loss: 848.2280883789062 = 0.5100495219230652 + 100.0 * 8.477180480957031
Epoch 1210, val loss: 0.5813813209533691
Epoch 1220, training loss: 848.0274658203125 = 0.5090377330780029 + 100.0 * 8.475184440612793
Epoch 1220, val loss: 0.5807757377624512
Epoch 1230, training loss: 847.965576171875 = 0.5080658197402954 + 100.0 * 8.47457504272461
Epoch 1230, val loss: 0.580158531665802
Epoch 1240, training loss: 847.9899291992188 = 0.5071089267730713 + 100.0 * 8.474828720092773
Epoch 1240, val loss: 0.5795482397079468
Epoch 1250, training loss: 847.9674072265625 = 0.5061420202255249 + 100.0 * 8.47461223602295
Epoch 1250, val loss: 0.5789821743965149
Epoch 1260, training loss: 847.8450927734375 = 0.5051896572113037 + 100.0 * 8.47339916229248
Epoch 1260, val loss: 0.5783638954162598
Epoch 1270, training loss: 847.8059692382812 = 0.504267156124115 + 100.0 * 8.473016738891602
Epoch 1270, val loss: 0.5777937173843384
Epoch 1280, training loss: 847.8078002929688 = 0.5033595561981201 + 100.0 * 8.473044395446777
Epoch 1280, val loss: 0.5772019028663635
Epoch 1290, training loss: 848.0296630859375 = 0.5024608969688416 + 100.0 * 8.475272178649902
Epoch 1290, val loss: 0.5765810012817383
Epoch 1300, training loss: 847.8171997070312 = 0.501532793045044 + 100.0 * 8.473156929016113
Epoch 1300, val loss: 0.5761135220527649
Epoch 1310, training loss: 847.6111450195312 = 0.5006276965141296 + 100.0 * 8.471105575561523
Epoch 1310, val loss: 0.5755468606948853
Epoch 1320, training loss: 847.5416259765625 = 0.4997713267803192 + 100.0 * 8.470418930053711
Epoch 1320, val loss: 0.5750135779380798
Epoch 1330, training loss: 847.5022583007812 = 0.49892306327819824 + 100.0 * 8.470033645629883
Epoch 1330, val loss: 0.5744964480400085
Epoch 1340, training loss: 847.5070190429688 = 0.49808645248413086 + 100.0 * 8.470088958740234
Epoch 1340, val loss: 0.5739632248878479
Epoch 1350, training loss: 848.0111083984375 = 0.497259259223938 + 100.0 * 8.475138664245605
Epoch 1350, val loss: 0.5733985304832458
Epoch 1360, training loss: 847.5442504882812 = 0.49635234475135803 + 100.0 * 8.470479011535645
Epoch 1360, val loss: 0.5730412006378174
Epoch 1370, training loss: 847.3676147460938 = 0.4955298900604248 + 100.0 * 8.468720436096191
Epoch 1370, val loss: 0.57249516248703
Epoch 1380, training loss: 847.312255859375 = 0.49471351504325867 + 100.0 * 8.468175888061523
Epoch 1380, val loss: 0.5720551013946533
Epoch 1390, training loss: 847.2827758789062 = 0.49391981959342957 + 100.0 * 8.467888832092285
Epoch 1390, val loss: 0.5716152191162109
Epoch 1400, training loss: 847.6142578125 = 0.4931280016899109 + 100.0 * 8.471211433410645
Epoch 1400, val loss: 0.5712325572967529
Epoch 1410, training loss: 847.5673828125 = 0.49232208728790283 + 100.0 * 8.47075080871582
Epoch 1410, val loss: 0.5707299113273621
Epoch 1420, training loss: 847.1976928710938 = 0.4915117621421814 + 100.0 * 8.467061996459961
Epoch 1420, val loss: 0.5702378153800964
Epoch 1430, training loss: 847.1714477539062 = 0.4907463490962982 + 100.0 * 8.46680736541748
Epoch 1430, val loss: 0.5698465704917908
Epoch 1440, training loss: 847.1201171875 = 0.4899995028972626 + 100.0 * 8.466300964355469
Epoch 1440, val loss: 0.5694363713264465
Epoch 1450, training loss: 847.4076538085938 = 0.48926544189453125 + 100.0 * 8.469183921813965
Epoch 1450, val loss: 0.5690679550170898
Epoch 1460, training loss: 847.1458740234375 = 0.48850229382514954 + 100.0 * 8.466573715209961
Epoch 1460, val loss: 0.568612813949585
Epoch 1470, training loss: 847.0866088867188 = 0.4877670109272003 + 100.0 * 8.465988159179688
Epoch 1470, val loss: 0.5681653022766113
Epoch 1480, training loss: 846.99755859375 = 0.48704299330711365 + 100.0 * 8.465105056762695
Epoch 1480, val loss: 0.5678194165229797
Epoch 1490, training loss: 846.9513549804688 = 0.4863450229167938 + 100.0 * 8.46465015411377
Epoch 1490, val loss: 0.5674328207969666
Epoch 1500, training loss: 846.9601440429688 = 0.48564738035202026 + 100.0 * 8.464744567871094
Epoch 1500, val loss: 0.5670495629310608
Epoch 1510, training loss: 847.2586669921875 = 0.4849478006362915 + 100.0 * 8.467737197875977
Epoch 1510, val loss: 0.5666213035583496
Epoch 1520, training loss: 846.8887329101562 = 0.4842294454574585 + 100.0 * 8.464044570922852
Epoch 1520, val loss: 0.5663244128227234
Epoch 1530, training loss: 846.8768920898438 = 0.48353320360183716 + 100.0 * 8.463933944702148
Epoch 1530, val loss: 0.5660656094551086
Epoch 1540, training loss: 846.8150024414062 = 0.4828561246395111 + 100.0 * 8.463321685791016
Epoch 1540, val loss: 0.5657116770744324
Epoch 1550, training loss: 846.819580078125 = 0.4821930229663849 + 100.0 * 8.463374137878418
Epoch 1550, val loss: 0.5654150247573853
Epoch 1560, training loss: 846.8794555664062 = 0.4815240800380707 + 100.0 * 8.463979721069336
Epoch 1560, val loss: 0.5651111602783203
Epoch 1570, training loss: 846.7510986328125 = 0.48087409138679504 + 100.0 * 8.462701797485352
Epoch 1570, val loss: 0.5647677183151245
Epoch 1580, training loss: 847.330810546875 = 0.4802128076553345 + 100.0 * 8.468505859375
Epoch 1580, val loss: 0.5646497011184692
Epoch 1590, training loss: 846.9716796875 = 0.4795302748680115 + 100.0 * 8.464920997619629
Epoch 1590, val loss: 0.5639963150024414
Epoch 1600, training loss: 846.6548461914062 = 0.47885268926620483 + 100.0 * 8.461759567260742
Epoch 1600, val loss: 0.5638103485107422
Epoch 1610, training loss: 846.5877075195312 = 0.4782225489616394 + 100.0 * 8.461094856262207
Epoch 1610, val loss: 0.5635302066802979
Epoch 1620, training loss: 846.5512084960938 = 0.4776156544685364 + 100.0 * 8.460736274719238
Epoch 1620, val loss: 0.5632138848304749
Epoch 1630, training loss: 846.5115356445312 = 0.47700706124305725 + 100.0 * 8.460345268249512
Epoch 1630, val loss: 0.5629891753196716
Epoch 1640, training loss: 846.4795532226562 = 0.4764028787612915 + 100.0 * 8.460031509399414
Epoch 1640, val loss: 0.5627246499061584
Epoch 1650, training loss: 846.5157470703125 = 0.47579824924468994 + 100.0 * 8.460399627685547
Epoch 1650, val loss: 0.5624861121177673
Epoch 1660, training loss: 846.6597900390625 = 0.4751695990562439 + 100.0 * 8.461846351623535
Epoch 1660, val loss: 0.5622390508651733
Epoch 1670, training loss: 846.4299926757812 = 0.47453415393829346 + 100.0 * 8.459554672241211
Epoch 1670, val loss: 0.5618817806243896
Epoch 1680, training loss: 846.415771484375 = 0.4739236831665039 + 100.0 * 8.459418296813965
Epoch 1680, val loss: 0.5616850256919861
Epoch 1690, training loss: 846.3394165039062 = 0.4733406901359558 + 100.0 * 8.458661079406738
Epoch 1690, val loss: 0.561435341835022
Epoch 1700, training loss: 846.4127807617188 = 0.47275590896606445 + 100.0 * 8.459400177001953
Epoch 1700, val loss: 0.5611526370048523
Epoch 1710, training loss: 846.36572265625 = 0.4721619486808777 + 100.0 * 8.458935737609863
Epoch 1710, val loss: 0.5609740614891052
Epoch 1720, training loss: 846.313720703125 = 0.47156867384910583 + 100.0 * 8.45842170715332
Epoch 1720, val loss: 0.5607175827026367
Epoch 1730, training loss: 846.8387451171875 = 0.4709850251674652 + 100.0 * 8.463677406311035
Epoch 1730, val loss: 0.5606704950332642
Epoch 1740, training loss: 846.3545532226562 = 0.4703716039657593 + 100.0 * 8.458841323852539
Epoch 1740, val loss: 0.5601661801338196
Epoch 1750, training loss: 846.1763305664062 = 0.46978339552879333 + 100.0 * 8.45706558227539
Epoch 1750, val loss: 0.5600048899650574
Epoch 1760, training loss: 846.1436767578125 = 0.46922945976257324 + 100.0 * 8.456744194030762
Epoch 1760, val loss: 0.5597856044769287
Epoch 1770, training loss: 846.107421875 = 0.4686773717403412 + 100.0 * 8.456387519836426
Epoch 1770, val loss: 0.559565007686615
Epoch 1780, training loss: 846.0752563476562 = 0.4681300222873688 + 100.0 * 8.456070899963379
Epoch 1780, val loss: 0.5593814253807068
Epoch 1790, training loss: 846.0599975585938 = 0.46757933497428894 + 100.0 * 8.455924034118652
Epoch 1790, val loss: 0.5591965913772583
Epoch 1800, training loss: 846.31787109375 = 0.4670255482196808 + 100.0 * 8.458508491516113
Epoch 1800, val loss: 0.5591092705726624
Epoch 1810, training loss: 845.9967651367188 = 0.4664328396320343 + 100.0 * 8.455303192138672
Epoch 1810, val loss: 0.5586932897567749
Epoch 1820, training loss: 845.9710083007812 = 0.4658609628677368 + 100.0 * 8.45505142211914
Epoch 1820, val loss: 0.5585159063339233
Epoch 1830, training loss: 845.9434814453125 = 0.4653121531009674 + 100.0 * 8.454781532287598
Epoch 1830, val loss: 0.558302640914917
Epoch 1840, training loss: 845.9114379882812 = 0.46477022767066956 + 100.0 * 8.454466819763184
Epoch 1840, val loss: 0.5581067204475403
Epoch 1850, training loss: 845.9306640625 = 0.46422988176345825 + 100.0 * 8.45466423034668
Epoch 1850, val loss: 0.5579611659049988
Epoch 1860, training loss: 846.3095703125 = 0.4636714458465576 + 100.0 * 8.45845890045166
Epoch 1860, val loss: 0.5577564835548401
Epoch 1870, training loss: 845.9157104492188 = 0.4631173610687256 + 100.0 * 8.4545259475708
Epoch 1870, val loss: 0.5575650930404663
Epoch 1880, training loss: 845.8945922851562 = 0.46256351470947266 + 100.0 * 8.454319953918457
Epoch 1880, val loss: 0.5573360919952393
Epoch 1890, training loss: 845.8536987304688 = 0.4620136618614197 + 100.0 * 8.453916549682617
Epoch 1890, val loss: 0.5571606755256653
Epoch 1900, training loss: 845.822265625 = 0.46147823333740234 + 100.0 * 8.453607559204102
Epoch 1900, val loss: 0.5569348931312561
Epoch 1910, training loss: 845.9612426757812 = 0.46093249320983887 + 100.0 * 8.455002784729004
Epoch 1910, val loss: 0.5567218661308289
Epoch 1920, training loss: 845.7615356445312 = 0.4603850543498993 + 100.0 * 8.453011512756348
Epoch 1920, val loss: 0.55666184425354
Epoch 1930, training loss: 845.71728515625 = 0.4598388969898224 + 100.0 * 8.452574729919434
Epoch 1930, val loss: 0.5564414858818054
Epoch 1940, training loss: 845.7420043945312 = 0.4592997431755066 + 100.0 * 8.452827453613281
Epoch 1940, val loss: 0.5562256574630737
Epoch 1950, training loss: 846.091796875 = 0.45876145362854004 + 100.0 * 8.456330299377441
Epoch 1950, val loss: 0.5559810400009155
Epoch 1960, training loss: 845.7240600585938 = 0.45816653966903687 + 100.0 * 8.452658653259277
Epoch 1960, val loss: 0.5559272766113281
Epoch 1970, training loss: 845.62548828125 = 0.4576374888420105 + 100.0 * 8.451678276062012
Epoch 1970, val loss: 0.5557307004928589
Epoch 1980, training loss: 845.632080078125 = 0.4570964574813843 + 100.0 * 8.451749801635742
Epoch 1980, val loss: 0.5555478930473328
Epoch 1990, training loss: 845.6525268554688 = 0.45655590295791626 + 100.0 * 8.451959609985352
Epoch 1990, val loss: 0.5554138422012329
Epoch 2000, training loss: 845.5466918945312 = 0.45600399374961853 + 100.0 * 8.450906753540039
Epoch 2000, val loss: 0.5552465319633484
Epoch 2010, training loss: 845.4926147460938 = 0.4554687440395355 + 100.0 * 8.450371742248535
Epoch 2010, val loss: 0.5550665259361267
Epoch 2020, training loss: 845.7706298828125 = 0.45493483543395996 + 100.0 * 8.453156471252441
Epoch 2020, val loss: 0.5549675226211548
Epoch 2030, training loss: 845.5515747070312 = 0.45434829592704773 + 100.0 * 8.450972557067871
Epoch 2030, val loss: 0.554630696773529
Epoch 2040, training loss: 845.4183959960938 = 0.4538104832172394 + 100.0 * 8.44964599609375
Epoch 2040, val loss: 0.5544912219047546
Epoch 2050, training loss: 845.3444213867188 = 0.4532679319381714 + 100.0 * 8.448911666870117
Epoch 2050, val loss: 0.554297149181366
Epoch 2060, training loss: 845.352783203125 = 0.45273637771606445 + 100.0 * 8.449000358581543
Epoch 2060, val loss: 0.5541581511497498
Epoch 2070, training loss: 845.8486328125 = 0.45220646262168884 + 100.0 * 8.453964233398438
Epoch 2070, val loss: 0.5541258454322815
Epoch 2080, training loss: 845.38623046875 = 0.4516352713108063 + 100.0 * 8.449345588684082
Epoch 2080, val loss: 0.5536589622497559
Epoch 2090, training loss: 845.2886352539062 = 0.45107749104499817 + 100.0 * 8.448375701904297
Epoch 2090, val loss: 0.5535982847213745
Epoch 2100, training loss: 845.2317504882812 = 0.4505445063114166 + 100.0 * 8.4478120803833
Epoch 2100, val loss: 0.5533937215805054
Epoch 2110, training loss: 845.4581909179688 = 0.4500049948692322 + 100.0 * 8.450081825256348
Epoch 2110, val loss: 0.553216278553009
Epoch 2120, training loss: 845.1832885742188 = 0.4494479298591614 + 100.0 * 8.447338104248047
Epoch 2120, val loss: 0.553199291229248
Epoch 2130, training loss: 845.1483154296875 = 0.4488855302333832 + 100.0 * 8.446993827819824
Epoch 2130, val loss: 0.5529407858848572
Epoch 2140, training loss: 845.1417236328125 = 0.44834157824516296 + 100.0 * 8.44693374633789
Epoch 2140, val loss: 0.5528865456581116
Epoch 2150, training loss: 845.2327270507812 = 0.44778749346733093 + 100.0 * 8.44784927368164
Epoch 2150, val loss: 0.5527169108390808
Epoch 2160, training loss: 845.0986938476562 = 0.4472328722476959 + 100.0 * 8.446515083312988
Epoch 2160, val loss: 0.5525497794151306
Epoch 2170, training loss: 845.3323364257812 = 0.44668710231781006 + 100.0 * 8.448856353759766
Epoch 2170, val loss: 0.5523903369903564
Epoch 2180, training loss: 845.0980834960938 = 0.44609785079956055 + 100.0 * 8.44651985168457
Epoch 2180, val loss: 0.552208423614502
Epoch 2190, training loss: 845.0276489257812 = 0.44553929567337036 + 100.0 * 8.445820808410645
Epoch 2190, val loss: 0.5519564747810364
Epoch 2200, training loss: 844.977294921875 = 0.4449823498725891 + 100.0 * 8.44532299041748
Epoch 2200, val loss: 0.5518811941146851
Epoch 2210, training loss: 845.1923828125 = 0.4444313943386078 + 100.0 * 8.447479248046875
Epoch 2210, val loss: 0.551783561706543
Epoch 2220, training loss: 845.241455078125 = 0.44386354088783264 + 100.0 * 8.447976112365723
Epoch 2220, val loss: 0.5516212582588196
Epoch 2230, training loss: 844.9988403320312 = 0.4432845711708069 + 100.0 * 8.445555686950684
Epoch 2230, val loss: 0.5513319373130798
Epoch 2240, training loss: 844.8914184570312 = 0.44272443652153015 + 100.0 * 8.444486618041992
Epoch 2240, val loss: 0.5512140393257141
Epoch 2250, training loss: 844.85595703125 = 0.44217348098754883 + 100.0 * 8.444137573242188
Epoch 2250, val loss: 0.5510386824607849
Epoch 2260, training loss: 844.9202880859375 = 0.4416223168373108 + 100.0 * 8.444786071777344
Epoch 2260, val loss: 0.5508421659469604
Epoch 2270, training loss: 845.1679077148438 = 0.4410507380962372 + 100.0 * 8.44726848602295
Epoch 2270, val loss: 0.5506308674812317
Epoch 2280, training loss: 844.8999633789062 = 0.4404524862766266 + 100.0 * 8.444595336914062
Epoch 2280, val loss: 0.5506786704063416
Epoch 2290, training loss: 844.7976684570312 = 0.43987345695495605 + 100.0 * 8.443577766418457
Epoch 2290, val loss: 0.5504409074783325
Epoch 2300, training loss: 844.744384765625 = 0.4393077492713928 + 100.0 * 8.443050384521484
Epoch 2300, val loss: 0.5503329038619995
Epoch 2310, training loss: 844.7498779296875 = 0.43874165415763855 + 100.0 * 8.443111419677734
Epoch 2310, val loss: 0.5502042174339294
Epoch 2320, training loss: 845.31591796875 = 0.43815043568611145 + 100.0 * 8.448777198791504
Epoch 2320, val loss: 0.5500990152359009
Epoch 2330, training loss: 844.8524780273438 = 0.43757355213165283 + 100.0 * 8.444149017333984
Epoch 2330, val loss: 0.5498848557472229
Epoch 2340, training loss: 844.7019653320312 = 0.4369763135910034 + 100.0 * 8.442649841308594
Epoch 2340, val loss: 0.5497636795043945
Epoch 2350, training loss: 844.8529052734375 = 0.43640702962875366 + 100.0 * 8.444165229797363
Epoch 2350, val loss: 0.5496886372566223
Epoch 2360, training loss: 844.6300048828125 = 0.43581950664520264 + 100.0 * 8.44194221496582
Epoch 2360, val loss: 0.549514651298523
Epoch 2370, training loss: 844.6531372070312 = 0.43523913621902466 + 100.0 * 8.442178726196289
Epoch 2370, val loss: 0.549373984336853
Epoch 2380, training loss: 844.900634765625 = 0.4346630871295929 + 100.0 * 8.444659233093262
Epoch 2380, val loss: 0.5494071841239929
Epoch 2390, training loss: 844.6981201171875 = 0.43404456973075867 + 100.0 * 8.442641258239746
Epoch 2390, val loss: 0.5490748286247253
Epoch 2400, training loss: 844.6139526367188 = 0.4334479570388794 + 100.0 * 8.441804885864258
Epoch 2400, val loss: 0.5488187074661255
Epoch 2410, training loss: 844.5167236328125 = 0.432853102684021 + 100.0 * 8.440838813781738
Epoch 2410, val loss: 0.5487616062164307
Epoch 2420, training loss: 844.497802734375 = 0.43226900696754456 + 100.0 * 8.440655708312988
Epoch 2420, val loss: 0.5486413240432739
Epoch 2430, training loss: 844.506591796875 = 0.4316781461238861 + 100.0 * 8.440749168395996
Epoch 2430, val loss: 0.5485175848007202
Epoch 2440, training loss: 844.85791015625 = 0.4310826063156128 + 100.0 * 8.444268226623535
Epoch 2440, val loss: 0.5484570264816284
Epoch 2450, training loss: 844.4953002929688 = 0.43044281005859375 + 100.0 * 8.440649032592773
Epoch 2450, val loss: 0.5481738448143005
Epoch 2460, training loss: 844.4461669921875 = 0.4298183023929596 + 100.0 * 8.440163612365723
Epoch 2460, val loss: 0.5480527281761169
Epoch 2470, training loss: 844.4923706054688 = 0.4292029142379761 + 100.0 * 8.440631866455078
Epoch 2470, val loss: 0.5479776263237
Epoch 2480, training loss: 844.6251220703125 = 0.42857876420021057 + 100.0 * 8.441965103149414
Epoch 2480, val loss: 0.547893762588501
Epoch 2490, training loss: 844.4402465820312 = 0.42793789505958557 + 100.0 * 8.440123558044434
Epoch 2490, val loss: 0.5474704504013062
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7818366311516995
0.8149677606317468
=== training gcn model ===
Epoch 0, training loss: 1059.326171875 = 1.0954173803329468 + 100.0 * 10.582306861877441
Epoch 0, val loss: 1.0937920808792114
Epoch 10, training loss: 1059.298828125 = 1.0917413234710693 + 100.0 * 10.582070350646973
Epoch 10, val loss: 1.0901811122894287
Epoch 20, training loss: 1059.196533203125 = 1.0878727436065674 + 100.0 * 10.581086158752441
Epoch 20, val loss: 1.0864076614379883
Epoch 30, training loss: 1058.77392578125 = 1.083774209022522 + 100.0 * 10.576902389526367
Epoch 30, val loss: 1.0824443101882935
Epoch 40, training loss: 1057.1651611328125 = 1.0792899131774902 + 100.0 * 10.560858726501465
Epoch 40, val loss: 1.078109622001648
Epoch 50, training loss: 1052.2008056640625 = 1.0742449760437012 + 100.0 * 10.511265754699707
Epoch 50, val loss: 1.0732814073562622
Epoch 60, training loss: 1039.474365234375 = 1.0690809488296509 + 100.0 * 10.384053230285645
Epoch 60, val loss: 1.0684841871261597
Epoch 70, training loss: 1011.8995361328125 = 1.064404845237732 + 100.0 * 10.108351707458496
Epoch 70, val loss: 1.064069390296936
Epoch 80, training loss: 983.9855346679688 = 1.059396743774414 + 100.0 * 9.829261779785156
Epoch 80, val loss: 1.059462308883667
Epoch 90, training loss: 963.0211791992188 = 1.0549031496047974 + 100.0 * 9.619662284851074
Epoch 90, val loss: 1.0553233623504639
Epoch 100, training loss: 950.5410766601562 = 1.0503818988800049 + 100.0 * 9.49490737915039
Epoch 100, val loss: 1.051099181175232
Epoch 110, training loss: 940.6043090820312 = 1.0457457304000854 + 100.0 * 9.395586013793945
Epoch 110, val loss: 1.046810507774353
Epoch 120, training loss: 932.6572265625 = 1.041386604309082 + 100.0 * 9.316158294677734
Epoch 120, val loss: 1.0427947044372559
Epoch 130, training loss: 928.4767456054688 = 1.0378543138504028 + 100.0 * 9.274389266967773
Epoch 130, val loss: 1.0395742654800415
Epoch 140, training loss: 923.7525024414062 = 1.0347076654434204 + 100.0 * 9.227177619934082
Epoch 140, val loss: 1.0365954637527466
Epoch 150, training loss: 915.637939453125 = 1.03184175491333 + 100.0 * 9.146060943603516
Epoch 150, val loss: 1.0339760780334473
Epoch 160, training loss: 904.9874267578125 = 1.030349612236023 + 100.0 * 9.039570808410645
Epoch 160, val loss: 1.0326268672943115
Epoch 170, training loss: 898.368408203125 = 1.0286880731582642 + 100.0 * 8.973397254943848
Epoch 170, val loss: 1.0308547019958496
Epoch 180, training loss: 895.6006469726562 = 1.0253362655639648 + 100.0 * 8.94575309753418
Epoch 180, val loss: 1.027453064918518
Epoch 190, training loss: 893.3480224609375 = 1.021004557609558 + 100.0 * 8.923270225524902
Epoch 190, val loss: 1.0232889652252197
Epoch 200, training loss: 891.1109008789062 = 1.0168969631195068 + 100.0 * 8.90093994140625
Epoch 200, val loss: 1.019416093826294
Epoch 210, training loss: 888.6090698242188 = 1.01311457157135 + 100.0 * 8.875959396362305
Epoch 210, val loss: 1.0158281326293945
Epoch 220, training loss: 885.9432983398438 = 1.009299397468567 + 100.0 * 8.849340438842773
Epoch 220, val loss: 1.0121809244155884
Epoch 230, training loss: 883.1517333984375 = 1.0052517652511597 + 100.0 * 8.821464538574219
Epoch 230, val loss: 1.0083354711532593
Epoch 240, training loss: 881.0264282226562 = 1.0007429122924805 + 100.0 * 8.800256729125977
Epoch 240, val loss: 1.0039265155792236
Epoch 250, training loss: 879.4336547851562 = 0.9955594539642334 + 100.0 * 8.784380912780762
Epoch 250, val loss: 0.9989295601844788
Epoch 260, training loss: 878.15185546875 = 0.9899884462356567 + 100.0 * 8.771618843078613
Epoch 260, val loss: 0.993597686290741
Epoch 270, training loss: 877.0585327148438 = 0.9840351939201355 + 100.0 * 8.76074504852295
Epoch 270, val loss: 0.987892210483551
Epoch 280, training loss: 876.255859375 = 0.9778076410293579 + 100.0 * 8.75278091430664
Epoch 280, val loss: 0.9819156527519226
Epoch 290, training loss: 875.2824096679688 = 0.9712415337562561 + 100.0 * 8.743111610412598
Epoch 290, val loss: 0.9756167531013489
Epoch 300, training loss: 874.2095947265625 = 0.964483380317688 + 100.0 * 8.732451438903809
Epoch 300, val loss: 0.9691985249519348
Epoch 310, training loss: 873.2442016601562 = 0.9575622081756592 + 100.0 * 8.72286605834961
Epoch 310, val loss: 0.9626036882400513
Epoch 320, training loss: 872.212158203125 = 0.9504374861717224 + 100.0 * 8.712616920471191
Epoch 320, val loss: 0.9557296633720398
Epoch 330, training loss: 871.1443481445312 = 0.9431019425392151 + 100.0 * 8.702012062072754
Epoch 330, val loss: 0.9487279653549194
Epoch 340, training loss: 870.4796142578125 = 0.9354541301727295 + 100.0 * 8.695442199707031
Epoch 340, val loss: 0.9414317607879639
Epoch 350, training loss: 869.42138671875 = 0.9275349974632263 + 100.0 * 8.684938430786133
Epoch 350, val loss: 0.93386310338974
Epoch 360, training loss: 868.556640625 = 0.9195355176925659 + 100.0 * 8.676370620727539
Epoch 360, val loss: 0.9262451529502869
Epoch 370, training loss: 867.714111328125 = 0.9114978909492493 + 100.0 * 8.668025970458984
Epoch 370, val loss: 0.9185768365859985
Epoch 380, training loss: 867.1296997070312 = 0.903332531452179 + 100.0 * 8.662263870239258
Epoch 380, val loss: 0.9107469916343689
Epoch 390, training loss: 866.018310546875 = 0.8950042724609375 + 100.0 * 8.651232719421387
Epoch 390, val loss: 0.9028521776199341
Epoch 400, training loss: 865.2714233398438 = 0.8867842555046082 + 100.0 * 8.64384651184082
Epoch 400, val loss: 0.8950347900390625
Epoch 410, training loss: 864.6751708984375 = 0.8785157799720764 + 100.0 * 8.63796615600586
Epoch 410, val loss: 0.8871420621871948
Epoch 420, training loss: 863.7278442382812 = 0.8701024055480957 + 100.0 * 8.62857723236084
Epoch 420, val loss: 0.8791658878326416
Epoch 430, training loss: 863.0551147460938 = 0.8616530895233154 + 100.0 * 8.62193489074707
Epoch 430, val loss: 0.8712008595466614
Epoch 440, training loss: 862.3923950195312 = 0.8531508445739746 + 100.0 * 8.615392684936523
Epoch 440, val loss: 0.8631657361984253
Epoch 450, training loss: 862.0989990234375 = 0.8445944786071777 + 100.0 * 8.612544059753418
Epoch 450, val loss: 0.855071485042572
Epoch 460, training loss: 861.3321533203125 = 0.8359623551368713 + 100.0 * 8.604962348937988
Epoch 460, val loss: 0.8469725847244263
Epoch 470, training loss: 860.7179565429688 = 0.8274086713790894 + 100.0 * 8.598905563354492
Epoch 470, val loss: 0.8389616012573242
Epoch 480, training loss: 860.20166015625 = 0.8188429474830627 + 100.0 * 8.593828201293945
Epoch 480, val loss: 0.830945611000061
Epoch 490, training loss: 859.893310546875 = 0.8102469444274902 + 100.0 * 8.59083080291748
Epoch 490, val loss: 0.8229237794876099
Epoch 500, training loss: 859.429931640625 = 0.801583468914032 + 100.0 * 8.586283683776855
Epoch 500, val loss: 0.8149417638778687
Epoch 510, training loss: 858.9268798828125 = 0.7929906249046326 + 100.0 * 8.581338882446289
Epoch 510, val loss: 0.8070756793022156
Epoch 520, training loss: 858.4817504882812 = 0.7844914793968201 + 100.0 * 8.576972961425781
Epoch 520, val loss: 0.7992859482765198
Epoch 530, training loss: 858.148681640625 = 0.776015043258667 + 100.0 * 8.573726654052734
Epoch 530, val loss: 0.7915568351745605
Epoch 540, training loss: 857.6875610351562 = 0.7675363421440125 + 100.0 * 8.56920051574707
Epoch 540, val loss: 0.7838248610496521
Epoch 550, training loss: 857.3665771484375 = 0.7592164278030396 + 100.0 * 8.566073417663574
Epoch 550, val loss: 0.7764023542404175
Epoch 560, training loss: 857.0130004882812 = 0.7509832978248596 + 100.0 * 8.562620162963867
Epoch 560, val loss: 0.7690102458000183
Epoch 570, training loss: 856.7620849609375 = 0.7428646087646484 + 100.0 * 8.560192108154297
Epoch 570, val loss: 0.7617549896240234
Epoch 580, training loss: 856.781005859375 = 0.7348149418830872 + 100.0 * 8.56046199798584
Epoch 580, val loss: 0.7546518445014954
Epoch 590, training loss: 856.2265014648438 = 0.7268902659416199 + 100.0 * 8.554996490478516
Epoch 590, val loss: 0.7477064728736877
Epoch 600, training loss: 855.943359375 = 0.7192627191543579 + 100.0 * 8.552241325378418
Epoch 600, val loss: 0.7411255836486816
Epoch 610, training loss: 855.6893920898438 = 0.7118361592292786 + 100.0 * 8.549775123596191
Epoch 610, val loss: 0.7346814870834351
Epoch 620, training loss: 855.6023559570312 = 0.7046302556991577 + 100.0 * 8.54897689819336
Epoch 620, val loss: 0.7285282611846924
Epoch 630, training loss: 855.3513793945312 = 0.6975870132446289 + 100.0 * 8.546538352966309
Epoch 630, val loss: 0.7225614786148071
Epoch 640, training loss: 855.0687866210938 = 0.6908761262893677 + 100.0 * 8.543779373168945
Epoch 640, val loss: 0.716918408870697
Epoch 650, training loss: 854.821044921875 = 0.6844069957733154 + 100.0 * 8.541366577148438
Epoch 650, val loss: 0.7115393280982971
Epoch 660, training loss: 854.77099609375 = 0.6782203316688538 + 100.0 * 8.54092788696289
Epoch 660, val loss: 0.7065279483795166
Epoch 670, training loss: 854.649169921875 = 0.6722201704978943 + 100.0 * 8.539769172668457
Epoch 670, val loss: 0.7015597820281982
Epoch 680, training loss: 854.29345703125 = 0.6665638089179993 + 100.0 * 8.536269187927246
Epoch 680, val loss: 0.6970419883728027
Epoch 690, training loss: 853.9962768554688 = 0.6612193584442139 + 100.0 * 8.533350944519043
Epoch 690, val loss: 0.6928123831748962
Epoch 700, training loss: 853.8096313476562 = 0.6561669111251831 + 100.0 * 8.531534194946289
Epoch 700, val loss: 0.6888930797576904
Epoch 710, training loss: 853.8492431640625 = 0.651292622089386 + 100.0 * 8.53197956085205
Epoch 710, val loss: 0.6850815415382385
Epoch 720, training loss: 853.512451171875 = 0.6466842889785767 + 100.0 * 8.528657913208008
Epoch 720, val loss: 0.6816831231117249
Epoch 730, training loss: 853.2473754882812 = 0.6424025893211365 + 100.0 * 8.526049613952637
Epoch 730, val loss: 0.6785255074501038
Epoch 740, training loss: 853.0167846679688 = 0.6383348107337952 + 100.0 * 8.523784637451172
Epoch 740, val loss: 0.6755449175834656
Epoch 750, training loss: 852.8258056640625 = 0.6344829797744751 + 100.0 * 8.521913528442383
Epoch 750, val loss: 0.6727991700172424
Epoch 760, training loss: 852.9329223632812 = 0.63083815574646 + 100.0 * 8.52302074432373
Epoch 760, val loss: 0.6701360940933228
Epoch 770, training loss: 852.7030639648438 = 0.6272973418235779 + 100.0 * 8.520757675170898
Epoch 770, val loss: 0.667839765548706
Epoch 780, training loss: 852.354248046875 = 0.6240411400794983 + 100.0 * 8.517302513122559
Epoch 780, val loss: 0.665626585483551
Epoch 790, training loss: 852.1570434570312 = 0.6209778785705566 + 100.0 * 8.515360832214355
Epoch 790, val loss: 0.6635854244232178
Epoch 800, training loss: 852.0220336914062 = 0.618079423904419 + 100.0 * 8.514039039611816
Epoch 800, val loss: 0.6617684960365295
Epoch 810, training loss: 851.9378662109375 = 0.6152938604354858 + 100.0 * 8.513225555419922
Epoch 810, val loss: 0.6599358320236206
Epoch 820, training loss: 851.7545166015625 = 0.6126644611358643 + 100.0 * 8.511418342590332
Epoch 820, val loss: 0.6583362221717834
Epoch 830, training loss: 851.564697265625 = 0.6102035641670227 + 100.0 * 8.509544372558594
Epoch 830, val loss: 0.6567704677581787
Epoch 840, training loss: 851.4207763671875 = 0.6078782081604004 + 100.0 * 8.508129119873047
Epoch 840, val loss: 0.6553670167922974
Epoch 850, training loss: 852.3485107421875 = 0.6056585907936096 + 100.0 * 8.517428398132324
Epoch 850, val loss: 0.6538833975791931
Epoch 860, training loss: 851.3416137695312 = 0.6034240126609802 + 100.0 * 8.5073823928833
Epoch 860, val loss: 0.6527979969978333
Epoch 870, training loss: 851.1414184570312 = 0.6014427542686462 + 100.0 * 8.505399703979492
Epoch 870, val loss: 0.65166175365448
Epoch 880, training loss: 850.969970703125 = 0.5995554327964783 + 100.0 * 8.503704071044922
Epoch 880, val loss: 0.6505839228630066
Epoch 890, training loss: 850.8350830078125 = 0.5977651476860046 + 100.0 * 8.502372741699219
Epoch 890, val loss: 0.6496071219444275
Epoch 900, training loss: 850.7216796875 = 0.5960527658462524 + 100.0 * 8.501255989074707
Epoch 900, val loss: 0.6486579179763794
Epoch 910, training loss: 850.6189575195312 = 0.5944060683250427 + 100.0 * 8.500245094299316
Epoch 910, val loss: 0.6477711796760559
Epoch 920, training loss: 850.5261840820312 = 0.5928243398666382 + 100.0 * 8.499333381652832
Epoch 920, val loss: 0.6469255685806274
Epoch 930, training loss: 851.199951171875 = 0.5912877321243286 + 100.0 * 8.506086349487305
Epoch 930, val loss: 0.6461236476898193
Epoch 940, training loss: 850.3767700195312 = 0.5897266864776611 + 100.0 * 8.497870445251465
Epoch 940, val loss: 0.6452745199203491
Epoch 950, training loss: 850.2858276367188 = 0.5882957577705383 + 100.0 * 8.49697494506836
Epoch 950, val loss: 0.6444759368896484
Epoch 960, training loss: 850.1968383789062 = 0.5869312286376953 + 100.0 * 8.496099472045898
Epoch 960, val loss: 0.6437821388244629
Epoch 970, training loss: 850.1142578125 = 0.5856208801269531 + 100.0 * 8.495285987854004
Epoch 970, val loss: 0.6431370973587036
Epoch 980, training loss: 850.3463134765625 = 0.5843316316604614 + 100.0 * 8.49761962890625
Epoch 980, val loss: 0.6425794363021851
Epoch 990, training loss: 850.310546875 = 0.5829982161521912 + 100.0 * 8.497275352478027
Epoch 990, val loss: 0.6416424512863159
Epoch 1000, training loss: 849.9042358398438 = 0.5817515254020691 + 100.0 * 8.49322509765625
Epoch 1000, val loss: 0.6410544514656067
Epoch 1010, training loss: 849.8626708984375 = 0.5805766582489014 + 100.0 * 8.492820739746094
Epoch 1010, val loss: 0.6404849290847778
Epoch 1020, training loss: 849.766845703125 = 0.5794494152069092 + 100.0 * 8.491873741149902
Epoch 1020, val loss: 0.6399347186088562
Epoch 1030, training loss: 849.6869506835938 = 0.5783459544181824 + 100.0 * 8.49108600616455
Epoch 1030, val loss: 0.6393405199050903
Epoch 1040, training loss: 849.613037109375 = 0.5772600769996643 + 100.0 * 8.490357398986816
Epoch 1040, val loss: 0.6388196349143982
Epoch 1050, training loss: 849.5571899414062 = 0.5761939287185669 + 100.0 * 8.4898099899292
Epoch 1050, val loss: 0.6383097767829895
Epoch 1060, training loss: 849.9888916015625 = 0.575123131275177 + 100.0 * 8.49413776397705
Epoch 1060, val loss: 0.6377913355827332
Epoch 1070, training loss: 849.5164184570312 = 0.5740576982498169 + 100.0 * 8.489423751831055
Epoch 1070, val loss: 0.6372373104095459
Epoch 1080, training loss: 849.3509521484375 = 0.5730379223823547 + 100.0 * 8.487778663635254
Epoch 1080, val loss: 0.6367164850234985
Epoch 1090, training loss: 849.2941284179688 = 0.5720553398132324 + 100.0 * 8.487220764160156
Epoch 1090, val loss: 0.6362301707267761
Epoch 1100, training loss: 849.2283325195312 = 0.571095883846283 + 100.0 * 8.486572265625
Epoch 1100, val loss: 0.6357441544532776
Epoch 1110, training loss: 849.3611450195312 = 0.5701374411582947 + 100.0 * 8.487910270690918
Epoch 1110, val loss: 0.6353253722190857
Epoch 1120, training loss: 849.2203979492188 = 0.5691754817962646 + 100.0 * 8.486512184143066
Epoch 1120, val loss: 0.6348111033439636
Epoch 1130, training loss: 849.4901733398438 = 0.5681981444358826 + 100.0 * 8.489219665527344
Epoch 1130, val loss: 0.634380578994751
Epoch 1140, training loss: 849.06787109375 = 0.5672420263290405 + 100.0 * 8.485006332397461
Epoch 1140, val loss: 0.6338832378387451
Epoch 1150, training loss: 848.9183959960938 = 0.5663377046585083 + 100.0 * 8.4835205078125
Epoch 1150, val loss: 0.6333552598953247
Epoch 1160, training loss: 848.8652954101562 = 0.5654559135437012 + 100.0 * 8.482998847961426
Epoch 1160, val loss: 0.632921040058136
Epoch 1170, training loss: 848.8026733398438 = 0.5645841360092163 + 100.0 * 8.482380867004395
Epoch 1170, val loss: 0.6325068473815918
Epoch 1180, training loss: 848.7489013671875 = 0.5637192130088806 + 100.0 * 8.481851577758789
Epoch 1180, val loss: 0.6320736408233643
Epoch 1190, training loss: 848.7100830078125 = 0.562858521938324 + 100.0 * 8.48147201538086
Epoch 1190, val loss: 0.6316101551055908
Epoch 1200, training loss: 849.1400756835938 = 0.5619926452636719 + 100.0 * 8.485780715942383
Epoch 1200, val loss: 0.6310466527938843
Epoch 1210, training loss: 848.7376708984375 = 0.5610721111297607 + 100.0 * 8.481765747070312
Epoch 1210, val loss: 0.630801260471344
Epoch 1220, training loss: 848.5333251953125 = 0.5602264404296875 + 100.0 * 8.479730606079102
Epoch 1220, val loss: 0.630293071269989
Epoch 1230, training loss: 848.5142211914062 = 0.5593957901000977 + 100.0 * 8.479548454284668
Epoch 1230, val loss: 0.6298306584358215
Epoch 1240, training loss: 849.242919921875 = 0.5585411787033081 + 100.0 * 8.486844062805176
Epoch 1240, val loss: 0.6293062567710876
Epoch 1250, training loss: 848.52734375 = 0.557633638381958 + 100.0 * 8.479697227478027
Epoch 1250, val loss: 0.6289988160133362
Epoch 1260, training loss: 848.4065551757812 = 0.5568195581436157 + 100.0 * 8.478497505187988
Epoch 1260, val loss: 0.6286161541938782
Epoch 1270, training loss: 848.2871704101562 = 0.5560300946235657 + 100.0 * 8.477311134338379
Epoch 1270, val loss: 0.6282243728637695
Epoch 1280, training loss: 848.240966796875 = 0.5552493333816528 + 100.0 * 8.47685718536377
Epoch 1280, val loss: 0.6277942061424255
Epoch 1290, training loss: 848.1915283203125 = 0.5544718503952026 + 100.0 * 8.476370811462402
Epoch 1290, val loss: 0.6274780631065369
Epoch 1300, training loss: 848.1514282226562 = 0.5536918640136719 + 100.0 * 8.475976943969727
Epoch 1300, val loss: 0.6270883679389954
Epoch 1310, training loss: 848.4577026367188 = 0.5528966784477234 + 100.0 * 8.479047775268555
Epoch 1310, val loss: 0.6266769766807556
Epoch 1320, training loss: 848.0903930664062 = 0.5520943403244019 + 100.0 * 8.475382804870605
Epoch 1320, val loss: 0.626398503780365
Epoch 1330, training loss: 848.0252075195312 = 0.5513105988502502 + 100.0 * 8.474739074707031
Epoch 1330, val loss: 0.6260566711425781
Epoch 1340, training loss: 847.9710693359375 = 0.5505373477935791 + 100.0 * 8.474205017089844
Epoch 1340, val loss: 0.625671923160553
Epoch 1350, training loss: 848.0291748046875 = 0.5497758388519287 + 100.0 * 8.474793434143066
Epoch 1350, val loss: 0.625340461730957
Epoch 1360, training loss: 848.0360107421875 = 0.5489628314971924 + 100.0 * 8.474870681762695
Epoch 1360, val loss: 0.6248130202293396
Epoch 1370, training loss: 847.9412231445312 = 0.5481552481651306 + 100.0 * 8.473930358886719
Epoch 1370, val loss: 0.6245589256286621
Epoch 1380, training loss: 847.8390502929688 = 0.5473799705505371 + 100.0 * 8.472916603088379
Epoch 1380, val loss: 0.6242678761482239
Epoch 1390, training loss: 847.7529296875 = 0.5466279983520508 + 100.0 * 8.472063064575195
Epoch 1390, val loss: 0.6238890290260315
Epoch 1400, training loss: 847.717041015625 = 0.5458825826644897 + 100.0 * 8.471711158752441
Epoch 1400, val loss: 0.6235482096672058
Epoch 1410, training loss: 848.0510864257812 = 0.545125424861908 + 100.0 * 8.475059509277344
Epoch 1410, val loss: 0.6233445405960083
Epoch 1420, training loss: 847.6978759765625 = 0.5443180799484253 + 100.0 * 8.471535682678223
Epoch 1420, val loss: 0.6227555274963379
Epoch 1430, training loss: 847.6649780273438 = 0.5435460209846497 + 100.0 * 8.471214294433594
Epoch 1430, val loss: 0.6224855184555054
Epoch 1440, training loss: 847.5804443359375 = 0.5428162217140198 + 100.0 * 8.470376014709473
Epoch 1440, val loss: 0.6220842599868774
Epoch 1450, training loss: 847.5114135742188 = 0.5420892834663391 + 100.0 * 8.469693183898926
Epoch 1450, val loss: 0.6218152642250061
Epoch 1460, training loss: 847.4579467773438 = 0.5413705110549927 + 100.0 * 8.469165802001953
Epoch 1460, val loss: 0.6214475035667419
Epoch 1470, training loss: 847.5955200195312 = 0.5406469702720642 + 100.0 * 8.470548629760742
Epoch 1470, val loss: 0.621059238910675
Epoch 1480, training loss: 847.4672241210938 = 0.5398669838905334 + 100.0 * 8.469273567199707
Epoch 1480, val loss: 0.6208956241607666
Epoch 1490, training loss: 847.4078979492188 = 0.5391295552253723 + 100.0 * 8.468688011169434
Epoch 1490, val loss: 0.6204225420951843
Epoch 1500, training loss: 847.5491943359375 = 0.5383933782577515 + 100.0 * 8.470108032226562
Epoch 1500, val loss: 0.6201194524765015
Epoch 1510, training loss: 847.326904296875 = 0.5376392006874084 + 100.0 * 8.46789264678955
Epoch 1510, val loss: 0.6198396682739258
Epoch 1520, training loss: 847.2339477539062 = 0.5369138121604919 + 100.0 * 8.466970443725586
Epoch 1520, val loss: 0.619490921497345
Epoch 1530, training loss: 847.2469482421875 = 0.5361894965171814 + 100.0 * 8.467107772827148
Epoch 1530, val loss: 0.6191803812980652
Epoch 1540, training loss: 847.5499877929688 = 0.5354418158531189 + 100.0 * 8.470145225524902
Epoch 1540, val loss: 0.6189077496528625
Epoch 1550, training loss: 847.2269287109375 = 0.5347025990486145 + 100.0 * 8.46692180633545
Epoch 1550, val loss: 0.6183673739433289
Epoch 1560, training loss: 847.0919189453125 = 0.5339663624763489 + 100.0 * 8.46557903289795
Epoch 1560, val loss: 0.6181401014328003
Epoch 1570, training loss: 847.0315551757812 = 0.5332689881324768 + 100.0 * 8.464982986450195
Epoch 1570, val loss: 0.6177794337272644
Epoch 1580, training loss: 847.0014038085938 = 0.5325642228126526 + 100.0 * 8.464688301086426
Epoch 1580, val loss: 0.6174764633178711
Epoch 1590, training loss: 847.1001586914062 = 0.5318529009819031 + 100.0 * 8.465682983398438
Epoch 1590, val loss: 0.6171332001686096
Epoch 1600, training loss: 847.1063842773438 = 0.5311046242713928 + 100.0 * 8.465752601623535
Epoch 1600, val loss: 0.6167555451393127
Epoch 1610, training loss: 847.0109252929688 = 0.5303563475608826 + 100.0 * 8.464805603027344
Epoch 1610, val loss: 0.6165204048156738
Epoch 1620, training loss: 847.193359375 = 0.5296101570129395 + 100.0 * 8.46663761138916
Epoch 1620, val loss: 0.6161487102508545
Epoch 1630, training loss: 846.9036865234375 = 0.5288558006286621 + 100.0 * 8.46374797821045
Epoch 1630, val loss: 0.6157628297805786
Epoch 1640, training loss: 846.8053588867188 = 0.5281104445457458 + 100.0 * 8.462772369384766
Epoch 1640, val loss: 0.6154394149780273
Epoch 1650, training loss: 846.8826904296875 = 0.5273688435554504 + 100.0 * 8.463553428649902
Epoch 1650, val loss: 0.615157425403595
Epoch 1660, training loss: 846.7246704101562 = 0.5266011357307434 + 100.0 * 8.461980819702148
Epoch 1660, val loss: 0.6147403717041016
Epoch 1670, training loss: 846.6935424804688 = 0.5258508920669556 + 100.0 * 8.461676597595215
Epoch 1670, val loss: 0.6143550872802734
Epoch 1680, training loss: 846.6524047851562 = 0.5251066088676453 + 100.0 * 8.461273193359375
Epoch 1680, val loss: 0.6140761971473694
Epoch 1690, training loss: 846.7805786132812 = 0.5243555307388306 + 100.0 * 8.462562561035156
Epoch 1690, val loss: 0.6136865019798279
Epoch 1700, training loss: 846.654296875 = 0.5235644578933716 + 100.0 * 8.461307525634766
Epoch 1700, val loss: 0.6133430004119873
Epoch 1710, training loss: 846.53173828125 = 0.522771418094635 + 100.0 * 8.460089683532715
Epoch 1710, val loss: 0.6129783987998962
Epoch 1720, training loss: 846.5167846679688 = 0.522006094455719 + 100.0 * 8.45994758605957
Epoch 1720, val loss: 0.6126184463500977
Epoch 1730, training loss: 846.5405883789062 = 0.5212311744689941 + 100.0 * 8.460193634033203
Epoch 1730, val loss: 0.6122113466262817
Epoch 1740, training loss: 846.7174072265625 = 0.5204248428344727 + 100.0 * 8.461969375610352
Epoch 1740, val loss: 0.6117588877677917
Epoch 1750, training loss: 846.4506225585938 = 0.5195731520652771 + 100.0 * 8.459310531616211
Epoch 1750, val loss: 0.6114806532859802
Epoch 1760, training loss: 846.3573608398438 = 0.5187827348709106 + 100.0 * 8.458385467529297
Epoch 1760, val loss: 0.61107337474823
Epoch 1770, training loss: 846.2973022460938 = 0.517994225025177 + 100.0 * 8.457793235778809
Epoch 1770, val loss: 0.6106721758842468
Epoch 1780, training loss: 846.2532348632812 = 0.5172125101089478 + 100.0 * 8.45736026763916
Epoch 1780, val loss: 0.6103858947753906
Epoch 1790, training loss: 846.3819580078125 = 0.5164195895195007 + 100.0 * 8.45865535736084
Epoch 1790, val loss: 0.6100746393203735
Epoch 1800, training loss: 846.331787109375 = 0.5155909657478333 + 100.0 * 8.458162307739258
Epoch 1800, val loss: 0.6094416975975037
Epoch 1810, training loss: 846.23828125 = 0.51475590467453 + 100.0 * 8.457235336303711
Epoch 1810, val loss: 0.6091786623001099
Epoch 1820, training loss: 846.5164184570312 = 0.5139335989952087 + 100.0 * 8.4600248336792
Epoch 1820, val loss: 0.608651876449585
Epoch 1830, training loss: 846.1395874023438 = 0.5130667686462402 + 100.0 * 8.456265449523926
Epoch 1830, val loss: 0.6083396673202515
Epoch 1840, training loss: 846.0901489257812 = 0.5122433304786682 + 100.0 * 8.455779075622559
Epoch 1840, val loss: 0.6079972386360168
Epoch 1850, training loss: 846.0288696289062 = 0.5114399194717407 + 100.0 * 8.455174446105957
Epoch 1850, val loss: 0.6075844168663025
Epoch 1860, training loss: 845.9934692382812 = 0.5106273293495178 + 100.0 * 8.454828262329102
Epoch 1860, val loss: 0.607250452041626
Epoch 1870, training loss: 846.0135498046875 = 0.509804904460907 + 100.0 * 8.455037117004395
Epoch 1870, val loss: 0.6069157719612122
Epoch 1880, training loss: 846.2738647460938 = 0.5089606642723083 + 100.0 * 8.457649230957031
Epoch 1880, val loss: 0.6063548922538757
Epoch 1890, training loss: 845.9523315429688 = 0.5080586075782776 + 100.0 * 8.454442977905273
Epoch 1890, val loss: 0.6060075163841248
Epoch 1900, training loss: 845.8751831054688 = 0.5071818232536316 + 100.0 * 8.453680038452148
Epoch 1900, val loss: 0.6056417226791382
Epoch 1910, training loss: 845.8699951171875 = 0.5063350796699524 + 100.0 * 8.453636169433594
Epoch 1910, val loss: 0.6051963567733765
Epoch 1920, training loss: 845.95654296875 = 0.5054788589477539 + 100.0 * 8.454510688781738
Epoch 1920, val loss: 0.6048048734664917
Epoch 1930, training loss: 845.7808837890625 = 0.5046107172966003 + 100.0 * 8.452762603759766
Epoch 1930, val loss: 0.6044690608978271
Epoch 1940, training loss: 845.82080078125 = 0.5037495493888855 + 100.0 * 8.453170776367188
Epoch 1940, val loss: 0.6040392518043518
Epoch 1950, training loss: 845.8565673828125 = 0.502873420715332 + 100.0 * 8.453536987304688
Epoch 1950, val loss: 0.603605329990387
Epoch 1960, training loss: 845.7318115234375 = 0.5019846558570862 + 100.0 * 8.452298164367676
Epoch 1960, val loss: 0.6031558513641357
Epoch 1970, training loss: 845.8369750976562 = 0.5010943412780762 + 100.0 * 8.45335865020752
Epoch 1970, val loss: 0.6026855707168579
Epoch 1980, training loss: 845.7252197265625 = 0.5001875162124634 + 100.0 * 8.452250480651855
Epoch 1980, val loss: 0.6022485494613647
Epoch 1990, training loss: 845.609375 = 0.49928563833236694 + 100.0 * 8.451101303100586
Epoch 1990, val loss: 0.6017263531684875
Epoch 2000, training loss: 845.62646484375 = 0.4983944892883301 + 100.0 * 8.45128059387207
Epoch 2000, val loss: 0.6012490391731262
Epoch 2010, training loss: 845.8495483398438 = 0.49748867750167847 + 100.0 * 8.453520774841309
Epoch 2010, val loss: 0.6006788015365601
Epoch 2020, training loss: 845.76123046875 = 0.49652284383773804 + 100.0 * 8.45264720916748
Epoch 2020, val loss: 0.6001584529876709
Epoch 2030, training loss: 845.57666015625 = 0.49557891488075256 + 100.0 * 8.450810432434082
Epoch 2030, val loss: 0.5996841192245483
Epoch 2040, training loss: 845.4747924804688 = 0.4946364164352417 + 100.0 * 8.449801445007324
Epoch 2040, val loss: 0.5991764664649963
Epoch 2050, training loss: 845.4608154296875 = 0.49370837211608887 + 100.0 * 8.449670791625977
Epoch 2050, val loss: 0.598732590675354
Epoch 2060, training loss: 845.7884521484375 = 0.4927537739276886 + 100.0 * 8.452957153320312
Epoch 2060, val loss: 0.5983107686042786
Epoch 2070, training loss: 845.4285888671875 = 0.4917370080947876 + 100.0 * 8.449368476867676
Epoch 2070, val loss: 0.5975394248962402
Epoch 2080, training loss: 845.36767578125 = 0.490747332572937 + 100.0 * 8.448769569396973
Epoch 2080, val loss: 0.5970648527145386
Epoch 2090, training loss: 845.3515014648438 = 0.48978617787361145 + 100.0 * 8.448616981506348
Epoch 2090, val loss: 0.5964803695678711
Epoch 2100, training loss: 845.431884765625 = 0.4888227880001068 + 100.0 * 8.449430465698242
Epoch 2100, val loss: 0.5959497094154358
Epoch 2110, training loss: 845.3796997070312 = 0.48781755566596985 + 100.0 * 8.448919296264648
Epoch 2110, val loss: 0.5955161452293396
Epoch 2120, training loss: 845.3253173828125 = 0.48681193590164185 + 100.0 * 8.448385238647461
Epoch 2120, val loss: 0.5949437618255615
Epoch 2130, training loss: 845.3467407226562 = 0.48580485582351685 + 100.0 * 8.448609352111816
Epoch 2130, val loss: 0.594367265701294
Epoch 2140, training loss: 845.4439697265625 = 0.48477527499198914 + 100.0 * 8.449591636657715
Epoch 2140, val loss: 0.593812882900238
Epoch 2150, training loss: 845.268798828125 = 0.4837341010570526 + 100.0 * 8.447851181030273
Epoch 2150, val loss: 0.5931692719459534
Epoch 2160, training loss: 845.204833984375 = 0.4826963543891907 + 100.0 * 8.447221755981445
Epoch 2160, val loss: 0.5926644206047058
Epoch 2170, training loss: 845.2460327148438 = 0.4816601276397705 + 100.0 * 8.447643280029297
Epoch 2170, val loss: 0.5920412540435791
Epoch 2180, training loss: 845.3455200195312 = 0.48059895634651184 + 100.0 * 8.448649406433105
Epoch 2180, val loss: 0.5914323925971985
Epoch 2190, training loss: 845.3840942382812 = 0.47948339581489563 + 100.0 * 8.44904613494873
Epoch 2190, val loss: 0.5908500552177429
Epoch 2200, training loss: 845.1566772460938 = 0.4783722758293152 + 100.0 * 8.446783065795898
Epoch 2200, val loss: 0.5903037786483765
Epoch 2210, training loss: 845.1258544921875 = 0.4772935211658478 + 100.0 * 8.44648551940918
Epoch 2210, val loss: 0.5897372961044312
Epoch 2220, training loss: 845.0844116210938 = 0.47622033953666687 + 100.0 * 8.44608211517334
Epoch 2220, val loss: 0.5891920924186707
Epoch 2230, training loss: 845.1063842773438 = 0.4751294255256653 + 100.0 * 8.446311950683594
Epoch 2230, val loss: 0.5886324048042297
Epoch 2240, training loss: 845.2570190429688 = 0.47401145100593567 + 100.0 * 8.447830200195312
Epoch 2240, val loss: 0.5879787802696228
Epoch 2250, training loss: 845.2552490234375 = 0.4728719890117645 + 100.0 * 8.447823524475098
Epoch 2250, val loss: 0.5873770117759705
Epoch 2260, training loss: 845.0189819335938 = 0.4717318117618561 + 100.0 * 8.445472717285156
Epoch 2260, val loss: 0.5866844058036804
Epoch 2270, training loss: 844.9727172851562 = 0.470620721578598 + 100.0 * 8.44502067565918
Epoch 2270, val loss: 0.5860157012939453
Epoch 2280, training loss: 844.9544677734375 = 0.4695078432559967 + 100.0 * 8.444849967956543
Epoch 2280, val loss: 0.5854472517967224
Epoch 2290, training loss: 844.9991455078125 = 0.46838855743408203 + 100.0 * 8.445307731628418
Epoch 2290, val loss: 0.5848020315170288
Epoch 2300, training loss: 845.2689208984375 = 0.46724003553390503 + 100.0 * 8.448017120361328
Epoch 2300, val loss: 0.5842217803001404
Epoch 2310, training loss: 844.957275390625 = 0.46603479981422424 + 100.0 * 8.44491195678711
Epoch 2310, val loss: 0.5837852358818054
Epoch 2320, training loss: 844.9564208984375 = 0.4648723006248474 + 100.0 * 8.444915771484375
Epoch 2320, val loss: 0.5830150246620178
Epoch 2330, training loss: 845.0070190429688 = 0.4637138247489929 + 100.0 * 8.445433616638184
Epoch 2330, val loss: 0.5824783444404602
Epoch 2340, training loss: 844.849853515625 = 0.4625490605831146 + 100.0 * 8.443873405456543
Epoch 2340, val loss: 0.5818308591842651
Epoch 2350, training loss: 844.8082885742188 = 0.46139076352119446 + 100.0 * 8.443469047546387
Epoch 2350, val loss: 0.5812008380889893
Epoch 2360, training loss: 844.7824096679688 = 0.4602245092391968 + 100.0 * 8.443222045898438
Epoch 2360, val loss: 0.5805805325508118
Epoch 2370, training loss: 844.795166015625 = 0.45905566215515137 + 100.0 * 8.443361282348633
Epoch 2370, val loss: 0.5799205303192139
Epoch 2380, training loss: 845.1244506835938 = 0.45787808299064636 + 100.0 * 8.44666576385498
Epoch 2380, val loss: 0.5792392492294312
Epoch 2390, training loss: 844.890869140625 = 0.45661407709121704 + 100.0 * 8.444342613220215
Epoch 2390, val loss: 0.5788208246231079
Epoch 2400, training loss: 844.7615356445312 = 0.4553912281990051 + 100.0 * 8.443061828613281
Epoch 2400, val loss: 0.5780119299888611
Epoch 2410, training loss: 844.7258911132812 = 0.45418819785118103 + 100.0 * 8.442717552185059
Epoch 2410, val loss: 0.5775599479675293
Epoch 2420, training loss: 844.90478515625 = 0.45297855138778687 + 100.0 * 8.444518089294434
Epoch 2420, val loss: 0.576845109462738
Epoch 2430, training loss: 844.7315673828125 = 0.4517449736595154 + 100.0 * 8.442798614501953
Epoch 2430, val loss: 0.5761935710906982
Epoch 2440, training loss: 844.647216796875 = 0.45049241185188293 + 100.0 * 8.441967010498047
Epoch 2440, val loss: 0.575600266456604
Epoch 2450, training loss: 844.612548828125 = 0.44924527406692505 + 100.0 * 8.441633224487305
Epoch 2450, val loss: 0.5750682353973389
Epoch 2460, training loss: 844.5869140625 = 0.4480104446411133 + 100.0 * 8.441389083862305
Epoch 2460, val loss: 0.5745227932929993
Epoch 2470, training loss: 844.7225341796875 = 0.4467655122280121 + 100.0 * 8.442757606506348
Epoch 2470, val loss: 0.574079692363739
Epoch 2480, training loss: 844.7019653320312 = 0.4454815983772278 + 100.0 * 8.442564964294434
Epoch 2480, val loss: 0.5731903314590454
Epoch 2490, training loss: 844.8255004882812 = 0.44416335225105286 + 100.0 * 8.44381332397461
Epoch 2490, val loss: 0.5726012587547302
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7706747843734145
0.8140259363906398
The final CL Acc:0.77135, 0.00830, The final GNN Acc:0.81417, 0.00060
raw graph: torch.Size([2, 88648])
add edge: torch.Size([2, 110814])
remove edge: torch.Size([2, 66286])
updated graph: torch.Size([2, 88452])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 1059.301513671875 = 1.0766648054122925 + 100.0 * 10.58224868774414
Epoch 0, val loss: 1.0767563581466675
Epoch 10, training loss: 1059.2459716796875 = 1.073608636856079 + 100.0 * 10.581724166870117
Epoch 10, val loss: 1.0737028121948242
Epoch 20, training loss: 1058.995849609375 = 1.0704171657562256 + 100.0 * 10.579254150390625
Epoch 20, val loss: 1.0705205202102661
Epoch 30, training loss: 1057.806884765625 = 1.0670536756515503 + 100.0 * 10.567399024963379
Epoch 30, val loss: 1.0671348571777344
Epoch 40, training loss: 1053.29931640625 = 1.0634262561798096 + 100.0 * 10.522358894348145
Epoch 40, val loss: 1.063490629196167
Epoch 50, training loss: 1041.4354248046875 = 1.059665322303772 + 100.0 * 10.403757095336914
Epoch 50, val loss: 1.0597634315490723
Epoch 60, training loss: 1018.2021484375 = 1.0563372373580933 + 100.0 * 10.17145824432373
Epoch 60, val loss: 1.0564401149749756
Epoch 70, training loss: 991.3986206054688 = 1.0527493953704834 + 100.0 * 9.903458595275879
Epoch 70, val loss: 1.0528782606124878
Epoch 80, training loss: 956.7513427734375 = 1.0487744808197021 + 100.0 * 9.557025909423828
Epoch 80, val loss: 1.0489927530288696
Epoch 90, training loss: 938.12109375 = 1.0450751781463623 + 100.0 * 9.370759963989258
Epoch 90, val loss: 1.0454325675964355
Epoch 100, training loss: 926.986572265625 = 1.0415600538253784 + 100.0 * 9.25944995880127
Epoch 100, val loss: 1.0421175956726074
Epoch 110, training loss: 921.25927734375 = 1.038338303565979 + 100.0 * 9.20220947265625
Epoch 110, val loss: 1.0390642881393433
Epoch 120, training loss: 915.96142578125 = 1.0356545448303223 + 100.0 * 9.14925765991211
Epoch 120, val loss: 1.036521077156067
Epoch 130, training loss: 909.7709350585938 = 1.0335965156555176 + 100.0 * 9.087373733520508
Epoch 130, val loss: 1.0345830917358398
Epoch 140, training loss: 904.0108032226562 = 1.0319119691848755 + 100.0 * 9.029788970947266
Epoch 140, val loss: 1.0329792499542236
Epoch 150, training loss: 900.3001708984375 = 1.0298508405685425 + 100.0 * 8.992703437805176
Epoch 150, val loss: 1.030917763710022
Epoch 160, training loss: 897.1231079101562 = 1.0272793769836426 + 100.0 * 8.960958480834961
Epoch 160, val loss: 1.0283952951431274
Epoch 170, training loss: 892.7056274414062 = 1.0250790119171143 + 100.0 * 8.916805267333984
Epoch 170, val loss: 1.0263285636901855
Epoch 180, training loss: 886.7274780273438 = 1.0235949754714966 + 100.0 * 8.857038497924805
Epoch 180, val loss: 1.0249649286270142
Epoch 190, training loss: 880.7601318359375 = 1.0224868059158325 + 100.0 * 8.79737663269043
Epoch 190, val loss: 1.0239133834838867
Epoch 200, training loss: 876.3590698242188 = 1.0207911729812622 + 100.0 * 8.753382682800293
Epoch 200, val loss: 1.022199273109436
Epoch 210, training loss: 872.97216796875 = 1.0181385278701782 + 100.0 * 8.7195405960083
Epoch 210, val loss: 1.019569993019104
Epoch 220, training loss: 870.6229248046875 = 1.0149980783462524 + 100.0 * 8.69607925415039
Epoch 220, val loss: 1.016440987586975
Epoch 230, training loss: 868.6148071289062 = 1.0115771293640137 + 100.0 * 8.676032066345215
Epoch 230, val loss: 1.0130316019058228
Epoch 240, training loss: 866.992431640625 = 1.00801682472229 + 100.0 * 8.659844398498535
Epoch 240, val loss: 1.0094833374023438
Epoch 250, training loss: 865.5193481445312 = 1.0043408870697021 + 100.0 * 8.645150184631348
Epoch 250, val loss: 1.0058549642562866
Epoch 260, training loss: 864.3228759765625 = 1.0005234479904175 + 100.0 * 8.633223533630371
Epoch 260, val loss: 1.0020818710327148
Epoch 270, training loss: 863.2744140625 = 0.9963278770446777 + 100.0 * 8.622780799865723
Epoch 270, val loss: 0.9979425668716431
Epoch 280, training loss: 862.35595703125 = 0.9917869567871094 + 100.0 * 8.613641738891602
Epoch 280, val loss: 0.9934762120246887
Epoch 290, training loss: 861.6648559570312 = 0.9868753552436829 + 100.0 * 8.606780052185059
Epoch 290, val loss: 0.9886424541473389
Epoch 300, training loss: 860.8875732421875 = 0.9815683364868164 + 100.0 * 8.59906005859375
Epoch 300, val loss: 0.9834574460983276
Epoch 310, training loss: 860.3025512695312 = 0.9758890867233276 + 100.0 * 8.593266487121582
Epoch 310, val loss: 0.9779138565063477
Epoch 320, training loss: 859.8903198242188 = 0.9697368144989014 + 100.0 * 8.589205741882324
Epoch 320, val loss: 0.9719081521034241
Epoch 330, training loss: 859.2736206054688 = 0.9632088541984558 + 100.0 * 8.583104133605957
Epoch 330, val loss: 0.9655369520187378
Epoch 340, training loss: 858.682373046875 = 0.956475019454956 + 100.0 * 8.577259063720703
Epoch 340, val loss: 0.9590160846710205
Epoch 350, training loss: 858.1026000976562 = 0.9494714736938477 + 100.0 * 8.571531295776367
Epoch 350, val loss: 0.9522138237953186
Epoch 360, training loss: 857.67626953125 = 0.9420705437660217 + 100.0 * 8.567341804504395
Epoch 360, val loss: 0.9450775980949402
Epoch 370, training loss: 856.9942016601562 = 0.9344119429588318 + 100.0 * 8.560598373413086
Epoch 370, val loss: 0.9376509785652161
Epoch 380, training loss: 856.3241577148438 = 0.9265784025192261 + 100.0 * 8.553976058959961
Epoch 380, val loss: 0.9300918579101562
Epoch 390, training loss: 855.713623046875 = 0.9184272289276123 + 100.0 * 8.547951698303223
Epoch 390, val loss: 0.9222319722175598
Epoch 400, training loss: 855.2224731445312 = 0.9100228548049927 + 100.0 * 8.543124198913574
Epoch 400, val loss: 0.9141517281532288
Epoch 410, training loss: 854.8119506835938 = 0.9012672901153564 + 100.0 * 8.539107322692871
Epoch 410, val loss: 0.9056577086448669
Epoch 420, training loss: 854.1905517578125 = 0.8922950625419617 + 100.0 * 8.53298282623291
Epoch 420, val loss: 0.89705491065979
Epoch 430, training loss: 853.7135620117188 = 0.883180558681488 + 100.0 * 8.528304100036621
Epoch 430, val loss: 0.8883129358291626
Epoch 440, training loss: 853.4224853515625 = 0.8737484216690063 + 100.0 * 8.525487899780273
Epoch 440, val loss: 0.8792605400085449
Epoch 450, training loss: 852.9249267578125 = 0.8641476035118103 + 100.0 * 8.520607948303223
Epoch 450, val loss: 0.8700734376907349
Epoch 460, training loss: 852.5460205078125 = 0.8545898795127869 + 100.0 * 8.516914367675781
Epoch 460, val loss: 0.8609703779220581
Epoch 470, training loss: 852.18896484375 = 0.844944179058075 + 100.0 * 8.513440132141113
Epoch 470, val loss: 0.8518125414848328
Epoch 480, training loss: 852.0958251953125 = 0.8352793455123901 + 100.0 * 8.512605667114258
Epoch 480, val loss: 0.8426427245140076
Epoch 490, training loss: 851.5820922851562 = 0.8255438208580017 + 100.0 * 8.50756549835205
Epoch 490, val loss: 0.8334733843803406
Epoch 500, training loss: 851.1581420898438 = 0.8160507678985596 + 100.0 * 8.50342082977295
Epoch 500, val loss: 0.8245284557342529
Epoch 510, training loss: 850.8052978515625 = 0.8067182898521423 + 100.0 * 8.499985694885254
Epoch 510, val loss: 0.8157860636711121
Epoch 520, training loss: 850.462158203125 = 0.7974902391433716 + 100.0 * 8.496646881103516
Epoch 520, val loss: 0.8071531057357788
Epoch 530, training loss: 850.2916259765625 = 0.7883214950561523 + 100.0 * 8.495033264160156
Epoch 530, val loss: 0.7986154556274414
Epoch 540, training loss: 850.1947021484375 = 0.7792639136314392 + 100.0 * 8.494154930114746
Epoch 540, val loss: 0.7902293801307678
Epoch 550, training loss: 849.5631103515625 = 0.7703754901885986 + 100.0 * 8.487927436828613
Epoch 550, val loss: 0.7819928526878357
Epoch 560, training loss: 849.15673828125 = 0.7617874145507812 + 100.0 * 8.483949661254883
Epoch 560, val loss: 0.7741029858589172
Epoch 570, training loss: 848.8167114257812 = 0.7534114718437195 + 100.0 * 8.480632781982422
Epoch 570, val loss: 0.7664192914962769
Epoch 580, training loss: 848.5391235351562 = 0.74521404504776 + 100.0 * 8.477938652038574
Epoch 580, val loss: 0.7589425444602966
Epoch 590, training loss: 848.515625 = 0.7371888160705566 + 100.0 * 8.477784156799316
Epoch 590, val loss: 0.7516278028488159
Epoch 600, training loss: 847.9700317382812 = 0.7293974161148071 + 100.0 * 8.472406387329102
Epoch 600, val loss: 0.7445724606513977
Epoch 610, training loss: 847.645263671875 = 0.7219117879867554 + 100.0 * 8.469233512878418
Epoch 610, val loss: 0.7378684878349304
Epoch 620, training loss: 847.375732421875 = 0.7146742343902588 + 100.0 * 8.4666109085083
Epoch 620, val loss: 0.7313709259033203
Epoch 630, training loss: 847.2041015625 = 0.7076459527015686 + 100.0 * 8.464964866638184
Epoch 630, val loss: 0.7251359224319458
Epoch 640, training loss: 846.93359375 = 0.7007813453674316 + 100.0 * 8.46232795715332
Epoch 640, val loss: 0.7190685868263245
Epoch 650, training loss: 846.7404174804688 = 0.6941239237785339 + 100.0 * 8.46046257019043
Epoch 650, val loss: 0.7131311297416687
Epoch 660, training loss: 846.3661499023438 = 0.6876484751701355 + 100.0 * 8.456785202026367
Epoch 660, val loss: 0.7075522541999817
Epoch 670, training loss: 846.1278686523438 = 0.681483268737793 + 100.0 * 8.454463958740234
Epoch 670, val loss: 0.7021905183792114
Epoch 680, training loss: 846.1201171875 = 0.6755207180976868 + 100.0 * 8.454445838928223
Epoch 680, val loss: 0.6970042586326599
Epoch 690, training loss: 845.7598876953125 = 0.669644296169281 + 100.0 * 8.450902938842773
Epoch 690, val loss: 0.6919384002685547
Epoch 700, training loss: 845.552001953125 = 0.6640151143074036 + 100.0 * 8.448880195617676
Epoch 700, val loss: 0.687140166759491
Epoch 710, training loss: 845.4611206054688 = 0.6585761904716492 + 100.0 * 8.448025703430176
Epoch 710, val loss: 0.6825241446495056
Epoch 720, training loss: 845.4242553710938 = 0.6532756090164185 + 100.0 * 8.447710037231445
Epoch 720, val loss: 0.67805016040802
Epoch 730, training loss: 845.1241455078125 = 0.6481476426124573 + 100.0 * 8.4447603225708
Epoch 730, val loss: 0.6736864447593689
Epoch 740, training loss: 844.9047241210938 = 0.6432632803916931 + 100.0 * 8.442614555358887
Epoch 740, val loss: 0.6696382164955139
Epoch 750, training loss: 844.771240234375 = 0.6385440826416016 + 100.0 * 8.441327095031738
Epoch 750, val loss: 0.6657018661499023
Epoch 760, training loss: 844.8510131835938 = 0.6339747309684753 + 100.0 * 8.442170143127441
Epoch 760, val loss: 0.6619217991828918
Epoch 770, training loss: 844.5588989257812 = 0.6295281052589417 + 100.0 * 8.43929386138916
Epoch 770, val loss: 0.6582750678062439
Epoch 780, training loss: 844.4463500976562 = 0.625282883644104 + 100.0 * 8.438210487365723
Epoch 780, val loss: 0.6547942757606506
Epoch 790, training loss: 844.3799438476562 = 0.6211119890213013 + 100.0 * 8.437588691711426
Epoch 790, val loss: 0.6513923406600952
Epoch 800, training loss: 844.1138305664062 = 0.6171053647994995 + 100.0 * 8.434967041015625
Epoch 800, val loss: 0.6481290459632874
Epoch 810, training loss: 844.0074462890625 = 0.6133056282997131 + 100.0 * 8.433941841125488
Epoch 810, val loss: 0.6450580358505249
Epoch 820, training loss: 843.8854370117188 = 0.6096348762512207 + 100.0 * 8.432758331298828
Epoch 820, val loss: 0.6421546339988708
Epoch 830, training loss: 844.0285034179688 = 0.6060627698898315 + 100.0 * 8.434224128723145
Epoch 830, val loss: 0.639325737953186
Epoch 840, training loss: 843.764404296875 = 0.6025128364562988 + 100.0 * 8.431618690490723
Epoch 840, val loss: 0.6364371180534363
Epoch 850, training loss: 843.614501953125 = 0.5991392731666565 + 100.0 * 8.430153846740723
Epoch 850, val loss: 0.6338310241699219
Epoch 860, training loss: 843.4811401367188 = 0.5959253311157227 + 100.0 * 8.428852081298828
Epoch 860, val loss: 0.6313086152076721
Epoch 870, training loss: 843.43994140625 = 0.5927887558937073 + 100.0 * 8.428471565246582
Epoch 870, val loss: 0.6288764476776123
Epoch 880, training loss: 843.4821166992188 = 0.5896817445755005 + 100.0 * 8.428924560546875
Epoch 880, val loss: 0.6263948082923889
Epoch 890, training loss: 843.266845703125 = 0.5866606831550598 + 100.0 * 8.426801681518555
Epoch 890, val loss: 0.6241183876991272
Epoch 900, training loss: 843.165283203125 = 0.583810031414032 + 100.0 * 8.425814628601074
Epoch 900, val loss: 0.6219009160995483
Epoch 910, training loss: 843.2484741210938 = 0.581015408039093 + 100.0 * 8.426674842834473
Epoch 910, val loss: 0.6197697520256042
Epoch 920, training loss: 843.0305786132812 = 0.5783032774925232 + 100.0 * 8.424522399902344
Epoch 920, val loss: 0.6176728010177612
Epoch 930, training loss: 842.9083862304688 = 0.5757269263267517 + 100.0 * 8.42332649230957
Epoch 930, val loss: 0.6157596111297607
Epoch 940, training loss: 842.8146362304688 = 0.573198676109314 + 100.0 * 8.422414779663086
Epoch 940, val loss: 0.613858699798584
Epoch 950, training loss: 842.969970703125 = 0.5707466006278992 + 100.0 * 8.423992156982422
Epoch 950, val loss: 0.6119918823242188
Epoch 960, training loss: 843.1197509765625 = 0.5681439638137817 + 100.0 * 8.425516128540039
Epoch 960, val loss: 0.6101167798042297
Epoch 970, training loss: 842.7200317382812 = 0.5657358765602112 + 100.0 * 8.42154312133789
Epoch 970, val loss: 0.6082652807235718
Epoch 980, training loss: 842.5591430664062 = 0.5635382533073425 + 100.0 * 8.41995620727539
Epoch 980, val loss: 0.6066650152206421
Epoch 990, training loss: 842.465087890625 = 0.5613784193992615 + 100.0 * 8.419036865234375
Epoch 990, val loss: 0.6050665378570557
Epoch 1000, training loss: 842.390869140625 = 0.559264600276947 + 100.0 * 8.418315887451172
Epoch 1000, val loss: 0.6035362482070923
Epoch 1010, training loss: 842.3174438476562 = 0.5572035312652588 + 100.0 * 8.4176025390625
Epoch 1010, val loss: 0.6020464301109314
Epoch 1020, training loss: 842.2440185546875 = 0.5551874041557312 + 100.0 * 8.416888236999512
Epoch 1020, val loss: 0.6005821824073792
Epoch 1030, training loss: 842.173583984375 = 0.5532128810882568 + 100.0 * 8.416203498840332
Epoch 1030, val loss: 0.5991673469543457
Epoch 1040, training loss: 842.2937622070312 = 0.5512800812721252 + 100.0 * 8.417425155639648
Epoch 1040, val loss: 0.5977456569671631
Epoch 1050, training loss: 842.3726806640625 = 0.5492870807647705 + 100.0 * 8.418233871459961
Epoch 1050, val loss: 0.5963355302810669
Epoch 1060, training loss: 842.043701171875 = 0.5473946928977966 + 100.0 * 8.414962768554688
Epoch 1060, val loss: 0.5950021743774414
Epoch 1070, training loss: 841.9296264648438 = 0.5456182956695557 + 100.0 * 8.413840293884277
Epoch 1070, val loss: 0.5937371850013733
Epoch 1080, training loss: 841.8448486328125 = 0.5438750982284546 + 100.0 * 8.413009643554688
Epoch 1080, val loss: 0.5925046801567078
Epoch 1090, training loss: 841.7910766601562 = 0.5421679019927979 + 100.0 * 8.41248893737793
Epoch 1090, val loss: 0.5913228392601013
Epoch 1100, training loss: 842.093017578125 = 0.5404484868049622 + 100.0 * 8.415525436401367
Epoch 1100, val loss: 0.59012371301651
Epoch 1110, training loss: 841.6561889648438 = 0.5387213230133057 + 100.0 * 8.411174774169922
Epoch 1110, val loss: 0.5889049172401428
Epoch 1120, training loss: 841.6727294921875 = 0.5370850563049316 + 100.0 * 8.411355972290039
Epoch 1120, val loss: 0.5877701044082642
Epoch 1130, training loss: 841.7517700195312 = 0.5354648232460022 + 100.0 * 8.412162780761719
Epoch 1130, val loss: 0.586691677570343
Epoch 1140, training loss: 841.470703125 = 0.5338641405105591 + 100.0 * 8.409368515014648
Epoch 1140, val loss: 0.5854763388633728
Epoch 1150, training loss: 841.423095703125 = 0.5323180556297302 + 100.0 * 8.408907890319824
Epoch 1150, val loss: 0.5844346284866333
Epoch 1160, training loss: 841.31640625 = 0.5307965278625488 + 100.0 * 8.407855987548828
Epoch 1160, val loss: 0.5834137797355652
Epoch 1170, training loss: 841.2509765625 = 0.5293043255805969 + 100.0 * 8.407217025756836
Epoch 1170, val loss: 0.5824136137962341
Epoch 1180, training loss: 841.5680541992188 = 0.5278156399726868 + 100.0 * 8.410402297973633
Epoch 1180, val loss: 0.5815107822418213
Epoch 1190, training loss: 841.5462646484375 = 0.526189923286438 + 100.0 * 8.410201072692871
Epoch 1190, val loss: 0.5801156163215637
Epoch 1200, training loss: 841.0775756835938 = 0.5246796011924744 + 100.0 * 8.405529022216797
Epoch 1200, val loss: 0.5791996121406555
Epoch 1210, training loss: 841.0640258789062 = 0.5232911705970764 + 100.0 * 8.405406951904297
Epoch 1210, val loss: 0.5783225893974304
Epoch 1220, training loss: 840.9564819335938 = 0.5219087600708008 + 100.0 * 8.404345512390137
Epoch 1220, val loss: 0.5774081349372864
Epoch 1230, training loss: 840.8912963867188 = 0.5205424427986145 + 100.0 * 8.403707504272461
Epoch 1230, val loss: 0.576492965221405
Epoch 1240, training loss: 840.8353881835938 = 0.519193708896637 + 100.0 * 8.403162002563477
Epoch 1240, val loss: 0.5756345391273499
Epoch 1250, training loss: 841.2371215820312 = 0.517822802066803 + 100.0 * 8.407193183898926
Epoch 1250, val loss: 0.5747184157371521
Epoch 1260, training loss: 840.7431640625 = 0.5164216160774231 + 100.0 * 8.402267456054688
Epoch 1260, val loss: 0.5738016963005066
Epoch 1270, training loss: 840.713623046875 = 0.5151224136352539 + 100.0 * 8.401985168457031
Epoch 1270, val loss: 0.5729093551635742
Epoch 1280, training loss: 840.6266479492188 = 0.5138388276100159 + 100.0 * 8.401127815246582
Epoch 1280, val loss: 0.5720950961112976
Epoch 1290, training loss: 840.8667602539062 = 0.5125347971916199 + 100.0 * 8.403542518615723
Epoch 1290, val loss: 0.5712811350822449
Epoch 1300, training loss: 840.5830078125 = 0.5112349987030029 + 100.0 * 8.400717735290527
Epoch 1300, val loss: 0.5704018473625183
Epoch 1310, training loss: 840.4534912109375 = 0.5099844336509705 + 100.0 * 8.399435043334961
Epoch 1310, val loss: 0.5695910453796387
Epoch 1320, training loss: 840.406005859375 = 0.5087381601333618 + 100.0 * 8.398972511291504
Epoch 1320, val loss: 0.5687779784202576
Epoch 1330, training loss: 840.7220458984375 = 0.5074900388717651 + 100.0 * 8.402145385742188
Epoch 1330, val loss: 0.5678872466087341
Epoch 1340, training loss: 840.3656616210938 = 0.5061821937561035 + 100.0 * 8.398594856262207
Epoch 1340, val loss: 0.5672208070755005
Epoch 1350, training loss: 840.2677001953125 = 0.5049583911895752 + 100.0 * 8.397627830505371
Epoch 1350, val loss: 0.5663619041442871
Epoch 1360, training loss: 840.5987548828125 = 0.5037263631820679 + 100.0 * 8.40095043182373
Epoch 1360, val loss: 0.5655955076217651
Epoch 1370, training loss: 840.2049560546875 = 0.5024471282958984 + 100.0 * 8.397025108337402
Epoch 1370, val loss: 0.5648373365402222
Epoch 1380, training loss: 840.0540161132812 = 0.5012465119361877 + 100.0 * 8.395527839660645
Epoch 1380, val loss: 0.5640637874603271
Epoch 1390, training loss: 839.9862670898438 = 0.5000534653663635 + 100.0 * 8.394862174987793
Epoch 1390, val loss: 0.563329815864563
Epoch 1400, training loss: 839.9747924804688 = 0.49887219071388245 + 100.0 * 8.394759178161621
Epoch 1400, val loss: 0.5626171827316284
Epoch 1410, training loss: 840.2078857421875 = 0.49766817688941956 + 100.0 * 8.397102355957031
Epoch 1410, val loss: 0.5617781281471252
Epoch 1420, training loss: 840.0082397460938 = 0.4963955581188202 + 100.0 * 8.395118713378906
Epoch 1420, val loss: 0.5609723329544067
Epoch 1430, training loss: 839.8771362304688 = 0.49517279863357544 + 100.0 * 8.393819808959961
Epoch 1430, val loss: 0.5602899789810181
Epoch 1440, training loss: 839.7611083984375 = 0.4939880669116974 + 100.0 * 8.392670631408691
Epoch 1440, val loss: 0.5594682693481445
Epoch 1450, training loss: 839.7112426757812 = 0.4928147792816162 + 100.0 * 8.392184257507324
Epoch 1450, val loss: 0.5587697625160217
Epoch 1460, training loss: 839.8883056640625 = 0.49162986874580383 + 100.0 * 8.393966674804688
Epoch 1460, val loss: 0.5579895973205566
Epoch 1470, training loss: 839.6094970703125 = 0.49038106203079224 + 100.0 * 8.391191482543945
Epoch 1470, val loss: 0.557294487953186
Epoch 1480, training loss: 839.6385498046875 = 0.48919036984443665 + 100.0 * 8.391493797302246
Epoch 1480, val loss: 0.5565037131309509
Epoch 1490, training loss: 839.8518676757812 = 0.4879820942878723 + 100.0 * 8.393638610839844
Epoch 1490, val loss: 0.5557630658149719
Epoch 1500, training loss: 839.4926147460938 = 0.4867416322231293 + 100.0 * 8.390058517456055
Epoch 1500, val loss: 0.5550580620765686
Epoch 1510, training loss: 839.4509887695312 = 0.48556557297706604 + 100.0 * 8.389654159545898
Epoch 1510, val loss: 0.5543375611305237
Epoch 1520, training loss: 839.3809204101562 = 0.4844205975532532 + 100.0 * 8.388964653015137
Epoch 1520, val loss: 0.5536153316497803
Epoch 1530, training loss: 839.3419799804688 = 0.4832768440246582 + 100.0 * 8.38858699798584
Epoch 1530, val loss: 0.5529181361198425
Epoch 1540, training loss: 839.494140625 = 0.4821251630783081 + 100.0 * 8.390120506286621
Epoch 1540, val loss: 0.5521329641342163
Epoch 1550, training loss: 839.309814453125 = 0.48089420795440674 + 100.0 * 8.388289451599121
Epoch 1550, val loss: 0.5513861179351807
Epoch 1560, training loss: 839.2465209960938 = 0.47970351576805115 + 100.0 * 8.38766860961914
Epoch 1560, val loss: 0.5506801605224609
Epoch 1570, training loss: 839.199462890625 = 0.4785543382167816 + 100.0 * 8.387208938598633
Epoch 1570, val loss: 0.5499024987220764
Epoch 1580, training loss: 839.1708374023438 = 0.47739362716674805 + 100.0 * 8.386934280395508
Epoch 1580, val loss: 0.5492036938667297
Epoch 1590, training loss: 839.4461669921875 = 0.47620800137519836 + 100.0 * 8.389699935913086
Epoch 1590, val loss: 0.5483508110046387
Epoch 1600, training loss: 839.1837768554688 = 0.47497570514678955 + 100.0 * 8.38708782196045
Epoch 1600, val loss: 0.547716498374939
Epoch 1610, training loss: 839.07373046875 = 0.4737716317176819 + 100.0 * 8.38599967956543
Epoch 1610, val loss: 0.5468608140945435
Epoch 1620, training loss: 839.0224609375 = 0.4725620150566101 + 100.0 * 8.385499000549316
Epoch 1620, val loss: 0.5460534691810608
Epoch 1630, training loss: 839.1036987304688 = 0.47134801745414734 + 100.0 * 8.386322975158691
Epoch 1630, val loss: 0.5452369451522827
Epoch 1640, training loss: 838.9386596679688 = 0.47011250257492065 + 100.0 * 8.384685516357422
Epoch 1640, val loss: 0.5444549322128296
Epoch 1650, training loss: 838.9981689453125 = 0.46889814734458923 + 100.0 * 8.385293006896973
Epoch 1650, val loss: 0.5436471700668335
Epoch 1660, training loss: 839.03369140625 = 0.4676474928855896 + 100.0 * 8.385660171508789
Epoch 1660, val loss: 0.5427236557006836
Epoch 1670, training loss: 838.8433837890625 = 0.4663811922073364 + 100.0 * 8.383769989013672
Epoch 1670, val loss: 0.5420536994934082
Epoch 1680, training loss: 838.7903442382812 = 0.46516403555870056 + 100.0 * 8.383252143859863
Epoch 1680, val loss: 0.5411868691444397
Epoch 1690, training loss: 838.7286987304688 = 0.4639444351196289 + 100.0 * 8.382647514343262
Epoch 1690, val loss: 0.5404152870178223
Epoch 1700, training loss: 838.7672119140625 = 0.4627133011817932 + 100.0 * 8.383045196533203
Epoch 1700, val loss: 0.5396623015403748
Epoch 1710, training loss: 838.97802734375 = 0.46143049001693726 + 100.0 * 8.38516616821289
Epoch 1710, val loss: 0.5388140082359314
Epoch 1720, training loss: 838.71337890625 = 0.4601374566555023 + 100.0 * 8.382532119750977
Epoch 1720, val loss: 0.5378082394599915
Epoch 1730, training loss: 838.614013671875 = 0.4588824212551117 + 100.0 * 8.381551742553711
Epoch 1730, val loss: 0.5370036363601685
Epoch 1740, training loss: 838.5846557617188 = 0.45762577652931213 + 100.0 * 8.381270408630371
Epoch 1740, val loss: 0.5361713171005249
Epoch 1750, training loss: 838.8659057617188 = 0.4563472867012024 + 100.0 * 8.384095191955566
Epoch 1750, val loss: 0.5352386832237244
Epoch 1760, training loss: 838.614013671875 = 0.45500460267066956 + 100.0 * 8.381589889526367
Epoch 1760, val loss: 0.5343621969223022
Epoch 1770, training loss: 838.5848388671875 = 0.4536949396133423 + 100.0 * 8.381311416625977
Epoch 1770, val loss: 0.5335115790367126
Epoch 1780, training loss: 838.4564819335938 = 0.4524019956588745 + 100.0 * 8.380041122436523
Epoch 1780, val loss: 0.5326563119888306
Epoch 1790, training loss: 838.3919067382812 = 0.45111724734306335 + 100.0 * 8.37940788269043
Epoch 1790, val loss: 0.5317908525466919
Epoch 1800, training loss: 838.3706665039062 = 0.4498230516910553 + 100.0 * 8.3792085647583
Epoch 1800, val loss: 0.5309380292892456
Epoch 1810, training loss: 838.8170166015625 = 0.44850224256515503 + 100.0 * 8.383685111999512
Epoch 1810, val loss: 0.5299855470657349
Epoch 1820, training loss: 838.5770874023438 = 0.4470886290073395 + 100.0 * 8.38129997253418
Epoch 1820, val loss: 0.5289771556854248
Epoch 1830, training loss: 838.3421630859375 = 0.4457221031188965 + 100.0 * 8.3789644241333
Epoch 1830, val loss: 0.5281538367271423
Epoch 1840, training loss: 838.2451171875 = 0.44439586997032166 + 100.0 * 8.378006935119629
Epoch 1840, val loss: 0.52734375
Epoch 1850, training loss: 838.213134765625 = 0.44307997822761536 + 100.0 * 8.377700805664062
Epoch 1850, val loss: 0.5264530777931213
Epoch 1860, training loss: 838.3281860351562 = 0.44175833463668823 + 100.0 * 8.378864288330078
Epoch 1860, val loss: 0.5255803465843201
Epoch 1870, training loss: 838.2501220703125 = 0.44037020206451416 + 100.0 * 8.378097534179688
Epoch 1870, val loss: 0.5246720910072327
Epoch 1880, training loss: 838.1322631835938 = 0.4389994442462921 + 100.0 * 8.376932144165039
Epoch 1880, val loss: 0.5236928462982178
Epoch 1890, training loss: 838.0873413085938 = 0.4376566708087921 + 100.0 * 8.376496315002441
Epoch 1890, val loss: 0.5227457284927368
Epoch 1900, training loss: 838.0940551757812 = 0.4363144040107727 + 100.0 * 8.376577377319336
Epoch 1900, val loss: 0.521804690361023
Epoch 1910, training loss: 838.3771362304688 = 0.4349499046802521 + 100.0 * 8.379422187805176
Epoch 1910, val loss: 0.520737886428833
Epoch 1920, training loss: 838.0855712890625 = 0.4335063099861145 + 100.0 * 8.376520156860352
Epoch 1920, val loss: 0.5200009346008301
Epoch 1930, training loss: 838.0028686523438 = 0.4321174919605255 + 100.0 * 8.375707626342773
Epoch 1930, val loss: 0.5189813375473022
Epoch 1940, training loss: 837.9730834960938 = 0.4307379722595215 + 100.0 * 8.375423431396484
Epoch 1940, val loss: 0.5181087255477905
Epoch 1950, training loss: 837.9276733398438 = 0.4293549060821533 + 100.0 * 8.374982833862305
Epoch 1950, val loss: 0.5171621441841125
Epoch 1960, training loss: 838.1570434570312 = 0.427961140871048 + 100.0 * 8.377290725708008
Epoch 1960, val loss: 0.5163371562957764
Epoch 1970, training loss: 837.8971557617188 = 0.4264843761920929 + 100.0 * 8.374706268310547
Epoch 1970, val loss: 0.5151437520980835
Epoch 1980, training loss: 837.961181640625 = 0.425052285194397 + 100.0 * 8.375361442565918
Epoch 1980, val loss: 0.5143048763275146
Epoch 1990, training loss: 837.8761596679688 = 0.4236072897911072 + 100.0 * 8.374526023864746
Epoch 1990, val loss: 0.5132208466529846
Epoch 2000, training loss: 837.8167724609375 = 0.42219510674476624 + 100.0 * 8.373946189880371
Epoch 2000, val loss: 0.5124109387397766
Epoch 2010, training loss: 837.7689208984375 = 0.4207764267921448 + 100.0 * 8.373481750488281
Epoch 2010, val loss: 0.5114309787750244
Epoch 2020, training loss: 837.8458251953125 = 0.4193525016307831 + 100.0 * 8.37426471710205
Epoch 2020, val loss: 0.510440468788147
Epoch 2030, training loss: 837.9158325195312 = 0.41786596179008484 + 100.0 * 8.374979972839355
Epoch 2030, val loss: 0.5094479918479919
Epoch 2040, training loss: 837.6891479492188 = 0.41635385155677795 + 100.0 * 8.372727394104004
Epoch 2040, val loss: 0.5084701180458069
Epoch 2050, training loss: 837.7101440429688 = 0.4149055778980255 + 100.0 * 8.372952461242676
Epoch 2050, val loss: 0.5076402425765991
Epoch 2060, training loss: 837.6315307617188 = 0.4134587049484253 + 100.0 * 8.372180938720703
Epoch 2060, val loss: 0.5066050887107849
Epoch 2070, training loss: 837.6123046875 = 0.41202089190483093 + 100.0 * 8.372002601623535
Epoch 2070, val loss: 0.5056912899017334
Epoch 2080, training loss: 837.75390625 = 0.4105609655380249 + 100.0 * 8.373433113098145
Epoch 2080, val loss: 0.5047243237495422
Epoch 2090, training loss: 837.5699462890625 = 0.4090602993965149 + 100.0 * 8.37160873413086
Epoch 2090, val loss: 0.503743052482605
Epoch 2100, training loss: 837.7749633789062 = 0.40757977962493896 + 100.0 * 8.373673439025879
Epoch 2100, val loss: 0.5028652548789978
Epoch 2110, training loss: 837.5415649414062 = 0.4060574471950531 + 100.0 * 8.371355056762695
Epoch 2110, val loss: 0.5017864108085632
Epoch 2120, training loss: 837.5181884765625 = 0.4045827090740204 + 100.0 * 8.371135711669922
Epoch 2120, val loss: 0.5007604360580444
Epoch 2130, training loss: 837.4679565429688 = 0.40312111377716064 + 100.0 * 8.370648384094238
Epoch 2130, val loss: 0.4998338520526886
Epoch 2140, training loss: 837.4354858398438 = 0.4016634523868561 + 100.0 * 8.370338439941406
Epoch 2140, val loss: 0.498934805393219
Epoch 2150, training loss: 837.4229736328125 = 0.4001948833465576 + 100.0 * 8.370227813720703
Epoch 2150, val loss: 0.497976154088974
Epoch 2160, training loss: 837.555908203125 = 0.39871281385421753 + 100.0 * 8.37157154083252
Epoch 2160, val loss: 0.4969211220741272
Epoch 2170, training loss: 837.83935546875 = 0.39716777205467224 + 100.0 * 8.374422073364258
Epoch 2170, val loss: 0.49585282802581787
Epoch 2180, training loss: 837.4837646484375 = 0.3956051170825958 + 100.0 * 8.370881080627441
Epoch 2180, val loss: 0.4950944185256958
Epoch 2190, training loss: 837.3430786132812 = 0.3940955102443695 + 100.0 * 8.369489669799805
Epoch 2190, val loss: 0.49401286244392395
Epoch 2200, training loss: 837.30126953125 = 0.39260390400886536 + 100.0 * 8.369087219238281
Epoch 2200, val loss: 0.4930689334869385
Epoch 2210, training loss: 837.2761840820312 = 0.39111971855163574 + 100.0 * 8.368850708007812
Epoch 2210, val loss: 0.49214616417884827
Epoch 2220, training loss: 837.2612915039062 = 0.38962849974632263 + 100.0 * 8.3687162399292
Epoch 2220, val loss: 0.4912171959877014
Epoch 2230, training loss: 837.5 = 0.388126939535141 + 100.0 * 8.371118545532227
Epoch 2230, val loss: 0.49029624462127686
Epoch 2240, training loss: 837.274169921875 = 0.38657346367836 + 100.0 * 8.368875503540039
Epoch 2240, val loss: 0.489228755235672
Epoch 2250, training loss: 837.3533325195312 = 0.3850555717945099 + 100.0 * 8.369682312011719
Epoch 2250, val loss: 0.4882950782775879
Epoch 2260, training loss: 837.2638549804688 = 0.3835422992706299 + 100.0 * 8.368803024291992
Epoch 2260, val loss: 0.487430214881897
Epoch 2270, training loss: 837.1665649414062 = 0.3820545971393585 + 100.0 * 8.367844581604004
Epoch 2270, val loss: 0.48660725355148315
Epoch 2280, training loss: 837.1907958984375 = 0.3805861473083496 + 100.0 * 8.368102073669434
Epoch 2280, val loss: 0.4858127534389496
Epoch 2290, training loss: 837.2943115234375 = 0.37910640239715576 + 100.0 * 8.369152069091797
Epoch 2290, val loss: 0.48490676283836365
Epoch 2300, training loss: 837.161865234375 = 0.3776046633720398 + 100.0 * 8.367842674255371
Epoch 2300, val loss: 0.48394274711608887
Epoch 2310, training loss: 837.3073120117188 = 0.3761177957057953 + 100.0 * 8.369312286376953
Epoch 2310, val loss: 0.4829353988170624
Epoch 2320, training loss: 837.059814453125 = 0.37461259961128235 + 100.0 * 8.366851806640625
Epoch 2320, val loss: 0.4821188151836395
Epoch 2330, training loss: 837.0451049804688 = 0.37314873933792114 + 100.0 * 8.366719245910645
Epoch 2330, val loss: 0.4812983572483063
Epoch 2340, training loss: 837.0336303710938 = 0.3716980218887329 + 100.0 * 8.366619110107422
Epoch 2340, val loss: 0.4804931879043579
Epoch 2350, training loss: 837.11767578125 = 0.3702465891838074 + 100.0 * 8.367474555969238
Epoch 2350, val loss: 0.47980499267578125
Epoch 2360, training loss: 837.3408203125 = 0.3687456250190735 + 100.0 * 8.369720458984375
Epoch 2360, val loss: 0.47889435291290283
Epoch 2370, training loss: 837.0316162109375 = 0.36723023653030396 + 100.0 * 8.366643905639648
Epoch 2370, val loss: 0.47787415981292725
Epoch 2380, training loss: 836.9546508789062 = 0.36576974391937256 + 100.0 * 8.365888595581055
Epoch 2380, val loss: 0.4771292507648468
Epoch 2390, training loss: 836.93408203125 = 0.36432287096977234 + 100.0 * 8.365697860717773
Epoch 2390, val loss: 0.47635477781295776
Epoch 2400, training loss: 836.9285888671875 = 0.36289140582084656 + 100.0 * 8.365656852722168
Epoch 2400, val loss: 0.4756181538105011
Epoch 2410, training loss: 837.10595703125 = 0.3614523410797119 + 100.0 * 8.36744499206543
Epoch 2410, val loss: 0.47481611371040344
Epoch 2420, training loss: 836.9559326171875 = 0.35997653007507324 + 100.0 * 8.365959167480469
Epoch 2420, val loss: 0.4738790690898895
Epoch 2430, training loss: 836.9041748046875 = 0.3585204780101776 + 100.0 * 8.365456581115723
Epoch 2430, val loss: 0.4731392562389374
Epoch 2440, training loss: 836.8452758789062 = 0.3570892810821533 + 100.0 * 8.36488151550293
Epoch 2440, val loss: 0.4724280834197998
Epoch 2450, training loss: 836.8780517578125 = 0.35567569732666016 + 100.0 * 8.36522388458252
Epoch 2450, val loss: 0.471686989068985
Epoch 2460, training loss: 836.9634399414062 = 0.3542540669441223 + 100.0 * 8.36609172821045
Epoch 2460, val loss: 0.47089073061943054
Epoch 2470, training loss: 836.9862670898438 = 0.3528437614440918 + 100.0 * 8.366333961486816
Epoch 2470, val loss: 0.4700455665588379
Epoch 2480, training loss: 837.0003051757812 = 0.3514132797718048 + 100.0 * 8.366488456726074
Epoch 2480, val loss: 0.46932193636894226
Epoch 2490, training loss: 836.8433227539062 = 0.3500231206417084 + 100.0 * 8.364933013916016
Epoch 2490, val loss: 0.4687656760215759
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8168442415017757
0.865391581540245
=== training gcn model ===
Epoch 0, training loss: 1059.312255859375 = 1.082236886024475 + 100.0 * 10.582300186157227
Epoch 0, val loss: 1.0816941261291504
Epoch 10, training loss: 1059.2806396484375 = 1.078974962234497 + 100.0 * 10.582015991210938
Epoch 10, val loss: 1.0784295797348022
Epoch 20, training loss: 1059.1473388671875 = 1.0754319429397583 + 100.0 * 10.580718994140625
Epoch 20, val loss: 1.0748968124389648
Epoch 30, training loss: 1058.5628662109375 = 1.0716367959976196 + 100.0 * 10.574912071228027
Epoch 30, val loss: 1.0711053609848022
Epoch 40, training loss: 1056.35986328125 = 1.067445158958435 + 100.0 * 10.552923202514648
Epoch 40, val loss: 1.0668871402740479
Epoch 50, training loss: 1050.19775390625 = 1.06277334690094 + 100.0 * 10.491350173950195
Epoch 50, val loss: 1.0621980428695679
Epoch 60, training loss: 1036.6622314453125 = 1.0582752227783203 + 100.0 * 10.356040000915527
Epoch 60, val loss: 1.0577529668807983
Epoch 70, training loss: 1015.2023315429688 = 1.0542089939117432 + 100.0 * 10.141481399536133
Epoch 70, val loss: 1.0536384582519531
Epoch 80, training loss: 987.6294555664062 = 1.0490386486053467 + 100.0 * 9.865804672241211
Epoch 80, val loss: 1.048378348350525
Epoch 90, training loss: 959.5086059570312 = 1.041298508644104 + 100.0 * 9.584672927856445
Epoch 90, val loss: 1.0407475233078003
Epoch 100, training loss: 943.7955322265625 = 1.032946228981018 + 100.0 * 9.42762565612793
Epoch 100, val loss: 1.0328422784805298
Epoch 110, training loss: 935.3831176757812 = 1.026331901550293 + 100.0 * 9.343567848205566
Epoch 110, val loss: 1.0266845226287842
Epoch 120, training loss: 929.6110229492188 = 1.0210888385772705 + 100.0 * 9.28589916229248
Epoch 120, val loss: 1.0217816829681396
Epoch 130, training loss: 926.3724365234375 = 1.0163946151733398 + 100.0 * 9.253560066223145
Epoch 130, val loss: 1.0172640085220337
Epoch 140, training loss: 924.4249267578125 = 1.0111126899719238 + 100.0 * 9.234138488769531
Epoch 140, val loss: 1.0119980573654175
Epoch 150, training loss: 921.8796997070312 = 1.0049402713775635 + 100.0 * 9.208747863769531
Epoch 150, val loss: 1.005942702293396
Epoch 160, training loss: 918.4722900390625 = 0.9989626407623291 + 100.0 * 9.17473316192627
Epoch 160, val loss: 1.0002096891403198
Epoch 170, training loss: 913.2825317382812 = 0.9939473271369934 + 100.0 * 9.122885704040527
Epoch 170, val loss: 0.9955266118049622
Epoch 180, training loss: 905.9830932617188 = 0.9902493953704834 + 100.0 * 9.049928665161133
Epoch 180, val loss: 0.9921504259109497
Epoch 190, training loss: 899.96826171875 = 0.9875214695930481 + 100.0 * 8.98980712890625
Epoch 190, val loss: 0.9895291924476624
Epoch 200, training loss: 895.9674072265625 = 0.9832801222801208 + 100.0 * 8.949841499328613
Epoch 200, val loss: 0.9853750467300415
Epoch 210, training loss: 889.990478515625 = 0.980324923992157 + 100.0 * 8.890101432800293
Epoch 210, val loss: 0.9829360246658325
Epoch 220, training loss: 884.7581176757812 = 0.9788674116134644 + 100.0 * 8.83779239654541
Epoch 220, val loss: 0.9816591143608093
Epoch 230, training loss: 880.339111328125 = 0.9762491583824158 + 100.0 * 8.793628692626953
Epoch 230, val loss: 0.9789864420890808
Epoch 240, training loss: 877.2925415039062 = 0.9717026352882385 + 100.0 * 8.763208389282227
Epoch 240, val loss: 0.9742850065231323
Epoch 250, training loss: 874.8792724609375 = 0.9651752710342407 + 100.0 * 8.739141464233398
Epoch 250, val loss: 0.9678018093109131
Epoch 260, training loss: 872.79248046875 = 0.9579150676727295 + 100.0 * 8.718345642089844
Epoch 260, val loss: 0.9607452750205994
Epoch 270, training loss: 871.075439453125 = 0.9507396221160889 + 100.0 * 8.701247215270996
Epoch 270, val loss: 0.9537349343299866
Epoch 280, training loss: 869.6740112304688 = 0.9431772828102112 + 100.0 * 8.687308311462402
Epoch 280, val loss: 0.9463258385658264
Epoch 290, training loss: 868.572021484375 = 0.9349089860916138 + 100.0 * 8.676370620727539
Epoch 290, val loss: 0.9383092522621155
Epoch 300, training loss: 867.2232666015625 = 0.926378071308136 + 100.0 * 8.662968635559082
Epoch 300, val loss: 0.9300356507301331
Epoch 310, training loss: 865.93505859375 = 0.9178547263145447 + 100.0 * 8.650172233581543
Epoch 310, val loss: 0.9217420816421509
Epoch 320, training loss: 864.44921875 = 0.9091299176216125 + 100.0 * 8.635400772094727
Epoch 320, val loss: 0.9133129119873047
Epoch 330, training loss: 863.0318603515625 = 0.9002974629402161 + 100.0 * 8.621315956115723
Epoch 330, val loss: 0.9047342538833618
Epoch 340, training loss: 862.0491333007812 = 0.8909503817558289 + 100.0 * 8.611581802368164
Epoch 340, val loss: 0.8956770300865173
Epoch 350, training loss: 861.1361083984375 = 0.8808444738388062 + 100.0 * 8.60255241394043
Epoch 350, val loss: 0.8858399987220764
Epoch 360, training loss: 860.4529418945312 = 0.8701453804969788 + 100.0 * 8.59582805633545
Epoch 360, val loss: 0.8755068778991699
Epoch 370, training loss: 859.8544921875 = 0.8591515421867371 + 100.0 * 8.589953422546387
Epoch 370, val loss: 0.8649032115936279
Epoch 380, training loss: 859.3116455078125 = 0.8479086756706238 + 100.0 * 8.584637641906738
Epoch 380, val loss: 0.8540992140769958
Epoch 390, training loss: 858.7432250976562 = 0.8364892601966858 + 100.0 * 8.57906723022461
Epoch 390, val loss: 0.8431586623191833
Epoch 400, training loss: 858.2334594726562 = 0.8251116871833801 + 100.0 * 8.57408332824707
Epoch 400, val loss: 0.832289457321167
Epoch 410, training loss: 857.6524658203125 = 0.813826322555542 + 100.0 * 8.56838607788086
Epoch 410, val loss: 0.8215265274047852
Epoch 420, training loss: 857.2047119140625 = 0.802590548992157 + 100.0 * 8.564021110534668
Epoch 420, val loss: 0.8108340501785278
Epoch 430, training loss: 856.5435791015625 = 0.7913534045219421 + 100.0 * 8.55752182006836
Epoch 430, val loss: 0.8001338839530945
Epoch 440, training loss: 855.8966064453125 = 0.7801705598831177 + 100.0 * 8.551164627075195
Epoch 440, val loss: 0.7895429134368896
Epoch 450, training loss: 855.7933959960938 = 0.7690067887306213 + 100.0 * 8.550244331359863
Epoch 450, val loss: 0.7789466977119446
Epoch 460, training loss: 854.948974609375 = 0.757778525352478 + 100.0 * 8.541912078857422
Epoch 460, val loss: 0.7683599591255188
Epoch 470, training loss: 854.4253540039062 = 0.7467984557151794 + 100.0 * 8.536785125732422
Epoch 470, val loss: 0.7579848170280457
Epoch 480, training loss: 853.9802856445312 = 0.7358812093734741 + 100.0 * 8.53244400024414
Epoch 480, val loss: 0.7477086782455444
Epoch 490, training loss: 853.5543823242188 = 0.7250069379806519 + 100.0 * 8.52829360961914
Epoch 490, val loss: 0.737480103969574
Epoch 500, training loss: 853.0775146484375 = 0.7143885493278503 + 100.0 * 8.52363109588623
Epoch 500, val loss: 0.7275266051292419
Epoch 510, training loss: 852.61669921875 = 0.7039992809295654 + 100.0 * 8.519126892089844
Epoch 510, val loss: 0.7177815437316895
Epoch 520, training loss: 852.6390991210938 = 0.693802535533905 + 100.0 * 8.519453048706055
Epoch 520, val loss: 0.7082179188728333
Epoch 530, training loss: 851.8629150390625 = 0.6836884021759033 + 100.0 * 8.511792182922363
Epoch 530, val loss: 0.6988426446914673
Epoch 540, training loss: 851.4742431640625 = 0.6739578247070312 + 100.0 * 8.508003234863281
Epoch 540, val loss: 0.6898075938224792
Epoch 550, training loss: 851.1134033203125 = 0.6644797325134277 + 100.0 * 8.504488945007324
Epoch 550, val loss: 0.6810428500175476
Epoch 560, training loss: 850.8417358398438 = 0.6552674174308777 + 100.0 * 8.501864433288574
Epoch 560, val loss: 0.6725454330444336
Epoch 570, training loss: 850.7544555664062 = 0.6462160348892212 + 100.0 * 8.501082420349121
Epoch 570, val loss: 0.6642006039619446
Epoch 580, training loss: 850.3672485351562 = 0.6375330090522766 + 100.0 * 8.497297286987305
Epoch 580, val loss: 0.6561888456344604
Epoch 590, training loss: 850.0020751953125 = 0.6292012333869934 + 100.0 * 8.493728637695312
Epoch 590, val loss: 0.6485452055931091
Epoch 600, training loss: 849.6834716796875 = 0.6211960315704346 + 100.0 * 8.490622520446777
Epoch 600, val loss: 0.6412225961685181
Epoch 610, training loss: 849.385986328125 = 0.6134889721870422 + 100.0 * 8.487725257873535
Epoch 610, val loss: 0.6341744065284729
Epoch 620, training loss: 849.1205444335938 = 0.6060580015182495 + 100.0 * 8.48514461517334
Epoch 620, val loss: 0.6273936629295349
Epoch 630, training loss: 849.3179931640625 = 0.5988505482673645 + 100.0 * 8.487191200256348
Epoch 630, val loss: 0.6208304762840271
Epoch 640, training loss: 848.7496337890625 = 0.5918670296669006 + 100.0 * 8.48157787322998
Epoch 640, val loss: 0.6144917011260986
Epoch 650, training loss: 848.3887329101562 = 0.5852890610694885 + 100.0 * 8.478034973144531
Epoch 650, val loss: 0.608508288860321
Epoch 660, training loss: 848.1002197265625 = 0.578988790512085 + 100.0 * 8.475212097167969
Epoch 660, val loss: 0.6028484106063843
Epoch 670, training loss: 847.843505859375 = 0.5729595422744751 + 100.0 * 8.472705841064453
Epoch 670, val loss: 0.5974423289299011
Epoch 680, training loss: 847.9618530273438 = 0.5671032667160034 + 100.0 * 8.473947525024414
Epoch 680, val loss: 0.5922091007232666
Epoch 690, training loss: 847.45751953125 = 0.5614792108535767 + 100.0 * 8.468960762023926
Epoch 690, val loss: 0.5871796607971191
Epoch 700, training loss: 847.1597290039062 = 0.5561496019363403 + 100.0 * 8.466035842895508
Epoch 700, val loss: 0.582448422908783
Epoch 710, training loss: 846.9594116210938 = 0.5510170459747314 + 100.0 * 8.464083671569824
Epoch 710, val loss: 0.5779316425323486
Epoch 720, training loss: 847.2705688476562 = 0.5460692048072815 + 100.0 * 8.467245101928711
Epoch 720, val loss: 0.5735750794410706
Epoch 730, training loss: 846.6600952148438 = 0.5412357449531555 + 100.0 * 8.461188316345215
Epoch 730, val loss: 0.5693216323852539
Epoch 740, training loss: 846.3806762695312 = 0.5366834998130798 + 100.0 * 8.458439826965332
Epoch 740, val loss: 0.5653434991836548
Epoch 750, training loss: 846.2057495117188 = 0.5322937369346619 + 100.0 * 8.456734657287598
Epoch 750, val loss: 0.5615586042404175
Epoch 760, training loss: 846.0367431640625 = 0.5280822515487671 + 100.0 * 8.455086708068848
Epoch 760, val loss: 0.5579264163970947
Epoch 770, training loss: 846.35498046875 = 0.5239973068237305 + 100.0 * 8.4583101272583
Epoch 770, val loss: 0.5544714331626892
Epoch 780, training loss: 845.8208618164062 = 0.5200287103652954 + 100.0 * 8.453008651733398
Epoch 780, val loss: 0.5510557293891907
Epoch 790, training loss: 845.6069946289062 = 0.516254723072052 + 100.0 * 8.450907707214355
Epoch 790, val loss: 0.5478955507278442
Epoch 800, training loss: 845.4495849609375 = 0.5126200318336487 + 100.0 * 8.449369430541992
Epoch 800, val loss: 0.5448753237724304
Epoch 810, training loss: 845.6193237304688 = 0.5091000199317932 + 100.0 * 8.451102256774902
Epoch 810, val loss: 0.541972815990448
Epoch 820, training loss: 845.1776123046875 = 0.5056877136230469 + 100.0 * 8.4467191696167
Epoch 820, val loss: 0.5391443967819214
Epoch 830, training loss: 845.1284790039062 = 0.5024307370185852 + 100.0 * 8.446260452270508
Epoch 830, val loss: 0.5364718437194824
Epoch 840, training loss: 844.9227294921875 = 0.49928218126296997 + 100.0 * 8.444234848022461
Epoch 840, val loss: 0.5339890122413635
Epoch 850, training loss: 844.7986450195312 = 0.4962557554244995 + 100.0 * 8.443023681640625
Epoch 850, val loss: 0.531583845615387
Epoch 860, training loss: 845.2777099609375 = 0.4933164119720459 + 100.0 * 8.447844505310059
Epoch 860, val loss: 0.5292924046516418
Epoch 870, training loss: 844.8758544921875 = 0.4903653860092163 + 100.0 * 8.443855285644531
Epoch 870, val loss: 0.5269330739974976
Epoch 880, training loss: 844.6063842773438 = 0.487628698348999 + 100.0 * 8.441187858581543
Epoch 880, val loss: 0.5248246192932129
Epoch 890, training loss: 844.4031982421875 = 0.4849998354911804 + 100.0 * 8.43918228149414
Epoch 890, val loss: 0.5228061676025391
Epoch 900, training loss: 844.2753295898438 = 0.4824535548686981 + 100.0 * 8.437928199768066
Epoch 900, val loss: 0.5208738446235657
Epoch 910, training loss: 844.1875610351562 = 0.4799911081790924 + 100.0 * 8.4370756149292
Epoch 910, val loss: 0.5190286040306091
Epoch 920, training loss: 844.2612915039062 = 0.4775957763195038 + 100.0 * 8.437836647033691
Epoch 920, val loss: 0.5172534584999084
Epoch 930, training loss: 844.2948608398438 = 0.4752189517021179 + 100.0 * 8.438196182250977
Epoch 930, val loss: 0.5154823064804077
Epoch 940, training loss: 844.0359497070312 = 0.47294315695762634 + 100.0 * 8.435629844665527
Epoch 940, val loss: 0.5138136744499207
Epoch 950, training loss: 843.8703002929688 = 0.47076624631881714 + 100.0 * 8.433995246887207
Epoch 950, val loss: 0.5122764110565186
Epoch 960, training loss: 843.7533569335938 = 0.4686537981033325 + 100.0 * 8.432847023010254
Epoch 960, val loss: 0.5107614994049072
Epoch 970, training loss: 843.6632080078125 = 0.46660351753234863 + 100.0 * 8.431965827941895
Epoch 970, val loss: 0.50934237241745
Epoch 980, training loss: 844.3984375 = 0.46457695960998535 + 100.0 * 8.439338684082031
Epoch 980, val loss: 0.5080810189247131
Epoch 990, training loss: 843.5335083007812 = 0.46254414319992065 + 100.0 * 8.430709838867188
Epoch 990, val loss: 0.5065345764160156
Epoch 1000, training loss: 843.4428100585938 = 0.46063321828842163 + 100.0 * 8.429821968078613
Epoch 1000, val loss: 0.5052218437194824
Epoch 1010, training loss: 843.3641967773438 = 0.4587787389755249 + 100.0 * 8.429054260253906
Epoch 1010, val loss: 0.5039269924163818
Epoch 1020, training loss: 843.2734375 = 0.4569788873195648 + 100.0 * 8.4281644821167
Epoch 1020, val loss: 0.502730131149292
Epoch 1030, training loss: 843.7725219726562 = 0.4551984369754791 + 100.0 * 8.433173179626465
Epoch 1030, val loss: 0.501583456993103
Epoch 1040, training loss: 843.3726806640625 = 0.45341983437538147 + 100.0 * 8.429192543029785
Epoch 1040, val loss: 0.5003402233123779
Epoch 1050, training loss: 843.0467529296875 = 0.45170900225639343 + 100.0 * 8.425950050354004
Epoch 1050, val loss: 0.49928146600723267
Epoch 1060, training loss: 842.944580078125 = 0.45004090666770935 + 100.0 * 8.424945831298828
Epoch 1060, val loss: 0.4981710910797119
Epoch 1070, training loss: 842.85888671875 = 0.44841787219047546 + 100.0 * 8.424104690551758
Epoch 1070, val loss: 0.4971485137939453
Epoch 1080, training loss: 842.7738647460938 = 0.4468165934085846 + 100.0 * 8.423270225524902
Epoch 1080, val loss: 0.4961443841457367
Epoch 1090, training loss: 842.73583984375 = 0.44524484872817993 + 100.0 * 8.422905921936035
Epoch 1090, val loss: 0.49514755606651306
Epoch 1100, training loss: 843.0487060546875 = 0.44368088245391846 + 100.0 * 8.426050186157227
Epoch 1100, val loss: 0.4941782057285309
Epoch 1110, training loss: 842.5765380859375 = 0.4421400725841522 + 100.0 * 8.421343803405762
Epoch 1110, val loss: 0.4932212233543396
Epoch 1120, training loss: 842.505126953125 = 0.44064992666244507 + 100.0 * 8.420644760131836
Epoch 1120, val loss: 0.49229833483695984
Epoch 1130, training loss: 842.5659790039062 = 0.43918588757514954 + 100.0 * 8.42126750946045
Epoch 1130, val loss: 0.49142828583717346
Epoch 1140, training loss: 842.3529052734375 = 0.4377158582210541 + 100.0 * 8.419151306152344
Epoch 1140, val loss: 0.49053120613098145
Epoch 1150, training loss: 842.266845703125 = 0.43628841638565063 + 100.0 * 8.418305397033691
Epoch 1150, val loss: 0.4896961748600006
Epoch 1160, training loss: 842.1893310546875 = 0.43490076065063477 + 100.0 * 8.4175443649292
Epoch 1160, val loss: 0.4888644218444824
Epoch 1170, training loss: 842.1206665039062 = 0.4335334599018097 + 100.0 * 8.416871070861816
Epoch 1170, val loss: 0.4880860447883606
Epoch 1180, training loss: 842.4033203125 = 0.43217772245407104 + 100.0 * 8.419711112976074
Epoch 1180, val loss: 0.48740148544311523
Epoch 1190, training loss: 842.2689819335938 = 0.43080541491508484 + 100.0 * 8.418381690979004
Epoch 1190, val loss: 0.4863472878932953
Epoch 1200, training loss: 841.9628295898438 = 0.4294690191745758 + 100.0 * 8.41533374786377
Epoch 1200, val loss: 0.48563164472579956
Epoch 1210, training loss: 841.8217163085938 = 0.4281731843948364 + 100.0 * 8.413935661315918
Epoch 1210, val loss: 0.48489996790885925
Epoch 1220, training loss: 841.7589111328125 = 0.42690184712409973 + 100.0 * 8.413320541381836
Epoch 1220, val loss: 0.48418089747428894
Epoch 1230, training loss: 841.6887817382812 = 0.4256451725959778 + 100.0 * 8.412631034851074
Epoch 1230, val loss: 0.48342984914779663
Epoch 1240, training loss: 842.4297485351562 = 0.42438897490501404 + 100.0 * 8.420053482055664
Epoch 1240, val loss: 0.4827466607093811
Epoch 1250, training loss: 841.8099365234375 = 0.42305585741996765 + 100.0 * 8.41386890411377
Epoch 1250, val loss: 0.48194706439971924
Epoch 1260, training loss: 841.5606689453125 = 0.4218006730079651 + 100.0 * 8.411388397216797
Epoch 1260, val loss: 0.4813558757305145
Epoch 1270, training loss: 841.466552734375 = 0.42059314250946045 + 100.0 * 8.410459518432617
Epoch 1270, val loss: 0.4805282652378082
Epoch 1280, training loss: 841.39111328125 = 0.4194088876247406 + 100.0 * 8.409716606140137
Epoch 1280, val loss: 0.4799393117427826
Epoch 1290, training loss: 841.3262939453125 = 0.41823330521583557 + 100.0 * 8.409080505371094
Epoch 1290, val loss: 0.4792514741420746
Epoch 1300, training loss: 841.2640991210938 = 0.4170699119567871 + 100.0 * 8.408470153808594
Epoch 1300, val loss: 0.4786359965801239
Epoch 1310, training loss: 841.2062377929688 = 0.4159121513366699 + 100.0 * 8.407903671264648
Epoch 1310, val loss: 0.47796308994293213
Epoch 1320, training loss: 841.1524047851562 = 0.41476675868034363 + 100.0 * 8.407376289367676
Epoch 1320, val loss: 0.47736668586730957
Epoch 1330, training loss: 841.3695678710938 = 0.4136362075805664 + 100.0 * 8.40955924987793
Epoch 1330, val loss: 0.47665125131607056
Epoch 1340, training loss: 841.1591796875 = 0.41245999932289124 + 100.0 * 8.407466888427734
Epoch 1340, val loss: 0.4761410653591156
Epoch 1350, training loss: 841.1021118164062 = 0.41134142875671387 + 100.0 * 8.40690803527832
Epoch 1350, val loss: 0.4754703938961029
Epoch 1360, training loss: 840.9426879882812 = 0.41023948788642883 + 100.0 * 8.405324935913086
Epoch 1360, val loss: 0.4748530685901642
Epoch 1370, training loss: 840.9346313476562 = 0.40916022658348083 + 100.0 * 8.405254364013672
Epoch 1370, val loss: 0.47426897287368774
Epoch 1380, training loss: 841.4573974609375 = 0.40807661414146423 + 100.0 * 8.410492897033691
Epoch 1380, val loss: 0.4736432433128357
Epoch 1390, training loss: 840.9539794921875 = 0.4069858491420746 + 100.0 * 8.40546989440918
Epoch 1390, val loss: 0.4730849266052246
Epoch 1400, training loss: 840.7933959960938 = 0.4059228003025055 + 100.0 * 8.403874397277832
Epoch 1400, val loss: 0.47248566150665283
Epoch 1410, training loss: 840.7053833007812 = 0.4048823416233063 + 100.0 * 8.40300464630127
Epoch 1410, val loss: 0.47194215655326843
Epoch 1420, training loss: 840.645751953125 = 0.40384894609451294 + 100.0 * 8.402419090270996
Epoch 1420, val loss: 0.471331387758255
Epoch 1430, training loss: 840.5855102539062 = 0.4028172194957733 + 100.0 * 8.401826858520508
Epoch 1430, val loss: 0.4708092212677002
Epoch 1440, training loss: 840.5527954101562 = 0.4017898738384247 + 100.0 * 8.401510238647461
Epoch 1440, val loss: 0.47028684616088867
Epoch 1450, training loss: 841.1492309570312 = 0.4007565975189209 + 100.0 * 8.407485008239746
Epoch 1450, val loss: 0.4699157476425171
Epoch 1460, training loss: 840.7517700195312 = 0.3997000455856323 + 100.0 * 8.403520584106445
Epoch 1460, val loss: 0.46904826164245605
Epoch 1470, training loss: 840.4640502929688 = 0.39867928624153137 + 100.0 * 8.400653839111328
Epoch 1470, val loss: 0.46851155161857605
Epoch 1480, training loss: 840.409423828125 = 0.3976869583129883 + 100.0 * 8.400116920471191
Epoch 1480, val loss: 0.46804773807525635
Epoch 1490, training loss: 840.3723754882812 = 0.396708220243454 + 100.0 * 8.39975643157959
Epoch 1490, val loss: 0.46755751967430115
Epoch 1500, training loss: 840.5296020507812 = 0.39573079347610474 + 100.0 * 8.401338577270508
Epoch 1500, val loss: 0.4669947028160095
Epoch 1510, training loss: 840.2516479492188 = 0.3947368562221527 + 100.0 * 8.398569107055664
Epoch 1510, val loss: 0.4664555788040161
Epoch 1520, training loss: 840.2778930664062 = 0.3937683701515198 + 100.0 * 8.39884090423584
Epoch 1520, val loss: 0.46598103642463684
Epoch 1530, training loss: 840.1536865234375 = 0.3928137421607971 + 100.0 * 8.397608757019043
Epoch 1530, val loss: 0.4654260575771332
Epoch 1540, training loss: 840.3522338867188 = 0.3918647766113281 + 100.0 * 8.399603843688965
Epoch 1540, val loss: 0.46498483419418335
Epoch 1550, training loss: 840.1629028320312 = 0.39089205861091614 + 100.0 * 8.397720336914062
Epoch 1550, val loss: 0.4643874764442444
Epoch 1560, training loss: 840.1526489257812 = 0.38994064927101135 + 100.0 * 8.397626876831055
Epoch 1560, val loss: 0.463941365480423
Epoch 1570, training loss: 840.0159912109375 = 0.38901033997535706 + 100.0 * 8.396269798278809
Epoch 1570, val loss: 0.46340736746788025
Epoch 1580, training loss: 839.9794921875 = 0.388090580701828 + 100.0 * 8.395914077758789
Epoch 1580, val loss: 0.4629576802253723
Epoch 1590, training loss: 840.612548828125 = 0.3871716558933258 + 100.0 * 8.402254104614258
Epoch 1590, val loss: 0.46246007084846497
Epoch 1600, training loss: 840.1248168945312 = 0.3862180709838867 + 100.0 * 8.397385597229004
Epoch 1600, val loss: 0.4619922637939453
Epoch 1610, training loss: 839.9325561523438 = 0.3853018879890442 + 100.0 * 8.395472526550293
Epoch 1610, val loss: 0.46150556206703186
Epoch 1620, training loss: 839.8346557617188 = 0.38440242409706116 + 100.0 * 8.394502639770508
Epoch 1620, val loss: 0.46108824014663696
Epoch 1630, training loss: 839.9178466796875 = 0.3835142254829407 + 100.0 * 8.395343780517578
Epoch 1630, val loss: 0.4605199992656708
Epoch 1640, training loss: 839.7682495117188 = 0.3826034665107727 + 100.0 * 8.393856048583984
Epoch 1640, val loss: 0.46013379096984863
Epoch 1650, training loss: 839.7056884765625 = 0.38170379400253296 + 100.0 * 8.393239974975586
Epoch 1650, val loss: 0.45975059270858765
Epoch 1660, training loss: 839.6603393554688 = 0.38081982731819153 + 100.0 * 8.39279556274414
Epoch 1660, val loss: 0.45919427275657654
Epoch 1670, training loss: 839.6123657226562 = 0.37994325160980225 + 100.0 * 8.392324447631836
Epoch 1670, val loss: 0.4588194191455841
Epoch 1680, training loss: 839.6099853515625 = 0.3790680468082428 + 100.0 * 8.392309188842773
Epoch 1680, val loss: 0.45841729640960693
Epoch 1690, training loss: 840.1463623046875 = 0.3781863749027252 + 100.0 * 8.397682189941406
Epoch 1690, val loss: 0.45799005031585693
Epoch 1700, training loss: 839.7387084960938 = 0.3772846460342407 + 100.0 * 8.393614768981934
Epoch 1700, val loss: 0.4574107229709625
Epoch 1710, training loss: 839.5228271484375 = 0.37640416622161865 + 100.0 * 8.391464233398438
Epoch 1710, val loss: 0.4570552408695221
Epoch 1720, training loss: 839.4439697265625 = 0.3755480945110321 + 100.0 * 8.390684127807617
Epoch 1720, val loss: 0.4567042291164398
Epoch 1730, training loss: 839.4857177734375 = 0.3746943473815918 + 100.0 * 8.39111042022705
Epoch 1730, val loss: 0.4563160538673401
Epoch 1740, training loss: 839.6829833984375 = 0.3738260269165039 + 100.0 * 8.393091201782227
Epoch 1740, val loss: 0.4558977782726288
Epoch 1750, training loss: 839.4249877929688 = 0.3729609549045563 + 100.0 * 8.390520095825195
Epoch 1750, val loss: 0.45540353655815125
Epoch 1760, training loss: 839.3015747070312 = 0.3721165060997009 + 100.0 * 8.389294624328613
Epoch 1760, val loss: 0.4550521969795227
Epoch 1770, training loss: 839.2549438476562 = 0.3712831437587738 + 100.0 * 8.388836860656738
Epoch 1770, val loss: 0.4546572268009186
Epoch 1780, training loss: 839.443115234375 = 0.37045368552207947 + 100.0 * 8.390726089477539
Epoch 1780, val loss: 0.4544496238231659
Epoch 1790, training loss: 839.23095703125 = 0.3695961833000183 + 100.0 * 8.3886137008667
Epoch 1790, val loss: 0.45381903648376465
Epoch 1800, training loss: 839.1964721679688 = 0.36875319480895996 + 100.0 * 8.388277053833008
Epoch 1800, val loss: 0.4535929262638092
Epoch 1810, training loss: 839.146484375 = 0.36793217062950134 + 100.0 * 8.387785911560059
Epoch 1810, val loss: 0.45312610268592834
Epoch 1820, training loss: 839.1585083007812 = 0.3671199679374695 + 100.0 * 8.387913703918457
Epoch 1820, val loss: 0.45278459787368774
Epoch 1830, training loss: 839.1758422851562 = 0.3663068413734436 + 100.0 * 8.388094902038574
Epoch 1830, val loss: 0.4524421989917755
Epoch 1840, training loss: 839.20751953125 = 0.3654957115650177 + 100.0 * 8.388420104980469
Epoch 1840, val loss: 0.45203697681427
Epoch 1850, training loss: 838.9744873046875 = 0.3646860420703888 + 100.0 * 8.38609790802002
Epoch 1850, val loss: 0.45170509815216064
Epoch 1860, training loss: 838.9588012695312 = 0.3638911843299866 + 100.0 * 8.38594913482666
Epoch 1860, val loss: 0.45140397548675537
Epoch 1870, training loss: 838.9541015625 = 0.3631030023097992 + 100.0 * 8.385910034179688
Epoch 1870, val loss: 0.4511173665523529
Epoch 1880, training loss: 839.1970825195312 = 0.36231279373168945 + 100.0 * 8.388347625732422
Epoch 1880, val loss: 0.45074477791786194
Epoch 1890, training loss: 838.9526977539062 = 0.36149147152900696 + 100.0 * 8.38591194152832
Epoch 1890, val loss: 0.4502197802066803
Epoch 1900, training loss: 838.8248291015625 = 0.36068177223205566 + 100.0 * 8.384641647338867
Epoch 1900, val loss: 0.4500141143798828
Epoch 1910, training loss: 838.7901611328125 = 0.3598940968513489 + 100.0 * 8.384302139282227
Epoch 1910, val loss: 0.4496849775314331
Epoch 1920, training loss: 838.7405395507812 = 0.3591148853302002 + 100.0 * 8.383813858032227
Epoch 1920, val loss: 0.4493400752544403
Epoch 1930, training loss: 838.7322998046875 = 0.3583361804485321 + 100.0 * 8.383739471435547
Epoch 1930, val loss: 0.4490026831626892
Epoch 1940, training loss: 839.1045532226562 = 0.35755622386932373 + 100.0 * 8.387470245361328
Epoch 1940, val loss: 0.44879060983657837
Epoch 1950, training loss: 838.7766723632812 = 0.3567408621311188 + 100.0 * 8.384199142456055
Epoch 1950, val loss: 0.4483092129230499
Epoch 1960, training loss: 838.7249755859375 = 0.35594442486763 + 100.0 * 8.383689880371094
Epoch 1960, val loss: 0.44800621271133423
Epoch 1970, training loss: 838.6421508789062 = 0.35517066717147827 + 100.0 * 8.382869720458984
Epoch 1970, val loss: 0.4476740062236786
Epoch 1980, training loss: 838.5980834960938 = 0.354403555393219 + 100.0 * 8.382436752319336
Epoch 1980, val loss: 0.44735440611839294
Epoch 1990, training loss: 838.8046264648438 = 0.35363978147506714 + 100.0 * 8.384510040283203
Epoch 1990, val loss: 0.4469439387321472
Epoch 2000, training loss: 838.55615234375 = 0.35285162925720215 + 100.0 * 8.382033348083496
Epoch 2000, val loss: 0.44676557183265686
Epoch 2010, training loss: 838.599365234375 = 0.3520788550376892 + 100.0 * 8.38247299194336
Epoch 2010, val loss: 0.4464796483516693
Epoch 2020, training loss: 838.5966186523438 = 0.35130536556243896 + 100.0 * 8.382452964782715
Epoch 2020, val loss: 0.4461669921875
Epoch 2030, training loss: 838.44140625 = 0.35053879022598267 + 100.0 * 8.380908966064453
Epoch 2030, val loss: 0.44584783911705017
Epoch 2040, training loss: 838.434326171875 = 0.3497832417488098 + 100.0 * 8.380845069885254
Epoch 2040, val loss: 0.44548988342285156
Epoch 2050, training loss: 838.45068359375 = 0.34902825951576233 + 100.0 * 8.381016731262207
Epoch 2050, val loss: 0.44524168968200684
Epoch 2060, training loss: 838.5247192382812 = 0.348267525434494 + 100.0 * 8.38176441192627
Epoch 2060, val loss: 0.4449348449707031
Epoch 2070, training loss: 838.309326171875 = 0.3475024104118347 + 100.0 * 8.379618644714355
Epoch 2070, val loss: 0.44465169310569763
Epoch 2080, training loss: 838.3384399414062 = 0.3467525541782379 + 100.0 * 8.37991714477539
Epoch 2080, val loss: 0.4443872570991516
Epoch 2090, training loss: 838.4300537109375 = 0.34599921107292175 + 100.0 * 8.380840301513672
Epoch 2090, val loss: 0.4440940022468567
Epoch 2100, training loss: 838.5922241210938 = 0.3452396094799042 + 100.0 * 8.38247013092041
Epoch 2100, val loss: 0.4437389373779297
Epoch 2110, training loss: 838.24072265625 = 0.34446027874946594 + 100.0 * 8.378962516784668
Epoch 2110, val loss: 0.44363734126091003
Epoch 2120, training loss: 838.2561645507812 = 0.34370455145835876 + 100.0 * 8.379124641418457
Epoch 2120, val loss: 0.44326817989349365
Epoch 2130, training loss: 838.3695068359375 = 0.3429548442363739 + 100.0 * 8.380265235900879
Epoch 2130, val loss: 0.4431285262107849
Epoch 2140, training loss: 838.1424560546875 = 0.34219637513160706 + 100.0 * 8.378002166748047
Epoch 2140, val loss: 0.4426857531070709
Epoch 2150, training loss: 838.1173095703125 = 0.34145015478134155 + 100.0 * 8.377758979797363
Epoch 2150, val loss: 0.4423963725566864
Epoch 2160, training loss: 838.082275390625 = 0.34070518612861633 + 100.0 * 8.377415657043457
Epoch 2160, val loss: 0.44207775592803955
Epoch 2170, training loss: 838.1177978515625 = 0.3399593234062195 + 100.0 * 8.377778053283691
Epoch 2170, val loss: 0.4417649805545807
Epoch 2180, training loss: 838.2523193359375 = 0.3392021954059601 + 100.0 * 8.379131317138672
Epoch 2180, val loss: 0.44154900312423706
Epoch 2190, training loss: 838.2583618164062 = 0.3384341895580292 + 100.0 * 8.379199028015137
Epoch 2190, val loss: 0.4413856565952301
Epoch 2200, training loss: 838.0535888671875 = 0.33766430616378784 + 100.0 * 8.377159118652344
Epoch 2200, val loss: 0.44114068150520325
Epoch 2210, training loss: 838.00439453125 = 0.3369154930114746 + 100.0 * 8.37667465209961
Epoch 2210, val loss: 0.44075411558151245
Epoch 2220, training loss: 837.9487915039062 = 0.33617836236953735 + 100.0 * 8.376126289367676
Epoch 2220, val loss: 0.4406202435493469
Epoch 2230, training loss: 837.9093627929688 = 0.3354435861110687 + 100.0 * 8.375739097595215
Epoch 2230, val loss: 0.44029173254966736
Epoch 2240, training loss: 838.0713500976562 = 0.33470988273620605 + 100.0 * 8.377366065979004
Epoch 2240, val loss: 0.44005003571510315
Epoch 2250, training loss: 837.9580688476562 = 0.33395659923553467 + 100.0 * 8.376240730285645
Epoch 2250, val loss: 0.43986776471138
Epoch 2260, training loss: 838.0081787109375 = 0.3332131505012512 + 100.0 * 8.376749992370605
Epoch 2260, val loss: 0.4396751821041107
Epoch 2270, training loss: 837.94091796875 = 0.33247217535972595 + 100.0 * 8.376084327697754
Epoch 2270, val loss: 0.4394124448299408
Epoch 2280, training loss: 837.7918701171875 = 0.3317374587059021 + 100.0 * 8.374601364135742
Epoch 2280, val loss: 0.4391883909702301
Epoch 2290, training loss: 837.7471923828125 = 0.3310098350048065 + 100.0 * 8.374161720275879
Epoch 2290, val loss: 0.43889567255973816
Epoch 2300, training loss: 837.9575805664062 = 0.33028462529182434 + 100.0 * 8.376273155212402
Epoch 2300, val loss: 0.43852168321609497
Epoch 2310, training loss: 837.7225341796875 = 0.3295210003852844 + 100.0 * 8.373929977416992
Epoch 2310, val loss: 0.4385911226272583
Epoch 2320, training loss: 837.693115234375 = 0.3287757635116577 + 100.0 * 8.373642921447754
Epoch 2320, val loss: 0.43822234869003296
Epoch 2330, training loss: 837.7621459960938 = 0.3280405104160309 + 100.0 * 8.374341011047363
Epoch 2330, val loss: 0.43812131881713867
Epoch 2340, training loss: 838.0281982421875 = 0.3272966146469116 + 100.0 * 8.377009391784668
Epoch 2340, val loss: 0.43785813450813293
Epoch 2350, training loss: 837.7312622070312 = 0.32654356956481934 + 100.0 * 8.37404727935791
Epoch 2350, val loss: 0.43774810433387756
Epoch 2360, training loss: 837.6318359375 = 0.3258078694343567 + 100.0 * 8.37306022644043
Epoch 2360, val loss: 0.4374336302280426
Epoch 2370, training loss: 837.5737915039062 = 0.32507726550102234 + 100.0 * 8.37248706817627
Epoch 2370, val loss: 0.43732747435569763
Epoch 2380, training loss: 837.5411987304688 = 0.3243481516838074 + 100.0 * 8.37216854095459
Epoch 2380, val loss: 0.4371020495891571
Epoch 2390, training loss: 837.5296020507812 = 0.3236165940761566 + 100.0 * 8.37205982208252
Epoch 2390, val loss: 0.4369165599346161
Epoch 2400, training loss: 838.1024780273438 = 0.32288089394569397 + 100.0 * 8.377796173095703
Epoch 2400, val loss: 0.4367443919181824
Epoch 2410, training loss: 837.7617797851562 = 0.32210737466812134 + 100.0 * 8.374397277832031
Epoch 2410, val loss: 0.436582088470459
Epoch 2420, training loss: 837.4976806640625 = 0.321349561214447 + 100.0 * 8.371763229370117
Epoch 2420, val loss: 0.4362596571445465
Epoch 2430, training loss: 837.480224609375 = 0.3206152319908142 + 100.0 * 8.371596336364746
Epoch 2430, val loss: 0.43620097637176514
Epoch 2440, training loss: 837.4234619140625 = 0.3198882043361664 + 100.0 * 8.3710355758667
Epoch 2440, val loss: 0.435974657535553
Epoch 2450, training loss: 837.4261474609375 = 0.3191581964492798 + 100.0 * 8.37106990814209
Epoch 2450, val loss: 0.4358096718788147
Epoch 2460, training loss: 837.9635009765625 = 0.31842318177223206 + 100.0 * 8.376450538635254
Epoch 2460, val loss: 0.43548819422721863
Epoch 2470, training loss: 837.5453491210938 = 0.317654550075531 + 100.0 * 8.37227725982666
Epoch 2470, val loss: 0.4357365667819977
Epoch 2480, training loss: 837.4120483398438 = 0.31690335273742676 + 100.0 * 8.370951652526855
Epoch 2480, val loss: 0.4353238642215729
Epoch 2490, training loss: 837.3785400390625 = 0.3161683678627014 + 100.0 * 8.370623588562012
Epoch 2490, val loss: 0.4353064298629761
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8361237950279046
0.8630007969282041
=== training gcn model ===
Epoch 0, training loss: 1059.34765625 = 1.1223498582839966 + 100.0 * 10.582253456115723
Epoch 0, val loss: 1.1196531057357788
Epoch 10, training loss: 1059.2904052734375 = 1.117383599281311 + 100.0 * 10.581729888916016
Epoch 10, val loss: 1.1146881580352783
Epoch 20, training loss: 1059.065185546875 = 1.1120004653930664 + 100.0 * 10.5795316696167
Epoch 20, val loss: 1.1093080043792725
Epoch 30, training loss: 1058.1317138671875 = 1.1060125827789307 + 100.0 * 10.570257186889648
Epoch 30, val loss: 1.1033375263214111
Epoch 40, training loss: 1054.7362060546875 = 1.0993452072143555 + 100.0 * 10.536368370056152
Epoch 40, val loss: 1.0967093706130981
Epoch 50, training loss: 1045.50244140625 = 1.091856598854065 + 100.0 * 10.444106101989746
Epoch 50, val loss: 1.0893027782440186
Epoch 60, training loss: 1025.711181640625 = 1.0840871334075928 + 100.0 * 10.246270179748535
Epoch 60, val loss: 1.0817242860794067
Epoch 70, training loss: 990.5031127929688 = 1.0755641460418701 + 100.0 * 9.894275665283203
Epoch 70, val loss: 1.0732462406158447
Epoch 80, training loss: 955.266357421875 = 1.0661771297454834 + 100.0 * 9.542001724243164
Epoch 80, val loss: 1.0641072988510132
Epoch 90, training loss: 943.64453125 = 1.0573556423187256 + 100.0 * 9.425871849060059
Epoch 90, val loss: 1.0557043552398682
Epoch 100, training loss: 935.5022583007812 = 1.0499311685562134 + 100.0 * 9.344523429870605
Epoch 100, val loss: 1.048755168914795
Epoch 110, training loss: 929.6243896484375 = 1.0436296463012695 + 100.0 * 9.285807609558105
Epoch 110, val loss: 1.0427651405334473
Epoch 120, training loss: 926.680419921875 = 1.0377393960952759 + 100.0 * 9.256426811218262
Epoch 120, val loss: 1.0370762348175049
Epoch 130, training loss: 925.184326171875 = 1.0322043895721436 + 100.0 * 9.241520881652832
Epoch 130, val loss: 1.0316920280456543
Epoch 140, training loss: 923.7261352539062 = 1.0269675254821777 + 100.0 * 9.226991653442383
Epoch 140, val loss: 1.0265932083129883
Epoch 150, training loss: 921.7803344726562 = 1.02212655544281 + 100.0 * 9.207581520080566
Epoch 150, val loss: 1.0219320058822632
Epoch 160, training loss: 919.1290283203125 = 1.0180538892745972 + 100.0 * 9.181109428405762
Epoch 160, val loss: 1.0180717706680298
Epoch 170, training loss: 914.8942260742188 = 1.015171766281128 + 100.0 * 9.138790130615234
Epoch 170, val loss: 1.015411376953125
Epoch 180, training loss: 907.774169921875 = 1.0141524076461792 + 100.0 * 9.06760025024414
Epoch 180, val loss: 1.014595627784729
Epoch 190, training loss: 899.76806640625 = 1.0142475366592407 + 100.0 * 8.98753833770752
Epoch 190, val loss: 1.0145879983901978
Epoch 200, training loss: 895.3607177734375 = 1.0121982097625732 + 100.0 * 8.943485260009766
Epoch 200, val loss: 1.012247085571289
Epoch 210, training loss: 889.1852416992188 = 1.0089340209960938 + 100.0 * 8.881763458251953
Epoch 210, val loss: 1.009301781654358
Epoch 220, training loss: 882.4913940429688 = 1.0093096494674683 + 100.0 * 8.814820289611816
Epoch 220, val loss: 1.009930968284607
Epoch 230, training loss: 878.2556762695312 = 1.0077520608901978 + 100.0 * 8.772479057312012
Epoch 230, val loss: 1.0081044435501099
Epoch 240, training loss: 875.1538696289062 = 1.003221869468689 + 100.0 * 8.741506576538086
Epoch 240, val loss: 1.003621220588684
Epoch 250, training loss: 872.420654296875 = 0.9986624717712402 + 100.0 * 8.71422004699707
Epoch 250, val loss: 0.9992972016334534
Epoch 260, training loss: 870.0173950195312 = 0.9942668080329895 + 100.0 * 8.690231323242188
Epoch 260, val loss: 0.9951133131980896
Epoch 270, training loss: 867.5630493164062 = 0.9896430969238281 + 100.0 * 8.66573429107666
Epoch 270, val loss: 0.9905475378036499
Epoch 280, training loss: 865.889892578125 = 0.9846426248550415 + 100.0 * 8.649052619934082
Epoch 280, val loss: 0.9856400489807129
Epoch 290, training loss: 864.6672973632812 = 0.9790211319923401 + 100.0 * 8.636882781982422
Epoch 290, val loss: 0.9801607131958008
Epoch 300, training loss: 863.7694091796875 = 0.9728268980979919 + 100.0 * 8.627965927124023
Epoch 300, val loss: 0.9741332530975342
Epoch 310, training loss: 862.8609619140625 = 0.9662542939186096 + 100.0 * 8.61894702911377
Epoch 310, val loss: 0.9677557945251465
Epoch 320, training loss: 861.8267822265625 = 0.959568202495575 + 100.0 * 8.608672142028809
Epoch 320, val loss: 0.9612996578216553
Epoch 330, training loss: 860.9125366210938 = 0.9526387453079224 + 100.0 * 8.59959888458252
Epoch 330, val loss: 0.9545817375183105
Epoch 340, training loss: 860.0253295898438 = 0.9453386664390564 + 100.0 * 8.590800285339355
Epoch 340, val loss: 0.9475203156471252
Epoch 350, training loss: 859.304931640625 = 0.9376704096794128 + 100.0 * 8.583672523498535
Epoch 350, val loss: 0.9400968551635742
Epoch 360, training loss: 858.4845581054688 = 0.9297704696655273 + 100.0 * 8.57554817199707
Epoch 360, val loss: 0.9324917793273926
Epoch 370, training loss: 857.7726440429688 = 0.9216529726982117 + 100.0 * 8.568510055541992
Epoch 370, val loss: 0.9246782064437866
Epoch 380, training loss: 857.1017456054688 = 0.9132938385009766 + 100.0 * 8.561884880065918
Epoch 380, val loss: 0.9166419506072998
Epoch 390, training loss: 856.544189453125 = 0.9047093987464905 + 100.0 * 8.556394577026367
Epoch 390, val loss: 0.9083616137504578
Epoch 400, training loss: 856.0305786132812 = 0.8958855867385864 + 100.0 * 8.551346778869629
Epoch 400, val loss: 0.8999130725860596
Epoch 410, training loss: 855.533203125 = 0.887010395526886 + 100.0 * 8.546462059020996
Epoch 410, val loss: 0.8913681507110596
Epoch 420, training loss: 855.2064819335938 = 0.8779441118240356 + 100.0 * 8.543285369873047
Epoch 420, val loss: 0.8825923800468445
Epoch 430, training loss: 854.5966186523438 = 0.8689118027687073 + 100.0 * 8.537277221679688
Epoch 430, val loss: 0.8738982677459717
Epoch 440, training loss: 854.0673828125 = 0.8598597645759583 + 100.0 * 8.532074928283691
Epoch 440, val loss: 0.8651516437530518
Epoch 450, training loss: 853.7987060546875 = 0.8505207300186157 + 100.0 * 8.529481887817383
Epoch 450, val loss: 0.8561333417892456
Epoch 460, training loss: 853.1898193359375 = 0.8411110043525696 + 100.0 * 8.523487091064453
Epoch 460, val loss: 0.8470425009727478
Epoch 470, training loss: 852.7852783203125 = 0.8316103219985962 + 100.0 * 8.519536972045898
Epoch 470, val loss: 0.8378055095672607
Epoch 480, training loss: 852.3840942382812 = 0.8219789266586304 + 100.0 * 8.515621185302734
Epoch 480, val loss: 0.828481912612915
Epoch 490, training loss: 852.04296875 = 0.8122732639312744 + 100.0 * 8.512307167053223
Epoch 490, val loss: 0.819044828414917
Epoch 500, training loss: 851.7891845703125 = 0.802455484867096 + 100.0 * 8.509867668151855
Epoch 500, val loss: 0.80952388048172
Epoch 510, training loss: 851.5011596679688 = 0.792415201663971 + 100.0 * 8.507087707519531
Epoch 510, val loss: 0.7997902035713196
Epoch 520, training loss: 851.1293334960938 = 0.782648503780365 + 100.0 * 8.503466606140137
Epoch 520, val loss: 0.7902817726135254
Epoch 530, training loss: 850.8749389648438 = 0.7728884816169739 + 100.0 * 8.501020431518555
Epoch 530, val loss: 0.7807489037513733
Epoch 540, training loss: 850.57177734375 = 0.763062059879303 + 100.0 * 8.498086929321289
Epoch 540, val loss: 0.7712060213088989
Epoch 550, training loss: 850.302734375 = 0.7534040212631226 + 100.0 * 8.495492935180664
Epoch 550, val loss: 0.7617741227149963
Epoch 560, training loss: 850.0416870117188 = 0.7438036203384399 + 100.0 * 8.492979049682617
Epoch 560, val loss: 0.7523687481880188
Epoch 570, training loss: 849.8385620117188 = 0.7341552972793579 + 100.0 * 8.491044044494629
Epoch 570, val loss: 0.7428563237190247
Epoch 580, training loss: 849.529052734375 = 0.7246571183204651 + 100.0 * 8.488043785095215
Epoch 580, val loss: 0.7335492372512817
Epoch 590, training loss: 849.2219848632812 = 0.7152501344680786 + 100.0 * 8.485067367553711
Epoch 590, val loss: 0.7242910265922546
Epoch 600, training loss: 848.9525146484375 = 0.7058815956115723 + 100.0 * 8.482466697692871
Epoch 600, val loss: 0.7150679230690002
Epoch 610, training loss: 848.7261352539062 = 0.6965474486351013 + 100.0 * 8.48029613494873
Epoch 610, val loss: 0.7058535814285278
Epoch 620, training loss: 848.4219970703125 = 0.6874237656593323 + 100.0 * 8.47734546661377
Epoch 620, val loss: 0.6968703866004944
Epoch 630, training loss: 848.1588745117188 = 0.678417980670929 + 100.0 * 8.474804878234863
Epoch 630, val loss: 0.6879338622093201
Epoch 640, training loss: 847.9122314453125 = 0.669492781162262 + 100.0 * 8.472427368164062
Epoch 640, val loss: 0.6790838241577148
Epoch 650, training loss: 847.9137573242188 = 0.6606428623199463 + 100.0 * 8.47253131866455
Epoch 650, val loss: 0.670269250869751
Epoch 660, training loss: 847.4706420898438 = 0.6519324779510498 + 100.0 * 8.46818733215332
Epoch 660, val loss: 0.661639392375946
Epoch 670, training loss: 847.2511596679688 = 0.6435092687606812 + 100.0 * 8.466076850891113
Epoch 670, val loss: 0.6532741785049438
Epoch 680, training loss: 847.1055908203125 = 0.6352428793907166 + 100.0 * 8.464703559875488
Epoch 680, val loss: 0.6450603008270264
Epoch 690, training loss: 846.8622436523438 = 0.6270669102668762 + 100.0 * 8.46235179901123
Epoch 690, val loss: 0.6369296908378601
Epoch 700, training loss: 846.69677734375 = 0.6193239092826843 + 100.0 * 8.460774421691895
Epoch 700, val loss: 0.6292197108268738
Epoch 710, training loss: 846.4615478515625 = 0.6117316484451294 + 100.0 * 8.458498001098633
Epoch 710, val loss: 0.6216779947280884
Epoch 720, training loss: 846.490234375 = 0.6043667197227478 + 100.0 * 8.458858489990234
Epoch 720, val loss: 0.6143224239349365
Epoch 730, training loss: 846.2042846679688 = 0.5971366167068481 + 100.0 * 8.456071853637695
Epoch 730, val loss: 0.6071792244911194
Epoch 740, training loss: 845.9437866210938 = 0.5902736783027649 + 100.0 * 8.453535079956055
Epoch 740, val loss: 0.6003630757331848
Epoch 750, training loss: 845.789306640625 = 0.5836137533187866 + 100.0 * 8.452056884765625
Epoch 750, val loss: 0.5936875343322754
Epoch 760, training loss: 845.6285400390625 = 0.5771700143814087 + 100.0 * 8.45051383972168
Epoch 760, val loss: 0.5873458385467529
Epoch 770, training loss: 846.0628051757812 = 0.5709485411643982 + 100.0 * 8.45491886138916
Epoch 770, val loss: 0.5810809135437012
Epoch 780, training loss: 845.4093627929688 = 0.5648126006126404 + 100.0 * 8.448445320129395
Epoch 780, val loss: 0.5750589966773987
Epoch 790, training loss: 845.2469482421875 = 0.559106171131134 + 100.0 * 8.446878433227539
Epoch 790, val loss: 0.5694474577903748
Epoch 800, training loss: 845.0986938476562 = 0.553517758846283 + 100.0 * 8.445451736450195
Epoch 800, val loss: 0.5639715194702148
Epoch 810, training loss: 844.9647827148438 = 0.5481796860694885 + 100.0 * 8.44416618347168
Epoch 810, val loss: 0.5587756037712097
Epoch 820, training loss: 844.8453979492188 = 0.5430233478546143 + 100.0 * 8.443023681640625
Epoch 820, val loss: 0.5537750720977783
Epoch 830, training loss: 845.1857299804688 = 0.5380191206932068 + 100.0 * 8.446476936340332
Epoch 830, val loss: 0.5489497184753418
Epoch 840, training loss: 844.6632690429688 = 0.5331479907035828 + 100.0 * 8.441301345825195
Epoch 840, val loss: 0.5442532896995544
Epoch 850, training loss: 844.5664672851562 = 0.5286258459091187 + 100.0 * 8.440378189086914
Epoch 850, val loss: 0.5398751497268677
Epoch 860, training loss: 844.4207153320312 = 0.524228572845459 + 100.0 * 8.43896484375
Epoch 860, val loss: 0.5356176495552063
Epoch 870, training loss: 844.3324584960938 = 0.5200197100639343 + 100.0 * 8.438124656677246
Epoch 870, val loss: 0.5316042900085449
Epoch 880, training loss: 844.264404296875 = 0.5159160494804382 + 100.0 * 8.437484741210938
Epoch 880, val loss: 0.5276928544044495
Epoch 890, training loss: 844.1298217773438 = 0.5120251178741455 + 100.0 * 8.436178207397461
Epoch 890, val loss: 0.5240485072135925
Epoch 900, training loss: 844.001220703125 = 0.5082820653915405 + 100.0 * 8.434928894042969
Epoch 900, val loss: 0.520555317401886
Epoch 910, training loss: 843.9074096679688 = 0.5046690702438354 + 100.0 * 8.434027671813965
Epoch 910, val loss: 0.517183780670166
Epoch 920, training loss: 843.8745727539062 = 0.5011515617370605 + 100.0 * 8.433733940124512
Epoch 920, val loss: 0.5139289498329163
Epoch 930, training loss: 843.6827392578125 = 0.4978475868701935 + 100.0 * 8.431848526000977
Epoch 930, val loss: 0.5109334588050842
Epoch 940, training loss: 843.5731811523438 = 0.4946924149990082 + 100.0 * 8.430785179138184
Epoch 940, val loss: 0.5079976916313171
Epoch 950, training loss: 843.4512329101562 = 0.4916294515132904 + 100.0 * 8.429595947265625
Epoch 950, val loss: 0.5052569508552551
Epoch 960, training loss: 843.4439697265625 = 0.48865729570388794 + 100.0 * 8.429553031921387
Epoch 960, val loss: 0.5025724172592163
Epoch 970, training loss: 843.2574462890625 = 0.4858259856700897 + 100.0 * 8.427716255187988
Epoch 970, val loss: 0.5000876188278198
Epoch 980, training loss: 843.1106567382812 = 0.4831525981426239 + 100.0 * 8.426275253295898
Epoch 980, val loss: 0.49771350622177124
Epoch 990, training loss: 843.3495483398438 = 0.48053494095802307 + 100.0 * 8.428689956665039
Epoch 990, val loss: 0.4953731894493103
Epoch 1000, training loss: 843.0462646484375 = 0.4779486060142517 + 100.0 * 8.42568302154541
Epoch 1000, val loss: 0.4931492805480957
Epoch 1010, training loss: 842.7846069335938 = 0.47557005286216736 + 100.0 * 8.423089981079102
Epoch 1010, val loss: 0.4910935163497925
Epoch 1020, training loss: 842.673583984375 = 0.4732091426849365 + 100.0 * 8.422003746032715
Epoch 1020, val loss: 0.4890553057193756
Epoch 1030, training loss: 842.5809936523438 = 0.47094714641571045 + 100.0 * 8.421100616455078
Epoch 1030, val loss: 0.48719969391822815
Epoch 1040, training loss: 842.8206787109375 = 0.46873021125793457 + 100.0 * 8.423519134521484
Epoch 1040, val loss: 0.4852522611618042
Epoch 1050, training loss: 842.5113525390625 = 0.46657392382621765 + 100.0 * 8.420448303222656
Epoch 1050, val loss: 0.4835287034511566
Epoch 1060, training loss: 842.2900390625 = 0.4645494520664215 + 100.0 * 8.418254852294922
Epoch 1060, val loss: 0.48182588815689087
Epoch 1070, training loss: 842.1607666015625 = 0.46256229281425476 + 100.0 * 8.41698169708252
Epoch 1070, val loss: 0.4802284836769104
Epoch 1080, training loss: 842.0927124023438 = 0.4606485068798065 + 100.0 * 8.41632080078125
Epoch 1080, val loss: 0.4787316918373108
Epoch 1090, training loss: 842.2031860351562 = 0.45873379707336426 + 100.0 * 8.417444229125977
Epoch 1090, val loss: 0.47717684507369995
Epoch 1100, training loss: 841.9815063476562 = 0.45692530274391174 + 100.0 * 8.41524600982666
Epoch 1100, val loss: 0.47580528259277344
Epoch 1110, training loss: 841.7963256835938 = 0.45520880818367004 + 100.0 * 8.413411140441895
Epoch 1110, val loss: 0.47439825534820557
Epoch 1120, training loss: 841.7116088867188 = 0.45351409912109375 + 100.0 * 8.412581443786621
Epoch 1120, val loss: 0.47306737303733826
Epoch 1130, training loss: 841.99951171875 = 0.45187467336654663 + 100.0 * 8.415475845336914
Epoch 1130, val loss: 0.47181540727615356
Epoch 1140, training loss: 841.6786499023438 = 0.4502265155315399 + 100.0 * 8.412283897399902
Epoch 1140, val loss: 0.4705601632595062
Epoch 1150, training loss: 841.4443359375 = 0.44869643449783325 + 100.0 * 8.409956932067871
Epoch 1150, val loss: 0.4693942964076996
Epoch 1160, training loss: 841.3380126953125 = 0.4471825957298279 + 100.0 * 8.408907890319824
Epoch 1160, val loss: 0.46826308965682983
Epoch 1170, training loss: 841.4400024414062 = 0.4457316994667053 + 100.0 * 8.409942626953125
Epoch 1170, val loss: 0.46719714999198914
Epoch 1180, training loss: 841.2590942382812 = 0.4442330002784729 + 100.0 * 8.408148765563965
Epoch 1180, val loss: 0.4660770297050476
Epoch 1190, training loss: 841.1337890625 = 0.4428826868534088 + 100.0 * 8.406908988952637
Epoch 1190, val loss: 0.4650978446006775
Epoch 1200, training loss: 841.0076904296875 = 0.4415203034877777 + 100.0 * 8.405661582946777
Epoch 1200, val loss: 0.46406853199005127
Epoch 1210, training loss: 840.9214477539062 = 0.44020313024520874 + 100.0 * 8.404812812805176
Epoch 1210, val loss: 0.4631507396697998
Epoch 1220, training loss: 840.8475952148438 = 0.43891993165016174 + 100.0 * 8.40408706665039
Epoch 1220, val loss: 0.4622150659561157
Epoch 1230, training loss: 841.3757934570312 = 0.4376039206981659 + 100.0 * 8.409381866455078
Epoch 1230, val loss: 0.461299866437912
Epoch 1240, training loss: 840.7133178710938 = 0.43635472655296326 + 100.0 * 8.402770042419434
Epoch 1240, val loss: 0.4603678584098816
Epoch 1250, training loss: 840.6749267578125 = 0.435151070356369 + 100.0 * 8.402397155761719
Epoch 1250, val loss: 0.45946958661079407
Epoch 1260, training loss: 840.5633544921875 = 0.43394073843955994 + 100.0 * 8.401293754577637
Epoch 1260, val loss: 0.45863112807273865
Epoch 1270, training loss: 840.8642578125 = 0.4327376186847687 + 100.0 * 8.404314994812012
Epoch 1270, val loss: 0.4577605426311493
Epoch 1280, training loss: 840.4535522460938 = 0.43155714869499207 + 100.0 * 8.400219917297363
Epoch 1280, val loss: 0.45694583654403687
Epoch 1290, training loss: 840.3685302734375 = 0.43045535683631897 + 100.0 * 8.399380683898926
Epoch 1290, val loss: 0.45611387491226196
Epoch 1300, training loss: 840.2503051757812 = 0.42934462428092957 + 100.0 * 8.398209571838379
Epoch 1300, val loss: 0.4553942084312439
Epoch 1310, training loss: 840.186767578125 = 0.4282626807689667 + 100.0 * 8.397584915161133
Epoch 1310, val loss: 0.4546554684638977
Epoch 1320, training loss: 840.128173828125 = 0.4272003173828125 + 100.0 * 8.39700984954834
Epoch 1320, val loss: 0.45394888520240784
Epoch 1330, training loss: 840.3880615234375 = 0.42613905668258667 + 100.0 * 8.399619102478027
Epoch 1330, val loss: 0.45322757959365845
Epoch 1340, training loss: 840.099609375 = 0.4250716269016266 + 100.0 * 8.396745681762695
Epoch 1340, val loss: 0.45249974727630615
Epoch 1350, training loss: 839.9762573242188 = 0.42406997084617615 + 100.0 * 8.395522117614746
Epoch 1350, val loss: 0.4518376588821411
Epoch 1360, training loss: 839.8781127929688 = 0.42306771874427795 + 100.0 * 8.394550323486328
Epoch 1360, val loss: 0.4511430859565735
Epoch 1370, training loss: 839.8067016601562 = 0.42207589745521545 + 100.0 * 8.39384651184082
Epoch 1370, val loss: 0.4505201280117035
Epoch 1380, training loss: 839.7394409179688 = 0.42110636830329895 + 100.0 * 8.393183708190918
Epoch 1380, val loss: 0.4498623013496399
Epoch 1390, training loss: 840.1619873046875 = 0.4201315939426422 + 100.0 * 8.397418975830078
Epoch 1390, val loss: 0.44919824600219727
Epoch 1400, training loss: 839.8518676757812 = 0.4191488027572632 + 100.0 * 8.394327163696289
Epoch 1400, val loss: 0.44854894280433655
Epoch 1410, training loss: 839.6507568359375 = 0.41820240020751953 + 100.0 * 8.392325401306152
Epoch 1410, val loss: 0.448003888130188
Epoch 1420, training loss: 839.5391845703125 = 0.41728833317756653 + 100.0 * 8.391219139099121
Epoch 1420, val loss: 0.44741523265838623
Epoch 1430, training loss: 839.4515991210938 = 0.4163837432861328 + 100.0 * 8.390352249145508
Epoch 1430, val loss: 0.4468395411968231
Epoch 1440, training loss: 839.4231567382812 = 0.4154887795448303 + 100.0 * 8.390076637268066
Epoch 1440, val loss: 0.44624948501586914
Epoch 1450, training loss: 839.8627319335938 = 0.41457703709602356 + 100.0 * 8.394481658935547
Epoch 1450, val loss: 0.4456794559955597
Epoch 1460, training loss: 839.462158203125 = 0.4136826694011688 + 100.0 * 8.390484809875488
Epoch 1460, val loss: 0.4451693296432495
Epoch 1470, training loss: 839.2796630859375 = 0.41282179951667786 + 100.0 * 8.388668060302734
Epoch 1470, val loss: 0.44455790519714355
Epoch 1480, training loss: 839.2052001953125 = 0.41196098923683167 + 100.0 * 8.387931823730469
Epoch 1480, val loss: 0.44405457377433777
Epoch 1490, training loss: 839.17578125 = 0.4111233949661255 + 100.0 * 8.387646675109863
Epoch 1490, val loss: 0.44352975487709045
Epoch 1500, training loss: 839.370849609375 = 0.41025975346565247 + 100.0 * 8.389605522155762
Epoch 1500, val loss: 0.44299888610839844
Epoch 1510, training loss: 839.14501953125 = 0.4094541668891907 + 100.0 * 8.38735580444336
Epoch 1510, val loss: 0.44247710704803467
Epoch 1520, training loss: 839.2365112304688 = 0.4086182415485382 + 100.0 * 8.38827896118164
Epoch 1520, val loss: 0.44196248054504395
Epoch 1530, training loss: 838.9749145507812 = 0.4077809751033783 + 100.0 * 8.385671615600586
Epoch 1530, val loss: 0.4414593577384949
Epoch 1540, training loss: 838.912841796875 = 0.40698474645614624 + 100.0 * 8.385058403015137
Epoch 1540, val loss: 0.44094908237457275
Epoch 1550, training loss: 838.862060546875 = 0.4061768054962158 + 100.0 * 8.38455867767334
Epoch 1550, val loss: 0.4405292272567749
Epoch 1560, training loss: 838.81298828125 = 0.4053826630115509 + 100.0 * 8.384076118469238
Epoch 1560, val loss: 0.44002246856689453
Epoch 1570, training loss: 838.7721557617188 = 0.4045889377593994 + 100.0 * 8.383675575256348
Epoch 1570, val loss: 0.4395809769630432
Epoch 1580, training loss: 838.93701171875 = 0.403796523809433 + 100.0 * 8.385332107543945
Epoch 1580, val loss: 0.4391196072101593
Epoch 1590, training loss: 838.7401733398438 = 0.402975857257843 + 100.0 * 8.38337230682373
Epoch 1590, val loss: 0.43868181109428406
Epoch 1600, training loss: 838.69970703125 = 0.4021908640861511 + 100.0 * 8.382975578308105
Epoch 1600, val loss: 0.43821850419044495
Epoch 1610, training loss: 838.6254272460938 = 0.40143024921417236 + 100.0 * 8.382240295410156
Epoch 1610, val loss: 0.4377598762512207
Epoch 1620, training loss: 838.579345703125 = 0.4006670415401459 + 100.0 * 8.381786346435547
Epoch 1620, val loss: 0.4373414218425751
Epoch 1630, training loss: 838.745361328125 = 0.39990440011024475 + 100.0 * 8.383454322814941
Epoch 1630, val loss: 0.43687358498573303
Epoch 1640, training loss: 838.518798828125 = 0.39912304282188416 + 100.0 * 8.381196975708008
Epoch 1640, val loss: 0.436553031206131
Epoch 1650, training loss: 838.462646484375 = 0.3983827829360962 + 100.0 * 8.380642890930176
Epoch 1650, val loss: 0.4360736906528473
Epoch 1660, training loss: 838.4188232421875 = 0.39763665199279785 + 100.0 * 8.38021183013916
Epoch 1660, val loss: 0.43571737408638
Epoch 1670, training loss: 838.42626953125 = 0.3968961834907532 + 100.0 * 8.380293846130371
Epoch 1670, val loss: 0.4353106915950775
Epoch 1680, training loss: 838.3693237304688 = 0.3961586058139801 + 100.0 * 8.379731178283691
Epoch 1680, val loss: 0.4349002242088318
Epoch 1690, training loss: 838.2835693359375 = 0.3954508602619171 + 100.0 * 8.378881454467773
Epoch 1690, val loss: 0.43449458479881287
Epoch 1700, training loss: 838.2708129882812 = 0.39473870396614075 + 100.0 * 8.378761291503906
Epoch 1700, val loss: 0.4341013729572296
Epoch 1710, training loss: 838.2711181640625 = 0.3940221071243286 + 100.0 * 8.37877082824707
Epoch 1710, val loss: 0.4337109625339508
Epoch 1720, training loss: 838.449462890625 = 0.3932897746562958 + 100.0 * 8.380561828613281
Epoch 1720, val loss: 0.4332836866378784
Epoch 1730, training loss: 838.2538452148438 = 0.3925630450248718 + 100.0 * 8.378612518310547
Epoch 1730, val loss: 0.43297168612480164
Epoch 1740, training loss: 838.1107177734375 = 0.3918726444244385 + 100.0 * 8.377188682556152
Epoch 1740, val loss: 0.4325437545776367
Epoch 1750, training loss: 838.046142578125 = 0.3911735415458679 + 100.0 * 8.37654972076416
Epoch 1750, val loss: 0.4321970045566559
Epoch 1760, training loss: 838.0206909179688 = 0.3904881477355957 + 100.0 * 8.376301765441895
Epoch 1760, val loss: 0.4318516254425049
Epoch 1770, training loss: 838.4093017578125 = 0.38979077339172363 + 100.0 * 8.380195617675781
Epoch 1770, val loss: 0.4314548075199127
Epoch 1780, training loss: 838.0933837890625 = 0.3890780210494995 + 100.0 * 8.377042770385742
Epoch 1780, val loss: 0.4311506450176239
Epoch 1790, training loss: 837.9534912109375 = 0.38839998841285706 + 100.0 * 8.375650405883789
Epoch 1790, val loss: 0.4307498335838318
Epoch 1800, training loss: 837.913330078125 = 0.38771525025367737 + 100.0 * 8.375256538391113
Epoch 1800, val loss: 0.430440753698349
Epoch 1810, training loss: 838.1159057617188 = 0.3870275318622589 + 100.0 * 8.377288818359375
Epoch 1810, val loss: 0.43005096912384033
Epoch 1820, training loss: 837.8673706054688 = 0.38632428646087646 + 100.0 * 8.374810218811035
Epoch 1820, val loss: 0.42979875206947327
Epoch 1830, training loss: 837.7760009765625 = 0.38566672801971436 + 100.0 * 8.373903274536133
Epoch 1830, val loss: 0.4293862581253052
Epoch 1840, training loss: 837.7252807617188 = 0.38498568534851074 + 100.0 * 8.37340259552002
Epoch 1840, val loss: 0.42913803458213806
Epoch 1850, training loss: 837.7098388671875 = 0.3843170702457428 + 100.0 * 8.373254776000977
Epoch 1850, val loss: 0.4287431240081787
Epoch 1860, training loss: 838.0518798828125 = 0.3836488127708435 + 100.0 * 8.37668228149414
Epoch 1860, val loss: 0.428429514169693
Epoch 1870, training loss: 837.7413940429688 = 0.3829273581504822 + 100.0 * 8.373584747314453
Epoch 1870, val loss: 0.42812424898147583
Epoch 1880, training loss: 837.6093139648438 = 0.3822688162326813 + 100.0 * 8.372270584106445
Epoch 1880, val loss: 0.4277496337890625
Epoch 1890, training loss: 837.5523071289062 = 0.3815951347351074 + 100.0 * 8.37170696258545
Epoch 1890, val loss: 0.42746999859809875
Epoch 1900, training loss: 837.5179443359375 = 0.38093554973602295 + 100.0 * 8.371370315551758
Epoch 1900, val loss: 0.42711469531059265
Epoch 1910, training loss: 837.509765625 = 0.3802645206451416 + 100.0 * 8.371294975280762
Epoch 1910, val loss: 0.4268267750740051
Epoch 1920, training loss: 838.2861328125 = 0.37956690788269043 + 100.0 * 8.37906551361084
Epoch 1920, val loss: 0.42647644877433777
Epoch 1930, training loss: 837.4774169921875 = 0.37888267636299133 + 100.0 * 8.37098503112793
Epoch 1930, val loss: 0.4261339604854584
Epoch 1940, training loss: 837.4603271484375 = 0.3782294690608978 + 100.0 * 8.370820999145508
Epoch 1940, val loss: 0.42577117681503296
Epoch 1950, training loss: 837.3667602539062 = 0.37756428122520447 + 100.0 * 8.369892120361328
Epoch 1950, val loss: 0.42555001378059387
Epoch 1960, training loss: 837.3329467773438 = 0.37691357731819153 + 100.0 * 8.369560241699219
Epoch 1960, val loss: 0.4251413643360138
Epoch 1970, training loss: 837.3056030273438 = 0.3762646019458771 + 100.0 * 8.369293212890625
Epoch 1970, val loss: 0.4249204993247986
Epoch 1980, training loss: 837.690673828125 = 0.37560945749282837 + 100.0 * 8.373150825500488
Epoch 1980, val loss: 0.4246542751789093
Epoch 1990, training loss: 837.440185546875 = 0.3749394714832306 + 100.0 * 8.370652198791504
Epoch 1990, val loss: 0.42425158619880676
Epoch 2000, training loss: 837.235595703125 = 0.37428751587867737 + 100.0 * 8.368613243103027
Epoch 2000, val loss: 0.4238671362400055
Epoch 2010, training loss: 837.197998046875 = 0.3736390471458435 + 100.0 * 8.368243217468262
Epoch 2010, val loss: 0.4237079620361328
Epoch 2020, training loss: 837.157470703125 = 0.3730083107948303 + 100.0 * 8.367844581604004
Epoch 2020, val loss: 0.4233531653881073
Epoch 2030, training loss: 837.1268920898438 = 0.37237095832824707 + 100.0 * 8.367545127868652
Epoch 2030, val loss: 0.4230852425098419
Epoch 2040, training loss: 837.1998291015625 = 0.371736615896225 + 100.0 * 8.368280410766602
Epoch 2040, val loss: 0.4228029251098633
Epoch 2050, training loss: 837.1444091796875 = 0.3710665702819824 + 100.0 * 8.367733001708984
Epoch 2050, val loss: 0.4225081503391266
Epoch 2060, training loss: 837.07373046875 = 0.3704259395599365 + 100.0 * 8.367033004760742
Epoch 2060, val loss: 0.42215967178344727
Epoch 2070, training loss: 837.0374755859375 = 0.36977484822273254 + 100.0 * 8.366677284240723
Epoch 2070, val loss: 0.4218916893005371
Epoch 2080, training loss: 836.99560546875 = 0.36913713812828064 + 100.0 * 8.366264343261719
Epoch 2080, val loss: 0.4216311275959015
Epoch 2090, training loss: 836.9658813476562 = 0.3684990406036377 + 100.0 * 8.365974426269531
Epoch 2090, val loss: 0.4213312268257141
Epoch 2100, training loss: 837.13134765625 = 0.36785438656806946 + 100.0 * 8.367634773254395
Epoch 2100, val loss: 0.4211055636405945
Epoch 2110, training loss: 836.9319458007812 = 0.3672110140323639 + 100.0 * 8.365647315979004
Epoch 2110, val loss: 0.42073240876197815
Epoch 2120, training loss: 836.9295654296875 = 0.366564005613327 + 100.0 * 8.365630149841309
Epoch 2120, val loss: 0.42046165466308594
Epoch 2130, training loss: 836.9468383789062 = 0.3659321367740631 + 100.0 * 8.365809440612793
Epoch 2130, val loss: 0.42017629742622375
Epoch 2140, training loss: 836.9772338867188 = 0.36528077721595764 + 100.0 * 8.366119384765625
Epoch 2140, val loss: 0.41991928219795227
Epoch 2150, training loss: 836.8201904296875 = 0.36462849378585815 + 100.0 * 8.364555358886719
Epoch 2150, val loss: 0.4196734130382538
Epoch 2160, training loss: 836.7874145507812 = 0.36398762464523315 + 100.0 * 8.36423397064209
Epoch 2160, val loss: 0.4194100797176361
Epoch 2170, training loss: 836.7521362304688 = 0.3633522093296051 + 100.0 * 8.363887786865234
Epoch 2170, val loss: 0.4191585183143616
Epoch 2180, training loss: 836.9521484375 = 0.3627086579799652 + 100.0 * 8.365894317626953
Epoch 2180, val loss: 0.4189351797103882
Epoch 2190, training loss: 836.7461547851562 = 0.36204424500465393 + 100.0 * 8.36384105682373
Epoch 2190, val loss: 0.41865620017051697
Epoch 2200, training loss: 836.715087890625 = 0.3613962233066559 + 100.0 * 8.363536834716797
Epoch 2200, val loss: 0.418348491191864
Epoch 2210, training loss: 836.6685180664062 = 0.360761821269989 + 100.0 * 8.363077163696289
Epoch 2210, val loss: 0.41810324788093567
Epoch 2220, training loss: 836.6326293945312 = 0.36013174057006836 + 100.0 * 8.362725257873535
Epoch 2220, val loss: 0.41783949732780457
Epoch 2230, training loss: 836.6519775390625 = 0.3595018982887268 + 100.0 * 8.362924575805664
Epoch 2230, val loss: 0.4175831377506256
Epoch 2240, training loss: 837.045654296875 = 0.3588545322418213 + 100.0 * 8.366868019104004
Epoch 2240, val loss: 0.41730841994285583
Epoch 2250, training loss: 836.63427734375 = 0.3581758737564087 + 100.0 * 8.362761497497559
Epoch 2250, val loss: 0.41706717014312744
Epoch 2260, training loss: 836.541748046875 = 0.3575343191623688 + 100.0 * 8.361842155456543
Epoch 2260, val loss: 0.41682037711143494
Epoch 2270, training loss: 836.5109252929688 = 0.35688862204551697 + 100.0 * 8.361540794372559
Epoch 2270, val loss: 0.4165556728839874
Epoch 2280, training loss: 836.4815673828125 = 0.35624149441719055 + 100.0 * 8.361252784729004
Epoch 2280, val loss: 0.4163135886192322
Epoch 2290, training loss: 836.4684448242188 = 0.35559579730033875 + 100.0 * 8.361128807067871
Epoch 2290, val loss: 0.4160546362400055
Epoch 2300, training loss: 836.7762451171875 = 0.35493823885917664 + 100.0 * 8.364212989807129
Epoch 2300, val loss: 0.4158412516117096
Epoch 2310, training loss: 836.4925537109375 = 0.3542793095111847 + 100.0 * 8.361382484436035
Epoch 2310, val loss: 0.41559597849845886
Epoch 2320, training loss: 836.4303588867188 = 0.3536142110824585 + 100.0 * 8.360767364501953
Epoch 2320, val loss: 0.4153219759464264
Epoch 2330, training loss: 836.379638671875 = 0.3529700040817261 + 100.0 * 8.36026668548584
Epoch 2330, val loss: 0.415153831243515
Epoch 2340, training loss: 836.3504638671875 = 0.3523155450820923 + 100.0 * 8.359981536865234
Epoch 2340, val loss: 0.4149111211299896
Epoch 2350, training loss: 836.5478515625 = 0.351665198802948 + 100.0 * 8.361961364746094
Epoch 2350, val loss: 0.414789080619812
Epoch 2360, training loss: 836.3568115234375 = 0.350991815328598 + 100.0 * 8.360057830810547
Epoch 2360, val loss: 0.4144662320613861
Epoch 2370, training loss: 836.3073120117188 = 0.35033512115478516 + 100.0 * 8.359569549560547
Epoch 2370, val loss: 0.4142313003540039
Epoch 2380, training loss: 836.2705688476562 = 0.3496883511543274 + 100.0 * 8.359209060668945
Epoch 2380, val loss: 0.4140343964099884
Epoch 2390, training loss: 836.2421875 = 0.34903833270072937 + 100.0 * 8.358931541442871
Epoch 2390, val loss: 0.41382935643196106
Epoch 2400, training loss: 836.306396484375 = 0.3483925759792328 + 100.0 * 8.359580039978027
Epoch 2400, val loss: 0.41359663009643555
Epoch 2410, training loss: 836.3794555664062 = 0.3477235734462738 + 100.0 * 8.36031723022461
Epoch 2410, val loss: 0.41339343786239624
Epoch 2420, training loss: 836.2520141601562 = 0.34705042839050293 + 100.0 * 8.359049797058105
Epoch 2420, val loss: 0.41322094202041626
Epoch 2430, training loss: 836.1699829101562 = 0.3464006781578064 + 100.0 * 8.358236312866211
Epoch 2430, val loss: 0.41298791766166687
Epoch 2440, training loss: 836.1171264648438 = 0.34573641419410706 + 100.0 * 8.35771369934082
Epoch 2440, val loss: 0.41279730200767517
Epoch 2450, training loss: 836.1227416992188 = 0.34507444500923157 + 100.0 * 8.357776641845703
Epoch 2450, val loss: 0.41258955001831055
Epoch 2460, training loss: 836.3497924804688 = 0.3444044888019562 + 100.0 * 8.360054016113281
Epoch 2460, val loss: 0.4123363196849823
Epoch 2470, training loss: 836.1582641601562 = 0.3437085449695587 + 100.0 * 8.358145713806152
Epoch 2470, val loss: 0.41218554973602295
Epoch 2480, training loss: 836.059814453125 = 0.343022882938385 + 100.0 * 8.357168197631836
Epoch 2480, val loss: 0.41197267174720764
Epoch 2490, training loss: 836.0245361328125 = 0.3423402011394501 + 100.0 * 8.35682201385498
Epoch 2490, val loss: 0.41176509857177734
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8447488584474885
0.862566108816924
The final CL Acc:0.83257, 0.01167, The final GNN Acc:0.86365, 0.00124
Begin epxeriment: noisy_level: 0.2 cont_weight: 100 epoch: der1:0.3 der2:0.2 dfr1:0.4 dfr2:0.3
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=2500, cl_num_layers=2, cl_num_proj_hidden=64, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=100.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.2, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=32, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, noisy_level=0.2, num_hidden=64, num_proj_hidden=64, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=40, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 88648])
add edge: torch.Size([2, 106424])
remove edge: torch.Size([2, 70982])
updated graph: torch.Size([2, 88758])
=== Noisy graph ===
/home/project-robust-contrastive/Robust-Contrastive-Learning/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 1059.3341064453125 = 1.1073579788208008 + 100.0 * 10.582267761230469
Epoch 0, val loss: 1.1054226160049438
Epoch 10, training loss: 1059.2847900390625 = 1.1029458045959473 + 100.0 * 10.581818580627441
Epoch 10, val loss: 1.1009929180145264
Epoch 20, training loss: 1059.0750732421875 = 1.098096251487732 + 100.0 * 10.579769134521484
Epoch 20, val loss: 1.0961586236953735
Epoch 30, training loss: 1058.1763916015625 = 1.0925992727279663 + 100.0 * 10.57083797454834
Epoch 30, val loss: 1.0906686782836914
Epoch 40, training loss: 1054.7686767578125 = 1.0862669944763184 + 100.0 * 10.536823272705078
Epoch 40, val loss: 1.0843554735183716
Epoch 50, training loss: 1044.2572021484375 = 1.079024076461792 + 100.0 * 10.431782722473145
Epoch 50, val loss: 1.0771682262420654
Epoch 60, training loss: 1019.1325073242188 = 1.070993423461914 + 100.0 * 10.180615425109863
Epoch 60, val loss: 1.0692366361618042
Epoch 70, training loss: 993.6227416992188 = 1.0618910789489746 + 100.0 * 9.92560863494873
Epoch 70, val loss: 1.060503602027893
Epoch 80, training loss: 982.4237060546875 = 1.0546778440475464 + 100.0 * 9.813690185546875
Epoch 80, val loss: 1.0537910461425781
Epoch 90, training loss: 968.0953369140625 = 1.0498424768447876 + 100.0 * 9.670454978942871
Epoch 90, val loss: 1.049506664276123
Epoch 100, training loss: 952.689208984375 = 1.0470471382141113 + 100.0 * 9.5164213180542
Epoch 100, val loss: 1.0471558570861816
Epoch 110, training loss: 938.05517578125 = 1.0452932119369507 + 100.0 * 9.370099067687988
Epoch 110, val loss: 1.0455635786056519
Epoch 120, training loss: 931.2051391601562 = 1.0425689220428467 + 100.0 * 9.301626205444336
Epoch 120, val loss: 1.042736291885376
Epoch 130, training loss: 928.9415893554688 = 1.0389800071716309 + 100.0 * 9.27902603149414
Epoch 130, val loss: 1.0392085313796997
Epoch 140, training loss: 926.4503784179688 = 1.035683035850525 + 100.0 * 9.254146575927734
Epoch 140, val loss: 1.036044716835022
Epoch 150, training loss: 922.8477783203125 = 1.0328541994094849 + 100.0 * 9.218149185180664
Epoch 150, val loss: 1.0333564281463623
Epoch 160, training loss: 916.3786010742188 = 1.0308505296707153 + 100.0 * 9.153477668762207
Epoch 160, val loss: 1.0315426588058472
Epoch 170, training loss: 905.1240844726562 = 1.0302660465240479 + 100.0 * 9.040938377380371
Epoch 170, val loss: 1.0310548543930054
Epoch 180, training loss: 896.56591796875 = 1.0296467542648315 + 100.0 * 8.955362319946289
Epoch 180, val loss: 1.0301564931869507
Epoch 190, training loss: 892.4010009765625 = 1.0269638299942017 + 100.0 * 8.913740158081055
Epoch 190, val loss: 1.0274237394332886
Epoch 200, training loss: 889.274169921875 = 1.0235822200775146 + 100.0 * 8.882506370544434
Epoch 200, val loss: 1.0241268873214722
Epoch 210, training loss: 886.10693359375 = 1.0201990604400635 + 100.0 * 8.85086727142334
Epoch 210, val loss: 1.0208743810653687
Epoch 220, training loss: 882.4277954101562 = 1.0170509815216064 + 100.0 * 8.814107894897461
Epoch 220, val loss: 1.0178449153900146
Epoch 230, training loss: 878.696533203125 = 1.0143303871154785 + 100.0 * 8.776822090148926
Epoch 230, val loss: 1.015217661857605
Epoch 240, training loss: 875.9481811523438 = 1.0112318992614746 + 100.0 * 8.749369621276855
Epoch 240, val loss: 1.012141466140747
Epoch 250, training loss: 873.4581298828125 = 1.0077272653579712 + 100.0 * 8.724503517150879
Epoch 250, val loss: 1.0087028741836548
Epoch 260, training loss: 871.6051025390625 = 1.0039244890213013 + 100.0 * 8.706011772155762
Epoch 260, val loss: 1.005012035369873
Epoch 270, training loss: 869.9020385742188 = 0.9998465180397034 + 100.0 * 8.689022064208984
Epoch 270, val loss: 1.0010156631469727
Epoch 280, training loss: 868.5296630859375 = 0.9954500794410706 + 100.0 * 8.675342559814453
Epoch 280, val loss: 0.9966916441917419
Epoch 290, training loss: 867.0477294921875 = 0.9907395243644714 + 100.0 * 8.66057014465332
Epoch 290, val loss: 0.9921592473983765
Epoch 300, training loss: 865.8179321289062 = 0.9858676195144653 + 100.0 * 8.648321151733398
Epoch 300, val loss: 0.9873839020729065
Epoch 310, training loss: 864.8036499023438 = 0.9807084202766418 + 100.0 * 8.638229370117188
Epoch 310, val loss: 0.9823631644248962
Epoch 320, training loss: 863.6473999023438 = 0.975357174873352 + 100.0 * 8.626720428466797
Epoch 320, val loss: 0.9771695733070374
Epoch 330, training loss: 862.6912841796875 = 0.9698153138160706 + 100.0 * 8.617215156555176
Epoch 330, val loss: 0.9717811346054077
Epoch 340, training loss: 861.7855834960938 = 0.9639224410057068 + 100.0 * 8.608216285705566
Epoch 340, val loss: 0.9660691618919373
Epoch 350, training loss: 861.0697021484375 = 0.9577212333679199 + 100.0 * 8.601119995117188
Epoch 350, val loss: 0.9600539803504944
Epoch 360, training loss: 860.38232421875 = 0.9512069225311279 + 100.0 * 8.594310760498047
Epoch 360, val loss: 0.9537353515625
Epoch 370, training loss: 859.7542724609375 = 0.944356381893158 + 100.0 * 8.588099479675293
Epoch 370, val loss: 0.9471020698547363
Epoch 380, training loss: 859.5532836914062 = 0.9371818900108337 + 100.0 * 8.586160659790039
Epoch 380, val loss: 0.9401513338088989
Epoch 390, training loss: 858.9122314453125 = 0.9296903610229492 + 100.0 * 8.579825401306152
Epoch 390, val loss: 0.9328994750976562
Epoch 400, training loss: 858.45751953125 = 0.9219774603843689 + 100.0 * 8.575355529785156
Epoch 400, val loss: 0.9254466891288757
Epoch 410, training loss: 858.05029296875 = 0.9140984416007996 + 100.0 * 8.571361541748047
Epoch 410, val loss: 0.9178435206413269
Epoch 420, training loss: 858.0250854492188 = 0.9060927629470825 + 100.0 * 8.571189880371094
Epoch 420, val loss: 0.9101245999336243
Epoch 430, training loss: 857.4629516601562 = 0.8979178667068481 + 100.0 * 8.565650939941406
Epoch 430, val loss: 0.9022648930549622
Epoch 440, training loss: 857.0707397460938 = 0.889616847038269 + 100.0 * 8.561811447143555
Epoch 440, val loss: 0.8943077325820923
Epoch 450, training loss: 856.6961669921875 = 0.8812054991722107 + 100.0 * 8.558149337768555
Epoch 450, val loss: 0.8862386345863342
Epoch 460, training loss: 856.3551025390625 = 0.8727239966392517 + 100.0 * 8.554823875427246
Epoch 460, val loss: 0.8781282901763916
Epoch 470, training loss: 856.340087890625 = 0.8641822934150696 + 100.0 * 8.55475902557373
Epoch 470, val loss: 0.8699612021446228
Epoch 480, training loss: 855.9725952148438 = 0.8554981350898743 + 100.0 * 8.55117130279541
Epoch 480, val loss: 0.8617156744003296
Epoch 490, training loss: 855.5086669921875 = 0.8468972444534302 + 100.0 * 8.54661750793457
Epoch 490, val loss: 0.8535377383232117
Epoch 500, training loss: 855.1953735351562 = 0.838333785533905 + 100.0 * 8.543570518493652
Epoch 500, val loss: 0.8454099297523499
Epoch 510, training loss: 854.9036865234375 = 0.8297841548919678 + 100.0 * 8.540739059448242
Epoch 510, val loss: 0.8373115062713623
Epoch 520, training loss: 854.6222534179688 = 0.8212651014328003 + 100.0 * 8.538009643554688
Epoch 520, val loss: 0.8292503952980042
Epoch 530, training loss: 854.9934692382812 = 0.8127594590187073 + 100.0 * 8.541807174682617
Epoch 530, val loss: 0.8212243914604187
Epoch 540, training loss: 854.2677001953125 = 0.8042351603507996 + 100.0 * 8.534634590148926
Epoch 540, val loss: 0.8132248520851135
Epoch 550, training loss: 853.9066162109375 = 0.795834481716156 + 100.0 * 8.531107902526855
Epoch 550, val loss: 0.8053640127182007
Epoch 560, training loss: 853.6294555664062 = 0.7875667214393616 + 100.0 * 8.528419494628906
Epoch 560, val loss: 0.7976548671722412
Epoch 570, training loss: 853.8125 = 0.7794079184532166 + 100.0 * 8.530330657958984
Epoch 570, val loss: 0.7900676131248474
Epoch 580, training loss: 853.4013061523438 = 0.7712477445602417 + 100.0 * 8.526300430297852
Epoch 580, val loss: 0.7825071215629578
Epoch 590, training loss: 853.0643310546875 = 0.7633258104324341 + 100.0 * 8.52301025390625
Epoch 590, val loss: 0.7751978039741516
Epoch 600, training loss: 852.8089599609375 = 0.7555902600288391 + 100.0 * 8.520533561706543
Epoch 600, val loss: 0.7681122422218323
Epoch 610, training loss: 852.590576171875 = 0.7480360269546509 + 100.0 * 8.518424987792969
Epoch 610, val loss: 0.7611947059631348
Epoch 620, training loss: 852.3994140625 = 0.7406260371208191 + 100.0 * 8.51658821105957
Epoch 620, val loss: 0.7544219493865967
Epoch 630, training loss: 852.2205810546875 = 0.7334179878234863 + 100.0 * 8.514871597290039
Epoch 630, val loss: 0.7478699684143066
Epoch 640, training loss: 852.0501098632812 = 0.7263889908790588 + 100.0 * 8.513236999511719
Epoch 640, val loss: 0.7415111660957336
Epoch 650, training loss: 851.8990478515625 = 0.7195656895637512 + 100.0 * 8.511795043945312
Epoch 650, val loss: 0.7353472709655762
Epoch 660, training loss: 851.9881591796875 = 0.712889552116394 + 100.0 * 8.512752532958984
Epoch 660, val loss: 0.7293663620948792
Epoch 670, training loss: 851.6996459960938 = 0.7064277529716492 + 100.0 * 8.509932518005371
Epoch 670, val loss: 0.7236190438270569
Epoch 680, training loss: 851.4968872070312 = 0.7002170085906982 + 100.0 * 8.507966995239258
Epoch 680, val loss: 0.7181177735328674
Epoch 690, training loss: 851.3771362304688 = 0.6942028403282166 + 100.0 * 8.506829261779785
Epoch 690, val loss: 0.7127991318702698
Epoch 700, training loss: 851.5508422851562 = 0.6883622407913208 + 100.0 * 8.508625030517578
Epoch 700, val loss: 0.7076682448387146
Epoch 710, training loss: 851.207275390625 = 0.6827622652053833 + 100.0 * 8.505245208740234
Epoch 710, val loss: 0.7028140425682068
Epoch 720, training loss: 851.02734375 = 0.6773712038993835 + 100.0 * 8.503499984741211
Epoch 720, val loss: 0.6981688737869263
Epoch 730, training loss: 850.8876342773438 = 0.672199010848999 + 100.0 * 8.502154350280762
Epoch 730, val loss: 0.6937639117240906
Epoch 740, training loss: 850.8179931640625 = 0.667238175868988 + 100.0 * 8.501507759094238
Epoch 740, val loss: 0.6895742416381836
Epoch 750, training loss: 850.8343505859375 = 0.6624785661697388 + 100.0 * 8.501718521118164
Epoch 750, val loss: 0.685593843460083
Epoch 760, training loss: 850.6124877929688 = 0.657886266708374 + 100.0 * 8.49954605102539
Epoch 760, val loss: 0.6817846894264221
Epoch 770, training loss: 850.5629272460938 = 0.6534436941146851 + 100.0 * 8.49909496307373
Epoch 770, val loss: 0.6781345009803772
Epoch 780, training loss: 850.3969116210938 = 0.6492568254470825 + 100.0 * 8.497476577758789
Epoch 780, val loss: 0.6747585535049438
Epoch 790, training loss: 850.637451171875 = 0.6452240943908691 + 100.0 * 8.499922752380371
Epoch 790, val loss: 0.6715360283851624
Epoch 800, training loss: 850.4365234375 = 0.6413533091545105 + 100.0 * 8.49795150756836
Epoch 800, val loss: 0.6685269474983215
Epoch 810, training loss: 850.1918334960938 = 0.6376654505729675 + 100.0 * 8.4955415725708
Epoch 810, val loss: 0.6656391620635986
Epoch 820, training loss: 850.0620727539062 = 0.634158194065094 + 100.0 * 8.494278907775879
Epoch 820, val loss: 0.6629645824432373
Epoch 830, training loss: 850.3511962890625 = 0.6307908296585083 + 100.0 * 8.497203826904297
Epoch 830, val loss: 0.660451352596283
Epoch 840, training loss: 850.002685546875 = 0.6275491714477539 + 100.0 * 8.493751525878906
Epoch 840, val loss: 0.658004641532898
Epoch 850, training loss: 849.8521118164062 = 0.6244866251945496 + 100.0 * 8.492276191711426
Epoch 850, val loss: 0.6558271646499634
Epoch 860, training loss: 849.7415161132812 = 0.6215648651123047 + 100.0 * 8.491199493408203
Epoch 860, val loss: 0.6537519693374634
Epoch 870, training loss: 849.6435546875 = 0.6187671422958374 + 100.0 * 8.49024772644043
Epoch 870, val loss: 0.6518131494522095
Epoch 880, training loss: 849.5899658203125 = 0.6160818338394165 + 100.0 * 8.489738464355469
Epoch 880, val loss: 0.6500037908554077
Epoch 890, training loss: 849.8557739257812 = 0.6134894490242004 + 100.0 * 8.492423057556152
Epoch 890, val loss: 0.6482763290405273
Epoch 900, training loss: 849.8530883789062 = 0.6109635829925537 + 100.0 * 8.49242115020752
Epoch 900, val loss: 0.6466454267501831
Epoch 910, training loss: 849.409912109375 = 0.6085700988769531 + 100.0 * 8.48801326751709
Epoch 910, val loss: 0.6451161503791809
Epoch 920, training loss: 849.325439453125 = 0.6062999367713928 + 100.0 * 8.487191200256348
Epoch 920, val loss: 0.6436832547187805
Epoch 930, training loss: 849.2446899414062 = 0.6041043996810913 + 100.0 * 8.486405372619629
Epoch 930, val loss: 0.6423904895782471
Epoch 940, training loss: 849.1724243164062 = 0.6020035147666931 + 100.0 * 8.48570442199707
Epoch 940, val loss: 0.6411636471748352
Epoch 950, training loss: 849.2919311523438 = 0.5999616384506226 + 100.0 * 8.486919403076172
Epoch 950, val loss: 0.6400216817855835
Epoch 960, training loss: 849.228271484375 = 0.5979490280151367 + 100.0 * 8.486303329467773
Epoch 960, val loss: 0.6387430429458618
Epoch 970, training loss: 849.080322265625 = 0.5960029363632202 + 100.0 * 8.484843254089355
Epoch 970, val loss: 0.6377187371253967
Epoch 980, training loss: 848.9677124023438 = 0.594172477722168 + 100.0 * 8.483735084533691
Epoch 980, val loss: 0.63675856590271
Epoch 990, training loss: 848.87548828125 = 0.5924139618873596 + 100.0 * 8.482831001281738
Epoch 990, val loss: 0.6358500123023987
Epoch 1000, training loss: 848.8040161132812 = 0.5907129049301147 + 100.0 * 8.482132911682129
Epoch 1000, val loss: 0.6349723935127258
Epoch 1010, training loss: 848.81494140625 = 0.5890536308288574 + 100.0 * 8.482258796691895
Epoch 1010, val loss: 0.634167492389679
Epoch 1020, training loss: 848.855712890625 = 0.5874074101448059 + 100.0 * 8.482683181762695
Epoch 1020, val loss: 0.6333230137825012
Epoch 1030, training loss: 848.6719970703125 = 0.5858141183853149 + 100.0 * 8.48086166381836
Epoch 1030, val loss: 0.6325773596763611
Epoch 1040, training loss: 848.5955200195312 = 0.5843033194541931 + 100.0 * 8.480112075805664
Epoch 1040, val loss: 0.6318421363830566
Epoch 1050, training loss: 848.49951171875 = 0.5828516483306885 + 100.0 * 8.479166984558105
Epoch 1050, val loss: 0.6311964988708496
Epoch 1060, training loss: 848.4312744140625 = 0.5814318656921387 + 100.0 * 8.478498458862305
Epoch 1060, val loss: 0.6305587887763977
Epoch 1070, training loss: 848.5307006835938 = 0.5800512433052063 + 100.0 * 8.479506492614746
Epoch 1070, val loss: 0.6299072504043579
Epoch 1080, training loss: 848.4754028320312 = 0.5786411166191101 + 100.0 * 8.478967666625977
Epoch 1080, val loss: 0.6293772459030151
Epoch 1090, training loss: 848.32080078125 = 0.5772935748100281 + 100.0 * 8.477435111999512
Epoch 1090, val loss: 0.6287212371826172
Epoch 1100, training loss: 848.2154541015625 = 0.5760108232498169 + 100.0 * 8.476394653320312
Epoch 1100, val loss: 0.6281918883323669
Epoch 1110, training loss: 848.1127319335938 = 0.5747655034065247 + 100.0 * 8.475379943847656
Epoch 1110, val loss: 0.6277047395706177
Epoch 1120, training loss: 848.0505981445312 = 0.573553740978241 + 100.0 * 8.474770545959473
Epoch 1120, val loss: 0.6272098422050476
Epoch 1130, training loss: 848.2175903320312 = 0.5723606944084167 + 100.0 * 8.476451873779297
Epoch 1130, val loss: 0.6266990303993225
Epoch 1140, training loss: 847.9661865234375 = 0.5711430907249451 + 100.0 * 8.473950386047363
Epoch 1140, val loss: 0.6262395977973938
Epoch 1150, training loss: 847.910888671875 = 0.5699608325958252 + 100.0 * 8.473409652709961
Epoch 1150, val loss: 0.6257254481315613
Epoch 1160, training loss: 847.8204345703125 = 0.5688300132751465 + 100.0 * 8.472516059875488
Epoch 1160, val loss: 0.6253003478050232
Epoch 1170, training loss: 847.7260131835938 = 0.5677224397659302 + 100.0 * 8.471582412719727
Epoch 1170, val loss: 0.624853253364563
Epoch 1180, training loss: 848.2318725585938 = 0.5666261315345764 + 100.0 * 8.476652145385742
Epoch 1180, val loss: 0.6244935989379883
Epoch 1190, training loss: 847.977294921875 = 0.5654869079589844 + 100.0 * 8.47411823272705
Epoch 1190, val loss: 0.6239145994186401
Epoch 1200, training loss: 847.5982055664062 = 0.5643745064735413 + 100.0 * 8.470337867736816
Epoch 1200, val loss: 0.6235220432281494
Epoch 1210, training loss: 847.4683837890625 = 0.5633271932601929 + 100.0 * 8.469050407409668
Epoch 1210, val loss: 0.6231942772865295
Epoch 1220, training loss: 847.38818359375 = 0.562308132648468 + 100.0 * 8.46825885772705
Epoch 1220, val loss: 0.6227432489395142
Epoch 1230, training loss: 847.3018798828125 = 0.5612869262695312 + 100.0 * 8.467406272888184
Epoch 1230, val loss: 0.6223680973052979
Epoch 1240, training loss: 847.2418212890625 = 0.5602803826332092 + 100.0 * 8.466814994812012
Epoch 1240, val loss: 0.6219856142997742
Epoch 1250, training loss: 847.4237670898438 = 0.559289813041687 + 100.0 * 8.468645095825195
Epoch 1250, val loss: 0.6215531229972839
Epoch 1260, training loss: 847.31201171875 = 0.5582536458969116 + 100.0 * 8.467537879943848
Epoch 1260, val loss: 0.6212254166603088
Epoch 1270, training loss: 847.2394409179688 = 0.5572469234466553 + 100.0 * 8.466821670532227
Epoch 1270, val loss: 0.6208453178405762
Epoch 1280, training loss: 847.1691284179688 = 0.5562543869018555 + 100.0 * 8.4661283493042
Epoch 1280, val loss: 0.6203463077545166
Epoch 1290, training loss: 847.0303344726562 = 0.5552648305892944 + 100.0 * 8.464751243591309
Epoch 1290, val loss: 0.6199694275856018
Epoch 1300, training loss: 846.9139404296875 = 0.5543293952941895 + 100.0 * 8.46359634399414
Epoch 1300, val loss: 0.6196129322052002
Epoch 1310, training loss: 846.8258666992188 = 0.5533946752548218 + 100.0 * 8.462724685668945
Epoch 1310, val loss: 0.6192331314086914
Epoch 1320, training loss: 846.8508911132812 = 0.5524661540985107 + 100.0 * 8.462984085083008
Epoch 1320, val loss: 0.6188439130783081
Epoch 1330, training loss: 847.0201416015625 = 0.5515264272689819 + 100.0 * 8.464686393737793
Epoch 1330, val loss: 0.6184523105621338
Epoch 1340, training loss: 846.824462890625 = 0.5505470037460327 + 100.0 * 8.462738990783691
Epoch 1340, val loss: 0.6180976629257202
Epoch 1350, training loss: 846.6796875 = 0.5496067404747009 + 100.0 * 8.46130084991455
Epoch 1350, val loss: 0.6176633834838867
Epoch 1360, training loss: 846.5529174804688 = 0.5486901998519897 + 100.0 * 8.460041999816895
Epoch 1360, val loss: 0.6172954440116882
Epoch 1370, training loss: 846.5174560546875 = 0.5477892160415649 + 100.0 * 8.459696769714355
Epoch 1370, val loss: 0.616888165473938
Epoch 1380, training loss: 846.7723999023438 = 0.5468816161155701 + 100.0 * 8.462255477905273
Epoch 1380, val loss: 0.6165107488632202
Epoch 1390, training loss: 846.4171752929688 = 0.5459195375442505 + 100.0 * 8.458712577819824
Epoch 1390, val loss: 0.6160914301872253
Epoch 1400, training loss: 846.3574829101562 = 0.5449891686439514 + 100.0 * 8.458125114440918
Epoch 1400, val loss: 0.6157008409500122
Epoch 1410, training loss: 846.3115844726562 = 0.5440961718559265 + 100.0 * 8.457674980163574
Epoch 1410, val loss: 0.6152933239936829
Epoch 1420, training loss: 846.3019409179688 = 0.5432022213935852 + 100.0 * 8.457587242126465
Epoch 1420, val loss: 0.6149046421051025
Epoch 1430, training loss: 846.73876953125 = 0.5422987341880798 + 100.0 * 8.46196460723877
Epoch 1430, val loss: 0.6144831776618958
Epoch 1440, training loss: 846.3450927734375 = 0.5413533449172974 + 100.0 * 8.458037376403809
Epoch 1440, val loss: 0.6141208410263062
Epoch 1450, training loss: 846.15478515625 = 0.5404503345489502 + 100.0 * 8.456143379211426
Epoch 1450, val loss: 0.6137160658836365
Epoch 1460, training loss: 846.0863647460938 = 0.5395592451095581 + 100.0 * 8.45546817779541
Epoch 1460, val loss: 0.6133187413215637
Epoch 1470, training loss: 846.0995483398438 = 0.5386680960655212 + 100.0 * 8.455608367919922
Epoch 1470, val loss: 0.6129447221755981
Epoch 1480, training loss: 846.8836059570312 = 0.5377585291862488 + 100.0 * 8.463458061218262
Epoch 1480, val loss: 0.6124444007873535
Epoch 1490, training loss: 846.396484375 = 0.5367633104324341 + 100.0 * 8.458597183227539
Epoch 1490, val loss: 0.6121011972427368
Epoch 1500, training loss: 845.9946899414062 = 0.5358372926712036 + 100.0 * 8.454588890075684
Epoch 1500, val loss: 0.6116759777069092
Epoch 1510, training loss: 845.8983764648438 = 0.5349583625793457 + 100.0 * 8.453634262084961
Epoch 1510, val loss: 0.6111785769462585
Epoch 1520, training loss: 845.859619140625 = 0.534072995185852 + 100.0 * 8.453255653381348
Epoch 1520, val loss: 0.6108192205429077
Epoch 1530, training loss: 845.8170776367188 = 0.5331901907920837 + 100.0 * 8.452838897705078
Epoch 1530, val loss: 0.6103858947753906
Epoch 1540, training loss: 845.77734375 = 0.5322968363761902 + 100.0 * 8.4524507522583
Epoch 1540, val loss: 0.6100240349769592
Epoch 1550, training loss: 845.917724609375 = 0.5314034223556519 + 100.0 * 8.453863143920898
Epoch 1550, val loss: 0.6095874905586243
Epoch 1560, training loss: 845.889404296875 = 0.5304615497589111 + 100.0 * 8.45358943939209
Epoch 1560, val loss: 0.6091378927230835
Epoch 1570, training loss: 845.7472534179688 = 0.5295084118843079 + 100.0 * 8.452178001403809
Epoch 1570, val loss: 0.608715295791626
Epoch 1580, training loss: 845.6856079101562 = 0.5286094546318054 + 100.0 * 8.451569557189941
Epoch 1580, val loss: 0.6082883477210999
Epoch 1590, training loss: 845.638427734375 = 0.527715802192688 + 100.0 * 8.451107025146484
Epoch 1590, val loss: 0.6079326272010803
Epoch 1600, training loss: 845.9548950195312 = 0.5268194675445557 + 100.0 * 8.454280853271484
Epoch 1600, val loss: 0.6075429916381836
Epoch 1610, training loss: 845.5625610351562 = 0.5258727073669434 + 100.0 * 8.450366973876953
Epoch 1610, val loss: 0.6070441007614136
Epoch 1620, training loss: 845.5341796875 = 0.5249428749084473 + 100.0 * 8.450092315673828
Epoch 1620, val loss: 0.6066683530807495
Epoch 1630, training loss: 845.5049438476562 = 0.5240431427955627 + 100.0 * 8.449809074401855
Epoch 1630, val loss: 0.6062053442001343
Epoch 1640, training loss: 845.4597778320312 = 0.5231348276138306 + 100.0 * 8.449366569519043
Epoch 1640, val loss: 0.6058409810066223
Epoch 1650, training loss: 845.4546508789062 = 0.5222222805023193 + 100.0 * 8.449324607849121
Epoch 1650, val loss: 0.6054717302322388
Epoch 1660, training loss: 845.7614135742188 = 0.5212950706481934 + 100.0 * 8.452401161193848
Epoch 1660, val loss: 0.6050964593887329
Epoch 1670, training loss: 845.578125 = 0.5203495025634766 + 100.0 * 8.450577735900879
Epoch 1670, val loss: 0.604456901550293
Epoch 1680, training loss: 845.482177734375 = 0.5193844437599182 + 100.0 * 8.449627876281738
Epoch 1680, val loss: 0.6041264533996582
Epoch 1690, training loss: 845.3310546875 = 0.5184692144393921 + 100.0 * 8.448125839233398
Epoch 1690, val loss: 0.6036696434020996
Epoch 1700, training loss: 845.2921142578125 = 0.5175539255142212 + 100.0 * 8.447745323181152
Epoch 1700, val loss: 0.6032924056053162
Epoch 1710, training loss: 845.511474609375 = 0.5166419744491577 + 100.0 * 8.44994831085205
Epoch 1710, val loss: 0.6028769016265869
Epoch 1720, training loss: 845.2593994140625 = 0.5156723856925964 + 100.0 * 8.447437286376953
Epoch 1720, val loss: 0.6023951172828674
Epoch 1730, training loss: 845.2021484375 = 0.5147172212600708 + 100.0 * 8.446874618530273
Epoch 1730, val loss: 0.6019794940948486
Epoch 1740, training loss: 845.174072265625 = 0.5138022303581238 + 100.0 * 8.446602821350098
Epoch 1740, val loss: 0.6015166640281677
Epoch 1750, training loss: 845.1492919921875 = 0.5128875970840454 + 100.0 * 8.446364402770996
Epoch 1750, val loss: 0.6011558175086975
Epoch 1760, training loss: 845.1259765625 = 0.5119701623916626 + 100.0 * 8.44614028930664
Epoch 1760, val loss: 0.6007259488105774
Epoch 1770, training loss: 845.0947265625 = 0.5110391974449158 + 100.0 * 8.445837020874023
Epoch 1770, val loss: 0.6003113985061646
Epoch 1780, training loss: 845.1013793945312 = 0.5100972652435303 + 100.0 * 8.445913314819336
Epoch 1780, val loss: 0.5998896360397339
Epoch 1790, training loss: 845.466796875 = 0.509134829044342 + 100.0 * 8.449576377868652
Epoch 1790, val loss: 0.5994982719421387
Epoch 1800, training loss: 845.2940673828125 = 0.5081291794776917 + 100.0 * 8.447859764099121
Epoch 1800, val loss: 0.5990852117538452
Epoch 1810, training loss: 845.056396484375 = 0.50715172290802 + 100.0 * 8.4454927444458
Epoch 1810, val loss: 0.5986040830612183
Epoch 1820, training loss: 845.2598266601562 = 0.5061877965927124 + 100.0 * 8.44753646850586
Epoch 1820, val loss: 0.5981918573379517
Epoch 1830, training loss: 844.9653930664062 = 0.5051962733268738 + 100.0 * 8.444602012634277
Epoch 1830, val loss: 0.5977039337158203
Epoch 1840, training loss: 844.9950561523438 = 0.5042228698730469 + 100.0 * 8.444908142089844
Epoch 1840, val loss: 0.5972809791564941
Epoch 1850, training loss: 844.9173583984375 = 0.5032556653022766 + 100.0 * 8.444141387939453
Epoch 1850, val loss: 0.5968539714813232
Epoch 1860, training loss: 844.8814697265625 = 0.502285361289978 + 100.0 * 8.443792343139648
Epoch 1860, val loss: 0.5964545011520386
Epoch 1870, training loss: 844.92724609375 = 0.5013110041618347 + 100.0 * 8.444259643554688
Epoch 1870, val loss: 0.5960465669631958
Epoch 1880, training loss: 845.4810180664062 = 0.5003101825714111 + 100.0 * 8.449807167053223
Epoch 1880, val loss: 0.5955283641815186
Epoch 1890, training loss: 844.9922485351562 = 0.4992723762989044 + 100.0 * 8.444930076599121
Epoch 1890, val loss: 0.5950279831886292
Epoch 1900, training loss: 844.812744140625 = 0.4982616901397705 + 100.0 * 8.443144798278809
Epoch 1900, val loss: 0.5946276187896729
Epoch 1910, training loss: 844.75341796875 = 0.49728551506996155 + 100.0 * 8.442561149597168
Epoch 1910, val loss: 0.5941967964172363
Epoch 1920, training loss: 844.7271118164062 = 0.49629855155944824 + 100.0 * 8.44230842590332
Epoch 1920, val loss: 0.5938119888305664
Epoch 1930, training loss: 844.74951171875 = 0.4953109323978424 + 100.0 * 8.44254207611084
Epoch 1930, val loss: 0.59337317943573
Epoch 1940, training loss: 844.9970703125 = 0.49429595470428467 + 100.0 * 8.445027351379395
Epoch 1940, val loss: 0.592954695224762
Epoch 1950, training loss: 844.994140625 = 0.493237167596817 + 100.0 * 8.445009231567383
Epoch 1950, val loss: 0.592494010925293
Epoch 1960, training loss: 844.7313232421875 = 0.4921903610229492 + 100.0 * 8.442391395568848
Epoch 1960, val loss: 0.5918880701065063
Epoch 1970, training loss: 844.625 = 0.4911648631095886 + 100.0 * 8.441338539123535
Epoch 1970, val loss: 0.5915647745132446
Epoch 1980, training loss: 844.6099243164062 = 0.4901520609855652 + 100.0 * 8.441197395324707
Epoch 1980, val loss: 0.5911047458648682
Epoch 1990, training loss: 844.6002197265625 = 0.4891282320022583 + 100.0 * 8.441110610961914
Epoch 1990, val loss: 0.5906876921653748
Epoch 2000, training loss: 844.7528686523438 = 0.4880939722061157 + 100.0 * 8.442647933959961
Epoch 2000, val loss: 0.5902748107910156
Epoch 2010, training loss: 844.6237182617188 = 0.4870245158672333 + 100.0 * 8.441367149353027
Epoch 2010, val loss: 0.5897933840751648
Epoch 2020, training loss: 844.5770874023438 = 0.48596566915512085 + 100.0 * 8.440911293029785
Epoch 2020, val loss: 0.5893236398696899
Epoch 2030, training loss: 844.4688720703125 = 0.48491498827934265 + 100.0 * 8.439839363098145
Epoch 2030, val loss: 0.58887779712677
Epoch 2040, training loss: 844.4660034179688 = 0.48386549949645996 + 100.0 * 8.439821243286133
Epoch 2040, val loss: 0.588443398475647
Epoch 2050, training loss: 845.274658203125 = 0.4827962815761566 + 100.0 * 8.447918891906738
Epoch 2050, val loss: 0.588094174861908
Epoch 2060, training loss: 844.7577514648438 = 0.481672465801239 + 100.0 * 8.442760467529297
Epoch 2060, val loss: 0.5873667597770691
Epoch 2070, training loss: 844.42236328125 = 0.48056432604789734 + 100.0 * 8.439417839050293
Epoch 2070, val loss: 0.5869141817092896
Epoch 2080, training loss: 844.3924560546875 = 0.4794920086860657 + 100.0 * 8.439129829406738
Epoch 2080, val loss: 0.5865002274513245
Epoch 2090, training loss: 844.3368530273438 = 0.47841954231262207 + 100.0 * 8.438584327697754
Epoch 2090, val loss: 0.5860544443130493
Epoch 2100, training loss: 844.35400390625 = 0.4773359000682831 + 100.0 * 8.438766479492188
Epoch 2100, val loss: 0.5855744481086731
Epoch 2110, training loss: 844.8567504882812 = 0.4762301743030548 + 100.0 * 8.443804740905762
Epoch 2110, val loss: 0.5851495265960693
Epoch 2120, training loss: 844.3779907226562 = 0.4750855565071106 + 100.0 * 8.4390287399292
Epoch 2120, val loss: 0.5846458077430725
Epoch 2130, training loss: 844.2440185546875 = 0.473976194858551 + 100.0 * 8.437700271606445
Epoch 2130, val loss: 0.5841356515884399
Epoch 2140, training loss: 844.2154541015625 = 0.4728892743587494 + 100.0 * 8.43742561340332
Epoch 2140, val loss: 0.5837042331695557
Epoch 2150, training loss: 844.2177124023438 = 0.471797913312912 + 100.0 * 8.437458992004395
Epoch 2150, val loss: 0.583250105381012
Epoch 2160, training loss: 844.3834838867188 = 0.4706996977329254 + 100.0 * 8.439127922058105
Epoch 2160, val loss: 0.5827012062072754
Epoch 2170, training loss: 844.164306640625 = 0.46954837441444397 + 100.0 * 8.4369478225708
Epoch 2170, val loss: 0.5824006795883179
Epoch 2180, training loss: 844.1514282226562 = 0.46841561794281006 + 100.0 * 8.436829566955566
Epoch 2180, val loss: 0.5818802118301392
Epoch 2190, training loss: 844.1542358398438 = 0.46728721261024475 + 100.0 * 8.436869621276855
Epoch 2190, val loss: 0.5815191268920898
Epoch 2200, training loss: 844.478515625 = 0.4661567211151123 + 100.0 * 8.440123558044434
Epoch 2200, val loss: 0.5809953808784485
Epoch 2210, training loss: 844.1348876953125 = 0.4649723768234253 + 100.0 * 8.436698913574219
Epoch 2210, val loss: 0.5807303190231323
Epoch 2220, training loss: 844.060546875 = 0.46382826566696167 + 100.0 * 8.435967445373535
Epoch 2220, val loss: 0.5801190733909607
Epoch 2230, training loss: 844.0314331054688 = 0.4626863896846771 + 100.0 * 8.435687065124512
Epoch 2230, val loss: 0.579753577709198
Epoch 2240, training loss: 844.0355834960938 = 0.4615379571914673 + 100.0 * 8.43574047088623
Epoch 2240, val loss: 0.5793775916099548
Epoch 2250, training loss: 844.43115234375 = 0.4603760838508606 + 100.0 * 8.43970775604248
Epoch 2250, val loss: 0.5790109038352966
Epoch 2260, training loss: 844.1011962890625 = 0.4591691792011261 + 100.0 * 8.436420440673828
Epoch 2260, val loss: 0.5784087181091309
Epoch 2270, training loss: 844.0762329101562 = 0.4579772651195526 + 100.0 * 8.436182975769043
Epoch 2270, val loss: 0.5779880285263062
Epoch 2280, training loss: 844.2905883789062 = 0.4567827582359314 + 100.0 * 8.438338279724121
Epoch 2280, val loss: 0.5775592923164368
Epoch 2290, training loss: 843.9095458984375 = 0.45557355880737305 + 100.0 * 8.434539794921875
Epoch 2290, val loss: 0.5770449638366699
Epoch 2300, training loss: 843.9048461914062 = 0.454388827085495 + 100.0 * 8.434504508972168
Epoch 2300, val loss: 0.5766245722770691
Epoch 2310, training loss: 843.8494873046875 = 0.4532071352005005 + 100.0 * 8.43396282196045
Epoch 2310, val loss: 0.5762158632278442
Epoch 2320, training loss: 843.8473510742188 = 0.4520184397697449 + 100.0 * 8.433953285217285
Epoch 2320, val loss: 0.5757948160171509
Epoch 2330, training loss: 843.9103393554688 = 0.4508177638053894 + 100.0 * 8.434595108032227
Epoch 2330, val loss: 0.575421929359436
Epoch 2340, training loss: 843.9337768554688 = 0.44959133863449097 + 100.0 * 8.434842109680176
Epoch 2340, val loss: 0.5749049782752991
Epoch 2350, training loss: 844.4306030273438 = 0.4483475983142853 + 100.0 * 8.43982219696045
Epoch 2350, val loss: 0.5742589831352234
Epoch 2360, training loss: 843.8428344726562 = 0.44704926013946533 + 100.0 * 8.433958053588867
Epoch 2360, val loss: 0.5738893747329712
Epoch 2370, training loss: 843.7672119140625 = 0.44580936431884766 + 100.0 * 8.43321418762207
Epoch 2370, val loss: 0.5734755396842957
Epoch 2380, training loss: 843.7118530273438 = 0.4445856511592865 + 100.0 * 8.432672500610352
Epoch 2380, val loss: 0.5728626847267151
Epoch 2390, training loss: 843.6843872070312 = 0.44335314631462097 + 100.0 * 8.43241024017334
Epoch 2390, val loss: 0.5724796056747437
Epoch 2400, training loss: 843.673828125 = 0.44211211800575256 + 100.0 * 8.432316780090332
Epoch 2400, val loss: 0.5720006823539734
Epoch 2410, training loss: 843.9799194335938 = 0.44086501002311707 + 100.0 * 8.43539047241211
Epoch 2410, val loss: 0.5714269280433655
Epoch 2420, training loss: 843.67041015625 = 0.4395561218261719 + 100.0 * 8.432308197021484
Epoch 2420, val loss: 0.5712209343910217
Epoch 2430, training loss: 843.7802124023438 = 0.43828174471855164 + 100.0 * 8.433419227600098
Epoch 2430, val loss: 0.5705627202987671
Epoch 2440, training loss: 843.7637939453125 = 0.43698644638061523 + 100.0 * 8.433267593383789
Epoch 2440, val loss: 0.5702822804450989
Epoch 2450, training loss: 843.6099243164062 = 0.4357123076915741 + 100.0 * 8.431741714477539
Epoch 2450, val loss: 0.5697622895240784
Epoch 2460, training loss: 843.5679931640625 = 0.43444737792015076 + 100.0 * 8.43133544921875
Epoch 2460, val loss: 0.5693933963775635
Epoch 2470, training loss: 843.5140991210938 = 0.433180034160614 + 100.0 * 8.430809020996094
Epoch 2470, val loss: 0.5688952207565308
Epoch 2480, training loss: 843.5538940429688 = 0.4319080114364624 + 100.0 * 8.431220054626465
Epoch 2480, val loss: 0.5684414505958557
Epoch 2490, training loss: 843.7333374023438 = 0.43060946464538574 + 100.0 * 8.433027267456055
Epoch 2490, val loss: 0.5680294632911682
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7645865043125317
0.8134463522422662
=== training gcn model ===
Epoch 0, training loss: 1059.3228759765625 = 1.092955231666565 + 100.0 * 10.582300186157227
Epoch 0, val loss: 1.0925188064575195
Epoch 10, training loss: 1059.2943115234375 = 1.0886449813842773 + 100.0 * 10.582056999206543
Epoch 10, val loss: 1.0882023572921753
Epoch 20, training loss: 1059.204833984375 = 1.0839478969573975 + 100.0 * 10.581208229064941
Epoch 20, val loss: 1.0835013389587402
Epoch 30, training loss: 1058.85888671875 = 1.0788228511810303 + 100.0 * 10.577799797058105
Epoch 30, val loss: 1.0783840417861938
Epoch 40, training loss: 1057.3807373046875 = 1.0733585357666016 + 100.0 * 10.563074111938477
Epoch 40, val loss: 1.0729293823242188
Epoch 50, training loss: 1051.1826171875 = 1.067407250404358 + 100.0 * 10.501152038574219
Epoch 50, val loss: 1.0670251846313477
Epoch 60, training loss: 1027.8961181640625 = 1.0613157749176025 + 100.0 * 10.268348693847656
Epoch 60, val loss: 1.061100959777832
Epoch 70, training loss: 979.9907836914062 = 1.0554026365280151 + 100.0 * 9.789353370666504
Epoch 70, val loss: 1.055458426475525
Epoch 80, training loss: 962.2411499023438 = 1.050881266593933 + 100.0 * 9.611902236938477
Epoch 80, val loss: 1.0513975620269775
Epoch 90, training loss: 952.9041137695312 = 1.0481666326522827 + 100.0 * 9.518559455871582
Epoch 90, val loss: 1.0487741231918335
Epoch 100, training loss: 940.2678833007812 = 1.045627236366272 + 100.0 * 9.39222240447998
Epoch 100, val loss: 1.0461987257003784
Epoch 110, training loss: 929.5260620117188 = 1.043025255203247 + 100.0 * 9.284830093383789
Epoch 110, val loss: 1.043639063835144
Epoch 120, training loss: 922.5420532226562 = 1.040383219718933 + 100.0 * 9.21501636505127
Epoch 120, val loss: 1.0411275625228882
Epoch 130, training loss: 915.2418212890625 = 1.0384396314620972 + 100.0 * 9.142033576965332
Epoch 130, val loss: 1.0393483638763428
Epoch 140, training loss: 908.4375610351562 = 1.0372024774551392 + 100.0 * 9.074003219604492
Epoch 140, val loss: 1.0381728410720825
Epoch 150, training loss: 904.3295288085938 = 1.0358226299285889 + 100.0 * 9.032937049865723
Epoch 150, val loss: 1.0367670059204102
Epoch 160, training loss: 899.8878784179688 = 1.0339453220367432 + 100.0 * 8.988539695739746
Epoch 160, val loss: 1.0348840951919556
Epoch 170, training loss: 894.9667358398438 = 1.0322965383529663 + 100.0 * 8.93934440612793
Epoch 170, val loss: 1.0332999229431152
Epoch 180, training loss: 892.213623046875 = 1.0308386087417603 + 100.0 * 8.91182804107666
Epoch 180, val loss: 1.031832218170166
Epoch 190, training loss: 889.646240234375 = 1.0288339853286743 + 100.0 * 8.886174201965332
Epoch 190, val loss: 1.0298259258270264
Epoch 200, training loss: 886.6257934570312 = 1.026879072189331 + 100.0 * 8.855989456176758
Epoch 200, val loss: 1.0279768705368042
Epoch 210, training loss: 882.9606323242188 = 1.0254122018814087 + 100.0 * 8.819352149963379
Epoch 210, val loss: 1.0266015529632568
Epoch 220, training loss: 879.5783081054688 = 1.0239880084991455 + 100.0 * 8.785543441772461
Epoch 220, val loss: 1.0251635313034058
Epoch 230, training loss: 877.4926147460938 = 1.021947979927063 + 100.0 * 8.7647066116333
Epoch 230, val loss: 1.023082971572876
Epoch 240, training loss: 876.1980590820312 = 1.0194000005722046 + 100.0 * 8.751786231994629
Epoch 240, val loss: 1.0205605030059814
Epoch 250, training loss: 875.1546630859375 = 1.0166630744934082 + 100.0 * 8.741379737854004
Epoch 250, val loss: 1.0179075002670288
Epoch 260, training loss: 874.1239013671875 = 1.013791799545288 + 100.0 * 8.731101036071777
Epoch 260, val loss: 1.015154242515564
Epoch 270, training loss: 873.0830688476562 = 1.0108394622802734 + 100.0 * 8.720722198486328
Epoch 270, val loss: 1.012327790260315
Epoch 280, training loss: 871.97265625 = 1.0078094005584717 + 100.0 * 8.709648132324219
Epoch 280, val loss: 1.0093969106674194
Epoch 290, training loss: 870.5674438476562 = 1.004709005355835 + 100.0 * 8.695627212524414
Epoch 290, val loss: 1.0064406394958496
Epoch 300, training loss: 869.1062622070312 = 1.0015864372253418 + 100.0 * 8.681046485900879
Epoch 300, val loss: 1.003470540046692
Epoch 310, training loss: 867.548095703125 = 0.9982801675796509 + 100.0 * 8.665497779846191
Epoch 310, val loss: 1.000286340713501
Epoch 320, training loss: 866.0040893554688 = 0.9947085380554199 + 100.0 * 8.650094032287598
Epoch 320, val loss: 0.9968451261520386
Epoch 330, training loss: 864.8384399414062 = 0.9907792210578918 + 100.0 * 8.638476371765137
Epoch 330, val loss: 0.9930497407913208
Epoch 340, training loss: 863.5908813476562 = 0.9864737391471863 + 100.0 * 8.626044273376465
Epoch 340, val loss: 0.9889175295829773
Epoch 350, training loss: 862.3543090820312 = 0.9819111227989197 + 100.0 * 8.613723754882812
Epoch 350, val loss: 0.9845444560050964
Epoch 360, training loss: 861.2969360351562 = 0.9770762324333191 + 100.0 * 8.603199005126953
Epoch 360, val loss: 0.9799187779426575
Epoch 370, training loss: 860.7049560546875 = 0.9718819260597229 + 100.0 * 8.597331047058105
Epoch 370, val loss: 0.9749965071678162
Epoch 380, training loss: 859.5181884765625 = 0.9664859771728516 + 100.0 * 8.585516929626465
Epoch 380, val loss: 0.9698230624198914
Epoch 390, training loss: 858.8213500976562 = 0.9609065055847168 + 100.0 * 8.578604698181152
Epoch 390, val loss: 0.9645422101020813
Epoch 400, training loss: 858.248291015625 = 0.9551148414611816 + 100.0 * 8.572931289672852
Epoch 400, val loss: 0.9590944051742554
Epoch 410, training loss: 857.7881469726562 = 0.9491530060768127 + 100.0 * 8.568389892578125
Epoch 410, val loss: 0.9534149169921875
Epoch 420, training loss: 857.1653442382812 = 0.9429923892021179 + 100.0 * 8.562223434448242
Epoch 420, val loss: 0.9476235508918762
Epoch 430, training loss: 856.7455444335938 = 0.9367304444313049 + 100.0 * 8.558088302612305
Epoch 430, val loss: 0.9417597055435181
Epoch 440, training loss: 856.3348999023438 = 0.9303562045097351 + 100.0 * 8.554045677185059
Epoch 440, val loss: 0.9357966184616089
Epoch 450, training loss: 855.9436645507812 = 0.9239028096199036 + 100.0 * 8.55019760131836
Epoch 450, val loss: 0.9297552704811096
Epoch 460, training loss: 855.5799560546875 = 0.9173489809036255 + 100.0 * 8.546626091003418
Epoch 460, val loss: 0.9236782193183899
Epoch 470, training loss: 855.3351440429688 = 0.9107460975646973 + 100.0 * 8.544243812561035
Epoch 470, val loss: 0.9175308346748352
Epoch 480, training loss: 854.962158203125 = 0.9040740728378296 + 100.0 * 8.540580749511719
Epoch 480, val loss: 0.9113645553588867
Epoch 490, training loss: 854.5781860351562 = 0.8974432945251465 + 100.0 * 8.5368070602417
Epoch 490, val loss: 0.9052498936653137
Epoch 500, training loss: 854.2317504882812 = 0.8908355236053467 + 100.0 * 8.533409118652344
Epoch 500, val loss: 0.8991878628730774
Epoch 510, training loss: 854.3908081054688 = 0.8842071294784546 + 100.0 * 8.535065650939941
Epoch 510, val loss: 0.8931456804275513
Epoch 520, training loss: 853.7703247070312 = 0.8775920271873474 + 100.0 * 8.52892780303955
Epoch 520, val loss: 0.887051522731781
Epoch 530, training loss: 853.48095703125 = 0.871028482913971 + 100.0 * 8.52609920501709
Epoch 530, val loss: 0.8810660243034363
Epoch 540, training loss: 853.2208862304688 = 0.8645612001419067 + 100.0 * 8.523563385009766
Epoch 540, val loss: 0.8751857876777649
Epoch 550, training loss: 852.9784545898438 = 0.8581904768943787 + 100.0 * 8.521202087402344
Epoch 550, val loss: 0.8694019317626953
Epoch 560, training loss: 853.0147094726562 = 0.8518661260604858 + 100.0 * 8.521628379821777
Epoch 560, val loss: 0.8636746406555176
Epoch 570, training loss: 852.66064453125 = 0.845605731010437 + 100.0 * 8.518150329589844
Epoch 570, val loss: 0.8580141067504883
Epoch 580, training loss: 852.3381958007812 = 0.8394582867622375 + 100.0 * 8.514986991882324
Epoch 580, val loss: 0.852490246295929
Epoch 590, training loss: 852.1095581054688 = 0.8334535956382751 + 100.0 * 8.512761116027832
Epoch 590, val loss: 0.847107470035553
Epoch 600, training loss: 852.2008666992188 = 0.8275473117828369 + 100.0 * 8.51373291015625
Epoch 600, val loss: 0.841812789440155
Epoch 610, training loss: 851.758056640625 = 0.8216894268989563 + 100.0 * 8.509363174438477
Epoch 610, val loss: 0.8366074562072754
Epoch 620, training loss: 851.5097045898438 = 0.8159756064414978 + 100.0 * 8.506937026977539
Epoch 620, val loss: 0.831523597240448
Epoch 630, training loss: 851.310791015625 = 0.8104338049888611 + 100.0 * 8.505003929138184
Epoch 630, val loss: 0.8266063928604126
Epoch 640, training loss: 851.0687866210938 = 0.8050193786621094 + 100.0 * 8.50263786315918
Epoch 640, val loss: 0.8218374848365784
Epoch 650, training loss: 850.86572265625 = 0.7997140288352966 + 100.0 * 8.500659942626953
Epoch 650, val loss: 0.8171619176864624
Epoch 660, training loss: 851.4305419921875 = 0.794542670249939 + 100.0 * 8.506360054016113
Epoch 660, val loss: 0.8125876784324646
Epoch 670, training loss: 850.533447265625 = 0.789261519908905 + 100.0 * 8.497442245483398
Epoch 670, val loss: 0.8079777359962463
Epoch 680, training loss: 850.3584594726562 = 0.7841877341270447 + 100.0 * 8.495742797851562
Epoch 680, val loss: 0.8035638928413391
Epoch 690, training loss: 850.1804809570312 = 0.7792697548866272 + 100.0 * 8.494011878967285
Epoch 690, val loss: 0.7993059754371643
Epoch 700, training loss: 850.0277709960938 = 0.7744421362876892 + 100.0 * 8.492533683776855
Epoch 700, val loss: 0.7951307892799377
Epoch 710, training loss: 849.939208984375 = 0.7696478366851807 + 100.0 * 8.491695404052734
Epoch 710, val loss: 0.7909930944442749
Epoch 720, training loss: 849.77490234375 = 0.764902651309967 + 100.0 * 8.490099906921387
Epoch 720, val loss: 0.7869315147399902
Epoch 730, training loss: 849.5686645507812 = 0.7602529525756836 + 100.0 * 8.488083839416504
Epoch 730, val loss: 0.7829797267913818
Epoch 740, training loss: 849.4778442382812 = 0.7557021379470825 + 100.0 * 8.487221717834473
Epoch 740, val loss: 0.7791398763656616
Epoch 750, training loss: 849.4436645507812 = 0.7512363791465759 + 100.0 * 8.486924171447754
Epoch 750, val loss: 0.7754015326499939
Epoch 760, training loss: 849.2141723632812 = 0.7468366622924805 + 100.0 * 8.484673500061035
Epoch 760, val loss: 0.7717151641845703
Epoch 770, training loss: 849.1221923828125 = 0.7425625920295715 + 100.0 * 8.483796119689941
Epoch 770, val loss: 0.7682010531425476
Epoch 780, training loss: 849.0135498046875 = 0.7383705973625183 + 100.0 * 8.482751846313477
Epoch 780, val loss: 0.7647457122802734
Epoch 790, training loss: 849.1069946289062 = 0.7342462539672852 + 100.0 * 8.48372745513916
Epoch 790, val loss: 0.7613648176193237
Epoch 800, training loss: 848.81103515625 = 0.7302097678184509 + 100.0 * 8.48080825805664
Epoch 800, val loss: 0.7581170797348022
Epoch 810, training loss: 848.7933349609375 = 0.7262693643569946 + 100.0 * 8.480670928955078
Epoch 810, val loss: 0.7549551725387573
Epoch 820, training loss: 848.6439819335938 = 0.7223914265632629 + 100.0 * 8.479215621948242
Epoch 820, val loss: 0.7518589496612549
Epoch 830, training loss: 848.5181274414062 = 0.7186279892921448 + 100.0 * 8.477994918823242
Epoch 830, val loss: 0.7488919496536255
Epoch 840, training loss: 848.4371337890625 = 0.7149761319160461 + 100.0 * 8.477221488952637
Epoch 840, val loss: 0.7460255026817322
Epoch 850, training loss: 848.358642578125 = 0.7114223837852478 + 100.0 * 8.476471900939941
Epoch 850, val loss: 0.7432555556297302
Epoch 860, training loss: 848.6445922851562 = 0.707945704460144 + 100.0 * 8.479366302490234
Epoch 860, val loss: 0.7405659556388855
Epoch 870, training loss: 848.4259643554688 = 0.7044463157653809 + 100.0 * 8.477214813232422
Epoch 870, val loss: 0.7378151416778564
Epoch 880, training loss: 848.1495361328125 = 0.701090931892395 + 100.0 * 8.47448444366455
Epoch 880, val loss: 0.7352275848388672
Epoch 890, training loss: 848.0345458984375 = 0.6978736519813538 + 100.0 * 8.473366737365723
Epoch 890, val loss: 0.7327834963798523
Epoch 900, training loss: 847.9674682617188 = 0.6947553157806396 + 100.0 * 8.472726821899414
Epoch 900, val loss: 0.7304234504699707
Epoch 910, training loss: 848.3720092773438 = 0.6916897296905518 + 100.0 * 8.476802825927734
Epoch 910, val loss: 0.7280648350715637
Epoch 920, training loss: 847.9285278320312 = 0.6886727809906006 + 100.0 * 8.47239875793457
Epoch 920, val loss: 0.7257919311523438
Epoch 930, training loss: 847.7677612304688 = 0.6857460141181946 + 100.0 * 8.470820426940918
Epoch 930, val loss: 0.7235901951789856
Epoch 940, training loss: 847.6837768554688 = 0.6829402446746826 + 100.0 * 8.470008850097656
Epoch 940, val loss: 0.7214697003364563
Epoch 950, training loss: 847.6216430664062 = 0.680243194103241 + 100.0 * 8.469413757324219
Epoch 950, val loss: 0.7194589376449585
Epoch 960, training loss: 847.9032592773438 = 0.6775939464569092 + 100.0 * 8.472256660461426
Epoch 960, val loss: 0.717450737953186
Epoch 970, training loss: 847.5459594726562 = 0.6749933362007141 + 100.0 * 8.468709945678711
Epoch 970, val loss: 0.7155360579490662
Epoch 980, training loss: 847.4047241210938 = 0.6724873185157776 + 100.0 * 8.46732234954834
Epoch 980, val loss: 0.7136550545692444
Epoch 990, training loss: 847.3467407226562 = 0.6700897216796875 + 100.0 * 8.466766357421875
Epoch 990, val loss: 0.7118806838989258
Epoch 1000, training loss: 847.3345336914062 = 0.667765736579895 + 100.0 * 8.466667175292969
Epoch 1000, val loss: 0.710184633731842
Epoch 1010, training loss: 847.27734375 = 0.6654568314552307 + 100.0 * 8.466118812561035
Epoch 1010, val loss: 0.7083876729011536
Epoch 1020, training loss: 847.1954345703125 = 0.6631985306739807 + 100.0 * 8.465322494506836
Epoch 1020, val loss: 0.7067467570304871
Epoch 1030, training loss: 847.2257690429688 = 0.6610283851623535 + 100.0 * 8.46564769744873
Epoch 1030, val loss: 0.7050433158874512
Epoch 1040, training loss: 847.056884765625 = 0.6589211225509644 + 100.0 * 8.463979721069336
Epoch 1040, val loss: 0.7034525275230408
Epoch 1050, training loss: 847.0194702148438 = 0.6568837761878967 + 100.0 * 8.46362590789795
Epoch 1050, val loss: 0.70194011926651
Epoch 1060, training loss: 846.947021484375 = 0.6549078822135925 + 100.0 * 8.462921142578125
Epoch 1060, val loss: 0.700420081615448
Epoch 1070, training loss: 847.3505859375 = 0.6529774069786072 + 100.0 * 8.466976165771484
Epoch 1070, val loss: 0.6988911032676697
Epoch 1080, training loss: 847.0614624023438 = 0.65101158618927 + 100.0 * 8.464104652404785
Epoch 1080, val loss: 0.6974436044692993
Epoch 1090, training loss: 846.8114013671875 = 0.6491405367851257 + 100.0 * 8.46162223815918
Epoch 1090, val loss: 0.6959784030914307
Epoch 1100, training loss: 846.7526245117188 = 0.647368311882019 + 100.0 * 8.461052894592285
Epoch 1100, val loss: 0.6945794820785522
Epoch 1110, training loss: 846.688720703125 = 0.6456575393676758 + 100.0 * 8.460431098937988
Epoch 1110, val loss: 0.6932399272918701
Epoch 1120, training loss: 846.6397705078125 = 0.643994152545929 + 100.0 * 8.45995807647705
Epoch 1120, val loss: 0.6919593811035156
Epoch 1130, training loss: 846.6744995117188 = 0.6423608064651489 + 100.0 * 8.460321426391602
Epoch 1130, val loss: 0.6906431913375854
Epoch 1140, training loss: 846.5986328125 = 0.640664279460907 + 100.0 * 8.459579467773438
Epoch 1140, val loss: 0.6892484426498413
Epoch 1150, training loss: 846.5289306640625 = 0.638997495174408 + 100.0 * 8.45889949798584
Epoch 1150, val loss: 0.6879163384437561
Epoch 1160, training loss: 846.484375 = 0.637448251247406 + 100.0 * 8.45846939086914
Epoch 1160, val loss: 0.6866698861122131
Epoch 1170, training loss: 846.4286499023438 = 0.6359766125679016 + 100.0 * 8.457926750183105
Epoch 1170, val loss: 0.6855001449584961
Epoch 1180, training loss: 846.3682861328125 = 0.6345476508140564 + 100.0 * 8.457337379455566
Epoch 1180, val loss: 0.6843429803848267
Epoch 1190, training loss: 846.3214721679688 = 0.6331472396850586 + 100.0 * 8.456883430480957
Epoch 1190, val loss: 0.6832085847854614
Epoch 1200, training loss: 846.7556762695312 = 0.6317664384841919 + 100.0 * 8.461238861083984
Epoch 1200, val loss: 0.6821221709251404
Epoch 1210, training loss: 846.4321899414062 = 0.6302794814109802 + 100.0 * 8.458019256591797
Epoch 1210, val loss: 0.6808131337165833
Epoch 1220, training loss: 846.2230224609375 = 0.6288993954658508 + 100.0 * 8.455941200256348
Epoch 1220, val loss: 0.679714024066925
Epoch 1230, training loss: 846.165771484375 = 0.6275825500488281 + 100.0 * 8.455382347106934
Epoch 1230, val loss: 0.678579568862915
Epoch 1240, training loss: 846.1287231445312 = 0.6262970566749573 + 100.0 * 8.455024719238281
Epoch 1240, val loss: 0.677532970905304
Epoch 1250, training loss: 846.420166015625 = 0.6250128746032715 + 100.0 * 8.457951545715332
Epoch 1250, val loss: 0.6764206290245056
Epoch 1260, training loss: 846.15234375 = 0.6236968040466309 + 100.0 * 8.455286026000977
Epoch 1260, val loss: 0.6753823757171631
Epoch 1270, training loss: 846.0477294921875 = 0.6224300265312195 + 100.0 * 8.454253196716309
Epoch 1270, val loss: 0.6742727756500244
Epoch 1280, training loss: 845.973876953125 = 0.6212044954299927 + 100.0 * 8.453526496887207
Epoch 1280, val loss: 0.6732781529426575
Epoch 1290, training loss: 845.9243774414062 = 0.6200065016746521 + 100.0 * 8.453043937683105
Epoch 1290, val loss: 0.6722474098205566
Epoch 1300, training loss: 845.8838500976562 = 0.6188277006149292 + 100.0 * 8.45265007019043
Epoch 1300, val loss: 0.671283483505249
Epoch 1310, training loss: 846.250732421875 = 0.6176480650901794 + 100.0 * 8.456330299377441
Epoch 1310, val loss: 0.6703795194625854
Epoch 1320, training loss: 846.0264282226562 = 0.616393506526947 + 100.0 * 8.454100608825684
Epoch 1320, val loss: 0.6691311597824097
Epoch 1330, training loss: 845.781005859375 = 0.6151844263076782 + 100.0 * 8.451658248901367
Epoch 1330, val loss: 0.6681233644485474
Epoch 1340, training loss: 845.7691650390625 = 0.6140384674072266 + 100.0 * 8.45155143737793
Epoch 1340, val loss: 0.667203426361084
Epoch 1350, training loss: 845.716064453125 = 0.6129207015037537 + 100.0 * 8.451031684875488
Epoch 1350, val loss: 0.6662065982818604
Epoch 1360, training loss: 845.8512573242188 = 0.6118083596229553 + 100.0 * 8.452394485473633
Epoch 1360, val loss: 0.6651742458343506
Epoch 1370, training loss: 845.669677734375 = 0.6106659173965454 + 100.0 * 8.450590133666992
Epoch 1370, val loss: 0.6643586158752441
Epoch 1380, training loss: 845.6328735351562 = 0.609541654586792 + 100.0 * 8.450233459472656
Epoch 1380, val loss: 0.6633049249649048
Epoch 1390, training loss: 845.5925903320312 = 0.6084556579589844 + 100.0 * 8.449841499328613
Epoch 1390, val loss: 0.6624475121498108
Epoch 1400, training loss: 846.0646362304688 = 0.607367753982544 + 100.0 * 8.454572677612305
Epoch 1400, val loss: 0.6614634394645691
Epoch 1410, training loss: 845.8472900390625 = 0.6061863303184509 + 100.0 * 8.452410697937012
Epoch 1410, val loss: 0.6604763865470886
Epoch 1420, training loss: 845.4931640625 = 0.6050512194633484 + 100.0 * 8.448881149291992
Epoch 1420, val loss: 0.659530520439148
Epoch 1430, training loss: 845.4887084960938 = 0.6039820313453674 + 100.0 * 8.448846817016602
Epoch 1430, val loss: 0.6585775017738342
Epoch 1440, training loss: 845.4244995117188 = 0.6029459238052368 + 100.0 * 8.44821548461914
Epoch 1440, val loss: 0.6577212810516357
Epoch 1450, training loss: 845.5386962890625 = 0.6019077301025391 + 100.0 * 8.44936752319336
Epoch 1450, val loss: 0.6568477153778076
Epoch 1460, training loss: 845.3585815429688 = 0.6008208394050598 + 100.0 * 8.447577476501465
Epoch 1460, val loss: 0.6559281349182129
Epoch 1470, training loss: 845.32275390625 = 0.5997522473335266 + 100.0 * 8.447230339050293
Epoch 1470, val loss: 0.6549981832504272
Epoch 1480, training loss: 845.3079223632812 = 0.5987176895141602 + 100.0 * 8.447092056274414
Epoch 1480, val loss: 0.654140055179596
Epoch 1490, training loss: 845.531494140625 = 0.5976929068565369 + 100.0 * 8.44933795928955
Epoch 1490, val loss: 0.6532646417617798
Epoch 1500, training loss: 845.2892456054688 = 0.5965983271598816 + 100.0 * 8.44692611694336
Epoch 1500, val loss: 0.6523348093032837
Epoch 1510, training loss: 845.221435546875 = 0.5955655574798584 + 100.0 * 8.446258544921875
Epoch 1510, val loss: 0.6514415144920349
Epoch 1520, training loss: 845.16748046875 = 0.5945444107055664 + 100.0 * 8.44572925567627
Epoch 1520, val loss: 0.6505958437919617
Epoch 1530, training loss: 845.13232421875 = 0.5935376882553101 + 100.0 * 8.445387840270996
Epoch 1530, val loss: 0.6497435569763184
Epoch 1540, training loss: 845.1663208007812 = 0.5925344228744507 + 100.0 * 8.445737838745117
Epoch 1540, val loss: 0.6489617228507996
Epoch 1550, training loss: 845.2442626953125 = 0.5914760231971741 + 100.0 * 8.446527481079102
Epoch 1550, val loss: 0.6480375528335571
Epoch 1560, training loss: 845.0470581054688 = 0.5903891324996948 + 100.0 * 8.44456672668457
Epoch 1560, val loss: 0.6471300721168518
Epoch 1570, training loss: 845.0892333984375 = 0.5893636345863342 + 100.0 * 8.444998741149902
Epoch 1570, val loss: 0.6462071537971497
Epoch 1580, training loss: 845.1276245117188 = 0.5883495211601257 + 100.0 * 8.445392608642578
Epoch 1580, val loss: 0.6454368829727173
Epoch 1590, training loss: 844.9736328125 = 0.5873236060142517 + 100.0 * 8.443862915039062
Epoch 1590, val loss: 0.6445142030715942
Epoch 1600, training loss: 844.9459838867188 = 0.5863159894943237 + 100.0 * 8.443596839904785
Epoch 1600, val loss: 0.6436852216720581
Epoch 1610, training loss: 844.939208984375 = 0.5853121876716614 + 100.0 * 8.443538665771484
Epoch 1610, val loss: 0.6429256200790405
Epoch 1620, training loss: 844.9567260742188 = 0.5842970609664917 + 100.0 * 8.443724632263184
Epoch 1620, val loss: 0.6420199871063232
Epoch 1630, training loss: 844.9227905273438 = 0.58327716588974 + 100.0 * 8.443395614624023
Epoch 1630, val loss: 0.6411718130111694
Epoch 1640, training loss: 844.8988647460938 = 0.5822523832321167 + 100.0 * 8.44316577911377
Epoch 1640, val loss: 0.6404555439949036
Epoch 1650, training loss: 844.9219360351562 = 0.5812332034111023 + 100.0 * 8.44340705871582
Epoch 1650, val loss: 0.6395475268363953
Epoch 1660, training loss: 844.7990112304688 = 0.5802026987075806 + 100.0 * 8.442188262939453
Epoch 1660, val loss: 0.6387850046157837
Epoch 1670, training loss: 844.716064453125 = 0.5791817903518677 + 100.0 * 8.44136905670166
Epoch 1670, val loss: 0.6378908753395081
Epoch 1680, training loss: 844.6659545898438 = 0.5782055258750916 + 100.0 * 8.440877914428711
Epoch 1680, val loss: 0.6371755003929138
Epoch 1690, training loss: 844.62255859375 = 0.5772340893745422 + 100.0 * 8.44045352935791
Epoch 1690, val loss: 0.6363577842712402
Epoch 1700, training loss: 844.5764770507812 = 0.576260507106781 + 100.0 * 8.44000244140625
Epoch 1700, val loss: 0.6356154084205627
Epoch 1710, training loss: 844.6949462890625 = 0.575291633605957 + 100.0 * 8.44119644165039
Epoch 1710, val loss: 0.6348921060562134
Epoch 1720, training loss: 844.5670166015625 = 0.5741976499557495 + 100.0 * 8.43992805480957
Epoch 1720, val loss: 0.6338942646980286
Epoch 1730, training loss: 844.6489868164062 = 0.5731276273727417 + 100.0 * 8.44075870513916
Epoch 1730, val loss: 0.6331151723861694
Epoch 1740, training loss: 844.4602661132812 = 0.5721054673194885 + 100.0 * 8.438881874084473
Epoch 1740, val loss: 0.6322807669639587
Epoch 1750, training loss: 844.4461059570312 = 0.5711223483085632 + 100.0 * 8.438750267028809
Epoch 1750, val loss: 0.631492555141449
Epoch 1760, training loss: 844.401123046875 = 0.5701529383659363 + 100.0 * 8.438309669494629
Epoch 1760, val loss: 0.6307650804519653
Epoch 1770, training loss: 844.5470581054688 = 0.5691729784011841 + 100.0 * 8.439779281616211
Epoch 1770, val loss: 0.6300261616706848
Epoch 1780, training loss: 844.3787841796875 = 0.5681034922599792 + 100.0 * 8.438106536865234
Epoch 1780, val loss: 0.6291046738624573
Epoch 1790, training loss: 844.4432983398438 = 0.5670332312583923 + 100.0 * 8.438762664794922
Epoch 1790, val loss: 0.6283152103424072
Epoch 1800, training loss: 844.3599243164062 = 0.5660156607627869 + 100.0 * 8.437938690185547
Epoch 1800, val loss: 0.627440333366394
Epoch 1810, training loss: 844.2771606445312 = 0.5650282502174377 + 100.0 * 8.437121391296387
Epoch 1810, val loss: 0.6266582608222961
Epoch 1820, training loss: 844.3116455078125 = 0.5640517473220825 + 100.0 * 8.43747615814209
Epoch 1820, val loss: 0.6258825063705444
Epoch 1830, training loss: 844.2732543945312 = 0.5630518794059753 + 100.0 * 8.437102317810059
Epoch 1830, val loss: 0.625117301940918
Epoch 1840, training loss: 844.2252197265625 = 0.5620523691177368 + 100.0 * 8.436631202697754
Epoch 1840, val loss: 0.6244415044784546
Epoch 1850, training loss: 844.5109252929688 = 0.5610572695732117 + 100.0 * 8.439498901367188
Epoch 1850, val loss: 0.6236894726753235
Epoch 1860, training loss: 844.3353881835938 = 0.5599722862243652 + 100.0 * 8.437753677368164
Epoch 1860, val loss: 0.6226494312286377
Epoch 1870, training loss: 844.2029418945312 = 0.5589228272438049 + 100.0 * 8.436440467834473
Epoch 1870, val loss: 0.6219809055328369
Epoch 1880, training loss: 844.0775146484375 = 0.5579163432121277 + 100.0 * 8.435195922851562
Epoch 1880, val loss: 0.6211824417114258
Epoch 1890, training loss: 844.0462036132812 = 0.556932270526886 + 100.0 * 8.434892654418945
Epoch 1890, val loss: 0.6204223036766052
Epoch 1900, training loss: 844.0284423828125 = 0.5559502243995667 + 100.0 * 8.434724807739258
Epoch 1900, val loss: 0.6197103261947632
Epoch 1910, training loss: 844.1492919921875 = 0.5549436807632446 + 100.0 * 8.435943603515625
Epoch 1910, val loss: 0.6188947558403015
Epoch 1920, training loss: 843.9808959960938 = 0.5538864731788635 + 100.0 * 8.434269905090332
Epoch 1920, val loss: 0.6181899905204773
Epoch 1930, training loss: 843.94921875 = 0.5528382062911987 + 100.0 * 8.433963775634766
Epoch 1930, val loss: 0.6173363327980042
Epoch 1940, training loss: 844.0025634765625 = 0.5518092513084412 + 100.0 * 8.434507369995117
Epoch 1940, val loss: 0.6165262460708618
Epoch 1950, training loss: 844.0706176757812 = 0.5507506728172302 + 100.0 * 8.435198783874512
Epoch 1950, val loss: 0.6157647967338562
Epoch 1960, training loss: 843.9729614257812 = 0.549686074256897 + 100.0 * 8.434232711791992
Epoch 1960, val loss: 0.6151183843612671
Epoch 1970, training loss: 843.8880004882812 = 0.5486469268798828 + 100.0 * 8.433393478393555
Epoch 1970, val loss: 0.6142536997795105
Epoch 1980, training loss: 843.8530883789062 = 0.547633707523346 + 100.0 * 8.43305492401123
Epoch 1980, val loss: 0.6136261224746704
Epoch 1990, training loss: 843.9122924804688 = 0.5466256737709045 + 100.0 * 8.433656692504883
Epoch 1990, val loss: 0.612875759601593
Epoch 2000, training loss: 843.842529296875 = 0.5455781817436218 + 100.0 * 8.432969093322754
Epoch 2000, val loss: 0.6121041774749756
Epoch 2010, training loss: 844.0775756835938 = 0.5445290207862854 + 100.0 * 8.435330390930176
Epoch 2010, val loss: 0.6114388108253479
Epoch 2020, training loss: 843.8807373046875 = 0.543463408946991 + 100.0 * 8.433372497558594
Epoch 2020, val loss: 0.610680341720581
Epoch 2030, training loss: 843.7620849609375 = 0.5424008369445801 + 100.0 * 8.432196617126465
Epoch 2030, val loss: 0.6099854707717896
Epoch 2040, training loss: 843.6983032226562 = 0.5413751602172852 + 100.0 * 8.43156909942627
Epoch 2040, val loss: 0.6091795563697815
Epoch 2050, training loss: 843.6802978515625 = 0.5403580069541931 + 100.0 * 8.43139934539795
Epoch 2050, val loss: 0.6085603833198547
Epoch 2060, training loss: 843.9363403320312 = 0.5393347144126892 + 100.0 * 8.43397045135498
Epoch 2060, val loss: 0.6078094244003296
Epoch 2070, training loss: 843.6400756835938 = 0.5382428169250488 + 100.0 * 8.431017875671387
Epoch 2070, val loss: 0.6072103977203369
Epoch 2080, training loss: 843.6018676757812 = 0.5371794700622559 + 100.0 * 8.430646896362305
Epoch 2080, val loss: 0.6064223647117615
Epoch 2090, training loss: 843.6905517578125 = 0.5361394286155701 + 100.0 * 8.431544303894043
Epoch 2090, val loss: 0.6057816743850708
Epoch 2100, training loss: 843.7881469726562 = 0.5350558161735535 + 100.0 * 8.432531356811523
Epoch 2100, val loss: 0.6049094200134277
Epoch 2110, training loss: 843.571044921875 = 0.5339364409446716 + 100.0 * 8.430371284484863
Epoch 2110, val loss: 0.6043314337730408
Epoch 2120, training loss: 843.5289306640625 = 0.5328859686851501 + 100.0 * 8.429960250854492
Epoch 2120, val loss: 0.6036049127578735
Epoch 2130, training loss: 843.4857177734375 = 0.5318605899810791 + 100.0 * 8.42953872680664
Epoch 2130, val loss: 0.6030071973800659
Epoch 2140, training loss: 843.4536743164062 = 0.5308433175086975 + 100.0 * 8.429228782653809
Epoch 2140, val loss: 0.602306067943573
Epoch 2150, training loss: 843.4803466796875 = 0.5298144817352295 + 100.0 * 8.429505348205566
Epoch 2150, val loss: 0.601606011390686
Epoch 2160, training loss: 843.7669067382812 = 0.5287460088729858 + 100.0 * 8.432381629943848
Epoch 2160, val loss: 0.6009007692337036
Epoch 2170, training loss: 843.5339965820312 = 0.5276307463645935 + 100.0 * 8.430063247680664
Epoch 2170, val loss: 0.6003487706184387
Epoch 2180, training loss: 843.414794921875 = 0.52654629945755 + 100.0 * 8.428882598876953
Epoch 2180, val loss: 0.5996076464653015
Epoch 2190, training loss: 843.415283203125 = 0.5254988074302673 + 100.0 * 8.428897857666016
Epoch 2190, val loss: 0.599073052406311
Epoch 2200, training loss: 843.5294189453125 = 0.5244368314743042 + 100.0 * 8.430049896240234
Epoch 2200, val loss: 0.598361611366272
Epoch 2210, training loss: 843.4024047851562 = 0.5233487486839294 + 100.0 * 8.428790092468262
Epoch 2210, val loss: 0.5976084470748901
Epoch 2220, training loss: 843.3470458984375 = 0.522257387638092 + 100.0 * 8.428247451782227
Epoch 2220, val loss: 0.5970200896263123
Epoch 2230, training loss: 843.3425903320312 = 0.5211765766143799 + 100.0 * 8.428214073181152
Epoch 2230, val loss: 0.5963571071624756
Epoch 2240, training loss: 843.4093017578125 = 0.5200986266136169 + 100.0 * 8.428892135620117
Epoch 2240, val loss: 0.5956085920333862
Epoch 2250, training loss: 843.2722778320312 = 0.5190050601959229 + 100.0 * 8.427533149719238
Epoch 2250, val loss: 0.5949925184249878
Epoch 2260, training loss: 843.25341796875 = 0.5179418921470642 + 100.0 * 8.42735481262207
Epoch 2260, val loss: 0.5943456292152405
Epoch 2270, training loss: 843.20556640625 = 0.5168682336807251 + 100.0 * 8.426887512207031
Epoch 2270, val loss: 0.5937222242355347
Epoch 2280, training loss: 843.391357421875 = 0.5157948732376099 + 100.0 * 8.428755760192871
Epoch 2280, val loss: 0.5930360555648804
Epoch 2290, training loss: 843.3191528320312 = 0.514670193195343 + 100.0 * 8.428045272827148
Epoch 2290, val loss: 0.5926625728607178
Epoch 2300, training loss: 843.1493530273438 = 0.5135293006896973 + 100.0 * 8.426358222961426
Epoch 2300, val loss: 0.5917884707450867
Epoch 2310, training loss: 843.1312255859375 = 0.5124244689941406 + 100.0 * 8.426187515258789
Epoch 2310, val loss: 0.591270923614502
Epoch 2320, training loss: 843.07421875 = 0.5113499760627747 + 100.0 * 8.425628662109375
Epoch 2320, val loss: 0.5906238555908203
Epoch 2330, training loss: 843.2679443359375 = 0.5102774500846863 + 100.0 * 8.427577018737793
Epoch 2330, val loss: 0.5901051759719849
Epoch 2340, training loss: 843.0333251953125 = 0.5091400742530823 + 100.0 * 8.425241470336914
Epoch 2340, val loss: 0.5894019603729248
Epoch 2350, training loss: 843.009033203125 = 0.5080306529998779 + 100.0 * 8.425009727478027
Epoch 2350, val loss: 0.5888190269470215
Epoch 2360, training loss: 842.9991455078125 = 0.5069498419761658 + 100.0 * 8.424921989440918
Epoch 2360, val loss: 0.5881651639938354
Epoch 2370, training loss: 843.2340087890625 = 0.5058749318122864 + 100.0 * 8.427281379699707
Epoch 2370, val loss: 0.5875323414802551
Epoch 2380, training loss: 842.9639892578125 = 0.5047318339347839 + 100.0 * 8.424592018127441
Epoch 2380, val loss: 0.5870016813278198
Epoch 2390, training loss: 842.9281616210938 = 0.503619372844696 + 100.0 * 8.424245834350586
Epoch 2390, val loss: 0.586311936378479
Epoch 2400, training loss: 842.9105834960938 = 0.5025320053100586 + 100.0 * 8.424080848693848
Epoch 2400, val loss: 0.5857689380645752
Epoch 2410, training loss: 843.0017700195312 = 0.5014545321464539 + 100.0 * 8.425003051757812
Epoch 2410, val loss: 0.5851413011550903
Epoch 2420, training loss: 842.9325561523438 = 0.5003443360328674 + 100.0 * 8.424322128295898
Epoch 2420, val loss: 0.5845074653625488
Epoch 2430, training loss: 842.8932495117188 = 0.49924740195274353 + 100.0 * 8.42393970489502
Epoch 2430, val loss: 0.5839252471923828
Epoch 2440, training loss: 842.861328125 = 0.49816152453422546 + 100.0 * 8.42363166809082
Epoch 2440, val loss: 0.5832799077033997
Epoch 2450, training loss: 843.0804443359375 = 0.49705609679222107 + 100.0 * 8.425833702087402
Epoch 2450, val loss: 0.5826594829559326
Epoch 2460, training loss: 842.9801635742188 = 0.49591097235679626 + 100.0 * 8.424842834472656
Epoch 2460, val loss: 0.5824131965637207
Epoch 2470, training loss: 842.816650390625 = 0.4947795569896698 + 100.0 * 8.423218727111816
Epoch 2470, val loss: 0.5815582871437073
Epoch 2480, training loss: 842.7457885742188 = 0.49369147419929504 + 100.0 * 8.422520637512207
Epoch 2480, val loss: 0.5811247825622559
Epoch 2490, training loss: 842.7363891601562 = 0.49262234568595886 + 100.0 * 8.42243766784668
Epoch 2490, val loss: 0.5804325342178345
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7549467275494672
0.8139534883720931
=== training gcn model ===
Epoch 0, training loss: 1059.3197021484375 = 1.0924229621887207 + 100.0 * 10.582273483276367
Epoch 0, val loss: 1.0918091535568237
Epoch 10, training loss: 1059.2763671875 = 1.0882678031921387 + 100.0 * 10.581880569458008
Epoch 10, val loss: 1.0876023769378662
Epoch 20, training loss: 1059.09130859375 = 1.0836597681045532 + 100.0 * 10.580077171325684
Epoch 20, val loss: 1.0829532146453857
Epoch 30, training loss: 1058.2703857421875 = 1.0786100625991821 + 100.0 * 10.571917533874512
Epoch 30, val loss: 1.0778754949569702
Epoch 40, training loss: 1055.2264404296875 = 1.0731202363967896 + 100.0 * 10.541533470153809
Epoch 40, val loss: 1.0723329782485962
Epoch 50, training loss: 1046.5220947265625 = 1.066989779472351 + 100.0 * 10.454550743103027
Epoch 50, val loss: 1.0662024021148682
Epoch 60, training loss: 1024.8458251953125 = 1.0605955123901367 + 100.0 * 10.237852096557617
Epoch 60, val loss: 1.0598257780075073
Epoch 70, training loss: 997.6925659179688 = 1.0532771348953247 + 100.0 * 9.966392517089844
Epoch 70, val loss: 1.0526885986328125
Epoch 80, training loss: 977.5433349609375 = 1.047981858253479 + 100.0 * 9.76495361328125
Epoch 80, val loss: 1.0478777885437012
Epoch 90, training loss: 960.7815551757812 = 1.0450167655944824 + 100.0 * 9.597365379333496
Epoch 90, val loss: 1.0454670190811157
Epoch 100, training loss: 943.5400390625 = 1.0441652536392212 + 100.0 * 9.424958229064941
Epoch 100, val loss: 1.044994592666626
Epoch 110, training loss: 930.7572631835938 = 1.0432547330856323 + 100.0 * 9.297140121459961
Epoch 110, val loss: 1.0441235303878784
Epoch 120, training loss: 925.5338745117188 = 1.0403003692626953 + 100.0 * 9.244935989379883
Epoch 120, val loss: 1.0411510467529297
Epoch 130, training loss: 916.0105590820312 = 1.038133144378662 + 100.0 * 9.149724006652832
Epoch 130, val loss: 1.039238691329956
Epoch 140, training loss: 902.283935546875 = 1.0371756553649902 + 100.0 * 9.012467384338379
Epoch 140, val loss: 1.038489580154419
Epoch 150, training loss: 890.1404418945312 = 1.036354660987854 + 100.0 * 8.891040802001953
Epoch 150, val loss: 1.0376747846603394
Epoch 160, training loss: 883.708251953125 = 1.034454107284546 + 100.0 * 8.826738357543945
Epoch 160, val loss: 1.0357537269592285
Epoch 170, training loss: 879.3641357421875 = 1.0317715406417847 + 100.0 * 8.783323287963867
Epoch 170, val loss: 1.0331212282180786
Epoch 180, training loss: 876.2949829101562 = 1.0290896892547607 + 100.0 * 8.75265884399414
Epoch 180, val loss: 1.0305838584899902
Epoch 190, training loss: 874.2545776367188 = 1.0264103412628174 + 100.0 * 8.732281684875488
Epoch 190, val loss: 1.0280272960662842
Epoch 200, training loss: 872.809814453125 = 1.0233075618743896 + 100.0 * 8.717864990234375
Epoch 200, val loss: 1.0250964164733887
Epoch 210, training loss: 871.53173828125 = 1.0200235843658447 + 100.0 * 8.705117225646973
Epoch 210, val loss: 1.0220781564712524
Epoch 220, training loss: 870.4486083984375 = 1.0166795253753662 + 100.0 * 8.694319725036621
Epoch 220, val loss: 1.0189573764801025
Epoch 230, training loss: 869.3576049804688 = 1.0131585597991943 + 100.0 * 8.683444023132324
Epoch 230, val loss: 1.0156539678573608
Epoch 240, training loss: 868.2639770507812 = 1.0094728469848633 + 100.0 * 8.672545433044434
Epoch 240, val loss: 1.0122003555297852
Epoch 250, training loss: 867.478515625 = 1.00559401512146 + 100.0 * 8.664729118347168
Epoch 250, val loss: 1.008602261543274
Epoch 260, training loss: 866.2528076171875 = 1.0015263557434082 + 100.0 * 8.652512550354004
Epoch 260, val loss: 1.0047824382781982
Epoch 270, training loss: 865.33984375 = 0.9972396492958069 + 100.0 * 8.643425941467285
Epoch 270, val loss: 1.0007658004760742
Epoch 280, training loss: 864.5073852539062 = 0.9926838874816895 + 100.0 * 8.635147094726562
Epoch 280, val loss: 0.9965077638626099
Epoch 290, training loss: 863.7650756835938 = 0.9878659844398499 + 100.0 * 8.627772331237793
Epoch 290, val loss: 0.9920032620429993
Epoch 300, training loss: 862.8673095703125 = 0.9827919602394104 + 100.0 * 8.618844985961914
Epoch 300, val loss: 0.9872687458992004
Epoch 310, training loss: 862.0017700195312 = 0.9774729609489441 + 100.0 * 8.61024284362793
Epoch 310, val loss: 0.9823518991470337
Epoch 320, training loss: 861.2435913085938 = 0.9719372391700745 + 100.0 * 8.602716445922852
Epoch 320, val loss: 0.9772043228149414
Epoch 330, training loss: 860.5458984375 = 0.966127336025238 + 100.0 * 8.595797538757324
Epoch 330, val loss: 0.9717856049537659
Epoch 340, training loss: 859.9098510742188 = 0.9600359797477722 + 100.0 * 8.589498519897461
Epoch 340, val loss: 0.9661624431610107
Epoch 350, training loss: 859.3054809570312 = 0.9537047147750854 + 100.0 * 8.583518028259277
Epoch 350, val loss: 0.960320234298706
Epoch 360, training loss: 858.88916015625 = 0.9471008777618408 + 100.0 * 8.579421043395996
Epoch 360, val loss: 0.9542074799537659
Epoch 370, training loss: 858.3272094726562 = 0.9402468800544739 + 100.0 * 8.573869705200195
Epoch 370, val loss: 0.9478582143783569
Epoch 380, training loss: 857.8633422851562 = 0.9331566691398621 + 100.0 * 8.56930160522461
Epoch 380, val loss: 0.9413226842880249
Epoch 390, training loss: 858.1575927734375 = 0.9259108901023865 + 100.0 * 8.572317123413086
Epoch 390, val loss: 0.9345592856407166
Epoch 400, training loss: 857.3290405273438 = 0.9183130264282227 + 100.0 * 8.564106941223145
Epoch 400, val loss: 0.9276328086853027
Epoch 410, training loss: 856.7782592773438 = 0.9106799960136414 + 100.0 * 8.558675765991211
Epoch 410, val loss: 0.920714259147644
Epoch 420, training loss: 856.4393920898438 = 0.9029520750045776 + 100.0 * 8.555364608764648
Epoch 420, val loss: 0.9136711955070496
Epoch 430, training loss: 856.066162109375 = 0.8951681852340698 + 100.0 * 8.55171012878418
Epoch 430, val loss: 0.9065759181976318
Epoch 440, training loss: 855.728759765625 = 0.887359082698822 + 100.0 * 8.54841423034668
Epoch 440, val loss: 0.8994856476783752
Epoch 450, training loss: 855.445068359375 = 0.8795276880264282 + 100.0 * 8.545655250549316
Epoch 450, val loss: 0.8924068808555603
Epoch 460, training loss: 855.5158081054688 = 0.8715958595275879 + 100.0 * 8.546442031860352
Epoch 460, val loss: 0.8852335214614868
Epoch 470, training loss: 854.861572265625 = 0.8637458086013794 + 100.0 * 8.53997802734375
Epoch 470, val loss: 0.8781811594963074
Epoch 480, training loss: 854.483154296875 = 0.8559767603874207 + 100.0 * 8.536272048950195
Epoch 480, val loss: 0.8712158799171448
Epoch 490, training loss: 854.199951171875 = 0.8482608795166016 + 100.0 * 8.533516883850098
Epoch 490, val loss: 0.8643025755882263
Epoch 500, training loss: 854.1226806640625 = 0.8406428098678589 + 100.0 * 8.532820701599121
Epoch 500, val loss: 0.8574879169464111
Epoch 510, training loss: 853.759521484375 = 0.8330695033073425 + 100.0 * 8.529264450073242
Epoch 510, val loss: 0.850783109664917
Epoch 520, training loss: 853.4752807617188 = 0.8256171941757202 + 100.0 * 8.526496887207031
Epoch 520, val loss: 0.8441051840782166
Epoch 530, training loss: 853.1978149414062 = 0.8183112740516663 + 100.0 * 8.523795127868652
Epoch 530, val loss: 0.8376510143280029
Epoch 540, training loss: 852.9041748046875 = 0.8111929297447205 + 100.0 * 8.520929336547852
Epoch 540, val loss: 0.8313335180282593
Epoch 550, training loss: 852.6297607421875 = 0.8042386174201965 + 100.0 * 8.518255233764648
Epoch 550, val loss: 0.8251968622207642
Epoch 560, training loss: 852.3899536132812 = 0.7974512577056885 + 100.0 * 8.515925407409668
Epoch 560, val loss: 0.8192158341407776
Epoch 570, training loss: 852.1868286132812 = 0.7908382415771484 + 100.0 * 8.513959884643555
Epoch 570, val loss: 0.8133962750434875
Epoch 580, training loss: 852.4815063476562 = 0.7843905687332153 + 100.0 * 8.516971588134766
Epoch 580, val loss: 0.8076871633529663
Epoch 590, training loss: 851.9530639648438 = 0.7780395150184631 + 100.0 * 8.511750221252441
Epoch 590, val loss: 0.8021302223205566
Epoch 600, training loss: 851.66796875 = 0.7719534039497375 + 100.0 * 8.508959770202637
Epoch 600, val loss: 0.7968153357505798
Epoch 610, training loss: 851.4693603515625 = 0.7660711407661438 + 100.0 * 8.507033348083496
Epoch 610, val loss: 0.7916842103004456
Epoch 620, training loss: 851.4359741210938 = 0.7603877186775208 + 100.0 * 8.506755828857422
Epoch 620, val loss: 0.7867597341537476
Epoch 630, training loss: 851.4451904296875 = 0.75483238697052 + 100.0 * 8.506903648376465
Epoch 630, val loss: 0.7818585634231567
Epoch 640, training loss: 851.1365356445312 = 0.7495051026344299 + 100.0 * 8.503870010375977
Epoch 640, val loss: 0.7772394418716431
Epoch 650, training loss: 850.935546875 = 0.7444345355033875 + 100.0 * 8.501911163330078
Epoch 650, val loss: 0.7728481292724609
Epoch 660, training loss: 850.7965698242188 = 0.7395662665367126 + 100.0 * 8.500570297241211
Epoch 660, val loss: 0.7686609625816345
Epoch 670, training loss: 850.6707153320312 = 0.7349140048027039 + 100.0 * 8.499358177185059
Epoch 670, val loss: 0.7646565437316895
Epoch 680, training loss: 850.56689453125 = 0.7304511070251465 + 100.0 * 8.498364448547363
Epoch 680, val loss: 0.7608239054679871
Epoch 690, training loss: 850.9873657226562 = 0.7261620163917542 + 100.0 * 8.502612113952637
Epoch 690, val loss: 0.757136881351471
Epoch 700, training loss: 850.5107421875 = 0.7219803929328918 + 100.0 * 8.49788761138916
Epoch 700, val loss: 0.7536057233810425
Epoch 710, training loss: 850.2684326171875 = 0.7180758714675903 + 100.0 * 8.495503425598145
Epoch 710, val loss: 0.7502933740615845
Epoch 720, training loss: 850.1547241210938 = 0.7143293619155884 + 100.0 * 8.494403839111328
Epoch 720, val loss: 0.7471432685852051
Epoch 730, training loss: 850.0427856445312 = 0.7107838988304138 + 100.0 * 8.49332046508789
Epoch 730, val loss: 0.7441685795783997
Epoch 740, training loss: 850.423583984375 = 0.7073715329170227 + 100.0 * 8.497161865234375
Epoch 740, val loss: 0.7413884997367859
Epoch 750, training loss: 849.90673828125 = 0.7040730714797974 + 100.0 * 8.492026329040527
Epoch 750, val loss: 0.7385690808296204
Epoch 760, training loss: 849.7990112304688 = 0.7009732723236084 + 100.0 * 8.49098014831543
Epoch 760, val loss: 0.7360231280326843
Epoch 770, training loss: 849.6773681640625 = 0.6980322003364563 + 100.0 * 8.489792823791504
Epoch 770, val loss: 0.7336254715919495
Epoch 780, training loss: 849.7875366210938 = 0.6952322721481323 + 100.0 * 8.490922927856445
Epoch 780, val loss: 0.7313748002052307
Epoch 790, training loss: 849.5233154296875 = 0.692494809627533 + 100.0 * 8.48830795288086
Epoch 790, val loss: 0.7291496396064758
Epoch 800, training loss: 849.474609375 = 0.6899319887161255 + 100.0 * 8.487846374511719
Epoch 800, val loss: 0.727104127407074
Epoch 810, training loss: 849.3348388671875 = 0.6875016689300537 + 100.0 * 8.486473083496094
Epoch 810, val loss: 0.7251853942871094
Epoch 820, training loss: 849.2291259765625 = 0.6851962208747864 + 100.0 * 8.48543930053711
Epoch 820, val loss: 0.7233669757843018
Epoch 830, training loss: 849.1502685546875 = 0.6829938888549805 + 100.0 * 8.484672546386719
Epoch 830, val loss: 0.721671462059021
Epoch 840, training loss: 849.45263671875 = 0.6808622479438782 + 100.0 * 8.487717628479004
Epoch 840, val loss: 0.7200582027435303
Epoch 850, training loss: 849.101318359375 = 0.6787930727005005 + 100.0 * 8.484225273132324
Epoch 850, val loss: 0.7184061408042908
Epoch 860, training loss: 849.036865234375 = 0.6768237948417664 + 100.0 * 8.483600616455078
Epoch 860, val loss: 0.7168931365013123
Epoch 870, training loss: 848.8382568359375 = 0.6749457120895386 + 100.0 * 8.481633186340332
Epoch 870, val loss: 0.7154912948608398
Epoch 880, training loss: 848.7562866210938 = 0.6731613874435425 + 100.0 * 8.480831146240234
Epoch 880, val loss: 0.7141625881195068
Epoch 890, training loss: 848.9907836914062 = 0.6714454293251038 + 100.0 * 8.483193397521973
Epoch 890, val loss: 0.7129036784172058
Epoch 900, training loss: 848.7380981445312 = 0.6697217226028442 + 100.0 * 8.480683326721191
Epoch 900, val loss: 0.7115652561187744
Epoch 910, training loss: 848.6199951171875 = 0.6681025624275208 + 100.0 * 8.47951889038086
Epoch 910, val loss: 0.7104222178459167
Epoch 920, training loss: 848.4569091796875 = 0.6665627956390381 + 100.0 * 8.477903366088867
Epoch 920, val loss: 0.7093032598495483
Epoch 930, training loss: 848.3929443359375 = 0.6650893688201904 + 100.0 * 8.477278709411621
Epoch 930, val loss: 0.7082736492156982
Epoch 940, training loss: 848.7031860351562 = 0.6636350750923157 + 100.0 * 8.480395317077637
Epoch 940, val loss: 0.7072497010231018
Epoch 950, training loss: 848.3787841796875 = 0.6622046232223511 + 100.0 * 8.477165222167969
Epoch 950, val loss: 0.7061622738838196
Epoch 960, training loss: 848.1693725585938 = 0.6608261466026306 + 100.0 * 8.475085258483887
Epoch 960, val loss: 0.7052099704742432
Epoch 970, training loss: 848.088134765625 = 0.6595155000686646 + 100.0 * 8.474286079406738
Epoch 970, val loss: 0.704325795173645
Epoch 980, training loss: 848.0183715820312 = 0.6582521200180054 + 100.0 * 8.473601341247559
Epoch 980, val loss: 0.7034285068511963
Epoch 990, training loss: 848.4773559570312 = 0.6570078730583191 + 100.0 * 8.478203773498535
Epoch 990, val loss: 0.7025320529937744
Epoch 1000, training loss: 848.0081787109375 = 0.6556926369667053 + 100.0 * 8.473525047302246
Epoch 1000, val loss: 0.701665461063385
Epoch 1010, training loss: 847.90673828125 = 0.6544647812843323 + 100.0 * 8.472522735595703
Epoch 1010, val loss: 0.700799286365509
Epoch 1020, training loss: 847.7691040039062 = 0.6533069610595703 + 100.0 * 8.471158027648926
Epoch 1020, val loss: 0.7000349760055542
Epoch 1030, training loss: 847.6768798828125 = 0.6522012948989868 + 100.0 * 8.470246315002441
Epoch 1030, val loss: 0.6992806792259216
Epoch 1040, training loss: 847.6397705078125 = 0.651117205619812 + 100.0 * 8.469886779785156
Epoch 1040, val loss: 0.6985566020011902
Epoch 1050, training loss: 847.905517578125 = 0.650017261505127 + 100.0 * 8.472555160522461
Epoch 1050, val loss: 0.6978294253349304
Epoch 1060, training loss: 847.5513916015625 = 0.6489207744598389 + 100.0 * 8.469024658203125
Epoch 1060, val loss: 0.6970526576042175
Epoch 1070, training loss: 847.5784301757812 = 0.6478517651557922 + 100.0 * 8.469305992126465
Epoch 1070, val loss: 0.6963445544242859
Epoch 1080, training loss: 847.4844360351562 = 0.646794855594635 + 100.0 * 8.468376159667969
Epoch 1080, val loss: 0.6956109404563904
Epoch 1090, training loss: 847.3640747070312 = 0.6457797884941101 + 100.0 * 8.467183113098145
Epoch 1090, val loss: 0.6949499249458313
Epoch 1100, training loss: 847.2904052734375 = 0.644787609577179 + 100.0 * 8.466456413269043
Epoch 1100, val loss: 0.6943004727363586
Epoch 1110, training loss: 847.3358764648438 = 0.6438101530075073 + 100.0 * 8.466920852661133
Epoch 1110, val loss: 0.6937004923820496
Epoch 1120, training loss: 847.1669921875 = 0.6428061723709106 + 100.0 * 8.465241432189941
Epoch 1120, val loss: 0.6929737329483032
Epoch 1130, training loss: 847.1597900390625 = 0.641828715801239 + 100.0 * 8.465179443359375
Epoch 1130, val loss: 0.6923454999923706
Epoch 1140, training loss: 847.3225708007812 = 0.640861451625824 + 100.0 * 8.466816902160645
Epoch 1140, val loss: 0.691666305065155
Epoch 1150, training loss: 847.0842895507812 = 0.6398923397064209 + 100.0 * 8.464444160461426
Epoch 1150, val loss: 0.6910268068313599
Epoch 1160, training loss: 846.9598999023438 = 0.638948917388916 + 100.0 * 8.46320915222168
Epoch 1160, val loss: 0.6904106736183167
Epoch 1170, training loss: 846.8972778320312 = 0.6380351185798645 + 100.0 * 8.462592124938965
Epoch 1170, val loss: 0.6898506283760071
Epoch 1180, training loss: 846.885009765625 = 0.6371328830718994 + 100.0 * 8.462478637695312
Epoch 1180, val loss: 0.6892317533493042
Epoch 1190, training loss: 847.2794189453125 = 0.6361921429634094 + 100.0 * 8.466432571411133
Epoch 1190, val loss: 0.6886235475540161
Epoch 1200, training loss: 846.81982421875 = 0.6352112293243408 + 100.0 * 8.461846351623535
Epoch 1200, val loss: 0.6879103183746338
Epoch 1210, training loss: 846.6727905273438 = 0.6342847943305969 + 100.0 * 8.4603853225708
Epoch 1210, val loss: 0.6873410940170288
Epoch 1220, training loss: 846.6239013671875 = 0.6333991885185242 + 100.0 * 8.459904670715332
Epoch 1220, val loss: 0.6867567300796509
Epoch 1230, training loss: 846.6160888671875 = 0.6325253844261169 + 100.0 * 8.45983600616455
Epoch 1230, val loss: 0.6861793398857117
Epoch 1240, training loss: 846.760986328125 = 0.6316261887550354 + 100.0 * 8.46129322052002
Epoch 1240, val loss: 0.68560391664505
Epoch 1250, training loss: 846.5551147460938 = 0.6307177543640137 + 100.0 * 8.459243774414062
Epoch 1250, val loss: 0.6849717497825623
Epoch 1260, training loss: 846.87548828125 = 0.6298304796218872 + 100.0 * 8.462456703186035
Epoch 1260, val loss: 0.6843897700309753
Epoch 1270, training loss: 846.48388671875 = 0.6288856863975525 + 100.0 * 8.458549499511719
Epoch 1270, val loss: 0.6837807893753052
Epoch 1280, training loss: 846.3880615234375 = 0.6280012130737305 + 100.0 * 8.457600593566895
Epoch 1280, val loss: 0.6831895112991333
Epoch 1290, training loss: 846.3037719726562 = 0.627150297164917 + 100.0 * 8.456766128540039
Epoch 1290, val loss: 0.682632327079773
Epoch 1300, training loss: 846.2595825195312 = 0.6263109445571899 + 100.0 * 8.45633316040039
Epoch 1300, val loss: 0.68207186460495
Epoch 1310, training loss: 846.238037109375 = 0.6254710555076599 + 100.0 * 8.456125259399414
Epoch 1310, val loss: 0.6815410256385803
Epoch 1320, training loss: 846.6257934570312 = 0.6245595216751099 + 100.0 * 8.460012435913086
Epoch 1320, val loss: 0.6809080839157104
Epoch 1330, training loss: 846.4024658203125 = 0.6235858798027039 + 100.0 * 8.457788467407227
Epoch 1330, val loss: 0.6802092790603638
Epoch 1340, training loss: 846.1387939453125 = 0.6226907968521118 + 100.0 * 8.455161094665527
Epoch 1340, val loss: 0.6795797944068909
Epoch 1350, training loss: 846.0360107421875 = 0.6218428015708923 + 100.0 * 8.454141616821289
Epoch 1350, val loss: 0.6790199279785156
Epoch 1360, training loss: 845.9932250976562 = 0.6210025548934937 + 100.0 * 8.45372200012207
Epoch 1360, val loss: 0.6784252524375916
Epoch 1370, training loss: 845.9696044921875 = 0.6201575398445129 + 100.0 * 8.45349407196045
Epoch 1370, val loss: 0.6778733134269714
Epoch 1380, training loss: 846.56396484375 = 0.6192841529846191 + 100.0 * 8.459446907043457
Epoch 1380, val loss: 0.6772686839103699
Epoch 1390, training loss: 846.1319580078125 = 0.6183311343193054 + 100.0 * 8.4551362991333
Epoch 1390, val loss: 0.6765594482421875
Epoch 1400, training loss: 845.9266967773438 = 0.6174100637435913 + 100.0 * 8.453092575073242
Epoch 1400, val loss: 0.6759418845176697
Epoch 1410, training loss: 845.8029174804688 = 0.6165472269058228 + 100.0 * 8.451863288879395
Epoch 1410, val loss: 0.6753584146499634
Epoch 1420, training loss: 845.7645263671875 = 0.6156898140907288 + 100.0 * 8.451488494873047
Epoch 1420, val loss: 0.6747613549232483
Epoch 1430, training loss: 846.1170043945312 = 0.6148205995559692 + 100.0 * 8.455021858215332
Epoch 1430, val loss: 0.6741271018981934
Epoch 1440, training loss: 845.8385620117188 = 0.6138975620269775 + 100.0 * 8.45224666595459
Epoch 1440, val loss: 0.6735150814056396
Epoch 1450, training loss: 845.6494140625 = 0.6129886507987976 + 100.0 * 8.450364112854004
Epoch 1450, val loss: 0.6729169487953186
Epoch 1460, training loss: 845.6329956054688 = 0.6121008396148682 + 100.0 * 8.45020866394043
Epoch 1460, val loss: 0.672335684299469
Epoch 1470, training loss: 845.7857055664062 = 0.611208438873291 + 100.0 * 8.45174503326416
Epoch 1470, val loss: 0.6717138290405273
Epoch 1480, training loss: 845.5888061523438 = 0.6102659106254578 + 100.0 * 8.449785232543945
Epoch 1480, val loss: 0.6710101366043091
Epoch 1490, training loss: 845.5223999023438 = 0.609325647354126 + 100.0 * 8.44913101196289
Epoch 1490, val loss: 0.6704285144805908
Epoch 1500, training loss: 845.7727661132812 = 0.6084043979644775 + 100.0 * 8.451643943786621
Epoch 1500, val loss: 0.669704258441925
Epoch 1510, training loss: 845.4379272460938 = 0.6074221730232239 + 100.0 * 8.448305130004883
Epoch 1510, val loss: 0.6691319942474365
Epoch 1520, training loss: 845.4151000976562 = 0.6064792275428772 + 100.0 * 8.44808578491211
Epoch 1520, val loss: 0.6684390902519226
Epoch 1530, training loss: 845.3736572265625 = 0.6055597066879272 + 100.0 * 8.447681427001953
Epoch 1530, val loss: 0.6678256392478943
Epoch 1540, training loss: 845.3385620117188 = 0.6046456098556519 + 100.0 * 8.447339057922363
Epoch 1540, val loss: 0.667227566242218
Epoch 1550, training loss: 845.4265747070312 = 0.603715717792511 + 100.0 * 8.44822883605957
Epoch 1550, val loss: 0.6665946841239929
Epoch 1560, training loss: 845.4992065429688 = 0.6027222275733948 + 100.0 * 8.448965072631836
Epoch 1560, val loss: 0.6658452749252319
Epoch 1570, training loss: 845.2972412109375 = 0.6017098426818848 + 100.0 * 8.446955680847168
Epoch 1570, val loss: 0.6651882529258728
Epoch 1580, training loss: 845.234375 = 0.600738525390625 + 100.0 * 8.446335792541504
Epoch 1580, val loss: 0.6644455790519714
Epoch 1590, training loss: 845.1758422851562 = 0.5997827053070068 + 100.0 * 8.445760726928711
Epoch 1590, val loss: 0.663803219795227
Epoch 1600, training loss: 845.1666870117188 = 0.5988265872001648 + 100.0 * 8.4456787109375
Epoch 1600, val loss: 0.6630982756614685
Epoch 1610, training loss: 845.408203125 = 0.597838282585144 + 100.0 * 8.448103904724121
Epoch 1610, val loss: 0.6623184084892273
Epoch 1620, training loss: 845.3511962890625 = 0.5967854857444763 + 100.0 * 8.44754409790039
Epoch 1620, val loss: 0.6617257595062256
Epoch 1630, training loss: 845.1809692382812 = 0.5957409143447876 + 100.0 * 8.445852279663086
Epoch 1630, val loss: 0.6609953045845032
Epoch 1640, training loss: 845.2656860351562 = 0.5947046875953674 + 100.0 * 8.446709632873535
Epoch 1640, val loss: 0.6602667570114136
Epoch 1650, training loss: 844.992919921875 = 0.5936511754989624 + 100.0 * 8.443992614746094
Epoch 1650, val loss: 0.6595658659934998
Epoch 1660, training loss: 844.9896240234375 = 0.5926302075386047 + 100.0 * 8.4439697265625
Epoch 1660, val loss: 0.658821165561676
Epoch 1670, training loss: 844.9168701171875 = 0.5916033983230591 + 100.0 * 8.443252563476562
Epoch 1670, val loss: 0.6581498384475708
Epoch 1680, training loss: 844.986572265625 = 0.5905675888061523 + 100.0 * 8.443960189819336
Epoch 1680, val loss: 0.6574289798736572
Epoch 1690, training loss: 844.9130859375 = 0.5894803404808044 + 100.0 * 8.443236351013184
Epoch 1690, val loss: 0.65674889087677
Epoch 1700, training loss: 844.8214111328125 = 0.5883903503417969 + 100.0 * 8.442330360412598
Epoch 1700, val loss: 0.6559710502624512
Epoch 1710, training loss: 844.8739624023438 = 0.5873205661773682 + 100.0 * 8.442866325378418
Epoch 1710, val loss: 0.6553035378456116
Epoch 1720, training loss: 845.1751098632812 = 0.5861813426017761 + 100.0 * 8.445889472961426
Epoch 1720, val loss: 0.6543911099433899
Epoch 1730, training loss: 844.74560546875 = 0.5849916338920593 + 100.0 * 8.441605567932129
Epoch 1730, val loss: 0.6536640524864197
Epoch 1740, training loss: 844.7457275390625 = 0.5838719606399536 + 100.0 * 8.441618919372559
Epoch 1740, val loss: 0.6528056859970093
Epoch 1750, training loss: 844.6806640625 = 0.5827818512916565 + 100.0 * 8.44097900390625
Epoch 1750, val loss: 0.6521261930465698
Epoch 1760, training loss: 844.635009765625 = 0.5816896557807922 + 100.0 * 8.440533638000488
Epoch 1760, val loss: 0.6513624787330627
Epoch 1770, training loss: 844.7138671875 = 0.5805774331092834 + 100.0 * 8.441332817077637
Epoch 1770, val loss: 0.6505609154701233
Epoch 1780, training loss: 844.6791381835938 = 0.5794048309326172 + 100.0 * 8.440997123718262
Epoch 1780, val loss: 0.6498003602027893
Epoch 1790, training loss: 844.5560913085938 = 0.5782198905944824 + 100.0 * 8.439778327941895
Epoch 1790, val loss: 0.6489810347557068
Epoch 1800, training loss: 844.5136108398438 = 0.5770596861839294 + 100.0 * 8.43936538696289
Epoch 1800, val loss: 0.6482085585594177
Epoch 1810, training loss: 844.5072021484375 = 0.575907826423645 + 100.0 * 8.439312934875488
Epoch 1810, val loss: 0.6474079489707947
Epoch 1820, training loss: 844.9110107421875 = 0.574731707572937 + 100.0 * 8.443363189697266
Epoch 1820, val loss: 0.6465537548065186
Epoch 1830, training loss: 844.559326171875 = 0.5734841823577881 + 100.0 * 8.439858436584473
Epoch 1830, val loss: 0.6457095742225647
Epoch 1840, training loss: 844.474365234375 = 0.5722572803497314 + 100.0 * 8.439021110534668
Epoch 1840, val loss: 0.6449160575866699
Epoch 1850, training loss: 844.4005126953125 = 0.5710611939430237 + 100.0 * 8.438294410705566
Epoch 1850, val loss: 0.6440132260322571
Epoch 1860, training loss: 844.469970703125 = 0.5698578357696533 + 100.0 * 8.439001083374023
Epoch 1860, val loss: 0.6431872248649597
Epoch 1870, training loss: 844.4600219726562 = 0.5686148405075073 + 100.0 * 8.43891429901123
Epoch 1870, val loss: 0.642329752445221
Epoch 1880, training loss: 844.5289306640625 = 0.567335307598114 + 100.0 * 8.439616203308105
Epoch 1880, val loss: 0.6414244771003723
Epoch 1890, training loss: 844.285400390625 = 0.5660573244094849 + 100.0 * 8.437193870544434
Epoch 1890, val loss: 0.6405434608459473
Epoch 1900, training loss: 844.2512817382812 = 0.5648081302642822 + 100.0 * 8.436864852905273
Epoch 1900, val loss: 0.6396581530570984
Epoch 1910, training loss: 844.2310180664062 = 0.5635634660720825 + 100.0 * 8.436675071716309
Epoch 1910, val loss: 0.6388305425643921
Epoch 1920, training loss: 844.7738647460938 = 0.5622971057891846 + 100.0 * 8.442115783691406
Epoch 1920, val loss: 0.637833297252655
Epoch 1930, training loss: 844.376708984375 = 0.5609286427497864 + 100.0 * 8.43815803527832
Epoch 1930, val loss: 0.6370772123336792
Epoch 1940, training loss: 844.1981201171875 = 0.5596070289611816 + 100.0 * 8.436385154724121
Epoch 1940, val loss: 0.636064350605011
Epoch 1950, training loss: 844.1248779296875 = 0.5583247542381287 + 100.0 * 8.435665130615234
Epoch 1950, val loss: 0.6351677775382996
Epoch 1960, training loss: 844.1170043945312 = 0.5570509433746338 + 100.0 * 8.435599327087402
Epoch 1960, val loss: 0.6343398690223694
Epoch 1970, training loss: 844.3758544921875 = 0.555757999420166 + 100.0 * 8.438200950622559
Epoch 1970, val loss: 0.6334318518638611
Epoch 1980, training loss: 844.150146484375 = 0.554398238658905 + 100.0 * 8.435957908630371
Epoch 1980, val loss: 0.6325675249099731
Epoch 1990, training loss: 844.033447265625 = 0.5530441999435425 + 100.0 * 8.43480396270752
Epoch 1990, val loss: 0.6315810084342957
Epoch 2000, training loss: 844.0901489257812 = 0.5517253875732422 + 100.0 * 8.435384750366211
Epoch 2000, val loss: 0.6307345032691956
Epoch 2010, training loss: 844.070556640625 = 0.5503917336463928 + 100.0 * 8.435201644897461
Epoch 2010, val loss: 0.6298432350158691
Epoch 2020, training loss: 844.034423828125 = 0.5490469932556152 + 100.0 * 8.434853553771973
Epoch 2020, val loss: 0.6288245320320129
Epoch 2030, training loss: 844.1088256835938 = 0.5477146506309509 + 100.0 * 8.4356107711792
Epoch 2030, val loss: 0.6280654668807983
Epoch 2040, training loss: 844.0089721679688 = 0.5463334918022156 + 100.0 * 8.434626579284668
Epoch 2040, val loss: 0.6269773840904236
Epoch 2050, training loss: 843.8953857421875 = 0.5449713468551636 + 100.0 * 8.433504104614258
Epoch 2050, val loss: 0.626035749912262
Epoch 2060, training loss: 843.9254150390625 = 0.5436290502548218 + 100.0 * 8.433817863464355
Epoch 2060, val loss: 0.6251272559165955
Epoch 2070, training loss: 843.9292602539062 = 0.542273759841919 + 100.0 * 8.433869361877441
Epoch 2070, val loss: 0.6241353750228882
Epoch 2080, training loss: 843.9542236328125 = 0.5409024953842163 + 100.0 * 8.434133529663086
Epoch 2080, val loss: 0.6231895089149475
Epoch 2090, training loss: 843.9194946289062 = 0.5395179390907288 + 100.0 * 8.433799743652344
Epoch 2090, val loss: 0.6223617792129517
Epoch 2100, training loss: 843.9412841796875 = 0.5381187200546265 + 100.0 * 8.43403148651123
Epoch 2100, val loss: 0.6214260458946228
Epoch 2110, training loss: 843.781005859375 = 0.5367137789726257 + 100.0 * 8.432442665100098
Epoch 2110, val loss: 0.6203823089599609
Epoch 2120, training loss: 843.8253784179688 = 0.5353459715843201 + 100.0 * 8.432900428771973
Epoch 2120, val loss: 0.6193878054618835
Epoch 2130, training loss: 843.8699340820312 = 0.5339645147323608 + 100.0 * 8.43336009979248
Epoch 2130, val loss: 0.6185336709022522
Epoch 2140, training loss: 843.7305297851562 = 0.5325782299041748 + 100.0 * 8.431979179382324
Epoch 2140, val loss: 0.6177902817726135
Epoch 2150, training loss: 843.7271118164062 = 0.5312055945396423 + 100.0 * 8.43195915222168
Epoch 2150, val loss: 0.6168532967567444
Epoch 2160, training loss: 843.9967651367188 = 0.5298229455947876 + 100.0 * 8.434669494628906
Epoch 2160, val loss: 0.615918755531311
Epoch 2170, training loss: 843.950439453125 = 0.528388500213623 + 100.0 * 8.434220314025879
Epoch 2170, val loss: 0.6148179769515991
Epoch 2180, training loss: 843.6522216796875 = 0.526960551738739 + 100.0 * 8.431252479553223
Epoch 2180, val loss: 0.6141058802604675
Epoch 2190, training loss: 843.5931396484375 = 0.5255914926528931 + 100.0 * 8.430675506591797
Epoch 2190, val loss: 0.6131815314292908
Epoch 2200, training loss: 843.5505981445312 = 0.5242449641227722 + 100.0 * 8.43026351928711
Epoch 2200, val loss: 0.612347424030304
Epoch 2210, training loss: 843.5504760742188 = 0.5228993892669678 + 100.0 * 8.430275917053223
Epoch 2210, val loss: 0.6114853024482727
Epoch 2220, training loss: 843.934326171875 = 0.5215336084365845 + 100.0 * 8.434127807617188
Epoch 2220, val loss: 0.610519528388977
Epoch 2230, training loss: 843.4895629882812 = 0.520082950592041 + 100.0 * 8.429695129394531
Epoch 2230, val loss: 0.6098014116287231
Epoch 2240, training loss: 843.4664306640625 = 0.5186760425567627 + 100.0 * 8.42947769165039
Epoch 2240, val loss: 0.6088935732841492
Epoch 2250, training loss: 843.4686279296875 = 0.5173090696334839 + 100.0 * 8.429512977600098
Epoch 2250, val loss: 0.6080427169799805
Epoch 2260, training loss: 843.4417724609375 = 0.5159581899642944 + 100.0 * 8.429258346557617
Epoch 2260, val loss: 0.6072781682014465
Epoch 2270, training loss: 843.7164306640625 = 0.5145941376686096 + 100.0 * 8.432018280029297
Epoch 2270, val loss: 0.6064622402191162
Epoch 2280, training loss: 843.411376953125 = 0.5131725668907166 + 100.0 * 8.42898178100586
Epoch 2280, val loss: 0.6056764125823975
Epoch 2290, training loss: 843.5820922851562 = 0.5117765665054321 + 100.0 * 8.430703163146973
Epoch 2290, val loss: 0.6049647927284241
Epoch 2300, training loss: 843.392822265625 = 0.5103557109832764 + 100.0 * 8.428824424743652
Epoch 2300, val loss: 0.6040229797363281
Epoch 2310, training loss: 843.3400268554688 = 0.5089597702026367 + 100.0 * 8.42831039428711
Epoch 2310, val loss: 0.6031926274299622
Epoch 2320, training loss: 843.3057861328125 = 0.5076079964637756 + 100.0 * 8.42798137664795
Epoch 2320, val loss: 0.6023930311203003
Epoch 2330, training loss: 843.2826538085938 = 0.5062689185142517 + 100.0 * 8.427763938903809
Epoch 2330, val loss: 0.6016431450843811
Epoch 2340, training loss: 843.3512573242188 = 0.5049310922622681 + 100.0 * 8.428462982177734
Epoch 2340, val loss: 0.6008490324020386
Epoch 2350, training loss: 843.8390502929688 = 0.5035256147384644 + 100.0 * 8.433355331420898
Epoch 2350, val loss: 0.6000009775161743
Epoch 2360, training loss: 843.2958984375 = 0.5020613074302673 + 100.0 * 8.427938461303711
Epoch 2360, val loss: 0.5992961525917053
Epoch 2370, training loss: 843.2177734375 = 0.5006890296936035 + 100.0 * 8.427170753479004
Epoch 2370, val loss: 0.5984938144683838
Epoch 2380, training loss: 843.1970825195312 = 0.4993593692779541 + 100.0 * 8.426977157592773
Epoch 2380, val loss: 0.5978527069091797
Epoch 2390, training loss: 843.1654052734375 = 0.4980398714542389 + 100.0 * 8.426673889160156
Epoch 2390, val loss: 0.5970882773399353
Epoch 2400, training loss: 843.156494140625 = 0.49671679735183716 + 100.0 * 8.426597595214844
Epoch 2400, val loss: 0.5964421629905701
Epoch 2410, training loss: 843.6727905273438 = 0.49538150429725647 + 100.0 * 8.431774139404297
Epoch 2410, val loss: 0.5958518981933594
Epoch 2420, training loss: 843.3170776367188 = 0.49395951628685 + 100.0 * 8.428231239318848
Epoch 2420, val loss: 0.5949237942695618
Epoch 2430, training loss: 843.1455078125 = 0.49258607625961304 + 100.0 * 8.426528930664062
Epoch 2430, val loss: 0.5941761136054993
Epoch 2440, training loss: 843.0831298828125 = 0.4912596642971039 + 100.0 * 8.425918579101562
Epoch 2440, val loss: 0.5935142040252686
Epoch 2450, training loss: 843.1597290039062 = 0.4899454712867737 + 100.0 * 8.426697731018066
Epoch 2450, val loss: 0.5928787589073181
Epoch 2460, training loss: 843.29541015625 = 0.4885982871055603 + 100.0 * 8.428068161010742
Epoch 2460, val loss: 0.5922164916992188
Epoch 2470, training loss: 843.1154174804688 = 0.48723194003105164 + 100.0 * 8.426281929016113
Epoch 2470, val loss: 0.5914376378059387
Epoch 2480, training loss: 843.0234985351562 = 0.4859079420566559 + 100.0 * 8.425375938415527
Epoch 2480, val loss: 0.5909234881401062
Epoch 2490, training loss: 843.1434936523438 = 0.48459386825561523 + 100.0 * 8.426589012145996
Epoch 2490, val loss: 0.590397298336029
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7559614408929477
0.8158371368543071
The final CL Acc:0.75850, 0.00432, The final GNN Acc:0.81441, 0.00103
raw graph: torch.Size([2, 88648])
add edge: torch.Size([2, 110620])
remove edge: torch.Size([2, 66408])
updated graph: torch.Size([2, 88380])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 1059.355224609375 = 1.126224160194397 + 100.0 * 10.582289695739746
Epoch 0, val loss: 1.1247180700302124
Epoch 10, training loss: 1059.316162109375 = 1.1207852363586426 + 100.0 * 10.581954002380371
Epoch 10, val loss: 1.1192872524261475
Epoch 20, training loss: 1059.1612548828125 = 1.1148014068603516 + 100.0 * 10.580465316772461
Epoch 20, val loss: 1.1133015155792236
Epoch 30, training loss: 1058.4835205078125 = 1.1080812215805054 + 100.0 * 10.57375431060791
Epoch 30, val loss: 1.106611967086792
Epoch 40, training loss: 1055.877197265625 = 1.1006848812103271 + 100.0 * 10.547764778137207
Epoch 40, val loss: 1.0992573499679565
Epoch 50, training loss: 1048.497314453125 = 1.0923585891723633 + 100.0 * 10.474048614501953
Epoch 50, val loss: 1.090990424156189
Epoch 60, training loss: 1032.4727783203125 = 1.0838464498519897 + 100.0 * 10.313889503479004
Epoch 60, val loss: 1.0826584100723267
Epoch 70, training loss: 1012.2850952148438 = 1.0751559734344482 + 100.0 * 10.112099647521973
Epoch 70, val loss: 1.0740406513214111
Epoch 80, training loss: 980.639892578125 = 1.0665147304534912 + 100.0 * 9.795733451843262
Epoch 80, val loss: 1.0656365156173706
Epoch 90, training loss: 946.9395751953125 = 1.0609583854675293 + 100.0 * 9.458786010742188
Epoch 90, val loss: 1.0603457689285278
Epoch 100, training loss: 929.5390014648438 = 1.0564656257629395 + 100.0 * 9.284825325012207
Epoch 100, val loss: 1.056070327758789
Epoch 110, training loss: 921.7308349609375 = 1.0528175830841064 + 100.0 * 9.206780433654785
Epoch 110, val loss: 1.0525343418121338
Epoch 120, training loss: 915.2242431640625 = 1.0493004322052002 + 100.0 * 9.141749382019043
Epoch 120, val loss: 1.0490574836730957
Epoch 130, training loss: 907.4609985351562 = 1.0460994243621826 + 100.0 * 9.064148902893066
Epoch 130, val loss: 1.0458687543869019
Epoch 140, training loss: 898.0646362304688 = 1.04298734664917 + 100.0 * 8.970216751098633
Epoch 140, val loss: 1.0427124500274658
Epoch 150, training loss: 891.2315673828125 = 1.0395705699920654 + 100.0 * 8.901920318603516
Epoch 150, val loss: 1.0392147302627563
Epoch 160, training loss: 888.7449340820312 = 1.0357006788253784 + 100.0 * 8.877092361450195
Epoch 160, val loss: 1.0352983474731445
Epoch 170, training loss: 887.1771850585938 = 1.0318154096603394 + 100.0 * 8.861454010009766
Epoch 170, val loss: 1.0313853025436401
Epoch 180, training loss: 885.532958984375 = 1.0279330015182495 + 100.0 * 8.845049858093262
Epoch 180, val loss: 1.027442455291748
Epoch 190, training loss: 883.6859741210938 = 1.0239956378936768 + 100.0 * 8.826620101928711
Epoch 190, val loss: 1.0234856605529785
Epoch 200, training loss: 881.3740234375 = 1.020289659500122 + 100.0 * 8.803537368774414
Epoch 200, val loss: 1.0197911262512207
Epoch 210, training loss: 878.1773681640625 = 1.0168989896774292 + 100.0 * 8.771604537963867
Epoch 210, val loss: 1.0164268016815186
Epoch 220, training loss: 874.0468139648438 = 1.0137815475463867 + 100.0 * 8.730330467224121
Epoch 220, val loss: 1.0133222341537476
Epoch 230, training loss: 870.6763916015625 = 1.0105226039886475 + 100.0 * 8.696659088134766
Epoch 230, val loss: 1.0100666284561157
Epoch 240, training loss: 867.6919555664062 = 1.0066646337509155 + 100.0 * 8.666852951049805
Epoch 240, val loss: 1.006140112876892
Epoch 250, training loss: 865.6173095703125 = 1.0021291971206665 + 100.0 * 8.646151542663574
Epoch 250, val loss: 1.0015650987625122
Epoch 260, training loss: 864.1176147460938 = 0.9970875382423401 + 100.0 * 8.631205558776855
Epoch 260, val loss: 0.9964061379432678
Epoch 270, training loss: 862.76904296875 = 0.9916684627532959 + 100.0 * 8.61777400970459
Epoch 270, val loss: 0.9909898638725281
Epoch 280, training loss: 861.6209106445312 = 0.9860339164733887 + 100.0 * 8.606348991394043
Epoch 280, val loss: 0.9853277802467346
Epoch 290, training loss: 860.5305786132812 = 0.9800993800163269 + 100.0 * 8.595504760742188
Epoch 290, val loss: 0.97934490442276
Epoch 300, training loss: 859.5311279296875 = 0.9738993048667908 + 100.0 * 8.585572242736816
Epoch 300, val loss: 0.9731113314628601
Epoch 310, training loss: 858.452880859375 = 0.9674054980278015 + 100.0 * 8.574854850769043
Epoch 310, val loss: 0.9665550589561462
Epoch 320, training loss: 857.904296875 = 0.9605956077575684 + 100.0 * 8.569437026977539
Epoch 320, val loss: 0.9595759510993958
Epoch 330, training loss: 856.6849365234375 = 0.9532477259635925 + 100.0 * 8.557316780090332
Epoch 330, val loss: 0.9522775411605835
Epoch 340, training loss: 855.9520874023438 = 0.9455159902572632 + 100.0 * 8.550065994262695
Epoch 340, val loss: 0.9445304870605469
Epoch 350, training loss: 855.2828979492188 = 0.9373562932014465 + 100.0 * 8.543455123901367
Epoch 350, val loss: 0.9362752437591553
Epoch 360, training loss: 854.8156127929688 = 0.9286538362503052 + 100.0 * 8.538869857788086
Epoch 360, val loss: 0.9275416731834412
Epoch 370, training loss: 854.3092041015625 = 0.9194210171699524 + 100.0 * 8.533897399902344
Epoch 370, val loss: 0.9183201789855957
Epoch 380, training loss: 853.8046875 = 0.9097298383712769 + 100.0 * 8.528949737548828
Epoch 380, val loss: 0.9087091088294983
Epoch 390, training loss: 853.3882446289062 = 0.8996660113334656 + 100.0 * 8.524886131286621
Epoch 390, val loss: 0.8986754417419434
Epoch 400, training loss: 853.0957641601562 = 0.8892301917076111 + 100.0 * 8.522065162658691
Epoch 400, val loss: 0.888349175453186
Epoch 410, training loss: 852.7511596679688 = 0.8785334825515747 + 100.0 * 8.518726348876953
Epoch 410, val loss: 0.8776015043258667
Epoch 420, training loss: 852.3173828125 = 0.8674869537353516 + 100.0 * 8.514498710632324
Epoch 420, val loss: 0.8667181730270386
Epoch 430, training loss: 852.1592407226562 = 0.8562653064727783 + 100.0 * 8.513030052185059
Epoch 430, val loss: 0.855626106262207
Epoch 440, training loss: 851.6441040039062 = 0.8448721170425415 + 100.0 * 8.5079927444458
Epoch 440, val loss: 0.8443640470504761
Epoch 450, training loss: 851.221923828125 = 0.8333758115768433 + 100.0 * 8.503885269165039
Epoch 450, val loss: 0.833054780960083
Epoch 460, training loss: 850.8461303710938 = 0.8218233585357666 + 100.0 * 8.500243186950684
Epoch 460, val loss: 0.8216388821601868
Epoch 470, training loss: 850.5336303710938 = 0.8101837038993835 + 100.0 * 8.497234344482422
Epoch 470, val loss: 0.8102425932884216
Epoch 480, training loss: 849.9772338867188 = 0.798534095287323 + 100.0 * 8.49178695678711
Epoch 480, val loss: 0.7989038825035095
Epoch 490, training loss: 849.5050659179688 = 0.7870047092437744 + 100.0 * 8.487180709838867
Epoch 490, val loss: 0.7876510620117188
Epoch 500, training loss: 849.1535034179688 = 0.7755289673805237 + 100.0 * 8.483779907226562
Epoch 500, val loss: 0.7764595150947571
Epoch 510, training loss: 848.736328125 = 0.7640464901924133 + 100.0 * 8.47972297668457
Epoch 510, val loss: 0.7654293775558472
Epoch 520, training loss: 848.3629150390625 = 0.7527376413345337 + 100.0 * 8.476101875305176
Epoch 520, val loss: 0.7544387578964233
Epoch 530, training loss: 848.41796875 = 0.7415357232093811 + 100.0 * 8.476764678955078
Epoch 530, val loss: 0.7436069846153259
Epoch 540, training loss: 847.7827758789062 = 0.730408251285553 + 100.0 * 8.470523834228516
Epoch 540, val loss: 0.7330304384231567
Epoch 550, training loss: 847.4403076171875 = 0.7195725440979004 + 100.0 * 8.467207908630371
Epoch 550, val loss: 0.7226480841636658
Epoch 560, training loss: 847.1417236328125 = 0.7088891267776489 + 100.0 * 8.46432876586914
Epoch 560, val loss: 0.712466299533844
Epoch 570, training loss: 846.8983154296875 = 0.6984163522720337 + 100.0 * 8.46199893951416
Epoch 570, val loss: 0.7024841904640198
Epoch 580, training loss: 846.706298828125 = 0.6881078481674194 + 100.0 * 8.460182189941406
Epoch 580, val loss: 0.6927353143692017
Epoch 590, training loss: 846.5159301757812 = 0.6780336499214172 + 100.0 * 8.458378791809082
Epoch 590, val loss: 0.6832372546195984
Epoch 600, training loss: 846.3115844726562 = 0.668263852596283 + 100.0 * 8.456433296203613
Epoch 600, val loss: 0.6740582585334778
Epoch 610, training loss: 846.2088012695312 = 0.6587275266647339 + 100.0 * 8.455500602722168
Epoch 610, val loss: 0.6651260256767273
Epoch 620, training loss: 845.9309692382812 = 0.649514377117157 + 100.0 * 8.452814102172852
Epoch 620, val loss: 0.656404972076416
Epoch 630, training loss: 845.8350830078125 = 0.6405408382415771 + 100.0 * 8.451945304870605
Epoch 630, val loss: 0.6480544209480286
Epoch 640, training loss: 845.6436767578125 = 0.6317956447601318 + 100.0 * 8.450119018554688
Epoch 640, val loss: 0.6400020718574524
Epoch 650, training loss: 845.42578125 = 0.623412013053894 + 100.0 * 8.448023796081543
Epoch 650, val loss: 0.632159948348999
Epoch 660, training loss: 845.330322265625 = 0.6152617931365967 + 100.0 * 8.447151184082031
Epoch 660, val loss: 0.6246558427810669
Epoch 670, training loss: 845.226318359375 = 0.6073102355003357 + 100.0 * 8.446189880371094
Epoch 670, val loss: 0.6173988580703735
Epoch 680, training loss: 844.945068359375 = 0.5996842980384827 + 100.0 * 8.443453788757324
Epoch 680, val loss: 0.6104304194450378
Epoch 690, training loss: 844.7532958984375 = 0.5923380255699158 + 100.0 * 8.441609382629395
Epoch 690, val loss: 0.6037552952766418
Epoch 700, training loss: 845.072998046875 = 0.5852228403091431 + 100.0 * 8.444877624511719
Epoch 700, val loss: 0.5973650813102722
Epoch 710, training loss: 844.5045166015625 = 0.5783885717391968 + 100.0 * 8.439261436462402
Epoch 710, val loss: 0.5910256505012512
Epoch 720, training loss: 844.3013305664062 = 0.5718237161636353 + 100.0 * 8.437294960021973
Epoch 720, val loss: 0.5851042866706848
Epoch 730, training loss: 844.1488037109375 = 0.5654913783073425 + 100.0 * 8.435832977294922
Epoch 730, val loss: 0.5794859528541565
Epoch 740, training loss: 843.9852294921875 = 0.5594462752342224 + 100.0 * 8.434257507324219
Epoch 740, val loss: 0.5740402340888977
Epoch 750, training loss: 843.924072265625 = 0.5536396503448486 + 100.0 * 8.433704376220703
Epoch 750, val loss: 0.5688613653182983
Epoch 760, training loss: 843.9655151367188 = 0.5480082035064697 + 100.0 * 8.434174537658691
Epoch 760, val loss: 0.5638922452926636
Epoch 770, training loss: 843.6366577148438 = 0.5426496267318726 + 100.0 * 8.430939674377441
Epoch 770, val loss: 0.5592043399810791
Epoch 780, training loss: 843.4583740234375 = 0.5375702977180481 + 100.0 * 8.429207801818848
Epoch 780, val loss: 0.5546861290931702
Epoch 790, training loss: 843.3960571289062 = 0.5326946377754211 + 100.0 * 8.428633689880371
Epoch 790, val loss: 0.5503823757171631
Epoch 800, training loss: 843.2071533203125 = 0.527989387512207 + 100.0 * 8.426791191101074
Epoch 800, val loss: 0.5462920069694519
Epoch 810, training loss: 843.10498046875 = 0.5235165953636169 + 100.0 * 8.425814628601074
Epoch 810, val loss: 0.5424087643623352
Epoch 820, training loss: 843.0435791015625 = 0.5192373394966125 + 100.0 * 8.425243377685547
Epoch 820, val loss: 0.5386728048324585
Epoch 830, training loss: 842.8557739257812 = 0.5151302814483643 + 100.0 * 8.423406600952148
Epoch 830, val loss: 0.5352199673652649
Epoch 840, training loss: 842.7343139648438 = 0.5112511515617371 + 100.0 * 8.42223072052002
Epoch 840, val loss: 0.5318650603294373
Epoch 850, training loss: 842.7489624023438 = 0.5075135231018066 + 100.0 * 8.422414779663086
Epoch 850, val loss: 0.5287349224090576
Epoch 860, training loss: 842.5973510742188 = 0.503934919834137 + 100.0 * 8.420934677124023
Epoch 860, val loss: 0.5256310701370239
Epoch 870, training loss: 842.4042358398438 = 0.500532865524292 + 100.0 * 8.419036865234375
Epoch 870, val loss: 0.5227009654045105
Epoch 880, training loss: 842.2905883789062 = 0.4972773790359497 + 100.0 * 8.417933464050293
Epoch 880, val loss: 0.5199857354164124
Epoch 890, training loss: 842.3681030273438 = 0.4941808581352234 + 100.0 * 8.418739318847656
Epoch 890, val loss: 0.5173072218894958
Epoch 900, training loss: 842.37939453125 = 0.49115702509880066 + 100.0 * 8.418882369995117
Epoch 900, val loss: 0.5147638916969299
Epoch 910, training loss: 841.9957885742188 = 0.48827821016311646 + 100.0 * 8.415075302124023
Epoch 910, val loss: 0.5124189853668213
Epoch 920, training loss: 841.9016723632812 = 0.4855685830116272 + 100.0 * 8.41416072845459
Epoch 920, val loss: 0.5100878477096558
Epoch 930, training loss: 841.7915649414062 = 0.48295319080352783 + 100.0 * 8.4130859375
Epoch 930, val loss: 0.5079119801521301
Epoch 940, training loss: 841.9589233398438 = 0.48045697808265686 + 100.0 * 8.41478443145752
Epoch 940, val loss: 0.5057523250579834
Epoch 950, training loss: 841.713623046875 = 0.4779965579509735 + 100.0 * 8.41235637664795
Epoch 950, val loss: 0.5038467645645142
Epoch 960, training loss: 841.6140747070312 = 0.4756947457790375 + 100.0 * 8.411383628845215
Epoch 960, val loss: 0.5018981695175171
Epoch 970, training loss: 841.5689697265625 = 0.47346827387809753 + 100.0 * 8.410955429077148
Epoch 970, val loss: 0.5000902414321899
Epoch 980, training loss: 841.37353515625 = 0.47134649753570557 + 100.0 * 8.409021377563477
Epoch 980, val loss: 0.49836844205856323
Epoch 990, training loss: 841.281494140625 = 0.4693097770214081 + 100.0 * 8.408122062683105
Epoch 990, val loss: 0.49665820598602295
Epoch 1000, training loss: 841.4057006835938 = 0.46735456585884094 + 100.0 * 8.409383773803711
Epoch 1000, val loss: 0.4949802756309509
Epoch 1010, training loss: 841.4492797851562 = 0.46542075276374817 + 100.0 * 8.409838676452637
Epoch 1010, val loss: 0.49352455139160156
Epoch 1020, training loss: 841.0584106445312 = 0.4635773003101349 + 100.0 * 8.405948638916016
Epoch 1020, val loss: 0.49202200770378113
Epoch 1030, training loss: 841.0216064453125 = 0.46183136105537415 + 100.0 * 8.405597686767578
Epoch 1030, val loss: 0.4905744194984436
Epoch 1040, training loss: 840.9242553710938 = 0.46016043424606323 + 100.0 * 8.404641151428223
Epoch 1040, val loss: 0.48920369148254395
Epoch 1050, training loss: 840.9474487304688 = 0.4585464894771576 + 100.0 * 8.404889106750488
Epoch 1050, val loss: 0.48788806796073914
Epoch 1060, training loss: 840.8657836914062 = 0.45694535970687866 + 100.0 * 8.404088020324707
Epoch 1060, val loss: 0.4866679906845093
Epoch 1070, training loss: 840.8174438476562 = 0.45541149377822876 + 100.0 * 8.403620719909668
Epoch 1070, val loss: 0.4854619801044464
Epoch 1080, training loss: 840.6456909179688 = 0.4539465606212616 + 100.0 * 8.401917457580566
Epoch 1080, val loss: 0.48433607816696167
Epoch 1090, training loss: 840.5925903320312 = 0.4525449573993683 + 100.0 * 8.401400566101074
Epoch 1090, val loss: 0.4831986427307129
Epoch 1100, training loss: 840.5714111328125 = 0.45117849111557007 + 100.0 * 8.401202201843262
Epoch 1100, val loss: 0.48216208815574646
Epoch 1110, training loss: 840.651611328125 = 0.44984787702560425 + 100.0 * 8.402017593383789
Epoch 1110, val loss: 0.481079638004303
Epoch 1120, training loss: 840.5429077148438 = 0.4485260844230652 + 100.0 * 8.400943756103516
Epoch 1120, val loss: 0.48018571734428406
Epoch 1130, training loss: 840.36328125 = 0.4472866356372833 + 100.0 * 8.399160385131836
Epoch 1130, val loss: 0.47913986444473267
Epoch 1140, training loss: 840.231689453125 = 0.44608497619628906 + 100.0 * 8.397855758666992
Epoch 1140, val loss: 0.4782957136631012
Epoch 1150, training loss: 840.16650390625 = 0.44492316246032715 + 100.0 * 8.397215843200684
Epoch 1150, val loss: 0.4774050712585449
Epoch 1160, training loss: 840.217041015625 = 0.44378426671028137 + 100.0 * 8.397732734680176
Epoch 1160, val loss: 0.4766160547733307
Epoch 1170, training loss: 840.0914306640625 = 0.4426514804363251 + 100.0 * 8.396488189697266
Epoch 1170, val loss: 0.4757154881954193
Epoch 1180, training loss: 840.1576538085938 = 0.4415470361709595 + 100.0 * 8.397161483764648
Epoch 1180, val loss: 0.4750193655490875
Epoch 1190, training loss: 839.934326171875 = 0.44047945737838745 + 100.0 * 8.394938468933105
Epoch 1190, val loss: 0.4741463363170624
Epoch 1200, training loss: 839.8453979492188 = 0.4394456446170807 + 100.0 * 8.394059181213379
Epoch 1200, val loss: 0.47353658080101013
Epoch 1210, training loss: 839.7859497070312 = 0.4384438693523407 + 100.0 * 8.393475532531738
Epoch 1210, val loss: 0.4728127717971802
Epoch 1220, training loss: 839.9371948242188 = 0.43744993209838867 + 100.0 * 8.394997596740723
Epoch 1220, val loss: 0.4722059667110443
Epoch 1230, training loss: 839.6915283203125 = 0.4364612102508545 + 100.0 * 8.392550468444824
Epoch 1230, val loss: 0.47160226106643677
Epoch 1240, training loss: 839.5658569335938 = 0.4355192482471466 + 100.0 * 8.391303062438965
Epoch 1240, val loss: 0.4709853529930115
Epoch 1250, training loss: 839.5611572265625 = 0.43458664417266846 + 100.0 * 8.391265869140625
Epoch 1250, val loss: 0.47045573592185974
Epoch 1260, training loss: 839.4417114257812 = 0.4336564838886261 + 100.0 * 8.390080451965332
Epoch 1260, val loss: 0.4699331223964691
Epoch 1270, training loss: 839.3755493164062 = 0.43274736404418945 + 100.0 * 8.38942813873291
Epoch 1270, val loss: 0.46946901082992554
Epoch 1280, training loss: 839.2770385742188 = 0.4318632185459137 + 100.0 * 8.38845157623291
Epoch 1280, val loss: 0.4689236879348755
Epoch 1290, training loss: 839.2870483398438 = 0.4310004413127899 + 100.0 * 8.38856029510498
Epoch 1290, val loss: 0.46842098236083984
Epoch 1300, training loss: 839.3414916992188 = 0.4301496744155884 + 100.0 * 8.389113426208496
Epoch 1300, val loss: 0.4679329991340637
Epoch 1310, training loss: 839.1105346679688 = 0.4292948246002197 + 100.0 * 8.386812210083008
Epoch 1310, val loss: 0.4675137996673584
Epoch 1320, training loss: 839.0614013671875 = 0.42848730087280273 + 100.0 * 8.386329650878906
Epoch 1320, val loss: 0.4670755863189697
Epoch 1330, training loss: 838.9685668945312 = 0.4277038872241974 + 100.0 * 8.385408401489258
Epoch 1330, val loss: 0.4666281044483185
Epoch 1340, training loss: 838.8947143554688 = 0.4269392490386963 + 100.0 * 8.38467788696289
Epoch 1340, val loss: 0.46626782417297363
Epoch 1350, training loss: 838.9390258789062 = 0.4261854588985443 + 100.0 * 8.385128021240234
Epoch 1350, val loss: 0.4658690392971039
Epoch 1360, training loss: 838.901123046875 = 0.4254176914691925 + 100.0 * 8.384757041931152
Epoch 1360, val loss: 0.4654202163219452
Epoch 1370, training loss: 838.854736328125 = 0.424663782119751 + 100.0 * 8.384300231933594
Epoch 1370, val loss: 0.4650513231754303
Epoch 1380, training loss: 838.704345703125 = 0.4239451289176941 + 100.0 * 8.382803916931152
Epoch 1380, val loss: 0.4645881652832031
Epoch 1390, training loss: 838.661376953125 = 0.4232410192489624 + 100.0 * 8.382381439208984
Epoch 1390, val loss: 0.4642307460308075
Epoch 1400, training loss: 838.7014770507812 = 0.4225524067878723 + 100.0 * 8.382789611816406
Epoch 1400, val loss: 0.4638180434703827
Epoch 1410, training loss: 838.7039184570312 = 0.4218308925628662 + 100.0 * 8.382821083068848
Epoch 1410, val loss: 0.4634500741958618
Epoch 1420, training loss: 838.5433959960938 = 0.42112627625465393 + 100.0 * 8.38122272491455
Epoch 1420, val loss: 0.4630832076072693
Epoch 1430, training loss: 838.5365600585938 = 0.4204542934894562 + 100.0 * 8.381160736083984
Epoch 1430, val loss: 0.46275636553764343
Epoch 1440, training loss: 838.4487915039062 = 0.41980040073394775 + 100.0 * 8.380290031433105
Epoch 1440, val loss: 0.46238088607788086
Epoch 1450, training loss: 838.4036865234375 = 0.419156551361084 + 100.0 * 8.37984561920166
Epoch 1450, val loss: 0.4620616137981415
Epoch 1460, training loss: 838.7894287109375 = 0.41851183772087097 + 100.0 * 8.383708953857422
Epoch 1460, val loss: 0.46171000599861145
Epoch 1470, training loss: 838.5242919921875 = 0.41783809661865234 + 100.0 * 8.381064414978027
Epoch 1470, val loss: 0.4613625109195709
Epoch 1480, training loss: 838.2930297851562 = 0.41720056533813477 + 100.0 * 8.378758430480957
Epoch 1480, val loss: 0.4610331356525421
Epoch 1490, training loss: 838.2680053710938 = 0.416577011346817 + 100.0 * 8.378514289855957
Epoch 1490, val loss: 0.46067380905151367
Epoch 1500, training loss: 838.2081909179688 = 0.4159722328186035 + 100.0 * 8.377922058105469
Epoch 1500, val loss: 0.4603685736656189
Epoch 1510, training loss: 838.1876831054688 = 0.41537144780158997 + 100.0 * 8.377723693847656
Epoch 1510, val loss: 0.46004918217658997
Epoch 1520, training loss: 838.5531616210938 = 0.4147644340991974 + 100.0 * 8.381383895874023
Epoch 1520, val loss: 0.45969343185424805
Epoch 1530, training loss: 838.2510375976562 = 0.4141484498977661 + 100.0 * 8.378369331359863
Epoch 1530, val loss: 0.45950746536254883
Epoch 1540, training loss: 838.0885620117188 = 0.41355156898498535 + 100.0 * 8.376749992370605
Epoch 1540, val loss: 0.45908817648887634
Epoch 1550, training loss: 838.0382080078125 = 0.4129703640937805 + 100.0 * 8.376252174377441
Epoch 1550, val loss: 0.4588499665260315
Epoch 1560, training loss: 838.5934448242188 = 0.41238871216773987 + 100.0 * 8.381810188293457
Epoch 1560, val loss: 0.45867788791656494
Epoch 1570, training loss: 838.22607421875 = 0.41177237033843994 + 100.0 * 8.378143310546875
Epoch 1570, val loss: 0.4581296741962433
Epoch 1580, training loss: 837.9732666015625 = 0.4111883044242859 + 100.0 * 8.37562084197998
Epoch 1580, val loss: 0.4578772783279419
Epoch 1590, training loss: 837.8956909179688 = 0.410626083612442 + 100.0 * 8.374850273132324
Epoch 1590, val loss: 0.4575907588005066
Epoch 1600, training loss: 837.86328125 = 0.410076767206192 + 100.0 * 8.374531745910645
Epoch 1600, val loss: 0.45728954672813416
Epoch 1610, training loss: 838.1934814453125 = 0.4095272421836853 + 100.0 * 8.377839088439941
Epoch 1610, val loss: 0.4569258987903595
Epoch 1620, training loss: 837.9374389648438 = 0.4089432656764984 + 100.0 * 8.375285148620605
Epoch 1620, val loss: 0.45680829882621765
Epoch 1630, training loss: 837.7822875976562 = 0.4083956778049469 + 100.0 * 8.373739242553711
Epoch 1630, val loss: 0.4564071297645569
Epoch 1640, training loss: 837.7061157226562 = 0.407850056886673 + 100.0 * 8.3729829788208
Epoch 1640, val loss: 0.456149160861969
Epoch 1650, training loss: 837.6728515625 = 0.407317578792572 + 100.0 * 8.372655868530273
Epoch 1650, val loss: 0.45590075850486755
Epoch 1660, training loss: 837.66943359375 = 0.4067865014076233 + 100.0 * 8.372626304626465
Epoch 1660, val loss: 0.45561304688453674
Epoch 1670, training loss: 837.92626953125 = 0.40624380111694336 + 100.0 * 8.375200271606445
Epoch 1670, val loss: 0.4553329348564148
Epoch 1680, training loss: 837.68798828125 = 0.4056970477104187 + 100.0 * 8.372822761535645
Epoch 1680, val loss: 0.4550187289714813
Epoch 1690, training loss: 837.9005737304688 = 0.4051455855369568 + 100.0 * 8.374954223632812
Epoch 1690, val loss: 0.4547256827354431
Epoch 1700, training loss: 837.6370239257812 = 0.40458786487579346 + 100.0 * 8.372323989868164
Epoch 1700, val loss: 0.454546719789505
Epoch 1710, training loss: 837.5100708007812 = 0.4040704369544983 + 100.0 * 8.371060371398926
Epoch 1710, val loss: 0.45418813824653625
Epoch 1720, training loss: 837.456787109375 = 0.403555691242218 + 100.0 * 8.370532035827637
Epoch 1720, val loss: 0.45401692390441895
Epoch 1730, training loss: 837.4100341796875 = 0.40304920077323914 + 100.0 * 8.37006950378418
Epoch 1730, val loss: 0.4537351131439209
Epoch 1740, training loss: 837.3789672851562 = 0.4025414288043976 + 100.0 * 8.36976432800293
Epoch 1740, val loss: 0.45348790287971497
Epoch 1750, training loss: 837.8321533203125 = 0.4020310938358307 + 100.0 * 8.374300956726074
Epoch 1750, val loss: 0.45326149463653564
Epoch 1760, training loss: 837.5994873046875 = 0.4014783799648285 + 100.0 * 8.371979713439941
Epoch 1760, val loss: 0.45296722650527954
Epoch 1770, training loss: 837.3092041015625 = 0.4009563624858856 + 100.0 * 8.3690824508667
Epoch 1770, val loss: 0.4526916444301605
Epoch 1780, training loss: 837.26806640625 = 0.4004453122615814 + 100.0 * 8.36867618560791
Epoch 1780, val loss: 0.45246410369873047
Epoch 1790, training loss: 837.234130859375 = 0.39994534850120544 + 100.0 * 8.368341445922852
Epoch 1790, val loss: 0.45223498344421387
Epoch 1800, training loss: 837.6288452148438 = 0.39944130182266235 + 100.0 * 8.372294425964355
Epoch 1800, val loss: 0.4521164298057556
Epoch 1810, training loss: 837.3141479492188 = 0.39889630675315857 + 100.0 * 8.369152069091797
Epoch 1810, val loss: 0.4516311287879944
Epoch 1820, training loss: 837.2665405273438 = 0.3983723521232605 + 100.0 * 8.368681907653809
Epoch 1820, val loss: 0.45148125290870667
Epoch 1830, training loss: 837.1270141601562 = 0.39787018299102783 + 100.0 * 8.367291450500488
Epoch 1830, val loss: 0.4511707127094269
Epoch 1840, training loss: 837.0751342773438 = 0.39737993478775024 + 100.0 * 8.366777420043945
Epoch 1840, val loss: 0.4509798586368561
Epoch 1850, training loss: 837.0473022460938 = 0.39689382910728455 + 100.0 * 8.366503715515137
Epoch 1850, val loss: 0.450730562210083
Epoch 1860, training loss: 837.0860595703125 = 0.3964053988456726 + 100.0 * 8.366896629333496
Epoch 1860, val loss: 0.4505605101585388
Epoch 1870, training loss: 837.145263671875 = 0.3958958387374878 + 100.0 * 8.367493629455566
Epoch 1870, val loss: 0.45025062561035156
Epoch 1880, training loss: 837.026123046875 = 0.395383358001709 + 100.0 * 8.366307258605957
Epoch 1880, val loss: 0.45005184412002563
Epoch 1890, training loss: 837.029052734375 = 0.39488959312438965 + 100.0 * 8.366341590881348
Epoch 1890, val loss: 0.4497016668319702
Epoch 1900, training loss: 837.070556640625 = 0.3943978250026703 + 100.0 * 8.366761207580566
Epoch 1900, val loss: 0.4495098292827606
Epoch 1910, training loss: 836.9144287109375 = 0.3939054608345032 + 100.0 * 8.365204811096191
Epoch 1910, val loss: 0.4493390917778015
Epoch 1920, training loss: 836.8788452148438 = 0.3934240937232971 + 100.0 * 8.364853858947754
Epoch 1920, val loss: 0.44907090067863464
Epoch 1930, training loss: 836.8539428710938 = 0.3929460048675537 + 100.0 * 8.364609718322754
Epoch 1930, val loss: 0.44885265827178955
Epoch 1940, training loss: 837.151611328125 = 0.39246392250061035 + 100.0 * 8.367591857910156
Epoch 1940, val loss: 0.448697030544281
Epoch 1950, training loss: 836.91552734375 = 0.39194753766059875 + 100.0 * 8.365235328674316
Epoch 1950, val loss: 0.44842708110809326
Epoch 1960, training loss: 836.8895874023438 = 0.391446590423584 + 100.0 * 8.364981651306152
Epoch 1960, val loss: 0.44810014963150024
Epoch 1970, training loss: 836.79443359375 = 0.39096394181251526 + 100.0 * 8.364034652709961
Epoch 1970, val loss: 0.4479423761367798
Epoch 1980, training loss: 836.735595703125 = 0.3904936909675598 + 100.0 * 8.36345100402832
Epoch 1980, val loss: 0.44771426916122437
Epoch 1990, training loss: 836.7039184570312 = 0.3900277018547058 + 100.0 * 8.363139152526855
Epoch 1990, val loss: 0.4475277066230774
Epoch 2000, training loss: 836.6851806640625 = 0.3895590007305145 + 100.0 * 8.362956047058105
Epoch 2000, val loss: 0.44731375575065613
Epoch 2010, training loss: 836.873046875 = 0.38908758759498596 + 100.0 * 8.364839553833008
Epoch 2010, val loss: 0.44715651869773865
Epoch 2020, training loss: 836.8141479492188 = 0.3885798752307892 + 100.0 * 8.364255905151367
Epoch 2020, val loss: 0.44683516025543213
Epoch 2030, training loss: 836.6709594726562 = 0.3880789875984192 + 100.0 * 8.362829208374023
Epoch 2030, val loss: 0.44663166999816895
Epoch 2040, training loss: 836.6382446289062 = 0.3875924050807953 + 100.0 * 8.362506866455078
Epoch 2040, val loss: 0.4464785158634186
Epoch 2050, training loss: 836.587646484375 = 0.38712212443351746 + 100.0 * 8.362005233764648
Epoch 2050, val loss: 0.44624242186546326
Epoch 2060, training loss: 836.7354125976562 = 0.3866536617279053 + 100.0 * 8.363487243652344
Epoch 2060, val loss: 0.44607052206993103
Epoch 2070, training loss: 836.5742797851562 = 0.38615742325782776 + 100.0 * 8.361881256103516
Epoch 2070, val loss: 0.4458364248275757
Epoch 2080, training loss: 836.5325927734375 = 0.38566842675209045 + 100.0 * 8.361469268798828
Epoch 2080, val loss: 0.445578008890152
Epoch 2090, training loss: 836.514404296875 = 0.38519012928009033 + 100.0 * 8.361291885375977
Epoch 2090, val loss: 0.4453575015068054
Epoch 2100, training loss: 836.4893188476562 = 0.3847227096557617 + 100.0 * 8.361045837402344
Epoch 2100, val loss: 0.44515660405158997
Epoch 2110, training loss: 836.4940795898438 = 0.384258896112442 + 100.0 * 8.361098289489746
Epoch 2110, val loss: 0.4449296295642853
Epoch 2120, training loss: 837.2964477539062 = 0.3837868571281433 + 100.0 * 8.369126319885254
Epoch 2120, val loss: 0.4445948302745819
Epoch 2130, training loss: 836.5789794921875 = 0.38325777649879456 + 100.0 * 8.361957550048828
Epoch 2130, val loss: 0.4445711672306061
Epoch 2140, training loss: 836.4358520507812 = 0.382770299911499 + 100.0 * 8.360530853271484
Epoch 2140, val loss: 0.44433993101119995
Epoch 2150, training loss: 836.4251098632812 = 0.3823011815547943 + 100.0 * 8.360427856445312
Epoch 2150, val loss: 0.4441002607345581
Epoch 2160, training loss: 836.3812255859375 = 0.3818376064300537 + 100.0 * 8.359993934631348
Epoch 2160, val loss: 0.4439365863800049
Epoch 2170, training loss: 836.360595703125 = 0.3813766837120056 + 100.0 * 8.359792709350586
Epoch 2170, val loss: 0.44373488426208496
Epoch 2180, training loss: 836.3582763671875 = 0.3809107840061188 + 100.0 * 8.359773635864258
Epoch 2180, val loss: 0.4435654580593109
Epoch 2190, training loss: 836.7400512695312 = 0.3804389536380768 + 100.0 * 8.363595962524414
Epoch 2190, val loss: 0.4434349536895752
Epoch 2200, training loss: 836.6480102539062 = 0.3799382448196411 + 100.0 * 8.362680435180664
Epoch 2200, val loss: 0.44304579496383667
Epoch 2210, training loss: 836.3453979492188 = 0.37943923473358154 + 100.0 * 8.359659194946289
Epoch 2210, val loss: 0.44284218549728394
Epoch 2220, training loss: 836.3115844726562 = 0.37896421551704407 + 100.0 * 8.359326362609863
Epoch 2220, val loss: 0.44271790981292725
Epoch 2230, training loss: 836.2648315429688 = 0.37850120663642883 + 100.0 * 8.358863830566406
Epoch 2230, val loss: 0.44245919585227966
Epoch 2240, training loss: 836.2595825195312 = 0.3780401647090912 + 100.0 * 8.35881519317627
Epoch 2240, val loss: 0.44225627183914185
Epoch 2250, training loss: 836.3446044921875 = 0.37757545709609985 + 100.0 * 8.359670639038086
Epoch 2250, val loss: 0.44202712178230286
Epoch 2260, training loss: 836.4417114257812 = 0.3770887851715088 + 100.0 * 8.36064624786377
Epoch 2260, val loss: 0.44181665778160095
Epoch 2270, training loss: 836.3016967773438 = 0.37659573554992676 + 100.0 * 8.359251022338867
Epoch 2270, val loss: 0.44164133071899414
Epoch 2280, training loss: 836.215087890625 = 0.37611809372901917 + 100.0 * 8.358389854431152
Epoch 2280, val loss: 0.4414164423942566
Epoch 2290, training loss: 836.1817626953125 = 0.3756501078605652 + 100.0 * 8.358060836791992
Epoch 2290, val loss: 0.4412187337875366
Epoch 2300, training loss: 836.212646484375 = 0.3751833736896515 + 100.0 * 8.35837459564209
Epoch 2300, val loss: 0.4410189688205719
Epoch 2310, training loss: 836.501708984375 = 0.37470030784606934 + 100.0 * 8.3612699508667
Epoch 2310, val loss: 0.44084256887435913
Epoch 2320, training loss: 836.264892578125 = 0.37420234084129333 + 100.0 * 8.358906745910645
Epoch 2320, val loss: 0.440593421459198
Epoch 2330, training loss: 836.1438598632812 = 0.3737165927886963 + 100.0 * 8.357701301574707
Epoch 2330, val loss: 0.44036024808883667
Epoch 2340, training loss: 836.09912109375 = 0.3732444643974304 + 100.0 * 8.357258796691895
Epoch 2340, val loss: 0.44016703963279724
Epoch 2350, training loss: 836.1009521484375 = 0.37277543544769287 + 100.0 * 8.357281684875488
Epoch 2350, val loss: 0.43998879194259644
Epoch 2360, training loss: 836.4139404296875 = 0.37230321764945984 + 100.0 * 8.360416412353516
Epoch 2360, val loss: 0.4399094581604004
Epoch 2370, training loss: 836.0748291015625 = 0.3717973530292511 + 100.0 * 8.357029914855957
Epoch 2370, val loss: 0.43953025341033936
Epoch 2380, training loss: 836.0390625 = 0.37130868434906006 + 100.0 * 8.356677055358887
Epoch 2380, val loss: 0.43934372067451477
Epoch 2390, training loss: 836.0265502929688 = 0.37083226442337036 + 100.0 * 8.35655689239502
Epoch 2390, val loss: 0.4391794502735138
Epoch 2400, training loss: 836.130126953125 = 0.3703610897064209 + 100.0 * 8.357597351074219
Epoch 2400, val loss: 0.4389733672142029
Epoch 2410, training loss: 836.1107177734375 = 0.36986589431762695 + 100.0 * 8.35740852355957
Epoch 2410, val loss: 0.43881869316101074
Epoch 2420, training loss: 836.0057983398438 = 0.3693685829639435 + 100.0 * 8.356364250183105
Epoch 2420, val loss: 0.4385819137096405
Epoch 2430, training loss: 835.9873046875 = 0.3688831627368927 + 100.0 * 8.356184005737305
Epoch 2430, val loss: 0.43841785192489624
Epoch 2440, training loss: 835.9555053710938 = 0.368410587310791 + 100.0 * 8.355871200561523
Epoch 2440, val loss: 0.43824949860572815
Epoch 2450, training loss: 835.9314575195312 = 0.36793798208236694 + 100.0 * 8.355635643005371
Epoch 2450, val loss: 0.4380873441696167
Epoch 2460, training loss: 835.964599609375 = 0.3674633204936981 + 100.0 * 8.355971336364746
Epoch 2460, val loss: 0.43790507316589355
Epoch 2470, training loss: 836.345947265625 = 0.3669695556163788 + 100.0 * 8.359789848327637
Epoch 2470, val loss: 0.4376840591430664
Epoch 2480, training loss: 835.9667358398438 = 0.36645257472991943 + 100.0 * 8.356002807617188
Epoch 2480, val loss: 0.43746498227119446
Epoch 2490, training loss: 835.9144897460938 = 0.3659493923187256 + 100.0 * 8.355484962463379
Epoch 2490, val loss: 0.4373406767845154
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8249619482496194
0.8621314207056437
=== training gcn model ===
Epoch 0, training loss: 1059.3291015625 = 1.098134160041809 + 100.0 * 10.58230972290039
Epoch 0, val loss: 1.09952974319458
Epoch 10, training loss: 1059.300537109375 = 1.0940759181976318 + 100.0 * 10.582064628601074
Epoch 10, val loss: 1.0954339504241943
Epoch 20, training loss: 1059.185791015625 = 1.0898046493530273 + 100.0 * 10.58095932006836
Epoch 20, val loss: 1.091120958328247
Epoch 30, training loss: 1058.6494140625 = 1.0852322578430176 + 100.0 * 10.575642585754395
Epoch 30, val loss: 1.0864856243133545
Epoch 40, training loss: 1056.379638671875 = 1.0800176858901978 + 100.0 * 10.552995681762695
Epoch 40, val loss: 1.0811548233032227
Epoch 50, training loss: 1049.4244384765625 = 1.0738489627838135 + 100.0 * 10.483505249023438
Epoch 50, val loss: 1.0748283863067627
Epoch 60, training loss: 1034.337158203125 = 1.0671859979629517 + 100.0 * 10.332700729370117
Epoch 60, val loss: 1.0680729150772095
Epoch 70, training loss: 1010.5969848632812 = 1.060767412185669 + 100.0 * 10.095361709594727
Epoch 70, val loss: 1.061484932899475
Epoch 80, training loss: 980.0637817382812 = 1.0542840957641602 + 100.0 * 9.790095329284668
Epoch 80, val loss: 1.055076003074646
Epoch 90, training loss: 958.95703125 = 1.0491958856582642 + 100.0 * 9.579078674316406
Epoch 90, val loss: 1.0501964092254639
Epoch 100, training loss: 944.3837280273438 = 1.044984221458435 + 100.0 * 9.433387756347656
Epoch 100, val loss: 1.0460319519042969
Epoch 110, training loss: 934.9419555664062 = 1.0405523777008057 + 100.0 * 9.339014053344727
Epoch 110, val loss: 1.041496992111206
Epoch 120, training loss: 929.1959228515625 = 1.0360223054885864 + 100.0 * 9.281599044799805
Epoch 120, val loss: 1.0368757247924805
Epoch 130, training loss: 926.6658325195312 = 1.032018780708313 + 100.0 * 9.256338119506836
Epoch 130, val loss: 1.0328197479248047
Epoch 140, training loss: 924.546875 = 1.0286250114440918 + 100.0 * 9.235182762145996
Epoch 140, val loss: 1.0293723344802856
Epoch 150, training loss: 921.75439453125 = 1.0253382921218872 + 100.0 * 9.207290649414062
Epoch 150, val loss: 1.0260931253433228
Epoch 160, training loss: 917.6458129882812 = 1.022415280342102 + 100.0 * 9.166234016418457
Epoch 160, val loss: 1.0232748985290527
Epoch 170, training loss: 910.8031005859375 = 1.0203971862792969 + 100.0 * 9.097826957702637
Epoch 170, val loss: 1.0213671922683716
Epoch 180, training loss: 901.7743530273438 = 1.019338607788086 + 100.0 * 9.007550239562988
Epoch 180, val loss: 1.0204097032546997
Epoch 190, training loss: 895.9683227539062 = 1.0182331800460815 + 100.0 * 8.949501037597656
Epoch 190, val loss: 1.0192068815231323
Epoch 200, training loss: 890.52783203125 = 1.0158663988113403 + 100.0 * 8.895119667053223
Epoch 200, val loss: 1.0168365240097046
Epoch 210, training loss: 883.738037109375 = 1.0140146017074585 + 100.0 * 8.827239990234375
Epoch 210, val loss: 1.0151602029800415
Epoch 220, training loss: 879.0368041992188 = 1.0133827924728394 + 100.0 * 8.780234336853027
Epoch 220, val loss: 1.0145280361175537
Epoch 230, training loss: 876.7959594726562 = 1.0115121603012085 + 100.0 * 8.757843971252441
Epoch 230, val loss: 1.012519359588623
Epoch 240, training loss: 874.7832641601562 = 1.0080323219299316 + 100.0 * 8.737751960754395
Epoch 240, val loss: 1.0090923309326172
Epoch 250, training loss: 872.7239990234375 = 1.0047965049743652 + 100.0 * 8.717191696166992
Epoch 250, val loss: 1.0059747695922852
Epoch 260, training loss: 870.4588012695312 = 1.002113699913025 + 100.0 * 8.69456672668457
Epoch 260, val loss: 1.0033494234085083
Epoch 270, training loss: 868.165283203125 = 0.9994804859161377 + 100.0 * 8.671658515930176
Epoch 270, val loss: 1.0007221698760986
Epoch 280, training loss: 866.056640625 = 0.9967818260192871 + 100.0 * 8.650598526000977
Epoch 280, val loss: 0.9980185627937317
Epoch 290, training loss: 864.2698974609375 = 0.9937415719032288 + 100.0 * 8.63276195526123
Epoch 290, val loss: 0.9949623346328735
Epoch 300, training loss: 862.8804931640625 = 0.9901540279388428 + 100.0 * 8.618903160095215
Epoch 300, val loss: 0.9913907647132874
Epoch 310, training loss: 861.8980102539062 = 0.9859824776649475 + 100.0 * 8.60912036895752
Epoch 310, val loss: 0.9872668981552124
Epoch 320, training loss: 860.9373168945312 = 0.9812870025634766 + 100.0 * 8.599560737609863
Epoch 320, val loss: 0.9826691150665283
Epoch 330, training loss: 860.2705078125 = 0.9762035608291626 + 100.0 * 8.59294319152832
Epoch 330, val loss: 0.9776984453201294
Epoch 340, training loss: 859.6189575195312 = 0.9707700610160828 + 100.0 * 8.586482048034668
Epoch 340, val loss: 0.9723770022392273
Epoch 350, training loss: 858.998779296875 = 0.9650174975395203 + 100.0 * 8.580337524414062
Epoch 350, val loss: 0.9668070673942566
Epoch 360, training loss: 858.5540161132812 = 0.9590346813201904 + 100.0 * 8.575949668884277
Epoch 360, val loss: 0.9609896540641785
Epoch 370, training loss: 857.8204345703125 = 0.9527547955513 + 100.0 * 8.568676948547363
Epoch 370, val loss: 0.9549185037612915
Epoch 380, training loss: 857.1703491210938 = 0.9463180303573608 + 100.0 * 8.562240600585938
Epoch 380, val loss: 0.9486743807792664
Epoch 390, training loss: 856.4318237304688 = 0.9396744966506958 + 100.0 * 8.55492115020752
Epoch 390, val loss: 0.9422540664672852
Epoch 400, training loss: 855.8463134765625 = 0.9328622221946716 + 100.0 * 8.549134254455566
Epoch 400, val loss: 0.9357399940490723
Epoch 410, training loss: 855.0278930664062 = 0.9257816076278687 + 100.0 * 8.541021347045898
Epoch 410, val loss: 0.9288035035133362
Epoch 420, training loss: 854.1966552734375 = 0.918542206287384 + 100.0 * 8.532781600952148
Epoch 420, val loss: 0.921821653842926
Epoch 430, training loss: 853.43017578125 = 0.9110414385795593 + 100.0 * 8.525191307067871
Epoch 430, val loss: 0.9145915508270264
Epoch 440, training loss: 852.7890625 = 0.903210461139679 + 100.0 * 8.518858909606934
Epoch 440, val loss: 0.9070533514022827
Epoch 450, training loss: 852.47802734375 = 0.895028829574585 + 100.0 * 8.515830039978027
Epoch 450, val loss: 0.899169385433197
Epoch 460, training loss: 851.780029296875 = 0.886387050151825 + 100.0 * 8.508935928344727
Epoch 460, val loss: 0.8908677101135254
Epoch 470, training loss: 851.3745727539062 = 0.8776125907897949 + 100.0 * 8.504969596862793
Epoch 470, val loss: 0.882469654083252
Epoch 480, training loss: 850.9404907226562 = 0.8686645030975342 + 100.0 * 8.500718116760254
Epoch 480, val loss: 0.8739067316055298
Epoch 490, training loss: 850.6203002929688 = 0.8595170974731445 + 100.0 * 8.497608184814453
Epoch 490, val loss: 0.865175187587738
Epoch 500, training loss: 850.4133911132812 = 0.8501573204994202 + 100.0 * 8.49563217163086
Epoch 500, val loss: 0.8562490940093994
Epoch 510, training loss: 850.0259399414062 = 0.840756893157959 + 100.0 * 8.491851806640625
Epoch 510, val loss: 0.8472690582275391
Epoch 520, training loss: 849.7332763671875 = 0.8313542604446411 + 100.0 * 8.489019393920898
Epoch 520, val loss: 0.8383383750915527
Epoch 530, training loss: 849.4904174804688 = 0.8219467997550964 + 100.0 * 8.486684799194336
Epoch 530, val loss: 0.8293938040733337
Epoch 540, training loss: 849.3716430664062 = 0.8125404715538025 + 100.0 * 8.485590934753418
Epoch 540, val loss: 0.8204525709152222
Epoch 550, training loss: 849.0709838867188 = 0.8031789660453796 + 100.0 * 8.482678413391113
Epoch 550, val loss: 0.8116147518157959
Epoch 560, training loss: 848.8109130859375 = 0.7939856648445129 + 100.0 * 8.480169296264648
Epoch 560, val loss: 0.802982747554779
Epoch 570, training loss: 848.5921630859375 = 0.7849622964859009 + 100.0 * 8.478072166442871
Epoch 570, val loss: 0.7944881319999695
Epoch 580, training loss: 848.4414672851562 = 0.7760816216468811 + 100.0 * 8.476654052734375
Epoch 580, val loss: 0.786109209060669
Epoch 590, training loss: 848.3010864257812 = 0.7672950625419617 + 100.0 * 8.475337982177734
Epoch 590, val loss: 0.777898371219635
Epoch 600, training loss: 847.9476928710938 = 0.7588735222816467 + 100.0 * 8.471888542175293
Epoch 600, val loss: 0.7700152397155762
Epoch 610, training loss: 847.7117309570312 = 0.7506279349327087 + 100.0 * 8.469611167907715
Epoch 610, val loss: 0.7623012065887451
Epoch 620, training loss: 847.4889526367188 = 0.7426010370254517 + 100.0 * 8.467463493347168
Epoch 620, val loss: 0.7548436522483826
Epoch 630, training loss: 847.2702026367188 = 0.7347723841667175 + 100.0 * 8.465353965759277
Epoch 630, val loss: 0.7475904226303101
Epoch 640, training loss: 847.4817504882812 = 0.72710782289505 + 100.0 * 8.467546463012695
Epoch 640, val loss: 0.74056077003479
Epoch 650, training loss: 846.8590087890625 = 0.719656765460968 + 100.0 * 8.461393356323242
Epoch 650, val loss: 0.7336363792419434
Epoch 660, training loss: 846.6438598632812 = 0.7125416994094849 + 100.0 * 8.45931339263916
Epoch 660, val loss: 0.7270788550376892
Epoch 670, training loss: 846.4509887695312 = 0.7055980563163757 + 100.0 * 8.457453727722168
Epoch 670, val loss: 0.7207062840461731
Epoch 680, training loss: 846.35986328125 = 0.6989017724990845 + 100.0 * 8.456609725952148
Epoch 680, val loss: 0.7145551443099976
Epoch 690, training loss: 846.1636352539062 = 0.6923050880432129 + 100.0 * 8.454712867736816
Epoch 690, val loss: 0.7085695266723633
Epoch 700, training loss: 845.927490234375 = 0.686062753200531 + 100.0 * 8.452414512634277
Epoch 700, val loss: 0.7028597593307495
Epoch 710, training loss: 845.7490234375 = 0.6799519062042236 + 100.0 * 8.450691223144531
Epoch 710, val loss: 0.697333812713623
Epoch 720, training loss: 845.827880859375 = 0.6740579009056091 + 100.0 * 8.4515380859375
Epoch 720, val loss: 0.6920095682144165
Epoch 730, training loss: 845.4686279296875 = 0.6683054566383362 + 100.0 * 8.448002815246582
Epoch 730, val loss: 0.6868013739585876
Epoch 740, training loss: 845.2918701171875 = 0.6628181338310242 + 100.0 * 8.446290016174316
Epoch 740, val loss: 0.681874692440033
Epoch 750, training loss: 845.1332397460938 = 0.6574864983558655 + 100.0 * 8.444757461547852
Epoch 750, val loss: 0.6771032214164734
Epoch 760, training loss: 845.1493530273438 = 0.6523160934448242 + 100.0 * 8.44497013092041
Epoch 760, val loss: 0.6724932193756104
Epoch 770, training loss: 844.91162109375 = 0.6472915410995483 + 100.0 * 8.442643165588379
Epoch 770, val loss: 0.6679946780204773
Epoch 780, training loss: 844.6774291992188 = 0.6424853801727295 + 100.0 * 8.440349578857422
Epoch 780, val loss: 0.6637551188468933
Epoch 790, training loss: 844.544189453125 = 0.6378458738327026 + 100.0 * 8.439064025878906
Epoch 790, val loss: 0.6596607565879822
Epoch 800, training loss: 844.4724731445312 = 0.6333259344100952 + 100.0 * 8.43839168548584
Epoch 800, val loss: 0.6556988954544067
Epoch 810, training loss: 844.2517700195312 = 0.6289747953414917 + 100.0 * 8.436227798461914
Epoch 810, val loss: 0.6518493890762329
Epoch 820, training loss: 844.1251831054688 = 0.6247575879096985 + 100.0 * 8.435004234313965
Epoch 820, val loss: 0.6481760144233704
Epoch 830, training loss: 844.058349609375 = 0.6206421256065369 + 100.0 * 8.43437671661377
Epoch 830, val loss: 0.6445877552032471
Epoch 840, training loss: 843.9912109375 = 0.6166773438453674 + 100.0 * 8.433745384216309
Epoch 840, val loss: 0.6411581635475159
Epoch 850, training loss: 843.8209228515625 = 0.6127792596817017 + 100.0 * 8.43208122253418
Epoch 850, val loss: 0.6377636194229126
Epoch 860, training loss: 843.5951538085938 = 0.6090825200080872 + 100.0 * 8.429861068725586
Epoch 860, val loss: 0.6345735192298889
Epoch 870, training loss: 843.4674072265625 = 0.6054788827896118 + 100.0 * 8.428619384765625
Epoch 870, val loss: 0.6315049529075623
Epoch 880, training loss: 843.439208984375 = 0.6020251512527466 + 100.0 * 8.42837142944336
Epoch 880, val loss: 0.6285404562950134
Epoch 890, training loss: 843.3245849609375 = 0.598590612411499 + 100.0 * 8.427260398864746
Epoch 890, val loss: 0.6256488561630249
Epoch 900, training loss: 843.1368408203125 = 0.5953598022460938 + 100.0 * 8.4254150390625
Epoch 900, val loss: 0.6228681802749634
Epoch 910, training loss: 843.0398559570312 = 0.5921974182128906 + 100.0 * 8.424476623535156
Epoch 910, val loss: 0.6201958060264587
Epoch 920, training loss: 843.0037841796875 = 0.5891250967979431 + 100.0 * 8.42414665222168
Epoch 920, val loss: 0.6176180243492126
Epoch 930, training loss: 842.8048706054688 = 0.5861272215843201 + 100.0 * 8.422187805175781
Epoch 930, val loss: 0.6151209473609924
Epoch 940, training loss: 842.654296875 = 0.5832562446594238 + 100.0 * 8.420710563659668
Epoch 940, val loss: 0.6126928925514221
Epoch 950, training loss: 842.5563354492188 = 0.5804561376571655 + 100.0 * 8.419758796691895
Epoch 950, val loss: 0.6103588938713074
Epoch 960, training loss: 842.5130004882812 = 0.5777347683906555 + 100.0 * 8.419352531433105
Epoch 960, val loss: 0.608103334903717
Epoch 970, training loss: 842.42041015625 = 0.5750543475151062 + 100.0 * 8.418453216552734
Epoch 970, val loss: 0.6058387756347656
Epoch 980, training loss: 842.268798828125 = 0.5724861025810242 + 100.0 * 8.416962623596191
Epoch 980, val loss: 0.6037060022354126
Epoch 990, training loss: 842.2019653320312 = 0.5699968338012695 + 100.0 * 8.416319847106934
Epoch 990, val loss: 0.6016092300415039
Epoch 1000, training loss: 842.10498046875 = 0.5675339102745056 + 100.0 * 8.415374755859375
Epoch 1000, val loss: 0.5995862483978271
Epoch 1010, training loss: 841.9052734375 = 0.565144419670105 + 100.0 * 8.41340160369873
Epoch 1010, val loss: 0.5976219177246094
Epoch 1020, training loss: 841.8046264648438 = 0.562852680683136 + 100.0 * 8.4124174118042
Epoch 1020, val loss: 0.5957298874855042
Epoch 1030, training loss: 841.7005004882812 = 0.5606240034103394 + 100.0 * 8.411398887634277
Epoch 1030, val loss: 0.5939114689826965
Epoch 1040, training loss: 841.6771850585938 = 0.5584421753883362 + 100.0 * 8.411187171936035
Epoch 1040, val loss: 0.5921381115913391
Epoch 1050, training loss: 841.5609130859375 = 0.5562955141067505 + 100.0 * 8.410046577453613
Epoch 1050, val loss: 0.5903533697128296
Epoch 1060, training loss: 841.4505004882812 = 0.5542070269584656 + 100.0 * 8.408963203430176
Epoch 1060, val loss: 0.588636040687561
Epoch 1070, training loss: 841.4022827148438 = 0.552178680896759 + 100.0 * 8.408500671386719
Epoch 1070, val loss: 0.5869856476783752
Epoch 1080, training loss: 841.376220703125 = 0.5501352548599243 + 100.0 * 8.4082612991333
Epoch 1080, val loss: 0.5853452086448669
Epoch 1090, training loss: 841.2330932617188 = 0.5482049584388733 + 100.0 * 8.406848907470703
Epoch 1090, val loss: 0.5837305188179016
Epoch 1100, training loss: 841.1082763671875 = 0.5463187098503113 + 100.0 * 8.405619621276855
Epoch 1100, val loss: 0.5822100043296814
Epoch 1110, training loss: 841.0294799804688 = 0.5444863438606262 + 100.0 * 8.404850006103516
Epoch 1110, val loss: 0.5807251930236816
Epoch 1120, training loss: 841.0978393554688 = 0.5426978468894958 + 100.0 * 8.405550956726074
Epoch 1120, val loss: 0.5792509913444519
Epoch 1130, training loss: 840.933349609375 = 0.5408467650413513 + 100.0 * 8.403924942016602
Epoch 1130, val loss: 0.5777464509010315
Epoch 1140, training loss: 840.8618774414062 = 0.5391172766685486 + 100.0 * 8.403227806091309
Epoch 1140, val loss: 0.5763157606124878
Epoch 1150, training loss: 840.7383422851562 = 0.5374095439910889 + 100.0 * 8.402009010314941
Epoch 1150, val loss: 0.5749307870864868
Epoch 1160, training loss: 840.6893920898438 = 0.535749614238739 + 100.0 * 8.401535987854004
Epoch 1160, val loss: 0.5735786557197571
Epoch 1170, training loss: 840.9053344726562 = 0.53410404920578 + 100.0 * 8.403712272644043
Epoch 1170, val loss: 0.5722516179084778
Epoch 1180, training loss: 840.596435546875 = 0.532492995262146 + 100.0 * 8.400639533996582
Epoch 1180, val loss: 0.5708606839179993
Epoch 1190, training loss: 840.4856567382812 = 0.530908465385437 + 100.0 * 8.399547576904297
Epoch 1190, val loss: 0.5695855021476746
Epoch 1200, training loss: 840.4270629882812 = 0.5293837189674377 + 100.0 * 8.398977279663086
Epoch 1200, val loss: 0.5683481693267822
Epoch 1210, training loss: 840.3754272460938 = 0.5278803706169128 + 100.0 * 8.398475646972656
Epoch 1210, val loss: 0.5671057105064392
Epoch 1220, training loss: 840.6597290039062 = 0.5263521671295166 + 100.0 * 8.401333808898926
Epoch 1220, val loss: 0.5658917427062988
Epoch 1230, training loss: 840.2536010742188 = 0.5248412489891052 + 100.0 * 8.397287368774414
Epoch 1230, val loss: 0.5646066069602966
Epoch 1240, training loss: 840.2242431640625 = 0.5233718156814575 + 100.0 * 8.397008895874023
Epoch 1240, val loss: 0.5633835196495056
Epoch 1250, training loss: 840.1364135742188 = 0.5219382047653198 + 100.0 * 8.39614486694336
Epoch 1250, val loss: 0.5622043013572693
Epoch 1260, training loss: 840.142333984375 = 0.5205288529396057 + 100.0 * 8.396218299865723
Epoch 1260, val loss: 0.561065137386322
Epoch 1270, training loss: 840.0874633789062 = 0.5191022753715515 + 100.0 * 8.395683288574219
Epoch 1270, val loss: 0.5598925948143005
Epoch 1280, training loss: 839.9857177734375 = 0.5177123546600342 + 100.0 * 8.39468002319336
Epoch 1280, val loss: 0.5587426424026489
Epoch 1290, training loss: 840.0611572265625 = 0.5163463950157166 + 100.0 * 8.395447731018066
Epoch 1290, val loss: 0.5576208233833313
Epoch 1300, training loss: 839.8931884765625 = 0.5149791240692139 + 100.0 * 8.393782615661621
Epoch 1300, val loss: 0.5564991235733032
Epoch 1310, training loss: 839.826904296875 = 0.5136550664901733 + 100.0 * 8.393132209777832
Epoch 1310, val loss: 0.5554037094116211
Epoch 1320, training loss: 839.8391723632812 = 0.512346088886261 + 100.0 * 8.393268585205078
Epoch 1320, val loss: 0.5543551445007324
Epoch 1330, training loss: 839.7484741210938 = 0.5110222101211548 + 100.0 * 8.392374038696289
Epoch 1330, val loss: 0.5532619953155518
Epoch 1340, training loss: 839.6521606445312 = 0.5097416639328003 + 100.0 * 8.391424179077148
Epoch 1340, val loss: 0.5521966814994812
Epoch 1350, training loss: 839.5932006835938 = 0.508474588394165 + 100.0 * 8.390847206115723
Epoch 1350, val loss: 0.5511700510978699
Epoch 1360, training loss: 839.5397338867188 = 0.5072329640388489 + 100.0 * 8.390324592590332
Epoch 1360, val loss: 0.5501527786254883
Epoch 1370, training loss: 839.490966796875 = 0.5059918761253357 + 100.0 * 8.389849662780762
Epoch 1370, val loss: 0.54915452003479
Epoch 1380, training loss: 839.8049926757812 = 0.5047376155853271 + 100.0 * 8.3930025100708
Epoch 1380, val loss: 0.5481204390525818
Epoch 1390, training loss: 839.66845703125 = 0.5034323930740356 + 100.0 * 8.391650199890137
Epoch 1390, val loss: 0.5470209717750549
Epoch 1400, training loss: 839.4230346679688 = 0.5021435022354126 + 100.0 * 8.389208793640137
Epoch 1400, val loss: 0.5459747910499573
Epoch 1410, training loss: 839.2962646484375 = 0.5009065866470337 + 100.0 * 8.387953758239746
Epoch 1410, val loss: 0.5449832081794739
Epoch 1420, training loss: 839.2296752929688 = 0.499717116355896 + 100.0 * 8.387299537658691
Epoch 1420, val loss: 0.5440102815628052
Epoch 1430, training loss: 839.1829833984375 = 0.4985448718070984 + 100.0 * 8.386844635009766
Epoch 1430, val loss: 0.5430592894554138
Epoch 1440, training loss: 839.130126953125 = 0.4973796606063843 + 100.0 * 8.386327743530273
Epoch 1440, val loss: 0.5421226620674133
Epoch 1450, training loss: 839.0853881835938 = 0.4962150454521179 + 100.0 * 8.385891914367676
Epoch 1450, val loss: 0.5411857962608337
Epoch 1460, training loss: 839.5949096679688 = 0.4950338304042816 + 100.0 * 8.390998840332031
Epoch 1460, val loss: 0.5402673482894897
Epoch 1470, training loss: 839.1268310546875 = 0.493747353553772 + 100.0 * 8.386330604553223
Epoch 1470, val loss: 0.5392166376113892
Epoch 1480, training loss: 838.997314453125 = 0.49255475401878357 + 100.0 * 8.385047912597656
Epoch 1480, val loss: 0.538218080997467
Epoch 1490, training loss: 838.9163818359375 = 0.49138379096984863 + 100.0 * 8.384249687194824
Epoch 1490, val loss: 0.5373077988624573
Epoch 1500, training loss: 838.8606567382812 = 0.49026137590408325 + 100.0 * 8.38370418548584
Epoch 1500, val loss: 0.5364410877227783
Epoch 1510, training loss: 838.8123779296875 = 0.48913460969924927 + 100.0 * 8.383232116699219
Epoch 1510, val loss: 0.5355429649353027
Epoch 1520, training loss: 838.808349609375 = 0.48799827694892883 + 100.0 * 8.383203506469727
Epoch 1520, val loss: 0.5346664786338806
Epoch 1530, training loss: 838.8834838867188 = 0.48680782318115234 + 100.0 * 8.383966445922852
Epoch 1530, val loss: 0.5337285995483398
Epoch 1540, training loss: 838.6765747070312 = 0.4856516420841217 + 100.0 * 8.381909370422363
Epoch 1540, val loss: 0.5327598452568054
Epoch 1550, training loss: 838.6446533203125 = 0.48450055718421936 + 100.0 * 8.381601333618164
Epoch 1550, val loss: 0.531865119934082
Epoch 1560, training loss: 838.6016235351562 = 0.483366996049881 + 100.0 * 8.381182670593262
Epoch 1560, val loss: 0.5309924483299255
Epoch 1570, training loss: 838.5966796875 = 0.48225462436676025 + 100.0 * 8.381144523620605
Epoch 1570, val loss: 0.5300901532173157
Epoch 1580, training loss: 838.7197265625 = 0.4810789227485657 + 100.0 * 8.382386207580566
Epoch 1580, val loss: 0.5292037129402161
Epoch 1590, training loss: 838.4785766601562 = 0.47994762659072876 + 100.0 * 8.379986763000488
Epoch 1590, val loss: 0.5282477736473083
Epoch 1600, training loss: 838.4493408203125 = 0.4788166284561157 + 100.0 * 8.379705429077148
Epoch 1600, val loss: 0.5274112820625305
Epoch 1610, training loss: 838.3975219726562 = 0.47771793603897095 + 100.0 * 8.37919807434082
Epoch 1610, val loss: 0.5265406370162964
Epoch 1620, training loss: 838.3488159179688 = 0.47663095593452454 + 100.0 * 8.378722190856934
Epoch 1620, val loss: 0.5256959795951843
Epoch 1630, training loss: 838.3281860351562 = 0.475534051656723 + 100.0 * 8.37852668762207
Epoch 1630, val loss: 0.5248692631721497
Epoch 1640, training loss: 838.4756469726562 = 0.4744155704975128 + 100.0 * 8.380012512207031
Epoch 1640, val loss: 0.5240175127983093
Epoch 1650, training loss: 838.40283203125 = 0.47329357266426086 + 100.0 * 8.379295349121094
Epoch 1650, val loss: 0.523062527179718
Epoch 1660, training loss: 838.307861328125 = 0.47214779257774353 + 100.0 * 8.37835693359375
Epoch 1660, val loss: 0.5221307277679443
Epoch 1670, training loss: 838.1817626953125 = 0.47103190422058105 + 100.0 * 8.377107620239258
Epoch 1670, val loss: 0.5212934017181396
Epoch 1680, training loss: 838.1254272460938 = 0.4699491262435913 + 100.0 * 8.376554489135742
Epoch 1680, val loss: 0.5204805135726929
Epoch 1690, training loss: 838.078857421875 = 0.46887436509132385 + 100.0 * 8.376099586486816
Epoch 1690, val loss: 0.519609272480011
Epoch 1700, training loss: 838.0576171875 = 0.4678027331829071 + 100.0 * 8.375898361206055
Epoch 1700, val loss: 0.5187731981277466
Epoch 1710, training loss: 838.3912353515625 = 0.466723769903183 + 100.0 * 8.379244804382324
Epoch 1710, val loss: 0.5178864598274231
Epoch 1720, training loss: 838.093017578125 = 0.46554434299468994 + 100.0 * 8.376275062561035
Epoch 1720, val loss: 0.5170698165893555
Epoch 1730, training loss: 837.9453125 = 0.46445032954216003 + 100.0 * 8.374808311462402
Epoch 1730, val loss: 0.5161896347999573
Epoch 1740, training loss: 837.91357421875 = 0.4633868634700775 + 100.0 * 8.374502182006836
Epoch 1740, val loss: 0.5153834819793701
Epoch 1750, training loss: 838.0899047851562 = 0.46230587363243103 + 100.0 * 8.376276016235352
Epoch 1750, val loss: 0.5145938992500305
Epoch 1760, training loss: 837.8917846679688 = 0.46122679114341736 + 100.0 * 8.374305725097656
Epoch 1760, val loss: 0.5137110948562622
Epoch 1770, training loss: 837.8346557617188 = 0.4601346254348755 + 100.0 * 8.37374496459961
Epoch 1770, val loss: 0.5129169821739197
Epoch 1780, training loss: 837.762451171875 = 0.45908164978027344 + 100.0 * 8.37303352355957
Epoch 1780, val loss: 0.5121015310287476
Epoch 1790, training loss: 837.7232055664062 = 0.4580412209033966 + 100.0 * 8.372651100158691
Epoch 1790, val loss: 0.511327862739563
Epoch 1800, training loss: 837.8052368164062 = 0.45699530839920044 + 100.0 * 8.373482704162598
Epoch 1800, val loss: 0.5105717182159424
Epoch 1810, training loss: 837.6846313476562 = 0.45592254400253296 + 100.0 * 8.372286796569824
Epoch 1810, val loss: 0.5097032785415649
Epoch 1820, training loss: 837.63818359375 = 0.4548616409301758 + 100.0 * 8.371833801269531
Epoch 1820, val loss: 0.5089142322540283
Epoch 1830, training loss: 837.5870361328125 = 0.4538182318210602 + 100.0 * 8.371332168579102
Epoch 1830, val loss: 0.5081178545951843
Epoch 1840, training loss: 837.6682739257812 = 0.45279067754745483 + 100.0 * 8.372154235839844
Epoch 1840, val loss: 0.5073533654212952
Epoch 1850, training loss: 837.5468139648438 = 0.4517306089401245 + 100.0 * 8.370950698852539
Epoch 1850, val loss: 0.5065488219261169
Epoch 1860, training loss: 837.4671630859375 = 0.450697660446167 + 100.0 * 8.37016487121582
Epoch 1860, val loss: 0.505790650844574
Epoch 1870, training loss: 837.476318359375 = 0.4496909976005554 + 100.0 * 8.37026596069336
Epoch 1870, val loss: 0.5050130486488342
Epoch 1880, training loss: 837.6170043945312 = 0.4486621022224426 + 100.0 * 8.371683120727539
Epoch 1880, val loss: 0.5042366981506348
Epoch 1890, training loss: 837.4476928710938 = 0.44760292768478394 + 100.0 * 8.370000839233398
Epoch 1890, val loss: 0.5034802556037903
Epoch 1900, training loss: 837.3566284179688 = 0.44657808542251587 + 100.0 * 8.369100570678711
Epoch 1900, val loss: 0.5026794672012329
Epoch 1910, training loss: 837.3659057617188 = 0.4455716013908386 + 100.0 * 8.369203567504883
Epoch 1910, val loss: 0.5019368529319763
Epoch 1920, training loss: 837.354248046875 = 0.4445500075817108 + 100.0 * 8.369096755981445
Epoch 1920, val loss: 0.5011616945266724
Epoch 1930, training loss: 837.2830200195312 = 0.4435284435749054 + 100.0 * 8.36839485168457
Epoch 1930, val loss: 0.5004506707191467
Epoch 1940, training loss: 837.196044921875 = 0.44250860810279846 + 100.0 * 8.367535591125488
Epoch 1940, val loss: 0.49969974160194397
Epoch 1950, training loss: 837.1929931640625 = 0.4415074288845062 + 100.0 * 8.367514610290527
Epoch 1950, val loss: 0.49892958998680115
Epoch 1960, training loss: 837.570068359375 = 0.44049766659736633 + 100.0 * 8.371295928955078
Epoch 1960, val loss: 0.49818724393844604
Epoch 1970, training loss: 837.1705322265625 = 0.4393947422504425 + 100.0 * 8.367311477661133
Epoch 1970, val loss: 0.49738115072250366
Epoch 1980, training loss: 837.0595703125 = 0.4383687376976013 + 100.0 * 8.366211891174316
Epoch 1980, val loss: 0.49665045738220215
Epoch 1990, training loss: 837.040283203125 = 0.43738123774528503 + 100.0 * 8.366028785705566
Epoch 1990, val loss: 0.49593621492385864
Epoch 2000, training loss: 836.9957885742188 = 0.43639424443244934 + 100.0 * 8.365593910217285
Epoch 2000, val loss: 0.4952271282672882
Epoch 2010, training loss: 836.9699096679688 = 0.43541932106018066 + 100.0 * 8.365345001220703
Epoch 2010, val loss: 0.49451717734336853
Epoch 2020, training loss: 837.328369140625 = 0.43443387746810913 + 100.0 * 8.368939399719238
Epoch 2020, val loss: 0.4937291145324707
Epoch 2030, training loss: 837.2265014648438 = 0.4333273768424988 + 100.0 * 8.367931365966797
Epoch 2030, val loss: 0.4930776059627533
Epoch 2040, training loss: 836.91943359375 = 0.43228092789649963 + 100.0 * 8.364871978759766
Epoch 2040, val loss: 0.49227476119995117
Epoch 2050, training loss: 836.8671875 = 0.4312661588191986 + 100.0 * 8.364358901977539
Epoch 2050, val loss: 0.4915623366832733
Epoch 2060, training loss: 836.8178100585938 = 0.43028491735458374 + 100.0 * 8.363875389099121
Epoch 2060, val loss: 0.49087145924568176
Epoch 2070, training loss: 836.8187866210938 = 0.4293057918548584 + 100.0 * 8.36389446258545
Epoch 2070, val loss: 0.49020758271217346
Epoch 2080, training loss: 836.9876708984375 = 0.4282911717891693 + 100.0 * 8.365593910217285
Epoch 2080, val loss: 0.4895130693912506
Epoch 2090, training loss: 836.7918090820312 = 0.42726609110832214 + 100.0 * 8.363645553588867
Epoch 2090, val loss: 0.48876091837882996
Epoch 2100, training loss: 836.7114868164062 = 0.42625370621681213 + 100.0 * 8.362852096557617
Epoch 2100, val loss: 0.48807576298713684
Epoch 2110, training loss: 836.671142578125 = 0.4252719283103943 + 100.0 * 8.362458229064941
Epoch 2110, val loss: 0.48742809891700745
Epoch 2120, training loss: 836.7174682617188 = 0.4242924749851227 + 100.0 * 8.362931251525879
Epoch 2120, val loss: 0.4867536127567291
Epoch 2130, training loss: 836.734130859375 = 0.42324987053871155 + 100.0 * 8.36310863494873
Epoch 2130, val loss: 0.4860391616821289
Epoch 2140, training loss: 836.58642578125 = 0.4222177267074585 + 100.0 * 8.361641883850098
Epoch 2140, val loss: 0.48532724380493164
Epoch 2150, training loss: 836.5610961914062 = 0.421215683221817 + 100.0 * 8.361398696899414
Epoch 2150, val loss: 0.4846148192882538
Epoch 2160, training loss: 836.5309448242188 = 0.42024070024490356 + 100.0 * 8.361106872558594
Epoch 2160, val loss: 0.48401302099227905
Epoch 2170, training loss: 836.5471801757812 = 0.41926413774490356 + 100.0 * 8.361279487609863
Epoch 2170, val loss: 0.48332738876342773
Epoch 2180, training loss: 836.7166137695312 = 0.4182499647140503 + 100.0 * 8.362983703613281
Epoch 2180, val loss: 0.48261818289756775
Epoch 2190, training loss: 836.5054931640625 = 0.41722336411476135 + 100.0 * 8.360882759094238
Epoch 2190, val loss: 0.48190978169441223
Epoch 2200, training loss: 836.4452514648438 = 0.4162079989910126 + 100.0 * 8.36029052734375
Epoch 2200, val loss: 0.4813336730003357
Epoch 2210, training loss: 836.4146118164062 = 0.4152302145957947 + 100.0 * 8.359993934631348
Epoch 2210, val loss: 0.4806464910507202
Epoch 2220, training loss: 836.4028930664062 = 0.4142753779888153 + 100.0 * 8.359886169433594
Epoch 2220, val loss: 0.4800397455692291
Epoch 2230, training loss: 836.6031494140625 = 0.413301557302475 + 100.0 * 8.361898422241211
Epoch 2230, val loss: 0.47937434911727905
Epoch 2240, training loss: 836.4302978515625 = 0.41229668259620667 + 100.0 * 8.360179901123047
Epoch 2240, val loss: 0.4786921739578247
Epoch 2250, training loss: 836.3661499023438 = 0.4113177955150604 + 100.0 * 8.359548568725586
Epoch 2250, val loss: 0.4780616760253906
Epoch 2260, training loss: 836.3252563476562 = 0.41034093499183655 + 100.0 * 8.359148979187012
Epoch 2260, val loss: 0.4774092137813568
Epoch 2270, training loss: 836.324462890625 = 0.4093652367591858 + 100.0 * 8.359150886535645
Epoch 2270, val loss: 0.4768170416355133
Epoch 2280, training loss: 836.3348999023438 = 0.408387154340744 + 100.0 * 8.359265327453613
Epoch 2280, val loss: 0.47627130150794983
Epoch 2290, training loss: 836.2073974609375 = 0.40741968154907227 + 100.0 * 8.357999801635742
Epoch 2290, val loss: 0.4755837619304657
Epoch 2300, training loss: 836.2001953125 = 0.4064652919769287 + 100.0 * 8.35793685913086
Epoch 2300, val loss: 0.47494933009147644
Epoch 2310, training loss: 836.1522827148438 = 0.40551307797431946 + 100.0 * 8.357467651367188
Epoch 2310, val loss: 0.47440919280052185
Epoch 2320, training loss: 836.2920532226562 = 0.40455812215805054 + 100.0 * 8.358875274658203
Epoch 2320, val loss: 0.47386273741722107
Epoch 2330, training loss: 836.1016845703125 = 0.403585284948349 + 100.0 * 8.35698127746582
Epoch 2330, val loss: 0.4731241762638092
Epoch 2340, training loss: 836.0764770507812 = 0.40263307094573975 + 100.0 * 8.356738090515137
Epoch 2340, val loss: 0.47256365418434143
Epoch 2350, training loss: 836.09619140625 = 0.4017001986503601 + 100.0 * 8.356945037841797
Epoch 2350, val loss: 0.47201451659202576
Epoch 2360, training loss: 836.257080078125 = 0.40076205134391785 + 100.0 * 8.358563423156738
Epoch 2360, val loss: 0.47140586376190186
Epoch 2370, training loss: 836.0858154296875 = 0.3997977077960968 + 100.0 * 8.356860160827637
Epoch 2370, val loss: 0.47097569704055786
Epoch 2380, training loss: 836.0646362304688 = 0.3988589346408844 + 100.0 * 8.356657981872559
Epoch 2380, val loss: 0.47028306126594543
Epoch 2390, training loss: 836.2030029296875 = 0.3978995680809021 + 100.0 * 8.358051300048828
Epoch 2390, val loss: 0.46980398893356323
Epoch 2400, training loss: 836.0029907226562 = 0.3969819247722626 + 100.0 * 8.356060028076172
Epoch 2400, val loss: 0.46923089027404785
Epoch 2410, training loss: 835.9185180664062 = 0.3960435390472412 + 100.0 * 8.355224609375
Epoch 2410, val loss: 0.46868300437927246
Epoch 2420, training loss: 835.8780517578125 = 0.3951493203639984 + 100.0 * 8.354828834533691
Epoch 2420, val loss: 0.4682120084762573
Epoch 2430, training loss: 835.8546142578125 = 0.394247442483902 + 100.0 * 8.35460376739502
Epoch 2430, val loss: 0.4676972031593323
Epoch 2440, training loss: 836.1375732421875 = 0.3933456540107727 + 100.0 * 8.357441902160645
Epoch 2440, val loss: 0.46718087792396545
Epoch 2450, training loss: 836.0043334960938 = 0.3923719823360443 + 100.0 * 8.356119155883789
Epoch 2450, val loss: 0.46671175956726074
Epoch 2460, training loss: 835.7950439453125 = 0.3914432227611542 + 100.0 * 8.354036331176758
Epoch 2460, val loss: 0.46612852811813354
Epoch 2470, training loss: 835.78076171875 = 0.39053595066070557 + 100.0 * 8.353901863098145
Epoch 2470, val loss: 0.4656812250614166
Epoch 2480, training loss: 835.743408203125 = 0.38964933156967163 + 100.0 * 8.353537559509277
Epoch 2480, val loss: 0.4651987850666046
Epoch 2490, training loss: 835.7977905273438 = 0.3887641131877899 + 100.0 * 8.354089736938477
Epoch 2490, val loss: 0.4647233784198761
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8138001014713343
0.8643048612620445
=== training gcn model ===
Epoch 0, training loss: 1059.33203125 = 1.1029067039489746 + 100.0 * 10.582291603088379
Epoch 0, val loss: 1.1018697023391724
Epoch 10, training loss: 1059.293212890625 = 1.0981613397598267 + 100.0 * 10.581951141357422
Epoch 10, val loss: 1.0970823764801025
Epoch 20, training loss: 1059.13671875 = 1.0929443836212158 + 100.0 * 10.580438613891602
Epoch 20, val loss: 1.091807246208191
Epoch 30, training loss: 1058.4276123046875 = 1.0870176553726196 + 100.0 * 10.573405265808105
Epoch 30, val loss: 1.0857936143875122
Epoch 40, training loss: 1055.6572265625 = 1.080348253250122 + 100.0 * 10.545768737792969
Epoch 40, val loss: 1.079032301902771
Epoch 50, training loss: 1047.8131103515625 = 1.0729516744613647 + 100.0 * 10.467401504516602
Epoch 50, val loss: 1.0715394020080566
Epoch 60, training loss: 1030.87841796875 = 1.0654146671295166 + 100.0 * 10.29813003540039
Epoch 60, val loss: 1.0639889240264893
Epoch 70, training loss: 1002.3406372070312 = 1.057836890220642 + 100.0 * 10.01282787322998
Epoch 70, val loss: 1.056295394897461
Epoch 80, training loss: 972.963623046875 = 1.0493454933166504 + 100.0 * 9.71914291381836
Epoch 80, val loss: 1.0480613708496094
Epoch 90, training loss: 963.7060546875 = 1.0427861213684082 + 100.0 * 9.626632690429688
Epoch 90, val loss: 1.0418388843536377
Epoch 100, training loss: 956.001708984375 = 1.0364933013916016 + 100.0 * 9.549652099609375
Epoch 100, val loss: 1.0357965230941772
Epoch 110, training loss: 945.1719360351562 = 1.0301733016967773 + 100.0 * 9.441417694091797
Epoch 110, val loss: 1.0296576023101807
Epoch 120, training loss: 930.7726440429688 = 1.0246630907058716 + 100.0 * 9.297479629516602
Epoch 120, val loss: 1.0243932008743286
Epoch 130, training loss: 915.1544189453125 = 1.0208766460418701 + 100.0 * 9.141335487365723
Epoch 130, val loss: 1.020969033241272
Epoch 140, training loss: 904.1123046875 = 1.0189566612243652 + 100.0 * 9.030933380126953
Epoch 140, val loss: 1.0192674398422241
Epoch 150, training loss: 899.9396362304688 = 1.0164093971252441 + 100.0 * 8.989232063293457
Epoch 150, val loss: 1.0166326761245728
Epoch 160, training loss: 895.662109375 = 1.012176513671875 + 100.0 * 8.946499824523926
Epoch 160, val loss: 1.012378454208374
Epoch 170, training loss: 890.1922607421875 = 1.0080114603042603 + 100.0 * 8.89184284210205
Epoch 170, val loss: 1.0083951950073242
Epoch 180, training loss: 885.9142456054688 = 1.0046101808547974 + 100.0 * 8.849096298217773
Epoch 180, val loss: 1.005197525024414
Epoch 190, training loss: 882.6480102539062 = 1.0011157989501953 + 100.0 * 8.816469192504883
Epoch 190, val loss: 1.001818060874939
Epoch 200, training loss: 879.2619018554688 = 0.997367262840271 + 100.0 * 8.782645225524902
Epoch 200, val loss: 0.9982055425643921
Epoch 210, training loss: 876.6946411132812 = 0.9935941696166992 + 100.0 * 8.757010459899902
Epoch 210, val loss: 0.9945620894432068
Epoch 220, training loss: 874.263916015625 = 0.989224374294281 + 100.0 * 8.732747077941895
Epoch 220, val loss: 0.990362286567688
Epoch 230, training loss: 872.0001220703125 = 0.9845455884933472 + 100.0 * 8.710155487060547
Epoch 230, val loss: 0.9859235286712646
Epoch 240, training loss: 869.7427368164062 = 0.979846715927124 + 100.0 * 8.687628746032715
Epoch 240, val loss: 0.9814741611480713
Epoch 250, training loss: 868.1019287109375 = 0.9749268293380737 + 100.0 * 8.671270370483398
Epoch 250, val loss: 0.9768065214157104
Epoch 260, training loss: 866.6571044921875 = 0.9695283770561218 + 100.0 * 8.656875610351562
Epoch 260, val loss: 0.9716190695762634
Epoch 270, training loss: 865.4703979492188 = 0.9636597037315369 + 100.0 * 8.64506721496582
Epoch 270, val loss: 0.9660200476646423
Epoch 280, training loss: 864.504638671875 = 0.9573500156402588 + 100.0 * 8.635473251342773
Epoch 280, val loss: 0.9599980115890503
Epoch 290, training loss: 863.7080688476562 = 0.950542688369751 + 100.0 * 8.627574920654297
Epoch 290, val loss: 0.9534993171691895
Epoch 300, training loss: 862.8697509765625 = 0.9433237314224243 + 100.0 * 8.619264602661133
Epoch 300, val loss: 0.9466792941093445
Epoch 310, training loss: 861.9739990234375 = 0.9358960390090942 + 100.0 * 8.610381126403809
Epoch 310, val loss: 0.9397124648094177
Epoch 320, training loss: 861.0280151367188 = 0.9283897876739502 + 100.0 * 8.600996017456055
Epoch 320, val loss: 0.9326549768447876
Epoch 330, training loss: 860.2554321289062 = 0.9206563234329224 + 100.0 * 8.593347549438477
Epoch 330, val loss: 0.9254946112632751
Epoch 340, training loss: 859.0850219726562 = 0.9128071665763855 + 100.0 * 8.581722259521484
Epoch 340, val loss: 0.9180465340614319
Epoch 350, training loss: 858.1390991210938 = 0.904727578163147 + 100.0 * 8.572343826293945
Epoch 350, val loss: 0.9105332493782043
Epoch 360, training loss: 857.7803344726562 = 0.8963119387626648 + 100.0 * 8.568840026855469
Epoch 360, val loss: 0.9027512073516846
Epoch 370, training loss: 856.767333984375 = 0.8874219059944153 + 100.0 * 8.558798789978027
Epoch 370, val loss: 0.8943290114402771
Epoch 380, training loss: 856.1915893554688 = 0.8781556487083435 + 100.0 * 8.553133964538574
Epoch 380, val loss: 0.8857362866401672
Epoch 390, training loss: 855.7085571289062 = 0.8686324954032898 + 100.0 * 8.548398971557617
Epoch 390, val loss: 0.8768535852432251
Epoch 400, training loss: 855.2777709960938 = 0.8588442802429199 + 100.0 * 8.544189453125
Epoch 400, val loss: 0.8678165674209595
Epoch 410, training loss: 854.8599243164062 = 0.8489537835121155 + 100.0 * 8.540109634399414
Epoch 410, val loss: 0.8586872220039368
Epoch 420, training loss: 854.4349975585938 = 0.8389817476272583 + 100.0 * 8.53596019744873
Epoch 420, val loss: 0.8495463728904724
Epoch 430, training loss: 854.0415649414062 = 0.8289223909378052 + 100.0 * 8.532126426696777
Epoch 430, val loss: 0.8403244018554688
Epoch 440, training loss: 853.6904907226562 = 0.8188008666038513 + 100.0 * 8.528717041015625
Epoch 440, val loss: 0.8310734033584595
Epoch 450, training loss: 853.12451171875 = 0.8087645769119263 + 100.0 * 8.523157119750977
Epoch 450, val loss: 0.8219487071037292
Epoch 460, training loss: 852.5883178710938 = 0.798785924911499 + 100.0 * 8.517895698547363
Epoch 460, val loss: 0.812872588634491
Epoch 470, training loss: 852.1328125 = 0.7888420820236206 + 100.0 * 8.513440132141113
Epoch 470, val loss: 0.8038622140884399
Epoch 480, training loss: 851.9866943359375 = 0.7788828611373901 + 100.0 * 8.512078285217285
Epoch 480, val loss: 0.7947865128517151
Epoch 490, training loss: 851.212158203125 = 0.7688055634498596 + 100.0 * 8.504433631896973
Epoch 490, val loss: 0.7857597470283508
Epoch 500, training loss: 850.811767578125 = 0.7588912844657898 + 100.0 * 8.500528335571289
Epoch 500, val loss: 0.7768270373344421
Epoch 510, training loss: 850.3886108398438 = 0.749049723148346 + 100.0 * 8.4963960647583
Epoch 510, val loss: 0.7679400444030762
Epoch 520, training loss: 850.01611328125 = 0.7392240166664124 + 100.0 * 8.492769241333008
Epoch 520, val loss: 0.7591292262077332
Epoch 530, training loss: 850.1327514648438 = 0.7294901609420776 + 100.0 * 8.494032859802246
Epoch 530, val loss: 0.7503229975700378
Epoch 540, training loss: 849.4940795898438 = 0.719692051410675 + 100.0 * 8.487744331359863
Epoch 540, val loss: 0.7416226267814636
Epoch 550, training loss: 849.1366577148438 = 0.7101251482963562 + 100.0 * 8.484265327453613
Epoch 550, val loss: 0.7330928444862366
Epoch 560, training loss: 848.8079223632812 = 0.7007024884223938 + 100.0 * 8.481072425842285
Epoch 560, val loss: 0.7247199416160583
Epoch 570, training loss: 848.5703125 = 0.6914460062980652 + 100.0 * 8.478788375854492
Epoch 570, val loss: 0.7165127396583557
Epoch 580, training loss: 848.5809936523438 = 0.6823340654373169 + 100.0 * 8.478986740112305
Epoch 580, val loss: 0.7084997892379761
Epoch 590, training loss: 848.1715087890625 = 0.673466145992279 + 100.0 * 8.474980354309082
Epoch 590, val loss: 0.7005293369293213
Epoch 600, training loss: 847.89501953125 = 0.6648213863372803 + 100.0 * 8.472302436828613
Epoch 600, val loss: 0.6929205060005188
Epoch 610, training loss: 847.6882934570312 = 0.6564465165138245 + 100.0 * 8.470318794250488
Epoch 610, val loss: 0.6855494976043701
Epoch 620, training loss: 847.6853637695312 = 0.6483431458473206 + 100.0 * 8.470370292663574
Epoch 620, val loss: 0.6784042119979858
Epoch 630, training loss: 847.3952026367188 = 0.6404445767402649 + 100.0 * 8.467547416687012
Epoch 630, val loss: 0.6714593172073364
Epoch 640, training loss: 847.1170654296875 = 0.6328653693199158 + 100.0 * 8.464841842651367
Epoch 640, val loss: 0.6648710370063782
Epoch 650, training loss: 846.8638305664062 = 0.6255976557731628 + 100.0 * 8.462382316589355
Epoch 650, val loss: 0.6585160493850708
Epoch 660, training loss: 846.707275390625 = 0.6186214089393616 + 100.0 * 8.46088695526123
Epoch 660, val loss: 0.6523913741111755
Epoch 670, training loss: 846.4678955078125 = 0.6117962598800659 + 100.0 * 8.458560943603516
Epoch 670, val loss: 0.6465373039245605
Epoch 680, training loss: 846.3207397460938 = 0.605289876461029 + 100.0 * 8.457154273986816
Epoch 680, val loss: 0.6408705711364746
Epoch 690, training loss: 846.1092529296875 = 0.5990737080574036 + 100.0 * 8.45510196685791
Epoch 690, val loss: 0.6354868412017822
Epoch 700, training loss: 845.9237670898438 = 0.5931005477905273 + 100.0 * 8.453307151794434
Epoch 700, val loss: 0.630354106426239
Epoch 710, training loss: 845.7271118164062 = 0.5873154401779175 + 100.0 * 8.451397895812988
Epoch 710, val loss: 0.6254021525382996
Epoch 720, training loss: 845.596923828125 = 0.5817697048187256 + 100.0 * 8.450151443481445
Epoch 720, val loss: 0.6206947565078735
Epoch 730, training loss: 845.3561401367188 = 0.5765135884284973 + 100.0 * 8.447795867919922
Epoch 730, val loss: 0.616207480430603
Epoch 740, training loss: 845.201904296875 = 0.5714706182479858 + 100.0 * 8.446304321289062
Epoch 740, val loss: 0.6119353175163269
Epoch 750, training loss: 845.3948974609375 = 0.5666090846061707 + 100.0 * 8.448283195495605
Epoch 750, val loss: 0.6078499555587769
Epoch 760, training loss: 844.9419555664062 = 0.56190025806427 + 100.0 * 8.443800926208496
Epoch 760, val loss: 0.6038961410522461
Epoch 770, training loss: 844.7808227539062 = 0.5574450492858887 + 100.0 * 8.44223403930664
Epoch 770, val loss: 0.6002016663551331
Epoch 780, training loss: 844.5990600585938 = 0.5531764030456543 + 100.0 * 8.440459251403809
Epoch 780, val loss: 0.5966681241989136
Epoch 790, training loss: 844.5240478515625 = 0.5490761995315552 + 100.0 * 8.439749717712402
Epoch 790, val loss: 0.5933013558387756
Epoch 800, training loss: 844.3572998046875 = 0.5450962781906128 + 100.0 * 8.438121795654297
Epoch 800, val loss: 0.5900669693946838
Epoch 810, training loss: 844.27197265625 = 0.5412910580635071 + 100.0 * 8.43730640411377
Epoch 810, val loss: 0.5869965553283691
Epoch 820, training loss: 844.1094360351562 = 0.5376635193824768 + 100.0 * 8.435717582702637
Epoch 820, val loss: 0.5840904116630554
Epoch 830, training loss: 843.9539184570312 = 0.534195601940155 + 100.0 * 8.434197425842285
Epoch 830, val loss: 0.5813412070274353
Epoch 840, training loss: 843.9265747070312 = 0.5308585166931152 + 100.0 * 8.43395709991455
Epoch 840, val loss: 0.5786781907081604
Epoch 850, training loss: 843.7994995117188 = 0.5275864005088806 + 100.0 * 8.432719230651855
Epoch 850, val loss: 0.5761909484863281
Epoch 860, training loss: 843.6717529296875 = 0.5244526267051697 + 100.0 * 8.431472778320312
Epoch 860, val loss: 0.5737166404724121
Epoch 870, training loss: 843.50927734375 = 0.5214833617210388 + 100.0 * 8.429878234863281
Epoch 870, val loss: 0.571466326713562
Epoch 880, training loss: 843.3883666992188 = 0.5186401009559631 + 100.0 * 8.42869758605957
Epoch 880, val loss: 0.5693392157554626
Epoch 890, training loss: 843.3154296875 = 0.5158737897872925 + 100.0 * 8.427995681762695
Epoch 890, val loss: 0.5672478675842285
Epoch 900, training loss: 843.2664184570312 = 0.5131826400756836 + 100.0 * 8.427532196044922
Epoch 900, val loss: 0.5652446150779724
Epoch 910, training loss: 843.0803833007812 = 0.5105890035629272 + 100.0 * 8.425698280334473
Epoch 910, val loss: 0.5633594989776611
Epoch 920, training loss: 843.1973266601562 = 0.5081194043159485 + 100.0 * 8.426892280578613
Epoch 920, val loss: 0.5615187287330627
Epoch 930, training loss: 842.9596557617188 = 0.5056458711624146 + 100.0 * 8.424539566040039
Epoch 930, val loss: 0.5598081350326538
Epoch 940, training loss: 842.8013916015625 = 0.5033179521560669 + 100.0 * 8.422981262207031
Epoch 940, val loss: 0.5581456422805786
Epoch 950, training loss: 842.697998046875 = 0.5010613203048706 + 100.0 * 8.421969413757324
Epoch 950, val loss: 0.5565868020057678
Epoch 960, training loss: 842.9827270507812 = 0.4988629221916199 + 100.0 * 8.42483901977539
Epoch 960, val loss: 0.5550734400749207
Epoch 970, training loss: 842.57958984375 = 0.49671608209609985 + 100.0 * 8.420828819274902
Epoch 970, val loss: 0.5535382032394409
Epoch 980, training loss: 842.442138671875 = 0.49468281865119934 + 100.0 * 8.419474601745605
Epoch 980, val loss: 0.5521906614303589
Epoch 990, training loss: 842.3637084960938 = 0.49271970987319946 + 100.0 * 8.418709754943848
Epoch 990, val loss: 0.5508444905281067
Epoch 1000, training loss: 842.2755126953125 = 0.49082857370376587 + 100.0 * 8.4178466796875
Epoch 1000, val loss: 0.5496093034744263
Epoch 1010, training loss: 842.37451171875 = 0.48898041248321533 + 100.0 * 8.418855667114258
Epoch 1010, val loss: 0.5484375953674316
Epoch 1020, training loss: 842.239990234375 = 0.48716408014297485 + 100.0 * 8.41752815246582
Epoch 1020, val loss: 0.5471007823944092
Epoch 1030, training loss: 842.0545654296875 = 0.4854303300380707 + 100.0 * 8.415691375732422
Epoch 1030, val loss: 0.5460175275802612
Epoch 1040, training loss: 842.054931640625 = 0.4837666153907776 + 100.0 * 8.415711402893066
Epoch 1040, val loss: 0.5449774861335754
Epoch 1050, training loss: 841.9690551757812 = 0.48212435841560364 + 100.0 * 8.41486930847168
Epoch 1050, val loss: 0.5439585447311401
Epoch 1060, training loss: 841.9056396484375 = 0.48054730892181396 + 100.0 * 8.414251327514648
Epoch 1060, val loss: 0.5429737567901611
Epoch 1070, training loss: 841.8157958984375 = 0.4790310859680176 + 100.0 * 8.413368225097656
Epoch 1070, val loss: 0.5420237183570862
Epoch 1080, training loss: 841.7283935546875 = 0.4775790870189667 + 100.0 * 8.412508010864258
Epoch 1080, val loss: 0.54114830493927
Epoch 1090, training loss: 841.668701171875 = 0.47617200016975403 + 100.0 * 8.411925315856934
Epoch 1090, val loss: 0.5403379797935486
Epoch 1100, training loss: 841.7684936523438 = 0.4748014509677887 + 100.0 * 8.41293716430664
Epoch 1100, val loss: 0.5395715832710266
Epoch 1110, training loss: 841.7230834960938 = 0.4734276235103607 + 100.0 * 8.412496566772461
Epoch 1110, val loss: 0.5386175513267517
Epoch 1120, training loss: 841.5165405273438 = 0.4721217453479767 + 100.0 * 8.410444259643555
Epoch 1120, val loss: 0.537936270236969
Epoch 1130, training loss: 841.450439453125 = 0.4708775579929352 + 100.0 * 8.409795761108398
Epoch 1130, val loss: 0.5372082591056824
Epoch 1140, training loss: 841.3818969726562 = 0.46967658400535583 + 100.0 * 8.409122467041016
Epoch 1140, val loss: 0.5365110635757446
Epoch 1150, training loss: 841.3135375976562 = 0.4685097932815552 + 100.0 * 8.40845012664795
Epoch 1150, val loss: 0.5358520150184631
Epoch 1160, training loss: 841.2591552734375 = 0.46737125515937805 + 100.0 * 8.407917976379395
Epoch 1160, val loss: 0.5352113842964172
Epoch 1170, training loss: 841.2073364257812 = 0.4662589430809021 + 100.0 * 8.407410621643066
Epoch 1170, val loss: 0.534574568271637
Epoch 1180, training loss: 842.1422729492188 = 0.46516263484954834 + 100.0 * 8.416770935058594
Epoch 1180, val loss: 0.533970057964325
Epoch 1190, training loss: 841.2364501953125 = 0.46400272846221924 + 100.0 * 8.407724380493164
Epoch 1190, val loss: 0.5332334041595459
Epoch 1200, training loss: 841.1080932617188 = 0.462943434715271 + 100.0 * 8.406451225280762
Epoch 1200, val loss: 0.5326468348503113
Epoch 1210, training loss: 841.0308227539062 = 0.46192944049835205 + 100.0 * 8.405689239501953
Epoch 1210, val loss: 0.5320841073989868
Epoch 1220, training loss: 840.9639892578125 = 0.4609561264514923 + 100.0 * 8.405030250549316
Epoch 1220, val loss: 0.5315889120101929
Epoch 1230, training loss: 840.90966796875 = 0.4600127339363098 + 100.0 * 8.404496192932129
Epoch 1230, val loss: 0.5310797095298767
Epoch 1240, training loss: 840.8628540039062 = 0.4590868651866913 + 100.0 * 8.404037475585938
Epoch 1240, val loss: 0.530559241771698
Epoch 1250, training loss: 840.9175415039062 = 0.4581783413887024 + 100.0 * 8.404593467712402
Epoch 1250, val loss: 0.5300604104995728
Epoch 1260, training loss: 840.843505859375 = 0.45724016427993774 + 100.0 * 8.403862953186035
Epoch 1260, val loss: 0.5295839309692383
Epoch 1270, training loss: 840.8134765625 = 0.4563353955745697 + 100.0 * 8.403571128845215
Epoch 1270, val loss: 0.5290742516517639
Epoch 1280, training loss: 840.7132568359375 = 0.45546406507492065 + 100.0 * 8.402578353881836
Epoch 1280, val loss: 0.5285738706588745
Epoch 1290, training loss: 840.6608276367188 = 0.4546249508857727 + 100.0 * 8.402061462402344
Epoch 1290, val loss: 0.5281632542610168
Epoch 1300, training loss: 840.6298217773438 = 0.4538002610206604 + 100.0 * 8.40176010131836
Epoch 1300, val loss: 0.5277512073516846
Epoch 1310, training loss: 840.8035278320312 = 0.45298272371292114 + 100.0 * 8.403505325317383
Epoch 1310, val loss: 0.5273112058639526
Epoch 1320, training loss: 840.6180419921875 = 0.4521538317203522 + 100.0 * 8.40165901184082
Epoch 1320, val loss: 0.5269214510917664
Epoch 1330, training loss: 840.5314331054688 = 0.45135298371315 + 100.0 * 8.400800704956055
Epoch 1330, val loss: 0.5264747738838196
Epoch 1340, training loss: 840.471435546875 = 0.4505458176136017 + 100.0 * 8.400208473205566
Epoch 1340, val loss: 0.5260635614395142
Epoch 1350, training loss: 840.5811157226562 = 0.4497746527194977 + 100.0 * 8.401313781738281
Epoch 1350, val loss: 0.5257205367088318
Epoch 1360, training loss: 840.4061279296875 = 0.4489622712135315 + 100.0 * 8.399571418762207
Epoch 1360, val loss: 0.5251294374465942
Epoch 1370, training loss: 840.4213256835938 = 0.4481987953186035 + 100.0 * 8.399731636047363
Epoch 1370, val loss: 0.524840235710144
Epoch 1380, training loss: 840.3311767578125 = 0.4474587142467499 + 100.0 * 8.398837089538574
Epoch 1380, val loss: 0.5243648886680603
Epoch 1390, training loss: 840.2720336914062 = 0.44674015045166016 + 100.0 * 8.398253440856934
Epoch 1390, val loss: 0.524014949798584
Epoch 1400, training loss: 840.3004760742188 = 0.44603651762008667 + 100.0 * 8.398544311523438
Epoch 1400, val loss: 0.5236585736274719
Epoch 1410, training loss: 840.255615234375 = 0.44530972838401794 + 100.0 * 8.398102760314941
Epoch 1410, val loss: 0.523209810256958
Epoch 1420, training loss: 840.1829223632812 = 0.4446004629135132 + 100.0 * 8.397383689880371
Epoch 1420, val loss: 0.5228783488273621
Epoch 1430, training loss: 840.1334228515625 = 0.44391125440597534 + 100.0 * 8.396895408630371
Epoch 1430, val loss: 0.5224665403366089
Epoch 1440, training loss: 840.130126953125 = 0.44323277473449707 + 100.0 * 8.396868705749512
Epoch 1440, val loss: 0.5221337676048279
Epoch 1450, training loss: 840.2661743164062 = 0.4425407648086548 + 100.0 * 8.398236274719238
Epoch 1450, val loss: 0.5217158794403076
Epoch 1460, training loss: 840.0985717773438 = 0.4418433904647827 + 100.0 * 8.396567344665527
Epoch 1460, val loss: 0.5214002132415771
Epoch 1470, training loss: 840.0145263671875 = 0.4411700963973999 + 100.0 * 8.395733833312988
Epoch 1470, val loss: 0.5209915637969971
Epoch 1480, training loss: 839.9381713867188 = 0.4405105710029602 + 100.0 * 8.394976615905762
Epoch 1480, val loss: 0.52070552110672
Epoch 1490, training loss: 839.8998413085938 = 0.43986696004867554 + 100.0 * 8.394599914550781
Epoch 1490, val loss: 0.520348310470581
Epoch 1500, training loss: 840.0291137695312 = 0.439225971698761 + 100.0 * 8.395898818969727
Epoch 1500, val loss: 0.5200294256210327
Epoch 1510, training loss: 839.9525146484375 = 0.43854010105133057 + 100.0 * 8.395139694213867
Epoch 1510, val loss: 0.5196609497070312
Epoch 1520, training loss: 839.8850708007812 = 0.43787282705307007 + 100.0 * 8.394472122192383
Epoch 1520, val loss: 0.5192670822143555
Epoch 1530, training loss: 839.7717895507812 = 0.4372197985649109 + 100.0 * 8.393345832824707
Epoch 1530, val loss: 0.5188599824905396
Epoch 1540, training loss: 839.7091064453125 = 0.43659278750419617 + 100.0 * 8.392724990844727
Epoch 1540, val loss: 0.5185499787330627
Epoch 1550, training loss: 839.7020874023438 = 0.43597733974456787 + 100.0 * 8.392661094665527
Epoch 1550, val loss: 0.5182377099990845
Epoch 1560, training loss: 839.8458862304688 = 0.435354620218277 + 100.0 * 8.394104957580566
Epoch 1560, val loss: 0.5178629755973816
Epoch 1570, training loss: 839.6837158203125 = 0.4347051978111267 + 100.0 * 8.39249038696289
Epoch 1570, val loss: 0.5174199342727661
Epoch 1580, training loss: 839.6843872070312 = 0.43407386541366577 + 100.0 * 8.392502784729004
Epoch 1580, val loss: 0.5171266794204712
Epoch 1590, training loss: 839.558349609375 = 0.43344876170158386 + 100.0 * 8.39124870300293
Epoch 1590, val loss: 0.5167232155799866
Epoch 1600, training loss: 839.5054931640625 = 0.43284085392951965 + 100.0 * 8.390726089477539
Epoch 1600, val loss: 0.5164512395858765
Epoch 1610, training loss: 839.6295776367188 = 0.4322333037853241 + 100.0 * 8.391973495483398
Epoch 1610, val loss: 0.5160879492759705
Epoch 1620, training loss: 839.458740234375 = 0.4316205382347107 + 100.0 * 8.390271186828613
Epoch 1620, val loss: 0.5156772136688232
Epoch 1630, training loss: 839.4151611328125 = 0.4310120940208435 + 100.0 * 8.389841079711914
Epoch 1630, val loss: 0.5153122544288635
Epoch 1640, training loss: 839.683349609375 = 0.4304022192955017 + 100.0 * 8.392529487609863
Epoch 1640, val loss: 0.5148953795433044
Epoch 1650, training loss: 839.4130859375 = 0.42976874113082886 + 100.0 * 8.389833450317383
Epoch 1650, val loss: 0.5146799087524414
Epoch 1660, training loss: 839.3092651367188 = 0.4291658401489258 + 100.0 * 8.388801574707031
Epoch 1660, val loss: 0.5141769647598267
Epoch 1670, training loss: 839.2485961914062 = 0.42857271432876587 + 100.0 * 8.388199806213379
Epoch 1670, val loss: 0.513913094997406
Epoch 1680, training loss: 839.2437744140625 = 0.4279863238334656 + 100.0 * 8.388157844543457
Epoch 1680, val loss: 0.51350337266922
Epoch 1690, training loss: 839.455078125 = 0.42737486958503723 + 100.0 * 8.390276908874512
Epoch 1690, val loss: 0.513143002986908
Epoch 1700, training loss: 839.2216796875 = 0.4267578721046448 + 100.0 * 8.387948989868164
Epoch 1700, val loss: 0.5127794146537781
Epoch 1710, training loss: 839.1278076171875 = 0.42616283893585205 + 100.0 * 8.387016296386719
Epoch 1710, val loss: 0.5123937726020813
Epoch 1720, training loss: 839.0768432617188 = 0.42557492852211 + 100.0 * 8.386512756347656
Epoch 1720, val loss: 0.5120809674263
Epoch 1730, training loss: 839.064453125 = 0.42498743534088135 + 100.0 * 8.386394500732422
Epoch 1730, val loss: 0.511688768863678
Epoch 1740, training loss: 839.3270263671875 = 0.42437535524368286 + 100.0 * 8.389026641845703
Epoch 1740, val loss: 0.5112459659576416
Epoch 1750, training loss: 839.0768432617188 = 0.4237608015537262 + 100.0 * 8.386530876159668
Epoch 1750, val loss: 0.5109052062034607
Epoch 1760, training loss: 839.004150390625 = 0.42314404249191284 + 100.0 * 8.385809898376465
Epoch 1760, val loss: 0.5105665922164917
Epoch 1770, training loss: 838.9208374023438 = 0.422549307346344 + 100.0 * 8.38498306274414
Epoch 1770, val loss: 0.5101435780525208
Epoch 1780, training loss: 838.8744506835938 = 0.42196837067604065 + 100.0 * 8.38452434539795
Epoch 1780, val loss: 0.5097848773002625
Epoch 1790, training loss: 838.8672485351562 = 0.4213806390762329 + 100.0 * 8.384458541870117
Epoch 1790, val loss: 0.5094592571258545
Epoch 1800, training loss: 839.13232421875 = 0.42077627778053284 + 100.0 * 8.387115478515625
Epoch 1800, val loss: 0.5091031193733215
Epoch 1810, training loss: 838.8682861328125 = 0.4201596975326538 + 100.0 * 8.384481430053711
Epoch 1810, val loss: 0.5086549520492554
Epoch 1820, training loss: 838.774169921875 = 0.4195520281791687 + 100.0 * 8.383545875549316
Epoch 1820, val loss: 0.508288562297821
Epoch 1830, training loss: 838.7242431640625 = 0.4189576208591461 + 100.0 * 8.383052825927734
Epoch 1830, val loss: 0.5079247355461121
Epoch 1840, training loss: 838.95458984375 = 0.418357253074646 + 100.0 * 8.38536262512207
Epoch 1840, val loss: 0.5076280832290649
Epoch 1850, training loss: 838.78173828125 = 0.4177248775959015 + 100.0 * 8.38364028930664
Epoch 1850, val loss: 0.5070545077323914
Epoch 1860, training loss: 838.7860107421875 = 0.41709572076797485 + 100.0 * 8.383688926696777
Epoch 1860, val loss: 0.5066964030265808
Epoch 1870, training loss: 838.6102294921875 = 0.4164785146713257 + 100.0 * 8.381937026977539
Epoch 1870, val loss: 0.5063381195068359
Epoch 1880, training loss: 838.6039428710938 = 0.41587722301483154 + 100.0 * 8.381880760192871
Epoch 1880, val loss: 0.5059311389923096
Epoch 1890, training loss: 838.5759887695312 = 0.4152745306491852 + 100.0 * 8.381607055664062
Epoch 1890, val loss: 0.5055996775627136
Epoch 1900, training loss: 838.5950927734375 = 0.41466307640075684 + 100.0 * 8.381804466247559
Epoch 1900, val loss: 0.5052082538604736
Epoch 1910, training loss: 838.607666015625 = 0.41403427720069885 + 100.0 * 8.381936073303223
Epoch 1910, val loss: 0.5047934055328369
Epoch 1920, training loss: 838.53662109375 = 0.41340696811676025 + 100.0 * 8.381232261657715
Epoch 1920, val loss: 0.5043167471885681
Epoch 1930, training loss: 838.5921630859375 = 0.4127712547779083 + 100.0 * 8.381793975830078
Epoch 1930, val loss: 0.5039322972297668
Epoch 1940, training loss: 838.6141357421875 = 0.41212010383605957 + 100.0 * 8.382019996643066
Epoch 1940, val loss: 0.5036201477050781
Epoch 1950, training loss: 838.4505004882812 = 0.4114725887775421 + 100.0 * 8.380390167236328
Epoch 1950, val loss: 0.5030965805053711
Epoch 1960, training loss: 838.424072265625 = 0.41083893179893494 + 100.0 * 8.380132675170898
Epoch 1960, val loss: 0.5027164220809937
Epoch 1970, training loss: 838.3677978515625 = 0.410213440656662 + 100.0 * 8.379575729370117
Epoch 1970, val loss: 0.502322256565094
Epoch 1980, training loss: 838.3473510742188 = 0.4095821678638458 + 100.0 * 8.379377365112305
Epoch 1980, val loss: 0.501930832862854
Epoch 1990, training loss: 838.3367309570312 = 0.4089384377002716 + 100.0 * 8.379278182983398
Epoch 1990, val loss: 0.5015317797660828
Epoch 2000, training loss: 838.5828247070312 = 0.4082716405391693 + 100.0 * 8.381745338439941
Epoch 2000, val loss: 0.5010702013969421
Epoch 2010, training loss: 838.5196533203125 = 0.4075964093208313 + 100.0 * 8.381120681762695
Epoch 2010, val loss: 0.5006813406944275
Epoch 2020, training loss: 838.3352661132812 = 0.40688997507095337 + 100.0 * 8.379283905029297
Epoch 2020, val loss: 0.5001733899116516
Epoch 2030, training loss: 838.2606201171875 = 0.4062216579914093 + 100.0 * 8.378543853759766
Epoch 2030, val loss: 0.4998514950275421
Epoch 2040, training loss: 838.222412109375 = 0.40556856989860535 + 100.0 * 8.378168106079102
Epoch 2040, val loss: 0.49940741062164307
Epoch 2050, training loss: 838.2084350585938 = 0.4049052894115448 + 100.0 * 8.378035545349121
Epoch 2050, val loss: 0.4989854097366333
Epoch 2060, training loss: 838.425048828125 = 0.40423205494880676 + 100.0 * 8.380208015441895
Epoch 2060, val loss: 0.49855470657348633
Epoch 2070, training loss: 838.3027954101562 = 0.4035075604915619 + 100.0 * 8.378993034362793
Epoch 2070, val loss: 0.49818331003189087
Epoch 2080, training loss: 838.2266235351562 = 0.4028039574623108 + 100.0 * 8.3782377243042
Epoch 2080, val loss: 0.49770623445510864
Epoch 2090, training loss: 838.1268920898438 = 0.40212365984916687 + 100.0 * 8.37724781036377
Epoch 2090, val loss: 0.49730655550956726
Epoch 2100, training loss: 838.0963745117188 = 0.40145090222358704 + 100.0 * 8.376949310302734
Epoch 2100, val loss: 0.49689966440200806
Epoch 2110, training loss: 838.2391357421875 = 0.40077075362205505 + 100.0 * 8.37838363647461
Epoch 2110, val loss: 0.4964257478713989
Epoch 2120, training loss: 838.070068359375 = 0.40004560351371765 + 100.0 * 8.376700401306152
Epoch 2120, val loss: 0.4960364103317261
Epoch 2130, training loss: 838.0521240234375 = 0.39934098720550537 + 100.0 * 8.376527786254883
Epoch 2130, val loss: 0.4955727458000183
Epoch 2140, training loss: 838.0386962890625 = 0.39865413308143616 + 100.0 * 8.376399993896484
Epoch 2140, val loss: 0.49523109197616577
Epoch 2150, training loss: 838.0064086914062 = 0.39797091484069824 + 100.0 * 8.376084327697754
Epoch 2150, val loss: 0.4947802722454071
Epoch 2160, training loss: 837.9795532226562 = 0.39728686213493347 + 100.0 * 8.375823020935059
Epoch 2160, val loss: 0.49438172578811646
Epoch 2170, training loss: 837.9649047851562 = 0.3965994715690613 + 100.0 * 8.375682830810547
Epoch 2170, val loss: 0.49394655227661133
Epoch 2180, training loss: 838.5784912109375 = 0.3958979547023773 + 100.0 * 8.381826400756836
Epoch 2180, val loss: 0.49341827630996704
Epoch 2190, training loss: 838.177978515625 = 0.3951314687728882 + 100.0 * 8.377828598022461
Epoch 2190, val loss: 0.49316293001174927
Epoch 2200, training loss: 837.97998046875 = 0.3943977952003479 + 100.0 * 8.375855445861816
Epoch 2200, val loss: 0.492671936750412
Epoch 2210, training loss: 837.8948364257812 = 0.3936822712421417 + 100.0 * 8.375011444091797
Epoch 2210, val loss: 0.49219244718551636
Epoch 2220, training loss: 837.8934936523438 = 0.39297959208488464 + 100.0 * 8.375004768371582
Epoch 2220, val loss: 0.4916945695877075
Epoch 2230, training loss: 837.9379272460938 = 0.39227473735809326 + 100.0 * 8.375456809997559
Epoch 2230, val loss: 0.49125364422798157
Epoch 2240, training loss: 837.9200439453125 = 0.39154160022735596 + 100.0 * 8.375285148620605
Epoch 2240, val loss: 0.4908495545387268
Epoch 2250, training loss: 837.8621215820312 = 0.3908005654811859 + 100.0 * 8.374712944030762
Epoch 2250, val loss: 0.4904639422893524
Epoch 2260, training loss: 837.7997436523438 = 0.39007681608200073 + 100.0 * 8.374096870422363
Epoch 2260, val loss: 0.4899952709674835
Epoch 2270, training loss: 837.8001708984375 = 0.389348566532135 + 100.0 * 8.37410831451416
Epoch 2270, val loss: 0.4895496666431427
Epoch 2280, training loss: 838.0386962890625 = 0.3886036276817322 + 100.0 * 8.376501083374023
Epoch 2280, val loss: 0.4891967177391052
Epoch 2290, training loss: 837.8140869140625 = 0.3878062963485718 + 100.0 * 8.374262809753418
Epoch 2290, val loss: 0.4886207580566406
Epoch 2300, training loss: 837.7781982421875 = 0.38703101873397827 + 100.0 * 8.37391185760498
Epoch 2300, val loss: 0.48815658688545227
Epoch 2310, training loss: 837.7341918945312 = 0.3862747848033905 + 100.0 * 8.373478889465332
Epoch 2310, val loss: 0.48767733573913574
Epoch 2320, training loss: 837.701416015625 = 0.38552847504615784 + 100.0 * 8.373159408569336
Epoch 2320, val loss: 0.4872957766056061
Epoch 2330, training loss: 837.7369384765625 = 0.38477659225463867 + 100.0 * 8.37352180480957
Epoch 2330, val loss: 0.4868479371070862
Epoch 2340, training loss: 837.8639526367188 = 0.3839920461177826 + 100.0 * 8.374799728393555
Epoch 2340, val loss: 0.48637571930885315
Epoch 2350, training loss: 837.7047119140625 = 0.38321444392204285 + 100.0 * 8.373214721679688
Epoch 2350, val loss: 0.4858839809894562
Epoch 2360, training loss: 837.6427612304688 = 0.3824385702610016 + 100.0 * 8.372603416442871
Epoch 2360, val loss: 0.4855000972747803
Epoch 2370, training loss: 837.611572265625 = 0.3816795349121094 + 100.0 * 8.372299194335938
Epoch 2370, val loss: 0.48510071635246277
Epoch 2380, training loss: 837.6238403320312 = 0.3809250593185425 + 100.0 * 8.372428894042969
Epoch 2380, val loss: 0.4846805930137634
Epoch 2390, training loss: 837.82861328125 = 0.3801472783088684 + 100.0 * 8.37448501586914
Epoch 2390, val loss: 0.4842201769351959
Epoch 2400, training loss: 837.6341552734375 = 0.37934717535972595 + 100.0 * 8.37254810333252
Epoch 2400, val loss: 0.4839014410972595
Epoch 2410, training loss: 837.5422973632812 = 0.37856513261795044 + 100.0 * 8.371637344360352
Epoch 2410, val loss: 0.48344314098358154
Epoch 2420, training loss: 837.5291748046875 = 0.37779733538627625 + 100.0 * 8.371513366699219
Epoch 2420, val loss: 0.48305994272232056
Epoch 2430, training loss: 837.7220458984375 = 0.3770275115966797 + 100.0 * 8.37345027923584
Epoch 2430, val loss: 0.48263028264045715
Epoch 2440, training loss: 837.5030517578125 = 0.37621331214904785 + 100.0 * 8.371268272399902
Epoch 2440, val loss: 0.48224639892578125
Epoch 2450, training loss: 837.4846801757812 = 0.3754137456417084 + 100.0 * 8.371092796325684
Epoch 2450, val loss: 0.4818810522556305
Epoch 2460, training loss: 837.4708251953125 = 0.37463104724884033 + 100.0 * 8.370962142944336
Epoch 2460, val loss: 0.4814208149909973
Epoch 2470, training loss: 837.5286254882812 = 0.3738510310649872 + 100.0 * 8.37154769897461
Epoch 2470, val loss: 0.48105865716934204
Epoch 2480, training loss: 837.4219360351562 = 0.3730529844760895 + 100.0 * 8.370489120483398
Epoch 2480, val loss: 0.480692058801651
Epoch 2490, training loss: 837.4129638671875 = 0.3722606897354126 + 100.0 * 8.370407104492188
Epoch 2490, val loss: 0.4803145229816437
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8051750380517504
0.8643773092805912
The final CL Acc:0.81465, 0.00810, The final GNN Acc:0.86360, 0.00104
Begin epxeriment: noisy_level: 0.2 cont_weight: 100 epoch: der1:0.3 der2:0.2 dfr1:0.4 dfr2:0.5
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=2500, cl_num_layers=2, cl_num_proj_hidden=64, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=100.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.2, drop_feat_rate_1=0.4, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=32, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, noisy_level=0.2, num_hidden=64, num_proj_hidden=64, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=40, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 88648])
add edge: torch.Size([2, 106454])
remove edge: torch.Size([2, 70884])
updated graph: torch.Size([2, 88690])
=== Noisy graph ===
/home/project-robust-contrastive/Robust-Contrastive-Learning/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 1059.3275146484375 = 1.0980743169784546 + 100.0 * 10.582294464111328
Epoch 0, val loss: 1.0968940258026123
Epoch 10, training loss: 1059.2923583984375 = 1.0942784547805786 + 100.0 * 10.581981658935547
Epoch 10, val loss: 1.0931390523910522
Epoch 20, training loss: 1059.1441650390625 = 1.090214490890503 + 100.0 * 10.58053970336914
Epoch 20, val loss: 1.0891081094741821
Epoch 30, training loss: 1058.47216796875 = 1.0857335329055786 + 100.0 * 10.573864936828613
Epoch 30, val loss: 1.08466637134552
Epoch 40, training loss: 1055.8441162109375 = 1.0805919170379639 + 100.0 * 10.547635078430176
Epoch 40, val loss: 1.0795689821243286
Epoch 50, training loss: 1047.9554443359375 = 1.074722409248352 + 100.0 * 10.468807220458984
Epoch 50, val loss: 1.0738450288772583
Epoch 60, training loss: 1028.2032470703125 = 1.069298267364502 + 100.0 * 10.271339416503906
Epoch 60, val loss: 1.068689227104187
Epoch 70, training loss: 1005.5354614257812 = 1.0640658140182495 + 100.0 * 10.044713973999023
Epoch 70, val loss: 1.0637633800506592
Epoch 80, training loss: 988.3952026367188 = 1.0596232414245605 + 100.0 * 9.873355865478516
Epoch 80, val loss: 1.0595030784606934
Epoch 90, training loss: 972.9755249023438 = 1.0552517175674438 + 100.0 * 9.719202995300293
Epoch 90, val loss: 1.0552250146865845
Epoch 100, training loss: 964.9705810546875 = 1.0513266324996948 + 100.0 * 9.639192581176758
Epoch 100, val loss: 1.051363468170166
Epoch 110, training loss: 957.8709106445312 = 1.0476409196853638 + 100.0 * 9.568232536315918
Epoch 110, val loss: 1.0477699041366577
Epoch 120, training loss: 942.70849609375 = 1.0442460775375366 + 100.0 * 9.416642189025879
Epoch 120, val loss: 1.0445622205734253
Epoch 130, training loss: 920.7601928710938 = 1.0419057607650757 + 100.0 * 9.197182655334473
Epoch 130, val loss: 1.0425306558609009
Epoch 140, training loss: 906.2822875976562 = 1.0405857563018799 + 100.0 * 9.052416801452637
Epoch 140, val loss: 1.041303277015686
Epoch 150, training loss: 895.8511962890625 = 1.0383259057998657 + 100.0 * 8.948128700256348
Epoch 150, val loss: 1.0389760732650757
Epoch 160, training loss: 890.1632080078125 = 1.0349740982055664 + 100.0 * 8.891282081604004
Epoch 160, val loss: 1.0357449054718018
Epoch 170, training loss: 886.1253051757812 = 1.0314902067184448 + 100.0 * 8.850937843322754
Epoch 170, val loss: 1.0324286222457886
Epoch 180, training loss: 883.09765625 = 1.028079628944397 + 100.0 * 8.820695877075195
Epoch 180, val loss: 1.0291744470596313
Epoch 190, training loss: 880.7869262695312 = 1.0244463682174683 + 100.0 * 8.797624588012695
Epoch 190, val loss: 1.025682806968689
Epoch 200, training loss: 878.8809814453125 = 1.0204172134399414 + 100.0 * 8.778605461120605
Epoch 200, val loss: 1.021795630455017
Epoch 210, training loss: 877.2706909179688 = 1.016052484512329 + 100.0 * 8.76254653930664
Epoch 210, val loss: 1.017588496208191
Epoch 220, training loss: 875.6703491210938 = 1.0114173889160156 + 100.0 * 8.746589660644531
Epoch 220, val loss: 1.0131510496139526
Epoch 230, training loss: 874.2174682617188 = 1.00650954246521 + 100.0 * 8.732109069824219
Epoch 230, val loss: 1.008443832397461
Epoch 240, training loss: 872.6050415039062 = 1.0012553930282593 + 100.0 * 8.71603775024414
Epoch 240, val loss: 1.0033822059631348
Epoch 250, training loss: 871.1715087890625 = 0.9956346750259399 + 100.0 * 8.701759338378906
Epoch 250, val loss: 0.9979739189147949
Epoch 260, training loss: 869.677490234375 = 0.9895560145378113 + 100.0 * 8.68687915802002
Epoch 260, val loss: 0.992135763168335
Epoch 270, training loss: 868.444580078125 = 0.9829944968223572 + 100.0 * 8.674615859985352
Epoch 270, val loss: 0.9858186841011047
Epoch 280, training loss: 867.386474609375 = 0.9758489727973938 + 100.0 * 8.664106369018555
Epoch 280, val loss: 0.9789334535598755
Epoch 290, training loss: 866.4307861328125 = 0.9681049585342407 + 100.0 * 8.654626846313477
Epoch 290, val loss: 0.9714665412902832
Epoch 300, training loss: 865.5488891601562 = 0.9597697257995605 + 100.0 * 8.645891189575195
Epoch 300, val loss: 0.9634741544723511
Epoch 310, training loss: 864.9414672851562 = 0.9508815407752991 + 100.0 * 8.63990592956543
Epoch 310, val loss: 0.9549210667610168
Epoch 320, training loss: 864.1004028320312 = 0.9414138793945312 + 100.0 * 8.631589889526367
Epoch 320, val loss: 0.9458456635475159
Epoch 330, training loss: 863.450927734375 = 0.9314143061637878 + 100.0 * 8.625195503234863
Epoch 330, val loss: 0.9362753033638
Epoch 340, training loss: 862.817626953125 = 0.9208906292915344 + 100.0 * 8.618967056274414
Epoch 340, val loss: 0.926214337348938
Epoch 350, training loss: 862.2824096679688 = 0.909938395023346 + 100.0 * 8.613724708557129
Epoch 350, val loss: 0.9157791137695312
Epoch 360, training loss: 861.7725830078125 = 0.898630678653717 + 100.0 * 8.608739852905273
Epoch 360, val loss: 0.9049854874610901
Epoch 370, training loss: 861.5401611328125 = 0.8869609236717224 + 100.0 * 8.606532096862793
Epoch 370, val loss: 0.8939244747161865
Epoch 380, training loss: 860.8711547851562 = 0.875127911567688 + 100.0 * 8.599960327148438
Epoch 380, val loss: 0.8827033042907715
Epoch 390, training loss: 860.3762817382812 = 0.8632237911224365 + 100.0 * 8.595130920410156
Epoch 390, val loss: 0.8714250326156616
Epoch 400, training loss: 859.9608764648438 = 0.8512417674064636 + 100.0 * 8.591095924377441
Epoch 400, val loss: 0.8601112961769104
Epoch 410, training loss: 860.2117309570312 = 0.8393188118934631 + 100.0 * 8.593724250793457
Epoch 410, val loss: 0.8488106727600098
Epoch 420, training loss: 859.326416015625 = 0.8274143934249878 + 100.0 * 8.584990501403809
Epoch 420, val loss: 0.8376737236976624
Epoch 430, training loss: 858.8089599609375 = 0.8156884908676147 + 100.0 * 8.579933166503906
Epoch 430, val loss: 0.8267142176628113
Epoch 440, training loss: 858.42041015625 = 0.8041837215423584 + 100.0 * 8.576162338256836
Epoch 440, val loss: 0.8159753084182739
Epoch 450, training loss: 858.0455322265625 = 0.7929172515869141 + 100.0 * 8.572525978088379
Epoch 450, val loss: 0.8054717183113098
Epoch 460, training loss: 857.964111328125 = 0.7819231748580933 + 100.0 * 8.571822166442871
Epoch 460, val loss: 0.7952361702919006
Epoch 470, training loss: 857.46875 = 0.7711341977119446 + 100.0 * 8.566976547241211
Epoch 470, val loss: 0.785248339176178
Epoch 480, training loss: 857.0955200195312 = 0.7607264518737793 + 100.0 * 8.563347816467285
Epoch 480, val loss: 0.775617778301239
Epoch 490, training loss: 856.8259887695312 = 0.7506064772605896 + 100.0 * 8.56075382232666
Epoch 490, val loss: 0.7662955522537231
Epoch 500, training loss: 856.5206909179688 = 0.7408028841018677 + 100.0 * 8.557799339294434
Epoch 500, val loss: 0.7572975158691406
Epoch 510, training loss: 856.2809448242188 = 0.7312989234924316 + 100.0 * 8.555496215820312
Epoch 510, val loss: 0.7486034035682678
Epoch 520, training loss: 856.2293090820312 = 0.7220574617385864 + 100.0 * 8.555072784423828
Epoch 520, val loss: 0.7401788234710693
Epoch 530, training loss: 855.87646484375 = 0.7131780385971069 + 100.0 * 8.55163288116455
Epoch 530, val loss: 0.7321086525917053
Epoch 540, training loss: 855.5663452148438 = 0.7045907378196716 + 100.0 * 8.548617362976074
Epoch 540, val loss: 0.7243490219116211
Epoch 550, training loss: 855.4296875 = 0.6962926983833313 + 100.0 * 8.547333717346191
Epoch 550, val loss: 0.7168874740600586
Epoch 560, training loss: 855.2400512695312 = 0.6882504224777222 + 100.0 * 8.545517921447754
Epoch 560, val loss: 0.709667980670929
Epoch 570, training loss: 855.0111083984375 = 0.6805258989334106 + 100.0 * 8.543305397033691
Epoch 570, val loss: 0.7027713656425476
Epoch 580, training loss: 854.8396606445312 = 0.6730414032936096 + 100.0 * 8.541666030883789
Epoch 580, val loss: 0.6961351037025452
Epoch 590, training loss: 854.7883911132812 = 0.6658154129981995 + 100.0 * 8.54122543334961
Epoch 590, val loss: 0.689769446849823
Epoch 600, training loss: 854.5023803710938 = 0.6588536500930786 + 100.0 * 8.538434982299805
Epoch 600, val loss: 0.6836565136909485
Epoch 610, training loss: 854.3255004882812 = 0.6521722674369812 + 100.0 * 8.536733627319336
Epoch 610, val loss: 0.6778123378753662
Epoch 620, training loss: 854.5642700195312 = 0.6457191705703735 + 100.0 * 8.539185523986816
Epoch 620, val loss: 0.6722361445426941
Epoch 630, training loss: 854.185302734375 = 0.6394872069358826 + 100.0 * 8.5354585647583
Epoch 630, val loss: 0.6668463349342346
Epoch 640, training loss: 853.856689453125 = 0.6335050463676453 + 100.0 * 8.532232284545898
Epoch 640, val loss: 0.6617351174354553
Epoch 650, training loss: 853.7122192382812 = 0.6277644634246826 + 100.0 * 8.530844688415527
Epoch 650, val loss: 0.656852126121521
Epoch 660, training loss: 853.6047973632812 = 0.6222306489944458 + 100.0 * 8.529825210571289
Epoch 660, val loss: 0.6522035598754883
Epoch 670, training loss: 853.4711303710938 = 0.6168756484985352 + 100.0 * 8.528542518615723
Epoch 670, val loss: 0.6477313041687012
Epoch 680, training loss: 853.2693481445312 = 0.6117681860923767 + 100.0 * 8.526576042175293
Epoch 680, val loss: 0.643496036529541
Epoch 690, training loss: 853.1080932617188 = 0.6068503260612488 + 100.0 * 8.525012016296387
Epoch 690, val loss: 0.6394713521003723
Epoch 700, training loss: 852.9633178710938 = 0.6021296977996826 + 100.0 * 8.523612022399902
Epoch 700, val loss: 0.6356297135353088
Epoch 710, training loss: 852.8931884765625 = 0.5975973606109619 + 100.0 * 8.522955894470215
Epoch 710, val loss: 0.631989061832428
Epoch 720, training loss: 852.72265625 = 0.5932146906852722 + 100.0 * 8.521294593811035
Epoch 720, val loss: 0.6284875273704529
Epoch 730, training loss: 852.665771484375 = 0.5890660881996155 + 100.0 * 8.520767211914062
Epoch 730, val loss: 0.6252160668373108
Epoch 740, training loss: 852.5006103515625 = 0.5850746035575867 + 100.0 * 8.519155502319336
Epoch 740, val loss: 0.6221564412117004
Epoch 750, training loss: 852.5621337890625 = 0.5812523365020752 + 100.0 * 8.519808769226074
Epoch 750, val loss: 0.6192063093185425
Epoch 760, training loss: 852.2470092773438 = 0.5775972008705139 + 100.0 * 8.516694068908691
Epoch 760, val loss: 0.6164100766181946
Epoch 770, training loss: 852.0965576171875 = 0.5741043090820312 + 100.0 * 8.51522445678711
Epoch 770, val loss: 0.6138208508491516
Epoch 780, training loss: 851.9970703125 = 0.5707647800445557 + 100.0 * 8.514263153076172
Epoch 780, val loss: 0.6113665699958801
Epoch 790, training loss: 852.380615234375 = 0.5675520300865173 + 100.0 * 8.5181303024292
Epoch 790, val loss: 0.6090186834335327
Epoch 800, training loss: 851.8197631835938 = 0.5644819140434265 + 100.0 * 8.512553215026855
Epoch 800, val loss: 0.6068116426467896
Epoch 810, training loss: 851.7439575195312 = 0.5615441203117371 + 100.0 * 8.511824607849121
Epoch 810, val loss: 0.6047897934913635
Epoch 820, training loss: 851.5977172851562 = 0.5587184429168701 + 100.0 * 8.510390281677246
Epoch 820, val loss: 0.6028336882591248
Epoch 830, training loss: 851.5222778320312 = 0.5560128092765808 + 100.0 * 8.509662628173828
Epoch 830, val loss: 0.6009582877159119
Epoch 840, training loss: 852.3290405273438 = 0.5534019470214844 + 100.0 * 8.517756462097168
Epoch 840, val loss: 0.5992389917373657
Epoch 850, training loss: 851.4059448242188 = 0.5508789420127869 + 100.0 * 8.508550643920898
Epoch 850, val loss: 0.597554087638855
Epoch 860, training loss: 851.275634765625 = 0.5484713315963745 + 100.0 * 8.507271766662598
Epoch 860, val loss: 0.5960044860839844
Epoch 870, training loss: 851.1853637695312 = 0.5461636185646057 + 100.0 * 8.506392478942871
Epoch 870, val loss: 0.5944892168045044
Epoch 880, training loss: 851.0859985351562 = 0.5439477562904358 + 100.0 * 8.505420684814453
Epoch 880, val loss: 0.5930760502815247
Epoch 890, training loss: 851.081787109375 = 0.541810154914856 + 100.0 * 8.505399703979492
Epoch 890, val loss: 0.591758668422699
Epoch 900, training loss: 851.0535888671875 = 0.5397201180458069 + 100.0 * 8.505138397216797
Epoch 900, val loss: 0.5905466675758362
Epoch 910, training loss: 850.8760986328125 = 0.5377219319343567 + 100.0 * 8.50338363647461
Epoch 910, val loss: 0.5892810821533203
Epoch 920, training loss: 850.771484375 = 0.5358008146286011 + 100.0 * 8.50235652923584
Epoch 920, val loss: 0.5881990194320679
Epoch 930, training loss: 850.68310546875 = 0.5339510440826416 + 100.0 * 8.50149154663086
Epoch 930, val loss: 0.5871014595031738
Epoch 940, training loss: 850.7175903320312 = 0.5321671962738037 + 100.0 * 8.501853942871094
Epoch 940, val loss: 0.5860627889633179
Epoch 950, training loss: 850.5720825195312 = 0.5304214358329773 + 100.0 * 8.50041675567627
Epoch 950, val loss: 0.585080087184906
Epoch 960, training loss: 850.5806274414062 = 0.5287354588508606 + 100.0 * 8.500518798828125
Epoch 960, val loss: 0.5840957164764404
Epoch 970, training loss: 850.384033203125 = 0.5270996689796448 + 100.0 * 8.49856948852539
Epoch 970, val loss: 0.5832760334014893
Epoch 980, training loss: 850.3181762695312 = 0.5255085825920105 + 100.0 * 8.497926712036133
Epoch 980, val loss: 0.5823876857757568
Epoch 990, training loss: 850.7357177734375 = 0.5239661931991577 + 100.0 * 8.502117156982422
Epoch 990, val loss: 0.5815213322639465
Epoch 1000, training loss: 850.4013671875 = 0.5224530100822449 + 100.0 * 8.498788833618164
Epoch 1000, val loss: 0.5807796716690063
Epoch 1010, training loss: 850.1227416992188 = 0.5209852457046509 + 100.0 * 8.496017456054688
Epoch 1010, val loss: 0.5800621509552002
Epoch 1020, training loss: 850.0316162109375 = 0.519554078578949 + 100.0 * 8.495121002197266
Epoch 1020, val loss: 0.5792561769485474
Epoch 1030, training loss: 849.9385375976562 = 0.5181590914726257 + 100.0 * 8.494203567504883
Epoch 1030, val loss: 0.5785549283027649
Epoch 1040, training loss: 849.8558959960938 = 0.5167925953865051 + 100.0 * 8.493391036987305
Epoch 1040, val loss: 0.5778802037239075
Epoch 1050, training loss: 849.781494140625 = 0.5154538750648499 + 100.0 * 8.492660522460938
Epoch 1050, val loss: 0.5772038102149963
Epoch 1060, training loss: 849.7635498046875 = 0.5141441822052002 + 100.0 * 8.492493629455566
Epoch 1060, val loss: 0.576556384563446
Epoch 1070, training loss: 850.0880126953125 = 0.5128491520881653 + 100.0 * 8.49575138092041
Epoch 1070, val loss: 0.5758883357048035
Epoch 1080, training loss: 849.7946166992188 = 0.5115756988525391 + 100.0 * 8.492830276489258
Epoch 1080, val loss: 0.575224757194519
Epoch 1090, training loss: 849.615966796875 = 0.5103387832641602 + 100.0 * 8.491056442260742
Epoch 1090, val loss: 0.5747132897377014
Epoch 1100, training loss: 849.571533203125 = 0.5091368556022644 + 100.0 * 8.490623474121094
Epoch 1100, val loss: 0.5740611553192139
Epoch 1110, training loss: 849.5293579101562 = 0.5079550743103027 + 100.0 * 8.490214347839355
Epoch 1110, val loss: 0.5734742283821106
Epoch 1120, training loss: 849.4022216796875 = 0.506793737411499 + 100.0 * 8.488954544067383
Epoch 1120, val loss: 0.5729394555091858
Epoch 1130, training loss: 849.2744140625 = 0.5056481957435608 + 100.0 * 8.487687110900879
Epoch 1130, val loss: 0.572346568107605
Epoch 1140, training loss: 849.2356567382812 = 0.5045243501663208 + 100.0 * 8.487311363220215
Epoch 1140, val loss: 0.5717893242835999
Epoch 1150, training loss: 849.5159912109375 = 0.5034167766571045 + 100.0 * 8.49012565612793
Epoch 1150, val loss: 0.5712609887123108
Epoch 1160, training loss: 849.30810546875 = 0.5023182034492493 + 100.0 * 8.488058090209961
Epoch 1160, val loss: 0.5707029104232788
Epoch 1170, training loss: 849.15625 = 0.5012325048446655 + 100.0 * 8.486550331115723
Epoch 1170, val loss: 0.57017982006073
Epoch 1180, training loss: 849.072998046875 = 0.5001722574234009 + 100.0 * 8.48572826385498
Epoch 1180, val loss: 0.5696460008621216
Epoch 1190, training loss: 848.9730834960938 = 0.49912887811660767 + 100.0 * 8.484739303588867
Epoch 1190, val loss: 0.5691448450088501
Epoch 1200, training loss: 849.4703369140625 = 0.4980952739715576 + 100.0 * 8.48972225189209
Epoch 1200, val loss: 0.5686911940574646
Epoch 1210, training loss: 849.0136108398438 = 0.4970634877681732 + 100.0 * 8.4851655960083
Epoch 1210, val loss: 0.5680550336837769
Epoch 1220, training loss: 848.802978515625 = 0.4960518181324005 + 100.0 * 8.48306941986084
Epoch 1220, val loss: 0.5676242709159851
Epoch 1230, training loss: 848.7289428710938 = 0.49506479501724243 + 100.0 * 8.482338905334473
Epoch 1230, val loss: 0.5671383142471313
Epoch 1240, training loss: 848.673828125 = 0.49408912658691406 + 100.0 * 8.481797218322754
Epoch 1240, val loss: 0.5666563510894775
Epoch 1250, training loss: 848.6864624023438 = 0.493124783039093 + 100.0 * 8.48193359375
Epoch 1250, val loss: 0.5661802887916565
Epoch 1260, training loss: 848.7833251953125 = 0.49215415120124817 + 100.0 * 8.482912063598633
Epoch 1260, val loss: 0.5658539533615112
Epoch 1270, training loss: 848.7625122070312 = 0.49118316173553467 + 100.0 * 8.482712745666504
Epoch 1270, val loss: 0.5652267336845398
Epoch 1280, training loss: 848.5786743164062 = 0.49023205041885376 + 100.0 * 8.480884552001953
Epoch 1280, val loss: 0.5648177862167358
Epoch 1290, training loss: 848.458251953125 = 0.4893045723438263 + 100.0 * 8.479689598083496
Epoch 1290, val loss: 0.5643415451049805
Epoch 1300, training loss: 848.3875732421875 = 0.4883863031864166 + 100.0 * 8.478991508483887
Epoch 1300, val loss: 0.5639725923538208
Epoch 1310, training loss: 848.3660278320312 = 0.487474262714386 + 100.0 * 8.478785514831543
Epoch 1310, val loss: 0.5635256767272949
Epoch 1320, training loss: 848.9734497070312 = 0.48656800389289856 + 100.0 * 8.484869003295898
Epoch 1320, val loss: 0.5631343126296997
Epoch 1330, training loss: 848.7703857421875 = 0.48564332723617554 + 100.0 * 8.482847213745117
Epoch 1330, val loss: 0.5626981258392334
Epoch 1340, training loss: 848.3071899414062 = 0.48473435640335083 + 100.0 * 8.478224754333496
Epoch 1340, val loss: 0.5621824860572815
Epoch 1350, training loss: 848.1849975585938 = 0.48385196924209595 + 100.0 * 8.477011680603027
Epoch 1350, val loss: 0.5618277192115784
Epoch 1360, training loss: 848.1257934570312 = 0.48297926783561707 + 100.0 * 8.476428031921387
Epoch 1360, val loss: 0.5613945722579956
Epoch 1370, training loss: 848.0802612304688 = 0.48211440443992615 + 100.0 * 8.475981712341309
Epoch 1370, val loss: 0.5609819293022156
Epoch 1380, training loss: 848.0739135742188 = 0.48125216364860535 + 100.0 * 8.475926399230957
Epoch 1380, val loss: 0.5605506896972656
Epoch 1390, training loss: 848.4678955078125 = 0.48038962483406067 + 100.0 * 8.479874610900879
Epoch 1390, val loss: 0.5601419806480408
Epoch 1400, training loss: 848.1082763671875 = 0.4795222580432892 + 100.0 * 8.476287841796875
Epoch 1400, val loss: 0.5597285628318787
Epoch 1410, training loss: 847.9654541015625 = 0.4786685109138489 + 100.0 * 8.474867820739746
Epoch 1410, val loss: 0.5592811703681946
Epoch 1420, training loss: 847.8841552734375 = 0.4778233468532562 + 100.0 * 8.4740629196167
Epoch 1420, val loss: 0.558877170085907
Epoch 1430, training loss: 847.8637084960938 = 0.47698351740837097 + 100.0 * 8.473867416381836
Epoch 1430, val loss: 0.5584002733230591
Epoch 1440, training loss: 848.07861328125 = 0.4761419892311096 + 100.0 * 8.476024627685547
Epoch 1440, val loss: 0.5580483675003052
Epoch 1450, training loss: 847.7940063476562 = 0.4753001630306244 + 100.0 * 8.473187446594238
Epoch 1450, val loss: 0.5576267838478088
Epoch 1460, training loss: 848.4197998046875 = 0.4744650721549988 + 100.0 * 8.479453086853027
Epoch 1460, val loss: 0.557346761226654
Epoch 1470, training loss: 847.862548828125 = 0.47362813353538513 + 100.0 * 8.473889350891113
Epoch 1470, val loss: 0.5567911863327026
Epoch 1480, training loss: 847.7023315429688 = 0.47279930114746094 + 100.0 * 8.472295761108398
Epoch 1480, val loss: 0.5563808679580688
Epoch 1490, training loss: 847.6019897460938 = 0.47198471426963806 + 100.0 * 8.47130012512207
Epoch 1490, val loss: 0.5559735298156738
Epoch 1500, training loss: 847.5504760742188 = 0.4711732566356659 + 100.0 * 8.470792770385742
Epoch 1500, val loss: 0.5556270480155945
Epoch 1510, training loss: 847.5084228515625 = 0.470365434885025 + 100.0 * 8.470380783081055
Epoch 1510, val loss: 0.5552106499671936
Epoch 1520, training loss: 847.49951171875 = 0.4695553779602051 + 100.0 * 8.47029972076416
Epoch 1520, val loss: 0.5548163652420044
Epoch 1530, training loss: 847.869873046875 = 0.46874427795410156 + 100.0 * 8.474011421203613
Epoch 1530, val loss: 0.5544229745864868
Epoch 1540, training loss: 848.006103515625 = 0.46791258454322815 + 100.0 * 8.475381851196289
Epoch 1540, val loss: 0.5539385676383972
Epoch 1550, training loss: 847.4236450195312 = 0.46708914637565613 + 100.0 * 8.469565391540527
Epoch 1550, val loss: 0.553602397441864
Epoch 1560, training loss: 847.3838500976562 = 0.46628037095069885 + 100.0 * 8.469175338745117
Epoch 1560, val loss: 0.5532145500183105
Epoch 1570, training loss: 847.31982421875 = 0.46547794342041016 + 100.0 * 8.468544006347656
Epoch 1570, val loss: 0.5527915358543396
Epoch 1580, training loss: 847.263916015625 = 0.4646843373775482 + 100.0 * 8.467992782592773
Epoch 1580, val loss: 0.552386999130249
Epoch 1590, training loss: 847.246826171875 = 0.4638901352882385 + 100.0 * 8.467829704284668
Epoch 1590, val loss: 0.5519595742225647
Epoch 1600, training loss: 847.7266235351562 = 0.46309468150138855 + 100.0 * 8.472635269165039
Epoch 1600, val loss: 0.5514875650405884
Epoch 1610, training loss: 847.3197021484375 = 0.46228450536727905 + 100.0 * 8.468574523925781
Epoch 1610, val loss: 0.5512045621871948
Epoch 1620, training loss: 847.215087890625 = 0.4614799916744232 + 100.0 * 8.467535972595215
Epoch 1620, val loss: 0.5508182644844055
Epoch 1630, training loss: 847.4613647460938 = 0.4606787860393524 + 100.0 * 8.470006942749023
Epoch 1630, val loss: 0.5503557920455933
Epoch 1640, training loss: 847.270263671875 = 0.4598788321018219 + 100.0 * 8.468103408813477
Epoch 1640, val loss: 0.5499839186668396
Epoch 1650, training loss: 847.1730346679688 = 0.45908311009407043 + 100.0 * 8.46713924407959
Epoch 1650, val loss: 0.5496069192886353
Epoch 1660, training loss: 847.08447265625 = 0.4582909941673279 + 100.0 * 8.466261863708496
Epoch 1660, val loss: 0.5491753816604614
Epoch 1670, training loss: 847.0623168945312 = 0.45750054717063904 + 100.0 * 8.466048240661621
Epoch 1670, val loss: 0.5488327145576477
Epoch 1680, training loss: 847.16748046875 = 0.45670419931411743 + 100.0 * 8.467107772827148
Epoch 1680, val loss: 0.5484195947647095
Epoch 1690, training loss: 846.938232421875 = 0.4559059739112854 + 100.0 * 8.464822769165039
Epoch 1690, val loss: 0.548008918762207
Epoch 1700, training loss: 846.9366455078125 = 0.45511338114738464 + 100.0 * 8.464815139770508
Epoch 1700, val loss: 0.5476246476173401
Epoch 1710, training loss: 846.9816284179688 = 0.4543197453022003 + 100.0 * 8.465272903442383
Epoch 1710, val loss: 0.5472224950790405
Epoch 1720, training loss: 846.9776000976562 = 0.4535231590270996 + 100.0 * 8.465240478515625
Epoch 1720, val loss: 0.546832799911499
Epoch 1730, training loss: 847.078857421875 = 0.4527243673801422 + 100.0 * 8.466261863708496
Epoch 1730, val loss: 0.5464669466018677
Epoch 1740, training loss: 846.8065795898438 = 0.45191970467567444 + 100.0 * 8.463546752929688
Epoch 1740, val loss: 0.5459661483764648
Epoch 1750, training loss: 846.7764892578125 = 0.4511284828186035 + 100.0 * 8.46325397491455
Epoch 1750, val loss: 0.5456278324127197
Epoch 1760, training loss: 846.9436645507812 = 0.4503372013568878 + 100.0 * 8.464933395385742
Epoch 1760, val loss: 0.5451948046684265
Epoch 1770, training loss: 846.7136840820312 = 0.449544221162796 + 100.0 * 8.462641716003418
Epoch 1770, val loss: 0.5447937846183777
Epoch 1780, training loss: 846.6834106445312 = 0.4487525224685669 + 100.0 * 8.462347030639648
Epoch 1780, val loss: 0.5444181561470032
Epoch 1790, training loss: 846.6754150390625 = 0.4479581415653229 + 100.0 * 8.462274551391602
Epoch 1790, val loss: 0.5439642667770386
Epoch 1800, training loss: 847.4510498046875 = 0.4471653997898102 + 100.0 * 8.470039367675781
Epoch 1800, val loss: 0.543603777885437
Epoch 1810, training loss: 846.9216918945312 = 0.4463510811328888 + 100.0 * 8.464753150939941
Epoch 1810, val loss: 0.5430586338043213
Epoch 1820, training loss: 846.6337890625 = 0.4455447793006897 + 100.0 * 8.461882591247559
Epoch 1820, val loss: 0.5427308082580566
Epoch 1830, training loss: 846.5219116210938 = 0.4447486996650696 + 100.0 * 8.460771560668945
Epoch 1830, val loss: 0.5422905087471008
Epoch 1840, training loss: 846.4862060546875 = 0.4439546465873718 + 100.0 * 8.46042251586914
Epoch 1840, val loss: 0.54185551404953
Epoch 1850, training loss: 846.4599609375 = 0.4431611895561218 + 100.0 * 8.46016788482666
Epoch 1850, val loss: 0.541495144367218
Epoch 1860, training loss: 846.5164794921875 = 0.44236740469932556 + 100.0 * 8.46074104309082
Epoch 1860, val loss: 0.5411220788955688
Epoch 1870, training loss: 846.9717407226562 = 0.4415642023086548 + 100.0 * 8.465301513671875
Epoch 1870, val loss: 0.5408992171287537
Epoch 1880, training loss: 846.629638671875 = 0.44073495268821716 + 100.0 * 8.461889266967773
Epoch 1880, val loss: 0.5402246713638306
Epoch 1890, training loss: 846.3880615234375 = 0.4399302005767822 + 100.0 * 8.459481239318848
Epoch 1890, val loss: 0.539955735206604
Epoch 1900, training loss: 846.3558959960938 = 0.43912971019744873 + 100.0 * 8.45916748046875
Epoch 1900, val loss: 0.5395910143852234
Epoch 1910, training loss: 846.2968139648438 = 0.43833687901496887 + 100.0 * 8.458584785461426
Epoch 1910, val loss: 0.5392444133758545
Epoch 1920, training loss: 846.3104248046875 = 0.43754249811172485 + 100.0 * 8.458728790283203
Epoch 1920, val loss: 0.5388675332069397
Epoch 1930, training loss: 846.9212646484375 = 0.4367463290691376 + 100.0 * 8.464844703674316
Epoch 1930, val loss: 0.5386080145835876
Epoch 1940, training loss: 846.408447265625 = 0.4359307587146759 + 100.0 * 8.459725379943848
Epoch 1940, val loss: 0.5381849408149719
Epoch 1950, training loss: 846.2172241210938 = 0.43512091040611267 + 100.0 * 8.457820892333984
Epoch 1950, val loss: 0.5377978682518005
Epoch 1960, training loss: 846.2157592773438 = 0.4343256652355194 + 100.0 * 8.45781421661377
Epoch 1960, val loss: 0.5375698208808899
Epoch 1970, training loss: 846.1807861328125 = 0.43352949619293213 + 100.0 * 8.457472801208496
Epoch 1970, val loss: 0.5372117757797241
Epoch 1980, training loss: 846.4060668945312 = 0.432727575302124 + 100.0 * 8.459733009338379
Epoch 1980, val loss: 0.5369022488594055
Epoch 1990, training loss: 846.1355590820312 = 0.43192389607429504 + 100.0 * 8.457036018371582
Epoch 1990, val loss: 0.5365448594093323
Epoch 2000, training loss: 846.3461303710938 = 0.4311175048351288 + 100.0 * 8.459150314331055
Epoch 2000, val loss: 0.5361859798431396
Epoch 2010, training loss: 846.0659790039062 = 0.4303058087825775 + 100.0 * 8.4563570022583
Epoch 2010, val loss: 0.5359561443328857
Epoch 2020, training loss: 846.0560302734375 = 0.42950811982154846 + 100.0 * 8.456265449523926
Epoch 2020, val loss: 0.5356004238128662
Epoch 2030, training loss: 846.029052734375 = 0.4287079870700836 + 100.0 * 8.456003189086914
Epoch 2030, val loss: 0.5352357625961304
Epoch 2040, training loss: 846.1817626953125 = 0.42791035771369934 + 100.0 * 8.457538604736328
Epoch 2040, val loss: 0.5349941253662109
Epoch 2050, training loss: 845.9947509765625 = 0.42709603905677795 + 100.0 * 8.455676078796387
Epoch 2050, val loss: 0.5347145199775696
Epoch 2060, training loss: 845.94921875 = 0.42628008127212524 + 100.0 * 8.455229759216309
Epoch 2060, val loss: 0.5343808531761169
Epoch 2070, training loss: 845.8976440429688 = 0.4254739284515381 + 100.0 * 8.454721450805664
Epoch 2070, val loss: 0.5340591073036194
Epoch 2080, training loss: 845.9329833984375 = 0.42466989159584045 + 100.0 * 8.455082893371582
Epoch 2080, val loss: 0.5337009429931641
Epoch 2090, training loss: 846.4684448242188 = 0.423859179019928 + 100.0 * 8.460445404052734
Epoch 2090, val loss: 0.5334476232528687
Epoch 2100, training loss: 845.895263671875 = 0.4230293035507202 + 100.0 * 8.45472240447998
Epoch 2100, val loss: 0.533182680606842
Epoch 2110, training loss: 845.7855834960938 = 0.42221763730049133 + 100.0 * 8.453633308410645
Epoch 2110, val loss: 0.5329008102416992
Epoch 2120, training loss: 845.7625732421875 = 0.42140746116638184 + 100.0 * 8.453412055969238
Epoch 2120, val loss: 0.532505214214325
Epoch 2130, training loss: 845.7432250976562 = 0.4206067621707916 + 100.0 * 8.453226089477539
Epoch 2130, val loss: 0.5323249697685242
Epoch 2140, training loss: 846.3245239257812 = 0.41981393098831177 + 100.0 * 8.459047317504883
Epoch 2140, val loss: 0.5318819880485535
Epoch 2150, training loss: 845.8792114257812 = 0.41898059844970703 + 100.0 * 8.454602241516113
Epoch 2150, val loss: 0.5318719148635864
Epoch 2160, training loss: 845.7108154296875 = 0.4181692898273468 + 100.0 * 8.452926635742188
Epoch 2160, val loss: 0.5314820408821106
Epoch 2170, training loss: 845.7488403320312 = 0.41735807061195374 + 100.0 * 8.453314781188965
Epoch 2170, val loss: 0.5311840772628784
Epoch 2180, training loss: 845.7083740234375 = 0.41654840111732483 + 100.0 * 8.45291805267334
Epoch 2180, val loss: 0.5310238599777222
Epoch 2190, training loss: 845.6627807617188 = 0.41574129462242126 + 100.0 * 8.452469825744629
Epoch 2190, val loss: 0.5308337807655334
Epoch 2200, training loss: 845.9451293945312 = 0.4149264097213745 + 100.0 * 8.455302238464355
Epoch 2200, val loss: 0.5305755138397217
Epoch 2210, training loss: 845.5406494140625 = 0.41409581899642944 + 100.0 * 8.451265335083008
Epoch 2210, val loss: 0.5302940607070923
Epoch 2220, training loss: 845.5062255859375 = 0.41327741742134094 + 100.0 * 8.450929641723633
Epoch 2220, val loss: 0.5301113724708557
Epoch 2230, training loss: 845.476806640625 = 0.41246506571769714 + 100.0 * 8.450643539428711
Epoch 2230, val loss: 0.5298657417297363
Epoch 2240, training loss: 845.483642578125 = 0.41165322065353394 + 100.0 * 8.450719833374023
Epoch 2240, val loss: 0.5296639204025269
Epoch 2250, training loss: 845.761474609375 = 0.4108406603336334 + 100.0 * 8.453506469726562
Epoch 2250, val loss: 0.5295387506484985
Epoch 2260, training loss: 845.5135498046875 = 0.4100032150745392 + 100.0 * 8.451035499572754
Epoch 2260, val loss: 0.5291992425918579
Epoch 2270, training loss: 845.4013671875 = 0.4091821014881134 + 100.0 * 8.449921607971191
Epoch 2270, val loss: 0.529060959815979
Epoch 2280, training loss: 845.3417358398438 = 0.40836301445961 + 100.0 * 8.449333190917969
Epoch 2280, val loss: 0.5287613868713379
Epoch 2290, training loss: 845.6842651367188 = 0.4075489938259125 + 100.0 * 8.452767372131348
Epoch 2290, val loss: 0.5285848379135132
Epoch 2300, training loss: 845.3734741210938 = 0.4067060053348541 + 100.0 * 8.449667930603027
Epoch 2300, val loss: 0.5283794403076172
Epoch 2310, training loss: 845.3109741210938 = 0.40587589144706726 + 100.0 * 8.449050903320312
Epoch 2310, val loss: 0.5282297134399414
Epoch 2320, training loss: 845.4548950195312 = 0.40504592657089233 + 100.0 * 8.450498580932617
Epoch 2320, val loss: 0.5279877781867981
Epoch 2330, training loss: 845.2542724609375 = 0.40421006083488464 + 100.0 * 8.448500633239746
Epoch 2330, val loss: 0.5278482437133789
Epoch 2340, training loss: 845.2481079101562 = 0.4033821225166321 + 100.0 * 8.448447227478027
Epoch 2340, val loss: 0.5277724862098694
Epoch 2350, training loss: 845.1421508789062 = 0.4025518596172333 + 100.0 * 8.447396278381348
Epoch 2350, val loss: 0.5275909304618835
Epoch 2360, training loss: 845.1669921875 = 0.40172648429870605 + 100.0 * 8.447652816772461
Epoch 2360, val loss: 0.5274272561073303
Epoch 2370, training loss: 845.53515625 = 0.4008960425853729 + 100.0 * 8.451342582702637
Epoch 2370, val loss: 0.5273330211639404
Epoch 2380, training loss: 845.2675170898438 = 0.4000582695007324 + 100.0 * 8.448674201965332
Epoch 2380, val loss: 0.5273270606994629
Epoch 2390, training loss: 845.1030883789062 = 0.39921605587005615 + 100.0 * 8.447038650512695
Epoch 2390, val loss: 0.527066707611084
Epoch 2400, training loss: 845.072509765625 = 0.3983820974826813 + 100.0 * 8.446741104125977
Epoch 2400, val loss: 0.5270600914955139
Epoch 2410, training loss: 845.4574584960938 = 0.39754775166511536 + 100.0 * 8.450599670410156
Epoch 2410, val loss: 0.5269901752471924
Epoch 2420, training loss: 845.0750732421875 = 0.39669033885002136 + 100.0 * 8.446784019470215
Epoch 2420, val loss: 0.5267933011054993
Epoch 2430, training loss: 844.9658813476562 = 0.39583906531333923 + 100.0 * 8.445700645446777
Epoch 2430, val loss: 0.5267856121063232
Epoch 2440, training loss: 844.9248657226562 = 0.3949899673461914 + 100.0 * 8.445298194885254
Epoch 2440, val loss: 0.5267462134361267
Epoch 2450, training loss: 844.9060668945312 = 0.39413875341415405 + 100.0 * 8.44511890411377
Epoch 2450, val loss: 0.526671826839447
Epoch 2460, training loss: 845.3623046875 = 0.3932892382144928 + 100.0 * 8.449689865112305
Epoch 2460, val loss: 0.5267190337181091
Epoch 2470, training loss: 845.1663818359375 = 0.3924162983894348 + 100.0 * 8.447739601135254
Epoch 2470, val loss: 0.5263631343841553
Epoch 2480, training loss: 844.9110717773438 = 0.39154449105262756 + 100.0 * 8.445195198059082
Epoch 2480, val loss: 0.5265063643455505
Epoch 2490, training loss: 844.8087158203125 = 0.39066916704177856 + 100.0 * 8.444180488586426
Epoch 2490, val loss: 0.5263147354125977
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7853881278538812
0.8105484315003986
=== training gcn model ===
Epoch 0, training loss: 1059.320068359375 = 1.0888581275939941 + 100.0 * 10.582311630249023
Epoch 0, val loss: 1.0894007682800293
Epoch 10, training loss: 1059.2913818359375 = 1.0849416255950928 + 100.0 * 10.582064628601074
Epoch 10, val loss: 1.0854846239089966
Epoch 20, training loss: 1059.1827392578125 = 1.0807926654815674 + 100.0 * 10.581019401550293
Epoch 20, val loss: 1.0813407897949219
Epoch 30, training loss: 1058.718017578125 = 1.0763856172561646 + 100.0 * 10.576416015625
Epoch 30, val loss: 1.0769604444503784
Epoch 40, training loss: 1056.86962890625 = 1.0716277360916138 + 100.0 * 10.557979583740234
Epoch 40, val loss: 1.072204828262329
Epoch 50, training loss: 1051.0064697265625 = 1.0663701295852661 + 100.0 * 10.49940013885498
Epoch 50, val loss: 1.0669562816619873
Epoch 60, training loss: 1035.6473388671875 = 1.061448574066162 + 100.0 * 10.34585952758789
Epoch 60, val loss: 1.062078595161438
Epoch 70, training loss: 1003.6510620117188 = 1.0568857192993164 + 100.0 * 10.025941848754883
Epoch 70, val loss: 1.0575296878814697
Epoch 80, training loss: 976.0911254882812 = 1.05241060256958 + 100.0 * 9.750387191772461
Epoch 80, val loss: 1.0530301332473755
Epoch 90, training loss: 964.8963623046875 = 1.0478553771972656 + 100.0 * 9.638484954833984
Epoch 90, val loss: 1.0484585762023926
Epoch 100, training loss: 956.0986328125 = 1.0432791709899902 + 100.0 * 9.550553321838379
Epoch 100, val loss: 1.043923258781433
Epoch 110, training loss: 941.0272216796875 = 1.0391579866409302 + 100.0 * 9.399880409240723
Epoch 110, val loss: 1.0399130582809448
Epoch 120, training loss: 928.2020874023438 = 1.0358073711395264 + 100.0 * 9.271662712097168
Epoch 120, val loss: 1.0367519855499268
Epoch 130, training loss: 920.8617553710938 = 1.0332483053207397 + 100.0 * 9.198285102844238
Epoch 130, val loss: 1.0343983173370361
Epoch 140, training loss: 913.7220458984375 = 1.030920386314392 + 100.0 * 9.126911163330078
Epoch 140, val loss: 1.0322177410125732
Epoch 150, training loss: 910.2797241210938 = 1.027838945388794 + 100.0 * 9.09251880645752
Epoch 150, val loss: 1.0292332172393799
Epoch 160, training loss: 907.8059692382812 = 1.023990511894226 + 100.0 * 9.067819595336914
Epoch 160, val loss: 1.0255651473999023
Epoch 170, training loss: 904.3648071289062 = 1.0204211473464966 + 100.0 * 9.033443450927734
Epoch 170, val loss: 1.0222225189208984
Epoch 180, training loss: 900.1461181640625 = 1.0172903537750244 + 100.0 * 8.991288185119629
Epoch 180, val loss: 1.0192564725875854
Epoch 190, training loss: 896.7229614257812 = 1.0138415098190308 + 100.0 * 8.957091331481934
Epoch 190, val loss: 1.015957236289978
Epoch 200, training loss: 894.4498291015625 = 1.0097228288650513 + 100.0 * 8.934401512145996
Epoch 200, val loss: 1.0120060443878174
Epoch 210, training loss: 892.6972045898438 = 1.005083680152893 + 100.0 * 8.916921615600586
Epoch 210, val loss: 1.007577657699585
Epoch 220, training loss: 890.936767578125 = 1.0004048347473145 + 100.0 * 8.89936351776123
Epoch 220, val loss: 1.0031434297561646
Epoch 230, training loss: 888.744384765625 = 0.9956584572792053 + 100.0 * 8.877487182617188
Epoch 230, val loss: 0.9986971616744995
Epoch 240, training loss: 886.0576171875 = 0.9909332394599915 + 100.0 * 8.850666999816895
Epoch 240, val loss: 0.9943010807037354
Epoch 250, training loss: 882.8577880859375 = 0.9862958788871765 + 100.0 * 8.81871509552002
Epoch 250, val loss: 0.989979088306427
Epoch 260, training loss: 879.9423217773438 = 0.9815248250961304 + 100.0 * 8.789608001708984
Epoch 260, val loss: 0.9855213165283203
Epoch 270, training loss: 877.742919921875 = 0.9761568307876587 + 100.0 * 8.767667770385742
Epoch 270, val loss: 0.9804874062538147
Epoch 280, training loss: 875.9790649414062 = 0.9700833559036255 + 100.0 * 8.750089645385742
Epoch 280, val loss: 0.9748100638389587
Epoch 290, training loss: 874.5321655273438 = 0.9633241295814514 + 100.0 * 8.735688209533691
Epoch 290, val loss: 0.968455970287323
Epoch 300, training loss: 873.2633056640625 = 0.9561189413070679 + 100.0 * 8.723072052001953
Epoch 300, val loss: 0.9617194533348083
Epoch 310, training loss: 872.2030639648438 = 0.9485409259796143 + 100.0 * 8.712545394897461
Epoch 310, val loss: 0.9545761346817017
Epoch 320, training loss: 871.22119140625 = 0.9405564665794373 + 100.0 * 8.70280647277832
Epoch 320, val loss: 0.9470943808555603
Epoch 330, training loss: 870.33935546875 = 0.9322325587272644 + 100.0 * 8.694070816040039
Epoch 330, val loss: 0.9392628073692322
Epoch 340, training loss: 869.4957275390625 = 0.9235586524009705 + 100.0 * 8.685721397399902
Epoch 340, val loss: 0.9311126470565796
Epoch 350, training loss: 868.8106689453125 = 0.9146175980567932 + 100.0 * 8.678960800170898
Epoch 350, val loss: 0.922710657119751
Epoch 360, training loss: 868.1753540039062 = 0.9053456783294678 + 100.0 * 8.672699928283691
Epoch 360, val loss: 0.9140583276748657
Epoch 370, training loss: 867.36083984375 = 0.8958794474601746 + 100.0 * 8.664649963378906
Epoch 370, val loss: 0.905196487903595
Epoch 380, training loss: 866.7391357421875 = 0.8862106204032898 + 100.0 * 8.658529281616211
Epoch 380, val loss: 0.8961454629898071
Epoch 390, training loss: 866.2225952148438 = 0.8763435482978821 + 100.0 * 8.653462409973145
Epoch 390, val loss: 0.8869439363479614
Epoch 400, training loss: 865.72802734375 = 0.8663303256034851 + 100.0 * 8.648616790771484
Epoch 400, val loss: 0.8776528835296631
Epoch 410, training loss: 865.0780029296875 = 0.8562735319137573 + 100.0 * 8.642217636108398
Epoch 410, val loss: 0.8683345317840576
Epoch 420, training loss: 864.7465209960938 = 0.846229076385498 + 100.0 * 8.639002799987793
Epoch 420, val loss: 0.859087347984314
Epoch 430, training loss: 863.9567260742188 = 0.8361865282058716 + 100.0 * 8.631205558776855
Epoch 430, val loss: 0.8497841358184814
Epoch 440, training loss: 863.3734741210938 = 0.8262841701507568 + 100.0 * 8.625472068786621
Epoch 440, val loss: 0.8407014608383179
Epoch 450, training loss: 862.8143920898438 = 0.8165440559387207 + 100.0 * 8.619978904724121
Epoch 450, val loss: 0.8317944407463074
Epoch 460, training loss: 862.2576293945312 = 0.8069629073143005 + 100.0 * 8.614506721496582
Epoch 460, val loss: 0.8230819702148438
Epoch 470, training loss: 861.737060546875 = 0.7975642681121826 + 100.0 * 8.609395027160645
Epoch 470, val loss: 0.8145891427993774
Epoch 480, training loss: 861.392822265625 = 0.7883327603340149 + 100.0 * 8.60604476928711
Epoch 480, val loss: 0.8062404990196228
Epoch 490, training loss: 860.9695434570312 = 0.7792379260063171 + 100.0 * 8.601902961730957
Epoch 490, val loss: 0.7981459498405457
Epoch 500, training loss: 860.4050903320312 = 0.7704840302467346 + 100.0 * 8.596345901489258
Epoch 500, val loss: 0.7903844118118286
Epoch 510, training loss: 859.939208984375 = 0.7620084285736084 + 100.0 * 8.591772079467773
Epoch 510, val loss: 0.7829305529594421
Epoch 520, training loss: 859.5426635742188 = 0.7538126707077026 + 100.0 * 8.587888717651367
Epoch 520, val loss: 0.7757895588874817
Epoch 530, training loss: 859.4357299804688 = 0.7458212375640869 + 100.0 * 8.586898803710938
Epoch 530, val loss: 0.7688484787940979
Epoch 540, training loss: 858.876220703125 = 0.7380568385124207 + 100.0 * 8.581381797790527
Epoch 540, val loss: 0.76216721534729
Epoch 550, training loss: 858.461669921875 = 0.7306145429611206 + 100.0 * 8.577310562133789
Epoch 550, val loss: 0.7558053731918335
Epoch 560, training loss: 858.1366577148438 = 0.7234424948692322 + 100.0 * 8.574131965637207
Epoch 560, val loss: 0.7497506737709045
Epoch 570, training loss: 857.837646484375 = 0.716520369052887 + 100.0 * 8.571211814880371
Epoch 570, val loss: 0.7439671158790588
Epoch 580, training loss: 857.564453125 = 0.709841251373291 + 100.0 * 8.568546295166016
Epoch 580, val loss: 0.7384321093559265
Epoch 590, training loss: 857.3507080078125 = 0.7034075260162354 + 100.0 * 8.566473007202148
Epoch 590, val loss: 0.7331607937812805
Epoch 600, training loss: 857.15625 = 0.6971761584281921 + 100.0 * 8.564590454101562
Epoch 600, val loss: 0.7280928492546082
Epoch 610, training loss: 856.9174194335938 = 0.6911910176277161 + 100.0 * 8.562262535095215
Epoch 610, val loss: 0.723292350769043
Epoch 620, training loss: 856.7427368164062 = 0.685474157333374 + 100.0 * 8.560572624206543
Epoch 620, val loss: 0.7187476754188538
Epoch 630, training loss: 856.574462890625 = 0.6800013184547424 + 100.0 * 8.558944702148438
Epoch 630, val loss: 0.7144821286201477
Epoch 640, training loss: 856.43896484375 = 0.674765408039093 + 100.0 * 8.557641983032227
Epoch 640, val loss: 0.710433840751648
Epoch 650, training loss: 856.2866821289062 = 0.6697348356246948 + 100.0 * 8.556169509887695
Epoch 650, val loss: 0.7066007852554321
Epoch 660, training loss: 856.1802978515625 = 0.6649304628372192 + 100.0 * 8.555153846740723
Epoch 660, val loss: 0.7030077576637268
Epoch 670, training loss: 856.0363159179688 = 0.6603560447692871 + 100.0 * 8.553759574890137
Epoch 670, val loss: 0.6996498703956604
Epoch 680, training loss: 856.4646606445312 = 0.655993640422821 + 100.0 * 8.558086395263672
Epoch 680, val loss: 0.6964318752288818
Epoch 690, training loss: 855.7979125976562 = 0.6517677307128906 + 100.0 * 8.551461219787598
Epoch 690, val loss: 0.6934879422187805
Epoch 700, training loss: 855.7020263671875 = 0.647789716720581 + 100.0 * 8.550542831420898
Epoch 700, val loss: 0.6907624006271362
Epoch 710, training loss: 855.5169677734375 = 0.6440220475196838 + 100.0 * 8.548728942871094
Epoch 710, val loss: 0.6881938576698303
Epoch 720, training loss: 855.3950805664062 = 0.6404357552528381 + 100.0 * 8.54754638671875
Epoch 720, val loss: 0.6858107447624207
Epoch 730, training loss: 855.2625122070312 = 0.6370116472244263 + 100.0 * 8.546255111694336
Epoch 730, val loss: 0.6836205124855042
Epoch 740, training loss: 855.153564453125 = 0.6337482929229736 + 100.0 * 8.545198440551758
Epoch 740, val loss: 0.6815707683563232
Epoch 750, training loss: 855.1124877929688 = 0.6306028366088867 + 100.0 * 8.544818878173828
Epoch 750, val loss: 0.6796430945396423
Epoch 760, training loss: 854.9641723632812 = 0.6275946497917175 + 100.0 * 8.543365478515625
Epoch 760, val loss: 0.6778281331062317
Epoch 770, training loss: 854.72021484375 = 0.624754786491394 + 100.0 * 8.54095458984375
Epoch 770, val loss: 0.6761931777000427
Epoch 780, training loss: 854.6924438476562 = 0.6220624446868896 + 100.0 * 8.540703773498535
Epoch 780, val loss: 0.6746464371681213
Epoch 790, training loss: 854.4710693359375 = 0.619432270526886 + 100.0 * 8.5385160446167
Epoch 790, val loss: 0.6733067631721497
Epoch 800, training loss: 854.365234375 = 0.6169372797012329 + 100.0 * 8.537483215332031
Epoch 800, val loss: 0.6719235777854919
Epoch 810, training loss: 854.1370239257812 = 0.6145852208137512 + 100.0 * 8.535224914550781
Epoch 810, val loss: 0.6707404255867004
Epoch 820, training loss: 853.989013671875 = 0.6123529672622681 + 100.0 * 8.533766746520996
Epoch 820, val loss: 0.6696890592575073
Epoch 830, training loss: 853.8443603515625 = 0.6102159023284912 + 100.0 * 8.532341003417969
Epoch 830, val loss: 0.6687040328979492
Epoch 840, training loss: 854.0323486328125 = 0.6081387996673584 + 100.0 * 8.534241676330566
Epoch 840, val loss: 0.6677762269973755
Epoch 850, training loss: 853.5900268554688 = 0.6061100363731384 + 100.0 * 8.529838562011719
Epoch 850, val loss: 0.6668330430984497
Epoch 860, training loss: 853.4559326171875 = 0.6041804552078247 + 100.0 * 8.528517723083496
Epoch 860, val loss: 0.6660440564155579
Epoch 870, training loss: 853.3075561523438 = 0.6023324728012085 + 100.0 * 8.52705192565918
Epoch 870, val loss: 0.6653100848197937
Epoch 880, training loss: 853.2161865234375 = 0.600551962852478 + 100.0 * 8.526156425476074
Epoch 880, val loss: 0.664609968662262
Epoch 890, training loss: 853.2091064453125 = 0.5988017916679382 + 100.0 * 8.526103019714355
Epoch 890, val loss: 0.6638914346694946
Epoch 900, training loss: 852.9830322265625 = 0.5970918536186218 + 100.0 * 8.523859024047852
Epoch 900, val loss: 0.663327157497406
Epoch 910, training loss: 852.8502197265625 = 0.5954581499099731 + 100.0 * 8.522547721862793
Epoch 910, val loss: 0.6627060770988464
Epoch 920, training loss: 852.7628173828125 = 0.5938819050788879 + 100.0 * 8.521689414978027
Epoch 920, val loss: 0.6621659398078918
Epoch 930, training loss: 852.822998046875 = 0.5923464298248291 + 100.0 * 8.522306442260742
Epoch 930, val loss: 0.661596953868866
Epoch 940, training loss: 852.6495361328125 = 0.5908182263374329 + 100.0 * 8.520586967468262
Epoch 940, val loss: 0.6610897183418274
Epoch 950, training loss: 852.5222778320312 = 0.5893411636352539 + 100.0 * 8.519329071044922
Epoch 950, val loss: 0.6606038212776184
Epoch 960, training loss: 852.4093627929688 = 0.5879271030426025 + 100.0 * 8.518214225769043
Epoch 960, val loss: 0.660198986530304
Epoch 970, training loss: 852.37841796875 = 0.5865675210952759 + 100.0 * 8.517918586730957
Epoch 970, val loss: 0.6598350405693054
Epoch 980, training loss: 852.40185546875 = 0.5852145552635193 + 100.0 * 8.518166542053223
Epoch 980, val loss: 0.6594047546386719
Epoch 990, training loss: 852.1898193359375 = 0.5838931798934937 + 100.0 * 8.516058921813965
Epoch 990, val loss: 0.658977210521698
Epoch 1000, training loss: 852.1078491210938 = 0.5826435089111328 + 100.0 * 8.515252113342285
Epoch 1000, val loss: 0.658674418926239
Epoch 1010, training loss: 852.1184692382812 = 0.581417441368103 + 100.0 * 8.51537036895752
Epoch 1010, val loss: 0.6583331823348999
Epoch 1020, training loss: 851.906982421875 = 0.5802149176597595 + 100.0 * 8.513267517089844
Epoch 1020, val loss: 0.6580188274383545
Epoch 1030, training loss: 851.8365478515625 = 0.5790674686431885 + 100.0 * 8.512575149536133
Epoch 1030, val loss: 0.6577625274658203
Epoch 1040, training loss: 851.724853515625 = 0.5779604911804199 + 100.0 * 8.511468887329102
Epoch 1040, val loss: 0.65750652551651
Epoch 1050, training loss: 851.7713012695312 = 0.5768811106681824 + 100.0 * 8.511943817138672
Epoch 1050, val loss: 0.6573017239570618
Epoch 1060, training loss: 851.7540893554688 = 0.5757719278335571 + 100.0 * 8.511783599853516
Epoch 1060, val loss: 0.6569934487342834
Epoch 1070, training loss: 851.609130859375 = 0.5746830701828003 + 100.0 * 8.510344505310059
Epoch 1070, val loss: 0.6566768288612366
Epoch 1080, training loss: 851.4337158203125 = 0.573675811290741 + 100.0 * 8.508600234985352
Epoch 1080, val loss: 0.6564947366714478
Epoch 1090, training loss: 851.3008422851562 = 0.5727157592773438 + 100.0 * 8.507281303405762
Epoch 1090, val loss: 0.6563223004341125
Epoch 1100, training loss: 851.232177734375 = 0.5717836618423462 + 100.0 * 8.506604194641113
Epoch 1100, val loss: 0.6561787724494934
Epoch 1110, training loss: 851.6162719726562 = 0.5708579421043396 + 100.0 * 8.510454177856445
Epoch 1110, val loss: 0.6561113595962524
Epoch 1120, training loss: 851.15478515625 = 0.5698917508125305 + 100.0 * 8.50584888458252
Epoch 1120, val loss: 0.6557842493057251
Epoch 1130, training loss: 851.068115234375 = 0.5689759850502014 + 100.0 * 8.50499153137207
Epoch 1130, val loss: 0.6555307507514954
Epoch 1140, training loss: 850.8905639648438 = 0.5681120157241821 + 100.0 * 8.50322437286377
Epoch 1140, val loss: 0.6554328203201294
Epoch 1150, training loss: 850.8231201171875 = 0.5672807097434998 + 100.0 * 8.502558708190918
Epoch 1150, val loss: 0.6553703546524048
Epoch 1160, training loss: 851.1724243164062 = 0.5664602518081665 + 100.0 * 8.506059646606445
Epoch 1160, val loss: 0.6553276777267456
Epoch 1170, training loss: 850.88427734375 = 0.5655680894851685 + 100.0 * 8.50318717956543
Epoch 1170, val loss: 0.6549134254455566
Epoch 1180, training loss: 850.6549682617188 = 0.5647360682487488 + 100.0 * 8.50090217590332
Epoch 1180, val loss: 0.6548959612846375
Epoch 1190, training loss: 850.5089111328125 = 0.5639491677284241 + 100.0 * 8.499449729919434
Epoch 1190, val loss: 0.654772162437439
Epoch 1200, training loss: 850.4545288085938 = 0.5631877183914185 + 100.0 * 8.498913764953613
Epoch 1200, val loss: 0.6546608209609985
Epoch 1210, training loss: 850.781982421875 = 0.5624191164970398 + 100.0 * 8.502195358276367
Epoch 1210, val loss: 0.654565155506134
Epoch 1220, training loss: 850.3521118164062 = 0.5616265535354614 + 100.0 * 8.497904777526855
Epoch 1220, val loss: 0.6544217467308044
Epoch 1230, training loss: 850.2383422851562 = 0.5608733892440796 + 100.0 * 8.496774673461914
Epoch 1230, val loss: 0.6543477177619934
Epoch 1240, training loss: 850.1812133789062 = 0.5601500272750854 + 100.0 * 8.496211051940918
Epoch 1240, val loss: 0.6542244553565979
Epoch 1250, training loss: 850.1964721679688 = 0.5594362020492554 + 100.0 * 8.496370315551758
Epoch 1250, val loss: 0.6541313529014587
Epoch 1260, training loss: 850.0463256835938 = 0.5587019324302673 + 100.0 * 8.49487590789795
Epoch 1260, val loss: 0.6540737152099609
Epoch 1270, training loss: 850.0042114257812 = 0.557985782623291 + 100.0 * 8.494462013244629
Epoch 1270, val loss: 0.6539451479911804
Epoch 1280, training loss: 851.0165405273438 = 0.5572504997253418 + 100.0 * 8.504592895507812
Epoch 1280, val loss: 0.6537522077560425
Epoch 1290, training loss: 850.0405883789062 = 0.5564495325088501 + 100.0 * 8.494841575622559
Epoch 1290, val loss: 0.6536615490913391
Epoch 1300, training loss: 849.83447265625 = 0.5557346343994141 + 100.0 * 8.49278736114502
Epoch 1300, val loss: 0.6535932421684265
Epoch 1310, training loss: 849.7326049804688 = 0.5550789833068848 + 100.0 * 8.491775512695312
Epoch 1310, val loss: 0.6535335779190063
Epoch 1320, training loss: 849.67041015625 = 0.5544416904449463 + 100.0 * 8.491159439086914
Epoch 1320, val loss: 0.653502881526947
Epoch 1330, training loss: 849.6134643554688 = 0.5538005232810974 + 100.0 * 8.490596771240234
Epoch 1330, val loss: 0.6534514427185059
Epoch 1340, training loss: 849.552734375 = 0.5531522631645203 + 100.0 * 8.489995956420898
Epoch 1340, val loss: 0.6533937454223633
Epoch 1350, training loss: 849.6270751953125 = 0.5524992942810059 + 100.0 * 8.490745544433594
Epoch 1350, val loss: 0.6532793045043945
Epoch 1360, training loss: 849.5055541992188 = 0.5517870783805847 + 100.0 * 8.489538192749023
Epoch 1360, val loss: 0.6532189846038818
Epoch 1370, training loss: 849.5109252929688 = 0.5510857701301575 + 100.0 * 8.489598274230957
Epoch 1370, val loss: 0.6530979871749878
Epoch 1380, training loss: 849.3582153320312 = 0.5504371523857117 + 100.0 * 8.488078117370605
Epoch 1380, val loss: 0.6530265212059021
Epoch 1390, training loss: 849.2950439453125 = 0.5498133301734924 + 100.0 * 8.487452507019043
Epoch 1390, val loss: 0.6529944539070129
Epoch 1400, training loss: 849.2376708984375 = 0.5491925477981567 + 100.0 * 8.486885070800781
Epoch 1400, val loss: 0.6529388427734375
Epoch 1410, training loss: 849.2809448242188 = 0.5485675930976868 + 100.0 * 8.487323760986328
Epoch 1410, val loss: 0.652827799320221
Epoch 1420, training loss: 849.26953125 = 0.5478909611701965 + 100.0 * 8.487215995788574
Epoch 1420, val loss: 0.6527628302574158
Epoch 1430, training loss: 849.2058715820312 = 0.5472120046615601 + 100.0 * 8.486586570739746
Epoch 1430, val loss: 0.6526541113853455
Epoch 1440, training loss: 849.0523681640625 = 0.5465762615203857 + 100.0 * 8.485057830810547
Epoch 1440, val loss: 0.6525693535804749
Epoch 1450, training loss: 849.021728515625 = 0.5459643006324768 + 100.0 * 8.484757423400879
Epoch 1450, val loss: 0.6525200605392456
Epoch 1460, training loss: 849.0259399414062 = 0.5453537702560425 + 100.0 * 8.484806060791016
Epoch 1460, val loss: 0.6524263024330139
Epoch 1470, training loss: 849.063232421875 = 0.544712245464325 + 100.0 * 8.485184669494629
Epoch 1470, val loss: 0.6523946523666382
Epoch 1480, training loss: 849.0629272460938 = 0.5440435409545898 + 100.0 * 8.485188484191895
Epoch 1480, val loss: 0.6523124575614929
Epoch 1490, training loss: 848.8888549804688 = 0.5433806777000427 + 100.0 * 8.483454704284668
Epoch 1490, val loss: 0.6521434187889099
Epoch 1500, training loss: 848.8125610351562 = 0.5427554249763489 + 100.0 * 8.482697486877441
Epoch 1500, val loss: 0.6520649194717407
Epoch 1510, training loss: 848.7644653320312 = 0.5421411991119385 + 100.0 * 8.482223510742188
Epoch 1510, val loss: 0.652031421661377
Epoch 1520, training loss: 848.8152465820312 = 0.541524350643158 + 100.0 * 8.48273754119873
Epoch 1520, val loss: 0.6519708633422852
Epoch 1530, training loss: 848.9403686523438 = 0.5408693552017212 + 100.0 * 8.483994483947754
Epoch 1530, val loss: 0.6518610715866089
Epoch 1540, training loss: 848.7670288085938 = 0.5402013063430786 + 100.0 * 8.482268333435059
Epoch 1540, val loss: 0.6516270637512207
Epoch 1550, training loss: 848.6508178710938 = 0.5395694375038147 + 100.0 * 8.481112480163574
Epoch 1550, val loss: 0.6515718102455139
Epoch 1560, training loss: 848.7077026367188 = 0.5389440059661865 + 100.0 * 8.481687545776367
Epoch 1560, val loss: 0.6514657735824585
Epoch 1570, training loss: 848.5517578125 = 0.5382887721061707 + 100.0 * 8.480134963989258
Epoch 1570, val loss: 0.6512777805328369
Epoch 1580, training loss: 848.50927734375 = 0.5376458168029785 + 100.0 * 8.479716300964355
Epoch 1580, val loss: 0.6511123776435852
Epoch 1590, training loss: 848.4711303710938 = 0.5370199680328369 + 100.0 * 8.479340553283691
Epoch 1590, val loss: 0.6510018706321716
Epoch 1600, training loss: 848.4635009765625 = 0.5363955497741699 + 100.0 * 8.479270935058594
Epoch 1600, val loss: 0.6508622169494629
Epoch 1610, training loss: 848.8026733398438 = 0.5357511043548584 + 100.0 * 8.48266887664795
Epoch 1610, val loss: 0.6505993008613586
Epoch 1620, training loss: 848.6812133789062 = 0.5350415706634521 + 100.0 * 8.481461524963379
Epoch 1620, val loss: 0.6505076289176941
Epoch 1630, training loss: 848.3432006835938 = 0.5343603491783142 + 100.0 * 8.47808837890625
Epoch 1630, val loss: 0.6503681540489197
Epoch 1640, training loss: 848.3142700195312 = 0.5337274074554443 + 100.0 * 8.477805137634277
Epoch 1640, val loss: 0.6501498222351074
Epoch 1650, training loss: 848.2361450195312 = 0.533107340335846 + 100.0 * 8.477030754089355
Epoch 1650, val loss: 0.650073766708374
Epoch 1660, training loss: 848.1971435546875 = 0.5324835777282715 + 100.0 * 8.476646423339844
Epoch 1660, val loss: 0.6499693393707275
Epoch 1670, training loss: 848.2577514648438 = 0.5318479537963867 + 100.0 * 8.477258682250977
Epoch 1670, val loss: 0.6498869061470032
Epoch 1680, training loss: 848.3902587890625 = 0.5311591029167175 + 100.0 * 8.478590965270996
Epoch 1680, val loss: 0.6496647596359253
Epoch 1690, training loss: 848.1529541015625 = 0.5304431915283203 + 100.0 * 8.476224899291992
Epoch 1690, val loss: 0.6493740081787109
Epoch 1700, training loss: 848.1055297851562 = 0.5297728776931763 + 100.0 * 8.475757598876953
Epoch 1700, val loss: 0.6491641402244568
Epoch 1710, training loss: 848.33984375 = 0.5291069149971008 + 100.0 * 8.478107452392578
Epoch 1710, val loss: 0.6489992141723633
Epoch 1720, training loss: 848.08349609375 = 0.5284153819084167 + 100.0 * 8.475550651550293
Epoch 1720, val loss: 0.6488099098205566
Epoch 1730, training loss: 847.9715576171875 = 0.5277471542358398 + 100.0 * 8.474437713623047
Epoch 1730, val loss: 0.6486178636550903
Epoch 1740, training loss: 847.9202270507812 = 0.5270847082138062 + 100.0 * 8.473931312561035
Epoch 1740, val loss: 0.6484435796737671
Epoch 1750, training loss: 848.0194091796875 = 0.5264092683792114 + 100.0 * 8.474929809570312
Epoch 1750, val loss: 0.6481861472129822
Epoch 1760, training loss: 847.9261474609375 = 0.5256764888763428 + 100.0 * 8.474004745483398
Epoch 1760, val loss: 0.6479209661483765
Epoch 1770, training loss: 847.8385620117188 = 0.5249412059783936 + 100.0 * 8.473135948181152
Epoch 1770, val loss: 0.6477500796318054
Epoch 1780, training loss: 847.7762451171875 = 0.5242385864257812 + 100.0 * 8.472519874572754
Epoch 1780, val loss: 0.6474602222442627
Epoch 1790, training loss: 847.7232666015625 = 0.5235410332679749 + 100.0 * 8.471997261047363
Epoch 1790, val loss: 0.6472747325897217
Epoch 1800, training loss: 847.91064453125 = 0.5228322148323059 + 100.0 * 8.473877906799316
Epoch 1800, val loss: 0.6471168398857117
Epoch 1810, training loss: 847.7133178710938 = 0.5220721364021301 + 100.0 * 8.471912384033203
Epoch 1810, val loss: 0.6466510891914368
Epoch 1820, training loss: 847.8181762695312 = 0.5213088989257812 + 100.0 * 8.472969055175781
Epoch 1820, val loss: 0.646453320980072
Epoch 1830, training loss: 847.6124267578125 = 0.5205444693565369 + 100.0 * 8.470918655395508
Epoch 1830, val loss: 0.6460728049278259
Epoch 1840, training loss: 847.611083984375 = 0.5197989344596863 + 100.0 * 8.47091293334961
Epoch 1840, val loss: 0.6458496451377869
Epoch 1850, training loss: 847.932373046875 = 0.5190414190292358 + 100.0 * 8.474133491516113
Epoch 1850, val loss: 0.6454794406890869
Epoch 1860, training loss: 847.5465087890625 = 0.5182227492332458 + 100.0 * 8.470282554626465
Epoch 1860, val loss: 0.6453081369400024
Epoch 1870, training loss: 847.471435546875 = 0.5174365043640137 + 100.0 * 8.469539642333984
Epoch 1870, val loss: 0.6449297070503235
Epoch 1880, training loss: 847.4151000976562 = 0.5166603326797485 + 100.0 * 8.468984603881836
Epoch 1880, val loss: 0.6447283029556274
Epoch 1890, training loss: 847.4732666015625 = 0.5158794522285461 + 100.0 * 8.469573974609375
Epoch 1890, val loss: 0.6444299817085266
Epoch 1900, training loss: 847.5465087890625 = 0.5150443315505981 + 100.0 * 8.470314979553223
Epoch 1900, val loss: 0.6441161632537842
Epoch 1910, training loss: 847.3800048828125 = 0.5141892433166504 + 100.0 * 8.468658447265625
Epoch 1910, val loss: 0.6437079310417175
Epoch 1920, training loss: 847.2757568359375 = 0.5133637189865112 + 100.0 * 8.467623710632324
Epoch 1920, val loss: 0.6433752775192261
Epoch 1930, training loss: 847.24462890625 = 0.5125533938407898 + 100.0 * 8.467320442199707
Epoch 1930, val loss: 0.6430886387825012
Epoch 1940, training loss: 847.2318115234375 = 0.5117377638816833 + 100.0 * 8.467201232910156
Epoch 1940, val loss: 0.642786979675293
Epoch 1950, training loss: 848.0354614257812 = 0.5108983516693115 + 100.0 * 8.475245475769043
Epoch 1950, val loss: 0.6425528526306152
Epoch 1960, training loss: 847.552490234375 = 0.5099431276321411 + 100.0 * 8.470425605773926
Epoch 1960, val loss: 0.6417975425720215
Epoch 1970, training loss: 847.1192626953125 = 0.5090432167053223 + 100.0 * 8.466102600097656
Epoch 1970, val loss: 0.6415354609489441
Epoch 1980, training loss: 847.111572265625 = 0.5081998109817505 + 100.0 * 8.466033935546875
Epoch 1980, val loss: 0.6412680149078369
Epoch 1990, training loss: 847.0636596679688 = 0.5073623061180115 + 100.0 * 8.46556282043457
Epoch 1990, val loss: 0.6408827900886536
Epoch 2000, training loss: 847.109375 = 0.5065076947212219 + 100.0 * 8.466028213500977
Epoch 2000, val loss: 0.6405901312828064
Epoch 2010, training loss: 847.1109008789062 = 0.5055993795394897 + 100.0 * 8.466053009033203
Epoch 2010, val loss: 0.640142023563385
Epoch 2020, training loss: 846.9783935546875 = 0.5046751499176025 + 100.0 * 8.464736938476562
Epoch 2020, val loss: 0.6398218870162964
Epoch 2030, training loss: 846.931396484375 = 0.5037786960601807 + 100.0 * 8.464276313781738
Epoch 2030, val loss: 0.6394128799438477
Epoch 2040, training loss: 847.1124877929688 = 0.502879798412323 + 100.0 * 8.466095924377441
Epoch 2040, val loss: 0.6390852332115173
Epoch 2050, training loss: 846.8900146484375 = 0.5019080638885498 + 100.0 * 8.463881492614746
Epoch 2050, val loss: 0.6385700702667236
Epoch 2060, training loss: 846.870849609375 = 0.5009433627128601 + 100.0 * 8.463699340820312
Epoch 2060, val loss: 0.6382070779800415
Epoch 2070, training loss: 846.8209228515625 = 0.5000081658363342 + 100.0 * 8.46320915222168
Epoch 2070, val loss: 0.6377525925636292
Epoch 2080, training loss: 846.7841186523438 = 0.4990684688091278 + 100.0 * 8.462850570678711
Epoch 2080, val loss: 0.6374080777168274
Epoch 2090, training loss: 847.0452270507812 = 0.4981122314929962 + 100.0 * 8.465471267700195
Epoch 2090, val loss: 0.6370128989219666
Epoch 2100, training loss: 846.8271484375 = 0.49708572030067444 + 100.0 * 8.463300704956055
Epoch 2100, val loss: 0.6364601850509644
Epoch 2110, training loss: 846.759765625 = 0.49606993794441223 + 100.0 * 8.462636947631836
Epoch 2110, val loss: 0.6359875202178955
Epoch 2120, training loss: 846.6883544921875 = 0.4950798749923706 + 100.0 * 8.461933135986328
Epoch 2120, val loss: 0.6356109380722046
Epoch 2130, training loss: 846.6929321289062 = 0.4940930902957916 + 100.0 * 8.46198844909668
Epoch 2130, val loss: 0.6351754665374756
Epoch 2140, training loss: 847.1886596679688 = 0.4930643141269684 + 100.0 * 8.46695613861084
Epoch 2140, val loss: 0.6346337795257568
Epoch 2150, training loss: 846.7391967773438 = 0.4919751286506653 + 100.0 * 8.462471961975098
Epoch 2150, val loss: 0.6343315839767456
Epoch 2160, training loss: 846.5897216796875 = 0.4909341037273407 + 100.0 * 8.46098804473877
Epoch 2160, val loss: 0.6337655782699585
Epoch 2170, training loss: 846.525146484375 = 0.4899153709411621 + 100.0 * 8.460351943969727
Epoch 2170, val loss: 0.633364737033844
Epoch 2180, training loss: 846.4924926757812 = 0.4888947904109955 + 100.0 * 8.460036277770996
Epoch 2180, val loss: 0.6328673958778381
Epoch 2190, training loss: 846.8944091796875 = 0.4878486096858978 + 100.0 * 8.464065551757812
Epoch 2190, val loss: 0.6323331594467163
Epoch 2200, training loss: 846.667236328125 = 0.48671582341194153 + 100.0 * 8.46180534362793
Epoch 2200, val loss: 0.6319554448127747
Epoch 2210, training loss: 846.401123046875 = 0.48561036586761475 + 100.0 * 8.459155082702637
Epoch 2210, val loss: 0.6313781142234802
Epoch 2220, training loss: 846.3692626953125 = 0.4845348298549652 + 100.0 * 8.458847045898438
Epoch 2220, val loss: 0.6309043765068054
Epoch 2230, training loss: 846.361572265625 = 0.48345261812210083 + 100.0 * 8.458781242370605
Epoch 2230, val loss: 0.6304202675819397
Epoch 2240, training loss: 846.5006713867188 = 0.48234865069389343 + 100.0 * 8.460183143615723
Epoch 2240, val loss: 0.6298657655715942
Epoch 2250, training loss: 846.3750610351562 = 0.48119229078292847 + 100.0 * 8.458938598632812
Epoch 2250, val loss: 0.6293531060218811
Epoch 2260, training loss: 846.475830078125 = 0.48001861572265625 + 100.0 * 8.45995807647705
Epoch 2260, val loss: 0.6287574768066406
Epoch 2270, training loss: 846.2665405273438 = 0.47884753346443176 + 100.0 * 8.457877159118652
Epoch 2270, val loss: 0.6282810568809509
Epoch 2280, training loss: 846.2217407226562 = 0.47770583629608154 + 100.0 * 8.457440376281738
Epoch 2280, val loss: 0.6277467012405396
Epoch 2290, training loss: 846.17138671875 = 0.4765647053718567 + 100.0 * 8.456948280334473
Epoch 2290, val loss: 0.6272668242454529
Epoch 2300, training loss: 846.2402954101562 = 0.47541436553001404 + 100.0 * 8.457649230957031
Epoch 2300, val loss: 0.6267538070678711
Epoch 2310, training loss: 846.3365478515625 = 0.4742029011249542 + 100.0 * 8.458623886108398
Epoch 2310, val loss: 0.6262511014938354
Epoch 2320, training loss: 846.1473999023438 = 0.4729540944099426 + 100.0 * 8.456744194030762
Epoch 2320, val loss: 0.6256867051124573
Epoch 2330, training loss: 846.0397338867188 = 0.47173595428466797 + 100.0 * 8.455679893493652
Epoch 2330, val loss: 0.625201940536499
Epoch 2340, training loss: 846.2094116210938 = 0.47053369879722595 + 100.0 * 8.457388877868652
Epoch 2340, val loss: 0.6247212886810303
Epoch 2350, training loss: 846.1297607421875 = 0.46926555037498474 + 100.0 * 8.456604957580566
Epoch 2350, val loss: 0.6242331266403198
Epoch 2360, training loss: 845.998046875 = 0.4679902493953705 + 100.0 * 8.455300331115723
Epoch 2360, val loss: 0.6236127614974976
Epoch 2370, training loss: 845.9713134765625 = 0.46674421429634094 + 100.0 * 8.455045700073242
Epoch 2370, val loss: 0.6230766177177429
Epoch 2380, training loss: 845.9131469726562 = 0.46550360321998596 + 100.0 * 8.454476356506348
Epoch 2380, val loss: 0.6225942969322205
Epoch 2390, training loss: 845.9083251953125 = 0.46424952149391174 + 100.0 * 8.45444107055664
Epoch 2390, val loss: 0.6220560669898987
Epoch 2400, training loss: 846.3471069335938 = 0.4629768133163452 + 100.0 * 8.458841323852539
Epoch 2400, val loss: 0.6214038729667664
Epoch 2410, training loss: 846.039794921875 = 0.4616275131702423 + 100.0 * 8.455781936645508
Epoch 2410, val loss: 0.6210837364196777
Epoch 2420, training loss: 845.9585571289062 = 0.46029233932495117 + 100.0 * 8.45498275756836
Epoch 2420, val loss: 0.6203067898750305
Epoch 2430, training loss: 845.8143920898438 = 0.45898354053497314 + 100.0 * 8.453554153442383
Epoch 2430, val loss: 0.6199379563331604
Epoch 2440, training loss: 845.7958374023438 = 0.45769473910331726 + 100.0 * 8.453381538391113
Epoch 2440, val loss: 0.6193262338638306
Epoch 2450, training loss: 845.8177490234375 = 0.4563915431499481 + 100.0 * 8.45361328125
Epoch 2450, val loss: 0.6188448667526245
Epoch 2460, training loss: 845.8736572265625 = 0.4550609290599823 + 100.0 * 8.454185485839844
Epoch 2460, val loss: 0.6182629466056824
Epoch 2470, training loss: 845.6858520507812 = 0.4537073075771332 + 100.0 * 8.45232105255127
Epoch 2470, val loss: 0.6177390813827515
Epoch 2480, training loss: 845.72998046875 = 0.4523695707321167 + 100.0 * 8.452775955200195
Epoch 2480, val loss: 0.6172402501106262
Epoch 2490, training loss: 845.6790771484375 = 0.451019287109375 + 100.0 * 8.45228099822998
Epoch 2490, val loss: 0.6167408227920532
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7600202942668696
0.8145330725204666
=== training gcn model ===
Epoch 0, training loss: 1059.3226318359375 = 1.0912551879882812 + 100.0 * 10.582313537597656
Epoch 0, val loss: 1.0907795429229736
Epoch 10, training loss: 1059.29443359375 = 1.0871018171310425 + 100.0 * 10.582073211669922
Epoch 10, val loss: 1.0866103172302246
Epoch 20, training loss: 1059.186279296875 = 1.0825244188308716 + 100.0 * 10.581037521362305
Epoch 20, val loss: 1.0820294618606567
Epoch 30, training loss: 1058.727783203125 = 1.0774835348129272 + 100.0 * 10.576502799987793
Epoch 30, val loss: 1.0769919157028198
Epoch 40, training loss: 1056.8966064453125 = 1.071907877922058 + 100.0 * 10.558247566223145
Epoch 40, val loss: 1.0713906288146973
Epoch 50, training loss: 1050.7423095703125 = 1.0653972625732422 + 100.0 * 10.496768951416016
Epoch 50, val loss: 1.064827561378479
Epoch 60, training loss: 1033.0341796875 = 1.0581955909729004 + 100.0 * 10.319759368896484
Epoch 60, val loss: 1.0576838254928589
Epoch 70, training loss: 998.8286743164062 = 1.0504558086395264 + 100.0 * 9.977782249450684
Epoch 70, val loss: 1.049981713294983
Epoch 80, training loss: 977.1988525390625 = 1.0436345338821411 + 100.0 * 9.761551856994629
Epoch 80, val loss: 1.043590784072876
Epoch 90, training loss: 968.4113159179688 = 1.0391652584075928 + 100.0 * 9.673721313476562
Epoch 90, val loss: 1.0394370555877686
Epoch 100, training loss: 960.7800903320312 = 1.0354807376861572 + 100.0 * 9.59744644165039
Epoch 100, val loss: 1.0359935760498047
Epoch 110, training loss: 947.1193237304688 = 1.0317965745925903 + 100.0 * 9.460875511169434
Epoch 110, val loss: 1.0324814319610596
Epoch 120, training loss: 934.40625 = 1.0283408164978027 + 100.0 * 9.333779335021973
Epoch 120, val loss: 1.029133915901184
Epoch 130, training loss: 931.5034790039062 = 1.0233469009399414 + 100.0 * 9.304800987243652
Epoch 130, val loss: 1.0241647958755493
Epoch 140, training loss: 928.58154296875 = 1.0186069011688232 + 100.0 * 9.275629043579102
Epoch 140, val loss: 1.0198160409927368
Epoch 150, training loss: 924.2088623046875 = 1.0148135423660278 + 100.0 * 9.231940269470215
Epoch 150, val loss: 1.0163590908050537
Epoch 160, training loss: 917.3795166015625 = 1.0113099813461304 + 100.0 * 9.163681983947754
Epoch 160, val loss: 1.0132173299789429
Epoch 170, training loss: 908.26025390625 = 1.009078025817871 + 100.0 * 9.072511672973633
Epoch 170, val loss: 1.0111973285675049
Epoch 180, training loss: 902.81201171875 = 1.005862832069397 + 100.0 * 9.018061637878418
Epoch 180, val loss: 1.0080476999282837
Epoch 190, training loss: 898.0850219726562 = 1.0010771751403809 + 100.0 * 8.970839500427246
Epoch 190, val loss: 1.0035419464111328
Epoch 200, training loss: 893.495361328125 = 0.9961118102073669 + 100.0 * 8.924992561340332
Epoch 200, val loss: 0.9989405274391174
Epoch 210, training loss: 890.1907958984375 = 0.9909456372261047 + 100.0 * 8.891998291015625
Epoch 210, val loss: 0.9941232800483704
Epoch 220, training loss: 887.47705078125 = 0.985797107219696 + 100.0 * 8.864912986755371
Epoch 220, val loss: 0.9893414378166199
Epoch 230, training loss: 885.25732421875 = 0.9803057909011841 + 100.0 * 8.84277057647705
Epoch 230, val loss: 0.9841756820678711
Epoch 240, training loss: 883.7244873046875 = 0.9741622805595398 + 100.0 * 8.827503204345703
Epoch 240, val loss: 0.978400468826294
Epoch 250, training loss: 882.3782348632812 = 0.9674429893493652 + 100.0 * 8.814107894897461
Epoch 250, val loss: 0.9721271991729736
Epoch 260, training loss: 881.0120849609375 = 0.9602615833282471 + 100.0 * 8.800518035888672
Epoch 260, val loss: 0.9654456377029419
Epoch 270, training loss: 879.95849609375 = 0.9528638124465942 + 100.0 * 8.790056228637695
Epoch 270, val loss: 0.9586107134819031
Epoch 280, training loss: 878.1336669921875 = 0.9452430009841919 + 100.0 * 8.771883964538574
Epoch 280, val loss: 0.9515199065208435
Epoch 290, training loss: 876.5775756835938 = 0.9374381899833679 + 100.0 * 8.756401062011719
Epoch 290, val loss: 0.9443318843841553
Epoch 300, training loss: 875.1904296875 = 0.9293044805526733 + 100.0 * 8.742610931396484
Epoch 300, val loss: 0.9367879033088684
Epoch 310, training loss: 874.2239379882812 = 0.920815646648407 + 100.0 * 8.733031272888184
Epoch 310, val loss: 0.9288962483406067
Epoch 320, training loss: 872.7545776367188 = 0.9118988513946533 + 100.0 * 8.718426704406738
Epoch 320, val loss: 0.9207279682159424
Epoch 330, training loss: 871.5562133789062 = 0.9029245376586914 + 100.0 * 8.70653247833252
Epoch 330, val loss: 0.9125400185585022
Epoch 340, training loss: 870.7080688476562 = 0.8938319683074951 + 100.0 * 8.698142051696777
Epoch 340, val loss: 0.9042437076568604
Epoch 350, training loss: 869.57763671875 = 0.8842845559120178 + 100.0 * 8.686933517456055
Epoch 350, val loss: 0.8955255746841431
Epoch 360, training loss: 868.6726684570312 = 0.8744401931762695 + 100.0 * 8.677982330322266
Epoch 360, val loss: 0.8865775465965271
Epoch 370, training loss: 867.8110961914062 = 0.8643845319747925 + 100.0 * 8.669466972351074
Epoch 370, val loss: 0.8774484395980835
Epoch 380, training loss: 866.9444580078125 = 0.8542602062225342 + 100.0 * 8.66090202331543
Epoch 380, val loss: 0.8683000802993774
Epoch 390, training loss: 866.5105590820312 = 0.8441776037216187 + 100.0 * 8.65666389465332
Epoch 390, val loss: 0.8591561913490295
Epoch 400, training loss: 865.497314453125 = 0.8339829444885254 + 100.0 * 8.64663314819336
Epoch 400, val loss: 0.8500326871871948
Epoch 410, training loss: 864.610107421875 = 0.8238916993141174 + 100.0 * 8.637862205505371
Epoch 410, val loss: 0.8410500288009644
Epoch 420, training loss: 863.8685302734375 = 0.8139486908912659 + 100.0 * 8.630545616149902
Epoch 420, val loss: 0.8321763873100281
Epoch 430, training loss: 863.1646728515625 = 0.8040485978126526 + 100.0 * 8.623605728149414
Epoch 430, val loss: 0.8234071135520935
Epoch 440, training loss: 863.6629638671875 = 0.794224202632904 + 100.0 * 8.628686904907227
Epoch 440, val loss: 0.8146440386772156
Epoch 450, training loss: 862.2469482421875 = 0.784261167049408 + 100.0 * 8.61462688446045
Epoch 450, val loss: 0.805872917175293
Epoch 460, training loss: 861.5342407226562 = 0.7745790481567383 + 100.0 * 8.607596397399902
Epoch 460, val loss: 0.797403872013092
Epoch 470, training loss: 861.0264892578125 = 0.765095591545105 + 100.0 * 8.602614402770996
Epoch 470, val loss: 0.7890958189964294
Epoch 480, training loss: 860.5478515625 = 0.7557893395423889 + 100.0 * 8.597920417785645
Epoch 480, val loss: 0.7809787392616272
Epoch 490, training loss: 860.1148681640625 = 0.7466626763343811 + 100.0 * 8.593682289123535
Epoch 490, val loss: 0.7730569839477539
Epoch 500, training loss: 859.7713012695312 = 0.7377181649208069 + 100.0 * 8.590335845947266
Epoch 500, val loss: 0.7653356194496155
Epoch 510, training loss: 859.63330078125 = 0.7289620637893677 + 100.0 * 8.589043617248535
Epoch 510, val loss: 0.7577648162841797
Epoch 520, training loss: 859.1267700195312 = 0.7203856706619263 + 100.0 * 8.584063529968262
Epoch 520, val loss: 0.750428318977356
Epoch 530, training loss: 858.7505493164062 = 0.712072491645813 + 100.0 * 8.580384254455566
Epoch 530, val loss: 0.7433388233184814
Epoch 540, training loss: 858.4631958007812 = 0.7040005922317505 + 100.0 * 8.577591896057129
Epoch 540, val loss: 0.7364776730537415
Epoch 550, training loss: 858.208251953125 = 0.6961718201637268 + 100.0 * 8.57512092590332
Epoch 550, val loss: 0.7298654913902283
Epoch 560, training loss: 858.8079833984375 = 0.6885919570922852 + 100.0 * 8.581193923950195
Epoch 560, val loss: 0.7234789729118347
Epoch 570, training loss: 857.8822631835938 = 0.6811570525169373 + 100.0 * 8.57201099395752
Epoch 570, val loss: 0.717279314994812
Epoch 580, training loss: 857.6409301757812 = 0.674091637134552 + 100.0 * 8.569668769836426
Epoch 580, val loss: 0.711439311504364
Epoch 590, training loss: 857.372314453125 = 0.667323648929596 + 100.0 * 8.567049980163574
Epoch 590, val loss: 0.7058645486831665
Epoch 600, training loss: 857.164306640625 = 0.6608448624610901 + 100.0 * 8.565034866333008
Epoch 600, val loss: 0.7005801796913147
Epoch 610, training loss: 856.9638671875 = 0.6546408534049988 + 100.0 * 8.563092231750488
Epoch 610, val loss: 0.6955805420875549
Epoch 620, training loss: 856.769287109375 = 0.6487038135528564 + 100.0 * 8.561205863952637
Epoch 620, val loss: 0.6908352971076965
Epoch 630, training loss: 856.58935546875 = 0.643033504486084 + 100.0 * 8.559463500976562
Epoch 630, val loss: 0.6863459944725037
Epoch 640, training loss: 857.1375732421875 = 0.6376204490661621 + 100.0 * 8.5649995803833
Epoch 640, val loss: 0.6820968389511108
Epoch 650, training loss: 856.3257446289062 = 0.6323762536048889 + 100.0 * 8.556933403015137
Epoch 650, val loss: 0.6780416965484619
Epoch 660, training loss: 856.1146850585938 = 0.6274462938308716 + 100.0 * 8.554872512817383
Epoch 660, val loss: 0.6743040084838867
Epoch 670, training loss: 855.8904418945312 = 0.622758150100708 + 100.0 * 8.552677154541016
Epoch 670, val loss: 0.6707746386528015
Epoch 680, training loss: 855.7267456054688 = 0.6182962656021118 + 100.0 * 8.551084518432617
Epoch 680, val loss: 0.6674845218658447
Epoch 690, training loss: 855.5866088867188 = 0.6140461564064026 + 100.0 * 8.549725532531738
Epoch 690, val loss: 0.6643840670585632
Epoch 700, training loss: 855.695068359375 = 0.609982967376709 + 100.0 * 8.550850868225098
Epoch 700, val loss: 0.6614570617675781
Epoch 710, training loss: 855.455078125 = 0.6061227321624756 + 100.0 * 8.548489570617676
Epoch 710, val loss: 0.658737063407898
Epoch 720, training loss: 855.176025390625 = 0.602483868598938 + 100.0 * 8.545735359191895
Epoch 720, val loss: 0.6562432646751404
Epoch 730, training loss: 854.978515625 = 0.599044144153595 + 100.0 * 8.543794631958008
Epoch 730, val loss: 0.653934895992279
Epoch 740, training loss: 854.8482055664062 = 0.5957714319229126 + 100.0 * 8.542524337768555
Epoch 740, val loss: 0.6517893075942993
Epoch 750, training loss: 855.2604370117188 = 0.592642068862915 + 100.0 * 8.546677589416504
Epoch 750, val loss: 0.6497653722763062
Epoch 760, training loss: 854.598876953125 = 0.5896138548851013 + 100.0 * 8.540092468261719
Epoch 760, val loss: 0.6478515863418579
Epoch 770, training loss: 854.4216918945312 = 0.5867846608161926 + 100.0 * 8.538349151611328
Epoch 770, val loss: 0.6461438536643982
Epoch 780, training loss: 854.2701416015625 = 0.5840912461280823 + 100.0 * 8.536860466003418
Epoch 780, val loss: 0.6445626020431519
Epoch 790, training loss: 854.1143188476562 = 0.5815188884735107 + 100.0 * 8.535327911376953
Epoch 790, val loss: 0.6430808901786804
Epoch 800, training loss: 853.9805297851562 = 0.5790656208992004 + 100.0 * 8.534014701843262
Epoch 800, val loss: 0.6417476534843445
Epoch 810, training loss: 854.3104248046875 = 0.576705276966095 + 100.0 * 8.537337303161621
Epoch 810, val loss: 0.6404703259468079
Epoch 820, training loss: 853.7006225585938 = 0.5744067430496216 + 100.0 * 8.531262397766113
Epoch 820, val loss: 0.6392737030982971
Epoch 830, training loss: 853.5770874023438 = 0.5722444653511047 + 100.0 * 8.530048370361328
Epoch 830, val loss: 0.6381867527961731
Epoch 840, training loss: 853.3865966796875 = 0.5701805949211121 + 100.0 * 8.52816390991211
Epoch 840, val loss: 0.6371818780899048
Epoch 850, training loss: 853.3394775390625 = 0.5682013630867004 + 100.0 * 8.52771282196045
Epoch 850, val loss: 0.6362993717193604
Epoch 860, training loss: 853.232177734375 = 0.5662764310836792 + 100.0 * 8.52665901184082
Epoch 860, val loss: 0.6354256868362427
Epoch 870, training loss: 853.1917724609375 = 0.5644152164459229 + 100.0 * 8.526273727416992
Epoch 870, val loss: 0.6346481442451477
Epoch 880, training loss: 852.9767456054688 = 0.5626364946365356 + 100.0 * 8.524141311645508
Epoch 880, val loss: 0.6339216232299805
Epoch 890, training loss: 852.7359619140625 = 0.5609191060066223 + 100.0 * 8.521750450134277
Epoch 890, val loss: 0.6332393288612366
Epoch 900, training loss: 852.7572631835938 = 0.5592707395553589 + 100.0 * 8.521980285644531
Epoch 900, val loss: 0.6325943470001221
Epoch 910, training loss: 852.5464477539062 = 0.5576702356338501 + 100.0 * 8.519887924194336
Epoch 910, val loss: 0.6320417523384094
Epoch 920, training loss: 852.41845703125 = 0.556140124797821 + 100.0 * 8.518623352050781
Epoch 920, val loss: 0.6315585374832153
Epoch 930, training loss: 852.34814453125 = 0.5546619892120361 + 100.0 * 8.517934799194336
Epoch 930, val loss: 0.6310930848121643
Epoch 940, training loss: 852.630615234375 = 0.5532371997833252 + 100.0 * 8.520773887634277
Epoch 940, val loss: 0.6307181715965271
Epoch 950, training loss: 852.2478637695312 = 0.5518301725387573 + 100.0 * 8.516960144042969
Epoch 950, val loss: 0.6303536295890808
Epoch 960, training loss: 852.0567016601562 = 0.550475537776947 + 100.0 * 8.51506233215332
Epoch 960, val loss: 0.6300033330917358
Epoch 970, training loss: 851.928955078125 = 0.5491766929626465 + 100.0 * 8.513797760009766
Epoch 970, val loss: 0.6297560930252075
Epoch 980, training loss: 851.8233642578125 = 0.5479097366333008 + 100.0 * 8.512754440307617
Epoch 980, val loss: 0.6295055747032166
Epoch 990, training loss: 851.9608154296875 = 0.5466855764389038 + 100.0 * 8.514141082763672
Epoch 990, val loss: 0.6293424367904663
Epoch 1000, training loss: 852.13623046875 = 0.5454090237617493 + 100.0 * 8.515908241271973
Epoch 1000, val loss: 0.6289030909538269
Epoch 1010, training loss: 851.69482421875 = 0.5442056059837341 + 100.0 * 8.511506080627441
Epoch 1010, val loss: 0.6287695169448853
Epoch 1020, training loss: 851.4820556640625 = 0.5430736541748047 + 100.0 * 8.509389877319336
Epoch 1020, val loss: 0.6286101937294006
Epoch 1030, training loss: 851.4011840820312 = 0.5419859290122986 + 100.0 * 8.508591651916504
Epoch 1030, val loss: 0.6284850835800171
Epoch 1040, training loss: 851.2972412109375 = 0.5409191846847534 + 100.0 * 8.507563591003418
Epoch 1040, val loss: 0.6283842325210571
Epoch 1050, training loss: 851.203125 = 0.5398736596107483 + 100.0 * 8.506632804870605
Epoch 1050, val loss: 0.6282603740692139
Epoch 1060, training loss: 851.1114501953125 = 0.5388524532318115 + 100.0 * 8.505725860595703
Epoch 1060, val loss: 0.6281887888908386
Epoch 1070, training loss: 851.1997680664062 = 0.53785240650177 + 100.0 * 8.506619453430176
Epoch 1070, val loss: 0.6281229257583618
Epoch 1080, training loss: 851.6922607421875 = 0.5368347764015198 + 100.0 * 8.511553764343262
Epoch 1080, val loss: 0.6280367970466614
Epoch 1090, training loss: 851.0911254882812 = 0.535820722579956 + 100.0 * 8.505553245544434
Epoch 1090, val loss: 0.6279894113540649
Epoch 1100, training loss: 850.7967529296875 = 0.5348766446113586 + 100.0 * 8.502618789672852
Epoch 1100, val loss: 0.6278714537620544
Epoch 1110, training loss: 850.67626953125 = 0.5339723825454712 + 100.0 * 8.501422882080078
Epoch 1110, val loss: 0.6278823614120483
Epoch 1120, training loss: 850.5905151367188 = 0.5330842137336731 + 100.0 * 8.500574111938477
Epoch 1120, val loss: 0.6279034614562988
Epoch 1130, training loss: 850.4992065429688 = 0.5322071313858032 + 100.0 * 8.499670028686523
Epoch 1130, val loss: 0.6279028058052063
Epoch 1140, training loss: 850.45068359375 = 0.5313428640365601 + 100.0 * 8.49919319152832
Epoch 1140, val loss: 0.6279503107070923
Epoch 1150, training loss: 851.0092163085938 = 0.5304759740829468 + 100.0 * 8.50478744506836
Epoch 1150, val loss: 0.6279323697090149
Epoch 1160, training loss: 850.3387451171875 = 0.5295736193656921 + 100.0 * 8.498091697692871
Epoch 1160, val loss: 0.6279333829879761
Epoch 1170, training loss: 850.2095336914062 = 0.5287243723869324 + 100.0 * 8.496808052062988
Epoch 1170, val loss: 0.6278606057167053
Epoch 1180, training loss: 850.1143798828125 = 0.5279154777526855 + 100.0 * 8.495864868164062
Epoch 1180, val loss: 0.6279021501541138
Epoch 1190, training loss: 850.0132446289062 = 0.5271191000938416 + 100.0 * 8.494861602783203
Epoch 1190, val loss: 0.6279770731925964
Epoch 1200, training loss: 849.924560546875 = 0.5263283848762512 + 100.0 * 8.493982315063477
Epoch 1200, val loss: 0.6280022263526917
Epoch 1210, training loss: 849.8497314453125 = 0.5255448818206787 + 100.0 * 8.493241310119629
Epoch 1210, val loss: 0.6280595064163208
Epoch 1220, training loss: 849.7979736328125 = 0.5247666239738464 + 100.0 * 8.492732048034668
Epoch 1220, val loss: 0.6281024217605591
Epoch 1230, training loss: 850.4048461914062 = 0.5239963531494141 + 100.0 * 8.498808860778809
Epoch 1230, val loss: 0.6281867623329163
Epoch 1240, training loss: 849.7703857421875 = 0.5231756567955017 + 100.0 * 8.492471694946289
Epoch 1240, val loss: 0.6281213760375977
Epoch 1250, training loss: 849.6690673828125 = 0.5224066972732544 + 100.0 * 8.491466522216797
Epoch 1250, val loss: 0.6281158328056335
Epoch 1260, training loss: 849.5296630859375 = 0.5216671228408813 + 100.0 * 8.490079879760742
Epoch 1260, val loss: 0.6281493306159973
Epoch 1270, training loss: 849.4542236328125 = 0.5209386348724365 + 100.0 * 8.489333152770996
Epoch 1270, val loss: 0.6282248497009277
Epoch 1280, training loss: 849.3941040039062 = 0.5202128291130066 + 100.0 * 8.488739013671875
Epoch 1280, val loss: 0.6282604336738586
Epoch 1290, training loss: 849.4081420898438 = 0.5194938778877258 + 100.0 * 8.488886833190918
Epoch 1290, val loss: 0.62832111120224
Epoch 1300, training loss: 850.23095703125 = 0.518772542476654 + 100.0 * 8.497121810913086
Epoch 1300, val loss: 0.628373920917511
Epoch 1310, training loss: 849.4876708984375 = 0.5179822444915771 + 100.0 * 8.489696502685547
Epoch 1310, val loss: 0.6282976269721985
Epoch 1320, training loss: 849.294921875 = 0.5172525644302368 + 100.0 * 8.487776756286621
Epoch 1320, val loss: 0.6282318234443665
Epoch 1330, training loss: 849.1339111328125 = 0.5165672302246094 + 100.0 * 8.486173629760742
Epoch 1330, val loss: 0.6283437609672546
Epoch 1340, training loss: 849.063720703125 = 0.5158876180648804 + 100.0 * 8.485478401184082
Epoch 1340, val loss: 0.6284357309341431
Epoch 1350, training loss: 849.0120849609375 = 0.5152069926261902 + 100.0 * 8.484969139099121
Epoch 1350, val loss: 0.6284707188606262
Epoch 1360, training loss: 848.9569091796875 = 0.5145260095596313 + 100.0 * 8.484423637390137
Epoch 1360, val loss: 0.6285305023193359
Epoch 1370, training loss: 848.92578125 = 0.5138490796089172 + 100.0 * 8.484119415283203
Epoch 1370, val loss: 0.6285738348960876
Epoch 1380, training loss: 849.60693359375 = 0.5131677389144897 + 100.0 * 8.490937232971191
Epoch 1380, val loss: 0.6285869479179382
Epoch 1390, training loss: 848.927734375 = 0.5124611258506775 + 100.0 * 8.484152793884277
Epoch 1390, val loss: 0.6286377310752869
Epoch 1400, training loss: 848.7864990234375 = 0.5117835402488708 + 100.0 * 8.482747077941895
Epoch 1400, val loss: 0.6286211013793945
Epoch 1410, training loss: 848.7344970703125 = 0.5111326575279236 + 100.0 * 8.482234001159668
Epoch 1410, val loss: 0.6286607980728149
Epoch 1420, training loss: 848.689453125 = 0.510487973690033 + 100.0 * 8.481789588928223
Epoch 1420, val loss: 0.6287108659744263
Epoch 1430, training loss: 848.8385009765625 = 0.5098421573638916 + 100.0 * 8.48328685760498
Epoch 1430, val loss: 0.6287221312522888
Epoch 1440, training loss: 848.6123046875 = 0.5091762542724609 + 100.0 * 8.48103141784668
Epoch 1440, val loss: 0.6287541389465332
Epoch 1450, training loss: 848.5697021484375 = 0.5085234642028809 + 100.0 * 8.480611801147461
Epoch 1450, val loss: 0.6287827491760254
Epoch 1460, training loss: 848.5130004882812 = 0.5078836679458618 + 100.0 * 8.480051040649414
Epoch 1460, val loss: 0.6287890076637268
Epoch 1470, training loss: 848.439208984375 = 0.5072557330131531 + 100.0 * 8.47931957244873
Epoch 1470, val loss: 0.6288514137268066
Epoch 1480, training loss: 848.8345947265625 = 0.5066197514533997 + 100.0 * 8.483280181884766
Epoch 1480, val loss: 0.6288197040557861
Epoch 1490, training loss: 848.4652099609375 = 0.5059776306152344 + 100.0 * 8.479592323303223
Epoch 1490, val loss: 0.6288962960243225
Epoch 1500, training loss: 848.3389282226562 = 0.5053438544273376 + 100.0 * 8.4783353805542
Epoch 1500, val loss: 0.6288571357727051
Epoch 1510, training loss: 848.59619140625 = 0.5047209858894348 + 100.0 * 8.480915069580078
Epoch 1510, val loss: 0.6288434267044067
Epoch 1520, training loss: 848.2103881835938 = 0.504082202911377 + 100.0 * 8.477063179016113
Epoch 1520, val loss: 0.6289202570915222
Epoch 1530, training loss: 848.1976928710938 = 0.5034620761871338 + 100.0 * 8.47694206237793
Epoch 1530, val loss: 0.6289091110229492
Epoch 1540, training loss: 848.1572875976562 = 0.5028555989265442 + 100.0 * 8.476544380187988
Epoch 1540, val loss: 0.6289256811141968
Epoch 1550, training loss: 848.0950927734375 = 0.5022533535957336 + 100.0 * 8.47592830657959
Epoch 1550, val loss: 0.6289699077606201
Epoch 1560, training loss: 848.42919921875 = 0.5016518831253052 + 100.0 * 8.479275703430176
Epoch 1560, val loss: 0.6290226578712463
Epoch 1570, training loss: 848.1514892578125 = 0.5010143518447876 + 100.0 * 8.4765043258667
Epoch 1570, val loss: 0.6289359331130981
Epoch 1580, training loss: 848.021728515625 = 0.500398576259613 + 100.0 * 8.475213050842285
Epoch 1580, val loss: 0.6289694309234619
Epoch 1590, training loss: 847.9428100585938 = 0.4997916519641876 + 100.0 * 8.474430084228516
Epoch 1590, val loss: 0.6289237141609192
Epoch 1600, training loss: 848.074951171875 = 0.4992024004459381 + 100.0 * 8.475757598876953
Epoch 1600, val loss: 0.6290001273155212
Epoch 1610, training loss: 847.9439697265625 = 0.4985758364200592 + 100.0 * 8.474453926086426
Epoch 1610, val loss: 0.6289051175117493
Epoch 1620, training loss: 847.8153076171875 = 0.49795228242874146 + 100.0 * 8.473174095153809
Epoch 1620, val loss: 0.6288774013519287
Epoch 1630, training loss: 847.7556762695312 = 0.49736154079437256 + 100.0 * 8.472582817077637
Epoch 1630, val loss: 0.6288824081420898
Epoch 1640, training loss: 847.7225341796875 = 0.4967780113220215 + 100.0 * 8.472257614135742
Epoch 1640, val loss: 0.6288729310035706
Epoch 1650, training loss: 847.6832275390625 = 0.4961969554424286 + 100.0 * 8.471870422363281
Epoch 1650, val loss: 0.628905713558197
Epoch 1660, training loss: 847.779541015625 = 0.4956112802028656 + 100.0 * 8.47283935546875
Epoch 1660, val loss: 0.6289091110229492
Epoch 1670, training loss: 847.6300659179688 = 0.49500203132629395 + 100.0 * 8.47135066986084
Epoch 1670, val loss: 0.6288037300109863
Epoch 1680, training loss: 847.582275390625 = 0.4943942427635193 + 100.0 * 8.470878601074219
Epoch 1680, val loss: 0.6287817358970642
Epoch 1690, training loss: 847.5513916015625 = 0.49380001425743103 + 100.0 * 8.470576286315918
Epoch 1690, val loss: 0.6287069320678711
Epoch 1700, training loss: 847.5079345703125 = 0.4932120144367218 + 100.0 * 8.470147132873535
Epoch 1700, val loss: 0.6287134885787964
Epoch 1710, training loss: 847.8855590820312 = 0.4926167130470276 + 100.0 * 8.473929405212402
Epoch 1710, val loss: 0.6285834312438965
Epoch 1720, training loss: 847.8540649414062 = 0.49198007583618164 + 100.0 * 8.473620414733887
Epoch 1720, val loss: 0.6286262273788452
Epoch 1730, training loss: 847.4547119140625 = 0.4913328289985657 + 100.0 * 8.469634056091309
Epoch 1730, val loss: 0.6284699440002441
Epoch 1740, training loss: 847.4293212890625 = 0.49073687195777893 + 100.0 * 8.469386100769043
Epoch 1740, val loss: 0.6284586191177368
Epoch 1750, training loss: 847.3328247070312 = 0.49015459418296814 + 100.0 * 8.468426704406738
Epoch 1750, val loss: 0.6284208297729492
Epoch 1760, training loss: 847.2908325195312 = 0.4895728826522827 + 100.0 * 8.468012809753418
Epoch 1760, val loss: 0.6284361481666565
Epoch 1770, training loss: 847.251953125 = 0.4889836013317108 + 100.0 * 8.467629432678223
Epoch 1770, val loss: 0.6284099817276001
Epoch 1780, training loss: 847.2214965820312 = 0.4883882999420166 + 100.0 * 8.467330932617188
Epoch 1780, val loss: 0.6283577680587769
Epoch 1790, training loss: 847.396240234375 = 0.4877886176109314 + 100.0 * 8.469084739685059
Epoch 1790, val loss: 0.628311038017273
Epoch 1800, training loss: 847.2769165039062 = 0.4871525466442108 + 100.0 * 8.467897415161133
Epoch 1800, val loss: 0.6283048987388611
Epoch 1810, training loss: 847.3079223632812 = 0.486505925655365 + 100.0 * 8.46821403503418
Epoch 1810, val loss: 0.6281529068946838
Epoch 1820, training loss: 847.1066284179688 = 0.48589226603507996 + 100.0 * 8.466207504272461
Epoch 1820, val loss: 0.6280795931816101
Epoch 1830, training loss: 847.0918579101562 = 0.4852921664714813 + 100.0 * 8.466065406799316
Epoch 1830, val loss: 0.6280797123908997
Epoch 1840, training loss: 847.1158447265625 = 0.4846847653388977 + 100.0 * 8.46631145477295
Epoch 1840, val loss: 0.6280049085617065
Epoch 1850, training loss: 847.4508056640625 = 0.4840599298477173 + 100.0 * 8.469667434692383
Epoch 1850, val loss: 0.6278933882713318
Epoch 1860, training loss: 847.0936279296875 = 0.48341554403305054 + 100.0 * 8.466102600097656
Epoch 1860, val loss: 0.6279315948486328
Epoch 1870, training loss: 846.99072265625 = 0.4827812612056732 + 100.0 * 8.465079307556152
Epoch 1870, val loss: 0.6278120279312134
Epoch 1880, training loss: 846.9322509765625 = 0.48216208815574646 + 100.0 * 8.464500427246094
Epoch 1880, val loss: 0.6277726888656616
Epoch 1890, training loss: 846.9047241210938 = 0.48153355717658997 + 100.0 * 8.464232444763184
Epoch 1890, val loss: 0.6276783347129822
Epoch 1900, training loss: 847.1190185546875 = 0.4808964133262634 + 100.0 * 8.466381072998047
Epoch 1900, val loss: 0.6276108622550964
Epoch 1910, training loss: 846.8899536132812 = 0.480223685503006 + 100.0 * 8.464097023010254
Epoch 1910, val loss: 0.6275028586387634
Epoch 1920, training loss: 846.8705444335938 = 0.47955822944641113 + 100.0 * 8.463910102844238
Epoch 1920, val loss: 0.6274429559707642
Epoch 1930, training loss: 846.81884765625 = 0.478902667760849 + 100.0 * 8.463399887084961
Epoch 1930, val loss: 0.6273218989372253
Epoch 1940, training loss: 846.7566528320312 = 0.47825297713279724 + 100.0 * 8.462783813476562
Epoch 1940, val loss: 0.6272762417793274
Epoch 1950, training loss: 846.7573852539062 = 0.4775986969470978 + 100.0 * 8.462798118591309
Epoch 1950, val loss: 0.6271807551383972
Epoch 1960, training loss: 847.1904907226562 = 0.4769328832626343 + 100.0 * 8.467135429382324
Epoch 1960, val loss: 0.6270230412483215
Epoch 1970, training loss: 846.851318359375 = 0.4762270450592041 + 100.0 * 8.463750839233398
Epoch 1970, val loss: 0.6269620060920715
Epoch 1980, training loss: 846.7310180664062 = 0.47553569078445435 + 100.0 * 8.462554931640625
Epoch 1980, val loss: 0.626812756061554
Epoch 1990, training loss: 846.6357421875 = 0.4748597741127014 + 100.0 * 8.46160888671875
Epoch 1990, val loss: 0.6267316937446594
Epoch 2000, training loss: 846.625732421875 = 0.47418108582496643 + 100.0 * 8.461515426635742
Epoch 2000, val loss: 0.6266175508499146
Epoch 2010, training loss: 846.729736328125 = 0.4734938144683838 + 100.0 * 8.462562561035156
Epoch 2010, val loss: 0.6265130043029785
Epoch 2020, training loss: 846.928466796875 = 0.4727882444858551 + 100.0 * 8.464556694030762
Epoch 2020, val loss: 0.6263955235481262
Epoch 2030, training loss: 846.579833984375 = 0.4720456004142761 + 100.0 * 8.461077690124512
Epoch 2030, val loss: 0.6261095404624939
Epoch 2040, training loss: 846.54150390625 = 0.47133198380470276 + 100.0 * 8.460701942443848
Epoch 2040, val loss: 0.6260188221931458
Epoch 2050, training loss: 846.4683837890625 = 0.4706273376941681 + 100.0 * 8.459977149963379
Epoch 2050, val loss: 0.6258701682090759
Epoch 2060, training loss: 846.4630737304688 = 0.469921350479126 + 100.0 * 8.459931373596191
Epoch 2060, val loss: 0.6257467269897461
Epoch 2070, training loss: 846.5711669921875 = 0.46921056509017944 + 100.0 * 8.461019515991211
Epoch 2070, val loss: 0.6256207227706909
Epoch 2080, training loss: 846.6841430664062 = 0.4684731066226959 + 100.0 * 8.462157249450684
Epoch 2080, val loss: 0.6254743933677673
Epoch 2090, training loss: 846.5166625976562 = 0.4677169620990753 + 100.0 * 8.460489273071289
Epoch 2090, val loss: 0.6252279281616211
Epoch 2100, training loss: 846.3729248046875 = 0.46698206663131714 + 100.0 * 8.459059715270996
Epoch 2100, val loss: 0.6251015663146973
Epoch 2110, training loss: 846.482177734375 = 0.4662518799304962 + 100.0 * 8.460159301757812
Epoch 2110, val loss: 0.6249468326568604
Epoch 2120, training loss: 846.394775390625 = 0.46550044417381287 + 100.0 * 8.4592924118042
Epoch 2120, val loss: 0.6247330904006958
Epoch 2130, training loss: 846.311767578125 = 0.4647431969642639 + 100.0 * 8.458470344543457
Epoch 2130, val loss: 0.6245618462562561
Epoch 2140, training loss: 846.3485107421875 = 0.4639866352081299 + 100.0 * 8.458845138549805
Epoch 2140, val loss: 0.6243963837623596
Epoch 2150, training loss: 846.640380859375 = 0.4632137417793274 + 100.0 * 8.461771965026855
Epoch 2150, val loss: 0.6242464780807495
Epoch 2160, training loss: 846.3108520507812 = 0.4624009430408478 + 100.0 * 8.458484649658203
Epoch 2160, val loss: 0.6238434314727783
Epoch 2170, training loss: 846.2174682617188 = 0.4616190493106842 + 100.0 * 8.457558631896973
Epoch 2170, val loss: 0.6237813830375671
Epoch 2180, training loss: 846.179931640625 = 0.4608355462551117 + 100.0 * 8.457191467285156
Epoch 2180, val loss: 0.6235080361366272
Epoch 2190, training loss: 846.4898071289062 = 0.4600450396537781 + 100.0 * 8.460297584533691
Epoch 2190, val loss: 0.6233302354812622
Epoch 2200, training loss: 846.1271362304688 = 0.4592136740684509 + 100.0 * 8.456679344177246
Epoch 2200, val loss: 0.6230751276016235
Epoch 2210, training loss: 846.0848388671875 = 0.4583953619003296 + 100.0 * 8.45626449584961
Epoch 2210, val loss: 0.6229122281074524
Epoch 2220, training loss: 846.1463012695312 = 0.45757779479026794 + 100.0 * 8.456887245178223
Epoch 2220, val loss: 0.6226228475570679
Epoch 2230, training loss: 846.5472412109375 = 0.4567330479621887 + 100.0 * 8.460905075073242
Epoch 2230, val loss: 0.6223787069320679
Epoch 2240, training loss: 846.135986328125 = 0.45585715770721436 + 100.0 * 8.456801414489746
Epoch 2240, val loss: 0.6221306920051575
Epoch 2250, training loss: 846.0261840820312 = 0.45500174164772034 + 100.0 * 8.455711364746094
Epoch 2250, val loss: 0.621860682964325
Epoch 2260, training loss: 845.9767456054688 = 0.45415177941322327 + 100.0 * 8.455225944519043
Epoch 2260, val loss: 0.6217101216316223
Epoch 2270, training loss: 845.9320068359375 = 0.45329582691192627 + 100.0 * 8.454787254333496
Epoch 2270, val loss: 0.6214765310287476
Epoch 2280, training loss: 845.9046630859375 = 0.4524306654930115 + 100.0 * 8.454522132873535
Epoch 2280, val loss: 0.6212509274482727
Epoch 2290, training loss: 846.2167358398438 = 0.4515555500984192 + 100.0 * 8.45765209197998
Epoch 2290, val loss: 0.6209638714790344
Epoch 2300, training loss: 845.9735107421875 = 0.4506263732910156 + 100.0 * 8.455228805541992
Epoch 2300, val loss: 0.6206619143486023
Epoch 2310, training loss: 845.9383544921875 = 0.44969943165779114 + 100.0 * 8.454886436462402
Epoch 2310, val loss: 0.620403528213501
Epoch 2320, training loss: 845.8470458984375 = 0.4487925171852112 + 100.0 * 8.45398235321045
Epoch 2320, val loss: 0.6202027797698975
Epoch 2330, training loss: 845.786865234375 = 0.44788286089897156 + 100.0 * 8.453390121459961
Epoch 2330, val loss: 0.6198834776878357
Epoch 2340, training loss: 845.802001953125 = 0.4469698667526245 + 100.0 * 8.453550338745117
Epoch 2340, val loss: 0.6197119355201721
Epoch 2350, training loss: 846.3546752929688 = 0.44604527950286865 + 100.0 * 8.459086418151855
Epoch 2350, val loss: 0.6195013523101807
Epoch 2360, training loss: 845.89697265625 = 0.4450604021549225 + 100.0 * 8.454519271850586
Epoch 2360, val loss: 0.619012176990509
Epoch 2370, training loss: 845.7507934570312 = 0.444105863571167 + 100.0 * 8.4530668258667
Epoch 2370, val loss: 0.6188899874687195
Epoch 2380, training loss: 845.6945190429688 = 0.44315940141677856 + 100.0 * 8.452513694763184
Epoch 2380, val loss: 0.6186062693595886
Epoch 2390, training loss: 845.704345703125 = 0.4422074556350708 + 100.0 * 8.452621459960938
Epoch 2390, val loss: 0.6184069514274597
Epoch 2400, training loss: 846.1106567382812 = 0.441236287355423 + 100.0 * 8.456694602966309
Epoch 2400, val loss: 0.6180885434150696
Epoch 2410, training loss: 845.6939086914062 = 0.44022536277770996 + 100.0 * 8.452536582946777
Epoch 2410, val loss: 0.6178434491157532
Epoch 2420, training loss: 845.5994262695312 = 0.43922826647758484 + 100.0 * 8.4516019821167
Epoch 2420, val loss: 0.6176111102104187
Epoch 2430, training loss: 845.5695190429688 = 0.43823111057281494 + 100.0 * 8.451313018798828
Epoch 2430, val loss: 0.6173970103263855
Epoch 2440, training loss: 845.91943359375 = 0.4372271001338959 + 100.0 * 8.454821586608887
Epoch 2440, val loss: 0.6172217130661011
Epoch 2450, training loss: 845.575439453125 = 0.4361777901649475 + 100.0 * 8.451393127441406
Epoch 2450, val loss: 0.6168856024742126
Epoch 2460, training loss: 845.5521240234375 = 0.4351366460323334 + 100.0 * 8.451169967651367
Epoch 2460, val loss: 0.6166182160377502
Epoch 2470, training loss: 845.513427734375 = 0.43409964442253113 + 100.0 * 8.450793266296387
Epoch 2470, val loss: 0.6164065599441528
Epoch 2480, training loss: 845.7711791992188 = 0.43305617570877075 + 100.0 * 8.453381538391113
Epoch 2480, val loss: 0.6161091923713684
Epoch 2490, training loss: 845.4507446289062 = 0.4319670498371124 + 100.0 * 8.450187683105469
Epoch 2490, val loss: 0.6158921718597412
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7661085743277524
0.8148953126132001
The final CL Acc:0.77051, 0.01081, The final GNN Acc:0.81333, 0.00197
raw graph: torch.Size([2, 88648])
add edge: torch.Size([2, 110852])
remove edge: torch.Size([2, 66482])
updated graph: torch.Size([2, 88686])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 1059.31689453125 = 1.0847828388214111 + 100.0 * 10.582320213317871
Epoch 0, val loss: 1.0837843418121338
Epoch 10, training loss: 1059.28564453125 = 1.081352710723877 + 100.0 * 10.582043647766113
Epoch 10, val loss: 1.0804229974746704
Epoch 20, training loss: 1059.1568603515625 = 1.07770836353302 + 100.0 * 10.580791473388672
Epoch 20, val loss: 1.0768507719039917
Epoch 30, training loss: 1058.5662841796875 = 1.07381010055542 + 100.0 * 10.57492446899414
Epoch 30, val loss: 1.0730386972427368
Epoch 40, training loss: 1055.964599609375 = 1.0695791244506836 + 100.0 * 10.5489501953125
Epoch 40, val loss: 1.0688694715499878
Epoch 50, training loss: 1045.9212646484375 = 1.06485116481781 + 100.0 * 10.448564529418945
Epoch 50, val loss: 1.0641957521438599
Epoch 60, training loss: 1014.8264770507812 = 1.059714436531067 + 100.0 * 10.137667655944824
Epoch 60, val loss: 1.059080958366394
Epoch 70, training loss: 974.72216796875 = 1.0539318323135376 + 100.0 * 9.736681938171387
Epoch 70, val loss: 1.0532381534576416
Epoch 80, training loss: 966.2532958984375 = 1.0482949018478394 + 100.0 * 9.652050018310547
Epoch 80, val loss: 1.0476162433624268
Epoch 90, training loss: 958.1819458007812 = 1.043539047241211 + 100.0 * 9.57138442993164
Epoch 90, val loss: 1.0428338050842285
Epoch 100, training loss: 944.822509765625 = 1.0391141176223755 + 100.0 * 9.437833786010742
Epoch 100, val loss: 1.038439393043518
Epoch 110, training loss: 930.9765014648438 = 1.0346941947937012 + 100.0 * 9.299418449401855
Epoch 110, val loss: 1.0341507196426392
Epoch 120, training loss: 925.2743530273438 = 1.0302053689956665 + 100.0 * 9.242441177368164
Epoch 120, val loss: 1.029741644859314
Epoch 130, training loss: 920.497802734375 = 1.0253621339797974 + 100.0 * 9.194724082946777
Epoch 130, val loss: 1.0249518156051636
Epoch 140, training loss: 912.806396484375 = 1.0212016105651855 + 100.0 * 9.117852210998535
Epoch 140, val loss: 1.0209827423095703
Epoch 150, training loss: 903.9985961914062 = 1.0184425115585327 + 100.0 * 9.029801368713379
Epoch 150, val loss: 1.018370270729065
Epoch 160, training loss: 899.5772705078125 = 1.0152010917663574 + 100.0 * 8.985620498657227
Epoch 160, val loss: 1.0150002241134644
Epoch 170, training loss: 894.044189453125 = 1.010205864906311 + 100.0 * 8.930339813232422
Epoch 170, val loss: 1.009835124015808
Epoch 180, training loss: 888.8099365234375 = 1.0049254894256592 + 100.0 * 8.878049850463867
Epoch 180, val loss: 1.0046223402023315
Epoch 190, training loss: 885.0475463867188 = 0.9996330738067627 + 100.0 * 8.840478897094727
Epoch 190, val loss: 0.9992882609367371
Epoch 200, training loss: 882.421875 = 0.9934406876564026 + 100.0 * 8.814284324645996
Epoch 200, val loss: 0.9930249452590942
Epoch 210, training loss: 880.7949829101562 = 0.986274003982544 + 100.0 * 8.798087120056152
Epoch 210, val loss: 0.9857752919197083
Epoch 220, training loss: 879.5938720703125 = 0.9782655239105225 + 100.0 * 8.786155700683594
Epoch 220, val loss: 0.9777451157569885
Epoch 230, training loss: 878.4671020507812 = 0.9697388410568237 + 100.0 * 8.77497386932373
Epoch 230, val loss: 0.9692475199699402
Epoch 240, training loss: 877.4630737304688 = 0.960861086845398 + 100.0 * 8.765022277832031
Epoch 240, val loss: 0.9603821039199829
Epoch 250, training loss: 876.3751220703125 = 0.9515008330345154 + 100.0 * 8.754236221313477
Epoch 250, val loss: 0.9510693550109863
Epoch 260, training loss: 874.9971313476562 = 0.9416332840919495 + 100.0 * 8.740554809570312
Epoch 260, val loss: 0.941264271736145
Epoch 270, training loss: 873.5516967773438 = 0.9313130378723145 + 100.0 * 8.726203918457031
Epoch 270, val loss: 0.931013286113739
Epoch 280, training loss: 872.0685424804688 = 0.9205295443534851 + 100.0 * 8.711480140686035
Epoch 280, val loss: 0.920322835445404
Epoch 290, training loss: 870.8932495117188 = 0.9091607332229614 + 100.0 * 8.699840545654297
Epoch 290, val loss: 0.9090440273284912
Epoch 300, training loss: 869.6508178710938 = 0.8970118165016174 + 100.0 * 8.687538146972656
Epoch 300, val loss: 0.8970442414283752
Epoch 310, training loss: 868.779296875 = 0.8841565847396851 + 100.0 * 8.678951263427734
Epoch 310, val loss: 0.8843679428100586
Epoch 320, training loss: 867.6824340820312 = 0.8706915974617004 + 100.0 * 8.66811752319336
Epoch 320, val loss: 0.8712394833564758
Epoch 330, training loss: 866.57080078125 = 0.8570123910903931 + 100.0 * 8.657137870788574
Epoch 330, val loss: 0.8578187823295593
Epoch 340, training loss: 865.4666137695312 = 0.8430658578872681 + 100.0 * 8.646235466003418
Epoch 340, val loss: 0.8442673683166504
Epoch 350, training loss: 864.399169921875 = 0.829014241695404 + 100.0 * 8.635701179504395
Epoch 350, val loss: 0.8305327296257019
Epoch 360, training loss: 863.381103515625 = 0.814816951751709 + 100.0 * 8.625662803649902
Epoch 360, val loss: 0.8166825771331787
Epoch 370, training loss: 862.2470092773438 = 0.8005096912384033 + 100.0 * 8.61446475982666
Epoch 370, val loss: 0.8028144836425781
Epoch 380, training loss: 861.6973266601562 = 0.7861543893814087 + 100.0 * 8.609111785888672
Epoch 380, val loss: 0.7888817191123962
Epoch 390, training loss: 860.5136108398438 = 0.7717505693435669 + 100.0 * 8.597418785095215
Epoch 390, val loss: 0.7748920917510986
Epoch 400, training loss: 859.8377685546875 = 0.7573785185813904 + 100.0 * 8.590804100036621
Epoch 400, val loss: 0.7609941959381104
Epoch 410, training loss: 859.2041015625 = 0.743090033531189 + 100.0 * 8.584609985351562
Epoch 410, val loss: 0.7471710443496704
Epoch 420, training loss: 858.6344604492188 = 0.7288748025894165 + 100.0 * 8.579055786132812
Epoch 420, val loss: 0.7335307598114014
Epoch 430, training loss: 858.0621337890625 = 0.7148661017417908 + 100.0 * 8.57347297668457
Epoch 430, val loss: 0.7199963331222534
Epoch 440, training loss: 857.5819091796875 = 0.7010771632194519 + 100.0 * 8.568808555603027
Epoch 440, val loss: 0.7068020701408386
Epoch 450, training loss: 857.1074829101562 = 0.687609076499939 + 100.0 * 8.56419849395752
Epoch 450, val loss: 0.6938202381134033
Epoch 460, training loss: 856.7354736328125 = 0.6743772625923157 + 100.0 * 8.5606107711792
Epoch 460, val loss: 0.6811453700065613
Epoch 470, training loss: 856.3931884765625 = 0.6614322066307068 + 100.0 * 8.557317733764648
Epoch 470, val loss: 0.6687723398208618
Epoch 480, training loss: 855.8860473632812 = 0.6489095091819763 + 100.0 * 8.55237102508545
Epoch 480, val loss: 0.6567838788032532
Epoch 490, training loss: 855.4745483398438 = 0.6368055939674377 + 100.0 * 8.548377990722656
Epoch 490, val loss: 0.6451933979988098
Epoch 500, training loss: 855.1263427734375 = 0.6251406669616699 + 100.0 * 8.545012474060059
Epoch 500, val loss: 0.6340205073356628
Epoch 510, training loss: 855.0138549804688 = 0.6139073371887207 + 100.0 * 8.543999671936035
Epoch 510, val loss: 0.6232447624206543
Epoch 520, training loss: 854.5911254882812 = 0.602961003780365 + 100.0 * 8.539881706237793
Epoch 520, val loss: 0.6129058599472046
Epoch 530, training loss: 854.1888427734375 = 0.5925586223602295 + 100.0 * 8.53596305847168
Epoch 530, val loss: 0.602995753288269
Epoch 540, training loss: 853.8562622070312 = 0.5825862884521484 + 100.0 * 8.532736778259277
Epoch 540, val loss: 0.5934688448905945
Epoch 550, training loss: 853.6063842773438 = 0.5730457305908203 + 100.0 * 8.530333518981934
Epoch 550, val loss: 0.5843898057937622
Epoch 560, training loss: 853.5457153320312 = 0.5638601183891296 + 100.0 * 8.529818534851074
Epoch 560, val loss: 0.5757006406784058
Epoch 570, training loss: 853.1561889648438 = 0.5551027655601501 + 100.0 * 8.526010513305664
Epoch 570, val loss: 0.5674483776092529
Epoch 580, training loss: 852.894287109375 = 0.5467644333839417 + 100.0 * 8.523475646972656
Epoch 580, val loss: 0.5595861077308655
Epoch 590, training loss: 852.6380615234375 = 0.5388423800468445 + 100.0 * 8.520992279052734
Epoch 590, val loss: 0.5521388053894043
Epoch 600, training loss: 852.4142456054688 = 0.5313141942024231 + 100.0 * 8.518829345703125
Epoch 600, val loss: 0.5451066493988037
Epoch 610, training loss: 852.7088623046875 = 0.5241479277610779 + 100.0 * 8.521846771240234
Epoch 610, val loss: 0.5383853316307068
Epoch 620, training loss: 852.0264892578125 = 0.5172704458236694 + 100.0 * 8.515091896057129
Epoch 620, val loss: 0.532117486000061
Epoch 630, training loss: 851.7789306640625 = 0.5107849836349487 + 100.0 * 8.512681007385254
Epoch 630, val loss: 0.526118278503418
Epoch 640, training loss: 851.5822143554688 = 0.5046331882476807 + 100.0 * 8.510775566101074
Epoch 640, val loss: 0.5204818844795227
Epoch 650, training loss: 851.3638305664062 = 0.4987976551055908 + 100.0 * 8.508650779724121
Epoch 650, val loss: 0.515201985836029
Epoch 660, training loss: 851.4371948242188 = 0.4932219386100769 + 100.0 * 8.509439468383789
Epoch 660, val loss: 0.5101871490478516
Epoch 670, training loss: 851.0243530273438 = 0.4879479706287384 + 100.0 * 8.505363464355469
Epoch 670, val loss: 0.5054611563682556
Epoch 680, training loss: 850.7308959960938 = 0.48296254873275757 + 100.0 * 8.502479553222656
Epoch 680, val loss: 0.5010104775428772
Epoch 690, training loss: 850.564697265625 = 0.4782435894012451 + 100.0 * 8.500864028930664
Epoch 690, val loss: 0.49684739112854004
Epoch 700, training loss: 850.440673828125 = 0.47371628880500793 + 100.0 * 8.499670028686523
Epoch 700, val loss: 0.4929450452327728
Epoch 710, training loss: 850.2286987304688 = 0.46941426396369934 + 100.0 * 8.49759292602539
Epoch 710, val loss: 0.4892730414867401
Epoch 720, training loss: 849.9874877929688 = 0.4653720259666443 + 100.0 * 8.495221138000488
Epoch 720, val loss: 0.4857824146747589
Epoch 730, training loss: 849.7830810546875 = 0.46153903007507324 + 100.0 * 8.493215560913086
Epoch 730, val loss: 0.48250269889831543
Epoch 740, training loss: 849.5918579101562 = 0.4579048454761505 + 100.0 * 8.491339683532715
Epoch 740, val loss: 0.4794866442680359
Epoch 750, training loss: 849.4097900390625 = 0.45444172620773315 + 100.0 * 8.489553451538086
Epoch 750, val loss: 0.4766412079334259
Epoch 760, training loss: 849.7154541015625 = 0.45113441348075867 + 100.0 * 8.492643356323242
Epoch 760, val loss: 0.4739372134208679
Epoch 770, training loss: 849.2359619140625 = 0.44797971844673157 + 100.0 * 8.487879753112793
Epoch 770, val loss: 0.4714195132255554
Epoch 780, training loss: 848.9640502929688 = 0.4449884593486786 + 100.0 * 8.485190391540527
Epoch 780, val loss: 0.4690123200416565
Epoch 790, training loss: 848.774658203125 = 0.44214195013046265 + 100.0 * 8.483325004577637
Epoch 790, val loss: 0.4667647182941437
Epoch 800, training loss: 848.7257690429688 = 0.43942970037460327 + 100.0 * 8.482863426208496
Epoch 800, val loss: 0.4646691679954529
Epoch 810, training loss: 848.4977416992188 = 0.43682950735092163 + 100.0 * 8.480608940124512
Epoch 810, val loss: 0.4627131223678589
Epoch 820, training loss: 848.4961547851562 = 0.4343377947807312 + 100.0 * 8.480618476867676
Epoch 820, val loss: 0.4608272314071655
Epoch 830, training loss: 848.1637573242188 = 0.43197375535964966 + 100.0 * 8.477317810058594
Epoch 830, val loss: 0.4590788185596466
Epoch 840, training loss: 847.9437255859375 = 0.42971372604370117 + 100.0 * 8.475140571594238
Epoch 840, val loss: 0.4574275016784668
Epoch 850, training loss: 847.858642578125 = 0.42755886912345886 + 100.0 * 8.474310874938965
Epoch 850, val loss: 0.4558802545070648
Epoch 860, training loss: 847.7139892578125 = 0.42547839879989624 + 100.0 * 8.472885131835938
Epoch 860, val loss: 0.4544130861759186
Epoch 870, training loss: 847.6344604492188 = 0.42348116636276245 + 100.0 * 8.4721097946167
Epoch 870, val loss: 0.45307981967926025
Epoch 880, training loss: 847.367431640625 = 0.421577125787735 + 100.0 * 8.46945858001709
Epoch 880, val loss: 0.4517182409763336
Epoch 890, training loss: 847.2279663085938 = 0.4197515547275543 + 100.0 * 8.468082427978516
Epoch 890, val loss: 0.4505144953727722
Epoch 900, training loss: 847.2377319335938 = 0.4179948568344116 + 100.0 * 8.4681978225708
Epoch 900, val loss: 0.4493434429168701
Epoch 910, training loss: 846.9170532226562 = 0.416285902261734 + 100.0 * 8.465007781982422
Epoch 910, val loss: 0.44828900694847107
Epoch 920, training loss: 846.8265380859375 = 0.4146548807621002 + 100.0 * 8.464118957519531
Epoch 920, val loss: 0.44723838567733765
Epoch 930, training loss: 846.7596435546875 = 0.4130828380584717 + 100.0 * 8.463465690612793
Epoch 930, val loss: 0.4462224245071411
Epoch 940, training loss: 846.652099609375 = 0.4115639925003052 + 100.0 * 8.46240520477295
Epoch 940, val loss: 0.445361465215683
Epoch 950, training loss: 846.4832153320312 = 0.4100798964500427 + 100.0 * 8.460731506347656
Epoch 950, val loss: 0.4443931579589844
Epoch 960, training loss: 846.3167724609375 = 0.4086533188819885 + 100.0 * 8.459081649780273
Epoch 960, val loss: 0.4436436891555786
Epoch 970, training loss: 846.1695556640625 = 0.4072795510292053 + 100.0 * 8.457622528076172
Epoch 970, val loss: 0.44282570481300354
Epoch 980, training loss: 846.5237426757812 = 0.4059440791606903 + 100.0 * 8.461177825927734
Epoch 980, val loss: 0.4420761168003082
Epoch 990, training loss: 846.161865234375 = 0.40463805198669434 + 100.0 * 8.457571983337402
Epoch 990, val loss: 0.44135934114456177
Epoch 1000, training loss: 845.9071655273438 = 0.4033726453781128 + 100.0 * 8.455038070678711
Epoch 1000, val loss: 0.4406568109989166
Epoch 1010, training loss: 845.7792358398438 = 0.4021526277065277 + 100.0 * 8.453770637512207
Epoch 1010, val loss: 0.4399765431880951
Epoch 1020, training loss: 845.7036743164062 = 0.4009753465652466 + 100.0 * 8.45302677154541
Epoch 1020, val loss: 0.4393843114376068
Epoch 1030, training loss: 845.9696655273438 = 0.3998274803161621 + 100.0 * 8.455698013305664
Epoch 1030, val loss: 0.4387851655483246
Epoch 1040, training loss: 845.8329467773438 = 0.39867135882377625 + 100.0 * 8.45434284210205
Epoch 1040, val loss: 0.4382317364215851
Epoch 1050, training loss: 845.48828125 = 0.3975702226161957 + 100.0 * 8.450906753540039
Epoch 1050, val loss: 0.4376697838306427
Epoch 1060, training loss: 845.4307250976562 = 0.39649638533592224 + 100.0 * 8.450342178344727
Epoch 1060, val loss: 0.4371100068092346
Epoch 1070, training loss: 845.3197021484375 = 0.395458847284317 + 100.0 * 8.44924259185791
Epoch 1070, val loss: 0.43665286898612976
Epoch 1080, training loss: 845.2698974609375 = 0.3944453001022339 + 100.0 * 8.44875431060791
Epoch 1080, val loss: 0.4361463785171509
Epoch 1090, training loss: 845.487548828125 = 0.39345473051071167 + 100.0 * 8.45094108581543
Epoch 1090, val loss: 0.4356650412082672
Epoch 1100, training loss: 845.1734619140625 = 0.3924582898616791 + 100.0 * 8.447810173034668
Epoch 1100, val loss: 0.4353122413158417
Epoch 1110, training loss: 845.08154296875 = 0.39150649309158325 + 100.0 * 8.446900367736816
Epoch 1110, val loss: 0.43480709195137024
Epoch 1120, training loss: 845.0073852539062 = 0.3905733823776245 + 100.0 * 8.446167945861816
Epoch 1120, val loss: 0.4344492256641388
Epoch 1130, training loss: 844.9371948242188 = 0.38966721296310425 + 100.0 * 8.445475578308105
Epoch 1130, val loss: 0.4340524971485138
Epoch 1140, training loss: 845.174072265625 = 0.3887777030467987 + 100.0 * 8.447853088378906
Epoch 1140, val loss: 0.4337053894996643
Epoch 1150, training loss: 844.9998779296875 = 0.38788989186286926 + 100.0 * 8.446120262145996
Epoch 1150, val loss: 0.4332985281944275
Epoch 1160, training loss: 844.78515625 = 0.3870253562927246 + 100.0 * 8.443981170654297
Epoch 1160, val loss: 0.43294650316238403
Epoch 1170, training loss: 844.68212890625 = 0.38618558645248413 + 100.0 * 8.442959785461426
Epoch 1170, val loss: 0.4326328635215759
Epoch 1180, training loss: 844.7451782226562 = 0.3853641152381897 + 100.0 * 8.443597793579102
Epoch 1180, val loss: 0.4323129951953888
Epoch 1190, training loss: 844.6392211914062 = 0.3845382332801819 + 100.0 * 8.442546844482422
Epoch 1190, val loss: 0.4320218563079834
Epoch 1200, training loss: 844.5255126953125 = 0.3837365210056305 + 100.0 * 8.441417694091797
Epoch 1200, val loss: 0.4316936731338501
Epoch 1210, training loss: 844.4535522460938 = 0.3829537332057953 + 100.0 * 8.440706253051758
Epoch 1210, val loss: 0.43140679597854614
Epoch 1220, training loss: 844.3663330078125 = 0.3821868598461151 + 100.0 * 8.439841270446777
Epoch 1220, val loss: 0.4311317801475525
Epoch 1230, training loss: 844.3062133789062 = 0.38143283128738403 + 100.0 * 8.439248085021973
Epoch 1230, val loss: 0.43085333704948425
Epoch 1240, training loss: 844.70947265625 = 0.38068079948425293 + 100.0 * 8.44328784942627
Epoch 1240, val loss: 0.43061280250549316
Epoch 1250, training loss: 844.357421875 = 0.3799351751804352 + 100.0 * 8.439774513244629
Epoch 1250, val loss: 0.43031826615333557
Epoch 1260, training loss: 844.1524658203125 = 0.3791995346546173 + 100.0 * 8.437732696533203
Epoch 1260, val loss: 0.4300229251384735
Epoch 1270, training loss: 844.0394287109375 = 0.3784812092781067 + 100.0 * 8.436609268188477
Epoch 1270, val loss: 0.429789662361145
Epoch 1280, training loss: 844.0650634765625 = 0.37777844071388245 + 100.0 * 8.436872482299805
Epoch 1280, val loss: 0.429548978805542
Epoch 1290, training loss: 844.208740234375 = 0.37707364559173584 + 100.0 * 8.438316345214844
Epoch 1290, val loss: 0.4292697608470917
Epoch 1300, training loss: 843.9063110351562 = 0.37637218832969666 + 100.0 * 8.43529987335205
Epoch 1300, val loss: 0.42908620834350586
Epoch 1310, training loss: 843.788330078125 = 0.3756893575191498 + 100.0 * 8.434126853942871
Epoch 1310, val loss: 0.428825706243515
Epoch 1320, training loss: 843.7289428710938 = 0.375021755695343 + 100.0 * 8.433539390563965
Epoch 1320, val loss: 0.4286123514175415
Epoch 1330, training loss: 843.6677856445312 = 0.37436026334762573 + 100.0 * 8.432933807373047
Epoch 1330, val loss: 0.4283917248249054
Epoch 1340, training loss: 843.7425537109375 = 0.373705118894577 + 100.0 * 8.433688163757324
Epoch 1340, val loss: 0.42814722657203674
Epoch 1350, training loss: 843.6063232421875 = 0.3730447292327881 + 100.0 * 8.432332992553711
Epoch 1350, val loss: 0.42797455191612244
Epoch 1360, training loss: 843.6641235351562 = 0.3723917305469513 + 100.0 * 8.432917594909668
Epoch 1360, val loss: 0.4277139902114868
Epoch 1370, training loss: 843.5241088867188 = 0.3717445433139801 + 100.0 * 8.431523323059082
Epoch 1370, val loss: 0.427560418844223
Epoch 1380, training loss: 843.4210205078125 = 0.3711090385913849 + 100.0 * 8.430499076843262
Epoch 1380, val loss: 0.42734742164611816
Epoch 1390, training loss: 843.4274291992188 = 0.37048038840293884 + 100.0 * 8.430569648742676
Epoch 1390, val loss: 0.42715030908584595
Epoch 1400, training loss: 843.4647827148438 = 0.36985480785369873 + 100.0 * 8.430949211120605
Epoch 1400, val loss: 0.4269644618034363
Epoch 1410, training loss: 843.311279296875 = 0.3692351281642914 + 100.0 * 8.429420471191406
Epoch 1410, val loss: 0.4267827272415161
Epoch 1420, training loss: 843.2415771484375 = 0.3686232566833496 + 100.0 * 8.428729057312012
Epoch 1420, val loss: 0.4265771806240082
Epoch 1430, training loss: 843.2384033203125 = 0.36801937222480774 + 100.0 * 8.428703308105469
Epoch 1430, val loss: 0.4263738691806793
Epoch 1440, training loss: 843.3258056640625 = 0.36741897463798523 + 100.0 * 8.429583549499512
Epoch 1440, val loss: 0.4261561334133148
Epoch 1450, training loss: 843.21826171875 = 0.36681559681892395 + 100.0 * 8.42851448059082
Epoch 1450, val loss: 0.42601466178894043
Epoch 1460, training loss: 843.3102416992188 = 0.36621949076652527 + 100.0 * 8.42944049835205
Epoch 1460, val loss: 0.42581743001937866
Epoch 1470, training loss: 842.99462890625 = 0.3656327426433563 + 100.0 * 8.426289558410645
Epoch 1470, val loss: 0.4256674647331238
Epoch 1480, training loss: 842.9369506835938 = 0.3650554418563843 + 100.0 * 8.425719261169434
Epoch 1480, val loss: 0.4254456162452698
Epoch 1490, training loss: 842.905029296875 = 0.36448389291763306 + 100.0 * 8.425405502319336
Epoch 1490, val loss: 0.4253102242946625
Epoch 1500, training loss: 843.2554931640625 = 0.36391207575798035 + 100.0 * 8.428915977478027
Epoch 1500, val loss: 0.4251098930835724
Epoch 1510, training loss: 842.9745483398438 = 0.3633344769477844 + 100.0 * 8.426112174987793
Epoch 1510, val loss: 0.4249732792377472
Epoch 1520, training loss: 842.7666015625 = 0.36276987195014954 + 100.0 * 8.42403793334961
Epoch 1520, val loss: 0.4247601330280304
Epoch 1530, training loss: 842.72021484375 = 0.36221176385879517 + 100.0 * 8.423580169677734
Epoch 1530, val loss: 0.4245973527431488
Epoch 1540, training loss: 842.7736206054688 = 0.36165866255760193 + 100.0 * 8.42411994934082
Epoch 1540, val loss: 0.4244304895401001
Epoch 1550, training loss: 842.837646484375 = 0.3611036539077759 + 100.0 * 8.424765586853027
Epoch 1550, val loss: 0.4242348074913025
Epoch 1560, training loss: 842.6217651367188 = 0.3605496883392334 + 100.0 * 8.422612190246582
Epoch 1560, val loss: 0.4241151213645935
Epoch 1570, training loss: 842.5589599609375 = 0.36000579595565796 + 100.0 * 8.421989440917969
Epoch 1570, val loss: 0.4239374101161957
Epoch 1580, training loss: 842.4671630859375 = 0.35947147011756897 + 100.0 * 8.421076774597168
Epoch 1580, val loss: 0.4237915575504303
Epoch 1590, training loss: 842.4772338867188 = 0.3589361906051636 + 100.0 * 8.421182632446289
Epoch 1590, val loss: 0.4236428737640381
Epoch 1600, training loss: 842.7683715820312 = 0.3583937883377075 + 100.0 * 8.424099922180176
Epoch 1600, val loss: 0.4234839975833893
Epoch 1610, training loss: 842.651123046875 = 0.3578406870365143 + 100.0 * 8.422932624816895
Epoch 1610, val loss: 0.4233098030090332
Epoch 1620, training loss: 842.3740234375 = 0.3573038876056671 + 100.0 * 8.420166969299316
Epoch 1620, val loss: 0.42313796281814575
Epoch 1630, training loss: 842.2219848632812 = 0.3567736744880676 + 100.0 * 8.418652534484863
Epoch 1630, val loss: 0.42297816276550293
Epoch 1640, training loss: 842.178955078125 = 0.35624924302101135 + 100.0 * 8.418227195739746
Epoch 1640, val loss: 0.4228494465351105
Epoch 1650, training loss: 842.203125 = 0.35572218894958496 + 100.0 * 8.418474197387695
Epoch 1650, val loss: 0.42269858717918396
Epoch 1660, training loss: 842.480224609375 = 0.35518717765808105 + 100.0 * 8.421250343322754
Epoch 1660, val loss: 0.42255449295043945
Epoch 1670, training loss: 842.197998046875 = 0.35465502738952637 + 100.0 * 8.41843318939209
Epoch 1670, val loss: 0.42236942052841187
Epoch 1680, training loss: 842.2100219726562 = 0.35412514209747314 + 100.0 * 8.418559074401855
Epoch 1680, val loss: 0.4222501516342163
Epoch 1690, training loss: 841.982421875 = 0.35360148549079895 + 100.0 * 8.416288375854492
Epoch 1690, val loss: 0.4220999479293823
Epoch 1700, training loss: 841.9137573242188 = 0.35307833552360535 + 100.0 * 8.415606498718262
Epoch 1700, val loss: 0.4219661056995392
Epoch 1710, training loss: 841.8447875976562 = 0.3525598347187042 + 100.0 * 8.414922714233398
Epoch 1710, val loss: 0.42183995246887207
Epoch 1720, training loss: 841.8157958984375 = 0.3520435690879822 + 100.0 * 8.414637565612793
Epoch 1720, val loss: 0.4216984510421753
Epoch 1730, training loss: 842.0764770507812 = 0.3515233099460602 + 100.0 * 8.41724967956543
Epoch 1730, val loss: 0.4215782880783081
Epoch 1740, training loss: 842.138427734375 = 0.3509853780269623 + 100.0 * 8.417874336242676
Epoch 1740, val loss: 0.4214099049568176
Epoch 1750, training loss: 841.7239990234375 = 0.350453644990921 + 100.0 * 8.413735389709473
Epoch 1750, val loss: 0.42125558853149414
Epoch 1760, training loss: 841.600830078125 = 0.34992897510528564 + 100.0 * 8.412508964538574
Epoch 1760, val loss: 0.4211520552635193
Epoch 1770, training loss: 841.571044921875 = 0.34940817952156067 + 100.0 * 8.412216186523438
Epoch 1770, val loss: 0.4210016429424286
Epoch 1780, training loss: 841.5642700195312 = 0.34888720512390137 + 100.0 * 8.412154197692871
Epoch 1780, val loss: 0.42091265320777893
Epoch 1790, training loss: 841.99365234375 = 0.34835997223854065 + 100.0 * 8.416452407836914
Epoch 1790, val loss: 0.42079299688339233
Epoch 1800, training loss: 841.5853881835938 = 0.34782832860946655 + 100.0 * 8.412375450134277
Epoch 1800, val loss: 0.42060279846191406
Epoch 1810, training loss: 841.4473266601562 = 0.3472985327243805 + 100.0 * 8.41100025177002
Epoch 1810, val loss: 0.42048129439353943
Epoch 1820, training loss: 841.5949096679688 = 0.34678035974502563 + 100.0 * 8.412481307983398
Epoch 1820, val loss: 0.4203379452228546
Epoch 1830, training loss: 841.3358154296875 = 0.34625038504600525 + 100.0 * 8.409895896911621
Epoch 1830, val loss: 0.42025965452194214
Epoch 1840, training loss: 841.28271484375 = 0.3457290232181549 + 100.0 * 8.409370422363281
Epoch 1840, val loss: 0.4200928807258606
Epoch 1850, training loss: 841.2271118164062 = 0.3452088236808777 + 100.0 * 8.408819198608398
Epoch 1850, val loss: 0.42000332474708557
Epoch 1860, training loss: 841.2023315429688 = 0.3446868360042572 + 100.0 * 8.408576965332031
Epoch 1860, val loss: 0.4198946952819824
Epoch 1870, training loss: 842.0179443359375 = 0.3441600203514099 + 100.0 * 8.41673755645752
Epoch 1870, val loss: 0.41979965567588806
Epoch 1880, training loss: 841.21240234375 = 0.3436180055141449 + 100.0 * 8.408687591552734
Epoch 1880, val loss: 0.41965964436531067
Epoch 1890, training loss: 841.1367797851562 = 0.3430849313735962 + 100.0 * 8.407937049865723
Epoch 1890, val loss: 0.4194928705692291
Epoch 1900, training loss: 841.0479736328125 = 0.34256234765052795 + 100.0 * 8.40705394744873
Epoch 1900, val loss: 0.41943466663360596
Epoch 1910, training loss: 840.9965209960938 = 0.34204262495040894 + 100.0 * 8.40654468536377
Epoch 1910, val loss: 0.41931235790252686
Epoch 1920, training loss: 841.0211181640625 = 0.3415220379829407 + 100.0 * 8.4067964553833
Epoch 1920, val loss: 0.4192103147506714
Epoch 1930, training loss: 841.32763671875 = 0.34099486470222473 + 100.0 * 8.409866333007812
Epoch 1930, val loss: 0.4191080629825592
Epoch 1940, training loss: 841.1505737304688 = 0.34045785665512085 + 100.0 * 8.408101081848145
Epoch 1940, val loss: 0.418989896774292
Epoch 1950, training loss: 840.9083251953125 = 0.3399297297000885 + 100.0 * 8.405684471130371
Epoch 1950, val loss: 0.4188646674156189
Epoch 1960, training loss: 840.855712890625 = 0.3394046723842621 + 100.0 * 8.405162811279297
Epoch 1960, val loss: 0.41874244809150696
Epoch 1970, training loss: 840.8565063476562 = 0.3388804793357849 + 100.0 * 8.405176162719727
Epoch 1970, val loss: 0.4186221659183502
Epoch 1980, training loss: 841.0564575195312 = 0.33834969997406006 + 100.0 * 8.407180786132812
Epoch 1980, val loss: 0.41848987340927124
Epoch 1990, training loss: 840.8248291015625 = 0.3378053307533264 + 100.0 * 8.40487003326416
Epoch 1990, val loss: 0.41846799850463867
Epoch 2000, training loss: 840.6940307617188 = 0.3372691869735718 + 100.0 * 8.40356731414795
Epoch 2000, val loss: 0.4182928800582886
Epoch 2010, training loss: 840.707275390625 = 0.3367369472980499 + 100.0 * 8.403705596923828
Epoch 2010, val loss: 0.4181792736053467
Epoch 2020, training loss: 840.8905029296875 = 0.3362003266811371 + 100.0 * 8.405543327331543
Epoch 2020, val loss: 0.41808968782424927
Epoch 2030, training loss: 840.770263671875 = 0.33566319942474365 + 100.0 * 8.404345512390137
Epoch 2030, val loss: 0.4179264008998871
Epoch 2040, training loss: 840.8323364257812 = 0.33511608839035034 + 100.0 * 8.404972076416016
Epoch 2040, val loss: 0.41782301664352417
Epoch 2050, training loss: 840.6307373046875 = 0.3345746695995331 + 100.0 * 8.402961730957031
Epoch 2050, val loss: 0.41769322752952576
Epoch 2060, training loss: 840.5533447265625 = 0.3340349793434143 + 100.0 * 8.402193069458008
Epoch 2060, val loss: 0.41759902238845825
Epoch 2070, training loss: 840.4944458007812 = 0.33349767327308655 + 100.0 * 8.401609420776367
Epoch 2070, val loss: 0.4174429178237915
Epoch 2080, training loss: 840.4608154296875 = 0.3329566717147827 + 100.0 * 8.401278495788574
Epoch 2080, val loss: 0.41734448075294495
Epoch 2090, training loss: 840.7785034179688 = 0.33241286873817444 + 100.0 * 8.404460906982422
Epoch 2090, val loss: 0.4172466993331909
Epoch 2100, training loss: 840.730712890625 = 0.33184802532196045 + 100.0 * 8.4039888381958
Epoch 2100, val loss: 0.41709256172180176
Epoch 2110, training loss: 840.5487060546875 = 0.33128824830055237 + 100.0 * 8.40217399597168
Epoch 2110, val loss: 0.4169173538684845
Epoch 2120, training loss: 840.3480834960938 = 0.33073633909225464 + 100.0 * 8.40017318725586
Epoch 2120, val loss: 0.4168357849121094
Epoch 2130, training loss: 840.3392944335938 = 0.3301887810230255 + 100.0 * 8.400091171264648
Epoch 2130, val loss: 0.41670235991477966
Epoch 2140, training loss: 840.3001708984375 = 0.3296424150466919 + 100.0 * 8.399704933166504
Epoch 2140, val loss: 0.41659271717071533
Epoch 2150, training loss: 840.3617553710938 = 0.32909175753593445 + 100.0 * 8.4003267288208
Epoch 2150, val loss: 0.41645747423171997
Epoch 2160, training loss: 840.4373168945312 = 0.32853102684020996 + 100.0 * 8.401087760925293
Epoch 2160, val loss: 0.4163016974925995
Epoch 2170, training loss: 840.4290771484375 = 0.32796239852905273 + 100.0 * 8.40101146697998
Epoch 2170, val loss: 0.4161958694458008
Epoch 2180, training loss: 840.2813720703125 = 0.32739436626434326 + 100.0 * 8.399539947509766
Epoch 2180, val loss: 0.4161064326763153
Epoch 2190, training loss: 840.1911010742188 = 0.3268340528011322 + 100.0 * 8.398642539978027
Epoch 2190, val loss: 0.4159846007823944
Epoch 2200, training loss: 840.1773071289062 = 0.32627496123313904 + 100.0 * 8.398509979248047
Epoch 2200, val loss: 0.41590672731399536
Epoch 2210, training loss: 840.4054565429688 = 0.32571229338645935 + 100.0 * 8.400797843933105
Epoch 2210, val loss: 0.41582438349723816
Epoch 2220, training loss: 840.1124267578125 = 0.3251337707042694 + 100.0 * 8.397872924804688
Epoch 2220, val loss: 0.415700763463974
Epoch 2230, training loss: 840.1383056640625 = 0.32456034421920776 + 100.0 * 8.398137092590332
Epoch 2230, val loss: 0.415549099445343
Epoch 2240, training loss: 840.0580444335938 = 0.3239849805831909 + 100.0 * 8.397340774536133
Epoch 2240, val loss: 0.41547662019729614
Epoch 2250, training loss: 840.0724487304688 = 0.32340630888938904 + 100.0 * 8.397490501403809
Epoch 2250, val loss: 0.41536232829093933
Epoch 2260, training loss: 840.3469848632812 = 0.3228193521499634 + 100.0 * 8.40024185180664
Epoch 2260, val loss: 0.4152054190635681
Epoch 2270, training loss: 840.13134765625 = 0.322221577167511 + 100.0 * 8.398091316223145
Epoch 2270, val loss: 0.4151000380516052
Epoch 2280, training loss: 839.9845581054688 = 0.32162100076675415 + 100.0 * 8.396629333496094
Epoch 2280, val loss: 0.41500124335289
Epoch 2290, training loss: 839.9254150390625 = 0.32102158665657043 + 100.0 * 8.39604377746582
Epoch 2290, val loss: 0.4149150848388672
Epoch 2300, training loss: 840.016845703125 = 0.3204217553138733 + 100.0 * 8.396964073181152
Epoch 2300, val loss: 0.41483673453330994
Epoch 2310, training loss: 840.0635986328125 = 0.3198067843914032 + 100.0 * 8.397438049316406
Epoch 2310, val loss: 0.4146665632724762
Epoch 2320, training loss: 839.8872680664062 = 0.3191843330860138 + 100.0 * 8.39568042755127
Epoch 2320, val loss: 0.4145766794681549
Epoch 2330, training loss: 839.8663940429688 = 0.3185689151287079 + 100.0 * 8.395478248596191
Epoch 2330, val loss: 0.41447436809539795
Epoch 2340, training loss: 839.8325805664062 = 0.31795409321784973 + 100.0 * 8.395146369934082
Epoch 2340, val loss: 0.4143640697002411
Epoch 2350, training loss: 839.9717407226562 = 0.3173408806324005 + 100.0 * 8.396544456481934
Epoch 2350, val loss: 0.41429534554481506
Epoch 2360, training loss: 839.887451171875 = 0.31671327352523804 + 100.0 * 8.395707130432129
Epoch 2360, val loss: 0.41414573788642883
Epoch 2370, training loss: 839.844970703125 = 0.3160841464996338 + 100.0 * 8.395288467407227
Epoch 2370, val loss: 0.4140014350414276
Epoch 2380, training loss: 839.7505493164062 = 0.3154529333114624 + 100.0 * 8.3943510055542
Epoch 2380, val loss: 0.4139289855957031
Epoch 2390, training loss: 839.8681640625 = 0.31481805443763733 + 100.0 * 8.395533561706543
Epoch 2390, val loss: 0.41381126642227173
Epoch 2400, training loss: 839.6937255859375 = 0.3141726553440094 + 100.0 * 8.39379596710205
Epoch 2400, val loss: 0.41367313265800476
Epoch 2410, training loss: 839.6923828125 = 0.31352922320365906 + 100.0 * 8.39378833770752
Epoch 2410, val loss: 0.41353514790534973
Epoch 2420, training loss: 839.6546020507812 = 0.3128839433193207 + 100.0 * 8.393417358398438
Epoch 2420, val loss: 0.4134426712989807
Epoch 2430, training loss: 839.7723388671875 = 0.3122358024120331 + 100.0 * 8.394600868225098
Epoch 2430, val loss: 0.41332632303237915
Epoch 2440, training loss: 839.6547241210938 = 0.3115752339363098 + 100.0 * 8.393431663513184
Epoch 2440, val loss: 0.41323572397232056
Epoch 2450, training loss: 839.7811279296875 = 0.31091615557670593 + 100.0 * 8.394701957702637
Epoch 2450, val loss: 0.4130591154098511
Epoch 2460, training loss: 839.5453491210938 = 0.31024789810180664 + 100.0 * 8.392351150512695
Epoch 2460, val loss: 0.41291898488998413
Epoch 2470, training loss: 839.5538330078125 = 0.3095855712890625 + 100.0 * 8.39244270324707
Epoch 2470, val loss: 0.4127122461795807
Epoch 2480, training loss: 839.520263671875 = 0.308919221162796 + 100.0 * 8.39211368560791
Epoch 2480, val loss: 0.4126040041446686
Epoch 2490, training loss: 839.5185546875 = 0.30824971199035645 + 100.0 * 8.39210319519043
Epoch 2490, val loss: 0.4124397933483124
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8315575849822425
0.8648119973918714
=== training gcn model ===
Epoch 0, training loss: 1059.3245849609375 = 1.0932807922363281 + 100.0 * 10.582313537597656
Epoch 0, val loss: 1.093902826309204
Epoch 10, training loss: 1059.298828125 = 1.089004635810852 + 100.0 * 10.582098007202148
Epoch 10, val loss: 1.0896178483963013
Epoch 20, training loss: 1059.1986083984375 = 1.084384799003601 + 100.0 * 10.58114242553711
Epoch 20, val loss: 1.0849659442901611
Epoch 30, training loss: 1058.7547607421875 = 1.0792980194091797 + 100.0 * 10.576754570007324
Epoch 30, val loss: 1.0798077583312988
Epoch 40, training loss: 1056.9637451171875 = 1.0735763311386108 + 100.0 * 10.558900833129883
Epoch 40, val loss: 1.073975682258606
Epoch 50, training loss: 1051.6217041015625 = 1.067226529121399 + 100.0 * 10.505544662475586
Epoch 50, val loss: 1.0674887895584106
Epoch 60, training loss: 1040.3756103515625 = 1.0611321926116943 + 100.0 * 10.393144607543945
Epoch 60, val loss: 1.061336874961853
Epoch 70, training loss: 1023.8765869140625 = 1.056232213973999 + 100.0 * 10.228203773498535
Epoch 70, val loss: 1.056253433227539
Epoch 80, training loss: 1014.0286254882812 = 1.0518978834152222 + 100.0 * 10.129767417907715
Epoch 80, val loss: 1.0517687797546387
Epoch 90, training loss: 993.8116455078125 = 1.0471575260162354 + 100.0 * 9.927644729614258
Epoch 90, val loss: 1.047014594078064
Epoch 100, training loss: 956.307861328125 = 1.0420268774032593 + 100.0 * 9.552658081054688
Epoch 100, val loss: 1.0420111417770386
Epoch 110, training loss: 938.55859375 = 1.0362225770950317 + 100.0 * 9.375224113464355
Epoch 110, val loss: 1.0363720655441284
Epoch 120, training loss: 928.2571411132812 = 1.0299019813537598 + 100.0 * 9.272272109985352
Epoch 120, val loss: 1.0302315950393677
Epoch 130, training loss: 924.0634765625 = 1.0237314701080322 + 100.0 * 9.23039722442627
Epoch 130, val loss: 1.0242061614990234
Epoch 140, training loss: 920.2392578125 = 1.017775058746338 + 100.0 * 9.192214965820312
Epoch 140, val loss: 1.0183115005493164
Epoch 150, training loss: 914.8884887695312 = 1.011864185333252 + 100.0 * 9.138766288757324
Epoch 150, val loss: 1.0124831199645996
Epoch 160, training loss: 907.9776000976562 = 1.0064150094985962 + 100.0 * 9.069711685180664
Epoch 160, val loss: 1.0072059631347656
Epoch 170, training loss: 901.4011840820312 = 1.001230001449585 + 100.0 * 9.003999710083008
Epoch 170, val loss: 1.0021809339523315
Epoch 180, training loss: 897.386474609375 = 0.9954813122749329 + 100.0 * 8.963910102844238
Epoch 180, val loss: 0.9965155720710754
Epoch 190, training loss: 893.865478515625 = 0.9888345003128052 + 100.0 * 8.928766250610352
Epoch 190, val loss: 0.9899871349334717
Epoch 200, training loss: 890.0951538085938 = 0.9822903275489807 + 100.0 * 8.891128540039062
Epoch 200, val loss: 0.9836821556091309
Epoch 210, training loss: 887.0601196289062 = 0.9760130643844604 + 100.0 * 8.860840797424316
Epoch 210, val loss: 0.9775746464729309
Epoch 220, training loss: 884.6732177734375 = 0.9687448740005493 + 100.0 * 8.837044715881348
Epoch 220, val loss: 0.9703971147537231
Epoch 230, training loss: 881.9244384765625 = 0.9606413841247559 + 100.0 * 8.809638023376465
Epoch 230, val loss: 0.9625335931777954
Epoch 240, training loss: 879.1364135742188 = 0.9527414441108704 + 100.0 * 8.78183650970459
Epoch 240, val loss: 0.9550065398216248
Epoch 250, training loss: 877.1903076171875 = 0.9444327354431152 + 100.0 * 8.762458801269531
Epoch 250, val loss: 0.9469907879829407
Epoch 260, training loss: 875.78857421875 = 0.9349068999290466 + 100.0 * 8.748536109924316
Epoch 260, val loss: 0.9376813769340515
Epoch 270, training loss: 874.1485595703125 = 0.9248024821281433 + 100.0 * 8.732237815856934
Epoch 270, val loss: 0.928054690361023
Epoch 280, training loss: 872.5294799804688 = 0.9147596955299377 + 100.0 * 8.716147422790527
Epoch 280, val loss: 0.9185265302658081
Epoch 290, training loss: 871.102294921875 = 0.9046015739440918 + 100.0 * 8.701976776123047
Epoch 290, val loss: 0.9087475538253784
Epoch 300, training loss: 869.9727172851562 = 0.8937921524047852 + 100.0 * 8.690789222717285
Epoch 300, val loss: 0.8983436822891235
Epoch 310, training loss: 868.929443359375 = 0.8823068141937256 + 100.0 * 8.680471420288086
Epoch 310, val loss: 0.8872279524803162
Epoch 320, training loss: 867.9285888671875 = 0.8702640533447266 + 100.0 * 8.670583724975586
Epoch 320, val loss: 0.8756734728813171
Epoch 330, training loss: 866.8740844726562 = 0.8579262495040894 + 100.0 * 8.660161972045898
Epoch 330, val loss: 0.8638696670532227
Epoch 340, training loss: 865.991943359375 = 0.8454272747039795 + 100.0 * 8.65146541595459
Epoch 340, val loss: 0.8518098592758179
Epoch 350, training loss: 865.0553588867188 = 0.832448422908783 + 100.0 * 8.642229080200195
Epoch 350, val loss: 0.8394802212715149
Epoch 360, training loss: 864.3804321289062 = 0.8191805481910706 + 100.0 * 8.635612487792969
Epoch 360, val loss: 0.8268160820007324
Epoch 370, training loss: 863.7811279296875 = 0.805553674697876 + 100.0 * 8.629755973815918
Epoch 370, val loss: 0.8138474225997925
Epoch 380, training loss: 863.22509765625 = 0.791724443435669 + 100.0 * 8.624333381652832
Epoch 380, val loss: 0.8007250428199768
Epoch 390, training loss: 862.6748657226562 = 0.7778724431991577 + 100.0 * 8.618969917297363
Epoch 390, val loss: 0.7875834703445435
Epoch 400, training loss: 862.2192993164062 = 0.7639681696891785 + 100.0 * 8.614553451538086
Epoch 400, val loss: 0.7743996381759644
Epoch 410, training loss: 861.5327758789062 = 0.7501587271690369 + 100.0 * 8.607826232910156
Epoch 410, val loss: 0.7613804936408997
Epoch 420, training loss: 860.7974853515625 = 0.7365533113479614 + 100.0 * 8.600608825683594
Epoch 420, val loss: 0.7485409379005432
Epoch 430, training loss: 860.212890625 = 0.7231465578079224 + 100.0 * 8.594897270202637
Epoch 430, val loss: 0.7359200716018677
Epoch 440, training loss: 859.3441162109375 = 0.7099539637565613 + 100.0 * 8.586341857910156
Epoch 440, val loss: 0.7236138582229614
Epoch 450, training loss: 858.6641235351562 = 0.697014570236206 + 100.0 * 8.579670906066895
Epoch 450, val loss: 0.7116069793701172
Epoch 460, training loss: 857.8402099609375 = 0.6843593120574951 + 100.0 * 8.571557998657227
Epoch 460, val loss: 0.6996375322341919
Epoch 470, training loss: 857.1143188476562 = 0.6718870997428894 + 100.0 * 8.564424514770508
Epoch 470, val loss: 0.6881368160247803
Epoch 480, training loss: 856.4849853515625 = 0.6597071886062622 + 100.0 * 8.558252334594727
Epoch 480, val loss: 0.676787793636322
Epoch 490, training loss: 856.0008544921875 = 0.6476859450340271 + 100.0 * 8.553531646728516
Epoch 490, val loss: 0.6658039093017578
Epoch 500, training loss: 855.4168090820312 = 0.6360637545585632 + 100.0 * 8.547807693481445
Epoch 500, val loss: 0.654909074306488
Epoch 510, training loss: 854.8822631835938 = 0.6247720718383789 + 100.0 * 8.542574882507324
Epoch 510, val loss: 0.6445972323417664
Epoch 520, training loss: 854.3859252929688 = 0.6139631271362305 + 100.0 * 8.5377197265625
Epoch 520, val loss: 0.6346842050552368
Epoch 530, training loss: 854.1094970703125 = 0.6035744547843933 + 100.0 * 8.535058975219727
Epoch 530, val loss: 0.6252430081367493
Epoch 540, training loss: 853.5379638671875 = 0.5935847759246826 + 100.0 * 8.529443740844727
Epoch 540, val loss: 0.616182804107666
Epoch 550, training loss: 853.0961303710938 = 0.58400559425354 + 100.0 * 8.525121688842773
Epoch 550, val loss: 0.6074456572532654
Epoch 560, training loss: 852.6893920898438 = 0.5748758912086487 + 100.0 * 8.52114486694336
Epoch 560, val loss: 0.5992133617401123
Epoch 570, training loss: 852.273681640625 = 0.5662282109260559 + 100.0 * 8.517074584960938
Epoch 570, val loss: 0.5914503931999207
Epoch 580, training loss: 851.9225463867188 = 0.5580171346664429 + 100.0 * 8.51364517211914
Epoch 580, val loss: 0.5840687155723572
Epoch 590, training loss: 851.6517944335938 = 0.5501169562339783 + 100.0 * 8.511016845703125
Epoch 590, val loss: 0.5771503448486328
Epoch 600, training loss: 851.2877807617188 = 0.5426207780838013 + 100.0 * 8.507452011108398
Epoch 600, val loss: 0.5705767273902893
Epoch 610, training loss: 850.9672241210938 = 0.5355880260467529 + 100.0 * 8.504316329956055
Epoch 610, val loss: 0.564402163028717
Epoch 620, training loss: 850.8718872070312 = 0.5289118885993958 + 100.0 * 8.503429412841797
Epoch 620, val loss: 0.5586186647415161
Epoch 630, training loss: 850.4658203125 = 0.522567629814148 + 100.0 * 8.499432563781738
Epoch 630, val loss: 0.5531186461448669
Epoch 640, training loss: 850.1223754882812 = 0.516593337059021 + 100.0 * 8.496057510375977
Epoch 640, val loss: 0.5480760335922241
Epoch 650, training loss: 849.8335571289062 = 0.5109142661094666 + 100.0 * 8.493226051330566
Epoch 650, val loss: 0.5433359146118164
Epoch 660, training loss: 849.6477661132812 = 0.5055122971534729 + 100.0 * 8.491422653198242
Epoch 660, val loss: 0.5387818813323975
Epoch 670, training loss: 849.400146484375 = 0.5004161596298218 + 100.0 * 8.488997459411621
Epoch 670, val loss: 0.5346527099609375
Epoch 680, training loss: 849.148681640625 = 0.4956381320953369 + 100.0 * 8.486530303955078
Epoch 680, val loss: 0.5307577252388
Epoch 690, training loss: 848.9093017578125 = 0.49108487367630005 + 100.0 * 8.484182357788086
Epoch 690, val loss: 0.5270458459854126
Epoch 700, training loss: 848.6892700195312 = 0.4867633879184723 + 100.0 * 8.482025146484375
Epoch 700, val loss: 0.5235899090766907
Epoch 710, training loss: 848.4800415039062 = 0.48265954852104187 + 100.0 * 8.479973793029785
Epoch 710, val loss: 0.5203554034233093
Epoch 720, training loss: 848.2748413085938 = 0.47875848412513733 + 100.0 * 8.477960586547852
Epoch 720, val loss: 0.5172935724258423
Epoch 730, training loss: 848.94287109375 = 0.4750382602214813 + 100.0 * 8.484678268432617
Epoch 730, val loss: 0.514290988445282
Epoch 740, training loss: 848.0198974609375 = 0.47141605615615845 + 100.0 * 8.475484848022461
Epoch 740, val loss: 0.511645495891571
Epoch 750, training loss: 847.7056884765625 = 0.46808162331581116 + 100.0 * 8.472375869750977
Epoch 750, val loss: 0.5091574192047119
Epoch 760, training loss: 847.5198974609375 = 0.46487072110176086 + 100.0 * 8.470550537109375
Epoch 760, val loss: 0.5066912770271301
Epoch 770, training loss: 847.2930908203125 = 0.46183937788009644 + 100.0 * 8.46831226348877
Epoch 770, val loss: 0.5044623017311096
Epoch 780, training loss: 847.1057739257812 = 0.4589143693447113 + 100.0 * 8.466468811035156
Epoch 780, val loss: 0.5022987723350525
Epoch 790, training loss: 847.2412719726562 = 0.45608726143836975 + 100.0 * 8.467851638793945
Epoch 790, val loss: 0.5002038478851318
Epoch 800, training loss: 846.85546875 = 0.4533786475658417 + 100.0 * 8.464020729064941
Epoch 800, val loss: 0.49835047125816345
Epoch 810, training loss: 846.5717163085938 = 0.45083582401275635 + 100.0 * 8.461209297180176
Epoch 810, val loss: 0.496573805809021
Epoch 820, training loss: 846.3543090820312 = 0.44838500022888184 + 100.0 * 8.459059715270996
Epoch 820, val loss: 0.4948568642139435
Epoch 830, training loss: 846.1949462890625 = 0.44604265689849854 + 100.0 * 8.457489013671875
Epoch 830, val loss: 0.4933187663555145
Epoch 840, training loss: 846.1299438476562 = 0.4437737762928009 + 100.0 * 8.45686149597168
Epoch 840, val loss: 0.49176105856895447
Epoch 850, training loss: 845.9026489257812 = 0.4416225850582123 + 100.0 * 8.454609870910645
Epoch 850, val loss: 0.4903844892978668
Epoch 860, training loss: 845.7222290039062 = 0.43957269191741943 + 100.0 * 8.452826499938965
Epoch 860, val loss: 0.48898983001708984
Epoch 870, training loss: 845.5722045898438 = 0.4375731348991394 + 100.0 * 8.451346397399902
Epoch 870, val loss: 0.4876996874809265
Epoch 880, training loss: 845.695556640625 = 0.4356386065483093 + 100.0 * 8.452598571777344
Epoch 880, val loss: 0.48643818497657776
Epoch 890, training loss: 845.3331298828125 = 0.4337686002254486 + 100.0 * 8.448993682861328
Epoch 890, val loss: 0.48533883690834045
Epoch 900, training loss: 845.177734375 = 0.431988000869751 + 100.0 * 8.447457313537598
Epoch 900, val loss: 0.4842129647731781
Epoch 910, training loss: 845.0291137695312 = 0.43025171756744385 + 100.0 * 8.445988655090332
Epoch 910, val loss: 0.48314592242240906
Epoch 920, training loss: 844.9075927734375 = 0.42857828736305237 + 100.0 * 8.44478988647461
Epoch 920, val loss: 0.4821561276912689
Epoch 930, training loss: 844.9957885742188 = 0.42694222927093506 + 100.0 * 8.445688247680664
Epoch 930, val loss: 0.48125961422920227
Epoch 940, training loss: 844.83740234375 = 0.4253719747066498 + 100.0 * 8.444120407104492
Epoch 940, val loss: 0.4801980257034302
Epoch 950, training loss: 844.5850830078125 = 0.42383328080177307 + 100.0 * 8.441612243652344
Epoch 950, val loss: 0.4793877899646759
Epoch 960, training loss: 844.490234375 = 0.4223470091819763 + 100.0 * 8.440678596496582
Epoch 960, val loss: 0.47852933406829834
Epoch 970, training loss: 844.3709106445312 = 0.4209067225456238 + 100.0 * 8.439499855041504
Epoch 970, val loss: 0.4777195155620575
Epoch 980, training loss: 844.7328491210938 = 0.4195144772529602 + 100.0 * 8.443133354187012
Epoch 980, val loss: 0.47696352005004883
Epoch 990, training loss: 844.3271484375 = 0.41812339425086975 + 100.0 * 8.43908977508545
Epoch 990, val loss: 0.4761849343776703
Epoch 1000, training loss: 844.1732177734375 = 0.41681018471717834 + 100.0 * 8.4375638961792
Epoch 1000, val loss: 0.47548121213912964
Epoch 1010, training loss: 844.0178833007812 = 0.41552498936653137 + 100.0 * 8.436023712158203
Epoch 1010, val loss: 0.4747552275657654
Epoch 1020, training loss: 843.9202880859375 = 0.4142756462097168 + 100.0 * 8.435060501098633
Epoch 1020, val loss: 0.47414401173591614
Epoch 1030, training loss: 843.8999633789062 = 0.4130590856075287 + 100.0 * 8.434868812561035
Epoch 1030, val loss: 0.4735562205314636
Epoch 1040, training loss: 843.7808837890625 = 0.41183653473854065 + 100.0 * 8.433690071105957
Epoch 1040, val loss: 0.47280579805374146
Epoch 1050, training loss: 843.7999877929688 = 0.41066601872444153 + 100.0 * 8.433893203735352
Epoch 1050, val loss: 0.4723604917526245
Epoch 1060, training loss: 843.6373291015625 = 0.4095320403575897 + 100.0 * 8.43227767944336
Epoch 1060, val loss: 0.47170713543891907
Epoch 1070, training loss: 843.530517578125 = 0.4084169864654541 + 100.0 * 8.431221008300781
Epoch 1070, val loss: 0.4711611568927765
Epoch 1080, training loss: 843.4507446289062 = 0.4073290526866913 + 100.0 * 8.430434226989746
Epoch 1080, val loss: 0.4706208407878876
Epoch 1090, training loss: 843.6015014648438 = 0.40626224875450134 + 100.0 * 8.431952476501465
Epoch 1090, val loss: 0.4701024889945984
Epoch 1100, training loss: 843.4668579101562 = 0.4051873981952667 + 100.0 * 8.43061637878418
Epoch 1100, val loss: 0.46956464648246765
Epoch 1110, training loss: 843.2664184570312 = 0.4041602313518524 + 100.0 * 8.428622245788574
Epoch 1110, val loss: 0.46905747056007385
Epoch 1120, training loss: 843.1597900390625 = 0.403148889541626 + 100.0 * 8.427566528320312
Epoch 1120, val loss: 0.46853259205818176
Epoch 1130, training loss: 843.0708618164062 = 0.40216153860092163 + 100.0 * 8.426687240600586
Epoch 1130, val loss: 0.46808987855911255
Epoch 1140, training loss: 842.9988403320312 = 0.40119534730911255 + 100.0 * 8.425976753234863
Epoch 1140, val loss: 0.46762731671333313
Epoch 1150, training loss: 843.234375 = 0.40023869276046753 + 100.0 * 8.428340911865234
Epoch 1150, val loss: 0.46716129779815674
Epoch 1160, training loss: 843.048583984375 = 0.39928799867630005 + 100.0 * 8.426492691040039
Epoch 1160, val loss: 0.46670955419540405
Epoch 1170, training loss: 842.81103515625 = 0.3983657658100128 + 100.0 * 8.424126625061035
Epoch 1170, val loss: 0.4662867486476898
Epoch 1180, training loss: 842.7084350585938 = 0.3974562883377075 + 100.0 * 8.423110008239746
Epoch 1180, val loss: 0.4658067524433136
Epoch 1190, training loss: 843.0156860351562 = 0.39656636118888855 + 100.0 * 8.426191329956055
Epoch 1190, val loss: 0.4655003845691681
Epoch 1200, training loss: 842.7445678710938 = 0.3956831395626068 + 100.0 * 8.42348861694336
Epoch 1200, val loss: 0.4649675190448761
Epoch 1210, training loss: 842.4895629882812 = 0.3948250412940979 + 100.0 * 8.420947074890137
Epoch 1210, val loss: 0.46456724405288696
Epoch 1220, training loss: 842.3955078125 = 0.3939801752567291 + 100.0 * 8.420015335083008
Epoch 1220, val loss: 0.4641859233379364
Epoch 1230, training loss: 842.4605712890625 = 0.39315730333328247 + 100.0 * 8.420674324035645
Epoch 1230, val loss: 0.4637671113014221
Epoch 1240, training loss: 842.2532958984375 = 0.39232203364372253 + 100.0 * 8.418609619140625
Epoch 1240, val loss: 0.4634842574596405
Epoch 1250, training loss: 842.2344970703125 = 0.39152368903160095 + 100.0 * 8.418429374694824
Epoch 1250, val loss: 0.4631107449531555
Epoch 1260, training loss: 842.1198120117188 = 0.3907301723957062 + 100.0 * 8.417290687561035
Epoch 1260, val loss: 0.46274903416633606
Epoch 1270, training loss: 842.1431274414062 = 0.3899531960487366 + 100.0 * 8.417531967163086
Epoch 1270, val loss: 0.4624263346195221
Epoch 1280, training loss: 841.9844970703125 = 0.3891799747943878 + 100.0 * 8.415953636169434
Epoch 1280, val loss: 0.4620758891105652
Epoch 1290, training loss: 841.9130249023438 = 0.38842058181762695 + 100.0 * 8.41524600982666
Epoch 1290, val loss: 0.4617653489112854
Epoch 1300, training loss: 841.7973022460938 = 0.387672483921051 + 100.0 * 8.414095878601074
Epoch 1300, val loss: 0.46141183376312256
Epoch 1310, training loss: 841.724853515625 = 0.3869344890117645 + 100.0 * 8.413378715515137
Epoch 1310, val loss: 0.461107075214386
Epoch 1320, training loss: 841.71484375 = 0.38620465993881226 + 100.0 * 8.413286209106445
Epoch 1320, val loss: 0.46077823638916016
Epoch 1330, training loss: 841.6150512695312 = 0.38545742630958557 + 100.0 * 8.412296295166016
Epoch 1330, val loss: 0.4604969322681427
Epoch 1340, training loss: 841.6666870117188 = 0.3847346007823944 + 100.0 * 8.412819862365723
Epoch 1340, val loss: 0.46024465560913086
Epoch 1350, training loss: 841.4744262695312 = 0.38402044773101807 + 100.0 * 8.410903930664062
Epoch 1350, val loss: 0.4598461985588074
Epoch 1360, training loss: 841.405029296875 = 0.38331151008605957 + 100.0 * 8.41021728515625
Epoch 1360, val loss: 0.45967698097229004
Epoch 1370, training loss: 841.7815551757812 = 0.3826097249984741 + 100.0 * 8.413989067077637
Epoch 1370, val loss: 0.4594186842441559
Epoch 1380, training loss: 841.4346313476562 = 0.38190147280693054 + 100.0 * 8.410527229309082
Epoch 1380, val loss: 0.45903539657592773
Epoch 1390, training loss: 841.244140625 = 0.3812089264392853 + 100.0 * 8.408629417419434
Epoch 1390, val loss: 0.45873841643333435
Epoch 1400, training loss: 841.1461181640625 = 0.38052868843078613 + 100.0 * 8.407655715942383
Epoch 1400, val loss: 0.4584514796733856
Epoch 1410, training loss: 841.1433715820312 = 0.379856675863266 + 100.0 * 8.407634735107422
Epoch 1410, val loss: 0.45818081498146057
Epoch 1420, training loss: 841.081298828125 = 0.3791656494140625 + 100.0 * 8.407021522521973
Epoch 1420, val loss: 0.45782893896102905
Epoch 1430, training loss: 841.1390991210938 = 0.3784927427768707 + 100.0 * 8.40760612487793
Epoch 1430, val loss: 0.45766502618789673
Epoch 1440, training loss: 840.9397583007812 = 0.3778277337551117 + 100.0 * 8.405619621276855
Epoch 1440, val loss: 0.45723316073417664
Epoch 1450, training loss: 840.868408203125 = 0.3771742582321167 + 100.0 * 8.404911994934082
Epoch 1450, val loss: 0.4570493698120117
Epoch 1460, training loss: 840.8110961914062 = 0.3765287399291992 + 100.0 * 8.404345512390137
Epoch 1460, val loss: 0.4567691683769226
Epoch 1470, training loss: 840.7546997070312 = 0.37588366866111755 + 100.0 * 8.403788566589355
Epoch 1470, val loss: 0.4565068185329437
Epoch 1480, training loss: 840.7258911132812 = 0.3752393424510956 + 100.0 * 8.4035062789917
Epoch 1480, val loss: 0.45626744627952576
Epoch 1490, training loss: 841.1513061523438 = 0.3745850920677185 + 100.0 * 8.407767295837402
Epoch 1490, val loss: 0.45600268244743347
Epoch 1500, training loss: 840.6499633789062 = 0.37391623854637146 + 100.0 * 8.40276050567627
Epoch 1500, val loss: 0.4557918906211853
Epoch 1510, training loss: 840.580322265625 = 0.37326711416244507 + 100.0 * 8.402070045471191
Epoch 1510, val loss: 0.45535093545913696
Epoch 1520, training loss: 840.5301513671875 = 0.3726291060447693 + 100.0 * 8.401575088500977
Epoch 1520, val loss: 0.4551585614681244
Epoch 1530, training loss: 840.4666748046875 = 0.37200024724006653 + 100.0 * 8.400946617126465
Epoch 1530, val loss: 0.4548835754394531
Epoch 1540, training loss: 840.5623168945312 = 0.3713712692260742 + 100.0 * 8.401908874511719
Epoch 1540, val loss: 0.4546971917152405
Epoch 1550, training loss: 840.379150390625 = 0.37072888016700745 + 100.0 * 8.400084495544434
Epoch 1550, val loss: 0.45440343022346497
Epoch 1560, training loss: 840.3461303710938 = 0.37010160088539124 + 100.0 * 8.399760246276855
Epoch 1560, val loss: 0.4541644752025604
Epoch 1570, training loss: 840.3607177734375 = 0.3694823086261749 + 100.0 * 8.399911880493164
Epoch 1570, val loss: 0.4538775384426117
Epoch 1580, training loss: 840.2706909179688 = 0.36886394023895264 + 100.0 * 8.399018287658691
Epoch 1580, val loss: 0.4536901116371155
Epoch 1590, training loss: 840.2130126953125 = 0.36825159192085266 + 100.0 * 8.39844799041748
Epoch 1590, val loss: 0.4533922076225281
Epoch 1600, training loss: 840.1804809570312 = 0.3676358759403229 + 100.0 * 8.398128509521484
Epoch 1600, val loss: 0.4531855285167694
Epoch 1610, training loss: 840.36181640625 = 0.36701691150665283 + 100.0 * 8.399948120117188
Epoch 1610, val loss: 0.4529280960559845
Epoch 1620, training loss: 840.126220703125 = 0.366382360458374 + 100.0 * 8.397598266601562
Epoch 1620, val loss: 0.452729195356369
Epoch 1630, training loss: 840.0607299804688 = 0.36575812101364136 + 100.0 * 8.396949768066406
Epoch 1630, val loss: 0.452401727437973
Epoch 1640, training loss: 839.990478515625 = 0.3651372492313385 + 100.0 * 8.39625358581543
Epoch 1640, val loss: 0.45223426818847656
Epoch 1650, training loss: 839.9345703125 = 0.3645166754722595 + 100.0 * 8.395700454711914
Epoch 1650, val loss: 0.4519772529602051
Epoch 1660, training loss: 839.90771484375 = 0.3638934791088104 + 100.0 * 8.395438194274902
Epoch 1660, val loss: 0.45176073908805847
Epoch 1670, training loss: 840.438232421875 = 0.36326292157173157 + 100.0 * 8.400749206542969
Epoch 1670, val loss: 0.4515376091003418
Epoch 1680, training loss: 839.879638671875 = 0.3626192510128021 + 100.0 * 8.395170211791992
Epoch 1680, val loss: 0.4513595998287201
Epoch 1690, training loss: 839.8496704101562 = 0.36198747158050537 + 100.0 * 8.394876480102539
Epoch 1690, val loss: 0.4510171711444855
Epoch 1700, training loss: 840.04345703125 = 0.36135372519493103 + 100.0 * 8.396821022033691
Epoch 1700, val loss: 0.4509325325489044
Epoch 1710, training loss: 839.77294921875 = 0.3607187271118164 + 100.0 * 8.394122123718262
Epoch 1710, val loss: 0.45059171319007874
Epoch 1720, training loss: 839.6767578125 = 0.36009085178375244 + 100.0 * 8.393166542053223
Epoch 1720, val loss: 0.45039787888526917
Epoch 1730, training loss: 839.6343994140625 = 0.3594636619091034 + 100.0 * 8.392749786376953
Epoch 1730, val loss: 0.4501599371433258
Epoch 1740, training loss: 839.6079711914062 = 0.3588379919528961 + 100.0 * 8.392491340637207
Epoch 1740, val loss: 0.44992029666900635
Epoch 1750, training loss: 839.6746826171875 = 0.3582092523574829 + 100.0 * 8.39316463470459
Epoch 1750, val loss: 0.4496820867061615
Epoch 1760, training loss: 839.5662231445312 = 0.35756638646125793 + 100.0 * 8.39208698272705
Epoch 1760, val loss: 0.4494699537754059
Epoch 1770, training loss: 839.6437377929688 = 0.35692906379699707 + 100.0 * 8.392868041992188
Epoch 1770, val loss: 0.4492255449295044
Epoch 1780, training loss: 839.5010375976562 = 0.3562838137149811 + 100.0 * 8.391448020935059
Epoch 1780, val loss: 0.44902175664901733
Epoch 1790, training loss: 839.4616088867188 = 0.35565000772476196 + 100.0 * 8.391059875488281
Epoch 1790, val loss: 0.44873476028442383
Epoch 1800, training loss: 839.4112548828125 = 0.3550184667110443 + 100.0 * 8.390562057495117
Epoch 1800, val loss: 0.4485669434070587
Epoch 1810, training loss: 839.3995361328125 = 0.354387104511261 + 100.0 * 8.390451431274414
Epoch 1810, val loss: 0.4482746720314026
Epoch 1820, training loss: 839.8428344726562 = 0.35374557971954346 + 100.0 * 8.394890785217285
Epoch 1820, val loss: 0.4479632079601288
Epoch 1830, training loss: 839.43115234375 = 0.3530816435813904 + 100.0 * 8.390780448913574
Epoch 1830, val loss: 0.44790661334991455
Epoch 1840, training loss: 839.27685546875 = 0.3524346947669983 + 100.0 * 8.389244079589844
Epoch 1840, val loss: 0.4475744664669037
Epoch 1850, training loss: 839.2413330078125 = 0.3517891466617584 + 100.0 * 8.388895034790039
Epoch 1850, val loss: 0.4473554193973541
Epoch 1860, training loss: 839.2379760742188 = 0.3511446416378021 + 100.0 * 8.38886833190918
Epoch 1860, val loss: 0.44716617465019226
Epoch 1870, training loss: 839.484375 = 0.35049083828926086 + 100.0 * 8.391339302062988
Epoch 1870, val loss: 0.44687458872795105
Epoch 1880, training loss: 839.2301025390625 = 0.34982386231422424 + 100.0 * 8.388802528381348
Epoch 1880, val loss: 0.4466889202594757
Epoch 1890, training loss: 839.1439819335938 = 0.3491640090942383 + 100.0 * 8.387948036193848
Epoch 1890, val loss: 0.4463953673839569
Epoch 1900, training loss: 839.0921020507812 = 0.34850165247917175 + 100.0 * 8.387435913085938
Epoch 1900, val loss: 0.44619137048721313
Epoch 1910, training loss: 839.233154296875 = 0.3478374183177948 + 100.0 * 8.388853073120117
Epoch 1910, val loss: 0.44596847891807556
Epoch 1920, training loss: 839.0451049804688 = 0.34715384244918823 + 100.0 * 8.386979103088379
Epoch 1920, val loss: 0.44576993584632874
Epoch 1930, training loss: 839.045654296875 = 0.3464784026145935 + 100.0 * 8.386991500854492
Epoch 1930, val loss: 0.44540175795555115
Epoch 1940, training loss: 838.9989013671875 = 0.3458063304424286 + 100.0 * 8.386530876159668
Epoch 1940, val loss: 0.4452691972255707
Epoch 1950, training loss: 838.9530639648438 = 0.3451327383518219 + 100.0 * 8.386078834533691
Epoch 1950, val loss: 0.4449796676635742
Epoch 1960, training loss: 839.0716552734375 = 0.3444575369358063 + 100.0 * 8.387271881103516
Epoch 1960, val loss: 0.44472426176071167
Epoch 1970, training loss: 838.939453125 = 0.3437466025352478 + 100.0 * 8.385956764221191
Epoch 1970, val loss: 0.4445481598377228
Epoch 1980, training loss: 838.9746704101562 = 0.3430398404598236 + 100.0 * 8.386316299438477
Epoch 1980, val loss: 0.44419121742248535
Epoch 1990, training loss: 838.8597412109375 = 0.34233012795448303 + 100.0 * 8.385173797607422
Epoch 1990, val loss: 0.44405633211135864
Epoch 2000, training loss: 838.8070068359375 = 0.3416268825531006 + 100.0 * 8.38465404510498
Epoch 2000, val loss: 0.44378507137298584
Epoch 2010, training loss: 838.7876586914062 = 0.3409184515476227 + 100.0 * 8.384467124938965
Epoch 2010, val loss: 0.44357597827911377
Epoch 2020, training loss: 838.9717407226562 = 0.340203195810318 + 100.0 * 8.38631534576416
Epoch 2020, val loss: 0.4433964192867279
Epoch 2030, training loss: 838.7725830078125 = 0.3394603431224823 + 100.0 * 8.384330749511719
Epoch 2030, val loss: 0.44308191537857056
Epoch 2040, training loss: 838.7435913085938 = 0.3387298285961151 + 100.0 * 8.384048461914062
Epoch 2040, val loss: 0.44281136989593506
Epoch 2050, training loss: 838.690673828125 = 0.33800041675567627 + 100.0 * 8.383526802062988
Epoch 2050, val loss: 0.44262006878852844
Epoch 2060, training loss: 838.647705078125 = 0.3372732996940613 + 100.0 * 8.38310432434082
Epoch 2060, val loss: 0.44239285588264465
Epoch 2070, training loss: 838.629150390625 = 0.3365462124347687 + 100.0 * 8.382925987243652
Epoch 2070, val loss: 0.44217944145202637
Epoch 2080, training loss: 839.2711181640625 = 0.3358171582221985 + 100.0 * 8.389352798461914
Epoch 2080, val loss: 0.4420210123062134
Epoch 2090, training loss: 838.7674560546875 = 0.33504876494407654 + 100.0 * 8.384324073791504
Epoch 2090, val loss: 0.44181933999061584
Epoch 2100, training loss: 838.5549926757812 = 0.3343012034893036 + 100.0 * 8.382206916809082
Epoch 2100, val loss: 0.44153380393981934
Epoch 2110, training loss: 838.5303955078125 = 0.3335629999637604 + 100.0 * 8.38196849822998
Epoch 2110, val loss: 0.44137588143348694
Epoch 2120, training loss: 838.4871215820312 = 0.33282893896102905 + 100.0 * 8.381543159484863
Epoch 2120, val loss: 0.4412073791027069
Epoch 2130, training loss: 838.4624633789062 = 0.3320922553539276 + 100.0 * 8.381303787231445
Epoch 2130, val loss: 0.44099268317222595
Epoch 2140, training loss: 838.8760986328125 = 0.3313537538051605 + 100.0 * 8.38544750213623
Epoch 2140, val loss: 0.440728098154068
Epoch 2150, training loss: 838.6067504882812 = 0.3305753469467163 + 100.0 * 8.38276195526123
Epoch 2150, val loss: 0.44074517488479614
Epoch 2160, training loss: 838.4166259765625 = 0.3298106789588928 + 100.0 * 8.380867958068848
Epoch 2160, val loss: 0.4404928684234619
Epoch 2170, training loss: 838.3600463867188 = 0.3290439546108246 + 100.0 * 8.38031005859375
Epoch 2170, val loss: 0.4403378963470459
Epoch 2180, training loss: 838.3277587890625 = 0.3282804489135742 + 100.0 * 8.37999439239502
Epoch 2180, val loss: 0.4402218461036682
Epoch 2190, training loss: 838.3378295898438 = 0.3275144696235657 + 100.0 * 8.38010311126709
Epoch 2190, val loss: 0.4400518238544464
Epoch 2200, training loss: 838.786865234375 = 0.3267368972301483 + 100.0 * 8.384601593017578
Epoch 2200, val loss: 0.4399556517601013
Epoch 2210, training loss: 838.4135131835938 = 0.3259468674659729 + 100.0 * 8.380875587463379
Epoch 2210, val loss: 0.43964698910713196
Epoch 2220, training loss: 838.2627563476562 = 0.3251625597476959 + 100.0 * 8.379376411437988
Epoch 2220, val loss: 0.43955686688423157
Epoch 2230, training loss: 838.2214965820312 = 0.3243858218193054 + 100.0 * 8.378971099853516
Epoch 2230, val loss: 0.43937137722969055
Epoch 2240, training loss: 838.4591064453125 = 0.3236026167869568 + 100.0 * 8.381355285644531
Epoch 2240, val loss: 0.43923625349998474
Epoch 2250, training loss: 838.1937255859375 = 0.3227968215942383 + 100.0 * 8.378708839416504
Epoch 2250, val loss: 0.43912291526794434
Epoch 2260, training loss: 838.1365356445312 = 0.3219967186450958 + 100.0 * 8.378145217895508
Epoch 2260, val loss: 0.43889665603637695
Epoch 2270, training loss: 838.0867919921875 = 0.3211892247200012 + 100.0 * 8.377655982971191
Epoch 2270, val loss: 0.4387787878513336
Epoch 2280, training loss: 838.0650024414062 = 0.3203825354576111 + 100.0 * 8.377446174621582
Epoch 2280, val loss: 0.438610702753067
Epoch 2290, training loss: 838.2244873046875 = 0.31957364082336426 + 100.0 * 8.379049301147461
Epoch 2290, val loss: 0.43846115469932556
Epoch 2300, training loss: 838.0614624023438 = 0.3187447488307953 + 100.0 * 8.377427101135254
Epoch 2300, val loss: 0.4382915496826172
Epoch 2310, training loss: 838.0355834960938 = 0.31791606545448303 + 100.0 * 8.377176284790039
Epoch 2310, val loss: 0.43803924322128296
Epoch 2320, training loss: 837.988525390625 = 0.31709328293800354 + 100.0 * 8.376714706420898
Epoch 2320, val loss: 0.43799975514411926
Epoch 2330, training loss: 837.9519653320312 = 0.3162711560726166 + 100.0 * 8.376357078552246
Epoch 2330, val loss: 0.43785449862480164
Epoch 2340, training loss: 838.0435791015625 = 0.3154464662075043 + 100.0 * 8.377281188964844
Epoch 2340, val loss: 0.43776074051856995
Epoch 2350, training loss: 838.021728515625 = 0.31459951400756836 + 100.0 * 8.377071380615234
Epoch 2350, val loss: 0.4376181364059448
Epoch 2360, training loss: 838.0431518554688 = 0.31375056505203247 + 100.0 * 8.377294540405273
Epoch 2360, val loss: 0.4373614192008972
Epoch 2370, training loss: 837.8675537109375 = 0.3129017651081085 + 100.0 * 8.3755464553833
Epoch 2370, val loss: 0.43717333674430847
Epoch 2380, training loss: 837.8060913085938 = 0.31205427646636963 + 100.0 * 8.374939918518066
Epoch 2380, val loss: 0.43707147240638733
Epoch 2390, training loss: 837.7836303710938 = 0.31120383739471436 + 100.0 * 8.374724388122559
Epoch 2390, val loss: 0.4369029998779297
Epoch 2400, training loss: 837.7930297851562 = 0.3103458881378174 + 100.0 * 8.374826431274414
Epoch 2400, val loss: 0.43678611516952515
Epoch 2410, training loss: 838.0308227539062 = 0.30947616696357727 + 100.0 * 8.377213478088379
Epoch 2410, val loss: 0.43669113516807556
Epoch 2420, training loss: 837.7535400390625 = 0.3085951805114746 + 100.0 * 8.374449729919434
Epoch 2420, val loss: 0.436428964138031
Epoch 2430, training loss: 837.681884765625 = 0.30771639943122864 + 100.0 * 8.373741149902344
Epoch 2430, val loss: 0.4363141357898712
Epoch 2440, training loss: 837.7166137695312 = 0.3068411946296692 + 100.0 * 8.37409782409668
Epoch 2440, val loss: 0.43615198135375977
Epoch 2450, training loss: 838.0026245117188 = 0.3059523105621338 + 100.0 * 8.37696647644043
Epoch 2450, val loss: 0.43603214621543884
Epoch 2460, training loss: 837.6752319335938 = 0.30505266785621643 + 100.0 * 8.373702049255371
Epoch 2460, val loss: 0.43571925163269043
Epoch 2470, training loss: 837.6333618164062 = 0.3041570782661438 + 100.0 * 8.373291969299316
Epoch 2470, val loss: 0.4357485771179199
Epoch 2480, training loss: 837.5866088867188 = 0.30326470732688904 + 100.0 * 8.372833251953125
Epoch 2480, val loss: 0.43555909395217896
Epoch 2490, training loss: 837.5784301757812 = 0.30237019062042236 + 100.0 * 8.372760772705078
Epoch 2490, val loss: 0.43542876839637756
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8280060882800608
0.8645946533362313
=== training gcn model ===
Epoch 0, training loss: 1059.330078125 = 1.1000566482543945 + 100.0 * 10.58229923248291
Epoch 0, val loss: 1.0999938249588013
Epoch 10, training loss: 1059.295654296875 = 1.0953730344772339 + 100.0 * 10.582002639770508
Epoch 10, val loss: 1.0953059196472168
Epoch 20, training loss: 1059.1697998046875 = 1.090322732925415 + 100.0 * 10.580794334411621
Epoch 20, val loss: 1.0902466773986816
Epoch 30, training loss: 1058.633544921875 = 1.0849275588989258 + 100.0 * 10.575485229492188
Epoch 30, val loss: 1.0848441123962402
Epoch 40, training loss: 1056.3074951171875 = 1.079190969467163 + 100.0 * 10.55228328704834
Epoch 40, val loss: 1.0790860652923584
Epoch 50, training loss: 1047.52587890625 = 1.0729573965072632 + 100.0 * 10.464529037475586
Epoch 50, val loss: 1.0728330612182617
Epoch 60, training loss: 1020.64306640625 = 1.066724181175232 + 100.0 * 10.19576358795166
Epoch 60, val loss: 1.0666314363479614
Epoch 70, training loss: 976.076416015625 = 1.060287594795227 + 100.0 * 9.750161170959473
Epoch 70, val loss: 1.0601705312728882
Epoch 80, training loss: 962.9777221679688 = 1.0541951656341553 + 100.0 * 9.619235038757324
Epoch 80, val loss: 1.0542625188827515
Epoch 90, training loss: 953.8890380859375 = 1.0492304563522339 + 100.0 * 9.528397560119629
Epoch 90, val loss: 1.0494089126586914
Epoch 100, training loss: 942.6403198242188 = 1.0454022884368896 + 100.0 * 9.415948867797852
Epoch 100, val loss: 1.0456702709197998
Epoch 110, training loss: 930.4191284179688 = 1.0421464443206787 + 100.0 * 9.293769836425781
Epoch 110, val loss: 1.0424747467041016
Epoch 120, training loss: 920.4151000976562 = 1.038730263710022 + 100.0 * 9.193763732910156
Epoch 120, val loss: 1.0391079187393188
Epoch 130, training loss: 912.0711669921875 = 1.034956455230713 + 100.0 * 9.11036205291748
Epoch 130, val loss: 1.0354537963867188
Epoch 140, training loss: 903.4971923828125 = 1.0318301916122437 + 100.0 * 9.024653434753418
Epoch 140, val loss: 1.0324718952178955
Epoch 150, training loss: 898.2376098632812 = 1.0290038585662842 + 100.0 * 8.972085952758789
Epoch 150, val loss: 1.0296088457107544
Epoch 160, training loss: 893.3674926757812 = 1.0253556966781616 + 100.0 * 8.923421859741211
Epoch 160, val loss: 1.0259475708007812
Epoch 170, training loss: 888.4584350585938 = 1.0216573476791382 + 100.0 * 8.874367713928223
Epoch 170, val loss: 1.0223064422607422
Epoch 180, training loss: 884.717529296875 = 1.0179314613342285 + 100.0 * 8.836996078491211
Epoch 180, val loss: 1.0185867547988892
Epoch 190, training loss: 880.7615966796875 = 1.0138977766036987 + 100.0 * 8.797476768493652
Epoch 190, val loss: 1.0145515203475952
Epoch 200, training loss: 877.3001098632812 = 1.009710669517517 + 100.0 * 8.762904167175293
Epoch 200, val loss: 1.0103856325149536
Epoch 210, training loss: 874.3738403320312 = 1.0053757429122925 + 100.0 * 8.733684539794922
Epoch 210, val loss: 1.0060960054397583
Epoch 220, training loss: 871.9183349609375 = 1.0006359815597534 + 100.0 * 8.709177017211914
Epoch 220, val loss: 1.00135338306427
Epoch 230, training loss: 869.9385375976562 = 0.9953109622001648 + 100.0 * 8.689432144165039
Epoch 230, val loss: 0.9960319399833679
Epoch 240, training loss: 868.55078125 = 0.9892978668212891 + 100.0 * 8.675614356994629
Epoch 240, val loss: 0.9900135397911072
Epoch 250, training loss: 867.2567749023438 = 0.9826170206069946 + 100.0 * 8.662741661071777
Epoch 250, val loss: 0.9833745360374451
Epoch 260, training loss: 866.1580200195312 = 0.9753653407096863 + 100.0 * 8.651826858520508
Epoch 260, val loss: 0.9761834144592285
Epoch 270, training loss: 865.1123657226562 = 0.9675984382629395 + 100.0 * 8.641448020935059
Epoch 270, val loss: 0.9684619307518005
Epoch 280, training loss: 864.25390625 = 0.9592483043670654 + 100.0 * 8.632946968078613
Epoch 280, val loss: 0.9601821899414062
Epoch 290, training loss: 863.1249389648438 = 0.950330913066864 + 100.0 * 8.621746063232422
Epoch 290, val loss: 0.9513481855392456
Epoch 300, training loss: 862.3133544921875 = 0.9408562779426575 + 100.0 * 8.613724708557129
Epoch 300, val loss: 0.9419556856155396
Epoch 310, training loss: 861.6153564453125 = 0.9306585788726807 + 100.0 * 8.606846809387207
Epoch 310, val loss: 0.9318675398826599
Epoch 320, training loss: 860.9130859375 = 0.9198014736175537 + 100.0 * 8.599932670593262
Epoch 320, val loss: 0.9211398363113403
Epoch 330, training loss: 860.3187866210938 = 0.9083687663078308 + 100.0 * 8.594103813171387
Epoch 330, val loss: 0.9098911285400391
Epoch 340, training loss: 859.7847900390625 = 0.8964363932609558 + 100.0 * 8.588883399963379
Epoch 340, val loss: 0.8981809616088867
Epoch 350, training loss: 859.2560424804688 = 0.8840522170066833 + 100.0 * 8.583720207214355
Epoch 350, val loss: 0.8860304355621338
Epoch 360, training loss: 858.6617431640625 = 0.8713281154632568 + 100.0 * 8.577903747558594
Epoch 360, val loss: 0.8736069202423096
Epoch 370, training loss: 858.1232299804688 = 0.8583723306655884 + 100.0 * 8.572648048400879
Epoch 370, val loss: 0.8609828948974609
Epoch 380, training loss: 857.7156982421875 = 0.8451910018920898 + 100.0 * 8.568704605102539
Epoch 380, val loss: 0.8481870293617249
Epoch 390, training loss: 857.099609375 = 0.8318507075309753 + 100.0 * 8.562677383422852
Epoch 390, val loss: 0.8352120518684387
Epoch 400, training loss: 856.5887451171875 = 0.8184055685997009 + 100.0 * 8.557703018188477
Epoch 400, val loss: 0.8222136497497559
Epoch 410, training loss: 856.3016357421875 = 0.8049401044845581 + 100.0 * 8.554966926574707
Epoch 410, val loss: 0.8092153072357178
Epoch 420, training loss: 855.7651977539062 = 0.7913995981216431 + 100.0 * 8.549737930297852
Epoch 420, val loss: 0.7962071299552917
Epoch 430, training loss: 855.2807006835938 = 0.7779711484909058 + 100.0 * 8.545027732849121
Epoch 430, val loss: 0.78333979845047
Epoch 440, training loss: 854.7847290039062 = 0.7647048234939575 + 100.0 * 8.540200233459473
Epoch 440, val loss: 0.7706396579742432
Epoch 450, training loss: 854.3755493164062 = 0.7516106367111206 + 100.0 * 8.536239624023438
Epoch 450, val loss: 0.7581255435943604
Epoch 460, training loss: 854.3140258789062 = 0.7385774254798889 + 100.0 * 8.535754203796387
Epoch 460, val loss: 0.7457566261291504
Epoch 470, training loss: 853.543701171875 = 0.7257500290870667 + 100.0 * 8.528179168701172
Epoch 470, val loss: 0.7335623502731323
Epoch 480, training loss: 853.0684814453125 = 0.7132311463356018 + 100.0 * 8.523551940917969
Epoch 480, val loss: 0.7217426300048828
Epoch 490, training loss: 852.6834106445312 = 0.7009660005569458 + 100.0 * 8.519824028015137
Epoch 490, val loss: 0.7101792693138123
Epoch 500, training loss: 852.3387451171875 = 0.6889556646347046 + 100.0 * 8.516497611999512
Epoch 500, val loss: 0.6988760232925415
Epoch 510, training loss: 852.3466186523438 = 0.6771252751350403 + 100.0 * 8.516695022583008
Epoch 510, val loss: 0.6878274083137512
Epoch 520, training loss: 851.6394653320312 = 0.6656138300895691 + 100.0 * 8.50973892211914
Epoch 520, val loss: 0.6770622730255127
Epoch 530, training loss: 851.3292236328125 = 0.6544511318206787 + 100.0 * 8.506747245788574
Epoch 530, val loss: 0.6666775941848755
Epoch 540, training loss: 851.0028686523438 = 0.6435936689376831 + 100.0 * 8.503592491149902
Epoch 540, val loss: 0.6566185355186462
Epoch 550, training loss: 851.1329956054688 = 0.6330262422561646 + 100.0 * 8.504999160766602
Epoch 550, val loss: 0.6468433737754822
Epoch 560, training loss: 850.4484252929688 = 0.6227505207061768 + 100.0 * 8.49825668334961
Epoch 560, val loss: 0.6373892426490784
Epoch 570, training loss: 850.1411743164062 = 0.6128431558609009 + 100.0 * 8.495283126831055
Epoch 570, val loss: 0.6283184885978699
Epoch 580, training loss: 849.8941040039062 = 0.6032550930976868 + 100.0 * 8.492908477783203
Epoch 580, val loss: 0.6195642948150635
Epoch 590, training loss: 849.64404296875 = 0.5939680337905884 + 100.0 * 8.490500450134277
Epoch 590, val loss: 0.6111152172088623
Epoch 600, training loss: 849.6443481445312 = 0.5849725604057312 + 100.0 * 8.490593910217285
Epoch 600, val loss: 0.6029298901557922
Epoch 610, training loss: 849.2817993164062 = 0.5762653350830078 + 100.0 * 8.487054824829102
Epoch 610, val loss: 0.5950739979743958
Epoch 620, training loss: 848.9935913085938 = 0.5679226517677307 + 100.0 * 8.484256744384766
Epoch 620, val loss: 0.5875811576843262
Epoch 630, training loss: 848.7805786132812 = 0.5599187016487122 + 100.0 * 8.482206344604492
Epoch 630, val loss: 0.580427885055542
Epoch 640, training loss: 848.73046875 = 0.5522437691688538 + 100.0 * 8.481781959533691
Epoch 640, val loss: 0.5735852122306824
Epoch 650, training loss: 848.658447265625 = 0.5448156595230103 + 100.0 * 8.481136322021484
Epoch 650, val loss: 0.5670052170753479
Epoch 660, training loss: 848.2640991210938 = 0.5377772450447083 + 100.0 * 8.477263450622559
Epoch 660, val loss: 0.5608190894126892
Epoch 670, training loss: 848.056396484375 = 0.5310813784599304 + 100.0 * 8.475253105163574
Epoch 670, val loss: 0.5549700260162354
Epoch 680, training loss: 847.8464965820312 = 0.5246920585632324 + 100.0 * 8.473217964172363
Epoch 680, val loss: 0.5494347810745239
Epoch 690, training loss: 847.6608276367188 = 0.5186063647270203 + 100.0 * 8.47142219543457
Epoch 690, val loss: 0.544227659702301
Epoch 700, training loss: 847.71142578125 = 0.5127959251403809 + 100.0 * 8.471985816955566
Epoch 700, val loss: 0.5393461585044861
Epoch 710, training loss: 847.64990234375 = 0.5072283744812012 + 100.0 * 8.471426963806152
Epoch 710, val loss: 0.5345758199691772
Epoch 720, training loss: 847.2114868164062 = 0.5019465088844299 + 100.0 * 8.467095375061035
Epoch 720, val loss: 0.5302121043205261
Epoch 730, training loss: 847.0350952148438 = 0.49695509672164917 + 100.0 * 8.465381622314453
Epoch 730, val loss: 0.5261085033416748
Epoch 740, training loss: 846.8988037109375 = 0.4922187030315399 + 100.0 * 8.464065551757812
Epoch 740, val loss: 0.5222573280334473
Epoch 750, training loss: 846.9765625 = 0.48770540952682495 + 100.0 * 8.464888572692871
Epoch 750, val loss: 0.5186371803283691
Epoch 760, training loss: 846.5708618164062 = 0.483405739068985 + 100.0 * 8.460874557495117
Epoch 760, val loss: 0.5152349472045898
Epoch 770, training loss: 846.4161987304688 = 0.4793377220630646 + 100.0 * 8.459368705749512
Epoch 770, val loss: 0.5120954513549805
Epoch 780, training loss: 846.7706298828125 = 0.4754590392112732 + 100.0 * 8.46295166015625
Epoch 780, val loss: 0.5091151595115662
Epoch 790, training loss: 846.1583862304688 = 0.47173696756362915 + 100.0 * 8.456866264343262
Epoch 790, val loss: 0.5062844753265381
Epoch 800, training loss: 846.0072631835938 = 0.46822136640548706 + 100.0 * 8.455390930175781
Epoch 800, val loss: 0.5036909580230713
Epoch 810, training loss: 845.8265991210938 = 0.46487632393836975 + 100.0 * 8.453617095947266
Epoch 810, val loss: 0.5012405514717102
Epoch 820, training loss: 845.6591796875 = 0.46169376373291016 + 100.0 * 8.451974868774414
Epoch 820, val loss: 0.4989420771598816
Epoch 830, training loss: 845.9041748046875 = 0.45864617824554443 + 100.0 * 8.454455375671387
Epoch 830, val loss: 0.4967390298843384
Epoch 840, training loss: 845.5484619140625 = 0.4557100832462311 + 100.0 * 8.450927734375
Epoch 840, val loss: 0.4947558343410492
Epoch 850, training loss: 845.430419921875 = 0.45290157198905945 + 100.0 * 8.449775695800781
Epoch 850, val loss: 0.49289265275001526
Epoch 860, training loss: 845.1173095703125 = 0.45022696256637573 + 100.0 * 8.446670532226562
Epoch 860, val loss: 0.49101245403289795
Epoch 870, training loss: 844.9892578125 = 0.4476753771305084 + 100.0 * 8.445415496826172
Epoch 870, val loss: 0.4893122911453247
Epoch 880, training loss: 844.8340454101562 = 0.4452430307865143 + 100.0 * 8.443887710571289
Epoch 880, val loss: 0.4877100884914398
Epoch 890, training loss: 844.6986694335938 = 0.4429079592227936 + 100.0 * 8.442557334899902
Epoch 890, val loss: 0.4862605035305023
Epoch 900, training loss: 844.9456176757812 = 0.4406677186489105 + 100.0 * 8.445049285888672
Epoch 900, val loss: 0.48476994037628174
Epoch 910, training loss: 844.735595703125 = 0.4384649395942688 + 100.0 * 8.442971229553223
Epoch 910, val loss: 0.4834935665130615
Epoch 920, training loss: 844.4563598632812 = 0.43637582659721375 + 100.0 * 8.440199851989746
Epoch 920, val loss: 0.48225992918014526
Epoch 930, training loss: 844.2442016601562 = 0.43437424302101135 + 100.0 * 8.438097953796387
Epoch 930, val loss: 0.4809889793395996
Epoch 940, training loss: 844.221923828125 = 0.4324430227279663 + 100.0 * 8.437894821166992
Epoch 940, val loss: 0.47984108328819275
Epoch 950, training loss: 843.9989013671875 = 0.4305744469165802 + 100.0 * 8.435683250427246
Epoch 950, val loss: 0.478745698928833
Epoch 960, training loss: 844.0404052734375 = 0.4287760257720947 + 100.0 * 8.436116218566895
Epoch 960, val loss: 0.4777034819126129
Epoch 970, training loss: 843.912353515625 = 0.42700180411338806 + 100.0 * 8.434853553771973
Epoch 970, val loss: 0.4767032861709595
Epoch 980, training loss: 843.8577270507812 = 0.4252947270870209 + 100.0 * 8.434324264526367
Epoch 980, val loss: 0.4757232367992401
Epoch 990, training loss: 843.614013671875 = 0.4236591160297394 + 100.0 * 8.431903839111328
Epoch 990, val loss: 0.47481629252433777
Epoch 1000, training loss: 843.5415649414062 = 0.4220774173736572 + 100.0 * 8.431195259094238
Epoch 1000, val loss: 0.47394123673439026
Epoch 1010, training loss: 843.4669799804688 = 0.42054152488708496 + 100.0 * 8.430464744567871
Epoch 1010, val loss: 0.4731226861476898
Epoch 1020, training loss: 843.6696166992188 = 0.4190455675125122 + 100.0 * 8.43250560760498
Epoch 1020, val loss: 0.47232457995414734
Epoch 1030, training loss: 843.3428955078125 = 0.4175724685192108 + 100.0 * 8.429252624511719
Epoch 1030, val loss: 0.47157424688339233
Epoch 1040, training loss: 843.3316650390625 = 0.4161547124385834 + 100.0 * 8.429155349731445
Epoch 1040, val loss: 0.4708589017391205
Epoch 1050, training loss: 843.12548828125 = 0.41478630900382996 + 100.0 * 8.427106857299805
Epoch 1050, val loss: 0.4701130986213684
Epoch 1060, training loss: 843.072998046875 = 0.41346150636672974 + 100.0 * 8.426595687866211
Epoch 1060, val loss: 0.46942275762557983
Epoch 1070, training loss: 843.1362915039062 = 0.4121662378311157 + 100.0 * 8.427241325378418
Epoch 1070, val loss: 0.46875861287117004
Epoch 1080, training loss: 842.9418334960938 = 0.4108871817588806 + 100.0 * 8.425309181213379
Epoch 1080, val loss: 0.46815717220306396
Epoch 1090, training loss: 842.8681030273438 = 0.40964674949645996 + 100.0 * 8.42458438873291
Epoch 1090, val loss: 0.4675123691558838
Epoch 1100, training loss: 842.7861328125 = 0.4084416329860687 + 100.0 * 8.423776626586914
Epoch 1100, val loss: 0.4669594466686249
Epoch 1110, training loss: 842.7197875976562 = 0.4072655439376831 + 100.0 * 8.423125267028809
Epoch 1110, val loss: 0.4663764238357544
Epoch 1120, training loss: 843.1779174804688 = 0.4061044752597809 + 100.0 * 8.427718162536621
Epoch 1120, val loss: 0.4658409059047699
Epoch 1130, training loss: 842.6931762695312 = 0.4049478769302368 + 100.0 * 8.422882080078125
Epoch 1130, val loss: 0.4653094708919525
Epoch 1140, training loss: 842.5234985351562 = 0.4038357436656952 + 100.0 * 8.421196937561035
Epoch 1140, val loss: 0.4647696912288666
Epoch 1150, training loss: 842.4558715820312 = 0.4027532935142517 + 100.0 * 8.420531272888184
Epoch 1150, val loss: 0.4642370939254761
Epoch 1160, training loss: 842.364013671875 = 0.40169182419776917 + 100.0 * 8.419623374938965
Epoch 1160, val loss: 0.4637797772884369
Epoch 1170, training loss: 842.5315551757812 = 0.40065300464630127 + 100.0 * 8.421309471130371
Epoch 1170, val loss: 0.4632573425769806
Epoch 1180, training loss: 842.2831420898438 = 0.3996109664440155 + 100.0 * 8.418835639953613
Epoch 1180, val loss: 0.4628855586051941
Epoch 1190, training loss: 842.2383422851562 = 0.3986092805862427 + 100.0 * 8.418396949768066
Epoch 1190, val loss: 0.4623415470123291
Epoch 1200, training loss: 842.2737426757812 = 0.3976227641105652 + 100.0 * 8.418761253356934
Epoch 1200, val loss: 0.46195295453071594
Epoch 1210, training loss: 842.1138916015625 = 0.39664021134376526 + 100.0 * 8.4171724319458
Epoch 1210, val loss: 0.4615165889263153
Epoch 1220, training loss: 842.0377807617188 = 0.3956764340400696 + 100.0 * 8.416420936584473
Epoch 1220, val loss: 0.4611043930053711
Epoch 1230, training loss: 841.9796142578125 = 0.39473387598991394 + 100.0 * 8.415848731994629
Epoch 1230, val loss: 0.46071329712867737
Epoch 1240, training loss: 841.9569091796875 = 0.39380937814712524 + 100.0 * 8.415631294250488
Epoch 1240, val loss: 0.460308700799942
Epoch 1250, training loss: 842.0728149414062 = 0.3928837776184082 + 100.0 * 8.416799545288086
Epoch 1250, val loss: 0.45993879437446594
Epoch 1260, training loss: 841.8433227539062 = 0.3919656574726105 + 100.0 * 8.41451358795166
Epoch 1260, val loss: 0.459540992975235
Epoch 1270, training loss: 841.792724609375 = 0.39107421040534973 + 100.0 * 8.414016723632812
Epoch 1270, val loss: 0.45915091037750244
Epoch 1280, training loss: 841.8644409179688 = 0.3901929259300232 + 100.0 * 8.414742469787598
Epoch 1280, val loss: 0.4588170349597931
Epoch 1290, training loss: 841.7421264648438 = 0.3893115520477295 + 100.0 * 8.413528442382812
Epoch 1290, val loss: 0.4584096372127533
Epoch 1300, training loss: 841.6534423828125 = 0.38844218850135803 + 100.0 * 8.412650108337402
Epoch 1300, val loss: 0.45810434222221375
Epoch 1310, training loss: 841.5903930664062 = 0.38759443163871765 + 100.0 * 8.412028312683105
Epoch 1310, val loss: 0.4577102065086365
Epoch 1320, training loss: 841.5538330078125 = 0.3867562413215637 + 100.0 * 8.411670684814453
Epoch 1320, val loss: 0.45738717913627625
Epoch 1330, training loss: 841.7821655273438 = 0.38592272996902466 + 100.0 * 8.413962364196777
Epoch 1330, val loss: 0.45701223611831665
Epoch 1340, training loss: 841.8160400390625 = 0.385077565908432 + 100.0 * 8.41430950164795
Epoch 1340, val loss: 0.4568398892879486
Epoch 1350, training loss: 841.4244995117188 = 0.3842497169971466 + 100.0 * 8.410402297973633
Epoch 1350, val loss: 0.45644134283065796
Epoch 1360, training loss: 841.3983764648438 = 0.38344043493270874 + 100.0 * 8.410149574279785
Epoch 1360, val loss: 0.4561236798763275
Epoch 1370, training loss: 841.322265625 = 0.38264238834381104 + 100.0 * 8.409396171569824
Epoch 1370, val loss: 0.45583707094192505
Epoch 1380, training loss: 841.2691040039062 = 0.3818528354167938 + 100.0 * 8.408872604370117
Epoch 1380, val loss: 0.4555634558200836
Epoch 1390, training loss: 841.40087890625 = 0.3810696601867676 + 100.0 * 8.410198211669922
Epoch 1390, val loss: 0.455226331949234
Epoch 1400, training loss: 841.2037353515625 = 0.38027888536453247 + 100.0 * 8.408234596252441
Epoch 1400, val loss: 0.4549754559993744
Epoch 1410, training loss: 841.1661376953125 = 0.37950417399406433 + 100.0 * 8.407866477966309
Epoch 1410, val loss: 0.45466455817222595
Epoch 1420, training loss: 841.1282958984375 = 0.37874656915664673 + 100.0 * 8.407495498657227
Epoch 1420, val loss: 0.4544019401073456
Epoch 1430, training loss: 841.564697265625 = 0.37799856066703796 + 100.0 * 8.411867141723633
Epoch 1430, val loss: 0.45409587025642395
Epoch 1440, training loss: 841.2899780273438 = 0.3772309720516205 + 100.0 * 8.409127235412598
Epoch 1440, val loss: 0.45392000675201416
Epoch 1450, training loss: 840.9987182617188 = 0.37649014592170715 + 100.0 * 8.406222343444824
Epoch 1450, val loss: 0.4536193311214447
Epoch 1460, training loss: 840.9598388671875 = 0.37576016783714294 + 100.0 * 8.405840873718262
Epoch 1460, val loss: 0.45338761806488037
Epoch 1470, training loss: 840.9306030273438 = 0.37504276633262634 + 100.0 * 8.405555725097656
Epoch 1470, val loss: 0.45317143201828003
Epoch 1480, training loss: 841.1595458984375 = 0.37432703375816345 + 100.0 * 8.407852172851562
Epoch 1480, val loss: 0.45292991399765015
Epoch 1490, training loss: 840.9365844726562 = 0.37360501289367676 + 100.0 * 8.405630111694336
Epoch 1490, val loss: 0.45274263620376587
Epoch 1500, training loss: 840.9349975585938 = 0.37290075421333313 + 100.0 * 8.405620574951172
Epoch 1500, val loss: 0.45249560475349426
Epoch 1510, training loss: 840.7396240234375 = 0.37219882011413574 + 100.0 * 8.403674125671387
Epoch 1510, val loss: 0.4522486925125122
Epoch 1520, training loss: 840.6990966796875 = 0.371513307094574 + 100.0 * 8.403275489807129
Epoch 1520, val loss: 0.4520045518875122
Epoch 1530, training loss: 840.6820068359375 = 0.37083151936531067 + 100.0 * 8.403111457824707
Epoch 1530, val loss: 0.4517694115638733
Epoch 1540, training loss: 840.68798828125 = 0.37014901638031006 + 100.0 * 8.403178215026855
Epoch 1540, val loss: 0.4515962302684784
Epoch 1550, training loss: 841.0990600585938 = 0.369461327791214 + 100.0 * 8.407296180725098
Epoch 1550, val loss: 0.45147866010665894
Epoch 1560, training loss: 840.6067504882812 = 0.3687666952610016 + 100.0 * 8.402379989624023
Epoch 1560, val loss: 0.45115506649017334
Epoch 1570, training loss: 840.46875 = 0.3680945336818695 + 100.0 * 8.401006698608398
Epoch 1570, val loss: 0.4509648084640503
Epoch 1580, training loss: 840.44140625 = 0.3674309551715851 + 100.0 * 8.400739669799805
Epoch 1580, val loss: 0.4508092999458313
Epoch 1590, training loss: 840.3849487304688 = 0.3667713403701782 + 100.0 * 8.400181770324707
Epoch 1590, val loss: 0.4505966901779175
Epoch 1600, training loss: 840.5021362304688 = 0.36611253023147583 + 100.0 * 8.401360511779785
Epoch 1600, val loss: 0.4504305422306061
Epoch 1610, training loss: 840.3023681640625 = 0.3654401898384094 + 100.0 * 8.399369239807129
Epoch 1610, val loss: 0.45015597343444824
Epoch 1620, training loss: 840.3604125976562 = 0.36477401852607727 + 100.0 * 8.399956703186035
Epoch 1620, val loss: 0.4500170648097992
Epoch 1630, training loss: 840.2750244140625 = 0.36412328481674194 + 100.0 * 8.39910888671875
Epoch 1630, val loss: 0.4498351812362671
Epoch 1640, training loss: 840.6165161132812 = 0.3634669780731201 + 100.0 * 8.402530670166016
Epoch 1640, val loss: 0.44969987869262695
Epoch 1650, training loss: 840.3008422851562 = 0.3628122806549072 + 100.0 * 8.399380683898926
Epoch 1650, val loss: 0.44935691356658936
Epoch 1660, training loss: 840.181396484375 = 0.3621634840965271 + 100.0 * 8.398192405700684
Epoch 1660, val loss: 0.44926831126213074
Epoch 1670, training loss: 840.3713989257812 = 0.3615227937698364 + 100.0 * 8.40009880065918
Epoch 1670, val loss: 0.4490329921245575
Epoch 1680, training loss: 840.0902099609375 = 0.36087116599082947 + 100.0 * 8.397293090820312
Epoch 1680, val loss: 0.4488263428211212
Epoch 1690, training loss: 839.9993896484375 = 0.36023348569869995 + 100.0 * 8.396391868591309
Epoch 1690, val loss: 0.44867897033691406
Epoch 1700, training loss: 839.9660034179688 = 0.35959771275520325 + 100.0 * 8.396063804626465
Epoch 1700, val loss: 0.4484991431236267
Epoch 1710, training loss: 840.019287109375 = 0.3589644432067871 + 100.0 * 8.39660358428955
Epoch 1710, val loss: 0.4483450949192047
Epoch 1720, training loss: 839.9766235351562 = 0.3583189845085144 + 100.0 * 8.396183013916016
Epoch 1720, val loss: 0.4481367766857147
Epoch 1730, training loss: 839.931884765625 = 0.3576746881008148 + 100.0 * 8.395742416381836
Epoch 1730, val loss: 0.44801169633865356
Epoch 1740, training loss: 839.8501586914062 = 0.35704344511032104 + 100.0 * 8.394930839538574
Epoch 1740, val loss: 0.44772836565971375
Epoch 1750, training loss: 839.7636108398438 = 0.356412410736084 + 100.0 * 8.394072532653809
Epoch 1750, val loss: 0.4476165771484375
Epoch 1760, training loss: 839.7717895507812 = 0.35578569769859314 + 100.0 * 8.394160270690918
Epoch 1760, val loss: 0.44744130969047546
Epoch 1770, training loss: 840.374755859375 = 0.35515210032463074 + 100.0 * 8.400196075439453
Epoch 1770, val loss: 0.44724348187446594
Epoch 1780, training loss: 839.804443359375 = 0.3545047342777252 + 100.0 * 8.394499778747559
Epoch 1780, val loss: 0.4471050202846527
Epoch 1790, training loss: 839.6323852539062 = 0.3538763225078583 + 100.0 * 8.39278507232666
Epoch 1790, val loss: 0.44691577553749084
Epoch 1800, training loss: 839.5919189453125 = 0.35325437784194946 + 100.0 * 8.392386436462402
Epoch 1800, val loss: 0.44679123163223267
Epoch 1810, training loss: 839.5748901367188 = 0.3526358902454376 + 100.0 * 8.39222240447998
Epoch 1810, val loss: 0.44662976264953613
Epoch 1820, training loss: 839.9995727539062 = 0.3520115911960602 + 100.0 * 8.396475791931152
Epoch 1820, val loss: 0.4465004503726959
Epoch 1830, training loss: 839.68603515625 = 0.35137736797332764 + 100.0 * 8.393346786499023
Epoch 1830, val loss: 0.44634541869163513
Epoch 1840, training loss: 839.4985961914062 = 0.35075169801712036 + 100.0 * 8.391478538513184
Epoch 1840, val loss: 0.4461299777030945
Epoch 1850, training loss: 839.439453125 = 0.3501354455947876 + 100.0 * 8.39089298248291
Epoch 1850, val loss: 0.4460459053516388
Epoch 1860, training loss: 839.4518432617188 = 0.34952452778816223 + 100.0 * 8.391022682189941
Epoch 1860, val loss: 0.4458601474761963
Epoch 1870, training loss: 840.0713500976562 = 0.34890905022621155 + 100.0 * 8.397224426269531
Epoch 1870, val loss: 0.4457080066204071
Epoch 1880, training loss: 839.5148315429688 = 0.3482673764228821 + 100.0 * 8.3916654586792
Epoch 1880, val loss: 0.44557225704193115
Epoch 1890, training loss: 839.3660278320312 = 0.3476460874080658 + 100.0 * 8.390183448791504
Epoch 1890, val loss: 0.44549307227134705
Epoch 1900, training loss: 839.3145751953125 = 0.34703329205513 + 100.0 * 8.38967514038086
Epoch 1900, val loss: 0.4453040361404419
Epoch 1910, training loss: 839.2575073242188 = 0.34641775488853455 + 100.0 * 8.389110565185547
Epoch 1910, val loss: 0.4451742470264435
Epoch 1920, training loss: 839.235595703125 = 0.3458026647567749 + 100.0 * 8.388897895812988
Epoch 1920, val loss: 0.4450419545173645
Epoch 1930, training loss: 839.487060546875 = 0.3451859652996063 + 100.0 * 8.39141845703125
Epoch 1930, val loss: 0.444873571395874
Epoch 1940, training loss: 839.2018432617188 = 0.34454455971717834 + 100.0 * 8.388572692871094
Epoch 1940, val loss: 0.44484806060791016
Epoch 1950, training loss: 839.172607421875 = 0.34391507506370544 + 100.0 * 8.388286590576172
Epoch 1950, val loss: 0.4446479082107544
Epoch 1960, training loss: 839.1385498046875 = 0.34329038858413696 + 100.0 * 8.38795280456543
Epoch 1960, val loss: 0.44455787539482117
Epoch 1970, training loss: 839.1243896484375 = 0.34267276525497437 + 100.0 * 8.3878173828125
Epoch 1970, val loss: 0.44451069831848145
Epoch 1980, training loss: 839.3394165039062 = 0.34204643964767456 + 100.0 * 8.389973640441895
Epoch 1980, val loss: 0.44439905881881714
Epoch 1990, training loss: 839.0733642578125 = 0.3414193093776703 + 100.0 * 8.387319564819336
Epoch 1990, val loss: 0.444162517786026
Epoch 2000, training loss: 839.1141357421875 = 0.340793251991272 + 100.0 * 8.387733459472656
Epoch 2000, val loss: 0.44405508041381836
Epoch 2010, training loss: 839.270263671875 = 0.3401653468608856 + 100.0 * 8.389301300048828
Epoch 2010, val loss: 0.44399014115333557
Epoch 2020, training loss: 839.0770263671875 = 0.3395357131958008 + 100.0 * 8.387374877929688
Epoch 2020, val loss: 0.4437980353832245
Epoch 2030, training loss: 838.9506225585938 = 0.33890989422798157 + 100.0 * 8.386116981506348
Epoch 2030, val loss: 0.44369620084762573
Epoch 2040, training loss: 838.9132080078125 = 0.33828768134117126 + 100.0 * 8.385748863220215
Epoch 2040, val loss: 0.44357502460479736
Epoch 2050, training loss: 839.045654296875 = 0.3376663327217102 + 100.0 * 8.387080192565918
Epoch 2050, val loss: 0.44349387288093567
Epoch 2060, training loss: 838.9459838867188 = 0.3370319902896881 + 100.0 * 8.386089324951172
Epoch 2060, val loss: 0.4433589577674866
Epoch 2070, training loss: 838.9146728515625 = 0.3364052176475525 + 100.0 * 8.385782241821289
Epoch 2070, val loss: 0.4432184398174286
Epoch 2080, training loss: 838.924560546875 = 0.3357808589935303 + 100.0 * 8.38588809967041
Epoch 2080, val loss: 0.4431265592575073
Epoch 2090, training loss: 838.865966796875 = 0.3351484537124634 + 100.0 * 8.385308265686035
Epoch 2090, val loss: 0.443019300699234
Epoch 2100, training loss: 838.8662109375 = 0.3345157206058502 + 100.0 * 8.385316848754883
Epoch 2100, val loss: 0.4429320991039276
Epoch 2110, training loss: 839.0974731445312 = 0.3338841497898102 + 100.0 * 8.387636184692383
Epoch 2110, val loss: 0.44279029965400696
Epoch 2120, training loss: 838.8108520507812 = 0.33324134349823 + 100.0 * 8.38477611541748
Epoch 2120, val loss: 0.44275298714637756
Epoch 2130, training loss: 838.7076416015625 = 0.3326166272163391 + 100.0 * 8.383749961853027
Epoch 2130, val loss: 0.44257766008377075
Epoch 2140, training loss: 838.66357421875 = 0.3319922685623169 + 100.0 * 8.383316040039062
Epoch 2140, val loss: 0.44256821274757385
Epoch 2150, training loss: 838.6376953125 = 0.3313738703727722 + 100.0 * 8.383063316345215
Epoch 2150, val loss: 0.4424345791339874
Epoch 2160, training loss: 838.9130859375 = 0.33075085282325745 + 100.0 * 8.385823249816895
Epoch 2160, val loss: 0.44234538078308105
Epoch 2170, training loss: 838.6300659179688 = 0.3301120102405548 + 100.0 * 8.382999420166016
Epoch 2170, val loss: 0.44222164154052734
Epoch 2180, training loss: 838.5745849609375 = 0.3294830918312073 + 100.0 * 8.382451057434082
Epoch 2180, val loss: 0.4420647919178009
Epoch 2190, training loss: 838.6757202148438 = 0.32886114716529846 + 100.0 * 8.383468627929688
Epoch 2190, val loss: 0.44196316599845886
Epoch 2200, training loss: 838.7078857421875 = 0.32822486758232117 + 100.0 * 8.383796691894531
Epoch 2200, val loss: 0.4419119954109192
Epoch 2210, training loss: 838.6192016601562 = 0.32758647203445435 + 100.0 * 8.382916450500488
Epoch 2210, val loss: 0.44182130694389343
Epoch 2220, training loss: 838.5064697265625 = 0.3269588053226471 + 100.0 * 8.381794929504395
Epoch 2220, val loss: 0.44167327880859375
Epoch 2230, training loss: 838.4599609375 = 0.32633209228515625 + 100.0 * 8.381336212158203
Epoch 2230, val loss: 0.44162803888320923
Epoch 2240, training loss: 838.419921875 = 0.3257085978984833 + 100.0 * 8.380942344665527
Epoch 2240, val loss: 0.44148388504981995
Epoch 2250, training loss: 838.4657592773438 = 0.3250803053379059 + 100.0 * 8.381406784057617
Epoch 2250, val loss: 0.4414002001285553
Epoch 2260, training loss: 838.7520751953125 = 0.32444000244140625 + 100.0 * 8.384276390075684
Epoch 2260, val loss: 0.44133734703063965
Epoch 2270, training loss: 838.7490234375 = 0.3237915635108948 + 100.0 * 8.384252548217773
Epoch 2270, val loss: 0.4410656988620758
Epoch 2280, training loss: 838.4141235351562 = 0.323133260011673 + 100.0 * 8.38090991973877
Epoch 2280, val loss: 0.4411410093307495
Epoch 2290, training loss: 838.3375854492188 = 0.32249224185943604 + 100.0 * 8.38015079498291
Epoch 2290, val loss: 0.4409337043762207
Epoch 2300, training loss: 838.2839965820312 = 0.3218538761138916 + 100.0 * 8.379621505737305
Epoch 2300, val loss: 0.44091516733169556
Epoch 2310, training loss: 838.2536010742188 = 0.32121703028678894 + 100.0 * 8.379323959350586
Epoch 2310, val loss: 0.4408036172389984
Epoch 2320, training loss: 838.239501953125 = 0.320579469203949 + 100.0 * 8.379189491271973
Epoch 2320, val loss: 0.4407083988189697
Epoch 2330, training loss: 838.5355834960938 = 0.3199425935745239 + 100.0 * 8.382156372070312
Epoch 2330, val loss: 0.4405592679977417
Epoch 2340, training loss: 838.5177612304688 = 0.31927984952926636 + 100.0 * 8.38198471069336
Epoch 2340, val loss: 0.44066479802131653
Epoch 2350, training loss: 838.2886962890625 = 0.3186274766921997 + 100.0 * 8.379700660705566
Epoch 2350, val loss: 0.44039425253868103
Epoch 2360, training loss: 838.1824951171875 = 0.31798112392425537 + 100.0 * 8.378644943237305
Epoch 2360, val loss: 0.4404468238353729
Epoch 2370, training loss: 838.1369018554688 = 0.31734317541122437 + 100.0 * 8.378195762634277
Epoch 2370, val loss: 0.44038286805152893
Epoch 2380, training loss: 838.2785034179688 = 0.3167073130607605 + 100.0 * 8.379617691040039
Epoch 2380, val loss: 0.44026583433151245
Epoch 2390, training loss: 838.1685180664062 = 0.31604960560798645 + 100.0 * 8.378524780273438
Epoch 2390, val loss: 0.4402899742126465
Epoch 2400, training loss: 838.094970703125 = 0.31539905071258545 + 100.0 * 8.377796173095703
Epoch 2400, val loss: 0.44017383456230164
Epoch 2410, training loss: 838.0659790039062 = 0.3147534132003784 + 100.0 * 8.377511978149414
Epoch 2410, val loss: 0.44014135003089905
Epoch 2420, training loss: 838.105224609375 = 0.3141076862812042 + 100.0 * 8.377911567687988
Epoch 2420, val loss: 0.4400911331176758
Epoch 2430, training loss: 838.3465576171875 = 0.3134543001651764 + 100.0 * 8.380331039428711
Epoch 2430, val loss: 0.440034419298172
Epoch 2440, training loss: 838.174072265625 = 0.31278538703918457 + 100.0 * 8.378612518310547
Epoch 2440, val loss: 0.4401564300060272
Epoch 2450, training loss: 837.98681640625 = 0.3121255338191986 + 100.0 * 8.376747131347656
Epoch 2450, val loss: 0.43996894359588623
Epoch 2460, training loss: 837.955078125 = 0.3114648461341858 + 100.0 * 8.376436233520508
Epoch 2460, val loss: 0.44000425934791565
Epoch 2470, training loss: 837.9478149414062 = 0.310804545879364 + 100.0 * 8.376370429992676
Epoch 2470, val loss: 0.4399748742580414
Epoch 2480, training loss: 838.1661376953125 = 0.31014156341552734 + 100.0 * 8.378560066223145
Epoch 2480, val loss: 0.4399617612361908
Epoch 2490, training loss: 838.0196533203125 = 0.3094625174999237 + 100.0 * 8.37710189819336
Epoch 2490, val loss: 0.4398689568042755
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8330796549974632
0.8630732449467508
The final CL Acc:0.83088, 0.00213, The final GNN Acc:0.86416, 0.00077
Begin epxeriment: noisy_level: 0.2 cont_weight: 100 epoch: der1:0.3 der2:0.3 dfr1:0.4 dfr2:0
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=2500, cl_num_layers=2, cl_num_proj_hidden=64, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=100.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.3, drop_feat_rate_1=0.4, drop_feat_rate_2=0.0, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=32, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, noisy_level=0.2, num_hidden=64, num_proj_hidden=64, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=40, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 88648])
add edge: torch.Size([2, 106392])
remove edge: torch.Size([2, 71220])
updated graph: torch.Size([2, 88964])
=== Noisy graph ===
/home/project-robust-contrastive/Robust-Contrastive-Learning/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 1006.5062866210938 = 1.0902774333953857 + 100.0 * 10.054160118103027
Epoch 0, val loss: 1.090683937072754
Epoch 10, training loss: 955.698486328125 = 1.086975336074829 + 100.0 * 9.546114921569824
Epoch 10, val loss: 1.0874056816101074
Epoch 20, training loss: 939.1385498046875 = 1.0838044881820679 + 100.0 * 9.380547523498535
Epoch 20, val loss: 1.084258794784546
Epoch 30, training loss: 928.656982421875 = 1.0808157920837402 + 100.0 * 9.275761604309082
Epoch 30, val loss: 1.0813004970550537
Epoch 40, training loss: 920.419189453125 = 1.0780307054519653 + 100.0 * 9.193411827087402
Epoch 40, val loss: 1.078548550605774
Epoch 50, training loss: 913.7599487304688 = 1.0754514932632446 + 100.0 * 9.126845359802246
Epoch 50, val loss: 1.0759979486465454
Epoch 60, training loss: 908.04833984375 = 1.0730819702148438 + 100.0 * 9.06975269317627
Epoch 60, val loss: 1.073652982711792
Epoch 70, training loss: 903.066650390625 = 1.07089364528656 + 100.0 * 9.019957542419434
Epoch 70, val loss: 1.071488618850708
Epoch 80, training loss: 898.7089233398438 = 1.0688968896865845 + 100.0 * 8.976400375366211
Epoch 80, val loss: 1.0695101022720337
Epoch 90, training loss: 894.8145141601562 = 1.0670759677886963 + 100.0 * 8.937474250793457
Epoch 90, val loss: 1.067708969116211
Epoch 100, training loss: 891.4483032226562 = 1.0654304027557373 + 100.0 * 8.903828620910645
Epoch 100, val loss: 1.066078782081604
Epoch 110, training loss: 888.3728637695312 = 1.0639533996582031 + 100.0 * 8.873088836669922
Epoch 110, val loss: 1.0646145343780518
Epoch 120, training loss: 885.7852172851562 = 1.0626270771026611 + 100.0 * 8.8472261428833
Epoch 120, val loss: 1.0633015632629395
Epoch 130, training loss: 883.390625 = 1.0614584684371948 + 100.0 * 8.823291778564453
Epoch 130, val loss: 1.0621461868286133
Epoch 140, training loss: 881.3309326171875 = 1.0604251623153687 + 100.0 * 8.802704811096191
Epoch 140, val loss: 1.0611240863800049
Epoch 150, training loss: 879.5619506835938 = 1.059531331062317 + 100.0 * 8.785024642944336
Epoch 150, val loss: 1.0602394342422485
Epoch 160, training loss: 877.8672485351562 = 1.0587611198425293 + 100.0 * 8.768084526062012
Epoch 160, val loss: 1.059477686882019
Epoch 170, training loss: 876.612548828125 = 1.0581066608428955 + 100.0 * 8.755544662475586
Epoch 170, val loss: 1.0588349103927612
Epoch 180, training loss: 875.2824096679688 = 1.0575597286224365 + 100.0 * 8.74224853515625
Epoch 180, val loss: 1.0582897663116455
Epoch 190, training loss: 874.2219848632812 = 1.057105541229248 + 100.0 * 8.731648445129395
Epoch 190, val loss: 1.057841420173645
Epoch 200, training loss: 873.3845825195312 = 1.056732416152954 + 100.0 * 8.723278045654297
Epoch 200, val loss: 1.057477593421936
Epoch 210, training loss: 872.6459350585938 = 1.0564292669296265 + 100.0 * 8.71589469909668
Epoch 210, val loss: 1.0571757555007935
Epoch 220, training loss: 871.8919677734375 = 1.0561872720718384 + 100.0 * 8.708357810974121
Epoch 220, val loss: 1.0569359064102173
Epoch 230, training loss: 871.1307983398438 = 1.0559884309768677 + 100.0 * 8.700748443603516
Epoch 230, val loss: 1.0567426681518555
Epoch 240, training loss: 870.498291015625 = 1.0558388233184814 + 100.0 * 8.694424629211426
Epoch 240, val loss: 1.0565943717956543
Epoch 250, training loss: 870.1132202148438 = 1.055724024772644 + 100.0 * 8.690574645996094
Epoch 250, val loss: 1.0564831495285034
Epoch 260, training loss: 869.427490234375 = 1.0556236505508423 + 100.0 * 8.68371868133545
Epoch 260, val loss: 1.0563836097717285
Epoch 270, training loss: 870.3318481445312 = 1.0555413961410522 + 100.0 * 8.692763328552246
Epoch 270, val loss: 1.0563114881515503
Epoch 280, training loss: 869.3470458984375 = 1.055484652519226 + 100.0 * 8.682915687561035
Epoch 280, val loss: 1.0562628507614136
Epoch 290, training loss: 868.0988159179688 = 1.0554224252700806 + 100.0 * 8.67043399810791
Epoch 290, val loss: 1.0561877489089966
Epoch 300, training loss: 868.1176147460938 = 1.0554131269454956 + 100.0 * 8.670621871948242
Epoch 300, val loss: 1.056180715560913
Epoch 310, training loss: 867.7384643554688 = 1.0553793907165527 + 100.0 * 8.666831016540527
Epoch 310, val loss: 1.0561549663543701
Epoch 320, training loss: 867.5955200195312 = 1.0553585290908813 + 100.0 * 8.665401458740234
Epoch 320, val loss: 1.0561299324035645
Epoch 330, training loss: 867.3818969726562 = 1.0553340911865234 + 100.0 * 8.663265228271484
Epoch 330, val loss: 1.0561097860336304
Epoch 340, training loss: 867.25390625 = 1.0553083419799805 + 100.0 * 8.661986351013184
Epoch 340, val loss: 1.0560857057571411
Epoch 350, training loss: 866.7584228515625 = 1.0552818775177002 + 100.0 * 8.657031059265137
Epoch 350, val loss: 1.0560635328292847
Epoch 360, training loss: 866.8823852539062 = 1.0552754402160645 + 100.0 * 8.658270835876465
Epoch 360, val loss: 1.05605149269104
Epoch 370, training loss: 866.4014892578125 = 1.055241346359253 + 100.0 * 8.653462409973145
Epoch 370, val loss: 1.0560253858566284
Epoch 380, training loss: 866.4475708007812 = 1.0552244186401367 + 100.0 * 8.653923034667969
Epoch 380, val loss: 1.0560110807418823
Epoch 390, training loss: 866.4487915039062 = 1.0552178621292114 + 100.0 * 8.653935432434082
Epoch 390, val loss: 1.0560104846954346
Epoch 400, training loss: 866.7222900390625 = 1.0552079677581787 + 100.0 * 8.656670570373535
Epoch 400, val loss: 1.0559955835342407
Epoch 410, training loss: 866.67626953125 = 1.0551884174346924 + 100.0 * 8.656210899353027
Epoch 410, val loss: 1.0559799671173096
Epoch 420, training loss: 866.3497314453125 = 1.0551615953445435 + 100.0 * 8.652945518493652
Epoch 420, val loss: 1.0559543371200562
Epoch 430, training loss: 866.5482788085938 = 1.0551562309265137 + 100.0 * 8.65493106842041
Epoch 430, val loss: 1.0559483766555786
Epoch 440, training loss: 866.6007690429688 = 1.055141568183899 + 100.0 * 8.65545654296875
Epoch 440, val loss: 1.055934190750122
Epoch 450, training loss: 866.7969360351562 = 1.0551176071166992 + 100.0 * 8.657418251037598
Epoch 450, val loss: 1.0559104681015015
Epoch 460, training loss: 866.6587524414062 = 1.055116891860962 + 100.0 * 8.656036376953125
Epoch 460, val loss: 1.0559035539627075
Epoch 470, training loss: 866.6757202148438 = 1.0551037788391113 + 100.0 * 8.656206130981445
Epoch 470, val loss: 1.0559033155441284
Epoch 480, training loss: 866.5459594726562 = 1.055040717124939 + 100.0 * 8.654909133911133
Epoch 480, val loss: 1.05584716796875
Epoch 490, training loss: 866.49658203125 = 1.0550645589828491 + 100.0 * 8.654415130615234
Epoch 490, val loss: 1.0558598041534424
Epoch 500, training loss: 867.3643798828125 = 1.0550798177719116 + 100.0 * 8.663093566894531
Epoch 500, val loss: 1.0558797121047974
Epoch 510, training loss: 867.3694458007812 = 1.0550693273544312 + 100.0 * 8.6631441116333
Epoch 510, val loss: 1.0558713674545288
Epoch 520, training loss: 867.3985595703125 = 1.055052638053894 + 100.0 * 8.663434982299805
Epoch 520, val loss: 1.0558565855026245
Epoch 530, training loss: 867.2017822265625 = 1.0550235509872437 + 100.0 * 8.661467552185059
Epoch 530, val loss: 1.0558310747146606
Epoch 540, training loss: 867.8728637695312 = 1.05502188205719 + 100.0 * 8.66817855834961
Epoch 540, val loss: 1.0558257102966309
Epoch 550, training loss: 867.8479614257812 = 1.0550165176391602 + 100.0 * 8.667929649353027
Epoch 550, val loss: 1.0558228492736816
Epoch 560, training loss: 867.748779296875 = 1.0549867153167725 + 100.0 * 8.666937828063965
Epoch 560, val loss: 1.055794358253479
Epoch 570, training loss: 868.1326293945312 = 1.054986834526062 + 100.0 * 8.6707763671875
Epoch 570, val loss: 1.0557931661605835
Epoch 580, training loss: 868.4409790039062 = 1.054978370666504 + 100.0 * 8.673859596252441
Epoch 580, val loss: 1.055788278579712
Epoch 590, training loss: 868.6088256835938 = 1.0549572706222534 + 100.0 * 8.675539016723633
Epoch 590, val loss: 1.0557681322097778
Epoch 600, training loss: 868.8738403320312 = 1.054951548576355 + 100.0 * 8.678189277648926
Epoch 600, val loss: 1.0557615756988525
Epoch 610, training loss: 868.6177978515625 = 1.0549181699752808 + 100.0 * 8.675628662109375
Epoch 610, val loss: 1.0557299852371216
Epoch 620, training loss: 868.6986694335938 = 1.0549249649047852 + 100.0 * 8.676437377929688
Epoch 620, val loss: 1.0557364225387573
Epoch 630, training loss: 868.9951782226562 = 1.0549230575561523 + 100.0 * 8.679402351379395
Epoch 630, val loss: 1.0557317733764648
Epoch 640, training loss: 868.7230224609375 = 1.0548919439315796 + 100.0 * 8.676681518554688
Epoch 640, val loss: 1.0557011365890503
Epoch 650, training loss: 868.89208984375 = 1.0548863410949707 + 100.0 * 8.678372383117676
Epoch 650, val loss: 1.0556994676589966
Epoch 660, training loss: 869.36279296875 = 1.054836630821228 + 100.0 * 8.683079719543457
Epoch 660, val loss: 1.0556426048278809
Epoch 670, training loss: 870.0198974609375 = 1.0548640489578247 + 100.0 * 8.689650535583496
Epoch 670, val loss: 1.0556793212890625
Epoch 680, training loss: 868.729736328125 = 1.05482816696167 + 100.0 * 8.676749229431152
Epoch 680, val loss: 1.0556467771530151
Epoch 690, training loss: 868.8643798828125 = 1.0548206567764282 + 100.0 * 8.678095817565918
Epoch 690, val loss: 1.0556401014328003
Epoch 700, training loss: 869.3504028320312 = 1.0548288822174072 + 100.0 * 8.682955741882324
Epoch 700, val loss: 1.0556467771530151
Epoch 710, training loss: 869.813232421875 = 1.0548295974731445 + 100.0 * 8.687583923339844
Epoch 710, val loss: 1.0556427240371704
Epoch 720, training loss: 869.3778686523438 = 1.0547853708267212 + 100.0 * 8.68323040008545
Epoch 720, val loss: 1.0555979013442993
Epoch 730, training loss: 869.7510375976562 = 1.0547866821289062 + 100.0 * 8.686962127685547
Epoch 730, val loss: 1.0556092262268066
Epoch 740, training loss: 870.287109375 = 1.0547858476638794 + 100.0 * 8.692322731018066
Epoch 740, val loss: 1.0556020736694336
Epoch 750, training loss: 870.3662719726562 = 1.0547727346420288 + 100.0 * 8.693115234375
Epoch 750, val loss: 1.0555870532989502
Epoch 760, training loss: 870.6404418945312 = 1.0547581911087036 + 100.0 * 8.695857048034668
Epoch 760, val loss: 1.0555797815322876
Epoch 770, training loss: 870.819091796875 = 1.0547407865524292 + 100.0 * 8.697643280029297
Epoch 770, val loss: 1.0555644035339355
Epoch 780, training loss: 870.8094482421875 = 1.0547202825546265 + 100.0 * 8.69754695892334
Epoch 780, val loss: 1.0555391311645508
Epoch 790, training loss: 871.14013671875 = 1.054710030555725 + 100.0 * 8.700854301452637
Epoch 790, val loss: 1.0555325746536255
Epoch 800, training loss: 871.1455688476562 = 1.0546904802322388 + 100.0 * 8.700908660888672
Epoch 800, val loss: 1.0555156469345093
Epoch 810, training loss: 871.2113647460938 = 1.0546817779541016 + 100.0 * 8.701566696166992
Epoch 810, val loss: 1.0555120706558228
Epoch 820, training loss: 871.5396118164062 = 1.054672122001648 + 100.0 * 8.704849243164062
Epoch 820, val loss: 1.0554969310760498
Epoch 830, training loss: 871.4603271484375 = 1.054648756980896 + 100.0 * 8.704056739807129
Epoch 830, val loss: 1.0554810762405396
Epoch 840, training loss: 871.794677734375 = 1.054645299911499 + 100.0 * 8.70740032196045
Epoch 840, val loss: 1.0554815530776978
Epoch 850, training loss: 872.1668090820312 = 1.0546375513076782 + 100.0 * 8.711121559143066
Epoch 850, val loss: 1.0554702281951904
Epoch 860, training loss: 871.878173828125 = 1.0546163320541382 + 100.0 * 8.708235740661621
Epoch 860, val loss: 1.0554512739181519
Epoch 870, training loss: 872.3983154296875 = 1.0546135902404785 + 100.0 * 8.7134370803833
Epoch 870, val loss: 1.0554490089416504
Epoch 880, training loss: 872.69580078125 = 1.0545942783355713 + 100.0 * 8.716412544250488
Epoch 880, val loss: 1.055431604385376
Epoch 890, training loss: 872.681640625 = 1.0545679330825806 + 100.0 * 8.716270446777344
Epoch 890, val loss: 1.0554087162017822
Epoch 900, training loss: 872.7227172851562 = 1.0545560121536255 + 100.0 * 8.716681480407715
Epoch 900, val loss: 1.0553988218307495
Epoch 910, training loss: 872.9307861328125 = 1.0545415878295898 + 100.0 * 8.718762397766113
Epoch 910, val loss: 1.0553772449493408
Epoch 920, training loss: 873.0130615234375 = 1.054531216621399 + 100.0 * 8.719585418701172
Epoch 920, val loss: 1.0553689002990723
Epoch 930, training loss: 873.1740112304688 = 1.0545051097869873 + 100.0 * 8.721195220947266
Epoch 930, val loss: 1.0553492307662964
Epoch 940, training loss: 873.2623291015625 = 1.054494023323059 + 100.0 * 8.722078323364258
Epoch 940, val loss: 1.055332064628601
Epoch 950, training loss: 873.4225463867188 = 1.054488182067871 + 100.0 * 8.72368049621582
Epoch 950, val loss: 1.0553311109542847
Epoch 960, training loss: 873.6027221679688 = 1.054470181465149 + 100.0 * 8.725482940673828
Epoch 960, val loss: 1.0553072690963745
Epoch 970, training loss: 873.8790893554688 = 1.054413914680481 + 100.0 * 8.728246688842773
Epoch 970, val loss: 1.055249571800232
Epoch 980, training loss: 873.578857421875 = 1.054399847984314 + 100.0 * 8.725244522094727
Epoch 980, val loss: 1.0552403926849365
Epoch 990, training loss: 872.6787719726562 = 1.0543638467788696 + 100.0 * 8.716243743896484
Epoch 990, val loss: 1.0552119016647339
Epoch 1000, training loss: 873.2247314453125 = 1.0543630123138428 + 100.0 * 8.72170352935791
Epoch 1000, val loss: 1.0552184581756592
Epoch 1010, training loss: 873.968017578125 = 1.0543721914291382 + 100.0 * 8.72913646697998
Epoch 1010, val loss: 1.0552247762680054
Epoch 1020, training loss: 874.3897705078125 = 1.0543749332427979 + 100.0 * 8.733353614807129
Epoch 1020, val loss: 1.0552265644073486
Epoch 1030, training loss: 874.180908203125 = 1.0543445348739624 + 100.0 * 8.731266021728516
Epoch 1030, val loss: 1.0552012920379639
Epoch 1040, training loss: 874.215087890625 = 1.0543171167373657 + 100.0 * 8.731607437133789
Epoch 1040, val loss: 1.0551819801330566
Epoch 1050, training loss: 874.5962524414062 = 1.0543203353881836 + 100.0 * 8.735419273376465
Epoch 1050, val loss: 1.0551819801330566
Epoch 1060, training loss: 874.83056640625 = 1.0543053150177002 + 100.0 * 8.737762451171875
Epoch 1060, val loss: 1.0551667213439941
Epoch 1070, training loss: 874.6592407226562 = 1.054259181022644 + 100.0 * 8.73604965209961
Epoch 1070, val loss: 1.0551021099090576
Epoch 1080, training loss: 875.6829223632812 = 1.0542727708816528 + 100.0 * 8.746286392211914
Epoch 1080, val loss: 1.0551508665084839
Epoch 1090, training loss: 873.3513793945312 = 1.0540963411331177 + 100.0 * 8.722972869873047
Epoch 1090, val loss: 1.0549863576889038
Epoch 1100, training loss: 875.1734008789062 = 1.0542032718658447 + 100.0 * 8.741191864013672
Epoch 1100, val loss: 1.0550713539123535
Epoch 1110, training loss: 875.1507568359375 = 1.0541900396347046 + 100.0 * 8.740965843200684
Epoch 1110, val loss: 1.0550272464752197
Epoch 1120, training loss: 875.6416625976562 = 1.054192304611206 + 100.0 * 8.745874404907227
Epoch 1120, val loss: 1.055052399635315
Epoch 1130, training loss: 876.138427734375 = 1.0542101860046387 + 100.0 * 8.750842094421387
Epoch 1130, val loss: 1.0550769567489624
Epoch 1140, training loss: 876.706298828125 = 1.0542230606079102 + 100.0 * 8.756521224975586
Epoch 1140, val loss: 1.0550901889801025
Epoch 1150, training loss: 877.2171630859375 = 1.0542374849319458 + 100.0 * 8.761629104614258
Epoch 1150, val loss: 1.0551037788391113
Epoch 1160, training loss: 877.013671875 = 1.05420982837677 + 100.0 * 8.759594917297363
Epoch 1160, val loss: 1.0550786256790161
Epoch 1170, training loss: 877.538330078125 = 1.0542047023773193 + 100.0 * 8.764841079711914
Epoch 1170, val loss: 1.0550779104232788
Epoch 1180, training loss: 877.9448852539062 = 1.054201602935791 + 100.0 * 8.768906593322754
Epoch 1180, val loss: 1.055068850517273
Epoch 1190, training loss: 878.0670166015625 = 1.0541796684265137 + 100.0 * 8.77012825012207
Epoch 1190, val loss: 1.0550458431243896
Epoch 1200, training loss: 878.1622314453125 = 1.0541534423828125 + 100.0 * 8.77108097076416
Epoch 1200, val loss: 1.0550271272659302
Epoch 1210, training loss: 878.5859375 = 1.0541441440582275 + 100.0 * 8.775318145751953
Epoch 1210, val loss: 1.0550214052200317
Epoch 1220, training loss: 878.7462768554688 = 1.054121971130371 + 100.0 * 8.776921272277832
Epoch 1220, val loss: 1.0549968481063843
Epoch 1230, training loss: 878.2225341796875 = 1.054075837135315 + 100.0 * 8.771684646606445
Epoch 1230, val loss: 1.0549637079238892
Epoch 1240, training loss: 878.6898193359375 = 1.0540697574615479 + 100.0 * 8.776357650756836
Epoch 1240, val loss: 1.054961919784546
Epoch 1250, training loss: 879.2545166015625 = 1.0540704727172852 + 100.0 * 8.782004356384277
Epoch 1250, val loss: 1.0549594163894653
Epoch 1260, training loss: 879.5272827148438 = 1.0540536642074585 + 100.0 * 8.7847318649292
Epoch 1260, val loss: 1.0549366474151611
Epoch 1270, training loss: 879.5307006835938 = 1.0540294647216797 + 100.0 * 8.784767150878906
Epoch 1270, val loss: 1.0549103021621704
Epoch 1280, training loss: 879.8243408203125 = 1.0540143251419067 + 100.0 * 8.787703514099121
Epoch 1280, val loss: 1.0549062490463257
Epoch 1290, training loss: 880.151123046875 = 1.0540059804916382 + 100.0 * 8.790970802307129
Epoch 1290, val loss: 1.0548934936523438
Epoch 1300, training loss: 880.0531005859375 = 1.0539699792861938 + 100.0 * 8.78999137878418
Epoch 1300, val loss: 1.0548630952835083
Epoch 1310, training loss: 880.48193359375 = 1.053955316543579 + 100.0 * 8.794280052185059
Epoch 1310, val loss: 1.0548521280288696
Epoch 1320, training loss: 880.410888671875 = 1.0539345741271973 + 100.0 * 8.793569564819336
Epoch 1320, val loss: 1.0548272132873535
Epoch 1330, training loss: 880.3480834960938 = 1.053911566734314 + 100.0 * 8.79294204711914
Epoch 1330, val loss: 1.0548166036605835
Epoch 1340, training loss: 881.00048828125 = 1.0539113283157349 + 100.0 * 8.799466133117676
Epoch 1340, val loss: 1.054811954498291
Epoch 1350, training loss: 881.0535278320312 = 1.0538843870162964 + 100.0 * 8.799996376037598
Epoch 1350, val loss: 1.0547884702682495
Epoch 1360, training loss: 881.037353515625 = 1.0538530349731445 + 100.0 * 8.799835205078125
Epoch 1360, val loss: 1.0547598600387573
Epoch 1370, training loss: 881.3304443359375 = 1.0538408756256104 + 100.0 * 8.802765846252441
Epoch 1370, val loss: 1.0547490119934082
Epoch 1380, training loss: 881.7083740234375 = 1.0538324117660522 + 100.0 * 8.80654525756836
Epoch 1380, val loss: 1.0547380447387695
Epoch 1390, training loss: 881.6918334960938 = 1.0538095235824585 + 100.0 * 8.806380271911621
Epoch 1390, val loss: 1.054713487625122
Epoch 1400, training loss: 881.6552734375 = 1.0537757873535156 + 100.0 * 8.806015014648438
Epoch 1400, val loss: 1.0546908378601074
Epoch 1410, training loss: 881.8949584960938 = 1.053762674331665 + 100.0 * 8.808411598205566
Epoch 1410, val loss: 1.0546778440475464
Epoch 1420, training loss: 882.1174926757812 = 1.0537505149841309 + 100.0 * 8.810637474060059
Epoch 1420, val loss: 1.0546703338623047
Epoch 1430, training loss: 882.166015625 = 1.0537177324295044 + 100.0 * 8.81112289428711
Epoch 1430, val loss: 1.054639458656311
Epoch 1440, training loss: 882.0884399414062 = 1.0536805391311646 + 100.0 * 8.810347557067871
Epoch 1440, val loss: 1.0546101331710815
Epoch 1450, training loss: 881.88671875 = 1.0536315441131592 + 100.0 * 8.808330535888672
Epoch 1450, val loss: 1.0545539855957031
Epoch 1460, training loss: 881.9368286132812 = 1.0536185503005981 + 100.0 * 8.808832168579102
Epoch 1460, val loss: 1.054542899131775
Epoch 1470, training loss: 882.5056762695312 = 1.0536154508590698 + 100.0 * 8.814520835876465
Epoch 1470, val loss: 1.0545532703399658
Epoch 1480, training loss: 883.0625 = 1.053618311882019 + 100.0 * 8.820089340209961
Epoch 1480, val loss: 1.05455482006073
Epoch 1490, training loss: 883.1470947265625 = 1.0535941123962402 + 100.0 * 8.820935249328613
Epoch 1490, val loss: 1.0545284748077393
Epoch 1500, training loss: 883.1309204101562 = 1.0535613298416138 + 100.0 * 8.820773124694824
Epoch 1500, val loss: 1.05449378490448
Epoch 1510, training loss: 883.4425048828125 = 1.0535393953323364 + 100.0 * 8.82388973236084
Epoch 1510, val loss: 1.0544837713241577
Epoch 1520, training loss: 883.4459228515625 = 1.053507924079895 + 100.0 * 8.82392406463623
Epoch 1520, val loss: 1.0544443130493164
Epoch 1530, training loss: 883.077392578125 = 1.0534512996673584 + 100.0 * 8.820239067077637
Epoch 1530, val loss: 1.0543873310089111
Epoch 1540, training loss: 883.1358642578125 = 1.0534166097640991 + 100.0 * 8.82082462310791
Epoch 1540, val loss: 1.0543708801269531
Epoch 1550, training loss: 883.66455078125 = 1.053437352180481 + 100.0 * 8.82611083984375
Epoch 1550, val loss: 1.0543887615203857
Epoch 1560, training loss: 883.8511962890625 = 1.0534154176712036 + 100.0 * 8.827978134155273
Epoch 1560, val loss: 1.0543673038482666
Epoch 1570, training loss: 883.5577392578125 = 1.0533276796340942 + 100.0 * 8.825043678283691
Epoch 1570, val loss: 1.0542899370193481
Epoch 1580, training loss: 883.256103515625 = 1.053240418434143 + 100.0 * 8.822029113769531
Epoch 1580, val loss: 1.0542232990264893
Epoch 1590, training loss: 882.7652587890625 = 1.0532264709472656 + 100.0 * 8.817120552062988
Epoch 1590, val loss: 1.0541908740997314
Epoch 1600, training loss: 882.9320678710938 = 1.0532184839248657 + 100.0 * 8.818788528442383
Epoch 1600, val loss: 1.0541733503341675
Epoch 1610, training loss: 883.59326171875 = 1.053226351737976 + 100.0 * 8.825400352478027
Epoch 1610, val loss: 1.054182767868042
Epoch 1620, training loss: 883.9203491210938 = 1.0532190799713135 + 100.0 * 8.8286714553833
Epoch 1620, val loss: 1.0541797876358032
Epoch 1630, training loss: 884.2662353515625 = 1.053216814994812 + 100.0 * 8.832130432128906
Epoch 1630, val loss: 1.0541895627975464
Epoch 1640, training loss: 884.9240112304688 = 1.0532209873199463 + 100.0 * 8.83870792388916
Epoch 1640, val loss: 1.0541925430297852
Epoch 1650, training loss: 884.9591674804688 = 1.0531914234161377 + 100.0 * 8.839059829711914
Epoch 1650, val loss: 1.0541642904281616
Epoch 1660, training loss: 885.0263061523438 = 1.053167700767517 + 100.0 * 8.839731216430664
Epoch 1660, val loss: 1.0541424751281738
Epoch 1670, training loss: 885.6484375 = 1.0531671047210693 + 100.0 * 8.845952987670898
Epoch 1670, val loss: 1.054135799407959
Epoch 1680, training loss: 885.67431640625 = 1.0531339645385742 + 100.0 * 8.846211433410645
Epoch 1680, val loss: 1.0540977716445923
Epoch 1690, training loss: 884.5426025390625 = 1.052974820137024 + 100.0 * 8.834896087646484
Epoch 1690, val loss: 1.053968071937561
Epoch 1700, training loss: 884.528076171875 = 1.0529557466506958 + 100.0 * 8.83475112915039
Epoch 1700, val loss: 1.053950548171997
Epoch 1710, training loss: 884.6083374023438 = 1.0529353618621826 + 100.0 * 8.835554122924805
Epoch 1710, val loss: 1.0539312362670898
Epoch 1720, training loss: 885.1678466796875 = 1.0529415607452393 + 100.0 * 8.84114933013916
Epoch 1720, val loss: 1.0539350509643555
Epoch 1730, training loss: 886.0252685546875 = 1.0529711246490479 + 100.0 * 8.849722862243652
Epoch 1730, val loss: 1.0539615154266357
Epoch 1740, training loss: 886.6930541992188 = 1.0529733896255493 + 100.0 * 8.856400489807129
Epoch 1740, val loss: 1.0539374351501465
Epoch 1750, training loss: 886.2326049804688 = 1.0529199838638306 + 100.0 * 8.851797103881836
Epoch 1750, val loss: 1.0538984537124634
Epoch 1760, training loss: 886.6239013671875 = 1.052897334098816 + 100.0 * 8.85571002960205
Epoch 1760, val loss: 1.0538827180862427
Epoch 1770, training loss: 886.9200439453125 = 1.0528780221939087 + 100.0 * 8.858672142028809
Epoch 1770, val loss: 1.0538676977157593
Epoch 1780, training loss: 887.1158447265625 = 1.0528545379638672 + 100.0 * 8.86063003540039
Epoch 1780, val loss: 1.0538504123687744
Epoch 1790, training loss: 887.1129150390625 = 1.0528205633163452 + 100.0 * 8.860601425170898
Epoch 1790, val loss: 1.0538111925125122
Epoch 1800, training loss: 887.3126220703125 = 1.0527915954589844 + 100.0 * 8.862598419189453
Epoch 1800, val loss: 1.0537859201431274
Epoch 1810, training loss: 887.349365234375 = 1.0527571439743042 + 100.0 * 8.862966537475586
Epoch 1810, val loss: 1.0537515878677368
Epoch 1820, training loss: 887.5693969726562 = 1.0527352094650269 + 100.0 * 8.865166664123535
Epoch 1820, val loss: 1.0537303686141968
Epoch 1830, training loss: 887.6837158203125 = 1.0527098178863525 + 100.0 * 8.866310119628906
Epoch 1830, val loss: 1.0537075996398926
Epoch 1840, training loss: 887.9389038085938 = 1.052686095237732 + 100.0 * 8.86886215209961
Epoch 1840, val loss: 1.0536797046661377
Epoch 1850, training loss: 887.8758544921875 = 1.0526423454284668 + 100.0 * 8.868232727050781
Epoch 1850, val loss: 1.0536444187164307
Epoch 1860, training loss: 887.3516845703125 = 1.0525692701339722 + 100.0 * 8.862991333007812
Epoch 1860, val loss: 1.0535627603530884
Epoch 1870, training loss: 887.0450439453125 = 1.0525165796279907 + 100.0 * 8.859925270080566
Epoch 1870, val loss: 1.0535396337509155
Epoch 1880, training loss: 887.85546875 = 1.0525171756744385 + 100.0 * 8.868029594421387
Epoch 1880, val loss: 1.0535458326339722
Epoch 1890, training loss: 888.6883544921875 = 1.0525294542312622 + 100.0 * 8.876358032226562
Epoch 1890, val loss: 1.053549885749817
Epoch 1900, training loss: 887.5494995117188 = 1.052391767501831 + 100.0 * 8.864971160888672
Epoch 1900, val loss: 1.0534095764160156
Epoch 1910, training loss: 887.7091064453125 = 1.0523546934127808 + 100.0 * 8.866567611694336
Epoch 1910, val loss: 1.053378939628601
Epoch 1920, training loss: 884.8306274414062 = 1.0520602464675903 + 100.0 * 8.837785720825195
Epoch 1920, val loss: 1.0531522035598755
Epoch 1930, training loss: 886.001220703125 = 1.0520987510681152 + 100.0 * 8.849491119384766
Epoch 1930, val loss: 1.0531333684921265
Epoch 1940, training loss: 886.9942626953125 = 1.0521663427352905 + 100.0 * 8.859420776367188
Epoch 1940, val loss: 1.0532063245773315
Epoch 1950, training loss: 887.4781494140625 = 1.0521748065948486 + 100.0 * 8.864259719848633
Epoch 1950, val loss: 1.0532273054122925
Epoch 1960, training loss: 888.003173828125 = 1.0521851778030396 + 100.0 * 8.86950969696045
Epoch 1960, val loss: 1.0532381534576416
Epoch 1970, training loss: 888.760009765625 = 1.052196741104126 + 100.0 * 8.87707805633545
Epoch 1970, val loss: 1.0532512664794922
Epoch 1980, training loss: 889.0255737304688 = 1.0521904230117798 + 100.0 * 8.87973403930664
Epoch 1980, val loss: 1.0532315969467163
Epoch 1990, training loss: 889.0867309570312 = 1.0521477460861206 + 100.0 * 8.880346298217773
Epoch 1990, val loss: 1.053192138671875
Epoch 2000, training loss: 889.2275390625 = 1.052114486694336 + 100.0 * 8.881753921508789
Epoch 2000, val loss: 1.0531617403030396
Epoch 2010, training loss: 889.5376586914062 = 1.05209481716156 + 100.0 * 8.884855270385742
Epoch 2010, val loss: 1.0531445741653442
Epoch 2020, training loss: 889.588134765625 = 1.0520577430725098 + 100.0 * 8.885360717773438
Epoch 2020, val loss: 1.0531047582626343
Epoch 2030, training loss: 889.6627197265625 = 1.0520190000534058 + 100.0 * 8.886107444763184
Epoch 2030, val loss: 1.053070068359375
Epoch 2040, training loss: 889.259765625 = 1.0519535541534424 + 100.0 * 8.882078170776367
Epoch 2040, val loss: 1.0530025959014893
Epoch 2050, training loss: 889.6928100585938 = 1.0519384145736694 + 100.0 * 8.886408805847168
Epoch 2050, val loss: 1.052993655204773
Epoch 2060, training loss: 890.3609008789062 = 1.0519306659698486 + 100.0 * 8.893089294433594
Epoch 2060, val loss: 1.0529876947402954
Epoch 2070, training loss: 890.2794799804688 = 1.0518780946731567 + 100.0 * 8.8922758102417
Epoch 2070, val loss: 1.0529371500015259
Epoch 2080, training loss: 890.1760864257812 = 1.051822543144226 + 100.0 * 8.891242980957031
Epoch 2080, val loss: 1.0528868436813354
Epoch 2090, training loss: 889.8648681640625 = 1.0517436265945435 + 100.0 * 8.888131141662598
Epoch 2090, val loss: 1.0528138875961304
Epoch 2100, training loss: 889.9649658203125 = 1.0517263412475586 + 100.0 * 8.889132499694824
Epoch 2100, val loss: 1.052799940109253
Epoch 2110, training loss: 890.6942138671875 = 1.0517222881317139 + 100.0 * 8.896425247192383
Epoch 2110, val loss: 1.0528037548065186
Epoch 2120, training loss: 891.2756958007812 = 1.0517152547836304 + 100.0 * 8.902239799499512
Epoch 2120, val loss: 1.0527846813201904
Epoch 2130, training loss: 890.7919921875 = 1.0516467094421387 + 100.0 * 8.897403717041016
Epoch 2130, val loss: 1.0527154207229614
Epoch 2140, training loss: 889.9244384765625 = 1.0515263080596924 + 100.0 * 8.888729095458984
Epoch 2140, val loss: 1.0526117086410522
Epoch 2150, training loss: 890.1104125976562 = 1.0514781475067139 + 100.0 * 8.890589714050293
Epoch 2150, val loss: 1.0525699853897095
Epoch 2160, training loss: 890.5316772460938 = 1.0514791011810303 + 100.0 * 8.89480209350586
Epoch 2160, val loss: 1.0525736808776855
Epoch 2170, training loss: 891.310791015625 = 1.0514739751815796 + 100.0 * 8.902593612670898
Epoch 2170, val loss: 1.0525785684585571
Epoch 2180, training loss: 891.6969604492188 = 1.0514713525772095 + 100.0 * 8.906455039978027
Epoch 2180, val loss: 1.052564024925232
Epoch 2190, training loss: 891.7699584960938 = 1.0514212846755981 + 100.0 * 8.907185554504395
Epoch 2190, val loss: 1.0525044202804565
Epoch 2200, training loss: 891.6424560546875 = 1.0513697862625122 + 100.0 * 8.90591049194336
Epoch 2200, val loss: 1.052467942237854
Epoch 2210, training loss: 891.762939453125 = 1.051322102546692 + 100.0 * 8.907115936279297
Epoch 2210, val loss: 1.052436113357544
Epoch 2220, training loss: 892.1788330078125 = 1.0513052940368652 + 100.0 * 8.911274909973145
Epoch 2220, val loss: 1.0524115562438965
Epoch 2230, training loss: 892.09423828125 = 1.0512440204620361 + 100.0 * 8.910429954528809
Epoch 2230, val loss: 1.0523377656936646
Epoch 2240, training loss: 891.840576171875 = 1.0511770248413086 + 100.0 * 8.907894134521484
Epoch 2240, val loss: 1.0522902011871338
Epoch 2250, training loss: 892.1220092773438 = 1.051142930984497 + 100.0 * 8.9107084274292
Epoch 2250, val loss: 1.0522700548171997
Epoch 2260, training loss: 892.3695678710938 = 1.0511060953140259 + 100.0 * 8.913185119628906
Epoch 2260, val loss: 1.0522208213806152
Epoch 2270, training loss: 892.1840209960938 = 1.0510295629501343 + 100.0 * 8.911330223083496
Epoch 2270, val loss: 1.0521563291549683
Epoch 2280, training loss: 891.8106689453125 = 1.050927758216858 + 100.0 * 8.907597541809082
Epoch 2280, val loss: 1.0520753860473633
Epoch 2290, training loss: 891.9481811523438 = 1.0508912801742554 + 100.0 * 8.90897274017334
Epoch 2290, val loss: 1.0520362854003906
Epoch 2300, training loss: 892.420654296875 = 1.0508813858032227 + 100.0 * 8.913697242736816
Epoch 2300, val loss: 1.0520274639129639
Epoch 2310, training loss: 892.7202758789062 = 1.0508699417114258 + 100.0 * 8.916694641113281
Epoch 2310, val loss: 1.0520260334014893
Epoch 2320, training loss: 892.8936157226562 = 1.0508081912994385 + 100.0 * 8.918428421020508
Epoch 2320, val loss: 1.0519778728485107
Epoch 2330, training loss: 892.904296875 = 1.0507675409317017 + 100.0 * 8.918535232543945
Epoch 2330, val loss: 1.0519269704818726
Epoch 2340, training loss: 893.1362915039062 = 1.0506982803344727 + 100.0 * 8.920855522155762
Epoch 2340, val loss: 1.0518749952316284
Epoch 2350, training loss: 892.8424682617188 = 1.0506365299224854 + 100.0 * 8.91791820526123
Epoch 2350, val loss: 1.0518137216567993
Epoch 2360, training loss: 892.5487060546875 = 1.0505571365356445 + 100.0 * 8.914981842041016
Epoch 2360, val loss: 1.0517429113388062
Epoch 2370, training loss: 892.0460815429688 = 1.0504335165023804 + 100.0 * 8.909956932067871
Epoch 2370, val loss: 1.051617980003357
Epoch 2380, training loss: 892.33154296875 = 1.050409197807312 + 100.0 * 8.912811279296875
Epoch 2380, val loss: 1.0516163110733032
Epoch 2390, training loss: 893.0643310546875 = 1.0504186153411865 + 100.0 * 8.92013931274414
Epoch 2390, val loss: 1.0516091585159302
Epoch 2400, training loss: 893.5299072265625 = 1.0504146814346313 + 100.0 * 8.924795150756836
Epoch 2400, val loss: 1.0516151189804077
Epoch 2410, training loss: 893.868408203125 = 1.0503928661346436 + 100.0 * 8.928179740905762
Epoch 2410, val loss: 1.051589012145996
Epoch 2420, training loss: 892.940185546875 = 1.050228238105774 + 100.0 * 8.918899536132812
Epoch 2420, val loss: 1.0514354705810547
Epoch 2430, training loss: 892.6566772460938 = 1.050153136253357 + 100.0 * 8.916065216064453
Epoch 2430, val loss: 1.0513701438903809
Epoch 2440, training loss: 893.423095703125 = 1.050168514251709 + 100.0 * 8.923728942871094
Epoch 2440, val loss: 1.0513849258422852
Epoch 2450, training loss: 894.0494384765625 = 1.0501642227172852 + 100.0 * 8.92999267578125
Epoch 2450, val loss: 1.0513819456100464
Epoch 2460, training loss: 893.9940795898438 = 1.0500907897949219 + 100.0 * 8.929439544677734
Epoch 2460, val loss: 1.051299810409546
Epoch 2470, training loss: 893.8416748046875 = 1.050020456314087 + 100.0 * 8.927916526794434
Epoch 2470, val loss: 1.051252007484436
Epoch 2480, training loss: 893.8527221679688 = 1.0499693155288696 + 100.0 * 8.928027153015137
Epoch 2480, val loss: 1.051206111907959
Epoch 2490, training loss: 894.0111694335938 = 1.0499292612075806 + 100.0 * 8.929612159729004
Epoch 2490, val loss: 1.0511624813079834
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.4076811594202898
0.8125769760197059
=== training gcn model ===
Epoch 0, training loss: 1000.64599609375 = 1.112795352935791 + 100.0 * 9.995331764221191
Epoch 0, val loss: 1.1121383905410767
Epoch 10, training loss: 955.544677734375 = 1.1085822582244873 + 100.0 * 9.544361114501953
Epoch 10, val loss: 1.1078970432281494
Epoch 20, training loss: 941.131103515625 = 1.1044119596481323 + 100.0 * 9.400266647338867
Epoch 20, val loss: 1.1037468910217285
Epoch 30, training loss: 930.2725830078125 = 1.1004517078399658 + 100.0 * 9.29172134399414
Epoch 30, val loss: 1.0998046398162842
Epoch 40, training loss: 922.0934448242188 = 1.0967048406600952 + 100.0 * 9.209967613220215
Epoch 40, val loss: 1.0960807800292969
Epoch 50, training loss: 915.3001098632812 = 1.093186616897583 + 100.0 * 9.142068862915039
Epoch 50, val loss: 1.0925827026367188
Epoch 60, training loss: 909.5150756835938 = 1.0898653268814087 + 100.0 * 9.08425235748291
Epoch 60, val loss: 1.089282751083374
Epoch 70, training loss: 904.4628295898438 = 1.0867538452148438 + 100.0 * 9.033761024475098
Epoch 70, val loss: 1.086193561553955
Epoch 80, training loss: 900.0579833984375 = 1.0838366746902466 + 100.0 * 8.989741325378418
Epoch 80, val loss: 1.0833020210266113
Epoch 90, training loss: 896.1765747070312 = 1.0810993909835815 + 100.0 * 8.95095443725586
Epoch 90, val loss: 1.0805917978286743
Epoch 100, training loss: 892.7238159179688 = 1.0785391330718994 + 100.0 * 8.916452407836914
Epoch 100, val loss: 1.0780586004257202
Epoch 110, training loss: 889.7335205078125 = 1.0761439800262451 + 100.0 * 8.886573791503906
Epoch 110, val loss: 1.0756937265396118
Epoch 120, training loss: 887.1921997070312 = 1.0739189386367798 + 100.0 * 8.861183166503906
Epoch 120, val loss: 1.073502779006958
Epoch 130, training loss: 884.765625 = 1.0718562602996826 + 100.0 * 8.83693790435791
Epoch 130, val loss: 1.0714746713638306
Epoch 140, training loss: 882.8092041015625 = 1.069949746131897 + 100.0 * 8.817392349243164
Epoch 140, val loss: 1.0696078538894653
Epoch 150, training loss: 881.1259765625 = 1.0681936740875244 + 100.0 * 8.800578117370605
Epoch 150, val loss: 1.067897081375122
Epoch 160, training loss: 879.4407958984375 = 1.066589117050171 + 100.0 * 8.78374195098877
Epoch 160, val loss: 1.0663338899612427
Epoch 170, training loss: 878.0874633789062 = 1.065129041671753 + 100.0 * 8.770223617553711
Epoch 170, val loss: 1.064923644065857
Epoch 180, training loss: 876.6709594726562 = 1.0638028383255005 + 100.0 * 8.756072044372559
Epoch 180, val loss: 1.0636435747146606
Epoch 190, training loss: 875.5294189453125 = 1.0626096725463867 + 100.0 * 8.744668006896973
Epoch 190, val loss: 1.0625038146972656
Epoch 200, training loss: 874.5166625976562 = 1.0615317821502686 + 100.0 * 8.734551429748535
Epoch 200, val loss: 1.0614840984344482
Epoch 210, training loss: 873.6700439453125 = 1.060580849647522 + 100.0 * 8.726094245910645
Epoch 210, val loss: 1.0605854988098145
Epoch 220, training loss: 873.0086669921875 = 1.059739112854004 + 100.0 * 8.719489097595215
Epoch 220, val loss: 1.059802532196045
Epoch 230, training loss: 872.4369506835938 = 1.059020757675171 + 100.0 * 8.71377944946289
Epoch 230, val loss: 1.0591391324996948
Epoch 240, training loss: 872.388916015625 = 1.058335304260254 + 100.0 * 8.713305473327637
Epoch 240, val loss: 1.0584999322891235
Epoch 250, training loss: 871.5371704101562 = 1.0578160285949707 + 100.0 * 8.704793930053711
Epoch 250, val loss: 1.0580322742462158
Epoch 260, training loss: 870.8676147460938 = 1.0573325157165527 + 100.0 * 8.698102951049805
Epoch 260, val loss: 1.0576012134552002
Epoch 270, training loss: 870.4454956054688 = 1.0569310188293457 + 100.0 * 8.693885803222656
Epoch 270, val loss: 1.057245135307312
Epoch 280, training loss: 870.1574096679688 = 1.05658757686615 + 100.0 * 8.691008567810059
Epoch 280, val loss: 1.0569429397583008
Epoch 290, training loss: 869.7954711914062 = 1.0562952756881714 + 100.0 * 8.687392234802246
Epoch 290, val loss: 1.0566946268081665
Epoch 300, training loss: 869.6582641601562 = 1.056063175201416 + 100.0 * 8.68602180480957
Epoch 300, val loss: 1.0565025806427002
Epoch 310, training loss: 869.6397705078125 = 1.055881142616272 + 100.0 * 8.68583869934082
Epoch 310, val loss: 1.0563424825668335
Epoch 320, training loss: 869.2277221679688 = 1.0557048320770264 + 100.0 * 8.681719779968262
Epoch 320, val loss: 1.0562026500701904
Epoch 330, training loss: 868.6746826171875 = 1.055568814277649 + 100.0 * 8.676191329956055
Epoch 330, val loss: 1.056098461151123
Epoch 340, training loss: 868.6553344726562 = 1.0554537773132324 + 100.0 * 8.67599868774414
Epoch 340, val loss: 1.0560154914855957
Epoch 350, training loss: 868.2297973632812 = 1.055364727973938 + 100.0 * 8.671744346618652
Epoch 350, val loss: 1.0559309720993042
Epoch 360, training loss: 868.0841064453125 = 1.055288314819336 + 100.0 * 8.6702880859375
Epoch 360, val loss: 1.055882215499878
Epoch 370, training loss: 868.2266235351562 = 1.055248498916626 + 100.0 * 8.671713829040527
Epoch 370, val loss: 1.0558534860610962
Epoch 380, training loss: 868.3548583984375 = 1.0551989078521729 + 100.0 * 8.672996520996094
Epoch 380, val loss: 1.0558192729949951
Epoch 390, training loss: 868.6309204101562 = 1.055161714553833 + 100.0 * 8.67575740814209
Epoch 390, val loss: 1.0558055639266968
Epoch 400, training loss: 868.2438354492188 = 1.0551246404647827 + 100.0 * 8.671887397766113
Epoch 400, val loss: 1.0557823181152344
Epoch 410, training loss: 868.0420532226562 = 1.0551005601882935 + 100.0 * 8.669869422912598
Epoch 410, val loss: 1.0557547807693481
Epoch 420, training loss: 867.8861694335938 = 1.05507493019104 + 100.0 * 8.66831111907959
Epoch 420, val loss: 1.0557372570037842
Epoch 430, training loss: 867.99755859375 = 1.0550639629364014 + 100.0 * 8.669425010681152
Epoch 430, val loss: 1.0557231903076172
Epoch 440, training loss: 867.783447265625 = 1.0550274848937988 + 100.0 * 8.66728401184082
Epoch 440, val loss: 1.055702567100525
Epoch 450, training loss: 867.9971313476562 = 1.0550180673599243 + 100.0 * 8.669421195983887
Epoch 450, val loss: 1.0557026863098145
Epoch 460, training loss: 868.2330322265625 = 1.0550075769424438 + 100.0 * 8.671780586242676
Epoch 460, val loss: 1.0556901693344116
Epoch 470, training loss: 867.8523559570312 = 1.0549834966659546 + 100.0 * 8.667973518371582
Epoch 470, val loss: 1.0556670427322388
Epoch 480, training loss: 867.8743286132812 = 1.0549547672271729 + 100.0 * 8.668193817138672
Epoch 480, val loss: 1.0556493997573853
Epoch 490, training loss: 868.0537719726562 = 1.0549548864364624 + 100.0 * 8.669988632202148
Epoch 490, val loss: 1.0556495189666748
Epoch 500, training loss: 868.1566772460938 = 1.0549384355545044 + 100.0 * 8.67101764678955
Epoch 500, val loss: 1.0556273460388184
Epoch 510, training loss: 868.3943481445312 = 1.0549042224884033 + 100.0 * 8.673394203186035
Epoch 510, val loss: 1.0556044578552246
Epoch 520, training loss: 868.1060180664062 = 1.0548936128616333 + 100.0 * 8.670511245727539
Epoch 520, val loss: 1.0555992126464844
Epoch 530, training loss: 868.3019409179688 = 1.0549044609069824 + 100.0 * 8.672470092773438
Epoch 530, val loss: 1.055606722831726
Epoch 540, training loss: 868.2022094726562 = 1.054903268814087 + 100.0 * 8.671472549438477
Epoch 540, val loss: 1.0556010007858276
Epoch 550, training loss: 868.6195678710938 = 1.0548899173736572 + 100.0 * 8.675646781921387
Epoch 550, val loss: 1.0555884838104248
Epoch 560, training loss: 868.5535278320312 = 1.0548723936080933 + 100.0 * 8.674986839294434
Epoch 560, val loss: 1.0555676221847534
Epoch 570, training loss: 868.700439453125 = 1.0548632144927979 + 100.0 * 8.6764554977417
Epoch 570, val loss: 1.0555557012557983
Epoch 580, training loss: 869.0266723632812 = 1.0548468828201294 + 100.0 * 8.679718017578125
Epoch 580, val loss: 1.0555413961410522
Epoch 590, training loss: 869.6851196289062 = 1.054646372795105 + 100.0 * 8.686305046081543
Epoch 590, val loss: 1.0553436279296875
Epoch 600, training loss: 872.3994750976562 = 1.0548287630081177 + 100.0 * 8.713446617126465
Epoch 600, val loss: 1.0555171966552734
Epoch 610, training loss: 869.6264038085938 = 1.0547330379486084 + 100.0 * 8.68571662902832
Epoch 610, val loss: 1.055439829826355
Epoch 620, training loss: 870.4737548828125 = 1.0547630786895752 + 100.0 * 8.69419002532959
Epoch 620, val loss: 1.0554585456848145
Epoch 630, training loss: 870.6779174804688 = 1.054776906967163 + 100.0 * 8.696231842041016
Epoch 630, val loss: 1.0554758310317993
Epoch 640, training loss: 870.8311767578125 = 1.05477774143219 + 100.0 * 8.69776439666748
Epoch 640, val loss: 1.0554778575897217
Epoch 650, training loss: 871.5415649414062 = 1.0547853708267212 + 100.0 * 8.704867362976074
Epoch 650, val loss: 1.055483341217041
Epoch 660, training loss: 872.09326171875 = 1.054781198501587 + 100.0 * 8.710384368896484
Epoch 660, val loss: 1.0554792881011963
Epoch 670, training loss: 872.0679931640625 = 1.054772138595581 + 100.0 * 8.710132598876953
Epoch 670, val loss: 1.0554718971252441
Epoch 680, training loss: 872.2102661132812 = 1.054763913154602 + 100.0 * 8.711555480957031
Epoch 680, val loss: 1.0554596185684204
Epoch 690, training loss: 872.599853515625 = 1.0547605752944946 + 100.0 * 8.71545124053955
Epoch 690, val loss: 1.0554568767547607
Epoch 700, training loss: 872.5704956054688 = 1.054745078086853 + 100.0 * 8.715157508850098
Epoch 700, val loss: 1.0554368495941162
Epoch 710, training loss: 872.69970703125 = 1.0547327995300293 + 100.0 * 8.716449737548828
Epoch 710, val loss: 1.055426001548767
Epoch 720, training loss: 872.9443359375 = 1.0547261238098145 + 100.0 * 8.71889591217041
Epoch 720, val loss: 1.0554171800613403
Epoch 730, training loss: 873.00244140625 = 1.0547096729278564 + 100.0 * 8.719477653503418
Epoch 730, val loss: 1.0554085969924927
Epoch 740, training loss: 873.0931396484375 = 1.054696798324585 + 100.0 * 8.72038459777832
Epoch 740, val loss: 1.0553970336914062
Epoch 750, training loss: 873.6619262695312 = 1.0546960830688477 + 100.0 * 8.726072311401367
Epoch 750, val loss: 1.0553948879241943
Epoch 760, training loss: 873.822021484375 = 1.0546875 + 100.0 * 8.727673530578613
Epoch 760, val loss: 1.055379033088684
Epoch 770, training loss: 873.8432006835938 = 1.0546748638153076 + 100.0 * 8.727885246276855
Epoch 770, val loss: 1.0553696155548096
Epoch 780, training loss: 873.94775390625 = 1.0546578168869019 + 100.0 * 8.728930473327637
Epoch 780, val loss: 1.0553557872772217
Epoch 790, training loss: 874.5128173828125 = 1.0546650886535645 + 100.0 * 8.734580993652344
Epoch 790, val loss: 1.0553563833236694
Epoch 800, training loss: 874.71337890625 = 1.0546406507492065 + 100.0 * 8.736587524414062
Epoch 800, val loss: 1.055326223373413
Epoch 810, training loss: 874.6077270507812 = 1.0546278953552246 + 100.0 * 8.735530853271484
Epoch 810, val loss: 1.0553196668624878
Epoch 820, training loss: 875.069580078125 = 1.0546236038208008 + 100.0 * 8.74014949798584
Epoch 820, val loss: 1.055322527885437
Epoch 830, training loss: 875.806884765625 = 1.0546221733093262 + 100.0 * 8.747522354125977
Epoch 830, val loss: 1.0553102493286133
Epoch 840, training loss: 875.9973754882812 = 1.054602861404419 + 100.0 * 8.749427795410156
Epoch 840, val loss: 1.0553014278411865
Epoch 850, training loss: 875.9463500976562 = 1.0545921325683594 + 100.0 * 8.748917579650879
Epoch 850, val loss: 1.0552875995635986
Epoch 860, training loss: 876.3264770507812 = 1.0545825958251953 + 100.0 * 8.752718925476074
Epoch 860, val loss: 1.0552729368209839
Epoch 870, training loss: 876.487548828125 = 1.0545692443847656 + 100.0 * 8.754329681396484
Epoch 870, val loss: 1.0552637577056885
Epoch 880, training loss: 876.1309204101562 = 1.0545059442520142 + 100.0 * 8.750763893127441
Epoch 880, val loss: 1.0552130937576294
Epoch 890, training loss: 875.9085083007812 = 1.0544962882995605 + 100.0 * 8.748539924621582
Epoch 890, val loss: 1.055199146270752
Epoch 900, training loss: 876.2455444335938 = 1.0545068979263306 + 100.0 * 8.751910209655762
Epoch 900, val loss: 1.05520498752594
Epoch 910, training loss: 876.7221069335938 = 1.0545220375061035 + 100.0 * 8.756675720214844
Epoch 910, val loss: 1.055214762687683
Epoch 920, training loss: 876.66552734375 = 1.0544977188110352 + 100.0 * 8.756110191345215
Epoch 920, val loss: 1.0551961660385132
Epoch 930, training loss: 877.1278686523438 = 1.0544955730438232 + 100.0 * 8.760733604431152
Epoch 930, val loss: 1.0551955699920654
Epoch 940, training loss: 877.162109375 = 1.0544886589050293 + 100.0 * 8.761075973510742
Epoch 940, val loss: 1.0551881790161133
Epoch 950, training loss: 876.8648071289062 = 1.0544602870941162 + 100.0 * 8.758103370666504
Epoch 950, val loss: 1.055162787437439
Epoch 960, training loss: 877.3980712890625 = 1.0544575452804565 + 100.0 * 8.763436317443848
Epoch 960, val loss: 1.0551589727401733
Epoch 970, training loss: 877.837890625 = 1.054448127746582 + 100.0 * 8.767834663391113
Epoch 970, val loss: 1.0551587343215942
Epoch 980, training loss: 877.6676635742188 = 1.0544168949127197 + 100.0 * 8.766132354736328
Epoch 980, val loss: 1.0551183223724365
Epoch 990, training loss: 877.402099609375 = 1.0543946027755737 + 100.0 * 8.763477325439453
Epoch 990, val loss: 1.055078387260437
Epoch 1000, training loss: 877.6150512695312 = 1.05437433719635 + 100.0 * 8.765606880187988
Epoch 1000, val loss: 1.0550768375396729
Epoch 1010, training loss: 878.2392578125 = 1.0543886423110962 + 100.0 * 8.771848678588867
Epoch 1010, val loss: 1.0551056861877441
Epoch 1020, training loss: 878.7933349609375 = 1.0543924570083618 + 100.0 * 8.777389526367188
Epoch 1020, val loss: 1.0550910234451294
Epoch 1030, training loss: 878.5407104492188 = 1.054358959197998 + 100.0 * 8.774863243103027
Epoch 1030, val loss: 1.0550750494003296
Epoch 1040, training loss: 878.8646240234375 = 1.0543370246887207 + 100.0 * 8.77810287475586
Epoch 1040, val loss: 1.0550411939620972
Epoch 1050, training loss: 878.9501342773438 = 1.0543324947357178 + 100.0 * 8.778958320617676
Epoch 1050, val loss: 1.055038571357727
Epoch 1060, training loss: 879.013671875 = 1.0543155670166016 + 100.0 * 8.779593467712402
Epoch 1060, val loss: 1.055034875869751
Epoch 1070, training loss: 879.183349609375 = 1.0543155670166016 + 100.0 * 8.781290054321289
Epoch 1070, val loss: 1.055030107498169
Epoch 1080, training loss: 879.346435546875 = 1.0543020963668823 + 100.0 * 8.782920837402344
Epoch 1080, val loss: 1.0550143718719482
Epoch 1090, training loss: 879.2036743164062 = 1.0542787313461304 + 100.0 * 8.781494140625
Epoch 1090, val loss: 1.0549883842468262
Epoch 1100, training loss: 879.6851806640625 = 1.0542680025100708 + 100.0 * 8.786309242248535
Epoch 1100, val loss: 1.0549808740615845
Epoch 1110, training loss: 879.9647216796875 = 1.0542685985565186 + 100.0 * 8.789104461669922
Epoch 1110, val loss: 1.0549852848052979
Epoch 1120, training loss: 879.8182373046875 = 1.0542453527450562 + 100.0 * 8.787639617919922
Epoch 1120, val loss: 1.0549627542495728
Epoch 1130, training loss: 879.7595825195312 = 1.054220199584961 + 100.0 * 8.787054061889648
Epoch 1130, val loss: 1.0549390316009521
Epoch 1140, training loss: 880.2427368164062 = 1.0542248487472534 + 100.0 * 8.791885375976562
Epoch 1140, val loss: 1.054950475692749
Epoch 1150, training loss: 880.5344848632812 = 1.0542066097259521 + 100.0 * 8.79480266571045
Epoch 1150, val loss: 1.0549254417419434
Epoch 1160, training loss: 880.4630126953125 = 1.0541902780532837 + 100.0 * 8.794088363647461
Epoch 1160, val loss: 1.0549132823944092
Epoch 1170, training loss: 880.8016967773438 = 1.0541776418685913 + 100.0 * 8.79747486114502
Epoch 1170, val loss: 1.0549116134643555
Epoch 1180, training loss: 880.6143188476562 = 1.0541578531265259 + 100.0 * 8.795601844787598
Epoch 1180, val loss: 1.05487060546875
Epoch 1190, training loss: 880.681640625 = 1.054126501083374 + 100.0 * 8.79627513885498
Epoch 1190, val loss: 1.054844856262207
Epoch 1200, training loss: 880.8890991210938 = 1.0541292428970337 + 100.0 * 8.798349380493164
Epoch 1200, val loss: 1.054856538772583
Epoch 1210, training loss: 881.41064453125 = 1.0541245937347412 + 100.0 * 8.80356502532959
Epoch 1210, val loss: 1.0548452138900757
Epoch 1220, training loss: 881.2478637695312 = 1.054095983505249 + 100.0 * 8.8019380569458
Epoch 1220, val loss: 1.0548274517059326
Epoch 1230, training loss: 881.724609375 = 1.0540882349014282 + 100.0 * 8.806705474853516
Epoch 1230, val loss: 1.0548220872879028
Epoch 1240, training loss: 881.7060546875 = 1.0540257692337036 + 100.0 * 8.806520462036133
Epoch 1240, val loss: 1.0547168254852295
Epoch 1250, training loss: 880.9898071289062 = 1.053978681564331 + 100.0 * 8.799358367919922
Epoch 1250, val loss: 1.054697036743164
Epoch 1260, training loss: 879.3864135742188 = 1.0539041757583618 + 100.0 * 8.7833251953125
Epoch 1260, val loss: 1.054642677307129
Epoch 1270, training loss: 879.9284057617188 = 1.053920865058899 + 100.0 * 8.788744926452637
Epoch 1270, val loss: 1.0546540021896362
Epoch 1280, training loss: 881.0164794921875 = 1.0539551973342896 + 100.0 * 8.799625396728516
Epoch 1280, val loss: 1.0546908378601074
Epoch 1290, training loss: 881.6026000976562 = 1.0539690256118774 + 100.0 * 8.805486679077148
Epoch 1290, val loss: 1.0547115802764893
Epoch 1300, training loss: 881.8159790039062 = 1.0539683103561401 + 100.0 * 8.80762004852295
Epoch 1300, val loss: 1.0547106266021729
Epoch 1310, training loss: 881.7960205078125 = 1.0539414882659912 + 100.0 * 8.80742073059082
Epoch 1310, val loss: 1.054679036140442
Epoch 1320, training loss: 881.8771362304688 = 1.053923487663269 + 100.0 * 8.808232307434082
Epoch 1320, val loss: 1.0546666383743286
Epoch 1330, training loss: 882.0607299804688 = 1.0539195537567139 + 100.0 * 8.810068130493164
Epoch 1330, val loss: 1.0546618700027466
Epoch 1340, training loss: 882.3074340820312 = 1.0538909435272217 + 100.0 * 8.812535285949707
Epoch 1340, val loss: 1.054632306098938
Epoch 1350, training loss: 882.2576293945312 = 1.0538727045059204 + 100.0 * 8.812037467956543
Epoch 1350, val loss: 1.0546295642852783
Epoch 1360, training loss: 882.7196655273438 = 1.0538675785064697 + 100.0 * 8.816658020019531
Epoch 1360, val loss: 1.0546163320541382
Epoch 1370, training loss: 881.2793579101562 = 1.0537382364273071 + 100.0 * 8.80225658416748
Epoch 1370, val loss: 1.0545291900634766
Epoch 1380, training loss: 881.2825317382812 = 1.0537195205688477 + 100.0 * 8.802288055419922
Epoch 1380, val loss: 1.0544856786727905
Epoch 1390, training loss: 882.2009887695312 = 1.0537595748901367 + 100.0 * 8.811471939086914
Epoch 1390, val loss: 1.0545095205307007
Epoch 1400, training loss: 882.7801513671875 = 1.053775429725647 + 100.0 * 8.81726360321045
Epoch 1400, val loss: 1.054534912109375
Epoch 1410, training loss: 883.1942138671875 = 1.0537806749343872 + 100.0 * 8.821404457092285
Epoch 1410, val loss: 1.0545278787612915
Epoch 1420, training loss: 882.8917236328125 = 1.0537385940551758 + 100.0 * 8.818380355834961
Epoch 1420, val loss: 1.0544832944869995
Epoch 1430, training loss: 882.8387451171875 = 1.0537045001983643 + 100.0 * 8.817850112915039
Epoch 1430, val loss: 1.05446457862854
Epoch 1440, training loss: 883.0476684570312 = 1.0536911487579346 + 100.0 * 8.819939613342285
Epoch 1440, val loss: 1.054449200630188
Epoch 1450, training loss: 883.4976196289062 = 1.0536911487579346 + 100.0 * 8.82443904876709
Epoch 1450, val loss: 1.0544583797454834
Epoch 1460, training loss: 883.458251953125 = 1.0536446571350098 + 100.0 * 8.82404613494873
Epoch 1460, val loss: 1.0543904304504395
Epoch 1470, training loss: 883.3377685546875 = 1.053611397743225 + 100.0 * 8.82284164428711
Epoch 1470, val loss: 1.0543992519378662
Epoch 1480, training loss: 883.8721313476562 = 1.05362868309021 + 100.0 * 8.828185081481934
Epoch 1480, val loss: 1.0544028282165527
Epoch 1490, training loss: 884.2367553710938 = 1.0536179542541504 + 100.0 * 8.831831932067871
Epoch 1490, val loss: 1.054386019706726
Epoch 1500, training loss: 883.6722412109375 = 1.0535534620285034 + 100.0 * 8.826187133789062
Epoch 1500, val loss: 1.0543192625045776
Epoch 1510, training loss: 883.4686279296875 = 1.053520917892456 + 100.0 * 8.824151039123535
Epoch 1510, val loss: 1.054301381111145
Epoch 1520, training loss: 883.8768920898438 = 1.053521990776062 + 100.0 * 8.82823371887207
Epoch 1520, val loss: 1.0543103218078613
Epoch 1530, training loss: 884.8363037109375 = 1.0535295009613037 + 100.0 * 8.837827682495117
Epoch 1530, val loss: 1.0543123483657837
Epoch 1540, training loss: 884.6024169921875 = 1.0534930229187012 + 100.0 * 8.835489273071289
Epoch 1540, val loss: 1.0542826652526855
Epoch 1550, training loss: 884.9417114257812 = 1.0534963607788086 + 100.0 * 8.838882446289062
Epoch 1550, val loss: 1.054264783859253
Epoch 1560, training loss: 884.3970947265625 = 1.053423523902893 + 100.0 * 8.833436965942383
Epoch 1560, val loss: 1.0542151927947998
Epoch 1570, training loss: 884.9334716796875 = 1.053429365158081 + 100.0 * 8.838800430297852
Epoch 1570, val loss: 1.054212212562561
Epoch 1580, training loss: 885.06884765625 = 1.0534170866012573 + 100.0 * 8.840154647827148
Epoch 1580, val loss: 1.0542104244232178
Epoch 1590, training loss: 885.0203247070312 = 1.0533863306045532 + 100.0 * 8.839669227600098
Epoch 1590, val loss: 1.0541638135910034
Epoch 1600, training loss: 885.1131591796875 = 1.0533480644226074 + 100.0 * 8.840598106384277
Epoch 1600, val loss: 1.0541203022003174
Epoch 1610, training loss: 885.2734375 = 1.0533192157745361 + 100.0 * 8.842201232910156
Epoch 1610, val loss: 1.054111361503601
Epoch 1620, training loss: 885.6710815429688 = 1.0533204078674316 + 100.0 * 8.846177101135254
Epoch 1620, val loss: 1.0541188716888428
Epoch 1630, training loss: 885.6478271484375 = 1.0532937049865723 + 100.0 * 8.845945358276367
Epoch 1630, val loss: 1.0541026592254639
Epoch 1640, training loss: 885.2821044921875 = 1.0532323122024536 + 100.0 * 8.842288970947266
Epoch 1640, val loss: 1.0540519952774048
Epoch 1650, training loss: 885.6947631835938 = 1.0532355308532715 + 100.0 * 8.846415519714355
Epoch 1650, val loss: 1.0540367364883423
Epoch 1660, training loss: 886.3807373046875 = 1.053226351737976 + 100.0 * 8.853275299072266
Epoch 1660, val loss: 1.0540356636047363
Epoch 1670, training loss: 886.5278930664062 = 1.053205966949463 + 100.0 * 8.85474681854248
Epoch 1670, val loss: 1.0540117025375366
Epoch 1680, training loss: 885.8333740234375 = 1.0531374216079712 + 100.0 * 8.84780216217041
Epoch 1680, val loss: 1.0539391040802002
Epoch 1690, training loss: 882.8936767578125 = 1.0528565645217896 + 100.0 * 8.818408012390137
Epoch 1690, val loss: 1.0537300109863281
Epoch 1700, training loss: 883.5003662109375 = 1.0528783798217773 + 100.0 * 8.824475288391113
Epoch 1700, val loss: 1.0536714792251587
Epoch 1710, training loss: 883.8052368164062 = 1.0528932809829712 + 100.0 * 8.827523231506348
Epoch 1710, val loss: 1.0537015199661255
Epoch 1720, training loss: 884.5473022460938 = 1.052946925163269 + 100.0 * 8.834943771362305
Epoch 1720, val loss: 1.0537619590759277
Epoch 1730, training loss: 885.4061279296875 = 1.0529887676239014 + 100.0 * 8.843531608581543
Epoch 1730, val loss: 1.0537996292114258
Epoch 1740, training loss: 885.82763671875 = 1.0530006885528564 + 100.0 * 8.847746849060059
Epoch 1740, val loss: 1.0538179874420166
Epoch 1750, training loss: 885.892333984375 = 1.0529649257659912 + 100.0 * 8.848393440246582
Epoch 1750, val loss: 1.0537861585617065
Epoch 1760, training loss: 886.1348876953125 = 1.0529463291168213 + 100.0 * 8.85081958770752
Epoch 1760, val loss: 1.0537782907485962
Epoch 1770, training loss: 886.4771728515625 = 1.0529322624206543 + 100.0 * 8.854242324829102
Epoch 1770, val loss: 1.0537605285644531
Epoch 1780, training loss: 886.4358520507812 = 1.052898645401001 + 100.0 * 8.853829383850098
Epoch 1780, val loss: 1.0537265539169312
Epoch 1790, training loss: 886.7860717773438 = 1.052877426147461 + 100.0 * 8.857332229614258
Epoch 1790, val loss: 1.0537121295928955
Epoch 1800, training loss: 887.0958251953125 = 1.0528610944747925 + 100.0 * 8.860429763793945
Epoch 1800, val loss: 1.0536926984786987
Epoch 1810, training loss: 887.1712036132812 = 1.0528244972229004 + 100.0 * 8.861184120178223
Epoch 1810, val loss: 1.0536558628082275
Epoch 1820, training loss: 887.063720703125 = 1.0527938604354858 + 100.0 * 8.860109329223633
Epoch 1820, val loss: 1.053637146949768
Epoch 1830, training loss: 887.3681640625 = 1.052771806716919 + 100.0 * 8.863153457641602
Epoch 1830, val loss: 1.0536209344863892
Epoch 1840, training loss: 887.9037475585938 = 1.052758812904358 + 100.0 * 8.868510246276855
Epoch 1840, val loss: 1.0536038875579834
Epoch 1850, training loss: 887.637451171875 = 1.0526988506317139 + 100.0 * 8.86584758758545
Epoch 1850, val loss: 1.0535327196121216
Epoch 1860, training loss: 887.1066284179688 = 1.0526357889175415 + 100.0 * 8.860540390014648
Epoch 1860, val loss: 1.0534961223602295
Epoch 1870, training loss: 887.7232055664062 = 1.0526312589645386 + 100.0 * 8.866705894470215
Epoch 1870, val loss: 1.0534954071044922
Epoch 1880, training loss: 888.10791015625 = 1.0526305437088013 + 100.0 * 8.870553016662598
Epoch 1880, val loss: 1.053491473197937
Epoch 1890, training loss: 888.2808837890625 = 1.0526028871536255 + 100.0 * 8.872282981872559
Epoch 1890, val loss: 1.053471326828003
Epoch 1900, training loss: 888.1820678710938 = 1.0525498390197754 + 100.0 * 8.871294975280762
Epoch 1900, val loss: 1.0534234046936035
Epoch 1910, training loss: 888.1859130859375 = 1.052519679069519 + 100.0 * 8.871334075927734
Epoch 1910, val loss: 1.0533995628356934
Epoch 1920, training loss: 888.4649658203125 = 1.0524985790252686 + 100.0 * 8.874124526977539
Epoch 1920, val loss: 1.0533851385116577
Epoch 1930, training loss: 888.9010620117188 = 1.052484393119812 + 100.0 * 8.878485679626465
Epoch 1930, val loss: 1.0533473491668701
Epoch 1940, training loss: 888.66943359375 = 1.0524299144744873 + 100.0 * 8.87617015838623
Epoch 1940, val loss: 1.0533126592636108
Epoch 1950, training loss: 888.8080444335938 = 1.052388310432434 + 100.0 * 8.877556800842285
Epoch 1950, val loss: 1.053278923034668
Epoch 1960, training loss: 889.0233764648438 = 1.052362322807312 + 100.0 * 8.87971019744873
Epoch 1960, val loss: 1.053260326385498
Epoch 1970, training loss: 889.272216796875 = 1.0523532629013062 + 100.0 * 8.882198333740234
Epoch 1970, val loss: 1.0532536506652832
Epoch 1980, training loss: 889.2183227539062 = 1.0523096323013306 + 100.0 * 8.881660461425781
Epoch 1980, val loss: 1.0532073974609375
Epoch 1990, training loss: 889.010986328125 = 1.0521899461746216 + 100.0 * 8.87958812713623
Epoch 1990, val loss: 1.0531096458435059
Epoch 2000, training loss: 888.275390625 = 1.0521390438079834 + 100.0 * 8.872232437133789
Epoch 2000, val loss: 1.0530532598495483
Epoch 2010, training loss: 888.6280517578125 = 1.0521304607391357 + 100.0 * 8.87575912475586
Epoch 2010, val loss: 1.0530534982681274
Epoch 2020, training loss: 889.3784790039062 = 1.0521445274353027 + 100.0 * 8.88326358795166
Epoch 2020, val loss: 1.0530561208724976
Epoch 2030, training loss: 889.9805297851562 = 1.052146315574646 + 100.0 * 8.889284133911133
Epoch 2030, val loss: 1.0530575513839722
Epoch 2040, training loss: 889.685302734375 = 1.0520884990692139 + 100.0 * 8.886332511901855
Epoch 2040, val loss: 1.0529929399490356
Epoch 2050, training loss: 889.746337890625 = 1.0520412921905518 + 100.0 * 8.886942863464355
Epoch 2050, val loss: 1.0529625415802002
Epoch 2060, training loss: 890.22509765625 = 1.0520318746566772 + 100.0 * 8.891731262207031
Epoch 2060, val loss: 1.0529637336730957
Epoch 2070, training loss: 890.45556640625 = 1.0520002841949463 + 100.0 * 8.894035339355469
Epoch 2070, val loss: 1.052918791770935
Epoch 2080, training loss: 890.32958984375 = 1.051947832107544 + 100.0 * 8.892776489257812
Epoch 2080, val loss: 1.0528693199157715
Epoch 2090, training loss: 890.6192016601562 = 1.0519241094589233 + 100.0 * 8.895672798156738
Epoch 2090, val loss: 1.0528603792190552
Epoch 2100, training loss: 890.9276733398438 = 1.051895022392273 + 100.0 * 8.898757934570312
Epoch 2100, val loss: 1.0528435707092285
Epoch 2110, training loss: 891.0697021484375 = 1.0518641471862793 + 100.0 * 8.900177955627441
Epoch 2110, val loss: 1.0528228282928467
Epoch 2120, training loss: 888.6654663085938 = 1.051358938217163 + 100.0 * 8.876141548156738
Epoch 2120, val loss: 1.0521255731582642
Epoch 2130, training loss: 889.8560180664062 = 1.0515891313552856 + 100.0 * 8.888044357299805
Epoch 2130, val loss: 1.0524663925170898
Epoch 2140, training loss: 888.3131103515625 = 1.0514289140701294 + 100.0 * 8.8726167678833
Epoch 2140, val loss: 1.052364468574524
Epoch 2150, training loss: 888.3641357421875 = 1.0514531135559082 + 100.0 * 8.873126983642578
Epoch 2150, val loss: 1.05240797996521
Epoch 2160, training loss: 888.853515625 = 1.051483392715454 + 100.0 * 8.878020286560059
Epoch 2160, val loss: 1.0524603128433228
Epoch 2170, training loss: 889.5450439453125 = 1.0515109300613403 + 100.0 * 8.88493537902832
Epoch 2170, val loss: 1.0524965524673462
Epoch 2180, training loss: 890.2911376953125 = 1.0515310764312744 + 100.0 * 8.892395973205566
Epoch 2180, val loss: 1.0525264739990234
Epoch 2190, training loss: 890.7623291015625 = 1.0515220165252686 + 100.0 * 8.89710807800293
Epoch 2190, val loss: 1.052517294883728
Epoch 2200, training loss: 890.2916870117188 = 1.051433801651001 + 100.0 * 8.892402648925781
Epoch 2200, val loss: 1.0524344444274902
Epoch 2210, training loss: 890.9894409179688 = 1.0514252185821533 + 100.0 * 8.89937973022461
Epoch 2210, val loss: 1.0524251461029053
Epoch 2220, training loss: 891.29443359375 = 1.051421046257019 + 100.0 * 8.902430534362793
Epoch 2220, val loss: 1.052429437637329
Epoch 2230, training loss: 891.6240234375 = 1.0513811111450195 + 100.0 * 8.905726432800293
Epoch 2230, val loss: 1.0524007081985474
Epoch 2240, training loss: 891.6239013671875 = 1.0513310432434082 + 100.0 * 8.905725479125977
Epoch 2240, val loss: 1.0523478984832764
Epoch 2250, training loss: 891.6716918945312 = 1.0512909889221191 + 100.0 * 8.906204223632812
Epoch 2250, val loss: 1.0523124933242798
Epoch 2260, training loss: 892.0733032226562 = 1.0512747764587402 + 100.0 * 8.9102201461792
Epoch 2260, val loss: 1.0523134469985962
Epoch 2270, training loss: 892.146728515625 = 1.051231026649475 + 100.0 * 8.910955429077148
Epoch 2270, val loss: 1.0522708892822266
Epoch 2280, training loss: 891.6873779296875 = 1.051150918006897 + 100.0 * 8.906362533569336
Epoch 2280, val loss: 1.0521962642669678
Epoch 2290, training loss: 891.9954223632812 = 1.051124930381775 + 100.0 * 8.909442901611328
Epoch 2290, val loss: 1.0521800518035889
Epoch 2300, training loss: 892.4559326171875 = 1.051111102104187 + 100.0 * 8.914048194885254
Epoch 2300, val loss: 1.05216646194458
Epoch 2310, training loss: 892.4203491210938 = 1.0510585308074951 + 100.0 * 8.913692474365234
Epoch 2310, val loss: 1.0521135330200195
Epoch 2320, training loss: 892.6265258789062 = 1.0510197877883911 + 100.0 * 8.915755271911621
Epoch 2320, val loss: 1.052083969116211
Epoch 2330, training loss: 892.8628540039062 = 1.0510023832321167 + 100.0 * 8.918118476867676
Epoch 2330, val loss: 1.0520565509796143
Epoch 2340, training loss: 893.0657348632812 = 1.0509395599365234 + 100.0 * 8.920147895812988
Epoch 2340, val loss: 1.0519912242889404
Epoch 2350, training loss: 892.3877563476562 = 1.0508544445037842 + 100.0 * 8.913369178771973
Epoch 2350, val loss: 1.051932454109192
Epoch 2360, training loss: 892.86181640625 = 1.0508273839950562 + 100.0 * 8.918109893798828
Epoch 2360, val loss: 1.0519064664840698
Epoch 2370, training loss: 893.2789916992188 = 1.0508149862289429 + 100.0 * 8.922281265258789
Epoch 2370, val loss: 1.051894187927246
Epoch 2380, training loss: 893.74267578125 = 1.0508021116256714 + 100.0 * 8.926918983459473
Epoch 2380, val loss: 1.0518550872802734
Epoch 2390, training loss: 893.126953125 = 1.0506938695907593 + 100.0 * 8.920762062072754
Epoch 2390, val loss: 1.051794171333313
Epoch 2400, training loss: 892.4337768554688 = 1.050542950630188 + 100.0 * 8.913832664489746
Epoch 2400, val loss: 1.0516412258148193
Epoch 2410, training loss: 892.7617797851562 = 1.0505290031433105 + 100.0 * 8.917112350463867
Epoch 2410, val loss: 1.051629900932312
Epoch 2420, training loss: 893.3102416992188 = 1.0505443811416626 + 100.0 * 8.92259693145752
Epoch 2420, val loss: 1.0516592264175415
Epoch 2430, training loss: 894.1266479492188 = 1.050550937652588 + 100.0 * 8.930761337280273
Epoch 2430, val loss: 1.051661729812622
Epoch 2440, training loss: 894.4816284179688 = 1.050541639328003 + 100.0 * 8.934310913085938
Epoch 2440, val loss: 1.0516504049301147
Epoch 2450, training loss: 893.8333129882812 = 1.0504378080368042 + 100.0 * 8.927828788757324
Epoch 2450, val loss: 1.0515227317810059
Epoch 2460, training loss: 893.8394165039062 = 1.0503625869750977 + 100.0 * 8.92789077758789
Epoch 2460, val loss: 1.0514944791793823
Epoch 2470, training loss: 894.1787719726562 = 1.0503536462783813 + 100.0 * 8.931283950805664
Epoch 2470, val loss: 1.0514870882034302
Epoch 2480, training loss: 894.6713256835938 = 1.050343632698059 + 100.0 * 8.936209678649902
Epoch 2480, val loss: 1.0514843463897705
Epoch 2490, training loss: 894.8213500976562 = 1.0503114461898804 + 100.0 * 8.937710762023926
Epoch 2490, val loss: 1.0514452457427979
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.40492753623188404
0.8140983844091865
=== training gcn model ===
Epoch 0, training loss: 1015.0740356445312 = 1.093064785003662 + 100.0 * 10.139809608459473
Epoch 0, val loss: 1.0921002626419067
Epoch 10, training loss: 962.918212890625 = 1.0897737741470337 + 100.0 * 9.618284225463867
Epoch 10, val loss: 1.0888391733169556
Epoch 20, training loss: 945.7579956054688 = 1.0866621732711792 + 100.0 * 9.4467134475708
Epoch 20, val loss: 1.085768222808838
Epoch 30, training loss: 935.0331420898438 = 1.0836824178695679 + 100.0 * 9.339494705200195
Epoch 30, val loss: 1.082825779914856
Epoch 40, training loss: 926.407958984375 = 1.0808945894241333 + 100.0 * 9.253271102905273
Epoch 40, val loss: 1.0800830125808716
Epoch 50, training loss: 919.2998046875 = 1.0782831907272339 + 100.0 * 9.182214736938477
Epoch 50, val loss: 1.0775175094604492
Epoch 60, training loss: 913.2245483398438 = 1.0758566856384277 + 100.0 * 9.12148666381836
Epoch 60, val loss: 1.0751426219940186
Epoch 70, training loss: 907.9219360351562 = 1.0736017227172852 + 100.0 * 9.068483352661133
Epoch 70, val loss: 1.0729374885559082
Epoch 80, training loss: 903.240966796875 = 1.0715161561965942 + 100.0 * 9.02169418334961
Epoch 80, val loss: 1.070906639099121
Epoch 90, training loss: 899.1021728515625 = 1.0695946216583252 + 100.0 * 8.980325698852539
Epoch 90, val loss: 1.069040060043335
Epoch 100, training loss: 895.5452880859375 = 1.0678305625915527 + 100.0 * 8.944774627685547
Epoch 100, val loss: 1.067338228225708
Epoch 110, training loss: 892.278564453125 = 1.0662165880203247 + 100.0 * 8.912123680114746
Epoch 110, val loss: 1.0657879114151
Epoch 120, training loss: 889.43603515625 = 1.064754605293274 + 100.0 * 8.883712768554688
Epoch 120, val loss: 1.0643939971923828
Epoch 130, training loss: 886.84765625 = 1.0634361505508423 + 100.0 * 8.857842445373535
Epoch 130, val loss: 1.063146710395813
Epoch 140, training loss: 884.6639404296875 = 1.062258243560791 + 100.0 * 8.836016654968262
Epoch 140, val loss: 1.0620399713516235
Epoch 150, training loss: 882.659423828125 = 1.0612037181854248 + 100.0 * 8.8159818649292
Epoch 150, val loss: 1.061058759689331
Epoch 160, training loss: 880.9462280273438 = 1.0602710247039795 + 100.0 * 8.798859596252441
Epoch 160, val loss: 1.0601972341537476
Epoch 170, training loss: 879.3499755859375 = 1.0594499111175537 + 100.0 * 8.782905578613281
Epoch 170, val loss: 1.0594524145126343
Epoch 180, training loss: 878.103515625 = 1.0587375164031982 + 100.0 * 8.770447731018066
Epoch 180, val loss: 1.0588091611862183
Epoch 190, training loss: 876.8890991210938 = 1.0581411123275757 + 100.0 * 8.758309364318848
Epoch 190, val loss: 1.0582764148712158
Epoch 200, training loss: 875.6656494140625 = 1.0576171875 + 100.0 * 8.74608039855957
Epoch 200, val loss: 1.0578172206878662
Epoch 210, training loss: 874.7642211914062 = 1.057176113128662 + 100.0 * 8.737070083618164
Epoch 210, val loss: 1.0574398040771484
Epoch 220, training loss: 873.7870483398438 = 1.0568053722381592 + 100.0 * 8.727302551269531
Epoch 220, val loss: 1.057124376296997
Epoch 230, training loss: 873.0635986328125 = 1.0564942359924316 + 100.0 * 8.720070838928223
Epoch 230, val loss: 1.056868076324463
Epoch 240, training loss: 872.5348510742188 = 1.0562498569488525 + 100.0 * 8.7147855758667
Epoch 240, val loss: 1.0566656589508057
Epoch 250, training loss: 872.0072021484375 = 1.0560386180877686 + 100.0 * 8.709511756896973
Epoch 250, val loss: 1.0564966201782227
Epoch 260, training loss: 871.5982666015625 = 1.0558750629425049 + 100.0 * 8.705424308776855
Epoch 260, val loss: 1.056369423866272
Epoch 270, training loss: 871.262451171875 = 1.0557383298873901 + 100.0 * 8.702067375183105
Epoch 270, val loss: 1.0562760829925537
Epoch 280, training loss: 870.80029296875 = 1.0556281805038452 + 100.0 * 8.697446823120117
Epoch 280, val loss: 1.0561894178390503
Epoch 290, training loss: 870.4957885742188 = 1.0555379390716553 + 100.0 * 8.694402694702148
Epoch 290, val loss: 1.0561282634735107
Epoch 300, training loss: 869.8997802734375 = 1.0554649829864502 + 100.0 * 8.688443183898926
Epoch 300, val loss: 1.056077480316162
Epoch 310, training loss: 869.7772827148438 = 1.055411696434021 + 100.0 * 8.68721866607666
Epoch 310, val loss: 1.0560442209243774
Epoch 320, training loss: 869.4259643554688 = 1.0553609132766724 + 100.0 * 8.683706283569336
Epoch 320, val loss: 1.0560096502304077
Epoch 330, training loss: 869.2902221679688 = 1.0553247928619385 + 100.0 * 8.68234920501709
Epoch 330, val loss: 1.0559802055358887
Epoch 340, training loss: 869.2420654296875 = 1.0552878379821777 + 100.0 * 8.681867599487305
Epoch 340, val loss: 1.0559583902359009
Epoch 350, training loss: 869.0853881835938 = 1.0552616119384766 + 100.0 * 8.680301666259766
Epoch 350, val loss: 1.055937647819519
Epoch 360, training loss: 868.8206787109375 = 1.0552277565002441 + 100.0 * 8.677654266357422
Epoch 360, val loss: 1.0559147596359253
Epoch 370, training loss: 868.74658203125 = 1.0552051067352295 + 100.0 * 8.67691421508789
Epoch 370, val loss: 1.0558973550796509
Epoch 380, training loss: 868.9381713867188 = 1.0551810264587402 + 100.0 * 8.67883014678955
Epoch 380, val loss: 1.055884599685669
Epoch 390, training loss: 868.5115966796875 = 1.0551607608795166 + 100.0 * 8.674564361572266
Epoch 390, val loss: 1.0558594465255737
Epoch 400, training loss: 868.4319458007812 = 1.0551308393478394 + 100.0 * 8.673768043518066
Epoch 400, val loss: 1.055838704109192
Epoch 410, training loss: 868.3743896484375 = 1.0551178455352783 + 100.0 * 8.673192977905273
Epoch 410, val loss: 1.055821418762207
Epoch 420, training loss: 868.7831420898438 = 1.0551143884658813 + 100.0 * 8.67728042602539
Epoch 420, val loss: 1.0558137893676758
Epoch 430, training loss: 868.032470703125 = 1.055042028427124 + 100.0 * 8.669774055480957
Epoch 430, val loss: 1.0557771921157837
Epoch 440, training loss: 868.2508544921875 = 1.055037260055542 + 100.0 * 8.671957969665527
Epoch 440, val loss: 1.0557464361190796
Epoch 450, training loss: 868.2758178710938 = 1.0550463199615479 + 100.0 * 8.672207832336426
Epoch 450, val loss: 1.055754542350769
Epoch 460, training loss: 868.3206787109375 = 1.0550248622894287 + 100.0 * 8.672656059265137
Epoch 460, val loss: 1.055740475654602
Epoch 470, training loss: 868.447998046875 = 1.0550154447555542 + 100.0 * 8.673930168151855
Epoch 470, val loss: 1.0557286739349365
Epoch 480, training loss: 868.1546630859375 = 1.0549787282943726 + 100.0 * 8.67099666595459
Epoch 480, val loss: 1.0556838512420654
Epoch 490, training loss: 868.110595703125 = 1.054962396621704 + 100.0 * 8.67055606842041
Epoch 490, val loss: 1.055672526359558
Epoch 500, training loss: 868.10400390625 = 1.0549646615982056 + 100.0 * 8.670490264892578
Epoch 500, val loss: 1.0556801557540894
Epoch 510, training loss: 868.360595703125 = 1.0549583435058594 + 100.0 * 8.673056602478027
Epoch 510, val loss: 1.0556707382202148
Epoch 520, training loss: 868.4063720703125 = 1.0549484491348267 + 100.0 * 8.673514366149902
Epoch 520, val loss: 1.0556578636169434
Epoch 530, training loss: 868.3534545898438 = 1.0549322366714478 + 100.0 * 8.672985076904297
Epoch 530, val loss: 1.0556418895721436
Epoch 540, training loss: 868.1286010742188 = 1.0548988580703735 + 100.0 * 8.670737266540527
Epoch 540, val loss: 1.0556119680404663
Epoch 550, training loss: 868.2455444335938 = 1.0548804998397827 + 100.0 * 8.671906471252441
Epoch 550, val loss: 1.055594563484192
Epoch 560, training loss: 868.4722900390625 = 1.0548484325408936 + 100.0 * 8.674174308776855
Epoch 560, val loss: 1.0555790662765503
Epoch 570, training loss: 867.7218627929688 = 1.054816484451294 + 100.0 * 8.666670799255371
Epoch 570, val loss: 1.0555408000946045
Epoch 580, training loss: 868.3119506835938 = 1.0548394918441772 + 100.0 * 8.672571182250977
Epoch 580, val loss: 1.0555509328842163
Epoch 590, training loss: 868.6693115234375 = 1.0548405647277832 + 100.0 * 8.67614459991455
Epoch 590, val loss: 1.0555464029312134
Epoch 600, training loss: 868.8536987304688 = 1.0548276901245117 + 100.0 * 8.67798900604248
Epoch 600, val loss: 1.0555380582809448
Epoch 610, training loss: 869.0653076171875 = 1.054805874824524 + 100.0 * 8.680105209350586
Epoch 610, val loss: 1.055517554283142
Epoch 620, training loss: 869.0106201171875 = 1.054787516593933 + 100.0 * 8.679557800292969
Epoch 620, val loss: 1.0555057525634766
Epoch 630, training loss: 869.3063354492188 = 1.0547785758972168 + 100.0 * 8.682516098022461
Epoch 630, val loss: 1.0554934740066528
Epoch 640, training loss: 869.4124755859375 = 1.0547683238983154 + 100.0 * 8.683577537536621
Epoch 640, val loss: 1.0554291009902954
Epoch 650, training loss: 869.5109252929688 = 1.0547206401824951 + 100.0 * 8.684561729431152
Epoch 650, val loss: 1.0554550886154175
Epoch 660, training loss: 868.2673950195312 = 1.0546436309814453 + 100.0 * 8.672127723693848
Epoch 660, val loss: 1.0553734302520752
Epoch 670, training loss: 868.9920654296875 = 1.0546760559082031 + 100.0 * 8.679373741149902
Epoch 670, val loss: 1.055383563041687
Epoch 680, training loss: 869.071044921875 = 1.0546650886535645 + 100.0 * 8.680163383483887
Epoch 680, val loss: 1.0553795099258423
Epoch 690, training loss: 869.2999267578125 = 1.054660439491272 + 100.0 * 8.682452201843262
Epoch 690, val loss: 1.0553687810897827
Epoch 700, training loss: 869.5743408203125 = 1.0546528100967407 + 100.0 * 8.685196876525879
Epoch 700, val loss: 1.0553638935089111
Epoch 710, training loss: 869.7135620117188 = 1.054631233215332 + 100.0 * 8.686589241027832
Epoch 710, val loss: 1.0553357601165771
Epoch 720, training loss: 870.0301513671875 = 1.0546075105667114 + 100.0 * 8.6897554397583
Epoch 720, val loss: 1.0553213357925415
Epoch 730, training loss: 869.6651611328125 = 1.0545495748519897 + 100.0 * 8.686105728149414
Epoch 730, val loss: 1.0552797317504883
Epoch 740, training loss: 869.4869995117188 = 1.054480791091919 + 100.0 * 8.684325218200684
Epoch 740, val loss: 1.0552031993865967
Epoch 750, training loss: 869.6110229492188 = 1.0545285940170288 + 100.0 * 8.685564994812012
Epoch 750, val loss: 1.0552400350570679
Epoch 760, training loss: 870.013427734375 = 1.0545254945755005 + 100.0 * 8.689589500427246
Epoch 760, val loss: 1.0552427768707275
Epoch 770, training loss: 869.93017578125 = 1.0545039176940918 + 100.0 * 8.688756942749023
Epoch 770, val loss: 1.0552254915237427
Epoch 780, training loss: 870.3991088867188 = 1.0545175075531006 + 100.0 * 8.693446159362793
Epoch 780, val loss: 1.0552339553833008
Epoch 790, training loss: 870.5379638671875 = 1.0544934272766113 + 100.0 * 8.69483470916748
Epoch 790, val loss: 1.0552054643630981
Epoch 800, training loss: 871.159912109375 = 1.0543802976608276 + 100.0 * 8.701055526733398
Epoch 800, val loss: 1.055080771446228
Epoch 810, training loss: 871.6724243164062 = 1.0544674396514893 + 100.0 * 8.70617961883545
Epoch 810, val loss: 1.0551819801330566
Epoch 820, training loss: 869.9525146484375 = 1.0543709993362427 + 100.0 * 8.688981056213379
Epoch 820, val loss: 1.0551071166992188
Epoch 830, training loss: 870.5015869140625 = 1.0543885231018066 + 100.0 * 8.694472312927246
Epoch 830, val loss: 1.0551118850708008
Epoch 840, training loss: 870.867431640625 = 1.0543909072875977 + 100.0 * 8.69813060760498
Epoch 840, val loss: 1.0551131963729858
Epoch 850, training loss: 871.17529296875 = 1.0543851852416992 + 100.0 * 8.70120906829834
Epoch 850, val loss: 1.055099368095398
Epoch 860, training loss: 871.58642578125 = 1.054358959197998 + 100.0 * 8.705320358276367
Epoch 860, val loss: 1.0550788640975952
Epoch 870, training loss: 871.7990112304688 = 1.054352879524231 + 100.0 * 8.707446098327637
Epoch 870, val loss: 1.0550764799118042
Epoch 880, training loss: 872.1422729492188 = 1.0543408393859863 + 100.0 * 8.7108793258667
Epoch 880, val loss: 1.0550549030303955
Epoch 890, training loss: 872.4200439453125 = 1.0543125867843628 + 100.0 * 8.71365737915039
Epoch 890, val loss: 1.0550280809402466
Epoch 900, training loss: 872.3220825195312 = 1.0542831420898438 + 100.0 * 8.712677955627441
Epoch 900, val loss: 1.055001974105835
Epoch 910, training loss: 872.8719482421875 = 1.0542781352996826 + 100.0 * 8.71817684173584
Epoch 910, val loss: 1.054991602897644
Epoch 920, training loss: 872.2592163085938 = 1.0542311668395996 + 100.0 * 8.71204948425293
Epoch 920, val loss: 1.0549530982971191
Epoch 930, training loss: 872.7738647460938 = 1.0542221069335938 + 100.0 * 8.717196464538574
Epoch 930, val loss: 1.0549441576004028
Epoch 940, training loss: 873.1185913085938 = 1.0542129278182983 + 100.0 * 8.720643997192383
Epoch 940, val loss: 1.0549330711364746
Epoch 950, training loss: 873.2131958007812 = 1.0541900396347046 + 100.0 * 8.721590042114258
Epoch 950, val loss: 1.0549156665802002
Epoch 960, training loss: 873.0999145507812 = 1.0541502237319946 + 100.0 * 8.720458030700684
Epoch 960, val loss: 1.0548796653747559
Epoch 970, training loss: 873.3170166015625 = 1.0541459321975708 + 100.0 * 8.722628593444824
Epoch 970, val loss: 1.0548644065856934
Epoch 980, training loss: 873.6354370117188 = 1.0541181564331055 + 100.0 * 8.725812911987305
Epoch 980, val loss: 1.0548399686813354
Epoch 990, training loss: 873.15576171875 = 1.0540574789047241 + 100.0 * 8.721016883850098
Epoch 990, val loss: 1.054821252822876
Epoch 1000, training loss: 873.241943359375 = 1.0540416240692139 + 100.0 * 8.721879005432129
Epoch 1000, val loss: 1.0547564029693604
Epoch 1010, training loss: 873.4470825195312 = 1.0540353059768677 + 100.0 * 8.723930358886719
Epoch 1010, val loss: 1.0547555685043335
Epoch 1020, training loss: 874.1429443359375 = 1.0540357828140259 + 100.0 * 8.730889320373535
Epoch 1020, val loss: 1.0547575950622559
Epoch 1030, training loss: 874.2553100585938 = 1.0540015697479248 + 100.0 * 8.732012748718262
Epoch 1030, val loss: 1.0547255277633667
Epoch 1040, training loss: 874.48193359375 = 1.0539836883544922 + 100.0 * 8.73427963256836
Epoch 1040, val loss: 1.054709792137146
Epoch 1050, training loss: 874.6464233398438 = 1.053962230682373 + 100.0 * 8.73592472076416
Epoch 1050, val loss: 1.0546954870224
Epoch 1060, training loss: 874.5746459960938 = 1.05392324924469 + 100.0 * 8.735207557678223
Epoch 1060, val loss: 1.0546417236328125
Epoch 1070, training loss: 875.1177368164062 = 1.053896427154541 + 100.0 * 8.740638732910156
Epoch 1070, val loss: 1.0546234846115112
Epoch 1080, training loss: 875.3673706054688 = 1.0538885593414307 + 100.0 * 8.743134498596191
Epoch 1080, val loss: 1.0546164512634277
Epoch 1090, training loss: 875.5406494140625 = 1.0538605451583862 + 100.0 * 8.744868278503418
Epoch 1090, val loss: 1.0545856952667236
Epoch 1100, training loss: 875.461669921875 = 1.0538312196731567 + 100.0 * 8.744078636169434
Epoch 1100, val loss: 1.0545661449432373
Epoch 1110, training loss: 875.8211669921875 = 1.0538206100463867 + 100.0 * 8.747673034667969
Epoch 1110, val loss: 1.0545488595962524
Epoch 1120, training loss: 875.8157348632812 = 1.0537631511688232 + 100.0 * 8.74761962890625
Epoch 1120, val loss: 1.054505467414856
Epoch 1130, training loss: 876.0048217773438 = 1.0537487268447876 + 100.0 * 8.749510765075684
Epoch 1130, val loss: 1.0544850826263428
Epoch 1140, training loss: 876.3717041015625 = 1.0537424087524414 + 100.0 * 8.753179550170898
Epoch 1140, val loss: 1.054465889930725
Epoch 1150, training loss: 874.9789428710938 = 1.053531289100647 + 100.0 * 8.739253997802734
Epoch 1150, val loss: 1.0542646646499634
Epoch 1160, training loss: 875.6277465820312 = 1.0535835027694702 + 100.0 * 8.745741844177246
Epoch 1160, val loss: 1.054309368133545
Epoch 1170, training loss: 875.6353149414062 = 1.0535961389541626 + 100.0 * 8.745817184448242
Epoch 1170, val loss: 1.054333209991455
Epoch 1180, training loss: 876.008544921875 = 1.0536020994186401 + 100.0 * 8.749549865722656
Epoch 1180, val loss: 1.0543400049209595
Epoch 1190, training loss: 876.763916015625 = 1.05359947681427 + 100.0 * 8.757102966308594
Epoch 1190, val loss: 1.0543421506881714
Epoch 1200, training loss: 876.6788940429688 = 1.0535637140274048 + 100.0 * 8.756253242492676
Epoch 1200, val loss: 1.0542995929718018
Epoch 1210, training loss: 876.9639282226562 = 1.053544282913208 + 100.0 * 8.759103775024414
Epoch 1210, val loss: 1.0542826652526855
Epoch 1220, training loss: 877.1709594726562 = 1.0535098314285278 + 100.0 * 8.761174201965332
Epoch 1220, val loss: 1.0542539358139038
Epoch 1230, training loss: 877.432861328125 = 1.053484559059143 + 100.0 * 8.7637939453125
Epoch 1230, val loss: 1.0542291402816772
Epoch 1240, training loss: 877.56884765625 = 1.053467035293579 + 100.0 * 8.765153884887695
Epoch 1240, val loss: 1.054206132888794
Epoch 1250, training loss: 877.4324340820312 = 1.0534107685089111 + 100.0 * 8.763790130615234
Epoch 1250, val loss: 1.0541595220565796
Epoch 1260, training loss: 877.6725463867188 = 1.053402304649353 + 100.0 * 8.766191482543945
Epoch 1260, val loss: 1.05414617061615
Epoch 1270, training loss: 878.1616821289062 = 1.0533894300460815 + 100.0 * 8.771082878112793
Epoch 1270, val loss: 1.054133415222168
Epoch 1280, training loss: 877.8540649414062 = 1.0533320903778076 + 100.0 * 8.768007278442383
Epoch 1280, val loss: 1.0540730953216553
Epoch 1290, training loss: 878.128662109375 = 1.0533115863800049 + 100.0 * 8.770753860473633
Epoch 1290, val loss: 1.0540599822998047
Epoch 1300, training loss: 878.4932861328125 = 1.0532900094985962 + 100.0 * 8.774399757385254
Epoch 1300, val loss: 1.0540318489074707
Epoch 1310, training loss: 880.0155639648438 = 1.0530061721801758 + 100.0 * 8.789626121520996
Epoch 1310, val loss: 1.0536623001098633
Epoch 1320, training loss: 878.0498657226562 = 1.0530680418014526 + 100.0 * 8.769968032836914
Epoch 1320, val loss: 1.0539116859436035
Epoch 1330, training loss: 876.3685302734375 = 1.0529518127441406 + 100.0 * 8.753155708312988
Epoch 1330, val loss: 1.053716778755188
Epoch 1340, training loss: 876.9613647460938 = 1.053002953529358 + 100.0 * 8.75908374786377
Epoch 1340, val loss: 1.053748607635498
Epoch 1350, training loss: 877.5650634765625 = 1.0530263185501099 + 100.0 * 8.765120506286621
Epoch 1350, val loss: 1.0537703037261963
Epoch 1360, training loss: 877.55908203125 = 1.053023099899292 + 100.0 * 8.765060424804688
Epoch 1360, val loss: 1.053772211074829
Epoch 1370, training loss: 878.4774780273438 = 1.0530273914337158 + 100.0 * 8.77424430847168
Epoch 1370, val loss: 1.0537757873535156
Epoch 1380, training loss: 878.5389404296875 = 1.0529911518096924 + 100.0 * 8.774859428405762
Epoch 1380, val loss: 1.0537484884262085
Epoch 1390, training loss: 878.8652954101562 = 1.0529731512069702 + 100.0 * 8.778122901916504
Epoch 1390, val loss: 1.053712248802185
Epoch 1400, training loss: 879.3098754882812 = 1.0529567003250122 + 100.0 * 8.78256893157959
Epoch 1400, val loss: 1.0537017583847046
Epoch 1410, training loss: 879.2396240234375 = 1.052922248840332 + 100.0 * 8.781867027282715
Epoch 1410, val loss: 1.0536643266677856
Epoch 1420, training loss: 879.3606567382812 = 1.0528746843338013 + 100.0 * 8.78307819366455
Epoch 1420, val loss: 1.0536248683929443
Epoch 1430, training loss: 879.62255859375 = 1.0528501272201538 + 100.0 * 8.785696983337402
Epoch 1430, val loss: 1.0535794496536255
Epoch 1440, training loss: 879.6827392578125 = 1.0527976751327515 + 100.0 * 8.786299705505371
Epoch 1440, val loss: 1.0535577535629272
Epoch 1450, training loss: 879.9270629882812 = 1.052730679512024 + 100.0 * 8.788743019104004
Epoch 1450, val loss: 1.0534740686416626
Epoch 1460, training loss: 878.8819580078125 = 1.0526386499404907 + 100.0 * 8.77829360961914
Epoch 1460, val loss: 1.0534090995788574
Epoch 1470, training loss: 878.947998046875 = 1.0525926351547241 + 100.0 * 8.778953552246094
Epoch 1470, val loss: 1.0533738136291504
Epoch 1480, training loss: 879.9402465820312 = 1.0526330471038818 + 100.0 * 8.7888765335083
Epoch 1480, val loss: 1.053387999534607
Epoch 1490, training loss: 880.83203125 = 1.0526337623596191 + 100.0 * 8.797794342041016
Epoch 1490, val loss: 1.0533857345581055
Epoch 1500, training loss: 880.6055908203125 = 1.0525743961334229 + 100.0 * 8.795530319213867
Epoch 1500, val loss: 1.0533310174942017
Epoch 1510, training loss: 880.9376831054688 = 1.052540898323059 + 100.0 * 8.798851013183594
Epoch 1510, val loss: 1.053312063217163
Epoch 1520, training loss: 881.3796997070312 = 1.0525269508361816 + 100.0 * 8.803271293640137
Epoch 1520, val loss: 1.053287386894226
Epoch 1530, training loss: 881.7130126953125 = 1.0525012016296387 + 100.0 * 8.806605339050293
Epoch 1530, val loss: 1.0532639026641846
Epoch 1540, training loss: 880.7891845703125 = 1.0523762702941895 + 100.0 * 8.797368049621582
Epoch 1540, val loss: 1.0531208515167236
Epoch 1550, training loss: 880.6725463867188 = 1.0522788763046265 + 100.0 * 8.796202659606934
Epoch 1550, val loss: 1.0530492067337036
Epoch 1560, training loss: 881.5484008789062 = 1.0523173809051514 + 100.0 * 8.804961204528809
Epoch 1560, val loss: 1.053080677986145
Epoch 1570, training loss: 882.1397094726562 = 1.0523200035095215 + 100.0 * 8.810873985290527
Epoch 1570, val loss: 1.0530922412872314
Epoch 1580, training loss: 882.3167114257812 = 1.0522949695587158 + 100.0 * 8.812644004821777
Epoch 1580, val loss: 1.0530637502670288
Epoch 1590, training loss: 882.3101806640625 = 1.0522387027740479 + 100.0 * 8.812579154968262
Epoch 1590, val loss: 1.0530179738998413
Epoch 1600, training loss: 882.5083618164062 = 1.0522186756134033 + 100.0 * 8.814560890197754
Epoch 1600, val loss: 1.0529861450195312
Epoch 1610, training loss: 882.643310546875 = 1.0521728992462158 + 100.0 * 8.815911293029785
Epoch 1610, val loss: 1.0529581308364868
Epoch 1620, training loss: 882.863525390625 = 1.0521624088287354 + 100.0 * 8.818113327026367
Epoch 1620, val loss: 1.0529335737228394
Epoch 1630, training loss: 882.7078247070312 = 1.0520838499069214 + 100.0 * 8.816557884216309
Epoch 1630, val loss: 1.0528591871261597
Epoch 1640, training loss: 882.6661376953125 = 1.0520141124725342 + 100.0 * 8.816141128540039
Epoch 1640, val loss: 1.0528078079223633
Epoch 1650, training loss: 883.13818359375 = 1.0520094633102417 + 100.0 * 8.82086181640625
Epoch 1650, val loss: 1.0528111457824707
Epoch 1660, training loss: 883.3931274414062 = 1.0519921779632568 + 100.0 * 8.823410987854004
Epoch 1660, val loss: 1.052779197692871
Epoch 1670, training loss: 883.3992309570312 = 1.051945447921753 + 100.0 * 8.82347297668457
Epoch 1670, val loss: 1.0527313947677612
Epoch 1680, training loss: 883.3075561523438 = 1.0518850088119507 + 100.0 * 8.822556495666504
Epoch 1680, val loss: 1.0526853799819946
Epoch 1690, training loss: 881.926513671875 = 1.0516963005065918 + 100.0 * 8.808748245239258
Epoch 1690, val loss: 1.0524940490722656
Epoch 1700, training loss: 881.893798828125 = 1.051595687866211 + 100.0 * 8.808422088623047
Epoch 1700, val loss: 1.052406907081604
Epoch 1710, training loss: 882.3206176757812 = 1.0515892505645752 + 100.0 * 8.812690734863281
Epoch 1710, val loss: 1.0524002313613892
Epoch 1720, training loss: 882.776123046875 = 1.051600694656372 + 100.0 * 8.817245483398438
Epoch 1720, val loss: 1.052411437034607
Epoch 1730, training loss: 883.32373046875 = 1.0515989065170288 + 100.0 * 8.822721481323242
Epoch 1730, val loss: 1.0524097681045532
Epoch 1740, training loss: 883.7088012695312 = 1.0515965223312378 + 100.0 * 8.82657241821289
Epoch 1740, val loss: 1.0524089336395264
Epoch 1750, training loss: 883.6154174804688 = 1.0515360832214355 + 100.0 * 8.825638771057129
Epoch 1750, val loss: 1.0523544549942017
Epoch 1760, training loss: 883.9677734375 = 1.0515092611312866 + 100.0 * 8.82916259765625
Epoch 1760, val loss: 1.0523196458816528
Epoch 1770, training loss: 884.1629028320312 = 1.0514652729034424 + 100.0 * 8.831114768981934
Epoch 1770, val loss: 1.0522876977920532
Epoch 1780, training loss: 884.379150390625 = 1.0514225959777832 + 100.0 * 8.833276748657227
Epoch 1780, val loss: 1.0522459745407104
Epoch 1790, training loss: 884.6920776367188 = 1.0513840913772583 + 100.0 * 8.836406707763672
Epoch 1790, val loss: 1.052200436592102
Epoch 1800, training loss: 884.7395629882812 = 1.051345705986023 + 100.0 * 8.836882591247559
Epoch 1800, val loss: 1.0521601438522339
Epoch 1810, training loss: 885.0570068359375 = 1.0512996912002563 + 100.0 * 8.840057373046875
Epoch 1810, val loss: 1.0521173477172852
Epoch 1820, training loss: 885.0160522460938 = 1.051231026649475 + 100.0 * 8.839648246765137
Epoch 1820, val loss: 1.0520464181900024
Epoch 1830, training loss: 885.168212890625 = 1.0511990785598755 + 100.0 * 8.841170310974121
Epoch 1830, val loss: 1.0520097017288208
Epoch 1840, training loss: 883.348876953125 = 1.0508962869644165 + 100.0 * 8.822979927062988
Epoch 1840, val loss: 1.0517746210098267
Epoch 1850, training loss: 883.040283203125 = 1.0507230758666992 + 100.0 * 8.81989574432373
Epoch 1850, val loss: 1.0515767335891724
Epoch 1860, training loss: 884.1157836914062 = 1.0508592128753662 + 100.0 * 8.830649375915527
Epoch 1860, val loss: 1.0516846179962158
Epoch 1870, training loss: 884.8772583007812 = 1.0508830547332764 + 100.0 * 8.838263511657715
Epoch 1870, val loss: 1.0517306327819824
Epoch 1880, training loss: 885.5525512695312 = 1.0509028434753418 + 100.0 * 8.845016479492188
Epoch 1880, val loss: 1.0517520904541016
Epoch 1890, training loss: 885.5079345703125 = 1.0508356094360352 + 100.0 * 8.844571113586426
Epoch 1890, val loss: 1.0517066717147827
Epoch 1900, training loss: 885.586669921875 = 1.0507910251617432 + 100.0 * 8.845358848571777
Epoch 1900, val loss: 1.0516574382781982
Epoch 1910, training loss: 886.0269775390625 = 1.050768494606018 + 100.0 * 8.849761962890625
Epoch 1910, val loss: 1.0516453981399536
Epoch 1920, training loss: 885.9865112304688 = 1.050719976425171 + 100.0 * 8.849357604980469
Epoch 1920, val loss: 1.0515676736831665
Epoch 1930, training loss: 886.0463256835938 = 1.0506542921066284 + 100.0 * 8.849956512451172
Epoch 1930, val loss: 1.0515252351760864
Epoch 1940, training loss: 886.2763061523438 = 1.050615668296814 + 100.0 * 8.852256774902344
Epoch 1940, val loss: 1.0514832735061646
Epoch 1950, training loss: 886.5611572265625 = 1.0505728721618652 + 100.0 * 8.85510540008545
Epoch 1950, val loss: 1.0514466762542725
Epoch 1960, training loss: 886.421142578125 = 1.0505101680755615 + 100.0 * 8.853706359863281
Epoch 1960, val loss: 1.0513747930526733
Epoch 1970, training loss: 886.6275024414062 = 1.0504529476165771 + 100.0 * 8.855770111083984
Epoch 1970, val loss: 1.0513262748718262
Epoch 1980, training loss: 887.0479125976562 = 1.0504175424575806 + 100.0 * 8.85997486114502
Epoch 1980, val loss: 1.0512902736663818
Epoch 1990, training loss: 887.0491943359375 = 1.0503531694412231 + 100.0 * 8.85998821258545
Epoch 1990, val loss: 1.0512449741363525
Epoch 2000, training loss: 886.8250122070312 = 1.0502479076385498 + 100.0 * 8.857748031616211
Epoch 2000, val loss: 1.051114559173584
Epoch 2010, training loss: 886.0497436523438 = 1.050112247467041 + 100.0 * 8.849996566772461
Epoch 2010, val loss: 1.0509933233261108
Epoch 2020, training loss: 886.352294921875 = 1.050095558166504 + 100.0 * 8.853021621704102
Epoch 2020, val loss: 1.0510072708129883
Epoch 2030, training loss: 887.1622314453125 = 1.0501198768615723 + 100.0 * 8.86112117767334
Epoch 2030, val loss: 1.051008701324463
Epoch 2040, training loss: 887.5570678710938 = 1.0500969886779785 + 100.0 * 8.865069389343262
Epoch 2040, val loss: 1.050968050956726
Epoch 2050, training loss: 887.5418701171875 = 1.0500167608261108 + 100.0 * 8.86491870880127
Epoch 2050, val loss: 1.050901174545288
Epoch 2060, training loss: 887.8986206054688 = 1.0499727725982666 + 100.0 * 8.868486404418945
Epoch 2060, val loss: 1.0508670806884766
Epoch 2070, training loss: 887.6222534179688 = 1.0498936176300049 + 100.0 * 8.865723609924316
Epoch 2070, val loss: 1.0507919788360596
Epoch 2080, training loss: 887.75341796875 = 1.0498242378234863 + 100.0 * 8.867035865783691
Epoch 2080, val loss: 1.050736665725708
Epoch 2090, training loss: 887.88232421875 = 1.0497982501983643 + 100.0 * 8.868325233459473
Epoch 2090, val loss: 1.0506811141967773
Epoch 2100, training loss: 888.0702514648438 = 1.0497219562530518 + 100.0 * 8.87020492553711
Epoch 2100, val loss: 1.0506293773651123
Epoch 2110, training loss: 888.1851806640625 = 1.0496633052825928 + 100.0 * 8.871355056762695
Epoch 2110, val loss: 1.0505784749984741
Epoch 2120, training loss: 888.4107666015625 = 1.0496129989624023 + 100.0 * 8.873611450195312
Epoch 2120, val loss: 1.0505268573760986
Epoch 2130, training loss: 888.2484130859375 = 1.049539566040039 + 100.0 * 8.871988296508789
Epoch 2130, val loss: 1.0504577159881592
Epoch 2140, training loss: 888.4558715820312 = 1.0494718551635742 + 100.0 * 8.874063491821289
Epoch 2140, val loss: 1.0504132509231567
Epoch 2150, training loss: 888.583984375 = 1.0494297742843628 + 100.0 * 8.875345230102539
Epoch 2150, val loss: 1.0503531694412231
Epoch 2160, training loss: 888.8596801757812 = 1.0493645668029785 + 100.0 * 8.878103256225586
Epoch 2160, val loss: 1.0503062009811401
Epoch 2170, training loss: 888.7205200195312 = 1.0492777824401855 + 100.0 * 8.876712799072266
Epoch 2170, val loss: 1.0502382516860962
Epoch 2180, training loss: 888.7548828125 = 1.0491982698440552 + 100.0 * 8.877057075500488
Epoch 2180, val loss: 1.0501205921173096
Epoch 2190, training loss: 889.027099609375 = 1.049150824546814 + 100.0 * 8.879779815673828
Epoch 2190, val loss: 1.050085186958313
Epoch 2200, training loss: 889.2301635742188 = 1.0491359233856201 + 100.0 * 8.881810188293457
Epoch 2200, val loss: 1.0500595569610596
Epoch 2210, training loss: 889.142333984375 = 1.0490586757659912 + 100.0 * 8.880932807922363
Epoch 2210, val loss: 1.049987554550171
Epoch 2220, training loss: 888.9817504882812 = 1.048944115638733 + 100.0 * 8.879327774047852
Epoch 2220, val loss: 1.0499038696289062
Epoch 2230, training loss: 889.2825317382812 = 1.0489128828048706 + 100.0 * 8.882336616516113
Epoch 2230, val loss: 1.0498530864715576
Epoch 2240, training loss: 889.5057373046875 = 1.048874855041504 + 100.0 * 8.884568214416504
Epoch 2240, val loss: 1.0498251914978027
Epoch 2250, training loss: 889.49072265625 = 1.048776626586914 + 100.0 * 8.884419441223145
Epoch 2250, val loss: 1.0497369766235352
Epoch 2260, training loss: 889.6271362304688 = 1.0487074851989746 + 100.0 * 8.885784149169922
Epoch 2260, val loss: 1.0496760606765747
Epoch 2270, training loss: 889.6576538085938 = 1.0486339330673218 + 100.0 * 8.886090278625488
Epoch 2270, val loss: 1.0496008396148682
Epoch 2280, training loss: 889.9238891601562 = 1.0486000776290894 + 100.0 * 8.888752937316895
Epoch 2280, val loss: 1.049564242362976
Epoch 2290, training loss: 890.1792602539062 = 1.0485585927963257 + 100.0 * 8.89130687713623
Epoch 2290, val loss: 1.0495280027389526
Epoch 2300, training loss: 890.03857421875 = 1.0484386682510376 + 100.0 * 8.889901161193848
Epoch 2300, val loss: 1.04939866065979
Epoch 2310, training loss: 889.7107543945312 = 1.0483574867248535 + 100.0 * 8.886624336242676
Epoch 2310, val loss: 1.0493395328521729
Epoch 2320, training loss: 890.101318359375 = 1.048307180404663 + 100.0 * 8.890530586242676
Epoch 2320, val loss: 1.049312710762024
Epoch 2330, training loss: 890.2425537109375 = 1.0482509136199951 + 100.0 * 8.891942977905273
Epoch 2330, val loss: 1.04923677444458
Epoch 2340, training loss: 890.3102416992188 = 1.0481740236282349 + 100.0 * 8.892621040344238
Epoch 2340, val loss: 1.0491387844085693
Epoch 2350, training loss: 890.1399536132812 = 1.0480692386627197 + 100.0 * 8.890918731689453
Epoch 2350, val loss: 1.049073576927185
Epoch 2360, training loss: 890.03759765625 = 1.0479618310928345 + 100.0 * 8.889896392822266
Epoch 2360, val loss: 1.0489782094955444
Epoch 2370, training loss: 890.44189453125 = 1.047934889793396 + 100.0 * 8.893939971923828
Epoch 2370, val loss: 1.0489290952682495
Epoch 2380, training loss: 890.8209838867188 = 1.0479222536087036 + 100.0 * 8.897730827331543
Epoch 2380, val loss: 1.0489232540130615
Epoch 2390, training loss: 890.954833984375 = 1.0478707551956177 + 100.0 * 8.899069786071777
Epoch 2390, val loss: 1.048801302909851
Epoch 2400, training loss: 890.595458984375 = 1.0477015972137451 + 100.0 * 8.895477294921875
Epoch 2400, val loss: 1.0487494468688965
Epoch 2410, training loss: 890.79443359375 = 1.0476597547531128 + 100.0 * 8.897467613220215
Epoch 2410, val loss: 1.0486907958984375
Epoch 2420, training loss: 891.173095703125 = 1.0476062297821045 + 100.0 * 8.901254653930664
Epoch 2420, val loss: 1.0486342906951904
Epoch 2430, training loss: 890.9522705078125 = 1.0474997758865356 + 100.0 * 8.8990478515625
Epoch 2430, val loss: 1.0485336780548096
Epoch 2440, training loss: 891.0153198242188 = 1.0474367141723633 + 100.0 * 8.899679183959961
Epoch 2440, val loss: 1.0484713315963745
Epoch 2450, training loss: 891.4283447265625 = 1.047387719154358 + 100.0 * 8.903809547424316
Epoch 2450, val loss: 1.0484226942062378
Epoch 2460, training loss: 891.415283203125 = 1.0473029613494873 + 100.0 * 8.903679847717285
Epoch 2460, val loss: 1.0483670234680176
Epoch 2470, training loss: 891.1336669921875 = 1.0471352338790894 + 100.0 * 8.90086555480957
Epoch 2470, val loss: 1.048216462135315
Epoch 2480, training loss: 890.8058471679688 = 1.04701566696167 + 100.0 * 8.897588729858398
Epoch 2480, val loss: 1.0480318069458008
Epoch 2490, training loss: 890.6739501953125 = 1.0468915700912476 + 100.0 * 8.896270751953125
Epoch 2490, val loss: 1.0479605197906494
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.4252173913043478
0.8138810403535464
The final CL Acc:0.41261, 0.00899, The final GNN Acc:0.81352, 0.00067
raw graph: torch.Size([2, 88648])
add edge: torch.Size([2, 110546])
remove edge: torch.Size([2, 66720])
updated graph: torch.Size([2, 88618])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 1004.2318115234375 = 1.1394081115722656 + 100.0 * 10.030923843383789
Epoch 0, val loss: 1.1386168003082275
Epoch 10, training loss: 954.6484985351562 = 1.134194254875183 + 100.0 * 9.53514289855957
Epoch 10, val loss: 1.1334168910980225
Epoch 20, training loss: 935.5711669921875 = 1.1292165517807007 + 100.0 * 9.344419479370117
Epoch 20, val loss: 1.1284502744674683
Epoch 30, training loss: 922.7291870117188 = 1.1244548559188843 + 100.0 * 9.216047286987305
Epoch 30, val loss: 1.1237074136734009
Epoch 40, training loss: 913.1248779296875 = 1.1199097633361816 + 100.0 * 9.120049476623535
Epoch 40, val loss: 1.1191810369491577
Epoch 50, training loss: 905.4652709960938 = 1.1155564785003662 + 100.0 * 9.043497085571289
Epoch 50, val loss: 1.1148476600646973
Epoch 60, training loss: 899.1593017578125 = 1.1113848686218262 + 100.0 * 8.98047924041748
Epoch 60, val loss: 1.110697627067566
Epoch 70, training loss: 893.843017578125 = 1.1073707342147827 + 100.0 * 8.927356719970703
Epoch 70, val loss: 1.106707215309143
Epoch 80, training loss: 889.2847290039062 = 1.1035031080245972 + 100.0 * 8.88181209564209
Epoch 80, val loss: 1.102865219116211
Epoch 90, training loss: 885.4661254882812 = 1.0997859239578247 + 100.0 * 8.843663215637207
Epoch 90, val loss: 1.0991767644882202
Epoch 100, training loss: 882.14697265625 = 1.0962014198303223 + 100.0 * 8.810507774353027
Epoch 100, val loss: 1.0956224203109741
Epoch 110, training loss: 879.3396606445312 = 1.0927636623382568 + 100.0 * 8.782468795776367
Epoch 110, val loss: 1.0922185182571411
Epoch 120, training loss: 876.9539794921875 = 1.0894731283187866 + 100.0 * 8.758645057678223
Epoch 120, val loss: 1.0889636278152466
Epoch 130, training loss: 874.9287109375 = 1.086336374282837 + 100.0 * 8.738423347473145
Epoch 130, val loss: 1.085866928100586
Epoch 140, training loss: 873.2042236328125 = 1.0833659172058105 + 100.0 * 8.721208572387695
Epoch 140, val loss: 1.0829344987869263
Epoch 150, training loss: 871.7494506835938 = 1.0805461406707764 + 100.0 * 8.70668888092041
Epoch 150, val loss: 1.0801554918289185
Epoch 160, training loss: 870.3126220703125 = 1.0779032707214355 + 100.0 * 8.692347526550293
Epoch 160, val loss: 1.0775578022003174
Epoch 170, training loss: 869.133056640625 = 1.075421929359436 + 100.0 * 8.68057632446289
Epoch 170, val loss: 1.0751214027404785
Epoch 180, training loss: 868.1060180664062 = 1.0731219053268433 + 100.0 * 8.670329093933105
Epoch 180, val loss: 1.0728683471679688
Epoch 190, training loss: 867.1981201171875 = 1.0709879398345947 + 100.0 * 8.661271095275879
Epoch 190, val loss: 1.0707850456237793
Epoch 200, training loss: 866.38720703125 = 1.0690380334854126 + 100.0 * 8.653182029724121
Epoch 200, val loss: 1.068881630897522
Epoch 210, training loss: 865.5416259765625 = 1.0672523975372314 + 100.0 * 8.644743919372559
Epoch 210, val loss: 1.0671451091766357
Epoch 220, training loss: 864.8804321289062 = 1.0656383037567139 + 100.0 * 8.638148307800293
Epoch 220, val loss: 1.0655831098556519
Epoch 230, training loss: 864.475830078125 = 1.0641871690750122 + 100.0 * 8.634116172790527
Epoch 230, val loss: 1.064182162284851
Epoch 240, training loss: 863.9868774414062 = 1.062877893447876 + 100.0 * 8.629240036010742
Epoch 240, val loss: 1.062923789024353
Epoch 250, training loss: 863.78515625 = 1.0617202520370483 + 100.0 * 8.62723445892334
Epoch 250, val loss: 1.06180739402771
Epoch 260, training loss: 863.5203247070312 = 1.060711145401001 + 100.0 * 8.624595642089844
Epoch 260, val loss: 1.0608608722686768
Epoch 270, training loss: 862.7916259765625 = 1.0598134994506836 + 100.0 * 8.617318153381348
Epoch 270, val loss: 1.0600045919418335
Epoch 280, training loss: 862.8756713867188 = 1.0590568780899048 + 100.0 * 8.618165969848633
Epoch 280, val loss: 1.0592899322509766
Epoch 290, training loss: 862.7137451171875 = 1.058397650718689 + 100.0 * 8.61655330657959
Epoch 290, val loss: 1.0586711168289185
Epoch 300, training loss: 862.4169311523438 = 1.0578290224075317 + 100.0 * 8.613591194152832
Epoch 300, val loss: 1.0581464767456055
Epoch 310, training loss: 862.0840454101562 = 1.0573430061340332 + 100.0 * 8.61026668548584
Epoch 310, val loss: 1.0577020645141602
Epoch 320, training loss: 862.4534912109375 = 1.0569140911102295 + 100.0 * 8.61396598815918
Epoch 320, val loss: 1.0572926998138428
Epoch 330, training loss: 862.7421875 = 1.0566359758377075 + 100.0 * 8.61685562133789
Epoch 330, val loss: 1.0570640563964844
Epoch 340, training loss: 861.78369140625 = 1.056290626525879 + 100.0 * 8.607274055480957
Epoch 340, val loss: 1.0567514896392822
Epoch 350, training loss: 862.1298828125 = 1.0561022758483887 + 100.0 * 8.610737800598145
Epoch 350, val loss: 1.0565786361694336
Epoch 360, training loss: 861.9426879882812 = 1.0559099912643433 + 100.0 * 8.608867645263672
Epoch 360, val loss: 1.0564217567443848
Epoch 370, training loss: 862.1885986328125 = 1.0557626485824585 + 100.0 * 8.611328125
Epoch 370, val loss: 1.0562958717346191
Epoch 380, training loss: 861.9555053710938 = 1.0556310415267944 + 100.0 * 8.608999252319336
Epoch 380, val loss: 1.0561878681182861
Epoch 390, training loss: 862.1298828125 = 1.0555278062820435 + 100.0 * 8.610743522644043
Epoch 390, val loss: 1.0561070442199707
Epoch 400, training loss: 862.30859375 = 1.0554406642913818 + 100.0 * 8.612531661987305
Epoch 400, val loss: 1.0560338497161865
Epoch 410, training loss: 862.1067504882812 = 1.0553723573684692 + 100.0 * 8.610513687133789
Epoch 410, val loss: 1.0559810400009155
Epoch 420, training loss: 862.1884155273438 = 1.0553175210952759 + 100.0 * 8.61133098602295
Epoch 420, val loss: 1.0559356212615967
Epoch 430, training loss: 862.1701049804688 = 1.0552756786346436 + 100.0 * 8.6111478805542
Epoch 430, val loss: 1.0559011697769165
Epoch 440, training loss: 862.4848022460938 = 1.055237889289856 + 100.0 * 8.614295959472656
Epoch 440, val loss: 1.0558768510818481
Epoch 450, training loss: 862.5961303710938 = 1.0552139282226562 + 100.0 * 8.615408897399902
Epoch 450, val loss: 1.0558620691299438
Epoch 460, training loss: 862.9201049804688 = 1.055193543434143 + 100.0 * 8.61864948272705
Epoch 460, val loss: 1.0558357238769531
Epoch 470, training loss: 862.968505859375 = 1.0551719665527344 + 100.0 * 8.619132995605469
Epoch 470, val loss: 1.0558290481567383
Epoch 480, training loss: 862.7903442382812 = 1.0551490783691406 + 100.0 * 8.617351531982422
Epoch 480, val loss: 1.0558055639266968
Epoch 490, training loss: 862.7551879882812 = 1.0551176071166992 + 100.0 * 8.617000579833984
Epoch 490, val loss: 1.0557860136032104
Epoch 500, training loss: 862.7589721679688 = 1.0550206899642944 + 100.0 * 8.617039680480957
Epoch 500, val loss: 1.055715560913086
Epoch 510, training loss: 863.2526245117188 = 1.0550613403320312 + 100.0 * 8.621975898742676
Epoch 510, val loss: 1.0557500123977661
Epoch 520, training loss: 862.966796875 = 1.0550860166549683 + 100.0 * 8.61911678314209
Epoch 520, val loss: 1.05575430393219
Epoch 530, training loss: 862.3224487304688 = 1.0550363063812256 + 100.0 * 8.61267375946045
Epoch 530, val loss: 1.0557243824005127
Epoch 540, training loss: 862.5391845703125 = 1.055029273033142 + 100.0 * 8.61484146118164
Epoch 540, val loss: 1.0557128190994263
Epoch 550, training loss: 863.0823364257812 = 1.0550521612167358 + 100.0 * 8.620272636413574
Epoch 550, val loss: 1.055733323097229
Epoch 560, training loss: 863.2820434570312 = 1.055046796798706 + 100.0 * 8.622269630432129
Epoch 560, val loss: 1.0557308197021484
Epoch 570, training loss: 863.4365234375 = 1.0550405979156494 + 100.0 * 8.623814582824707
Epoch 570, val loss: 1.0557234287261963
Epoch 580, training loss: 863.587890625 = 1.0550252199172974 + 100.0 * 8.625328063964844
Epoch 580, val loss: 1.0557063817977905
Epoch 590, training loss: 863.8642578125 = 1.0550053119659424 + 100.0 * 8.628092765808105
Epoch 590, val loss: 1.0556915998458862
Epoch 600, training loss: 863.5789184570312 = 1.0549792051315308 + 100.0 * 8.625239372253418
Epoch 600, val loss: 1.0556687116622925
Epoch 610, training loss: 863.9971313476562 = 1.0550014972686768 + 100.0 * 8.62942123413086
Epoch 610, val loss: 1.0556941032409668
Epoch 620, training loss: 864.2601318359375 = 1.0549970865249634 + 100.0 * 8.632051467895508
Epoch 620, val loss: 1.0556883811950684
Epoch 630, training loss: 864.25634765625 = 1.0549402236938477 + 100.0 * 8.632014274597168
Epoch 630, val loss: 1.0556011199951172
Epoch 640, training loss: 865.3615112304688 = 1.054988145828247 + 100.0 * 8.643065452575684
Epoch 640, val loss: 1.0556704998016357
Epoch 650, training loss: 865.2241821289062 = 1.0549910068511963 + 100.0 * 8.641692161560059
Epoch 650, val loss: 1.0556923151016235
Epoch 660, training loss: 865.4951782226562 = 1.0549746751785278 + 100.0 * 8.644401550292969
Epoch 660, val loss: 1.0556741952896118
Epoch 670, training loss: 865.4421997070312 = 1.0549675226211548 + 100.0 * 8.643872261047363
Epoch 670, val loss: 1.055660367012024
Epoch 680, training loss: 865.8797607421875 = 1.0549710988998413 + 100.0 * 8.648247718811035
Epoch 680, val loss: 1.055672287940979
Epoch 690, training loss: 865.4674072265625 = 1.0549399852752686 + 100.0 * 8.644124984741211
Epoch 690, val loss: 1.0556398630142212
Epoch 700, training loss: 865.7250366210938 = 1.054912805557251 + 100.0 * 8.646700859069824
Epoch 700, val loss: 1.0556039810180664
Epoch 710, training loss: 865.9102172851562 = 1.0549298524856567 + 100.0 * 8.648552894592285
Epoch 710, val loss: 1.055632472038269
Epoch 720, training loss: 866.4140625 = 1.0549392700195312 + 100.0 * 8.65359115600586
Epoch 720, val loss: 1.0556432008743286
Epoch 730, training loss: 866.9615478515625 = 1.0549490451812744 + 100.0 * 8.659066200256348
Epoch 730, val loss: 1.0556505918502808
Epoch 740, training loss: 867.0444946289062 = 1.0549360513687134 + 100.0 * 8.659895896911621
Epoch 740, val loss: 1.0556375980377197
Epoch 750, training loss: 867.0222778320312 = 1.0549182891845703 + 100.0 * 8.659673690795898
Epoch 750, val loss: 1.0556260347366333
Epoch 760, training loss: 867.1397094726562 = 1.0549167394638062 + 100.0 * 8.660847663879395
Epoch 760, val loss: 1.0556130409240723
Epoch 770, training loss: 867.4043579101562 = 1.054908275604248 + 100.0 * 8.663494110107422
Epoch 770, val loss: 1.0556161403656006
Epoch 780, training loss: 867.5068969726562 = 1.0548932552337646 + 100.0 * 8.664520263671875
Epoch 780, val loss: 1.0555936098098755
Epoch 790, training loss: 867.2796630859375 = 1.0548831224441528 + 100.0 * 8.662247657775879
Epoch 790, val loss: 1.0555819272994995
Epoch 800, training loss: 867.8074951171875 = 1.0548824071884155 + 100.0 * 8.667526245117188
Epoch 800, val loss: 1.0555846691131592
Epoch 810, training loss: 868.3134155273438 = 1.0548909902572632 + 100.0 * 8.672585487365723
Epoch 810, val loss: 1.055598497390747
Epoch 820, training loss: 868.2381591796875 = 1.0548728704452515 + 100.0 * 8.671833038330078
Epoch 820, val loss: 1.0555768013000488
Epoch 830, training loss: 868.3425903320312 = 1.0548654794692993 + 100.0 * 8.672877311706543
Epoch 830, val loss: 1.055577039718628
Epoch 840, training loss: 868.9671630859375 = 1.0548651218414307 + 100.0 * 8.679122924804688
Epoch 840, val loss: 1.0555682182312012
Epoch 850, training loss: 868.4437866210938 = 1.0548502206802368 + 100.0 * 8.67388916015625
Epoch 850, val loss: 1.0555591583251953
Epoch 860, training loss: 868.818603515625 = 1.0548495054244995 + 100.0 * 8.677637100219727
Epoch 860, val loss: 1.0555603504180908
Epoch 870, training loss: 869.1891479492188 = 1.0548537969589233 + 100.0 * 8.681343078613281
Epoch 870, val loss: 1.0555657148361206
Epoch 880, training loss: 869.3928833007812 = 1.0548490285873413 + 100.0 * 8.683380126953125
Epoch 880, val loss: 1.055559515953064
Epoch 890, training loss: 869.5825805664062 = 1.054847240447998 + 100.0 * 8.685276985168457
Epoch 890, val loss: 1.0555545091629028
Epoch 900, training loss: 869.8160400390625 = 1.0548462867736816 + 100.0 * 8.68761157989502
Epoch 900, val loss: 1.0555524826049805
Epoch 910, training loss: 869.0186767578125 = 1.0547877550125122 + 100.0 * 8.679638862609863
Epoch 910, val loss: 1.055497407913208
Epoch 920, training loss: 869.4320068359375 = 1.0547876358032227 + 100.0 * 8.683772087097168
Epoch 920, val loss: 1.0554994344711304
Epoch 930, training loss: 869.8141479492188 = 1.054803729057312 + 100.0 * 8.687593460083008
Epoch 930, val loss: 1.0555195808410645
Epoch 940, training loss: 870.4599609375 = 1.0548217296600342 + 100.0 * 8.694051742553711
Epoch 940, val loss: 1.0555309057235718
Epoch 950, training loss: 870.8612060546875 = 1.0548228025436401 + 100.0 * 8.698063850402832
Epoch 950, val loss: 1.0555288791656494
Epoch 960, training loss: 870.5032958984375 = 1.0547934770584106 + 100.0 * 8.69448471069336
Epoch 960, val loss: 1.055505394935608
Epoch 970, training loss: 870.6792602539062 = 1.0547953844070435 + 100.0 * 8.696244239807129
Epoch 970, val loss: 1.0555108785629272
Epoch 980, training loss: 871.0448608398438 = 1.0548059940338135 + 100.0 * 8.69990062713623
Epoch 980, val loss: 1.055515170097351
Epoch 990, training loss: 871.23681640625 = 1.0547915697097778 + 100.0 * 8.701820373535156
Epoch 990, val loss: 1.0555047988891602
Epoch 1000, training loss: 871.2130126953125 = 1.0547821521759033 + 100.0 * 8.701581954956055
Epoch 1000, val loss: 1.0554916858673096
Epoch 1010, training loss: 871.3405151367188 = 1.0547754764556885 + 100.0 * 8.702857971191406
Epoch 1010, val loss: 1.0554884672164917
Epoch 1020, training loss: 871.6864624023438 = 1.054772973060608 + 100.0 * 8.706316947937012
Epoch 1020, val loss: 1.0554875135421753
Epoch 1030, training loss: 871.7310791015625 = 1.054770588874817 + 100.0 * 8.70676326751709
Epoch 1030, val loss: 1.05548894405365
Epoch 1040, training loss: 871.925537109375 = 1.0547691583633423 + 100.0 * 8.708707809448242
Epoch 1040, val loss: 1.0554797649383545
Epoch 1050, training loss: 872.0935668945312 = 1.054762601852417 + 100.0 * 8.71038818359375
Epoch 1050, val loss: 1.05547297000885
Epoch 1060, training loss: 872.2090454101562 = 1.0547515153884888 + 100.0 * 8.711543083190918
Epoch 1060, val loss: 1.0554652214050293
Epoch 1070, training loss: 872.6187744140625 = 1.054761290550232 + 100.0 * 8.7156400680542
Epoch 1070, val loss: 1.0554742813110352
Epoch 1080, training loss: 872.6641235351562 = 1.0547521114349365 + 100.0 * 8.716094017028809
Epoch 1080, val loss: 1.0554603338241577
Epoch 1090, training loss: 872.5018310546875 = 1.0547412633895874 + 100.0 * 8.714470863342285
Epoch 1090, val loss: 1.0554534196853638
Epoch 1100, training loss: 872.8170166015625 = 1.0547367334365845 + 100.0 * 8.717622756958008
Epoch 1100, val loss: 1.0554566383361816
Epoch 1110, training loss: 873.2015380859375 = 1.0547387599945068 + 100.0 * 8.721467971801758
Epoch 1110, val loss: 1.0554530620574951
Epoch 1120, training loss: 873.2383422851562 = 1.0547244548797607 + 100.0 * 8.72183609008789
Epoch 1120, val loss: 1.0554392337799072
Epoch 1130, training loss: 873.0941772460938 = 1.054714560508728 + 100.0 * 8.7203950881958
Epoch 1130, val loss: 1.0554287433624268
Epoch 1140, training loss: 873.5804443359375 = 1.0547150373458862 + 100.0 * 8.725257873535156
Epoch 1140, val loss: 1.0554317235946655
Epoch 1150, training loss: 872.9325561523438 = 1.054679036140442 + 100.0 * 8.718778610229492
Epoch 1150, val loss: 1.055393934249878
Epoch 1160, training loss: 873.542236328125 = 1.054686427116394 + 100.0 * 8.724875450134277
Epoch 1160, val loss: 1.055414080619812
Epoch 1170, training loss: 874.0631103515625 = 1.0547025203704834 + 100.0 * 8.730084419250488
Epoch 1170, val loss: 1.0554109811782837
Epoch 1180, training loss: 874.2185668945312 = 1.0546951293945312 + 100.0 * 8.73163890838623
Epoch 1180, val loss: 1.0554118156433105
Epoch 1190, training loss: 874.38671875 = 1.0546848773956299 + 100.0 * 8.733320236206055
Epoch 1190, val loss: 1.0554043054580688
Epoch 1200, training loss: 874.4572143554688 = 1.0546741485595703 + 100.0 * 8.734025001525879
Epoch 1200, val loss: 1.0553947687149048
Epoch 1210, training loss: 874.3319702148438 = 1.0546647310256958 + 100.0 * 8.732772827148438
Epoch 1210, val loss: 1.055376648902893
Epoch 1220, training loss: 874.6325073242188 = 1.054647445678711 + 100.0 * 8.73577880859375
Epoch 1220, val loss: 1.0553579330444336
Epoch 1230, training loss: 874.0686645507812 = 1.054612398147583 + 100.0 * 8.730140686035156
Epoch 1230, val loss: 1.0553303956985474
Epoch 1240, training loss: 874.2486572265625 = 1.0546143054962158 + 100.0 * 8.731940269470215
Epoch 1240, val loss: 1.0553367137908936
Epoch 1250, training loss: 874.572998046875 = 1.0546268224716187 + 100.0 * 8.735183715820312
Epoch 1250, val loss: 1.0553492307662964
Epoch 1260, training loss: 875.0997314453125 = 1.0546352863311768 + 100.0 * 8.740450859069824
Epoch 1260, val loss: 1.0553494691848755
Epoch 1270, training loss: 874.6619873046875 = 1.0546019077301025 + 100.0 * 8.73607349395752
Epoch 1270, val loss: 1.0553289651870728
Epoch 1280, training loss: 875.3972778320312 = 1.054621934890747 + 100.0 * 8.743426322937012
Epoch 1280, val loss: 1.0553386211395264
Epoch 1290, training loss: 875.6221313476562 = 1.0546183586120605 + 100.0 * 8.745675086975098
Epoch 1290, val loss: 1.0553297996520996
Epoch 1300, training loss: 875.199951171875 = 1.0545947551727295 + 100.0 * 8.741454124450684
Epoch 1300, val loss: 1.0553086996078491
Epoch 1310, training loss: 875.65869140625 = 1.0545921325683594 + 100.0 * 8.746041297912598
Epoch 1310, val loss: 1.0553102493286133
Epoch 1320, training loss: 875.9381103515625 = 1.0545930862426758 + 100.0 * 8.748835563659668
Epoch 1320, val loss: 1.0553110837936401
Epoch 1330, training loss: 875.7256469726562 = 1.0545629262924194 + 100.0 * 8.746710777282715
Epoch 1330, val loss: 1.055263638496399
Epoch 1340, training loss: 875.7487182617188 = 1.0545499324798584 + 100.0 * 8.746941566467285
Epoch 1340, val loss: 1.0552692413330078
Epoch 1350, training loss: 876.030517578125 = 1.0545530319213867 + 100.0 * 8.749759674072266
Epoch 1350, val loss: 1.0552741289138794
Epoch 1360, training loss: 876.4182739257812 = 1.0545583963394165 + 100.0 * 8.753637313842773
Epoch 1360, val loss: 1.0552741289138794
Epoch 1370, training loss: 876.3766479492188 = 1.05454421043396 + 100.0 * 8.753220558166504
Epoch 1370, val loss: 1.055261254310608
Epoch 1380, training loss: 876.3905029296875 = 1.054533839225769 + 100.0 * 8.7533597946167
Epoch 1380, val loss: 1.0552531480789185
Epoch 1390, training loss: 876.5549926757812 = 1.0545259714126587 + 100.0 * 8.7550048828125
Epoch 1390, val loss: 1.0552431344985962
Epoch 1400, training loss: 876.7589111328125 = 1.0545204877853394 + 100.0 * 8.757043838500977
Epoch 1400, val loss: 1.0552175045013428
Epoch 1410, training loss: 876.1103515625 = 1.0544683933258057 + 100.0 * 8.750558853149414
Epoch 1410, val loss: 1.0551899671554565
Epoch 1420, training loss: 875.6365356445312 = 1.0544276237487793 + 100.0 * 8.745820999145508
Epoch 1420, val loss: 1.055161952972412
Epoch 1430, training loss: 875.835693359375 = 1.0544304847717285 + 100.0 * 8.747812271118164
Epoch 1430, val loss: 1.0551574230194092
Epoch 1440, training loss: 876.4205322265625 = 1.054451584815979 + 100.0 * 8.753661155700684
Epoch 1440, val loss: 1.0551728010177612
Epoch 1450, training loss: 877.0203247070312 = 1.0544633865356445 + 100.0 * 8.759658813476562
Epoch 1450, val loss: 1.055182695388794
Epoch 1460, training loss: 877.5016479492188 = 1.054466724395752 + 100.0 * 8.764472007751465
Epoch 1460, val loss: 1.0551899671554565
Epoch 1470, training loss: 877.5206909179688 = 1.0544480085372925 + 100.0 * 8.764662742614746
Epoch 1470, val loss: 1.0551625490188599
Epoch 1480, training loss: 877.5401611328125 = 1.0544297695159912 + 100.0 * 8.764857292175293
Epoch 1480, val loss: 1.0551475286483765
Epoch 1490, training loss: 877.912109375 = 1.0544286966323853 + 100.0 * 8.768576622009277
Epoch 1490, val loss: 1.0551445484161377
Epoch 1500, training loss: 878.0413208007812 = 1.0544248819351196 + 100.0 * 8.769868850708008
Epoch 1500, val loss: 1.0551294088363647
Epoch 1510, training loss: 877.6377563476562 = 1.0543873310089111 + 100.0 * 8.765833854675293
Epoch 1510, val loss: 1.0551016330718994
Epoch 1520, training loss: 877.728759765625 = 1.0543808937072754 + 100.0 * 8.766743659973145
Epoch 1520, val loss: 1.0550932884216309
Epoch 1530, training loss: 878.1752319335938 = 1.0543863773345947 + 100.0 * 8.771208763122559
Epoch 1530, val loss: 1.0551059246063232
Epoch 1540, training loss: 878.2911376953125 = 1.0543721914291382 + 100.0 * 8.772367477416992
Epoch 1540, val loss: 1.0550912618637085
Epoch 1550, training loss: 878.3170776367188 = 1.054355263710022 + 100.0 * 8.772626876831055
Epoch 1550, val loss: 1.0550789833068848
Epoch 1560, training loss: 878.6568603515625 = 1.054351806640625 + 100.0 * 8.77602481842041
Epoch 1560, val loss: 1.0550732612609863
Epoch 1570, training loss: 878.7815551757812 = 1.054337501525879 + 100.0 * 8.77727222442627
Epoch 1570, val loss: 1.0550613403320312
Epoch 1580, training loss: 879.0194702148438 = 1.0543358325958252 + 100.0 * 8.779651641845703
Epoch 1580, val loss: 1.0550562143325806
Epoch 1590, training loss: 878.9448852539062 = 1.0543186664581299 + 100.0 * 8.778905868530273
Epoch 1590, val loss: 1.0550355911254883
Epoch 1600, training loss: 877.8582153320312 = 1.0542570352554321 + 100.0 * 8.76803970336914
Epoch 1600, val loss: 1.0549801588058472
Epoch 1610, training loss: 878.6331176757812 = 1.0542267560958862 + 100.0 * 8.775789260864258
Epoch 1610, val loss: 1.0549639463424683
Epoch 1620, training loss: 878.44482421875 = 1.0542317628860474 + 100.0 * 8.773905754089355
Epoch 1620, val loss: 1.0549598932266235
Epoch 1630, training loss: 879.3466186523438 = 1.0542457103729248 + 100.0 * 8.782923698425293
Epoch 1630, val loss: 1.0549671649932861
Epoch 1640, training loss: 879.781005859375 = 1.0542594194412231 + 100.0 * 8.787267684936523
Epoch 1640, val loss: 1.054972529411316
Epoch 1650, training loss: 880.1134643554688 = 1.0542629957199097 + 100.0 * 8.790592193603516
Epoch 1650, val loss: 1.054970145225525
Epoch 1660, training loss: 880.1087036132812 = 1.0542476177215576 + 100.0 * 8.790544509887695
Epoch 1660, val loss: 1.0549510717391968
Epoch 1670, training loss: 880.2591552734375 = 1.0542272329330444 + 100.0 * 8.792049407958984
Epoch 1670, val loss: 1.0549407005310059
Epoch 1680, training loss: 880.22802734375 = 1.0542114973068237 + 100.0 * 8.791738510131836
Epoch 1680, val loss: 1.0549243688583374
Epoch 1690, training loss: 880.1560668945312 = 1.0541636943817139 + 100.0 * 8.791019439697266
Epoch 1690, val loss: 1.0548961162567139
Epoch 1700, training loss: 880.3301391601562 = 1.0541236400604248 + 100.0 * 8.792759895324707
Epoch 1700, val loss: 1.0548620223999023
Epoch 1710, training loss: 879.8237915039062 = 1.0541080236434937 + 100.0 * 8.787696838378906
Epoch 1710, val loss: 1.0548348426818848
Epoch 1720, training loss: 880.655517578125 = 1.054129719734192 + 100.0 * 8.796013832092285
Epoch 1720, val loss: 1.0548498630523682
Epoch 1730, training loss: 881.0654296875 = 1.0541400909423828 + 100.0 * 8.8001127243042
Epoch 1730, val loss: 1.0548582077026367
Epoch 1740, training loss: 881.0257568359375 = 1.0541247129440308 + 100.0 * 8.799715995788574
Epoch 1740, val loss: 1.0548399686813354
Epoch 1750, training loss: 880.9500732421875 = 1.0541071891784668 + 100.0 * 8.798959732055664
Epoch 1750, val loss: 1.0548261404037476
Epoch 1760, training loss: 881.377685546875 = 1.0541067123413086 + 100.0 * 8.80323600769043
Epoch 1760, val loss: 1.0548259019851685
Epoch 1770, training loss: 881.6238403320312 = 1.0541036128997803 + 100.0 * 8.805697441101074
Epoch 1770, val loss: 1.054808497428894
Epoch 1780, training loss: 881.560791015625 = 1.0540837049484253 + 100.0 * 8.80506706237793
Epoch 1780, val loss: 1.0547878742218018
Epoch 1790, training loss: 881.30078125 = 1.054051160812378 + 100.0 * 8.802467346191406
Epoch 1790, val loss: 1.054774284362793
Epoch 1800, training loss: 881.6807861328125 = 1.0540494918823242 + 100.0 * 8.806266784667969
Epoch 1800, val loss: 1.0547605752944946
Epoch 1810, training loss: 881.8764038085938 = 1.0540428161621094 + 100.0 * 8.808223724365234
Epoch 1810, val loss: 1.0547523498535156
Epoch 1820, training loss: 881.9635620117188 = 1.054030418395996 + 100.0 * 8.80909538269043
Epoch 1820, val loss: 1.054743766784668
Epoch 1830, training loss: 882.2166748046875 = 1.0540223121643066 + 100.0 * 8.811626434326172
Epoch 1830, val loss: 1.0547304153442383
Epoch 1840, training loss: 882.048828125 = 1.0539922714233398 + 100.0 * 8.809947967529297
Epoch 1840, val loss: 1.0547006130218506
Epoch 1850, training loss: 882.1633911132812 = 1.0539761781692505 + 100.0 * 8.811094284057617
Epoch 1850, val loss: 1.054693341255188
Epoch 1860, training loss: 882.5198974609375 = 1.0539734363555908 + 100.0 * 8.814659118652344
Epoch 1860, val loss: 1.054687261581421
Epoch 1870, training loss: 882.5701904296875 = 1.053953766822815 + 100.0 * 8.815162658691406
Epoch 1870, val loss: 1.0546592473983765
Epoch 1880, training loss: 882.1982421875 = 1.053918719291687 + 100.0 * 8.811443328857422
Epoch 1880, val loss: 1.054640769958496
Epoch 1890, training loss: 882.68212890625 = 1.0538623332977295 + 100.0 * 8.816283226013184
Epoch 1890, val loss: 1.0545810461044312
Epoch 1900, training loss: 881.367919921875 = 1.0537866353988647 + 100.0 * 8.803141593933105
Epoch 1900, val loss: 1.0544912815093994
Epoch 1910, training loss: 881.3580322265625 = 1.0537614822387695 + 100.0 * 8.8030424118042
Epoch 1910, val loss: 1.054487943649292
Epoch 1920, training loss: 881.5701293945312 = 1.0537701845169067 + 100.0 * 8.805163383483887
Epoch 1920, val loss: 1.0545042753219604
Epoch 1930, training loss: 882.0512084960938 = 1.0537874698638916 + 100.0 * 8.809974670410156
Epoch 1930, val loss: 1.0545190572738647
Epoch 1940, training loss: 882.5794677734375 = 1.0538023710250854 + 100.0 * 8.81525707244873
Epoch 1940, val loss: 1.0545374155044556
Epoch 1950, training loss: 882.78564453125 = 1.0537970066070557 + 100.0 * 8.8173189163208
Epoch 1950, val loss: 1.0545156002044678
Epoch 1960, training loss: 883.5411987304688 = 1.053812861442566 + 100.0 * 8.824873924255371
Epoch 1960, val loss: 1.0545437335968018
Epoch 1970, training loss: 883.6383056640625 = 1.053802490234375 + 100.0 * 8.825844764709473
Epoch 1970, val loss: 1.0545300245285034
Epoch 1980, training loss: 883.1728515625 = 1.0537598133087158 + 100.0 * 8.82119083404541
Epoch 1980, val loss: 1.0544921159744263
Epoch 1990, training loss: 883.2134399414062 = 1.053733468055725 + 100.0 * 8.8215970993042
Epoch 1990, val loss: 1.0544718503952026
Epoch 2000, training loss: 883.6062622070312 = 1.053734540939331 + 100.0 * 8.825525283813477
Epoch 2000, val loss: 1.054466962814331
Epoch 2010, training loss: 884.0328979492188 = 1.0537388324737549 + 100.0 * 8.829792022705078
Epoch 2010, val loss: 1.0544744729995728
Epoch 2020, training loss: 884.19189453125 = 1.053727626800537 + 100.0 * 8.831381797790527
Epoch 2020, val loss: 1.054463505744934
Epoch 2030, training loss: 883.9368896484375 = 1.0536936521530151 + 100.0 * 8.828831672668457
Epoch 2030, val loss: 1.054425835609436
Epoch 2040, training loss: 884.015380859375 = 1.0536704063415527 + 100.0 * 8.829617500305176
Epoch 2040, val loss: 1.0544050931930542
Epoch 2050, training loss: 884.2874755859375 = 1.0536634922027588 + 100.0 * 8.832338333129883
Epoch 2050, val loss: 1.0543928146362305
Epoch 2060, training loss: 884.4693603515625 = 1.0536459684371948 + 100.0 * 8.83415699005127
Epoch 2060, val loss: 1.054387092590332
Epoch 2070, training loss: 884.6945190429688 = 1.0536353588104248 + 100.0 * 8.836408615112305
Epoch 2070, val loss: 1.0543718338012695
Epoch 2080, training loss: 884.79638671875 = 1.0536187887191772 + 100.0 * 8.837428092956543
Epoch 2080, val loss: 1.0543580055236816
Epoch 2090, training loss: 884.3121337890625 = 1.0535523891448975 + 100.0 * 8.832586288452148
Epoch 2090, val loss: 1.0542867183685303
Epoch 2100, training loss: 884.3724365234375 = 1.0535411834716797 + 100.0 * 8.833189010620117
Epoch 2100, val loss: 1.0542658567428589
Epoch 2110, training loss: 884.6851196289062 = 1.0535417795181274 + 100.0 * 8.836316108703613
Epoch 2110, val loss: 1.0542731285095215
Epoch 2120, training loss: 885.1751098632812 = 1.0535387992858887 + 100.0 * 8.841216087341309
Epoch 2120, val loss: 1.054274559020996
Epoch 2130, training loss: 883.75634765625 = 1.0534228086471558 + 100.0 * 8.82702922821045
Epoch 2130, val loss: 1.0541526079177856
Epoch 2140, training loss: 884.1026000976562 = 1.0533982515335083 + 100.0 * 8.83049201965332
Epoch 2140, val loss: 1.0541378259658813
Epoch 2150, training loss: 884.2595825195312 = 1.0533976554870605 + 100.0 * 8.832061767578125
Epoch 2150, val loss: 1.0541311502456665
Epoch 2160, training loss: 884.81640625 = 1.053405523300171 + 100.0 * 8.837630271911621
Epoch 2160, val loss: 1.0541422367095947
Epoch 2170, training loss: 885.3045043945312 = 1.053404450416565 + 100.0 * 8.842511177062988
Epoch 2170, val loss: 1.0541411638259888
Epoch 2180, training loss: 885.5057373046875 = 1.0534005165100098 + 100.0 * 8.844523429870605
Epoch 2180, val loss: 1.0541372299194336
Epoch 2190, training loss: 885.6550903320312 = 1.0533833503723145 + 100.0 * 8.846016883850098
Epoch 2190, val loss: 1.05411696434021
Epoch 2200, training loss: 885.532470703125 = 1.053352952003479 + 100.0 * 8.844791412353516
Epoch 2200, val loss: 1.054085612297058
Epoch 2210, training loss: 885.749755859375 = 1.0533442497253418 + 100.0 * 8.846963882446289
Epoch 2210, val loss: 1.054077386856079
Epoch 2220, training loss: 886.0588989257812 = 1.0533348321914673 + 100.0 * 8.850055694580078
Epoch 2220, val loss: 1.054067850112915
Epoch 2230, training loss: 886.2431030273438 = 1.0533018112182617 + 100.0 * 8.851898193359375
Epoch 2230, val loss: 1.0540307760238647
Epoch 2240, training loss: 886.3681030273438 = 1.0532888174057007 + 100.0 * 8.853148460388184
Epoch 2240, val loss: 1.0540238618850708
Epoch 2250, training loss: 886.4539794921875 = 1.0532665252685547 + 100.0 * 8.85400676727295
Epoch 2250, val loss: 1.0539960861206055
Epoch 2260, training loss: 886.356689453125 = 1.0532432794570923 + 100.0 * 8.853034973144531
Epoch 2260, val loss: 1.0539807081222534
Epoch 2270, training loss: 886.65234375 = 1.0532230138778687 + 100.0 * 8.85599136352539
Epoch 2270, val loss: 1.0539554357528687
Epoch 2280, training loss: 886.5888671875 = 1.0531920194625854 + 100.0 * 8.85535717010498
Epoch 2280, val loss: 1.0539171695709229
Epoch 2290, training loss: 886.5899047851562 = 1.0531657934188843 + 100.0 * 8.855367660522461
Epoch 2290, val loss: 1.0539051294326782
Epoch 2300, training loss: 886.9352416992188 = 1.0531586408615112 + 100.0 * 8.858820915222168
Epoch 2300, val loss: 1.0538972616195679
Epoch 2310, training loss: 887.0570068359375 = 1.0531309843063354 + 100.0 * 8.860038757324219
Epoch 2310, val loss: 1.053863763809204
Epoch 2320, training loss: 886.571044921875 = 1.0530667304992676 + 100.0 * 8.855179786682129
Epoch 2320, val loss: 1.0537832975387573
Epoch 2330, training loss: 885.5347900390625 = 1.0529695749282837 + 100.0 * 8.844818115234375
Epoch 2330, val loss: 1.0537056922912598
Epoch 2340, training loss: 885.474365234375 = 1.0529297590255737 + 100.0 * 8.84421443939209
Epoch 2340, val loss: 1.0536764860153198
Epoch 2350, training loss: 886.0934448242188 = 1.0529520511627197 + 100.0 * 8.850404739379883
Epoch 2350, val loss: 1.0536876916885376
Epoch 2360, training loss: 886.7910766601562 = 1.052973747253418 + 100.0 * 8.857380867004395
Epoch 2360, val loss: 1.0537084341049194
Epoch 2370, training loss: 887.2766723632812 = 1.0529764890670776 + 100.0 * 8.862236976623535
Epoch 2370, val loss: 1.0537073612213135
Epoch 2380, training loss: 886.7913208007812 = 1.052916407585144 + 100.0 * 8.857383728027344
Epoch 2380, val loss: 1.0536556243896484
Epoch 2390, training loss: 887.2089233398438 = 1.0529086589813232 + 100.0 * 8.861559867858887
Epoch 2390, val loss: 1.0536483526229858
Epoch 2400, training loss: 887.5584106445312 = 1.0529049634933472 + 100.0 * 8.865055084228516
Epoch 2400, val loss: 1.0536322593688965
Epoch 2410, training loss: 887.535400390625 = 1.0528780221939087 + 100.0 * 8.864825248718262
Epoch 2410, val loss: 1.0536078214645386
Epoch 2420, training loss: 887.4314575195312 = 1.0528405904769897 + 100.0 * 8.863785743713379
Epoch 2420, val loss: 1.0535708665847778
Epoch 2430, training loss: 887.7388305664062 = 1.0528225898742676 + 100.0 * 8.866860389709473
Epoch 2430, val loss: 1.0535484552383423
Epoch 2440, training loss: 887.44482421875 = 1.0527784824371338 + 100.0 * 8.863920211791992
Epoch 2440, val loss: 1.0535190105438232
Epoch 2450, training loss: 887.8224487304688 = 1.0527701377868652 + 100.0 * 8.867696762084961
Epoch 2450, val loss: 1.0534946918487549
Epoch 2460, training loss: 888.2299194335938 = 1.052756667137146 + 100.0 * 8.871771812438965
Epoch 2460, val loss: 1.0534882545471191
Epoch 2470, training loss: 887.393310546875 = 1.052692174911499 + 100.0 * 8.86340618133545
Epoch 2470, val loss: 1.0534148216247559
Epoch 2480, training loss: 887.3335571289062 = 1.0526280403137207 + 100.0 * 8.862809181213379
Epoch 2480, val loss: 1.0533387660980225
Epoch 2490, training loss: 887.711669921875 = 1.0526044368743896 + 100.0 * 8.86659049987793
Epoch 2490, val loss: 1.0533299446105957
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.39666666666666667
0.864449757299138
=== training gcn model ===
Epoch 0, training loss: 992.3921508789062 = 1.0937319993972778 + 100.0 * 9.912983894348145
Epoch 0, val loss: 1.0936685800552368
Epoch 10, training loss: 948.0001831054688 = 1.0902109146118164 + 100.0 * 9.469099998474121
Epoch 10, val loss: 1.0901589393615723
Epoch 20, training loss: 930.7315063476562 = 1.0868632793426514 + 100.0 * 9.296446800231934
Epoch 20, val loss: 1.0868339538574219
Epoch 30, training loss: 918.6268920898438 = 1.0837231874465942 + 100.0 * 9.175431251525879
Epoch 30, val loss: 1.0837162733078003
Epoch 40, training loss: 909.5475463867188 = 1.0807738304138184 + 100.0 * 9.084668159484863
Epoch 40, val loss: 1.0807898044586182
Epoch 50, training loss: 902.1939086914062 = 1.07802152633667 + 100.0 * 9.01115894317627
Epoch 50, val loss: 1.0780634880065918
Epoch 60, training loss: 896.0501708984375 = 1.0754549503326416 + 100.0 * 8.949747085571289
Epoch 60, val loss: 1.0755233764648438
Epoch 70, training loss: 890.8862915039062 = 1.073073148727417 + 100.0 * 8.89813232421875
Epoch 70, val loss: 1.0731710195541382
Epoch 80, training loss: 886.5967407226562 = 1.0708746910095215 + 100.0 * 8.85525894165039
Epoch 80, val loss: 1.0710045099258423
Epoch 90, training loss: 882.9932861328125 = 1.0688621997833252 + 100.0 * 8.819244384765625
Epoch 90, val loss: 1.0690224170684814
Epoch 100, training loss: 879.7705078125 = 1.067009687423706 + 100.0 * 8.78703498840332
Epoch 100, val loss: 1.0672045946121216
Epoch 110, training loss: 877.0642700195312 = 1.0653280019760132 + 100.0 * 8.759989738464355
Epoch 110, val loss: 1.0655566453933716
Epoch 120, training loss: 874.7094116210938 = 1.0638152360916138 + 100.0 * 8.736455917358398
Epoch 120, val loss: 1.0640811920166016
Epoch 130, training loss: 872.6936645507812 = 1.062461495399475 + 100.0 * 8.716312408447266
Epoch 130, val loss: 1.0627611875534058
Epoch 140, training loss: 870.9512329101562 = 1.0612642765045166 + 100.0 * 8.698899269104004
Epoch 140, val loss: 1.0615994930267334
Epoch 150, training loss: 869.4639892578125 = 1.0602149963378906 + 100.0 * 8.684037208557129
Epoch 150, val loss: 1.0605846643447876
Epoch 160, training loss: 868.1976318359375 = 1.0593005418777466 + 100.0 * 8.671382904052734
Epoch 160, val loss: 1.0597037076950073
Epoch 170, training loss: 867.204345703125 = 1.0585136413574219 + 100.0 * 8.661458015441895
Epoch 170, val loss: 1.0589468479156494
Epoch 180, training loss: 866.3897705078125 = 1.0578505992889404 + 100.0 * 8.653319358825684
Epoch 180, val loss: 1.0583195686340332
Epoch 190, training loss: 865.324951171875 = 1.0572923421859741 + 100.0 * 8.64267635345459
Epoch 190, val loss: 1.05779230594635
Epoch 200, training loss: 864.5805053710938 = 1.0568194389343262 + 100.0 * 8.635236740112305
Epoch 200, val loss: 1.0573478937149048
Epoch 210, training loss: 863.8333129882812 = 1.0564295053482056 + 100.0 * 8.627768516540527
Epoch 210, val loss: 1.0569812059402466
Epoch 220, training loss: 863.6932983398438 = 1.056097149848938 + 100.0 * 8.626372337341309
Epoch 220, val loss: 1.0566824674606323
Epoch 230, training loss: 863.0296020507812 = 1.055862307548523 + 100.0 * 8.61973762512207
Epoch 230, val loss: 1.0564565658569336
Epoch 240, training loss: 862.5888061523438 = 1.0556637048721313 + 100.0 * 8.615331649780273
Epoch 240, val loss: 1.0562822818756104
Epoch 250, training loss: 862.1981201171875 = 1.055499792098999 + 100.0 * 8.61142635345459
Epoch 250, val loss: 1.0561379194259644
Epoch 260, training loss: 862.0892944335938 = 1.0553767681121826 + 100.0 * 8.610339164733887
Epoch 260, val loss: 1.0560321807861328
Epoch 270, training loss: 861.6455688476562 = 1.0552653074264526 + 100.0 * 8.605903625488281
Epoch 270, val loss: 1.0559337139129639
Epoch 280, training loss: 861.3057250976562 = 1.0551711320877075 + 100.0 * 8.602505683898926
Epoch 280, val loss: 1.0558538436889648
Epoch 290, training loss: 861.2048950195312 = 1.0551023483276367 + 100.0 * 8.601497650146484
Epoch 290, val loss: 1.055793285369873
Epoch 300, training loss: 861.0538330078125 = 1.0550439357757568 + 100.0 * 8.599987983703613
Epoch 300, val loss: 1.0557444095611572
Epoch 310, training loss: 860.915771484375 = 1.0549931526184082 + 100.0 * 8.598608016967773
Epoch 310, val loss: 1.0557111501693726
Epoch 320, training loss: 861.1015014648438 = 1.05495285987854 + 100.0 * 8.600465774536133
Epoch 320, val loss: 1.0556710958480835
Epoch 330, training loss: 860.81396484375 = 1.0549179315567017 + 100.0 * 8.597590446472168
Epoch 330, val loss: 1.0556405782699585
Epoch 340, training loss: 860.7470703125 = 1.0548852682113647 + 100.0 * 8.596921920776367
Epoch 340, val loss: 1.0556093454360962
Epoch 350, training loss: 860.78759765625 = 1.0548487901687622 + 100.0 * 8.59732723236084
Epoch 350, val loss: 1.05557382106781
Epoch 360, training loss: 860.4635620117188 = 1.0548121929168701 + 100.0 * 8.594087600708008
Epoch 360, val loss: 1.0555427074432373
Epoch 370, training loss: 860.67138671875 = 1.0548012256622314 + 100.0 * 8.596165657043457
Epoch 370, val loss: 1.0555312633514404
Epoch 380, training loss: 860.7592163085938 = 1.05476713180542 + 100.0 * 8.597044944763184
Epoch 380, val loss: 1.0555046796798706
Epoch 390, training loss: 860.739013671875 = 1.0547459125518799 + 100.0 * 8.596842765808105
Epoch 390, val loss: 1.0554720163345337
Epoch 400, training loss: 860.6430053710938 = 1.0546839237213135 + 100.0 * 8.5958833694458
Epoch 400, val loss: 1.055431842803955
Epoch 410, training loss: 860.7872314453125 = 1.0546727180480957 + 100.0 * 8.597325325012207
Epoch 410, val loss: 1.0554172992706299
Epoch 420, training loss: 861.65966796875 = 1.0546681880950928 + 100.0 * 8.606049537658691
Epoch 420, val loss: 1.055418610572815
Epoch 430, training loss: 860.1805419921875 = 1.0546185970306396 + 100.0 * 8.591259002685547
Epoch 430, val loss: 1.0553553104400635
Epoch 440, training loss: 861.0718994140625 = 1.054627537727356 + 100.0 * 8.600172996520996
Epoch 440, val loss: 1.0553687810897827
Epoch 450, training loss: 860.72021484375 = 1.0545819997787476 + 100.0 * 8.596656799316406
Epoch 450, val loss: 1.0553325414657593
Epoch 460, training loss: 860.7298583984375 = 1.0545616149902344 + 100.0 * 8.596753120422363
Epoch 460, val loss: 1.055315375328064
Epoch 470, training loss: 860.9578247070312 = 1.054567575454712 + 100.0 * 8.599032402038574
Epoch 470, val loss: 1.0553138256072998
Epoch 480, training loss: 861.2371215820312 = 1.054557204246521 + 100.0 * 8.601825714111328
Epoch 480, val loss: 1.0553046464920044
Epoch 490, training loss: 861.29541015625 = 1.0545414686203003 + 100.0 * 8.602408409118652
Epoch 490, val loss: 1.0552854537963867
Epoch 500, training loss: 861.4039916992188 = 1.0545145273208618 + 100.0 * 8.603494644165039
Epoch 500, val loss: 1.0552618503570557
Epoch 510, training loss: 861.8750610351562 = 1.0544813871383667 + 100.0 * 8.608205795288086
Epoch 510, val loss: 1.0552279949188232
Epoch 520, training loss: 862.9257202148438 = 1.0545156002044678 + 100.0 * 8.618712425231934
Epoch 520, val loss: 1.055253028869629
Epoch 530, training loss: 862.1896362304688 = 1.0544524192810059 + 100.0 * 8.61135196685791
Epoch 530, val loss: 1.0551930665969849
Epoch 540, training loss: 861.6347045898438 = 1.0544095039367676 + 100.0 * 8.605803489685059
Epoch 540, val loss: 1.055165410041809
Epoch 550, training loss: 862.0360107421875 = 1.05441153049469 + 100.0 * 8.609816551208496
Epoch 550, val loss: 1.0551562309265137
Epoch 560, training loss: 862.550048828125 = 1.054412841796875 + 100.0 * 8.614956855773926
Epoch 560, val loss: 1.0551464557647705
Epoch 570, training loss: 862.7827758789062 = 1.054381251335144 + 100.0 * 8.617283821105957
Epoch 570, val loss: 1.0551267862319946
Epoch 580, training loss: 862.9052124023438 = 1.0543709993362427 + 100.0 * 8.618508338928223
Epoch 580, val loss: 1.0551085472106934
Epoch 590, training loss: 863.3048706054688 = 1.0543557405471802 + 100.0 * 8.622505187988281
Epoch 590, val loss: 1.0550988912582397
Epoch 600, training loss: 863.570556640625 = 1.054339051246643 + 100.0 * 8.625162124633789
Epoch 600, val loss: 1.0550816059112549
Epoch 610, training loss: 863.8465576171875 = 1.0543200969696045 + 100.0 * 8.627922058105469
Epoch 610, val loss: 1.0550581216812134
Epoch 620, training loss: 863.9974975585938 = 1.05430269241333 + 100.0 * 8.62943172454834
Epoch 620, val loss: 1.0550366640090942
Epoch 630, training loss: 864.1190185546875 = 1.054279088973999 + 100.0 * 8.630647659301758
Epoch 630, val loss: 1.055022120475769
Epoch 640, training loss: 864.1181640625 = 1.0542621612548828 + 100.0 * 8.63063907623291
Epoch 640, val loss: 1.0550035238265991
Epoch 650, training loss: 864.3838500976562 = 1.054248571395874 + 100.0 * 8.633296012878418
Epoch 650, val loss: 1.0549886226654053
Epoch 660, training loss: 864.5984497070312 = 1.0542371273040771 + 100.0 * 8.635441780090332
Epoch 660, val loss: 1.054968237876892
Epoch 670, training loss: 864.9107055664062 = 1.054221272468567 + 100.0 * 8.638565063476562
Epoch 670, val loss: 1.0549575090408325
Epoch 680, training loss: 865.162109375 = 1.0542055368423462 + 100.0 * 8.64107894897461
Epoch 680, val loss: 1.0549384355545044
Epoch 690, training loss: 865.085693359375 = 1.054175615310669 + 100.0 * 8.640315055847168
Epoch 690, val loss: 1.0549075603485107
Epoch 700, training loss: 865.188232421875 = 1.0541568994522095 + 100.0 * 8.641341209411621
Epoch 700, val loss: 1.054884672164917
Epoch 710, training loss: 865.523193359375 = 1.0541479587554932 + 100.0 * 8.64469051361084
Epoch 710, val loss: 1.0548831224441528
Epoch 720, training loss: 865.552001953125 = 1.054131031036377 + 100.0 * 8.644978523254395
Epoch 720, val loss: 1.05485999584198
Epoch 730, training loss: 865.8264770507812 = 1.0541194677352905 + 100.0 * 8.647723197937012
Epoch 730, val loss: 1.0548529624938965
Epoch 740, training loss: 865.2650756835938 = 1.0540810823440552 + 100.0 * 8.642109870910645
Epoch 740, val loss: 1.0548051595687866
Epoch 750, training loss: 865.0718994140625 = 1.0540175437927246 + 100.0 * 8.640178680419922
Epoch 750, val loss: 1.0547798871994019
Epoch 760, training loss: 865.4791870117188 = 1.0540059804916382 + 100.0 * 8.644251823425293
Epoch 760, val loss: 1.0547327995300293
Epoch 770, training loss: 865.4852905273438 = 1.0540043115615845 + 100.0 * 8.644312858581543
Epoch 770, val loss: 1.0547349452972412
Epoch 780, training loss: 865.2044067382812 = 1.0539600849151611 + 100.0 * 8.641504287719727
Epoch 780, val loss: 1.0546960830688477
Epoch 790, training loss: 865.6722412109375 = 1.0539617538452148 + 100.0 * 8.646183013916016
Epoch 790, val loss: 1.0546857118606567
Epoch 800, training loss: 866.38037109375 = 1.0539700984954834 + 100.0 * 8.653264045715332
Epoch 800, val loss: 1.0547003746032715
Epoch 810, training loss: 866.874267578125 = 1.0539671182632446 + 100.0 * 8.658203125
Epoch 810, val loss: 1.0546929836273193
Epoch 820, training loss: 866.8232421875 = 1.05394446849823 + 100.0 * 8.657692909240723
Epoch 820, val loss: 1.0546691417694092
Epoch 830, training loss: 866.91015625 = 1.0539230108261108 + 100.0 * 8.658562660217285
Epoch 830, val loss: 1.0546458959579468
Epoch 840, training loss: 866.9004516601562 = 1.0538884401321411 + 100.0 * 8.658465385437012
Epoch 840, val loss: 1.0546132326126099
Epoch 850, training loss: 867.1871948242188 = 1.0538768768310547 + 100.0 * 8.661333084106445
Epoch 850, val loss: 1.0545998811721802
Epoch 860, training loss: 867.6215209960938 = 1.0538638830184937 + 100.0 * 8.66567611694336
Epoch 860, val loss: 1.0545870065689087
Epoch 870, training loss: 867.8787231445312 = 1.0538567304611206 + 100.0 * 8.668249130249023
Epoch 870, val loss: 1.0545706748962402
Epoch 880, training loss: 868.1470947265625 = 1.0538336038589478 + 100.0 * 8.67093276977539
Epoch 880, val loss: 1.054552435874939
Epoch 890, training loss: 868.1495971679688 = 1.0537903308868408 + 100.0 * 8.670958518981934
Epoch 890, val loss: 1.0545079708099365
Epoch 900, training loss: 867.7872314453125 = 1.0537512302398682 + 100.0 * 8.66733455657959
Epoch 900, val loss: 1.0544769763946533
Epoch 910, training loss: 867.558837890625 = 1.0537103414535522 + 100.0 * 8.665051460266113
Epoch 910, val loss: 1.0544428825378418
Epoch 920, training loss: 867.4358520507812 = 1.0536937713623047 + 100.0 * 8.66382122039795
Epoch 920, val loss: 1.0544277429580688
Epoch 930, training loss: 868.455322265625 = 1.0537010431289673 + 100.0 * 8.674015998840332
Epoch 930, val loss: 1.0544203519821167
Epoch 940, training loss: 869.0770874023438 = 1.0537134408950806 + 100.0 * 8.6802339553833
Epoch 940, val loss: 1.0544331073760986
Epoch 950, training loss: 869.3112182617188 = 1.053699254989624 + 100.0 * 8.682575225830078
Epoch 950, val loss: 1.0544134378433228
Epoch 960, training loss: 868.9837036132812 = 1.053638219833374 + 100.0 * 8.679300308227539
Epoch 960, val loss: 1.054381251335144
Epoch 970, training loss: 868.7186889648438 = 1.053592562675476 + 100.0 * 8.676651000976562
Epoch 970, val loss: 1.0543242692947388
Epoch 980, training loss: 868.9193115234375 = 1.053572177886963 + 100.0 * 8.678657531738281
Epoch 980, val loss: 1.054299235343933
Epoch 990, training loss: 869.6469116210938 = 1.0535838603973389 + 100.0 * 8.685933113098145
Epoch 990, val loss: 1.0543036460876465
Epoch 1000, training loss: 869.7754516601562 = 1.053563117980957 + 100.0 * 8.68721866607666
Epoch 1000, val loss: 1.0542881488800049
Epoch 1010, training loss: 870.4871826171875 = 1.0535633563995361 + 100.0 * 8.6943359375
Epoch 1010, val loss: 1.0542891025543213
Epoch 1020, training loss: 870.82861328125 = 1.0535502433776855 + 100.0 * 8.69775104522705
Epoch 1020, val loss: 1.0542683601379395
Epoch 1030, training loss: 870.7727661132812 = 1.0535222291946411 + 100.0 * 8.697192192077637
Epoch 1030, val loss: 1.0542410612106323
Epoch 1040, training loss: 871.1595458984375 = 1.0535088777542114 + 100.0 * 8.70106029510498
Epoch 1040, val loss: 1.0542271137237549
Epoch 1050, training loss: 871.3348999023438 = 1.0534796714782715 + 100.0 * 8.702814102172852
Epoch 1050, val loss: 1.0542083978652954
Epoch 1060, training loss: 871.585205078125 = 1.053464651107788 + 100.0 * 8.705317497253418
Epoch 1060, val loss: 1.0541781187057495
Epoch 1070, training loss: 871.2021484375 = 1.0534217357635498 + 100.0 * 8.70148754119873
Epoch 1070, val loss: 1.05414617061615
Epoch 1080, training loss: 871.6803588867188 = 1.0534120798110962 + 100.0 * 8.706269264221191
Epoch 1080, val loss: 1.0541268587112427
Epoch 1090, training loss: 871.5578002929688 = 1.053382158279419 + 100.0 * 8.70504379272461
Epoch 1090, val loss: 1.054104208946228
Epoch 1100, training loss: 871.92724609375 = 1.0533589124679565 + 100.0 * 8.708739280700684
Epoch 1100, val loss: 1.0540876388549805
Epoch 1110, training loss: 871.7289428710938 = 1.0533088445663452 + 100.0 * 8.706756591796875
Epoch 1110, val loss: 1.0540472269058228
Epoch 1120, training loss: 872.1239624023438 = 1.0532865524291992 + 100.0 * 8.71070671081543
Epoch 1120, val loss: 1.0540215969085693
Epoch 1130, training loss: 872.496826171875 = 1.0532855987548828 + 100.0 * 8.714435577392578
Epoch 1130, val loss: 1.0540151596069336
Epoch 1140, training loss: 872.5337524414062 = 1.0532559156417847 + 100.0 * 8.714804649353027
Epoch 1140, val loss: 1.0539885759353638
Epoch 1150, training loss: 872.7493896484375 = 1.0532399415969849 + 100.0 * 8.716961860656738
Epoch 1150, val loss: 1.0539706945419312
Epoch 1160, training loss: 872.9973754882812 = 1.0532176494598389 + 100.0 * 8.719441413879395
Epoch 1160, val loss: 1.0539470911026
Epoch 1170, training loss: 872.9494018554688 = 1.053175926208496 + 100.0 * 8.718962669372559
Epoch 1170, val loss: 1.0539144277572632
Epoch 1180, training loss: 872.8641967773438 = 1.0531330108642578 + 100.0 * 8.718110084533691
Epoch 1180, val loss: 1.0538634061813354
Epoch 1190, training loss: 873.3259887695312 = 1.0531182289123535 + 100.0 * 8.722728729248047
Epoch 1190, val loss: 1.0538486242294312
Epoch 1200, training loss: 873.675048828125 = 1.0531083345413208 + 100.0 * 8.726219177246094
Epoch 1200, val loss: 1.0538406372070312
Epoch 1210, training loss: 873.885498046875 = 1.0530835390090942 + 100.0 * 8.728323936462402
Epoch 1210, val loss: 1.0538057088851929
Epoch 1220, training loss: 874.0787353515625 = 1.0530623197555542 + 100.0 * 8.730257034301758
Epoch 1220, val loss: 1.0537827014923096
Epoch 1230, training loss: 874.2186279296875 = 1.0530369281768799 + 100.0 * 8.731656074523926
Epoch 1230, val loss: 1.0537554025650024
Epoch 1240, training loss: 874.3171997070312 = 1.053004264831543 + 100.0 * 8.73264217376709
Epoch 1240, val loss: 1.0537328720092773
Epoch 1250, training loss: 874.3865356445312 = 1.0529520511627197 + 100.0 * 8.733335494995117
Epoch 1250, val loss: 1.0536917448043823
Epoch 1260, training loss: 874.7333374023438 = 1.0529389381408691 + 100.0 * 8.736804008483887
Epoch 1260, val loss: 1.0536692142486572
Epoch 1270, training loss: 874.7520141601562 = 1.052907943725586 + 100.0 * 8.736990928649902
Epoch 1270, val loss: 1.0536277294158936
Epoch 1280, training loss: 874.689453125 = 1.0528411865234375 + 100.0 * 8.736366271972656
Epoch 1280, val loss: 1.0535364151000977
Epoch 1290, training loss: 874.3683471679688 = 1.0527862310409546 + 100.0 * 8.733155250549316
Epoch 1290, val loss: 1.0535178184509277
Epoch 1300, training loss: 874.8574829101562 = 1.0527890920639038 + 100.0 * 8.738046646118164
Epoch 1300, val loss: 1.0535104274749756
Epoch 1310, training loss: 875.1101684570312 = 1.0527690649032593 + 100.0 * 8.74057388305664
Epoch 1310, val loss: 1.0534961223602295
Epoch 1320, training loss: 875.3533935546875 = 1.0527487993240356 + 100.0 * 8.743006706237793
Epoch 1320, val loss: 1.053466796875
Epoch 1330, training loss: 875.4981689453125 = 1.0527238845825195 + 100.0 * 8.744454383850098
Epoch 1330, val loss: 1.0534453392028809
Epoch 1340, training loss: 875.6617431640625 = 1.0526925325393677 + 100.0 * 8.74609088897705
Epoch 1340, val loss: 1.0534111261367798
Epoch 1350, training loss: 875.8453369140625 = 1.052659034729004 + 100.0 * 8.747926712036133
Epoch 1350, val loss: 1.053385615348816
Epoch 1360, training loss: 875.9779663085938 = 1.0526328086853027 + 100.0 * 8.749253273010254
Epoch 1360, val loss: 1.053357481956482
Epoch 1370, training loss: 875.9295043945312 = 1.0525834560394287 + 100.0 * 8.74876880645752
Epoch 1370, val loss: 1.0532951354980469
Epoch 1380, training loss: 876.361328125 = 1.0525583028793335 + 100.0 * 8.753087997436523
Epoch 1380, val loss: 1.053282618522644
Epoch 1390, training loss: 876.578857421875 = 1.0525319576263428 + 100.0 * 8.755263328552246
Epoch 1390, val loss: 1.0532621145248413
Epoch 1400, training loss: 876.771484375 = 1.0524992942810059 + 100.0 * 8.757189750671387
Epoch 1400, val loss: 1.0532145500183105
Epoch 1410, training loss: 876.8787841796875 = 1.052456021308899 + 100.0 * 8.75826358795166
Epoch 1410, val loss: 1.0531771183013916
Epoch 1420, training loss: 876.7786254882812 = 1.0524195432662964 + 100.0 * 8.757262229919434
Epoch 1420, val loss: 1.0531436204910278
Epoch 1430, training loss: 876.9334716796875 = 1.0523678064346313 + 100.0 * 8.758810997009277
Epoch 1430, val loss: 1.0530965328216553
Epoch 1440, training loss: 876.8181762695312 = 1.05233633518219 + 100.0 * 8.757658958435059
Epoch 1440, val loss: 1.0530630350112915
Epoch 1450, training loss: 877.2942504882812 = 1.0523203611373901 + 100.0 * 8.762419700622559
Epoch 1450, val loss: 1.0530351400375366
Epoch 1460, training loss: 877.2935180664062 = 1.052278757095337 + 100.0 * 8.762412071228027
Epoch 1460, val loss: 1.0529879331588745
Epoch 1470, training loss: 877.4187622070312 = 1.0522454977035522 + 100.0 * 8.763665199279785
Epoch 1470, val loss: 1.0529651641845703
Epoch 1480, training loss: 877.4982299804688 = 1.052211046218872 + 100.0 * 8.764460563659668
Epoch 1480, val loss: 1.0529286861419678
Epoch 1490, training loss: 877.5858764648438 = 1.0521618127822876 + 100.0 * 8.765336990356445
Epoch 1490, val loss: 1.0528779029846191
Epoch 1500, training loss: 877.1798095703125 = 1.0520902872085571 + 100.0 * 8.761277198791504
Epoch 1500, val loss: 1.0527839660644531
Epoch 1510, training loss: 877.1990966796875 = 1.0520247220993042 + 100.0 * 8.761470794677734
Epoch 1510, val loss: 1.0527368783950806
Epoch 1520, training loss: 877.1776733398438 = 1.0519402027130127 + 100.0 * 8.76125717163086
Epoch 1520, val loss: 1.0526673793792725
Epoch 1530, training loss: 875.669677734375 = 1.051824927330017 + 100.0 * 8.74617862701416
Epoch 1530, val loss: 1.0525797605514526
Epoch 1540, training loss: 876.215576171875 = 1.051809549331665 + 100.0 * 8.75163745880127
Epoch 1540, val loss: 1.0525513887405396
Epoch 1550, training loss: 876.78759765625 = 1.0518252849578857 + 100.0 * 8.757357597351074
Epoch 1550, val loss: 1.0525455474853516
Epoch 1560, training loss: 877.4942626953125 = 1.0518298149108887 + 100.0 * 8.764424324035645
Epoch 1560, val loss: 1.0525473356246948
Epoch 1570, training loss: 878.0641479492188 = 1.0518321990966797 + 100.0 * 8.770123481750488
Epoch 1570, val loss: 1.0525527000427246
Epoch 1580, training loss: 878.4526977539062 = 1.0518107414245605 + 100.0 * 8.774008750915527
Epoch 1580, val loss: 1.0525271892547607
Epoch 1590, training loss: 878.3748779296875 = 1.0517663955688477 + 100.0 * 8.773231506347656
Epoch 1590, val loss: 1.0524808168411255
Epoch 1600, training loss: 878.3932495117188 = 1.0517197847366333 + 100.0 * 8.773415565490723
Epoch 1600, val loss: 1.0524344444274902
Epoch 1610, training loss: 878.4915771484375 = 1.0516782999038696 + 100.0 * 8.774398803710938
Epoch 1610, val loss: 1.0523847341537476
Epoch 1620, training loss: 878.7582397460938 = 1.051635980606079 + 100.0 * 8.777066230773926
Epoch 1620, val loss: 1.0523545742034912
Epoch 1630, training loss: 878.9039916992188 = 1.051601767539978 + 100.0 * 8.778524398803711
Epoch 1630, val loss: 1.0523144006729126
Epoch 1640, training loss: 879.1616821289062 = 1.0515669584274292 + 100.0 * 8.78110122680664
Epoch 1640, val loss: 1.0522923469543457
Epoch 1650, training loss: 879.421630859375 = 1.0515437126159668 + 100.0 * 8.783700942993164
Epoch 1650, val loss: 1.0522552728652954
Epoch 1660, training loss: 879.2633056640625 = 1.0514672994613647 + 100.0 * 8.782118797302246
Epoch 1660, val loss: 1.052183985710144
Epoch 1670, training loss: 878.7946166992188 = 1.0513688325881958 + 100.0 * 8.777432441711426
Epoch 1670, val loss: 1.0520734786987305
Epoch 1680, training loss: 878.9547729492188 = 1.0513170957565308 + 100.0 * 8.779034614562988
Epoch 1680, val loss: 1.052038550376892
Epoch 1690, training loss: 879.2730102539062 = 1.0513135194778442 + 100.0 * 8.782217025756836
Epoch 1690, val loss: 1.0520312786102295
Epoch 1700, training loss: 879.5733642578125 = 1.0512996912002563 + 100.0 * 8.785221099853516
Epoch 1700, val loss: 1.0520271062850952
Epoch 1710, training loss: 879.5938110351562 = 1.0512354373931885 + 100.0 * 8.785426139831543
Epoch 1710, val loss: 1.0519710779190063
Epoch 1720, training loss: 879.67919921875 = 1.051199197769165 + 100.0 * 8.786279678344727
Epoch 1720, val loss: 1.051928162574768
Epoch 1730, training loss: 879.913818359375 = 1.0511654615402222 + 100.0 * 8.788626670837402
Epoch 1730, val loss: 1.0518808364868164
Epoch 1740, training loss: 880.3018798828125 = 1.0511255264282227 + 100.0 * 8.79250717163086
Epoch 1740, val loss: 1.0518543720245361
Epoch 1750, training loss: 880.3050537109375 = 1.0510849952697754 + 100.0 * 8.792539596557617
Epoch 1750, val loss: 1.0517996549606323
Epoch 1760, training loss: 878.7105712890625 = 1.0508673191070557 + 100.0 * 8.776597023010254
Epoch 1760, val loss: 1.051629662513733
Epoch 1770, training loss: 878.6590576171875 = 1.050795078277588 + 100.0 * 8.776082992553711
Epoch 1770, val loss: 1.0515527725219727
Epoch 1780, training loss: 879.4462280273438 = 1.0508137941360474 + 100.0 * 8.783953666687012
Epoch 1780, val loss: 1.0515646934509277
Epoch 1790, training loss: 879.7606201171875 = 1.0508230924606323 + 100.0 * 8.787097930908203
Epoch 1790, val loss: 1.051556944847107
Epoch 1800, training loss: 880.6786499023438 = 1.050827980041504 + 100.0 * 8.79627799987793
Epoch 1800, val loss: 1.051558494567871
Epoch 1810, training loss: 880.2427978515625 = 1.050736665725708 + 100.0 * 8.79192066192627
Epoch 1810, val loss: 1.0514464378356934
Epoch 1820, training loss: 880.3611450195312 = 1.0506819486618042 + 100.0 * 8.793105125427246
Epoch 1820, val loss: 1.0514079332351685
Epoch 1830, training loss: 880.9327392578125 = 1.05066978931427 + 100.0 * 8.798820495605469
Epoch 1830, val loss: 1.0513986349105835
Epoch 1840, training loss: 881.1597900390625 = 1.050635814666748 + 100.0 * 8.801091194152832
Epoch 1840, val loss: 1.0513560771942139
Epoch 1850, training loss: 880.9913330078125 = 1.050554633140564 + 100.0 * 8.799407958984375
Epoch 1850, val loss: 1.0512775182724
Epoch 1860, training loss: 880.9970703125 = 1.050490140914917 + 100.0 * 8.799466133117676
Epoch 1860, val loss: 1.051217794418335
Epoch 1870, training loss: 881.2674560546875 = 1.0504597425460815 + 100.0 * 8.802169799804688
Epoch 1870, val loss: 1.0511904954910278
Epoch 1880, training loss: 881.3538208007812 = 1.0504127740859985 + 100.0 * 8.803033828735352
Epoch 1880, val loss: 1.0511386394500732
Epoch 1890, training loss: 881.2852783203125 = 1.0503507852554321 + 100.0 * 8.802349090576172
Epoch 1890, val loss: 1.0510839223861694
Epoch 1900, training loss: 881.0552368164062 = 1.0502461194992065 + 100.0 * 8.800049781799316
Epoch 1900, val loss: 1.0509933233261108
Epoch 1910, training loss: 881.2789916992188 = 1.0501859188079834 + 100.0 * 8.802288055419922
Epoch 1910, val loss: 1.050944209098816
Epoch 1920, training loss: 881.5278930664062 = 1.0501766204833984 + 100.0 * 8.804777145385742
Epoch 1920, val loss: 1.0509169101715088
Epoch 1930, training loss: 881.8848876953125 = 1.0501378774642944 + 100.0 * 8.808347702026367
Epoch 1930, val loss: 1.0508724451065063
Epoch 1940, training loss: 882.0072631835938 = 1.0500913858413696 + 100.0 * 8.809571266174316
Epoch 1940, val loss: 1.0508036613464355
Epoch 1950, training loss: 881.6159057617188 = 1.0499783754348755 + 100.0 * 8.805659294128418
Epoch 1950, val loss: 1.050700068473816
Epoch 1960, training loss: 881.6407470703125 = 1.0499240159988403 + 100.0 * 8.805908203125
Epoch 1960, val loss: 1.0506646633148193
Epoch 1970, training loss: 882.1193237304688 = 1.049905776977539 + 100.0 * 8.810693740844727
Epoch 1970, val loss: 1.0506372451782227
Epoch 1980, training loss: 881.7150268554688 = 1.049791932106018 + 100.0 * 8.806652069091797
Epoch 1980, val loss: 1.0505234003067017
Epoch 1990, training loss: 881.3425903320312 = 1.0496269464492798 + 100.0 * 8.802929878234863
Epoch 1990, val loss: 1.0503407716751099
Epoch 2000, training loss: 881.0045166015625 = 1.0495153665542603 + 100.0 * 8.79955005645752
Epoch 2000, val loss: 1.0502853393554688
Epoch 2010, training loss: 881.1713256835938 = 1.0494418144226074 + 100.0 * 8.80121898651123
Epoch 2010, val loss: 1.050230622291565
Epoch 2020, training loss: 881.279052734375 = 1.0494014024734497 + 100.0 * 8.80229663848877
Epoch 2020, val loss: 1.050171971321106
Epoch 2030, training loss: 881.9718627929688 = 1.0494163036346436 + 100.0 * 8.809224128723145
Epoch 2030, val loss: 1.0501753091812134
Epoch 2040, training loss: 882.5602416992188 = 1.049420714378357 + 100.0 * 8.815108299255371
Epoch 2040, val loss: 1.0501673221588135
Epoch 2050, training loss: 882.2426147460938 = 1.0493543148040771 + 100.0 * 8.811932563781738
Epoch 2050, val loss: 1.0500963926315308
Epoch 2060, training loss: 882.510009765625 = 1.049299716949463 + 100.0 * 8.814606666564941
Epoch 2060, val loss: 1.0500521659851074
Epoch 2070, training loss: 882.8837890625 = 1.0492662191390991 + 100.0 * 8.818345069885254
Epoch 2070, val loss: 1.0500049591064453
Epoch 2080, training loss: 883.130615234375 = 1.0492169857025146 + 100.0 * 8.82081413269043
Epoch 2080, val loss: 1.049949049949646
Epoch 2090, training loss: 883.2255249023438 = 1.0491583347320557 + 100.0 * 8.82176399230957
Epoch 2090, val loss: 1.049909234046936
Epoch 2100, training loss: 883.1253662109375 = 1.049086332321167 + 100.0 * 8.820762634277344
Epoch 2100, val loss: 1.0498230457305908
Epoch 2110, training loss: 882.9984130859375 = 1.0490022897720337 + 100.0 * 8.819494247436523
Epoch 2110, val loss: 1.0497491359710693
Epoch 2120, training loss: 883.2681884765625 = 1.0489517450332642 + 100.0 * 8.822192192077637
Epoch 2120, val loss: 1.0497044324874878
Epoch 2130, training loss: 883.3412475585938 = 1.04889714717865 + 100.0 * 8.82292366027832
Epoch 2130, val loss: 1.0496323108673096
Epoch 2140, training loss: 883.3829345703125 = 1.0488123893737793 + 100.0 * 8.823341369628906
Epoch 2140, val loss: 1.0495301485061646
Epoch 2150, training loss: 883.2882080078125 = 1.0487453937530518 + 100.0 * 8.822394371032715
Epoch 2150, val loss: 1.0494805574417114
Epoch 2160, training loss: 883.3753662109375 = 1.0486735105514526 + 100.0 * 8.823266983032227
Epoch 2160, val loss: 1.0494227409362793
Epoch 2170, training loss: 883.2275390625 = 1.048567533493042 + 100.0 * 8.821789741516113
Epoch 2170, val loss: 1.0493121147155762
Epoch 2180, training loss: 883.4537963867188 = 1.048512578010559 + 100.0 * 8.824052810668945
Epoch 2180, val loss: 1.0492587089538574
Epoch 2190, training loss: 883.8106079101562 = 1.0484790802001953 + 100.0 * 8.827621459960938
Epoch 2190, val loss: 1.0492005348205566
Epoch 2200, training loss: 883.7345581054688 = 1.0483884811401367 + 100.0 * 8.826861381530762
Epoch 2200, val loss: 1.0491095781326294
Epoch 2210, training loss: 883.6822509765625 = 1.0483044385910034 + 100.0 * 8.826339721679688
Epoch 2210, val loss: 1.0490659475326538
Epoch 2220, training loss: 884.1721801757812 = 1.0482723712921143 + 100.0 * 8.831238746643066
Epoch 2220, val loss: 1.0490162372589111
Epoch 2230, training loss: 884.3621826171875 = 1.048227310180664 + 100.0 * 8.833139419555664
Epoch 2230, val loss: 1.048965573310852
Epoch 2240, training loss: 884.2371215820312 = 1.0481237173080444 + 100.0 * 8.831890106201172
Epoch 2240, val loss: 1.0488437414169312
Epoch 2250, training loss: 884.0361328125 = 1.0479986667633057 + 100.0 * 8.82988166809082
Epoch 2250, val loss: 1.048742413520813
Epoch 2260, training loss: 883.2745361328125 = 1.0474441051483154 + 100.0 * 8.822271347045898
Epoch 2260, val loss: 1.048022747039795
Epoch 2270, training loss: 880.176513671875 = 1.0469282865524292 + 100.0 * 8.791296005249023
Epoch 2270, val loss: 1.0477060079574585
Epoch 2280, training loss: 877.21044921875 = 1.0460848808288574 + 100.0 * 8.761643409729004
Epoch 2280, val loss: 1.0469233989715576
Epoch 2290, training loss: 879.3851318359375 = 1.0468525886535645 + 100.0 * 8.783382415771484
Epoch 2290, val loss: 1.0475623607635498
Epoch 2300, training loss: 879.1049194335938 = 1.0466666221618652 + 100.0 * 8.780582427978516
Epoch 2300, val loss: 1.0474497079849243
Epoch 2310, training loss: 880.18603515625 = 1.046836018562317 + 100.0 * 8.79139232635498
Epoch 2310, val loss: 1.0475784540176392
Epoch 2320, training loss: 880.965087890625 = 1.046901822090149 + 100.0 * 8.799181938171387
Epoch 2320, val loss: 1.0476421117782593
Epoch 2330, training loss: 881.6692504882812 = 1.0469458103179932 + 100.0 * 8.806222915649414
Epoch 2330, val loss: 1.047674298286438
Epoch 2340, training loss: 882.2288208007812 = 1.0469335317611694 + 100.0 * 8.811819076538086
Epoch 2340, val loss: 1.0476648807525635
Epoch 2350, training loss: 882.6792602539062 = 1.0469259023666382 + 100.0 * 8.816323280334473
Epoch 2350, val loss: 1.0476467609405518
Epoch 2360, training loss: 882.843017578125 = 1.0468790531158447 + 100.0 * 8.817961692810059
Epoch 2360, val loss: 1.047593593597412
Epoch 2370, training loss: 883.08837890625 = 1.0468186140060425 + 100.0 * 8.820415496826172
Epoch 2370, val loss: 1.0475401878356934
Epoch 2380, training loss: 883.2789306640625 = 1.0467488765716553 + 100.0 * 8.822321891784668
Epoch 2380, val loss: 1.0474764108657837
Epoch 2390, training loss: 883.5416259765625 = 1.0466928482055664 + 100.0 * 8.824949264526367
Epoch 2390, val loss: 1.0474121570587158
Epoch 2400, training loss: 883.5195922851562 = 1.0465961694717407 + 100.0 * 8.824729919433594
Epoch 2400, val loss: 1.0473135709762573
Epoch 2410, training loss: 883.6611328125 = 1.0465272665023804 + 100.0 * 8.826146125793457
Epoch 2410, val loss: 1.0472508668899536
Epoch 2420, training loss: 883.9989624023438 = 1.0464779138565063 + 100.0 * 8.829524993896484
Epoch 2420, val loss: 1.0471985340118408
Epoch 2430, training loss: 884.0261840820312 = 1.0463786125183105 + 100.0 * 8.829797744750977
Epoch 2430, val loss: 1.0471147298812866
Epoch 2440, training loss: 884.0097045898438 = 1.0462913513183594 + 100.0 * 8.829634666442871
Epoch 2440, val loss: 1.0470118522644043
Epoch 2450, training loss: 884.3330688476562 = 1.046216607093811 + 100.0 * 8.832868576049805
Epoch 2450, val loss: 1.0469411611557007
Epoch 2460, training loss: 884.2681274414062 = 1.0461355447769165 + 100.0 * 8.832220077514648
Epoch 2460, val loss: 1.0468471050262451
Epoch 2470, training loss: 884.3386840820312 = 1.0460454225540161 + 100.0 * 8.832926750183105
Epoch 2470, val loss: 1.046756386756897
Epoch 2480, training loss: 884.3182373046875 = 1.0459566116333008 + 100.0 * 8.832722663879395
Epoch 2480, val loss: 1.0466687679290771
Epoch 2490, training loss: 884.582275390625 = 1.0458669662475586 + 100.0 * 8.83536434173584
Epoch 2490, val loss: 1.0465821027755737
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.40695652173913044
0.8634354850394842
=== training gcn model ===
Epoch 0, training loss: 1001.8053588867188 = 1.0788905620574951 + 100.0 * 10.007264137268066
Epoch 0, val loss: 1.077871322631836
Epoch 10, training loss: 952.6885986328125 = 1.0763221979141235 + 100.0 * 9.516122817993164
Epoch 10, val loss: 1.075357437133789
Epoch 20, training loss: 934.2744140625 = 1.0739448070526123 + 100.0 * 9.33200454711914
Epoch 20, val loss: 1.0730260610580444
Epoch 30, training loss: 922.7097778320312 = 1.0716760158538818 + 100.0 * 9.216381072998047
Epoch 30, val loss: 1.0708222389221191
Epoch 40, training loss: 913.6519775390625 = 1.069586992263794 + 100.0 * 9.125823974609375
Epoch 40, val loss: 1.068809151649475
Epoch 50, training loss: 906.1332397460938 = 1.0677154064178467 + 100.0 * 9.050655364990234
Epoch 50, val loss: 1.067021369934082
Epoch 60, training loss: 899.7789306640625 = 1.066024661064148 + 100.0 * 8.987129211425781
Epoch 60, val loss: 1.0654195547103882
Epoch 70, training loss: 894.2884521484375 = 1.0644911527633667 + 100.0 * 8.932239532470703
Epoch 70, val loss: 1.063981533050537
Epoch 80, training loss: 889.574951171875 = 1.0631133317947388 + 100.0 * 8.88511848449707
Epoch 80, val loss: 1.062703251838684
Epoch 90, training loss: 885.3831787109375 = 1.0618839263916016 + 100.0 * 8.843213081359863
Epoch 90, val loss: 1.0615770816802979
Epoch 100, training loss: 881.8515014648438 = 1.0608030557632446 + 100.0 * 8.807907104492188
Epoch 100, val loss: 1.0605944395065308
Epoch 110, training loss: 878.7608032226562 = 1.0598483085632324 + 100.0 * 8.777009010314941
Epoch 110, val loss: 1.0597387552261353
Epoch 120, training loss: 875.9464111328125 = 1.059017300605774 + 100.0 * 8.748873710632324
Epoch 120, val loss: 1.058997631072998
Epoch 130, training loss: 873.6651611328125 = 1.0583035945892334 + 100.0 * 8.726068496704102
Epoch 130, val loss: 1.0583696365356445
Epoch 140, training loss: 871.7347412109375 = 1.0577038526535034 + 100.0 * 8.706770896911621
Epoch 140, val loss: 1.057851791381836
Epoch 150, training loss: 869.7870483398438 = 1.057182788848877 + 100.0 * 8.687298774719238
Epoch 150, val loss: 1.0574021339416504
Epoch 160, training loss: 868.37158203125 = 1.0567591190338135 + 100.0 * 8.673148155212402
Epoch 160, val loss: 1.0570456981658936
Epoch 170, training loss: 866.9168701171875 = 1.0564112663269043 + 100.0 * 8.658604621887207
Epoch 170, val loss: 1.056761622428894
Epoch 180, training loss: 865.8092651367188 = 1.0561189651489258 + 100.0 * 8.647531509399414
Epoch 180, val loss: 1.056525707244873
Epoch 190, training loss: 864.809814453125 = 1.0558863878250122 + 100.0 * 8.63753890991211
Epoch 190, val loss: 1.056342601776123
Epoch 200, training loss: 863.8825073242188 = 1.0556442737579346 + 100.0 * 8.628268241882324
Epoch 200, val loss: 1.0561573505401611
Epoch 210, training loss: 863.2166748046875 = 1.0555700063705444 + 100.0 * 8.621611595153809
Epoch 210, val loss: 1.0561052560806274
Epoch 220, training loss: 862.2643432617188 = 1.0554206371307373 + 100.0 * 8.612089157104492
Epoch 220, val loss: 1.0559941530227661
Epoch 230, training loss: 861.7073974609375 = 1.0553500652313232 + 100.0 * 8.606520652770996
Epoch 230, val loss: 1.0559428930282593
Epoch 240, training loss: 861.2413330078125 = 1.0552386045455933 + 100.0 * 8.601861000061035
Epoch 240, val loss: 1.0558570623397827
Epoch 250, training loss: 860.8120727539062 = 1.0552148818969727 + 100.0 * 8.59756851196289
Epoch 250, val loss: 1.0558491945266724
Epoch 260, training loss: 860.3200073242188 = 1.0551681518554688 + 100.0 * 8.59264850616455
Epoch 260, val loss: 1.0558254718780518
Epoch 270, training loss: 859.619384765625 = 1.0551118850708008 + 100.0 * 8.58564281463623
Epoch 270, val loss: 1.0557819604873657
Epoch 280, training loss: 859.3395385742188 = 1.05507230758667 + 100.0 * 8.582844734191895
Epoch 280, val loss: 1.055751085281372
Epoch 290, training loss: 859.030029296875 = 1.055038332939148 + 100.0 * 8.579750061035156
Epoch 290, val loss: 1.055723786354065
Epoch 300, training loss: 858.7320556640625 = 1.0549979209899902 + 100.0 * 8.576770782470703
Epoch 300, val loss: 1.0556899309158325
Epoch 310, training loss: 858.5962524414062 = 1.0549695491790771 + 100.0 * 8.57541275024414
Epoch 310, val loss: 1.0556681156158447
Epoch 320, training loss: 858.4805297851562 = 1.0549379587173462 + 100.0 * 8.57425594329834
Epoch 320, val loss: 1.055643081665039
Epoch 330, training loss: 858.0435791015625 = 1.054876685142517 + 100.0 * 8.569887161254883
Epoch 330, val loss: 1.0555812120437622
Epoch 340, training loss: 858.0509033203125 = 1.0548590421676636 + 100.0 * 8.569960594177246
Epoch 340, val loss: 1.055562973022461
Epoch 350, training loss: 857.8104858398438 = 1.0548404455184937 + 100.0 * 8.567556381225586
Epoch 350, val loss: 1.055548071861267
Epoch 360, training loss: 857.7921142578125 = 1.0548069477081299 + 100.0 * 8.567373275756836
Epoch 360, val loss: 1.0555222034454346
Epoch 370, training loss: 857.4888305664062 = 1.0547683238983154 + 100.0 * 8.564340591430664
Epoch 370, val loss: 1.0554864406585693
Epoch 380, training loss: 857.5352783203125 = 1.054738998413086 + 100.0 * 8.564805030822754
Epoch 380, val loss: 1.0554554462432861
Epoch 390, training loss: 857.4268798828125 = 1.0547068119049072 + 100.0 * 8.563721656799316
Epoch 390, val loss: 1.055416226387024
Epoch 400, training loss: 857.3516235351562 = 1.0546740293502808 + 100.0 * 8.562969207763672
Epoch 400, val loss: 1.0553852319717407
Epoch 410, training loss: 857.28662109375 = 1.054638147354126 + 100.0 * 8.5623197555542
Epoch 410, val loss: 1.0553556680679321
Epoch 420, training loss: 857.340576171875 = 1.0546120405197144 + 100.0 * 8.562859535217285
Epoch 420, val loss: 1.055329442024231
Epoch 430, training loss: 857.2467041015625 = 1.0545815229415894 + 100.0 * 8.561921119689941
Epoch 430, val loss: 1.0552937984466553
Epoch 440, training loss: 857.2202758789062 = 1.054539680480957 + 100.0 * 8.561656951904297
Epoch 440, val loss: 1.0552486181259155
Epoch 450, training loss: 857.1239013671875 = 1.054506778717041 + 100.0 * 8.560693740844727
Epoch 450, val loss: 1.0552226305007935
Epoch 460, training loss: 857.029296875 = 1.0544638633728027 + 100.0 * 8.559748649597168
Epoch 460, val loss: 1.0551807880401611
Epoch 470, training loss: 856.9987182617188 = 1.054439902305603 + 100.0 * 8.559442520141602
Epoch 470, val loss: 1.0551577806472778
Epoch 480, training loss: 857.0816650390625 = 1.0544209480285645 + 100.0 * 8.560272216796875
Epoch 480, val loss: 1.055132269859314
Epoch 490, training loss: 857.1666870117188 = 1.0543851852416992 + 100.0 * 8.56112289428711
Epoch 490, val loss: 1.0551034212112427
Epoch 500, training loss: 856.9902954101562 = 1.0543506145477295 + 100.0 * 8.559359550476074
Epoch 500, val loss: 1.0550646781921387
Epoch 510, training loss: 857.1285400390625 = 1.0543169975280762 + 100.0 * 8.560742378234863
Epoch 510, val loss: 1.055032730102539
Epoch 520, training loss: 857.2767944335938 = 1.054287075996399 + 100.0 * 8.562225341796875
Epoch 520, val loss: 1.0549997091293335
Epoch 530, training loss: 857.0469360351562 = 1.0541212558746338 + 100.0 * 8.559927940368652
Epoch 530, val loss: 1.0548673868179321
Epoch 540, training loss: 857.1468505859375 = 1.0541762113571167 + 100.0 * 8.56092643737793
Epoch 540, val loss: 1.0548816919326782
Epoch 550, training loss: 857.2344360351562 = 1.0541424751281738 + 100.0 * 8.561802864074707
Epoch 550, val loss: 1.05485200881958
Epoch 560, training loss: 857.4228515625 = 1.0541280508041382 + 100.0 * 8.563687324523926
Epoch 560, val loss: 1.05483877658844
Epoch 570, training loss: 857.4776611328125 = 1.0540839433670044 + 100.0 * 8.56423568725586
Epoch 570, val loss: 1.0547934770584106
Epoch 580, training loss: 857.7413940429688 = 1.0540447235107422 + 100.0 * 8.566873550415039
Epoch 580, val loss: 1.0547517538070679
Epoch 590, training loss: 857.5779418945312 = 1.05400550365448 + 100.0 * 8.565238952636719
Epoch 590, val loss: 1.0547189712524414
Epoch 600, training loss: 857.7217407226562 = 1.0539605617523193 + 100.0 * 8.566678047180176
Epoch 600, val loss: 1.0546824932098389
Epoch 610, training loss: 857.8433227539062 = 1.0539220571517944 + 100.0 * 8.567893981933594
Epoch 610, val loss: 1.0546307563781738
Epoch 620, training loss: 857.2693481445312 = 1.0538016557693481 + 100.0 * 8.562155723571777
Epoch 620, val loss: 1.0545450448989868
Epoch 630, training loss: 857.624267578125 = 1.0537195205688477 + 100.0 * 8.565705299377441
Epoch 630, val loss: 1.0544284582138062
Epoch 640, training loss: 857.928955078125 = 1.053708553314209 + 100.0 * 8.56875228881836
Epoch 640, val loss: 1.0544148683547974
Epoch 650, training loss: 857.8662109375 = 1.0537056922912598 + 100.0 * 8.568124771118164
Epoch 650, val loss: 1.0544205904006958
Epoch 660, training loss: 858.08544921875 = 1.0536856651306152 + 100.0 * 8.570317268371582
Epoch 660, val loss: 1.0544017553329468
Epoch 670, training loss: 858.4033203125 = 1.0536723136901855 + 100.0 * 8.57349681854248
Epoch 670, val loss: 1.0543824434280396
Epoch 680, training loss: 858.5238037109375 = 1.05363130569458 + 100.0 * 8.574701309204102
Epoch 680, val loss: 1.0543475151062012
Epoch 690, training loss: 858.736083984375 = 1.053590178489685 + 100.0 * 8.576825141906738
Epoch 690, val loss: 1.0542995929718018
Epoch 700, training loss: 858.816650390625 = 1.0535385608673096 + 100.0 * 8.577630996704102
Epoch 700, val loss: 1.0542500019073486
Epoch 710, training loss: 859.1114501953125 = 1.0534956455230713 + 100.0 * 8.58057975769043
Epoch 710, val loss: 1.0542057752609253
Epoch 720, training loss: 859.13720703125 = 1.0534436702728271 + 100.0 * 8.58083724975586
Epoch 720, val loss: 1.0541577339172363
Epoch 730, training loss: 859.0850830078125 = 1.053390622138977 + 100.0 * 8.580316543579102
Epoch 730, val loss: 1.0540918111801147
Epoch 740, training loss: 859.115966796875 = 1.0533238649368286 + 100.0 * 8.580626487731934
Epoch 740, val loss: 1.0540372133255005
Epoch 750, training loss: 859.2229614257812 = 1.0532983541488647 + 100.0 * 8.581696510314941
Epoch 750, val loss: 1.0540064573287964
Epoch 760, training loss: 859.7136840820312 = 1.0532538890838623 + 100.0 * 8.586604118347168
Epoch 760, val loss: 1.0539565086364746
Epoch 770, training loss: 859.4185791015625 = 1.0531747341156006 + 100.0 * 8.583654403686523
Epoch 770, val loss: 1.0538780689239502
Epoch 780, training loss: 859.4010620117188 = 1.0531140565872192 + 100.0 * 8.583479881286621
Epoch 780, val loss: 1.053824782371521
Epoch 790, training loss: 859.8809814453125 = 1.053107500076294 + 100.0 * 8.588278770446777
Epoch 790, val loss: 1.0538146495819092
Epoch 800, training loss: 860.0201416015625 = 1.0530564785003662 + 100.0 * 8.58967113494873
Epoch 800, val loss: 1.0537545680999756
Epoch 810, training loss: 860.2554321289062 = 1.053018569946289 + 100.0 * 8.592023849487305
Epoch 810, val loss: 1.0537104606628418
Epoch 820, training loss: 860.4147338867188 = 1.0529587268829346 + 100.0 * 8.59361743927002
Epoch 820, val loss: 1.0536466836929321
Epoch 830, training loss: 860.5459594726562 = 1.0529097318649292 + 100.0 * 8.594930648803711
Epoch 830, val loss: 1.0536022186279297
Epoch 840, training loss: 860.8401489257812 = 1.0528408288955688 + 100.0 * 8.597872734069824
Epoch 840, val loss: 1.053512454032898
Epoch 850, training loss: 860.3037719726562 = 1.0527504682540894 + 100.0 * 8.592510223388672
Epoch 850, val loss: 1.0534449815750122
Epoch 860, training loss: 860.5037841796875 = 1.0526752471923828 + 100.0 * 8.594511032104492
Epoch 860, val loss: 1.053375482559204
Epoch 870, training loss: 860.8977661132812 = 1.0526647567749023 + 100.0 * 8.598450660705566
Epoch 870, val loss: 1.0533591508865356
Epoch 880, training loss: 860.9205932617188 = 1.052612543106079 + 100.0 * 8.598679542541504
Epoch 880, val loss: 1.053301215171814
Epoch 890, training loss: 860.7007446289062 = 1.052513599395752 + 100.0 * 8.596482276916504
Epoch 890, val loss: 1.053199291229248
Epoch 900, training loss: 860.9188232421875 = 1.0524729490280151 + 100.0 * 8.598663330078125
Epoch 900, val loss: 1.0531680583953857
Epoch 910, training loss: 861.3287353515625 = 1.0524505376815796 + 100.0 * 8.602763175964355
Epoch 910, val loss: 1.0531452894210815
Epoch 920, training loss: 861.739501953125 = 1.0524033308029175 + 100.0 * 8.606870651245117
Epoch 920, val loss: 1.053076148033142
Epoch 930, training loss: 861.3370361328125 = 1.052308201789856 + 100.0 * 8.6028470993042
Epoch 930, val loss: 1.0530058145523071
Epoch 940, training loss: 861.8115844726562 = 1.0522816181182861 + 100.0 * 8.607592582702637
Epoch 940, val loss: 1.0529788732528687
Epoch 950, training loss: 861.8601684570312 = 1.0522091388702393 + 100.0 * 8.60807991027832
Epoch 950, val loss: 1.052899956703186
Epoch 960, training loss: 862.0025634765625 = 1.052133321762085 + 100.0 * 8.609504699707031
Epoch 960, val loss: 1.0528323650360107
Epoch 970, training loss: 862.4495849609375 = 1.0521165132522583 + 100.0 * 8.613974571228027
Epoch 970, val loss: 1.0527949333190918
Epoch 980, training loss: 862.1078491210938 = 1.0520002841949463 + 100.0 * 8.61055850982666
Epoch 980, val loss: 1.052695393562317
Epoch 990, training loss: 861.8734741210938 = 1.0519161224365234 + 100.0 * 8.60821533203125
Epoch 990, val loss: 1.0525883436203003
Epoch 1000, training loss: 861.8209228515625 = 1.0518202781677246 + 100.0 * 8.607690811157227
Epoch 1000, val loss: 1.0525057315826416
Epoch 1010, training loss: 862.2401123046875 = 1.0517890453338623 + 100.0 * 8.611883163452148
Epoch 1010, val loss: 1.052479863166809
Epoch 1020, training loss: 862.7542114257812 = 1.051783561706543 + 100.0 * 8.617024421691895
Epoch 1020, val loss: 1.0524736642837524
Epoch 1030, training loss: 863.033935546875 = 1.0517287254333496 + 100.0 * 8.619821548461914
Epoch 1030, val loss: 1.0523954629898071
Epoch 1040, training loss: 862.872802734375 = 1.0516091585159302 + 100.0 * 8.61821174621582
Epoch 1040, val loss: 1.0522829294204712
Epoch 1050, training loss: 863.0570068359375 = 1.0515592098236084 + 100.0 * 8.620054244995117
Epoch 1050, val loss: 1.0522485971450806
Epoch 1060, training loss: 863.2780151367188 = 1.0515140295028687 + 100.0 * 8.622264862060547
Epoch 1060, val loss: 1.052196741104126
Epoch 1070, training loss: 863.4063110351562 = 1.0514357089996338 + 100.0 * 8.62354850769043
Epoch 1070, val loss: 1.0521286725997925
Epoch 1080, training loss: 863.7485961914062 = 1.051385760307312 + 100.0 * 8.626972198486328
Epoch 1080, val loss: 1.052071213722229
Epoch 1090, training loss: 863.8786010742188 = 1.0513255596160889 + 100.0 * 8.628273010253906
Epoch 1090, val loss: 1.0520068407058716
Epoch 1100, training loss: 864.2001342773438 = 1.051271915435791 + 100.0 * 8.631488800048828
Epoch 1100, val loss: 1.0519423484802246
Epoch 1110, training loss: 863.5529174804688 = 1.051080346107483 + 100.0 * 8.625018119812012
Epoch 1110, val loss: 1.0517677068710327
Epoch 1120, training loss: 863.4586181640625 = 1.0510047674179077 + 100.0 * 8.624075889587402
Epoch 1120, val loss: 1.0517131090164185
Epoch 1130, training loss: 864.1124267578125 = 1.0510088205337524 + 100.0 * 8.630614280700684
Epoch 1130, val loss: 1.051702857017517
Epoch 1140, training loss: 864.6979370117188 = 1.0510069131851196 + 100.0 * 8.636468887329102
Epoch 1140, val loss: 1.0516752004623413
Epoch 1150, training loss: 864.6636352539062 = 1.0509192943572998 + 100.0 * 8.636127471923828
Epoch 1150, val loss: 1.0515676736831665
Epoch 1160, training loss: 865.0414428710938 = 1.050837755203247 + 100.0 * 8.63990592956543
Epoch 1160, val loss: 1.051497220993042
Epoch 1170, training loss: 864.876708984375 = 1.050746202468872 + 100.0 * 8.638259887695312
Epoch 1170, val loss: 1.05143404006958
Epoch 1180, training loss: 865.3170166015625 = 1.0507280826568604 + 100.0 * 8.64266300201416
Epoch 1180, val loss: 1.0513875484466553
Epoch 1190, training loss: 865.28662109375 = 1.0506446361541748 + 100.0 * 8.642359733581543
Epoch 1190, val loss: 1.051291584968567
Epoch 1200, training loss: 864.9015502929688 = 1.0504580736160278 + 100.0 * 8.638510704040527
Epoch 1200, val loss: 1.0511327981948853
Epoch 1210, training loss: 865.2317504882812 = 1.0503848791122437 + 100.0 * 8.641813278198242
Epoch 1210, val loss: 1.0510722398757935
Epoch 1220, training loss: 865.6226806640625 = 1.0503864288330078 + 100.0 * 8.645722389221191
Epoch 1220, val loss: 1.0510770082473755
Epoch 1230, training loss: 865.984619140625 = 1.0503578186035156 + 100.0 * 8.64934253692627
Epoch 1230, val loss: 1.051026701927185
Epoch 1240, training loss: 865.6843872070312 = 1.0501867532730103 + 100.0 * 8.646342277526855
Epoch 1240, val loss: 1.0508449077606201
Epoch 1250, training loss: 865.6243286132812 = 1.050085186958313 + 100.0 * 8.645742416381836
Epoch 1250, val loss: 1.0507526397705078
Epoch 1260, training loss: 865.5645141601562 = 1.0500198602676392 + 100.0 * 8.64514446258545
Epoch 1260, val loss: 1.0506755113601685
Epoch 1270, training loss: 866.0427856445312 = 1.0499911308288574 + 100.0 * 8.649928092956543
Epoch 1270, val loss: 1.0506538152694702
Epoch 1280, training loss: 866.41943359375 = 1.0499523878097534 + 100.0 * 8.653695106506348
Epoch 1280, val loss: 1.0505928993225098
Epoch 1290, training loss: 866.5172119140625 = 1.0498684644699097 + 100.0 * 8.65467357635498
Epoch 1290, val loss: 1.0504958629608154
Epoch 1300, training loss: 866.5621948242188 = 1.0497827529907227 + 100.0 * 8.655123710632324
Epoch 1300, val loss: 1.0504049062728882
Epoch 1310, training loss: 866.37744140625 = 1.049633264541626 + 100.0 * 8.653278350830078
Epoch 1310, val loss: 1.0502938032150269
Epoch 1320, training loss: 866.4913330078125 = 1.0495376586914062 + 100.0 * 8.654417991638184
Epoch 1320, val loss: 1.0502229928970337
Epoch 1330, training loss: 866.8162231445312 = 1.0494818687438965 + 100.0 * 8.65766716003418
Epoch 1330, val loss: 1.050144910812378
Epoch 1340, training loss: 867.1746215820312 = 1.049446940422058 + 100.0 * 8.66125202178955
Epoch 1340, val loss: 1.050089955329895
Epoch 1350, training loss: 867.1587524414062 = 1.0493649244308472 + 100.0 * 8.661093711853027
Epoch 1350, val loss: 1.049996018409729
Epoch 1360, training loss: 867.1755981445312 = 1.0492624044418335 + 100.0 * 8.661263465881348
Epoch 1360, val loss: 1.0499004125595093
Epoch 1370, training loss: 867.1897583007812 = 1.049185872077942 + 100.0 * 8.661405563354492
Epoch 1370, val loss: 1.0498276948928833
Epoch 1380, training loss: 867.5260620117188 = 1.0491138696670532 + 100.0 * 8.664769172668457
Epoch 1380, val loss: 1.049783706665039
Epoch 1390, training loss: 867.1348876953125 = 1.048961877822876 + 100.0 * 8.660859107971191
Epoch 1390, val loss: 1.0496302843093872
Epoch 1400, training loss: 867.5841674804688 = 1.0488842725753784 + 100.0 * 8.665352821350098
Epoch 1400, val loss: 1.0495555400848389
Epoch 1410, training loss: 867.9921875 = 1.048843502998352 + 100.0 * 8.66943359375
Epoch 1410, val loss: 1.049505591392517
Epoch 1420, training loss: 868.2351684570312 = 1.0488044023513794 + 100.0 * 8.671863555908203
Epoch 1420, val loss: 1.0494487285614014
Epoch 1430, training loss: 867.7503051757812 = 1.0485821962356567 + 100.0 * 8.667016983032227
Epoch 1430, val loss: 1.049234390258789
Epoch 1440, training loss: 867.6376342773438 = 1.0483732223510742 + 100.0 * 8.665892601013184
Epoch 1440, val loss: 1.0490021705627441
Epoch 1450, training loss: 866.8499755859375 = 1.0481231212615967 + 100.0 * 8.658019065856934
Epoch 1450, val loss: 1.04881751537323
Epoch 1460, training loss: 867.432861328125 = 1.048170566558838 + 100.0 * 8.663846969604492
Epoch 1460, val loss: 1.0488148927688599
Epoch 1470, training loss: 867.69921875 = 1.0481290817260742 + 100.0 * 8.666510581970215
Epoch 1470, val loss: 1.048774242401123
Epoch 1480, training loss: 868.0748291015625 = 1.048122763633728 + 100.0 * 8.670267105102539
Epoch 1480, val loss: 1.0487741231918335
Epoch 1490, training loss: 868.5526733398438 = 1.0480960607528687 + 100.0 * 8.67504596710205
Epoch 1490, val loss: 1.0487428903579712
Epoch 1500, training loss: 868.7739868164062 = 1.0479954481124878 + 100.0 * 8.677260398864746
Epoch 1500, val loss: 1.0486421585083008
Epoch 1510, training loss: 868.5167846679688 = 1.0478417873382568 + 100.0 * 8.674689292907715
Epoch 1510, val loss: 1.0485092401504517
Epoch 1520, training loss: 868.8441162109375 = 1.047784447669983 + 100.0 * 8.677963256835938
Epoch 1520, val loss: 1.048444151878357
Epoch 1530, training loss: 869.1178588867188 = 1.0477179288864136 + 100.0 * 8.68070125579834
Epoch 1530, val loss: 1.0483683347702026
Epoch 1540, training loss: 868.9259643554688 = 1.0475826263427734 + 100.0 * 8.678783416748047
Epoch 1540, val loss: 1.0482194423675537
Epoch 1550, training loss: 868.6546630859375 = 1.0473856925964355 + 100.0 * 8.67607307434082
Epoch 1550, val loss: 1.0480073690414429
Epoch 1560, training loss: 868.5638427734375 = 1.0472846031188965 + 100.0 * 8.675165176391602
Epoch 1560, val loss: 1.0479384660720825
Epoch 1570, training loss: 868.900390625 = 1.0472217798233032 + 100.0 * 8.678531646728516
Epoch 1570, val loss: 1.0478887557983398
Epoch 1580, training loss: 869.1773071289062 = 1.047173261642456 + 100.0 * 8.68130111694336
Epoch 1580, val loss: 1.047839879989624
Epoch 1590, training loss: 869.2470703125 = 1.0470749139785767 + 100.0 * 8.682000160217285
Epoch 1590, val loss: 1.0477265119552612
Epoch 1600, training loss: 869.6597900390625 = 1.046998143196106 + 100.0 * 8.686127662658691
Epoch 1600, val loss: 1.047669768333435
Epoch 1610, training loss: 869.4126586914062 = 1.0468413829803467 + 100.0 * 8.683658599853516
Epoch 1610, val loss: 1.0475189685821533
Epoch 1620, training loss: 869.62451171875 = 1.046740174293518 + 100.0 * 8.68577766418457
Epoch 1620, val loss: 1.0474152565002441
Epoch 1630, training loss: 869.9657592773438 = 1.0466923713684082 + 100.0 * 8.689190864562988
Epoch 1630, val loss: 1.0473436117172241
Epoch 1640, training loss: 869.8939208984375 = 1.046541690826416 + 100.0 * 8.68847370147705
Epoch 1640, val loss: 1.0472413301467896
Epoch 1650, training loss: 870.1285400390625 = 1.0464421510696411 + 100.0 * 8.690820693969727
Epoch 1650, val loss: 1.0471243858337402
Epoch 1660, training loss: 869.6826171875 = 1.0462136268615723 + 100.0 * 8.68636417388916
Epoch 1660, val loss: 1.0468124151229858
Epoch 1670, training loss: 869.2892456054688 = 1.0460221767425537 + 100.0 * 8.682432174682617
Epoch 1670, val loss: 1.0466759204864502
Epoch 1680, training loss: 869.6118774414062 = 1.0459566116333008 + 100.0 * 8.685659408569336
Epoch 1680, val loss: 1.0466371774673462
Epoch 1690, training loss: 870.2867431640625 = 1.0459645986557007 + 100.0 * 8.692407608032227
Epoch 1690, val loss: 1.0466628074645996
Epoch 1700, training loss: 870.5989990234375 = 1.0459275245666504 + 100.0 * 8.695530891418457
Epoch 1700, val loss: 1.0466363430023193
Epoch 1710, training loss: 870.46630859375 = 1.0457628965377808 + 100.0 * 8.694205284118652
Epoch 1710, val loss: 1.0464808940887451
Epoch 1720, training loss: 870.7191162109375 = 1.0456764698028564 + 100.0 * 8.696734428405762
Epoch 1720, val loss: 1.0464071035385132
Epoch 1730, training loss: 870.8853759765625 = 1.0456104278564453 + 100.0 * 8.698397636413574
Epoch 1730, val loss: 1.0462942123413086
Epoch 1740, training loss: 870.896484375 = 1.0454753637313843 + 100.0 * 8.69851016998291
Epoch 1740, val loss: 1.0461574792861938
Epoch 1750, training loss: 871.0752563476562 = 1.045385479927063 + 100.0 * 8.700298309326172
Epoch 1750, val loss: 1.046087384223938
Epoch 1760, training loss: 871.0781860351562 = 1.045214056968689 + 100.0 * 8.700329780578613
Epoch 1760, val loss: 1.0459223985671997
Epoch 1770, training loss: 871.0095825195312 = 1.0450973510742188 + 100.0 * 8.699645042419434
Epoch 1770, val loss: 1.0458056926727295
Epoch 1780, training loss: 871.0264892578125 = 1.0449804067611694 + 100.0 * 8.699814796447754
Epoch 1780, val loss: 1.045684814453125
Epoch 1790, training loss: 871.279296875 = 1.044909954071045 + 100.0 * 8.702343940734863
Epoch 1790, val loss: 1.0455974340438843
Epoch 1800, training loss: 871.6426391601562 = 1.0448229312896729 + 100.0 * 8.705978393554688
Epoch 1800, val loss: 1.045552372932434
Epoch 1810, training loss: 871.70068359375 = 1.0446687936782837 + 100.0 * 8.706560134887695
Epoch 1810, val loss: 1.04537034034729
Epoch 1820, training loss: 871.5382080078125 = 1.0445201396942139 + 100.0 * 8.704936981201172
Epoch 1820, val loss: 1.0452144145965576
Epoch 1830, training loss: 871.7582397460938 = 1.0444226264953613 + 100.0 * 8.707138061523438
Epoch 1830, val loss: 1.045106053352356
Epoch 1840, training loss: 871.7015380859375 = 1.044071912765503 + 100.0 * 8.706574440002441
Epoch 1840, val loss: 1.044716715812683
Epoch 1850, training loss: 871.264404296875 = 1.043954849243164 + 100.0 * 8.702204704284668
Epoch 1850, val loss: 1.0446573495864868
Epoch 1860, training loss: 871.662109375 = 1.0439649820327759 + 100.0 * 8.706181526184082
Epoch 1860, val loss: 1.0446622371673584
Epoch 1870, training loss: 872.1696166992188 = 1.0439672470092773 + 100.0 * 8.711256980895996
Epoch 1870, val loss: 1.0446656942367554
Epoch 1880, training loss: 872.4412231445312 = 1.0439302921295166 + 100.0 * 8.713973045349121
Epoch 1880, val loss: 1.0446346998214722
Epoch 1890, training loss: 872.1107177734375 = 1.0436897277832031 + 100.0 * 8.710670471191406
Epoch 1890, val loss: 1.0444164276123047
Epoch 1900, training loss: 872.3788452148438 = 1.0436288118362427 + 100.0 * 8.71335220336914
Epoch 1900, val loss: 1.0443437099456787
Epoch 1910, training loss: 872.718505859375 = 1.0435707569122314 + 100.0 * 8.71674919128418
Epoch 1910, val loss: 1.0442813634872437
Epoch 1920, training loss: 872.8636474609375 = 1.0434378385543823 + 100.0 * 8.718201637268066
Epoch 1920, val loss: 1.044152021408081
Epoch 1930, training loss: 872.6875 = 1.043266773223877 + 100.0 * 8.716442108154297
Epoch 1930, val loss: 1.0440033674240112
Epoch 1940, training loss: 872.7777709960938 = 1.0431386232376099 + 100.0 * 8.71734619140625
Epoch 1940, val loss: 1.043867826461792
Epoch 1950, training loss: 873.1005249023438 = 1.043082356452942 + 100.0 * 8.720574378967285
Epoch 1950, val loss: 1.04379141330719
Epoch 1960, training loss: 871.0751342773438 = 1.042167067527771 + 100.0 * 8.700329780578613
Epoch 1960, val loss: 1.0428917407989502
Epoch 1970, training loss: 870.0659790039062 = 1.0411843061447144 + 100.0 * 8.690247535705566
Epoch 1970, val loss: 1.041975736618042
Epoch 1980, training loss: 870.2778930664062 = 1.0408192873001099 + 100.0 * 8.692370414733887
Epoch 1980, val loss: 1.041085958480835
Epoch 1990, training loss: 870.3836059570312 = 1.0412393808364868 + 100.0 * 8.6934232711792
Epoch 1990, val loss: 1.0420372486114502
Epoch 2000, training loss: 869.472412109375 = 1.0408049821853638 + 100.0 * 8.68431568145752
Epoch 2000, val loss: 1.0415472984313965
Epoch 2010, training loss: 869.6675415039062 = 1.0408508777618408 + 100.0 * 8.686266899108887
Epoch 2010, val loss: 1.0416433811187744
Epoch 2020, training loss: 870.269287109375 = 1.0411477088928223 + 100.0 * 8.692281723022461
Epoch 2020, val loss: 1.0418657064437866
Epoch 2030, training loss: 870.816650390625 = 1.0411560535430908 + 100.0 * 8.697754859924316
Epoch 2030, val loss: 1.0419490337371826
Epoch 2040, training loss: 871.3624877929688 = 1.0412240028381348 + 100.0 * 8.70321273803711
Epoch 2040, val loss: 1.0419793128967285
Epoch 2050, training loss: 872.0218505859375 = 1.0412201881408691 + 100.0 * 8.709806442260742
Epoch 2050, val loss: 1.041982650756836
Epoch 2060, training loss: 872.2374877929688 = 1.041180968284607 + 100.0 * 8.711962699890137
Epoch 2060, val loss: 1.0419278144836426
Epoch 2070, training loss: 872.4371337890625 = 1.041085124015808 + 100.0 * 8.713960647583008
Epoch 2070, val loss: 1.0418235063552856
Epoch 2080, training loss: 872.5194091796875 = 1.0409657955169678 + 100.0 * 8.714784622192383
Epoch 2080, val loss: 1.0417213439941406
Epoch 2090, training loss: 872.7568969726562 = 1.0408730506896973 + 100.0 * 8.71716022491455
Epoch 2090, val loss: 1.0416245460510254
Epoch 2100, training loss: 872.7708740234375 = 1.0407097339630127 + 100.0 * 8.717301368713379
Epoch 2100, val loss: 1.041466474533081
Epoch 2110, training loss: 872.8634033203125 = 1.0405875444412231 + 100.0 * 8.718228340148926
Epoch 2110, val loss: 1.0413256883621216
Epoch 2120, training loss: 873.1859130859375 = 1.0405040979385376 + 100.0 * 8.721453666687012
Epoch 2120, val loss: 1.0412685871124268
Epoch 2130, training loss: 873.3385620117188 = 1.0404034852981567 + 100.0 * 8.722981452941895
Epoch 2130, val loss: 1.0411436557769775
Epoch 2140, training loss: 873.4409790039062 = 1.0402525663375854 + 100.0 * 8.724007606506348
Epoch 2140, val loss: 1.0409852266311646
Epoch 2150, training loss: 873.2993774414062 = 1.040060043334961 + 100.0 * 8.722593307495117
Epoch 2150, val loss: 1.040786623954773
Epoch 2160, training loss: 873.4900512695312 = 1.0399216413497925 + 100.0 * 8.724501609802246
Epoch 2160, val loss: 1.0406873226165771
Epoch 2170, training loss: 873.5514526367188 = 1.0397896766662598 + 100.0 * 8.725116729736328
Epoch 2170, val loss: 1.0405607223510742
Epoch 2180, training loss: 873.8148803710938 = 1.039708137512207 + 100.0 * 8.727751731872559
Epoch 2180, val loss: 1.040482521057129
Epoch 2190, training loss: 873.8890991210938 = 1.0395479202270508 + 100.0 * 8.728495597839355
Epoch 2190, val loss: 1.0402820110321045
Epoch 2200, training loss: 872.567626953125 = 1.0386312007904053 + 100.0 * 8.715290069580078
Epoch 2200, val loss: 1.0394123792648315
Epoch 2210, training loss: 873.11279296875 = 1.0388606786727905 + 100.0 * 8.720739364624023
Epoch 2210, val loss: 1.0395959615707397
Epoch 2220, training loss: 873.6345825195312 = 1.0388890504837036 + 100.0 * 8.725956916809082
Epoch 2220, val loss: 1.039731502532959
Epoch 2230, training loss: 873.8292236328125 = 1.0388352870941162 + 100.0 * 8.727904319763184
Epoch 2230, val loss: 1.0396863222122192
Epoch 2240, training loss: 874.2277221679688 = 1.0387582778930664 + 100.0 * 8.731889724731445
Epoch 2240, val loss: 1.0395961999893188
Epoch 2250, training loss: 874.2382202148438 = 1.038572072982788 + 100.0 * 8.731996536254883
Epoch 2250, val loss: 1.039414405822754
Epoch 2260, training loss: 874.2825317382812 = 1.0384430885314941 + 100.0 * 8.732440948486328
Epoch 2260, val loss: 1.0392611026763916
Epoch 2270, training loss: 874.2877197265625 = 1.0382716655731201 + 100.0 * 8.732494354248047
Epoch 2270, val loss: 1.0391321182250977
Epoch 2280, training loss: 874.5926513671875 = 1.038169264793396 + 100.0 * 8.73554515838623
Epoch 2280, val loss: 1.038999319076538
Epoch 2290, training loss: 874.8988037109375 = 1.0380644798278809 + 100.0 * 8.738607406616211
Epoch 2290, val loss: 1.0389045476913452
Epoch 2300, training loss: 875.0283203125 = 1.0378831624984741 + 100.0 * 8.739904403686523
Epoch 2300, val loss: 1.0387212038040161
Epoch 2310, training loss: 874.511474609375 = 1.0375497341156006 + 100.0 * 8.734739303588867
Epoch 2310, val loss: 1.038434386253357
Epoch 2320, training loss: 874.5217895507812 = 1.0373005867004395 + 100.0 * 8.734845161437988
Epoch 2320, val loss: 1.038182258605957
Epoch 2330, training loss: 874.8723754882812 = 1.037305235862732 + 100.0 * 8.738350868225098
Epoch 2330, val loss: 1.0381274223327637
Epoch 2340, training loss: 874.3576049804688 = 1.03701913356781 + 100.0 * 8.733205795288086
Epoch 2340, val loss: 1.0378974676132202
Epoch 2350, training loss: 874.8072509765625 = 1.0369576215744019 + 100.0 * 8.737702369689941
Epoch 2350, val loss: 1.0378575325012207
Epoch 2360, training loss: 875.0747680664062 = 1.0369126796722412 + 100.0 * 8.740378379821777
Epoch 2360, val loss: 1.0377800464630127
Epoch 2370, training loss: 875.1666870117188 = 1.0367259979248047 + 100.0 * 8.741299629211426
Epoch 2370, val loss: 1.037583351135254
Epoch 2380, training loss: 875.2113647460938 = 1.0365597009658813 + 100.0 * 8.741747856140137
Epoch 2380, val loss: 1.037461519241333
Epoch 2390, training loss: 874.9817504882812 = 1.036283254623413 + 100.0 * 8.739455223083496
Epoch 2390, val loss: 1.0372250080108643
Epoch 2400, training loss: 875.1873168945312 = 1.0361732244491577 + 100.0 * 8.741511344909668
Epoch 2400, val loss: 1.0371172428131104
Epoch 2410, training loss: 875.5413208007812 = 1.0360887050628662 + 100.0 * 8.745052337646484
Epoch 2410, val loss: 1.0369950532913208
Epoch 2420, training loss: 875.7064819335938 = 1.0359610319137573 + 100.0 * 8.746705055236816
Epoch 2420, val loss: 1.0368373394012451
Epoch 2430, training loss: 875.7327880859375 = 1.0357619524002075 + 100.0 * 8.746970176696777
Epoch 2430, val loss: 1.0366636514663696
Epoch 2440, training loss: 875.770263671875 = 1.035606861114502 + 100.0 * 8.747346878051758
Epoch 2440, val loss: 1.0364980697631836
Epoch 2450, training loss: 875.7849731445312 = 1.0354106426239014 + 100.0 * 8.747495651245117
Epoch 2450, val loss: 1.0363426208496094
Epoch 2460, training loss: 875.858642578125 = 1.0352582931518555 + 100.0 * 8.748233795166016
Epoch 2460, val loss: 1.0361634492874146
Epoch 2470, training loss: 876.0642700195312 = 1.0351536273956299 + 100.0 * 8.750290870666504
Epoch 2470, val loss: 1.0360685586929321
Epoch 2480, training loss: 876.1851196289062 = 1.0350115299224854 + 100.0 * 8.751501083374023
Epoch 2480, val loss: 1.035904884338379
Epoch 2490, training loss: 876.3004760742188 = 1.0348261594772339 + 100.0 * 8.752655982971191
Epoch 2490, val loss: 1.0357534885406494
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.4885507246376811
0.864667101354778
The final CL Acc:0.43072, 0.04110, The final GNN Acc:0.86418, 0.00054
Begin epxeriment: noisy_level: 0.2 cont_weight: 100 epoch: der1:0.3 der2:0.3 dfr1:0.4 dfr2:0.4
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=2500, cl_num_layers=2, cl_num_proj_hidden=64, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=100.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.3, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=32, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, noisy_level=0.2, num_hidden=64, num_proj_hidden=64, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=40, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 88648])
add edge: torch.Size([2, 106314])
remove edge: torch.Size([2, 70944])
updated graph: torch.Size([2, 88610])
=== Noisy graph ===
/home/project-robust-contrastive/Robust-Contrastive-Learning/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 1033.1793212890625 = 1.088482141494751 + 100.0 * 10.320908546447754
Epoch 0, val loss: 1.0905866622924805
Epoch 10, training loss: 991.0699462890625 = 1.0852423906326294 + 100.0 * 9.899847030639648
Epoch 10, val loss: 1.0873973369598389
Epoch 20, training loss: 971.5297241210938 = 1.082291603088379 + 100.0 * 9.704474449157715
Epoch 20, val loss: 1.0844569206237793
Epoch 30, training loss: 957.7348022460938 = 1.0794020891189575 + 100.0 * 9.566554069519043
Epoch 30, val loss: 1.0816001892089844
Epoch 40, training loss: 946.7052612304688 = 1.0767686367034912 + 100.0 * 9.456284523010254
Epoch 40, val loss: 1.0789765119552612
Epoch 50, training loss: 937.5554809570312 = 1.074280858039856 + 100.0 * 9.364811897277832
Epoch 50, val loss: 1.076487421989441
Epoch 60, training loss: 929.9361572265625 = 1.0719462633132935 + 100.0 * 9.288641929626465
Epoch 60, val loss: 1.0741394758224487
Epoch 70, training loss: 923.4554443359375 = 1.0697540044784546 + 100.0 * 9.223856925964355
Epoch 70, val loss: 1.0719199180603027
Epoch 80, training loss: 917.8370971679688 = 1.0676665306091309 + 100.0 * 9.167694091796875
Epoch 80, val loss: 1.069792628288269
Epoch 90, training loss: 912.8932495117188 = 1.0656979084014893 + 100.0 * 9.11827564239502
Epoch 90, val loss: 1.0677756071090698
Epoch 100, training loss: 908.5838623046875 = 1.06380033493042 + 100.0 * 9.075201034545898
Epoch 100, val loss: 1.0658272504806519
Epoch 110, training loss: 904.8180541992188 = 1.0619935989379883 + 100.0 * 9.03756046295166
Epoch 110, val loss: 1.0639691352844238
Epoch 120, training loss: 901.4970092773438 = 1.060232400894165 + 100.0 * 9.00436782836914
Epoch 120, val loss: 1.0621618032455444
Epoch 130, training loss: 898.6712036132812 = 1.0585271120071411 + 100.0 * 8.976126670837402
Epoch 130, val loss: 1.060401201248169
Epoch 140, training loss: 896.2772827148438 = 1.0568667650222778 + 100.0 * 8.952203750610352
Epoch 140, val loss: 1.0586882829666138
Epoch 150, training loss: 894.306884765625 = 1.0551745891571045 + 100.0 * 8.932517051696777
Epoch 150, val loss: 1.05696702003479
Epoch 160, training loss: 892.4500732421875 = 1.0535515546798706 + 100.0 * 8.913965225219727
Epoch 160, val loss: 1.055340051651001
Epoch 170, training loss: 890.5003051757812 = 1.0519071817398071 + 100.0 * 8.894484519958496
Epoch 170, val loss: 1.0536588430404663
Epoch 180, training loss: 889.4070434570312 = 1.050354242324829 + 100.0 * 8.883566856384277
Epoch 180, val loss: 1.0521116256713867
Epoch 190, training loss: 888.1135864257812 = 1.0486271381378174 + 100.0 * 8.870649337768555
Epoch 190, val loss: 1.0503709316253662
Epoch 200, training loss: 887.36767578125 = 1.0469794273376465 + 100.0 * 8.86320686340332
Epoch 200, val loss: 1.0487498044967651
Epoch 210, training loss: 886.541259765625 = 1.04526686668396 + 100.0 * 8.854959487915039
Epoch 210, val loss: 1.0470346212387085
Epoch 220, training loss: 885.5172729492188 = 1.0434434413909912 + 100.0 * 8.844738006591797
Epoch 220, val loss: 1.0452607870101929
Epoch 230, training loss: 884.745361328125 = 1.0415358543395996 + 100.0 * 8.837038040161133
Epoch 230, val loss: 1.043402910232544
Epoch 240, training loss: 884.2630004882812 = 1.039574384689331 + 100.0 * 8.832234382629395
Epoch 240, val loss: 1.0415047407150269
Epoch 250, training loss: 883.672119140625 = 1.0374969244003296 + 100.0 * 8.826346397399902
Epoch 250, val loss: 1.039486289024353
Epoch 260, training loss: 883.1912841796875 = 1.0352988243103027 + 100.0 * 8.82155990600586
Epoch 260, val loss: 1.037375569343567
Epoch 270, training loss: 883.179443359375 = 1.032943844795227 + 100.0 * 8.821464538574219
Epoch 270, val loss: 1.035111665725708
Epoch 280, training loss: 882.0785522460938 = 1.0304385423660278 + 100.0 * 8.810481071472168
Epoch 280, val loss: 1.0327614545822144
Epoch 290, training loss: 881.6865234375 = 1.027854561805725 + 100.0 * 8.806587219238281
Epoch 290, val loss: 1.030198574066162
Epoch 300, training loss: 881.3936157226562 = 1.0250743627548218 + 100.0 * 8.803685188293457
Epoch 300, val loss: 1.027571439743042
Epoch 310, training loss: 881.3521728515625 = 1.022168755531311 + 100.0 * 8.803299903869629
Epoch 310, val loss: 1.0248130559921265
Epoch 320, training loss: 881.2221069335938 = 1.0190500020980835 + 100.0 * 8.802030563354492
Epoch 320, val loss: 1.0218709707260132
Epoch 330, training loss: 881.6041259765625 = 1.0158101320266724 + 100.0 * 8.805883407592773
Epoch 330, val loss: 1.0188018083572388
Epoch 340, training loss: 880.3433837890625 = 1.0122017860412598 + 100.0 * 8.793312072753906
Epoch 340, val loss: 1.0153709650039673
Epoch 350, training loss: 880.1613159179688 = 1.0087226629257202 + 100.0 * 8.791525840759277
Epoch 350, val loss: 1.0120528936386108
Epoch 360, training loss: 880.3594360351562 = 1.004904866218567 + 100.0 * 8.793545722961426
Epoch 360, val loss: 1.008428931236267
Epoch 370, training loss: 880.389404296875 = 1.001002311706543 + 100.0 * 8.79388427734375
Epoch 370, val loss: 1.004737138748169
Epoch 380, training loss: 880.4100341796875 = 0.9968308210372925 + 100.0 * 8.794132232666016
Epoch 380, val loss: 1.0007809400558472
Epoch 390, training loss: 880.178955078125 = 0.9924939870834351 + 100.0 * 8.791864395141602
Epoch 390, val loss: 0.9967185258865356
Epoch 400, training loss: 880.3566284179688 = 0.988059401512146 + 100.0 * 8.793685913085938
Epoch 400, val loss: 0.9925389885902405
Epoch 410, training loss: 880.249267578125 = 0.9833795428276062 + 100.0 * 8.792658805847168
Epoch 410, val loss: 0.9881045818328857
Epoch 420, training loss: 879.8355102539062 = 0.9784680008888245 + 100.0 * 8.788570404052734
Epoch 420, val loss: 0.9835278391838074
Epoch 430, training loss: 879.9708862304688 = 0.9734155535697937 + 100.0 * 8.7899751663208
Epoch 430, val loss: 0.9787465333938599
Epoch 440, training loss: 879.9699096679688 = 0.9684044718742371 + 100.0 * 8.79001522064209
Epoch 440, val loss: 0.9740134477615356
Epoch 450, training loss: 880.1494750976562 = 0.9633047580718994 + 100.0 * 8.791861534118652
Epoch 450, val loss: 0.9692144989967346
Epoch 460, training loss: 879.9930419921875 = 0.9579518437385559 + 100.0 * 8.790350914001465
Epoch 460, val loss: 0.9642283916473389
Epoch 470, training loss: 880.18115234375 = 0.9525968432426453 + 100.0 * 8.792285919189453
Epoch 470, val loss: 0.9591954350471497
Epoch 480, training loss: 880.3258666992188 = 0.9471429586410522 + 100.0 * 8.793787002563477
Epoch 480, val loss: 0.9541098475456238
Epoch 490, training loss: 880.339111328125 = 0.9415297508239746 + 100.0 * 8.793975830078125
Epoch 490, val loss: 0.9488698840141296
Epoch 500, training loss: 880.8351440429688 = 0.9358536601066589 + 100.0 * 8.798993110656738
Epoch 500, val loss: 0.9435455203056335
Epoch 510, training loss: 880.7031860351562 = 0.9300482869148254 + 100.0 * 8.797731399536133
Epoch 510, val loss: 0.9382686018943787
Epoch 520, training loss: 880.50048828125 = 0.9242496490478516 + 100.0 * 8.795762062072754
Epoch 520, val loss: 0.9328808784484863
Epoch 530, training loss: 880.6947021484375 = 0.9185067415237427 + 100.0 * 8.797761917114258
Epoch 530, val loss: 0.927525520324707
Epoch 540, training loss: 881.0698852539062 = 0.9126057624816895 + 100.0 * 8.801572799682617
Epoch 540, val loss: 0.9220775365829468
Epoch 550, training loss: 881.0731201171875 = 0.9066534042358398 + 100.0 * 8.801664352416992
Epoch 550, val loss: 0.9165800213813782
Epoch 560, training loss: 881.1939086914062 = 0.9005155563354492 + 100.0 * 8.802933692932129
Epoch 560, val loss: 0.9109026193618774
Epoch 570, training loss: 881.19970703125 = 0.8945348858833313 + 100.0 * 8.803051948547363
Epoch 570, val loss: 0.9055047631263733
Epoch 580, training loss: 881.0283813476562 = 0.8885668516159058 + 100.0 * 8.801398277282715
Epoch 580, val loss: 0.9000840187072754
Epoch 590, training loss: 881.2646484375 = 0.8826707005500793 + 100.0 * 8.80381965637207
Epoch 590, val loss: 0.8946378827095032
Epoch 600, training loss: 881.6865844726562 = 0.8767328262329102 + 100.0 * 8.808098793029785
Epoch 600, val loss: 0.8892536759376526
Epoch 610, training loss: 881.792236328125 = 0.87065190076828 + 100.0 * 8.809215545654297
Epoch 610, val loss: 0.8836544752120972
Epoch 620, training loss: 882.2303466796875 = 0.8646829724311829 + 100.0 * 8.8136568069458
Epoch 620, val loss: 0.8782298564910889
Epoch 630, training loss: 881.8718872070312 = 0.8585946559906006 + 100.0 * 8.81013298034668
Epoch 630, val loss: 0.8727476596832275
Epoch 640, training loss: 882.2139892578125 = 0.8526951670646667 + 100.0 * 8.813612937927246
Epoch 640, val loss: 0.8672915697097778
Epoch 650, training loss: 883.372314453125 = 0.846809446811676 + 100.0 * 8.825255393981934
Epoch 650, val loss: 0.8619293570518494
Epoch 660, training loss: 882.5620727539062 = 0.8405845165252686 + 100.0 * 8.817214965820312
Epoch 660, val loss: 0.8563013076782227
Epoch 670, training loss: 882.4197387695312 = 0.8347113728523254 + 100.0 * 8.815850257873535
Epoch 670, val loss: 0.851062536239624
Epoch 680, training loss: 882.4495239257812 = 0.8285952806472778 + 100.0 * 8.816208839416504
Epoch 680, val loss: 0.845576822757721
Epoch 690, training loss: 882.7777099609375 = 0.8226549625396729 + 100.0 * 8.819550514221191
Epoch 690, val loss: 0.8401631116867065
Epoch 700, training loss: 883.8429565429688 = 0.8167195916175842 + 100.0 * 8.830262184143066
Epoch 700, val loss: 0.8347740173339844
Epoch 710, training loss: 883.2277221679688 = 0.8106071352958679 + 100.0 * 8.82417106628418
Epoch 710, val loss: 0.8291776776313782
Epoch 720, training loss: 883.8759765625 = 0.8048239946365356 + 100.0 * 8.830711364746094
Epoch 720, val loss: 0.8241124153137207
Epoch 730, training loss: 883.0494995117188 = 0.7986357808113098 + 100.0 * 8.822508811950684
Epoch 730, val loss: 0.8186458349227905
Epoch 740, training loss: 883.5886840820312 = 0.7928743958473206 + 100.0 * 8.827958106994629
Epoch 740, val loss: 0.8134751319885254
Epoch 750, training loss: 883.8494873046875 = 0.7866421341896057 + 100.0 * 8.830628395080566
Epoch 750, val loss: 0.8080115914344788
Epoch 760, training loss: 883.443115234375 = 0.7808481454849243 + 100.0 * 8.82662296295166
Epoch 760, val loss: 0.8027118444442749
Epoch 770, training loss: 884.0552978515625 = 0.7751470804214478 + 100.0 * 8.832801818847656
Epoch 770, val loss: 0.7976499199867249
Epoch 780, training loss: 884.51806640625 = 0.7693571448326111 + 100.0 * 8.83748722076416
Epoch 780, val loss: 0.7925747036933899
Epoch 790, training loss: 884.940673828125 = 0.7634953856468201 + 100.0 * 8.841772079467773
Epoch 790, val loss: 0.7873466610908508
Epoch 800, training loss: 884.9899291992188 = 0.7575343251228333 + 100.0 * 8.842324256896973
Epoch 800, val loss: 0.7820664644241333
Epoch 810, training loss: 885.5213623046875 = 0.7517387270927429 + 100.0 * 8.847696304321289
Epoch 810, val loss: 0.7769203782081604
Epoch 820, training loss: 884.8362426757812 = 0.7457232475280762 + 100.0 * 8.84090518951416
Epoch 820, val loss: 0.7714422941207886
Epoch 830, training loss: 884.0647583007812 = 0.7397544384002686 + 100.0 * 8.833250045776367
Epoch 830, val loss: 0.7661996483802795
Epoch 840, training loss: 885.0386352539062 = 0.7344014048576355 + 100.0 * 8.843042373657227
Epoch 840, val loss: 0.7615642547607422
Epoch 850, training loss: 885.1612548828125 = 0.7287501692771912 + 100.0 * 8.844325065612793
Epoch 850, val loss: 0.7567089796066284
Epoch 860, training loss: 886.054931640625 = 0.7235803008079529 + 100.0 * 8.853313446044922
Epoch 860, val loss: 0.752089262008667
Epoch 870, training loss: 886.3307495117188 = 0.7177767753601074 + 100.0 * 8.85612964630127
Epoch 870, val loss: 0.7471578121185303
Epoch 880, training loss: 886.4085083007812 = 0.7122589945793152 + 100.0 * 8.856962203979492
Epoch 880, val loss: 0.7423129081726074
Epoch 890, training loss: 886.2212524414062 = 0.7068129181861877 + 100.0 * 8.855144500732422
Epoch 890, val loss: 0.7374597191810608
Epoch 900, training loss: 886.5257568359375 = 0.7011714577674866 + 100.0 * 8.858245849609375
Epoch 900, val loss: 0.7325226664543152
Epoch 910, training loss: 885.5125122070312 = 0.6960272192955017 + 100.0 * 8.848164558410645
Epoch 910, val loss: 0.7281537055969238
Epoch 920, training loss: 886.5632934570312 = 0.691107451915741 + 100.0 * 8.858721733093262
Epoch 920, val loss: 0.724040687084198
Epoch 930, training loss: 887.2213745117188 = 0.6862720251083374 + 100.0 * 8.865350723266602
Epoch 930, val loss: 0.7198878526687622
Epoch 940, training loss: 887.0362548828125 = 0.6812158823013306 + 100.0 * 8.863550186157227
Epoch 940, val loss: 0.7155497670173645
Epoch 950, training loss: 887.2939453125 = 0.6763870716094971 + 100.0 * 8.866175651550293
Epoch 950, val loss: 0.7115360498428345
Epoch 960, training loss: 887.2761840820312 = 0.671738862991333 + 100.0 * 8.866044044494629
Epoch 960, val loss: 0.7076343894004822
Epoch 970, training loss: 887.0225219726562 = 0.6670984625816345 + 100.0 * 8.863554000854492
Epoch 970, val loss: 0.7037652134895325
Epoch 980, training loss: 887.7564697265625 = 0.6626793146133423 + 100.0 * 8.870938301086426
Epoch 980, val loss: 0.70017009973526
Epoch 990, training loss: 888.3467407226562 = 0.6583896279335022 + 100.0 * 8.876883506774902
Epoch 990, val loss: 0.6965130567550659
Epoch 1000, training loss: 889.111083984375 = 0.6542069911956787 + 100.0 * 8.884568214416504
Epoch 1000, val loss: 0.6931110620498657
Epoch 1010, training loss: 888.2254028320312 = 0.6499471664428711 + 100.0 * 8.875754356384277
Epoch 1010, val loss: 0.6896374821662903
Epoch 1020, training loss: 888.4579467773438 = 0.6459908485412598 + 100.0 * 8.878119468688965
Epoch 1020, val loss: 0.6864138841629028
Epoch 1030, training loss: 888.690185546875 = 0.6420891880989075 + 100.0 * 8.880480766296387
Epoch 1030, val loss: 0.683223307132721
Epoch 1040, training loss: 889.1093139648438 = 0.6383371353149414 + 100.0 * 8.884709358215332
Epoch 1040, val loss: 0.680190920829773
Epoch 1050, training loss: 889.4349975585938 = 0.634649395942688 + 100.0 * 8.8880033493042
Epoch 1050, val loss: 0.6772424578666687
Epoch 1060, training loss: 889.600830078125 = 0.6310298442840576 + 100.0 * 8.889698028564453
Epoch 1060, val loss: 0.6744399070739746
Epoch 1070, training loss: 889.5394897460938 = 0.6275222897529602 + 100.0 * 8.889120101928711
Epoch 1070, val loss: 0.6716960072517395
Epoch 1080, training loss: 888.9969482421875 = 0.6240347027778625 + 100.0 * 8.883728981018066
Epoch 1080, val loss: 0.6689358949661255
Epoch 1090, training loss: 889.3292236328125 = 0.6207479238510132 + 100.0 * 8.8870849609375
Epoch 1090, val loss: 0.666495680809021
Epoch 1100, training loss: 889.8434448242188 = 0.6175702810287476 + 100.0 * 8.892258644104004
Epoch 1100, val loss: 0.6639714241027832
Epoch 1110, training loss: 890.4553833007812 = 0.6145336627960205 + 100.0 * 8.898407936096191
Epoch 1110, val loss: 0.6617493629455566
Epoch 1120, training loss: 890.6206665039062 = 0.6114498972892761 + 100.0 * 8.900092124938965
Epoch 1120, val loss: 0.6594294905662537
Epoch 1130, training loss: 889.443603515625 = 0.6082937121391296 + 100.0 * 8.88835334777832
Epoch 1130, val loss: 0.6571521759033203
Epoch 1140, training loss: 890.1453247070312 = 0.6054274439811707 + 100.0 * 8.89539909362793
Epoch 1140, val loss: 0.6551297903060913
Epoch 1150, training loss: 890.3069458007812 = 0.6027054786682129 + 100.0 * 8.897042274475098
Epoch 1150, val loss: 0.653136670589447
Epoch 1160, training loss: 889.1767578125 = 0.6000500917434692 + 100.0 * 8.885766983032227
Epoch 1160, val loss: 0.6512908935546875
Epoch 1170, training loss: 890.2344360351562 = 0.5974600315093994 + 100.0 * 8.896369934082031
Epoch 1170, val loss: 0.6494503617286682
Epoch 1180, training loss: 891.3501586914062 = 0.5949581265449524 + 100.0 * 8.907551765441895
Epoch 1180, val loss: 0.6477242708206177
Epoch 1190, training loss: 891.1866455078125 = 0.5923390984535217 + 100.0 * 8.905942916870117
Epoch 1190, val loss: 0.6459060907363892
Epoch 1200, training loss: 891.3099365234375 = 0.5898371338844299 + 100.0 * 8.907200813293457
Epoch 1200, val loss: 0.6442226767539978
Epoch 1210, training loss: 891.7608032226562 = 0.5874418616294861 + 100.0 * 8.911733627319336
Epoch 1210, val loss: 0.6425805687904358
Epoch 1220, training loss: 892.1461791992188 = 0.5851165652275085 + 100.0 * 8.915610313415527
Epoch 1220, val loss: 0.6410368084907532
Epoch 1230, training loss: 892.3101196289062 = 0.5827635526657104 + 100.0 * 8.91727352142334
Epoch 1230, val loss: 0.6395756602287292
Epoch 1240, training loss: 892.155029296875 = 0.5804250240325928 + 100.0 * 8.915745735168457
Epoch 1240, val loss: 0.6380409002304077
Epoch 1250, training loss: 892.1196899414062 = 0.5782126784324646 + 100.0 * 8.915414810180664
Epoch 1250, val loss: 0.6365085244178772
Epoch 1260, training loss: 892.3681030273438 = 0.5759707689285278 + 100.0 * 8.91792106628418
Epoch 1260, val loss: 0.635191023349762
Epoch 1270, training loss: 892.70703125 = 0.5739122629165649 + 100.0 * 8.921331405639648
Epoch 1270, val loss: 0.6339213848114014
Epoch 1280, training loss: 892.9151611328125 = 0.5718086957931519 + 100.0 * 8.923433303833008
Epoch 1280, val loss: 0.632638692855835
Epoch 1290, training loss: 893.0048217773438 = 0.5697498321533203 + 100.0 * 8.92435073852539
Epoch 1290, val loss: 0.6312534809112549
Epoch 1300, training loss: 892.6847534179688 = 0.5677292943000793 + 100.0 * 8.921170234680176
Epoch 1300, val loss: 0.6301412582397461
Epoch 1310, training loss: 893.0339965820312 = 0.5658738613128662 + 100.0 * 8.924681663513184
Epoch 1310, val loss: 0.6289839744567871
Epoch 1320, training loss: 893.9086303710938 = 0.5639970302581787 + 100.0 * 8.933445930480957
Epoch 1320, val loss: 0.6279323101043701
Epoch 1330, training loss: 894.2313842773438 = 0.5621084570884705 + 100.0 * 8.936692237854004
Epoch 1330, val loss: 0.6267698407173157
Epoch 1340, training loss: 894.2032470703125 = 0.560223400592804 + 100.0 * 8.936429977416992
Epoch 1340, val loss: 0.6256899833679199
Epoch 1350, training loss: 894.2769775390625 = 0.5584633350372314 + 100.0 * 8.937185287475586
Epoch 1350, val loss: 0.6247086524963379
Epoch 1360, training loss: 894.658447265625 = 0.5567349195480347 + 100.0 * 8.941017150878906
Epoch 1360, val loss: 0.6237523555755615
Epoch 1370, training loss: 894.71923828125 = 0.5549478530883789 + 100.0 * 8.941642761230469
Epoch 1370, val loss: 0.6227108836174011
Epoch 1380, training loss: 894.1555786132812 = 0.5532000660896301 + 100.0 * 8.936023712158203
Epoch 1380, val loss: 0.6221557259559631
Epoch 1390, training loss: 893.5907592773438 = 0.5514761209487915 + 100.0 * 8.93039321899414
Epoch 1390, val loss: 0.6208903789520264
Epoch 1400, training loss: 894.3223266601562 = 0.5499071478843689 + 100.0 * 8.937724113464355
Epoch 1400, val loss: 0.6202173233032227
Epoch 1410, training loss: 894.3817138671875 = 0.5483409762382507 + 100.0 * 8.938333511352539
Epoch 1410, val loss: 0.619334876537323
Epoch 1420, training loss: 895.275390625 = 0.5468119382858276 + 100.0 * 8.947285652160645
Epoch 1420, val loss: 0.618489146232605
Epoch 1430, training loss: 895.4605102539062 = 0.5452373027801514 + 100.0 * 8.949152946472168
Epoch 1430, val loss: 0.6177070736885071
Epoch 1440, training loss: 895.5582275390625 = 0.5436218976974487 + 100.0 * 8.950145721435547
Epoch 1440, val loss: 0.6169138550758362
Epoch 1450, training loss: 895.7391357421875 = 0.5420883893966675 + 100.0 * 8.951970100402832
Epoch 1450, val loss: 0.6161221861839294
Epoch 1460, training loss: 896.06298828125 = 0.5405902862548828 + 100.0 * 8.95522403717041
Epoch 1460, val loss: 0.6154260039329529
Epoch 1470, training loss: 896.1199951171875 = 0.5390845537185669 + 100.0 * 8.955809593200684
Epoch 1470, val loss: 0.6148018836975098
Epoch 1480, training loss: 895.68359375 = 0.537549614906311 + 100.0 * 8.951460838317871
Epoch 1480, val loss: 0.6140360236167908
Epoch 1490, training loss: 896.0714111328125 = 0.5361328125 + 100.0 * 8.955352783203125
Epoch 1490, val loss: 0.6134585738182068
Epoch 1500, training loss: 896.4257202148438 = 0.5347362756729126 + 100.0 * 8.95890998840332
Epoch 1500, val loss: 0.6127387285232544
Epoch 1510, training loss: 896.7383422851562 = 0.5333417057991028 + 100.0 * 8.962050437927246
Epoch 1510, val loss: 0.6121375560760498
Epoch 1520, training loss: 897.2001342773438 = 0.5319626927375793 + 100.0 * 8.966681480407715
Epoch 1520, val loss: 0.6116257905960083
Epoch 1530, training loss: 897.072265625 = 0.530576765537262 + 100.0 * 8.96541690826416
Epoch 1530, val loss: 0.6110025644302368
Epoch 1540, training loss: 897.0308227539062 = 0.5291941165924072 + 100.0 * 8.96501636505127
Epoch 1540, val loss: 0.6103643178939819
Epoch 1550, training loss: 897.2935791015625 = 0.5277984738349915 + 100.0 * 8.967658042907715
Epoch 1550, val loss: 0.6098300814628601
Epoch 1560, training loss: 897.6629028320312 = 0.5264744758605957 + 100.0 * 8.97136402130127
Epoch 1560, val loss: 0.6092289090156555
Epoch 1570, training loss: 897.862548828125 = 0.5251896381378174 + 100.0 * 8.973373413085938
Epoch 1570, val loss: 0.608685314655304
Epoch 1580, training loss: 897.0634765625 = 0.5237799286842346 + 100.0 * 8.965396881103516
Epoch 1580, val loss: 0.6081154942512512
Epoch 1590, training loss: 894.7545166015625 = 0.5222377181053162 + 100.0 * 8.942322731018066
Epoch 1590, val loss: 0.6074562072753906
Epoch 1600, training loss: 896.0360717773438 = 0.5210111737251282 + 100.0 * 8.955150604248047
Epoch 1600, val loss: 0.6069666743278503
Epoch 1610, training loss: 896.8902587890625 = 0.5198290944099426 + 100.0 * 8.963704109191895
Epoch 1610, val loss: 0.6064392328262329
Epoch 1620, training loss: 897.7056274414062 = 0.5186856985092163 + 100.0 * 8.971869468688965
Epoch 1620, val loss: 0.6061692833900452
Epoch 1630, training loss: 898.5272216796875 = 0.5175117254257202 + 100.0 * 8.980096817016602
Epoch 1630, val loss: 0.6056532859802246
Epoch 1640, training loss: 897.9580078125 = 0.5162133574485779 + 100.0 * 8.974417686462402
Epoch 1640, val loss: 0.6051608324050903
Epoch 1650, training loss: 898.31591796875 = 0.5150430202484131 + 100.0 * 8.978009223937988
Epoch 1650, val loss: 0.6046797633171082
Epoch 1660, training loss: 897.6700439453125 = 0.5137419700622559 + 100.0 * 8.971563339233398
Epoch 1660, val loss: 0.6042031645774841
Epoch 1670, training loss: 898.1979370117188 = 0.5125619769096375 + 100.0 * 8.976853370666504
Epoch 1670, val loss: 0.6037935614585876
Epoch 1680, training loss: 898.8695678710938 = 0.5113895535469055 + 100.0 * 8.98358154296875
Epoch 1680, val loss: 0.6033018231391907
Epoch 1690, training loss: 899.0882568359375 = 0.5101940035820007 + 100.0 * 8.985780715942383
Epoch 1690, val loss: 0.6028128266334534
Epoch 1700, training loss: 899.3383178710938 = 0.5090062618255615 + 100.0 * 8.988292694091797
Epoch 1700, val loss: 0.6024429202079773
Epoch 1710, training loss: 899.2962646484375 = 0.5078180432319641 + 100.0 * 8.987884521484375
Epoch 1710, val loss: 0.6019859910011292
Epoch 1720, training loss: 899.4109497070312 = 0.5066533088684082 + 100.0 * 8.989043235778809
Epoch 1720, val loss: 0.6015260815620422
Epoch 1730, training loss: 899.6504516601562 = 0.5054883360862732 + 100.0 * 8.991449356079102
Epoch 1730, val loss: 0.6011348962783813
Epoch 1740, training loss: 899.6951293945312 = 0.5043365359306335 + 100.0 * 8.991908073425293
Epoch 1740, val loss: 0.6008317470550537
Epoch 1750, training loss: 899.9557495117188 = 0.5032023787498474 + 100.0 * 8.994525909423828
Epoch 1750, val loss: 0.6002772450447083
Epoch 1760, training loss: 899.4895629882812 = 0.5019920468330383 + 100.0 * 8.989875793457031
Epoch 1760, val loss: 0.5999313592910767
Epoch 1770, training loss: 899.8596801757812 = 0.5008881092071533 + 100.0 * 8.993587493896484
Epoch 1770, val loss: 0.5996689200401306
Epoch 1780, training loss: 900.1275024414062 = 0.4997992217540741 + 100.0 * 8.99627685546875
Epoch 1780, val loss: 0.5992100834846497
Epoch 1790, training loss: 900.6801147460938 = 0.49865734577178955 + 100.0 * 9.001814842224121
Epoch 1790, val loss: 0.5987693071365356
Epoch 1800, training loss: 900.6943359375 = 0.49750739336013794 + 100.0 * 9.001968383789062
Epoch 1800, val loss: 0.5983723998069763
Epoch 1810, training loss: 900.0169067382812 = 0.4963657855987549 + 100.0 * 8.995205879211426
Epoch 1810, val loss: 0.5980433821678162
Epoch 1820, training loss: 900.4898681640625 = 0.4952857196331024 + 100.0 * 8.999945640563965
Epoch 1820, val loss: 0.5977160334587097
Epoch 1830, training loss: 901.1471557617188 = 0.4941917359828949 + 100.0 * 9.006529808044434
Epoch 1830, val loss: 0.5973275899887085
Epoch 1840, training loss: 901.2108764648438 = 0.4930582642555237 + 100.0 * 9.00717830657959
Epoch 1840, val loss: 0.5968208312988281
Epoch 1850, training loss: 900.9844360351562 = 0.4919649064540863 + 100.0 * 9.004924774169922
Epoch 1850, val loss: 0.5965969562530518
Epoch 1860, training loss: 901.3328247070312 = 0.49088162183761597 + 100.0 * 9.008419036865234
Epoch 1860, val loss: 0.5962932109832764
Epoch 1870, training loss: 900.7452392578125 = 0.48994362354278564 + 100.0 * 9.00255298614502
Epoch 1870, val loss: 0.594868004322052
Epoch 1880, training loss: 899.9395141601562 = 0.4889479875564575 + 100.0 * 8.994505882263184
Epoch 1880, val loss: 0.5967032313346863
Epoch 1890, training loss: 900.2496337890625 = 0.48769310116767883 + 100.0 * 8.99761962890625
Epoch 1890, val loss: 0.5945283770561218
Epoch 1900, training loss: 900.609375 = 0.48656535148620605 + 100.0 * 9.001228332519531
Epoch 1900, val loss: 0.5946942567825317
Epoch 1910, training loss: 901.4353637695312 = 0.48553189635276794 + 100.0 * 9.009498596191406
Epoch 1910, val loss: 0.5943945646286011
Epoch 1920, training loss: 901.7479858398438 = 0.4844450056552887 + 100.0 * 9.012635231018066
Epoch 1920, val loss: 0.5938246250152588
Epoch 1930, training loss: 901.4961547851562 = 0.48334068059921265 + 100.0 * 9.010128021240234
Epoch 1930, val loss: 0.5935702919960022
Epoch 1940, training loss: 901.6990966796875 = 0.4822827875614166 + 100.0 * 9.012167930603027
Epoch 1940, val loss: 0.5929901599884033
Epoch 1950, training loss: 901.47509765625 = 0.4812209904193878 + 100.0 * 9.009939193725586
Epoch 1950, val loss: 0.5928175449371338
Epoch 1960, training loss: 901.9786987304688 = 0.48015096783638 + 100.0 * 9.014985084533691
Epoch 1960, val loss: 0.5925036668777466
Epoch 1970, training loss: 902.4078979492188 = 0.47908851504325867 + 100.0 * 9.019288063049316
Epoch 1970, val loss: 0.5921953916549683
Epoch 1980, training loss: 901.9096069335938 = 0.4779813289642334 + 100.0 * 9.01431655883789
Epoch 1980, val loss: 0.5917598009109497
Epoch 1990, training loss: 902.0702514648438 = 0.4769284725189209 + 100.0 * 9.0159330368042
Epoch 1990, val loss: 0.5916246771812439
Epoch 2000, training loss: 902.7901611328125 = 0.47588440775871277 + 100.0 * 9.02314281463623
Epoch 2000, val loss: 0.5912927985191345
Epoch 2010, training loss: 902.9176025390625 = 0.47479143738746643 + 100.0 * 9.024428367614746
Epoch 2010, val loss: 0.5909217596054077
Epoch 2020, training loss: 903.1487426757812 = 0.47373491525650024 + 100.0 * 9.026749610900879
Epoch 2020, val loss: 0.5907135009765625
Epoch 2030, training loss: 903.0728759765625 = 0.47265323996543884 + 100.0 * 9.026001930236816
Epoch 2030, val loss: 0.5904191732406616
Epoch 2040, training loss: 902.9500122070312 = 0.4715959429740906 + 100.0 * 9.024784088134766
Epoch 2040, val loss: 0.5901988744735718
Epoch 2050, training loss: 903.3052368164062 = 0.4705106317996979 + 100.0 * 9.02834701538086
Epoch 2050, val loss: 0.5898683667182922
Epoch 2060, training loss: 903.3920288085938 = 0.46942779421806335 + 100.0 * 9.029226303100586
Epoch 2060, val loss: 0.58965665102005
Epoch 2070, training loss: 903.1553344726562 = 0.4684073030948639 + 100.0 * 9.026869773864746
Epoch 2070, val loss: 0.5894735455513
Epoch 2080, training loss: 903.02587890625 = 0.46734341979026794 + 100.0 * 9.025585174560547
Epoch 2080, val loss: 0.5891080498695374
Epoch 2090, training loss: 902.96337890625 = 0.4663097560405731 + 100.0 * 9.024971008300781
Epoch 2090, val loss: 0.5887105464935303
Epoch 2100, training loss: 903.6448364257812 = 0.46526816487312317 + 100.0 * 9.031795501708984
Epoch 2100, val loss: 0.5886201858520508
Epoch 2110, training loss: 903.7107543945312 = 0.464185506105423 + 100.0 * 9.032465934753418
Epoch 2110, val loss: 0.5883569121360779
Epoch 2120, training loss: 903.1707153320312 = 0.4630526304244995 + 100.0 * 9.027076721191406
Epoch 2120, val loss: 0.5881526470184326
Epoch 2130, training loss: 903.0505981445312 = 0.46195119619369507 + 100.0 * 9.025886535644531
Epoch 2130, val loss: 0.5879901051521301
Epoch 2140, training loss: 903.3025512695312 = 0.46089762449264526 + 100.0 * 9.028416633605957
Epoch 2140, val loss: 0.587632954120636
Epoch 2150, training loss: 903.849365234375 = 0.45981431007385254 + 100.0 * 9.033895492553711
Epoch 2150, val loss: 0.5871550440788269
Epoch 2160, training loss: 904.4888916015625 = 0.4586995244026184 + 100.0 * 9.040302276611328
Epoch 2160, val loss: 0.5869808793067932
Epoch 2170, training loss: 904.0596923828125 = 0.45756471157073975 + 100.0 * 9.03602123260498
Epoch 2170, val loss: 0.5867412090301514
Epoch 2180, training loss: 904.1243286132812 = 0.45649170875549316 + 100.0 * 9.036678314208984
Epoch 2180, val loss: 0.5865132808685303
Epoch 2190, training loss: 904.6544799804688 = 0.45539167523384094 + 100.0 * 9.041991233825684
Epoch 2190, val loss: 0.5863094925880432
Epoch 2200, training loss: 905.1851806640625 = 0.45430999994277954 + 100.0 * 9.047308921813965
Epoch 2200, val loss: 0.5861125588417053
Epoch 2210, training loss: 905.2718505859375 = 0.4532260298728943 + 100.0 * 9.048186302185059
Epoch 2210, val loss: 0.5859159231185913
Epoch 2220, training loss: 905.2611694335938 = 0.45215684175491333 + 100.0 * 9.048089981079102
Epoch 2220, val loss: 0.5857811570167542
Epoch 2230, training loss: 905.5740356445312 = 0.45108580589294434 + 100.0 * 9.051229476928711
Epoch 2230, val loss: 0.5854979157447815
Epoch 2240, training loss: 905.209716796875 = 0.44995972514152527 + 100.0 * 9.047597885131836
Epoch 2240, val loss: 0.5853045582771301
Epoch 2250, training loss: 905.1602783203125 = 0.4489012658596039 + 100.0 * 9.047113418579102
Epoch 2250, val loss: 0.5852351188659668
Epoch 2260, training loss: 905.4675903320312 = 0.4478346109390259 + 100.0 * 9.05019760131836
Epoch 2260, val loss: 0.5849103927612305
Epoch 2270, training loss: 906.003662109375 = 0.4467571973800659 + 100.0 * 9.05556869506836
Epoch 2270, val loss: 0.5846925973892212
Epoch 2280, training loss: 906.2124633789062 = 0.44565629959106445 + 100.0 * 9.05766773223877
Epoch 2280, val loss: 0.5845440030097961
Epoch 2290, training loss: 905.2437744140625 = 0.44452160596847534 + 100.0 * 9.047992706298828
Epoch 2290, val loss: 0.5844942927360535
Epoch 2300, training loss: 905.315185546875 = 0.4434438645839691 + 100.0 * 9.048717498779297
Epoch 2300, val loss: 0.5841027498245239
Epoch 2310, training loss: 905.7317504882812 = 0.4423564076423645 + 100.0 * 9.05289363861084
Epoch 2310, val loss: 0.5840020775794983
Epoch 2320, training loss: 906.5972290039062 = 0.4412599205970764 + 100.0 * 9.061559677124023
Epoch 2320, val loss: 0.5837128758430481
Epoch 2330, training loss: 905.5889282226562 = 0.4401076138019562 + 100.0 * 9.051487922668457
Epoch 2330, val loss: 0.5839348435401917
Epoch 2340, training loss: 904.8080444335938 = 0.43894264101982117 + 100.0 * 9.04369068145752
Epoch 2340, val loss: 0.5834435224533081
Epoch 2350, training loss: 905.1505126953125 = 0.4378529191017151 + 100.0 * 9.047126770019531
Epoch 2350, val loss: 0.5834694504737854
Epoch 2360, training loss: 904.47802734375 = 0.4367671012878418 + 100.0 * 9.040412902832031
Epoch 2360, val loss: 0.5836527347564697
Epoch 2370, training loss: 903.3782958984375 = 0.4356471002101898 + 100.0 * 9.029426574707031
Epoch 2370, val loss: 0.5831624269485474
Epoch 2380, training loss: 903.2332153320312 = 0.43457213044166565 + 100.0 * 9.027986526489258
Epoch 2380, val loss: 0.5833804607391357
Epoch 2390, training loss: 902.8540649414062 = 0.43354976177215576 + 100.0 * 9.024205207824707
Epoch 2390, val loss: 0.5832139849662781
Epoch 2400, training loss: 902.8335571289062 = 0.4323861598968506 + 100.0 * 9.024011611938477
Epoch 2400, val loss: 0.5826385617256165
Epoch 2410, training loss: 903.33203125 = 0.43134382367134094 + 100.0 * 9.029006958007812
Epoch 2410, val loss: 0.5825403332710266
Epoch 2420, training loss: 904.3843383789062 = 0.43013304471969604 + 100.0 * 9.039542198181152
Epoch 2420, val loss: 0.5821534395217896
Epoch 2430, training loss: 905.0657958984375 = 0.428994357585907 + 100.0 * 9.046367645263672
Epoch 2430, val loss: 0.5821025371551514
Epoch 2440, training loss: 905.6843872070312 = 0.427839070558548 + 100.0 * 9.052565574645996
Epoch 2440, val loss: 0.5818246006965637
Epoch 2450, training loss: 906.2462158203125 = 0.4266788959503174 + 100.0 * 9.058195114135742
Epoch 2450, val loss: 0.5815703868865967
Epoch 2460, training loss: 906.3732299804688 = 0.42549610137939453 + 100.0 * 9.059477806091309
Epoch 2460, val loss: 0.5815696120262146
Epoch 2470, training loss: 906.472900390625 = 0.4243355095386505 + 100.0 * 9.06048583984375
Epoch 2470, val loss: 0.5814344882965088
Epoch 2480, training loss: 906.7418212890625 = 0.42318058013916016 + 100.0 * 9.063186645507812
Epoch 2480, val loss: 0.5813012719154358
Epoch 2490, training loss: 906.8607177734375 = 0.4220329225063324 + 100.0 * 9.064386367797852
Epoch 2490, val loss: 0.5813098549842834
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7656521739130434
0.8143881764833732
=== training gcn model ===
Epoch 0, training loss: 1023.3992309570312 = 1.1275081634521484 + 100.0 * 10.22271728515625
Epoch 0, val loss: 1.1273244619369507
Epoch 10, training loss: 981.7211303710938 = 1.1223136186599731 + 100.0 * 9.805988311767578
Epoch 10, val loss: 1.1222219467163086
Epoch 20, training loss: 963.8399658203125 = 1.1173804998397827 + 100.0 * 9.627225875854492
Epoch 20, val loss: 1.1173139810562134
Epoch 30, training loss: 950.4295043945312 = 1.1126246452331543 + 100.0 * 9.493168830871582
Epoch 30, val loss: 1.1125950813293457
Epoch 40, training loss: 940.0923461914062 = 1.108016848564148 + 100.0 * 9.389842987060547
Epoch 40, val loss: 1.1080260276794434
Epoch 50, training loss: 931.6900024414062 = 1.1036045551300049 + 100.0 * 9.305864334106445
Epoch 50, val loss: 1.1036452054977417
Epoch 60, training loss: 924.5519409179688 = 1.099319338798523 + 100.0 * 9.234526634216309
Epoch 60, val loss: 1.0993980169296265
Epoch 70, training loss: 918.3567504882812 = 1.0951550006866455 + 100.0 * 9.172616004943848
Epoch 70, val loss: 1.0952719449996948
Epoch 80, training loss: 912.9190063476562 = 1.091141700744629 + 100.0 * 9.118278503417969
Epoch 80, val loss: 1.0912915468215942
Epoch 90, training loss: 908.2891845703125 = 1.0872023105621338 + 100.0 * 9.072019577026367
Epoch 90, val loss: 1.087396264076233
Epoch 100, training loss: 904.2484130859375 = 1.0834027528762817 + 100.0 * 9.03165054321289
Epoch 100, val loss: 1.0836375951766968
Epoch 110, training loss: 900.6566772460938 = 1.0797027349472046 + 100.0 * 8.995769500732422
Epoch 110, val loss: 1.0799866914749146
Epoch 120, training loss: 897.4053955078125 = 1.0761165618896484 + 100.0 * 8.963293075561523
Epoch 120, val loss: 1.0764447450637817
Epoch 130, training loss: 894.815673828125 = 1.0726553201675415 + 100.0 * 8.937430381774902
Epoch 130, val loss: 1.0730259418487549
Epoch 140, training loss: 892.4840087890625 = 1.069322109222412 + 100.0 * 8.914146423339844
Epoch 140, val loss: 1.0697674751281738
Epoch 150, training loss: 890.4951782226562 = 1.066144347190857 + 100.0 * 8.89428997039795
Epoch 150, val loss: 1.0666245222091675
Epoch 160, training loss: 888.73193359375 = 1.0630606412887573 + 100.0 * 8.876688957214355
Epoch 160, val loss: 1.0635931491851807
Epoch 170, training loss: 887.1161499023438 = 1.0601134300231934 + 100.0 * 8.860560417175293
Epoch 170, val loss: 1.0607056617736816
Epoch 180, training loss: 886.1756591796875 = 1.0572755336761475 + 100.0 * 8.851183891296387
Epoch 180, val loss: 1.0579674243927002
Epoch 190, training loss: 884.8070678710938 = 1.0545990467071533 + 100.0 * 8.8375244140625
Epoch 190, val loss: 1.0553369522094727
Epoch 200, training loss: 883.8699951171875 = 1.052052617073059 + 100.0 * 8.828179359436035
Epoch 200, val loss: 1.0528457164764404
Epoch 210, training loss: 883.2603759765625 = 1.0496149063110352 + 100.0 * 8.822107315063477
Epoch 210, val loss: 1.0504412651062012
Epoch 220, training loss: 882.4129028320312 = 1.0472848415374756 + 100.0 * 8.813655853271484
Epoch 220, val loss: 1.0481898784637451
Epoch 230, training loss: 881.8278198242188 = 1.0450623035430908 + 100.0 * 8.807827949523926
Epoch 230, val loss: 1.0460354089736938
Epoch 240, training loss: 881.2459716796875 = 1.042871117591858 + 100.0 * 8.802031517028809
Epoch 240, val loss: 1.0439023971557617
Epoch 250, training loss: 880.7909545898438 = 1.0407733917236328 + 100.0 * 8.797501564025879
Epoch 250, val loss: 1.0418628454208374
Epoch 260, training loss: 880.5211791992188 = 1.0386425256729126 + 100.0 * 8.794825553894043
Epoch 260, val loss: 1.0398162603378296
Epoch 270, training loss: 879.913330078125 = 1.0365406274795532 + 100.0 * 8.78876781463623
Epoch 270, val loss: 1.0377333164215088
Epoch 280, training loss: 880.0420532226562 = 1.0345314741134644 + 100.0 * 8.790075302124023
Epoch 280, val loss: 1.0358072519302368
Epoch 290, training loss: 879.644287109375 = 1.032405138015747 + 100.0 * 8.786118507385254
Epoch 290, val loss: 1.0337597131729126
Epoch 300, training loss: 879.6187133789062 = 1.030189871788025 + 100.0 * 8.785884857177734
Epoch 300, val loss: 1.0316240787506104
Epoch 310, training loss: 879.8248901367188 = 1.0279896259307861 + 100.0 * 8.787968635559082
Epoch 310, val loss: 1.0294690132141113
Epoch 320, training loss: 879.7993774414062 = 1.0256702899932861 + 100.0 * 8.787736892700195
Epoch 320, val loss: 1.0272239446640015
Epoch 330, training loss: 879.5902709960938 = 1.0232478380203247 + 100.0 * 8.785670280456543
Epoch 330, val loss: 1.024886131286621
Epoch 340, training loss: 879.5628662109375 = 1.0207310914993286 + 100.0 * 8.785421371459961
Epoch 340, val loss: 1.0224382877349854
Epoch 350, training loss: 879.1398315429688 = 1.0180877447128296 + 100.0 * 8.781217575073242
Epoch 350, val loss: 1.0198943614959717
Epoch 360, training loss: 879.5941162109375 = 1.0153650045394897 + 100.0 * 8.785787582397461
Epoch 360, val loss: 1.0172234773635864
Epoch 370, training loss: 879.6107177734375 = 1.0124889612197876 + 100.0 * 8.785982131958008
Epoch 370, val loss: 1.0144422054290771
Epoch 380, training loss: 879.4046630859375 = 1.0094155073165894 + 100.0 * 8.783952713012695
Epoch 380, val loss: 1.011467456817627
Epoch 390, training loss: 879.5225219726562 = 1.006232738494873 + 100.0 * 8.785162925720215
Epoch 390, val loss: 1.0083736181259155
Epoch 400, training loss: 879.3529052734375 = 1.002824068069458 + 100.0 * 8.783500671386719
Epoch 400, val loss: 1.0050551891326904
Epoch 410, training loss: 879.7220458984375 = 0.9992375373840332 + 100.0 * 8.787227630615234
Epoch 410, val loss: 1.0015854835510254
Epoch 420, training loss: 880.5801391601562 = 0.9955175518989563 + 100.0 * 8.795845985412598
Epoch 420, val loss: 0.9980190992355347
Epoch 430, training loss: 879.6279907226562 = 0.9914354681968689 + 100.0 * 8.786365509033203
Epoch 430, val loss: 0.9941049218177795
Epoch 440, training loss: 879.7391357421875 = 0.9873604774475098 + 100.0 * 8.787517547607422
Epoch 440, val loss: 0.9901610016822815
Epoch 450, training loss: 880.097412109375 = 0.9830731749534607 + 100.0 * 8.791143417358398
Epoch 450, val loss: 0.9860153794288635
Epoch 460, training loss: 880.2392578125 = 0.9785087704658508 + 100.0 * 8.792607307434082
Epoch 460, val loss: 0.981563150882721
Epoch 470, training loss: 880.387451171875 = 0.9737545251846313 + 100.0 * 8.794137001037598
Epoch 470, val loss: 0.9770772457122803
Epoch 480, training loss: 880.533447265625 = 0.9688622355461121 + 100.0 * 8.795645713806152
Epoch 480, val loss: 0.9723727107048035
Epoch 490, training loss: 880.9296875 = 0.9637299180030823 + 100.0 * 8.799659729003906
Epoch 490, val loss: 0.9674501419067383
Epoch 500, training loss: 881.1334228515625 = 0.9583199620246887 + 100.0 * 8.801751136779785
Epoch 500, val loss: 0.9623082280158997
Epoch 510, training loss: 881.6055908203125 = 0.9527351260185242 + 100.0 * 8.806528091430664
Epoch 510, val loss: 0.9569804072380066
Epoch 520, training loss: 881.386962890625 = 0.9468632340431213 + 100.0 * 8.804401397705078
Epoch 520, val loss: 0.9512718319892883
Epoch 530, training loss: 881.7108154296875 = 0.9409980773925781 + 100.0 * 8.807698249816895
Epoch 530, val loss: 0.9457204937934875
Epoch 540, training loss: 881.2493286132812 = 0.9347084164619446 + 100.0 * 8.803146362304688
Epoch 540, val loss: 0.9397534728050232
Epoch 550, training loss: 881.2398071289062 = 0.9281969666481018 + 100.0 * 8.803115844726562
Epoch 550, val loss: 0.9335296154022217
Epoch 560, training loss: 881.90673828125 = 0.9216721653938293 + 100.0 * 8.809850692749023
Epoch 560, val loss: 0.9273859858512878
Epoch 570, training loss: 882.44384765625 = 0.915078341960907 + 100.0 * 8.815287590026855
Epoch 570, val loss: 0.9210901260375977
Epoch 580, training loss: 882.5634155273438 = 0.908193826675415 + 100.0 * 8.81655216217041
Epoch 580, val loss: 0.9146450757980347
Epoch 590, training loss: 882.4323120117188 = 0.9011792540550232 + 100.0 * 8.815311431884766
Epoch 590, val loss: 0.9079317450523376
Epoch 600, training loss: 882.5807495117188 = 0.894034743309021 + 100.0 * 8.816866874694824
Epoch 600, val loss: 0.9013237953186035
Epoch 610, training loss: 882.8469848632812 = 0.8870570063591003 + 100.0 * 8.819599151611328
Epoch 610, val loss: 0.8947497010231018
Epoch 620, training loss: 883.2947387695312 = 0.8801004886627197 + 100.0 * 8.824146270751953
Epoch 620, val loss: 0.8882313370704651
Epoch 630, training loss: 883.4811401367188 = 0.872978150844574 + 100.0 * 8.826081275939941
Epoch 630, val loss: 0.8815056085586548
Epoch 640, training loss: 883.5747680664062 = 0.8657081723213196 + 100.0 * 8.8270902633667
Epoch 640, val loss: 0.8746566772460938
Epoch 650, training loss: 883.5599365234375 = 0.8586528897285461 + 100.0 * 8.82701301574707
Epoch 650, val loss: 0.8681701421737671
Epoch 660, training loss: 883.4259643554688 = 0.8516112565994263 + 100.0 * 8.825743675231934
Epoch 660, val loss: 0.8615212440490723
Epoch 670, training loss: 884.5059204101562 = 0.8445361852645874 + 100.0 * 8.836613655090332
Epoch 670, val loss: 0.8549972772598267
Epoch 680, training loss: 884.0960693359375 = 0.8374433517456055 + 100.0 * 8.832586288452148
Epoch 680, val loss: 0.8485628366470337
Epoch 690, training loss: 884.464111328125 = 0.830572783946991 + 100.0 * 8.836335182189941
Epoch 690, val loss: 0.8422488570213318
Epoch 700, training loss: 884.8600463867188 = 0.8237836956977844 + 100.0 * 8.840362548828125
Epoch 700, val loss: 0.8359867334365845
Epoch 710, training loss: 885.3107299804688 = 0.8170973062515259 + 100.0 * 8.84493637084961
Epoch 710, val loss: 0.8298228979110718
Epoch 720, training loss: 883.8516845703125 = 0.8099401593208313 + 100.0 * 8.83041763305664
Epoch 720, val loss: 0.8233752846717834
Epoch 730, training loss: 884.3451538085938 = 0.8033849596977234 + 100.0 * 8.835417747497559
Epoch 730, val loss: 0.8172200918197632
Epoch 740, training loss: 884.84814453125 = 0.7971352338790894 + 100.0 * 8.840510368347168
Epoch 740, val loss: 0.8114450573921204
Epoch 750, training loss: 885.0968627929688 = 0.7908302545547485 + 100.0 * 8.843060493469238
Epoch 750, val loss: 0.8058340549468994
Epoch 760, training loss: 885.5031127929688 = 0.7845980525016785 + 100.0 * 8.847185134887695
Epoch 760, val loss: 0.8002201318740845
Epoch 770, training loss: 886.09765625 = 0.7785218954086304 + 100.0 * 8.853191375732422
Epoch 770, val loss: 0.7946427464485168
Epoch 780, training loss: 886.27587890625 = 0.7724630236625671 + 100.0 * 8.855033874511719
Epoch 780, val loss: 0.7891564965248108
Epoch 790, training loss: 886.4845581054688 = 0.76659095287323 + 100.0 * 8.857179641723633
Epoch 790, val loss: 0.7838599681854248
Epoch 800, training loss: 886.7798461914062 = 0.7607974410057068 + 100.0 * 8.860190391540527
Epoch 800, val loss: 0.7786672115325928
Epoch 810, training loss: 887.3204956054688 = 0.7551987767219543 + 100.0 * 8.865653038024902
Epoch 810, val loss: 0.7736105918884277
Epoch 820, training loss: 886.9358520507812 = 0.7494444251060486 + 100.0 * 8.86186408996582
Epoch 820, val loss: 0.768481433391571
Epoch 830, training loss: 887.55859375 = 0.7440927028656006 + 100.0 * 8.868144989013672
Epoch 830, val loss: 0.7637813091278076
Epoch 840, training loss: 887.2808227539062 = 0.7389068603515625 + 100.0 * 8.865419387817383
Epoch 840, val loss: 0.7590142488479614
Epoch 850, training loss: 887.155517578125 = 0.7334445714950562 + 100.0 * 8.86422061920166
Epoch 850, val loss: 0.7539718747138977
Epoch 860, training loss: 888.6343994140625 = 0.7290472388267517 + 100.0 * 8.879053115844727
Epoch 860, val loss: 0.750392735004425
Epoch 870, training loss: 887.362548828125 = 0.7234662771224976 + 100.0 * 8.8663911819458
Epoch 870, val loss: 0.7453603744506836
Epoch 880, training loss: 887.7023315429688 = 0.7186260223388672 + 100.0 * 8.869836807250977
Epoch 880, val loss: 0.7410740256309509
Epoch 890, training loss: 887.8075561523438 = 0.713979184627533 + 100.0 * 8.870935440063477
Epoch 890, val loss: 0.7369837760925293
Epoch 900, training loss: 887.8812255859375 = 0.709280252456665 + 100.0 * 8.871719360351562
Epoch 900, val loss: 0.7327916026115417
Epoch 910, training loss: 888.2872924804688 = 0.704721212387085 + 100.0 * 8.875825881958008
Epoch 910, val loss: 0.7288458943367004
Epoch 920, training loss: 888.7078857421875 = 0.7002559304237366 + 100.0 * 8.88007640838623
Epoch 920, val loss: 0.7248541712760925
Epoch 930, training loss: 888.6986694335938 = 0.69581139087677 + 100.0 * 8.88002872467041
Epoch 930, val loss: 0.7209674715995789
Epoch 940, training loss: 888.6458129882812 = 0.6915595531463623 + 100.0 * 8.879542350769043
Epoch 940, val loss: 0.7172970771789551
Epoch 950, training loss: 889.1702270507812 = 0.687400758266449 + 100.0 * 8.884828567504883
Epoch 950, val loss: 0.7136378288269043
Epoch 960, training loss: 889.439453125 = 0.6833528280258179 + 100.0 * 8.887560844421387
Epoch 960, val loss: 0.7100404500961304
Epoch 970, training loss: 889.4758911132812 = 0.6792418360710144 + 100.0 * 8.88796615600586
Epoch 970, val loss: 0.7064527869224548
Epoch 980, training loss: 889.2415771484375 = 0.6753703951835632 + 100.0 * 8.885662078857422
Epoch 980, val loss: 0.7030961513519287
Epoch 990, training loss: 889.1328735351562 = 0.6714777946472168 + 100.0 * 8.884613990783691
Epoch 990, val loss: 0.6996946930885315
Epoch 1000, training loss: 889.5492553710938 = 0.6677972674369812 + 100.0 * 8.888814926147461
Epoch 1000, val loss: 0.6965714693069458
Epoch 1010, training loss: 890.060791015625 = 0.6643006801605225 + 100.0 * 8.893964767456055
Epoch 1010, val loss: 0.693534791469574
Epoch 1020, training loss: 890.2922973632812 = 0.6607578992843628 + 100.0 * 8.896315574645996
Epoch 1020, val loss: 0.6904467344284058
Epoch 1030, training loss: 890.26611328125 = 0.6573320031166077 + 100.0 * 8.896087646484375
Epoch 1030, val loss: 0.6875160932540894
Epoch 1040, training loss: 890.5601196289062 = 0.6539977192878723 + 100.0 * 8.89906120300293
Epoch 1040, val loss: 0.6846175789833069
Epoch 1050, training loss: 891.3182373046875 = 0.6506742238998413 + 100.0 * 8.906675338745117
Epoch 1050, val loss: 0.6817858815193176
Epoch 1060, training loss: 888.9625854492188 = 0.6474475264549255 + 100.0 * 8.883151054382324
Epoch 1060, val loss: 0.6789242625236511
Epoch 1070, training loss: 890.0973510742188 = 0.6440725922584534 + 100.0 * 8.894533157348633
Epoch 1070, val loss: 0.6760331988334656
Epoch 1080, training loss: 890.1008911132812 = 0.641086220741272 + 100.0 * 8.894598007202148
Epoch 1080, val loss: 0.6734920144081116
Epoch 1090, training loss: 888.1525268554688 = 0.6382533311843872 + 100.0 * 8.875143051147461
Epoch 1090, val loss: 0.6710342168807983
Epoch 1100, training loss: 889.4186401367188 = 0.6356719732284546 + 100.0 * 8.887829780578613
Epoch 1100, val loss: 0.6687765717506409
Epoch 1110, training loss: 889.5913696289062 = 0.6329385042190552 + 100.0 * 8.8895845413208
Epoch 1110, val loss: 0.6664828062057495
Epoch 1120, training loss: 890.128662109375 = 0.6302550435066223 + 100.0 * 8.894984245300293
Epoch 1120, val loss: 0.6641983389854431
Epoch 1130, training loss: 890.5774536132812 = 0.6276211142539978 + 100.0 * 8.899497985839844
Epoch 1130, val loss: 0.6619623899459839
Epoch 1140, training loss: 890.9688720703125 = 0.6250079274177551 + 100.0 * 8.903438568115234
Epoch 1140, val loss: 0.6598109006881714
Epoch 1150, training loss: 890.8062133789062 = 0.6224908232688904 + 100.0 * 8.901837348937988
Epoch 1150, val loss: 0.6576409339904785
Epoch 1160, training loss: 891.5963134765625 = 0.620060920715332 + 100.0 * 8.909762382507324
Epoch 1160, val loss: 0.6555808782577515
Epoch 1170, training loss: 891.72314453125 = 0.6176308393478394 + 100.0 * 8.911055564880371
Epoch 1170, val loss: 0.653570294380188
Epoch 1180, training loss: 891.4946899414062 = 0.61524897813797 + 100.0 * 8.908794403076172
Epoch 1180, val loss: 0.6515862941741943
Epoch 1190, training loss: 891.9575805664062 = 0.6129295229911804 + 100.0 * 8.913446426391602
Epoch 1190, val loss: 0.6496792435646057
Epoch 1200, training loss: 892.2517700195312 = 0.6107322573661804 + 100.0 * 8.916410446166992
Epoch 1200, val loss: 0.6479284167289734
Epoch 1210, training loss: 892.4705200195312 = 0.6085413098335266 + 100.0 * 8.918620109558105
Epoch 1210, val loss: 0.6461836695671082
Epoch 1220, training loss: 892.7815551757812 = 0.6063703894615173 + 100.0 * 8.921751976013184
Epoch 1220, val loss: 0.644408106803894
Epoch 1230, training loss: 893.0267333984375 = 0.6042622327804565 + 100.0 * 8.924224853515625
Epoch 1230, val loss: 0.6428101062774658
Epoch 1240, training loss: 892.7351684570312 = 0.6022397875785828 + 100.0 * 8.921329498291016
Epoch 1240, val loss: 0.6412563323974609
Epoch 1250, training loss: 892.4312133789062 = 0.6002636551856995 + 100.0 * 8.918309211730957
Epoch 1250, val loss: 0.6396602392196655
Epoch 1260, training loss: 893.1633911132812 = 0.5984221696853638 + 100.0 * 8.925649642944336
Epoch 1260, val loss: 0.6382564306259155
Epoch 1270, training loss: 893.7356567382812 = 0.5965166091918945 + 100.0 * 8.931391716003418
Epoch 1270, val loss: 0.6368135809898376
Epoch 1280, training loss: 892.8206176757812 = 0.5944570899009705 + 100.0 * 8.922261238098145
Epoch 1280, val loss: 0.6352541446685791
Epoch 1290, training loss: 893.0699462890625 = 0.5926807522773743 + 100.0 * 8.924773216247559
Epoch 1290, val loss: 0.6339576840400696
Epoch 1300, training loss: 893.7710571289062 = 0.5910165309906006 + 100.0 * 8.931800842285156
Epoch 1300, val loss: 0.6327409744262695
Epoch 1310, training loss: 893.822021484375 = 0.5892799496650696 + 100.0 * 8.932327270507812
Epoch 1310, val loss: 0.6314831376075745
Epoch 1320, training loss: 894.3413696289062 = 0.587609589099884 + 100.0 * 8.937538146972656
Epoch 1320, val loss: 0.6304105520248413
Epoch 1330, training loss: 894.6390991210938 = 0.5859661102294922 + 100.0 * 8.940531730651855
Epoch 1330, val loss: 0.6292101144790649
Epoch 1340, training loss: 894.487548828125 = 0.584264874458313 + 100.0 * 8.939032554626465
Epoch 1340, val loss: 0.6280706524848938
Epoch 1350, training loss: 894.9308471679688 = 0.582709789276123 + 100.0 * 8.9434814453125
Epoch 1350, val loss: 0.6270135641098022
Epoch 1360, training loss: 895.3173828125 = 0.581186830997467 + 100.0 * 8.947361946105957
Epoch 1360, val loss: 0.6260212063789368
Epoch 1370, training loss: 895.4265747070312 = 0.579649806022644 + 100.0 * 8.948469161987305
Epoch 1370, val loss: 0.6250220537185669
Epoch 1380, training loss: 895.608642578125 = 0.5781506896018982 + 100.0 * 8.950304985046387
Epoch 1380, val loss: 0.6240936517715454
Epoch 1390, training loss: 895.36328125 = 0.5765323042869568 + 100.0 * 8.947867393493652
Epoch 1390, val loss: 0.6230185031890869
Epoch 1400, training loss: 894.5424194335938 = 0.5750918984413147 + 100.0 * 8.93967342376709
Epoch 1400, val loss: 0.6221482157707214
Epoch 1410, training loss: 895.408203125 = 0.5736352205276489 + 100.0 * 8.948346138000488
Epoch 1410, val loss: 0.6211370229721069
Epoch 1420, training loss: 894.7114868164062 = 0.5720949769020081 + 100.0 * 8.941393852233887
Epoch 1420, val loss: 0.6201605200767517
Epoch 1430, training loss: 895.3455200195312 = 0.5708885192871094 + 100.0 * 8.947746276855469
Epoch 1430, val loss: 0.6194312572479248
Epoch 1440, training loss: 895.7041015625 = 0.5696094036102295 + 100.0 * 8.951345443725586
Epoch 1440, val loss: 0.6188182830810547
Epoch 1450, training loss: 896.49169921875 = 0.5682886242866516 + 100.0 * 8.959234237670898
Epoch 1450, val loss: 0.6179444193840027
Epoch 1460, training loss: 896.42822265625 = 0.5669335722923279 + 100.0 * 8.958612442016602
Epoch 1460, val loss: 0.6171913743019104
Epoch 1470, training loss: 896.5546875 = 0.5656534433364868 + 100.0 * 8.959890365600586
Epoch 1470, val loss: 0.6164747476577759
Epoch 1480, training loss: 896.8363647460938 = 0.5643748641014099 + 100.0 * 8.962719917297363
Epoch 1480, val loss: 0.6157570481300354
Epoch 1490, training loss: 897.1406860351562 = 0.5631083846092224 + 100.0 * 8.965775489807129
Epoch 1490, val loss: 0.6150472164154053
Epoch 1500, training loss: 896.4356689453125 = 0.5617768168449402 + 100.0 * 8.958739280700684
Epoch 1500, val loss: 0.6143149733543396
Epoch 1510, training loss: 897.2913818359375 = 0.5605248212814331 + 100.0 * 8.967308044433594
Epoch 1510, val loss: 0.6136480569839478
Epoch 1520, training loss: 895.5263671875 = 0.559197187423706 + 100.0 * 8.949671745300293
Epoch 1520, val loss: 0.6128903031349182
Epoch 1530, training loss: 896.0211791992188 = 0.5578974485397339 + 100.0 * 8.954632759094238
Epoch 1530, val loss: 0.6122307777404785
Epoch 1540, training loss: 896.5197143554688 = 0.5567246079444885 + 100.0 * 8.959630012512207
Epoch 1540, val loss: 0.6117076277732849
Epoch 1550, training loss: 897.3760986328125 = 0.555557906627655 + 100.0 * 8.968205451965332
Epoch 1550, val loss: 0.6111026406288147
Epoch 1560, training loss: 897.7525634765625 = 0.5543798804283142 + 100.0 * 8.9719820022583
Epoch 1560, val loss: 0.6105948090553284
Epoch 1570, training loss: 898.114501953125 = 0.5531864762306213 + 100.0 * 8.975613594055176
Epoch 1570, val loss: 0.6100445985794067
Epoch 1580, training loss: 897.930908203125 = 0.5520116090774536 + 100.0 * 8.97378921508789
Epoch 1580, val loss: 0.6094332933425903
Epoch 1590, training loss: 897.7435302734375 = 0.5508016347885132 + 100.0 * 8.971927642822266
Epoch 1590, val loss: 0.6089381575584412
Epoch 1600, training loss: 898.0357055664062 = 0.5496408343315125 + 100.0 * 8.974861145019531
Epoch 1600, val loss: 0.6084346771240234
Epoch 1610, training loss: 898.7566528320312 = 0.548510730266571 + 100.0 * 8.982081413269043
Epoch 1610, val loss: 0.6079302430152893
Epoch 1620, training loss: 898.7116088867188 = 0.5473496317863464 + 100.0 * 8.981642723083496
Epoch 1620, val loss: 0.6073683500289917
Epoch 1630, training loss: 897.8396606445312 = 0.5460875630378723 + 100.0 * 8.972935676574707
Epoch 1630, val loss: 0.6070073843002319
Epoch 1640, training loss: 895.5435791015625 = 0.5448576807975769 + 100.0 * 8.949987411499023
Epoch 1640, val loss: 0.605895459651947
Epoch 1650, training loss: 897.21533203125 = 0.5438350439071655 + 100.0 * 8.966714859008789
Epoch 1650, val loss: 0.6058080792427063
Epoch 1660, training loss: 898.303466796875 = 0.5427318811416626 + 100.0 * 8.977607727050781
Epoch 1660, val loss: 0.6052164435386658
Epoch 1670, training loss: 899.242919921875 = 0.5416427254676819 + 100.0 * 8.98701286315918
Epoch 1670, val loss: 0.6048979759216309
Epoch 1680, training loss: 899.7039794921875 = 0.5405260324478149 + 100.0 * 8.991634368896484
Epoch 1680, val loss: 0.6044169664382935
Epoch 1690, training loss: 899.7733764648438 = 0.5393871068954468 + 100.0 * 8.992340087890625
Epoch 1690, val loss: 0.6040143966674805
Epoch 1700, training loss: 899.9786376953125 = 0.5382866859436035 + 100.0 * 8.994403839111328
Epoch 1700, val loss: 0.6037017703056335
Epoch 1710, training loss: 899.9882202148438 = 0.5371483564376831 + 100.0 * 8.994510650634766
Epoch 1710, val loss: 0.6030670404434204
Epoch 1720, training loss: 900.6047973632812 = 0.5360250473022461 + 100.0 * 9.000687599182129
Epoch 1720, val loss: 0.6026631593704224
Epoch 1730, training loss: 900.6619262695312 = 0.5349053144454956 + 100.0 * 9.001270294189453
Epoch 1730, val loss: 0.6022212505340576
Epoch 1740, training loss: 899.4910888671875 = 0.5336582660675049 + 100.0 * 8.989574432373047
Epoch 1740, val loss: 0.6018430590629578
Epoch 1750, training loss: 899.6119384765625 = 0.5325952172279358 + 100.0 * 8.990793228149414
Epoch 1750, val loss: 0.6013610363006592
Epoch 1760, training loss: 900.4896240234375 = 0.531482458114624 + 100.0 * 8.999581336975098
Epoch 1760, val loss: 0.6009467244148254
Epoch 1770, training loss: 901.3084716796875 = 0.5304191708564758 + 100.0 * 9.007781028747559
Epoch 1770, val loss: 0.6006184816360474
Epoch 1780, training loss: 901.2720336914062 = 0.5293092131614685 + 100.0 * 9.007427215576172
Epoch 1780, val loss: 0.6001763939857483
Epoch 1790, training loss: 901.5479736328125 = 0.5281785726547241 + 100.0 * 9.010197639465332
Epoch 1790, val loss: 0.5997938513755798
Epoch 1800, training loss: 901.9539184570312 = 0.5270786881446838 + 100.0 * 9.014267921447754
Epoch 1800, val loss: 0.5993871092796326
Epoch 1810, training loss: 901.885498046875 = 0.5259503126144409 + 100.0 * 9.013595581054688
Epoch 1810, val loss: 0.5989751815795898
Epoch 1820, training loss: 901.7787475585938 = 0.5248160362243652 + 100.0 * 9.01253890991211
Epoch 1820, val loss: 0.5985601544380188
Epoch 1830, training loss: 902.24560546875 = 0.5237203240394592 + 100.0 * 9.017218589782715
Epoch 1830, val loss: 0.5981758236885071
Epoch 1840, training loss: 902.6336059570312 = 0.5226154327392578 + 100.0 * 9.021109580993652
Epoch 1840, val loss: 0.5978198647499084
Epoch 1850, training loss: 901.258056640625 = 0.5216143131256104 + 100.0 * 9.007364273071289
Epoch 1850, val loss: 0.5978586077690125
Epoch 1860, training loss: 901.134765625 = 0.5205285549163818 + 100.0 * 9.006142616271973
Epoch 1860, val loss: 0.5971912741661072
Epoch 1870, training loss: 901.9974975585938 = 0.5194912552833557 + 100.0 * 9.014780044555664
Epoch 1870, val loss: 0.5969542264938354
Epoch 1880, training loss: 902.5640869140625 = 0.5183719396591187 + 100.0 * 9.02045726776123
Epoch 1880, val loss: 0.596506655216217
Epoch 1890, training loss: 903.160400390625 = 0.517236590385437 + 100.0 * 9.026432037353516
Epoch 1890, val loss: 0.5961995124816895
Epoch 1900, training loss: 903.2401123046875 = 0.5161207914352417 + 100.0 * 9.027239799499512
Epoch 1900, val loss: 0.5958940982818604
Epoch 1910, training loss: 903.0919189453125 = 0.5150012373924255 + 100.0 * 9.025769233703613
Epoch 1910, val loss: 0.5955802202224731
Epoch 1920, training loss: 903.3094482421875 = 0.5139087438583374 + 100.0 * 9.027955055236816
Epoch 1920, val loss: 0.5952847003936768
Epoch 1930, training loss: 903.7908935546875 = 0.5128130912780762 + 100.0 * 9.032780647277832
Epoch 1930, val loss: 0.5949589610099792
Epoch 1940, training loss: 903.5889892578125 = 0.5116832852363586 + 100.0 * 9.030773162841797
Epoch 1940, val loss: 0.5945900678634644
Epoch 1950, training loss: 903.7821655273438 = 0.5105541944503784 + 100.0 * 9.032715797424316
Epoch 1950, val loss: 0.594224214553833
Epoch 1960, training loss: 903.7691650390625 = 0.5094244480133057 + 100.0 * 9.032597541809082
Epoch 1960, val loss: 0.5939337611198425
Epoch 1970, training loss: 903.732666015625 = 0.508289098739624 + 100.0 * 9.032243728637695
Epoch 1970, val loss: 0.5936163663864136
Epoch 1980, training loss: 904.2240600585938 = 0.5072004795074463 + 100.0 * 9.037168502807617
Epoch 1980, val loss: 0.5932814478874207
Epoch 1990, training loss: 903.2252807617188 = 0.5060468912124634 + 100.0 * 9.027192115783691
Epoch 1990, val loss: 0.5930562019348145
Epoch 2000, training loss: 902.79052734375 = 0.505035400390625 + 100.0 * 9.022854804992676
Epoch 2000, val loss: 0.5925425887107849
Epoch 2010, training loss: 902.8186645507812 = 0.503753125667572 + 100.0 * 9.023149490356445
Epoch 2010, val loss: 0.59243243932724
Epoch 2020, training loss: 903.762939453125 = 0.5025988221168518 + 100.0 * 9.03260326385498
Epoch 2020, val loss: 0.5921223759651184
Epoch 2030, training loss: 904.3368530273438 = 0.5014894604682922 + 100.0 * 9.03835391998291
Epoch 2030, val loss: 0.5917820930480957
Epoch 2040, training loss: 904.833740234375 = 0.5003598928451538 + 100.0 * 9.043334007263184
Epoch 2040, val loss: 0.5914376974105835
Epoch 2050, training loss: 904.5663452148438 = 0.4991913437843323 + 100.0 * 9.040671348571777
Epoch 2050, val loss: 0.5911484360694885
Epoch 2060, training loss: 904.9800415039062 = 0.49804428219795227 + 100.0 * 9.044819831848145
Epoch 2060, val loss: 0.5907823443412781
Epoch 2070, training loss: 904.8924560546875 = 0.4968702495098114 + 100.0 * 9.04395580291748
Epoch 2070, val loss: 0.590463399887085
Epoch 2080, training loss: 904.7471313476562 = 0.4957060217857361 + 100.0 * 9.042513847351074
Epoch 2080, val loss: 0.5901669263839722
Epoch 2090, training loss: 905.0376586914062 = 0.4945512115955353 + 100.0 * 9.045431137084961
Epoch 2090, val loss: 0.5898687243461609
Epoch 2100, training loss: 905.5587768554688 = 0.49339210987091064 + 100.0 * 9.050653457641602
Epoch 2100, val loss: 0.5895727872848511
Epoch 2110, training loss: 905.4609985351562 = 0.49222585558891296 + 100.0 * 9.049687385559082
Epoch 2110, val loss: 0.5893092751502991
Epoch 2120, training loss: 905.4614868164062 = 0.4910518229007721 + 100.0 * 9.049704551696777
Epoch 2120, val loss: 0.5890070199966431
Epoch 2130, training loss: 905.5524291992188 = 0.4898834228515625 + 100.0 * 9.050625801086426
Epoch 2130, val loss: 0.5887721180915833
Epoch 2140, training loss: 905.9837036132812 = 0.4887368977069855 + 100.0 * 9.054949760437012
Epoch 2140, val loss: 0.5885047912597656
Epoch 2150, training loss: 905.39990234375 = 0.48754507303237915 + 100.0 * 9.049123764038086
Epoch 2150, val loss: 0.5882090330123901
Epoch 2160, training loss: 904.5433349609375 = 0.48632562160491943 + 100.0 * 9.040570259094238
Epoch 2160, val loss: 0.5879486203193665
Epoch 2170, training loss: 905.4242553710938 = 0.48516911268234253 + 100.0 * 9.04939079284668
Epoch 2170, val loss: 0.5877922177314758
Epoch 2180, training loss: 905.8360595703125 = 0.48399215936660767 + 100.0 * 9.053520202636719
Epoch 2180, val loss: 0.5875356793403625
Epoch 2190, training loss: 906.4481201171875 = 0.48281773924827576 + 100.0 * 9.059653282165527
Epoch 2190, val loss: 0.5873633027076721
Epoch 2200, training loss: 906.5736083984375 = 0.48163172602653503 + 100.0 * 9.060919761657715
Epoch 2200, val loss: 0.5871754884719849
Epoch 2210, training loss: 906.4727172851562 = 0.4804120361804962 + 100.0 * 9.05992317199707
Epoch 2210, val loss: 0.5869380235671997
Epoch 2220, training loss: 906.3790893554688 = 0.4792092740535736 + 100.0 * 9.058999061584473
Epoch 2220, val loss: 0.5867934226989746
Epoch 2230, training loss: 906.4789428710938 = 0.47802233695983887 + 100.0 * 9.060009002685547
Epoch 2230, val loss: 0.5865479707717896
Epoch 2240, training loss: 906.9928588867188 = 0.4768208861351013 + 100.0 * 9.065160751342773
Epoch 2240, val loss: 0.5863391757011414
Epoch 2250, training loss: 906.7071533203125 = 0.47561097145080566 + 100.0 * 9.062315940856934
Epoch 2250, val loss: 0.5861896276473999
Epoch 2260, training loss: 906.838134765625 = 0.47439512610435486 + 100.0 * 9.063637733459473
Epoch 2260, val loss: 0.5860031247138977
Epoch 2270, training loss: 906.9212036132812 = 0.47316792607307434 + 100.0 * 9.064480781555176
Epoch 2270, val loss: 0.5858532786369324
Epoch 2280, training loss: 907.033935546875 = 0.471911758184433 + 100.0 * 9.065620422363281
Epoch 2280, val loss: 0.5856757164001465
Epoch 2290, training loss: 905.39306640625 = 0.4707421660423279 + 100.0 * 9.049222946166992
Epoch 2290, val loss: 0.5855313539505005
Epoch 2300, training loss: 905.4072265625 = 0.4699474275112152 + 100.0 * 9.049372673034668
Epoch 2300, val loss: 0.5853919982910156
Epoch 2310, training loss: 903.6007080078125 = 0.46853938698768616 + 100.0 * 9.03132152557373
Epoch 2310, val loss: 0.5853652954101562
Epoch 2320, training loss: 903.6029052734375 = 0.4673427641391754 + 100.0 * 9.031355857849121
Epoch 2320, val loss: 0.5848217606544495
Epoch 2330, training loss: 903.4196166992188 = 0.4660644233226776 + 100.0 * 9.029535293579102
Epoch 2330, val loss: 0.5847172737121582
Epoch 2340, training loss: 903.3667602539062 = 0.46468105912208557 + 100.0 * 9.029021263122559
Epoch 2340, val loss: 0.5850147008895874
Epoch 2350, training loss: 903.7120971679688 = 0.4635023772716522 + 100.0 * 9.032485961914062
Epoch 2350, val loss: 0.5847801566123962
Epoch 2360, training loss: 904.8863525390625 = 0.4622980058193207 + 100.0 * 9.044240951538086
Epoch 2360, val loss: 0.584357500076294
Epoch 2370, training loss: 905.3655395507812 = 0.4609919786453247 + 100.0 * 9.04904556274414
Epoch 2370, val loss: 0.5843682289123535
Epoch 2380, training loss: 906.0667724609375 = 0.45973795652389526 + 100.0 * 9.056070327758789
Epoch 2380, val loss: 0.5842754244804382
Epoch 2390, training loss: 906.1409301757812 = 0.45842692255973816 + 100.0 * 9.056824684143066
Epoch 2390, val loss: 0.5841033458709717
Epoch 2400, training loss: 906.3611450195312 = 0.45712810754776 + 100.0 * 9.059040069580078
Epoch 2400, val loss: 0.5838817358016968
Epoch 2410, training loss: 906.5574951171875 = 0.4558098018169403 + 100.0 * 9.061017036437988
Epoch 2410, val loss: 0.5837386250495911
Epoch 2420, training loss: 906.9491577148438 = 0.4544883966445923 + 100.0 * 9.064947128295898
Epoch 2420, val loss: 0.5836227536201477
Epoch 2430, training loss: 907.00390625 = 0.4531506299972534 + 100.0 * 9.065507888793945
Epoch 2430, val loss: 0.5834957957267761
Epoch 2440, training loss: 906.9957885742188 = 0.4517943859100342 + 100.0 * 9.06544017791748
Epoch 2440, val loss: 0.583375096321106
Epoch 2450, training loss: 907.3423461914062 = 0.45045942068099976 + 100.0 * 9.06891918182373
Epoch 2450, val loss: 0.5832706093788147
Epoch 2460, training loss: 907.58935546875 = 0.44911739230155945 + 100.0 * 9.071402549743652
Epoch 2460, val loss: 0.5832281112670898
Epoch 2470, training loss: 907.8346557617188 = 0.44777244329452515 + 100.0 * 9.073868751525879
Epoch 2470, val loss: 0.5831276178359985
Epoch 2480, training loss: 907.71142578125 = 0.4463919699192047 + 100.0 * 9.072649955749512
Epoch 2480, val loss: 0.5830427408218384
Epoch 2490, training loss: 907.6478271484375 = 0.44501933455467224 + 100.0 * 9.072028160095215
Epoch 2490, val loss: 0.5830363035202026
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7608695652173912
0.8140259363906398
=== training gcn model ===
Epoch 0, training loss: 1024.8182373046875 = 1.09915292263031 + 100.0 * 10.237191200256348
Epoch 0, val loss: 1.0978423357009888
Epoch 10, training loss: 984.0687255859375 = 1.0954344272613525 + 100.0 * 9.829732894897461
Epoch 10, val loss: 1.0942023992538452
Epoch 20, training loss: 966.1558227539062 = 1.0920580625534058 + 100.0 * 9.65063762664795
Epoch 20, val loss: 1.0908679962158203
Epoch 30, training loss: 952.4290771484375 = 1.0888811349868774 + 100.0 * 9.513401985168457
Epoch 30, val loss: 1.0877375602722168
Epoch 40, training loss: 941.589111328125 = 1.0858689546585083 + 100.0 * 9.40503215789795
Epoch 40, val loss: 1.0847769975662231
Epoch 50, training loss: 932.4988403320312 = 1.0830260515213013 + 100.0 * 9.31415843963623
Epoch 50, val loss: 1.0819907188415527
Epoch 60, training loss: 924.73486328125 = 1.0803256034851074 + 100.0 * 9.23654556274414
Epoch 60, val loss: 1.0793489217758179
Epoch 70, training loss: 918.1138305664062 = 1.0777488946914673 + 100.0 * 9.170360565185547
Epoch 70, val loss: 1.0768396854400635
Epoch 80, training loss: 912.3911743164062 = 1.0752754211425781 + 100.0 * 9.1131591796875
Epoch 80, val loss: 1.074440360069275
Epoch 90, training loss: 907.4736938476562 = 1.0729247331619263 + 100.0 * 9.064007759094238
Epoch 90, val loss: 1.0721768140792847
Epoch 100, training loss: 903.3550415039062 = 1.070672869682312 + 100.0 * 9.022843360900879
Epoch 100, val loss: 1.070012092590332
Epoch 110, training loss: 899.7755737304688 = 1.0685150623321533 + 100.0 * 8.987070083618164
Epoch 110, val loss: 1.0679491758346558
Epoch 120, training loss: 896.61962890625 = 1.0664225816726685 + 100.0 * 8.95553207397461
Epoch 120, val loss: 1.0659618377685547
Epoch 130, training loss: 894.0538330078125 = 1.0644184350967407 + 100.0 * 8.92989444732666
Epoch 130, val loss: 1.0640640258789062
Epoch 140, training loss: 891.7479858398438 = 1.0624619722366333 + 100.0 * 8.906855583190918
Epoch 140, val loss: 1.062211275100708
Epoch 150, training loss: 890.0189819335938 = 1.0605591535568237 + 100.0 * 8.8895845413208
Epoch 150, val loss: 1.0604376792907715
Epoch 160, training loss: 888.0839233398438 = 1.0586479902267456 + 100.0 * 8.87025260925293
Epoch 160, val loss: 1.058661699295044
Epoch 170, training loss: 886.7489013671875 = 1.0567723512649536 + 100.0 * 8.856921195983887
Epoch 170, val loss: 1.0569214820861816
Epoch 180, training loss: 885.7560424804688 = 1.054898977279663 + 100.0 * 8.84701156616211
Epoch 180, val loss: 1.0551897287368774
Epoch 190, training loss: 884.8201904296875 = 1.0529669523239136 + 100.0 * 8.837672233581543
Epoch 190, val loss: 1.0534188747406006
Epoch 200, training loss: 883.8893432617188 = 1.0510183572769165 + 100.0 * 8.828383445739746
Epoch 200, val loss: 1.051636815071106
Epoch 210, training loss: 883.3118896484375 = 1.0490341186523438 + 100.0 * 8.82262897491455
Epoch 210, val loss: 1.049806833267212
Epoch 220, training loss: 882.467529296875 = 1.0469748973846436 + 100.0 * 8.814205169677734
Epoch 220, val loss: 1.0479328632354736
Epoch 230, training loss: 882.0642700195312 = 1.0448505878448486 + 100.0 * 8.81019401550293
Epoch 230, val loss: 1.0459853410720825
Epoch 240, training loss: 881.7091064453125 = 1.0426661968231201 + 100.0 * 8.80666446685791
Epoch 240, val loss: 1.0439821481704712
Epoch 250, training loss: 881.7543334960938 = 1.040234923362732 + 100.0 * 8.807141304016113
Epoch 250, val loss: 1.0417585372924805
Epoch 260, training loss: 880.8594360351562 = 1.0377591848373413 + 100.0 * 8.798216819763184
Epoch 260, val loss: 1.0394482612609863
Epoch 270, training loss: 881.1383056640625 = 1.0352646112442017 + 100.0 * 8.801030158996582
Epoch 270, val loss: 1.037231683731079
Epoch 280, training loss: 880.4285888671875 = 1.0324382781982422 + 100.0 * 8.793961524963379
Epoch 280, val loss: 1.0346384048461914
Epoch 290, training loss: 880.6961059570312 = 1.029581069946289 + 100.0 * 8.79666519165039
Epoch 290, val loss: 1.0320377349853516
Epoch 300, training loss: 880.294189453125 = 1.0264403820037842 + 100.0 * 8.792677879333496
Epoch 300, val loss: 1.02916419506073
Epoch 310, training loss: 880.6983032226562 = 1.023107886314392 + 100.0 * 8.796751976013184
Epoch 310, val loss: 1.0260868072509766
Epoch 320, training loss: 880.7113037109375 = 1.0195235013961792 + 100.0 * 8.796917915344238
Epoch 320, val loss: 1.0227925777435303
Epoch 330, training loss: 880.7673950195312 = 1.0158058404922485 + 100.0 * 8.797515869140625
Epoch 330, val loss: 1.019478678703308
Epoch 340, training loss: 880.9010620117188 = 1.0116872787475586 + 100.0 * 8.798893928527832
Epoch 340, val loss: 1.0156581401824951
Epoch 350, training loss: 880.8923950195312 = 1.0073890686035156 + 100.0 * 8.798850059509277
Epoch 350, val loss: 1.01174795627594
Epoch 360, training loss: 881.2566528320312 = 1.0028992891311646 + 100.0 * 8.802536964416504
Epoch 360, val loss: 1.0076320171356201
Epoch 370, training loss: 881.3706665039062 = 0.9979327321052551 + 100.0 * 8.803727149963379
Epoch 370, val loss: 1.0031362771987915
Epoch 380, training loss: 881.6769409179688 = 0.9929109215736389 + 100.0 * 8.806839942932129
Epoch 380, val loss: 0.9985408186912537
Epoch 390, training loss: 881.6857299804688 = 0.9875982403755188 + 100.0 * 8.806981086730957
Epoch 390, val loss: 0.9937249422073364
Epoch 400, training loss: 881.8483276367188 = 0.9820090532302856 + 100.0 * 8.808663368225098
Epoch 400, val loss: 0.9886206388473511
Epoch 410, training loss: 882.2694702148438 = 0.9760767221450806 + 100.0 * 8.812933921813965
Epoch 410, val loss: 0.9832688570022583
Epoch 420, training loss: 882.310791015625 = 0.9699418544769287 + 100.0 * 8.813407897949219
Epoch 420, val loss: 0.9777058959007263
Epoch 430, training loss: 881.8417358398438 = 0.9633817672729492 + 100.0 * 8.808783531188965
Epoch 430, val loss: 0.9718021750450134
Epoch 440, training loss: 882.2221069335938 = 0.9568479061126709 + 100.0 * 8.812652587890625
Epoch 440, val loss: 0.9658433794975281
Epoch 450, training loss: 882.5164794921875 = 0.9499428272247314 + 100.0 * 8.815665245056152
Epoch 450, val loss: 0.9595711827278137
Epoch 460, training loss: 882.2681884765625 = 0.9427568316459656 + 100.0 * 8.813254356384277
Epoch 460, val loss: 0.953198254108429
Epoch 470, training loss: 882.2929077148438 = 0.9354221224784851 + 100.0 * 8.81357479095459
Epoch 470, val loss: 0.9464980959892273
Epoch 480, training loss: 882.7713012695312 = 0.9280460476875305 + 100.0 * 8.818432807922363
Epoch 480, val loss: 0.9398559927940369
Epoch 490, training loss: 882.984130859375 = 0.9204846024513245 + 100.0 * 8.820636749267578
Epoch 490, val loss: 0.9331609606742859
Epoch 500, training loss: 883.1996459960938 = 0.9127722978591919 + 100.0 * 8.822868347167969
Epoch 500, val loss: 0.9261376261711121
Epoch 510, training loss: 883.4251708984375 = 0.9049376249313354 + 100.0 * 8.825202941894531
Epoch 510, val loss: 0.9191401600837708
Epoch 520, training loss: 883.7265014648438 = 0.8970552086830139 + 100.0 * 8.82829475402832
Epoch 520, val loss: 0.9121620655059814
Epoch 530, training loss: 883.819580078125 = 0.8891153931617737 + 100.0 * 8.829304695129395
Epoch 530, val loss: 0.905029296875
Epoch 540, training loss: 884.4547729492188 = 0.8812456727027893 + 100.0 * 8.835735321044922
Epoch 540, val loss: 0.8980352878570557
Epoch 550, training loss: 884.0379638671875 = 0.8731570243835449 + 100.0 * 8.831647872924805
Epoch 550, val loss: 0.8908724784851074
Epoch 560, training loss: 883.9108276367188 = 0.8649969100952148 + 100.0 * 8.830458641052246
Epoch 560, val loss: 0.883705198764801
Epoch 570, training loss: 884.6937866210938 = 0.8570863008499146 + 100.0 * 8.838366508483887
Epoch 570, val loss: 0.8764350414276123
Epoch 580, training loss: 885.1358642578125 = 0.849530816078186 + 100.0 * 8.842863082885742
Epoch 580, val loss: 0.8699272871017456
Epoch 590, training loss: 884.2841796875 = 0.841101884841919 + 100.0 * 8.834430694580078
Epoch 590, val loss: 0.8625509738922119
Epoch 600, training loss: 884.8944702148438 = 0.8332700729370117 + 100.0 * 8.840612411499023
Epoch 600, val loss: 0.8555393815040588
Epoch 610, training loss: 885.3973388671875 = 0.825598955154419 + 100.0 * 8.845717430114746
Epoch 610, val loss: 0.8486887216567993
Epoch 620, training loss: 885.8346557617188 = 0.8179527521133423 + 100.0 * 8.850167274475098
Epoch 620, val loss: 0.8420546650886536
Epoch 630, training loss: 886.202392578125 = 0.8102887272834778 + 100.0 * 8.853920936584473
Epoch 630, val loss: 0.8352557420730591
Epoch 640, training loss: 886.3919677734375 = 0.8025591969490051 + 100.0 * 8.855894088745117
Epoch 640, val loss: 0.828544020652771
Epoch 650, training loss: 886.383544921875 = 0.7948006391525269 + 100.0 * 8.855887413024902
Epoch 650, val loss: 0.8216983079910278
Epoch 660, training loss: 886.1375122070312 = 0.7870975732803345 + 100.0 * 8.853504180908203
Epoch 660, val loss: 0.8149064183235168
Epoch 670, training loss: 886.5238647460938 = 0.7795984745025635 + 100.0 * 8.857442855834961
Epoch 670, val loss: 0.80845707654953
Epoch 680, training loss: 886.9111328125 = 0.7722359895706177 + 100.0 * 8.86138916015625
Epoch 680, val loss: 0.8020275235176086
Epoch 690, training loss: 887.4359130859375 = 0.7649443745613098 + 100.0 * 8.86670970916748
Epoch 690, val loss: 0.7955260872840881
Epoch 700, training loss: 887.3224487304688 = 0.7574301362037659 + 100.0 * 8.865650177001953
Epoch 700, val loss: 0.7890225052833557
Epoch 710, training loss: 887.60498046875 = 0.7502516508102417 + 100.0 * 8.868547439575195
Epoch 710, val loss: 0.7826940417289734
Epoch 720, training loss: 888.6349487304688 = 0.7432736754417419 + 100.0 * 8.87891674041748
Epoch 720, val loss: 0.7766201496124268
Epoch 730, training loss: 888.5984497070312 = 0.7364605069160461 + 100.0 * 8.878620147705078
Epoch 730, val loss: 0.7705710530281067
Epoch 740, training loss: 888.9244995117188 = 0.7298945188522339 + 100.0 * 8.881945610046387
Epoch 740, val loss: 0.764821469783783
Epoch 750, training loss: 888.86083984375 = 0.7233889698982239 + 100.0 * 8.88137435913086
Epoch 750, val loss: 0.7592222094535828
Epoch 760, training loss: 889.2047119140625 = 0.7170839905738831 + 100.0 * 8.884876251220703
Epoch 760, val loss: 0.7536383867263794
Epoch 770, training loss: 889.6724853515625 = 0.7108182311058044 + 100.0 * 8.889616966247559
Epoch 770, val loss: 0.7482909560203552
Epoch 780, training loss: 888.6390991210938 = 0.7046908140182495 + 100.0 * 8.87934398651123
Epoch 780, val loss: 0.7428542971611023
Epoch 790, training loss: 887.3302001953125 = 0.6981362700462341 + 100.0 * 8.866320610046387
Epoch 790, val loss: 0.7366969585418701
Epoch 800, training loss: 888.2774047851562 = 0.6936541199684143 + 100.0 * 8.875837326049805
Epoch 800, val loss: 0.7328504920005798
Epoch 810, training loss: 888.3681030273438 = 0.6877686381340027 + 100.0 * 8.876803398132324
Epoch 810, val loss: 0.728200376033783
Epoch 820, training loss: 888.27587890625 = 0.6825178861618042 + 100.0 * 8.875933647155762
Epoch 820, val loss: 0.7238495945930481
Epoch 830, training loss: 888.5184936523438 = 0.6773545145988464 + 100.0 * 8.878411293029785
Epoch 830, val loss: 0.7188271284103394
Epoch 840, training loss: 889.5433349609375 = 0.6725513339042664 + 100.0 * 8.888708114624023
Epoch 840, val loss: 0.7147481441497803
Epoch 850, training loss: 889.7705078125 = 0.6677774786949158 + 100.0 * 8.891027450561523
Epoch 850, val loss: 0.7106807231903076
Epoch 860, training loss: 890.7832641601562 = 0.663205623626709 + 100.0 * 8.901200294494629
Epoch 860, val loss: 0.7066899538040161
Epoch 870, training loss: 890.9793090820312 = 0.6586024761199951 + 100.0 * 8.903206825256348
Epoch 870, val loss: 0.7026877999305725
Epoch 880, training loss: 891.4393920898438 = 0.6542012691497803 + 100.0 * 8.907852172851562
Epoch 880, val loss: 0.6988171339035034
Epoch 890, training loss: 891.937744140625 = 0.6500120162963867 + 100.0 * 8.912877082824707
Epoch 890, val loss: 0.6952309608459473
Epoch 900, training loss: 892.0009765625 = 0.6458919644355774 + 100.0 * 8.913551330566406
Epoch 900, val loss: 0.6916115880012512
Epoch 910, training loss: 891.8031616210938 = 0.6419340372085571 + 100.0 * 8.911612510681152
Epoch 910, val loss: 0.6881663203239441
Epoch 920, training loss: 892.3380737304688 = 0.6382997632026672 + 100.0 * 8.916997909545898
Epoch 920, val loss: 0.6852286458015442
Epoch 930, training loss: 892.2423095703125 = 0.634672999382019 + 100.0 * 8.91607666015625
Epoch 930, val loss: 0.6820815801620483
Epoch 940, training loss: 892.82568359375 = 0.6312345266342163 + 100.0 * 8.921944618225098
Epoch 940, val loss: 0.6791148781776428
Epoch 950, training loss: 893.2845458984375 = 0.6279014348983765 + 100.0 * 8.926566123962402
Epoch 950, val loss: 0.6762332320213318
Epoch 960, training loss: 893.7288208007812 = 0.6246192455291748 + 100.0 * 8.931041717529297
Epoch 960, val loss: 0.6734532713890076
Epoch 970, training loss: 893.8192138671875 = 0.6213826537132263 + 100.0 * 8.931978225708008
Epoch 970, val loss: 0.6707789897918701
Epoch 980, training loss: 894.2744140625 = 0.6183697581291199 + 100.0 * 8.93656063079834
Epoch 980, val loss: 0.6682111620903015
Epoch 990, training loss: 894.2416381835938 = 0.6154387593269348 + 100.0 * 8.936262130737305
Epoch 990, val loss: 0.665748655796051
Epoch 1000, training loss: 894.7875366210938 = 0.6126730442047119 + 100.0 * 8.94174861907959
Epoch 1000, val loss: 0.6634009480476379
Epoch 1010, training loss: 895.267822265625 = 0.610119104385376 + 100.0 * 8.946577072143555
Epoch 1010, val loss: 0.6610670685768127
Epoch 1020, training loss: 895.0025634765625 = 0.607251763343811 + 100.0 * 8.943953514099121
Epoch 1020, val loss: 0.6591227054595947
Epoch 1030, training loss: 895.5392456054688 = 0.6048204302787781 + 100.0 * 8.949344635009766
Epoch 1030, val loss: 0.6567118167877197
Epoch 1040, training loss: 894.7655639648438 = 0.6022427082061768 + 100.0 * 8.941633224487305
Epoch 1040, val loss: 0.6547951102256775
Epoch 1050, training loss: 895.4307861328125 = 0.600188136100769 + 100.0 * 8.9483060836792
Epoch 1050, val loss: 0.653234601020813
Epoch 1060, training loss: 895.49169921875 = 0.5978263020515442 + 100.0 * 8.948938369750977
Epoch 1060, val loss: 0.651240348815918
Epoch 1070, training loss: 896.0694580078125 = 0.5957201719284058 + 100.0 * 8.954737663269043
Epoch 1070, val loss: 0.6495257616043091
Epoch 1080, training loss: 896.5951538085938 = 0.5936217308044434 + 100.0 * 8.960015296936035
Epoch 1080, val loss: 0.647978663444519
Epoch 1090, training loss: 895.7525024414062 = 0.5913656949996948 + 100.0 * 8.951611518859863
Epoch 1090, val loss: 0.6461511254310608
Epoch 1100, training loss: 896.1954345703125 = 0.5894825458526611 + 100.0 * 8.956059455871582
Epoch 1100, val loss: 0.6447218060493469
Epoch 1110, training loss: 896.600341796875 = 0.5876267552375793 + 100.0 * 8.960126876831055
Epoch 1110, val loss: 0.6434360146522522
Epoch 1120, training loss: 897.5599975585938 = 0.5859465003013611 + 100.0 * 8.969740867614746
Epoch 1120, val loss: 0.6420544385910034
Epoch 1130, training loss: 897.9109497070312 = 0.584196925163269 + 100.0 * 8.973267555236816
Epoch 1130, val loss: 0.640736997127533
Epoch 1140, training loss: 896.841552734375 = 0.582268476486206 + 100.0 * 8.962593078613281
Epoch 1140, val loss: 0.6395627856254578
Epoch 1150, training loss: 897.44970703125 = 0.5806924104690552 + 100.0 * 8.968689918518066
Epoch 1150, val loss: 0.6385276913642883
Epoch 1160, training loss: 898.3416137695312 = 0.5792126655578613 + 100.0 * 8.97762393951416
Epoch 1160, val loss: 0.6372164487838745
Epoch 1170, training loss: 898.549560546875 = 0.5776179432868958 + 100.0 * 8.979719161987305
Epoch 1170, val loss: 0.6360766291618347
Epoch 1180, training loss: 898.7710571289062 = 0.5760742425918579 + 100.0 * 8.981949806213379
Epoch 1180, val loss: 0.6350884437561035
Epoch 1190, training loss: 898.7050170898438 = 0.5745744705200195 + 100.0 * 8.981304168701172
Epoch 1190, val loss: 0.6339653134346008
Epoch 1200, training loss: 899.236083984375 = 0.5730995535850525 + 100.0 * 8.986629486083984
Epoch 1200, val loss: 0.6330544948577881
Epoch 1210, training loss: 899.8117065429688 = 0.5719565153121948 + 100.0 * 8.99239730834961
Epoch 1210, val loss: 0.6324347853660583
Epoch 1220, training loss: 899.8428344726562 = 0.5705246925354004 + 100.0 * 8.99272346496582
Epoch 1220, val loss: 0.6315523386001587
Epoch 1230, training loss: 900.0379638671875 = 0.5692493319511414 + 100.0 * 8.9946870803833
Epoch 1230, val loss: 0.6306681036949158
Epoch 1240, training loss: 899.5423583984375 = 0.5677722692489624 + 100.0 * 8.98974609375
Epoch 1240, val loss: 0.6299173831939697
Epoch 1250, training loss: 899.8693237304688 = 0.5665438175201416 + 100.0 * 8.993027687072754
Epoch 1250, val loss: 0.6291157007217407
Epoch 1260, training loss: 900.0887451171875 = 0.5652108788490295 + 100.0 * 8.995235443115234
Epoch 1260, val loss: 0.6277084946632385
Epoch 1270, training loss: 901.9375 = 0.5641111135482788 + 100.0 * 9.013733863830566
Epoch 1270, val loss: 0.6274940371513367
Epoch 1280, training loss: 899.8861083984375 = 0.5627182126045227 + 100.0 * 8.993233680725098
Epoch 1280, val loss: 0.6267585754394531
Epoch 1290, training loss: 899.8751220703125 = 0.561559796333313 + 100.0 * 8.993135452270508
Epoch 1290, val loss: 0.6260538101196289
Epoch 1300, training loss: 900.5978393554688 = 0.5603363513946533 + 100.0 * 9.000374794006348
Epoch 1300, val loss: 0.6253429651260376
Epoch 1310, training loss: 901.3479614257812 = 0.559245228767395 + 100.0 * 9.00788688659668
Epoch 1310, val loss: 0.6248220205307007
Epoch 1320, training loss: 901.6832275390625 = 0.5581033825874329 + 100.0 * 9.011251449584961
Epoch 1320, val loss: 0.6243247985839844
Epoch 1330, training loss: 901.3176879882812 = 0.5569486618041992 + 100.0 * 9.007607460021973
Epoch 1330, val loss: 0.6236928105354309
Epoch 1340, training loss: 901.8076782226562 = 0.5559350848197937 + 100.0 * 9.012517929077148
Epoch 1340, val loss: 0.6233850121498108
Epoch 1350, training loss: 902.4203491210938 = 0.5548741221427917 + 100.0 * 9.018654823303223
Epoch 1350, val loss: 0.6227365732192993
Epoch 1360, training loss: 902.495361328125 = 0.5537581443786621 + 100.0 * 9.019415855407715
Epoch 1360, val loss: 0.622198224067688
Epoch 1370, training loss: 902.575439453125 = 0.5526742339134216 + 100.0 * 9.020227432250977
Epoch 1370, val loss: 0.6218112111091614
Epoch 1380, training loss: 903.1072387695312 = 0.5516666173934937 + 100.0 * 9.025555610656738
Epoch 1380, val loss: 0.6212314367294312
Epoch 1390, training loss: 903.6048583984375 = 0.5506229400634766 + 100.0 * 9.030542373657227
Epoch 1390, val loss: 0.6207578778266907
Epoch 1400, training loss: 903.4010620117188 = 0.5495281219482422 + 100.0 * 9.028515815734863
Epoch 1400, val loss: 0.6204072833061218
Epoch 1410, training loss: 903.9676513671875 = 0.5484756231307983 + 100.0 * 9.034192085266113
Epoch 1410, val loss: 0.6201106905937195
Epoch 1420, training loss: 902.8048706054688 = 0.5474156737327576 + 100.0 * 9.022574424743652
Epoch 1420, val loss: 0.6194972991943359
Epoch 1430, training loss: 903.1048583984375 = 0.5464683175086975 + 100.0 * 9.02558422088623
Epoch 1430, val loss: 0.6190536618232727
Epoch 1440, training loss: 904.098876953125 = 0.5455842018127441 + 100.0 * 9.03553295135498
Epoch 1440, val loss: 0.6187816262245178
Epoch 1450, training loss: 903.9082641601562 = 0.5445395112037659 + 100.0 * 9.033637046813965
Epoch 1450, val loss: 0.6183527112007141
Epoch 1460, training loss: 904.5280151367188 = 0.5435593724250793 + 100.0 * 9.039844512939453
Epoch 1460, val loss: 0.6180567145347595
Epoch 1470, training loss: 904.9176635742188 = 0.5425933599472046 + 100.0 * 9.043750762939453
Epoch 1470, val loss: 0.617702841758728
Epoch 1480, training loss: 904.5228271484375 = 0.5415321588516235 + 100.0 * 9.039813041687012
Epoch 1480, val loss: 0.6172502040863037
Epoch 1490, training loss: 904.5267333984375 = 0.5404841899871826 + 100.0 * 9.039862632751465
Epoch 1490, val loss: 0.6171119809150696
Epoch 1500, training loss: 904.61865234375 = 0.5394588112831116 + 100.0 * 9.040792465209961
Epoch 1500, val loss: 0.6168814301490784
Epoch 1510, training loss: 904.5180053710938 = 0.5385071039199829 + 100.0 * 9.039794921875
Epoch 1510, val loss: 0.6164270043373108
Epoch 1520, training loss: 904.96923828125 = 0.5375986099243164 + 100.0 * 9.044316291809082
Epoch 1520, val loss: 0.6161098480224609
Epoch 1530, training loss: 905.7470703125 = 0.5366644263267517 + 100.0 * 9.052103996276855
Epoch 1530, val loss: 0.6157727837562561
Epoch 1540, training loss: 905.8211669921875 = 0.535713255405426 + 100.0 * 9.052854537963867
Epoch 1540, val loss: 0.6155393719673157
Epoch 1550, training loss: 906.2646484375 = 0.5347973108291626 + 100.0 * 9.05729866027832
Epoch 1550, val loss: 0.6152483820915222
Epoch 1560, training loss: 906.3853759765625 = 0.5338137149810791 + 100.0 * 9.058515548706055
Epoch 1560, val loss: 0.6149190068244934
Epoch 1570, training loss: 906.6803588867188 = 0.5329210758209229 + 100.0 * 9.061474800109863
Epoch 1570, val loss: 0.6147926449775696
Epoch 1580, training loss: 906.7836303710938 = 0.5319746732711792 + 100.0 * 9.062516212463379
Epoch 1580, val loss: 0.6144292950630188
Epoch 1590, training loss: 906.0384521484375 = 0.5309944748878479 + 100.0 * 9.055074691772461
Epoch 1590, val loss: 0.6142920851707458
Epoch 1600, training loss: 906.871826171875 = 0.5301700830459595 + 100.0 * 9.063416481018066
Epoch 1600, val loss: 0.613848865032196
Epoch 1610, training loss: 906.734619140625 = 0.5291958451271057 + 100.0 * 9.062054634094238
Epoch 1610, val loss: 0.6135497093200684
Epoch 1620, training loss: 907.119140625 = 0.5283059477806091 + 100.0 * 9.065908432006836
Epoch 1620, val loss: 0.6134469509124756
Epoch 1630, training loss: 907.7772216796875 = 0.5273991227149963 + 100.0 * 9.072498321533203
Epoch 1630, val loss: 0.6130841970443726
Epoch 1640, training loss: 907.2191162109375 = 0.5264357328414917 + 100.0 * 9.066926956176758
Epoch 1640, val loss: 0.6128835678100586
Epoch 1650, training loss: 907.474853515625 = 0.5255131125450134 + 100.0 * 9.069493293762207
Epoch 1650, val loss: 0.6128507852554321
Epoch 1660, training loss: 908.3346557617188 = 0.5246469378471375 + 100.0 * 9.078100204467773
Epoch 1660, val loss: 0.6124392747879028
Epoch 1670, training loss: 908.4191284179688 = 0.5237206816673279 + 100.0 * 9.078953742980957
Epoch 1670, val loss: 0.6122555732727051
Epoch 1680, training loss: 908.3634643554688 = 0.5227671265602112 + 100.0 * 9.078407287597656
Epoch 1680, val loss: 0.6120173931121826
Epoch 1690, training loss: 908.9254150390625 = 0.5218833088874817 + 100.0 * 9.08403491973877
Epoch 1690, val loss: 0.6118414998054504
Epoch 1700, training loss: 908.2680053710938 = 0.5209276080131531 + 100.0 * 9.077470779418945
Epoch 1700, val loss: 0.6115817427635193
Epoch 1710, training loss: 908.7416381835938 = 0.5200482606887817 + 100.0 * 9.082216262817383
Epoch 1710, val loss: 0.6114906668663025
Epoch 1720, training loss: 908.840087890625 = 0.5190870761871338 + 100.0 * 9.083209991455078
Epoch 1720, val loss: 0.6111333966255188
Epoch 1730, training loss: 908.8942260742188 = 0.5181728005409241 + 100.0 * 9.083760261535645
Epoch 1730, val loss: 0.6110365390777588
Epoch 1740, training loss: 908.995361328125 = 0.517255425453186 + 100.0 * 9.0847806930542
Epoch 1740, val loss: 0.6107349395751953
Epoch 1750, training loss: 909.0841064453125 = 0.5163117051124573 + 100.0 * 9.085678100585938
Epoch 1750, val loss: 0.6106951236724854
Epoch 1760, training loss: 909.4453125 = 0.5153956413269043 + 100.0 * 9.089299201965332
Epoch 1760, val loss: 0.610394299030304
Epoch 1770, training loss: 909.75048828125 = 0.5144819617271423 + 100.0 * 9.092360496520996
Epoch 1770, val loss: 0.61012864112854
Epoch 1780, training loss: 909.881591796875 = 0.5135534405708313 + 100.0 * 9.093680381774902
Epoch 1780, val loss: 0.6098206639289856
Epoch 1790, training loss: 909.3399658203125 = 0.5125905275344849 + 100.0 * 9.088274002075195
Epoch 1790, val loss: 0.609573245048523
Epoch 1800, training loss: 906.4781494140625 = 0.5114344358444214 + 100.0 * 9.059667587280273
Epoch 1800, val loss: 0.609007716178894
Epoch 1810, training loss: 908.5933837890625 = 0.5104454755783081 + 100.0 * 9.080829620361328
Epoch 1810, val loss: 0.6092438101768494
Epoch 1820, training loss: 905.845703125 = 0.5093388557434082 + 100.0 * 9.053363800048828
Epoch 1820, val loss: 0.6087508797645569
Epoch 1830, training loss: 906.4406127929688 = 0.5085367560386658 + 100.0 * 9.059320449829102
Epoch 1830, val loss: 0.6085585355758667
Epoch 1840, training loss: 907.4109497070312 = 0.5077228546142578 + 100.0 * 9.069031715393066
Epoch 1840, val loss: 0.6085798740386963
Epoch 1850, training loss: 908.0774536132812 = 0.5068691372871399 + 100.0 * 9.075705528259277
Epoch 1850, val loss: 0.6082786917686462
Epoch 1860, training loss: 908.5440673828125 = 0.5058596730232239 + 100.0 * 9.080382347106934
Epoch 1860, val loss: 0.608111560344696
Epoch 1870, training loss: 909.1709594726562 = 0.504923403263092 + 100.0 * 9.086660385131836
Epoch 1870, val loss: 0.6076323390007019
Epoch 1880, training loss: 909.6515502929688 = 0.5040016174316406 + 100.0 * 9.091475486755371
Epoch 1880, val loss: 0.6076565384864807
Epoch 1890, training loss: 909.6889038085938 = 0.503013014793396 + 100.0 * 9.091858863830566
Epoch 1890, val loss: 0.6073841452598572
Epoch 1900, training loss: 910.1414184570312 = 0.5020788908004761 + 100.0 * 9.096393585205078
Epoch 1900, val loss: 0.6072249412536621
Epoch 1910, training loss: 910.1453857421875 = 0.5011099576950073 + 100.0 * 9.096443176269531
Epoch 1910, val loss: 0.60685133934021
Epoch 1920, training loss: 908.3461303710938 = 0.49993279576301575 + 100.0 * 9.078461647033691
Epoch 1920, val loss: 0.6061943173408508
Epoch 1930, training loss: 909.8986206054688 = 0.49889516830444336 + 100.0 * 9.09399700164795
Epoch 1930, val loss: 0.6059502363204956
Epoch 1940, training loss: 907.9581298828125 = 0.4978088140487671 + 100.0 * 9.074603080749512
Epoch 1940, val loss: 0.6069648265838623
Epoch 1950, training loss: 907.6802978515625 = 0.49682164192199707 + 100.0 * 9.071834564208984
Epoch 1950, val loss: 0.6057708263397217
Epoch 1960, training loss: 908.6354370117188 = 0.49592602252960205 + 100.0 * 9.081395149230957
Epoch 1960, val loss: 0.60603928565979
Epoch 1970, training loss: 909.5579223632812 = 0.49506446719169617 + 100.0 * 9.090628623962402
Epoch 1970, val loss: 0.6055107116699219
Epoch 1980, training loss: 910.3406372070312 = 0.49411606788635254 + 100.0 * 9.098464965820312
Epoch 1980, val loss: 0.6056922674179077
Epoch 1990, training loss: 911.04931640625 = 0.4931439459323883 + 100.0 * 9.105561256408691
Epoch 1990, val loss: 0.6052377223968506
Epoch 2000, training loss: 911.1282348632812 = 0.4921474754810333 + 100.0 * 9.106361389160156
Epoch 2000, val loss: 0.6050511598587036
Epoch 2010, training loss: 911.41796875 = 0.4911566972732544 + 100.0 * 9.109268188476562
Epoch 2010, val loss: 0.6049190759658813
Epoch 2020, training loss: 911.6951904296875 = 0.4901970624923706 + 100.0 * 9.11205005645752
Epoch 2020, val loss: 0.6047620177268982
Epoch 2030, training loss: 911.8851928710938 = 0.4892009496688843 + 100.0 * 9.113960266113281
Epoch 2030, val loss: 0.604568362236023
Epoch 2040, training loss: 911.8170166015625 = 0.4882028102874756 + 100.0 * 9.113287925720215
Epoch 2040, val loss: 0.6043313145637512
Epoch 2050, training loss: 911.78466796875 = 0.4871934652328491 + 100.0 * 9.112975120544434
Epoch 2050, val loss: 0.6041699051856995
Epoch 2060, training loss: 911.9757080078125 = 0.486187219619751 + 100.0 * 9.11489486694336
Epoch 2060, val loss: 0.6040773391723633
Epoch 2070, training loss: 912.5257568359375 = 0.4852210581302643 + 100.0 * 9.120405197143555
Epoch 2070, val loss: 0.6039809584617615
Epoch 2080, training loss: 912.170654296875 = 0.48415014147758484 + 100.0 * 9.116865158081055
Epoch 2080, val loss: 0.6038253307342529
Epoch 2090, training loss: 912.1298217773438 = 0.48315536975860596 + 100.0 * 9.116466522216797
Epoch 2090, val loss: 0.6035699844360352
Epoch 2100, training loss: 912.5819091796875 = 0.48218199610710144 + 100.0 * 9.120997428894043
Epoch 2100, val loss: 0.6034610867500305
Epoch 2110, training loss: 913.119873046875 = 0.48117130994796753 + 100.0 * 9.126386642456055
Epoch 2110, val loss: 0.6032553911209106
Epoch 2120, training loss: 912.6920166015625 = 0.48012760281562805 + 100.0 * 9.122118949890137
Epoch 2120, val loss: 0.6031467318534851
Epoch 2130, training loss: 913.0369873046875 = 0.47913795709609985 + 100.0 * 9.125578880310059
Epoch 2130, val loss: 0.6029162406921387
Epoch 2140, training loss: 913.5986938476562 = 0.47813448309898376 + 100.0 * 9.131205558776855
Epoch 2140, val loss: 0.6027328968048096
Epoch 2150, training loss: 913.6106567382812 = 0.4771227538585663 + 100.0 * 9.131335258483887
Epoch 2150, val loss: 0.6026584506034851
Epoch 2160, training loss: 913.536376953125 = 0.4760798513889313 + 100.0 * 9.130602836608887
Epoch 2160, val loss: 0.6024335622787476
Epoch 2170, training loss: 913.7818603515625 = 0.4750487208366394 + 100.0 * 9.133068084716797
Epoch 2170, val loss: 0.6022608876228333
Epoch 2180, training loss: 914.1724243164062 = 0.47405481338500977 + 100.0 * 9.136983871459961
Epoch 2180, val loss: 0.6021034121513367
Epoch 2190, training loss: 913.7159423828125 = 0.47301536798477173 + 100.0 * 9.132429122924805
Epoch 2190, val loss: 0.6019299626350403
Epoch 2200, training loss: 913.7655639648438 = 0.4720042943954468 + 100.0 * 9.132935523986816
Epoch 2200, val loss: 0.6018883585929871
Epoch 2210, training loss: 914.1547241210938 = 0.4709843397140503 + 100.0 * 9.136837005615234
Epoch 2210, val loss: 0.6016768217086792
Epoch 2220, training loss: 914.3091430664062 = 0.4699651002883911 + 100.0 * 9.138391494750977
Epoch 2220, val loss: 0.6015135645866394
Epoch 2230, training loss: 914.6292114257812 = 0.4689539968967438 + 100.0 * 9.141602516174316
Epoch 2230, val loss: 0.6013030409812927
Epoch 2240, training loss: 914.363525390625 = 0.46790987253189087 + 100.0 * 9.138956069946289
Epoch 2240, val loss: 0.6009674072265625
Epoch 2250, training loss: 914.3472900390625 = 0.46693673729896545 + 100.0 * 9.138803482055664
Epoch 2250, val loss: 0.6009513139724731
Epoch 2260, training loss: 914.6173706054688 = 0.46585631370544434 + 100.0 * 9.141514778137207
Epoch 2260, val loss: 0.6009012460708618
Epoch 2270, training loss: 913.3736572265625 = 0.4646943509578705 + 100.0 * 9.12908935546875
Epoch 2270, val loss: 0.6004678606987
Epoch 2280, training loss: 913.8734741210938 = 0.463727742433548 + 100.0 * 9.1340970993042
Epoch 2280, val loss: 0.6003386378288269
Epoch 2290, training loss: 914.3773193359375 = 0.46272042393684387 + 100.0 * 9.139145851135254
Epoch 2290, val loss: 0.6002650260925293
Epoch 2300, training loss: 915.272705078125 = 0.4616830348968506 + 100.0 * 9.148110389709473
Epoch 2300, val loss: 0.6000133752822876
Epoch 2310, training loss: 915.7062377929688 = 0.460632860660553 + 100.0 * 9.152456283569336
Epoch 2310, val loss: 0.599646806716919
Epoch 2320, training loss: 915.1434326171875 = 0.4595160484313965 + 100.0 * 9.146839141845703
Epoch 2320, val loss: 0.5996717810630798
Epoch 2330, training loss: 915.3302001953125 = 0.45847347378730774 + 100.0 * 9.148716926574707
Epoch 2330, val loss: 0.5994888544082642
Epoch 2340, training loss: 915.7102661132812 = 0.4574163258075714 + 100.0 * 9.152528762817383
Epoch 2340, val loss: 0.5992957949638367
Epoch 2350, training loss: 915.9456787109375 = 0.4563318192958832 + 100.0 * 9.154892921447754
Epoch 2350, val loss: 0.5991446375846863
Epoch 2360, training loss: 915.229248046875 = 0.4551840126514435 + 100.0 * 9.147740364074707
Epoch 2360, val loss: 0.5990009307861328
Epoch 2370, training loss: 915.7129516601562 = 0.4541640877723694 + 100.0 * 9.152587890625
Epoch 2370, val loss: 0.5987877249717712
Epoch 2380, training loss: 916.0721435546875 = 0.4530922472476959 + 100.0 * 9.156190872192383
Epoch 2380, val loss: 0.5985203981399536
Epoch 2390, training loss: 916.1124267578125 = 0.4520079791545868 + 100.0 * 9.156603813171387
Epoch 2390, val loss: 0.598319947719574
Epoch 2400, training loss: 915.9404296875 = 0.4509165287017822 + 100.0 * 9.154894828796387
Epoch 2400, val loss: 0.5982954502105713
Epoch 2410, training loss: 916.4558715820312 = 0.44987058639526367 + 100.0 * 9.160059928894043
Epoch 2410, val loss: 0.5980616211891174
Epoch 2420, training loss: 916.5363159179688 = 0.44876718521118164 + 100.0 * 9.16087532043457
Epoch 2420, val loss: 0.5977869033813477
Epoch 2430, training loss: 916.0562133789062 = 0.44768455624580383 + 100.0 * 9.156085014343262
Epoch 2430, val loss: 0.5975612998008728
Epoch 2440, training loss: 915.8558349609375 = 0.44665855169296265 + 100.0 * 9.154091835021973
Epoch 2440, val loss: 0.5976483821868896
Epoch 2450, training loss: 916.212646484375 = 0.4455673098564148 + 100.0 * 9.157670974731445
Epoch 2450, val loss: 0.5974594950675964
Epoch 2460, training loss: 916.6923828125 = 0.4444517493247986 + 100.0 * 9.162479400634766
Epoch 2460, val loss: 0.5972328782081604
Epoch 2470, training loss: 916.8939819335938 = 0.4433385729789734 + 100.0 * 9.164505958557129
Epoch 2470, val loss: 0.5969568490982056
Epoch 2480, training loss: 917.1905517578125 = 0.4422217607498169 + 100.0 * 9.16748332977295
Epoch 2480, val loss: 0.5968382954597473
Epoch 2490, training loss: 917.0609741210938 = 0.4411008954048157 + 100.0 * 9.16619873046875
Epoch 2490, val loss: 0.5965863466262817
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7636231884057971
0.8124320799826126
The final CL Acc:0.76338, 0.00196, The final GNN Acc:0.81362, 0.00085
raw graph: torch.Size([2, 88648])
add edge: torch.Size([2, 110746])
remove edge: torch.Size([2, 66572])
updated graph: torch.Size([2, 88670])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 1026.5694580078125 = 1.121780514717102 + 100.0 * 10.254476547241211
Epoch 0, val loss: 1.121626615524292
Epoch 10, training loss: 982.1732177734375 = 1.1168980598449707 + 100.0 * 9.810563087463379
Epoch 10, val loss: 1.1168124675750732
Epoch 20, training loss: 959.405029296875 = 1.1126761436462402 + 100.0 * 9.582923889160156
Epoch 20, val loss: 1.1126066446304321
Epoch 30, training loss: 943.7216186523438 = 1.1085604429244995 + 100.0 * 9.426130294799805
Epoch 30, val loss: 1.108506441116333
Epoch 40, training loss: 931.5885009765625 = 1.1046322584152222 + 100.0 * 9.304839134216309
Epoch 40, val loss: 1.1045987606048584
Epoch 50, training loss: 921.90478515625 = 1.1008543968200684 + 100.0 * 9.208039283752441
Epoch 50, val loss: 1.1008384227752686
Epoch 60, training loss: 914.0335083007812 = 1.097210168838501 + 100.0 * 9.129363059997559
Epoch 60, val loss: 1.097212553024292
Epoch 70, training loss: 907.5281982421875 = 1.093692421913147 + 100.0 * 9.064345359802246
Epoch 70, val loss: 1.0937234163284302
Epoch 80, training loss: 902.0174560546875 = 1.0902955532073975 + 100.0 * 9.009271621704102
Epoch 80, val loss: 1.0903575420379639
Epoch 90, training loss: 897.45166015625 = 1.0869885683059692 + 100.0 * 8.96364688873291
Epoch 90, val loss: 1.0870801210403442
Epoch 100, training loss: 893.7717895507812 = 1.0837887525558472 + 100.0 * 8.9268798828125
Epoch 100, val loss: 1.0839159488677979
Epoch 110, training loss: 890.63623046875 = 1.0806646347045898 + 100.0 * 8.89555549621582
Epoch 110, val loss: 1.0808309316635132
Epoch 120, training loss: 887.9515991210938 = 1.0775964260101318 + 100.0 * 8.86874008178711
Epoch 120, val loss: 1.0778067111968994
Epoch 130, training loss: 885.6663818359375 = 1.0745882987976074 + 100.0 * 8.845917701721191
Epoch 130, val loss: 1.074846863746643
Epoch 140, training loss: 883.652099609375 = 1.07166588306427 + 100.0 * 8.825804710388184
Epoch 140, val loss: 1.071972131729126
Epoch 150, training loss: 882.00244140625 = 1.0687907934188843 + 100.0 * 8.80933666229248
Epoch 150, val loss: 1.0691444873809814
Epoch 160, training loss: 880.4000244140625 = 1.0659780502319336 + 100.0 * 8.793340682983398
Epoch 160, val loss: 1.0663979053497314
Epoch 170, training loss: 878.7728271484375 = 1.0632261037826538 + 100.0 * 8.777095794677734
Epoch 170, val loss: 1.0637129545211792
Epoch 180, training loss: 877.9298706054688 = 1.060502290725708 + 100.0 * 8.768693923950195
Epoch 180, val loss: 1.0610644817352295
Epoch 190, training loss: 876.889892578125 = 1.0578229427337646 + 100.0 * 8.758320808410645
Epoch 190, val loss: 1.0584580898284912
Epoch 200, training loss: 876.0022583007812 = 1.0551525354385376 + 100.0 * 8.749470710754395
Epoch 200, val loss: 1.0558632612228394
Epoch 210, training loss: 875.3231811523438 = 1.0525344610214233 + 100.0 * 8.742706298828125
Epoch 210, val loss: 1.0533409118652344
Epoch 220, training loss: 874.8311157226562 = 1.0499496459960938 + 100.0 * 8.737812042236328
Epoch 220, val loss: 1.0508133172988892
Epoch 230, training loss: 874.3377075195312 = 1.0473517179489136 + 100.0 * 8.732903480529785
Epoch 230, val loss: 1.0483391284942627
Epoch 240, training loss: 874.1858520507812 = 1.0447571277618408 + 100.0 * 8.73141098022461
Epoch 240, val loss: 1.0458589792251587
Epoch 250, training loss: 873.531982421875 = 1.0421923398971558 + 100.0 * 8.724898338317871
Epoch 250, val loss: 1.0433707237243652
Epoch 260, training loss: 872.9874877929688 = 1.039566159248352 + 100.0 * 8.71947956085205
Epoch 260, val loss: 1.0408629179000854
Epoch 270, training loss: 873.064697265625 = 1.0369635820388794 + 100.0 * 8.720276832580566
Epoch 270, val loss: 1.0383579730987549
Epoch 280, training loss: 873.4339599609375 = 1.0342071056365967 + 100.0 * 8.723998069763184
Epoch 280, val loss: 1.0358299016952515
Epoch 290, training loss: 871.9324340820312 = 1.0313187837600708 + 100.0 * 8.70901107788086
Epoch 290, val loss: 1.0330395698547363
Epoch 300, training loss: 872.2420043945312 = 1.0286728143692017 + 100.0 * 8.712133407592773
Epoch 300, val loss: 1.0304384231567383
Epoch 310, training loss: 872.5079345703125 = 1.02584969997406 + 100.0 * 8.714820861816406
Epoch 310, val loss: 1.0277611017227173
Epoch 320, training loss: 872.3798828125 = 1.0228056907653809 + 100.0 * 8.713570594787598
Epoch 320, val loss: 1.0248414278030396
Epoch 330, training loss: 872.7711181640625 = 1.0198215246200562 + 100.0 * 8.717513084411621
Epoch 330, val loss: 1.0219743251800537
Epoch 340, training loss: 872.9773559570312 = 1.0166101455688477 + 100.0 * 8.71960735321045
Epoch 340, val loss: 1.018926739692688
Epoch 350, training loss: 873.0989379882812 = 1.0132695436477661 + 100.0 * 8.720856666564941
Epoch 350, val loss: 1.0157356262207031
Epoch 360, training loss: 872.9304809570312 = 1.0096806287765503 + 100.0 * 8.719207763671875
Epoch 360, val loss: 1.0123502016067505
Epoch 370, training loss: 873.3298950195312 = 1.0060933828353882 + 100.0 * 8.723237991333008
Epoch 370, val loss: 1.0089160203933716
Epoch 380, training loss: 874.1298828125 = 1.0024641752243042 + 100.0 * 8.731274604797363
Epoch 380, val loss: 1.0054818391799927
Epoch 390, training loss: 873.188232421875 = 0.9981870055198669 + 100.0 * 8.721900939941406
Epoch 390, val loss: 1.0013906955718994
Epoch 400, training loss: 873.5175170898438 = 0.9941600561141968 + 100.0 * 8.725234031677246
Epoch 400, val loss: 0.9975346922874451
Epoch 410, training loss: 873.56396484375 = 0.9897282123565674 + 100.0 * 8.72574234008789
Epoch 410, val loss: 0.9933182597160339
Epoch 420, training loss: 874.0490112304688 = 0.985209584236145 + 100.0 * 8.730637550354004
Epoch 420, val loss: 0.9890301823616028
Epoch 430, training loss: 874.437255859375 = 0.9804906845092773 + 100.0 * 8.734567642211914
Epoch 430, val loss: 0.9845736622810364
Epoch 440, training loss: 874.6513671875 = 0.9754879474639893 + 100.0 * 8.736759185791016
Epoch 440, val loss: 0.9797983169555664
Epoch 450, training loss: 874.4995727539062 = 0.9702069163322449 + 100.0 * 8.7352933883667
Epoch 450, val loss: 0.9748121500015259
Epoch 460, training loss: 874.8749389648438 = 0.9647997617721558 + 100.0 * 8.73910140991211
Epoch 460, val loss: 0.9697356820106506
Epoch 470, training loss: 875.1212158203125 = 0.959134042263031 + 100.0 * 8.741621017456055
Epoch 470, val loss: 0.9643633961677551
Epoch 480, training loss: 875.4508666992188 = 0.9532294869422913 + 100.0 * 8.744976043701172
Epoch 480, val loss: 0.9587633609771729
Epoch 490, training loss: 875.813720703125 = 0.9470005035400391 + 100.0 * 8.748666763305664
Epoch 490, val loss: 0.9529666304588318
Epoch 500, training loss: 876.73046875 = 0.9407325387001038 + 100.0 * 8.75789737701416
Epoch 500, val loss: 0.9469934105873108
Epoch 510, training loss: 875.3939819335938 = 0.9335653781890869 + 100.0 * 8.744604110717773
Epoch 510, val loss: 0.9404405355453491
Epoch 520, training loss: 875.7764282226562 = 0.9266163110733032 + 100.0 * 8.74849796295166
Epoch 520, val loss: 0.9338229298591614
Epoch 530, training loss: 876.2077026367188 = 0.9196836948394775 + 100.0 * 8.752880096435547
Epoch 530, val loss: 0.9273794293403625
Epoch 540, training loss: 876.4208374023438 = 0.9124158024787903 + 100.0 * 8.755084037780762
Epoch 540, val loss: 0.9205901622772217
Epoch 550, training loss: 877.02392578125 = 0.9049350023269653 + 100.0 * 8.761190414428711
Epoch 550, val loss: 0.9136329293251038
Epoch 560, training loss: 877.2828369140625 = 0.8971924185752869 + 100.0 * 8.763855934143066
Epoch 560, val loss: 0.9064620137214661
Epoch 570, training loss: 877.941162109375 = 0.8892084360122681 + 100.0 * 8.770519256591797
Epoch 570, val loss: 0.8990406394004822
Epoch 580, training loss: 878.352783203125 = 0.881010115146637 + 100.0 * 8.774718284606934
Epoch 580, val loss: 0.8915017247200012
Epoch 590, training loss: 877.577880859375 = 0.8725069165229797 + 100.0 * 8.767053604125977
Epoch 590, val loss: 0.8836868405342102
Epoch 600, training loss: 878.2882080078125 = 0.864287793636322 + 100.0 * 8.774239540100098
Epoch 600, val loss: 0.8759516477584839
Epoch 610, training loss: 878.8486938476562 = 0.8557685017585754 + 100.0 * 8.779929161071777
Epoch 610, val loss: 0.868207573890686
Epoch 620, training loss: 879.312255859375 = 0.847134530544281 + 100.0 * 8.784651756286621
Epoch 620, val loss: 0.8603528738021851
Epoch 630, training loss: 879.1260986328125 = 0.838158130645752 + 100.0 * 8.782879829406738
Epoch 630, val loss: 0.8521470427513123
Epoch 640, training loss: 879.37744140625 = 0.8292964100837708 + 100.0 * 8.785481452941895
Epoch 640, val loss: 0.8441432118415833
Epoch 650, training loss: 879.9442138671875 = 0.8206214308738708 + 100.0 * 8.79123592376709
Epoch 650, val loss: 0.8361541032791138
Epoch 660, training loss: 880.4411010742188 = 0.8117018938064575 + 100.0 * 8.796294212341309
Epoch 660, val loss: 0.8280503749847412
Epoch 670, training loss: 880.776123046875 = 0.8029876351356506 + 100.0 * 8.799731254577637
Epoch 670, val loss: 0.8201536536216736
Epoch 680, training loss: 880.6949462890625 = 0.7939981818199158 + 100.0 * 8.799009323120117
Epoch 680, val loss: 0.8119993209838867
Epoch 690, training loss: 880.6460571289062 = 0.785082995891571 + 100.0 * 8.798609733581543
Epoch 690, val loss: 0.8038843274116516
Epoch 700, training loss: 880.8499145507812 = 0.7761626243591309 + 100.0 * 8.800737380981445
Epoch 700, val loss: 0.795859694480896
Epoch 710, training loss: 881.4392700195312 = 0.7675933241844177 + 100.0 * 8.806716918945312
Epoch 710, val loss: 0.7881749868392944
Epoch 720, training loss: 881.6487426757812 = 0.7588725090026855 + 100.0 * 8.80889892578125
Epoch 720, val loss: 0.7803188562393188
Epoch 730, training loss: 882.199951171875 = 0.7503848075866699 + 100.0 * 8.814496040344238
Epoch 730, val loss: 0.7726768851280212
Epoch 740, training loss: 881.16015625 = 0.7414577007293701 + 100.0 * 8.804186820983887
Epoch 740, val loss: 0.7647520899772644
Epoch 750, training loss: 882.925048828125 = 0.7333957552909851 + 100.0 * 8.821916580200195
Epoch 750, val loss: 0.7574557662010193
Epoch 760, training loss: 880.1189575195312 = 0.7244448661804199 + 100.0 * 8.7939453125
Epoch 760, val loss: 0.7494860887527466
Epoch 770, training loss: 883.0895385742188 = 0.716960608959198 + 100.0 * 8.823725700378418
Epoch 770, val loss: 0.742548942565918
Epoch 780, training loss: 880.914306640625 = 0.7080286741256714 + 100.0 * 8.80206298828125
Epoch 780, val loss: 0.734734296798706
Epoch 790, training loss: 881.240966796875 = 0.6999582648277283 + 100.0 * 8.805410385131836
Epoch 790, val loss: 0.7276418805122375
Epoch 800, training loss: 881.4457397460938 = 0.6921287178993225 + 100.0 * 8.807536125183105
Epoch 800, val loss: 0.7205845713615417
Epoch 810, training loss: 881.7128295898438 = 0.6840788125991821 + 100.0 * 8.810287475585938
Epoch 810, val loss: 0.7136490941047668
Epoch 820, training loss: 881.7808837890625 = 0.6767066121101379 + 100.0 * 8.811041831970215
Epoch 820, val loss: 0.7071090340614319
Epoch 830, training loss: 882.2718505859375 = 0.6695054173469543 + 100.0 * 8.816023826599121
Epoch 830, val loss: 0.700700581073761
Epoch 840, training loss: 882.86865234375 = 0.6624997854232788 + 100.0 * 8.822061538696289
Epoch 840, val loss: 0.694519579410553
Epoch 850, training loss: 883.3068237304688 = 0.6554128527641296 + 100.0 * 8.82651424407959
Epoch 850, val loss: 0.6883036494255066
Epoch 860, training loss: 884.1012573242188 = 0.6485544443130493 + 100.0 * 8.834527015686035
Epoch 860, val loss: 0.6822193264961243
Epoch 870, training loss: 884.6348266601562 = 0.6417818665504456 + 100.0 * 8.839930534362793
Epoch 870, val loss: 0.6762550473213196
Epoch 880, training loss: 884.943603515625 = 0.634987473487854 + 100.0 * 8.843086242675781
Epoch 880, val loss: 0.6702826619148254
Epoch 890, training loss: 883.4369506835938 = 0.6279826164245605 + 100.0 * 8.828089714050293
Epoch 890, val loss: 0.6639536619186401
Epoch 900, training loss: 885.0205078125 = 0.6216571927070618 + 100.0 * 8.843988418579102
Epoch 900, val loss: 0.6586372256278992
Epoch 910, training loss: 885.4544677734375 = 0.6154590845108032 + 100.0 * 8.848389625549316
Epoch 910, val loss: 0.6531854271888733
Epoch 920, training loss: 885.791015625 = 0.6093450784683228 + 100.0 * 8.851816177368164
Epoch 920, val loss: 0.6477891802787781
Epoch 930, training loss: 886.1878051757812 = 0.6034294366836548 + 100.0 * 8.855843544006348
Epoch 930, val loss: 0.6426632404327393
Epoch 940, training loss: 886.1087036132812 = 0.597476065158844 + 100.0 * 8.855112075805664
Epoch 940, val loss: 0.6374872922897339
Epoch 950, training loss: 886.4661865234375 = 0.5918127298355103 + 100.0 * 8.858743667602539
Epoch 950, val loss: 0.6326740980148315
Epoch 960, training loss: 886.5310668945312 = 0.5862658023834229 + 100.0 * 8.859448432922363
Epoch 960, val loss: 0.6278891563415527
Epoch 970, training loss: 887.2062377929688 = 0.5809257626533508 + 100.0 * 8.866252899169922
Epoch 970, val loss: 0.6232472658157349
Epoch 980, training loss: 887.2817993164062 = 0.5756082534790039 + 100.0 * 8.867061614990234
Epoch 980, val loss: 0.61868816614151
Epoch 990, training loss: 887.585205078125 = 0.5705747604370117 + 100.0 * 8.870146751403809
Epoch 990, val loss: 0.6143603324890137
Epoch 1000, training loss: 888.044921875 = 0.5656373500823975 + 100.0 * 8.87479305267334
Epoch 1000, val loss: 0.6101049184799194
Epoch 1010, training loss: 888.1571655273438 = 0.5611621737480164 + 100.0 * 8.875960350036621
Epoch 1010, val loss: 0.6062154769897461
Epoch 1020, training loss: 887.0304565429688 = 0.556194543838501 + 100.0 * 8.864742279052734
Epoch 1020, val loss: 0.6020188331604004
Epoch 1030, training loss: 886.5223388671875 = 0.5512524247169495 + 100.0 * 8.859710693359375
Epoch 1030, val loss: 0.5980850458145142
Epoch 1040, training loss: 887.5653686523438 = 0.5472423434257507 + 100.0 * 8.8701810836792
Epoch 1040, val loss: 0.5944200158119202
Epoch 1050, training loss: 887.9395141601562 = 0.5429931879043579 + 100.0 * 8.8739652633667
Epoch 1050, val loss: 0.5909073352813721
Epoch 1060, training loss: 888.7453002929688 = 0.5389862656593323 + 100.0 * 8.882062911987305
Epoch 1060, val loss: 0.5875312089920044
Epoch 1070, training loss: 889.1708984375 = 0.5350832939147949 + 100.0 * 8.886358261108398
Epoch 1070, val loss: 0.5842432975769043
Epoch 1080, training loss: 889.3798217773438 = 0.5312382578849792 + 100.0 * 8.8884859085083
Epoch 1080, val loss: 0.5810155272483826
Epoch 1090, training loss: 889.6310424804688 = 0.5274916887283325 + 100.0 * 8.891036033630371
Epoch 1090, val loss: 0.5778833627700806
Epoch 1100, training loss: 890.1387329101562 = 0.5238465070724487 + 100.0 * 8.896148681640625
Epoch 1100, val loss: 0.5748578310012817
Epoch 1110, training loss: 889.9149169921875 = 0.520167350769043 + 100.0 * 8.89394760131836
Epoch 1110, val loss: 0.5718059539794922
Epoch 1120, training loss: 890.3071899414062 = 0.5166676640510559 + 100.0 * 8.897905349731445
Epoch 1120, val loss: 0.5692313313484192
Epoch 1130, training loss: 888.98681640625 = 0.5132013559341431 + 100.0 * 8.884736061096191
Epoch 1130, val loss: 0.5659182667732239
Epoch 1140, training loss: 886.721435546875 = 0.5098501443862915 + 100.0 * 8.862115859985352
Epoch 1140, val loss: 0.5633767247200012
Epoch 1150, training loss: 887.84814453125 = 0.5069628953933716 + 100.0 * 8.873412132263184
Epoch 1150, val loss: 0.560971736907959
Epoch 1160, training loss: 888.3539428710938 = 0.5039557814598083 + 100.0 * 8.878499984741211
Epoch 1160, val loss: 0.5586397051811218
Epoch 1170, training loss: 889.0928344726562 = 0.5011298656463623 + 100.0 * 8.885916709899902
Epoch 1170, val loss: 0.5563271045684814
Epoch 1180, training loss: 889.8373413085938 = 0.4983435571193695 + 100.0 * 8.893389701843262
Epoch 1180, val loss: 0.5541241765022278
Epoch 1190, training loss: 890.4729614257812 = 0.49567654728889465 + 100.0 * 8.899772644042969
Epoch 1190, val loss: 0.5520843863487244
Epoch 1200, training loss: 891.0053100585938 = 0.4930517077445984 + 100.0 * 8.905122756958008
Epoch 1200, val loss: 0.550028383731842
Epoch 1210, training loss: 891.3374633789062 = 0.49043354392051697 + 100.0 * 8.908470153808594
Epoch 1210, val loss: 0.5479649901390076
Epoch 1220, training loss: 891.2294921875 = 0.48781663179397583 + 100.0 * 8.907417297363281
Epoch 1220, val loss: 0.5458119511604309
Epoch 1230, training loss: 890.8787841796875 = 0.4853442311286926 + 100.0 * 8.903934478759766
Epoch 1230, val loss: 0.5439711213111877
Epoch 1240, training loss: 891.2822875976562 = 0.48304250836372375 + 100.0 * 8.907992362976074
Epoch 1240, val loss: 0.5423393845558167
Epoch 1250, training loss: 892.0572509765625 = 0.4807732105255127 + 100.0 * 8.915764808654785
Epoch 1250, val loss: 0.5406948328018188
Epoch 1260, training loss: 892.0410766601562 = 0.4784803092479706 + 100.0 * 8.915626525878906
Epoch 1260, val loss: 0.5390024781227112
Epoch 1270, training loss: 892.2337646484375 = 0.4762851595878601 + 100.0 * 8.917574882507324
Epoch 1270, val loss: 0.537364661693573
Epoch 1280, training loss: 892.797607421875 = 0.4741572439670563 + 100.0 * 8.923233985900879
Epoch 1280, val loss: 0.5358121395111084
Epoch 1290, training loss: 892.9168090820312 = 0.4720138609409332 + 100.0 * 8.924448013305664
Epoch 1290, val loss: 0.5342496037483215
Epoch 1300, training loss: 892.171142578125 = 0.46993517875671387 + 100.0 * 8.917012214660645
Epoch 1300, val loss: 0.5326482057571411
Epoch 1310, training loss: 890.8074340820312 = 0.46738532185554504 + 100.0 * 8.903400421142578
Epoch 1310, val loss: 0.5307433009147644
Epoch 1320, training loss: 890.8512573242188 = 0.46535563468933105 + 100.0 * 8.90385913848877
Epoch 1320, val loss: 0.5294491648674011
Epoch 1330, training loss: 891.1444091796875 = 0.46364519000053406 + 100.0 * 8.906807899475098
Epoch 1330, val loss: 0.5282754302024841
Epoch 1340, training loss: 892.1036376953125 = 0.4620423913002014 + 100.0 * 8.91641616821289
Epoch 1340, val loss: 0.5271280407905579
Epoch 1350, training loss: 893.3541259765625 = 0.4603525996208191 + 100.0 * 8.928937911987305
Epoch 1350, val loss: 0.5260209441184998
Epoch 1360, training loss: 893.9783935546875 = 0.45865553617477417 + 100.0 * 8.935196876525879
Epoch 1360, val loss: 0.5249034762382507
Epoch 1370, training loss: 894.1299438476562 = 0.45694300532341003 + 100.0 * 8.936729431152344
Epoch 1370, val loss: 0.5237182378768921
Epoch 1380, training loss: 894.1174926757812 = 0.45525702834129333 + 100.0 * 8.936622619628906
Epoch 1380, val loss: 0.5226417779922485
Epoch 1390, training loss: 894.7850952148438 = 0.4536226987838745 + 100.0 * 8.943314552307129
Epoch 1390, val loss: 0.5215534567832947
Epoch 1400, training loss: 894.98828125 = 0.4519954323768616 + 100.0 * 8.94536304473877
Epoch 1400, val loss: 0.5204703211784363
Epoch 1410, training loss: 894.8086547851562 = 0.4503397047519684 + 100.0 * 8.943583488464355
Epoch 1410, val loss: 0.5194039344787598
Epoch 1420, training loss: 895.1019287109375 = 0.4487802982330322 + 100.0 * 8.946531295776367
Epoch 1420, val loss: 0.5183990597724915
Epoch 1430, training loss: 895.3745727539062 = 0.44724413752555847 + 100.0 * 8.949273109436035
Epoch 1430, val loss: 0.5173932313919067
Epoch 1440, training loss: 895.7655029296875 = 0.44575509428977966 + 100.0 * 8.953197479248047
Epoch 1440, val loss: 0.5164473056793213
Epoch 1450, training loss: 895.708251953125 = 0.44425487518310547 + 100.0 * 8.95263957977295
Epoch 1450, val loss: 0.5154537558555603
Epoch 1460, training loss: 895.4251708984375 = 0.44275638461112976 + 100.0 * 8.949824333190918
Epoch 1460, val loss: 0.5145529508590698
Epoch 1470, training loss: 895.6229858398438 = 0.4413318634033203 + 100.0 * 8.95181655883789
Epoch 1470, val loss: 0.5136831998825073
Epoch 1480, training loss: 896.0844116210938 = 0.43994858860969543 + 100.0 * 8.95644474029541
Epoch 1480, val loss: 0.5127511620521545
Epoch 1490, training loss: 896.6176147460938 = 0.43855491280555725 + 100.0 * 8.961791038513184
Epoch 1490, val loss: 0.5119295120239258
Epoch 1500, training loss: 896.4829711914062 = 0.43720516562461853 + 100.0 * 8.960457801818848
Epoch 1500, val loss: 0.5110892653465271
Epoch 1510, training loss: 896.90576171875 = 0.4359060525894165 + 100.0 * 8.964698791503906
Epoch 1510, val loss: 0.5103105902671814
Epoch 1520, training loss: 896.7932739257812 = 0.43455880880355835 + 100.0 * 8.963586807250977
Epoch 1520, val loss: 0.509530246257782
Epoch 1530, training loss: 896.2507934570312 = 0.43320217728614807 + 100.0 * 8.958175659179688
Epoch 1530, val loss: 0.508706271648407
Epoch 1540, training loss: 896.5734252929688 = 0.4320513904094696 + 100.0 * 8.961413383483887
Epoch 1540, val loss: 0.5079177021980286
Epoch 1550, training loss: 896.5421752929688 = 0.4308428466320038 + 100.0 * 8.961112976074219
Epoch 1550, val loss: 0.5073459148406982
Epoch 1560, training loss: 895.316162109375 = 0.4295305013656616 + 100.0 * 8.948866844177246
Epoch 1560, val loss: 0.5064412355422974
Epoch 1570, training loss: 896.4674072265625 = 0.4284120798110962 + 100.0 * 8.960390090942383
Epoch 1570, val loss: 0.5057541131973267
Epoch 1580, training loss: 896.9754638671875 = 0.4272048771381378 + 100.0 * 8.965482711791992
Epoch 1580, val loss: 0.5051140785217285
Epoch 1590, training loss: 897.49951171875 = 0.4260166883468628 + 100.0 * 8.970734596252441
Epoch 1590, val loss: 0.5043919682502747
Epoch 1600, training loss: 897.0502319335938 = 0.4248289465904236 + 100.0 * 8.966254234313965
Epoch 1600, val loss: 0.5036722421646118
Epoch 1610, training loss: 897.6489868164062 = 0.42369285225868225 + 100.0 * 8.97225284576416
Epoch 1610, val loss: 0.5030537843704224
Epoch 1620, training loss: 897.9755859375 = 0.422563761472702 + 100.0 * 8.975530624389648
Epoch 1620, val loss: 0.5024173259735107
Epoch 1630, training loss: 898.059814453125 = 0.42150139808654785 + 100.0 * 8.976383209228516
Epoch 1630, val loss: 0.5017887949943542
Epoch 1640, training loss: 898.3726806640625 = 0.42042049765586853 + 100.0 * 8.979522705078125
Epoch 1640, val loss: 0.5011655688285828
Epoch 1650, training loss: 898.3324584960938 = 0.41932421922683716 + 100.0 * 8.979131698608398
Epoch 1650, val loss: 0.5006234049797058
Epoch 1660, training loss: 898.5386962890625 = 0.418249249458313 + 100.0 * 8.98120403289795
Epoch 1660, val loss: 0.4998893141746521
Epoch 1670, training loss: 899.0260009765625 = 0.4172196686267853 + 100.0 * 8.986087799072266
Epoch 1670, val loss: 0.4992215037345886
Epoch 1680, training loss: 896.2499389648438 = 0.41596293449401855 + 100.0 * 8.95833969116211
Epoch 1680, val loss: 0.49848082661628723
Epoch 1690, training loss: 897.183349609375 = 0.4149866998195648 + 100.0 * 8.967683792114258
Epoch 1690, val loss: 0.49798986315727234
Epoch 1700, training loss: 895.6689453125 = 0.4140816032886505 + 100.0 * 8.95254898071289
Epoch 1700, val loss: 0.49766406416893005
Epoch 1710, training loss: 895.7723388671875 = 0.41310790181159973 + 100.0 * 8.953592300415039
Epoch 1710, val loss: 0.49727994203567505
Epoch 1720, training loss: 897.2703857421875 = 0.4121532440185547 + 100.0 * 8.968582153320312
Epoch 1720, val loss: 0.4965701997280121
Epoch 1730, training loss: 897.9909057617188 = 0.41113603115081787 + 100.0 * 8.975797653198242
Epoch 1730, val loss: 0.4959181249141693
Epoch 1740, training loss: 898.3836059570312 = 0.4101249873638153 + 100.0 * 8.979735374450684
Epoch 1740, val loss: 0.49531805515289307
Epoch 1750, training loss: 898.7667846679688 = 0.4091395139694214 + 100.0 * 8.983576774597168
Epoch 1750, val loss: 0.4947727918624878
Epoch 1760, training loss: 899.3169555664062 = 0.40816211700439453 + 100.0 * 8.98908805847168
Epoch 1760, val loss: 0.49417051672935486
Epoch 1770, training loss: 899.4370727539062 = 0.40718555450439453 + 100.0 * 8.990299224853516
Epoch 1770, val loss: 0.493579626083374
Epoch 1780, training loss: 899.6829223632812 = 0.40619590878486633 + 100.0 * 8.992767333984375
Epoch 1780, val loss: 0.492986261844635
Epoch 1790, training loss: 899.4065551757812 = 0.4052172303199768 + 100.0 * 8.990013122558594
Epoch 1790, val loss: 0.4923149347305298
Epoch 1800, training loss: 899.2781372070312 = 0.40422675013542175 + 100.0 * 8.988739013671875
Epoch 1800, val loss: 0.4919162690639496
Epoch 1810, training loss: 899.4450073242188 = 0.4032827615737915 + 100.0 * 8.99041748046875
Epoch 1810, val loss: 0.4913281500339508
Epoch 1820, training loss: 899.866943359375 = 0.40235430002212524 + 100.0 * 8.994646072387695
Epoch 1820, val loss: 0.4907810389995575
Epoch 1830, training loss: 900.0176391601562 = 0.401411771774292 + 100.0 * 8.996162414550781
Epoch 1830, val loss: 0.4903140962123871
Epoch 1840, training loss: 900.1307373046875 = 0.40047967433929443 + 100.0 * 8.997303009033203
Epoch 1840, val loss: 0.4897274971008301
Epoch 1850, training loss: 900.3549194335938 = 0.39956119656562805 + 100.0 * 8.999553680419922
Epoch 1850, val loss: 0.48921748995780945
Epoch 1860, training loss: 900.85888671875 = 0.39862677454948425 + 100.0 * 9.004602432250977
Epoch 1860, val loss: 0.48862582445144653
Epoch 1870, training loss: 900.4424438476562 = 0.3976936340332031 + 100.0 * 9.000447273254395
Epoch 1870, val loss: 0.4880231022834778
Epoch 1880, training loss: 900.1565551757812 = 0.39676034450531006 + 100.0 * 8.997597694396973
Epoch 1880, val loss: 0.4876254200935364
Epoch 1890, training loss: 900.4812622070312 = 0.39584285020828247 + 100.0 * 9.0008544921875
Epoch 1890, val loss: 0.4870828688144684
Epoch 1900, training loss: 901.0137939453125 = 0.3949660360813141 + 100.0 * 9.00618839263916
Epoch 1900, val loss: 0.48652413487434387
Epoch 1910, training loss: 900.9711303710938 = 0.39404287934303284 + 100.0 * 9.005770683288574
Epoch 1910, val loss: 0.48602330684661865
Epoch 1920, training loss: 901.0589599609375 = 0.3931523859500885 + 100.0 * 9.006658554077148
Epoch 1920, val loss: 0.485577791929245
Epoch 1930, training loss: 901.56787109375 = 0.39226651191711426 + 100.0 * 9.01175594329834
Epoch 1930, val loss: 0.48507148027420044
Epoch 1940, training loss: 901.9501342773438 = 0.39138203859329224 + 100.0 * 9.01558780670166
Epoch 1940, val loss: 0.4845857620239258
Epoch 1950, training loss: 901.8760375976562 = 0.39048534631729126 + 100.0 * 9.01485538482666
Epoch 1950, val loss: 0.48411253094673157
Epoch 1960, training loss: 901.9052734375 = 0.3896007537841797 + 100.0 * 9.015156745910645
Epoch 1960, val loss: 0.48360416293144226
Epoch 1970, training loss: 902.001220703125 = 0.38872185349464417 + 100.0 * 9.016124725341797
Epoch 1970, val loss: 0.4831337332725525
Epoch 1980, training loss: 902.1884155273438 = 0.38784855604171753 + 100.0 * 9.01800537109375
Epoch 1980, val loss: 0.48258671164512634
Epoch 1990, training loss: 902.0773315429688 = 0.3870489001274109 + 100.0 * 9.016902923583984
Epoch 1990, val loss: 0.48254215717315674
Epoch 2000, training loss: 902.2394409179688 = 0.386157363653183 + 100.0 * 9.018532752990723
Epoch 2000, val loss: 0.48151662945747375
Epoch 2010, training loss: 902.4290771484375 = 0.3852684199810028 + 100.0 * 9.020438194274902
Epoch 2010, val loss: 0.48126164078712463
Epoch 2020, training loss: 903.0075073242188 = 0.3844262957572937 + 100.0 * 9.026230812072754
Epoch 2020, val loss: 0.4807399809360504
Epoch 2030, training loss: 902.6423950195312 = 0.38355743885040283 + 100.0 * 9.022588729858398
Epoch 2030, val loss: 0.4802681505680084
Epoch 2040, training loss: 901.332275390625 = 0.38267940282821655 + 100.0 * 9.009495735168457
Epoch 2040, val loss: 0.47966909408569336
Epoch 2050, training loss: 901.5159301757812 = 0.38186076283454895 + 100.0 * 9.011341094970703
Epoch 2050, val loss: 0.4793384373188019
Epoch 2060, training loss: 902.01025390625 = 0.38109374046325684 + 100.0 * 9.016291618347168
Epoch 2060, val loss: 0.4789890944957733
Epoch 2070, training loss: 902.6818237304688 = 0.38029372692108154 + 100.0 * 9.023015022277832
Epoch 2070, val loss: 0.4786258935928345
Epoch 2080, training loss: 903.3297119140625 = 0.37950292229652405 + 100.0 * 9.029501914978027
Epoch 2080, val loss: 0.478145956993103
Epoch 2090, training loss: 903.5635375976562 = 0.37866973876953125 + 100.0 * 9.031848907470703
Epoch 2090, val loss: 0.4777205288410187
Epoch 2100, training loss: 903.5418090820312 = 0.3778248429298401 + 100.0 * 9.03164005279541
Epoch 2100, val loss: 0.47726091742515564
Epoch 2110, training loss: 903.6651000976562 = 0.376980721950531 + 100.0 * 9.032881736755371
Epoch 2110, val loss: 0.47684139013290405
Epoch 2120, training loss: 903.63525390625 = 0.3761559724807739 + 100.0 * 9.032590866088867
Epoch 2120, val loss: 0.47639113664627075
Epoch 2130, training loss: 903.60400390625 = 0.3753439784049988 + 100.0 * 9.032286643981934
Epoch 2130, val loss: 0.4759943187236786
Epoch 2140, training loss: 903.2774658203125 = 0.3745150864124298 + 100.0 * 9.029029846191406
Epoch 2140, val loss: 0.4755944013595581
Epoch 2150, training loss: 903.4501953125 = 0.37365075945854187 + 100.0 * 9.030765533447266
Epoch 2150, val loss: 0.47515302896499634
Epoch 2160, training loss: 903.7882690429688 = 0.372841477394104 + 100.0 * 9.034153938293457
Epoch 2160, val loss: 0.4746622145175934
Epoch 2170, training loss: 904.3488159179688 = 0.372067928314209 + 100.0 * 9.039767265319824
Epoch 2170, val loss: 0.47437092661857605
Epoch 2180, training loss: 904.5296020507812 = 0.3712696135044098 + 100.0 * 9.041583061218262
Epoch 2180, val loss: 0.4739285111427307
Epoch 2190, training loss: 904.4599609375 = 0.37045884132385254 + 100.0 * 9.040894508361816
Epoch 2190, val loss: 0.4735652804374695
Epoch 2200, training loss: 904.6433715820312 = 0.36966413259506226 + 100.0 * 9.042737007141113
Epoch 2200, val loss: 0.47319239377975464
Epoch 2210, training loss: 902.908203125 = 0.36914315819740295 + 100.0 * 9.025390625
Epoch 2210, val loss: 0.4728347361087799
Epoch 2220, training loss: 902.7089233398438 = 0.36842080950737 + 100.0 * 9.023405075073242
Epoch 2220, val loss: 0.4728025794029236
Epoch 2230, training loss: 903.2899780273438 = 0.3676324486732483 + 100.0 * 9.029223442077637
Epoch 2230, val loss: 0.4722340404987335
Epoch 2240, training loss: 903.9464721679688 = 0.36686304211616516 + 100.0 * 9.035796165466309
Epoch 2240, val loss: 0.47169333696365356
Epoch 2250, training loss: 904.8162231445312 = 0.366062730550766 + 100.0 * 9.044501304626465
Epoch 2250, val loss: 0.47141414880752563
Epoch 2260, training loss: 905.3297119140625 = 0.3652377128601074 + 100.0 * 9.049644470214844
Epoch 2260, val loss: 0.47094815969467163
Epoch 2270, training loss: 905.7254638671875 = 0.3644045293331146 + 100.0 * 9.053610801696777
Epoch 2270, val loss: 0.4705841839313507
Epoch 2280, training loss: 905.6810302734375 = 0.36355191469192505 + 100.0 * 9.05317497253418
Epoch 2280, val loss: 0.4702131450176239
Epoch 2290, training loss: 905.6458129882812 = 0.3627060055732727 + 100.0 * 9.052830696105957
Epoch 2290, val loss: 0.4698447287082672
Epoch 2300, training loss: 905.758056640625 = 0.36187997460365295 + 100.0 * 9.053961753845215
Epoch 2300, val loss: 0.4694887101650238
Epoch 2310, training loss: 906.3036499023438 = 0.361043244600296 + 100.0 * 9.059426307678223
Epoch 2310, val loss: 0.4692043364048004
Epoch 2320, training loss: 906.35009765625 = 0.3602108061313629 + 100.0 * 9.059898376464844
Epoch 2320, val loss: 0.4685116410255432
Epoch 2330, training loss: 905.8870239257812 = 0.3594386577606201 + 100.0 * 9.055275917053223
Epoch 2330, val loss: 0.4685240089893341
Epoch 2340, training loss: 905.59423828125 = 0.3585963845252991 + 100.0 * 9.052356719970703
Epoch 2340, val loss: 0.46830037236213684
Epoch 2350, training loss: 906.0391235351562 = 0.35776975750923157 + 100.0 * 9.05681324005127
Epoch 2350, val loss: 0.4677954912185669
Epoch 2360, training loss: 906.3975830078125 = 0.3569334149360657 + 100.0 * 9.060406684875488
Epoch 2360, val loss: 0.4674643874168396
Epoch 2370, training loss: 906.8904418945312 = 0.3560917377471924 + 100.0 * 9.065343856811523
Epoch 2370, val loss: 0.4671061933040619
Epoch 2380, training loss: 906.8272705078125 = 0.35523438453674316 + 100.0 * 9.064720153808594
Epoch 2380, val loss: 0.46675726771354675
Epoch 2390, training loss: 906.9905395507812 = 0.3543834388256073 + 100.0 * 9.066361427307129
Epoch 2390, val loss: 0.46635133028030396
Epoch 2400, training loss: 906.5848388671875 = 0.35352256894111633 + 100.0 * 9.062313079833984
Epoch 2400, val loss: 0.46604064106941223
Epoch 2410, training loss: 906.7963256835938 = 0.3526744246482849 + 100.0 * 9.064436912536621
Epoch 2410, val loss: 0.46567773818969727
Epoch 2420, training loss: 907.288818359375 = 0.35181745886802673 + 100.0 * 9.06937026977539
Epoch 2420, val loss: 0.4653186500072479
Epoch 2430, training loss: 907.220947265625 = 0.3509728014469147 + 100.0 * 9.068699836730957
Epoch 2430, val loss: 0.4649847745895386
Epoch 2440, training loss: 907.17041015625 = 0.35011735558509827 + 100.0 * 9.06820297241211
Epoch 2440, val loss: 0.4646020531654358
Epoch 2450, training loss: 907.4487915039062 = 0.349267840385437 + 100.0 * 9.070995330810547
Epoch 2450, val loss: 0.4642668664455414
Epoch 2460, training loss: 907.720947265625 = 0.348418653011322 + 100.0 * 9.073725700378418
Epoch 2460, val loss: 0.46388810873031616
Epoch 2470, training loss: 907.684326171875 = 0.3475637435913086 + 100.0 * 9.073368072509766
Epoch 2470, val loss: 0.46349212527275085
Epoch 2480, training loss: 907.3662719726562 = 0.34670746326446533 + 100.0 * 9.070196151733398
Epoch 2480, val loss: 0.46323275566101074
Epoch 2490, training loss: 907.5492553710938 = 0.3458589017391205 + 100.0 * 9.072033882141113
Epoch 2490, val loss: 0.4627842903137207
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8223188405797102
0.8661885097442585
=== training gcn model ===
Epoch 0, training loss: 1026.6484375 = 1.1050224304199219 + 100.0 * 10.2554349899292
Epoch 0, val loss: 1.1027017831802368
Epoch 10, training loss: 983.3781127929688 = 1.1010655164718628 + 100.0 * 9.822770118713379
Epoch 10, val loss: 1.0987995862960815
Epoch 20, training loss: 958.4210205078125 = 1.097456932067871 + 100.0 * 9.573235511779785
Epoch 20, val loss: 1.0952577590942383
Epoch 30, training loss: 941.1805419921875 = 1.0940405130386353 + 100.0 * 9.400864601135254
Epoch 30, val loss: 1.091907024383545
Epoch 40, training loss: 928.6837768554688 = 1.090814471244812 + 100.0 * 9.27592945098877
Epoch 40, val loss: 1.088760495185852
Epoch 50, training loss: 919.273193359375 = 1.087769627571106 + 100.0 * 9.181854248046875
Epoch 50, val loss: 1.0858055353164673
Epoch 60, training loss: 912.0064086914062 = 1.084860920906067 + 100.0 * 9.10921573638916
Epoch 60, val loss: 1.0829992294311523
Epoch 70, training loss: 906.0584716796875 = 1.0820701122283936 + 100.0 * 9.049763679504395
Epoch 70, val loss: 1.0803250074386597
Epoch 80, training loss: 901.0198974609375 = 1.0793911218643188 + 100.0 * 8.999404907226562
Epoch 80, val loss: 1.0777676105499268
Epoch 90, training loss: 896.72705078125 = 1.076802372932434 + 100.0 * 8.956502914428711
Epoch 90, val loss: 1.0753085613250732
Epoch 100, training loss: 892.98876953125 = 1.0743046998977661 + 100.0 * 8.919144630432129
Epoch 100, val loss: 1.0729458332061768
Epoch 110, training loss: 889.814697265625 = 1.071876049041748 + 100.0 * 8.887428283691406
Epoch 110, val loss: 1.0706530809402466
Epoch 120, training loss: 887.1115112304688 = 1.0695116519927979 + 100.0 * 8.860420227050781
Epoch 120, val loss: 1.0684157609939575
Epoch 130, training loss: 884.7233276367188 = 1.0672169923782349 + 100.0 * 8.83656120300293
Epoch 130, val loss: 1.0662550926208496
Epoch 140, training loss: 882.5775146484375 = 1.0649574995040894 + 100.0 * 8.815125465393066
Epoch 140, val loss: 1.0641275644302368
Epoch 150, training loss: 880.7537231445312 = 1.0627830028533936 + 100.0 * 8.79690933227539
Epoch 150, val loss: 1.0620781183242798
Epoch 160, training loss: 879.1057739257812 = 1.060610055923462 + 100.0 * 8.780451774597168
Epoch 160, val loss: 1.0600277185440063
Epoch 170, training loss: 877.8095092773438 = 1.0584834814071655 + 100.0 * 8.767510414123535
Epoch 170, val loss: 1.0580167770385742
Epoch 180, training loss: 876.53173828125 = 1.056380271911621 + 100.0 * 8.754753112792969
Epoch 180, val loss: 1.0560479164123535
Epoch 190, training loss: 875.318115234375 = 1.054261565208435 + 100.0 * 8.74263858795166
Epoch 190, val loss: 1.054052710533142
Epoch 200, training loss: 874.5433959960938 = 1.052168369293213 + 100.0 * 8.734911918640137
Epoch 200, val loss: 1.0520713329315186
Epoch 210, training loss: 873.8361206054688 = 1.0499985218048096 + 100.0 * 8.727861404418945
Epoch 210, val loss: 1.0500454902648926
Epoch 220, training loss: 873.0084228515625 = 1.0478179454803467 + 100.0 * 8.719606399536133
Epoch 220, val loss: 1.0479624271392822
Epoch 230, training loss: 872.6111450195312 = 1.0455822944641113 + 100.0 * 8.715655326843262
Epoch 230, val loss: 1.045853853225708
Epoch 240, training loss: 872.1580810546875 = 1.0432685613632202 + 100.0 * 8.711148262023926
Epoch 240, val loss: 1.0436499118804932
Epoch 250, training loss: 871.4358520507812 = 1.040780782699585 + 100.0 * 8.703950881958008
Epoch 250, val loss: 1.0412554740905762
Epoch 260, training loss: 871.4266357421875 = 1.0383000373840332 + 100.0 * 8.703883171081543
Epoch 260, val loss: 1.038875699043274
Epoch 270, training loss: 871.0047607421875 = 1.0356075763702393 + 100.0 * 8.699691772460938
Epoch 270, val loss: 1.0363337993621826
Epoch 280, training loss: 870.2981567382812 = 1.0326095819473267 + 100.0 * 8.692655563354492
Epoch 280, val loss: 1.0334473848342896
Epoch 290, training loss: 870.4327392578125 = 1.0295706987380981 + 100.0 * 8.694031715393066
Epoch 290, val loss: 1.0305153131484985
Epoch 300, training loss: 870.0177001953125 = 1.0263190269470215 + 100.0 * 8.689913749694824
Epoch 300, val loss: 1.0274181365966797
Epoch 310, training loss: 869.96923828125 = 1.0229285955429077 + 100.0 * 8.689462661743164
Epoch 310, val loss: 1.0240944623947144
Epoch 320, training loss: 869.9328002929688 = 1.0193235874176025 + 100.0 * 8.68913459777832
Epoch 320, val loss: 1.020650863647461
Epoch 330, training loss: 869.5035400390625 = 1.015334963798523 + 100.0 * 8.684882164001465
Epoch 330, val loss: 1.0167618989944458
Epoch 340, training loss: 869.896728515625 = 1.0112563371658325 + 100.0 * 8.688855171203613
Epoch 340, val loss: 1.0128200054168701
Epoch 350, training loss: 869.522705078125 = 1.00680673122406 + 100.0 * 8.685158729553223
Epoch 350, val loss: 1.0085619688034058
Epoch 360, training loss: 869.65771484375 = 1.002123475074768 + 100.0 * 8.686555862426758
Epoch 360, val loss: 1.0040273666381836
Epoch 370, training loss: 869.6774291992188 = 0.9971602559089661 + 100.0 * 8.686802864074707
Epoch 370, val loss: 0.9992455244064331
Epoch 380, training loss: 869.5050659179688 = 0.9919700026512146 + 100.0 * 8.685131072998047
Epoch 380, val loss: 0.9943354725837708
Epoch 390, training loss: 869.7440185546875 = 0.98652583360672 + 100.0 * 8.687575340270996
Epoch 390, val loss: 0.9890850782394409
Epoch 400, training loss: 869.6878662109375 = 0.9807052612304688 + 100.0 * 8.687071800231934
Epoch 400, val loss: 0.9835054874420166
Epoch 410, training loss: 869.4779052734375 = 0.9746096730232239 + 100.0 * 8.685032844543457
Epoch 410, val loss: 0.9777583479881287
Epoch 420, training loss: 869.6397705078125 = 0.9683700203895569 + 100.0 * 8.686714172363281
Epoch 420, val loss: 0.9717557430267334
Epoch 430, training loss: 869.56689453125 = 0.9618716239929199 + 100.0 * 8.686050415039062
Epoch 430, val loss: 0.9655324220657349
Epoch 440, training loss: 869.9881591796875 = 0.9550576210021973 + 100.0 * 8.690330505371094
Epoch 440, val loss: 0.9590969681739807
Epoch 450, training loss: 869.9269409179688 = 0.9478693604469299 + 100.0 * 8.689790725708008
Epoch 450, val loss: 0.9521870613098145
Epoch 460, training loss: 869.4779052734375 = 0.9391836524009705 + 100.0 * 8.685386657714844
Epoch 460, val loss: 0.9441449642181396
Epoch 470, training loss: 872.73681640625 = 0.9335419535636902 + 100.0 * 8.718032836914062
Epoch 470, val loss: 0.9385553002357483
Epoch 480, training loss: 868.8728637695312 = 0.9242474436759949 + 100.0 * 8.679486274719238
Epoch 480, val loss: 0.9298561811447144
Epoch 490, training loss: 870.3118286132812 = 0.9175297617912292 + 100.0 * 8.69394302368164
Epoch 490, val loss: 0.9236449599266052
Epoch 500, training loss: 869.1220703125 = 0.9090020060539246 + 100.0 * 8.682130813598633
Epoch 500, val loss: 0.9155704975128174
Epoch 510, training loss: 869.9848022460938 = 0.9009360671043396 + 100.0 * 8.690838813781738
Epoch 510, val loss: 0.9080623984336853
Epoch 520, training loss: 869.9419555664062 = 0.8926724791526794 + 100.0 * 8.690492630004883
Epoch 520, val loss: 0.900370717048645
Epoch 530, training loss: 870.5341186523438 = 0.8842236399650574 + 100.0 * 8.69649887084961
Epoch 530, val loss: 0.892438530921936
Epoch 540, training loss: 871.2769165039062 = 0.8755369186401367 + 100.0 * 8.70401382446289
Epoch 540, val loss: 0.8843505382537842
Epoch 550, training loss: 871.633056640625 = 0.8668792247772217 + 100.0 * 8.707661628723145
Epoch 550, val loss: 0.8763904571533203
Epoch 560, training loss: 871.7440795898438 = 0.8579640984535217 + 100.0 * 8.708861351013184
Epoch 560, val loss: 0.8680890202522278
Epoch 570, training loss: 871.9009399414062 = 0.8488962650299072 + 100.0 * 8.71052074432373
Epoch 570, val loss: 0.8596323728561401
Epoch 580, training loss: 872.0157470703125 = 0.8399234414100647 + 100.0 * 8.711758613586426
Epoch 580, val loss: 0.8513510823249817
Epoch 590, training loss: 872.36669921875 = 0.8309646248817444 + 100.0 * 8.715356826782227
Epoch 590, val loss: 0.843035101890564
Epoch 600, training loss: 872.4874877929688 = 0.8218289613723755 + 100.0 * 8.716656684875488
Epoch 600, val loss: 0.8345033526420593
Epoch 610, training loss: 872.4637451171875 = 0.8127048015594482 + 100.0 * 8.716510772705078
Epoch 610, val loss: 0.8262661099433899
Epoch 620, training loss: 873.041748046875 = 0.8035600185394287 + 100.0 * 8.722381591796875
Epoch 620, val loss: 0.8177035450935364
Epoch 630, training loss: 873.1406860351562 = 0.7944954633712769 + 100.0 * 8.723462104797363
Epoch 630, val loss: 0.8093141913414001
Epoch 640, training loss: 872.7866821289062 = 0.7852163314819336 + 100.0 * 8.720014572143555
Epoch 640, val loss: 0.8007477521896362
Epoch 650, training loss: 873.6316528320312 = 0.7759063839912415 + 100.0 * 8.728557586669922
Epoch 650, val loss: 0.7923250198364258
Epoch 660, training loss: 873.1173095703125 = 0.7664450407028198 + 100.0 * 8.723508834838867
Epoch 660, val loss: 0.7833094000816345
Epoch 670, training loss: 873.0708618164062 = 0.7579618096351624 + 100.0 * 8.723129272460938
Epoch 670, val loss: 0.7761663198471069
Epoch 680, training loss: 872.9679565429688 = 0.7493133544921875 + 100.0 * 8.722186088562012
Epoch 680, val loss: 0.7679072022438049
Epoch 690, training loss: 873.8461303710938 = 0.7407345175743103 + 100.0 * 8.731054306030273
Epoch 690, val loss: 0.7601286172866821
Epoch 700, training loss: 874.2161254882812 = 0.732364296913147 + 100.0 * 8.734837532043457
Epoch 700, val loss: 0.7524092197418213
Epoch 710, training loss: 874.7686157226562 = 0.7240141034126282 + 100.0 * 8.740446090698242
Epoch 710, val loss: 0.7447763085365295
Epoch 720, training loss: 874.9625854492188 = 0.7156556248664856 + 100.0 * 8.742469787597656
Epoch 720, val loss: 0.7371852993965149
Epoch 730, training loss: 874.9024658203125 = 0.7072923183441162 + 100.0 * 8.741951942443848
Epoch 730, val loss: 0.7295680642127991
Epoch 740, training loss: 875.4169311523438 = 0.6991913914680481 + 100.0 * 8.747177124023438
Epoch 740, val loss: 0.7222424149513245
Epoch 750, training loss: 876.1919555664062 = 0.6911796927452087 + 100.0 * 8.75500774383545
Epoch 750, val loss: 0.7148792147636414
Epoch 760, training loss: 876.440185546875 = 0.6832302212715149 + 100.0 * 8.757569313049316
Epoch 760, val loss: 0.7076619863510132
Epoch 770, training loss: 876.939453125 = 0.67537921667099 + 100.0 * 8.762640953063965
Epoch 770, val loss: 0.7004995942115784
Epoch 780, training loss: 877.1893920898438 = 0.6677605509757996 + 100.0 * 8.765215873718262
Epoch 780, val loss: 0.6935508251190186
Epoch 790, training loss: 877.4105834960938 = 0.6602568030357361 + 100.0 * 8.767502784729004
Epoch 790, val loss: 0.6866658329963684
Epoch 800, training loss: 873.587890625 = 0.6492400169372559 + 100.0 * 8.729386329650879
Epoch 800, val loss: 0.6767030358314514
Epoch 810, training loss: 875.6949462890625 = 0.6444681882858276 + 100.0 * 8.750504493713379
Epoch 810, val loss: 0.6720716953277588
Epoch 820, training loss: 875.68359375 = 0.6381417512893677 + 100.0 * 8.750454902648926
Epoch 820, val loss: 0.6661636233329773
Epoch 830, training loss: 874.5128784179688 = 0.6310322284698486 + 100.0 * 8.738818168640137
Epoch 830, val loss: 0.6603184342384338
Epoch 840, training loss: 876.190673828125 = 0.6249451637268066 + 100.0 * 8.755657196044922
Epoch 840, val loss: 0.6544250249862671
Epoch 850, training loss: 876.505859375 = 0.6185379028320312 + 100.0 * 8.758872985839844
Epoch 850, val loss: 0.6486417055130005
Epoch 860, training loss: 877.2732543945312 = 0.6120885014533997 + 100.0 * 8.76661205291748
Epoch 860, val loss: 0.6428698301315308
Epoch 870, training loss: 878.050048828125 = 0.60589998960495 + 100.0 * 8.774441719055176
Epoch 870, val loss: 0.6371837854385376
Epoch 880, training loss: 877.9793701171875 = 0.5997104644775391 + 100.0 * 8.773796081542969
Epoch 880, val loss: 0.6316730976104736
Epoch 890, training loss: 878.6661376953125 = 0.5937318801879883 + 100.0 * 8.780723571777344
Epoch 890, val loss: 0.626349151134491
Epoch 900, training loss: 878.8355102539062 = 0.5878848433494568 + 100.0 * 8.782476425170898
Epoch 900, val loss: 0.6210275292396545
Epoch 910, training loss: 879.3955078125 = 0.5821508169174194 + 100.0 * 8.78813362121582
Epoch 910, val loss: 0.6158909797668457
Epoch 920, training loss: 879.2335205078125 = 0.5764903426170349 + 100.0 * 8.78657054901123
Epoch 920, val loss: 0.6108009219169617
Epoch 930, training loss: 879.5841064453125 = 0.571019172668457 + 100.0 * 8.790130615234375
Epoch 930, val loss: 0.6059769988059998
Epoch 940, training loss: 880.1126098632812 = 0.5658553838729858 + 100.0 * 8.795467376708984
Epoch 940, val loss: 0.601318359375
Epoch 950, training loss: 880.2288208007812 = 0.5606442093849182 + 100.0 * 8.79668140411377
Epoch 950, val loss: 0.5966743230819702
Epoch 960, training loss: 879.9985961914062 = 0.5555900931358337 + 100.0 * 8.794429779052734
Epoch 960, val loss: 0.5922231078147888
Epoch 970, training loss: 880.54541015625 = 0.5506554841995239 + 100.0 * 8.799947738647461
Epoch 970, val loss: 0.5878766775131226
Epoch 980, training loss: 880.8643188476562 = 0.5460940003395081 + 100.0 * 8.803182601928711
Epoch 980, val loss: 0.5838219523429871
Epoch 990, training loss: 881.4526977539062 = 0.541576623916626 + 100.0 * 8.809111595153809
Epoch 990, val loss: 0.579903244972229
Epoch 1000, training loss: 881.260009765625 = 0.5371086597442627 + 100.0 * 8.807229042053223
Epoch 1000, val loss: 0.576005220413208
Epoch 1010, training loss: 881.6708984375 = 0.5327862501144409 + 100.0 * 8.811381340026855
Epoch 1010, val loss: 0.5721551775932312
Epoch 1020, training loss: 881.8060913085938 = 0.528606653213501 + 100.0 * 8.812774658203125
Epoch 1020, val loss: 0.5685428977012634
Epoch 1030, training loss: 882.2532958984375 = 0.524560809135437 + 100.0 * 8.81728744506836
Epoch 1030, val loss: 0.5650889873504639
Epoch 1040, training loss: 882.0926513671875 = 0.5205895304679871 + 100.0 * 8.815720558166504
Epoch 1040, val loss: 0.561627209186554
Epoch 1050, training loss: 882.6988525390625 = 0.5168637633323669 + 100.0 * 8.821820259094238
Epoch 1050, val loss: 0.558431088924408
Epoch 1060, training loss: 882.6727294921875 = 0.5130006670951843 + 100.0 * 8.8215970993042
Epoch 1060, val loss: 0.5550844073295593
Epoch 1070, training loss: 882.7855834960938 = 0.5093137621879578 + 100.0 * 8.822762489318848
Epoch 1070, val loss: 0.5519802570343018
Epoch 1080, training loss: 883.4789428710938 = 0.5058853626251221 + 100.0 * 8.829730987548828
Epoch 1080, val loss: 0.5490497946739197
Epoch 1090, training loss: 883.10791015625 = 0.5022574067115784 + 100.0 * 8.826056480407715
Epoch 1090, val loss: 0.5462891459465027
Epoch 1100, training loss: 879.6016235351562 = 0.4979889392852783 + 100.0 * 8.791036605834961
Epoch 1100, val loss: 0.5429292321205139
Epoch 1110, training loss: 880.7329711914062 = 0.49558255076408386 + 100.0 * 8.802373886108398
Epoch 1110, val loss: 0.5401164293289185
Epoch 1120, training loss: 881.1793823242188 = 0.4925744831562042 + 100.0 * 8.806868553161621
Epoch 1120, val loss: 0.5380657911300659
Epoch 1130, training loss: 881.2815551757812 = 0.4896312654018402 + 100.0 * 8.8079195022583
Epoch 1130, val loss: 0.5356130003929138
Epoch 1140, training loss: 882.0840454101562 = 0.4868300259113312 + 100.0 * 8.815972328186035
Epoch 1140, val loss: 0.5332908630371094
Epoch 1150, training loss: 882.8461303710938 = 0.4841453433036804 + 100.0 * 8.823619842529297
Epoch 1150, val loss: 0.5310775637626648
Epoch 1160, training loss: 883.185546875 = 0.4814777672290802 + 100.0 * 8.827040672302246
Epoch 1160, val loss: 0.528903067111969
Epoch 1170, training loss: 883.7435302734375 = 0.4788295030593872 + 100.0 * 8.832647323608398
Epoch 1170, val loss: 0.5268899202346802
Epoch 1180, training loss: 884.4075317382812 = 0.47618013620376587 + 100.0 * 8.839313507080078
Epoch 1180, val loss: 0.5247207880020142
Epoch 1190, training loss: 884.676025390625 = 0.4735734164714813 + 100.0 * 8.842024803161621
Epoch 1190, val loss: 0.5226429104804993
Epoch 1200, training loss: 884.8822021484375 = 0.4710432291030884 + 100.0 * 8.844111442565918
Epoch 1200, val loss: 0.5206387042999268
Epoch 1210, training loss: 885.1485595703125 = 0.468561589717865 + 100.0 * 8.846799850463867
Epoch 1210, val loss: 0.5186721086502075
Epoch 1220, training loss: 885.1484985351562 = 0.4661109149456024 + 100.0 * 8.846823692321777
Epoch 1220, val loss: 0.5166968703269958
Epoch 1230, training loss: 885.3901977539062 = 0.4637797176837921 + 100.0 * 8.849264144897461
Epoch 1230, val loss: 0.5148292779922485
Epoch 1240, training loss: 885.2125854492188 = 0.46145811676979065 + 100.0 * 8.847511291503906
Epoch 1240, val loss: 0.513062059879303
Epoch 1250, training loss: 885.9689331054688 = 0.4592413604259491 + 100.0 * 8.855096817016602
Epoch 1250, val loss: 0.5113039016723633
Epoch 1260, training loss: 886.2094116210938 = 0.45704928040504456 + 100.0 * 8.857523918151855
Epoch 1260, val loss: 0.5096056461334229
Epoch 1270, training loss: 886.412353515625 = 0.4548789858818054 + 100.0 * 8.859574317932129
Epoch 1270, val loss: 0.5079191327095032
Epoch 1280, training loss: 886.0194702148438 = 0.45274001359939575 + 100.0 * 8.855667114257812
Epoch 1280, val loss: 0.5062575936317444
Epoch 1290, training loss: 886.7415771484375 = 0.4507232904434204 + 100.0 * 8.862908363342285
Epoch 1290, val loss: 0.5047261714935303
Epoch 1300, training loss: 886.4354248046875 = 0.44862279295921326 + 100.0 * 8.859868049621582
Epoch 1300, val loss: 0.5031958222389221
Epoch 1310, training loss: 885.47021484375 = 0.44662022590637207 + 100.0 * 8.850235939025879
Epoch 1310, val loss: 0.5016891956329346
Epoch 1320, training loss: 886.2883911132812 = 0.4447886347770691 + 100.0 * 8.858436584472656
Epoch 1320, val loss: 0.5004324913024902
Epoch 1330, training loss: 886.279052734375 = 0.4429742395877838 + 100.0 * 8.858360290527344
Epoch 1330, val loss: 0.498819500207901
Epoch 1340, training loss: 886.2936401367188 = 0.44103479385375977 + 100.0 * 8.858526229858398
Epoch 1340, val loss: 0.49757254123687744
Epoch 1350, training loss: 886.4149169921875 = 0.43927544355392456 + 100.0 * 8.859756469726562
Epoch 1350, val loss: 0.4961555004119873
Epoch 1360, training loss: 887.188720703125 = 0.4375942349433899 + 100.0 * 8.867510795593262
Epoch 1360, val loss: 0.4949326813220978
Epoch 1370, training loss: 887.4397583007812 = 0.43582889437675476 + 100.0 * 8.870038986206055
Epoch 1370, val loss: 0.49360257387161255
Epoch 1380, training loss: 887.979736328125 = 0.4341117739677429 + 100.0 * 8.875456809997559
Epoch 1380, val loss: 0.4923238158226013
Epoch 1390, training loss: 887.8441162109375 = 0.4324079751968384 + 100.0 * 8.874116897583008
Epoch 1390, val loss: 0.4911116361618042
Epoch 1400, training loss: 888.044677734375 = 0.4307517409324646 + 100.0 * 8.876139640808105
Epoch 1400, val loss: 0.48989254236221313
Epoch 1410, training loss: 888.5267333984375 = 0.42916297912597656 + 100.0 * 8.880975723266602
Epoch 1410, val loss: 0.48881009221076965
Epoch 1420, training loss: 888.591796875 = 0.4275585114955902 + 100.0 * 8.88164234161377
Epoch 1420, val loss: 0.4875537157058716
Epoch 1430, training loss: 888.2510375976562 = 0.4259297847747803 + 100.0 * 8.878251075744629
Epoch 1430, val loss: 0.48638689517974854
Epoch 1440, training loss: 888.4480590820312 = 0.42438116669654846 + 100.0 * 8.880236625671387
Epoch 1440, val loss: 0.48544034361839294
Epoch 1450, training loss: 888.9848022460938 = 0.42288851737976074 + 100.0 * 8.885619163513184
Epoch 1450, val loss: 0.48445072770118713
Epoch 1460, training loss: 888.9408569335938 = 0.421394944190979 + 100.0 * 8.885194778442383
Epoch 1460, val loss: 0.48325157165527344
Epoch 1470, training loss: 889.2301635742188 = 0.41991889476776123 + 100.0 * 8.888102531433105
Epoch 1470, val loss: 0.4824107587337494
Epoch 1480, training loss: 889.27685546875 = 0.41848087310791016 + 100.0 * 8.88858413696289
Epoch 1480, val loss: 0.4813854992389679
Epoch 1490, training loss: 889.609619140625 = 0.41705793142318726 + 100.0 * 8.891925811767578
Epoch 1490, val loss: 0.48036086559295654
Epoch 1500, training loss: 889.9315185546875 = 0.41573938727378845 + 100.0 * 8.895157814025879
Epoch 1500, val loss: 0.4794757068157196
Epoch 1510, training loss: 886.8453979492188 = 0.4138997197151184 + 100.0 * 8.864315032958984
Epoch 1510, val loss: 0.47846493124961853
Epoch 1520, training loss: 887.6466064453125 = 0.4128184914588928 + 100.0 * 8.872337341308594
Epoch 1520, val loss: 0.477464497089386
Epoch 1530, training loss: 887.5140991210938 = 0.4114852845668793 + 100.0 * 8.871026039123535
Epoch 1530, val loss: 0.47675028443336487
Epoch 1540, training loss: 889.246337890625 = 0.41034743189811707 + 100.0 * 8.888360023498535
Epoch 1540, val loss: 0.47604110836982727
Epoch 1550, training loss: 889.9506225585938 = 0.40909260511398315 + 100.0 * 8.895415306091309
Epoch 1550, val loss: 0.47531142830848694
Epoch 1560, training loss: 890.5169677734375 = 0.40781569480895996 + 100.0 * 8.901091575622559
Epoch 1560, val loss: 0.47446465492248535
Epoch 1570, training loss: 890.7473754882812 = 0.4065341353416443 + 100.0 * 8.90340805053711
Epoch 1570, val loss: 0.47355133295059204
Epoch 1580, training loss: 890.9491577148438 = 0.4052421748638153 + 100.0 * 8.905439376831055
Epoch 1580, val loss: 0.47271832823753357
Epoch 1590, training loss: 891.5416259765625 = 0.4040490388870239 + 100.0 * 8.911375999450684
Epoch 1590, val loss: 0.4719274640083313
Epoch 1600, training loss: 891.746337890625 = 0.40285220742225647 + 100.0 * 8.913434982299805
Epoch 1600, val loss: 0.4712335765361786
Epoch 1610, training loss: 891.3925170898438 = 0.40160810947418213 + 100.0 * 8.90990924835205
Epoch 1610, val loss: 0.47032755613327026
Epoch 1620, training loss: 891.7811279296875 = 0.4004409611225128 + 100.0 * 8.913806915283203
Epoch 1620, val loss: 0.4696961045265198
Epoch 1630, training loss: 892.0086669921875 = 0.3992980122566223 + 100.0 * 8.916093826293945
Epoch 1630, val loss: 0.46896809339523315
Epoch 1640, training loss: 892.0318603515625 = 0.39815768599510193 + 100.0 * 8.916337013244629
Epoch 1640, val loss: 0.46824344992637634
Epoch 1650, training loss: 892.0326538085938 = 0.39698436856269836 + 100.0 * 8.916357040405273
Epoch 1650, val loss: 0.467628538608551
Epoch 1660, training loss: 892.124755859375 = 0.3958754241466522 + 100.0 * 8.917288780212402
Epoch 1660, val loss: 0.4668169319629669
Epoch 1670, training loss: 892.4447631835938 = 0.3947593569755554 + 100.0 * 8.920499801635742
Epoch 1670, val loss: 0.4665895402431488
Epoch 1680, training loss: 892.3539428710938 = 0.393669456243515 + 100.0 * 8.919602394104004
Epoch 1680, val loss: 0.465565025806427
Epoch 1690, training loss: 892.5263061523438 = 0.3926035165786743 + 100.0 * 8.921337127685547
Epoch 1690, val loss: 0.46496424078941345
Epoch 1700, training loss: 892.9205322265625 = 0.3915558159351349 + 100.0 * 8.92529010772705
Epoch 1700, val loss: 0.46434006094932556
Epoch 1710, training loss: 893.2465209960938 = 0.3905339241027832 + 100.0 * 8.928559303283691
Epoch 1710, val loss: 0.46372735500335693
Epoch 1720, training loss: 892.2785034179688 = 0.38937556743621826 + 100.0 * 8.918890953063965
Epoch 1720, val loss: 0.463140606880188
Epoch 1730, training loss: 892.9181518554688 = 0.388342022895813 + 100.0 * 8.925297737121582
Epoch 1730, val loss: 0.46254900097846985
Epoch 1740, training loss: 893.2391357421875 = 0.38732632994651794 + 100.0 * 8.928518295288086
Epoch 1740, val loss: 0.46197509765625
Epoch 1750, training loss: 893.6287231445312 = 0.3863217234611511 + 100.0 * 8.932424545288086
Epoch 1750, val loss: 0.46143028140068054
Epoch 1760, training loss: 893.9664916992188 = 0.38531365990638733 + 100.0 * 8.935811996459961
Epoch 1760, val loss: 0.46086907386779785
Epoch 1770, training loss: 894.0252685546875 = 0.38429856300354004 + 100.0 * 8.936409950256348
Epoch 1770, val loss: 0.4604077637195587
Epoch 1780, training loss: 894.0571899414062 = 0.38332080841064453 + 100.0 * 8.936738967895508
Epoch 1780, val loss: 0.45987504720687866
Epoch 1790, training loss: 894.2872924804688 = 0.3823506534099579 + 100.0 * 8.93904972076416
Epoch 1790, val loss: 0.4592655599117279
Epoch 1800, training loss: 891.1744384765625 = 0.3812725245952606 + 100.0 * 8.907931327819824
Epoch 1800, val loss: 0.45862817764282227
Epoch 1810, training loss: 892.6018676757812 = 0.3802838623523712 + 100.0 * 8.922215461730957
Epoch 1810, val loss: 0.4582091271877289
Epoch 1820, training loss: 892.5254516601562 = 0.37924736738204956 + 100.0 * 8.921462059020996
Epoch 1820, val loss: 0.4574926495552063
Epoch 1830, training loss: 892.9170532226562 = 0.3784423768520355 + 100.0 * 8.925386428833008
Epoch 1830, val loss: 0.45711252093315125
Epoch 1840, training loss: 893.5095825195312 = 0.3775818645954132 + 100.0 * 8.931320190429688
Epoch 1840, val loss: 0.4563829004764557
Epoch 1850, training loss: 894.0084228515625 = 0.3766818344593048 + 100.0 * 8.936317443847656
Epoch 1850, val loss: 0.45618700981140137
Epoch 1860, training loss: 894.4425659179688 = 0.3757433295249939 + 100.0 * 8.940668106079102
Epoch 1860, val loss: 0.4555209279060364
Epoch 1870, training loss: 894.0134887695312 = 0.37477850914001465 + 100.0 * 8.936387062072754
Epoch 1870, val loss: 0.4550842046737671
Epoch 1880, training loss: 894.5155029296875 = 0.3738972842693329 + 100.0 * 8.941415786743164
Epoch 1880, val loss: 0.45444390177726746
Epoch 1890, training loss: 894.9404907226562 = 0.3729741871356964 + 100.0 * 8.945674896240234
Epoch 1890, val loss: 0.4541381001472473
Epoch 1900, training loss: 894.9382934570312 = 0.37207144498825073 + 100.0 * 8.945662498474121
Epoch 1900, val loss: 0.4535614550113678
Epoch 1910, training loss: 895.0675048828125 = 0.37115493416786194 + 100.0 * 8.9469633102417
Epoch 1910, val loss: 0.45320481061935425
Epoch 1920, training loss: 895.3499755859375 = 0.3702499270439148 + 100.0 * 8.949797630310059
Epoch 1920, val loss: 0.452750563621521
Epoch 1930, training loss: 895.4694213867188 = 0.3693561851978302 + 100.0 * 8.951000213623047
Epoch 1930, val loss: 0.4522118866443634
Epoch 1940, training loss: 895.9425659179688 = 0.36847469210624695 + 100.0 * 8.955740928649902
Epoch 1940, val loss: 0.4517688751220703
Epoch 1950, training loss: 895.8416137695312 = 0.36757493019104004 + 100.0 * 8.954740524291992
Epoch 1950, val loss: 0.45130014419555664
Epoch 1960, training loss: 894.6732177734375 = 0.36657002568244934 + 100.0 * 8.943066596984863
Epoch 1960, val loss: 0.4507969915866852
Epoch 1970, training loss: 895.3499145507812 = 0.36576583981513977 + 100.0 * 8.949841499328613
Epoch 1970, val loss: 0.450421005487442
Epoch 1980, training loss: 895.9885864257812 = 0.36494728922843933 + 100.0 * 8.956236839294434
Epoch 1980, val loss: 0.45007097721099854
Epoch 1990, training loss: 896.22216796875 = 0.3640907108783722 + 100.0 * 8.95858097076416
Epoch 1990, val loss: 0.449714720249176
Epoch 2000, training loss: 896.3350219726562 = 0.363246887922287 + 100.0 * 8.959717750549316
Epoch 2000, val loss: 0.44938069581985474
Epoch 2010, training loss: 896.474609375 = 0.36239951848983765 + 100.0 * 8.961121559143066
Epoch 2010, val loss: 0.4488627314567566
Epoch 2020, training loss: 896.2529907226562 = 0.36153003573417664 + 100.0 * 8.958914756774902
Epoch 2020, val loss: 0.4485377073287964
Epoch 2030, training loss: 896.5816040039062 = 0.36068347096443176 + 100.0 * 8.962209701538086
Epoch 2030, val loss: 0.44820764660835266
Epoch 2040, training loss: 896.2218017578125 = 0.35985106229782104 + 100.0 * 8.958619117736816
Epoch 2040, val loss: 0.44777557253837585
Epoch 2050, training loss: 896.5859985351562 = 0.3590218424797058 + 100.0 * 8.96226978302002
Epoch 2050, val loss: 0.447536826133728
Epoch 2060, training loss: 897.0530395507812 = 0.3582058846950531 + 100.0 * 8.966948509216309
Epoch 2060, val loss: 0.44710367918014526
Epoch 2070, training loss: 897.1054077148438 = 0.357345849275589 + 100.0 * 8.967480659484863
Epoch 2070, val loss: 0.44669121503829956
Epoch 2080, training loss: 897.2451171875 = 0.35649433732032776 + 100.0 * 8.968886375427246
Epoch 2080, val loss: 0.44631022214889526
Epoch 2090, training loss: 897.3668212890625 = 0.35565704107284546 + 100.0 * 8.970111846923828
Epoch 2090, val loss: 0.44593819975852966
Epoch 2100, training loss: 897.2113647460938 = 0.35480910539627075 + 100.0 * 8.968565940856934
Epoch 2100, val loss: 0.4454309046268463
Epoch 2110, training loss: 896.275390625 = 0.3538858890533447 + 100.0 * 8.95921516418457
Epoch 2110, val loss: 0.4452763497829437
Epoch 2120, training loss: 896.5474243164062 = 0.3531050980091095 + 100.0 * 8.961943626403809
Epoch 2120, val loss: 0.4449087083339691
Epoch 2130, training loss: 897.2035522460938 = 0.3523043096065521 + 100.0 * 8.968512535095215
Epoch 2130, val loss: 0.4443278908729553
Epoch 2140, training loss: 897.47802734375 = 0.35146868228912354 + 100.0 * 8.97126579284668
Epoch 2140, val loss: 0.4439649283885956
Epoch 2150, training loss: 897.438720703125 = 0.35063567757606506 + 100.0 * 8.970880508422852
Epoch 2150, val loss: 0.4438014328479767
Epoch 2160, training loss: 897.6533813476562 = 0.3498222231864929 + 100.0 * 8.97303581237793
Epoch 2160, val loss: 0.4432784914970398
Epoch 2170, training loss: 898.1946411132812 = 0.34901681542396545 + 100.0 * 8.978456497192383
Epoch 2170, val loss: 0.4429623782634735
Epoch 2180, training loss: 896.311279296875 = 0.34811192750930786 + 100.0 * 8.95963191986084
Epoch 2180, val loss: 0.44252434372901917
Epoch 2190, training loss: 896.7017822265625 = 0.347320556640625 + 100.0 * 8.963544845581055
Epoch 2190, val loss: 0.44220632314682007
Epoch 2200, training loss: 897.5809326171875 = 0.3465709090232849 + 100.0 * 8.972343444824219
Epoch 2200, val loss: 0.44209742546081543
Epoch 2210, training loss: 898.2384033203125 = 0.3457883298397064 + 100.0 * 8.978926658630371
Epoch 2210, val loss: 0.4416562020778656
Epoch 2220, training loss: 898.4561157226562 = 0.34501326084136963 + 100.0 * 8.981110572814941
Epoch 2220, val loss: 0.44108304381370544
Epoch 2230, training loss: 897.9636840820312 = 0.34415268898010254 + 100.0 * 8.976195335388184
Epoch 2230, val loss: 0.440693199634552
Epoch 2240, training loss: 898.513671875 = 0.3433518409729004 + 100.0 * 8.981703758239746
Epoch 2240, val loss: 0.4408794641494751
Epoch 2250, training loss: 898.5787353515625 = 0.3425522446632385 + 100.0 * 8.982361793518066
Epoch 2250, val loss: 0.4407975375652313
Epoch 2260, training loss: 899.0071411132812 = 0.341746985912323 + 100.0 * 8.986654281616211
Epoch 2260, val loss: 0.4402001202106476
Epoch 2270, training loss: 897.8540649414062 = 0.34093040227890015 + 100.0 * 8.975131034851074
Epoch 2270, val loss: 0.44012629985809326
Epoch 2280, training loss: 896.9274291992188 = 0.34029024839401245 + 100.0 * 8.965871810913086
Epoch 2280, val loss: 0.43878602981567383
Epoch 2290, training loss: 897.3523559570312 = 0.3396626114845276 + 100.0 * 8.97012710571289
Epoch 2290, val loss: 0.4406690001487732
Epoch 2300, training loss: 898.5049438476562 = 0.33890512585639954 + 100.0 * 8.981659889221191
Epoch 2300, val loss: 0.43874120712280273
Epoch 2310, training loss: 898.9616088867188 = 0.33812278509140015 + 100.0 * 8.986234664916992
Epoch 2310, val loss: 0.43918073177337646
Epoch 2320, training loss: 899.6165771484375 = 0.33731794357299805 + 100.0 * 8.992792129516602
Epoch 2320, val loss: 0.4387586712837219
Epoch 2330, training loss: 900.2664794921875 = 0.3364677131175995 + 100.0 * 8.999300003051758
Epoch 2330, val loss: 0.43824130296707153
Epoch 2340, training loss: 900.7136840820312 = 0.3356568217277527 + 100.0 * 9.003780364990234
Epoch 2340, val loss: 0.4382786750793457
Epoch 2350, training loss: 900.9891357421875 = 0.33483633399009705 + 100.0 * 9.006543159484863
Epoch 2350, val loss: 0.4378167688846588
Epoch 2360, training loss: 901.3131103515625 = 0.3339780867099762 + 100.0 * 9.009791374206543
Epoch 2360, val loss: 0.4378918409347534
Epoch 2370, training loss: 901.52978515625 = 0.33315637707710266 + 100.0 * 9.011966705322266
Epoch 2370, val loss: 0.43736666440963745
Epoch 2380, training loss: 901.6079711914062 = 0.33234867453575134 + 100.0 * 9.01275634765625
Epoch 2380, val loss: 0.4372251033782959
Epoch 2390, training loss: 901.7928466796875 = 0.331541508436203 + 100.0 * 9.014613151550293
Epoch 2390, val loss: 0.43695878982543945
Epoch 2400, training loss: 902.0612182617188 = 0.33074328303337097 + 100.0 * 9.017304420471191
Epoch 2400, val loss: 0.43674594163894653
Epoch 2410, training loss: 902.0366821289062 = 0.32992520928382874 + 100.0 * 9.017067909240723
Epoch 2410, val loss: 0.4364502429962158
Epoch 2420, training loss: 902.2755126953125 = 0.32914385199546814 + 100.0 * 9.019463539123535
Epoch 2420, val loss: 0.43615612387657166
Epoch 2430, training loss: 902.4254150390625 = 0.32836413383483887 + 100.0 * 9.020970344543457
Epoch 2430, val loss: 0.43599632382392883
Epoch 2440, training loss: 902.5349731445312 = 0.3275754749774933 + 100.0 * 9.022073745727539
Epoch 2440, val loss: 0.4357966184616089
Epoch 2450, training loss: 902.3690795898438 = 0.3267681300640106 + 100.0 * 9.02042293548584
Epoch 2450, val loss: 0.43548375368118286
Epoch 2460, training loss: 901.0477294921875 = 0.3259677588939667 + 100.0 * 9.007217407226562
Epoch 2460, val loss: 0.4354407787322998
Epoch 2470, training loss: 901.4718017578125 = 0.3253125250339508 + 100.0 * 9.011465072631836
Epoch 2470, val loss: 0.4362546503543854
Epoch 2480, training loss: 900.976806640625 = 0.3245706260204315 + 100.0 * 9.006522178649902
Epoch 2480, val loss: 0.43435800075531006
Epoch 2490, training loss: 901.7774658203125 = 0.3237648904323578 + 100.0 * 9.01453685760498
Epoch 2490, val loss: 0.4358123242855072
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8379710144927536
0.8628559008911107
=== training gcn model ===
Epoch 0, training loss: 1015.8970336914062 = 1.091917634010315 + 100.0 * 10.148051261901855
Epoch 0, val loss: 1.0912659168243408
Epoch 10, training loss: 970.5133666992188 = 1.088553547859192 + 100.0 * 9.69424819946289
Epoch 10, val loss: 1.0880110263824463
Epoch 20, training loss: 949.583984375 = 1.0855475664138794 + 100.0 * 9.484984397888184
Epoch 20, val loss: 1.0850510597229004
Epoch 30, training loss: 934.2612915039062 = 1.0826051235198975 + 100.0 * 9.331787109375
Epoch 30, val loss: 1.082160234451294
Epoch 40, training loss: 922.4140014648438 = 1.079818606376648 + 100.0 * 9.21334171295166
Epoch 40, val loss: 1.0794264078140259
Epoch 50, training loss: 913.0653686523438 = 1.0771899223327637 + 100.0 * 9.119881629943848
Epoch 50, val loss: 1.0768463611602783
Epoch 60, training loss: 905.6607666015625 = 1.0746632814407349 + 100.0 * 9.04586124420166
Epoch 60, val loss: 1.0743706226348877
Epoch 70, training loss: 899.7040405273438 = 1.0722626447677612 + 100.0 * 8.98631763458252
Epoch 70, val loss: 1.0720269680023193
Epoch 80, training loss: 894.7510375976562 = 1.0700002908706665 + 100.0 * 8.936810493469238
Epoch 80, val loss: 1.069814682006836
Epoch 90, training loss: 890.5792846679688 = 1.0678404569625854 + 100.0 * 8.89511489868164
Epoch 90, val loss: 1.0677114725112915
Epoch 100, training loss: 887.0594482421875 = 1.0657697916030884 + 100.0 * 8.859936714172363
Epoch 100, val loss: 1.0657017230987549
Epoch 110, training loss: 884.1520385742188 = 1.0638225078582764 + 100.0 * 8.83088207244873
Epoch 110, val loss: 1.0638115406036377
Epoch 120, training loss: 881.6268310546875 = 1.0619266033172607 + 100.0 * 8.805648803710938
Epoch 120, val loss: 1.061980962753296
Epoch 130, training loss: 879.450927734375 = 1.0601164102554321 + 100.0 * 8.783907890319824
Epoch 130, val loss: 1.0602436065673828
Epoch 140, training loss: 877.45166015625 = 1.0583782196044922 + 100.0 * 8.763933181762695
Epoch 140, val loss: 1.0585671663284302
Epoch 150, training loss: 875.7808227539062 = 1.056711196899414 + 100.0 * 8.747241020202637
Epoch 150, val loss: 1.0569789409637451
Epoch 160, training loss: 874.2770385742188 = 1.0550678968429565 + 100.0 * 8.732219696044922
Epoch 160, val loss: 1.0554181337356567
Epoch 170, training loss: 873.0033569335938 = 1.0534964799880981 + 100.0 * 8.719498634338379
Epoch 170, val loss: 1.0539261102676392
Epoch 180, training loss: 871.9276733398438 = 1.0519553422927856 + 100.0 * 8.708757400512695
Epoch 180, val loss: 1.0524423122406006
Epoch 190, training loss: 871.2032470703125 = 1.0503915548324585 + 100.0 * 8.701528549194336
Epoch 190, val loss: 1.050967812538147
Epoch 200, training loss: 870.4349365234375 = 1.0487688779830933 + 100.0 * 8.693861961364746
Epoch 200, val loss: 1.0494335889816284
Epoch 210, training loss: 869.875 = 1.047192096710205 + 100.0 * 8.688278198242188
Epoch 210, val loss: 1.0479635000228882
Epoch 220, training loss: 869.4024658203125 = 1.04558527469635 + 100.0 * 8.683568954467773
Epoch 220, val loss: 1.0464470386505127
Epoch 230, training loss: 869.0357055664062 = 1.043883204460144 + 100.0 * 8.67991828918457
Epoch 230, val loss: 1.044840693473816
Epoch 240, training loss: 868.6248168945312 = 1.0421127080917358 + 100.0 * 8.675827026367188
Epoch 240, val loss: 1.0431205034255981
Epoch 250, training loss: 868.8637084960938 = 1.0402476787567139 + 100.0 * 8.678235054016113
Epoch 250, val loss: 1.0414395332336426
Epoch 260, training loss: 867.7547607421875 = 1.038231611251831 + 100.0 * 8.667165756225586
Epoch 260, val loss: 1.039530634880066
Epoch 270, training loss: 867.6919555664062 = 1.03621506690979 + 100.0 * 8.666557312011719
Epoch 270, val loss: 1.037598967552185
Epoch 280, training loss: 867.7689819335938 = 1.0340219736099243 + 100.0 * 8.667349815368652
Epoch 280, val loss: 1.0355316400527954
Epoch 290, training loss: 867.311767578125 = 1.0316258668899536 + 100.0 * 8.662801742553711
Epoch 290, val loss: 1.0332976579666138
Epoch 300, training loss: 866.7564697265625 = 1.029050588607788 + 100.0 * 8.65727424621582
Epoch 300, val loss: 1.0308656692504883
Epoch 310, training loss: 866.5118408203125 = 1.0262712240219116 + 100.0 * 8.654855728149414
Epoch 310, val loss: 1.028279185295105
Epoch 320, training loss: 867.0342407226562 = 1.0234023332595825 + 100.0 * 8.66010856628418
Epoch 320, val loss: 1.0255579948425293
Epoch 330, training loss: 866.5501098632812 = 1.0203807353973389 + 100.0 * 8.65529727935791
Epoch 330, val loss: 1.0226291418075562
Epoch 340, training loss: 867.0010986328125 = 1.0173122882843018 + 100.0 * 8.65983772277832
Epoch 340, val loss: 1.0198006629943848
Epoch 350, training loss: 866.735595703125 = 1.013673186302185 + 100.0 * 8.657218933105469
Epoch 350, val loss: 1.0164176225662231
Epoch 360, training loss: 866.796875 = 1.0099438428878784 + 100.0 * 8.657869338989258
Epoch 360, val loss: 1.0129085779190063
Epoch 370, training loss: 866.999755859375 = 1.006028652191162 + 100.0 * 8.659936904907227
Epoch 370, val loss: 1.0091443061828613
Epoch 380, training loss: 866.8995361328125 = 1.0017890930175781 + 100.0 * 8.658977508544922
Epoch 380, val loss: 1.0051181316375732
Epoch 390, training loss: 867.1431884765625 = 0.9973443746566772 + 100.0 * 8.661458969116211
Epoch 390, val loss: 1.000931978225708
Epoch 400, training loss: 867.3554077148438 = 0.9927089214324951 + 100.0 * 8.663626670837402
Epoch 400, val loss: 0.9965605139732361
Epoch 410, training loss: 866.485595703125 = 0.9874426126480103 + 100.0 * 8.65498161315918
Epoch 410, val loss: 0.9915404319763184
Epoch 420, training loss: 867.231689453125 = 0.9825054407119751 + 100.0 * 8.662491798400879
Epoch 420, val loss: 0.9868403673171997
Epoch 430, training loss: 867.392822265625 = 0.9772210717201233 + 100.0 * 8.664155960083008
Epoch 430, val loss: 0.9818857908248901
Epoch 440, training loss: 868.0908813476562 = 0.971472442150116 + 100.0 * 8.671194076538086
Epoch 440, val loss: 0.9764085412025452
Epoch 450, training loss: 867.2327270507812 = 0.9651657938957214 + 100.0 * 8.662675857543945
Epoch 450, val loss: 0.9706520438194275
Epoch 460, training loss: 866.9151611328125 = 0.9588658809661865 + 100.0 * 8.659563064575195
Epoch 460, val loss: 0.9645939469337463
Epoch 470, training loss: 867.2884521484375 = 0.9525377750396729 + 100.0 * 8.663359642028809
Epoch 470, val loss: 0.9585987329483032
Epoch 480, training loss: 867.7574462890625 = 0.9458509683609009 + 100.0 * 8.668115615844727
Epoch 480, val loss: 0.9523604512214661
Epoch 490, training loss: 868.5491943359375 = 0.9389097094535828 + 100.0 * 8.676102638244629
Epoch 490, val loss: 0.9457337260246277
Epoch 500, training loss: 868.2617797851562 = 0.9315922260284424 + 100.0 * 8.673301696777344
Epoch 500, val loss: 0.9388498067855835
Epoch 510, training loss: 868.59765625 = 0.9242005348205566 + 100.0 * 8.676734924316406
Epoch 510, val loss: 0.9319533705711365
Epoch 520, training loss: 868.9949951171875 = 0.9165049195289612 + 100.0 * 8.680785179138184
Epoch 520, val loss: 0.9246417880058289
Epoch 530, training loss: 869.25048828125 = 0.9085769057273865 + 100.0 * 8.683419227600098
Epoch 530, val loss: 0.9171197414398193
Epoch 540, training loss: 869.135498046875 = 0.9002751708030701 + 100.0 * 8.682352066040039
Epoch 540, val loss: 0.9093335866928101
Epoch 550, training loss: 869.3755493164062 = 0.8920275568962097 + 100.0 * 8.684835433959961
Epoch 550, val loss: 0.9015060067176819
Epoch 560, training loss: 869.67724609375 = 0.8835016489028931 + 100.0 * 8.68793773651123
Epoch 560, val loss: 0.8936414122581482
Epoch 570, training loss: 869.990478515625 = 0.8749144077301025 + 100.0 * 8.691155433654785
Epoch 570, val loss: 0.8854100108146667
Epoch 580, training loss: 869.8656616210938 = 0.8659728169441223 + 100.0 * 8.689996719360352
Epoch 580, val loss: 0.8769259452819824
Epoch 590, training loss: 870.0750122070312 = 0.8571404814720154 + 100.0 * 8.692178726196289
Epoch 590, val loss: 0.8686838746070862
Epoch 600, training loss: 869.771240234375 = 0.847675621509552 + 100.0 * 8.68923568725586
Epoch 600, val loss: 0.8597944974899292
Epoch 610, training loss: 869.9166870117188 = 0.8387682437896729 + 100.0 * 8.690779685974121
Epoch 610, val loss: 0.8511161208152771
Epoch 620, training loss: 870.5814208984375 = 0.8292874097824097 + 100.0 * 8.697521209716797
Epoch 620, val loss: 0.8421128392219543
Epoch 630, training loss: 871.105712890625 = 0.8207008838653564 + 100.0 * 8.702850341796875
Epoch 630, val loss: 0.834250807762146
Epoch 640, training loss: 869.7896118164062 = 0.8104547262191772 + 100.0 * 8.689791679382324
Epoch 640, val loss: 0.824868381023407
Epoch 650, training loss: 869.6859130859375 = 0.8009166121482849 + 100.0 * 8.688850402832031
Epoch 650, val loss: 0.8155577778816223
Epoch 660, training loss: 870.862060546875 = 0.7916725873947144 + 100.0 * 8.700703620910645
Epoch 660, val loss: 0.8068530559539795
Epoch 670, training loss: 871.3139038085938 = 0.7822491526603699 + 100.0 * 8.705316543579102
Epoch 670, val loss: 0.7980837225914001
Epoch 680, training loss: 871.4034423828125 = 0.7725532054901123 + 100.0 * 8.706308364868164
Epoch 680, val loss: 0.7890174984931946
Epoch 690, training loss: 871.8432006835938 = 0.7629954218864441 + 100.0 * 8.71080207824707
Epoch 690, val loss: 0.7799780368804932
Epoch 700, training loss: 871.72705078125 = 0.7530810832977295 + 100.0 * 8.709739685058594
Epoch 700, val loss: 0.7705695629119873
Epoch 710, training loss: 871.0078735351562 = 0.7433369159698486 + 100.0 * 8.702645301818848
Epoch 710, val loss: 0.7613312602043152
Epoch 720, training loss: 871.4663696289062 = 0.7341017723083496 + 100.0 * 8.707322120666504
Epoch 720, val loss: 0.7526776790618896
Epoch 730, training loss: 871.9448852539062 = 0.7249536514282227 + 100.0 * 8.712199211120605
Epoch 730, val loss: 0.7439524531364441
Epoch 740, training loss: 872.267578125 = 0.7156909108161926 + 100.0 * 8.715518951416016
Epoch 740, val loss: 0.7351697087287903
Epoch 750, training loss: 871.5315551757812 = 0.7044762372970581 + 100.0 * 8.708271026611328
Epoch 750, val loss: 0.7238534688949585
Epoch 760, training loss: 869.7594604492188 = 0.6946200132369995 + 100.0 * 8.690648078918457
Epoch 760, val loss: 0.7152227759361267
Epoch 770, training loss: 872.7220458984375 = 0.6874822974205017 + 100.0 * 8.720345497131348
Epoch 770, val loss: 0.7081989645957947
Epoch 780, training loss: 870.2279052734375 = 0.6773728728294373 + 100.0 * 8.695505142211914
Epoch 780, val loss: 0.6987887620925903
Epoch 790, training loss: 872.2584838867188 = 0.6710241436958313 + 100.0 * 8.715874671936035
Epoch 790, val loss: 0.6928034424781799
Epoch 800, training loss: 871.8695068359375 = 0.6627005338668823 + 100.0 * 8.712067604064941
Epoch 800, val loss: 0.6846829056739807
Epoch 810, training loss: 872.58447265625 = 0.6548813581466675 + 100.0 * 8.719295501708984
Epoch 810, val loss: 0.677508533000946
Epoch 820, training loss: 873.013671875 = 0.6471951603889465 + 100.0 * 8.723664283752441
Epoch 820, val loss: 0.670257031917572
Epoch 830, training loss: 873.7105712890625 = 0.6398171186447144 + 100.0 * 8.730707168579102
Epoch 830, val loss: 0.6633135080337524
Epoch 840, training loss: 874.124267578125 = 0.6323291063308716 + 100.0 * 8.734919548034668
Epoch 840, val loss: 0.656288206577301
Epoch 850, training loss: 874.134521484375 = 0.6249106526374817 + 100.0 * 8.735095977783203
Epoch 850, val loss: 0.6493518352508545
Epoch 860, training loss: 874.8452758789062 = 0.6178914308547974 + 100.0 * 8.742273330688477
Epoch 860, val loss: 0.6427585482597351
Epoch 870, training loss: 875.0869750976562 = 0.6111480593681335 + 100.0 * 8.744758605957031
Epoch 870, val loss: 0.6364052295684814
Epoch 880, training loss: 875.6285400390625 = 0.6045908331871033 + 100.0 * 8.750239372253418
Epoch 880, val loss: 0.6302866339683533
Epoch 890, training loss: 875.63037109375 = 0.5980508327484131 + 100.0 * 8.750323295593262
Epoch 890, val loss: 0.6241847276687622
Epoch 900, training loss: 875.7955322265625 = 0.5918914675712585 + 100.0 * 8.752036094665527
Epoch 900, val loss: 0.6183916926383972
Epoch 910, training loss: 874.767578125 = 0.5853316783905029 + 100.0 * 8.741822242736816
Epoch 910, val loss: 0.6123687624931335
Epoch 920, training loss: 876.0404052734375 = 0.5804601907730103 + 100.0 * 8.754599571228027
Epoch 920, val loss: 0.607757031917572
Epoch 930, training loss: 876.6674194335938 = 0.5751699209213257 + 100.0 * 8.7609224319458
Epoch 930, val loss: 0.6028830409049988
Epoch 940, training loss: 877.0814819335938 = 0.5698776245117188 + 100.0 * 8.765115737915039
Epoch 940, val loss: 0.5979714393615723
Epoch 950, training loss: 877.3133544921875 = 0.5646703243255615 + 100.0 * 8.767486572265625
Epoch 950, val loss: 0.5931410789489746
Epoch 960, training loss: 877.4945678710938 = 0.559633195400238 + 100.0 * 8.769349098205566
Epoch 960, val loss: 0.5885359644889832
Epoch 970, training loss: 877.3873901367188 = 0.5547140836715698 + 100.0 * 8.768326759338379
Epoch 970, val loss: 0.5840381383895874
Epoch 980, training loss: 877.941650390625 = 0.5500543713569641 + 100.0 * 8.773916244506836
Epoch 980, val loss: 0.5797123908996582
Epoch 990, training loss: 876.7515869140625 = 0.545081377029419 + 100.0 * 8.762064933776855
Epoch 990, val loss: 0.5750139951705933
Epoch 1000, training loss: 876.7550659179688 = 0.5410282611846924 + 100.0 * 8.762140274047852
Epoch 1000, val loss: 0.5713148713111877
Epoch 1010, training loss: 876.96533203125 = 0.536816418170929 + 100.0 * 8.76428508758545
Epoch 1010, val loss: 0.5677337050437927
Epoch 1020, training loss: 877.1801147460938 = 0.5330259203910828 + 100.0 * 8.766470909118652
Epoch 1020, val loss: 0.564078152179718
Epoch 1030, training loss: 877.038818359375 = 0.5289531350135803 + 100.0 * 8.765098571777344
Epoch 1030, val loss: 0.5608024001121521
Epoch 1040, training loss: 878.0557250976562 = 0.5257059931755066 + 100.0 * 8.775300025939941
Epoch 1040, val loss: 0.5577719807624817
Epoch 1050, training loss: 878.8736572265625 = 0.522368848323822 + 100.0 * 8.783513069152832
Epoch 1050, val loss: 0.5548491477966309
Epoch 1060, training loss: 879.1500854492188 = 0.5189951658248901 + 100.0 * 8.786311149597168
Epoch 1060, val loss: 0.5519437193870544
Epoch 1070, training loss: 879.4118041992188 = 0.5157190561294556 + 100.0 * 8.788960456848145
Epoch 1070, val loss: 0.5491129755973816
Epoch 1080, training loss: 879.8427124023438 = 0.5125747323036194 + 100.0 * 8.793301582336426
Epoch 1080, val loss: 0.5463496446609497
Epoch 1090, training loss: 879.6675415039062 = 0.5093480348587036 + 100.0 * 8.791582107543945
Epoch 1090, val loss: 0.5435007810592651
Epoch 1100, training loss: 879.9570922851562 = 0.5064678192138672 + 100.0 * 8.794506072998047
Epoch 1100, val loss: 0.5412041544914246
Epoch 1110, training loss: 880.5798950195312 = 0.5036463737487793 + 100.0 * 8.800762176513672
Epoch 1110, val loss: 0.5388116836547852
Epoch 1120, training loss: 880.8668823242188 = 0.5008852481842041 + 100.0 * 8.803659439086914
Epoch 1120, val loss: 0.5364585518836975
Epoch 1130, training loss: 881.097412109375 = 0.49826255440711975 + 100.0 * 8.805991172790527
Epoch 1130, val loss: 0.5343044996261597
Epoch 1140, training loss: 881.5053100585938 = 0.4957224428653717 + 100.0 * 8.81009578704834
Epoch 1140, val loss: 0.532284140586853
Epoch 1150, training loss: 881.3308715820312 = 0.4931396245956421 + 100.0 * 8.808377265930176
Epoch 1150, val loss: 0.5302150845527649
Epoch 1160, training loss: 881.8944091796875 = 0.4907572567462921 + 100.0 * 8.81403636932373
Epoch 1160, val loss: 0.5283388495445251
Epoch 1170, training loss: 882.4130859375 = 0.4884616732597351 + 100.0 * 8.819246292114258
Epoch 1170, val loss: 0.5265271067619324
Epoch 1180, training loss: 882.2432861328125 = 0.48613351583480835 + 100.0 * 8.817571640014648
Epoch 1180, val loss: 0.5246453881263733
Epoch 1190, training loss: 882.19921875 = 0.4838266968727112 + 100.0 * 8.817153930664062
Epoch 1190, val loss: 0.5228760242462158
Epoch 1200, training loss: 882.4122314453125 = 0.4816860854625702 + 100.0 * 8.819305419921875
Epoch 1200, val loss: 0.5212775468826294
Epoch 1210, training loss: 882.953125 = 0.47968512773513794 + 100.0 * 8.824734687805176
Epoch 1210, val loss: 0.5198054909706116
Epoch 1220, training loss: 883.3306884765625 = 0.4776693284511566 + 100.0 * 8.828530311584473
Epoch 1220, val loss: 0.5182867646217346
Epoch 1230, training loss: 883.3192749023438 = 0.4756855070590973 + 100.0 * 8.828435897827148
Epoch 1230, val loss: 0.5167026519775391
Epoch 1240, training loss: 883.0150146484375 = 0.4737761914730072 + 100.0 * 8.82541275024414
Epoch 1240, val loss: 0.5154717564582825
Epoch 1250, training loss: 883.7612915039062 = 0.47196972370147705 + 100.0 * 8.832893371582031
Epoch 1250, val loss: 0.514058530330658
Epoch 1260, training loss: 884.2422485351562 = 0.4701586961746216 + 100.0 * 8.83772087097168
Epoch 1260, val loss: 0.5127325654029846
Epoch 1270, training loss: 884.2295532226562 = 0.46834370493888855 + 100.0 * 8.83761215209961
Epoch 1270, val loss: 0.5115247964859009
Epoch 1280, training loss: 884.202392578125 = 0.46654239296913147 + 100.0 * 8.837358474731445
Epoch 1280, val loss: 0.5102751851081848
Epoch 1290, training loss: 884.6942749023438 = 0.46487465500831604 + 100.0 * 8.842293739318848
Epoch 1290, val loss: 0.5090645551681519
Epoch 1300, training loss: 883.5808715820312 = 0.4630582928657532 + 100.0 * 8.831177711486816
Epoch 1300, val loss: 0.507866382598877
Epoch 1310, training loss: 883.8613891601562 = 0.4614526629447937 + 100.0 * 8.833999633789062
Epoch 1310, val loss: 0.506597101688385
Epoch 1320, training loss: 883.5812377929688 = 0.459932416677475 + 100.0 * 8.831212997436523
Epoch 1320, val loss: 0.5058087706565857
Epoch 1330, training loss: 884.2202758789062 = 0.45848193764686584 + 100.0 * 8.837617874145508
Epoch 1330, val loss: 0.5049670934677124
Epoch 1340, training loss: 885.0989990234375 = 0.4571157693862915 + 100.0 * 8.846419334411621
Epoch 1340, val loss: 0.5040696263313293
Epoch 1350, training loss: 885.265380859375 = 0.4556230902671814 + 100.0 * 8.848097801208496
Epoch 1350, val loss: 0.5031125545501709
Epoch 1360, training loss: 885.5972290039062 = 0.45420053601264954 + 100.0 * 8.85142993927002
Epoch 1360, val loss: 0.5022547841072083
Epoch 1370, training loss: 886.2503051757812 = 0.45280954241752625 + 100.0 * 8.857975006103516
Epoch 1370, val loss: 0.5013986229896545
Epoch 1380, training loss: 886.350341796875 = 0.4514571726322174 + 100.0 * 8.858988761901855
Epoch 1380, val loss: 0.5005814433097839
Epoch 1390, training loss: 885.4752197265625 = 0.4500458538532257 + 100.0 * 8.850251197814941
Epoch 1390, val loss: 0.4997394382953644
Epoch 1400, training loss: 885.7060546875 = 0.44873809814453125 + 100.0 * 8.85257339477539
Epoch 1400, val loss: 0.4989300072193146
Epoch 1410, training loss: 886.2722778320312 = 0.4474949538707733 + 100.0 * 8.858247756958008
Epoch 1410, val loss: 0.49823102355003357
Epoch 1420, training loss: 886.3507080078125 = 0.4462414085865021 + 100.0 * 8.859045028686523
Epoch 1420, val loss: 0.49750998616218567
Epoch 1430, training loss: 886.89892578125 = 0.4449937045574188 + 100.0 * 8.86453914642334
Epoch 1430, val loss: 0.49673160910606384
Epoch 1440, training loss: 886.603271484375 = 0.4437006711959839 + 100.0 * 8.861595153808594
Epoch 1440, val loss: 0.4960503578186035
Epoch 1450, training loss: 886.9976806640625 = 0.4424981474876404 + 100.0 * 8.865551948547363
Epoch 1450, val loss: 0.49536579847335815
Epoch 1460, training loss: 887.2686157226562 = 0.4413444697856903 + 100.0 * 8.86827278137207
Epoch 1460, val loss: 0.49467650055885315
Epoch 1470, training loss: 887.1556396484375 = 0.44010332226753235 + 100.0 * 8.867155075073242
Epoch 1470, val loss: 0.49404457211494446
Epoch 1480, training loss: 887.632568359375 = 0.4389646351337433 + 100.0 * 8.871935844421387
Epoch 1480, val loss: 0.4933365285396576
Epoch 1490, training loss: 887.9860229492188 = 0.4378400444984436 + 100.0 * 8.875481605529785
Epoch 1490, val loss: 0.49267783761024475
Epoch 1500, training loss: 887.4639892578125 = 0.4366622567176819 + 100.0 * 8.87027359008789
Epoch 1500, val loss: 0.4923175871372223
Epoch 1510, training loss: 887.66259765625 = 0.43555697798728943 + 100.0 * 8.872270584106445
Epoch 1510, val loss: 0.4914492070674896
Epoch 1520, training loss: 888.0867309570312 = 0.43444350361824036 + 100.0 * 8.8765230178833
Epoch 1520, val loss: 0.4909006357192993
Epoch 1530, training loss: 888.141845703125 = 0.4333718419075012 + 100.0 * 8.877084732055664
Epoch 1530, val loss: 0.4903961718082428
Epoch 1540, training loss: 888.3822021484375 = 0.4323582351207733 + 100.0 * 8.879498481750488
Epoch 1540, val loss: 0.4898483157157898
Epoch 1550, training loss: 888.5755615234375 = 0.4313052296638489 + 100.0 * 8.881442070007324
Epoch 1550, val loss: 0.4893202483654022
Epoch 1560, training loss: 888.831787109375 = 0.430276095867157 + 100.0 * 8.884015083312988
Epoch 1560, val loss: 0.48880189657211304
Epoch 1570, training loss: 888.414306640625 = 0.42923110723495483 + 100.0 * 8.879850387573242
Epoch 1570, val loss: 0.4882759749889374
Epoch 1580, training loss: 888.6303100585938 = 0.4282384514808655 + 100.0 * 8.882020950317383
Epoch 1580, val loss: 0.48777371644973755
Epoch 1590, training loss: 889.0869140625 = 0.4272492825984955 + 100.0 * 8.8865966796875
Epoch 1590, val loss: 0.4873761832714081
Epoch 1600, training loss: 889.1153564453125 = 0.4262169599533081 + 100.0 * 8.88689136505127
Epoch 1600, val loss: 0.4868137240409851
Epoch 1610, training loss: 889.0797729492188 = 0.42520850896835327 + 100.0 * 8.886545181274414
Epoch 1610, val loss: 0.4863959848880768
Epoch 1620, training loss: 889.4515380859375 = 0.42422863841056824 + 100.0 * 8.890273094177246
Epoch 1620, val loss: 0.48601922392845154
Epoch 1630, training loss: 888.9003295898438 = 0.4232341945171356 + 100.0 * 8.884771347045898
Epoch 1630, val loss: 0.48537129163742065
Epoch 1640, training loss: 889.1456909179688 = 0.4222945272922516 + 100.0 * 8.88723373413086
Epoch 1640, val loss: 0.4850098788738251
Epoch 1650, training loss: 889.7969360351562 = 0.421344518661499 + 100.0 * 8.893755912780762
Epoch 1650, val loss: 0.4846186637878418
Epoch 1660, training loss: 890.0087280273438 = 0.4203509986400604 + 100.0 * 8.895883560180664
Epoch 1660, val loss: 0.4841657280921936
Epoch 1670, training loss: 890.1390380859375 = 0.4193970263004303 + 100.0 * 8.897196769714355
Epoch 1670, val loss: 0.48376530408859253
Epoch 1680, training loss: 889.8547973632812 = 0.4184392988681793 + 100.0 * 8.894363403320312
Epoch 1680, val loss: 0.483410507440567
Epoch 1690, training loss: 890.3756103515625 = 0.4175323247909546 + 100.0 * 8.899580955505371
Epoch 1690, val loss: 0.4830421507358551
Epoch 1700, training loss: 890.423828125 = 0.41660916805267334 + 100.0 * 8.90007209777832
Epoch 1700, val loss: 0.4826410710811615
Epoch 1710, training loss: 890.4765014648438 = 0.4156813621520996 + 100.0 * 8.90060806274414
Epoch 1710, val loss: 0.48232123255729675
Epoch 1720, training loss: 890.3345336914062 = 0.4147896468639374 + 100.0 * 8.899197578430176
Epoch 1720, val loss: 0.48199227452278137
Epoch 1730, training loss: 890.6358642578125 = 0.4139081537723541 + 100.0 * 8.902219772338867
Epoch 1730, val loss: 0.48160701990127563
Epoch 1740, training loss: 890.6604614257812 = 0.41300708055496216 + 100.0 * 8.902474403381348
Epoch 1740, val loss: 0.4812639653682709
Epoch 1750, training loss: 891.0538330078125 = 0.4121472239494324 + 100.0 * 8.906416893005371
Epoch 1750, val loss: 0.4809649586677551
Epoch 1760, training loss: 891.4117431640625 = 0.41127637028694153 + 100.0 * 8.910004615783691
Epoch 1760, val loss: 0.48068967461586
Epoch 1770, training loss: 891.1470947265625 = 0.41040292382240295 + 100.0 * 8.907366752624512
Epoch 1770, val loss: 0.4802663028240204
Epoch 1780, training loss: 891.2271118164062 = 0.40952110290527344 + 100.0 * 8.908175468444824
Epoch 1780, val loss: 0.48002099990844727
Epoch 1790, training loss: 890.497802734375 = 0.4085848033428192 + 100.0 * 8.90089225769043
Epoch 1790, val loss: 0.479753315448761
Epoch 1800, training loss: 890.9647827148438 = 0.4077970087528229 + 100.0 * 8.905570030212402
Epoch 1800, val loss: 0.4794919788837433
Epoch 1810, training loss: 890.9027709960938 = 0.40682849287986755 + 100.0 * 8.904959678649902
Epoch 1810, val loss: 0.47907790541648865
Epoch 1820, training loss: 890.5574951171875 = 0.4060228168964386 + 100.0 * 8.901515007019043
Epoch 1820, val loss: 0.4788776636123657
Epoch 1830, training loss: 890.7412719726562 = 0.4051986038684845 + 100.0 * 8.903360366821289
Epoch 1830, val loss: 0.4783121347427368
Epoch 1840, training loss: 891.4483642578125 = 0.4044024348258972 + 100.0 * 8.910439491271973
Epoch 1840, val loss: 0.47815924882888794
Epoch 1850, training loss: 891.5726318359375 = 0.4035774767398834 + 100.0 * 8.911690711975098
Epoch 1850, val loss: 0.4778706729412079
Epoch 1860, training loss: 891.3790893554688 = 0.40267413854599 + 100.0 * 8.909764289855957
Epoch 1860, val loss: 0.47773340344429016
Epoch 1870, training loss: 891.5317993164062 = 0.4018712043762207 + 100.0 * 8.911299705505371
Epoch 1870, val loss: 0.47723835706710815
Epoch 1880, training loss: 891.9995727539062 = 0.40105611085891724 + 100.0 * 8.915985107421875
Epoch 1880, val loss: 0.47702670097351074
Epoch 1890, training loss: 892.4365234375 = 0.40024176239967346 + 100.0 * 8.92036247253418
Epoch 1890, val loss: 0.47680848836898804
Epoch 1900, training loss: 891.6998291015625 = 0.39936941862106323 + 100.0 * 8.913004875183105
Epoch 1900, val loss: 0.4766010642051697
Epoch 1910, training loss: 892.326416015625 = 0.3986143171787262 + 100.0 * 8.919278144836426
Epoch 1910, val loss: 0.4761975407600403
Epoch 1920, training loss: 892.84765625 = 0.3978092670440674 + 100.0 * 8.924498558044434
Epoch 1920, val loss: 0.4760648012161255
Epoch 1930, training loss: 892.8533325195312 = 0.396941214799881 + 100.0 * 8.924564361572266
Epoch 1930, val loss: 0.4757857322692871
Epoch 1940, training loss: 892.6646118164062 = 0.39611342549324036 + 100.0 * 8.922684669494629
Epoch 1940, val loss: 0.47556430101394653
Epoch 1950, training loss: 892.9135131835938 = 0.39529311656951904 + 100.0 * 8.925182342529297
Epoch 1950, val loss: 0.47536227107048035
Epoch 1960, training loss: 892.9747924804688 = 0.39449939131736755 + 100.0 * 8.925803184509277
Epoch 1960, val loss: 0.4750886857509613
Epoch 1970, training loss: 893.0911254882812 = 0.3936820328235626 + 100.0 * 8.926974296569824
Epoch 1970, val loss: 0.4748678207397461
Epoch 1980, training loss: 892.8408813476562 = 0.3928481340408325 + 100.0 * 8.924480438232422
Epoch 1980, val loss: 0.4746008813381195
Epoch 1990, training loss: 893.1473999023438 = 0.3920651972293854 + 100.0 * 8.927553176879883
Epoch 1990, val loss: 0.47440215945243835
Epoch 2000, training loss: 893.4956665039062 = 0.39127349853515625 + 100.0 * 8.93104362487793
Epoch 2000, val loss: 0.47413119673728943
Epoch 2010, training loss: 893.397216796875 = 0.3904436528682709 + 100.0 * 8.930068016052246
Epoch 2010, val loss: 0.47400400042533875
Epoch 2020, training loss: 893.4631958007812 = 0.38965263962745667 + 100.0 * 8.93073558807373
Epoch 2020, val loss: 0.4736204147338867
Epoch 2030, training loss: 893.9462890625 = 0.38886359333992004 + 100.0 * 8.935574531555176
Epoch 2030, val loss: 0.47350966930389404
Epoch 2040, training loss: 893.9285278320312 = 0.3880466818809509 + 100.0 * 8.935404777526855
Epoch 2040, val loss: 0.47325921058654785
Epoch 2050, training loss: 894.1585083007812 = 0.3872588276863098 + 100.0 * 8.937712669372559
Epoch 2050, val loss: 0.47305062413215637
Epoch 2060, training loss: 892.1118774414062 = 0.38640478253364563 + 100.0 * 8.917254447937012
Epoch 2060, val loss: 0.472928911447525
Epoch 2070, training loss: 892.5479736328125 = 0.38561221957206726 + 100.0 * 8.921623229980469
Epoch 2070, val loss: 0.472476601600647
Epoch 2080, training loss: 892.71337890625 = 0.3850328028202057 + 100.0 * 8.923283576965332
Epoch 2080, val loss: 0.47174757719039917
Epoch 2090, training loss: 894.3377685546875 = 0.3846651613712311 + 100.0 * 8.939531326293945
Epoch 2090, val loss: 0.4720189571380615
Epoch 2100, training loss: 892.0424194335938 = 0.38372138142585754 + 100.0 * 8.916586875915527
Epoch 2100, val loss: 0.471455842256546
Epoch 2110, training loss: 893.3550415039062 = 0.3831104040145874 + 100.0 * 8.929718971252441
Epoch 2110, val loss: 0.4720664322376251
Epoch 2120, training loss: 893.4533081054688 = 0.38231250643730164 + 100.0 * 8.930709838867188
Epoch 2120, val loss: 0.4715162515640259
Epoch 2130, training loss: 893.9690551757812 = 0.38151511549949646 + 100.0 * 8.935874938964844
Epoch 2130, val loss: 0.4716606140136719
Epoch 2140, training loss: 894.6854248046875 = 0.38074612617492676 + 100.0 * 8.943046569824219
Epoch 2140, val loss: 0.4712833762168884
Epoch 2150, training loss: 895.0397338867188 = 0.3799539804458618 + 100.0 * 8.946598052978516
Epoch 2150, val loss: 0.4712553918361664
Epoch 2160, training loss: 895.2904663085938 = 0.3791436553001404 + 100.0 * 8.949112892150879
Epoch 2160, val loss: 0.47106611728668213
Epoch 2170, training loss: 895.7369384765625 = 0.37836402654647827 + 100.0 * 8.953585624694824
Epoch 2170, val loss: 0.47092586755752563
Epoch 2180, training loss: 895.6220703125 = 0.3775644600391388 + 100.0 * 8.952445030212402
Epoch 2180, val loss: 0.47069957852363586
Epoch 2190, training loss: 896.048828125 = 0.37677717208862305 + 100.0 * 8.956720352172852
Epoch 2190, val loss: 0.47062283754348755
Epoch 2200, training loss: 896.0682983398438 = 0.37597039341926575 + 100.0 * 8.956923484802246
Epoch 2200, val loss: 0.4704113304615021
Epoch 2210, training loss: 896.4833374023438 = 0.3751804828643799 + 100.0 * 8.961081504821777
Epoch 2210, val loss: 0.4703083634376526
Epoch 2220, training loss: 896.4522094726562 = 0.37437692284584045 + 100.0 * 8.96077823638916
Epoch 2220, val loss: 0.47010132670402527
Epoch 2230, training loss: 896.2533569335938 = 0.3735494315624237 + 100.0 * 8.9587984085083
Epoch 2230, val loss: 0.4698869585990906
Epoch 2240, training loss: 896.56005859375 = 0.37275129556655884 + 100.0 * 8.961873054504395
Epoch 2240, val loss: 0.4697454273700714
Epoch 2250, training loss: 896.9492797851562 = 0.37197116017341614 + 100.0 * 8.965773582458496
Epoch 2250, val loss: 0.46961885690689087
Epoch 2260, training loss: 896.9984741210938 = 0.37117645144462585 + 100.0 * 8.966273307800293
Epoch 2260, val loss: 0.4695049524307251
Epoch 2270, training loss: 897.0753173828125 = 0.3703795075416565 + 100.0 * 8.967049598693848
Epoch 2270, val loss: 0.4694032669067383
Epoch 2280, training loss: 893.4808959960938 = 0.3694181442260742 + 100.0 * 8.931114196777344
Epoch 2280, val loss: 0.4692775011062622
Epoch 2290, training loss: 893.7149047851562 = 0.3688889443874359 + 100.0 * 8.933460235595703
Epoch 2290, val loss: 0.46849027276039124
Epoch 2300, training loss: 893.9271850585938 = 0.3680858910083771 + 100.0 * 8.935590744018555
Epoch 2300, val loss: 0.46938642859458923
Epoch 2310, training loss: 893.9461059570312 = 0.36735105514526367 + 100.0 * 8.935787200927734
Epoch 2310, val loss: 0.4682314097881317
Epoch 2320, training loss: 894.6077270507812 = 0.36657774448394775 + 100.0 * 8.942411422729492
Epoch 2320, val loss: 0.4681199789047241
Epoch 2330, training loss: 895.7020874023438 = 0.36574962735176086 + 100.0 * 8.953363418579102
Epoch 2330, val loss: 0.4682832956314087
Epoch 2340, training loss: 896.3265991210938 = 0.36490339040756226 + 100.0 * 8.959616661071777
Epoch 2340, val loss: 0.46801671385765076
Epoch 2350, training loss: 896.739013671875 = 0.3641108274459839 + 100.0 * 8.963748931884766
Epoch 2350, val loss: 0.4679538607597351
Epoch 2360, training loss: 897.1831665039062 = 0.36329272389411926 + 100.0 * 8.968198776245117
Epoch 2360, val loss: 0.4678502082824707
Epoch 2370, training loss: 897.3660278320312 = 0.36248263716697693 + 100.0 * 8.970035552978516
Epoch 2370, val loss: 0.4676865041255951
Epoch 2380, training loss: 897.4268188476562 = 0.36165502667427063 + 100.0 * 8.970651626586914
Epoch 2380, val loss: 0.4675405025482178
Epoch 2390, training loss: 897.7994384765625 = 0.36085131764411926 + 100.0 * 8.974386215209961
Epoch 2390, val loss: 0.4673416316509247
Epoch 2400, training loss: 897.92724609375 = 0.36002951860427856 + 100.0 * 8.975671768188477
Epoch 2400, val loss: 0.4672682285308838
Epoch 2410, training loss: 897.9188232421875 = 0.3592240810394287 + 100.0 * 8.975595474243164
Epoch 2410, val loss: 0.46698224544525146
Epoch 2420, training loss: 898.0518188476562 = 0.3584107458591461 + 100.0 * 8.976934432983398
Epoch 2420, val loss: 0.4668442904949188
Epoch 2430, training loss: 898.5413208007812 = 0.35760489106178284 + 100.0 * 8.981837272644043
Epoch 2430, val loss: 0.4666767120361328
Epoch 2440, training loss: 897.8775634765625 = 0.356781005859375 + 100.0 * 8.975207328796387
Epoch 2440, val loss: 0.4665917158126831
Epoch 2450, training loss: 898.2058715820312 = 0.35598790645599365 + 100.0 * 8.978498458862305
Epoch 2450, val loss: 0.46641913056373596
Epoch 2460, training loss: 898.5564575195312 = 0.35518285632133484 + 100.0 * 8.982012748718262
Epoch 2460, val loss: 0.46618178486824036
Epoch 2470, training loss: 898.5821533203125 = 0.3543582856655121 + 100.0 * 8.982277870178223
Epoch 2470, val loss: 0.46611765027046204
Epoch 2480, training loss: 898.6346435546875 = 0.35353776812553406 + 100.0 * 8.982810974121094
Epoch 2480, val loss: 0.46591874957084656
Epoch 2490, training loss: 898.8330688476562 = 0.35272204875946045 + 100.0 * 8.984803199768066
Epoch 2490, val loss: 0.46584993600845337
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8210144927536231
0.8639426211693111
The final CL Acc:0.82710, 0.00770, The final GNN Acc:0.86433, 0.00139
Begin epxeriment: noisy_level: 0.2 cont_weight: 100 epoch: der1:0.3 der2:0.3 dfr1:0.4 dfr2:0.3
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=2500, cl_num_layers=2, cl_num_proj_hidden=64, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=100.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.3, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=32, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, noisy_level=0.2, num_hidden=64, num_proj_hidden=64, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=40, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 88648])
add edge: torch.Size([2, 106548])
remove edge: torch.Size([2, 70898])
updated graph: torch.Size([2, 88798])
=== Noisy graph ===
/home/project-robust-contrastive/Robust-Contrastive-Learning/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 1016.2421264648438 = 1.1095229387283325 + 100.0 * 10.151326179504395
Epoch 0, val loss: 1.111148715019226
Epoch 10, training loss: 973.9456176757812 = 1.1055413484573364 + 100.0 * 9.728401184082031
Epoch 10, val loss: 1.1071351766586304
Epoch 20, training loss: 956.9794311523438 = 1.1017441749572754 + 100.0 * 9.55877685546875
Epoch 20, val loss: 1.1033167839050293
Epoch 30, training loss: 944.3156127929688 = 1.0981372594833374 + 100.0 * 9.432174682617188
Epoch 30, val loss: 1.0996894836425781
Epoch 40, training loss: 934.5403442382812 = 1.0947096347808838 + 100.0 * 9.334456443786621
Epoch 40, val loss: 1.0962430238723755
Epoch 50, training loss: 926.4679565429688 = 1.091450810432434 + 100.0 * 9.253765106201172
Epoch 50, val loss: 1.0929676294326782
Epoch 60, training loss: 919.6107177734375 = 1.0883499383926392 + 100.0 * 9.185223579406738
Epoch 60, val loss: 1.089851975440979
Epoch 70, training loss: 913.7902221679688 = 1.0853760242462158 + 100.0 * 9.12704849243164
Epoch 70, val loss: 1.0868664979934692
Epoch 80, training loss: 908.7205810546875 = 1.08253812789917 + 100.0 * 9.076380729675293
Epoch 80, val loss: 1.0840188264846802
Epoch 90, training loss: 904.344482421875 = 1.0798115730285645 + 100.0 * 9.032646179199219
Epoch 90, val loss: 1.081285834312439
Epoch 100, training loss: 900.4668579101562 = 1.0771971940994263 + 100.0 * 8.993896484375
Epoch 100, val loss: 1.0786681175231934
Epoch 110, training loss: 897.1915893554688 = 1.074663519859314 + 100.0 * 8.961169242858887
Epoch 110, val loss: 1.0761330127716064
Epoch 120, training loss: 894.2471923828125 = 1.072248935699463 + 100.0 * 8.93174934387207
Epoch 120, val loss: 1.0737167596817017
Epoch 130, training loss: 891.5536499023438 = 1.0699154138565063 + 100.0 * 8.904837608337402
Epoch 130, val loss: 1.0713813304901123
Epoch 140, training loss: 889.2706298828125 = 1.0676649808883667 + 100.0 * 8.88202953338623
Epoch 140, val loss: 1.069129467010498
Epoch 150, training loss: 887.288818359375 = 1.0654969215393066 + 100.0 * 8.86223316192627
Epoch 150, val loss: 1.0669792890548706
Epoch 160, training loss: 885.8480224609375 = 1.0634071826934814 + 100.0 * 8.847846031188965
Epoch 160, val loss: 1.0648956298828125
Epoch 170, training loss: 884.2185668945312 = 1.0613975524902344 + 100.0 * 8.831571578979492
Epoch 170, val loss: 1.0628952980041504
Epoch 180, training loss: 882.9384155273438 = 1.0594617128372192 + 100.0 * 8.8187894821167
Epoch 180, val loss: 1.060957908630371
Epoch 190, training loss: 881.9859619140625 = 1.0575729608535767 + 100.0 * 8.809284210205078
Epoch 190, val loss: 1.0590763092041016
Epoch 200, training loss: 881.0328369140625 = 1.0557193756103516 + 100.0 * 8.799771308898926
Epoch 200, val loss: 1.0572582483291626
Epoch 210, training loss: 880.290283203125 = 1.0539357662200928 + 100.0 * 8.792363166809082
Epoch 210, val loss: 1.055465579032898
Epoch 220, training loss: 879.6373901367188 = 1.052145004272461 + 100.0 * 8.785852432250977
Epoch 220, val loss: 1.0537002086639404
Epoch 230, training loss: 878.96875 = 1.0503991842269897 + 100.0 * 8.779183387756348
Epoch 230, val loss: 1.0519565343856812
Epoch 240, training loss: 878.735107421875 = 1.0487343072891235 + 100.0 * 8.776864051818848
Epoch 240, val loss: 1.0503028631210327
Epoch 250, training loss: 878.1996459960938 = 1.0470229387283325 + 100.0 * 8.771526336669922
Epoch 250, val loss: 1.0486438274383545
Epoch 260, training loss: 877.79541015625 = 1.0453212261199951 + 100.0 * 8.767500877380371
Epoch 260, val loss: 1.046960711479187
Epoch 270, training loss: 877.3568115234375 = 1.0435744524002075 + 100.0 * 8.763132095336914
Epoch 270, val loss: 1.0452717542648315
Epoch 280, training loss: 878.7496948242188 = 1.0418128967285156 + 100.0 * 8.777078628540039
Epoch 280, val loss: 1.043605923652649
Epoch 290, training loss: 878.37255859375 = 1.040031909942627 + 100.0 * 8.773324966430664
Epoch 290, val loss: 1.0418286323547363
Epoch 300, training loss: 878.3107299804688 = 1.0382323265075684 + 100.0 * 8.772725105285645
Epoch 300, val loss: 1.0401363372802734
Epoch 310, training loss: 877.8137817382812 = 1.0362752676010132 + 100.0 * 8.767775535583496
Epoch 310, val loss: 1.0382399559020996
Epoch 320, training loss: 877.9359130859375 = 1.0342897176742554 + 100.0 * 8.76901626586914
Epoch 320, val loss: 1.0363553762435913
Epoch 330, training loss: 877.9100952148438 = 1.0322375297546387 + 100.0 * 8.768778800964355
Epoch 330, val loss: 1.0343862771987915
Epoch 340, training loss: 877.8989868164062 = 1.0301450490951538 + 100.0 * 8.768688201904297
Epoch 340, val loss: 1.0324076414108276
Epoch 350, training loss: 877.8059692382812 = 1.027843952178955 + 100.0 * 8.767781257629395
Epoch 350, val loss: 1.0302720069885254
Epoch 360, training loss: 877.896728515625 = 1.0255377292633057 + 100.0 * 8.768712043762207
Epoch 360, val loss: 1.0280625820159912
Epoch 370, training loss: 878.11328125 = 1.023112416267395 + 100.0 * 8.770901679992676
Epoch 370, val loss: 1.0257229804992676
Epoch 380, training loss: 877.9784545898438 = 1.0204626321792603 + 100.0 * 8.769579887390137
Epoch 380, val loss: 1.0232775211334229
Epoch 390, training loss: 878.161376953125 = 1.0176788568496704 + 100.0 * 8.77143669128418
Epoch 390, val loss: 1.020643711090088
Epoch 400, training loss: 877.9281616210938 = 1.0147143602371216 + 100.0 * 8.769134521484375
Epoch 400, val loss: 1.0178859233856201
Epoch 410, training loss: 878.0776977539062 = 1.0116537809371948 + 100.0 * 8.770660400390625
Epoch 410, val loss: 1.0149595737457275
Epoch 420, training loss: 878.2994384765625 = 1.0084031820297241 + 100.0 * 8.772910118103027
Epoch 420, val loss: 1.0119328498840332
Epoch 430, training loss: 878.216796875 = 1.0049463510513306 + 100.0 * 8.77211856842041
Epoch 430, val loss: 1.0086878538131714
Epoch 440, training loss: 878.3687744140625 = 1.00131356716156 + 100.0 * 8.773674011230469
Epoch 440, val loss: 1.005307912826538
Epoch 450, training loss: 878.38916015625 = 0.9975576400756836 + 100.0 * 8.773916244506836
Epoch 450, val loss: 1.00181245803833
Epoch 460, training loss: 878.8197021484375 = 0.9936527013778687 + 100.0 * 8.778260231018066
Epoch 460, val loss: 0.9981212019920349
Epoch 470, training loss: 879.0232543945312 = 0.9895067811012268 + 100.0 * 8.7803373336792
Epoch 470, val loss: 0.9942939877510071
Epoch 480, training loss: 878.9908447265625 = 0.9851715564727783 + 100.0 * 8.780056953430176
Epoch 480, val loss: 0.9902470707893372
Epoch 490, training loss: 879.0841064453125 = 0.9806429147720337 + 100.0 * 8.781034469604492
Epoch 490, val loss: 0.9860343337059021
Epoch 500, training loss: 879.144775390625 = 0.976033091545105 + 100.0 * 8.78168773651123
Epoch 500, val loss: 0.9816952347755432
Epoch 510, training loss: 879.2913208007812 = 0.9711921215057373 + 100.0 * 8.783201217651367
Epoch 510, val loss: 0.9771704077720642
Epoch 520, training loss: 879.6712036132812 = 0.9662458896636963 + 100.0 * 8.787049293518066
Epoch 520, val loss: 0.9725930094718933
Epoch 530, training loss: 879.920166015625 = 0.9609859585762024 + 100.0 * 8.789591789245605
Epoch 530, val loss: 0.9678093791007996
Epoch 540, training loss: 879.4476318359375 = 0.9555590152740479 + 100.0 * 8.784920692443848
Epoch 540, val loss: 0.9628213047981262
Epoch 550, training loss: 880.0554809570312 = 0.9502155184745789 + 100.0 * 8.79105281829834
Epoch 550, val loss: 0.9578628540039062
Epoch 560, training loss: 880.4547119140625 = 0.9446069002151489 + 100.0 * 8.795101165771484
Epoch 560, val loss: 0.9526402950286865
Epoch 570, training loss: 880.6578369140625 = 0.9388399720191956 + 100.0 * 8.797189712524414
Epoch 570, val loss: 0.9473085999488831
Epoch 580, training loss: 880.6594848632812 = 0.9328718781471252 + 100.0 * 8.797266006469727
Epoch 580, val loss: 0.9418882131576538
Epoch 590, training loss: 881.0789184570312 = 0.9268037676811218 + 100.0 * 8.801521301269531
Epoch 590, val loss: 0.9362608790397644
Epoch 600, training loss: 881.6250610351562 = 0.920481264591217 + 100.0 * 8.807045936584473
Epoch 600, val loss: 0.9306766390800476
Epoch 610, training loss: 881.5337524414062 = 0.9140812158584595 + 100.0 * 8.806197166442871
Epoch 610, val loss: 0.9243081212043762
Epoch 620, training loss: 880.923828125 = 0.9078800678253174 + 100.0 * 8.800159454345703
Epoch 620, val loss: 0.91898113489151
Epoch 630, training loss: 881.3185424804688 = 0.9018458724021912 + 100.0 * 8.804166793823242
Epoch 630, val loss: 0.9134056568145752
Epoch 640, training loss: 881.4343872070312 = 0.8952789902687073 + 100.0 * 8.805391311645508
Epoch 640, val loss: 0.9074984788894653
Epoch 650, training loss: 881.46240234375 = 0.8886937499046326 + 100.0 * 8.805737495422363
Epoch 650, val loss: 0.9015514254570007
Epoch 660, training loss: 881.939453125 = 0.8822571635246277 + 100.0 * 8.810571670532227
Epoch 660, val loss: 0.895703911781311
Epoch 670, training loss: 882.70166015625 = 0.8757261037826538 + 100.0 * 8.818259239196777
Epoch 670, val loss: 0.8897781372070312
Epoch 680, training loss: 882.5013427734375 = 0.868908703327179 + 100.0 * 8.816324234008789
Epoch 680, val loss: 0.8836649656295776
Epoch 690, training loss: 883.1228637695312 = 0.8622255921363831 + 100.0 * 8.822606086730957
Epoch 690, val loss: 0.8775809407234192
Epoch 700, training loss: 883.865966796875 = 0.8555784225463867 + 100.0 * 8.830103874206543
Epoch 700, val loss: 0.8715465664863586
Epoch 710, training loss: 883.7525634765625 = 0.84881591796875 + 100.0 * 8.8290376663208
Epoch 710, val loss: 0.865493893623352
Epoch 720, training loss: 884.0730590820312 = 0.8421107530593872 + 100.0 * 8.83230972290039
Epoch 720, val loss: 0.8595389723777771
Epoch 730, training loss: 884.0379028320312 = 0.8353095650672913 + 100.0 * 8.832025527954102
Epoch 730, val loss: 0.8532958030700684
Epoch 740, training loss: 884.1806640625 = 0.8285183906555176 + 100.0 * 8.833521842956543
Epoch 740, val loss: 0.8473313450813293
Epoch 750, training loss: 885.205078125 = 0.8221209645271301 + 100.0 * 8.843829154968262
Epoch 750, val loss: 0.8415722846984863
Epoch 760, training loss: 885.91650390625 = 0.8155208826065063 + 100.0 * 8.8510103225708
Epoch 760, val loss: 0.8356897234916687
Epoch 770, training loss: 884.0899658203125 = 0.8086836934089661 + 100.0 * 8.832813262939453
Epoch 770, val loss: 0.8297101259231567
Epoch 780, training loss: 884.919189453125 = 0.8023859262466431 + 100.0 * 8.841168403625488
Epoch 780, val loss: 0.8240527510643005
Epoch 790, training loss: 885.7073364257812 = 0.796208381652832 + 100.0 * 8.849111557006836
Epoch 790, val loss: 0.8185654282569885
Epoch 800, training loss: 885.9183959960938 = 0.7898076176643372 + 100.0 * 8.851285934448242
Epoch 800, val loss: 0.812824547290802
Epoch 810, training loss: 885.8052978515625 = 0.7834429740905762 + 100.0 * 8.850218772888184
Epoch 810, val loss: 0.8077995181083679
Epoch 820, training loss: 880.64990234375 = 0.7730646729469299 + 100.0 * 8.798768043518066
Epoch 820, val loss: 0.7976918816566467
Epoch 830, training loss: 884.9539794921875 = 0.7701869010925293 + 100.0 * 8.841837882995605
Epoch 830, val loss: 0.7958205938339233
Epoch 840, training loss: 885.6764526367188 = 0.7650119662284851 + 100.0 * 8.849114418029785
Epoch 840, val loss: 0.7908439636230469
Epoch 850, training loss: 882.6851196289062 = 0.7584536671638489 + 100.0 * 8.819266319274902
Epoch 850, val loss: 0.7854856252670288
Epoch 860, training loss: 885.0408935546875 = 0.7534046173095703 + 100.0 * 8.842874526977539
Epoch 860, val loss: 0.7810207009315491
Epoch 870, training loss: 885.1674194335938 = 0.7479878664016724 + 100.0 * 8.844194412231445
Epoch 870, val loss: 0.7764471173286438
Epoch 880, training loss: 885.9644775390625 = 0.74277663230896 + 100.0 * 8.852216720581055
Epoch 880, val loss: 0.7719016075134277
Epoch 890, training loss: 887.498291015625 = 0.7376965284347534 + 100.0 * 8.867606163024902
Epoch 890, val loss: 0.7675407528877258
Epoch 900, training loss: 887.9044189453125 = 0.7324671745300293 + 100.0 * 8.871719360351562
Epoch 900, val loss: 0.7630552053451538
Epoch 910, training loss: 888.16796875 = 0.727379322052002 + 100.0 * 8.874405860900879
Epoch 910, val loss: 0.7586461305618286
Epoch 920, training loss: 888.8690795898438 = 0.7223856449127197 + 100.0 * 8.88146686553955
Epoch 920, val loss: 0.7543426156044006
Epoch 930, training loss: 888.8759155273438 = 0.7174167037010193 + 100.0 * 8.881585121154785
Epoch 930, val loss: 0.750275731086731
Epoch 940, training loss: 889.15185546875 = 0.712609052658081 + 100.0 * 8.884392738342285
Epoch 940, val loss: 0.7461854219436646
Epoch 950, training loss: 889.7178955078125 = 0.7080224752426147 + 100.0 * 8.890098571777344
Epoch 950, val loss: 0.7423637509346008
Epoch 960, training loss: 890.0662231445312 = 0.70348060131073 + 100.0 * 8.893627166748047
Epoch 960, val loss: 0.7386221289634705
Epoch 970, training loss: 890.2596435546875 = 0.699038565158844 + 100.0 * 8.89560604095459
Epoch 970, val loss: 0.7349755167961121
Epoch 980, training loss: 889.518798828125 = 0.6944717764854431 + 100.0 * 8.888243675231934
Epoch 980, val loss: 0.7310308218002319
Epoch 990, training loss: 890.3618774414062 = 0.690538763999939 + 100.0 * 8.896713256835938
Epoch 990, val loss: 0.7279272079467773
Epoch 1000, training loss: 890.878662109375 = 0.6865355968475342 + 100.0 * 8.901921272277832
Epoch 1000, val loss: 0.7247089743614197
Epoch 1010, training loss: 891.181640625 = 0.6826338171958923 + 100.0 * 8.904990196228027
Epoch 1010, val loss: 0.721675455570221
Epoch 1020, training loss: 891.4165649414062 = 0.678760290145874 + 100.0 * 8.907378196716309
Epoch 1020, val loss: 0.7185396552085876
Epoch 1030, training loss: 891.5684204101562 = 0.6750232577323914 + 100.0 * 8.908933639526367
Epoch 1030, val loss: 0.7155671715736389
Epoch 1040, training loss: 891.0401000976562 = 0.6710794568061829 + 100.0 * 8.903690338134766
Epoch 1040, val loss: 0.7124636769294739
Epoch 1050, training loss: 891.213623046875 = 0.6674988865852356 + 100.0 * 8.905461311340332
Epoch 1050, val loss: 0.7097564339637756
Epoch 1060, training loss: 891.7559814453125 = 0.6643722653388977 + 100.0 * 8.910916328430176
Epoch 1060, val loss: 0.70735764503479
Epoch 1070, training loss: 892.1946411132812 = 0.661052942276001 + 100.0 * 8.915335655212402
Epoch 1070, val loss: 0.7049244046211243
Epoch 1080, training loss: 892.9378662109375 = 0.6578583121299744 + 100.0 * 8.922800064086914
Epoch 1080, val loss: 0.7025715112686157
Epoch 1090, training loss: 892.8548583984375 = 0.6546255350112915 + 100.0 * 8.922002792358398
Epoch 1090, val loss: 0.7001662254333496
Epoch 1100, training loss: 892.6339111328125 = 0.6514307856559753 + 100.0 * 8.919824600219727
Epoch 1100, val loss: 0.6977779865264893
Epoch 1110, training loss: 892.205078125 = 0.6482769846916199 + 100.0 * 8.915568351745605
Epoch 1110, val loss: 0.6957024931907654
Epoch 1120, training loss: 892.9912719726562 = 0.645423412322998 + 100.0 * 8.923458099365234
Epoch 1120, val loss: 0.6936209201812744
Epoch 1130, training loss: 893.648681640625 = 0.6426383852958679 + 100.0 * 8.930060386657715
Epoch 1130, val loss: 0.6917039752006531
Epoch 1140, training loss: 893.9758911132812 = 0.6399043202400208 + 100.0 * 8.93336009979248
Epoch 1140, val loss: 0.6898570656776428
Epoch 1150, training loss: 894.4074096679688 = 0.6372601389884949 + 100.0 * 8.937701225280762
Epoch 1150, val loss: 0.6880420446395874
Epoch 1160, training loss: 894.57373046875 = 0.634631335735321 + 100.0 * 8.939391136169434
Epoch 1160, val loss: 0.6862952709197998
Epoch 1170, training loss: 894.6895141601562 = 0.6320405006408691 + 100.0 * 8.940574645996094
Epoch 1170, val loss: 0.6846457719802856
Epoch 1180, training loss: 894.7979736328125 = 0.6295639276504517 + 100.0 * 8.941683769226074
Epoch 1180, val loss: 0.6829777359962463
Epoch 1190, training loss: 895.0845947265625 = 0.627132773399353 + 100.0 * 8.944574356079102
Epoch 1190, val loss: 0.6815274357795715
Epoch 1200, training loss: 895.3104858398438 = 0.6247690320014954 + 100.0 * 8.946857452392578
Epoch 1200, val loss: 0.679990291595459
Epoch 1210, training loss: 895.4496459960938 = 0.6224924325942993 + 100.0 * 8.948271751403809
Epoch 1210, val loss: 0.6785982251167297
Epoch 1220, training loss: 895.5421142578125 = 0.620232343673706 + 100.0 * 8.94921875
Epoch 1220, val loss: 0.6772576570510864
Epoch 1230, training loss: 895.3848266601562 = 0.6181321144104004 + 100.0 * 8.947667121887207
Epoch 1230, val loss: 0.6762263178825378
Epoch 1240, training loss: 895.2546997070312 = 0.6158971786499023 + 100.0 * 8.946388244628906
Epoch 1240, val loss: 0.6745174527168274
Epoch 1250, training loss: 895.470458984375 = 0.6138308048248291 + 100.0 * 8.948566436767578
Epoch 1250, val loss: 0.6735685467720032
Epoch 1260, training loss: 895.8984375 = 0.611746072769165 + 100.0 * 8.952866554260254
Epoch 1260, val loss: 0.6724585294723511
Epoch 1270, training loss: 896.6620483398438 = 0.6099679470062256 + 100.0 * 8.96052074432373
Epoch 1270, val loss: 0.671414315700531
Epoch 1280, training loss: 895.7222290039062 = 0.6079900860786438 + 100.0 * 8.951142311096191
Epoch 1280, val loss: 0.6704070568084717
Epoch 1290, training loss: 895.97998046875 = 0.6060645580291748 + 100.0 * 8.953739166259766
Epoch 1290, val loss: 0.6692506670951843
Epoch 1300, training loss: 896.6279296875 = 0.6042723059654236 + 100.0 * 8.960236549377441
Epoch 1300, val loss: 0.6682981848716736
Epoch 1310, training loss: 896.9554443359375 = 0.6025031208992004 + 100.0 * 8.963529586791992
Epoch 1310, val loss: 0.6673586964607239
Epoch 1320, training loss: 897.5020141601562 = 0.600792407989502 + 100.0 * 8.969012260437012
Epoch 1320, val loss: 0.666487455368042
Epoch 1330, training loss: 897.6193237304688 = 0.5990211367607117 + 100.0 * 8.970203399658203
Epoch 1330, val loss: 0.6655574440956116
Epoch 1340, training loss: 897.4656982421875 = 0.597338080406189 + 100.0 * 8.968683242797852
Epoch 1340, val loss: 0.6647844314575195
Epoch 1350, training loss: 898.1376953125 = 0.595725417137146 + 100.0 * 8.975419998168945
Epoch 1350, val loss: 0.6639969348907471
Epoch 1360, training loss: 898.4015502929688 = 0.5940946340560913 + 100.0 * 8.978074073791504
Epoch 1360, val loss: 0.6632465124130249
Epoch 1370, training loss: 898.037109375 = 0.5924360156059265 + 100.0 * 8.974447250366211
Epoch 1370, val loss: 0.6625193953514099
Epoch 1380, training loss: 898.5254516601562 = 0.59092777967453 + 100.0 * 8.979345321655273
Epoch 1380, val loss: 0.6617587208747864
Epoch 1390, training loss: 898.877685546875 = 0.5894043445587158 + 100.0 * 8.982882499694824
Epoch 1390, val loss: 0.6611177921295166
Epoch 1400, training loss: 899.0941772460938 = 0.5878998637199402 + 100.0 * 8.985062599182129
Epoch 1400, val loss: 0.6604049205780029
Epoch 1410, training loss: 899.0867309570312 = 0.5863602161407471 + 100.0 * 8.985003471374512
Epoch 1410, val loss: 0.6596659421920776
Epoch 1420, training loss: 899.6292724609375 = 0.5849368572235107 + 100.0 * 8.990443229675293
Epoch 1420, val loss: 0.6590551137924194
Epoch 1430, training loss: 899.8356323242188 = 0.5835248827934265 + 100.0 * 8.992521286010742
Epoch 1430, val loss: 0.658454954624176
Epoch 1440, training loss: 900.2215576171875 = 0.5821283459663391 + 100.0 * 8.996394157409668
Epoch 1440, val loss: 0.6578621864318848
Epoch 1450, training loss: 900.0630493164062 = 0.5806807279586792 + 100.0 * 8.994823455810547
Epoch 1450, val loss: 0.6572703719139099
Epoch 1460, training loss: 900.1030883789062 = 0.5793158411979675 + 100.0 * 8.995237350463867
Epoch 1460, val loss: 0.6566964983940125
Epoch 1470, training loss: 900.3326416015625 = 0.5779964923858643 + 100.0 * 8.997546195983887
Epoch 1470, val loss: 0.6561565399169922
Epoch 1480, training loss: 900.0956420898438 = 0.5766504406929016 + 100.0 * 8.995189666748047
Epoch 1480, val loss: 0.6556732058525085
Epoch 1490, training loss: 894.2169799804688 = 0.5748211741447449 + 100.0 * 8.936421394348145
Epoch 1490, val loss: 0.6548548936843872
Epoch 1500, training loss: 895.3362426757812 = 0.5739297866821289 + 100.0 * 8.947623252868652
Epoch 1500, val loss: 0.6544429063796997
Epoch 1510, training loss: 896.8816528320312 = 0.5726543664932251 + 100.0 * 8.963089942932129
Epoch 1510, val loss: 0.6536359190940857
Epoch 1520, training loss: 895.7276611328125 = 0.5712801218032837 + 100.0 * 8.951563835144043
Epoch 1520, val loss: 0.6532984375953674
Epoch 1530, training loss: 897.6443481445312 = 0.5702256560325623 + 100.0 * 8.970741271972656
Epoch 1530, val loss: 0.6526681184768677
Epoch 1540, training loss: 898.1458129882812 = 0.5690503120422363 + 100.0 * 8.975768089294434
Epoch 1540, val loss: 0.6523902416229248
Epoch 1550, training loss: 899.3247680664062 = 0.5679816007614136 + 100.0 * 8.987567901611328
Epoch 1550, val loss: 0.6521799564361572
Epoch 1560, training loss: 899.809326171875 = 0.5668084621429443 + 100.0 * 8.992424964904785
Epoch 1560, val loss: 0.6515828371047974
Epoch 1570, training loss: 900.1922607421875 = 0.5656296610832214 + 100.0 * 8.99626636505127
Epoch 1570, val loss: 0.6513376832008362
Epoch 1580, training loss: 900.9757690429688 = 0.5644575357437134 + 100.0 * 9.00411319732666
Epoch 1580, val loss: 0.6508005261421204
Epoch 1590, training loss: 901.2158203125 = 0.5632936954498291 + 100.0 * 9.006525039672852
Epoch 1590, val loss: 0.650429904460907
Epoch 1600, training loss: 901.1337890625 = 0.5620843768119812 + 100.0 * 9.005717277526855
Epoch 1600, val loss: 0.650005042552948
Epoch 1610, training loss: 901.4334106445312 = 0.5609069466590881 + 100.0 * 9.0087251663208
Epoch 1610, val loss: 0.6495499610900879
Epoch 1620, training loss: 901.8623657226562 = 0.55972820520401 + 100.0 * 9.013026237487793
Epoch 1620, val loss: 0.64918053150177
Epoch 1630, training loss: 902.3533935546875 = 0.5587618947029114 + 100.0 * 9.017946243286133
Epoch 1630, val loss: 0.6490804553031921
Epoch 1640, training loss: 902.292236328125 = 0.5574154257774353 + 100.0 * 9.017348289489746
Epoch 1640, val loss: 0.6482991576194763
Epoch 1650, training loss: 901.9215087890625 = 0.5561414361000061 + 100.0 * 9.013653755187988
Epoch 1650, val loss: 0.6479300260543823
Epoch 1660, training loss: 901.9218139648438 = 0.555019736289978 + 100.0 * 9.013668060302734
Epoch 1660, val loss: 0.6474190354347229
Epoch 1670, training loss: 902.5001831054688 = 0.5538639426231384 + 100.0 * 9.019462585449219
Epoch 1670, val loss: 0.6470551490783691
Epoch 1680, training loss: 902.986572265625 = 0.5526930093765259 + 100.0 * 9.024338722229004
Epoch 1680, val loss: 0.6466670632362366
Epoch 1690, training loss: 902.6056518554688 = 0.5514657497406006 + 100.0 * 9.02054214477539
Epoch 1690, val loss: 0.6462370157241821
Epoch 1700, training loss: 902.8743896484375 = 0.5502739548683167 + 100.0 * 9.02324104309082
Epoch 1700, val loss: 0.6457494497299194
Epoch 1710, training loss: 903.4124145507812 = 0.54913330078125 + 100.0 * 9.028633117675781
Epoch 1710, val loss: 0.6453146934509277
Epoch 1720, training loss: 903.2952270507812 = 0.5478941798210144 + 100.0 * 9.027473449707031
Epoch 1720, val loss: 0.6448374390602112
Epoch 1730, training loss: 903.7559204101562 = 0.5466709733009338 + 100.0 * 9.032092094421387
Epoch 1730, val loss: 0.6443480253219604
Epoch 1740, training loss: 903.9702758789062 = 0.5455028414726257 + 100.0 * 9.034247398376465
Epoch 1740, val loss: 0.6438543200492859
Epoch 1750, training loss: 903.2586669921875 = 0.544247031211853 + 100.0 * 9.027144432067871
Epoch 1750, val loss: 0.6434532403945923
Epoch 1760, training loss: 903.8074340820312 = 0.5430575609207153 + 100.0 * 9.032644271850586
Epoch 1760, val loss: 0.6428150534629822
Epoch 1770, training loss: 904.0741577148438 = 0.5418677926063538 + 100.0 * 9.035323143005371
Epoch 1770, val loss: 0.6425148248672485
Epoch 1780, training loss: 904.6951293945312 = 0.5406977534294128 + 100.0 * 9.041543960571289
Epoch 1780, val loss: 0.6418833136558533
Epoch 1790, training loss: 904.6610717773438 = 0.539480447769165 + 100.0 * 9.041215896606445
Epoch 1790, val loss: 0.6414688229560852
Epoch 1800, training loss: 904.9879150390625 = 0.5382991433143616 + 100.0 * 9.044496536254883
Epoch 1800, val loss: 0.64105623960495
Epoch 1810, training loss: 905.0015258789062 = 0.5371472835540771 + 100.0 * 9.04464340209961
Epoch 1810, val loss: 0.6405954360961914
Epoch 1820, training loss: 904.9257202148438 = 0.5359647870063782 + 100.0 * 9.04389762878418
Epoch 1820, val loss: 0.6401238441467285
Epoch 1830, training loss: 904.9822998046875 = 0.5347331166267395 + 100.0 * 9.044475555419922
Epoch 1830, val loss: 0.6397228240966797
Epoch 1840, training loss: 905.1446533203125 = 0.5336074233055115 + 100.0 * 9.046110153198242
Epoch 1840, val loss: 0.6393268704414368
Epoch 1850, training loss: 905.264892578125 = 0.5324784517288208 + 100.0 * 9.047324180603027
Epoch 1850, val loss: 0.6389970183372498
Epoch 1860, training loss: 905.4669799804688 = 0.5313051342964172 + 100.0 * 9.049356460571289
Epoch 1860, val loss: 0.6385192275047302
Epoch 1870, training loss: 905.6498413085938 = 0.5301503539085388 + 100.0 * 9.051197052001953
Epoch 1870, val loss: 0.6381226181983948
Epoch 1880, training loss: 905.7914428710938 = 0.5289916396141052 + 100.0 * 9.052624702453613
Epoch 1880, val loss: 0.6376675367355347
Epoch 1890, training loss: 905.9349975585938 = 0.5278318524360657 + 100.0 * 9.054071426391602
Epoch 1890, val loss: 0.6373321413993835
Epoch 1900, training loss: 905.9118041992188 = 0.5266992449760437 + 100.0 * 9.053851127624512
Epoch 1900, val loss: 0.636843740940094
Epoch 1910, training loss: 906.2907104492188 = 0.5255475044250488 + 100.0 * 9.05765151977539
Epoch 1910, val loss: 0.6365529894828796
Epoch 1920, training loss: 906.4530029296875 = 0.524393618106842 + 100.0 * 9.059286117553711
Epoch 1920, val loss: 0.6360617280006409
Epoch 1930, training loss: 906.2528076171875 = 0.5232280492782593 + 100.0 * 9.057295799255371
Epoch 1930, val loss: 0.6357468366622925
Epoch 1940, training loss: 906.0944213867188 = 0.5220322608947754 + 100.0 * 9.055724143981934
Epoch 1940, val loss: 0.6351563334465027
Epoch 1950, training loss: 906.6505126953125 = 0.5209442973136902 + 100.0 * 9.061295509338379
Epoch 1950, val loss: 0.6349958181381226
Epoch 1960, training loss: 906.9942016601562 = 0.5198046565055847 + 100.0 * 9.064743995666504
Epoch 1960, val loss: 0.6345521807670593
Epoch 1970, training loss: 905.6072998046875 = 0.5185986757278442 + 100.0 * 9.050887107849121
Epoch 1970, val loss: 0.634270429611206
Epoch 1980, training loss: 904.508056640625 = 0.5173933506011963 + 100.0 * 9.03990650177002
Epoch 1980, val loss: 0.6336620450019836
Epoch 1990, training loss: 905.2180786132812 = 0.516190767288208 + 100.0 * 9.047019004821777
Epoch 1990, val loss: 0.6325961351394653
Epoch 2000, training loss: 902.331298828125 = 0.5144877433776855 + 100.0 * 9.018168449401855
Epoch 2000, val loss: 0.6323151588439941
Epoch 2010, training loss: 903.4021606445312 = 0.5135228037834167 + 100.0 * 9.028885841369629
Epoch 2010, val loss: 0.6318296790122986
Epoch 2020, training loss: 904.7276000976562 = 0.5124133229255676 + 100.0 * 9.042152404785156
Epoch 2020, val loss: 0.6316764950752258
Epoch 2030, training loss: 905.6716918945312 = 0.5113738179206848 + 100.0 * 9.051603317260742
Epoch 2030, val loss: 0.6315704584121704
Epoch 2040, training loss: 906.5146484375 = 0.5103136301040649 + 100.0 * 9.060043334960938
Epoch 2040, val loss: 0.631093442440033
Epoch 2050, training loss: 907.0721435546875 = 0.5091512203216553 + 100.0 * 9.065629959106445
Epoch 2050, val loss: 0.6308590173721313
Epoch 2060, training loss: 907.1917724609375 = 0.507940411567688 + 100.0 * 9.066838264465332
Epoch 2060, val loss: 0.6304889917373657
Epoch 2070, training loss: 907.2545776367188 = 0.5067519545555115 + 100.0 * 9.06747817993164
Epoch 2070, val loss: 0.6301822066307068
Epoch 2080, training loss: 907.65478515625 = 0.5055775046348572 + 100.0 * 9.071492195129395
Epoch 2080, val loss: 0.629838764667511
Epoch 2090, training loss: 907.7672119140625 = 0.5043854117393494 + 100.0 * 9.072628021240234
Epoch 2090, val loss: 0.629505455493927
Epoch 2100, training loss: 907.8187255859375 = 0.5031639933586121 + 100.0 * 9.073155403137207
Epoch 2100, val loss: 0.6291805505752563
Epoch 2110, training loss: 907.9945678710938 = 0.5019912719726562 + 100.0 * 9.074925422668457
Epoch 2110, val loss: 0.6288625597953796
Epoch 2120, training loss: 908.2587280273438 = 0.5008227825164795 + 100.0 * 9.077579498291016
Epoch 2120, val loss: 0.6285187602043152
Epoch 2130, training loss: 908.1184692382812 = 0.49960437417030334 + 100.0 * 9.076188087463379
Epoch 2130, val loss: 0.6282196640968323
Epoch 2140, training loss: 908.1110229492188 = 0.4983943700790405 + 100.0 * 9.076126098632812
Epoch 2140, val loss: 0.6278997659683228
Epoch 2150, training loss: 908.0513305664062 = 0.4971962869167328 + 100.0 * 9.075541496276855
Epoch 2150, val loss: 0.627646267414093
Epoch 2160, training loss: 908.6893310546875 = 0.4960273206233978 + 100.0 * 9.08193302154541
Epoch 2160, val loss: 0.6272681951522827
Epoch 2170, training loss: 908.7384643554688 = 0.49479660391807556 + 100.0 * 9.082436561584473
Epoch 2170, val loss: 0.6269723176956177
Epoch 2180, training loss: 908.7274169921875 = 0.49357104301452637 + 100.0 * 9.082338333129883
Epoch 2180, val loss: 0.6266749501228333
Epoch 2190, training loss: 908.94775390625 = 0.4923735558986664 + 100.0 * 9.084553718566895
Epoch 2190, val loss: 0.6263877749443054
Epoch 2200, training loss: 907.8935546875 = 0.49102509021759033 + 100.0 * 9.07402515411377
Epoch 2200, val loss: 0.6259159445762634
Epoch 2210, training loss: 908.2492065429688 = 0.48979583382606506 + 100.0 * 9.077593803405762
Epoch 2210, val loss: 0.6256605982780457
Epoch 2220, training loss: 908.99072265625 = 0.4886195659637451 + 100.0 * 9.085021018981934
Epoch 2220, val loss: 0.625262975692749
Epoch 2230, training loss: 909.2144165039062 = 0.487423300743103 + 100.0 * 9.08726978302002
Epoch 2230, val loss: 0.6249538064002991
Epoch 2240, training loss: 908.9143676757812 = 0.486173152923584 + 100.0 * 9.084281921386719
Epoch 2240, val loss: 0.6246758699417114
Epoch 2250, training loss: 908.7171020507812 = 0.4848048985004425 + 100.0 * 9.08232307434082
Epoch 2250, val loss: 0.6242164969444275
Epoch 2260, training loss: 909.1337890625 = 0.4835776388645172 + 100.0 * 9.086502075195312
Epoch 2260, val loss: 0.6240523457527161
Epoch 2270, training loss: 909.7077026367188 = 0.4823930561542511 + 100.0 * 9.092252731323242
Epoch 2270, val loss: 0.6235926747322083
Epoch 2280, training loss: 909.8157348632812 = 0.4811360538005829 + 100.0 * 9.093345642089844
Epoch 2280, val loss: 0.6233668327331543
Epoch 2290, training loss: 909.6446533203125 = 0.47988155484199524 + 100.0 * 9.09164810180664
Epoch 2290, val loss: 0.6230810284614563
Epoch 2300, training loss: 909.8028564453125 = 0.4785989820957184 + 100.0 * 9.093242645263672
Epoch 2300, val loss: 0.6227613687515259
Epoch 2310, training loss: 909.8676147460938 = 0.47733908891677856 + 100.0 * 9.093902587890625
Epoch 2310, val loss: 0.6225087642669678
Epoch 2320, training loss: 910.1976318359375 = 0.47606512904167175 + 100.0 * 9.09721565246582
Epoch 2320, val loss: 0.6221650838851929
Epoch 2330, training loss: 910.3074340820312 = 0.47476625442504883 + 100.0 * 9.098326683044434
Epoch 2330, val loss: 0.6218687295913696
Epoch 2340, training loss: 910.69482421875 = 0.4734673798084259 + 100.0 * 9.102213859558105
Epoch 2340, val loss: 0.621599018573761
Epoch 2350, training loss: 910.2335815429688 = 0.47217392921447754 + 100.0 * 9.097614288330078
Epoch 2350, val loss: 0.6214078068733215
Epoch 2360, training loss: 910.1783447265625 = 0.47083306312561035 + 100.0 * 9.097075462341309
Epoch 2360, val loss: 0.6210649013519287
Epoch 2370, training loss: 910.6973876953125 = 0.4695294499397278 + 100.0 * 9.102278709411621
Epoch 2370, val loss: 0.6206740736961365
Epoch 2380, training loss: 911.2793579101562 = 0.4682150185108185 + 100.0 * 9.108111381530762
Epoch 2380, val loss: 0.6204314827919006
Epoch 2390, training loss: 910.9672241210938 = 0.46687060594558716 + 100.0 * 9.105003356933594
Epoch 2390, val loss: 0.6201093792915344
Epoch 2400, training loss: 910.534912109375 = 0.4655327796936035 + 100.0 * 9.100693702697754
Epoch 2400, val loss: 0.6198182702064514
Epoch 2410, training loss: 910.5531005859375 = 0.4640859365463257 + 100.0 * 9.100890159606934
Epoch 2410, val loss: 0.6193703413009644
Epoch 2420, training loss: 911.2440185546875 = 0.46280181407928467 + 100.0 * 9.10781192779541
Epoch 2420, val loss: 0.6191059350967407
Epoch 2430, training loss: 912.188720703125 = 0.4614705741405487 + 100.0 * 9.11727237701416
Epoch 2430, val loss: 0.6188175678253174
Epoch 2440, training loss: 912.830078125 = 0.4601404070854187 + 100.0 * 9.123699188232422
Epoch 2440, val loss: 0.6183820962905884
Epoch 2450, training loss: 912.3695678710938 = 0.4586748480796814 + 100.0 * 9.119109153747559
Epoch 2450, val loss: 0.6181653738021851
Epoch 2460, training loss: 911.894775390625 = 0.4572853446006775 + 100.0 * 9.114375114440918
Epoch 2460, val loss: 0.6177738308906555
Epoch 2470, training loss: 912.5929565429688 = 0.45592236518859863 + 100.0 * 9.121370315551758
Epoch 2470, val loss: 0.6174618005752563
Epoch 2480, training loss: 913.0201416015625 = 0.4545546770095825 + 100.0 * 9.125656127929688
Epoch 2480, val loss: 0.6171832084655762
Epoch 2490, training loss: 913.1376342773438 = 0.45316705107688904 + 100.0 * 9.12684440612793
Epoch 2490, val loss: 0.6169648766517639
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7613043478260869
0.8185901615590815
=== training gcn model ===
Epoch 0, training loss: 1016.5180053710938 = 1.1096556186676025 + 100.0 * 10.154083251953125
Epoch 0, val loss: 1.1107832193374634
Epoch 10, training loss: 974.6310424804688 = 1.1057626008987427 + 100.0 * 9.735252380371094
Epoch 10, val loss: 1.1069121360778809
Epoch 20, training loss: 957.250244140625 = 1.1021140813827515 + 100.0 * 9.561481475830078
Epoch 20, val loss: 1.1032650470733643
Epoch 30, training loss: 944.0325317382812 = 1.0985987186431885 + 100.0 * 9.429339408874512
Epoch 30, val loss: 1.099757194519043
Epoch 40, training loss: 933.5897827148438 = 1.0952439308166504 + 100.0 * 9.324945449829102
Epoch 40, val loss: 1.0964046716690063
Epoch 50, training loss: 925.1112670898438 = 1.092043399810791 + 100.0 * 9.240192413330078
Epoch 50, val loss: 1.0932036638259888
Epoch 60, training loss: 917.9969482421875 = 1.0889651775360107 + 100.0 * 9.169079780578613
Epoch 60, val loss: 1.0901216268539429
Epoch 70, training loss: 912.0211181640625 = 1.0860031843185425 + 100.0 * 9.10935115814209
Epoch 70, val loss: 1.0871583223342896
Epoch 80, training loss: 906.8986206054688 = 1.0831676721572876 + 100.0 * 9.058154106140137
Epoch 80, val loss: 1.0843216180801392
Epoch 90, training loss: 902.5086669921875 = 1.0804301500320435 + 100.0 * 9.0142822265625
Epoch 90, val loss: 1.0815799236297607
Epoch 100, training loss: 898.66552734375 = 1.077805757522583 + 100.0 * 8.975876808166504
Epoch 100, val loss: 1.0789533853530884
Epoch 110, training loss: 895.4447631835938 = 1.0752860307693481 + 100.0 * 8.943695068359375
Epoch 110, val loss: 1.0764296054840088
Epoch 120, training loss: 892.7810668945312 = 1.072882056236267 + 100.0 * 8.917081832885742
Epoch 120, val loss: 1.0740245580673218
Epoch 130, training loss: 890.4005126953125 = 1.0705938339233398 + 100.0 * 8.893299102783203
Epoch 130, val loss: 1.0717239379882812
Epoch 140, training loss: 888.3191528320312 = 1.0684064626693726 + 100.0 * 8.872507095336914
Epoch 140, val loss: 1.0695308446884155
Epoch 150, training loss: 886.5034790039062 = 1.0663405656814575 + 100.0 * 8.854371070861816
Epoch 150, val loss: 1.067456841468811
Epoch 160, training loss: 884.92138671875 = 1.0643606185913086 + 100.0 * 8.838570594787598
Epoch 160, val loss: 1.0654648542404175
Epoch 170, training loss: 883.5199584960938 = 1.0624967813491821 + 100.0 * 8.82457447052002
Epoch 170, val loss: 1.0635796785354614
Epoch 180, training loss: 882.332763671875 = 1.060716986656189 + 100.0 * 8.81272029876709
Epoch 180, val loss: 1.0617921352386475
Epoch 190, training loss: 881.3948364257812 = 1.0590455532073975 + 100.0 * 8.80335807800293
Epoch 190, val loss: 1.060113787651062
Epoch 200, training loss: 880.1954956054688 = 1.0574482679367065 + 100.0 * 8.791380882263184
Epoch 200, val loss: 1.058510184288025
Epoch 210, training loss: 879.4156494140625 = 1.055958867073059 + 100.0 * 8.783596992492676
Epoch 210, val loss: 1.0569950342178345
Epoch 220, training loss: 878.7222900390625 = 1.0545392036437988 + 100.0 * 8.776677131652832
Epoch 220, val loss: 1.0555652379989624
Epoch 230, training loss: 878.151611328125 = 1.0531507730484009 + 100.0 * 8.770984649658203
Epoch 230, val loss: 1.0541470050811768
Epoch 240, training loss: 877.6024780273438 = 1.0518113374710083 + 100.0 * 8.765506744384766
Epoch 240, val loss: 1.0528295040130615
Epoch 250, training loss: 876.95458984375 = 1.0505343675613403 + 100.0 * 8.759040832519531
Epoch 250, val loss: 1.051553726196289
Epoch 260, training loss: 876.5659790039062 = 1.0492759943008423 + 100.0 * 8.755167007446289
Epoch 260, val loss: 1.0502820014953613
Epoch 270, training loss: 876.3539428710938 = 1.0480355024337769 + 100.0 * 8.753059387207031
Epoch 270, val loss: 1.0490679740905762
Epoch 280, training loss: 876.302490234375 = 1.0468353033065796 + 100.0 * 8.752556800842285
Epoch 280, val loss: 1.0478662252426147
Epoch 290, training loss: 875.9674072265625 = 1.0455454587936401 + 100.0 * 8.749218940734863
Epoch 290, val loss: 1.0466151237487793
Epoch 300, training loss: 875.462890625 = 1.044248104095459 + 100.0 * 8.744186401367188
Epoch 300, val loss: 1.0453683137893677
Epoch 310, training loss: 875.5150146484375 = 1.042979121208191 + 100.0 * 8.744720458984375
Epoch 310, val loss: 1.0440917015075684
Epoch 320, training loss: 875.6956176757812 = 1.041628122329712 + 100.0 * 8.746540069580078
Epoch 320, val loss: 1.0428098440170288
Epoch 330, training loss: 875.1516723632812 = 1.0402023792266846 + 100.0 * 8.741114616394043
Epoch 330, val loss: 1.0414071083068848
Epoch 340, training loss: 875.2417602539062 = 1.0388346910476685 + 100.0 * 8.742029190063477
Epoch 340, val loss: 1.0401164293289185
Epoch 350, training loss: 875.1952514648438 = 1.0373332500457764 + 100.0 * 8.741579055786133
Epoch 350, val loss: 1.0386831760406494
Epoch 360, training loss: 875.7001953125 = 1.0357540845870972 + 100.0 * 8.746644020080566
Epoch 360, val loss: 1.0371445417404175
Epoch 370, training loss: 875.0064697265625 = 1.0340867042541504 + 100.0 * 8.739724159240723
Epoch 370, val loss: 1.0355722904205322
Epoch 380, training loss: 874.8527221679688 = 1.0323783159255981 + 100.0 * 8.738204002380371
Epoch 380, val loss: 1.033949613571167
Epoch 390, training loss: 874.9086303710938 = 1.0305367708206177 + 100.0 * 8.738780975341797
Epoch 390, val loss: 1.032189130783081
Epoch 400, training loss: 875.1944580078125 = 1.0286422967910767 + 100.0 * 8.741658210754395
Epoch 400, val loss: 1.0304094552993774
Epoch 410, training loss: 875.4774169921875 = 1.0265498161315918 + 100.0 * 8.744508743286133
Epoch 410, val loss: 1.028462529182434
Epoch 420, training loss: 875.3716430664062 = 1.0245951414108276 + 100.0 * 8.743470191955566
Epoch 420, val loss: 1.0265934467315674
Epoch 430, training loss: 875.1604614257812 = 1.0222595930099487 + 100.0 * 8.741381645202637
Epoch 430, val loss: 1.0243362188339233
Epoch 440, training loss: 875.1263427734375 = 1.0199291706085205 + 100.0 * 8.741064071655273
Epoch 440, val loss: 1.0221620798110962
Epoch 450, training loss: 875.6503295898438 = 1.0174957513809204 + 100.0 * 8.746328353881836
Epoch 450, val loss: 1.0199224948883057
Epoch 460, training loss: 875.7061767578125 = 1.0149434804916382 + 100.0 * 8.746912002563477
Epoch 460, val loss: 1.0174846649169922
Epoch 470, training loss: 876.1763916015625 = 1.0122780799865723 + 100.0 * 8.751641273498535
Epoch 470, val loss: 1.014991044998169
Epoch 480, training loss: 875.33837890625 = 1.0094192028045654 + 100.0 * 8.743289947509766
Epoch 480, val loss: 1.012277364730835
Epoch 490, training loss: 875.8928833007812 = 1.0065076351165771 + 100.0 * 8.748863220214844
Epoch 490, val loss: 1.0095069408416748
Epoch 500, training loss: 876.3319091796875 = 1.0034501552581787 + 100.0 * 8.753284454345703
Epoch 500, val loss: 1.0066654682159424
Epoch 510, training loss: 875.9878540039062 = 1.000156283378601 + 100.0 * 8.749876976013184
Epoch 510, val loss: 1.003592610359192
Epoch 520, training loss: 876.611328125 = 0.9968822002410889 + 100.0 * 8.756144523620605
Epoch 520, val loss: 1.0004642009735107
Epoch 530, training loss: 876.80859375 = 0.9934607744216919 + 100.0 * 8.758151054382324
Epoch 530, val loss: 0.9972218871116638
Epoch 540, training loss: 876.6819458007812 = 0.9897750020027161 + 100.0 * 8.756921768188477
Epoch 540, val loss: 0.9937693476676941
Epoch 550, training loss: 876.942626953125 = 0.9860431551933289 + 100.0 * 8.759566307067871
Epoch 550, val loss: 0.9902462959289551
Epoch 560, training loss: 877.1978149414062 = 0.982148289680481 + 100.0 * 8.76215648651123
Epoch 560, val loss: 0.9866204261779785
Epoch 570, training loss: 877.5336303710938 = 0.9782593846321106 + 100.0 * 8.76555347442627
Epoch 570, val loss: 0.9829201102256775
Epoch 580, training loss: 877.3414306640625 = 0.9739357233047485 + 100.0 * 8.76367473602295
Epoch 580, val loss: 0.9788281321525574
Epoch 590, training loss: 877.4871215820312 = 0.9697105288505554 + 100.0 * 8.76517391204834
Epoch 590, val loss: 0.9748318195343018
Epoch 600, training loss: 878.1638793945312 = 0.9655631184577942 + 100.0 * 8.77198314666748
Epoch 600, val loss: 0.9709059000015259
Epoch 610, training loss: 878.1110229492188 = 0.9610462188720703 + 100.0 * 8.771499633789062
Epoch 610, val loss: 0.9666473865509033
Epoch 620, training loss: 878.2559204101562 = 0.9565297365188599 + 100.0 * 8.772994041442871
Epoch 620, val loss: 0.962466835975647
Epoch 630, training loss: 878.8890380859375 = 0.9520357847213745 + 100.0 * 8.779370307922363
Epoch 630, val loss: 0.9580974578857422
Epoch 640, training loss: 878.0470581054688 = 0.9471262693405151 + 100.0 * 8.77099895477295
Epoch 640, val loss: 0.9535520672798157
Epoch 650, training loss: 878.5759887695312 = 0.9424583315849304 + 100.0 * 8.776335716247559
Epoch 650, val loss: 0.949228048324585
Epoch 660, training loss: 879.0030517578125 = 0.9376669526100159 + 100.0 * 8.780653953552246
Epoch 660, val loss: 0.9445881247520447
Epoch 670, training loss: 879.4147338867188 = 0.932773768901825 + 100.0 * 8.784819602966309
Epoch 670, val loss: 0.9400672316551208
Epoch 680, training loss: 879.1915283203125 = 0.9276707768440247 + 100.0 * 8.782638549804688
Epoch 680, val loss: 0.9353369474411011
Epoch 690, training loss: 879.4962768554688 = 0.9227659702301025 + 100.0 * 8.785735130310059
Epoch 690, val loss: 0.9306650161743164
Epoch 700, training loss: 879.4845581054688 = 0.9177991151809692 + 100.0 * 8.785667419433594
Epoch 700, val loss: 0.9259233474731445
Epoch 710, training loss: 879.9512939453125 = 0.9127687811851501 + 100.0 * 8.790385246276855
Epoch 710, val loss: 0.9212185740470886
Epoch 720, training loss: 880.4531860351562 = 0.9077064394950867 + 100.0 * 8.795454978942871
Epoch 720, val loss: 0.9164613485336304
Epoch 730, training loss: 879.9954223632812 = 0.902521550655365 + 100.0 * 8.790928840637207
Epoch 730, val loss: 0.9115313291549683
Epoch 740, training loss: 880.4486694335938 = 0.897527813911438 + 100.0 * 8.795511245727539
Epoch 740, val loss: 0.9068217873573303
Epoch 750, training loss: 880.75634765625 = 0.8925262093544006 + 100.0 * 8.798638343811035
Epoch 750, val loss: 0.9020810127258301
Epoch 760, training loss: 880.8156127929688 = 0.8873517513275146 + 100.0 * 8.799283027648926
Epoch 760, val loss: 0.8973255157470703
Epoch 770, training loss: 881.0150146484375 = 0.882270336151123 + 100.0 * 8.8013277053833
Epoch 770, val loss: 0.8925076723098755
Epoch 780, training loss: 881.5647583007812 = 0.8772789239883423 + 100.0 * 8.806875228881836
Epoch 780, val loss: 0.8877952694892883
Epoch 790, training loss: 881.5987548828125 = 0.8720338940620422 + 100.0 * 8.807267189025879
Epoch 790, val loss: 0.8829028010368347
Epoch 800, training loss: 881.7188720703125 = 0.8669251203536987 + 100.0 * 8.80851936340332
Epoch 800, val loss: 0.8781314492225647
Epoch 810, training loss: 881.7882080078125 = 0.8618496060371399 + 100.0 * 8.809263229370117
Epoch 810, val loss: 0.8732929229736328
Epoch 820, training loss: 881.9135131835938 = 0.85676109790802 + 100.0 * 8.810567855834961
Epoch 820, val loss: 0.8685747385025024
Epoch 830, training loss: 882.1502075195312 = 0.8516842722892761 + 100.0 * 8.81298542022705
Epoch 830, val loss: 0.8638054728507996
Epoch 840, training loss: 882.0999145507812 = 0.8465859293937683 + 100.0 * 8.812533378601074
Epoch 840, val loss: 0.8589338660240173
Epoch 850, training loss: 882.017333984375 = 0.8414397835731506 + 100.0 * 8.811758995056152
Epoch 850, val loss: 0.8542102575302124
Epoch 860, training loss: 882.095703125 = 0.8363854885101318 + 100.0 * 8.812593460083008
Epoch 860, val loss: 0.8494803309440613
Epoch 870, training loss: 882.3355102539062 = 0.8314518332481384 + 100.0 * 8.815040588378906
Epoch 870, val loss: 0.8448379635810852
Epoch 880, training loss: 880.9472045898438 = 0.8256731629371643 + 100.0 * 8.801215171813965
Epoch 880, val loss: 0.8397147059440613
Epoch 890, training loss: 881.8499755859375 = 0.8212541937828064 + 100.0 * 8.810287475585938
Epoch 890, val loss: 0.8352063298225403
Epoch 900, training loss: 882.41064453125 = 0.816354513168335 + 100.0 * 8.815942764282227
Epoch 900, val loss: 0.8308846950531006
Epoch 910, training loss: 882.3184814453125 = 0.8115620017051697 + 100.0 * 8.815069198608398
Epoch 910, val loss: 0.8264127373695374
Epoch 920, training loss: 882.87109375 = 0.80681312084198 + 100.0 * 8.820642471313477
Epoch 920, val loss: 0.8219646215438843
Epoch 930, training loss: 883.5255737304688 = 0.8020505309104919 + 100.0 * 8.827235221862793
Epoch 930, val loss: 0.8175407648086548
Epoch 940, training loss: 883.6756591796875 = 0.7969947457313538 + 100.0 * 8.828786849975586
Epoch 940, val loss: 0.8128086924552917
Epoch 950, training loss: 883.4784545898438 = 0.7919433116912842 + 100.0 * 8.826865196228027
Epoch 950, val loss: 0.8081856369972229
Epoch 960, training loss: 883.912841796875 = 0.7870115637779236 + 100.0 * 8.831258773803711
Epoch 960, val loss: 0.8034908771514893
Epoch 970, training loss: 883.9363403320312 = 0.7822377681732178 + 100.0 * 8.831541061401367
Epoch 970, val loss: 0.799226701259613
Epoch 980, training loss: 884.2918701171875 = 0.7776370048522949 + 100.0 * 8.835142135620117
Epoch 980, val loss: 0.7950473427772522
Epoch 990, training loss: 884.2985229492188 = 0.7728996276855469 + 100.0 * 8.835256576538086
Epoch 990, val loss: 0.7905367612838745
Epoch 1000, training loss: 884.3186645507812 = 0.7681233286857605 + 100.0 * 8.835505485534668
Epoch 1000, val loss: 0.7862799167633057
Epoch 1010, training loss: 884.8023071289062 = 0.7635795474052429 + 100.0 * 8.840387344360352
Epoch 1010, val loss: 0.7821255922317505
Epoch 1020, training loss: 884.8856201171875 = 0.7588417530059814 + 100.0 * 8.841267585754395
Epoch 1020, val loss: 0.7777366638183594
Epoch 1030, training loss: 885.0828247070312 = 0.7543105483055115 + 100.0 * 8.843284606933594
Epoch 1030, val loss: 0.7736587524414062
Epoch 1040, training loss: 884.7188720703125 = 0.7497270703315735 + 100.0 * 8.839691162109375
Epoch 1040, val loss: 0.7695100903511047
Epoch 1050, training loss: 884.949462890625 = 0.7452380657196045 + 100.0 * 8.842041969299316
Epoch 1050, val loss: 0.7654617428779602
Epoch 1060, training loss: 885.2520751953125 = 0.7409200668334961 + 100.0 * 8.845111846923828
Epoch 1060, val loss: 0.7616326212882996
Epoch 1070, training loss: 885.754638671875 = 0.7367794513702393 + 100.0 * 8.850178718566895
Epoch 1070, val loss: 0.7578082084655762
Epoch 1080, training loss: 885.6469116210938 = 0.7323158979415894 + 100.0 * 8.849145889282227
Epoch 1080, val loss: 0.7538959980010986
Epoch 1090, training loss: 886.1048583984375 = 0.7280513644218445 + 100.0 * 8.853768348693848
Epoch 1090, val loss: 0.7499637603759766
Epoch 1100, training loss: 885.6533203125 = 0.7237634658813477 + 100.0 * 8.849295616149902
Epoch 1100, val loss: 0.7461724281311035
Epoch 1110, training loss: 885.8032836914062 = 0.7198055982589722 + 100.0 * 8.850834846496582
Epoch 1110, val loss: 0.7427501082420349
Epoch 1120, training loss: 885.8907470703125 = 0.7158716320991516 + 100.0 * 8.8517484664917
Epoch 1120, val loss: 0.7392461895942688
Epoch 1130, training loss: 886.9348754882812 = 0.7121924161911011 + 100.0 * 8.862226486206055
Epoch 1130, val loss: 0.7359964847564697
Epoch 1140, training loss: 886.7902221679688 = 0.7082564234733582 + 100.0 * 8.860819816589355
Epoch 1140, val loss: 0.732452392578125
Epoch 1150, training loss: 886.6538696289062 = 0.7043243646621704 + 100.0 * 8.859495162963867
Epoch 1150, val loss: 0.7290877103805542
Epoch 1160, training loss: 887.1878051757812 = 0.7006904482841492 + 100.0 * 8.86487102508545
Epoch 1160, val loss: 0.7258132100105286
Epoch 1170, training loss: 887.3057250976562 = 0.6970286965370178 + 100.0 * 8.866086959838867
Epoch 1170, val loss: 0.7226361632347107
Epoch 1180, training loss: 887.2059936523438 = 0.6934353709220886 + 100.0 * 8.86512565612793
Epoch 1180, val loss: 0.7195613384246826
Epoch 1190, training loss: 887.6043701171875 = 0.6899500489234924 + 100.0 * 8.869144439697266
Epoch 1190, val loss: 0.7165074944496155
Epoch 1200, training loss: 887.4801025390625 = 0.6864808201789856 + 100.0 * 8.867936134338379
Epoch 1200, val loss: 0.7137026190757751
Epoch 1210, training loss: 888.0214233398438 = 0.6833165287971497 + 100.0 * 8.873381614685059
Epoch 1210, val loss: 0.710896909236908
Epoch 1220, training loss: 888.0418090820312 = 0.6800054907798767 + 100.0 * 8.873618125915527
Epoch 1220, val loss: 0.708136796951294
Epoch 1230, training loss: 888.1029663085938 = 0.6767240762710571 + 100.0 * 8.874262809753418
Epoch 1230, val loss: 0.7054240107536316
Epoch 1240, training loss: 888.266357421875 = 0.6736632585525513 + 100.0 * 8.875926971435547
Epoch 1240, val loss: 0.7027677297592163
Epoch 1250, training loss: 888.38232421875 = 0.6705794334411621 + 100.0 * 8.877117156982422
Epoch 1250, val loss: 0.7002493739128113
Epoch 1260, training loss: 888.6527709960938 = 0.6676692962646484 + 100.0 * 8.879851341247559
Epoch 1260, val loss: 0.6977188587188721
Epoch 1270, training loss: 888.65087890625 = 0.6646373867988586 + 100.0 * 8.879862785339355
Epoch 1270, val loss: 0.6952751278877258
Epoch 1280, training loss: 888.69189453125 = 0.6617568731307983 + 100.0 * 8.880301475524902
Epoch 1280, val loss: 0.6930286884307861
Epoch 1290, training loss: 889.348876953125 = 0.6590322852134705 + 100.0 * 8.886898040771484
Epoch 1290, val loss: 0.6908178925514221
Epoch 1300, training loss: 889.5560302734375 = 0.6563382148742676 + 100.0 * 8.888997077941895
Epoch 1300, val loss: 0.688612699508667
Epoch 1310, training loss: 889.5914306640625 = 0.6537313461303711 + 100.0 * 8.889376640319824
Epoch 1310, val loss: 0.686567485332489
Epoch 1320, training loss: 890.4883422851562 = 0.651091992855072 + 100.0 * 8.898372650146484
Epoch 1320, val loss: 0.6844256520271301
Epoch 1330, training loss: 888.6058349609375 = 0.6482064723968506 + 100.0 * 8.879576683044434
Epoch 1330, val loss: 0.682273268699646
Epoch 1340, training loss: 886.1220092773438 = 0.6454885005950928 + 100.0 * 8.854764938354492
Epoch 1340, val loss: 0.6800950169563293
Epoch 1350, training loss: 888.9212036132812 = 0.6436986327171326 + 100.0 * 8.88277530670166
Epoch 1350, val loss: 0.6786045432090759
Epoch 1360, training loss: 887.8319091796875 = 0.641318142414093 + 100.0 * 8.871906280517578
Epoch 1360, val loss: 0.6771482229232788
Epoch 1370, training loss: 888.7608642578125 = 0.639327347278595 + 100.0 * 8.88121509552002
Epoch 1370, val loss: 0.6754519939422607
Epoch 1380, training loss: 889.553955078125 = 0.6374058723449707 + 100.0 * 8.889165878295898
Epoch 1380, val loss: 0.6740479469299316
Epoch 1390, training loss: 890.2108154296875 = 0.6354233622550964 + 100.0 * 8.895753860473633
Epoch 1390, val loss: 0.6725250482559204
Epoch 1400, training loss: 890.1310424804688 = 0.6333855390548706 + 100.0 * 8.894976615905762
Epoch 1400, val loss: 0.671036958694458
Epoch 1410, training loss: 890.5986328125 = 0.6314080953598022 + 100.0 * 8.899672508239746
Epoch 1410, val loss: 0.6695660948753357
Epoch 1420, training loss: 890.7487182617188 = 0.6294795274734497 + 100.0 * 8.901192665100098
Epoch 1420, val loss: 0.6681004166603088
Epoch 1430, training loss: 890.8658447265625 = 0.627546489238739 + 100.0 * 8.902382850646973
Epoch 1430, val loss: 0.6668019890785217
Epoch 1440, training loss: 890.8207397460938 = 0.6257038712501526 + 100.0 * 8.901949882507324
Epoch 1440, val loss: 0.6654901504516602
Epoch 1450, training loss: 891.1665649414062 = 0.6239821910858154 + 100.0 * 8.905426025390625
Epoch 1450, val loss: 0.6642870903015137
Epoch 1460, training loss: 890.619873046875 = 0.6220043897628784 + 100.0 * 8.899978637695312
Epoch 1460, val loss: 0.6627947688102722
Epoch 1470, training loss: 890.8753662109375 = 0.6203345060348511 + 100.0 * 8.902549743652344
Epoch 1470, val loss: 0.6617922782897949
Epoch 1480, training loss: 891.7325439453125 = 0.6187663674354553 + 100.0 * 8.911137580871582
Epoch 1480, val loss: 0.6606733798980713
Epoch 1490, training loss: 891.6934204101562 = 0.6171421408653259 + 100.0 * 8.910762786865234
Epoch 1490, val loss: 0.659650444984436
Epoch 1500, training loss: 891.732666015625 = 0.6156033873558044 + 100.0 * 8.911170959472656
Epoch 1500, val loss: 0.6586054563522339
Epoch 1510, training loss: 892.0540161132812 = 0.6140680909156799 + 100.0 * 8.914399147033691
Epoch 1510, val loss: 0.6576626300811768
Epoch 1520, training loss: 892.033935546875 = 0.6126015186309814 + 100.0 * 8.914213180541992
Epoch 1520, val loss: 0.6567541360855103
Epoch 1530, training loss: 892.3902587890625 = 0.6111583709716797 + 100.0 * 8.917791366577148
Epoch 1530, val loss: 0.6558476090431213
Epoch 1540, training loss: 892.1270141601562 = 0.609569787979126 + 100.0 * 8.91517448425293
Epoch 1540, val loss: 0.654883861541748
Epoch 1550, training loss: 892.0729370117188 = 0.607963502407074 + 100.0 * 8.914649963378906
Epoch 1550, val loss: 0.6538036465644836
Epoch 1560, training loss: 891.822021484375 = 0.606633186340332 + 100.0 * 8.912154197692871
Epoch 1560, val loss: 0.65300053358078
Epoch 1570, training loss: 891.7425537109375 = 0.6053447127342224 + 100.0 * 8.911372184753418
Epoch 1570, val loss: 0.6522063612937927
Epoch 1580, training loss: 892.541748046875 = 0.6041364073753357 + 100.0 * 8.919376373291016
Epoch 1580, val loss: 0.651544451713562
Epoch 1590, training loss: 893.4290161132812 = 0.6029702425003052 + 100.0 * 8.928260803222656
Epoch 1590, val loss: 0.6508836150169373
Epoch 1600, training loss: 893.452880859375 = 0.6017202734947205 + 100.0 * 8.928511619567871
Epoch 1600, val loss: 0.650178074836731
Epoch 1610, training loss: 891.4752807617188 = 0.6002349853515625 + 100.0 * 8.908750534057617
Epoch 1610, val loss: 0.6491628289222717
Epoch 1620, training loss: 892.1712646484375 = 0.5991732478141785 + 100.0 * 8.91572093963623
Epoch 1620, val loss: 0.6489605903625488
Epoch 1630, training loss: 892.7654418945312 = 0.5981606245040894 + 100.0 * 8.921672821044922
Epoch 1630, val loss: 0.6482039093971252
Epoch 1640, training loss: 893.3738403320312 = 0.5969930291175842 + 100.0 * 8.92776870727539
Epoch 1640, val loss: 0.6476835608482361
Epoch 1650, training loss: 894.2008666992188 = 0.5959604382514954 + 100.0 * 8.936049461364746
Epoch 1650, val loss: 0.647101104259491
Epoch 1660, training loss: 894.598876953125 = 0.5948827862739563 + 100.0 * 8.94003963470459
Epoch 1660, val loss: 0.6464517116546631
Epoch 1670, training loss: 894.1156616210938 = 0.593664824962616 + 100.0 * 8.935219764709473
Epoch 1670, val loss: 0.6458620429039001
Epoch 1680, training loss: 894.67578125 = 0.5926456451416016 + 100.0 * 8.940831184387207
Epoch 1680, val loss: 0.645346462726593
Epoch 1690, training loss: 895.0436401367188 = 0.5916363000869751 + 100.0 * 8.944519996643066
Epoch 1690, val loss: 0.6448111534118652
Epoch 1700, training loss: 895.025146484375 = 0.5905759930610657 + 100.0 * 8.944345474243164
Epoch 1700, val loss: 0.6443090438842773
Epoch 1710, training loss: 895.2907104492188 = 0.5895566344261169 + 100.0 * 8.947011947631836
Epoch 1710, val loss: 0.6438450217247009
Epoch 1720, training loss: 894.9801025390625 = 0.5885085463523865 + 100.0 * 8.943916320800781
Epoch 1720, val loss: 0.643288254737854
Epoch 1730, training loss: 895.2750244140625 = 0.5875881910324097 + 100.0 * 8.946874618530273
Epoch 1730, val loss: 0.6429446935653687
Epoch 1740, training loss: 895.2227783203125 = 0.5866093635559082 + 100.0 * 8.946361541748047
Epoch 1740, val loss: 0.6423524022102356
Epoch 1750, training loss: 895.9148559570312 = 0.5857117772102356 + 100.0 * 8.953291893005371
Epoch 1750, val loss: 0.6419565677642822
Epoch 1760, training loss: 896.0017700195312 = 0.5847506523132324 + 100.0 * 8.954170227050781
Epoch 1760, val loss: 0.6415195465087891
Epoch 1770, training loss: 895.7526245117188 = 0.5837528705596924 + 100.0 * 8.951688766479492
Epoch 1770, val loss: 0.6410909295082092
Epoch 1780, training loss: 895.8850708007812 = 0.5827458500862122 + 100.0 * 8.953022956848145
Epoch 1780, val loss: 0.6406241655349731
Epoch 1790, training loss: 895.888427734375 = 0.5818774700164795 + 100.0 * 8.953065872192383
Epoch 1790, val loss: 0.6402935981750488
Epoch 1800, training loss: 896.4526977539062 = 0.5810335278511047 + 100.0 * 8.95871639251709
Epoch 1800, val loss: 0.6399256587028503
Epoch 1810, training loss: 896.6181640625 = 0.5800906419754028 + 100.0 * 8.960380554199219
Epoch 1810, val loss: 0.6394827365875244
Epoch 1820, training loss: 896.4968872070312 = 0.5791627168655396 + 100.0 * 8.959177017211914
Epoch 1820, val loss: 0.6390773057937622
Epoch 1830, training loss: 896.752197265625 = 0.5782437920570374 + 100.0 * 8.961739540100098
Epoch 1830, val loss: 0.6386444568634033
Epoch 1840, training loss: 896.5455932617188 = 0.5773200988769531 + 100.0 * 8.95968246459961
Epoch 1840, val loss: 0.638367235660553
Epoch 1850, training loss: 896.8851928710938 = 0.5764600038528442 + 100.0 * 8.96308708190918
Epoch 1850, val loss: 0.6380248069763184
Epoch 1860, training loss: 897.1669311523438 = 0.5756319165229797 + 100.0 * 8.965912818908691
Epoch 1860, val loss: 0.6378053426742554
Epoch 1870, training loss: 897.2328491210938 = 0.574817419052124 + 100.0 * 8.966580390930176
Epoch 1870, val loss: 0.6374579668045044
Epoch 1880, training loss: 897.0269165039062 = 0.5738970637321472 + 100.0 * 8.964529991149902
Epoch 1880, val loss: 0.6371088027954102
Epoch 1890, training loss: 896.935791015625 = 0.5730934739112854 + 100.0 * 8.963626861572266
Epoch 1890, val loss: 0.6368268728256226
Epoch 1900, training loss: 897.2913208007812 = 0.5722851753234863 + 100.0 * 8.967190742492676
Epoch 1900, val loss: 0.636580765247345
Epoch 1910, training loss: 898.0487060546875 = 0.5714945793151855 + 100.0 * 8.974772453308105
Epoch 1910, val loss: 0.6362528800964355
Epoch 1920, training loss: 896.7420654296875 = 0.5703953504562378 + 100.0 * 8.961716651916504
Epoch 1920, val loss: 0.6356318593025208
Epoch 1930, training loss: 896.0045776367188 = 0.5694615840911865 + 100.0 * 8.954351425170898
Epoch 1930, val loss: 0.6351374387741089
Epoch 1940, training loss: 896.5927124023438 = 0.5687692165374756 + 100.0 * 8.96023941040039
Epoch 1940, val loss: 0.6351391673088074
Epoch 1950, training loss: 897.5480346679688 = 0.568091094493866 + 100.0 * 8.969799041748047
Epoch 1950, val loss: 0.634800910949707
Epoch 1960, training loss: 898.4200439453125 = 0.5673519372940063 + 100.0 * 8.978527069091797
Epoch 1960, val loss: 0.6346454620361328
Epoch 1970, training loss: 898.3761596679688 = 0.5665621757507324 + 100.0 * 8.978096008300781
Epoch 1970, val loss: 0.6343159675598145
Epoch 1980, training loss: 898.1002197265625 = 0.5657328963279724 + 100.0 * 8.97534465789795
Epoch 1980, val loss: 0.6340227127075195
Epoch 1990, training loss: 898.3902587890625 = 0.5649387836456299 + 100.0 * 8.978253364562988
Epoch 1990, val loss: 0.6337754726409912
Epoch 2000, training loss: 899.177734375 = 0.564180850982666 + 100.0 * 8.986135482788086
Epoch 2000, val loss: 0.6335189938545227
Epoch 2010, training loss: 898.1055908203125 = 0.5632734298706055 + 100.0 * 8.975422859191895
Epoch 2010, val loss: 0.6332762837409973
Epoch 2020, training loss: 897.9215087890625 = 0.5624455809593201 + 100.0 * 8.973590850830078
Epoch 2020, val loss: 0.6329975128173828
Epoch 2030, training loss: 897.615234375 = 0.5615695118904114 + 100.0 * 8.970536231994629
Epoch 2030, val loss: 0.6325691938400269
Epoch 2040, training loss: 897.3780517578125 = 0.5607039332389832 + 100.0 * 8.968173027038574
Epoch 2040, val loss: 0.6321542859077454
Epoch 2050, training loss: 897.7893676757812 = 0.5600096583366394 + 100.0 * 8.972293853759766
Epoch 2050, val loss: 0.6321008801460266
Epoch 2060, training loss: 898.4236450195312 = 0.5593627691268921 + 100.0 * 8.978642463684082
Epoch 2060, val loss: 0.631780743598938
Epoch 2070, training loss: 899.2627563476562 = 0.5587006211280823 + 100.0 * 8.987040519714355
Epoch 2070, val loss: 0.6316549181938171
Epoch 2080, training loss: 899.1732788085938 = 0.5579022169113159 + 100.0 * 8.986153602600098
Epoch 2080, val loss: 0.6313503980636597
Epoch 2090, training loss: 899.407470703125 = 0.5571078658103943 + 100.0 * 8.988503456115723
Epoch 2090, val loss: 0.6311111450195312
Epoch 2100, training loss: 899.9840087890625 = 0.556329071521759 + 100.0 * 8.994277000427246
Epoch 2100, val loss: 0.6308425068855286
Epoch 2110, training loss: 899.9379272460938 = 0.5555185079574585 + 100.0 * 8.993824005126953
Epoch 2110, val loss: 0.6305769681930542
Epoch 2120, training loss: 900.0570678710938 = 0.5547084212303162 + 100.0 * 8.995023727416992
Epoch 2120, val loss: 0.6303163170814514
Epoch 2130, training loss: 900.21630859375 = 0.5539006590843201 + 100.0 * 8.996623992919922
Epoch 2130, val loss: 0.6300848126411438
Epoch 2140, training loss: 900.0980834960938 = 0.5530818104743958 + 100.0 * 8.995450019836426
Epoch 2140, val loss: 0.6297809481620789
Epoch 2150, training loss: 900.3697509765625 = 0.5523089170455933 + 100.0 * 8.998174667358398
Epoch 2150, val loss: 0.6295554041862488
Epoch 2160, training loss: 900.9320068359375 = 0.5515066981315613 + 100.0 * 9.003805160522461
Epoch 2160, val loss: 0.6292974352836609
Epoch 2170, training loss: 900.80224609375 = 0.5506501197814941 + 100.0 * 9.00251579284668
Epoch 2170, val loss: 0.628952145576477
Epoch 2180, training loss: 900.9248657226562 = 0.5498488545417786 + 100.0 * 9.00374984741211
Epoch 2180, val loss: 0.6287235021591187
Epoch 2190, training loss: 900.8854370117188 = 0.549035370349884 + 100.0 * 9.003364562988281
Epoch 2190, val loss: 0.6284142136573792
Epoch 2200, training loss: 901.3993530273438 = 0.548237681388855 + 100.0 * 9.008511543273926
Epoch 2200, val loss: 0.6281830668449402
Epoch 2210, training loss: 900.9393310546875 = 0.5473421812057495 + 100.0 * 9.00391960144043
Epoch 2210, val loss: 0.6278612017631531
Epoch 2220, training loss: 900.3212280273438 = 0.546406090259552 + 100.0 * 8.997748374938965
Epoch 2220, val loss: 0.6275469660758972
Epoch 2230, training loss: 900.596923828125 = 0.545583963394165 + 100.0 * 9.000513076782227
Epoch 2230, val loss: 0.6273181438446045
Epoch 2240, training loss: 901.1981811523438 = 0.5448277592658997 + 100.0 * 9.0065336227417
Epoch 2240, val loss: 0.6270484924316406
Epoch 2250, training loss: 901.6632080078125 = 0.5440614223480225 + 100.0 * 9.011191368103027
Epoch 2250, val loss: 0.6268535852432251
Epoch 2260, training loss: 901.423583984375 = 0.5431734919548035 + 100.0 * 9.008804321289062
Epoch 2260, val loss: 0.6264879703521729
Epoch 2270, training loss: 900.646240234375 = 0.5423036217689514 + 100.0 * 9.001039505004883
Epoch 2270, val loss: 0.6262366771697998
Epoch 2280, training loss: 900.7954711914062 = 0.5414811968803406 + 100.0 * 9.00253963470459
Epoch 2280, val loss: 0.6260372996330261
Epoch 2290, training loss: 901.6184692382812 = 0.5406677722930908 + 100.0 * 9.010778427124023
Epoch 2290, val loss: 0.6257059574127197
Epoch 2300, training loss: 902.2077026367188 = 0.5399280190467834 + 100.0 * 9.016677856445312
Epoch 2300, val loss: 0.6255991458892822
Epoch 2310, training loss: 902.824462890625 = 0.5391541123390198 + 100.0 * 9.022852897644043
Epoch 2310, val loss: 0.6253553628921509
Epoch 2320, training loss: 902.574951171875 = 0.5382753014564514 + 100.0 * 9.020366668701172
Epoch 2320, val loss: 0.6250064969062805
Epoch 2330, training loss: 902.8062133789062 = 0.5374265313148499 + 100.0 * 9.022687911987305
Epoch 2330, val loss: 0.6247886419296265
Epoch 2340, training loss: 903.1743774414062 = 0.5366214513778687 + 100.0 * 9.02637767791748
Epoch 2340, val loss: 0.624626100063324
Epoch 2350, training loss: 902.9344482421875 = 0.5357409119606018 + 100.0 * 9.02398681640625
Epoch 2350, val loss: 0.6244944930076599
Epoch 2360, training loss: 902.7041625976562 = 0.5348616242408752 + 100.0 * 9.021693229675293
Epoch 2360, val loss: 0.6241418123245239
Epoch 2370, training loss: 902.9840698242188 = 0.5340335369110107 + 100.0 * 9.024499893188477
Epoch 2370, val loss: 0.6238778233528137
Epoch 2380, training loss: 903.6640014648438 = 0.5332229137420654 + 100.0 * 9.0313081741333
Epoch 2380, val loss: 0.6237012147903442
Epoch 2390, training loss: 903.9205322265625 = 0.5323980450630188 + 100.0 * 9.033881187438965
Epoch 2390, val loss: 0.6234487891197205
Epoch 2400, training loss: 900.5142211914062 = 0.5313998460769653 + 100.0 * 8.999828338623047
Epoch 2400, val loss: 0.6230126619338989
Epoch 2410, training loss: 897.47998046875 = 0.5305085182189941 + 100.0 * 8.969494819641113
Epoch 2410, val loss: 0.6231638789176941
Epoch 2420, training loss: 898.132080078125 = 0.5293579697608948 + 100.0 * 8.976027488708496
Epoch 2420, val loss: 0.6220791935920715
Epoch 2430, training loss: 899.4398193359375 = 0.5285583734512329 + 100.0 * 8.989112854003906
Epoch 2430, val loss: 0.6223316192626953
Epoch 2440, training loss: 900.1432495117188 = 0.5279781818389893 + 100.0 * 8.996152877807617
Epoch 2440, val loss: 0.6218228936195374
Epoch 2450, training loss: 900.9275512695312 = 0.5272762775421143 + 100.0 * 9.004002571105957
Epoch 2450, val loss: 0.6220173239707947
Epoch 2460, training loss: 901.7830810546875 = 0.526490330696106 + 100.0 * 9.012565612792969
Epoch 2460, val loss: 0.621525228023529
Epoch 2470, training loss: 902.6318969726562 = 0.5256666541099548 + 100.0 * 9.021061897277832
Epoch 2470, val loss: 0.6214098334312439
Epoch 2480, training loss: 903.1162719726562 = 0.5247623324394226 + 100.0 * 9.025915145874023
Epoch 2480, val loss: 0.621195912361145
Epoch 2490, training loss: 903.3792724609375 = 0.5238572359085083 + 100.0 * 9.02855396270752
Epoch 2490, val loss: 0.6208953857421875
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.752463768115942
0.8120698398898791
=== training gcn model ===
Epoch 0, training loss: 1012.009765625 = 1.0971503257751465 + 100.0 * 10.109126091003418
Epoch 0, val loss: 1.0987282991409302
Epoch 10, training loss: 969.3390502929688 = 1.0933417081832886 + 100.0 * 9.682456970214844
Epoch 10, val loss: 1.0949431657791138
Epoch 20, training loss: 952.1775512695312 = 1.0897657871246338 + 100.0 * 9.51087760925293
Epoch 20, val loss: 1.0913610458374023
Epoch 30, training loss: 939.6892700195312 = 1.086387038230896 + 100.0 * 9.386029243469238
Epoch 30, val loss: 1.0879805088043213
Epoch 40, training loss: 929.9107055664062 = 1.0832366943359375 + 100.0 * 9.288274765014648
Epoch 40, val loss: 1.084823489189148
Epoch 50, training loss: 921.9701538085938 = 1.0803087949752808 + 100.0 * 9.208898544311523
Epoch 50, val loss: 1.081887125968933
Epoch 60, training loss: 915.35302734375 = 1.077597737312317 + 100.0 * 9.142754554748535
Epoch 60, val loss: 1.0791579484939575
Epoch 70, training loss: 909.6715698242188 = 1.0750592947006226 + 100.0 * 9.085965156555176
Epoch 70, val loss: 1.0766006708145142
Epoch 80, training loss: 904.8657836914062 = 1.07273268699646 + 100.0 * 9.037930488586426
Epoch 80, val loss: 1.074250340461731
Epoch 90, training loss: 900.7272338867188 = 1.0705788135528564 + 100.0 * 8.996566772460938
Epoch 90, val loss: 1.0720666646957397
Epoch 100, training loss: 897.3048706054688 = 1.0685566663742065 + 100.0 * 8.962363243103027
Epoch 100, val loss: 1.0700137615203857
Epoch 110, training loss: 894.3535766601562 = 1.0666946172714233 + 100.0 * 8.932868957519531
Epoch 110, val loss: 1.068117618560791
Epoch 120, training loss: 891.727294921875 = 1.064961314201355 + 100.0 * 8.906623840332031
Epoch 120, val loss: 1.0663535594940186
Epoch 130, training loss: 889.3948364257812 = 1.0633678436279297 + 100.0 * 8.883315086364746
Epoch 130, val loss: 1.0647233724594116
Epoch 140, training loss: 887.3888549804688 = 1.0619151592254639 + 100.0 * 8.863269805908203
Epoch 140, val loss: 1.0632389783859253
Epoch 150, training loss: 885.6340942382812 = 1.060550332069397 + 100.0 * 8.845735549926758
Epoch 150, val loss: 1.061841368675232
Epoch 160, training loss: 884.0062255859375 = 1.05930757522583 + 100.0 * 8.829468727111816
Epoch 160, val loss: 1.0605695247650146
Epoch 170, training loss: 882.7159423828125 = 1.0581520795822144 + 100.0 * 8.816577911376953
Epoch 170, val loss: 1.0593806505203247
Epoch 180, training loss: 881.3638916015625 = 1.0570844411849976 + 100.0 * 8.803068161010742
Epoch 180, val loss: 1.058279037475586
Epoch 190, training loss: 880.3336791992188 = 1.0561014413833618 + 100.0 * 8.792776107788086
Epoch 190, val loss: 1.057268500328064
Epoch 200, training loss: 879.4959106445312 = 1.0551854372024536 + 100.0 * 8.784407615661621
Epoch 200, val loss: 1.0563122034072876
Epoch 210, training loss: 878.723388671875 = 1.0543365478515625 + 100.0 * 8.776690483093262
Epoch 210, val loss: 1.0554314851760864
Epoch 220, training loss: 877.9402465820312 = 1.0534981489181519 + 100.0 * 8.768867492675781
Epoch 220, val loss: 1.054558277130127
Epoch 230, training loss: 877.540283203125 = 1.0527207851409912 + 100.0 * 8.764875411987305
Epoch 230, val loss: 1.053753137588501
Epoch 240, training loss: 877.0611572265625 = 1.0519013404846191 + 100.0 * 8.760092735290527
Epoch 240, val loss: 1.0529181957244873
Epoch 250, training loss: 876.2933349609375 = 1.0511161088943481 + 100.0 * 8.752422332763672
Epoch 250, val loss: 1.052107334136963
Epoch 260, training loss: 876.0956420898438 = 1.050357699394226 + 100.0 * 8.750452995300293
Epoch 260, val loss: 1.051343321800232
Epoch 270, training loss: 875.7958984375 = 1.049565315246582 + 100.0 * 8.74746322631836
Epoch 270, val loss: 1.0505117177963257
Epoch 280, training loss: 875.2810668945312 = 1.0487221479415894 + 100.0 * 8.742323875427246
Epoch 280, val loss: 1.049659013748169
Epoch 290, training loss: 875.0105590820312 = 1.0478309392929077 + 100.0 * 8.73962688446045
Epoch 290, val loss: 1.0487630367279053
Epoch 300, training loss: 874.8937377929688 = 1.0469059944152832 + 100.0 * 8.738468170166016
Epoch 300, val loss: 1.0478284358978271
Epoch 310, training loss: 874.5423583984375 = 1.0459152460098267 + 100.0 * 8.734964370727539
Epoch 310, val loss: 1.0468388795852661
Epoch 320, training loss: 874.221923828125 = 1.0448461771011353 + 100.0 * 8.731770515441895
Epoch 320, val loss: 1.0457568168640137
Epoch 330, training loss: 873.9180297851562 = 1.0436749458312988 + 100.0 * 8.728743553161621
Epoch 330, val loss: 1.0446233749389648
Epoch 340, training loss: 873.7473754882812 = 1.0424836874008179 + 100.0 * 8.727048873901367
Epoch 340, val loss: 1.043452501296997
Epoch 350, training loss: 873.5567016601562 = 1.04120671749115 + 100.0 * 8.725154876708984
Epoch 350, val loss: 1.0421643257141113
Epoch 360, training loss: 873.5115966796875 = 1.039851427078247 + 100.0 * 8.724717140197754
Epoch 360, val loss: 1.0408360958099365
Epoch 370, training loss: 873.426513671875 = 1.0384482145309448 + 100.0 * 8.723880767822266
Epoch 370, val loss: 1.0394505262374878
Epoch 380, training loss: 873.390380859375 = 1.0369082689285278 + 100.0 * 8.72353458404541
Epoch 380, val loss: 1.0379045009613037
Epoch 390, training loss: 873.2887573242188 = 1.035224437713623 + 100.0 * 8.722535133361816
Epoch 390, val loss: 1.0362827777862549
Epoch 400, training loss: 873.4183959960938 = 1.0334434509277344 + 100.0 * 8.723849296569824
Epoch 400, val loss: 1.0345525741577148
Epoch 410, training loss: 872.9032592773438 = 1.031600832939148 + 100.0 * 8.718716621398926
Epoch 410, val loss: 1.0328080654144287
Epoch 420, training loss: 873.3065795898438 = 1.0296012163162231 + 100.0 * 8.722769737243652
Epoch 420, val loss: 1.030862808227539
Epoch 430, training loss: 872.8458862304688 = 1.027483344078064 + 100.0 * 8.718184471130371
Epoch 430, val loss: 1.0287727117538452
Epoch 440, training loss: 872.9962158203125 = 1.0254102945327759 + 100.0 * 8.719708442687988
Epoch 440, val loss: 1.0267407894134521
Epoch 450, training loss: 872.9622192382812 = 1.0231051445007324 + 100.0 * 8.719390869140625
Epoch 450, val loss: 1.024499773979187
Epoch 460, training loss: 873.1355590820312 = 1.0206974744796753 + 100.0 * 8.721148490905762
Epoch 460, val loss: 1.022159457206726
Epoch 470, training loss: 873.443603515625 = 1.0181068181991577 + 100.0 * 8.724254608154297
Epoch 470, val loss: 1.0196561813354492
Epoch 480, training loss: 873.5323486328125 = 1.0153212547302246 + 100.0 * 8.725170135498047
Epoch 480, val loss: 1.0169575214385986
Epoch 490, training loss: 873.54541015625 = 1.0124073028564453 + 100.0 * 8.725330352783203
Epoch 490, val loss: 1.0141533613204956
Epoch 500, training loss: 873.730712890625 = 1.0093592405319214 + 100.0 * 8.727213859558105
Epoch 500, val loss: 1.011246919631958
Epoch 510, training loss: 873.7178344726562 = 1.0061428546905518 + 100.0 * 8.727116584777832
Epoch 510, val loss: 1.0081511735916138
Epoch 520, training loss: 874.1881713867188 = 1.0027899742126465 + 100.0 * 8.731853485107422
Epoch 520, val loss: 1.0048969984054565
Epoch 530, training loss: 874.1641235351562 = 0.9991878271102905 + 100.0 * 8.731649398803711
Epoch 530, val loss: 1.001432180404663
Epoch 540, training loss: 874.1954345703125 = 0.9954334497451782 + 100.0 * 8.732000350952148
Epoch 540, val loss: 0.9978971481323242
Epoch 550, training loss: 874.3424682617188 = 0.9914768934249878 + 100.0 * 8.73351001739502
Epoch 550, val loss: 0.9940000176429749
Epoch 560, training loss: 873.9746704101562 = 0.9874072670936584 + 100.0 * 8.729872703552246
Epoch 560, val loss: 0.9900583028793335
Epoch 570, training loss: 874.1357421875 = 0.9832217693328857 + 100.0 * 8.731525421142578
Epoch 570, val loss: 0.9860819578170776
Epoch 580, training loss: 874.4508666992188 = 0.9788490533828735 + 100.0 * 8.734720230102539
Epoch 580, val loss: 0.9818504452705383
Epoch 590, training loss: 874.4102172851562 = 0.9742643237113953 + 100.0 * 8.734359741210938
Epoch 590, val loss: 0.9774803519248962
Epoch 600, training loss: 874.7501831054688 = 0.9695277214050293 + 100.0 * 8.73780632019043
Epoch 600, val loss: 0.9729098677635193
Epoch 610, training loss: 875.0136108398438 = 0.9645920395851135 + 100.0 * 8.740489959716797
Epoch 610, val loss: 0.968180775642395
Epoch 620, training loss: 874.9310913085938 = 0.9595258831977844 + 100.0 * 8.739715576171875
Epoch 620, val loss: 0.9632656574249268
Epoch 630, training loss: 874.9552612304688 = 0.9541431069374084 + 100.0 * 8.740011215209961
Epoch 630, val loss: 0.9581942558288574
Epoch 640, training loss: 875.3777465820312 = 0.9489306807518005 + 100.0 * 8.744288444519043
Epoch 640, val loss: 0.9531114101409912
Epoch 650, training loss: 875.7161865234375 = 0.9435268044471741 + 100.0 * 8.747726440429688
Epoch 650, val loss: 0.9477854371070862
Epoch 660, training loss: 875.5242309570312 = 0.9377633333206177 + 100.0 * 8.745864868164062
Epoch 660, val loss: 0.9423291087150574
Epoch 670, training loss: 875.8821411132812 = 0.9319897294044495 + 100.0 * 8.74950122833252
Epoch 670, val loss: 0.9367820620536804
Epoch 680, training loss: 876.0270385742188 = 0.9261643290519714 + 100.0 * 8.751008987426758
Epoch 680, val loss: 0.9311924576759338
Epoch 690, training loss: 876.0633544921875 = 0.9202619194984436 + 100.0 * 8.75143051147461
Epoch 690, val loss: 0.925494909286499
Epoch 700, training loss: 875.8818969726562 = 0.9143271446228027 + 100.0 * 8.749675750732422
Epoch 700, val loss: 0.9198154807090759
Epoch 710, training loss: 876.324462890625 = 0.9083900451660156 + 100.0 * 8.75416088104248
Epoch 710, val loss: 0.9140780568122864
Epoch 720, training loss: 876.3934326171875 = 0.9022571444511414 + 100.0 * 8.754911422729492
Epoch 720, val loss: 0.908090353012085
Epoch 730, training loss: 876.9457397460938 = 0.8960350751876831 + 100.0 * 8.760497093200684
Epoch 730, val loss: 0.9021323323249817
Epoch 740, training loss: 876.6172485351562 = 0.8899073600769043 + 100.0 * 8.75727367401123
Epoch 740, val loss: 0.8963395953178406
Epoch 750, training loss: 876.1936645507812 = 0.8835437297821045 + 100.0 * 8.753101348876953
Epoch 750, val loss: 0.8899955749511719
Epoch 760, training loss: 876.6547241210938 = 0.8779577016830444 + 100.0 * 8.757767677307129
Epoch 760, val loss: 0.8846381306648254
Epoch 770, training loss: 876.59423828125 = 0.8720126748085022 + 100.0 * 8.757222175598145
Epoch 770, val loss: 0.8789226412773132
Epoch 780, training loss: 877.5797729492188 = 0.8662248849868774 + 100.0 * 8.767135620117188
Epoch 780, val loss: 0.8734070062637329
Epoch 790, training loss: 877.6621704101562 = 0.8604345917701721 + 100.0 * 8.768017768859863
Epoch 790, val loss: 0.8677093386650085
Epoch 800, training loss: 878.084716796875 = 0.8544496297836304 + 100.0 * 8.772302627563477
Epoch 800, val loss: 0.8619435429573059
Epoch 810, training loss: 878.470947265625 = 0.8484949469566345 + 100.0 * 8.776224136352539
Epoch 810, val loss: 0.8559311032295227
Epoch 820, training loss: 878.4618530273438 = 0.8426001667976379 + 100.0 * 8.776192665100098
Epoch 820, val loss: 0.8504953980445862
Epoch 830, training loss: 878.5311279296875 = 0.8369856476783752 + 100.0 * 8.776941299438477
Epoch 830, val loss: 0.8449758291244507
Epoch 840, training loss: 878.5974731445312 = 0.8312668800354004 + 100.0 * 8.77766227722168
Epoch 840, val loss: 0.8394162058830261
Epoch 850, training loss: 878.7466430664062 = 0.825722336769104 + 100.0 * 8.77920913696289
Epoch 850, val loss: 0.8339878916740417
Epoch 860, training loss: 878.9921264648438 = 0.8201643228530884 + 100.0 * 8.781719207763672
Epoch 860, val loss: 0.8285663723945618
Epoch 870, training loss: 879.3292236328125 = 0.814614474773407 + 100.0 * 8.78514575958252
Epoch 870, val loss: 0.8233360648155212
Epoch 880, training loss: 879.0216064453125 = 0.8089661002159119 + 100.0 * 8.782126426696777
Epoch 880, val loss: 0.8178093433380127
Epoch 890, training loss: 879.4253540039062 = 0.8035852909088135 + 100.0 * 8.78621768951416
Epoch 890, val loss: 0.8125856518745422
Epoch 900, training loss: 879.5234375 = 0.7981979250907898 + 100.0 * 8.787252426147461
Epoch 900, val loss: 0.8073305487632751
Epoch 910, training loss: 879.9172973632812 = 0.7929612994194031 + 100.0 * 8.791243553161621
Epoch 910, val loss: 0.8022356629371643
Epoch 920, training loss: 880.1463623046875 = 0.7877309918403625 + 100.0 * 8.793586730957031
Epoch 920, val loss: 0.7971537113189697
Epoch 930, training loss: 880.1520385742188 = 0.7825769782066345 + 100.0 * 8.793694496154785
Epoch 930, val loss: 0.7921757698059082
Epoch 940, training loss: 880.4727172851562 = 0.7774214148521423 + 100.0 * 8.796953201293945
Epoch 940, val loss: 0.7871848940849304
Epoch 950, training loss: 880.6527709960938 = 0.7722642421722412 + 100.0 * 8.798805236816406
Epoch 950, val loss: 0.7821969389915466
Epoch 960, training loss: 880.48095703125 = 0.7670367956161499 + 100.0 * 8.797139167785645
Epoch 960, val loss: 0.777263879776001
Epoch 970, training loss: 880.6178588867188 = 0.7621819376945496 + 100.0 * 8.798556327819824
Epoch 970, val loss: 0.7724750638008118
Epoch 980, training loss: 879.4611206054688 = 0.7568025588989258 + 100.0 * 8.787043571472168
Epoch 980, val loss: 0.7673242688179016
Epoch 990, training loss: 880.5358276367188 = 0.7523890137672424 + 100.0 * 8.797834396362305
Epoch 990, val loss: 0.7631157636642456
Epoch 1000, training loss: 880.807373046875 = 0.7475761771202087 + 100.0 * 8.80059814453125
Epoch 1000, val loss: 0.758451521396637
Epoch 1010, training loss: 881.560302734375 = 0.7430011630058289 + 100.0 * 8.808173179626465
Epoch 1010, val loss: 0.7540979981422424
Epoch 1020, training loss: 881.7786254882812 = 0.7385510206222534 + 100.0 * 8.81040096282959
Epoch 1020, val loss: 0.7498316764831543
Epoch 1030, training loss: 881.7918701171875 = 0.7339653968811035 + 100.0 * 8.810579299926758
Epoch 1030, val loss: 0.7454058527946472
Epoch 1040, training loss: 887.6602172851562 = 0.7285245656967163 + 100.0 * 8.869317054748535
Epoch 1040, val loss: 0.739428699016571
Epoch 1050, training loss: 875.5886840820312 = 0.722777247428894 + 100.0 * 8.748659133911133
Epoch 1050, val loss: 0.7342540621757507
Epoch 1060, training loss: 882.6390380859375 = 0.7207263112068176 + 100.0 * 8.819183349609375
Epoch 1060, val loss: 0.7333860397338867
Epoch 1070, training loss: 876.5099487304688 = 0.714482843875885 + 100.0 * 8.757954597473145
Epoch 1070, val loss: 0.7271252870559692
Epoch 1080, training loss: 878.9398803710938 = 0.7118316888809204 + 100.0 * 8.782279968261719
Epoch 1080, val loss: 0.7237203121185303
Epoch 1090, training loss: 878.4267578125 = 0.7070901393890381 + 100.0 * 8.777196884155273
Epoch 1090, val loss: 0.7194111347198486
Epoch 1100, training loss: 879.1959838867188 = 0.7032696008682251 + 100.0 * 8.784927368164062
Epoch 1100, val loss: 0.7159553170204163
Epoch 1110, training loss: 880.5850830078125 = 0.6999690532684326 + 100.0 * 8.798851013183594
Epoch 1110, val loss: 0.7128331065177917
Epoch 1120, training loss: 881.091552734375 = 0.696033775806427 + 100.0 * 8.803955078125
Epoch 1120, val loss: 0.7089874148368835
Epoch 1130, training loss: 881.349365234375 = 0.6920644044876099 + 100.0 * 8.806572914123535
Epoch 1130, val loss: 0.705238938331604
Epoch 1140, training loss: 881.7577514648438 = 0.6881260871887207 + 100.0 * 8.810696601867676
Epoch 1140, val loss: 0.7015738487243652
Epoch 1150, training loss: 881.9346313476562 = 0.6842992305755615 + 100.0 * 8.81250286102295
Epoch 1150, val loss: 0.6979197263717651
Epoch 1160, training loss: 882.43408203125 = 0.680529773235321 + 100.0 * 8.817535400390625
Epoch 1160, val loss: 0.6942642331123352
Epoch 1170, training loss: 882.85595703125 = 0.6767939329147339 + 100.0 * 8.821791648864746
Epoch 1170, val loss: 0.6908218860626221
Epoch 1180, training loss: 882.8062744140625 = 0.673175036907196 + 100.0 * 8.821331024169922
Epoch 1180, val loss: 0.687430202960968
Epoch 1190, training loss: 883.0888671875 = 0.6694732904434204 + 100.0 * 8.824193954467773
Epoch 1190, val loss: 0.6839671730995178
Epoch 1200, training loss: 883.22119140625 = 0.6660363078117371 + 100.0 * 8.825551986694336
Epoch 1200, val loss: 0.6806793212890625
Epoch 1210, training loss: 883.5636596679688 = 0.6626640558242798 + 100.0 * 8.829010009765625
Epoch 1210, val loss: 0.6775705218315125
Epoch 1220, training loss: 883.8336181640625 = 0.6593465209007263 + 100.0 * 8.831742286682129
Epoch 1220, val loss: 0.6744958162307739
Epoch 1230, training loss: 883.9052734375 = 0.6556587219238281 + 100.0 * 8.832496643066406
Epoch 1230, val loss: 0.6707274913787842
Epoch 1240, training loss: 883.5324096679688 = 0.6526675820350647 + 100.0 * 8.828797340393066
Epoch 1240, val loss: 0.6681114435195923
Epoch 1250, training loss: 884.2516479492188 = 0.6499081254005432 + 100.0 * 8.836017608642578
Epoch 1250, val loss: 0.666069507598877
Epoch 1260, training loss: 884.7503662109375 = 0.6469785571098328 + 100.0 * 8.841033935546875
Epoch 1260, val loss: 0.6630587577819824
Epoch 1270, training loss: 884.9027099609375 = 0.644008219242096 + 100.0 * 8.8425874710083
Epoch 1270, val loss: 0.6602247953414917
Epoch 1280, training loss: 885.1021118164062 = 0.6411519646644592 + 100.0 * 8.844609260559082
Epoch 1280, val loss: 0.6577831506729126
Epoch 1290, training loss: 884.9088745117188 = 0.6381977200508118 + 100.0 * 8.842706680297852
Epoch 1290, val loss: 0.6552221775054932
Epoch 1300, training loss: 885.03271484375 = 0.6353467702865601 + 100.0 * 8.843973159790039
Epoch 1300, val loss: 0.652572512626648
Epoch 1310, training loss: 885.3363037109375 = 0.6327757239341736 + 100.0 * 8.84703540802002
Epoch 1310, val loss: 0.6503733396530151
Epoch 1320, training loss: 885.5542602539062 = 0.6302515268325806 + 100.0 * 8.84924030303955
Epoch 1320, val loss: 0.64812833070755
Epoch 1330, training loss: 885.9684448242188 = 0.6277600526809692 + 100.0 * 8.85340690612793
Epoch 1330, val loss: 0.6459400057792664
Epoch 1340, training loss: 886.0492553710938 = 0.625312089920044 + 100.0 * 8.854239463806152
Epoch 1340, val loss: 0.6438477635383606
Epoch 1350, training loss: 886.4105224609375 = 0.6228523850440979 + 100.0 * 8.857876777648926
Epoch 1350, val loss: 0.6416980624198914
Epoch 1360, training loss: 885.514892578125 = 0.6201014518737793 + 100.0 * 8.848947525024414
Epoch 1360, val loss: 0.6394751667976379
Epoch 1370, training loss: 885.966552734375 = 0.6180566549301147 + 100.0 * 8.853485107421875
Epoch 1370, val loss: 0.6376621127128601
Epoch 1380, training loss: 886.5438842773438 = 0.6159225702285767 + 100.0 * 8.85927963256836
Epoch 1380, val loss: 0.6357381343841553
Epoch 1390, training loss: 887.1484985351562 = 0.6138807535171509 + 100.0 * 8.86534595489502
Epoch 1390, val loss: 0.634016215801239
Epoch 1400, training loss: 887.402099609375 = 0.6117975115776062 + 100.0 * 8.867902755737305
Epoch 1400, val loss: 0.6322594285011292
Epoch 1410, training loss: 887.38427734375 = 0.6096302270889282 + 100.0 * 8.867746353149414
Epoch 1410, val loss: 0.6304876804351807
Epoch 1420, training loss: 887.6266479492188 = 0.6074684858322144 + 100.0 * 8.87019157409668
Epoch 1420, val loss: 0.6287898421287537
Epoch 1430, training loss: 887.91162109375 = 0.6053029894828796 + 100.0 * 8.873063087463379
Epoch 1430, val loss: 0.6269223690032959
Epoch 1440, training loss: 888.274658203125 = 0.6032512187957764 + 100.0 * 8.876713752746582
Epoch 1440, val loss: 0.625288188457489
Epoch 1450, training loss: 888.14794921875 = 0.6011508107185364 + 100.0 * 8.875468254089355
Epoch 1450, val loss: 0.6236604452133179
Epoch 1460, training loss: 888.3609619140625 = 0.5991690158843994 + 100.0 * 8.877617835998535
Epoch 1460, val loss: 0.6221170425415039
Epoch 1470, training loss: 888.1995239257812 = 0.5970086455345154 + 100.0 * 8.876025199890137
Epoch 1470, val loss: 0.6203405857086182
Epoch 1480, training loss: 888.1323852539062 = 0.5950172543525696 + 100.0 * 8.875373840332031
Epoch 1480, val loss: 0.6188633441925049
Epoch 1490, training loss: 887.8684692382812 = 0.5930731892585754 + 100.0 * 8.872754096984863
Epoch 1490, val loss: 0.6174595355987549
Epoch 1500, training loss: 888.6914672851562 = 0.5912763476371765 + 100.0 * 8.881002426147461
Epoch 1500, val loss: 0.6161848902702332
Epoch 1510, training loss: 889.3157958984375 = 0.5897427201271057 + 100.0 * 8.887260437011719
Epoch 1510, val loss: 0.6150632500648499
Epoch 1520, training loss: 888.950439453125 = 0.5879970788955688 + 100.0 * 8.883624076843262
Epoch 1520, val loss: 0.6137827634811401
Epoch 1530, training loss: 888.73046875 = 0.586234450340271 + 100.0 * 8.881442070007324
Epoch 1530, val loss: 0.6125437617301941
Epoch 1540, training loss: 889.5975952148438 = 0.5847901701927185 + 100.0 * 8.890128135681152
Epoch 1540, val loss: 0.6115778684616089
Epoch 1550, training loss: 889.8627319335938 = 0.5832104682922363 + 100.0 * 8.89279556274414
Epoch 1550, val loss: 0.6105241775512695
Epoch 1560, training loss: 889.8671264648438 = 0.5816885828971863 + 100.0 * 8.892854690551758
Epoch 1560, val loss: 0.6093699932098389
Epoch 1570, training loss: 890.2584838867188 = 0.5802405476570129 + 100.0 * 8.896781921386719
Epoch 1570, val loss: 0.608460545539856
Epoch 1580, training loss: 889.7728881835938 = 0.5786172151565552 + 100.0 * 8.891942977905273
Epoch 1580, val loss: 0.6072773337364197
Epoch 1590, training loss: 890.1967163085938 = 0.5771065950393677 + 100.0 * 8.896196365356445
Epoch 1590, val loss: 0.606388509273529
Epoch 1600, training loss: 890.6067504882812 = 0.5757979154586792 + 100.0 * 8.900309562683105
Epoch 1600, val loss: 0.605450451374054
Epoch 1610, training loss: 890.8818359375 = 0.5743591785430908 + 100.0 * 8.903075218200684
Epoch 1610, val loss: 0.604651689529419
Epoch 1620, training loss: 890.2184448242188 = 0.5729246139526367 + 100.0 * 8.896454811096191
Epoch 1620, val loss: 0.6037538647651672
Epoch 1630, training loss: 890.3155517578125 = 0.5714814066886902 + 100.0 * 8.897440910339355
Epoch 1630, val loss: 0.6028191447257996
Epoch 1640, training loss: 891.1279907226562 = 0.5702626705169678 + 100.0 * 8.905577659606934
Epoch 1640, val loss: 0.6021298766136169
Epoch 1650, training loss: 891.3906860351562 = 0.5689941644668579 + 100.0 * 8.908217430114746
Epoch 1650, val loss: 0.6013219356536865
Epoch 1660, training loss: 890.3467407226562 = 0.5676183700561523 + 100.0 * 8.897790908813477
Epoch 1660, val loss: 0.6004977822303772
Epoch 1670, training loss: 889.9398803710938 = 0.5662283897399902 + 100.0 * 8.893736839294434
Epoch 1670, val loss: 0.5995845198631287
Epoch 1680, training loss: 890.6920776367188 = 0.5650919079780579 + 100.0 * 8.901269912719727
Epoch 1680, val loss: 0.5990406274795532
Epoch 1690, training loss: 891.4130249023438 = 0.564018726348877 + 100.0 * 8.908490180969238
Epoch 1690, val loss: 0.5983264446258545
Epoch 1700, training loss: 891.947021484375 = 0.562853217124939 + 100.0 * 8.913841247558594
Epoch 1700, val loss: 0.5976788997650146
Epoch 1710, training loss: 891.3295288085938 = 0.5616106986999512 + 100.0 * 8.907679557800293
Epoch 1710, val loss: 0.5970677733421326
Epoch 1720, training loss: 891.9232177734375 = 0.5604159235954285 + 100.0 * 8.913627624511719
Epoch 1720, val loss: 0.5962969064712524
Epoch 1730, training loss: 892.2332153320312 = 0.5592643618583679 + 100.0 * 8.916739463806152
Epoch 1730, val loss: 0.5956540703773499
Epoch 1740, training loss: 892.1338500976562 = 0.5581253170967102 + 100.0 * 8.915757179260254
Epoch 1740, val loss: 0.5950585603713989
Epoch 1750, training loss: 892.0892333984375 = 0.5568466186523438 + 100.0 * 8.915324211120605
Epoch 1750, val loss: 0.5944519639015198
Epoch 1760, training loss: 892.4548950195312 = 0.5557959675788879 + 100.0 * 8.918991088867188
Epoch 1760, val loss: 0.5936247110366821
Epoch 1770, training loss: 892.7218627929688 = 0.5547093152999878 + 100.0 * 8.921671867370605
Epoch 1770, val loss: 0.5931613445281982
Epoch 1780, training loss: 892.5543823242188 = 0.553570568561554 + 100.0 * 8.920007705688477
Epoch 1780, val loss: 0.5924121737480164
Epoch 1790, training loss: 892.6203002929688 = 0.5524644255638123 + 100.0 * 8.92067813873291
Epoch 1790, val loss: 0.5919179916381836
Epoch 1800, training loss: 892.7392578125 = 0.5514079332351685 + 100.0 * 8.921878814697266
Epoch 1800, val loss: 0.5912708640098572
Epoch 1810, training loss: 892.9658203125 = 0.550391435623169 + 100.0 * 8.924154281616211
Epoch 1810, val loss: 0.5907357931137085
Epoch 1820, training loss: 892.6514282226562 = 0.5492708086967468 + 100.0 * 8.921021461486816
Epoch 1820, val loss: 0.5901846289634705
Epoch 1830, training loss: 893.0556030273438 = 0.5482030510902405 + 100.0 * 8.925073623657227
Epoch 1830, val loss: 0.5895360708236694
Epoch 1840, training loss: 893.7344970703125 = 0.5471832752227783 + 100.0 * 8.931873321533203
Epoch 1840, val loss: 0.5891193151473999
Epoch 1850, training loss: 893.4810180664062 = 0.5461164712905884 + 100.0 * 8.929348945617676
Epoch 1850, val loss: 0.5886160731315613
Epoch 1860, training loss: 893.843994140625 = 0.5451151132583618 + 100.0 * 8.932989120483398
Epoch 1860, val loss: 0.5879706144332886
Epoch 1870, training loss: 893.9159545898438 = 0.5440605282783508 + 100.0 * 8.93371868133545
Epoch 1870, val loss: 0.5873140096664429
Epoch 1880, training loss: 892.4820556640625 = 0.5428988337516785 + 100.0 * 8.919391632080078
Epoch 1880, val loss: 0.5869376063346863
Epoch 1890, training loss: 892.4034423828125 = 0.5417631268501282 + 100.0 * 8.918617248535156
Epoch 1890, val loss: 0.5862234234809875
Epoch 1900, training loss: 893.0281372070312 = 0.5408110022544861 + 100.0 * 8.924873352050781
Epoch 1900, val loss: 0.5858651399612427
Epoch 1910, training loss: 893.9692993164062 = 0.5399190783500671 + 100.0 * 8.934293746948242
Epoch 1910, val loss: 0.5852454304695129
Epoch 1920, training loss: 894.5406494140625 = 0.5389837622642517 + 100.0 * 8.940016746520996
Epoch 1920, val loss: 0.5848241448402405
Epoch 1930, training loss: 894.5480346679688 = 0.5379859209060669 + 100.0 * 8.94010066986084
Epoch 1930, val loss: 0.5843906402587891
Epoch 1940, training loss: 894.3015747070312 = 0.5369651317596436 + 100.0 * 8.93764591217041
Epoch 1940, val loss: 0.5838085412979126
Epoch 1950, training loss: 894.5699462890625 = 0.535991370677948 + 100.0 * 8.940339088439941
Epoch 1950, val loss: 0.5833266377449036
Epoch 1960, training loss: 894.981689453125 = 0.5350362062454224 + 100.0 * 8.944466590881348
Epoch 1960, val loss: 0.5828495621681213
Epoch 1970, training loss: 894.9014892578125 = 0.5340202450752258 + 100.0 * 8.94367504119873
Epoch 1970, val loss: 0.5823367238044739
Epoch 1980, training loss: 895.1170654296875 = 0.5330527424812317 + 100.0 * 8.945839881896973
Epoch 1980, val loss: 0.5818135142326355
Epoch 1990, training loss: 895.228271484375 = 0.5320987105369568 + 100.0 * 8.946961402893066
Epoch 1990, val loss: 0.5813983678817749
Epoch 2000, training loss: 895.0277099609375 = 0.5310901403427124 + 100.0 * 8.944966316223145
Epoch 2000, val loss: 0.5807798504829407
Epoch 2010, training loss: 895.133544921875 = 0.5301207304000854 + 100.0 * 8.94603443145752
Epoch 2010, val loss: 0.580360472202301
Epoch 2020, training loss: 895.7098999023438 = 0.5291618704795837 + 100.0 * 8.951807022094727
Epoch 2020, val loss: 0.5798044800758362
Epoch 2030, training loss: 895.4073486328125 = 0.5280858278274536 + 100.0 * 8.948792457580566
Epoch 2030, val loss: 0.5793275833129883
Epoch 2040, training loss: 895.6644897460938 = 0.5271438360214233 + 100.0 * 8.951373100280762
Epoch 2040, val loss: 0.5787339806556702
Epoch 2050, training loss: 895.2846069335938 = 0.5260958671569824 + 100.0 * 8.947585105895996
Epoch 2050, val loss: 0.578197717666626
Epoch 2060, training loss: 895.274169921875 = 0.52518230676651 + 100.0 * 8.947489738464355
Epoch 2060, val loss: 0.5778250098228455
Epoch 2070, training loss: 895.7020263671875 = 0.5242977142333984 + 100.0 * 8.951777458190918
Epoch 2070, val loss: 0.5772064924240112
Epoch 2080, training loss: 896.2061767578125 = 0.5234286189079285 + 100.0 * 8.956827163696289
Epoch 2080, val loss: 0.576926052570343
Epoch 2090, training loss: 894.6134033203125 = 0.5222612023353577 + 100.0 * 8.940911293029785
Epoch 2090, val loss: 0.5761941075325012
Epoch 2100, training loss: 894.987548828125 = 0.5213598012924194 + 100.0 * 8.944662094116211
Epoch 2100, val loss: 0.5757896900177002
Epoch 2110, training loss: 895.794677734375 = 0.5204477310180664 + 100.0 * 8.952742576599121
Epoch 2110, val loss: 0.575469970703125
Epoch 2120, training loss: 896.5226440429688 = 0.519597053527832 + 100.0 * 8.960030555725098
Epoch 2120, val loss: 0.575072705745697
Epoch 2130, training loss: 896.3165283203125 = 0.5186250805854797 + 100.0 * 8.957979202270508
Epoch 2130, val loss: 0.5745260119438171
Epoch 2140, training loss: 896.4595336914062 = 0.5176559090614319 + 100.0 * 8.959419250488281
Epoch 2140, val loss: 0.5741884112358093
Epoch 2150, training loss: 896.7564086914062 = 0.5167500972747803 + 100.0 * 8.962396621704102
Epoch 2150, val loss: 0.5737114548683167
Epoch 2160, training loss: 897.1532592773438 = 0.5158113241195679 + 100.0 * 8.966374397277832
Epoch 2160, val loss: 0.5733384490013123
Epoch 2170, training loss: 897.0821533203125 = 0.514833927154541 + 100.0 * 8.965673446655273
Epoch 2170, val loss: 0.5728870630264282
Epoch 2180, training loss: 896.9468383789062 = 0.5138121843338013 + 100.0 * 8.964330673217773
Epoch 2180, val loss: 0.5724404454231262
Epoch 2190, training loss: 897.2846069335938 = 0.512866199016571 + 100.0 * 8.967717170715332
Epoch 2190, val loss: 0.5718918442726135
Epoch 2200, training loss: 897.6707153320312 = 0.5119134783744812 + 100.0 * 8.971588134765625
Epoch 2200, val loss: 0.5715950131416321
Epoch 2210, training loss: 897.7596435546875 = 0.5109289288520813 + 100.0 * 8.972487449645996
Epoch 2210, val loss: 0.5709313154220581
Epoch 2220, training loss: 897.587646484375 = 0.509905993938446 + 100.0 * 8.97077751159668
Epoch 2220, val loss: 0.570607602596283
Epoch 2230, training loss: 897.7330932617188 = 0.5089817643165588 + 100.0 * 8.972241401672363
Epoch 2230, val loss: 0.5703561902046204
Epoch 2240, training loss: 897.9131469726562 = 0.5080069303512573 + 100.0 * 8.974051475524902
Epoch 2240, val loss: 0.5697915554046631
Epoch 2250, training loss: 898.0841674804688 = 0.5070732831954956 + 100.0 * 8.975770950317383
Epoch 2250, val loss: 0.5695023536682129
Epoch 2260, training loss: 898.0584106445312 = 0.5061126351356506 + 100.0 * 8.975522994995117
Epoch 2260, val loss: 0.5689537525177002
Epoch 2270, training loss: 898.3055419921875 = 0.5051257610321045 + 100.0 * 8.978004455566406
Epoch 2270, val loss: 0.5686535835266113
Epoch 2280, training loss: 898.7953491210938 = 0.5041986107826233 + 100.0 * 8.982911109924316
Epoch 2280, val loss: 0.5682274699211121
Epoch 2290, training loss: 897.421630859375 = 0.5032168030738831 + 100.0 * 8.969183921813965
Epoch 2290, val loss: 0.567611038684845
Epoch 2300, training loss: 896.345947265625 = 0.5023157000541687 + 100.0 * 8.958436012268066
Epoch 2300, val loss: 0.5675588846206665
Epoch 2310, training loss: 896.7451782226562 = 0.501339852809906 + 100.0 * 8.962438583374023
Epoch 2310, val loss: 0.5672439336776733
Epoch 2320, training loss: 897.531982421875 = 0.5003790259361267 + 100.0 * 8.970315933227539
Epoch 2320, val loss: 0.5663802623748779
Epoch 2330, training loss: 897.5420532226562 = 0.49932098388671875 + 100.0 * 8.970427513122559
Epoch 2330, val loss: 0.5664923191070557
Epoch 2340, training loss: 898.052490234375 = 0.4984501302242279 + 100.0 * 8.975540161132812
Epoch 2340, val loss: 0.5662277936935425
Epoch 2350, training loss: 898.79443359375 = 0.4976179003715515 + 100.0 * 8.9829683303833
Epoch 2350, val loss: 0.5657246708869934
Epoch 2360, training loss: 899.1076049804688 = 0.49663102626800537 + 100.0 * 8.986109733581543
Epoch 2360, val loss: 0.5653809309005737
Epoch 2370, training loss: 899.0122680664062 = 0.49558305740356445 + 100.0 * 8.985166549682617
Epoch 2370, val loss: 0.5649914145469666
Epoch 2380, training loss: 899.105224609375 = 0.4946199953556061 + 100.0 * 8.986105918884277
Epoch 2380, val loss: 0.5645231008529663
Epoch 2390, training loss: 899.149169921875 = 0.49363505840301514 + 100.0 * 8.986555099487305
Epoch 2390, val loss: 0.564196765422821
Epoch 2400, training loss: 899.3751831054688 = 0.4926760196685791 + 100.0 * 8.988824844360352
Epoch 2400, val loss: 0.5637527704238892
Epoch 2410, training loss: 899.4110107421875 = 0.491680771112442 + 100.0 * 8.989192962646484
Epoch 2410, val loss: 0.5634073615074158
Epoch 2420, training loss: 899.2241821289062 = 0.49065667390823364 + 100.0 * 8.987335205078125
Epoch 2420, val loss: 0.5631063580513
Epoch 2430, training loss: 899.4873046875 = 0.489707887172699 + 100.0 * 8.989975929260254
Epoch 2430, val loss: 0.5626887083053589
Epoch 2440, training loss: 899.6460571289062 = 0.48870742321014404 + 100.0 * 8.991573333740234
Epoch 2440, val loss: 0.562272846698761
Epoch 2450, training loss: 899.0689697265625 = 0.48768800497055054 + 100.0 * 8.98581314086914
Epoch 2450, val loss: 0.5620346069335938
Epoch 2460, training loss: 899.3655395507812 = 0.4866921603679657 + 100.0 * 8.988788604736328
Epoch 2460, val loss: 0.5615714192390442
Epoch 2470, training loss: 899.8543701171875 = 0.4857579171657562 + 100.0 * 8.993685722351074
Epoch 2470, val loss: 0.5612187385559082
Epoch 2480, training loss: 900.0389404296875 = 0.48480910062789917 + 100.0 * 8.9955415725708
Epoch 2480, val loss: 0.5609835386276245
Epoch 2490, training loss: 899.6895751953125 = 0.48377811908721924 + 100.0 * 8.992057800292969
Epoch 2490, val loss: 0.5603421330451965
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7692753623188405
0.8133739042237196
The final CL Acc:0.76101, 0.00687, The final GNN Acc:0.81468, 0.00282
raw graph: torch.Size([2, 88648])
add edge: torch.Size([2, 110618])
remove edge: torch.Size([2, 66736])
updated graph: torch.Size([2, 88706])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 1014.718505859375 = 1.0876379013061523 + 100.0 * 10.136308670043945
Epoch 0, val loss: 1.089221715927124
Epoch 10, training loss: 971.0451049804688 = 1.084547758102417 + 100.0 * 9.699605941772461
Epoch 10, val loss: 1.0861488580703735
Epoch 20, training loss: 950.237060546875 = 1.081716775894165 + 100.0 * 9.49155330657959
Epoch 20, val loss: 1.0833156108856201
Epoch 30, training loss: 934.926513671875 = 1.0789819955825806 + 100.0 * 9.338475227355957
Epoch 30, val loss: 1.0805827379226685
Epoch 40, training loss: 923.5833129882812 = 1.0764119625091553 + 100.0 * 9.225069046020508
Epoch 40, val loss: 1.0780153274536133
Epoch 50, training loss: 914.5966796875 = 1.0739915370941162 + 100.0 * 9.13522720336914
Epoch 50, val loss: 1.0755926370620728
Epoch 60, training loss: 907.1874389648438 = 1.0717331171035767 + 100.0 * 9.0611572265625
Epoch 60, val loss: 1.0733191967010498
Epoch 70, training loss: 900.9373779296875 = 1.0695858001708984 + 100.0 * 8.998678207397461
Epoch 70, val loss: 1.0711588859558105
Epoch 80, training loss: 895.6481323242188 = 1.067583680152893 + 100.0 * 8.945805549621582
Epoch 80, val loss: 1.0691415071487427
Epoch 90, training loss: 891.0831909179688 = 1.0656887292861938 + 100.0 * 8.900175094604492
Epoch 90, val loss: 1.067218542098999
Epoch 100, training loss: 887.0851440429688 = 1.06389582157135 + 100.0 * 8.860212326049805
Epoch 100, val loss: 1.0653955936431885
Epoch 110, training loss: 883.7273559570312 = 1.0622124671936035 + 100.0 * 8.826651573181152
Epoch 110, val loss: 1.0636935234069824
Epoch 120, training loss: 880.8508911132812 = 1.060584545135498 + 100.0 * 8.797903060913086
Epoch 120, val loss: 1.06203031539917
Epoch 130, training loss: 878.4244384765625 = 1.059046983718872 + 100.0 * 8.773653984069824
Epoch 130, val loss: 1.0604661703109741
Epoch 140, training loss: 876.263427734375 = 1.0575847625732422 + 100.0 * 8.752058982849121
Epoch 140, val loss: 1.0589550733566284
Epoch 150, training loss: 874.3861694335938 = 1.0561439990997314 + 100.0 * 8.73330020904541
Epoch 150, val loss: 1.0574884414672852
Epoch 160, training loss: 872.8621215820312 = 1.0547553300857544 + 100.0 * 8.718073844909668
Epoch 160, val loss: 1.0560649633407593
Epoch 170, training loss: 871.3911743164062 = 1.0533881187438965 + 100.0 * 8.703377723693848
Epoch 170, val loss: 1.054664969444275
Epoch 180, training loss: 870.1506958007812 = 1.0520412921905518 + 100.0 * 8.690986633300781
Epoch 180, val loss: 1.05328369140625
Epoch 190, training loss: 869.1878662109375 = 1.0507436990737915 + 100.0 * 8.681371688842773
Epoch 190, val loss: 1.051959753036499
Epoch 200, training loss: 868.3157348632812 = 1.049425721168518 + 100.0 * 8.672662734985352
Epoch 200, val loss: 1.0506234169006348
Epoch 210, training loss: 867.6226196289062 = 1.0480399131774902 + 100.0 * 8.665745735168457
Epoch 210, val loss: 1.049245834350586
Epoch 220, training loss: 866.9161376953125 = 1.046607255935669 + 100.0 * 8.658695220947266
Epoch 220, val loss: 1.047798991203308
Epoch 230, training loss: 866.3762817382812 = 1.0451147556304932 + 100.0 * 8.653311729431152
Epoch 230, val loss: 1.046295404434204
Epoch 240, training loss: 866.1549072265625 = 1.0436640977859497 + 100.0 * 8.65111255645752
Epoch 240, val loss: 1.0448814630508423
Epoch 250, training loss: 865.7653198242188 = 1.041983962059021 + 100.0 * 8.647233009338379
Epoch 250, val loss: 1.043268084526062
Epoch 260, training loss: 865.4708251953125 = 1.040241003036499 + 100.0 * 8.644306182861328
Epoch 260, val loss: 1.0415700674057007
Epoch 270, training loss: 865.2197265625 = 1.03849196434021 + 100.0 * 8.641812324523926
Epoch 270, val loss: 1.0398191213607788
Epoch 280, training loss: 864.3757934570312 = 1.0363765954971313 + 100.0 * 8.633394241333008
Epoch 280, val loss: 1.0378355979919434
Epoch 290, training loss: 864.5380859375 = 1.0344088077545166 + 100.0 * 8.63503646850586
Epoch 290, val loss: 1.0359045267105103
Epoch 300, training loss: 864.7147827148438 = 1.032219409942627 + 100.0 * 8.636825561523438
Epoch 300, val loss: 1.033768892288208
Epoch 310, training loss: 864.4898681640625 = 1.0297913551330566 + 100.0 * 8.634600639343262
Epoch 310, val loss: 1.0314743518829346
Epoch 320, training loss: 864.5313110351562 = 1.027300238609314 + 100.0 * 8.635040283203125
Epoch 320, val loss: 1.0290815830230713
Epoch 330, training loss: 864.573486328125 = 1.0245428085327148 + 100.0 * 8.635489463806152
Epoch 330, val loss: 1.0264711380004883
Epoch 340, training loss: 864.6172485351562 = 1.021570086479187 + 100.0 * 8.635956764221191
Epoch 340, val loss: 1.023675799369812
Epoch 350, training loss: 864.5557250976562 = 1.018453598022461 + 100.0 * 8.63537311553955
Epoch 350, val loss: 1.0206665992736816
Epoch 360, training loss: 865.0025024414062 = 1.0151548385620117 + 100.0 * 8.639873504638672
Epoch 360, val loss: 1.017500400543213
Epoch 370, training loss: 864.9747924804688 = 1.0115731954574585 + 100.0 * 8.639632225036621
Epoch 370, val loss: 1.0141043663024902
Epoch 380, training loss: 865.1209106445312 = 1.007739782333374 + 100.0 * 8.641131401062012
Epoch 380, val loss: 1.0104453563690186
Epoch 390, training loss: 865.3037109375 = 1.0039082765579224 + 100.0 * 8.642997741699219
Epoch 390, val loss: 1.0067849159240723
Epoch 400, training loss: 865.443115234375 = 0.9996334314346313 + 100.0 * 8.644434928894043
Epoch 400, val loss: 1.0028010606765747
Epoch 410, training loss: 865.7570190429688 = 0.9952035546302795 + 100.0 * 8.647618293762207
Epoch 410, val loss: 0.9985365271568298
Epoch 420, training loss: 865.6597900390625 = 0.9904570579528809 + 100.0 * 8.646693229675293
Epoch 420, val loss: 0.9940618276596069
Epoch 430, training loss: 866.1251831054688 = 0.9855577349662781 + 100.0 * 8.651396751403809
Epoch 430, val loss: 0.989438533782959
Epoch 440, training loss: 866.666259765625 = 0.9804100394248962 + 100.0 * 8.656858444213867
Epoch 440, val loss: 0.9845455884933472
Epoch 450, training loss: 866.1980590820312 = 0.9749159812927246 + 100.0 * 8.652231216430664
Epoch 450, val loss: 0.9794552326202393
Epoch 460, training loss: 866.2731323242188 = 0.9694452881813049 + 100.0 * 8.653037071228027
Epoch 460, val loss: 0.9742053151130676
Epoch 470, training loss: 866.6190185546875 = 0.9636884331703186 + 100.0 * 8.656553268432617
Epoch 470, val loss: 0.9687178134918213
Epoch 480, training loss: 866.608642578125 = 0.9575390815734863 + 100.0 * 8.656511306762695
Epoch 480, val loss: 0.9629564881324768
Epoch 490, training loss: 866.9286499023438 = 0.9514740109443665 + 100.0 * 8.659771919250488
Epoch 490, val loss: 0.957173764705658
Epoch 500, training loss: 867.8140258789062 = 0.9454414248466492 + 100.0 * 8.668685913085938
Epoch 500, val loss: 0.9514111876487732
Epoch 510, training loss: 867.8882446289062 = 0.9388899207115173 + 100.0 * 8.669493675231934
Epoch 510, val loss: 0.9453108906745911
Epoch 520, training loss: 867.7174072265625 = 0.9323990345001221 + 100.0 * 8.667850494384766
Epoch 520, val loss: 0.9391151070594788
Epoch 530, training loss: 868.0979614257812 = 0.9257129430770874 + 100.0 * 8.671722412109375
Epoch 530, val loss: 0.9328305125236511
Epoch 540, training loss: 868.3157348632812 = 0.9188395738601685 + 100.0 * 8.673969268798828
Epoch 540, val loss: 0.9262495636940002
Epoch 550, training loss: 868.1058959960938 = 0.911794126033783 + 100.0 * 8.671940803527832
Epoch 550, val loss: 0.9196057915687561
Epoch 560, training loss: 868.5331420898438 = 0.9047676920890808 + 100.0 * 8.676283836364746
Epoch 560, val loss: 0.9130151271820068
Epoch 570, training loss: 868.7161865234375 = 0.8976922035217285 + 100.0 * 8.678184509277344
Epoch 570, val loss: 0.906367838382721
Epoch 580, training loss: 868.4337158203125 = 0.890323281288147 + 100.0 * 8.675434112548828
Epoch 580, val loss: 0.899434506893158
Epoch 590, training loss: 869.08447265625 = 0.8831289410591125 + 100.0 * 8.682013511657715
Epoch 590, val loss: 0.8925985097885132
Epoch 600, training loss: 869.161376953125 = 0.8759426474571228 + 100.0 * 8.682854652404785
Epoch 600, val loss: 0.8858554363250732
Epoch 610, training loss: 869.4212646484375 = 0.8687588572502136 + 100.0 * 8.685524940490723
Epoch 610, val loss: 0.8790758848190308
Epoch 620, training loss: 869.5650024414062 = 0.8614813685417175 + 100.0 * 8.687034606933594
Epoch 620, val loss: 0.8722016215324402
Epoch 630, training loss: 869.6084594726562 = 0.8541844487190247 + 100.0 * 8.687542915344238
Epoch 630, val loss: 0.8652967214584351
Epoch 640, training loss: 870.4354248046875 = 0.8466695547103882 + 100.0 * 8.695887565612793
Epoch 640, val loss: 0.857866644859314
Epoch 650, training loss: 871.1981811523438 = 0.8412193059921265 + 100.0 * 8.703569412231445
Epoch 650, val loss: 0.8532522916793823
Epoch 660, training loss: 870.1603393554688 = 0.8336693048477173 + 100.0 * 8.693266868591309
Epoch 660, val loss: 0.8462797999382019
Epoch 670, training loss: 870.7926635742188 = 0.8262497186660767 + 100.0 * 8.699664115905762
Epoch 670, val loss: 0.8394537568092346
Epoch 680, training loss: 870.1405639648438 = 0.8187021017074585 + 100.0 * 8.693218231201172
Epoch 680, val loss: 0.8322116136550903
Epoch 690, training loss: 871.226806640625 = 0.8112813830375671 + 100.0 * 8.704154968261719
Epoch 690, val loss: 0.825028121471405
Epoch 700, training loss: 871.783935546875 = 0.8042859435081482 + 100.0 * 8.709796905517578
Epoch 700, val loss: 0.8185165524482727
Epoch 710, training loss: 872.6314697265625 = 0.7970020771026611 + 100.0 * 8.718344688415527
Epoch 710, val loss: 0.8117326498031616
Epoch 720, training loss: 872.914794921875 = 0.7895750999450684 + 100.0 * 8.72125244140625
Epoch 720, val loss: 0.8047448396682739
Epoch 730, training loss: 873.2581787109375 = 0.7823321223258972 + 100.0 * 8.72475814819336
Epoch 730, val loss: 0.7979297637939453
Epoch 740, training loss: 873.726806640625 = 0.7750077247619629 + 100.0 * 8.729517936706543
Epoch 740, val loss: 0.7911927700042725
Epoch 750, training loss: 873.8546142578125 = 0.7677838802337646 + 100.0 * 8.730868339538574
Epoch 750, val loss: 0.784434974193573
Epoch 760, training loss: 874.1640625 = 0.7604755163192749 + 100.0 * 8.73403549194336
Epoch 760, val loss: 0.777625322341919
Epoch 770, training loss: 874.5155029296875 = 0.7533756494522095 + 100.0 * 8.737621307373047
Epoch 770, val loss: 0.771019458770752
Epoch 780, training loss: 874.7279663085938 = 0.7462650537490845 + 100.0 * 8.739816665649414
Epoch 780, val loss: 0.7644100189208984
Epoch 790, training loss: 874.9967651367188 = 0.7393113970756531 + 100.0 * 8.742574691772461
Epoch 790, val loss: 0.7579510807991028
Epoch 800, training loss: 875.474853515625 = 0.7323607802391052 + 100.0 * 8.747425079345703
Epoch 800, val loss: 0.7514277100563049
Epoch 810, training loss: 875.430908203125 = 0.7252141833305359 + 100.0 * 8.74705696105957
Epoch 810, val loss: 0.7448760271072388
Epoch 820, training loss: 875.8794555664062 = 0.7183927297592163 + 100.0 * 8.75161075592041
Epoch 820, val loss: 0.7384937405586243
Epoch 830, training loss: 876.0681762695312 = 0.7114766836166382 + 100.0 * 8.75356674194336
Epoch 830, val loss: 0.7320066690444946
Epoch 840, training loss: 876.3864135742188 = 0.7047397494316101 + 100.0 * 8.756816864013672
Epoch 840, val loss: 0.7258152365684509
Epoch 850, training loss: 876.6121826171875 = 0.6976731419563293 + 100.0 * 8.75914478302002
Epoch 850, val loss: 0.7193304896354675
Epoch 860, training loss: 876.2575073242188 = 0.691304087638855 + 100.0 * 8.755661964416504
Epoch 860, val loss: 0.7133690714836121
Epoch 870, training loss: 876.9669799804688 = 0.6845556497573853 + 100.0 * 8.762824058532715
Epoch 870, val loss: 0.707207202911377
Epoch 880, training loss: 877.2243041992188 = 0.67806077003479 + 100.0 * 8.765462875366211
Epoch 880, val loss: 0.7012292146682739
Epoch 890, training loss: 877.6632690429688 = 0.6715022325515747 + 100.0 * 8.769917488098145
Epoch 890, val loss: 0.6951697468757629
Epoch 900, training loss: 877.9146118164062 = 0.6650977730751038 + 100.0 * 8.77249526977539
Epoch 900, val loss: 0.6892000436782837
Epoch 910, training loss: 877.17138671875 = 0.658725917339325 + 100.0 * 8.76512622833252
Epoch 910, val loss: 0.6837109327316284
Epoch 920, training loss: 877.3172607421875 = 0.6524084806442261 + 100.0 * 8.766648292541504
Epoch 920, val loss: 0.6774256229400635
Epoch 930, training loss: 878.021728515625 = 0.646449089050293 + 100.0 * 8.77375316619873
Epoch 930, val loss: 0.6720187067985535
Epoch 940, training loss: 878.623291015625 = 0.6405180096626282 + 100.0 * 8.779828071594238
Epoch 940, val loss: 0.6665635704994202
Epoch 950, training loss: 878.6098022460938 = 0.6343607306480408 + 100.0 * 8.779754638671875
Epoch 950, val loss: 0.6608724594116211
Epoch 960, training loss: 878.9653930664062 = 0.6284086108207703 + 100.0 * 8.783370018005371
Epoch 960, val loss: 0.6556674242019653
Epoch 970, training loss: 880.7653198242188 = 0.6244717240333557 + 100.0 * 8.801408767700195
Epoch 970, val loss: 0.6516916155815125
Epoch 980, training loss: 880.6393432617188 = 0.6185541152954102 + 100.0 * 8.80020809173584
Epoch 980, val loss: 0.6464009284973145
Epoch 990, training loss: 880.9421997070312 = 0.613171398639679 + 100.0 * 8.803290367126465
Epoch 990, val loss: 0.6413800716400146
Epoch 1000, training loss: 881.7131958007812 = 0.6078721880912781 + 100.0 * 8.811053276062012
Epoch 1000, val loss: 0.6365972757339478
Epoch 1010, training loss: 881.6640625 = 0.6025977730751038 + 100.0 * 8.810614585876465
Epoch 1010, val loss: 0.6316993832588196
Epoch 1020, training loss: 881.93701171875 = 0.5973528027534485 + 100.0 * 8.813396453857422
Epoch 1020, val loss: 0.6269391179084778
Epoch 1030, training loss: 882.0156860351562 = 0.5922638773918152 + 100.0 * 8.814233779907227
Epoch 1030, val loss: 0.6222225427627563
Epoch 1040, training loss: 882.38232421875 = 0.5872375965118408 + 100.0 * 8.817951202392578
Epoch 1040, val loss: 0.6176350712776184
Epoch 1050, training loss: 882.6324462890625 = 0.5825213193893433 + 100.0 * 8.820499420166016
Epoch 1050, val loss: 0.6132504343986511
Epoch 1060, training loss: 882.7277221679688 = 0.5776600241661072 + 100.0 * 8.821500778198242
Epoch 1060, val loss: 0.6087856292724609
Epoch 1070, training loss: 882.3968505859375 = 0.5727686882019043 + 100.0 * 8.818241119384766
Epoch 1070, val loss: 0.6043650507926941
Epoch 1080, training loss: 883.185302734375 = 0.5679758191108704 + 100.0 * 8.826172828674316
Epoch 1080, val loss: 0.6002035140991211
Epoch 1090, training loss: 883.2304077148438 = 0.5631729364395142 + 100.0 * 8.826672554016113
Epoch 1090, val loss: 0.5958362817764282
Epoch 1100, training loss: 883.877197265625 = 0.5586352944374084 + 100.0 * 8.833185195922852
Epoch 1100, val loss: 0.5918349623680115
Epoch 1110, training loss: 883.7755737304688 = 0.5542441606521606 + 100.0 * 8.832213401794434
Epoch 1110, val loss: 0.5880581140518188
Epoch 1120, training loss: 883.965087890625 = 0.5497278571128845 + 100.0 * 8.834153175354004
Epoch 1120, val loss: 0.5836694836616516
Epoch 1130, training loss: 884.4068603515625 = 0.5452339053153992 + 100.0 * 8.838616371154785
Epoch 1130, val loss: 0.5798282027244568
Epoch 1140, training loss: 884.383544921875 = 0.5409457683563232 + 100.0 * 8.838425636291504
Epoch 1140, val loss: 0.5760385394096375
Epoch 1150, training loss: 884.4427490234375 = 0.5368571877479553 + 100.0 * 8.839058876037598
Epoch 1150, val loss: 0.5723505020141602
Epoch 1160, training loss: 884.9888916015625 = 0.5330011248588562 + 100.0 * 8.844558715820312
Epoch 1160, val loss: 0.5690901875495911
Epoch 1170, training loss: 884.3775024414062 = 0.5290758013725281 + 100.0 * 8.838484764099121
Epoch 1170, val loss: 0.5653959512710571
Epoch 1180, training loss: 884.1898193359375 = 0.5251854062080383 + 100.0 * 8.83664608001709
Epoch 1180, val loss: 0.5621920228004456
Epoch 1190, training loss: 884.532470703125 = 0.5217615365982056 + 100.0 * 8.840106964111328
Epoch 1190, val loss: 0.5590729117393494
Epoch 1200, training loss: 885.08935546875 = 0.518545925617218 + 100.0 * 8.845707893371582
Epoch 1200, val loss: 0.5562663674354553
Epoch 1210, training loss: 885.5696411132812 = 0.5151100158691406 + 100.0 * 8.850544929504395
Epoch 1210, val loss: 0.5533586144447327
Epoch 1220, training loss: 884.8682250976562 = 0.5116971731185913 + 100.0 * 8.843564987182617
Epoch 1220, val loss: 0.550280749797821
Epoch 1230, training loss: 884.96435546875 = 0.5083500742912292 + 100.0 * 8.844559669494629
Epoch 1230, val loss: 0.5474633574485779
Epoch 1240, training loss: 885.6478881835938 = 0.5054695010185242 + 100.0 * 8.851424217224121
Epoch 1240, val loss: 0.5449950695037842
Epoch 1250, training loss: 886.4112548828125 = 0.5026395320892334 + 100.0 * 8.859086036682129
Epoch 1250, val loss: 0.5427326560020447
Epoch 1260, training loss: 886.7736206054688 = 0.49990320205688477 + 100.0 * 8.862737655639648
Epoch 1260, val loss: 0.5401368737220764
Epoch 1270, training loss: 886.0509643554688 = 0.49679499864578247 + 100.0 * 8.855542182922363
Epoch 1270, val loss: 0.537668764591217
Epoch 1280, training loss: 886.6864624023438 = 0.49414676427841187 + 100.0 * 8.861923217773438
Epoch 1280, val loss: 0.5353232026100159
Epoch 1290, training loss: 887.1640014648438 = 0.49153968691825867 + 100.0 * 8.866724967956543
Epoch 1290, val loss: 0.5332591533660889
Epoch 1300, training loss: 886.201171875 = 0.4887431263923645 + 100.0 * 8.857124328613281
Epoch 1300, val loss: 0.530434787273407
Epoch 1310, training loss: 887.4852905273438 = 0.4859548807144165 + 100.0 * 8.869993209838867
Epoch 1310, val loss: 0.5285019278526306
Epoch 1320, training loss: 884.7053833007812 = 0.48346561193466187 + 100.0 * 8.842219352722168
Epoch 1320, val loss: 0.5263144969940186
Epoch 1330, training loss: 886.6671142578125 = 0.48193663358688354 + 100.0 * 8.861851692199707
Epoch 1330, val loss: 0.525268018245697
Epoch 1340, training loss: 887.89501953125 = 0.4797692596912384 + 100.0 * 8.874152183532715
Epoch 1340, val loss: 0.5237058997154236
Epoch 1350, training loss: 888.7855834960938 = 0.4774062931537628 + 100.0 * 8.883081436157227
Epoch 1350, val loss: 0.5214990377426147
Epoch 1360, training loss: 888.5936279296875 = 0.47500258684158325 + 100.0 * 8.881186485290527
Epoch 1360, val loss: 0.5196875333786011
Epoch 1370, training loss: 889.0311279296875 = 0.4727173149585724 + 100.0 * 8.885583877563477
Epoch 1370, val loss: 0.517558217048645
Epoch 1380, training loss: 889.709228515625 = 0.47076672315597534 + 100.0 * 8.89238452911377
Epoch 1380, val loss: 0.515998899936676
Epoch 1390, training loss: 890.184326171875 = 0.46873795986175537 + 100.0 * 8.89715576171875
Epoch 1390, val loss: 0.5145285725593567
Epoch 1400, training loss: 890.362060546875 = 0.46676725149154663 + 100.0 * 8.89895248413086
Epoch 1400, val loss: 0.512683093547821
Epoch 1410, training loss: 890.358154296875 = 0.46473661065101624 + 100.0 * 8.898934364318848
Epoch 1410, val loss: 0.5111679434776306
Epoch 1420, training loss: 888.4899291992188 = 0.46319901943206787 + 100.0 * 8.880267143249512
Epoch 1420, val loss: 0.5100356936454773
Epoch 1430, training loss: 889.5244750976562 = 0.46160373091697693 + 100.0 * 8.890628814697266
Epoch 1430, val loss: 0.5083665251731873
Epoch 1440, training loss: 889.9774169921875 = 0.45965197682380676 + 100.0 * 8.895177841186523
Epoch 1440, val loss: 0.5071465373039246
Epoch 1450, training loss: 890.2124633789062 = 0.4575243890285492 + 100.0 * 8.897549629211426
Epoch 1450, val loss: 0.5054466724395752
Epoch 1460, training loss: 890.8490600585938 = 0.4558035731315613 + 100.0 * 8.903932571411133
Epoch 1460, val loss: 0.5040452480316162
Epoch 1470, training loss: 891.5293579101562 = 0.45406198501586914 + 100.0 * 8.91075325012207
Epoch 1470, val loss: 0.5026580095291138
Epoch 1480, training loss: 891.2847900390625 = 0.45226314663887024 + 100.0 * 8.9083251953125
Epoch 1480, val loss: 0.501287043094635
Epoch 1490, training loss: 891.6703491210938 = 0.45063596963882446 + 100.0 * 8.91219711303711
Epoch 1490, val loss: 0.4999544024467468
Epoch 1500, training loss: 891.9778442382812 = 0.4490082263946533 + 100.0 * 8.915287971496582
Epoch 1500, val loss: 0.4987636208534241
Epoch 1510, training loss: 892.4188232421875 = 0.44743412733078003 + 100.0 * 8.919713973999023
Epoch 1510, val loss: 0.49736538529396057
Epoch 1520, training loss: 892.2131958007812 = 0.44580352306365967 + 100.0 * 8.91767406463623
Epoch 1520, val loss: 0.4962720274925232
Epoch 1530, training loss: 892.53173828125 = 0.44428354501724243 + 100.0 * 8.92087459564209
Epoch 1530, val loss: 0.4949970543384552
Epoch 1540, training loss: 892.8896484375 = 0.4427777826786041 + 100.0 * 8.924468994140625
Epoch 1540, val loss: 0.49393248558044434
Epoch 1550, training loss: 892.3067626953125 = 0.44124290347099304 + 100.0 * 8.918655395507812
Epoch 1550, val loss: 0.4928186237812042
Epoch 1560, training loss: 892.7976684570312 = 0.4397832155227661 + 100.0 * 8.923579216003418
Epoch 1560, val loss: 0.49160775542259216
Epoch 1570, training loss: 893.36865234375 = 0.4384182393550873 + 100.0 * 8.929302215576172
Epoch 1570, val loss: 0.4905579686164856
Epoch 1580, training loss: 893.4961547851562 = 0.43700549006462097 + 100.0 * 8.930591583251953
Epoch 1580, val loss: 0.4896080195903778
Epoch 1590, training loss: 893.6180419921875 = 0.4355716407299042 + 100.0 * 8.931824684143066
Epoch 1590, val loss: 0.4885543882846832
Epoch 1600, training loss: 893.4806518554688 = 0.434175044298172 + 100.0 * 8.930464744567871
Epoch 1600, val loss: 0.4874686002731323
Epoch 1610, training loss: 893.0726318359375 = 0.4331229031085968 + 100.0 * 8.926395416259766
Epoch 1610, val loss: 0.486323744058609
Epoch 1620, training loss: 891.9908447265625 = 0.43170708417892456 + 100.0 * 8.9155912399292
Epoch 1620, val loss: 0.48641452193260193
Epoch 1630, training loss: 892.6821899414062 = 0.43024611473083496 + 100.0 * 8.92251968383789
Epoch 1630, val loss: 0.4844959080219269
Epoch 1640, training loss: 893.080322265625 = 0.4289745092391968 + 100.0 * 8.926513671875
Epoch 1640, val loss: 0.483995258808136
Epoch 1650, training loss: 893.8333740234375 = 0.4277248680591583 + 100.0 * 8.934056282043457
Epoch 1650, val loss: 0.4830317497253418
Epoch 1660, training loss: 894.4767456054688 = 0.42650800943374634 + 100.0 * 8.940502166748047
Epoch 1660, val loss: 0.4820149540901184
Epoch 1670, training loss: 894.3438720703125 = 0.4251900613307953 + 100.0 * 8.939187049865723
Epoch 1670, val loss: 0.4810699224472046
Epoch 1680, training loss: 894.6570434570312 = 0.4239180386066437 + 100.0 * 8.942331314086914
Epoch 1680, val loss: 0.4801918566226959
Epoch 1690, training loss: 894.6480712890625 = 0.4226442277431488 + 100.0 * 8.942254066467285
Epoch 1690, val loss: 0.4793393015861511
Epoch 1700, training loss: 893.7216186523438 = 0.42140594124794006 + 100.0 * 8.933002471923828
Epoch 1700, val loss: 0.4784766137599945
Epoch 1710, training loss: 893.8928833007812 = 0.42023348808288574 + 100.0 * 8.93472671508789
Epoch 1710, val loss: 0.47774288058280945
Epoch 1720, training loss: 894.5119018554688 = 0.4190455675125122 + 100.0 * 8.94092845916748
Epoch 1720, val loss: 0.47687339782714844
Epoch 1730, training loss: 894.9426879882812 = 0.4178851544857025 + 100.0 * 8.945247650146484
Epoch 1730, val loss: 0.4761543273925781
Epoch 1740, training loss: 895.3982543945312 = 0.4167502224445343 + 100.0 * 8.949814796447754
Epoch 1740, val loss: 0.47527626156806946
Epoch 1750, training loss: 895.2942504882812 = 0.4155811369419098 + 100.0 * 8.948786735534668
Epoch 1750, val loss: 0.474441796541214
Epoch 1760, training loss: 895.6326293945312 = 0.4144520163536072 + 100.0 * 8.952181816101074
Epoch 1760, val loss: 0.47372621297836304
Epoch 1770, training loss: 895.1976928710938 = 0.4132399260997772 + 100.0 * 8.947844505310059
Epoch 1770, val loss: 0.47297993302345276
Epoch 1780, training loss: 895.0469360351562 = 0.4121386408805847 + 100.0 * 8.946348190307617
Epoch 1780, val loss: 0.47216686606407166
Epoch 1790, training loss: 895.1777954101562 = 0.41105207800865173 + 100.0 * 8.947667121887207
Epoch 1790, val loss: 0.4716338515281677
Epoch 1800, training loss: 895.99951171875 = 0.41002246737480164 + 100.0 * 8.955894470214844
Epoch 1800, val loss: 0.4708541929721832
Epoch 1810, training loss: 896.3366088867188 = 0.4089930057525635 + 100.0 * 8.95927619934082
Epoch 1810, val loss: 0.47020992636680603
Epoch 1820, training loss: 896.45751953125 = 0.407894492149353 + 100.0 * 8.960495948791504
Epoch 1820, val loss: 0.46961256861686707
Epoch 1830, training loss: 896.7459716796875 = 0.40684711933135986 + 100.0 * 8.963391304016113
Epoch 1830, val loss: 0.4688514769077301
Epoch 1840, training loss: 897.0598754882812 = 0.4057930111885071 + 100.0 * 8.966540336608887
Epoch 1840, val loss: 0.4682559072971344
Epoch 1850, training loss: 897.0172729492188 = 0.4047331213951111 + 100.0 * 8.96612548828125
Epoch 1850, val loss: 0.4676054120063782
Epoch 1860, training loss: 897.4364013671875 = 0.4036960005760193 + 100.0 * 8.970327377319336
Epoch 1860, val loss: 0.4668545126914978
Epoch 1870, training loss: 897.5010986328125 = 0.4026187062263489 + 100.0 * 8.97098445892334
Epoch 1870, val loss: 0.46632635593414307
Epoch 1880, training loss: 897.463134765625 = 0.401591956615448 + 100.0 * 8.97061538696289
Epoch 1880, val loss: 0.4655871093273163
Epoch 1890, training loss: 897.6012573242188 = 0.40059563517570496 + 100.0 * 8.972006797790527
Epoch 1890, val loss: 0.46506190299987793
Epoch 1900, training loss: 896.719482421875 = 0.39954814314842224 + 100.0 * 8.963199615478516
Epoch 1900, val loss: 0.4642117917537689
Epoch 1910, training loss: 895.53564453125 = 0.39847609400749207 + 100.0 * 8.951371192932129
Epoch 1910, val loss: 0.46358585357666016
Epoch 1920, training loss: 895.42041015625 = 0.3972705900669098 + 100.0 * 8.950231552124023
Epoch 1920, val loss: 0.4628277122974396
Epoch 1930, training loss: 894.38330078125 = 0.3962354063987732 + 100.0 * 8.939870834350586
Epoch 1930, val loss: 0.46192389726638794
Epoch 1940, training loss: 895.9874267578125 = 0.3955247402191162 + 100.0 * 8.95591926574707
Epoch 1940, val loss: 0.46247556805610657
Epoch 1950, training loss: 896.7988891601562 = 0.3944105803966522 + 100.0 * 8.964044570922852
Epoch 1950, val loss: 0.46127787232398987
Epoch 1960, training loss: 897.4195556640625 = 0.3933916687965393 + 100.0 * 8.970261573791504
Epoch 1960, val loss: 0.4607539474964142
Epoch 1970, training loss: 898.0892944335938 = 0.39249736070632935 + 100.0 * 8.976967811584473
Epoch 1970, val loss: 0.460277259349823
Epoch 1980, training loss: 898.52978515625 = 0.3915485739707947 + 100.0 * 8.981382369995117
Epoch 1980, val loss: 0.45956647396087646
Epoch 1990, training loss: 898.558837890625 = 0.3905728757381439 + 100.0 * 8.981682777404785
Epoch 1990, val loss: 0.4591772258281708
Epoch 2000, training loss: 898.8391723632812 = 0.3895963728427887 + 100.0 * 8.984496116638184
Epoch 2000, val loss: 0.45843705534935
Epoch 2010, training loss: 898.9400024414062 = 0.38862571120262146 + 100.0 * 8.985513687133789
Epoch 2010, val loss: 0.45787152647972107
Epoch 2020, training loss: 898.63037109375 = 0.3876371681690216 + 100.0 * 8.982427597045898
Epoch 2020, val loss: 0.45720309019088745
Epoch 2030, training loss: 899.1903076171875 = 0.3867107629776001 + 100.0 * 8.988036155700684
Epoch 2030, val loss: 0.4567336440086365
Epoch 2040, training loss: 899.272216796875 = 0.3857608139514923 + 100.0 * 8.98886489868164
Epoch 2040, val loss: 0.4561227560043335
Epoch 2050, training loss: 899.4580688476562 = 0.3847864270210266 + 100.0 * 8.99073314666748
Epoch 2050, val loss: 0.45563650131225586
Epoch 2060, training loss: 899.6805419921875 = 0.38384324312210083 + 100.0 * 8.992966651916504
Epoch 2060, val loss: 0.4550662040710449
Epoch 2070, training loss: 899.50146484375 = 0.38289591670036316 + 100.0 * 8.991186141967773
Epoch 2070, val loss: 0.4545722007751465
Epoch 2080, training loss: 899.4082641601562 = 0.3819533586502075 + 100.0 * 8.990262985229492
Epoch 2080, val loss: 0.4540056884288788
Epoch 2090, training loss: 900.0747680664062 = 0.38108694553375244 + 100.0 * 8.996936798095703
Epoch 2090, val loss: 0.45348861813545227
Epoch 2100, training loss: 900.2960205078125 = 0.3801446259021759 + 100.0 * 8.99915885925293
Epoch 2100, val loss: 0.45298415422439575
Epoch 2110, training loss: 900.240234375 = 0.3791999816894531 + 100.0 * 8.998610496520996
Epoch 2110, val loss: 0.4524837136268616
Epoch 2120, training loss: 899.955810546875 = 0.37849634885787964 + 100.0 * 8.995773315429688
Epoch 2120, val loss: 0.45198091864585876
Epoch 2130, training loss: 899.712646484375 = 0.3775697946548462 + 100.0 * 8.993350982666016
Epoch 2130, val loss: 0.4517858922481537
Epoch 2140, training loss: 897.6572875976562 = 0.37641414999961853 + 100.0 * 8.972808837890625
Epoch 2140, val loss: 0.4510783851146698
Epoch 2150, training loss: 898.1406860351562 = 0.3756008744239807 + 100.0 * 8.97765064239502
Epoch 2150, val loss: 0.4501056373119354
Epoch 2160, training loss: 898.4833374023438 = 0.3746805489063263 + 100.0 * 8.981086730957031
Epoch 2160, val loss: 0.450093150138855
Epoch 2170, training loss: 899.266845703125 = 0.3738988935947418 + 100.0 * 8.988929748535156
Epoch 2170, val loss: 0.4498644173145294
Epoch 2180, training loss: 899.7150268554688 = 0.3730582892894745 + 100.0 * 8.993419647216797
Epoch 2180, val loss: 0.4490535259246826
Epoch 2190, training loss: 900.4046630859375 = 0.37222614884376526 + 100.0 * 9.000324249267578
Epoch 2190, val loss: 0.4489384889602661
Epoch 2200, training loss: 900.8010864257812 = 0.3713502883911133 + 100.0 * 9.004297256469727
Epoch 2200, val loss: 0.4482932686805725
Epoch 2210, training loss: 900.9445190429688 = 0.37041160464286804 + 100.0 * 9.005741119384766
Epoch 2210, val loss: 0.44789642095565796
Epoch 2220, training loss: 901.148681640625 = 0.36950433254241943 + 100.0 * 9.007791519165039
Epoch 2220, val loss: 0.4474448263645172
Epoch 2230, training loss: 901.4252319335938 = 0.3685932755470276 + 100.0 * 9.010566711425781
Epoch 2230, val loss: 0.44683539867401123
Epoch 2240, training loss: 901.1951904296875 = 0.3676837384700775 + 100.0 * 9.008275032043457
Epoch 2240, val loss: 0.4464840590953827
Epoch 2250, training loss: 901.5199584960938 = 0.3667756915092468 + 100.0 * 9.011531829833984
Epoch 2250, val loss: 0.4459554851055145
Epoch 2260, training loss: 901.66357421875 = 0.36585402488708496 + 100.0 * 9.012977600097656
Epoch 2260, val loss: 0.4454877972602844
Epoch 2270, training loss: 901.8675537109375 = 0.36493974924087524 + 100.0 * 9.015026092529297
Epoch 2270, val loss: 0.4450593888759613
Epoch 2280, training loss: 901.9510498046875 = 0.36403971910476685 + 100.0 * 9.015870094299316
Epoch 2280, val loss: 0.4444809854030609
Epoch 2290, training loss: 902.1378784179688 = 0.3631192147731781 + 100.0 * 9.01774787902832
Epoch 2290, val loss: 0.44407427310943604
Epoch 2300, training loss: 902.063232421875 = 0.36220186948776245 + 100.0 * 9.017010688781738
Epoch 2300, val loss: 0.4435631334781647
Epoch 2310, training loss: 902.0248413085938 = 0.3612644672393799 + 100.0 * 9.01663589477539
Epoch 2310, val loss: 0.44307783246040344
Epoch 2320, training loss: 902.54833984375 = 0.36037197709083557 + 100.0 * 9.021880149841309
Epoch 2320, val loss: 0.44254201650619507
Epoch 2330, training loss: 902.3552856445312 = 0.35945385694503784 + 100.0 * 9.01995849609375
Epoch 2330, val loss: 0.44209232926368713
Epoch 2340, training loss: 902.6471557617188 = 0.3585229218006134 + 100.0 * 9.022886276245117
Epoch 2340, val loss: 0.44164741039276123
Epoch 2350, training loss: 902.4019165039062 = 0.357570618391037 + 100.0 * 9.0204439163208
Epoch 2350, val loss: 0.44113925099372864
Epoch 2360, training loss: 900.1300048828125 = 0.3568626344203949 + 100.0 * 8.99773120880127
Epoch 2360, val loss: 0.44163212180137634
Epoch 2370, training loss: 900.2095947265625 = 0.35594648122787476 + 100.0 * 8.998536109924316
Epoch 2370, val loss: 0.4410330057144165
Epoch 2380, training loss: 898.0829467773438 = 0.35487377643585205 + 100.0 * 8.977280616760254
Epoch 2380, val loss: 0.4401477873325348
Epoch 2390, training loss: 899.2911376953125 = 0.35411500930786133 + 100.0 * 8.989370346069336
Epoch 2390, val loss: 0.43981271982192993
Epoch 2400, training loss: 900.064697265625 = 0.3533821702003479 + 100.0 * 8.997113227844238
Epoch 2400, val loss: 0.4385986626148224
Epoch 2410, training loss: 900.9585571289062 = 0.35240811109542847 + 100.0 * 9.006061553955078
Epoch 2410, val loss: 0.43847331404685974
Epoch 2420, training loss: 901.7021484375 = 0.3514706790447235 + 100.0 * 9.013506889343262
Epoch 2420, val loss: 0.43834760785102844
Epoch 2430, training loss: 902.2189331054688 = 0.3505497872829437 + 100.0 * 9.018684387207031
Epoch 2430, val loss: 0.43775948882102966
Epoch 2440, training loss: 902.4861450195312 = 0.3496124744415283 + 100.0 * 9.02136516571045
Epoch 2440, val loss: 0.4374173581600189
Epoch 2450, training loss: 902.7960205078125 = 0.3486817479133606 + 100.0 * 9.024473190307617
Epoch 2450, val loss: 0.4369904100894928
Epoch 2460, training loss: 902.6111450195312 = 0.3477197587490082 + 100.0 * 9.022634506225586
Epoch 2460, val loss: 0.43653205037117004
Epoch 2470, training loss: 902.9845581054688 = 0.34678611159324646 + 100.0 * 9.02637767791748
Epoch 2470, val loss: 0.43617522716522217
Epoch 2480, training loss: 903.3093872070312 = 0.34585317969322205 + 100.0 * 9.029635429382324
Epoch 2480, val loss: 0.43571212887763977
Epoch 2490, training loss: 903.0079345703125 = 0.34488987922668457 + 100.0 * 9.026630401611328
Epoch 2490, val loss: 0.4354117512702942
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8337681159420289
0.8640875172064045
=== training gcn model ===
Epoch 0, training loss: 1015.1475219726562 = 1.0909490585327148 + 100.0 * 10.140565872192383
Epoch 0, val loss: 1.089617371559143
Epoch 10, training loss: 970.3526000976562 = 1.087734341621399 + 100.0 * 9.692648887634277
Epoch 10, val loss: 1.0864678621292114
Epoch 20, training loss: 950.8180541992188 = 1.084702730178833 + 100.0 * 9.497333526611328
Epoch 20, val loss: 1.0834962129592896
Epoch 30, training loss: 936.2119750976562 = 1.0817792415618896 + 100.0 * 9.351302146911621
Epoch 30, val loss: 1.0806469917297363
Epoch 40, training loss: 925.0923461914062 = 1.0790255069732666 + 100.0 * 9.240133285522461
Epoch 40, val loss: 1.077973484992981
Epoch 50, training loss: 916.2953491210938 = 1.0764167308807373 + 100.0 * 9.152189254760742
Epoch 50, val loss: 1.0754472017288208
Epoch 60, training loss: 909.0487060546875 = 1.0739506483078003 + 100.0 * 9.079747200012207
Epoch 60, val loss: 1.0730695724487305
Epoch 70, training loss: 902.9639282226562 = 1.07163405418396 + 100.0 * 9.018922805786133
Epoch 70, val loss: 1.0708448886871338
Epoch 80, training loss: 897.6961059570312 = 1.0694390535354614 + 100.0 * 8.966266632080078
Epoch 80, val loss: 1.0687466859817505
Epoch 90, training loss: 893.1841430664062 = 1.0673784017562866 + 100.0 * 8.921167373657227
Epoch 90, val loss: 1.0667860507965088
Epoch 100, training loss: 889.3076782226562 = 1.0654418468475342 + 100.0 * 8.88242244720459
Epoch 100, val loss: 1.0649490356445312
Epoch 110, training loss: 886.2288208007812 = 1.0635857582092285 + 100.0 * 8.851652145385742
Epoch 110, val loss: 1.0631930828094482
Epoch 120, training loss: 883.30810546875 = 1.0618611574172974 + 100.0 * 8.82246208190918
Epoch 120, val loss: 1.0615805387496948
Epoch 130, training loss: 880.847412109375 = 1.060217022895813 + 100.0 * 8.797871589660645
Epoch 130, val loss: 1.060039758682251
Epoch 140, training loss: 878.7496337890625 = 1.0586962699890137 + 100.0 * 8.776908874511719
Epoch 140, val loss: 1.0586230754852295
Epoch 150, training loss: 876.7687377929688 = 1.0572044849395752 + 100.0 * 8.757115364074707
Epoch 150, val loss: 1.0572435855865479
Epoch 160, training loss: 875.065673828125 = 1.0557901859283447 + 100.0 * 8.74009895324707
Epoch 160, val loss: 1.0559343099594116
Epoch 170, training loss: 873.72119140625 = 1.0544612407684326 + 100.0 * 8.726667404174805
Epoch 170, val loss: 1.0547022819519043
Epoch 180, training loss: 872.361572265625 = 1.0531418323516846 + 100.0 * 8.71308422088623
Epoch 180, val loss: 1.0534815788269043
Epoch 190, training loss: 871.2774047851562 = 1.0518817901611328 + 100.0 * 8.702255249023438
Epoch 190, val loss: 1.0523102283477783
Epoch 200, training loss: 870.2107543945312 = 1.0506112575531006 + 100.0 * 8.691601753234863
Epoch 200, val loss: 1.0511289834976196
Epoch 210, training loss: 869.283447265625 = 1.0493155717849731 + 100.0 * 8.682341575622559
Epoch 210, val loss: 1.0499308109283447
Epoch 220, training loss: 868.6751098632812 = 1.0480364561080933 + 100.0 * 8.676270484924316
Epoch 220, val loss: 1.0487254858016968
Epoch 230, training loss: 868.1141357421875 = 1.0467158555984497 + 100.0 * 8.670674324035645
Epoch 230, val loss: 1.047500729560852
Epoch 240, training loss: 867.5711059570312 = 1.045326828956604 + 100.0 * 8.665257453918457
Epoch 240, val loss: 1.0461721420288086
Epoch 250, training loss: 867.0775756835938 = 1.043832540512085 + 100.0 * 8.660337448120117
Epoch 250, val loss: 1.044732689857483
Epoch 260, training loss: 866.8355712890625 = 1.0423330068588257 + 100.0 * 8.65793228149414
Epoch 260, val loss: 1.0432910919189453
Epoch 270, training loss: 866.686767578125 = 1.040755271911621 + 100.0 * 8.65645980834961
Epoch 270, val loss: 1.0417817831039429
Epoch 280, training loss: 865.9558715820312 = 1.0388773679733276 + 100.0 * 8.649169921875
Epoch 280, val loss: 1.040018081665039
Epoch 290, training loss: 866.4002685546875 = 1.037183165550232 + 100.0 * 8.653631210327148
Epoch 290, val loss: 1.0383477210998535
Epoch 300, training loss: 865.7689819335938 = 1.0351011753082275 + 100.0 * 8.6473388671875
Epoch 300, val loss: 1.0364124774932861
Epoch 310, training loss: 865.5872802734375 = 1.033058524131775 + 100.0 * 8.64554214477539
Epoch 310, val loss: 1.0344041585922241
Epoch 320, training loss: 865.5051879882812 = 1.0307923555374146 + 100.0 * 8.644743919372559
Epoch 320, val loss: 1.0322266817092896
Epoch 330, training loss: 865.3383178710938 = 1.0283180475234985 + 100.0 * 8.643099784851074
Epoch 330, val loss: 1.0298166275024414
Epoch 340, training loss: 865.4947509765625 = 1.0256544351577759 + 100.0 * 8.644691467285156
Epoch 340, val loss: 1.0271952152252197
Epoch 350, training loss: 864.9998779296875 = 1.0227974653244019 + 100.0 * 8.6397705078125
Epoch 350, val loss: 1.024450421333313
Epoch 360, training loss: 865.04248046875 = 1.0196778774261475 + 100.0 * 8.640228271484375
Epoch 360, val loss: 1.021466851234436
Epoch 370, training loss: 865.1027221679688 = 1.0164588689804077 + 100.0 * 8.640862464904785
Epoch 370, val loss: 1.018370270729065
Epoch 380, training loss: 865.3193969726562 = 1.0129581689834595 + 100.0 * 8.643064498901367
Epoch 380, val loss: 1.0149929523468018
Epoch 390, training loss: 865.2969970703125 = 1.0091750621795654 + 100.0 * 8.642878532409668
Epoch 390, val loss: 1.0113948583602905
Epoch 400, training loss: 865.0278930664062 = 1.0052192211151123 + 100.0 * 8.640226364135742
Epoch 400, val loss: 1.0075559616088867
Epoch 410, training loss: 865.1107788085938 = 1.001004934310913 + 100.0 * 8.641098022460938
Epoch 410, val loss: 1.0034637451171875
Epoch 420, training loss: 865.3386840820312 = 0.9964534640312195 + 100.0 * 8.64342212677002
Epoch 420, val loss: 0.9990987777709961
Epoch 430, training loss: 865.1508178710938 = 0.9918728470802307 + 100.0 * 8.641589164733887
Epoch 430, val loss: 0.9946219325065613
Epoch 440, training loss: 865.6756591796875 = 0.9871886372566223 + 100.0 * 8.64688491821289
Epoch 440, val loss: 0.9901033043861389
Epoch 450, training loss: 865.7467651367188 = 0.9820013046264648 + 100.0 * 8.647647857666016
Epoch 450, val loss: 0.9852274060249329
Epoch 460, training loss: 865.8023681640625 = 0.9766519069671631 + 100.0 * 8.6482572555542
Epoch 460, val loss: 0.9798932671546936
Epoch 470, training loss: 865.7261962890625 = 0.9710782170295715 + 100.0 * 8.647551536560059
Epoch 470, val loss: 0.9747800230979919
Epoch 480, training loss: 866.1759643554688 = 0.9655177593231201 + 100.0 * 8.652104377746582
Epoch 480, val loss: 0.9694634079933167
Epoch 490, training loss: 866.4000854492188 = 0.9595884680747986 + 100.0 * 8.654404640197754
Epoch 490, val loss: 0.9637906551361084
Epoch 500, training loss: 866.495849609375 = 0.9534444808959961 + 100.0 * 8.655424118041992
Epoch 500, val loss: 0.9580144286155701
Epoch 510, training loss: 866.6976928710938 = 0.9471822381019592 + 100.0 * 8.65750503540039
Epoch 510, val loss: 0.9520654082298279
Epoch 520, training loss: 866.96875 = 0.9408010840415955 + 100.0 * 8.660279273986816
Epoch 520, val loss: 0.9459778666496277
Epoch 530, training loss: 866.832763671875 = 0.9339674711227417 + 100.0 * 8.658987998962402
Epoch 530, val loss: 0.9395519495010376
Epoch 540, training loss: 866.755615234375 = 0.9273529648780823 + 100.0 * 8.658282279968262
Epoch 540, val loss: 0.9332813024520874
Epoch 550, training loss: 867.2100830078125 = 0.9205282926559448 + 100.0 * 8.662895202636719
Epoch 550, val loss: 0.9268749952316284
Epoch 560, training loss: 867.5518188476562 = 0.9135205745697021 + 100.0 * 8.666382789611816
Epoch 560, val loss: 0.9202607870101929
Epoch 570, training loss: 867.9744262695312 = 0.9064710736274719 + 100.0 * 8.670679092407227
Epoch 570, val loss: 0.9135600924491882
Epoch 580, training loss: 867.6621704101562 = 0.8990578651428223 + 100.0 * 8.667631149291992
Epoch 580, val loss: 0.9066206812858582
Epoch 590, training loss: 867.8648071289062 = 0.8917734026908875 + 100.0 * 8.669730186462402
Epoch 590, val loss: 0.8998140692710876
Epoch 600, training loss: 868.0775146484375 = 0.8844076991081238 + 100.0 * 8.671931266784668
Epoch 600, val loss: 0.8929537534713745
Epoch 610, training loss: 868.5786743164062 = 0.8770493865013123 + 100.0 * 8.677016258239746
Epoch 610, val loss: 0.8860135674476624
Epoch 620, training loss: 868.8031616210938 = 0.8694869875907898 + 100.0 * 8.679336547851562
Epoch 620, val loss: 0.8789098858833313
Epoch 630, training loss: 868.4452514648438 = 0.8618748784065247 + 100.0 * 8.675833702087402
Epoch 630, val loss: 0.8719462752342224
Epoch 640, training loss: 868.9110107421875 = 0.8544960618019104 + 100.0 * 8.680564880371094
Epoch 640, val loss: 0.8649986982345581
Epoch 650, training loss: 868.9566040039062 = 0.8465891480445862 + 100.0 * 8.681099891662598
Epoch 650, val loss: 0.8575215339660645
Epoch 660, training loss: 869.572021484375 = 0.8398054838180542 + 100.0 * 8.687322616577148
Epoch 660, val loss: 0.8509660959243774
Epoch 670, training loss: 868.9136352539062 = 0.8318611979484558 + 100.0 * 8.680817604064941
Epoch 670, val loss: 0.8438012599945068
Epoch 680, training loss: 869.0531616210938 = 0.8241414427757263 + 100.0 * 8.682290077209473
Epoch 680, val loss: 0.8366719484329224
Epoch 690, training loss: 869.2494506835938 = 0.8166869878768921 + 100.0 * 8.684327125549316
Epoch 690, val loss: 0.8295506834983826
Epoch 700, training loss: 869.271240234375 = 0.8093302249908447 + 100.0 * 8.684618949890137
Epoch 700, val loss: 0.8226252794265747
Epoch 710, training loss: 869.6040649414062 = 0.8021641969680786 + 100.0 * 8.688018798828125
Epoch 710, val loss: 0.8160042762756348
Epoch 720, training loss: 870.01025390625 = 0.794876754283905 + 100.0 * 8.692153930664062
Epoch 720, val loss: 0.8091263771057129
Epoch 730, training loss: 869.8411254882812 = 0.7873468995094299 + 100.0 * 8.690537452697754
Epoch 730, val loss: 0.8021786212921143
Epoch 740, training loss: 870.2154541015625 = 0.78020840883255 + 100.0 * 8.694352149963379
Epoch 740, val loss: 0.7954005002975464
Epoch 750, training loss: 869.9779052734375 = 0.7729964852333069 + 100.0 * 8.692049026489258
Epoch 750, val loss: 0.7887762188911438
Epoch 760, training loss: 870.4705200195312 = 0.7660491466522217 + 100.0 * 8.697044372558594
Epoch 760, val loss: 0.7822971940040588
Epoch 770, training loss: 870.7198486328125 = 0.75899338722229 + 100.0 * 8.69960880279541
Epoch 770, val loss: 0.7756487131118774
Epoch 780, training loss: 870.6693725585938 = 0.751863956451416 + 100.0 * 8.699174880981445
Epoch 780, val loss: 0.7691320180892944
Epoch 790, training loss: 871.3023681640625 = 0.7452319860458374 + 100.0 * 8.705571174621582
Epoch 790, val loss: 0.7626686096191406
Epoch 800, training loss: 871.5145874023438 = 0.7379258275032043 + 100.0 * 8.70776653289795
Epoch 800, val loss: 0.7555755972862244
Epoch 810, training loss: 870.5149536132812 = 0.7311291098594666 + 100.0 * 8.697837829589844
Epoch 810, val loss: 0.7495604157447815
Epoch 820, training loss: 869.9037475585938 = 0.7249693870544434 + 100.0 * 8.691787719726562
Epoch 820, val loss: 0.7441384196281433
Epoch 830, training loss: 870.729736328125 = 0.7188105583190918 + 100.0 * 8.700109481811523
Epoch 830, val loss: 0.7382257580757141
Epoch 840, training loss: 870.8931274414062 = 0.7123916745185852 + 100.0 * 8.701807022094727
Epoch 840, val loss: 0.7323008179664612
Epoch 850, training loss: 871.1925659179688 = 0.7060213685035706 + 100.0 * 8.704865455627441
Epoch 850, val loss: 0.7263129353523254
Epoch 860, training loss: 871.998291015625 = 0.6999151706695557 + 100.0 * 8.712984085083008
Epoch 860, val loss: 0.7206047773361206
Epoch 870, training loss: 872.2341918945312 = 0.6937165260314941 + 100.0 * 8.715404510498047
Epoch 870, val loss: 0.7149208188056946
Epoch 880, training loss: 872.586669921875 = 0.6875506639480591 + 100.0 * 8.71899127960205
Epoch 880, val loss: 0.709212601184845
Epoch 890, training loss: 872.6965942382812 = 0.6815505027770996 + 100.0 * 8.720149993896484
Epoch 890, val loss: 0.7035991549491882
Epoch 900, training loss: 872.7908325195312 = 0.6754902601242065 + 100.0 * 8.721153259277344
Epoch 900, val loss: 0.6980881094932556
Epoch 910, training loss: 873.092041015625 = 0.6697252988815308 + 100.0 * 8.724223136901855
Epoch 910, val loss: 0.6928367018699646
Epoch 920, training loss: 873.5454711914062 = 0.6640750765800476 + 100.0 * 8.728814125061035
Epoch 920, val loss: 0.6876562833786011
Epoch 930, training loss: 873.6582641601562 = 0.6585484743118286 + 100.0 * 8.729996681213379
Epoch 930, val loss: 0.6824793815612793
Epoch 940, training loss: 873.2700805664062 = 0.65285325050354 + 100.0 * 8.72617244720459
Epoch 940, val loss: 0.6773204803466797
Epoch 950, training loss: 873.2600708007812 = 0.6473071575164795 + 100.0 * 8.726127624511719
Epoch 950, val loss: 0.6723449230194092
Epoch 960, training loss: 873.5927734375 = 0.6423296928405762 + 100.0 * 8.729504585266113
Epoch 960, val loss: 0.6678799390792847
Epoch 970, training loss: 874.1514282226562 = 0.637171745300293 + 100.0 * 8.735142707824707
Epoch 970, val loss: 0.6631426811218262
Epoch 980, training loss: 874.1139526367188 = 0.6323105096817017 + 100.0 * 8.734816551208496
Epoch 980, val loss: 0.658603310585022
Epoch 990, training loss: 874.1858520507812 = 0.6272563338279724 + 100.0 * 8.735586166381836
Epoch 990, val loss: 0.6541023850440979
Epoch 1000, training loss: 874.35791015625 = 0.622488260269165 + 100.0 * 8.737354278564453
Epoch 1000, val loss: 0.6499771475791931
Epoch 1010, training loss: 874.80126953125 = 0.6179049015045166 + 100.0 * 8.741833686828613
Epoch 1010, val loss: 0.6457815766334534
Epoch 1020, training loss: 874.8002319335938 = 0.6133764982223511 + 100.0 * 8.741868019104004
Epoch 1020, val loss: 0.6418296098709106
Epoch 1030, training loss: 874.2337646484375 = 0.6084062457084656 + 100.0 * 8.73625373840332
Epoch 1030, val loss: 0.6372673511505127
Epoch 1040, training loss: 874.64990234375 = 0.6040405035018921 + 100.0 * 8.740458488464355
Epoch 1040, val loss: 0.633452832698822
Epoch 1050, training loss: 875.0076904296875 = 0.5998615026473999 + 100.0 * 8.744078636169434
Epoch 1050, val loss: 0.6297826170921326
Epoch 1060, training loss: 875.5130004882812 = 0.5955020189285278 + 100.0 * 8.749175071716309
Epoch 1060, val loss: 0.6258338093757629
Epoch 1070, training loss: 875.804931640625 = 0.5905864238739014 + 100.0 * 8.752143859863281
Epoch 1070, val loss: 0.6214216947555542
Epoch 1080, training loss: 875.8695678710938 = 0.5855942964553833 + 100.0 * 8.752840042114258
Epoch 1080, val loss: 0.6171528100967407
Epoch 1090, training loss: 876.156494140625 = 0.5808886289596558 + 100.0 * 8.755756378173828
Epoch 1090, val loss: 0.61296546459198
Epoch 1100, training loss: 876.1991577148438 = 0.5762649774551392 + 100.0 * 8.75622844696045
Epoch 1100, val loss: 0.6088882088661194
Epoch 1110, training loss: 876.1002807617188 = 0.5716557502746582 + 100.0 * 8.75528621673584
Epoch 1110, val loss: 0.6049611568450928
Epoch 1120, training loss: 876.5545654296875 = 0.5673189163208008 + 100.0 * 8.759872436523438
Epoch 1120, val loss: 0.601233184337616
Epoch 1130, training loss: 876.474365234375 = 0.5629276037216187 + 100.0 * 8.759114265441895
Epoch 1130, val loss: 0.5973692536354065
Epoch 1140, training loss: 876.766357421875 = 0.5588555335998535 + 100.0 * 8.762075424194336
Epoch 1140, val loss: 0.5939540266990662
Epoch 1150, training loss: 881.15673828125 = 0.5551071166992188 + 100.0 * 8.806015968322754
Epoch 1150, val loss: 0.5899568796157837
Epoch 1160, training loss: 869.1664428710938 = 0.5439165234565735 + 100.0 * 8.686224937438965
Epoch 1160, val loss: 0.5810409784317017
Epoch 1170, training loss: 876.4723510742188 = 0.5450050830841064 + 100.0 * 8.759273529052734
Epoch 1170, val loss: 0.5831671953201294
Epoch 1180, training loss: 873.1904296875 = 0.5424501895904541 + 100.0 * 8.726479530334473
Epoch 1180, val loss: 0.5798813700675964
Epoch 1190, training loss: 872.0708618164062 = 0.5398510098457336 + 100.0 * 8.715310096740723
Epoch 1190, val loss: 0.5777034759521484
Epoch 1200, training loss: 873.543212890625 = 0.5372236967086792 + 100.0 * 8.730059623718262
Epoch 1200, val loss: 0.5758367776870728
Epoch 1210, training loss: 873.5333251953125 = 0.5338762998580933 + 100.0 * 8.729994773864746
Epoch 1210, val loss: 0.5727348923683167
Epoch 1220, training loss: 874.4934692382812 = 0.5310721397399902 + 100.0 * 8.7396240234375
Epoch 1220, val loss: 0.5705333948135376
Epoch 1230, training loss: 875.147216796875 = 0.5282047986984253 + 100.0 * 8.746190071105957
Epoch 1230, val loss: 0.5682143568992615
Epoch 1240, training loss: 875.5438842773438 = 0.5251882076263428 + 100.0 * 8.750186920166016
Epoch 1240, val loss: 0.5657712817192078
Epoch 1250, training loss: 875.8886108398438 = 0.5222774744033813 + 100.0 * 8.753663063049316
Epoch 1250, val loss: 0.5634843707084656
Epoch 1260, training loss: 876.2060546875 = 0.519427478313446 + 100.0 * 8.756866455078125
Epoch 1260, val loss: 0.5612008571624756
Epoch 1270, training loss: 876.500244140625 = 0.5166172385215759 + 100.0 * 8.759836196899414
Epoch 1270, val loss: 0.5590443015098572
Epoch 1280, training loss: 876.0528564453125 = 0.5137021541595459 + 100.0 * 8.755392074584961
Epoch 1280, val loss: 0.5567041039466858
Epoch 1290, training loss: 876.3851318359375 = 0.5110807418823242 + 100.0 * 8.758740425109863
Epoch 1290, val loss: 0.5547529458999634
Epoch 1300, training loss: 877.1864013671875 = 0.508651614189148 + 100.0 * 8.766777038574219
Epoch 1300, val loss: 0.5529307723045349
Epoch 1310, training loss: 877.4618530273438 = 0.506069004535675 + 100.0 * 8.76955795288086
Epoch 1310, val loss: 0.5509020090103149
Epoch 1320, training loss: 874.3809814453125 = 0.5026453733444214 + 100.0 * 8.738783836364746
Epoch 1320, val loss: 0.5477288961410522
Epoch 1330, training loss: 878.01953125 = 0.5010750889778137 + 100.0 * 8.775184631347656
Epoch 1330, val loss: 0.5470632910728455
Epoch 1340, training loss: 874.5411987304688 = 0.4977284073829651 + 100.0 * 8.740434646606445
Epoch 1340, val loss: 0.5440692901611328
Epoch 1350, training loss: 875.6888427734375 = 0.4954480230808258 + 100.0 * 8.751934051513672
Epoch 1350, val loss: 0.5424360036849976
Epoch 1360, training loss: 876.9159545898438 = 0.4936840534210205 + 100.0 * 8.764222145080566
Epoch 1360, val loss: 0.5416157245635986
Epoch 1370, training loss: 877.2952880859375 = 0.4913402497768402 + 100.0 * 8.76803970336914
Epoch 1370, val loss: 0.5396842360496521
Epoch 1380, training loss: 878.0095825195312 = 0.48929503560066223 + 100.0 * 8.775202751159668
Epoch 1380, val loss: 0.5383253693580627
Epoch 1390, training loss: 878.4873657226562 = 0.4871806800365448 + 100.0 * 8.780001640319824
Epoch 1390, val loss: 0.536820650100708
Epoch 1400, training loss: 878.8098754882812 = 0.4850150942802429 + 100.0 * 8.783248901367188
Epoch 1400, val loss: 0.5352696776390076
Epoch 1410, training loss: 878.9907836914062 = 0.48291292786598206 + 100.0 * 8.785079002380371
Epoch 1410, val loss: 0.5339153409004211
Epoch 1420, training loss: 879.1422729492188 = 0.48081207275390625 + 100.0 * 8.786614418029785
Epoch 1420, val loss: 0.5324648022651672
Epoch 1430, training loss: 879.5614013671875 = 0.47880619764328003 + 100.0 * 8.790825843811035
Epoch 1430, val loss: 0.5310775637626648
Epoch 1440, training loss: 879.3414916992188 = 0.47678059339523315 + 100.0 * 8.788646697998047
Epoch 1440, val loss: 0.5297984480857849
Epoch 1450, training loss: 879.8602905273438 = 0.47482284903526306 + 100.0 * 8.793854713439941
Epoch 1450, val loss: 0.5283675193786621
Epoch 1460, training loss: 880.3455810546875 = 0.47286996245384216 + 100.0 * 8.798727035522461
Epoch 1460, val loss: 0.5270276069641113
Epoch 1470, training loss: 880.0869750976562 = 0.4708345830440521 + 100.0 * 8.796161651611328
Epoch 1470, val loss: 0.5257691144943237
Epoch 1480, training loss: 880.3514404296875 = 0.4689497947692871 + 100.0 * 8.79882526397705
Epoch 1480, val loss: 0.5245183706283569
Epoch 1490, training loss: 880.6682739257812 = 0.46707993745803833 + 100.0 * 8.802011489868164
Epoch 1490, val loss: 0.5232669711112976
Epoch 1500, training loss: 881.0531005859375 = 0.46522262692451477 + 100.0 * 8.805878639221191
Epoch 1500, val loss: 0.5219501852989197
Epoch 1510, training loss: 881.00048828125 = 0.4633612036705017 + 100.0 * 8.805371284484863
Epoch 1510, val loss: 0.5207183361053467
Epoch 1520, training loss: 881.4817504882812 = 0.46158567070961 + 100.0 * 8.810201644897461
Epoch 1520, val loss: 0.5196603536605835
Epoch 1530, training loss: 881.4124145507812 = 0.45979586243629456 + 100.0 * 8.809526443481445
Epoch 1530, val loss: 0.5184929370880127
Epoch 1540, training loss: 881.8389892578125 = 0.4580770432949066 + 100.0 * 8.813809394836426
Epoch 1540, val loss: 0.5173285603523254
Epoch 1550, training loss: 882.0439453125 = 0.4563310742378235 + 100.0 * 8.815876007080078
Epoch 1550, val loss: 0.5162084698677063
Epoch 1560, training loss: 882.0722045898438 = 0.454632431268692 + 100.0 * 8.81617546081543
Epoch 1560, val loss: 0.515179455280304
Epoch 1570, training loss: 882.3056640625 = 0.45295295119285583 + 100.0 * 8.818527221679688
Epoch 1570, val loss: 0.5140429735183716
Epoch 1580, training loss: 882.3899536132812 = 0.4512912631034851 + 100.0 * 8.81938648223877
Epoch 1580, val loss: 0.5130131244659424
Epoch 1590, training loss: 882.4837646484375 = 0.4496401250362396 + 100.0 * 8.820341110229492
Epoch 1590, val loss: 0.5120704770088196
Epoch 1600, training loss: 882.3116455078125 = 0.44794145226478577 + 100.0 * 8.818636894226074
Epoch 1600, val loss: 0.5108929872512817
Epoch 1610, training loss: 882.5406494140625 = 0.4463365077972412 + 100.0 * 8.820942878723145
Epoch 1610, val loss: 0.5099851489067078
Epoch 1620, training loss: 883.0060424804688 = 0.4447943866252899 + 100.0 * 8.82561206817627
Epoch 1620, val loss: 0.508981466293335
Epoch 1630, training loss: 882.6984252929688 = 0.44320860505104065 + 100.0 * 8.822551727294922
Epoch 1630, val loss: 0.507957398891449
Epoch 1640, training loss: 882.9703369140625 = 0.44167351722717285 + 100.0 * 8.825286865234375
Epoch 1640, val loss: 0.5071020722389221
Epoch 1650, training loss: 883.5064697265625 = 0.4401230812072754 + 100.0 * 8.830663681030273
Epoch 1650, val loss: 0.5061590671539307
Epoch 1660, training loss: 883.87158203125 = 0.4385875165462494 + 100.0 * 8.834329605102539
Epoch 1660, val loss: 0.50517338514328
Epoch 1670, training loss: 883.3600463867188 = 0.43693235516548157 + 100.0 * 8.829231262207031
Epoch 1670, val loss: 0.5043475031852722
Epoch 1680, training loss: 883.5933227539062 = 0.4354446232318878 + 100.0 * 8.831579208374023
Epoch 1680, val loss: 0.5033419728279114
Epoch 1690, training loss: 884.353759765625 = 0.43400004506111145 + 100.0 * 8.839197158813477
Epoch 1690, val loss: 0.5025562047958374
Epoch 1700, training loss: 883.8938598632812 = 0.4323999881744385 + 100.0 * 8.834614753723145
Epoch 1700, val loss: 0.5016019344329834
Epoch 1710, training loss: 884.0634155273438 = 0.43094342947006226 + 100.0 * 8.836324691772461
Epoch 1710, val loss: 0.5006075501441956
Epoch 1720, training loss: 884.3668212890625 = 0.4294402301311493 + 100.0 * 8.839373588562012
Epoch 1720, val loss: 0.49979448318481445
Epoch 1730, training loss: 884.858642578125 = 0.42799127101898193 + 100.0 * 8.844306945800781
Epoch 1730, val loss: 0.4988971948623657
Epoch 1740, training loss: 884.8257446289062 = 0.4264766275882721 + 100.0 * 8.843993186950684
Epoch 1740, val loss: 0.4979933500289917
Epoch 1750, training loss: 884.913818359375 = 0.42505553364753723 + 100.0 * 8.844887733459473
Epoch 1750, val loss: 0.4970908761024475
Epoch 1760, training loss: 885.0281372070312 = 0.4236113131046295 + 100.0 * 8.84604549407959
Epoch 1760, val loss: 0.49625712633132935
Epoch 1770, training loss: 885.4695434570312 = 0.4221973419189453 + 100.0 * 8.850473403930664
Epoch 1770, val loss: 0.49541175365448
Epoch 1780, training loss: 884.2338256835938 = 0.42067286372184753 + 100.0 * 8.83813190460205
Epoch 1780, val loss: 0.49417129158973694
Epoch 1790, training loss: 883.2423706054688 = 0.41899868845939636 + 100.0 * 8.82823371887207
Epoch 1790, val loss: 0.493675172328949
Epoch 1800, training loss: 883.3703002929688 = 0.41769924759864807 + 100.0 * 8.8295259475708
Epoch 1800, val loss: 0.492953896522522
Epoch 1810, training loss: 883.51953125 = 0.4163615107536316 + 100.0 * 8.831031799316406
Epoch 1810, val loss: 0.4920276701450348
Epoch 1820, training loss: 884.36181640625 = 0.4151051640510559 + 100.0 * 8.83946704864502
Epoch 1820, val loss: 0.49132439494132996
Epoch 1830, training loss: 885.00830078125 = 0.4137876629829407 + 100.0 * 8.845945358276367
Epoch 1830, val loss: 0.49054962396621704
Epoch 1840, training loss: 885.4207763671875 = 0.41242337226867676 + 100.0 * 8.850083351135254
Epoch 1840, val loss: 0.4897342324256897
Epoch 1850, training loss: 885.3858642578125 = 0.41103312373161316 + 100.0 * 8.849748611450195
Epoch 1850, val loss: 0.4888751804828644
Epoch 1860, training loss: 885.5695190429688 = 0.4096762239933014 + 100.0 * 8.851598739624023
Epoch 1860, val loss: 0.48807668685913086
Epoch 1870, training loss: 885.9797973632812 = 0.4083384871482849 + 100.0 * 8.855714797973633
Epoch 1870, val loss: 0.48724085092544556
Epoch 1880, training loss: 885.9502563476562 = 0.4069577753543854 + 100.0 * 8.855432510375977
Epoch 1880, val loss: 0.4865354597568512
Epoch 1890, training loss: 885.8826293945312 = 0.40560129284858704 + 100.0 * 8.85477066040039
Epoch 1890, val loss: 0.4857580065727234
Epoch 1900, training loss: 886.0628662109375 = 0.40426352620124817 + 100.0 * 8.856586456298828
Epoch 1900, val loss: 0.4850502610206604
Epoch 1910, training loss: 886.2689208984375 = 0.4029284417629242 + 100.0 * 8.858659744262695
Epoch 1910, val loss: 0.48427870869636536
Epoch 1920, training loss: 886.29736328125 = 0.4015752673149109 + 100.0 * 8.85895824432373
Epoch 1920, val loss: 0.4835343062877655
Epoch 1930, training loss: 886.63818359375 = 0.4002499580383301 + 100.0 * 8.86237907409668
Epoch 1930, val loss: 0.4828346371650696
Epoch 1940, training loss: 886.400390625 = 0.3989090621471405 + 100.0 * 8.860014915466309
Epoch 1940, val loss: 0.4821527302265167
Epoch 1950, training loss: 886.7188110351562 = 0.3976181447505951 + 100.0 * 8.863211631774902
Epoch 1950, val loss: 0.4813612401485443
Epoch 1960, training loss: 887.025390625 = 0.3963181674480438 + 100.0 * 8.866291046142578
Epoch 1960, val loss: 0.48071739077568054
Epoch 1970, training loss: 886.8785400390625 = 0.3949620723724365 + 100.0 * 8.864835739135742
Epoch 1970, val loss: 0.48003116250038147
Epoch 1980, training loss: 887.460693359375 = 0.393696129322052 + 100.0 * 8.870670318603516
Epoch 1980, val loss: 0.4793083965778351
Epoch 1990, training loss: 887.1773071289062 = 0.3923743665218353 + 100.0 * 8.867849349975586
Epoch 1990, val loss: 0.4786912500858307
Epoch 2000, training loss: 887.6171875 = 0.39111897349357605 + 100.0 * 8.872261047363281
Epoch 2000, val loss: 0.4779641330242157
Epoch 2010, training loss: 887.8616943359375 = 0.3898170590400696 + 100.0 * 8.87471866607666
Epoch 2010, val loss: 0.47732508182525635
Epoch 2020, training loss: 887.8602294921875 = 0.3885459899902344 + 100.0 * 8.874716758728027
Epoch 2020, val loss: 0.47665688395500183
Epoch 2030, training loss: 887.8800048828125 = 0.3872426748275757 + 100.0 * 8.874927520751953
Epoch 2030, val loss: 0.47611385583877563
Epoch 2040, training loss: 887.6643676757812 = 0.385976642370224 + 100.0 * 8.872783660888672
Epoch 2040, val loss: 0.4754198491573334
Epoch 2050, training loss: 888.1213989257812 = 0.3847615122795105 + 100.0 * 8.877366065979004
Epoch 2050, val loss: 0.4748516380786896
Epoch 2060, training loss: 888.4415893554688 = 0.3834875524044037 + 100.0 * 8.88058090209961
Epoch 2060, val loss: 0.4742046296596527
Epoch 2070, training loss: 888.6419067382812 = 0.3822144567966461 + 100.0 * 8.882596969604492
Epoch 2070, val loss: 0.4735885262489319
Epoch 2080, training loss: 888.7234497070312 = 0.3809588849544525 + 100.0 * 8.883424758911133
Epoch 2080, val loss: 0.4729509949684143
Epoch 2090, training loss: 889.029052734375 = 0.37971141934394836 + 100.0 * 8.886493682861328
Epoch 2090, val loss: 0.4724373519420624
Epoch 2100, training loss: 889.3198852539062 = 0.37846627831459045 + 100.0 * 8.889413833618164
Epoch 2100, val loss: 0.4718206822872162
Epoch 2110, training loss: 889.078369140625 = 0.3772197365760803 + 100.0 * 8.887011528015137
Epoch 2110, val loss: 0.47128793597221375
Epoch 2120, training loss: 889.1968994140625 = 0.37597429752349854 + 100.0 * 8.888209342956543
Epoch 2120, val loss: 0.4706399738788605
Epoch 2130, training loss: 888.969970703125 = 0.37470728158950806 + 100.0 * 8.885952949523926
Epoch 2130, val loss: 0.46989890933036804
Epoch 2140, training loss: 884.908203125 = 0.37330231070518494 + 100.0 * 8.845349311828613
Epoch 2140, val loss: 0.47014203667640686
Epoch 2150, training loss: 883.73828125 = 0.37222912907600403 + 100.0 * 8.833660125732422
Epoch 2150, val loss: 0.4683254659175873
Epoch 2160, training loss: 883.4345703125 = 0.3708023130893707 + 100.0 * 8.83063793182373
Epoch 2160, val loss: 0.4679647982120514
Epoch 2170, training loss: 885.4213256835938 = 0.3697020709514618 + 100.0 * 8.850516319274902
Epoch 2170, val loss: 0.46773388981819153
Epoch 2180, training loss: 885.5140991210938 = 0.36863255500793457 + 100.0 * 8.851454734802246
Epoch 2180, val loss: 0.4670395851135254
Epoch 2190, training loss: 885.9602661132812 = 0.3674263656139374 + 100.0 * 8.855928421020508
Epoch 2190, val loss: 0.466480553150177
Epoch 2200, training loss: 886.9027709960938 = 0.3662872314453125 + 100.0 * 8.865365028381348
Epoch 2200, val loss: 0.46593913435935974
Epoch 2210, training loss: 887.8615112304688 = 0.365125834941864 + 100.0 * 8.874963760375977
Epoch 2210, val loss: 0.46550115942955017
Epoch 2220, training loss: 888.3435668945312 = 0.3639557957649231 + 100.0 * 8.879796028137207
Epoch 2220, val loss: 0.4650222361087799
Epoch 2230, training loss: 888.4977416992188 = 0.36277884244918823 + 100.0 * 8.881349563598633
Epoch 2230, val loss: 0.4645277261734009
Epoch 2240, training loss: 888.6162719726562 = 0.36160367727279663 + 100.0 * 8.882546424865723
Epoch 2240, val loss: 0.4640443027019501
Epoch 2250, training loss: 888.8599243164062 = 0.36044710874557495 + 100.0 * 8.884994506835938
Epoch 2250, val loss: 0.46353915333747864
Epoch 2260, training loss: 889.0270385742188 = 0.3592703938484192 + 100.0 * 8.886677742004395
Epoch 2260, val loss: 0.46308448910713196
Epoch 2270, training loss: 889.3369750976562 = 0.358121395111084 + 100.0 * 8.889788627624512
Epoch 2270, val loss: 0.46259844303131104
Epoch 2280, training loss: 889.3460083007812 = 0.35694873332977295 + 100.0 * 8.889890670776367
Epoch 2280, val loss: 0.4622129201889038
Epoch 2290, training loss: 888.6893310546875 = 0.35654956102371216 + 100.0 * 8.88332748413086
Epoch 2290, val loss: 0.4615969955921173
Epoch 2300, training loss: 889.8161010742188 = 0.3555160462856293 + 100.0 * 8.89460563659668
Epoch 2300, val loss: 0.46184176206588745
Epoch 2310, training loss: 891.4133911132812 = 0.35431763529777527 + 100.0 * 8.910591125488281
Epoch 2310, val loss: 0.4609493017196655
Epoch 2320, training loss: 891.74072265625 = 0.3531564772129059 + 100.0 * 8.913875579833984
Epoch 2320, val loss: 0.46069079637527466
Epoch 2330, training loss: 891.970458984375 = 0.3519744575023651 + 100.0 * 8.916184425354004
Epoch 2330, val loss: 0.4600521922111511
Epoch 2340, training loss: 892.4525756835938 = 0.3508490025997162 + 100.0 * 8.92101764678955
Epoch 2340, val loss: 0.45980313420295715
Epoch 2350, training loss: 893.0165405273438 = 0.34975025057792664 + 100.0 * 8.926668167114258
Epoch 2350, val loss: 0.45934584736824036
Epoch 2360, training loss: 893.2584228515625 = 0.3486545979976654 + 100.0 * 8.929098129272461
Epoch 2360, val loss: 0.45915600657463074
Epoch 2370, training loss: 892.5392456054688 = 0.347533643245697 + 100.0 * 8.921916961669922
Epoch 2370, val loss: 0.45882928371429443
Epoch 2380, training loss: 892.10107421875 = 0.34641388058662415 + 100.0 * 8.917546272277832
Epoch 2380, val loss: 0.4581789970397949
Epoch 2390, training loss: 892.6657104492188 = 0.34524843096733093 + 100.0 * 8.92320442199707
Epoch 2390, val loss: 0.45765286684036255
Epoch 2400, training loss: 893.1820068359375 = 0.3441586196422577 + 100.0 * 8.928378105163574
Epoch 2400, val loss: 0.45758387446403503
Epoch 2410, training loss: 893.5682983398438 = 0.34303024411201477 + 100.0 * 8.932252883911133
Epoch 2410, val loss: 0.45720624923706055
Epoch 2420, training loss: 893.9564208984375 = 0.3418918251991272 + 100.0 * 8.936144828796387
Epoch 2420, val loss: 0.45698222517967224
Epoch 2430, training loss: 893.8064575195312 = 0.34074676036834717 + 100.0 * 8.934657096862793
Epoch 2430, val loss: 0.4566099941730499
Epoch 2440, training loss: 893.7081298828125 = 0.33961185812950134 + 100.0 * 8.933685302734375
Epoch 2440, val loss: 0.4563218653202057
Epoch 2450, training loss: 894.0341186523438 = 0.33850064873695374 + 100.0 * 8.936956405639648
Epoch 2450, val loss: 0.45595821738243103
Epoch 2460, training loss: 894.4281616210938 = 0.3374124765396118 + 100.0 * 8.94090747833252
Epoch 2460, val loss: 0.4556657075881958
Epoch 2470, training loss: 894.5928955078125 = 0.3362896144390106 + 100.0 * 8.94256591796875
Epoch 2470, val loss: 0.4554099142551422
Epoch 2480, training loss: 894.6670532226562 = 0.33517563343048096 + 100.0 * 8.943318367004395
Epoch 2480, val loss: 0.4551798701286316
Epoch 2490, training loss: 894.4415893554688 = 0.3340473175048828 + 100.0 * 8.941075325012207
Epoch 2490, val loss: 0.45474326610565186
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8386956521739131
0.8647395493733248
=== training gcn model ===
Epoch 0, training loss: 1023.8423461914062 = 1.0940724611282349 + 100.0 * 10.227482795715332
Epoch 0, val loss: 1.0933752059936523
Epoch 10, training loss: 978.852294921875 = 1.0906082391738892 + 100.0 * 9.777616500854492
Epoch 10, val loss: 1.0899680852890015
Epoch 20, training loss: 955.9901123046875 = 1.0872735977172852 + 100.0 * 9.549028396606445
Epoch 20, val loss: 1.0867029428482056
Epoch 30, training loss: 940.2928466796875 = 1.084140658378601 + 100.0 * 9.39208698272705
Epoch 30, val loss: 1.0836302042007446
Epoch 40, training loss: 928.3585815429688 = 1.0812091827392578 + 100.0 * 9.272773742675781
Epoch 40, val loss: 1.0807534456253052
Epoch 50, training loss: 918.8712768554688 = 1.07847261428833 + 100.0 * 9.17792797088623
Epoch 50, val loss: 1.0780616998672485
Epoch 60, training loss: 911.3095092773438 = 1.0758994817733765 + 100.0 * 9.102335929870605
Epoch 60, val loss: 1.0755335092544556
Epoch 70, training loss: 904.92138671875 = 1.073470115661621 + 100.0 * 9.03847885131836
Epoch 70, val loss: 1.073145866394043
Epoch 80, training loss: 899.6472778320312 = 1.0712062120437622 + 100.0 * 8.985760688781738
Epoch 80, val loss: 1.0709285736083984
Epoch 90, training loss: 895.162109375 = 1.069087028503418 + 100.0 * 8.940930366516113
Epoch 90, val loss: 1.068845510482788
Epoch 100, training loss: 891.3538818359375 = 1.0670762062072754 + 100.0 * 8.902868270874023
Epoch 100, val loss: 1.0668797492980957
Epoch 110, training loss: 888.0150146484375 = 1.0651895999908447 + 100.0 * 8.869498252868652
Epoch 110, val loss: 1.0650359392166138
Epoch 120, training loss: 885.21630859375 = 1.063400149345398 + 100.0 * 8.84152889251709
Epoch 120, val loss: 1.063293695449829
Epoch 130, training loss: 882.7835083007812 = 1.0617138147354126 + 100.0 * 8.817217826843262
Epoch 130, val loss: 1.0616475343704224
Epoch 140, training loss: 880.8121948242188 = 1.0601081848144531 + 100.0 * 8.797520637512207
Epoch 140, val loss: 1.0600956678390503
Epoch 150, training loss: 879.0320434570312 = 1.0585771799087524 + 100.0 * 8.77973461151123
Epoch 150, val loss: 1.058607578277588
Epoch 160, training loss: 877.4702758789062 = 1.0571231842041016 + 100.0 * 8.764131546020508
Epoch 160, val loss: 1.057198405265808
Epoch 170, training loss: 876.1698608398438 = 1.0557481050491333 + 100.0 * 8.751141548156738
Epoch 170, val loss: 1.0558655261993408
Epoch 180, training loss: 874.9247436523438 = 1.0543872117996216 + 100.0 * 8.738703727722168
Epoch 180, val loss: 1.0545462369918823
Epoch 190, training loss: 874.117919921875 = 1.053067684173584 + 100.0 * 8.7306489944458
Epoch 190, val loss: 1.053268551826477
Epoch 200, training loss: 873.23486328125 = 1.0517809391021729 + 100.0 * 8.721831321716309
Epoch 200, val loss: 1.052005648612976
Epoch 210, training loss: 872.95166015625 = 1.0505287647247314 + 100.0 * 8.719011306762695
Epoch 210, val loss: 1.0507967472076416
Epoch 220, training loss: 872.1862182617188 = 1.04921555519104 + 100.0 * 8.711370468139648
Epoch 220, val loss: 1.049533724784851
Epoch 230, training loss: 871.4581298828125 = 1.0478646755218506 + 100.0 * 8.704102516174316
Epoch 230, val loss: 1.0482301712036133
Epoch 240, training loss: 871.1594848632812 = 1.046534538269043 + 100.0 * 8.701129913330078
Epoch 240, val loss: 1.0469088554382324
Epoch 250, training loss: 870.715576171875 = 1.0451445579528809 + 100.0 * 8.696703910827637
Epoch 250, val loss: 1.0455633401870728
Epoch 260, training loss: 870.3284301757812 = 1.0436370372772217 + 100.0 * 8.692848205566406
Epoch 260, val loss: 1.0440664291381836
Epoch 270, training loss: 870.0997314453125 = 1.042162299156189 + 100.0 * 8.69057559967041
Epoch 270, val loss: 1.0426377058029175
Epoch 280, training loss: 869.7500610351562 = 1.0405951738357544 + 100.0 * 8.687094688415527
Epoch 280, val loss: 1.041101098060608
Epoch 290, training loss: 869.5928955078125 = 1.038879632949829 + 100.0 * 8.685540199279785
Epoch 290, val loss: 1.0394153594970703
Epoch 300, training loss: 869.1502685546875 = 1.037028431892395 + 100.0 * 8.681132316589355
Epoch 300, val loss: 1.0375947952270508
Epoch 310, training loss: 868.9815063476562 = 1.0351296663284302 + 100.0 * 8.679463386535645
Epoch 310, val loss: 1.0357356071472168
Epoch 320, training loss: 868.8430786132812 = 1.0331178903579712 + 100.0 * 8.678099632263184
Epoch 320, val loss: 1.0337387323379517
Epoch 330, training loss: 868.8121948242188 = 1.0310088396072388 + 100.0 * 8.677811622619629
Epoch 330, val loss: 1.0316182374954224
Epoch 340, training loss: 868.7369384765625 = 1.028722882270813 + 100.0 * 8.677082061767578
Epoch 340, val loss: 1.0293419361114502
Epoch 350, training loss: 869.1884765625 = 1.0262868404388428 + 100.0 * 8.681621551513672
Epoch 350, val loss: 1.026951789855957
Epoch 360, training loss: 868.2785034179688 = 1.023691177368164 + 100.0 * 8.672548294067383
Epoch 360, val loss: 1.0243231058120728
Epoch 370, training loss: 868.342041015625 = 1.0210098028182983 + 100.0 * 8.673210144042969
Epoch 370, val loss: 1.0217067003250122
Epoch 380, training loss: 868.4335327148438 = 1.0181009769439697 + 100.0 * 8.674154281616211
Epoch 380, val loss: 1.0187937021255493
Epoch 390, training loss: 870.5211791992188 = 1.0148407220840454 + 100.0 * 8.695063591003418
Epoch 390, val loss: 1.0154304504394531
Epoch 400, training loss: 871.5445556640625 = 1.0121346712112427 + 100.0 * 8.705324172973633
Epoch 400, val loss: 1.0129690170288086
Epoch 410, training loss: 868.021728515625 = 1.0078119039535522 + 100.0 * 8.67013931274414
Epoch 410, val loss: 1.0085484981536865
Epoch 420, training loss: 867.8379516601562 = 1.0034552812576294 + 100.0 * 8.668344497680664
Epoch 420, val loss: 1.0041083097457886
Epoch 430, training loss: 869.6104125976562 = 1.0004773139953613 + 100.0 * 8.6860990524292
Epoch 430, val loss: 1.0013264417648315
Epoch 440, training loss: 867.69580078125 = 0.9961913228034973 + 100.0 * 8.666996002197266
Epoch 440, val loss: 0.9970179200172424
Epoch 450, training loss: 867.8601684570312 = 0.9917755126953125 + 100.0 * 8.668684005737305
Epoch 450, val loss: 0.9926379323005676
Epoch 460, training loss: 868.34130859375 = 0.9875020384788513 + 100.0 * 8.673538208007812
Epoch 460, val loss: 0.9884125590324402
Epoch 470, training loss: 868.4369506835938 = 0.9827626943588257 + 100.0 * 8.674541473388672
Epoch 470, val loss: 0.9836864471435547
Epoch 480, training loss: 868.86572265625 = 0.9780007600784302 + 100.0 * 8.678876876831055
Epoch 480, val loss: 0.9789566397666931
Epoch 490, training loss: 869.0943603515625 = 0.972906768321991 + 100.0 * 8.681214332580566
Epoch 490, val loss: 0.9738388657569885
Epoch 500, training loss: 869.275390625 = 0.9675467014312744 + 100.0 * 8.68307876586914
Epoch 500, val loss: 0.9685116410255432
Epoch 510, training loss: 869.459228515625 = 0.962052047252655 + 100.0 * 8.684971809387207
Epoch 510, val loss: 0.9630451202392578
Epoch 520, training loss: 869.5879516601562 = 0.9563271999359131 + 100.0 * 8.68631649017334
Epoch 520, val loss: 0.9573315382003784
Epoch 530, training loss: 869.6801147460938 = 0.9504449367523193 + 100.0 * 8.687296867370605
Epoch 530, val loss: 0.951519787311554
Epoch 540, training loss: 869.765869140625 = 0.9443562030792236 + 100.0 * 8.688215255737305
Epoch 540, val loss: 0.9454968571662903
Epoch 550, training loss: 870.0764770507812 = 0.9381757378578186 + 100.0 * 8.691383361816406
Epoch 550, val loss: 0.939312219619751
Epoch 560, training loss: 870.5967407226562 = 0.9317665100097656 + 100.0 * 8.696649551391602
Epoch 560, val loss: 0.932971179485321
Epoch 570, training loss: 868.945556640625 = 0.9244616627693176 + 100.0 * 8.680211067199707
Epoch 570, val loss: 0.9257329702377319
Epoch 580, training loss: 870.278076171875 = 0.9175716638565063 + 100.0 * 8.693605422973633
Epoch 580, val loss: 0.918464720249176
Epoch 590, training loss: 870.101806640625 = 0.9110178351402283 + 100.0 * 8.69190788269043
Epoch 590, val loss: 0.9125654101371765
Epoch 600, training loss: 870.48046875 = 0.9041657447814941 + 100.0 * 8.695762634277344
Epoch 600, val loss: 0.905304491519928
Epoch 610, training loss: 870.036865234375 = 0.8977953195571899 + 100.0 * 8.691390991210938
Epoch 610, val loss: 0.8992698192596436
Epoch 620, training loss: 869.9896850585938 = 0.8905572891235352 + 100.0 * 8.690991401672363
Epoch 620, val loss: 0.8920833468437195
Epoch 630, training loss: 870.9011840820312 = 0.8834818601608276 + 100.0 * 8.700177192687988
Epoch 630, val loss: 0.8851605653762817
Epoch 640, training loss: 871.0269165039062 = 0.8762717843055725 + 100.0 * 8.701506614685059
Epoch 640, val loss: 0.8780063986778259
Epoch 650, training loss: 871.58642578125 = 0.8690985441207886 + 100.0 * 8.707173347473145
Epoch 650, val loss: 0.8709210157394409
Epoch 660, training loss: 871.6773681640625 = 0.861691415309906 + 100.0 * 8.70815658569336
Epoch 660, val loss: 0.8634941577911377
Epoch 670, training loss: 872.1613159179688 = 0.8544142246246338 + 100.0 * 8.713068962097168
Epoch 670, val loss: 0.8564205765724182
Epoch 680, training loss: 872.53466796875 = 0.8471682071685791 + 100.0 * 8.716875076293945
Epoch 680, val loss: 0.8492429256439209
Epoch 690, training loss: 872.8251953125 = 0.8398009538650513 + 100.0 * 8.719854354858398
Epoch 690, val loss: 0.8420177102088928
Epoch 700, training loss: 873.3899536132812 = 0.8324944376945496 + 100.0 * 8.725574493408203
Epoch 700, val loss: 0.8347394466400146
Epoch 710, training loss: 873.17138671875 = 0.8250384330749512 + 100.0 * 8.723464012145996
Epoch 710, val loss: 0.8274814486503601
Epoch 720, training loss: 872.2603759765625 = 0.8167385458946228 + 100.0 * 8.714436531066895
Epoch 720, val loss: 0.8194178938865662
Epoch 730, training loss: 873.341064453125 = 0.8104276061058044 + 100.0 * 8.725306510925293
Epoch 730, val loss: 0.8130626082420349
Epoch 740, training loss: 873.8025512695312 = 0.8033191561698914 + 100.0 * 8.729991912841797
Epoch 740, val loss: 0.8061696290969849
Epoch 750, training loss: 874.1504516601562 = 0.7963753938674927 + 100.0 * 8.733540534973145
Epoch 750, val loss: 0.7994363307952881
Epoch 760, training loss: 874.3758544921875 = 0.7894940376281738 + 100.0 * 8.73586368560791
Epoch 760, val loss: 0.7926365733146667
Epoch 770, training loss: 874.6434936523438 = 0.7825035452842712 + 100.0 * 8.738609313964844
Epoch 770, val loss: 0.7857936024665833
Epoch 780, training loss: 874.9429931640625 = 0.775763750076294 + 100.0 * 8.74167251586914
Epoch 780, val loss: 0.7791911363601685
Epoch 790, training loss: 874.5510864257812 = 0.7687651515007019 + 100.0 * 8.737823486328125
Epoch 790, val loss: 0.7724054455757141
Epoch 800, training loss: 875.1318969726562 = 0.7623702883720398 + 100.0 * 8.743695259094238
Epoch 800, val loss: 0.7661159038543701
Epoch 810, training loss: 875.602783203125 = 0.7559504508972168 + 100.0 * 8.748468399047852
Epoch 810, val loss: 0.7599174380302429
Epoch 820, training loss: 875.775390625 = 0.7495633363723755 + 100.0 * 8.750258445739746
Epoch 820, val loss: 0.7536442875862122
Epoch 830, training loss: 875.6165771484375 = 0.7432257533073425 + 100.0 * 8.748733520507812
Epoch 830, val loss: 0.7475160956382751
Epoch 840, training loss: 874.935302734375 = 0.7367755174636841 + 100.0 * 8.741985321044922
Epoch 840, val loss: 0.7409828901290894
Epoch 850, training loss: 874.1651000976562 = 0.730259895324707 + 100.0 * 8.73434829711914
Epoch 850, val loss: 0.7350209355354309
Epoch 860, training loss: 875.24755859375 = 0.7243583798408508 + 100.0 * 8.745231628417969
Epoch 860, val loss: 0.7290894389152527
Epoch 870, training loss: 875.670166015625 = 0.7186422348022461 + 100.0 * 8.749515533447266
Epoch 870, val loss: 0.7237645983695984
Epoch 880, training loss: 875.6761474609375 = 0.713414192199707 + 100.0 * 8.749627113342285
Epoch 880, val loss: 0.7187198400497437
Epoch 890, training loss: 876.2841186523438 = 0.7080430388450623 + 100.0 * 8.755760192871094
Epoch 890, val loss: 0.7135664224624634
Epoch 900, training loss: 876.8134765625 = 0.7027671933174133 + 100.0 * 8.761107444763184
Epoch 900, val loss: 0.7084787487983704
Epoch 910, training loss: 877.292236328125 = 0.6975810527801514 + 100.0 * 8.765946388244629
Epoch 910, val loss: 0.7035492062568665
Epoch 920, training loss: 877.0936889648438 = 0.6921961307525635 + 100.0 * 8.764015197753906
Epoch 920, val loss: 0.6984121799468994
Epoch 930, training loss: 877.1354370117188 = 0.6871893405914307 + 100.0 * 8.764482498168945
Epoch 930, val loss: 0.693688154220581
Epoch 940, training loss: 877.9088134765625 = 0.682440996170044 + 100.0 * 8.772263526916504
Epoch 940, val loss: 0.6892142295837402
Epoch 950, training loss: 877.9444580078125 = 0.6776556372642517 + 100.0 * 8.77266788482666
Epoch 950, val loss: 0.6845368146896362
Epoch 960, training loss: 878.513671875 = 0.672960102558136 + 100.0 * 8.778407096862793
Epoch 960, val loss: 0.680237352848053
Epoch 970, training loss: 878.47119140625 = 0.6683484315872192 + 100.0 * 8.77802848815918
Epoch 970, val loss: 0.6758961081504822
Epoch 980, training loss: 878.09228515625 = 0.6637452244758606 + 100.0 * 8.774285316467285
Epoch 980, val loss: 0.6716016530990601
Epoch 990, training loss: 878.4269409179688 = 0.6594991087913513 + 100.0 * 8.777674674987793
Epoch 990, val loss: 0.6676380038261414
Epoch 1000, training loss: 878.9862670898438 = 0.6554868221282959 + 100.0 * 8.783308029174805
Epoch 1000, val loss: 0.6638896465301514
Epoch 1010, training loss: 879.5403442382812 = 0.6512696146965027 + 100.0 * 8.788890838623047
Epoch 1010, val loss: 0.6599851250648499
Epoch 1020, training loss: 878.0980224609375 = 0.6472959518432617 + 100.0 * 8.774507522583008
Epoch 1020, val loss: 0.6561978459358215
Epoch 1030, training loss: 878.739013671875 = 0.6434669494628906 + 100.0 * 8.78095531463623
Epoch 1030, val loss: 0.6528384685516357
Epoch 1040, training loss: 879.4335327148438 = 0.6398558616638184 + 100.0 * 8.78793716430664
Epoch 1040, val loss: 0.6496908664703369
Epoch 1050, training loss: 879.9497680664062 = 0.6362302899360657 + 100.0 * 8.793135643005371
Epoch 1050, val loss: 0.6463319063186646
Epoch 1060, training loss: 880.2966918945312 = 0.6324661374092102 + 100.0 * 8.796642303466797
Epoch 1060, val loss: 0.6429362297058105
Epoch 1070, training loss: 880.5595092773438 = 0.628849446773529 + 100.0 * 8.799306869506836
Epoch 1070, val loss: 0.6398129463195801
Epoch 1080, training loss: 880.3027954101562 = 0.6251951456069946 + 100.0 * 8.796775817871094
Epoch 1080, val loss: 0.6366122364997864
Epoch 1090, training loss: 879.2256469726562 = 0.6215076446533203 + 100.0 * 8.786041259765625
Epoch 1090, val loss: 0.6334484815597534
Epoch 1100, training loss: 879.833740234375 = 0.6182668209075928 + 100.0 * 8.792154312133789
Epoch 1100, val loss: 0.6304539442062378
Epoch 1110, training loss: 880.9098510742188 = 0.6153368949890137 + 100.0 * 8.802945137023926
Epoch 1110, val loss: 0.6279716491699219
Epoch 1120, training loss: 881.521728515625 = 0.612372636795044 + 100.0 * 8.809093475341797
Epoch 1120, val loss: 0.6253930330276489
Epoch 1130, training loss: 881.8986206054688 = 0.6093119382858276 + 100.0 * 8.81289291381836
Epoch 1130, val loss: 0.6227830648422241
Epoch 1140, training loss: 879.4306640625 = 0.6053773760795593 + 100.0 * 8.788252830505371
Epoch 1140, val loss: 0.6193394660949707
Epoch 1150, training loss: 879.1156005859375 = 0.6030423045158386 + 100.0 * 8.785125732421875
Epoch 1150, val loss: 0.6173180937767029
Epoch 1160, training loss: 880.2352905273438 = 0.6004238128662109 + 100.0 * 8.796348571777344
Epoch 1160, val loss: 0.6151454448699951
Epoch 1170, training loss: 880.6150512695312 = 0.5976608991622925 + 100.0 * 8.80017375946045
Epoch 1170, val loss: 0.6129992008209229
Epoch 1180, training loss: 881.5060424804688 = 0.5951634049415588 + 100.0 * 8.80910873413086
Epoch 1180, val loss: 0.6109883189201355
Epoch 1190, training loss: 882.0138549804688 = 0.592597484588623 + 100.0 * 8.814212799072266
Epoch 1190, val loss: 0.6088154911994934
Epoch 1200, training loss: 882.6255493164062 = 0.5900224447250366 + 100.0 * 8.820355415344238
Epoch 1200, val loss: 0.6067709922790527
Epoch 1210, training loss: 882.337890625 = 0.5874001979827881 + 100.0 * 8.8175048828125
Epoch 1210, val loss: 0.6046108603477478
Epoch 1220, training loss: 882.8096313476562 = 0.5849672555923462 + 100.0 * 8.822246551513672
Epoch 1220, val loss: 0.6026650071144104
Epoch 1230, training loss: 883.1586303710938 = 0.5825279951095581 + 100.0 * 8.825760841369629
Epoch 1230, val loss: 0.6007256507873535
Epoch 1240, training loss: 883.6498413085938 = 0.5801606178283691 + 100.0 * 8.830697059631348
Epoch 1240, val loss: 0.598853349685669
Epoch 1250, training loss: 883.51123046875 = 0.577674925327301 + 100.0 * 8.82933521270752
Epoch 1250, val loss: 0.5968447327613831
Epoch 1260, training loss: 883.7208251953125 = 0.5753874182701111 + 100.0 * 8.831454277038574
Epoch 1260, val loss: 0.5950658917427063
Epoch 1270, training loss: 883.9999389648438 = 0.5731625556945801 + 100.0 * 8.834267616271973
Epoch 1270, val loss: 0.5933688879013062
Epoch 1280, training loss: 883.7957153320312 = 0.5708668231964111 + 100.0 * 8.83224868774414
Epoch 1280, val loss: 0.5915787816047668
Epoch 1290, training loss: 883.9605712890625 = 0.5687088370323181 + 100.0 * 8.833918571472168
Epoch 1290, val loss: 0.589942216873169
Epoch 1300, training loss: 884.5855712890625 = 0.5666588544845581 + 100.0 * 8.840188980102539
Epoch 1300, val loss: 0.5885015726089478
Epoch 1310, training loss: 884.6724243164062 = 0.5645627379417419 + 100.0 * 8.841078758239746
Epoch 1310, val loss: 0.5867400169372559
Epoch 1320, training loss: 884.8654174804688 = 0.5624863505363464 + 100.0 * 8.843029022216797
Epoch 1320, val loss: 0.5852488875389099
Epoch 1330, training loss: 884.2203979492188 = 0.5603587627410889 + 100.0 * 8.836600303649902
Epoch 1330, val loss: 0.5837008357048035
Epoch 1340, training loss: 884.6464233398438 = 0.5582761168479919 + 100.0 * 8.84088134765625
Epoch 1340, val loss: 0.5821335911750793
Epoch 1350, training loss: 885.0505981445312 = 0.5563910603523254 + 100.0 * 8.844942092895508
Epoch 1350, val loss: 0.5807808637619019
Epoch 1360, training loss: 885.6339721679688 = 0.554527997970581 + 100.0 * 8.850794792175293
Epoch 1360, val loss: 0.5794270634651184
Epoch 1370, training loss: 885.6444091796875 = 0.5525243282318115 + 100.0 * 8.850918769836426
Epoch 1370, val loss: 0.5779978036880493
Epoch 1380, training loss: 885.9784545898438 = 0.5505796074867249 + 100.0 * 8.854278564453125
Epoch 1380, val loss: 0.5765107870101929
Epoch 1390, training loss: 885.1063232421875 = 0.5483379364013672 + 100.0 * 8.845580101013184
Epoch 1390, val loss: 0.5750564336776733
Epoch 1400, training loss: 884.3673706054688 = 0.5464732646942139 + 100.0 * 8.83820915222168
Epoch 1400, val loss: 0.5736759901046753
Epoch 1410, training loss: 885.1336059570312 = 0.5449556112289429 + 100.0 * 8.84588623046875
Epoch 1410, val loss: 0.5726698637008667
Epoch 1420, training loss: 885.7199096679688 = 0.5432829856872559 + 100.0 * 8.851766586303711
Epoch 1420, val loss: 0.5714816451072693
Epoch 1430, training loss: 886.0317993164062 = 0.541504979133606 + 100.0 * 8.854903221130371
Epoch 1430, val loss: 0.5703386068344116
Epoch 1440, training loss: 886.28125 = 0.5397888422012329 + 100.0 * 8.857414245605469
Epoch 1440, val loss: 0.5692168474197388
Epoch 1450, training loss: 886.583740234375 = 0.5381069779396057 + 100.0 * 8.860456466674805
Epoch 1450, val loss: 0.5680351257324219
Epoch 1460, training loss: 886.4803466796875 = 0.5364235639572144 + 100.0 * 8.8594388961792
Epoch 1460, val loss: 0.5668989419937134
Epoch 1470, training loss: 886.7648315429688 = 0.5347433686256409 + 100.0 * 8.862300872802734
Epoch 1470, val loss: 0.5658376216888428
Epoch 1480, training loss: 886.9077758789062 = 0.5331649780273438 + 100.0 * 8.863746643066406
Epoch 1480, val loss: 0.5646890997886658
Epoch 1490, training loss: 886.7935180664062 = 0.5314783453941345 + 100.0 * 8.86262035369873
Epoch 1490, val loss: 0.5635659098625183
Epoch 1500, training loss: 886.9485473632812 = 0.5298989415168762 + 100.0 * 8.86418628692627
Epoch 1500, val loss: 0.5625253319740295
Epoch 1510, training loss: 887.3887329101562 = 0.5283832550048828 + 100.0 * 8.868603706359863
Epoch 1510, val loss: 0.5615687966346741
Epoch 1520, training loss: 887.2451782226562 = 0.5267236828804016 + 100.0 * 8.86718463897705
Epoch 1520, val loss: 0.5604733824729919
Epoch 1530, training loss: 887.6119995117188 = 0.5251956582069397 + 100.0 * 8.870867729187012
Epoch 1530, val loss: 0.5594444870948792
Epoch 1540, training loss: 887.7745971679688 = 0.5236798524856567 + 100.0 * 8.872509002685547
Epoch 1540, val loss: 0.5584657788276672
Epoch 1550, training loss: 887.942626953125 = 0.5221796035766602 + 100.0 * 8.874204635620117
Epoch 1550, val loss: 0.557502031326294
Epoch 1560, training loss: 888.285400390625 = 0.5205948948860168 + 100.0 * 8.87764835357666
Epoch 1560, val loss: 0.5565213561058044
Epoch 1570, training loss: 887.826416015625 = 0.5189761519432068 + 100.0 * 8.873074531555176
Epoch 1570, val loss: 0.5555056929588318
Epoch 1580, training loss: 888.3739013671875 = 0.5176001787185669 + 100.0 * 8.878562927246094
Epoch 1580, val loss: 0.5547194480895996
Epoch 1590, training loss: 885.1990966796875 = 0.5151585340499878 + 100.0 * 8.846839904785156
Epoch 1590, val loss: 0.55313640832901
Epoch 1600, training loss: 885.5430297851562 = 0.5139326453208923 + 100.0 * 8.85029125213623
Epoch 1600, val loss: 0.5521180629730225
Epoch 1610, training loss: 886.7769165039062 = 0.5128629207611084 + 100.0 * 8.862640380859375
Epoch 1610, val loss: 0.5518090724945068
Epoch 1620, training loss: 887.246337890625 = 0.511616051197052 + 100.0 * 8.867347717285156
Epoch 1620, val loss: 0.5509839057922363
Epoch 1630, training loss: 888.1460571289062 = 0.5103532671928406 + 100.0 * 8.876357078552246
Epoch 1630, val loss: 0.5503511428833008
Epoch 1640, training loss: 888.0339965820312 = 0.5089396238327026 + 100.0 * 8.875250816345215
Epoch 1640, val loss: 0.5493861436843872
Epoch 1650, training loss: 888.6904907226562 = 0.507622241973877 + 100.0 * 8.881828308105469
Epoch 1650, val loss: 0.5485202670097351
Epoch 1660, training loss: 888.8727416992188 = 0.5062495470046997 + 100.0 * 8.883665084838867
Epoch 1660, val loss: 0.5477710962295532
Epoch 1670, training loss: 889.121337890625 = 0.5049034357070923 + 100.0 * 8.886164665222168
Epoch 1670, val loss: 0.5469552278518677
Epoch 1680, training loss: 888.665283203125 = 0.5034090876579285 + 100.0 * 8.88161849975586
Epoch 1680, val loss: 0.5459799766540527
Epoch 1690, training loss: 888.8002319335938 = 0.5020686984062195 + 100.0 * 8.882981300354004
Epoch 1690, val loss: 0.5451874732971191
Epoch 1700, training loss: 889.394287109375 = 0.5007964372634888 + 100.0 * 8.888935089111328
Epoch 1700, val loss: 0.5443616509437561
Epoch 1710, training loss: 889.2692260742188 = 0.49939948320388794 + 100.0 * 8.88769817352295
Epoch 1710, val loss: 0.5435692667961121
Epoch 1720, training loss: 889.4569702148438 = 0.4980572462081909 + 100.0 * 8.889589309692383
Epoch 1720, val loss: 0.5428462028503418
Epoch 1730, training loss: 889.8215942382812 = 0.49678122997283936 + 100.0 * 8.893248558044434
Epoch 1730, val loss: 0.5420580506324768
Epoch 1740, training loss: 889.9782104492188 = 0.495443195104599 + 100.0 * 8.894827842712402
Epoch 1740, val loss: 0.5413053631782532
Epoch 1750, training loss: 889.4563598632812 = 0.49406003952026367 + 100.0 * 8.889622688293457
Epoch 1750, val loss: 0.5404172539710999
Epoch 1760, training loss: 889.746337890625 = 0.4927363097667694 + 100.0 * 8.892536163330078
Epoch 1760, val loss: 0.5396764278411865
Epoch 1770, training loss: 890.3674926757812 = 0.4915083944797516 + 100.0 * 8.898759841918945
Epoch 1770, val loss: 0.5389888286590576
Epoch 1780, training loss: 890.422119140625 = 0.49016034603118896 + 100.0 * 8.899319648742676
Epoch 1780, val loss: 0.5382313132286072
Epoch 1790, training loss: 890.2196655273438 = 0.48882949352264404 + 100.0 * 8.897308349609375
Epoch 1790, val loss: 0.5374676585197449
Epoch 1800, training loss: 890.4951782226562 = 0.48751336336135864 + 100.0 * 8.900076866149902
Epoch 1800, val loss: 0.5367735028266907
Epoch 1810, training loss: 890.5990600585938 = 0.48620080947875977 + 100.0 * 8.901128768920898
Epoch 1810, val loss: 0.5359898805618286
Epoch 1820, training loss: 891.0051879882812 = 0.48490750789642334 + 100.0 * 8.905202865600586
Epoch 1820, val loss: 0.5353976488113403
Epoch 1830, training loss: 890.9267578125 = 0.48351943492889404 + 100.0 * 8.90443229675293
Epoch 1830, val loss: 0.5345748662948608
Epoch 1840, training loss: 891.0892333984375 = 0.4821530878543854 + 100.0 * 8.906070709228516
Epoch 1840, val loss: 0.5337199568748474
Epoch 1850, training loss: 890.9421997070312 = 0.480801522731781 + 100.0 * 8.904614448547363
Epoch 1850, val loss: 0.5329632759094238
Epoch 1860, training loss: 891.3426513671875 = 0.47946521639823914 + 100.0 * 8.908631324768066
Epoch 1860, val loss: 0.5322276949882507
Epoch 1870, training loss: 891.6162109375 = 0.4780576229095459 + 100.0 * 8.911381721496582
Epoch 1870, val loss: 0.5312744379043579
Epoch 1880, training loss: 891.55712890625 = 0.4765479862689972 + 100.0 * 8.910805702209473
Epoch 1880, val loss: 0.5300827026367188
Epoch 1890, training loss: 891.6963500976562 = 0.4750690758228302 + 100.0 * 8.912212371826172
Epoch 1890, val loss: 0.5299060940742493
Epoch 1900, training loss: 891.9762573242188 = 0.4735391139984131 + 100.0 * 8.915027618408203
Epoch 1900, val loss: 0.5285305976867676
Epoch 1910, training loss: 891.7338256835938 = 0.4720068871974945 + 100.0 * 8.912618637084961
Epoch 1910, val loss: 0.5280770659446716
Epoch 1920, training loss: 890.0840454101562 = 0.4702255129814148 + 100.0 * 8.896138191223145
Epoch 1920, val loss: 0.5266706943511963
Epoch 1930, training loss: 890.90771484375 = 0.46875908970832825 + 100.0 * 8.904389381408691
Epoch 1930, val loss: 0.5263851284980774
Epoch 1940, training loss: 889.8399047851562 = 0.46734464168548584 + 100.0 * 8.893725395202637
Epoch 1940, val loss: 0.5248382091522217
Epoch 1950, training loss: 889.3159790039062 = 0.4657680094242096 + 100.0 * 8.88850212097168
Epoch 1950, val loss: 0.5244398713111877
Epoch 1960, training loss: 890.5680541992188 = 0.46454760432243347 + 100.0 * 8.90103530883789
Epoch 1960, val loss: 0.5238693952560425
Epoch 1970, training loss: 891.0798950195312 = 0.46308398246765137 + 100.0 * 8.906167984008789
Epoch 1970, val loss: 0.5228731632232666
Epoch 1980, training loss: 891.5197143554688 = 0.4617375433444977 + 100.0 * 8.910579681396484
Epoch 1980, val loss: 0.5223438739776611
Epoch 1990, training loss: 892.1044921875 = 0.46041274070739746 + 100.0 * 8.916440963745117
Epoch 1990, val loss: 0.5215741395950317
Epoch 2000, training loss: 891.6759643554688 = 0.45892882347106934 + 100.0 * 8.91217041015625
Epoch 2000, val loss: 0.5208721160888672
Epoch 2010, training loss: 891.8106079101562 = 0.45756202936172485 + 100.0 * 8.913530349731445
Epoch 2010, val loss: 0.5199834108352661
Epoch 2020, training loss: 892.2860107421875 = 0.4562738835811615 + 100.0 * 8.918296813964844
Epoch 2020, val loss: 0.5193448066711426
Epoch 2030, training loss: 892.4903564453125 = 0.4548923969268799 + 100.0 * 8.920354843139648
Epoch 2030, val loss: 0.5188339352607727
Epoch 2040, training loss: 892.3463745117188 = 0.45348697900772095 + 100.0 * 8.918929100036621
Epoch 2040, val loss: 0.5180745720863342
Epoch 2050, training loss: 892.5546264648438 = 0.4520673155784607 + 100.0 * 8.921025276184082
Epoch 2050, val loss: 0.5170410871505737
Epoch 2060, training loss: 892.802978515625 = 0.4507112503051758 + 100.0 * 8.92352294921875
Epoch 2060, val loss: 0.5165064334869385
Epoch 2070, training loss: 892.8809814453125 = 0.4493310749530792 + 100.0 * 8.92431640625
Epoch 2070, val loss: 0.5157522559165955
Epoch 2080, training loss: 892.9841918945312 = 0.44796106219291687 + 100.0 * 8.925362586975098
Epoch 2080, val loss: 0.5153932571411133
Epoch 2090, training loss: 892.4942626953125 = 0.4465826153755188 + 100.0 * 8.920476913452148
Epoch 2090, val loss: 0.5143538117408752
Epoch 2100, training loss: 891.1961669921875 = 0.44498732686042786 + 100.0 * 8.907511711120605
Epoch 2100, val loss: 0.5134857296943665
Epoch 2110, training loss: 891.5425415039062 = 0.44364669919013977 + 100.0 * 8.910988807678223
Epoch 2110, val loss: 0.5129589438438416
Epoch 2120, training loss: 892.1767578125 = 0.4424443244934082 + 100.0 * 8.917343139648438
Epoch 2120, val loss: 0.5122137069702148
Epoch 2130, training loss: 892.802001953125 = 0.4412943124771118 + 100.0 * 8.923606872558594
Epoch 2130, val loss: 0.5118395090103149
Epoch 2140, training loss: 893.4663696289062 = 0.439942866563797 + 100.0 * 8.930264472961426
Epoch 2140, val loss: 0.51121586561203
Epoch 2150, training loss: 893.2044067382812 = 0.4385712444782257 + 100.0 * 8.927658081054688
Epoch 2150, val loss: 0.5103529095649719
Epoch 2160, training loss: 893.552001953125 = 0.43725818395614624 + 100.0 * 8.931147575378418
Epoch 2160, val loss: 0.5100070834159851
Epoch 2170, training loss: 893.6499633789062 = 0.43590036034584045 + 100.0 * 8.932140350341797
Epoch 2170, val loss: 0.5097510814666748
Epoch 2180, training loss: 889.1834106445312 = 0.43510866165161133 + 100.0 * 8.887482643127441
Epoch 2180, val loss: 0.5078816413879395
Epoch 2190, training loss: 889.3554077148438 = 0.4345338046550751 + 100.0 * 8.889208793640137
Epoch 2190, val loss: 0.5096544027328491
Epoch 2200, training loss: 887.4788818359375 = 0.43327054381370544 + 100.0 * 8.870455741882324
Epoch 2200, val loss: 0.5091630220413208
Epoch 2210, training loss: 889.596923828125 = 0.43237683176994324 + 100.0 * 8.891645431518555
Epoch 2210, val loss: 0.5069344639778137
Epoch 2220, training loss: 890.288330078125 = 0.4310097396373749 + 100.0 * 8.89857292175293
Epoch 2220, val loss: 0.5070046782493591
Epoch 2230, training loss: 891.6119995117188 = 0.42977049946784973 + 100.0 * 8.911822319030762
Epoch 2230, val loss: 0.50665682554245
Epoch 2240, training loss: 891.0950927734375 = 0.4282305836677551 + 100.0 * 8.906668663024902
Epoch 2240, val loss: 0.505808413028717
Epoch 2250, training loss: 891.8782958984375 = 0.4269556701183319 + 100.0 * 8.91451358795166
Epoch 2250, val loss: 0.5052233338356018
Epoch 2260, training loss: 892.9087524414062 = 0.4257284700870514 + 100.0 * 8.924830436706543
Epoch 2260, val loss: 0.5042164325714111
Epoch 2270, training loss: 893.2723388671875 = 0.4244477450847626 + 100.0 * 8.928479194641113
Epoch 2270, val loss: 0.5042777061462402
Epoch 2280, training loss: 893.9095458984375 = 0.4231778681278229 + 100.0 * 8.934864044189453
Epoch 2280, val loss: 0.5034180879592896
Epoch 2290, training loss: 894.4581298828125 = 0.4219290614128113 + 100.0 * 8.940361976623535
Epoch 2290, val loss: 0.5029945969581604
Epoch 2300, training loss: 894.8526000976562 = 0.4206549823284149 + 100.0 * 8.944319725036621
Epoch 2300, val loss: 0.5023244619369507
Epoch 2310, training loss: 894.9757080078125 = 0.41937586665153503 + 100.0 * 8.945563316345215
Epoch 2310, val loss: 0.5018734931945801
Epoch 2320, training loss: 895.3309936523438 = 0.41811689734458923 + 100.0 * 8.949129104614258
Epoch 2320, val loss: 0.5013079643249512
Epoch 2330, training loss: 895.4664916992188 = 0.41683727502822876 + 100.0 * 8.950496673583984
Epoch 2330, val loss: 0.5007928013801575
Epoch 2340, training loss: 895.5966186523438 = 0.415539026260376 + 100.0 * 8.951810836791992
Epoch 2340, val loss: 0.5002299547195435
Epoch 2350, training loss: 895.75146484375 = 0.41423121094703674 + 100.0 * 8.95337200164795
Epoch 2350, val loss: 0.49963557720184326
Epoch 2360, training loss: 895.9202880859375 = 0.4129723012447357 + 100.0 * 8.955073356628418
Epoch 2360, val loss: 0.49910250306129456
Epoch 2370, training loss: 896.219970703125 = 0.4116825461387634 + 100.0 * 8.958083152770996
Epoch 2370, val loss: 0.4984651803970337
Epoch 2380, training loss: 896.2930908203125 = 0.4103946089744568 + 100.0 * 8.958827018737793
Epoch 2380, val loss: 0.49795666337013245
Epoch 2390, training loss: 896.2830200195312 = 0.40911540389060974 + 100.0 * 8.958739280700684
Epoch 2390, val loss: 0.49741131067276
Epoch 2400, training loss: 896.5178833007812 = 0.4078243374824524 + 100.0 * 8.961100578308105
Epoch 2400, val loss: 0.49695199728012085
Epoch 2410, training loss: 896.5362548828125 = 0.4065605700016022 + 100.0 * 8.961297035217285
Epoch 2410, val loss: 0.4963815212249756
Epoch 2420, training loss: 896.7864990234375 = 0.4053119719028473 + 100.0 * 8.963811874389648
Epoch 2420, val loss: 0.4958115518093109
Epoch 2430, training loss: 897.0720825195312 = 0.4040501117706299 + 100.0 * 8.966680526733398
Epoch 2430, val loss: 0.4953218400478363
Epoch 2440, training loss: 897.1280517578125 = 0.4027658998966217 + 100.0 * 8.967252731323242
Epoch 2440, val loss: 0.494782418012619
Epoch 2450, training loss: 896.8009033203125 = 0.40144792199134827 + 100.0 * 8.963994979858398
Epoch 2450, val loss: 0.4941581189632416
Epoch 2460, training loss: 895.910888671875 = 0.4001469910144806 + 100.0 * 8.955107688903809
Epoch 2460, val loss: 0.4938472807407379
Epoch 2470, training loss: 896.0758056640625 = 0.3987675905227661 + 100.0 * 8.956770896911621
Epoch 2470, val loss: 0.49321264028549194
Epoch 2480, training loss: 896.5138549804688 = 0.39755138754844666 + 100.0 * 8.961163520812988
Epoch 2480, val loss: 0.4927227199077606
Epoch 2490, training loss: 897.05078125 = 0.39633962512016296 + 100.0 * 8.966544151306152
Epoch 2490, val loss: 0.4923042356967926
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8118840579710145
0.8641599652249512
The final CL Acc:0.82812, 0.01165, The final GNN Acc:0.86433, 0.00029
Begin epxeriment: noisy_level: 0.2 cont_weight: 100 epoch: der1:0.3 der2:0.3 dfr1:0.4 dfr2:0.5
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=2500, cl_num_layers=2, cl_num_proj_hidden=64, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=100.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.3, drop_feat_rate_1=0.4, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=32, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, noisy_level=0.2, num_hidden=64, num_proj_hidden=64, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=40, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 88648])
add edge: torch.Size([2, 106372])
remove edge: torch.Size([2, 70968])
updated graph: torch.Size([2, 88692])
=== Noisy graph ===
/home/project-robust-contrastive/Robust-Contrastive-Learning/model.py:227: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 1036.338623046875 = 1.1021302938461304 + 100.0 * 10.352364540100098
Epoch 0, val loss: 1.1012619733810425
Epoch 10, training loss: 999.0082397460938 = 1.1009902954101562 + 100.0 * 9.979072570800781
Epoch 10, val loss: 1.1001908779144287
Epoch 20, training loss: 978.5648193359375 = 1.1004210710525513 + 100.0 * 9.774643898010254
Epoch 20, val loss: 1.0996350049972534
Epoch 30, training loss: 963.394287109375 = 1.0997750759124756 + 100.0 * 9.622944831848145
Epoch 30, val loss: 1.0989978313446045
Epoch 40, training loss: 951.3402709960938 = 1.0991588830947876 + 100.0 * 9.502410888671875
Epoch 40, val loss: 1.0983940362930298
Epoch 50, training loss: 941.3727416992188 = 1.0985277891159058 + 100.0 * 9.402742385864258
Epoch 50, val loss: 1.0977733135223389
Epoch 60, training loss: 933.1506958007812 = 1.097875714302063 + 100.0 * 9.320528030395508
Epoch 60, val loss: 1.0971364974975586
Epoch 70, training loss: 926.27685546875 = 1.0971932411193848 + 100.0 * 9.25179672241211
Epoch 70, val loss: 1.0964679718017578
Epoch 80, training loss: 920.4192504882812 = 1.096521019935608 + 100.0 * 9.193227767944336
Epoch 80, val loss: 1.0958101749420166
Epoch 90, training loss: 915.3316040039062 = 1.0957984924316406 + 100.0 * 9.14235782623291
Epoch 90, val loss: 1.0951063632965088
Epoch 100, training loss: 910.9917602539062 = 1.095065712928772 + 100.0 * 9.098966598510742
Epoch 100, val loss: 1.0943846702575684
Epoch 110, training loss: 907.0950317382812 = 1.0943187475204468 + 100.0 * 9.060007095336914
Epoch 110, val loss: 1.0936474800109863
Epoch 120, training loss: 903.5986938476562 = 1.0935393571853638 + 100.0 * 9.02505111694336
Epoch 120, val loss: 1.092882513999939
Epoch 130, training loss: 900.4964599609375 = 1.092751145362854 + 100.0 * 8.994036674499512
Epoch 130, val loss: 1.0921014547348022
Epoch 140, training loss: 898.010986328125 = 1.0919315814971924 + 100.0 * 8.96919059753418
Epoch 140, val loss: 1.0912915468215942
Epoch 150, training loss: 895.72802734375 = 1.0910998582839966 + 100.0 * 8.946369171142578
Epoch 150, val loss: 1.0904642343521118
Epoch 160, training loss: 894.1229248046875 = 1.0902806520462036 + 100.0 * 8.930326461791992
Epoch 160, val loss: 1.0896517038345337
Epoch 170, training loss: 892.5349731445312 = 1.0894032716751099 + 100.0 * 8.91445541381836
Epoch 170, val loss: 1.088800311088562
Epoch 180, training loss: 891.3696899414062 = 1.088498830795288 + 100.0 * 8.902812004089355
Epoch 180, val loss: 1.0879058837890625
Epoch 190, training loss: 889.990478515625 = 1.0875513553619385 + 100.0 * 8.889029502868652
Epoch 190, val loss: 1.0869735479354858
Epoch 200, training loss: 889.4879760742188 = 1.0866488218307495 + 100.0 * 8.884013175964355
Epoch 200, val loss: 1.086076021194458
Epoch 210, training loss: 888.5142822265625 = 1.085692048072815 + 100.0 * 8.874285697937012
Epoch 210, val loss: 1.0851376056671143
Epoch 220, training loss: 887.8201293945312 = 1.0847257375717163 + 100.0 * 8.867354393005371
Epoch 220, val loss: 1.0841795206069946
Epoch 230, training loss: 887.5081787109375 = 1.083748698234558 + 100.0 * 8.86424446105957
Epoch 230, val loss: 1.0832064151763916
Epoch 240, training loss: 887.3248901367188 = 1.0827569961547852 + 100.0 * 8.862421035766602
Epoch 240, val loss: 1.0822312831878662
Epoch 250, training loss: 886.7197875976562 = 1.0817389488220215 + 100.0 * 8.856380462646484
Epoch 250, val loss: 1.0812493562698364
Epoch 260, training loss: 886.5840454101562 = 1.0807675123214722 + 100.0 * 8.855032920837402
Epoch 260, val loss: 1.0802918672561646
Epoch 270, training loss: 886.1331176757812 = 1.0797936916351318 + 100.0 * 8.850533485412598
Epoch 270, val loss: 1.079343318939209
Epoch 280, training loss: 886.0247192382812 = 1.0788586139678955 + 100.0 * 8.849458694458008
Epoch 280, val loss: 1.0784329175949097
Epoch 290, training loss: 885.7762451171875 = 1.0779380798339844 + 100.0 * 8.846982955932617
Epoch 290, val loss: 1.0775330066680908
Epoch 300, training loss: 885.9647827148438 = 1.0770660638809204 + 100.0 * 8.848876953125
Epoch 300, val loss: 1.076698899269104
Epoch 310, training loss: 885.8231811523438 = 1.0762600898742676 + 100.0 * 8.847469329833984
Epoch 310, val loss: 1.0759187936782837
Epoch 320, training loss: 886.0259399414062 = 1.0754847526550293 + 100.0 * 8.849504470825195
Epoch 320, val loss: 1.0751824378967285
Epoch 330, training loss: 886.3206176757812 = 1.074723482131958 + 100.0 * 8.852458953857422
Epoch 330, val loss: 1.074456810951233
Epoch 340, training loss: 886.5274047851562 = 1.073983907699585 + 100.0 * 8.854534149169922
Epoch 340, val loss: 1.0737491846084595
Epoch 350, training loss: 886.1825561523438 = 1.073227047920227 + 100.0 * 8.851093292236328
Epoch 350, val loss: 1.0730478763580322
Epoch 360, training loss: 886.7877807617188 = 1.0725263357162476 + 100.0 * 8.857152938842773
Epoch 360, val loss: 1.0723646879196167
Epoch 370, training loss: 886.1188354492188 = 1.071752905845642 + 100.0 * 8.850470542907715
Epoch 370, val loss: 1.0716325044631958
Epoch 380, training loss: 886.5447387695312 = 1.070983648300171 + 100.0 * 8.854737281799316
Epoch 380, val loss: 1.0709075927734375
Epoch 390, training loss: 887.181884765625 = 1.070236325263977 + 100.0 * 8.861116409301758
Epoch 390, val loss: 1.0701802968978882
Epoch 400, training loss: 886.6279907226562 = 1.0694783926010132 + 100.0 * 8.855585098266602
Epoch 400, val loss: 1.069496512413025
Epoch 410, training loss: 886.2307739257812 = 1.0686254501342773 + 100.0 * 8.851621627807617
Epoch 410, val loss: 1.0686753988265991
Epoch 420, training loss: 887.144775390625 = 1.0678859949111938 + 100.0 * 8.860769271850586
Epoch 420, val loss: 1.067977786064148
Epoch 430, training loss: 887.4652099609375 = 1.067079782485962 + 100.0 * 8.863981246948242
Epoch 430, val loss: 1.0672130584716797
Epoch 440, training loss: 887.591796875 = 1.066274642944336 + 100.0 * 8.865255355834961
Epoch 440, val loss: 1.0664429664611816
Epoch 450, training loss: 887.8367309570312 = 1.0654518604278564 + 100.0 * 8.86771297454834
Epoch 450, val loss: 1.0656750202178955
Epoch 460, training loss: 888.188720703125 = 1.0646065473556519 + 100.0 * 8.871240615844727
Epoch 460, val loss: 1.0648703575134277
Epoch 470, training loss: 888.396240234375 = 1.063763976097107 + 100.0 * 8.873324394226074
Epoch 470, val loss: 1.064075231552124
Epoch 480, training loss: 888.0401611328125 = 1.062893271446228 + 100.0 * 8.869772911071777
Epoch 480, val loss: 1.0632489919662476
Epoch 490, training loss: 888.9724731445312 = 1.0619966983795166 + 100.0 * 8.879104614257812
Epoch 490, val loss: 1.0624045133590698
Epoch 500, training loss: 889.2252197265625 = 1.0611143112182617 + 100.0 * 8.881641387939453
Epoch 500, val loss: 1.0615649223327637
Epoch 510, training loss: 889.6554565429688 = 1.060210943222046 + 100.0 * 8.885952949523926
Epoch 510, val loss: 1.0607187747955322
Epoch 520, training loss: 889.961181640625 = 1.0592834949493408 + 100.0 * 8.889019012451172
Epoch 520, val loss: 1.0598429441452026
Epoch 530, training loss: 890.1445922851562 = 1.0583590269088745 + 100.0 * 8.890862464904785
Epoch 530, val loss: 1.058970332145691
Epoch 540, training loss: 890.46337890625 = 1.0574142932891846 + 100.0 * 8.894059181213379
Epoch 540, val loss: 1.0580759048461914
Epoch 550, training loss: 890.0577392578125 = 1.0564278364181519 + 100.0 * 8.890012741088867
Epoch 550, val loss: 1.0571486949920654
Epoch 560, training loss: 890.2762451171875 = 1.0554428100585938 + 100.0 * 8.892208099365234
Epoch 560, val loss: 1.0562140941619873
Epoch 570, training loss: 890.3171997070312 = 1.0544540882110596 + 100.0 * 8.892627716064453
Epoch 570, val loss: 1.0552821159362793
Epoch 580, training loss: 889.7495727539062 = 1.053398609161377 + 100.0 * 8.886961936950684
Epoch 580, val loss: 1.0542902946472168
Epoch 590, training loss: 890.43701171875 = 1.0523799657821655 + 100.0 * 8.89384651184082
Epoch 590, val loss: 1.05331289768219
Epoch 600, training loss: 890.6046142578125 = 1.0513525009155273 + 100.0 * 8.895532608032227
Epoch 600, val loss: 1.0523438453674316
Epoch 610, training loss: 891.44873046875 = 1.0502923727035522 + 100.0 * 8.903984069824219
Epoch 610, val loss: 1.0513310432434082
Epoch 620, training loss: 892.1659545898438 = 1.0492380857467651 + 100.0 * 8.91116714477539
Epoch 620, val loss: 1.0503385066986084
Epoch 630, training loss: 891.860595703125 = 1.0481078624725342 + 100.0 * 8.908124923706055
Epoch 630, val loss: 1.0492788553237915
Epoch 640, training loss: 892.6578979492188 = 1.0469969511032104 + 100.0 * 8.916109085083008
Epoch 640, val loss: 1.0482412576675415
Epoch 650, training loss: 893.023193359375 = 1.0458810329437256 + 100.0 * 8.91977310180664
Epoch 650, val loss: 1.047182321548462
Epoch 660, training loss: 892.9207153320312 = 1.044707179069519 + 100.0 * 8.918760299682617
Epoch 660, val loss: 1.0460755825042725
Epoch 670, training loss: 893.0218505859375 = 1.0435211658477783 + 100.0 * 8.919783592224121
Epoch 670, val loss: 1.0449731349945068
Epoch 680, training loss: 893.6197509765625 = 1.042346477508545 + 100.0 * 8.925773620605469
Epoch 680, val loss: 1.0438538789749146
Epoch 690, training loss: 893.9371948242188 = 1.041138768196106 + 100.0 * 8.928960800170898
Epoch 690, val loss: 1.042711615562439
Epoch 700, training loss: 894.1273803710938 = 1.0398961305618286 + 100.0 * 8.930874824523926
Epoch 700, val loss: 1.0415244102478027
Epoch 710, training loss: 894.8906860351562 = 1.038602590560913 + 100.0 * 8.938521385192871
Epoch 710, val loss: 1.0403063297271729
Epoch 720, training loss: 894.0438842773438 = 1.037327527999878 + 100.0 * 8.930065155029297
Epoch 720, val loss: 1.039090633392334
Epoch 730, training loss: 894.9781494140625 = 1.0360363721847534 + 100.0 * 8.939421653747559
Epoch 730, val loss: 1.0378918647766113
Epoch 740, training loss: 890.1590576171875 = 1.0343233346939087 + 100.0 * 8.891247749328613
Epoch 740, val loss: 1.0361875295639038
Epoch 750, training loss: 895.8162841796875 = 1.0333833694458008 + 100.0 * 8.947829246520996
Epoch 750, val loss: 1.035502552986145
Epoch 760, training loss: 892.9256591796875 = 1.0321269035339355 + 100.0 * 8.918935775756836
Epoch 760, val loss: 1.034130334854126
Epoch 770, training loss: 893.7698974609375 = 1.0309112071990967 + 100.0 * 8.927390098571777
Epoch 770, val loss: 1.0330413579940796
Epoch 780, training loss: 892.71728515625 = 1.0295737981796265 + 100.0 * 8.916876792907715
Epoch 780, val loss: 1.0317540168762207
Epoch 790, training loss: 894.0580444335938 = 1.0282752513885498 + 100.0 * 8.9302978515625
Epoch 790, val loss: 1.030587077140808
Epoch 800, training loss: 895.1846313476562 = 1.0269347429275513 + 100.0 * 8.941576957702637
Epoch 800, val loss: 1.0292718410491943
Epoch 810, training loss: 892.6751098632812 = 1.0254380702972412 + 100.0 * 8.916496276855469
Epoch 810, val loss: 1.027884602546692
Epoch 820, training loss: 895.5696411132812 = 1.024139642715454 + 100.0 * 8.945454597473145
Epoch 820, val loss: 1.0266597270965576
Epoch 830, training loss: 894.3156127929688 = 1.0228240489959717 + 100.0 * 8.932928085327148
Epoch 830, val loss: 1.0254194736480713
Epoch 840, training loss: 894.6715087890625 = 1.0215343236923218 + 100.0 * 8.93649959564209
Epoch 840, val loss: 1.0241912603378296
Epoch 850, training loss: 896.4335327148438 = 1.0201616287231445 + 100.0 * 8.954133987426758
Epoch 850, val loss: 1.0229283571243286
Epoch 860, training loss: 896.7590942382812 = 1.0188246965408325 + 100.0 * 8.957403182983398
Epoch 860, val loss: 1.0216622352600098
Epoch 870, training loss: 897.2042846679688 = 1.017454981803894 + 100.0 * 8.961868286132812
Epoch 870, val loss: 1.0203604698181152
Epoch 880, training loss: 897.216064453125 = 1.0160444974899292 + 100.0 * 8.961999893188477
Epoch 880, val loss: 1.0190335512161255
Epoch 890, training loss: 896.9213256835938 = 1.0146064758300781 + 100.0 * 8.959067344665527
Epoch 890, val loss: 1.0176846981048584
Epoch 900, training loss: 897.7230834960938 = 1.013265609741211 + 100.0 * 8.967098236083984
Epoch 900, val loss: 1.0164210796356201
Epoch 910, training loss: 897.6590576171875 = 1.01180100440979 + 100.0 * 8.966472625732422
Epoch 910, val loss: 1.015035629272461
Epoch 920, training loss: 898.1318359375 = 1.0103650093078613 + 100.0 * 8.971214294433594
Epoch 920, val loss: 1.0136970281600952
Epoch 930, training loss: 898.1958618164062 = 1.0089143514633179 + 100.0 * 8.971869468688965
Epoch 930, val loss: 1.0123211145401
Epoch 940, training loss: 898.5169067382812 = 1.0074493885040283 + 100.0 * 8.97509479522705
Epoch 940, val loss: 1.010951042175293
Epoch 950, training loss: 898.8600463867188 = 1.0059657096862793 + 100.0 * 8.978540420532227
Epoch 950, val loss: 1.0095465183258057
Epoch 960, training loss: 898.0368041992188 = 1.0044466257095337 + 100.0 * 8.97032356262207
Epoch 960, val loss: 1.0081140995025635
Epoch 970, training loss: 898.6442260742188 = 1.0029572248458862 + 100.0 * 8.976412773132324
Epoch 970, val loss: 1.0067152976989746
Epoch 980, training loss: 899.0780029296875 = 1.0014851093292236 + 100.0 * 8.980765342712402
Epoch 980, val loss: 1.0053032636642456
Epoch 990, training loss: 899.6222534179688 = 0.9999791383743286 + 100.0 * 8.986222267150879
Epoch 990, val loss: 1.0039037466049194
Epoch 1000, training loss: 899.695556640625 = 0.9984486103057861 + 100.0 * 8.986970901489258
Epoch 1000, val loss: 1.0024402141571045
Epoch 1010, training loss: 900.1767578125 = 0.9969285726547241 + 100.0 * 8.991798400878906
Epoch 1010, val loss: 1.0010159015655518
Epoch 1020, training loss: 900.3448486328125 = 0.9953716397285461 + 100.0 * 8.993494987487793
Epoch 1020, val loss: 0.9995466470718384
Epoch 1030, training loss: 900.0252075195312 = 0.9938225150108337 + 100.0 * 8.990313529968262
Epoch 1030, val loss: 0.9980711936950684
Epoch 1040, training loss: 900.421630859375 = 0.9922911524772644 + 100.0 * 8.994293212890625
Epoch 1040, val loss: 0.9966306090354919
Epoch 1050, training loss: 900.688720703125 = 0.9906906485557556 + 100.0 * 8.996980667114258
Epoch 1050, val loss: 0.9951573610305786
Epoch 1060, training loss: 900.3801879882812 = 0.9891427755355835 + 100.0 * 8.993910789489746
Epoch 1060, val loss: 0.9936931133270264
Epoch 1070, training loss: 901.0765991210938 = 0.9875943660736084 + 100.0 * 9.000889778137207
Epoch 1070, val loss: 0.9922511577606201
Epoch 1080, training loss: 901.4972534179688 = 0.9859784841537476 + 100.0 * 9.005112648010254
Epoch 1080, val loss: 0.9907183647155762
Epoch 1090, training loss: 901.45556640625 = 0.9843671917915344 + 100.0 * 9.004712104797363
Epoch 1090, val loss: 0.9892175793647766
Epoch 1100, training loss: 898.8150634765625 = 0.9825689196586609 + 100.0 * 8.978324890136719
Epoch 1100, val loss: 0.9875231981277466
Epoch 1110, training loss: 901.1439819335938 = 0.9811941385269165 + 100.0 * 9.001627922058105
Epoch 1110, val loss: 0.9862065315246582
Epoch 1120, training loss: 900.9840698242188 = 0.9796097278594971 + 100.0 * 9.000044822692871
Epoch 1120, val loss: 0.9847317337989807
Epoch 1130, training loss: 901.4661865234375 = 0.9780288934707642 + 100.0 * 9.004881858825684
Epoch 1130, val loss: 0.9832519292831421
Epoch 1140, training loss: 902.1194458007812 = 0.9763897657394409 + 100.0 * 9.011430740356445
Epoch 1140, val loss: 0.9817084670066833
Epoch 1150, training loss: 902.8917236328125 = 0.9748323559761047 + 100.0 * 9.019168853759766
Epoch 1150, val loss: 0.9802094101905823
Epoch 1160, training loss: 903.2100830078125 = 0.9731467962265015 + 100.0 * 9.022369384765625
Epoch 1160, val loss: 0.9786813855171204
Epoch 1170, training loss: 903.6542358398438 = 0.9715332984924316 + 100.0 * 9.026826858520508
Epoch 1170, val loss: 0.9771407246589661
Epoch 1180, training loss: 903.291259765625 = 0.9698742628097534 + 100.0 * 9.023214340209961
Epoch 1180, val loss: 0.9755919575691223
Epoch 1190, training loss: 903.230224609375 = 0.9681956171989441 + 100.0 * 9.02262020111084
Epoch 1190, val loss: 0.9740109443664551
Epoch 1200, training loss: 903.664794921875 = 0.96657794713974 + 100.0 * 9.026982307434082
Epoch 1200, val loss: 0.9724923968315125
Epoch 1210, training loss: 904.3175659179688 = 0.9649238586425781 + 100.0 * 9.033526420593262
Epoch 1210, val loss: 0.9709465503692627
Epoch 1220, training loss: 904.4907836914062 = 0.9632678627967834 + 100.0 * 9.03527545928955
Epoch 1220, val loss: 0.9693686366081238
Epoch 1230, training loss: 904.1356811523438 = 0.9615316987037659 + 100.0 * 9.03174114227295
Epoch 1230, val loss: 0.9677430987358093
Epoch 1240, training loss: 904.5033569335938 = 0.9598810076713562 + 100.0 * 9.03543472290039
Epoch 1240, val loss: 0.9661946892738342
Epoch 1250, training loss: 905.3926391601562 = 0.9582634568214417 + 100.0 * 9.044343948364258
Epoch 1250, val loss: 0.9646788239479065
Epoch 1260, training loss: 905.4359741210938 = 0.9565424919128418 + 100.0 * 9.044794082641602
Epoch 1260, val loss: 0.9630811214447021
Epoch 1270, training loss: 905.6722412109375 = 0.9548735022544861 + 100.0 * 9.047173500061035
Epoch 1270, val loss: 0.9615192413330078
Epoch 1280, training loss: 906.05078125 = 0.9531877636909485 + 100.0 * 9.050975799560547
Epoch 1280, val loss: 0.9599351286888123
Epoch 1290, training loss: 905.280517578125 = 0.9514170289039612 + 100.0 * 9.043291091918945
Epoch 1290, val loss: 0.9582516551017761
Epoch 1300, training loss: 905.7886962890625 = 0.9497657418251038 + 100.0 * 9.048389434814453
Epoch 1300, val loss: 0.9566844701766968
Epoch 1310, training loss: 905.6307373046875 = 0.9479578137397766 + 100.0 * 9.046828269958496
Epoch 1310, val loss: 0.9550558924674988
Epoch 1320, training loss: 906.0307006835938 = 0.9462823867797852 + 100.0 * 9.050844192504883
Epoch 1320, val loss: 0.9534782767295837
Epoch 1330, training loss: 906.4291381835938 = 0.9445924758911133 + 100.0 * 9.054845809936523
Epoch 1330, val loss: 0.951883852481842
Epoch 1340, training loss: 906.6681518554688 = 0.9428458213806152 + 100.0 * 9.057252883911133
Epoch 1340, val loss: 0.9502424001693726
Epoch 1350, training loss: 906.7214965820312 = 0.941124677658081 + 100.0 * 9.057804107666016
Epoch 1350, val loss: 0.9486104846000671
Epoch 1360, training loss: 906.3709106445312 = 0.9393582344055176 + 100.0 * 9.054315567016602
Epoch 1360, val loss: 0.946946382522583
Epoch 1370, training loss: 906.912353515625 = 0.9376962780952454 + 100.0 * 9.059746742248535
Epoch 1370, val loss: 0.9454172849655151
Epoch 1380, training loss: 907.5253295898438 = 0.9360174536705017 + 100.0 * 9.065893173217773
Epoch 1380, val loss: 0.9438403248786926
Epoch 1390, training loss: 907.6654663085938 = 0.9342909455299377 + 100.0 * 9.067312240600586
Epoch 1390, val loss: 0.9422038197517395
Epoch 1400, training loss: 907.7944946289062 = 0.9325125217437744 + 100.0 * 9.068619728088379
Epoch 1400, val loss: 0.9405555725097656
Epoch 1410, training loss: 907.6203002929688 = 0.9307008385658264 + 100.0 * 9.066895484924316
Epoch 1410, val loss: 0.9388753175735474
Epoch 1420, training loss: 908.266357421875 = 0.9289852976799011 + 100.0 * 9.073373794555664
Epoch 1420, val loss: 0.9372457265853882
Epoch 1430, training loss: 908.1619262695312 = 0.9271695017814636 + 100.0 * 9.072347640991211
Epoch 1430, val loss: 0.9355790615081787
Epoch 1440, training loss: 908.4808349609375 = 0.9253709316253662 + 100.0 * 9.075554847717285
Epoch 1440, val loss: 0.9338643550872803
Epoch 1450, training loss: 908.10302734375 = 0.9235794544219971 + 100.0 * 9.071794509887695
Epoch 1450, val loss: 0.9322288632392883
Epoch 1460, training loss: 904.6925048828125 = 0.9217179417610168 + 100.0 * 9.037708282470703
Epoch 1460, val loss: 0.9304807186126709
Epoch 1470, training loss: 907.3331298828125 = 0.920254647731781 + 100.0 * 9.064128875732422
Epoch 1470, val loss: 0.9291206002235413
Epoch 1480, training loss: 903.3020629882812 = 0.9181347489356995 + 100.0 * 9.023838996887207
Epoch 1480, val loss: 0.9270649552345276
Epoch 1490, training loss: 906.86083984375 = 0.9161787033081055 + 100.0 * 9.059446334838867
Epoch 1490, val loss: 0.9251915216445923
Epoch 1500, training loss: 904.386962890625 = 0.9146100282669067 + 100.0 * 9.034723281860352
Epoch 1500, val loss: 0.9240590929985046
Epoch 1510, training loss: 905.9171142578125 = 0.9129695296287537 + 100.0 * 9.050041198730469
Epoch 1510, val loss: 0.9222484827041626
Epoch 1520, training loss: 906.2021484375 = 0.9111629724502563 + 100.0 * 9.052909851074219
Epoch 1520, val loss: 0.9206494688987732
Epoch 1530, training loss: 906.34765625 = 0.9093047976493835 + 100.0 * 9.054383277893066
Epoch 1530, val loss: 0.9189174771308899
Epoch 1540, training loss: 907.5376586914062 = 0.9075777530670166 + 100.0 * 9.066300392150879
Epoch 1540, val loss: 0.9172999262809753
Epoch 1550, training loss: 908.4583740234375 = 0.9057759642601013 + 100.0 * 9.075526237487793
Epoch 1550, val loss: 0.9156306982040405
Epoch 1560, training loss: 908.311279296875 = 0.9039013385772705 + 100.0 * 9.074073791503906
Epoch 1560, val loss: 0.9138888120651245
Epoch 1570, training loss: 908.7835693359375 = 0.9020746946334839 + 100.0 * 9.078814506530762
Epoch 1570, val loss: 0.9121888279914856
Epoch 1580, training loss: 909.1843872070312 = 0.9002402424812317 + 100.0 * 9.082840919494629
Epoch 1580, val loss: 0.9104853272438049
Epoch 1590, training loss: 909.2896118164062 = 0.8983437418937683 + 100.0 * 9.08391284942627
Epoch 1590, val loss: 0.9087249636650085
Epoch 1600, training loss: 909.4141845703125 = 0.8964136242866516 + 100.0 * 9.085177421569824
Epoch 1600, val loss: 0.9069475531578064
Epoch 1610, training loss: 909.8427734375 = 0.8945505619049072 + 100.0 * 9.089482307434082
Epoch 1610, val loss: 0.9052236080169678
Epoch 1620, training loss: 909.7963256835938 = 0.8926616311073303 + 100.0 * 9.08903694152832
Epoch 1620, val loss: 0.9034773111343384
Epoch 1630, training loss: 909.92529296875 = 0.8907359838485718 + 100.0 * 9.09034538269043
Epoch 1630, val loss: 0.901666522026062
Epoch 1640, training loss: 909.7145385742188 = 0.8887447118759155 + 100.0 * 9.088257789611816
Epoch 1640, val loss: 0.8998313546180725
Epoch 1650, training loss: 910.3076171875 = 0.8868511319160461 + 100.0 * 9.094207763671875
Epoch 1650, val loss: 0.8980822563171387
Epoch 1660, training loss: 910.0845947265625 = 0.8849080204963684 + 100.0 * 9.091997146606445
Epoch 1660, val loss: 0.8962905406951904
Epoch 1670, training loss: 910.4636840820312 = 0.8829972743988037 + 100.0 * 9.095807075500488
Epoch 1670, val loss: 0.8944849967956543
Epoch 1680, training loss: 911.1896362304688 = 0.8810781240463257 + 100.0 * 9.1030855178833
Epoch 1680, val loss: 0.8926910161972046
Epoch 1690, training loss: 910.8919067382812 = 0.879065990447998 + 100.0 * 9.100128173828125
Epoch 1690, val loss: 0.8908290266990662
Epoch 1700, training loss: 910.6847534179688 = 0.8770748972892761 + 100.0 * 9.098076820373535
Epoch 1700, val loss: 0.8889638781547546
Epoch 1710, training loss: 910.7888793945312 = 0.8750982880592346 + 100.0 * 9.099137306213379
Epoch 1710, val loss: 0.8871354460716248
Epoch 1720, training loss: 911.0177001953125 = 0.8730878233909607 + 100.0 * 9.101446151733398
Epoch 1720, val loss: 0.885289192199707
Epoch 1730, training loss: 911.77294921875 = 0.8711947798728943 + 100.0 * 9.109017372131348
Epoch 1730, val loss: 0.8834947347640991
Epoch 1740, training loss: 911.8580322265625 = 0.8692257404327393 + 100.0 * 9.109888076782227
Epoch 1740, val loss: 0.8816744089126587
Epoch 1750, training loss: 912.3715209960938 = 0.8672378659248352 + 100.0 * 9.115042686462402
Epoch 1750, val loss: 0.8798101544380188
Epoch 1760, training loss: 912.4296264648438 = 0.8652022480964661 + 100.0 * 9.115644454956055
Epoch 1760, val loss: 0.8779617547988892
Epoch 1770, training loss: 912.3616943359375 = 0.8631871938705444 + 100.0 * 9.114985466003418
Epoch 1770, val loss: 0.8760998249053955
Epoch 1780, training loss: 912.2889404296875 = 0.8611670136451721 + 100.0 * 9.114277839660645
Epoch 1780, val loss: 0.8742504715919495
Epoch 1790, training loss: 910.898193359375 = 0.8589119911193848 + 100.0 * 9.100393295288086
Epoch 1790, val loss: 0.8721449375152588
Epoch 1800, training loss: 910.8642578125 = 0.8568433523178101 + 100.0 * 9.10007381439209
Epoch 1800, val loss: 0.8702329993247986
Epoch 1810, training loss: 910.988525390625 = 0.8548051118850708 + 100.0 * 9.101337432861328
Epoch 1810, val loss: 0.8683627843856812
Epoch 1820, training loss: 912.0990600585938 = 0.8528615236282349 + 100.0 * 9.112462043762207
Epoch 1820, val loss: 0.866598904132843
Epoch 1830, training loss: 912.8425903320312 = 0.8508942723274231 + 100.0 * 9.119916915893555
Epoch 1830, val loss: 0.8648110032081604
Epoch 1840, training loss: 913.4227294921875 = 0.848999559879303 + 100.0 * 9.125737190246582
Epoch 1840, val loss: 0.8630535006523132
Epoch 1850, training loss: 913.1268310546875 = 0.8470313549041748 + 100.0 * 9.122797966003418
Epoch 1850, val loss: 0.861283540725708
Epoch 1860, training loss: 912.8966064453125 = 0.8450093865394592 + 100.0 * 9.120515823364258
Epoch 1860, val loss: 0.8593891859054565
Epoch 1870, training loss: 912.707275390625 = 0.8430753946304321 + 100.0 * 9.11864185333252
Epoch 1870, val loss: 0.8576483726501465
Epoch 1880, training loss: 913.4220581054688 = 0.8411912322044373 + 100.0 * 9.125808715820312
Epoch 1880, val loss: 0.8559158444404602
Epoch 1890, training loss: 913.7156982421875 = 0.8392949104309082 + 100.0 * 9.128764152526855
Epoch 1890, val loss: 0.8541849255561829
Epoch 1900, training loss: 913.8312377929688 = 0.8373467326164246 + 100.0 * 9.129939079284668
Epoch 1900, val loss: 0.8523834943771362
Epoch 1910, training loss: 913.2733764648438 = 0.8354423642158508 + 100.0 * 9.12437915802002
Epoch 1910, val loss: 0.8506576418876648
Epoch 1920, training loss: 913.5343017578125 = 0.8335639834403992 + 100.0 * 9.127007484436035
Epoch 1920, val loss: 0.8489363193511963
Epoch 1930, training loss: 914.12255859375 = 0.8316605091094971 + 100.0 * 9.132908821105957
Epoch 1930, val loss: 0.8472155332565308
Epoch 1940, training loss: 914.0862426757812 = 0.8297672271728516 + 100.0 * 9.132564544677734
Epoch 1940, val loss: 0.8454951643943787
Epoch 1950, training loss: 914.3978271484375 = 0.8279005885124207 + 100.0 * 9.135699272155762
Epoch 1950, val loss: 0.843774139881134
Epoch 1960, training loss: 914.5870971679688 = 0.8259807229042053 + 100.0 * 9.137611389160156
Epoch 1960, val loss: 0.842041552066803
Epoch 1970, training loss: 914.9761352539062 = 0.8241152763366699 + 100.0 * 9.141520500183105
Epoch 1970, val loss: 0.8403079509735107
Epoch 1980, training loss: 915.1985473632812 = 0.8222163915634155 + 100.0 * 9.143763542175293
Epoch 1980, val loss: 0.8386055827140808
Epoch 1990, training loss: 915.2275390625 = 0.8203363418579102 + 100.0 * 9.144072532653809
Epoch 1990, val loss: 0.8369026780128479
Epoch 2000, training loss: 915.2373657226562 = 0.8184539079666138 + 100.0 * 9.14418888092041
Epoch 2000, val loss: 0.8351687788963318
Epoch 2010, training loss: 914.4983520507812 = 0.8164947032928467 + 100.0 * 9.136818885803223
Epoch 2010, val loss: 0.8334065675735474
Epoch 2020, training loss: 914.9685668945312 = 0.8146442770957947 + 100.0 * 9.141539573669434
Epoch 2020, val loss: 0.8317451477050781
Epoch 2030, training loss: 915.3927001953125 = 0.8128146529197693 + 100.0 * 9.145798683166504
Epoch 2030, val loss: 0.8300712704658508
Epoch 2040, training loss: 916.0528564453125 = 0.8109983801841736 + 100.0 * 9.152419090270996
Epoch 2040, val loss: 0.8284215331077576
Epoch 2050, training loss: 915.1569213867188 = 0.8090527653694153 + 100.0 * 9.143478393554688
Epoch 2050, val loss: 0.8266627192497253
Epoch 2060, training loss: 914.9238891601562 = 0.8071756362915039 + 100.0 * 9.141166687011719
Epoch 2060, val loss: 0.8249474167823792
Epoch 2070, training loss: 915.6361083984375 = 0.805400013923645 + 100.0 * 9.148306846618652
Epoch 2070, val loss: 0.8233509063720703
Epoch 2080, training loss: 916.2872314453125 = 0.8035861253738403 + 100.0 * 9.154836654663086
Epoch 2080, val loss: 0.8217130303382874
Epoch 2090, training loss: 916.473388671875 = 0.8017820119857788 + 100.0 * 9.156716346740723
Epoch 2090, val loss: 0.8200715780258179
Epoch 2100, training loss: 915.5671997070312 = 0.799914538860321 + 100.0 * 9.147672653198242
Epoch 2100, val loss: 0.818372368812561
Epoch 2110, training loss: 915.8320922851562 = 0.7980793714523315 + 100.0 * 9.15034008026123
Epoch 2110, val loss: 0.8167096376419067
Epoch 2120, training loss: 915.8519897460938 = 0.796302318572998 + 100.0 * 9.150556564331055
Epoch 2120, val loss: 0.815114438533783
Epoch 2130, training loss: 916.3359375 = 0.7945429682731628 + 100.0 * 9.155413627624512
Epoch 2130, val loss: 0.8135368227958679
Epoch 2140, training loss: 916.3182983398438 = 0.7927976250648499 + 100.0 * 9.155255317687988
Epoch 2140, val loss: 0.8119460344314575
Epoch 2150, training loss: 916.412109375 = 0.7909963726997375 + 100.0 * 9.156210899353027
Epoch 2150, val loss: 0.8103480339050293
Epoch 2160, training loss: 916.16162109375 = 0.789216160774231 + 100.0 * 9.15372371673584
Epoch 2160, val loss: 0.80875164270401
Epoch 2170, training loss: 914.6820068359375 = 0.7870492935180664 + 100.0 * 9.138949394226074
Epoch 2170, val loss: 0.8068188428878784
Epoch 2180, training loss: 914.8419189453125 = 0.7853333950042725 + 100.0 * 9.140565872192383
Epoch 2180, val loss: 0.8053023219108582
Epoch 2190, training loss: 914.056884765625 = 0.7835938334465027 + 100.0 * 9.132733345031738
Epoch 2190, val loss: 0.8036960959434509
Epoch 2200, training loss: 914.3275146484375 = 0.7819376587867737 + 100.0 * 9.135456085205078
Epoch 2200, val loss: 0.8022562265396118
Epoch 2210, training loss: 915.63134765625 = 0.7804437875747681 + 100.0 * 9.14850902557373
Epoch 2210, val loss: 0.8009582161903381
Epoch 2220, training loss: 915.4841918945312 = 0.778721272945404 + 100.0 * 9.147054672241211
Epoch 2220, val loss: 0.799432098865509
Epoch 2230, training loss: 916.4937744140625 = 0.7771292328834534 + 100.0 * 9.157166481018066
Epoch 2230, val loss: 0.7979931235313416
Epoch 2240, training loss: 917.7427978515625 = 0.7755022048950195 + 100.0 * 9.169672966003418
Epoch 2240, val loss: 0.7965344786643982
Epoch 2250, training loss: 918.48828125 = 0.7738634943962097 + 100.0 * 9.177144050598145
Epoch 2250, val loss: 0.7950628399848938
Epoch 2260, training loss: 918.562744140625 = 0.7721572518348694 + 100.0 * 9.177906036376953
Epoch 2260, val loss: 0.7935546040534973
Epoch 2270, training loss: 918.88134765625 = 0.7704869508743286 + 100.0 * 9.181108474731445
Epoch 2270, val loss: 0.7920700311660767
Epoch 2280, training loss: 919.0384521484375 = 0.7688056826591492 + 100.0 * 9.182696342468262
Epoch 2280, val loss: 0.7905731201171875
Epoch 2290, training loss: 919.1414184570312 = 0.7671085596084595 + 100.0 * 9.183743476867676
Epoch 2290, val loss: 0.7890781164169312
Epoch 2300, training loss: 919.2689819335938 = 0.765426516532898 + 100.0 * 9.185035705566406
Epoch 2300, val loss: 0.7875735759735107
Epoch 2310, training loss: 919.5147705078125 = 0.7637679576873779 + 100.0 * 9.187509536743164
Epoch 2310, val loss: 0.7861239314079285
Epoch 2320, training loss: 919.1893310546875 = 0.7620904445648193 + 100.0 * 9.184272766113281
Epoch 2320, val loss: 0.7846519351005554
Epoch 2330, training loss: 919.6605834960938 = 0.7604745626449585 + 100.0 * 9.189001083374023
Epoch 2330, val loss: 0.7832221984863281
Epoch 2340, training loss: 919.9501342773438 = 0.7588322758674622 + 100.0 * 9.191912651062012
Epoch 2340, val loss: 0.7817838191986084
Epoch 2350, training loss: 919.7725219726562 = 0.7571784257888794 + 100.0 * 9.190153121948242
Epoch 2350, val loss: 0.7803033590316772
Epoch 2360, training loss: 919.7059326171875 = 0.7555323243141174 + 100.0 * 9.18950366973877
Epoch 2360, val loss: 0.7788803577423096
Epoch 2370, training loss: 919.8248291015625 = 0.753920316696167 + 100.0 * 9.190709114074707
Epoch 2370, val loss: 0.7774606943130493
Epoch 2380, training loss: 920.1997680664062 = 0.7523147463798523 + 100.0 * 9.194474220275879
Epoch 2380, val loss: 0.7760565280914307
Epoch 2390, training loss: 920.5501708984375 = 0.750715434551239 + 100.0 * 9.197994232177734
Epoch 2390, val loss: 0.7746506333351135
Epoch 2400, training loss: 920.13232421875 = 0.7490812540054321 + 100.0 * 9.193832397460938
Epoch 2400, val loss: 0.7732356190681458
Epoch 2410, training loss: 920.1846313476562 = 0.7475056648254395 + 100.0 * 9.194371223449707
Epoch 2410, val loss: 0.7718303203582764
Epoch 2420, training loss: 920.4835205078125 = 0.7459213137626648 + 100.0 * 9.197376251220703
Epoch 2420, val loss: 0.7704623341560364
Epoch 2430, training loss: 920.8587036132812 = 0.7443590760231018 + 100.0 * 9.201143264770508
Epoch 2430, val loss: 0.769084095954895
Epoch 2440, training loss: 921.0220947265625 = 0.742790162563324 + 100.0 * 9.20279312133789
Epoch 2440, val loss: 0.7677225470542908
Epoch 2450, training loss: 921.0389404296875 = 0.7412210702896118 + 100.0 * 9.202977180480957
Epoch 2450, val loss: 0.7663534283638
Epoch 2460, training loss: 921.0589599609375 = 0.7396570444107056 + 100.0 * 9.203192710876465
Epoch 2460, val loss: 0.7649794816970825
Epoch 2470, training loss: 921.405517578125 = 0.7381330132484436 + 100.0 * 9.206673622131348
Epoch 2470, val loss: 0.7636507153511047
Epoch 2480, training loss: 921.5064697265625 = 0.7365810871124268 + 100.0 * 9.207698822021484
Epoch 2480, val loss: 0.7623192667961121
Epoch 2490, training loss: 920.2301635742188 = 0.7350094318389893 + 100.0 * 9.194952011108398
Epoch 2490, val loss: 0.7609626054763794
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7423188405797101
0.8169238571325075
=== training gcn model ===
Epoch 0, training loss: 1033.0091552734375 = 1.092306137084961 + 100.0 * 10.319169044494629
Epoch 0, val loss: 1.0931140184402466
Epoch 10, training loss: 993.240966796875 = 1.0921568870544434 + 100.0 * 9.921487808227539
Epoch 10, val loss: 1.093000888824463
Epoch 20, training loss: 972.9977416992188 = 1.092066764831543 + 100.0 * 9.719057083129883
Epoch 20, val loss: 1.0929374694824219
Epoch 30, training loss: 958.2913818359375 = 1.0919338464736938 + 100.0 * 9.57199478149414
Epoch 30, val loss: 1.092824935913086
Epoch 40, training loss: 946.956298828125 = 1.0917949676513672 + 100.0 * 9.45864486694336
Epoch 40, val loss: 1.0927079916000366
Epoch 50, training loss: 938.087646484375 = 1.0916520357131958 + 100.0 * 9.369959831237793
Epoch 50, val loss: 1.0925894975662231
Epoch 60, training loss: 930.7096557617188 = 1.091504454612732 + 100.0 * 9.296181678771973
Epoch 60, val loss: 1.0924644470214844
Epoch 70, training loss: 924.4730834960938 = 1.091355323791504 + 100.0 * 9.233817100524902
Epoch 70, val loss: 1.0923359394073486
Epoch 80, training loss: 919.0108032226562 = 1.0912097692489624 + 100.0 * 9.17919635772705
Epoch 80, val loss: 1.0922107696533203
Epoch 90, training loss: 914.1583251953125 = 1.0910512208938599 + 100.0 * 9.130672454833984
Epoch 90, val loss: 1.0920734405517578
Epoch 100, training loss: 910.054931640625 = 1.0908912420272827 + 100.0 * 9.089640617370605
Epoch 100, val loss: 1.091933250427246
Epoch 110, training loss: 906.2852783203125 = 1.0907236337661743 + 100.0 * 9.051945686340332
Epoch 110, val loss: 1.0917845964431763
Epoch 120, training loss: 903.1026611328125 = 1.0905494689941406 + 100.0 * 9.020120620727539
Epoch 120, val loss: 1.0916318893432617
Epoch 130, training loss: 900.463623046875 = 1.0903693437576294 + 100.0 * 8.993732452392578
Epoch 130, val loss: 1.0914723873138428
Epoch 140, training loss: 898.0867309570312 = 1.0901741981506348 + 100.0 * 8.969965934753418
Epoch 140, val loss: 1.091304898262024
Epoch 150, training loss: 895.9341430664062 = 1.0899780988693237 + 100.0 * 8.948441505432129
Epoch 150, val loss: 1.0911240577697754
Epoch 160, training loss: 894.34912109375 = 1.089774489402771 + 100.0 * 8.93259334564209
Epoch 160, val loss: 1.0909483432769775
Epoch 170, training loss: 892.4843139648438 = 1.0895487070083618 + 100.0 * 8.913948059082031
Epoch 170, val loss: 1.0907500982284546
Epoch 180, training loss: 891.2188720703125 = 1.089316487312317 + 100.0 * 8.90129566192627
Epoch 180, val loss: 1.090545892715454
Epoch 190, training loss: 890.0921630859375 = 1.0890806913375854 + 100.0 * 8.890030860900879
Epoch 190, val loss: 1.0903401374816895
Epoch 200, training loss: 889.1016845703125 = 1.0888315439224243 + 100.0 * 8.880128860473633
Epoch 200, val loss: 1.0901235342025757
Epoch 210, training loss: 888.5132446289062 = 1.0885804891586304 + 100.0 * 8.874246597290039
Epoch 210, val loss: 1.0899041891098022
Epoch 220, training loss: 887.4619140625 = 1.0883164405822754 + 100.0 * 8.863736152648926
Epoch 220, val loss: 1.0896706581115723
Epoch 230, training loss: 886.7728881835938 = 1.0880379676818848 + 100.0 * 8.85684871673584
Epoch 230, val loss: 1.08942711353302
Epoch 240, training loss: 886.4593505859375 = 1.0877653360366821 + 100.0 * 8.853715896606445
Epoch 240, val loss: 1.0891873836517334
Epoch 250, training loss: 885.63671875 = 1.0874704122543335 + 100.0 * 8.845492362976074
Epoch 250, val loss: 1.0889254808425903
Epoch 260, training loss: 885.3375854492188 = 1.0871785879135132 + 100.0 * 8.842504501342773
Epoch 260, val loss: 1.088672399520874
Epoch 270, training loss: 884.44970703125 = 1.0868642330169678 + 100.0 * 8.83362865447998
Epoch 270, val loss: 1.0883913040161133
Epoch 280, training loss: 885.3138427734375 = 1.0865700244903564 + 100.0 * 8.842272758483887
Epoch 280, val loss: 1.0881351232528687
Epoch 290, training loss: 884.8486938476562 = 1.0862475633621216 + 100.0 * 8.837624549865723
Epoch 290, val loss: 1.087852120399475
Epoch 300, training loss: 884.6431884765625 = 1.0859174728393555 + 100.0 * 8.835572242736816
Epoch 300, val loss: 1.0875660181045532
Epoch 310, training loss: 883.9996337890625 = 1.0855919122695923 + 100.0 * 8.829140663146973
Epoch 310, val loss: 1.0872828960418701
Epoch 320, training loss: 884.0582885742188 = 1.0852566957473755 + 100.0 * 8.829730033874512
Epoch 320, val loss: 1.0869879722595215
Epoch 330, training loss: 883.7578735351562 = 1.0849111080169678 + 100.0 * 8.826729774475098
Epoch 330, val loss: 1.0866851806640625
Epoch 340, training loss: 883.8250122070312 = 1.0845595598220825 + 100.0 * 8.827404975891113
Epoch 340, val loss: 1.0863779783248901
Epoch 350, training loss: 884.0135498046875 = 1.0842108726501465 + 100.0 * 8.829293251037598
Epoch 350, val loss: 1.0860730409622192
Epoch 360, training loss: 883.76416015625 = 1.0838403701782227 + 100.0 * 8.826803207397461
Epoch 360, val loss: 1.085748314857483
Epoch 370, training loss: 883.9769897460938 = 1.0834686756134033 + 100.0 * 8.828934669494629
Epoch 370, val loss: 1.0854203701019287
Epoch 380, training loss: 883.9165649414062 = 1.0830916166305542 + 100.0 * 8.82833480834961
Epoch 380, val loss: 1.0850900411605835
Epoch 390, training loss: 883.7928466796875 = 1.0827085971832275 + 100.0 * 8.827101707458496
Epoch 390, val loss: 1.0847575664520264
Epoch 400, training loss: 883.9241333007812 = 1.0823228359222412 + 100.0 * 8.828417778015137
Epoch 400, val loss: 1.0844178199768066
Epoch 410, training loss: 883.89794921875 = 1.0819276571273804 + 100.0 * 8.828160285949707
Epoch 410, val loss: 1.0840682983398438
Epoch 420, training loss: 884.0176391601562 = 1.0815362930297852 + 100.0 * 8.829360961914062
Epoch 420, val loss: 1.083723545074463
Epoch 430, training loss: 884.0115966796875 = 1.0811322927474976 + 100.0 * 8.829304695129395
Epoch 430, val loss: 1.0833719968795776
Epoch 440, training loss: 883.87646484375 = 1.0807229280471802 + 100.0 * 8.827957153320312
Epoch 440, val loss: 1.0830106735229492
Epoch 450, training loss: 883.7316284179688 = 1.0803050994873047 + 100.0 * 8.826513290405273
Epoch 450, val loss: 1.0826537609100342
Epoch 460, training loss: 883.9590454101562 = 1.079881191253662 + 100.0 * 8.828791618347168
Epoch 460, val loss: 1.0822793245315552
Epoch 470, training loss: 884.2586669921875 = 1.0794553756713867 + 100.0 * 8.831791877746582
Epoch 470, val loss: 1.0819017887115479
Epoch 480, training loss: 884.9138793945312 = 1.079016089439392 + 100.0 * 8.838348388671875
Epoch 480, val loss: 1.0815167427062988
Epoch 490, training loss: 884.193359375 = 1.0785785913467407 + 100.0 * 8.831148147583008
Epoch 490, val loss: 1.081140398979187
Epoch 500, training loss: 883.9697875976562 = 1.0781391859054565 + 100.0 * 8.828916549682617
Epoch 500, val loss: 1.080753207206726
Epoch 510, training loss: 884.2628173828125 = 1.0777045488357544 + 100.0 * 8.8318510055542
Epoch 510, val loss: 1.0803765058517456
Epoch 520, training loss: 884.687255859375 = 1.0772634744644165 + 100.0 * 8.836099624633789
Epoch 520, val loss: 1.0799922943115234
Epoch 530, training loss: 884.9856567382812 = 1.0768194198608398 + 100.0 * 8.839088439941406
Epoch 530, val loss: 1.0795990228652954
Epoch 540, training loss: 884.9313354492188 = 1.0763529539108276 + 100.0 * 8.838549613952637
Epoch 540, val loss: 1.0791977643966675
Epoch 550, training loss: 884.5263671875 = 1.0758877992630005 + 100.0 * 8.834505081176758
Epoch 550, val loss: 1.078782320022583
Epoch 560, training loss: 885.0023803710938 = 1.0752285718917847 + 100.0 * 8.839271545410156
Epoch 560, val loss: 1.0782015323638916
Epoch 570, training loss: 885.1090698242188 = 1.0746796131134033 + 100.0 * 8.840343475341797
Epoch 570, val loss: 1.0777090787887573
Epoch 580, training loss: 885.9425048828125 = 1.0741204023361206 + 100.0 * 8.848684310913086
Epoch 580, val loss: 1.077209234237671
Epoch 590, training loss: 886.0465087890625 = 1.0735530853271484 + 100.0 * 8.849729537963867
Epoch 590, val loss: 1.0767050981521606
Epoch 600, training loss: 886.291259765625 = 1.0729906558990479 + 100.0 * 8.852182388305664
Epoch 600, val loss: 1.0762100219726562
Epoch 610, training loss: 886.1223754882812 = 1.072424054145813 + 100.0 * 8.850499153137207
Epoch 610, val loss: 1.075710415840149
Epoch 620, training loss: 885.9375 = 1.0718506574630737 + 100.0 * 8.84865665435791
Epoch 620, val loss: 1.075205683708191
Epoch 630, training loss: 885.9127197265625 = 1.0712746381759644 + 100.0 * 8.848414421081543
Epoch 630, val loss: 1.074708342552185
Epoch 640, training loss: 886.1685180664062 = 1.070725679397583 + 100.0 * 8.850977897644043
Epoch 640, val loss: 1.0742231607437134
Epoch 650, training loss: 886.505859375 = 1.070178508758545 + 100.0 * 8.85435676574707
Epoch 650, val loss: 1.0737357139587402
Epoch 660, training loss: 887.266357421875 = 1.0696192979812622 + 100.0 * 8.861967086791992
Epoch 660, val loss: 1.0732346773147583
Epoch 670, training loss: 887.4934692382812 = 1.0690398216247559 + 100.0 * 8.86424446105957
Epoch 670, val loss: 1.0727368593215942
Epoch 680, training loss: 887.2517700195312 = 1.068453311920166 + 100.0 * 8.861832618713379
Epoch 680, val loss: 1.0722072124481201
Epoch 690, training loss: 887.838134765625 = 1.0678783655166626 + 100.0 * 8.86770248413086
Epoch 690, val loss: 1.0717029571533203
Epoch 700, training loss: 887.7166748046875 = 1.0672798156738281 + 100.0 * 8.866494178771973
Epoch 700, val loss: 1.0711742639541626
Epoch 710, training loss: 887.336669921875 = 1.0666583776474 + 100.0 * 8.862700462341309
Epoch 710, val loss: 1.070626974105835
Epoch 720, training loss: 887.8778686523438 = 1.0660632848739624 + 100.0 * 8.868118286132812
Epoch 720, val loss: 1.0700960159301758
Epoch 730, training loss: 887.74951171875 = 1.065395474433899 + 100.0 * 8.866841316223145
Epoch 730, val loss: 1.0695037841796875
Epoch 740, training loss: 888.6458129882812 = 1.0648330450057983 + 100.0 * 8.875809669494629
Epoch 740, val loss: 1.0689913034439087
Epoch 750, training loss: 888.864990234375 = 1.064192771911621 + 100.0 * 8.878007888793945
Epoch 750, val loss: 1.0684348344802856
Epoch 760, training loss: 888.9540405273438 = 1.063539743423462 + 100.0 * 8.878905296325684
Epoch 760, val loss: 1.0678660869598389
Epoch 770, training loss: 888.9619750976562 = 1.0628811120986938 + 100.0 * 8.87899112701416
Epoch 770, val loss: 1.0672948360443115
Epoch 780, training loss: 889.1846313476562 = 1.0622453689575195 + 100.0 * 8.881223678588867
Epoch 780, val loss: 1.066729187965393
Epoch 790, training loss: 889.2473754882812 = 1.0615757703781128 + 100.0 * 8.881857872009277
Epoch 790, val loss: 1.0661451816558838
Epoch 800, training loss: 889.8053588867188 = 1.0609397888183594 + 100.0 * 8.887444496154785
Epoch 800, val loss: 1.0655767917633057
Epoch 810, training loss: 890.169921875 = 1.0602827072143555 + 100.0 * 8.891096115112305
Epoch 810, val loss: 1.0649980306625366
Epoch 820, training loss: 890.184326171875 = 1.0596264600753784 + 100.0 * 8.891246795654297
Epoch 820, val loss: 1.064414143562317
Epoch 830, training loss: 890.7177734375 = 1.0589677095413208 + 100.0 * 8.896588325500488
Epoch 830, val loss: 1.0638325214385986
Epoch 840, training loss: 889.2838134765625 = 1.0582399368286133 + 100.0 * 8.882255554199219
Epoch 840, val loss: 1.0632036924362183
Epoch 850, training loss: 889.9693603515625 = 1.0575988292694092 + 100.0 * 8.889117240905762
Epoch 850, val loss: 1.062638282775879
Epoch 860, training loss: 890.965087890625 = 1.0569406747817993 + 100.0 * 8.899081230163574
Epoch 860, val loss: 1.062042236328125
Epoch 870, training loss: 891.351318359375 = 1.0562670230865479 + 100.0 * 8.902950286865234
Epoch 870, val loss: 1.0614551305770874
Epoch 880, training loss: 891.1719970703125 = 1.055593490600586 + 100.0 * 8.901164054870605
Epoch 880, val loss: 1.0608502626419067
Epoch 890, training loss: 891.4574584960938 = 1.0549116134643555 + 100.0 * 8.904025077819824
Epoch 890, val loss: 1.060243010520935
Epoch 900, training loss: 891.7224731445312 = 1.0542302131652832 + 100.0 * 8.906682014465332
Epoch 900, val loss: 1.059651494026184
Epoch 910, training loss: 892.5208740234375 = 1.053585410118103 + 100.0 * 8.9146728515625
Epoch 910, val loss: 1.0590579509735107
Epoch 920, training loss: 892.390625 = 1.0528733730316162 + 100.0 * 8.91337776184082
Epoch 920, val loss: 1.0584520101547241
Epoch 930, training loss: 891.6034545898438 = 1.0521605014801025 + 100.0 * 8.905512809753418
Epoch 930, val loss: 1.0578234195709229
Epoch 940, training loss: 891.1826171875 = 1.051477313041687 + 100.0 * 8.901311874389648
Epoch 940, val loss: 1.0572130680084229
Epoch 950, training loss: 892.3270874023438 = 1.050870418548584 + 100.0 * 8.912762641906738
Epoch 950, val loss: 1.056687831878662
Epoch 960, training loss: 892.0423583984375 = 1.050108790397644 + 100.0 * 8.90992259979248
Epoch 960, val loss: 1.0560119152069092
Epoch 970, training loss: 892.2109985351562 = 1.0494123697280884 + 100.0 * 8.911615371704102
Epoch 970, val loss: 1.0554008483886719
Epoch 980, training loss: 892.6170654296875 = 1.0487213134765625 + 100.0 * 8.91568374633789
Epoch 980, val loss: 1.0547938346862793
Epoch 990, training loss: 892.1343383789062 = 1.0479462146759033 + 100.0 * 8.910863876342773
Epoch 990, val loss: 1.054135799407959
Epoch 1000, training loss: 891.588623046875 = 1.0472664833068848 + 100.0 * 8.905413627624512
Epoch 1000, val loss: 1.0534950494766235
Epoch 1010, training loss: 892.0260620117188 = 1.0465699434280396 + 100.0 * 8.909794807434082
Epoch 1010, val loss: 1.052887201309204
Epoch 1020, training loss: 892.6497802734375 = 1.045872449874878 + 100.0 * 8.916038513183594
Epoch 1020, val loss: 1.052276611328125
Epoch 1030, training loss: 893.4478149414062 = 1.0451765060424805 + 100.0 * 8.924026489257812
Epoch 1030, val loss: 1.051662802696228
Epoch 1040, training loss: 893.8373413085938 = 1.0444598197937012 + 100.0 * 8.927928924560547
Epoch 1040, val loss: 1.0510306358337402
Epoch 1050, training loss: 894.3300170898438 = 1.0437389612197876 + 100.0 * 8.932862281799316
Epoch 1050, val loss: 1.0503913164138794
Epoch 1060, training loss: 894.3681640625 = 1.0430020093917847 + 100.0 * 8.93325138092041
Epoch 1060, val loss: 1.0497474670410156
Epoch 1070, training loss: 894.9182739257812 = 1.042270541191101 + 100.0 * 8.938759803771973
Epoch 1070, val loss: 1.0491098165512085
Epoch 1080, training loss: 894.67626953125 = 1.0415363311767578 + 100.0 * 8.936347007751465
Epoch 1080, val loss: 1.0484482049942017
Epoch 1090, training loss: 894.6897583007812 = 1.0407835245132446 + 100.0 * 8.936490058898926
Epoch 1090, val loss: 1.0477805137634277
Epoch 1100, training loss: 895.2684326171875 = 1.0400409698486328 + 100.0 * 8.942283630371094
Epoch 1100, val loss: 1.0471248626708984
Epoch 1110, training loss: 892.3056640625 = 1.039258360862732 + 100.0 * 8.912664413452148
Epoch 1110, val loss: 1.0464668273925781
Epoch 1120, training loss: 893.5278930664062 = 1.038414478302002 + 100.0 * 8.924895286560059
Epoch 1120, val loss: 1.045719861984253
Epoch 1130, training loss: 893.629150390625 = 1.0376859903335571 + 100.0 * 8.925914764404297
Epoch 1130, val loss: 1.0450453758239746
Epoch 1140, training loss: 894.3411865234375 = 1.03694748878479 + 100.0 * 8.933042526245117
Epoch 1140, val loss: 1.044402003288269
Epoch 1150, training loss: 894.11474609375 = 1.0361517667770386 + 100.0 * 8.9307861328125
Epoch 1150, val loss: 1.043710708618164
Epoch 1160, training loss: 895.1417846679688 = 1.035372257232666 + 100.0 * 8.94106388092041
Epoch 1160, val loss: 1.0430376529693604
Epoch 1170, training loss: 895.4686279296875 = 1.0345715284347534 + 100.0 * 8.944340705871582
Epoch 1170, val loss: 1.0423259735107422
Epoch 1180, training loss: 896.2178344726562 = 1.0337872505187988 + 100.0 * 8.9518404006958
Epoch 1180, val loss: 1.0416220426559448
Epoch 1190, training loss: 896.2718505859375 = 1.0329591035842896 + 100.0 * 8.952388763427734
Epoch 1190, val loss: 1.0409139394760132
Epoch 1200, training loss: 896.3524780273438 = 1.0321648120880127 + 100.0 * 8.953203201293945
Epoch 1200, val loss: 1.040219783782959
Epoch 1210, training loss: 897.2315673828125 = 1.0313279628753662 + 100.0 * 8.962002754211426
Epoch 1210, val loss: 1.0394859313964844
Epoch 1220, training loss: 897.8660888671875 = 1.0305588245391846 + 100.0 * 8.968355178833008
Epoch 1220, val loss: 1.0387930870056152
Epoch 1230, training loss: 897.6845092773438 = 1.0297577381134033 + 100.0 * 8.966547012329102
Epoch 1230, val loss: 1.038103699684143
Epoch 1240, training loss: 897.093994140625 = 1.0289459228515625 + 100.0 * 8.960650444030762
Epoch 1240, val loss: 1.0373936891555786
Epoch 1250, training loss: 897.608154296875 = 1.028145432472229 + 100.0 * 8.965800285339355
Epoch 1250, val loss: 1.0366936922073364
Epoch 1260, training loss: 897.4925537109375 = 1.0273363590240479 + 100.0 * 8.964652061462402
Epoch 1260, val loss: 1.0359845161437988
Epoch 1270, training loss: 898.2548828125 = 1.0265312194824219 + 100.0 * 8.972283363342285
Epoch 1270, val loss: 1.0352764129638672
Epoch 1280, training loss: 898.5924072265625 = 1.0256961584091187 + 100.0 * 8.975666999816895
Epoch 1280, val loss: 1.0345513820648193
Epoch 1290, training loss: 898.7750244140625 = 1.0248966217041016 + 100.0 * 8.977500915527344
Epoch 1290, val loss: 1.033846378326416
Epoch 1300, training loss: 898.7730102539062 = 1.0240652561187744 + 100.0 * 8.977489471435547
Epoch 1300, val loss: 1.033124566078186
Epoch 1310, training loss: 899.0026245117188 = 1.0232501029968262 + 100.0 * 8.979793548583984
Epoch 1310, val loss: 1.0323994159698486
Epoch 1320, training loss: 898.7496948242188 = 1.0224062204360962 + 100.0 * 8.977272987365723
Epoch 1320, val loss: 1.031672716140747
Epoch 1330, training loss: 899.2400512695312 = 1.0215911865234375 + 100.0 * 8.982184410095215
Epoch 1330, val loss: 1.0309640169143677
Epoch 1340, training loss: 899.7244873046875 = 1.0207490921020508 + 100.0 * 8.987037658691406
Epoch 1340, val loss: 1.0302295684814453
Epoch 1350, training loss: 899.6528930664062 = 1.019923448562622 + 100.0 * 8.986330032348633
Epoch 1350, val loss: 1.0295056104660034
Epoch 1360, training loss: 900.4119262695312 = 1.0190978050231934 + 100.0 * 8.993927955627441
Epoch 1360, val loss: 1.0287762880325317
Epoch 1370, training loss: 900.0533447265625 = 1.018235206604004 + 100.0 * 8.990350723266602
Epoch 1370, val loss: 1.028028130531311
Epoch 1380, training loss: 900.3241577148438 = 1.0173910856246948 + 100.0 * 8.993067741394043
Epoch 1380, val loss: 1.0272928476333618
Epoch 1390, training loss: 900.7763671875 = 1.016599416732788 + 100.0 * 8.997597694396973
Epoch 1390, val loss: 1.0266053676605225
Epoch 1400, training loss: 901.1639404296875 = 1.0157674551010132 + 100.0 * 9.001482009887695
Epoch 1400, val loss: 1.0258735418319702
Epoch 1410, training loss: 901.4319458007812 = 1.0149154663085938 + 100.0 * 9.004170417785645
Epoch 1410, val loss: 1.0251456499099731
Epoch 1420, training loss: 901.1306762695312 = 1.0140628814697266 + 100.0 * 9.001166343688965
Epoch 1420, val loss: 1.0243810415267944
Epoch 1430, training loss: 900.472900390625 = 1.0131899118423462 + 100.0 * 8.994597434997559
Epoch 1430, val loss: 1.0235989093780518
Epoch 1440, training loss: 900.9479370117188 = 1.0123239755630493 + 100.0 * 8.999356269836426
Epoch 1440, val loss: 1.0228817462921143
Epoch 1450, training loss: 901.0322875976562 = 1.0114516019821167 + 100.0 * 9.000207901000977
Epoch 1450, val loss: 1.0221023559570312
Epoch 1460, training loss: 901.10009765625 = 1.010574460029602 + 100.0 * 9.000895500183105
Epoch 1460, val loss: 1.0213422775268555
Epoch 1470, training loss: 901.363037109375 = 1.0096914768218994 + 100.0 * 9.003533363342285
Epoch 1470, val loss: 1.0205869674682617
Epoch 1480, training loss: 901.6918334960938 = 1.0088025331497192 + 100.0 * 9.006830215454102
Epoch 1480, val loss: 1.019813895225525
Epoch 1490, training loss: 902.3322143554688 = 1.0079036951065063 + 100.0 * 9.013243675231934
Epoch 1490, val loss: 1.019037127494812
Epoch 1500, training loss: 902.9400024414062 = 1.0070183277130127 + 100.0 * 9.019330024719238
Epoch 1500, val loss: 1.0182547569274902
Epoch 1510, training loss: 902.8157958984375 = 1.0061062574386597 + 100.0 * 9.018096923828125
Epoch 1510, val loss: 1.017465353012085
Epoch 1520, training loss: 903.0011596679688 = 1.0051894187927246 + 100.0 * 9.019959449768066
Epoch 1520, val loss: 1.0166740417480469
Epoch 1530, training loss: 902.4021606445312 = 1.0043270587921143 + 100.0 * 9.013978004455566
Epoch 1530, val loss: 1.0159428119659424
Epoch 1540, training loss: 903.6845703125 = 1.0034637451171875 + 100.0 * 9.026810646057129
Epoch 1540, val loss: 1.0151559114456177
Epoch 1550, training loss: 902.0353393554688 = 1.0024168491363525 + 100.0 * 9.010329246520996
Epoch 1550, val loss: 1.0142829418182373
Epoch 1560, training loss: 902.6321411132812 = 1.001528263092041 + 100.0 * 9.016305923461914
Epoch 1560, val loss: 1.0135058164596558
Epoch 1570, training loss: 903.2659912109375 = 1.0006077289581299 + 100.0 * 9.022653579711914
Epoch 1570, val loss: 1.0127030611038208
Epoch 1580, training loss: 903.2327270507812 = 0.999671995639801 + 100.0 * 9.022330284118652
Epoch 1580, val loss: 1.0118964910507202
Epoch 1590, training loss: 903.6311645507812 = 0.9987357258796692 + 100.0 * 9.026324272155762
Epoch 1590, val loss: 1.0110818147659302
Epoch 1600, training loss: 904.184326171875 = 0.9978033304214478 + 100.0 * 9.031865119934082
Epoch 1600, val loss: 1.0102794170379639
Epoch 1610, training loss: 904.3598022460938 = 0.9968725442886353 + 100.0 * 9.033629417419434
Epoch 1610, val loss: 1.0094646215438843
Epoch 1620, training loss: 904.283203125 = 0.9959148168563843 + 100.0 * 9.032873153686523
Epoch 1620, val loss: 1.0086370706558228
Epoch 1630, training loss: 904.6993408203125 = 0.9948947429656982 + 100.0 * 9.037044525146484
Epoch 1630, val loss: 1.0077215433120728
Epoch 1640, training loss: 903.6380615234375 = 0.9939210414886475 + 100.0 * 9.02644157409668
Epoch 1640, val loss: 1.0068645477294922
Epoch 1650, training loss: 902.1549682617188 = 0.9929309487342834 + 100.0 * 9.01162052154541
Epoch 1650, val loss: 1.0060547590255737
Epoch 1660, training loss: 902.6309204101562 = 0.9920133352279663 + 100.0 * 9.016388893127441
Epoch 1660, val loss: 1.00528085231781
Epoch 1670, training loss: 902.7957153320312 = 0.9910928606987 + 100.0 * 9.018046379089355
Epoch 1670, val loss: 1.0044447183609009
Epoch 1680, training loss: 903.1839599609375 = 0.9901016354560852 + 100.0 * 9.02193832397461
Epoch 1680, val loss: 1.0036054849624634
Epoch 1690, training loss: 904.1023559570312 = 0.9891759753227234 + 100.0 * 9.031131744384766
Epoch 1690, val loss: 1.002793788909912
Epoch 1700, training loss: 904.65673828125 = 0.9881994128227234 + 100.0 * 9.0366849899292
Epoch 1700, val loss: 1.0019513368606567
Epoch 1710, training loss: 905.0003662109375 = 0.9872095584869385 + 100.0 * 9.040131568908691
Epoch 1710, val loss: 1.0010888576507568
Epoch 1720, training loss: 905.191162109375 = 0.9862004518508911 + 100.0 * 9.042049407958984
Epoch 1720, val loss: 1.00020170211792
Epoch 1730, training loss: 904.8971557617188 = 0.9851794838905334 + 100.0 * 9.039119720458984
Epoch 1730, val loss: 0.9993091225624084
Epoch 1740, training loss: 905.0906372070312 = 0.984169602394104 + 100.0 * 9.041064262390137
Epoch 1740, val loss: 0.9984304904937744
Epoch 1750, training loss: 905.475341796875 = 0.9831456542015076 + 100.0 * 9.044921875
Epoch 1750, val loss: 0.9975314736366272
Epoch 1760, training loss: 905.1195068359375 = 0.9820111393928528 + 100.0 * 9.041375160217285
Epoch 1760, val loss: 0.9965437650680542
Epoch 1770, training loss: 905.5673828125 = 0.9809231162071228 + 100.0 * 9.045865058898926
Epoch 1770, val loss: 0.995576798915863
Epoch 1780, training loss: 905.3577270507812 = 0.9797928333282471 + 100.0 * 9.043779373168945
Epoch 1780, val loss: 0.9945880174636841
Epoch 1790, training loss: 905.8324584960938 = 0.978640615940094 + 100.0 * 9.048538208007812
Epoch 1790, val loss: 0.9935752153396606
Epoch 1800, training loss: 906.2848510742188 = 0.9774524569511414 + 100.0 * 9.05307388305664
Epoch 1800, val loss: 0.9925225973129272
Epoch 1810, training loss: 906.6114501953125 = 0.9762088060379028 + 100.0 * 9.056352615356445
Epoch 1810, val loss: 0.9914320111274719
Epoch 1820, training loss: 906.7369384765625 = 0.9749026894569397 + 100.0 * 9.05762004852295
Epoch 1820, val loss: 0.9902744293212891
Epoch 1830, training loss: 906.1490478515625 = 0.9735483527183533 + 100.0 * 9.05175495147705
Epoch 1830, val loss: 0.989109992980957
Epoch 1840, training loss: 906.9879150390625 = 0.9721465110778809 + 100.0 * 9.060157775878906
Epoch 1840, val loss: 0.9879062175750732
Epoch 1850, training loss: 907.0341796875 = 0.9707077145576477 + 100.0 * 9.06063461303711
Epoch 1850, val loss: 0.9866778254508972
Epoch 1860, training loss: 907.3094482421875 = 0.96932053565979 + 100.0 * 9.063401222229004
Epoch 1860, val loss: 0.9854854345321655
Epoch 1870, training loss: 907.1505126953125 = 0.9679542183876038 + 100.0 * 9.0618257522583
Epoch 1870, val loss: 0.9843102693557739
Epoch 1880, training loss: 906.6886596679688 = 0.9666131734848022 + 100.0 * 9.057220458984375
Epoch 1880, val loss: 0.9831444621086121
Epoch 1890, training loss: 906.1998291015625 = 0.9653347134590149 + 100.0 * 9.052345275878906
Epoch 1890, val loss: 0.9820504188537598
Epoch 1900, training loss: 906.6627807617188 = 0.9641379117965698 + 100.0 * 9.056986808776855
Epoch 1900, val loss: 0.9810158610343933
Epoch 1910, training loss: 907.316162109375 = 0.9629492163658142 + 100.0 * 9.063531875610352
Epoch 1910, val loss: 0.9799694418907166
Epoch 1920, training loss: 907.751220703125 = 0.9617699384689331 + 100.0 * 9.067893981933594
Epoch 1920, val loss: 0.9789596199989319
Epoch 1930, training loss: 907.693603515625 = 0.9605907201766968 + 100.0 * 9.067330360412598
Epoch 1930, val loss: 0.977935791015625
Epoch 1940, training loss: 907.8235473632812 = 0.9594225883483887 + 100.0 * 9.068641662597656
Epoch 1940, val loss: 0.9769396185874939
Epoch 1950, training loss: 908.2713012695312 = 0.958280622959137 + 100.0 * 9.07313060760498
Epoch 1950, val loss: 0.9759513139724731
Epoch 1960, training loss: 908.1970825195312 = 0.9571276903152466 + 100.0 * 9.072399139404297
Epoch 1960, val loss: 0.9749457836151123
Epoch 1970, training loss: 908.0088500976562 = 0.9559541344642639 + 100.0 * 9.070528984069824
Epoch 1970, val loss: 0.9739424586296082
Epoch 1980, training loss: 908.4153442382812 = 0.9548247456550598 + 100.0 * 9.074604988098145
Epoch 1980, val loss: 0.9729528427124023
Epoch 1990, training loss: 908.5869750976562 = 0.9536664485931396 + 100.0 * 9.076333045959473
Epoch 1990, val loss: 0.9719456434249878
Epoch 2000, training loss: 908.8091430664062 = 0.9525209665298462 + 100.0 * 9.078566551208496
Epoch 2000, val loss: 0.9709624648094177
Epoch 2010, training loss: 909.0843505859375 = 0.9514055848121643 + 100.0 * 9.081329345703125
Epoch 2010, val loss: 0.9699895977973938
Epoch 2020, training loss: 909.4879150390625 = 0.950289785861969 + 100.0 * 9.085375785827637
Epoch 2020, val loss: 0.9690119028091431
Epoch 2030, training loss: 909.4492797851562 = 0.9491486549377441 + 100.0 * 9.085000991821289
Epoch 2030, val loss: 0.9680411219596863
Epoch 2040, training loss: 909.3713989257812 = 0.9480255246162415 + 100.0 * 9.084234237670898
Epoch 2040, val loss: 0.9670512080192566
Epoch 2050, training loss: 909.4741821289062 = 0.9468949437141418 + 100.0 * 9.085272789001465
Epoch 2050, val loss: 0.9660710096359253
Epoch 2060, training loss: 897.7506713867188 = 0.9452075362205505 + 100.0 * 8.96805477142334
Epoch 2060, val loss: 0.9645596742630005
Epoch 2070, training loss: 892.503173828125 = 0.9440439939498901 + 100.0 * 8.9155912399292
Epoch 2070, val loss: 0.9635609984397888
Epoch 2080, training loss: 904.095947265625 = 0.9438492059707642 + 100.0 * 9.03152084350586
Epoch 2080, val loss: 0.9632836580276489
Epoch 2090, training loss: 898.7694702148438 = 0.9428207874298096 + 100.0 * 8.978266716003418
Epoch 2090, val loss: 0.9623816013336182
Epoch 2100, training loss: 902.0455322265625 = 0.9418396353721619 + 100.0 * 9.01103687286377
Epoch 2100, val loss: 0.9616026878356934
Epoch 2110, training loss: 902.302001953125 = 0.9407739043235779 + 100.0 * 9.013611793518066
Epoch 2110, val loss: 0.9607178568840027
Epoch 2120, training loss: 903.9544677734375 = 0.9397031664848328 + 100.0 * 9.030147552490234
Epoch 2120, val loss: 0.9597797989845276
Epoch 2130, training loss: 905.4379272460938 = 0.9386226534843445 + 100.0 * 9.04499340057373
Epoch 2130, val loss: 0.9588478207588196
Epoch 2140, training loss: 906.3290405273438 = 0.9375643730163574 + 100.0 * 9.053915023803711
Epoch 2140, val loss: 0.9579420685768127
Epoch 2150, training loss: 907.1129760742188 = 0.936511754989624 + 100.0 * 9.06176471710205
Epoch 2150, val loss: 0.9570347666740417
Epoch 2160, training loss: 907.0879516601562 = 0.9354279041290283 + 100.0 * 9.061525344848633
Epoch 2160, val loss: 0.9561011791229248
Epoch 2170, training loss: 907.634521484375 = 0.9343752861022949 + 100.0 * 9.067001342773438
Epoch 2170, val loss: 0.9551833271980286
Epoch 2180, training loss: 908.0396118164062 = 0.9333173632621765 + 100.0 * 9.071063041687012
Epoch 2180, val loss: 0.9542722105979919
Epoch 2190, training loss: 908.3192138671875 = 0.9322518110275269 + 100.0 * 9.073869705200195
Epoch 2190, val loss: 0.9533553719520569
Epoch 2200, training loss: 908.6492309570312 = 0.9311884045600891 + 100.0 * 9.077179908752441
Epoch 2200, val loss: 0.9524397253990173
Epoch 2210, training loss: 908.9361572265625 = 0.9301483631134033 + 100.0 * 9.080060005187988
Epoch 2210, val loss: 0.9515352845191956
Epoch 2220, training loss: 909.0630493164062 = 0.92910236120224 + 100.0 * 9.081339836120605
Epoch 2220, val loss: 0.9506354331970215
Epoch 2230, training loss: 908.0006103515625 = 0.9280267357826233 + 100.0 * 9.070725440979004
Epoch 2230, val loss: 0.9497225880622864
Epoch 2240, training loss: 908.0116577148438 = 0.9272018074989319 + 100.0 * 9.070844650268555
Epoch 2240, val loss: 0.9490067362785339
Epoch 2250, training loss: 910.6339111328125 = 0.9262840151786804 + 100.0 * 9.097076416015625
Epoch 2250, val loss: 0.9481948614120483
Epoch 2260, training loss: 908.0606079101562 = 0.9251176118850708 + 100.0 * 9.071354866027832
Epoch 2260, val loss: 0.9471645355224609
Epoch 2270, training loss: 908.1551513671875 = 0.9240213632583618 + 100.0 * 9.072311401367188
Epoch 2270, val loss: 0.9462274312973022
Epoch 2280, training loss: 908.9024658203125 = 0.92299485206604 + 100.0 * 9.079794883728027
Epoch 2280, val loss: 0.9453355669975281
Epoch 2290, training loss: 909.80126953125 = 0.921974241733551 + 100.0 * 9.08879280090332
Epoch 2290, val loss: 0.9444596171379089
Epoch 2300, training loss: 909.7847900390625 = 0.9209396839141846 + 100.0 * 9.088638305664062
Epoch 2300, val loss: 0.9435625672340393
Epoch 2310, training loss: 910.3513793945312 = 0.9199170470237732 + 100.0 * 9.094314575195312
Epoch 2310, val loss: 0.9426892995834351
Epoch 2320, training loss: 910.74755859375 = 0.9189013242721558 + 100.0 * 9.098286628723145
Epoch 2320, val loss: 0.9418185949325562
Epoch 2330, training loss: 911.0653076171875 = 0.9178915023803711 + 100.0 * 9.101473808288574
Epoch 2330, val loss: 0.9409322738647461
Epoch 2340, training loss: 910.7984619140625 = 0.916869580745697 + 100.0 * 9.09881591796875
Epoch 2340, val loss: 0.9400515556335449
Epoch 2350, training loss: 911.1358642578125 = 0.9158589839935303 + 100.0 * 9.102200508117676
Epoch 2350, val loss: 0.9391875267028809
Epoch 2360, training loss: 911.10498046875 = 0.9148394465446472 + 100.0 * 9.101901054382324
Epoch 2360, val loss: 0.9382917881011963
Epoch 2370, training loss: 911.4197998046875 = 0.9138261675834656 + 100.0 * 9.105059623718262
Epoch 2370, val loss: 0.9374244809150696
Epoch 2380, training loss: 911.7555541992188 = 0.9128126502037048 + 100.0 * 9.108427047729492
Epoch 2380, val loss: 0.9365503191947937
Epoch 2390, training loss: 911.879638671875 = 0.9117956757545471 + 100.0 * 9.109678268432617
Epoch 2390, val loss: 0.9356797337532043
Epoch 2400, training loss: 911.90234375 = 0.9107877016067505 + 100.0 * 9.109915733337402
Epoch 2400, val loss: 0.9348119497299194
Epoch 2410, training loss: 912.0409545898438 = 0.9097742438316345 + 100.0 * 9.111311912536621
Epoch 2410, val loss: 0.933942437171936
Epoch 2420, training loss: 912.1937866210938 = 0.9087647199630737 + 100.0 * 9.112850189208984
Epoch 2420, val loss: 0.9330843687057495
Epoch 2430, training loss: 912.361572265625 = 0.9077509641647339 + 100.0 * 9.114538192749023
Epoch 2430, val loss: 0.9322320222854614
Epoch 2440, training loss: 912.4998779296875 = 0.9067424535751343 + 100.0 * 9.115931510925293
Epoch 2440, val loss: 0.9313822388648987
Epoch 2450, training loss: 912.4366455078125 = 0.9057371020317078 + 100.0 * 9.11530876159668
Epoch 2450, val loss: 0.9305447340011597
Epoch 2460, training loss: 912.4895629882812 = 0.904735267162323 + 100.0 * 9.115848541259766
Epoch 2460, val loss: 0.9297055602073669
Epoch 2470, training loss: 912.7438354492188 = 0.9037681221961975 + 100.0 * 9.118400573730469
Epoch 2470, val loss: 0.9288896918296814
Epoch 2480, training loss: 913.4837036132812 = 0.9028137922286987 + 100.0 * 9.125808715820312
Epoch 2480, val loss: 0.9280667901039124
Epoch 2490, training loss: 913.0189819335938 = 0.9018487930297852 + 100.0 * 9.121170997619629
Epoch 2490, val loss: 0.9272539019584656
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.5126086956521739
0.8148228645946534
=== training gcn model ===
Epoch 0, training loss: 1032.707275390625 = 1.097936987876892 + 100.0 * 10.316093444824219
Epoch 0, val loss: 1.0987653732299805
Epoch 10, training loss: 992.1318969726562 = 1.0972710847854614 + 100.0 * 9.910346031188965
Epoch 10, val loss: 1.0982048511505127
Epoch 20, training loss: 971.2462158203125 = 1.096805214881897 + 100.0 * 9.701494216918945
Epoch 20, val loss: 1.097779631614685
Epoch 30, training loss: 957.1871337890625 = 1.0963399410247803 + 100.0 * 9.560908317565918
Epoch 30, val loss: 1.097344160079956
Epoch 40, training loss: 946.4349975585938 = 1.0958882570266724 + 100.0 * 9.453391075134277
Epoch 40, val loss: 1.096922755241394
Epoch 50, training loss: 937.56396484375 = 1.0954333543777466 + 100.0 * 9.36468505859375
Epoch 50, val loss: 1.0964933633804321
Epoch 60, training loss: 929.97998046875 = 1.094977617263794 + 100.0 * 9.288849830627441
Epoch 60, val loss: 1.0960639715194702
Epoch 70, training loss: 923.4657592773438 = 1.0945179462432861 + 100.0 * 9.223711967468262
Epoch 70, val loss: 1.095633625984192
Epoch 80, training loss: 917.771240234375 = 1.0940604209899902 + 100.0 * 9.16677188873291
Epoch 80, val loss: 1.0952011346817017
Epoch 90, training loss: 912.887451171875 = 1.093600869178772 + 100.0 * 9.117938041687012
Epoch 90, val loss: 1.0947664976119995
Epoch 100, training loss: 908.7525024414062 = 1.0931276082992554 + 100.0 * 9.076593399047852
Epoch 100, val loss: 1.0943143367767334
Epoch 110, training loss: 905.25146484375 = 1.092598795890808 + 100.0 * 9.04158878326416
Epoch 110, val loss: 1.0937858819961548
Epoch 120, training loss: 902.1576538085938 = 1.0917088985443115 + 100.0 * 9.010659217834473
Epoch 120, val loss: 1.0928703546524048
Epoch 130, training loss: 899.5866088867188 = 1.0908037424087524 + 100.0 * 8.984957695007324
Epoch 130, val loss: 1.0919520854949951
Epoch 140, training loss: 897.3349609375 = 1.089940071105957 + 100.0 * 8.96245002746582
Epoch 140, val loss: 1.0910743474960327
Epoch 150, training loss: 895.1211547851562 = 1.0890753269195557 + 100.0 * 8.94032096862793
Epoch 150, val loss: 1.0902092456817627
Epoch 160, training loss: 893.2460327148438 = 1.0882130861282349 + 100.0 * 8.921578407287598
Epoch 160, val loss: 1.0893381834030151
Epoch 170, training loss: 891.7858276367188 = 1.087380051612854 + 100.0 * 8.906984329223633
Epoch 170, val loss: 1.0885037183761597
Epoch 180, training loss: 890.34130859375 = 1.0865322351455688 + 100.0 * 8.892547607421875
Epoch 180, val loss: 1.087663173675537
Epoch 190, training loss: 889.029052734375 = 1.0857223272323608 + 100.0 * 8.879433631896973
Epoch 190, val loss: 1.0868566036224365
Epoch 200, training loss: 888.0292358398438 = 1.0849591493606567 + 100.0 * 8.8694429397583
Epoch 200, val loss: 1.0860942602157593
Epoch 210, training loss: 887.3129272460938 = 1.0842740535736084 + 100.0 * 8.862286567687988
Epoch 210, val loss: 1.0854332447052002
Epoch 220, training loss: 886.5123901367188 = 1.0836654901504517 + 100.0 * 8.854287147521973
Epoch 220, val loss: 1.084841251373291
Epoch 230, training loss: 885.5526123046875 = 1.083119511604309 + 100.0 * 8.844695091247559
Epoch 230, val loss: 1.0843048095703125
Epoch 240, training loss: 884.8694458007812 = 1.082600712776184 + 100.0 * 8.837868690490723
Epoch 240, val loss: 1.0837976932525635
Epoch 250, training loss: 884.3984375 = 1.082067608833313 + 100.0 * 8.833163261413574
Epoch 250, val loss: 1.083279013633728
Epoch 260, training loss: 884.0166015625 = 1.081539511680603 + 100.0 * 8.829350471496582
Epoch 260, val loss: 1.0827672481536865
Epoch 270, training loss: 883.6734619140625 = 1.0810049772262573 + 100.0 * 8.82592487335205
Epoch 270, val loss: 1.0822439193725586
Epoch 280, training loss: 883.4239501953125 = 1.080426812171936 + 100.0 * 8.823434829711914
Epoch 280, val loss: 1.081680178642273
Epoch 290, training loss: 884.0165405273438 = 1.0799388885498047 + 100.0 * 8.829365730285645
Epoch 290, val loss: 1.0812132358551025
Epoch 300, training loss: 883.0760498046875 = 1.0793309211730957 + 100.0 * 8.819967269897461
Epoch 300, val loss: 1.0806350708007812
Epoch 310, training loss: 882.7611694335938 = 1.0787720680236816 + 100.0 * 8.816823959350586
Epoch 310, val loss: 1.0800789594650269
Epoch 320, training loss: 882.47802734375 = 1.0782146453857422 + 100.0 * 8.813998222351074
Epoch 320, val loss: 1.079540491104126
Epoch 330, training loss: 882.1631469726562 = 1.0776264667510986 + 100.0 * 8.8108549118042
Epoch 330, val loss: 1.0789785385131836
Epoch 340, training loss: 882.2534790039062 = 1.0770198106765747 + 100.0 * 8.81176471710205
Epoch 340, val loss: 1.0783902406692505
Epoch 350, training loss: 882.7175903320312 = 1.0764374732971191 + 100.0 * 8.816411972045898
Epoch 350, val loss: 1.077830195426941
Epoch 360, training loss: 881.6861572265625 = 1.0758098363876343 + 100.0 * 8.806103706359863
Epoch 360, val loss: 1.0772242546081543
Epoch 370, training loss: 882.4561767578125 = 1.075237512588501 + 100.0 * 8.813809394836426
Epoch 370, val loss: 1.0766667127609253
Epoch 380, training loss: 882.28662109375 = 1.074615716934204 + 100.0 * 8.812119483947754
Epoch 380, val loss: 1.076074242591858
Epoch 390, training loss: 882.0679931640625 = 1.0739907026290894 + 100.0 * 8.809940338134766
Epoch 390, val loss: 1.075467586517334
Epoch 400, training loss: 882.5107421875 = 1.0733509063720703 + 100.0 * 8.814373970031738
Epoch 400, val loss: 1.074867606163025
Epoch 410, training loss: 882.6918334960938 = 1.0727182626724243 + 100.0 * 8.816191673278809
Epoch 410, val loss: 1.0742501020431519
Epoch 420, training loss: 882.7138671875 = 1.0720677375793457 + 100.0 * 8.816417694091797
Epoch 420, val loss: 1.0736268758773804
Epoch 430, training loss: 882.7025756835938 = 1.0714328289031982 + 100.0 * 8.816311836242676
Epoch 430, val loss: 1.0730159282684326
Epoch 440, training loss: 882.3041381835938 = 1.0707447528839111 + 100.0 * 8.812334060668945
Epoch 440, val loss: 1.0723578929901123
Epoch 450, training loss: 883.1365966796875 = 1.0701230764389038 + 100.0 * 8.820664405822754
Epoch 450, val loss: 1.0717592239379883
Epoch 460, training loss: 882.7548828125 = 1.0694488286972046 + 100.0 * 8.816854476928711
Epoch 460, val loss: 1.071118950843811
Epoch 470, training loss: 883.1748046875 = 1.0687623023986816 + 100.0 * 8.821060180664062
Epoch 470, val loss: 1.0704772472381592
Epoch 480, training loss: 883.2544555664062 = 1.0680766105651855 + 100.0 * 8.821864128112793
Epoch 480, val loss: 1.0698124170303345
Epoch 490, training loss: 883.6033325195312 = 1.0673927068710327 + 100.0 * 8.825359344482422
Epoch 490, val loss: 1.069158911705017
Epoch 500, training loss: 883.5762329101562 = 1.0666992664337158 + 100.0 * 8.825095176696777
Epoch 500, val loss: 1.068500280380249
Epoch 510, training loss: 884.0170288085938 = 1.0659868717193604 + 100.0 * 8.829510688781738
Epoch 510, val loss: 1.067824125289917
Epoch 520, training loss: 884.4327392578125 = 1.0652837753295898 + 100.0 * 8.833674430847168
Epoch 520, val loss: 1.0671659708023071
Epoch 530, training loss: 884.1864624023438 = 1.0645102262496948 + 100.0 * 8.831219673156738
Epoch 530, val loss: 1.0664384365081787
Epoch 540, training loss: 883.7969360351562 = 1.0637990236282349 + 100.0 * 8.82733154296875
Epoch 540, val loss: 1.0657637119293213
Epoch 550, training loss: 884.0347900390625 = 1.0630625486373901 + 100.0 * 8.829717636108398
Epoch 550, val loss: 1.0650662183761597
Epoch 560, training loss: 884.7637329101562 = 1.0623353719711304 + 100.0 * 8.837014198303223
Epoch 560, val loss: 1.0643811225891113
Epoch 570, training loss: 884.899169921875 = 1.0615887641906738 + 100.0 * 8.83837604522705
Epoch 570, val loss: 1.0636974573135376
Epoch 580, training loss: 885.0032348632812 = 1.0608712434768677 + 100.0 * 8.839424133300781
Epoch 580, val loss: 1.062998652458191
Epoch 590, training loss: 884.5953369140625 = 1.0600554943084717 + 100.0 * 8.835352897644043
Epoch 590, val loss: 1.062240481376648
Epoch 600, training loss: 884.4457397460938 = 1.059300422668457 + 100.0 * 8.833864212036133
Epoch 600, val loss: 1.0615094900131226
Epoch 610, training loss: 884.9110107421875 = 1.0585594177246094 + 100.0 * 8.83852481842041
Epoch 610, val loss: 1.0608503818511963
Epoch 620, training loss: 885.2969970703125 = 1.05778169631958 + 100.0 * 8.842391967773438
Epoch 620, val loss: 1.0601075887680054
Epoch 630, training loss: 885.6010131835938 = 1.0570247173309326 + 100.0 * 8.845439910888672
Epoch 630, val loss: 1.0594005584716797
Epoch 640, training loss: 886.08056640625 = 1.056239366531372 + 100.0 * 8.85024356842041
Epoch 640, val loss: 1.0586624145507812
Epoch 650, training loss: 886.0105590820312 = 1.0554426908493042 + 100.0 * 8.8495512008667
Epoch 650, val loss: 1.0578919649124146
Epoch 660, training loss: 886.4935302734375 = 1.05465829372406 + 100.0 * 8.854388236999512
Epoch 660, val loss: 1.0571750402450562
Epoch 670, training loss: 886.6217041015625 = 1.0538437366485596 + 100.0 * 8.85567855834961
Epoch 670, val loss: 1.0564236640930176
Epoch 680, training loss: 886.7335205078125 = 1.0530109405517578 + 100.0 * 8.856804847717285
Epoch 680, val loss: 1.0556365251541138
Epoch 690, training loss: 887.0628051757812 = 1.052198052406311 + 100.0 * 8.860106468200684
Epoch 690, val loss: 1.054889440536499
Epoch 700, training loss: 887.2947387695312 = 1.051388144493103 + 100.0 * 8.862433433532715
Epoch 700, val loss: 1.0541187524795532
Epoch 710, training loss: 887.1187133789062 = 1.050512671470642 + 100.0 * 8.860681533813477
Epoch 710, val loss: 1.0532994270324707
Epoch 720, training loss: 887.6503295898438 = 1.0496177673339844 + 100.0 * 8.866006851196289
Epoch 720, val loss: 1.0524885654449463
Epoch 730, training loss: 887.4905395507812 = 1.048773169517517 + 100.0 * 8.864418029785156
Epoch 730, val loss: 1.0516985654830933
Epoch 740, training loss: 887.8740234375 = 1.0479412078857422 + 100.0 * 8.868261337280273
Epoch 740, val loss: 1.0509294271469116
Epoch 750, training loss: 887.6394653320312 = 1.0470199584960938 + 100.0 * 8.865924835205078
Epoch 750, val loss: 1.0500701665878296
Epoch 760, training loss: 887.985595703125 = 1.0461360216140747 + 100.0 * 8.869394302368164
Epoch 760, val loss: 1.0492448806762695
Epoch 770, training loss: 888.3795776367188 = 1.0452460050582886 + 100.0 * 8.873343467712402
Epoch 770, val loss: 1.0484085083007812
Epoch 780, training loss: 888.8116455078125 = 1.0443289279937744 + 100.0 * 8.877673149108887
Epoch 780, val loss: 1.047554850578308
Epoch 790, training loss: 888.9595947265625 = 1.0433990955352783 + 100.0 * 8.879161834716797
Epoch 790, val loss: 1.0466924905776978
Epoch 800, training loss: 889.5303955078125 = 1.0424903631210327 + 100.0 * 8.884879112243652
Epoch 800, val loss: 1.0458546876907349
Epoch 810, training loss: 889.8390502929688 = 1.041526436805725 + 100.0 * 8.887975692749023
Epoch 810, val loss: 1.044975757598877
Epoch 820, training loss: 890.3211059570312 = 1.0406008958816528 + 100.0 * 8.892805099487305
Epoch 820, val loss: 1.044113039970398
Epoch 830, training loss: 890.463134765625 = 1.0396476984024048 + 100.0 * 8.894234657287598
Epoch 830, val loss: 1.0432223081588745
Epoch 840, training loss: 890.923095703125 = 1.0386672019958496 + 100.0 * 8.898843765258789
Epoch 840, val loss: 1.0423201322555542
Epoch 850, training loss: 891.0328369140625 = 1.0376800298690796 + 100.0 * 8.899951934814453
Epoch 850, val loss: 1.041405200958252
Epoch 860, training loss: 891.299560546875 = 1.0366634130477905 + 100.0 * 8.902628898620605
Epoch 860, val loss: 1.0404943227767944
Epoch 870, training loss: 891.6364135742188 = 1.0356736183166504 + 100.0 * 8.906007766723633
Epoch 870, val loss: 1.039583444595337
Epoch 880, training loss: 891.6591796875 = 1.0346590280532837 + 100.0 * 8.906245231628418
Epoch 880, val loss: 1.0386507511138916
Epoch 890, training loss: 891.5338745117188 = 1.0336006879806519 + 100.0 * 8.90500259399414
Epoch 890, val loss: 1.037674069404602
Epoch 900, training loss: 891.4039306640625 = 1.0325273275375366 + 100.0 * 8.903714179992676
Epoch 900, val loss: 1.036696195602417
Epoch 910, training loss: 891.7339477539062 = 1.031416654586792 + 100.0 * 8.907025337219238
Epoch 910, val loss: 1.035684585571289
Epoch 920, training loss: 891.7825927734375 = 1.030400276184082 + 100.0 * 8.907522201538086
Epoch 920, val loss: 1.0347501039505005
Epoch 930, training loss: 892.55126953125 = 1.0293604135513306 + 100.0 * 8.9152193069458
Epoch 930, val loss: 1.0338184833526611
Epoch 940, training loss: 892.811279296875 = 1.028297781944275 + 100.0 * 8.917829513549805
Epoch 940, val loss: 1.0328216552734375
Epoch 950, training loss: 893.1635131835938 = 1.0271943807601929 + 100.0 * 8.92136287689209
Epoch 950, val loss: 1.0318200588226318
Epoch 960, training loss: 893.3167724609375 = 1.0260790586471558 + 100.0 * 8.922906875610352
Epoch 960, val loss: 1.0307904481887817
Epoch 970, training loss: 893.1925659179688 = 1.0248746871948242 + 100.0 * 8.921676635742188
Epoch 970, val loss: 1.0297157764434814
Epoch 980, training loss: 893.134765625 = 1.0237488746643066 + 100.0 * 8.921110153198242
Epoch 980, val loss: 1.0286462306976318
Epoch 990, training loss: 894.0144653320312 = 1.0225350856781006 + 100.0 * 8.929919242858887
Epoch 990, val loss: 1.0275486707687378
Epoch 1000, training loss: 894.02587890625 = 1.0212593078613281 + 100.0 * 8.930046081542969
Epoch 1000, val loss: 1.0263657569885254
Epoch 1010, training loss: 894.54345703125 = 1.0200257301330566 + 100.0 * 8.935234069824219
Epoch 1010, val loss: 1.0252184867858887
Epoch 1020, training loss: 894.6083374023438 = 1.0187394618988037 + 100.0 * 8.935895919799805
Epoch 1020, val loss: 1.024027943611145
Epoch 1030, training loss: 895.0286865234375 = 1.0174452066421509 + 100.0 * 8.940112113952637
Epoch 1030, val loss: 1.022835373878479
Epoch 1040, training loss: 895.1133422851562 = 1.016128659248352 + 100.0 * 8.940972328186035
Epoch 1040, val loss: 1.0216304063796997
Epoch 1050, training loss: 895.1764526367188 = 1.0148439407348633 + 100.0 * 8.94161605834961
Epoch 1050, val loss: 1.0204428434371948
Epoch 1060, training loss: 895.7404174804688 = 1.0135518312454224 + 100.0 * 8.94726848602295
Epoch 1060, val loss: 1.0192500352859497
Epoch 1070, training loss: 896.1167602539062 = 1.0122652053833008 + 100.0 * 8.951045036315918
Epoch 1070, val loss: 1.0180604457855225
Epoch 1080, training loss: 896.2637329101562 = 1.01093590259552 + 100.0 * 8.95252799987793
Epoch 1080, val loss: 1.0168514251708984
Epoch 1090, training loss: 896.8482666015625 = 1.0096367597579956 + 100.0 * 8.958386421203613
Epoch 1090, val loss: 1.0156402587890625
Epoch 1100, training loss: 896.8065185546875 = 1.0083162784576416 + 100.0 * 8.957982063293457
Epoch 1100, val loss: 1.014395833015442
Epoch 1110, training loss: 896.4182739257812 = 1.006888508796692 + 100.0 * 8.954113960266113
Epoch 1110, val loss: 1.0131034851074219
Epoch 1120, training loss: 896.3065795898438 = 1.005564570426941 + 100.0 * 8.953010559082031
Epoch 1120, val loss: 1.0119189023971558
Epoch 1130, training loss: 896.9776000976562 = 1.004260778427124 + 100.0 * 8.959733009338379
Epoch 1130, val loss: 1.0106807947158813
Epoch 1140, training loss: 897.2787475585938 = 1.0029054880142212 + 100.0 * 8.96275806427002
Epoch 1140, val loss: 1.0094664096832275
Epoch 1150, training loss: 897.7351684570312 = 1.001545786857605 + 100.0 * 8.967336654663086
Epoch 1150, val loss: 1.0082210302352905
Epoch 1160, training loss: 896.154052734375 = 0.9999169111251831 + 100.0 * 8.95154094696045
Epoch 1160, val loss: 1.0067540407180786
Epoch 1170, training loss: 896.59765625 = 0.9986415505409241 + 100.0 * 8.955989837646484
Epoch 1170, val loss: 1.005537509918213
Epoch 1180, training loss: 897.6275024414062 = 0.9973504543304443 + 100.0 * 8.966300964355469
Epoch 1180, val loss: 1.004343032836914
Epoch 1190, training loss: 897.6353759765625 = 0.996027410030365 + 100.0 * 8.96639347076416
Epoch 1190, val loss: 1.0031330585479736
Epoch 1200, training loss: 898.2730102539062 = 0.9946825504302979 + 100.0 * 8.972783088684082
Epoch 1200, val loss: 1.0018887519836426
Epoch 1210, training loss: 898.3785400390625 = 0.9932548999786377 + 100.0 * 8.97385311126709
Epoch 1210, val loss: 1.0005898475646973
Epoch 1220, training loss: 898.8202514648438 = 0.9918714165687561 + 100.0 * 8.978283882141113
Epoch 1220, val loss: 0.9993206262588501
Epoch 1230, training loss: 898.7308959960938 = 0.9903950691223145 + 100.0 * 8.977404594421387
Epoch 1230, val loss: 0.9979552626609802
Epoch 1240, training loss: 899.2384643554688 = 0.9889903664588928 + 100.0 * 8.982494354248047
Epoch 1240, val loss: 0.9966831803321838
Epoch 1250, training loss: 899.419677734375 = 0.9875491261482239 + 100.0 * 8.984321594238281
Epoch 1250, val loss: 0.9953612089157104
Epoch 1260, training loss: 899.126220703125 = 0.9860963225364685 + 100.0 * 8.981401443481445
Epoch 1260, val loss: 0.9940003752708435
Epoch 1270, training loss: 899.5916137695312 = 0.9846448302268982 + 100.0 * 8.986069679260254
Epoch 1270, val loss: 0.9926698207855225
Epoch 1280, training loss: 899.3150024414062 = 0.9831478595733643 + 100.0 * 8.983318328857422
Epoch 1280, val loss: 0.9913087487220764
Epoch 1290, training loss: 899.7357177734375 = 0.9817066788673401 + 100.0 * 8.987540245056152
Epoch 1290, val loss: 0.9899834394454956
Epoch 1300, training loss: 900.2251586914062 = 0.9802584052085876 + 100.0 * 8.992448806762695
Epoch 1300, val loss: 0.988660454750061
Epoch 1310, training loss: 899.8776245117188 = 0.9787293672561646 + 100.0 * 8.988988876342773
Epoch 1310, val loss: 0.987267255783081
Epoch 1320, training loss: 899.98095703125 = 0.9772195816040039 + 100.0 * 8.990036964416504
Epoch 1320, val loss: 0.9858661890029907
Epoch 1330, training loss: 900.5641479492188 = 0.9757682681083679 + 100.0 * 8.99588394165039
Epoch 1330, val loss: 0.9845589399337769
Epoch 1340, training loss: 901.0481567382812 = 0.9742665886878967 + 100.0 * 9.000739097595215
Epoch 1340, val loss: 0.9831750988960266
Epoch 1350, training loss: 900.735107421875 = 0.9726718068122864 + 100.0 * 8.997624397277832
Epoch 1350, val loss: 0.9816935658454895
Epoch 1360, training loss: 901.0027465820312 = 0.9711642265319824 + 100.0 * 9.00031566619873
Epoch 1360, val loss: 0.9803146719932556
Epoch 1370, training loss: 901.6420288085938 = 0.969597339630127 + 100.0 * 9.00672435760498
Epoch 1370, val loss: 0.9788740873336792
Epoch 1380, training loss: 901.6315307617188 = 0.967925488948822 + 100.0 * 9.006636619567871
Epoch 1380, val loss: 0.9773082137107849
Epoch 1390, training loss: 901.5633544921875 = 0.9661568999290466 + 100.0 * 9.005971908569336
Epoch 1390, val loss: 0.9757052659988403
Epoch 1400, training loss: 901.8854370117188 = 0.9643497467041016 + 100.0 * 9.009210586547852
Epoch 1400, val loss: 0.9740412831306458
Epoch 1410, training loss: 902.1417236328125 = 0.9624034762382507 + 100.0 * 9.01179313659668
Epoch 1410, val loss: 0.9722320437431335
Epoch 1420, training loss: 902.3300170898438 = 0.9604535698890686 + 100.0 * 9.01369571685791
Epoch 1420, val loss: 0.9704223871231079
Epoch 1430, training loss: 902.40234375 = 0.9585493206977844 + 100.0 * 9.014437675476074
Epoch 1430, val loss: 0.9686506986618042
Epoch 1440, training loss: 902.7105712890625 = 0.9566896557807922 + 100.0 * 9.017539024353027
Epoch 1440, val loss: 0.9669172763824463
Epoch 1450, training loss: 902.8785400390625 = 0.9548717141151428 + 100.0 * 9.01923656463623
Epoch 1450, val loss: 0.9652286767959595
Epoch 1460, training loss: 902.9207153320312 = 0.9530165791511536 + 100.0 * 9.01967716217041
Epoch 1460, val loss: 0.9635318517684937
Epoch 1470, training loss: 903.3173828125 = 0.9512168169021606 + 100.0 * 9.023661613464355
Epoch 1470, val loss: 0.9618688225746155
Epoch 1480, training loss: 902.9032592773438 = 0.9493001699447632 + 100.0 * 9.019539833068848
Epoch 1480, val loss: 0.9601362347602844
Epoch 1490, training loss: 903.1900024414062 = 0.9474454522132874 + 100.0 * 9.022425651550293
Epoch 1490, val loss: 0.9584736227989197
Epoch 1500, training loss: 902.46484375 = 0.9455764293670654 + 100.0 * 9.015192985534668
Epoch 1500, val loss: 0.9566391110420227
Epoch 1510, training loss: 902.3283081054688 = 0.943777322769165 + 100.0 * 9.013845443725586
Epoch 1510, val loss: 0.9550728797912598
Epoch 1520, training loss: 902.3654174804688 = 0.9419054985046387 + 100.0 * 9.014235496520996
Epoch 1520, val loss: 0.9533453583717346
Epoch 1530, training loss: 903.0171508789062 = 0.9400560855865479 + 100.0 * 9.020771026611328
Epoch 1530, val loss: 0.9516597390174866
Epoch 1540, training loss: 903.8118286132812 = 0.9382449388504028 + 100.0 * 9.028736114501953
Epoch 1540, val loss: 0.9499918222427368
Epoch 1550, training loss: 904.0640258789062 = 0.9363604187965393 + 100.0 * 9.03127670288086
Epoch 1550, val loss: 0.9482960104942322
Epoch 1560, training loss: 903.8477172851562 = 0.9344602823257446 + 100.0 * 9.029132843017578
Epoch 1560, val loss: 0.9465559720993042
Epoch 1570, training loss: 904.2754516601562 = 0.9325892329216003 + 100.0 * 9.033428192138672
Epoch 1570, val loss: 0.9448288679122925
Epoch 1580, training loss: 904.64208984375 = 0.930687427520752 + 100.0 * 9.037114143371582
Epoch 1580, val loss: 0.9430958032608032
Epoch 1590, training loss: 904.5458374023438 = 0.928755521774292 + 100.0 * 9.036170959472656
Epoch 1590, val loss: 0.9413208961486816
Epoch 1600, training loss: 904.80322265625 = 0.9268384575843811 + 100.0 * 9.038763999938965
Epoch 1600, val loss: 0.9395809173583984
Epoch 1610, training loss: 905.0100708007812 = 0.9249022006988525 + 100.0 * 9.040851593017578
Epoch 1610, val loss: 0.9378339648246765
Epoch 1620, training loss: 905.1677856445312 = 0.9229444861412048 + 100.0 * 9.042448043823242
Epoch 1620, val loss: 0.936043918132782
Epoch 1630, training loss: 905.1182250976562 = 0.9209952354431152 + 100.0 * 9.041972160339355
Epoch 1630, val loss: 0.9342753887176514
Epoch 1640, training loss: 905.5269165039062 = 0.9190578460693359 + 100.0 * 9.0460786819458
Epoch 1640, val loss: 0.9325065016746521
Epoch 1650, training loss: 905.2330932617188 = 0.9170374274253845 + 100.0 * 9.043160438537598
Epoch 1650, val loss: 0.9306327104568481
Epoch 1660, training loss: 903.9479370117188 = 0.9149960279464722 + 100.0 * 9.030329704284668
Epoch 1660, val loss: 0.928810179233551
Epoch 1670, training loss: 904.5758666992188 = 0.9130697846412659 + 100.0 * 9.036627769470215
Epoch 1670, val loss: 0.9270908236503601
Epoch 1680, training loss: 905.5592651367188 = 0.9112364053726196 + 100.0 * 9.046480178833008
Epoch 1680, val loss: 0.9253835082054138
Epoch 1690, training loss: 906.2682495117188 = 0.909270167350769 + 100.0 * 9.053589820861816
Epoch 1690, val loss: 0.9236123561859131
Epoch 1700, training loss: 906.6663208007812 = 0.9072999358177185 + 100.0 * 9.05759048461914
Epoch 1700, val loss: 0.9217954874038696
Epoch 1710, training loss: 906.6488647460938 = 0.9052896499633789 + 100.0 * 9.057435989379883
Epoch 1710, val loss: 0.9199655652046204
Epoch 1720, training loss: 906.6011962890625 = 0.9032627940177917 + 100.0 * 9.056979179382324
Epoch 1720, val loss: 0.9181471467018127
Epoch 1730, training loss: 907.0863647460938 = 0.901280403137207 + 100.0 * 9.061850547790527
Epoch 1730, val loss: 0.9163380265235901
Epoch 1740, training loss: 906.582275390625 = 0.8991888761520386 + 100.0 * 9.056831359863281
Epoch 1740, val loss: 0.9144717454910278
Epoch 1750, training loss: 905.2940673828125 = 0.8969787955284119 + 100.0 * 9.043971061706543
Epoch 1750, val loss: 0.9124830365180969
Epoch 1760, training loss: 905.1407470703125 = 0.8950328230857849 + 100.0 * 9.042457580566406
Epoch 1760, val loss: 0.9107170104980469
Epoch 1770, training loss: 906.0328979492188 = 0.8930511474609375 + 100.0 * 9.051398277282715
Epoch 1770, val loss: 0.908902645111084
Epoch 1780, training loss: 906.9943237304688 = 0.8911264538764954 + 100.0 * 9.06103229522705
Epoch 1780, val loss: 0.9071757197380066
Epoch 1790, training loss: 907.460205078125 = 0.8890865445137024 + 100.0 * 9.06571102142334
Epoch 1790, val loss: 0.9053312540054321
Epoch 1800, training loss: 907.491943359375 = 0.8869917988777161 + 100.0 * 9.066049575805664
Epoch 1800, val loss: 0.9034429788589478
Epoch 1810, training loss: 907.4277954101562 = 0.8849637508392334 + 100.0 * 9.065428733825684
Epoch 1810, val loss: 0.90159010887146
Epoch 1820, training loss: 908.0216674804688 = 0.8829231858253479 + 100.0 * 9.07138729095459
Epoch 1820, val loss: 0.8997421264648438
Epoch 1830, training loss: 908.4085083007812 = 0.880889356136322 + 100.0 * 9.075276374816895
Epoch 1830, val loss: 0.897889256477356
Epoch 1840, training loss: 907.9415893554688 = 0.8787720203399658 + 100.0 * 9.07062816619873
Epoch 1840, val loss: 0.8959562182426453
Epoch 1850, training loss: 907.7083740234375 = 0.8766273260116577 + 100.0 * 9.068317413330078
Epoch 1850, val loss: 0.8940532207489014
Epoch 1860, training loss: 908.1510009765625 = 0.8745954036712646 + 100.0 * 9.07276439666748
Epoch 1860, val loss: 0.8922027349472046
Epoch 1870, training loss: 908.8890991210938 = 0.8725235462188721 + 100.0 * 9.08016586303711
Epoch 1870, val loss: 0.8903341293334961
Epoch 1880, training loss: 908.5407104492188 = 0.8703938722610474 + 100.0 * 9.076703071594238
Epoch 1880, val loss: 0.8883974552154541
Epoch 1890, training loss: 908.1954345703125 = 0.8682865500450134 + 100.0 * 9.073271751403809
Epoch 1890, val loss: 0.8865233063697815
Epoch 1900, training loss: 908.495849609375 = 0.8662431836128235 + 100.0 * 9.076295852661133
Epoch 1900, val loss: 0.8846759796142578
Epoch 1910, training loss: 909.4906616210938 = 0.8642127513885498 + 100.0 * 9.086264610290527
Epoch 1910, val loss: 0.8828418850898743
Epoch 1920, training loss: 909.7376708984375 = 0.8621256947517395 + 100.0 * 9.08875560760498
Epoch 1920, val loss: 0.8809418678283691
Epoch 1930, training loss: 908.671875 = 0.8599328398704529 + 100.0 * 9.078119277954102
Epoch 1930, val loss: 0.879043459892273
Epoch 1940, training loss: 908.3251953125 = 0.857832670211792 + 100.0 * 9.074673652648926
Epoch 1940, val loss: 0.8771545886993408
Epoch 1950, training loss: 908.86572265625 = 0.8558065295219421 + 100.0 * 9.080099105834961
Epoch 1950, val loss: 0.875284731388092
Epoch 1960, training loss: 909.77783203125 = 0.8537421226501465 + 100.0 * 9.089241027832031
Epoch 1960, val loss: 0.8734350800514221
Epoch 1970, training loss: 910.2185668945312 = 0.8516955971717834 + 100.0 * 9.093668937683105
Epoch 1970, val loss: 0.8715936541557312
Epoch 1980, training loss: 910.2507934570312 = 0.8495767712593079 + 100.0 * 9.094012260437012
Epoch 1980, val loss: 0.8696851134300232
Epoch 1990, training loss: 910.09716796875 = 0.8474843502044678 + 100.0 * 9.092496871948242
Epoch 1990, val loss: 0.8678179979324341
Epoch 2000, training loss: 910.48876953125 = 0.8454230427742004 + 100.0 * 9.096433639526367
Epoch 2000, val loss: 0.8659785985946655
Epoch 2010, training loss: 910.6510620117188 = 0.8433712124824524 + 100.0 * 9.098076820373535
Epoch 2010, val loss: 0.8641418218612671
Epoch 2020, training loss: 910.75830078125 = 0.8412399291992188 + 100.0 * 9.099170684814453
Epoch 2020, val loss: 0.8622227311134338
Epoch 2030, training loss: 910.6419677734375 = 0.8391550183296204 + 100.0 * 9.098028182983398
Epoch 2030, val loss: 0.8603646755218506
Epoch 2040, training loss: 910.8416137695312 = 0.8370764851570129 + 100.0 * 9.100045204162598
Epoch 2040, val loss: 0.8584964275360107
Epoch 2050, training loss: 911.4456176757812 = 0.8350112438201904 + 100.0 * 9.10610580444336
Epoch 2050, val loss: 0.8566661477088928
Epoch 2060, training loss: 910.117431640625 = 0.8328387141227722 + 100.0 * 9.092845916748047
Epoch 2060, val loss: 0.8547462224960327
Epoch 2070, training loss: 910.0769653320312 = 0.8307926058769226 + 100.0 * 9.092461585998535
Epoch 2070, val loss: 0.8528985977172852
Epoch 2080, training loss: 911.1484375 = 0.828799307346344 + 100.0 * 9.103196144104004
Epoch 2080, val loss: 0.8511232137680054
Epoch 2090, training loss: 911.8690185546875 = 0.8267675638198853 + 100.0 * 9.110422134399414
Epoch 2090, val loss: 0.8492920398712158
Epoch 2100, training loss: 911.755126953125 = 0.8247225284576416 + 100.0 * 9.109304428100586
Epoch 2100, val loss: 0.8474425673484802
Epoch 2110, training loss: 912.0469360351562 = 0.8227269053459167 + 100.0 * 9.112241744995117
Epoch 2110, val loss: 0.8456724286079407
Epoch 2120, training loss: 912.484375 = 0.8207094073295593 + 100.0 * 9.116636276245117
Epoch 2120, val loss: 0.8438793420791626
Epoch 2130, training loss: 912.3110961914062 = 0.818653404712677 + 100.0 * 9.114924430847168
Epoch 2130, val loss: 0.8420342803001404
Epoch 2140, training loss: 912.3981323242188 = 0.8166195750236511 + 100.0 * 9.115815162658691
Epoch 2140, val loss: 0.840241551399231
Epoch 2150, training loss: 911.5601806640625 = 0.814419150352478 + 100.0 * 9.107458114624023
Epoch 2150, val loss: 0.8382443785667419
Epoch 2160, training loss: 910.9207763671875 = 0.8126130104064941 + 100.0 * 9.101081848144531
Epoch 2160, val loss: 0.836717963218689
Epoch 2170, training loss: 910.2259521484375 = 0.8105356097221375 + 100.0 * 9.094154357910156
Epoch 2170, val loss: 0.8348255157470703
Epoch 2180, training loss: 910.6889038085938 = 0.8087010979652405 + 100.0 * 9.098801612854004
Epoch 2180, val loss: 0.8331964612007141
Epoch 2190, training loss: 911.2473754882812 = 0.8066415190696716 + 100.0 * 9.10440731048584
Epoch 2190, val loss: 0.8313484787940979
Epoch 2200, training loss: 912.26220703125 = 0.8047203421592712 + 100.0 * 9.114574432373047
Epoch 2200, val loss: 0.829639732837677
Epoch 2210, training loss: 913.1449584960938 = 0.8027805685997009 + 100.0 * 9.123421669006348
Epoch 2210, val loss: 0.8279294967651367
Epoch 2220, training loss: 912.9291381835938 = 0.8007740378379822 + 100.0 * 9.121283531188965
Epoch 2220, val loss: 0.8261417746543884
Epoch 2230, training loss: 913.102783203125 = 0.7988067865371704 + 100.0 * 9.123039245605469
Epoch 2230, val loss: 0.8243992328643799
Epoch 2240, training loss: 914.0108032226562 = 0.7968772649765015 + 100.0 * 9.132139205932617
Epoch 2240, val loss: 0.8226923942565918
Epoch 2250, training loss: 913.7001342773438 = 0.7949084043502808 + 100.0 * 9.12905216217041
Epoch 2250, val loss: 0.8209453821182251
Epoch 2260, training loss: 913.89892578125 = 0.7929595708847046 + 100.0 * 9.131059646606445
Epoch 2260, val loss: 0.8192280530929565
Epoch 2270, training loss: 914.0950317382812 = 0.791011393070221 + 100.0 * 9.133040428161621
Epoch 2270, val loss: 0.8174976110458374
Epoch 2280, training loss: 914.3107299804688 = 0.7890766859054565 + 100.0 * 9.13521671295166
Epoch 2280, val loss: 0.8157747387886047
Epoch 2290, training loss: 914.150634765625 = 0.7871361374855042 + 100.0 * 9.133635520935059
Epoch 2290, val loss: 0.8140760660171509
Epoch 2300, training loss: 914.530517578125 = 0.7852346301078796 + 100.0 * 9.137453079223633
Epoch 2300, val loss: 0.8124017119407654
Epoch 2310, training loss: 912.3313598632812 = 0.7831995487213135 + 100.0 * 9.11548137664795
Epoch 2310, val loss: 0.8105597496032715
Epoch 2320, training loss: 913.0045166015625 = 0.7815553545951843 + 100.0 * 9.12222957611084
Epoch 2320, val loss: 0.8092937469482422
Epoch 2330, training loss: 912.076416015625 = 0.7798486948013306 + 100.0 * 9.11296558380127
Epoch 2330, val loss: 0.8077117800712585
Epoch 2340, training loss: 912.4207153320312 = 0.7779802083969116 + 100.0 * 9.116427421569824
Epoch 2340, val loss: 0.8060507774353027
Epoch 2350, training loss: 913.59130859375 = 0.7762430906295776 + 100.0 * 9.128150939941406
Epoch 2350, val loss: 0.804509699344635
Epoch 2360, training loss: 914.4978637695312 = 0.7744904160499573 + 100.0 * 9.13723373413086
Epoch 2360, val loss: 0.8029757738113403
Epoch 2370, training loss: 915.3518676757812 = 0.7726744413375854 + 100.0 * 9.145792007446289
Epoch 2370, val loss: 0.8013855814933777
Epoch 2380, training loss: 915.4535522460938 = 0.7707921862602234 + 100.0 * 9.146827697753906
Epoch 2380, val loss: 0.7997475862503052
Epoch 2390, training loss: 915.645751953125 = 0.7689487934112549 + 100.0 * 9.148768424987793
Epoch 2390, val loss: 0.7981289625167847
Epoch 2400, training loss: 916.1598510742188 = 0.7671096920967102 + 100.0 * 9.15392780303955
Epoch 2400, val loss: 0.7965226173400879
Epoch 2410, training loss: 916.4008178710938 = 0.7652590870857239 + 100.0 * 9.156355857849121
Epoch 2410, val loss: 0.7948942184448242
Epoch 2420, training loss: 916.0333251953125 = 0.7633789777755737 + 100.0 * 9.15269947052002
Epoch 2420, val loss: 0.7932732105255127
Epoch 2430, training loss: 916.6132202148438 = 0.7615454792976379 + 100.0 * 9.158516883850098
Epoch 2430, val loss: 0.791663408279419
Epoch 2440, training loss: 916.897216796875 = 0.7597053647041321 + 100.0 * 9.161375045776367
Epoch 2440, val loss: 0.7900746464729309
Epoch 2450, training loss: 916.8949584960938 = 0.757838785648346 + 100.0 * 9.161371231079102
Epoch 2450, val loss: 0.7884378433227539
Epoch 2460, training loss: 916.4642333984375 = 0.7559821605682373 + 100.0 * 9.157082557678223
Epoch 2460, val loss: 0.7868234515190125
Epoch 2470, training loss: 916.9742431640625 = 0.754176914691925 + 100.0 * 9.162200927734375
Epoch 2470, val loss: 0.785284161567688
Epoch 2480, training loss: 917.3551025390625 = 0.7523701786994934 + 100.0 * 9.166027069091797
Epoch 2480, val loss: 0.7837086319923401
Epoch 2490, training loss: 916.9801635742188 = 0.750531017780304 + 100.0 * 9.162296295166016
Epoch 2490, val loss: 0.7821139097213745
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7044927536231884
0.8168514091139608
The final CL Acc:0.65314, 0.10056, The final GNN Acc:0.81620, 0.00097
raw graph: torch.Size([2, 88648])
add edge: torch.Size([2, 111140])
remove edge: torch.Size([2, 66192])
updated graph: torch.Size([2, 88684])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 1038.7940673828125 = 1.0890878438949585 + 100.0 * 10.377049446105957
Epoch 0, val loss: 1.0897517204284668
Epoch 10, training loss: 998.6182250976562 = 1.088793158531189 + 100.0 * 9.97529411315918
Epoch 10, val loss: 1.089436650276184
Epoch 20, training loss: 976.3911743164062 = 1.0884840488433838 + 100.0 * 9.753026962280273
Epoch 20, val loss: 1.0891268253326416
Epoch 30, training loss: 960.5673217773438 = 1.088166356086731 + 100.0 * 9.594791412353516
Epoch 30, val loss: 1.0888139009475708
Epoch 40, training loss: 947.6547241210938 = 1.0878596305847168 + 100.0 * 9.465668678283691
Epoch 40, val loss: 1.088516354560852
Epoch 50, training loss: 937.00341796875 = 1.0875699520111084 + 100.0 * 9.359158515930176
Epoch 50, val loss: 1.0882313251495361
Epoch 60, training loss: 928.1793823242188 = 1.087266445159912 + 100.0 * 9.270920753479004
Epoch 60, val loss: 1.087932825088501
Epoch 70, training loss: 920.7330322265625 = 1.0869641304016113 + 100.0 * 9.196460723876953
Epoch 70, val loss: 1.087637186050415
Epoch 80, training loss: 914.4337158203125 = 1.0866509675979614 + 100.0 * 9.13347053527832
Epoch 80, val loss: 1.0873291492462158
Epoch 90, training loss: 909.140625 = 1.0863271951675415 + 100.0 * 9.080543518066406
Epoch 90, val loss: 1.087011456489563
Epoch 100, training loss: 904.6669311523438 = 1.085979700088501 + 100.0 * 9.035809516906738
Epoch 100, val loss: 1.086669921875
Epoch 110, training loss: 900.7442016601562 = 1.0856293439865112 + 100.0 * 8.996585845947266
Epoch 110, val loss: 1.086328387260437
Epoch 120, training loss: 897.3577270507812 = 1.085266351699829 + 100.0 * 8.962724685668945
Epoch 120, val loss: 1.0859719514846802
Epoch 130, training loss: 894.5391845703125 = 1.0848968029022217 + 100.0 * 8.934542655944824
Epoch 130, val loss: 1.0856109857559204
Epoch 140, training loss: 892.0075073242188 = 1.0844976902008057 + 100.0 * 8.90923023223877
Epoch 140, val loss: 1.0852222442626953
Epoch 150, training loss: 890.019775390625 = 1.084093451499939 + 100.0 * 8.88935661315918
Epoch 150, val loss: 1.0848244428634644
Epoch 160, training loss: 888.2099609375 = 1.0836665630340576 + 100.0 * 8.871262550354004
Epoch 160, val loss: 1.084403395652771
Epoch 170, training loss: 886.3525390625 = 1.0832053422927856 + 100.0 * 8.852693557739258
Epoch 170, val loss: 1.0839569568634033
Epoch 180, training loss: 884.906005859375 = 1.082722783088684 + 100.0 * 8.83823299407959
Epoch 180, val loss: 1.083490252494812
Epoch 190, training loss: 883.6000366210938 = 1.0822137594223022 + 100.0 * 8.825178146362305
Epoch 190, val loss: 1.0829843282699585
Epoch 200, training loss: 882.7767333984375 = 1.0816659927368164 + 100.0 * 8.816950798034668
Epoch 200, val loss: 1.0824490785598755
Epoch 210, training loss: 881.6412963867188 = 1.0811930894851685 + 100.0 * 8.805601119995117
Epoch 210, val loss: 1.0819861888885498
Epoch 220, training loss: 881.1704711914062 = 1.0806373357772827 + 100.0 * 8.800898551940918
Epoch 220, val loss: 1.0814380645751953
Epoch 230, training loss: 880.2139892578125 = 1.080038070678711 + 100.0 * 8.791339874267578
Epoch 230, val loss: 1.0808507204055786
Epoch 240, training loss: 879.4595336914062 = 1.0795094966888428 + 100.0 * 8.78380012512207
Epoch 240, val loss: 1.0803312063217163
Epoch 250, training loss: 879.7107543945312 = 1.0788824558258057 + 100.0 * 8.7863187789917
Epoch 250, val loss: 1.079733967781067
Epoch 260, training loss: 878.1947631835938 = 1.078267216682434 + 100.0 * 8.771164894104004
Epoch 260, val loss: 1.0791531801223755
Epoch 270, training loss: 878.0590209960938 = 1.0776867866516113 + 100.0 * 8.769813537597656
Epoch 270, val loss: 1.0785752534866333
Epoch 280, training loss: 877.7266235351562 = 1.0770516395568848 + 100.0 * 8.766495704650879
Epoch 280, val loss: 1.0779690742492676
Epoch 290, training loss: 878.034423828125 = 1.0764108896255493 + 100.0 * 8.769579887390137
Epoch 290, val loss: 1.077345371246338
Epoch 300, training loss: 878.2127075195312 = 1.0757460594177246 + 100.0 * 8.771369934082031
Epoch 300, val loss: 1.0767065286636353
Epoch 310, training loss: 878.199951171875 = 1.0750623941421509 + 100.0 * 8.771248817443848
Epoch 310, val loss: 1.0760529041290283
Epoch 320, training loss: 878.083740234375 = 1.0743616819381714 + 100.0 * 8.77009391784668
Epoch 320, val loss: 1.0753827095031738
Epoch 330, training loss: 877.9808349609375 = 1.0736498832702637 + 100.0 * 8.769071578979492
Epoch 330, val loss: 1.0746963024139404
Epoch 340, training loss: 878.0198364257812 = 1.0729132890701294 + 100.0 * 8.769469261169434
Epoch 340, val loss: 1.0739822387695312
Epoch 350, training loss: 877.866943359375 = 1.072161316871643 + 100.0 * 8.767948150634766
Epoch 350, val loss: 1.073264718055725
Epoch 360, training loss: 878.31396484375 = 1.0714105367660522 + 100.0 * 8.772425651550293
Epoch 360, val loss: 1.0725351572036743
Epoch 370, training loss: 878.4825439453125 = 1.0706294775009155 + 100.0 * 8.77411937713623
Epoch 370, val loss: 1.071792483329773
Epoch 380, training loss: 878.5020751953125 = 1.069825530052185 + 100.0 * 8.774322509765625
Epoch 380, val loss: 1.0709943771362305
Epoch 390, training loss: 878.2598266601562 = 1.0689845085144043 + 100.0 * 8.7719087600708
Epoch 390, val loss: 1.0702067613601685
Epoch 400, training loss: 878.38720703125 = 1.0681499242782593 + 100.0 * 8.77319049835205
Epoch 400, val loss: 1.069407343864441
Epoch 410, training loss: 878.62890625 = 1.06729257106781 + 100.0 * 8.775615692138672
Epoch 410, val loss: 1.0685871839523315
Epoch 420, training loss: 878.7067260742188 = 1.066427230834961 + 100.0 * 8.776403427124023
Epoch 420, val loss: 1.0677567720413208
Epoch 430, training loss: 879.0674438476562 = 1.0655409097671509 + 100.0 * 8.78001880645752
Epoch 430, val loss: 1.0668987035751343
Epoch 440, training loss: 879.2140502929688 = 1.06464684009552 + 100.0 * 8.781494140625
Epoch 440, val loss: 1.0660573244094849
Epoch 450, training loss: 879.2069091796875 = 1.0637022256851196 + 100.0 * 8.781432151794434
Epoch 450, val loss: 1.0651404857635498
Epoch 460, training loss: 879.2070922851562 = 1.0627895593643188 + 100.0 * 8.781442642211914
Epoch 460, val loss: 1.0642807483673096
Epoch 470, training loss: 879.59912109375 = 1.0618247985839844 + 100.0 * 8.785372734069824
Epoch 470, val loss: 1.0633500814437866
Epoch 480, training loss: 880.241455078125 = 1.0608904361724854 + 100.0 * 8.791805267333984
Epoch 480, val loss: 1.0624513626098633
Epoch 490, training loss: 880.6774291992188 = 1.0599287748336792 + 100.0 * 8.796175003051758
Epoch 490, val loss: 1.06153404712677
Epoch 500, training loss: 880.8714599609375 = 1.0589418411254883 + 100.0 * 8.798125267028809
Epoch 500, val loss: 1.0605800151824951
Epoch 510, training loss: 880.878173828125 = 1.0579243898391724 + 100.0 * 8.798202514648438
Epoch 510, val loss: 1.059611201286316
Epoch 520, training loss: 881.4612426757812 = 1.0569056272506714 + 100.0 * 8.804043769836426
Epoch 520, val loss: 1.0586451292037964
Epoch 530, training loss: 880.571533203125 = 1.055795669555664 + 100.0 * 8.795157432556152
Epoch 530, val loss: 1.057588815689087
Epoch 540, training loss: 880.9205932617188 = 1.0547304153442383 + 100.0 * 8.79865837097168
Epoch 540, val loss: 1.0565615892410278
Epoch 550, training loss: 881.4444580078125 = 1.0536731481552124 + 100.0 * 8.803908348083496
Epoch 550, val loss: 1.0555585622787476
Epoch 560, training loss: 882.0479736328125 = 1.0526026487350464 + 100.0 * 8.809953689575195
Epoch 560, val loss: 1.0545371770858765
Epoch 570, training loss: 882.3275146484375 = 1.0514748096466064 + 100.0 * 8.812760353088379
Epoch 570, val loss: 1.0534498691558838
Epoch 580, training loss: 882.7274780273438 = 1.0503442287445068 + 100.0 * 8.816771507263184
Epoch 580, val loss: 1.052382230758667
Epoch 590, training loss: 882.9968872070312 = 1.0491859912872314 + 100.0 * 8.819477081298828
Epoch 590, val loss: 1.0512754917144775
Epoch 600, training loss: 883.3713989257812 = 1.0480151176452637 + 100.0 * 8.823233604431152
Epoch 600, val loss: 1.0501585006713867
Epoch 610, training loss: 883.4693603515625 = 1.0468019247055054 + 100.0 * 8.824225425720215
Epoch 610, val loss: 1.04901123046875
Epoch 620, training loss: 884.2672119140625 = 1.045620322227478 + 100.0 * 8.832216262817383
Epoch 620, val loss: 1.047881841659546
Epoch 630, training loss: 884.169189453125 = 1.0443726778030396 + 100.0 * 8.83124828338623
Epoch 630, val loss: 1.0466760396957397
Epoch 640, training loss: 883.5238037109375 = 1.0431052446365356 + 100.0 * 8.824807167053223
Epoch 640, val loss: 1.0454773902893066
Epoch 650, training loss: 882.891357421875 = 1.041826605796814 + 100.0 * 8.818495750427246
Epoch 650, val loss: 1.0442726612091064
Epoch 660, training loss: 884.2954711914062 = 1.0406785011291504 + 100.0 * 8.832548141479492
Epoch 660, val loss: 1.0431437492370605
Epoch 670, training loss: 884.3547973632812 = 1.0393271446228027 + 100.0 * 8.833154678344727
Epoch 670, val loss: 1.0418870449066162
Epoch 680, training loss: 883.7157592773438 = 1.0380514860153198 + 100.0 * 8.826777458190918
Epoch 680, val loss: 1.0406540632247925
Epoch 690, training loss: 884.946533203125 = 1.0367459058761597 + 100.0 * 8.83909797668457
Epoch 690, val loss: 1.0394209623336792
Epoch 700, training loss: 884.4572143554688 = 1.0353868007659912 + 100.0 * 8.83421802520752
Epoch 700, val loss: 1.0381380319595337
Epoch 710, training loss: 885.239013671875 = 1.0340492725372314 + 100.0 * 8.842049598693848
Epoch 710, val loss: 1.0368601083755493
Epoch 720, training loss: 885.4675903320312 = 1.0327057838439941 + 100.0 * 8.844348907470703
Epoch 720, val loss: 1.0355720520019531
Epoch 730, training loss: 886.010986328125 = 1.031348705291748 + 100.0 * 8.849796295166016
Epoch 730, val loss: 1.0342763662338257
Epoch 740, training loss: 886.674560546875 = 1.0300191640853882 + 100.0 * 8.8564453125
Epoch 740, val loss: 1.0330066680908203
Epoch 750, training loss: 886.7723388671875 = 1.028635025024414 + 100.0 * 8.857437133789062
Epoch 750, val loss: 1.0316873788833618
Epoch 760, training loss: 886.5180053710938 = 1.0272144079208374 + 100.0 * 8.854907989501953
Epoch 760, val loss: 1.0303186178207397
Epoch 770, training loss: 887.0287475585938 = 1.0257548093795776 + 100.0 * 8.860030174255371
Epoch 770, val loss: 1.0289463996887207
Epoch 780, training loss: 887.06103515625 = 1.0243364572525024 + 100.0 * 8.860366821289062
Epoch 780, val loss: 1.0275951623916626
Epoch 790, training loss: 887.3421630859375 = 1.022876262664795 + 100.0 * 8.863192558288574
Epoch 790, val loss: 1.0261929035186768
Epoch 800, training loss: 888.6801147460938 = 1.0214308500289917 + 100.0 * 8.8765869140625
Epoch 800, val loss: 1.024804949760437
Epoch 810, training loss: 888.6309814453125 = 1.0199201107025146 + 100.0 * 8.876111030578613
Epoch 810, val loss: 1.0233795642852783
Epoch 820, training loss: 888.7303466796875 = 1.0184165239334106 + 100.0 * 8.877119064331055
Epoch 820, val loss: 1.0219309329986572
Epoch 830, training loss: 889.2845458984375 = 1.016876459121704 + 100.0 * 8.882676124572754
Epoch 830, val loss: 1.0204710960388184
Epoch 840, training loss: 889.8274536132812 = 1.0153404474258423 + 100.0 * 8.888121604919434
Epoch 840, val loss: 1.0189956426620483
Epoch 850, training loss: 890.0189208984375 = 1.0137180089950562 + 100.0 * 8.89005184173584
Epoch 850, val loss: 1.01747727394104
Epoch 860, training loss: 890.2070922851562 = 1.0121233463287354 + 100.0 * 8.891949653625488
Epoch 860, val loss: 1.0159233808517456
Epoch 870, training loss: 890.3152465820312 = 1.0104625225067139 + 100.0 * 8.893048286437988
Epoch 870, val loss: 1.014352798461914
Epoch 880, training loss: 890.4404296875 = 1.0087817907333374 + 100.0 * 8.894316673278809
Epoch 880, val loss: 1.0127484798431396
Epoch 890, training loss: 890.6774291992188 = 1.0070979595184326 + 100.0 * 8.896703720092773
Epoch 890, val loss: 1.011123776435852
Epoch 900, training loss: 890.9671020507812 = 1.0053409337997437 + 100.0 * 8.899617195129395
Epoch 900, val loss: 1.009446144104004
Epoch 910, training loss: 891.35791015625 = 1.0035454034805298 + 100.0 * 8.903543472290039
Epoch 910, val loss: 1.007741093635559
Epoch 920, training loss: 891.7553100585938 = 1.0017582178115845 + 100.0 * 8.907535552978516
Epoch 920, val loss: 1.0060343742370605
Epoch 930, training loss: 891.6875610351562 = 0.9998999834060669 + 100.0 * 8.906876564025879
Epoch 930, val loss: 1.0042634010314941
Epoch 940, training loss: 891.6918334960938 = 0.998017430305481 + 100.0 * 8.906937599182129
Epoch 940, val loss: 1.0024669170379639
Epoch 950, training loss: 892.1348876953125 = 0.996190071105957 + 100.0 * 8.911386489868164
Epoch 950, val loss: 1.0007147789001465
Epoch 960, training loss: 891.9014282226562 = 0.9943257570266724 + 100.0 * 8.90907096862793
Epoch 960, val loss: 0.9989044070243835
Epoch 970, training loss: 892.34716796875 = 0.9924320578575134 + 100.0 * 8.91354751586914
Epoch 970, val loss: 0.9970905184745789
Epoch 980, training loss: 892.9786987304688 = 0.9905956387519836 + 100.0 * 8.919880867004395
Epoch 980, val loss: 0.9953488111495972
Epoch 990, training loss: 893.2305908203125 = 0.9886965751647949 + 100.0 * 8.922418594360352
Epoch 990, val loss: 0.9935300350189209
Epoch 1000, training loss: 893.510498046875 = 0.9868161678314209 + 100.0 * 8.925236701965332
Epoch 1000, val loss: 0.9917282462120056
Epoch 1010, training loss: 893.9157104492188 = 0.9849067330360413 + 100.0 * 8.92930793762207
Epoch 1010, val loss: 0.98993980884552
Epoch 1020, training loss: 892.694580078125 = 0.9829519391059875 + 100.0 * 8.917116165161133
Epoch 1020, val loss: 0.9880464673042297
Epoch 1030, training loss: 892.8098754882812 = 0.9811263084411621 + 100.0 * 8.91828727722168
Epoch 1030, val loss: 0.9862738251686096
Epoch 1040, training loss: 892.984375 = 0.9792299270629883 + 100.0 * 8.920051574707031
Epoch 1040, val loss: 0.9844663143157959
Epoch 1050, training loss: 893.5560913085938 = 0.9773014783859253 + 100.0 * 8.925787925720215
Epoch 1050, val loss: 0.9826555848121643
Epoch 1060, training loss: 894.0217895507812 = 0.9753584861755371 + 100.0 * 8.930464744567871
Epoch 1060, val loss: 0.9807899594306946
Epoch 1070, training loss: 894.1461181640625 = 0.9734081029891968 + 100.0 * 8.931727409362793
Epoch 1070, val loss: 0.9789305329322815
Epoch 1080, training loss: 894.8629760742188 = 0.9714471101760864 + 100.0 * 8.938915252685547
Epoch 1080, val loss: 0.9770604372024536
Epoch 1090, training loss: 894.1256713867188 = 0.9693732261657715 + 100.0 * 8.931563377380371
Epoch 1090, val loss: 0.9750919342041016
Epoch 1100, training loss: 894.5557861328125 = 0.9673904180526733 + 100.0 * 8.935883522033691
Epoch 1100, val loss: 0.9732054471969604
Epoch 1110, training loss: 894.99365234375 = 0.9653941988945007 + 100.0 * 8.940282821655273
Epoch 1110, val loss: 0.9712958931922913
Epoch 1120, training loss: 895.3114013671875 = 0.9633744955062866 + 100.0 * 8.943480491638184
Epoch 1120, val loss: 0.9693595170974731
Epoch 1130, training loss: 895.8163452148438 = 0.961413562297821 + 100.0 * 8.948549270629883
Epoch 1130, val loss: 0.9674659967422485
Epoch 1140, training loss: 896.3900756835938 = 0.9593919515609741 + 100.0 * 8.954306602478027
Epoch 1140, val loss: 0.9655392169952393
Epoch 1150, training loss: 896.557861328125 = 0.9573639035224915 + 100.0 * 8.956005096435547
Epoch 1150, val loss: 0.9636036157608032
Epoch 1160, training loss: 896.0017700195312 = 0.9552788734436035 + 100.0 * 8.950465202331543
Epoch 1160, val loss: 0.9616284370422363
Epoch 1170, training loss: 896.3240356445312 = 0.9532539248466492 + 100.0 * 8.953707695007324
Epoch 1170, val loss: 0.9596815705299377
Epoch 1180, training loss: 896.179931640625 = 0.9512217044830322 + 100.0 * 8.952286720275879
Epoch 1180, val loss: 0.9577128887176514
Epoch 1190, training loss: 896.6968383789062 = 0.9491457939147949 + 100.0 * 8.957476615905762
Epoch 1190, val loss: 0.955720841884613
Epoch 1200, training loss: 897.0296630859375 = 0.9470534324645996 + 100.0 * 8.96082592010498
Epoch 1200, val loss: 0.9537352323532104
Epoch 1210, training loss: 895.6861572265625 = 0.9448375701904297 + 100.0 * 8.947413444519043
Epoch 1210, val loss: 0.951587438583374
Epoch 1220, training loss: 896.8465576171875 = 0.9430299401283264 + 100.0 * 8.95903491973877
Epoch 1220, val loss: 0.9498916268348694
Epoch 1230, training loss: 895.383544921875 = 0.940898060798645 + 100.0 * 8.944426536560059
Epoch 1230, val loss: 0.9478431344032288
Epoch 1240, training loss: 896.0867919921875 = 0.9389182329177856 + 100.0 * 8.951478958129883
Epoch 1240, val loss: 0.9458941221237183
Epoch 1250, training loss: 897.3206787109375 = 0.936877965927124 + 100.0 * 8.963837623596191
Epoch 1250, val loss: 0.9440404176712036
Epoch 1260, training loss: 896.01904296875 = 0.9347063302993774 + 100.0 * 8.950843811035156
Epoch 1260, val loss: 0.9419613480567932
Epoch 1270, training loss: 897.1710815429688 = 0.9326532483100891 + 100.0 * 8.962384223937988
Epoch 1270, val loss: 0.9399840235710144
Epoch 1280, training loss: 897.9901733398438 = 0.930570662021637 + 100.0 * 8.970596313476562
Epoch 1280, val loss: 0.9379985332489014
Epoch 1290, training loss: 898.7298583984375 = 0.928455650806427 + 100.0 * 8.97801399230957
Epoch 1290, val loss: 0.9359771013259888
Epoch 1300, training loss: 898.4312133789062 = 0.9263420104980469 + 100.0 * 8.975049018859863
Epoch 1300, val loss: 0.9339616894721985
Epoch 1310, training loss: 898.8342895507812 = 0.924193263053894 + 100.0 * 8.979101181030273
Epoch 1310, val loss: 0.9319257736206055
Epoch 1320, training loss: 899.1439819335938 = 0.9220985174179077 + 100.0 * 8.982218742370605
Epoch 1320, val loss: 0.9298877120018005
Epoch 1330, training loss: 899.3098754882812 = 0.9199588298797607 + 100.0 * 8.983899116516113
Epoch 1330, val loss: 0.9278733134269714
Epoch 1340, training loss: 899.588623046875 = 0.917849600315094 + 100.0 * 8.98670768737793
Epoch 1340, val loss: 0.9258534908294678
Epoch 1350, training loss: 900.1551513671875 = 0.9156979322433472 + 100.0 * 8.99239444732666
Epoch 1350, val loss: 0.9237949252128601
Epoch 1360, training loss: 899.9658203125 = 0.9135662913322449 + 100.0 * 8.990522384643555
Epoch 1360, val loss: 0.9217313528060913
Epoch 1370, training loss: 899.916015625 = 0.9114221930503845 + 100.0 * 8.990045547485352
Epoch 1370, val loss: 0.9197062253952026
Epoch 1380, training loss: 900.3690185546875 = 0.9093021750450134 + 100.0 * 8.994597434997559
Epoch 1380, val loss: 0.917690098285675
Epoch 1390, training loss: 900.5460815429688 = 0.9071422219276428 + 100.0 * 8.996389389038086
Epoch 1390, val loss: 0.9156275987625122
Epoch 1400, training loss: 900.3607177734375 = 0.904998779296875 + 100.0 * 8.99455738067627
Epoch 1400, val loss: 0.9135866165161133
Epoch 1410, training loss: 900.6421508789062 = 0.9028947353363037 + 100.0 * 8.997392654418945
Epoch 1410, val loss: 0.9115365147590637
Epoch 1420, training loss: 900.6483764648438 = 0.9007216095924377 + 100.0 * 8.997476577758789
Epoch 1420, val loss: 0.9095116257667542
Epoch 1430, training loss: 901.1834106445312 = 0.8986203670501709 + 100.0 * 9.002847671508789
Epoch 1430, val loss: 0.9074992537498474
Epoch 1440, training loss: 901.720947265625 = 0.8965056538581848 + 100.0 * 9.008244514465332
Epoch 1440, val loss: 0.9054717421531677
Epoch 1450, training loss: 901.1228637695312 = 0.8943483233451843 + 100.0 * 9.00228500366211
Epoch 1450, val loss: 0.9034148454666138
Epoch 1460, training loss: 901.4501953125 = 0.8922047019004822 + 100.0 * 9.005579948425293
Epoch 1460, val loss: 0.9014005661010742
Epoch 1470, training loss: 901.8021850585938 = 0.8900856971740723 + 100.0 * 9.00912094116211
Epoch 1470, val loss: 0.8993678689002991
Epoch 1480, training loss: 901.8258666992188 = 0.887962818145752 + 100.0 * 9.009379386901855
Epoch 1480, val loss: 0.8973408937454224
Epoch 1490, training loss: 902.2217407226562 = 0.8858150839805603 + 100.0 * 9.013359069824219
Epoch 1490, val loss: 0.8952980041503906
Epoch 1500, training loss: 901.6292724609375 = 0.8836900591850281 + 100.0 * 9.007455825805664
Epoch 1500, val loss: 0.8933049440383911
Epoch 1510, training loss: 901.9111938476562 = 0.881570041179657 + 100.0 * 9.010295867919922
Epoch 1510, val loss: 0.8912829160690308
Epoch 1520, training loss: 902.2013549804688 = 0.879447340965271 + 100.0 * 9.013218879699707
Epoch 1520, val loss: 0.889238715171814
Epoch 1530, training loss: 902.3057250976562 = 0.877358078956604 + 100.0 * 9.014283180236816
Epoch 1530, val loss: 0.8872319459915161
Epoch 1540, training loss: 902.6781005859375 = 0.8752599954605103 + 100.0 * 9.018028259277344
Epoch 1540, val loss: 0.8852289915084839
Epoch 1550, training loss: 902.8092651367188 = 0.8731063008308411 + 100.0 * 9.01936149597168
Epoch 1550, val loss: 0.8831994533538818
Epoch 1560, training loss: 902.8695678710938 = 0.8709970712661743 + 100.0 * 9.019986152648926
Epoch 1560, val loss: 0.8811898827552795
Epoch 1570, training loss: 903.1629638671875 = 0.8688965439796448 + 100.0 * 9.022940635681152
Epoch 1570, val loss: 0.8791897296905518
Epoch 1580, training loss: 903.3527221679688 = 0.8667874932289124 + 100.0 * 9.024859428405762
Epoch 1580, val loss: 0.8771952986717224
Epoch 1590, training loss: 903.4850463867188 = 0.8647190928459167 + 100.0 * 9.026203155517578
Epoch 1590, val loss: 0.8752055168151855
Epoch 1600, training loss: 903.6653442382812 = 0.8626072406768799 + 100.0 * 9.028027534484863
Epoch 1600, val loss: 0.8732011914253235
Epoch 1610, training loss: 903.5259399414062 = 0.8605347275733948 + 100.0 * 9.026654243469238
Epoch 1610, val loss: 0.8712210655212402
Epoch 1620, training loss: 904.0297241210938 = 0.8584477305412292 + 100.0 * 9.031712532043457
Epoch 1620, val loss: 0.869179368019104
Epoch 1630, training loss: 903.0186767578125 = 0.8563166856765747 + 100.0 * 9.021623611450195
Epoch 1630, val loss: 0.8671823740005493
Epoch 1640, training loss: 901.4765625 = 0.8541788458824158 + 100.0 * 9.006223678588867
Epoch 1640, val loss: 0.865188717842102
Epoch 1650, training loss: 903.0172729492188 = 0.8522612452507019 + 100.0 * 9.021650314331055
Epoch 1650, val loss: 0.8633617758750916
Epoch 1660, training loss: 903.6453247070312 = 0.8502347469329834 + 100.0 * 9.02795124053955
Epoch 1660, val loss: 0.86146080493927
Epoch 1670, training loss: 904.1246948242188 = 0.8482398986816406 + 100.0 * 9.032764434814453
Epoch 1670, val loss: 0.8595640659332275
Epoch 1680, training loss: 904.29296875 = 0.8462039232254028 + 100.0 * 9.034467697143555
Epoch 1680, val loss: 0.8576590418815613
Epoch 1690, training loss: 904.4462890625 = 0.8441946506500244 + 100.0 * 9.03602123260498
Epoch 1690, val loss: 0.8557634353637695
Epoch 1700, training loss: 904.5486450195312 = 0.84218430519104 + 100.0 * 9.037064552307129
Epoch 1700, val loss: 0.8538650870323181
Epoch 1710, training loss: 905.0504150390625 = 0.8401821851730347 + 100.0 * 9.042101860046387
Epoch 1710, val loss: 0.8519868850708008
Epoch 1720, training loss: 905.2313842773438 = 0.8381845951080322 + 100.0 * 9.04393196105957
Epoch 1720, val loss: 0.8500737547874451
Epoch 1730, training loss: 904.9229736328125 = 0.8361638188362122 + 100.0 * 9.040867805480957
Epoch 1730, val loss: 0.8481680154800415
Epoch 1740, training loss: 905.3175659179688 = 0.8341621160507202 + 100.0 * 9.04483413696289
Epoch 1740, val loss: 0.8462786078453064
Epoch 1750, training loss: 903.8706665039062 = 0.8320662379264832 + 100.0 * 9.030385971069336
Epoch 1750, val loss: 0.8443272113800049
Epoch 1760, training loss: 903.2271118164062 = 0.8300662040710449 + 100.0 * 9.023970603942871
Epoch 1760, val loss: 0.8424591422080994
Epoch 1770, training loss: 904.5770263671875 = 0.8280941247940063 + 100.0 * 9.037489891052246
Epoch 1770, val loss: 0.8405805230140686
Epoch 1780, training loss: 904.9033813476562 = 0.8261250853538513 + 100.0 * 9.040772438049316
Epoch 1780, val loss: 0.8387439846992493
Epoch 1790, training loss: 905.3338012695312 = 0.8241465091705322 + 100.0 * 9.045096397399902
Epoch 1790, val loss: 0.8368791937828064
Epoch 1800, training loss: 905.9820556640625 = 0.8222141265869141 + 100.0 * 9.05159854888916
Epoch 1800, val loss: 0.8350425362586975
Epoch 1810, training loss: 905.7096557617188 = 0.8202462196350098 + 100.0 * 9.048893928527832
Epoch 1810, val loss: 0.8332050442695618
Epoch 1820, training loss: 906.0881958007812 = 0.8182951807975769 + 100.0 * 9.052699089050293
Epoch 1820, val loss: 0.8313602805137634
Epoch 1830, training loss: 906.5418090820312 = 0.8163744211196899 + 100.0 * 9.057254791259766
Epoch 1830, val loss: 0.8295560479164124
Epoch 1840, training loss: 906.392333984375 = 0.8144216537475586 + 100.0 * 9.055779457092285
Epoch 1840, val loss: 0.8277153372764587
Epoch 1850, training loss: 906.3525390625 = 0.8124679327011108 + 100.0 * 9.055400848388672
Epoch 1850, val loss: 0.8258689045906067
Epoch 1860, training loss: 906.1181640625 = 0.8105074763298035 + 100.0 * 9.05307674407959
Epoch 1860, val loss: 0.8240002989768982
Epoch 1870, training loss: 906.7439575195312 = 0.8085730075836182 + 100.0 * 9.059353828430176
Epoch 1870, val loss: 0.8221904039382935
Epoch 1880, training loss: 906.2471313476562 = 0.8066874146461487 + 100.0 * 9.054404258728027
Epoch 1880, val loss: 0.820397138595581
Epoch 1890, training loss: 906.5349731445312 = 0.8047788143157959 + 100.0 * 9.057302474975586
Epoch 1890, val loss: 0.8185793161392212
Epoch 1900, training loss: 906.99267578125 = 0.8028820753097534 + 100.0 * 9.061898231506348
Epoch 1900, val loss: 0.8167792558670044
Epoch 1910, training loss: 907.3814697265625 = 0.8010101318359375 + 100.0 * 9.065804481506348
Epoch 1910, val loss: 0.8149926066398621
Epoch 1920, training loss: 907.35888671875 = 0.7991324067115784 + 100.0 * 9.065597534179688
Epoch 1920, val loss: 0.8132208585739136
Epoch 1930, training loss: 907.2520751953125 = 0.7972459197044373 + 100.0 * 9.06454849243164
Epoch 1930, val loss: 0.8114742040634155
Epoch 1940, training loss: 907.6629028320312 = 0.7953997254371643 + 100.0 * 9.06867504119873
Epoch 1940, val loss: 0.8097200393676758
Epoch 1950, training loss: 907.8495483398438 = 0.7935590744018555 + 100.0 * 9.07055950164795
Epoch 1950, val loss: 0.8079653978347778
Epoch 1960, training loss: 907.9307861328125 = 0.7916924953460693 + 100.0 * 9.071391105651855
Epoch 1960, val loss: 0.8062110543251038
Epoch 1970, training loss: 907.2501831054688 = 0.7898015379905701 + 100.0 * 9.064603805541992
Epoch 1970, val loss: 0.8044353723526001
Epoch 1980, training loss: 907.4173583984375 = 0.7879880666732788 + 100.0 * 9.066293716430664
Epoch 1980, val loss: 0.8027107119560242
Epoch 1990, training loss: 907.7701416015625 = 0.7861717343330383 + 100.0 * 9.069839477539062
Epoch 1990, val loss: 0.801007866859436
Epoch 2000, training loss: 908.61181640625 = 0.784349262714386 + 100.0 * 9.078274726867676
Epoch 2000, val loss: 0.7992834448814392
Epoch 2010, training loss: 908.8848266601562 = 0.7825448513031006 + 100.0 * 9.081023216247559
Epoch 2010, val loss: 0.797548770904541
Epoch 2020, training loss: 908.212890625 = 0.7807129621505737 + 100.0 * 9.074321746826172
Epoch 2020, val loss: 0.795855700969696
Epoch 2030, training loss: 908.2998657226562 = 0.7789477705955505 + 100.0 * 9.075209617614746
Epoch 2030, val loss: 0.7941879630088806
Epoch 2040, training loss: 909.2659912109375 = 0.7771976590156555 + 100.0 * 9.084887504577637
Epoch 2040, val loss: 0.7925176024436951
Epoch 2050, training loss: 909.0693969726562 = 0.7754145860671997 + 100.0 * 9.082940101623535
Epoch 2050, val loss: 0.7908303141593933
Epoch 2060, training loss: 906.4115600585938 = 0.7735846042633057 + 100.0 * 9.056380271911621
Epoch 2060, val loss: 0.7891513109207153
Epoch 2070, training loss: 905.5750122070312 = 0.7718411684036255 + 100.0 * 9.0480318069458
Epoch 2070, val loss: 0.7874976396560669
Epoch 2080, training loss: 895.2717895507812 = 0.7698319554328918 + 100.0 * 8.945019721984863
Epoch 2080, val loss: 0.785524308681488
Epoch 2090, training loss: 882.671875 = 0.7675882577896118 + 100.0 * 8.819043159484863
Epoch 2090, val loss: 0.7838214635848999
Epoch 2100, training loss: 886.998779296875 = 0.7670832276344299 + 100.0 * 8.862317085266113
Epoch 2100, val loss: 0.7831584215164185
Epoch 2110, training loss: 895.4050903320312 = 0.7661496996879578 + 100.0 * 8.946389198303223
Epoch 2110, val loss: 0.782123863697052
Epoch 2120, training loss: 897.752197265625 = 0.7646214365959167 + 100.0 * 8.96987533569336
Epoch 2120, val loss: 0.780767023563385
Epoch 2130, training loss: 900.1109008789062 = 0.7629714608192444 + 100.0 * 8.993478775024414
Epoch 2130, val loss: 0.7791949510574341
Epoch 2140, training loss: 902.749267578125 = 0.7613869905471802 + 100.0 * 9.019878387451172
Epoch 2140, val loss: 0.7777173519134521
Epoch 2150, training loss: 903.5966796875 = 0.7597624659538269 + 100.0 * 9.028368949890137
Epoch 2150, val loss: 0.7761911749839783
Epoch 2160, training loss: 904.6829223632812 = 0.7581937909126282 + 100.0 * 9.039247512817383
Epoch 2160, val loss: 0.7747148275375366
Epoch 2170, training loss: 905.8114013671875 = 0.7565469145774841 + 100.0 * 9.050548553466797
Epoch 2170, val loss: 0.773144543170929
Epoch 2180, training loss: 906.5513916015625 = 0.754917562007904 + 100.0 * 9.057964324951172
Epoch 2180, val loss: 0.771596372127533
Epoch 2190, training loss: 905.0330200195312 = 0.7532063126564026 + 100.0 * 9.042798042297363
Epoch 2190, val loss: 0.7700127959251404
Epoch 2200, training loss: 906.0160522460938 = 0.7515840530395508 + 100.0 * 9.052644729614258
Epoch 2200, val loss: 0.7684893608093262
Epoch 2210, training loss: 906.2781982421875 = 0.7499499320983887 + 100.0 * 9.055282592773438
Epoch 2210, val loss: 0.7669678330421448
Epoch 2220, training loss: 907.7911376953125 = 0.7484185099601746 + 100.0 * 9.070426940917969
Epoch 2220, val loss: 0.7655221819877625
Epoch 2230, training loss: 908.199462890625 = 0.7468744516372681 + 100.0 * 9.074525833129883
Epoch 2230, val loss: 0.7640668153762817
Epoch 2240, training loss: 909.1082153320312 = 0.7453441023826599 + 100.0 * 9.08362865447998
Epoch 2240, val loss: 0.7626153230667114
Epoch 2250, training loss: 909.4555053710938 = 0.7437885999679565 + 100.0 * 9.087117195129395
Epoch 2250, val loss: 0.761161208152771
Epoch 2260, training loss: 909.3609008789062 = 0.7422299385070801 + 100.0 * 9.086186408996582
Epoch 2260, val loss: 0.7596905827522278
Epoch 2270, training loss: 909.9482421875 = 0.7407017946243286 + 100.0 * 9.09207534790039
Epoch 2270, val loss: 0.7582602500915527
Epoch 2280, training loss: 910.29638671875 = 0.7391661405563354 + 100.0 * 9.095572471618652
Epoch 2280, val loss: 0.756833016872406
Epoch 2290, training loss: 910.5591430664062 = 0.7376308441162109 + 100.0 * 9.098215103149414
Epoch 2290, val loss: 0.7554062008857727
Epoch 2300, training loss: 910.5877685546875 = 0.736118495464325 + 100.0 * 9.098516464233398
Epoch 2300, val loss: 0.7539766430854797
Epoch 2310, training loss: 910.7990112304688 = 0.7346299886703491 + 100.0 * 9.1006441116333
Epoch 2310, val loss: 0.7525813579559326
Epoch 2320, training loss: 911.1109619140625 = 0.7331286668777466 + 100.0 * 9.103777885437012
Epoch 2320, val loss: 0.7511742115020752
Epoch 2330, training loss: 910.4188842773438 = 0.7316356301307678 + 100.0 * 9.096872329711914
Epoch 2330, val loss: 0.7497254014015198
Epoch 2340, training loss: 910.3016967773438 = 0.7303217649459839 + 100.0 * 9.09571361541748
Epoch 2340, val loss: 0.7485304474830627
Epoch 2350, training loss: 910.0704956054688 = 0.7289420962333679 + 100.0 * 9.093415260314941
Epoch 2350, val loss: 0.7472329139709473
Epoch 2360, training loss: 910.828369140625 = 0.7275775671005249 + 100.0 * 9.101007461547852
Epoch 2360, val loss: 0.7459733486175537
Epoch 2370, training loss: 911.8320922851562 = 0.7261773347854614 + 100.0 * 9.111059188842773
Epoch 2370, val loss: 0.744676411151886
Epoch 2380, training loss: 911.8156127929688 = 0.7247706651687622 + 100.0 * 9.110908508300781
Epoch 2380, val loss: 0.743373453617096
Epoch 2390, training loss: 912.217041015625 = 0.7233840823173523 + 100.0 * 9.114936828613281
Epoch 2390, val loss: 0.7420657873153687
Epoch 2400, training loss: 912.1036376953125 = 0.7219446897506714 + 100.0 * 9.11381721496582
Epoch 2400, val loss: 0.7407159209251404
Epoch 2410, training loss: 912.4619140625 = 0.7205668687820435 + 100.0 * 9.117413520812988
Epoch 2410, val loss: 0.7394555807113647
Epoch 2420, training loss: 912.6849975585938 = 0.7191909551620483 + 100.0 * 9.119658470153809
Epoch 2420, val loss: 0.738180935382843
Epoch 2430, training loss: 912.645263671875 = 0.7178180813789368 + 100.0 * 9.119274139404297
Epoch 2430, val loss: 0.7368902564048767
Epoch 2440, training loss: 912.2142944335938 = 0.7164503931999207 + 100.0 * 9.114978790283203
Epoch 2440, val loss: 0.7356225252151489
Epoch 2450, training loss: 912.30908203125 = 0.7151252627372742 + 100.0 * 9.115939140319824
Epoch 2450, val loss: 0.7343852519989014
Epoch 2460, training loss: 912.7340087890625 = 0.7138882279396057 + 100.0 * 9.120201110839844
Epoch 2460, val loss: 0.7333176136016846
Epoch 2470, training loss: 913.7208251953125 = 0.712615430355072 + 100.0 * 9.130082130432129
Epoch 2470, val loss: 0.7320566773414612
Epoch 2480, training loss: 913.8985595703125 = 0.7111754417419434 + 100.0 * 9.131874084472656
Epoch 2480, val loss: 0.7306861281394958
Epoch 2490, training loss: 914.3239135742188 = 0.7098327279090881 + 100.0 * 9.136140823364258
Epoch 2490, val loss: 0.7294366359710693
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7144927536231884
0.863725277113671
=== training gcn model ===
Epoch 0, training loss: 1016.1698608398438 = 1.0986114740371704 + 100.0 * 10.150712013244629
Epoch 0, val loss: 1.098612904548645
Epoch 10, training loss: 973.54736328125 = 1.0986114740371704 + 100.0 * 9.7244873046875
Epoch 10, val loss: 1.098612904548645
Epoch 20, training loss: 952.7450561523438 = 1.0986114740371704 + 100.0 * 9.516464233398438
Epoch 20, val loss: 1.098612904548645
Epoch 30, training loss: 937.6736450195312 = 1.0986114740371704 + 100.0 * 9.365750312805176
Epoch 30, val loss: 1.098612904548645
Epoch 40, training loss: 926.1099853515625 = 1.0986114740371704 + 100.0 * 9.250113487243652
Epoch 40, val loss: 1.098612904548645
Epoch 50, training loss: 916.8911743164062 = 1.0986114740371704 + 100.0 * 9.157925605773926
Epoch 50, val loss: 1.098612904548645
Epoch 60, training loss: 909.4960327148438 = 1.0986114740371704 + 100.0 * 9.08397388458252
Epoch 60, val loss: 1.098612904548645
Epoch 70, training loss: 903.3847045898438 = 1.0986109972000122 + 100.0 * 9.022860527038574
Epoch 70, val loss: 1.0986127853393555
Epoch 80, training loss: 898.130615234375 = 1.098554015159607 + 100.0 * 8.970320701599121
Epoch 80, val loss: 1.0985420942306519
Epoch 90, training loss: 893.6366577148438 = 1.0981950759887695 + 100.0 * 8.925384521484375
Epoch 90, val loss: 1.0982121229171753
Epoch 100, training loss: 889.7884521484375 = 1.0977904796600342 + 100.0 * 8.886906623840332
Epoch 100, val loss: 1.097849726676941
Epoch 110, training loss: 886.5612182617188 = 1.0974016189575195 + 100.0 * 8.85463809967041
Epoch 110, val loss: 1.0975019931793213
Epoch 120, training loss: 883.823974609375 = 1.0970234870910645 + 100.0 * 8.827269554138184
Epoch 120, val loss: 1.0971640348434448
Epoch 130, training loss: 881.5718383789062 = 1.096656084060669 + 100.0 * 8.8047513961792
Epoch 130, val loss: 1.0968313217163086
Epoch 140, training loss: 879.8204956054688 = 1.0962860584259033 + 100.0 * 8.78724193572998
Epoch 140, val loss: 1.0964972972869873
Epoch 150, training loss: 878.2269287109375 = 1.0959172248840332 + 100.0 * 8.771309852600098
Epoch 150, val loss: 1.0961614847183228
Epoch 160, training loss: 876.8267211914062 = 1.0955440998077393 + 100.0 * 8.757311820983887
Epoch 160, val loss: 1.0958201885223389
Epoch 170, training loss: 875.6008911132812 = 1.0951756238937378 + 100.0 * 8.745057106018066
Epoch 170, val loss: 1.0954821109771729
Epoch 180, training loss: 874.5462036132812 = 1.0947966575622559 + 100.0 * 8.734514236450195
Epoch 180, val loss: 1.095132827758789
Epoch 190, training loss: 873.9683227539062 = 1.0944148302078247 + 100.0 * 8.728738784790039
Epoch 190, val loss: 1.094786524772644
Epoch 200, training loss: 873.365478515625 = 1.094030499458313 + 100.0 * 8.7227144241333
Epoch 200, val loss: 1.094429612159729
Epoch 210, training loss: 872.8836669921875 = 1.0936421155929565 + 100.0 * 8.717900276184082
Epoch 210, val loss: 1.0940768718719482
Epoch 220, training loss: 871.9953002929688 = 1.0932539701461792 + 100.0 * 8.709020614624023
Epoch 220, val loss: 1.0937138795852661
Epoch 230, training loss: 871.5296020507812 = 1.0928560495376587 + 100.0 * 8.704367637634277
Epoch 230, val loss: 1.0933458805084229
Epoch 240, training loss: 871.4852905273438 = 1.0924607515335083 + 100.0 * 8.703927993774414
Epoch 240, val loss: 1.0929813385009766
Epoch 250, training loss: 870.819091796875 = 1.0920380353927612 + 100.0 * 8.697270393371582
Epoch 250, val loss: 1.09259033203125
Epoch 260, training loss: 870.6464233398438 = 1.0916337966918945 + 100.0 * 8.695548057556152
Epoch 260, val loss: 1.0922108888626099
Epoch 270, training loss: 870.663330078125 = 1.0912196636199951 + 100.0 * 8.695720672607422
Epoch 270, val loss: 1.0918278694152832
Epoch 280, training loss: 870.109619140625 = 1.090667724609375 + 100.0 * 8.690189361572266
Epoch 280, val loss: 1.0913059711456299
Epoch 290, training loss: 871.616455078125 = 1.0903517007827759 + 100.0 * 8.70526123046875
Epoch 290, val loss: 1.0910214185714722
Epoch 300, training loss: 868.9008178710938 = 1.0898873805999756 + 100.0 * 8.678109169006348
Epoch 300, val loss: 1.0905804634094238
Epoch 310, training loss: 869.768798828125 = 1.089461326599121 + 100.0 * 8.686793327331543
Epoch 310, val loss: 1.09018874168396
Epoch 320, training loss: 869.4173583984375 = 1.0890071392059326 + 100.0 * 8.683283805847168
Epoch 320, val loss: 1.0897541046142578
Epoch 330, training loss: 869.7805786132812 = 1.088547945022583 + 100.0 * 8.686920166015625
Epoch 330, val loss: 1.0893290042877197
Epoch 340, training loss: 870.2381591796875 = 1.0880985260009766 + 100.0 * 8.691500663757324
Epoch 340, val loss: 1.0889097452163696
Epoch 350, training loss: 869.9376831054688 = 1.0876284837722778 + 100.0 * 8.68850040435791
Epoch 350, val loss: 1.0884687900543213
Epoch 360, training loss: 870.1569213867188 = 1.0871472358703613 + 100.0 * 8.69069766998291
Epoch 360, val loss: 1.0880168676376343
Epoch 370, training loss: 870.4093627929688 = 1.0866587162017822 + 100.0 * 8.69322681427002
Epoch 370, val loss: 1.0875639915466309
Epoch 380, training loss: 870.44384765625 = 1.0861537456512451 + 100.0 * 8.69357681274414
Epoch 380, val loss: 1.0870870351791382
Epoch 390, training loss: 870.78369140625 = 1.085625410079956 + 100.0 * 8.696980476379395
Epoch 390, val loss: 1.08659029006958
Epoch 400, training loss: 870.7684326171875 = 1.0850802659988403 + 100.0 * 8.696833610534668
Epoch 400, val loss: 1.0860705375671387
Epoch 410, training loss: 870.84521484375 = 1.0845078229904175 + 100.0 * 8.697607040405273
Epoch 410, val loss: 1.0855330228805542
Epoch 420, training loss: 869.8118896484375 = 1.0839287042617798 + 100.0 * 8.68727970123291
Epoch 420, val loss: 1.0849695205688477
Epoch 430, training loss: 871.217041015625 = 1.0833486318588257 + 100.0 * 8.701336860656738
Epoch 430, val loss: 1.0844297409057617
Epoch 440, training loss: 871.1011352539062 = 1.0827404260635376 + 100.0 * 8.700183868408203
Epoch 440, val loss: 1.0838638544082642
Epoch 450, training loss: 870.4500732421875 = 1.082104206085205 + 100.0 * 8.693679809570312
Epoch 450, val loss: 1.0832380056381226
Epoch 460, training loss: 870.2365112304688 = 1.0814796686172485 + 100.0 * 8.691550254821777
Epoch 460, val loss: 1.0826523303985596
Epoch 470, training loss: 871.1014404296875 = 1.0808690786361694 + 100.0 * 8.70020580291748
Epoch 470, val loss: 1.082075834274292
Epoch 480, training loss: 871.5178833007812 = 1.0802481174468994 + 100.0 * 8.704376220703125
Epoch 480, val loss: 1.0814894437789917
Epoch 490, training loss: 872.1199340820312 = 1.0796107053756714 + 100.0 * 8.710403442382812
Epoch 490, val loss: 1.0808897018432617
Epoch 500, training loss: 872.4678955078125 = 1.0789693593978882 + 100.0 * 8.713889122009277
Epoch 500, val loss: 1.080282211303711
Epoch 510, training loss: 872.6220703125 = 1.0783206224441528 + 100.0 * 8.715437889099121
Epoch 510, val loss: 1.0796736478805542
Epoch 520, training loss: 872.952880859375 = 1.0776784420013428 + 100.0 * 8.718751907348633
Epoch 520, val loss: 1.079068660736084
Epoch 530, training loss: 872.9783325195312 = 1.0770233869552612 + 100.0 * 8.719013214111328
Epoch 530, val loss: 1.0784469842910767
Epoch 540, training loss: 873.0111694335938 = 1.0763546228408813 + 100.0 * 8.719347953796387
Epoch 540, val loss: 1.0778204202651978
Epoch 550, training loss: 873.33837890625 = 1.0756999254226685 + 100.0 * 8.722626686096191
Epoch 550, val loss: 1.0772079229354858
Epoch 560, training loss: 873.65869140625 = 1.0750418901443481 + 100.0 * 8.725836753845215
Epoch 560, val loss: 1.0765994787216187
Epoch 570, training loss: 874.0330810546875 = 1.0743823051452637 + 100.0 * 8.729586601257324
Epoch 570, val loss: 1.0759780406951904
Epoch 580, training loss: 873.8709716796875 = 1.0736738443374634 + 100.0 * 8.727972984313965
Epoch 580, val loss: 1.075322151184082
Epoch 590, training loss: 874.1231079101562 = 1.0729907751083374 + 100.0 * 8.730501174926758
Epoch 590, val loss: 1.0746880769729614
Epoch 600, training loss: 874.3013305664062 = 1.0723096132278442 + 100.0 * 8.732290267944336
Epoch 600, val loss: 1.0740410089492798
Epoch 610, training loss: 874.6231689453125 = 1.071614146232605 + 100.0 * 8.735515594482422
Epoch 610, val loss: 1.0733935832977295
Epoch 620, training loss: 875.2493286132812 = 1.0709205865859985 + 100.0 * 8.74178409576416
Epoch 620, val loss: 1.0727406740188599
Epoch 630, training loss: 875.6634521484375 = 1.0701948404312134 + 100.0 * 8.745932579040527
Epoch 630, val loss: 1.072056531906128
Epoch 640, training loss: 875.8225708007812 = 1.0694609880447388 + 100.0 * 8.747530937194824
Epoch 640, val loss: 1.0713717937469482
Epoch 650, training loss: 876.2545166015625 = 1.0687224864959717 + 100.0 * 8.75185775756836
Epoch 650, val loss: 1.0706838369369507
Epoch 660, training loss: 875.3075561523438 = 1.067968726158142 + 100.0 * 8.742395401000977
Epoch 660, val loss: 1.0699635744094849
Epoch 670, training loss: 876.1058959960938 = 1.0672022104263306 + 100.0 * 8.750387191772461
Epoch 670, val loss: 1.0692650079727173
Epoch 680, training loss: 876.194580078125 = 1.0663965940475464 + 100.0 * 8.75128173828125
Epoch 680, val loss: 1.068494200706482
Epoch 690, training loss: 875.9530639648438 = 1.0656750202178955 + 100.0 * 8.748873710632324
Epoch 690, val loss: 1.0678212642669678
Epoch 700, training loss: 876.1199951171875 = 1.0648967027664185 + 100.0 * 8.750551223754883
Epoch 700, val loss: 1.0671026706695557
Epoch 710, training loss: 876.5689086914062 = 1.064145803451538 + 100.0 * 8.755047798156738
Epoch 710, val loss: 1.0663968324661255
Epoch 720, training loss: 877.2607421875 = 1.0633866786956787 + 100.0 * 8.76197338104248
Epoch 720, val loss: 1.065683364868164
Epoch 730, training loss: 877.559814453125 = 1.0626287460327148 + 100.0 * 8.764971733093262
Epoch 730, val loss: 1.0649675130844116
Epoch 740, training loss: 877.7518920898438 = 1.0618643760681152 + 100.0 * 8.766900062561035
Epoch 740, val loss: 1.0642503499984741
Epoch 750, training loss: 878.05419921875 = 1.0611037015914917 + 100.0 * 8.769930839538574
Epoch 750, val loss: 1.06352961063385
Epoch 760, training loss: 877.6210327148438 = 1.0603423118591309 + 100.0 * 8.765606880187988
Epoch 760, val loss: 1.062814474105835
Epoch 770, training loss: 877.58935546875 = 1.0595730543136597 + 100.0 * 8.765297889709473
Epoch 770, val loss: 1.0620918273925781
Epoch 780, training loss: 878.1580810546875 = 1.058807373046875 + 100.0 * 8.770992279052734
Epoch 780, val loss: 1.061383605003357
Epoch 790, training loss: 878.3653564453125 = 1.058030605316162 + 100.0 * 8.773073196411133
Epoch 790, val loss: 1.0606547594070435
Epoch 800, training loss: 879.07275390625 = 1.0572470426559448 + 100.0 * 8.780155181884766
Epoch 800, val loss: 1.0599156618118286
Epoch 810, training loss: 879.1613159179688 = 1.0564485788345337 + 100.0 * 8.781048774719238
Epoch 810, val loss: 1.0591729879379272
Epoch 820, training loss: 879.1148681640625 = 1.0556361675262451 + 100.0 * 8.78059196472168
Epoch 820, val loss: 1.0584138631820679
Epoch 830, training loss: 879.5504760742188 = 1.0548245906829834 + 100.0 * 8.784956932067871
Epoch 830, val loss: 1.0576488971710205
Epoch 840, training loss: 880.1430053710938 = 1.0540214776992798 + 100.0 * 8.790889739990234
Epoch 840, val loss: 1.0568876266479492
Epoch 850, training loss: 880.1072387695312 = 1.0531879663467407 + 100.0 * 8.79054069519043
Epoch 850, val loss: 1.0561026334762573
Epoch 860, training loss: 880.6205444335938 = 1.0523567199707031 + 100.0 * 8.795681953430176
Epoch 860, val loss: 1.0553194284439087
Epoch 870, training loss: 880.8765258789062 = 1.0514899492263794 + 100.0 * 8.798250198364258
Epoch 870, val loss: 1.0545077323913574
Epoch 880, training loss: 880.5050048828125 = 1.0506683588027954 + 100.0 * 8.794543266296387
Epoch 880, val loss: 1.0537397861480713
Epoch 890, training loss: 880.8075561523438 = 1.0498583316802979 + 100.0 * 8.797576904296875
Epoch 890, val loss: 1.0529732704162598
Epoch 900, training loss: 881.2528076171875 = 1.0490272045135498 + 100.0 * 8.802038192749023
Epoch 900, val loss: 1.0521820783615112
Epoch 910, training loss: 881.3875732421875 = 1.0481791496276855 + 100.0 * 8.803394317626953
Epoch 910, val loss: 1.051381230354309
Epoch 920, training loss: 881.5084838867188 = 1.0473397970199585 + 100.0 * 8.804611206054688
Epoch 920, val loss: 1.0505841970443726
Epoch 930, training loss: 881.6072387695312 = 1.0464593172073364 + 100.0 * 8.805607795715332
Epoch 930, val loss: 1.0497511625289917
Epoch 940, training loss: 881.5663452148438 = 1.0456013679504395 + 100.0 * 8.805207252502441
Epoch 940, val loss: 1.048945665359497
Epoch 950, training loss: 881.9646606445312 = 1.0447765588760376 + 100.0 * 8.809198379516602
Epoch 950, val loss: 1.0481643676757812
Epoch 960, training loss: 882.400634765625 = 1.0439348220825195 + 100.0 * 8.813567161560059
Epoch 960, val loss: 1.0473836660385132
Epoch 970, training loss: 882.5426025390625 = 1.0430982112884521 + 100.0 * 8.814994812011719
Epoch 970, val loss: 1.04657781124115
Epoch 980, training loss: 882.5021362304688 = 1.0422279834747314 + 100.0 * 8.81459903717041
Epoch 980, val loss: 1.0457407236099243
Epoch 990, training loss: 883.0206298828125 = 1.0413689613342285 + 100.0 * 8.819792747497559
Epoch 990, val loss: 1.044940710067749
Epoch 1000, training loss: 883.3689575195312 = 1.040511131286621 + 100.0 * 8.823284149169922
Epoch 1000, val loss: 1.0441240072250366
Epoch 1010, training loss: 883.2761840820312 = 1.0396019220352173 + 100.0 * 8.822365760803223
Epoch 1010, val loss: 1.0432828664779663
Epoch 1020, training loss: 883.2482299804688 = 1.0387489795684814 + 100.0 * 8.822094917297363
Epoch 1020, val loss: 1.0424762964248657
Epoch 1030, training loss: 883.3116455078125 = 1.0378867387771606 + 100.0 * 8.822737693786621
Epoch 1030, val loss: 1.0416371822357178
Epoch 1040, training loss: 883.9132690429688 = 1.0370349884033203 + 100.0 * 8.82876205444336
Epoch 1040, val loss: 1.040826678276062
Epoch 1050, training loss: 883.5819702148438 = 1.0361356735229492 + 100.0 * 8.825458526611328
Epoch 1050, val loss: 1.0399892330169678
Epoch 1060, training loss: 883.615234375 = 1.0352405309677124 + 100.0 * 8.825799942016602
Epoch 1060, val loss: 1.0391569137573242
Epoch 1070, training loss: 884.2489624023438 = 1.0343749523162842 + 100.0 * 8.832145690917969
Epoch 1070, val loss: 1.0383270978927612
Epoch 1080, training loss: 884.5979614257812 = 1.0334879159927368 + 100.0 * 8.835644721984863
Epoch 1080, val loss: 1.0374691486358643
Epoch 1090, training loss: 884.9066162109375 = 1.0325829982757568 + 100.0 * 8.838740348815918
Epoch 1090, val loss: 1.036623239517212
Epoch 1100, training loss: 884.5491333007812 = 1.0316911935806274 + 100.0 * 8.835174560546875
Epoch 1100, val loss: 1.0357815027236938
Epoch 1110, training loss: 885.0007934570312 = 1.030826210975647 + 100.0 * 8.839699745178223
Epoch 1110, val loss: 1.034945011138916
Epoch 1120, training loss: 885.4695434570312 = 1.0299172401428223 + 100.0 * 8.844396591186523
Epoch 1120, val loss: 1.0340831279754639
Epoch 1130, training loss: 885.7597045898438 = 1.0290086269378662 + 100.0 * 8.847307205200195
Epoch 1130, val loss: 1.0332111120224
Epoch 1140, training loss: 885.6659545898438 = 1.0281072854995728 + 100.0 * 8.846378326416016
Epoch 1140, val loss: 1.0323584079742432
Epoch 1150, training loss: 886.1326293945312 = 1.0272045135498047 + 100.0 * 8.851054191589355
Epoch 1150, val loss: 1.0314925909042358
Epoch 1160, training loss: 885.3032836914062 = 1.026227355003357 + 100.0 * 8.84277057647705
Epoch 1160, val loss: 1.0305674076080322
Epoch 1170, training loss: 885.154541015625 = 1.0253483057022095 + 100.0 * 8.841292381286621
Epoch 1170, val loss: 1.0297350883483887
Epoch 1180, training loss: 885.7467041015625 = 1.0244632959365845 + 100.0 * 8.847222328186035
Epoch 1180, val loss: 1.028873324394226
Epoch 1190, training loss: 886.2359008789062 = 1.023533582687378 + 100.0 * 8.852123260498047
Epoch 1190, val loss: 1.027993083000183
Epoch 1200, training loss: 886.7741088867188 = 1.0226202011108398 + 100.0 * 8.857514381408691
Epoch 1200, val loss: 1.027123212814331
Epoch 1210, training loss: 887.0386962890625 = 1.0216861963272095 + 100.0 * 8.860170364379883
Epoch 1210, val loss: 1.026227593421936
Epoch 1220, training loss: 887.0111083984375 = 1.0207754373550415 + 100.0 * 8.859903335571289
Epoch 1220, val loss: 1.0253503322601318
Epoch 1230, training loss: 887.2809448242188 = 1.019838571548462 + 100.0 * 8.862610816955566
Epoch 1230, val loss: 1.024449348449707
Epoch 1240, training loss: 887.1695556640625 = 1.0189001560211182 + 100.0 * 8.861506462097168
Epoch 1240, val loss: 1.0235735177993774
Epoch 1250, training loss: 887.3384399414062 = 1.017972707748413 + 100.0 * 8.863204956054688
Epoch 1250, val loss: 1.022668480873108
Epoch 1260, training loss: 887.3973999023438 = 1.0170644521713257 + 100.0 * 8.863802909851074
Epoch 1260, val loss: 1.0217924118041992
Epoch 1270, training loss: 887.7782592773438 = 1.0161296129226685 + 100.0 * 8.867621421813965
Epoch 1270, val loss: 1.0209076404571533
Epoch 1280, training loss: 887.7199096679688 = 1.0151900053024292 + 100.0 * 8.867047309875488
Epoch 1280, val loss: 1.0200059413909912
Epoch 1290, training loss: 887.8823852539062 = 1.0142664909362793 + 100.0 * 8.868680953979492
Epoch 1290, val loss: 1.0191175937652588
Epoch 1300, training loss: 888.0445556640625 = 1.0133506059646606 + 100.0 * 8.870311737060547
Epoch 1300, val loss: 1.0182380676269531
Epoch 1310, training loss: 888.1673583984375 = 1.0124030113220215 + 100.0 * 8.871549606323242
Epoch 1310, val loss: 1.0173187255859375
Epoch 1320, training loss: 888.4141845703125 = 1.0114535093307495 + 100.0 * 8.874027252197266
Epoch 1320, val loss: 1.016419768333435
Epoch 1330, training loss: 888.3421020507812 = 1.0105127096176147 + 100.0 * 8.873315811157227
Epoch 1330, val loss: 1.0155242681503296
Epoch 1340, training loss: 888.4891967773438 = 1.0095746517181396 + 100.0 * 8.874795913696289
Epoch 1340, val loss: 1.0146225690841675
Epoch 1350, training loss: 888.6416625976562 = 1.0086225271224976 + 100.0 * 8.876330375671387
Epoch 1350, val loss: 1.0136977434158325
Epoch 1360, training loss: 888.5064086914062 = 1.0076524019241333 + 100.0 * 8.874987602233887
Epoch 1360, val loss: 1.0127489566802979
Epoch 1370, training loss: 888.8446044921875 = 1.0067191123962402 + 100.0 * 8.878378868103027
Epoch 1370, val loss: 1.0118600130081177
Epoch 1380, training loss: 888.9300537109375 = 1.0057227611541748 + 100.0 * 8.879242897033691
Epoch 1380, val loss: 1.0109083652496338
Epoch 1390, training loss: 889.1530151367188 = 1.0047967433929443 + 100.0 * 8.881482124328613
Epoch 1390, val loss: 1.010031819343567
Epoch 1400, training loss: 889.7260131835938 = 1.0039111375808716 + 100.0 * 8.887221336364746
Epoch 1400, val loss: 1.0091627836227417
Epoch 1410, training loss: 889.5435180664062 = 1.0029797554016113 + 100.0 * 8.885405540466309
Epoch 1410, val loss: 1.0082640647888184
Epoch 1420, training loss: 889.7344360351562 = 1.0020451545715332 + 100.0 * 8.887323379516602
Epoch 1420, val loss: 1.0073587894439697
Epoch 1430, training loss: 889.7526245117188 = 1.001064419746399 + 100.0 * 8.887516021728516
Epoch 1430, val loss: 1.0064245462417603
Epoch 1440, training loss: 890.4185180664062 = 1.000123143196106 + 100.0 * 8.894184112548828
Epoch 1440, val loss: 1.005510687828064
Epoch 1450, training loss: 890.2766723632812 = 0.9991905689239502 + 100.0 * 8.89277458190918
Epoch 1450, val loss: 1.0045998096466064
Epoch 1460, training loss: 890.021240234375 = 0.9982590079307556 + 100.0 * 8.890230178833008
Epoch 1460, val loss: 1.0037094354629517
Epoch 1470, training loss: 890.34619140625 = 0.9973240494728088 + 100.0 * 8.893488883972168
Epoch 1470, val loss: 1.0028083324432373
Epoch 1480, training loss: 890.8887939453125 = 0.9964101910591125 + 100.0 * 8.898923873901367
Epoch 1480, val loss: 1.0019323825836182
Epoch 1490, training loss: 890.780029296875 = 0.9954866766929626 + 100.0 * 8.897845268249512
Epoch 1490, val loss: 1.0010414123535156
Epoch 1500, training loss: 890.8179321289062 = 0.9945437908172607 + 100.0 * 8.898233413696289
Epoch 1500, val loss: 1.000145673751831
Epoch 1510, training loss: 890.8435668945312 = 0.9936171174049377 + 100.0 * 8.898499488830566
Epoch 1510, val loss: 0.9992472529411316
Epoch 1520, training loss: 891.187255859375 = 0.9927107095718384 + 100.0 * 8.901945114135742
Epoch 1520, val loss: 0.998367965221405
Epoch 1530, training loss: 891.5238037109375 = 0.9917895793914795 + 100.0 * 8.905320167541504
Epoch 1530, val loss: 0.9974899888038635
Epoch 1540, training loss: 891.4398803710938 = 0.990872323513031 + 100.0 * 8.90449047088623
Epoch 1540, val loss: 0.996616542339325
Epoch 1550, training loss: 891.7276000976562 = 0.9900567531585693 + 100.0 * 8.90737533569336
Epoch 1550, val loss: 0.9958082437515259
Epoch 1560, training loss: 890.6197509765625 = 0.9889534115791321 + 100.0 * 8.896307945251465
Epoch 1560, val loss: 0.9947829842567444
Epoch 1570, training loss: 890.4030151367188 = 0.9881937503814697 + 100.0 * 8.894147872924805
Epoch 1570, val loss: 0.9940345287322998
Epoch 1580, training loss: 890.1860961914062 = 0.9873338937759399 + 100.0 * 8.891987800598145
Epoch 1580, val loss: 0.9931617975234985
Epoch 1590, training loss: 889.9971923828125 = 0.9864484071731567 + 100.0 * 8.890107154846191
Epoch 1590, val loss: 0.9923409819602966
Epoch 1600, training loss: 890.75146484375 = 0.9855636954307556 + 100.0 * 8.897659301757812
Epoch 1600, val loss: 0.9914876222610474
Epoch 1610, training loss: 891.3684692382812 = 0.9846899509429932 + 100.0 * 8.903838157653809
Epoch 1610, val loss: 0.9906467795372009
Epoch 1620, training loss: 891.8897094726562 = 0.9837998151779175 + 100.0 * 8.909058570861816
Epoch 1620, val loss: 0.9897946119308472
Epoch 1630, training loss: 891.1901245117188 = 0.9828770756721497 + 100.0 * 8.90207290649414
Epoch 1630, val loss: 0.9889085292816162
Epoch 1640, training loss: 891.54443359375 = 0.9819955229759216 + 100.0 * 8.905624389648438
Epoch 1640, val loss: 0.9880647659301758
Epoch 1650, training loss: 892.4765625 = 0.9811401963233948 + 100.0 * 8.91495418548584
Epoch 1650, val loss: 0.9872287511825562
Epoch 1660, training loss: 892.8975219726562 = 0.9802711009979248 + 100.0 * 8.919172286987305
Epoch 1660, val loss: 0.9863910675048828
Epoch 1670, training loss: 893.154541015625 = 0.9793910384178162 + 100.0 * 8.921751976013184
Epoch 1670, val loss: 0.9855404496192932
Epoch 1680, training loss: 892.2146606445312 = 0.9785639643669128 + 100.0 * 8.912361145019531
Epoch 1680, val loss: 0.9847521185874939
Epoch 1690, training loss: 892.9688720703125 = 0.9778394103050232 + 100.0 * 8.919910430908203
Epoch 1690, val loss: 0.9840180277824402
Epoch 1700, training loss: 893.5069580078125 = 0.9769900441169739 + 100.0 * 8.925299644470215
Epoch 1700, val loss: 0.9832198619842529
Epoch 1710, training loss: 893.5966186523438 = 0.976142406463623 + 100.0 * 8.926204681396484
Epoch 1710, val loss: 0.9824116826057434
Epoch 1720, training loss: 894.04052734375 = 0.9753492474555969 + 100.0 * 8.930651664733887
Epoch 1720, val loss: 0.9816524982452393
Epoch 1730, training loss: 894.7750244140625 = 0.9745384454727173 + 100.0 * 8.938004493713379
Epoch 1730, val loss: 0.9808744192123413
Epoch 1740, training loss: 894.7210083007812 = 0.973684549331665 + 100.0 * 8.93747329711914
Epoch 1740, val loss: 0.9800466895103455
Epoch 1750, training loss: 895.0863037109375 = 0.972838282585144 + 100.0 * 8.941134452819824
Epoch 1750, val loss: 0.9792439341545105
Epoch 1760, training loss: 895.5892333984375 = 0.9720059633255005 + 100.0 * 8.946172714233398
Epoch 1760, val loss: 0.9784396886825562
Epoch 1770, training loss: 895.39208984375 = 0.9711788892745972 + 100.0 * 8.944209098815918
Epoch 1770, val loss: 0.9776396751403809
Epoch 1780, training loss: 895.066162109375 = 0.9703550934791565 + 100.0 * 8.940958023071289
Epoch 1780, val loss: 0.9768564105033875
Epoch 1790, training loss: 895.499267578125 = 0.9695467352867126 + 100.0 * 8.945297241210938
Epoch 1790, val loss: 0.976078987121582
Epoch 1800, training loss: 893.7852172851562 = 0.9687025547027588 + 100.0 * 8.928165435791016
Epoch 1800, val loss: 0.9752782583236694
Epoch 1810, training loss: 893.0491333007812 = 0.9679033160209656 + 100.0 * 8.920812606811523
Epoch 1810, val loss: 0.9745090007781982
Epoch 1820, training loss: 893.9819946289062 = 0.9670884013175964 + 100.0 * 8.93014907836914
Epoch 1820, val loss: 0.973722517490387
Epoch 1830, training loss: 895.0570678710938 = 0.9663289189338684 + 100.0 * 8.94090747833252
Epoch 1830, val loss: 0.9730001091957092
Epoch 1840, training loss: 895.6909790039062 = 0.9655556082725525 + 100.0 * 8.947254180908203
Epoch 1840, val loss: 0.9722719788551331
Epoch 1850, training loss: 896.2218017578125 = 0.9647859334945679 + 100.0 * 8.952569961547852
Epoch 1850, val loss: 0.9715301990509033
Epoch 1860, training loss: 896.3034057617188 = 0.9640026092529297 + 100.0 * 8.953393936157227
Epoch 1860, val loss: 0.9707725644111633
Epoch 1870, training loss: 896.059326171875 = 0.9632081985473633 + 100.0 * 8.950961112976074
Epoch 1870, val loss: 0.9700197577476501
Epoch 1880, training loss: 896.7105712890625 = 0.9624248147010803 + 100.0 * 8.957481384277344
Epoch 1880, val loss: 0.9692745208740234
Epoch 1890, training loss: 896.512939453125 = 0.9616537690162659 + 100.0 * 8.955513000488281
Epoch 1890, val loss: 0.9685341119766235
Epoch 1900, training loss: 896.5829467773438 = 0.9608769416809082 + 100.0 * 8.956220626831055
Epoch 1900, val loss: 0.9677995443344116
Epoch 1910, training loss: 896.8322143554688 = 0.9601048827171326 + 100.0 * 8.958721160888672
Epoch 1910, val loss: 0.967059850692749
Epoch 1920, training loss: 896.6672973632812 = 0.9593140482902527 + 100.0 * 8.957079887390137
Epoch 1920, val loss: 0.9663171768188477
Epoch 1930, training loss: 897.0841674804688 = 0.95858234167099 + 100.0 * 8.96125602722168
Epoch 1930, val loss: 0.965593159198761
Epoch 1940, training loss: 897.4922485351562 = 0.9578524827957153 + 100.0 * 8.965344429016113
Epoch 1940, val loss: 0.964906632900238
Epoch 1950, training loss: 897.6697998046875 = 0.9571083784103394 + 100.0 * 8.967126846313477
Epoch 1950, val loss: 0.9642031788825989
Epoch 1960, training loss: 897.6275634765625 = 0.9563573598861694 + 100.0 * 8.96671199798584
Epoch 1960, val loss: 0.9634839296340942
Epoch 1970, training loss: 897.6995239257812 = 0.9556141495704651 + 100.0 * 8.967438697814941
Epoch 1970, val loss: 0.9627882838249207
Epoch 1980, training loss: 898.0313110351562 = 0.9549031853675842 + 100.0 * 8.97076416015625
Epoch 1980, val loss: 0.9621058106422424
Epoch 1990, training loss: 898.0029296875 = 0.9541783928871155 + 100.0 * 8.970487594604492
Epoch 1990, val loss: 0.9614097476005554
Epoch 2000, training loss: 898.2689208984375 = 0.9534597396850586 + 100.0 * 8.97315502166748
Epoch 2000, val loss: 0.9607343077659607
Epoch 2010, training loss: 898.0872192382812 = 0.9527246952056885 + 100.0 * 8.971344947814941
Epoch 2010, val loss: 0.9600345492362976
Epoch 2020, training loss: 897.9075317382812 = 0.9520324468612671 + 100.0 * 8.969554901123047
Epoch 2020, val loss: 0.9593700170516968
Epoch 2030, training loss: 898.2881469726562 = 0.9513213038444519 + 100.0 * 8.973368644714355
Epoch 2030, val loss: 0.9587153196334839
Epoch 2040, training loss: 898.2108154296875 = 0.9506222009658813 + 100.0 * 8.972601890563965
Epoch 2040, val loss: 0.9580478072166443
Epoch 2050, training loss: 898.6618041992188 = 0.9499391317367554 + 100.0 * 8.977118492126465
Epoch 2050, val loss: 0.9574174880981445
Epoch 2060, training loss: 898.7568359375 = 0.9492278099060059 + 100.0 * 8.978075981140137
Epoch 2060, val loss: 0.9567531943321228
Epoch 2070, training loss: 898.941162109375 = 0.9485633969306946 + 100.0 * 8.979926109313965
Epoch 2070, val loss: 0.9561333060264587
Epoch 2080, training loss: 899.0602416992188 = 0.9478825330734253 + 100.0 * 8.981123924255371
Epoch 2080, val loss: 0.955507755279541
Epoch 2090, training loss: 898.6763305664062 = 0.9471924304962158 + 100.0 * 8.977291107177734
Epoch 2090, val loss: 0.9548568725585938
Epoch 2100, training loss: 898.9769287109375 = 0.9465123414993286 + 100.0 * 8.980303764343262
Epoch 2100, val loss: 0.9542272686958313
Epoch 2110, training loss: 899.372314453125 = 0.9458293914794922 + 100.0 * 8.984265327453613
Epoch 2110, val loss: 0.9536052346229553
Epoch 2120, training loss: 899.1724243164062 = 0.9451420903205872 + 100.0 * 8.98227310180664
Epoch 2120, val loss: 0.952977180480957
Epoch 2130, training loss: 899.0925903320312 = 0.9444556832313538 + 100.0 * 8.981481552124023
Epoch 2130, val loss: 0.9523465037345886
Epoch 2140, training loss: 899.4575805664062 = 0.9437698721885681 + 100.0 * 8.985137939453125
Epoch 2140, val loss: 0.951711118221283
Epoch 2150, training loss: 900.0139770507812 = 0.9430762529373169 + 100.0 * 8.99070930480957
Epoch 2150, val loss: 0.9510633945465088
Epoch 2160, training loss: 900.1148071289062 = 0.9423860311508179 + 100.0 * 8.991724014282227
Epoch 2160, val loss: 0.950426459312439
Epoch 2170, training loss: 899.6630249023438 = 0.9416865110397339 + 100.0 * 8.987213134765625
Epoch 2170, val loss: 0.9497659206390381
Epoch 2180, training loss: 896.3175048828125 = 0.9409334659576416 + 100.0 * 8.953765869140625
Epoch 2180, val loss: 0.94904625415802
Epoch 2190, training loss: 897.097412109375 = 0.940277099609375 + 100.0 * 8.96157169342041
Epoch 2190, val loss: 0.948460042476654
Epoch 2200, training loss: 897.1049194335938 = 0.9396266341209412 + 100.0 * 8.961652755737305
Epoch 2200, val loss: 0.9478368759155273
Epoch 2210, training loss: 897.951171875 = 0.9390178322792053 + 100.0 * 8.970121383666992
Epoch 2210, val loss: 0.9472796320915222
Epoch 2220, training loss: 898.4888305664062 = 0.9384138584136963 + 100.0 * 8.975503921508789
Epoch 2220, val loss: 0.946719229221344
Epoch 2230, training loss: 899.2457885742188 = 0.9378330707550049 + 100.0 * 8.98307991027832
Epoch 2230, val loss: 0.9461847543716431
Epoch 2240, training loss: 899.8267211914062 = 0.9372341632843018 + 100.0 * 8.98889446258545
Epoch 2240, val loss: 0.9456227421760559
Epoch 2250, training loss: 899.6220703125 = 0.9366200566291809 + 100.0 * 8.986854553222656
Epoch 2250, val loss: 0.9450498223304749
Epoch 2260, training loss: 899.9584350585938 = 0.9360197186470032 + 100.0 * 8.99022388458252
Epoch 2260, val loss: 0.9444998502731323
Epoch 2270, training loss: 900.3013916015625 = 0.9354291558265686 + 100.0 * 8.993659973144531
Epoch 2270, val loss: 0.943949282169342
Epoch 2280, training loss: 900.4733276367188 = 0.9348419308662415 + 100.0 * 8.99538516998291
Epoch 2280, val loss: 0.9434013366699219
Epoch 2290, training loss: 899.999755859375 = 0.9342454671859741 + 100.0 * 8.990654945373535
Epoch 2290, val loss: 0.9428614377975464
Epoch 2300, training loss: 900.1196899414062 = 0.9336704015731812 + 100.0 * 8.991860389709473
Epoch 2300, val loss: 0.9423313736915588
Epoch 2310, training loss: 900.3192138671875 = 0.9331100583076477 + 100.0 * 8.993861198425293
Epoch 2310, val loss: 0.9418097734451294
Epoch 2320, training loss: 900.5182495117188 = 0.9325438141822815 + 100.0 * 8.995857238769531
Epoch 2320, val loss: 0.9412782192230225
Epoch 2330, training loss: 898.3857421875 = 0.931942343711853 + 100.0 * 8.97453784942627
Epoch 2330, val loss: 0.9407238960266113
Epoch 2340, training loss: 898.7687377929688 = 0.9314306974411011 + 100.0 * 8.978372573852539
Epoch 2340, val loss: 0.9402358531951904
Epoch 2350, training loss: 899.759033203125 = 0.9308837652206421 + 100.0 * 8.98828125
Epoch 2350, val loss: 0.9397160410881042
Epoch 2360, training loss: 900.2694702148438 = 0.9303449988365173 + 100.0 * 8.993391036987305
Epoch 2360, val loss: 0.9392193555831909
Epoch 2370, training loss: 900.6932983398438 = 0.929814875125885 + 100.0 * 8.997634887695312
Epoch 2370, val loss: 0.938726007938385
Epoch 2380, training loss: 900.881103515625 = 0.9292722344398499 + 100.0 * 8.999518394470215
Epoch 2380, val loss: 0.9382218718528748
Epoch 2390, training loss: 900.885009765625 = 0.9287653565406799 + 100.0 * 8.99956226348877
Epoch 2390, val loss: 0.9377673268318176
Epoch 2400, training loss: 900.7142944335938 = 0.9282404184341431 + 100.0 * 8.9978609085083
Epoch 2400, val loss: 0.9372890591621399
Epoch 2410, training loss: 900.8001098632812 = 0.9277316331863403 + 100.0 * 8.998723983764648
Epoch 2410, val loss: 0.9368279576301575
Epoch 2420, training loss: 900.8651733398438 = 0.9272181391716003 + 100.0 * 8.99937915802002
Epoch 2420, val loss: 0.9363692402839661
Epoch 2430, training loss: 901.0107421875 = 0.9267129898071289 + 100.0 * 9.000840187072754
Epoch 2430, val loss: 0.9359201192855835
Epoch 2440, training loss: 901.2223510742188 = 0.92621248960495 + 100.0 * 9.002961158752441
Epoch 2440, val loss: 0.9354694485664368
Epoch 2450, training loss: 901.1393432617188 = 0.9257152676582336 + 100.0 * 9.00213623046875
Epoch 2450, val loss: 0.9350205063819885
Epoch 2460, training loss: 901.0550537109375 = 0.9252186417579651 + 100.0 * 9.001297950744629
Epoch 2460, val loss: 0.9345800876617432
Epoch 2470, training loss: 901.265869140625 = 0.9247335195541382 + 100.0 * 9.003411293029785
Epoch 2470, val loss: 0.9341496825218201
Epoch 2480, training loss: 901.4682006835938 = 0.924253523349762 + 100.0 * 9.005439758300781
Epoch 2480, val loss: 0.9337156414985657
Epoch 2490, training loss: 900.7749633789062 = 0.9237604737281799 + 100.0 * 8.998512268066406
Epoch 2490, val loss: 0.9332740902900696
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.48768115942028983
0.861841628631457
=== training gcn model ===
Epoch 0, training loss: 1020.8583374023438 = 1.0980751514434814 + 100.0 * 10.197602272033691
Epoch 0, val loss: 1.09792160987854
Epoch 10, training loss: 976.5966796875 = 1.0979102849960327 + 100.0 * 9.754987716674805
Epoch 10, val loss: 1.0977437496185303
Epoch 20, training loss: 954.789794921875 = 1.0977336168289185 + 100.0 * 9.536920547485352
Epoch 20, val loss: 1.0975549221038818
Epoch 30, training loss: 939.9684448242188 = 1.0975401401519775 + 100.0 * 9.38870906829834
Epoch 30, val loss: 1.0973491668701172
Epoch 40, training loss: 928.5352172851562 = 1.0973683595657349 + 100.0 * 9.274378776550293
Epoch 40, val loss: 1.0971674919128418
Epoch 50, training loss: 919.5750732421875 = 1.0971858501434326 + 100.0 * 9.184779167175293
Epoch 50, val loss: 1.0969668626785278
Epoch 60, training loss: 912.1708374023438 = 1.0969862937927246 + 100.0 * 9.110738754272461
Epoch 60, val loss: 1.0967531204223633
Epoch 70, training loss: 905.8619384765625 = 1.0967812538146973 + 100.0 * 9.047651290893555
Epoch 70, val loss: 1.0965341329574585
Epoch 80, training loss: 900.6862182617188 = 1.0965652465820312 + 100.0 * 8.995896339416504
Epoch 80, val loss: 1.0963003635406494
Epoch 90, training loss: 896.08837890625 = 1.0963389873504639 + 100.0 * 8.949920654296875
Epoch 90, val loss: 1.096058964729309
Epoch 100, training loss: 892.20166015625 = 1.0961138010025024 + 100.0 * 8.911055564880371
Epoch 100, val loss: 1.09581458568573
Epoch 110, training loss: 888.825927734375 = 1.0958611965179443 + 100.0 * 8.877300262451172
Epoch 110, val loss: 1.0955452919006348
Epoch 120, training loss: 886.1078491210938 = 1.0956053733825684 + 100.0 * 8.850122451782227
Epoch 120, val loss: 1.0952742099761963
Epoch 130, training loss: 883.446533203125 = 1.0953340530395508 + 100.0 * 8.823512077331543
Epoch 130, val loss: 1.09499192237854
Epoch 140, training loss: 881.4320678710938 = 1.0950614213943481 + 100.0 * 8.803370475769043
Epoch 140, val loss: 1.0947010517120361
Epoch 150, training loss: 879.472900390625 = 1.0947706699371338 + 100.0 * 8.783781051635742
Epoch 150, val loss: 1.094398856163025
Epoch 160, training loss: 877.8448486328125 = 1.0944880247116089 + 100.0 * 8.76750373840332
Epoch 160, val loss: 1.0940988063812256
Epoch 170, training loss: 876.833740234375 = 1.0941814184188843 + 100.0 * 8.75739574432373
Epoch 170, val loss: 1.0937798023223877
Epoch 180, training loss: 875.1744384765625 = 1.0938565731048584 + 100.0 * 8.740805625915527
Epoch 180, val loss: 1.0934473276138306
Epoch 190, training loss: 874.3020629882812 = 1.0935397148132324 + 100.0 * 8.732085227966309
Epoch 190, val loss: 1.0931161642074585
Epoch 200, training loss: 873.2308349609375 = 1.0931968688964844 + 100.0 * 8.721376419067383
Epoch 200, val loss: 1.092759609222412
Epoch 210, training loss: 872.22216796875 = 1.0928453207015991 + 100.0 * 8.71129322052002
Epoch 210, val loss: 1.0923970937728882
Epoch 220, training loss: 871.6150512695312 = 1.0924770832061768 + 100.0 * 8.705225944519043
Epoch 220, val loss: 1.0920159816741943
Epoch 230, training loss: 870.9612426757812 = 1.0920970439910889 + 100.0 * 8.698691368103027
Epoch 230, val loss: 1.0916380882263184
Epoch 240, training loss: 870.1336059570312 = 1.091705083847046 + 100.0 * 8.69041919708252
Epoch 240, val loss: 1.0912357568740845
Epoch 250, training loss: 869.855712890625 = 1.0913115739822388 + 100.0 * 8.687644004821777
Epoch 250, val loss: 1.0908215045928955
Epoch 260, training loss: 869.398193359375 = 1.0909066200256348 + 100.0 * 8.683073043823242
Epoch 260, val loss: 1.0904228687286377
Epoch 270, training loss: 868.9865112304688 = 1.090485692024231 + 100.0 * 8.678959846496582
Epoch 270, val loss: 1.0899968147277832
Epoch 280, training loss: 868.9885864257812 = 1.0900542736053467 + 100.0 * 8.678985595703125
Epoch 280, val loss: 1.0895520448684692
Epoch 290, training loss: 868.6572265625 = 1.0896116495132446 + 100.0 * 8.675676345825195
Epoch 290, val loss: 1.0891104936599731
Epoch 300, training loss: 868.4249877929688 = 1.0891661643981934 + 100.0 * 8.673357963562012
Epoch 300, val loss: 1.088662028312683
Epoch 310, training loss: 868.153076171875 = 1.0887110233306885 + 100.0 * 8.67064380645752
Epoch 310, val loss: 1.0882055759429932
Epoch 320, training loss: 868.0014038085938 = 1.0882335901260376 + 100.0 * 8.6691312789917
Epoch 320, val loss: 1.087731957435608
Epoch 330, training loss: 867.6206665039062 = 1.0877584218978882 + 100.0 * 8.665328979492188
Epoch 330, val loss: 1.0872478485107422
Epoch 340, training loss: 867.688720703125 = 1.0872867107391357 + 100.0 * 8.666014671325684
Epoch 340, val loss: 1.086774468421936
Epoch 350, training loss: 867.5526733398438 = 1.0868016481399536 + 100.0 * 8.664658546447754
Epoch 350, val loss: 1.086289405822754
Epoch 360, training loss: 867.4277954101562 = 1.0862960815429688 + 100.0 * 8.66341495513916
Epoch 360, val loss: 1.0857852697372437
Epoch 370, training loss: 867.915771484375 = 1.0857959985733032 + 100.0 * 8.668299674987793
Epoch 370, val loss: 1.0852733850479126
Epoch 380, training loss: 867.7752075195312 = 1.0852755308151245 + 100.0 * 8.666899681091309
Epoch 380, val loss: 1.0847630500793457
Epoch 390, training loss: 868.0283813476562 = 1.0847305059432983 + 100.0 * 8.66943645477295
Epoch 390, val loss: 1.084224820137024
Epoch 400, training loss: 867.2518920898438 = 1.0841854810714722 + 100.0 * 8.661677360534668
Epoch 400, val loss: 1.0836899280548096
Epoch 410, training loss: 867.7317504882812 = 1.0836502313613892 + 100.0 * 8.666481018066406
Epoch 410, val loss: 1.0831589698791504
Epoch 420, training loss: 868.0894775390625 = 1.0830763578414917 + 100.0 * 8.670063972473145
Epoch 420, val loss: 1.0825579166412354
Epoch 430, training loss: 868.369873046875 = 1.0825151205062866 + 100.0 * 8.672873497009277
Epoch 430, val loss: 1.0820105075836182
Epoch 440, training loss: 868.0733642578125 = 1.0818642377853394 + 100.0 * 8.669915199279785
Epoch 440, val loss: 1.0813779830932617
Epoch 450, training loss: 868.2368774414062 = 1.081281065940857 + 100.0 * 8.671555519104004
Epoch 450, val loss: 1.080763816833496
Epoch 460, training loss: 865.8899536132812 = 1.0805238485336304 + 100.0 * 8.648094177246094
Epoch 460, val loss: 1.080021619796753
Epoch 470, training loss: 870.0455322265625 = 1.08004629611969 + 100.0 * 8.689655303955078
Epoch 470, val loss: 1.079578161239624
Epoch 480, training loss: 866.2320556640625 = 1.0792040824890137 + 100.0 * 8.651528358459473
Epoch 480, val loss: 1.0786939859390259
Epoch 490, training loss: 867.8529052734375 = 1.0785974264144897 + 100.0 * 8.667742729187012
Epoch 490, val loss: 1.0781117677688599
Epoch 500, training loss: 867.5972290039062 = 1.077925682067871 + 100.0 * 8.665192604064941
Epoch 500, val loss: 1.0774391889572144
Epoch 510, training loss: 868.3013305664062 = 1.0772340297698975 + 100.0 * 8.6722412109375
Epoch 510, val loss: 1.0767381191253662
Epoch 520, training loss: 868.6287231445312 = 1.0765222311019897 + 100.0 * 8.675521850585938
Epoch 520, val loss: 1.076028823852539
Epoch 530, training loss: 868.9642333984375 = 1.0757793188095093 + 100.0 * 8.678884506225586
Epoch 530, val loss: 1.0752886533737183
Epoch 540, training loss: 869.6256713867188 = 1.075049638748169 + 100.0 * 8.685505867004395
Epoch 540, val loss: 1.074555516242981
Epoch 550, training loss: 870.6767578125 = 1.0743257999420166 + 100.0 * 8.696023941040039
Epoch 550, val loss: 1.0738401412963867
Epoch 560, training loss: 869.6233520507812 = 1.0735732316970825 + 100.0 * 8.685498237609863
Epoch 560, val loss: 1.073086142539978
Epoch 570, training loss: 870.4148559570312 = 1.072832465171814 + 100.0 * 8.69342041015625
Epoch 570, val loss: 1.0723389387130737
Epoch 580, training loss: 870.5499267578125 = 1.0720881223678589 + 100.0 * 8.694778442382812
Epoch 580, val loss: 1.0715832710266113
Epoch 590, training loss: 870.8040161132812 = 1.0713366270065308 + 100.0 * 8.69732666015625
Epoch 590, val loss: 1.070837140083313
Epoch 600, training loss: 871.1631469726562 = 1.0705797672271729 + 100.0 * 8.700925827026367
Epoch 600, val loss: 1.0700880289077759
Epoch 610, training loss: 871.509765625 = 1.0698176622390747 + 100.0 * 8.704399108886719
Epoch 610, val loss: 1.069327712059021
Epoch 620, training loss: 872.172119140625 = 1.0690815448760986 + 100.0 * 8.711030006408691
Epoch 620, val loss: 1.0685782432556152
Epoch 630, training loss: 871.4160766601562 = 1.0682765245437622 + 100.0 * 8.70347785949707
Epoch 630, val loss: 1.06776762008667
Epoch 640, training loss: 872.1102294921875 = 1.0674971342086792 + 100.0 * 8.710427284240723
Epoch 640, val loss: 1.0669927597045898
Epoch 650, training loss: 872.2405395507812 = 1.0667082071304321 + 100.0 * 8.711738586425781
Epoch 650, val loss: 1.0662169456481934
Epoch 660, training loss: 872.552001953125 = 1.0659373998641968 + 100.0 * 8.714860916137695
Epoch 660, val loss: 1.0654321908950806
Epoch 670, training loss: 872.9696044921875 = 1.0651299953460693 + 100.0 * 8.71904468536377
Epoch 670, val loss: 1.064631462097168
Epoch 680, training loss: 873.0513916015625 = 1.0643199682235718 + 100.0 * 8.719870567321777
Epoch 680, val loss: 1.063830852508545
Epoch 690, training loss: 873.1409301757812 = 1.063525915145874 + 100.0 * 8.720773696899414
Epoch 690, val loss: 1.0630286931991577
Epoch 700, training loss: 873.273193359375 = 1.0627079010009766 + 100.0 * 8.722105026245117
Epoch 700, val loss: 1.0622164011001587
Epoch 710, training loss: 873.8110961914062 = 1.0618869066238403 + 100.0 * 8.727492332458496
Epoch 710, val loss: 1.0614041090011597
Epoch 720, training loss: 873.8428955078125 = 1.0610573291778564 + 100.0 * 8.727818489074707
Epoch 720, val loss: 1.0605661869049072
Epoch 730, training loss: 874.0066528320312 = 1.0602071285247803 + 100.0 * 8.729464530944824
Epoch 730, val loss: 1.0597140789031982
Epoch 740, training loss: 873.5099487304688 = 1.0593559741973877 + 100.0 * 8.724506378173828
Epoch 740, val loss: 1.0589064359664917
Epoch 750, training loss: 873.3346557617188 = 1.0585252046585083 + 100.0 * 8.722761154174805
Epoch 750, val loss: 1.0580832958221436
Epoch 760, training loss: 874.5555419921875 = 1.0577343702316284 + 100.0 * 8.734977722167969
Epoch 760, val loss: 1.0572857856750488
Epoch 770, training loss: 874.91064453125 = 1.0569086074829102 + 100.0 * 8.738537788391113
Epoch 770, val loss: 1.0564600229263306
Epoch 780, training loss: 875.2736206054688 = 1.056061863899231 + 100.0 * 8.742175102233887
Epoch 780, val loss: 1.0556271076202393
Epoch 790, training loss: 875.1912231445312 = 1.0552031993865967 + 100.0 * 8.741360664367676
Epoch 790, val loss: 1.0547733306884766
Epoch 800, training loss: 875.8400268554688 = 1.0543659925460815 + 100.0 * 8.747856140136719
Epoch 800, val loss: 1.0539422035217285
Epoch 810, training loss: 876.1652221679688 = 1.0535176992416382 + 100.0 * 8.751116752624512
Epoch 810, val loss: 1.0530983209609985
Epoch 820, training loss: 876.4728393554688 = 1.0526713132858276 + 100.0 * 8.754201889038086
Epoch 820, val loss: 1.0522505044937134
Epoch 830, training loss: 876.7001342773438 = 1.051830768585205 + 100.0 * 8.75648307800293
Epoch 830, val loss: 1.051425576210022
Epoch 840, training loss: 876.9873046875 = 1.0509697198867798 + 100.0 * 8.759363174438477
Epoch 840, val loss: 1.0505716800689697
Epoch 850, training loss: 876.9685668945312 = 1.050126552581787 + 100.0 * 8.759184837341309
Epoch 850, val loss: 1.0497275590896606
Epoch 860, training loss: 877.3755493164062 = 1.049258828163147 + 100.0 * 8.763262748718262
Epoch 860, val loss: 1.0488693714141846
Epoch 870, training loss: 877.684814453125 = 1.0484131574630737 + 100.0 * 8.766364097595215
Epoch 870, val loss: 1.0480334758758545
Epoch 880, training loss: 877.78955078125 = 1.0475549697875977 + 100.0 * 8.767419815063477
Epoch 880, val loss: 1.0471830368041992
Epoch 890, training loss: 877.9957885742188 = 1.046689510345459 + 100.0 * 8.769491195678711
Epoch 890, val loss: 1.0463247299194336
Epoch 900, training loss: 878.24462890625 = 1.0458461046218872 + 100.0 * 8.771987915039062
Epoch 900, val loss: 1.045485019683838
Epoch 910, training loss: 878.316162109375 = 1.0449799299240112 + 100.0 * 8.772711753845215
Epoch 910, val loss: 1.0446373224258423
Epoch 920, training loss: 878.3339233398438 = 1.0441036224365234 + 100.0 * 8.772897720336914
Epoch 920, val loss: 1.043745756149292
Epoch 930, training loss: 878.4217529296875 = 1.0432240962982178 + 100.0 * 8.773785591125488
Epoch 930, val loss: 1.0428985357284546
Epoch 940, training loss: 878.6429443359375 = 1.0423845052719116 + 100.0 * 8.776005744934082
Epoch 940, val loss: 1.0420585870742798
Epoch 950, training loss: 879.1620483398438 = 1.0415253639221191 + 100.0 * 8.781205177307129
Epoch 950, val loss: 1.0412076711654663
Epoch 960, training loss: 879.616455078125 = 1.0406653881072998 + 100.0 * 8.785758018493652
Epoch 960, val loss: 1.0403563976287842
Epoch 970, training loss: 879.55419921875 = 1.0397781133651733 + 100.0 * 8.785143852233887
Epoch 970, val loss: 1.039465308189392
Epoch 980, training loss: 880.8740234375 = 1.0390021800994873 + 100.0 * 8.79835033416748
Epoch 980, val loss: 1.0387095212936401
Epoch 990, training loss: 878.9968872070312 = 1.0380498170852661 + 100.0 * 8.77958869934082
Epoch 990, val loss: 1.0377646684646606
Epoch 1000, training loss: 879.0153198242188 = 1.0371525287628174 + 100.0 * 8.779781341552734
Epoch 1000, val loss: 1.0368856191635132
Epoch 1010, training loss: 879.7376098632812 = 1.036373257637024 + 100.0 * 8.787012100219727
Epoch 1010, val loss: 1.0361067056655884
Epoch 1020, training loss: 880.2149047851562 = 1.0355416536331177 + 100.0 * 8.791793823242188
Epoch 1020, val loss: 1.0352891683578491
Epoch 1030, training loss: 880.4488525390625 = 1.0346946716308594 + 100.0 * 8.79414176940918
Epoch 1030, val loss: 1.0344504117965698
Epoch 1040, training loss: 880.6902465820312 = 1.0338417291641235 + 100.0 * 8.796564102172852
Epoch 1040, val loss: 1.0336110591888428
Epoch 1050, training loss: 880.9926147460938 = 1.0329856872558594 + 100.0 * 8.799596786499023
Epoch 1050, val loss: 1.0327680110931396
Epoch 1060, training loss: 881.0433959960938 = 1.0321154594421387 + 100.0 * 8.8001127243042
Epoch 1060, val loss: 1.0319287776947021
Epoch 1070, training loss: 881.3184204101562 = 1.0312668085098267 + 100.0 * 8.802871704101562
Epoch 1070, val loss: 1.03107750415802
Epoch 1080, training loss: 881.7344970703125 = 1.0304080247879028 + 100.0 * 8.80704116821289
Epoch 1080, val loss: 1.0302517414093018
Epoch 1090, training loss: 881.5311889648438 = 1.0295469760894775 + 100.0 * 8.80501651763916
Epoch 1090, val loss: 1.0294017791748047
Epoch 1100, training loss: 882.0765991210938 = 1.028696894645691 + 100.0 * 8.810479164123535
Epoch 1100, val loss: 1.0285695791244507
Epoch 1110, training loss: 882.4901123046875 = 1.0278637409210205 + 100.0 * 8.814621925354004
Epoch 1110, val loss: 1.0277550220489502
Epoch 1120, training loss: 881.99462890625 = 1.0270134210586548 + 100.0 * 8.809676170349121
Epoch 1120, val loss: 1.0269005298614502
Epoch 1130, training loss: 881.2338256835938 = 1.0261001586914062 + 100.0 * 8.802077293395996
Epoch 1130, val loss: 1.026037335395813
Epoch 1140, training loss: 881.6361083984375 = 1.0252383947372437 + 100.0 * 8.806108474731445
Epoch 1140, val loss: 1.0251777172088623
Epoch 1150, training loss: 881.39892578125 = 1.024397850036621 + 100.0 * 8.80374526977539
Epoch 1150, val loss: 1.0243507623672485
Epoch 1160, training loss: 881.6765747070312 = 1.023535966873169 + 100.0 * 8.806529998779297
Epoch 1160, val loss: 1.02349853515625
Epoch 1170, training loss: 882.086181640625 = 1.0227100849151611 + 100.0 * 8.81063461303711
Epoch 1170, val loss: 1.0226768255233765
Epoch 1180, training loss: 882.7672729492188 = 1.0219312906265259 + 100.0 * 8.817453384399414
Epoch 1180, val loss: 1.0219100713729858
Epoch 1190, training loss: 883.3367309570312 = 1.021121621131897 + 100.0 * 8.823156356811523
Epoch 1190, val loss: 1.0211093425750732
Epoch 1200, training loss: 883.6417846679688 = 1.0203070640563965 + 100.0 * 8.826214790344238
Epoch 1200, val loss: 1.0203018188476562
Epoch 1210, training loss: 883.8388061523438 = 1.019468903541565 + 100.0 * 8.828193664550781
Epoch 1210, val loss: 1.0194848775863647
Epoch 1220, training loss: 884.2093505859375 = 1.0186375379562378 + 100.0 * 8.831907272338867
Epoch 1220, val loss: 1.0186692476272583
Epoch 1230, training loss: 883.6382446289062 = 1.0177022218704224 + 100.0 * 8.826205253601074
Epoch 1230, val loss: 1.0177537202835083
Epoch 1240, training loss: 882.9464111328125 = 1.016908049583435 + 100.0 * 8.819294929504395
Epoch 1240, val loss: 1.0170034170150757
Epoch 1250, training loss: 883.5449829101562 = 1.016113519668579 + 100.0 * 8.825288772583008
Epoch 1250, val loss: 1.0162084102630615
Epoch 1260, training loss: 884.2518920898438 = 1.0153394937515259 + 100.0 * 8.832365989685059
Epoch 1260, val loss: 1.0154598951339722
Epoch 1270, training loss: 884.6937866210938 = 1.0145881175994873 + 100.0 * 8.8367919921875
Epoch 1270, val loss: 1.0147063732147217
Epoch 1280, training loss: 885.2577514648438 = 1.013767957687378 + 100.0 * 8.842439651489258
Epoch 1280, val loss: 1.0139071941375732
Epoch 1290, training loss: 885.6122436523438 = 1.012958288192749 + 100.0 * 8.845993041992188
Epoch 1290, val loss: 1.0131115913391113
Epoch 1300, training loss: 885.6028442382812 = 1.0121337175369263 + 100.0 * 8.845907211303711
Epoch 1300, val loss: 1.0122953653335571
Epoch 1310, training loss: 885.49755859375 = 1.011309027671814 + 100.0 * 8.844862937927246
Epoch 1310, val loss: 1.0114918947219849
Epoch 1320, training loss: 885.7249145507812 = 1.0104800462722778 + 100.0 * 8.84714412689209
Epoch 1320, val loss: 1.0106886625289917
Epoch 1330, training loss: 885.9706420898438 = 1.0096759796142578 + 100.0 * 8.849609375
Epoch 1330, val loss: 1.009889006614685
Epoch 1340, training loss: 886.0140380859375 = 1.008859634399414 + 100.0 * 8.850051879882812
Epoch 1340, val loss: 1.0090934038162231
Epoch 1350, training loss: 886.32861328125 = 1.0080382823944092 + 100.0 * 8.853205680847168
Epoch 1350, val loss: 1.0083009004592896
Epoch 1360, training loss: 886.8442993164062 = 1.007224678993225 + 100.0 * 8.858370780944824
Epoch 1360, val loss: 1.0074973106384277
Epoch 1370, training loss: 887.0675048828125 = 1.0064067840576172 + 100.0 * 8.860610961914062
Epoch 1370, val loss: 1.0067112445831299
Epoch 1380, training loss: 887.3057861328125 = 1.0055978298187256 + 100.0 * 8.863001823425293
Epoch 1380, val loss: 1.0059187412261963
Epoch 1390, training loss: 887.3340454101562 = 1.0047893524169922 + 100.0 * 8.863292694091797
Epoch 1390, val loss: 1.0051264762878418
Epoch 1400, training loss: 885.8511962890625 = 1.0039348602294922 + 100.0 * 8.848472595214844
Epoch 1400, val loss: 1.0042800903320312
Epoch 1410, training loss: 885.6707763671875 = 1.0031054019927979 + 100.0 * 8.84667682647705
Epoch 1410, val loss: 1.00345778465271
Epoch 1420, training loss: 886.1073608398438 = 1.0023047924041748 + 100.0 * 8.85105037689209
Epoch 1420, val loss: 1.0026962757110596
Epoch 1430, training loss: 886.7233276367188 = 1.0015300512313843 + 100.0 * 8.857217788696289
Epoch 1430, val loss: 1.0019316673278809
Epoch 1440, training loss: 887.4927978515625 = 1.0007438659667969 + 100.0 * 8.864920616149902
Epoch 1440, val loss: 1.0011773109436035
Epoch 1450, training loss: 887.8964233398438 = 0.9999533295631409 + 100.0 * 8.868965148925781
Epoch 1450, val loss: 1.0004245042800903
Epoch 1460, training loss: 887.9129028320312 = 0.9991605281829834 + 100.0 * 8.86913776397705
Epoch 1460, val loss: 0.9996564984321594
Epoch 1470, training loss: 888.2481079101562 = 0.9983744621276855 + 100.0 * 8.87249755859375
Epoch 1470, val loss: 0.9988904595375061
Epoch 1480, training loss: 888.323486328125 = 0.997582733631134 + 100.0 * 8.873259544372559
Epoch 1480, val loss: 0.9981188774108887
Epoch 1490, training loss: 888.4437255859375 = 0.9967737197875977 + 100.0 * 8.874469757080078
Epoch 1490, val loss: 0.997340202331543
Epoch 1500, training loss: 888.5677490234375 = 0.9960059523582458 + 100.0 * 8.875717163085938
Epoch 1500, val loss: 0.9965834617614746
Epoch 1510, training loss: 888.8927001953125 = 0.9952319860458374 + 100.0 * 8.878974914550781
Epoch 1510, val loss: 0.995840847492218
Epoch 1520, training loss: 889.1424560546875 = 0.9944441318511963 + 100.0 * 8.88148021697998
Epoch 1520, val loss: 0.9950636029243469
Epoch 1530, training loss: 889.1441650390625 = 0.9936473369598389 + 100.0 * 8.881505012512207
Epoch 1530, val loss: 0.994269073009491
Epoch 1540, training loss: 888.7576293945312 = 0.9928592443466187 + 100.0 * 8.877647399902344
Epoch 1540, val loss: 0.9935398697853088
Epoch 1550, training loss: 888.7061767578125 = 0.9920808672904968 + 100.0 * 8.877140998840332
Epoch 1550, val loss: 0.9927729964256287
Epoch 1560, training loss: 888.2717895507812 = 0.9913221001625061 + 100.0 * 8.872804641723633
Epoch 1560, val loss: 0.9920874834060669
Epoch 1570, training loss: 886.3497924804688 = 0.9904580116271973 + 100.0 * 8.853592872619629
Epoch 1570, val loss: 0.9912100434303284
Epoch 1580, training loss: 887.91064453125 = 0.9897820353507996 + 100.0 * 8.869208335876465
Epoch 1580, val loss: 0.9905575513839722
Epoch 1590, training loss: 888.3309326171875 = 0.9890829920768738 + 100.0 * 8.873418807983398
Epoch 1590, val loss: 0.9898973703384399
Epoch 1600, training loss: 888.2874145507812 = 0.9883573055267334 + 100.0 * 8.872990608215332
Epoch 1600, val loss: 0.9891870021820068
Epoch 1610, training loss: 889.2821044921875 = 0.9876212477684021 + 100.0 * 8.88294506072998
Epoch 1610, val loss: 0.9884674549102783
Epoch 1620, training loss: 890.0538330078125 = 0.9868876338005066 + 100.0 * 8.890669822692871
Epoch 1620, val loss: 0.9877466559410095
Epoch 1630, training loss: 890.3611450195312 = 0.9861306548118591 + 100.0 * 8.893750190734863
Epoch 1630, val loss: 0.987001359462738
Epoch 1640, training loss: 890.473388671875 = 0.9853692650794983 + 100.0 * 8.894880294799805
Epoch 1640, val loss: 0.9862567782402039
Epoch 1650, training loss: 890.6478271484375 = 0.9846087098121643 + 100.0 * 8.896632194519043
Epoch 1650, val loss: 0.9855247139930725
Epoch 1660, training loss: 890.6602783203125 = 0.9838589429855347 + 100.0 * 8.896763801574707
Epoch 1660, val loss: 0.9847914576530457
Epoch 1670, training loss: 891.3136596679688 = 0.9831458330154419 + 100.0 * 8.903305053710938
Epoch 1670, val loss: 0.9840925931930542
Epoch 1680, training loss: 891.394287109375 = 0.9824123978614807 + 100.0 * 8.904118537902832
Epoch 1680, val loss: 0.9833798408508301
Epoch 1690, training loss: 891.7945556640625 = 0.981662929058075 + 100.0 * 8.90812873840332
Epoch 1690, val loss: 0.9826530814170837
Epoch 1700, training loss: 890.0654296875 = 0.9808474779129028 + 100.0 * 8.890846252441406
Epoch 1700, val loss: 0.9818222522735596
Epoch 1710, training loss: 890.298095703125 = 0.9800937175750732 + 100.0 * 8.893179893493652
Epoch 1710, val loss: 0.9811212420463562
Epoch 1720, training loss: 890.937255859375 = 0.9793804287910461 + 100.0 * 8.899579048156738
Epoch 1720, val loss: 0.9804511666297913
Epoch 1730, training loss: 891.94580078125 = 0.9786898493766785 + 100.0 * 8.90967082977295
Epoch 1730, val loss: 0.9797825813293457
Epoch 1740, training loss: 892.58447265625 = 0.9779688119888306 + 100.0 * 8.916065216064453
Epoch 1740, val loss: 0.9790917038917542
Epoch 1750, training loss: 892.6698608398438 = 0.977226972579956 + 100.0 * 8.916926383972168
Epoch 1750, val loss: 0.978375256061554
Epoch 1760, training loss: 892.9134521484375 = 0.9764943718910217 + 100.0 * 8.9193696975708
Epoch 1760, val loss: 0.9776747822761536
Epoch 1770, training loss: 893.37109375 = 0.9757742881774902 + 100.0 * 8.92395305633545
Epoch 1770, val loss: 0.976982831954956
Epoch 1780, training loss: 889.0789184570312 = 0.9747708439826965 + 100.0 * 8.881041526794434
Epoch 1780, val loss: 0.9758883714675903
Epoch 1790, training loss: 873.3237915039062 = 0.9735758304595947 + 100.0 * 8.723502159118652
Epoch 1790, val loss: 0.9749361872673035
Epoch 1800, training loss: 888.4199829101562 = 0.9738739728927612 + 100.0 * 8.87446117401123
Epoch 1800, val loss: 0.9751676917076111
Epoch 1810, training loss: 881.0404663085938 = 0.9730280637741089 + 100.0 * 8.800674438476562
Epoch 1810, val loss: 0.9743948578834534
Epoch 1820, training loss: 885.0370483398438 = 0.9725874066352844 + 100.0 * 8.840644836425781
Epoch 1820, val loss: 0.9738883376121521
Epoch 1830, training loss: 884.6875 = 0.9718747735023499 + 100.0 * 8.837156295776367
Epoch 1830, val loss: 0.9732425808906555
Epoch 1840, training loss: 886.2386474609375 = 0.9712154269218445 + 100.0 * 8.85267448425293
Epoch 1840, val loss: 0.9726012349128723
Epoch 1850, training loss: 887.7741088867188 = 0.9705674648284912 + 100.0 * 8.868035316467285
Epoch 1850, val loss: 0.9719741940498352
Epoch 1860, training loss: 888.4988403320312 = 0.9698455333709717 + 100.0 * 8.875289916992188
Epoch 1860, val loss: 0.9712892174720764
Epoch 1870, training loss: 889.583984375 = 0.969144880771637 + 100.0 * 8.886148452758789
Epoch 1870, val loss: 0.9706205129623413
Epoch 1880, training loss: 890.1395874023438 = 0.9684429168701172 + 100.0 * 8.891711235046387
Epoch 1880, val loss: 0.9699482917785645
Epoch 1890, training loss: 890.7650146484375 = 0.9677475094795227 + 100.0 * 8.897972106933594
Epoch 1890, val loss: 0.9692899584770203
Epoch 1900, training loss: 890.969482421875 = 0.9670425653457642 + 100.0 * 8.9000244140625
Epoch 1900, val loss: 0.9686123728752136
Epoch 1910, training loss: 891.3991088867188 = 0.9663480520248413 + 100.0 * 8.904327392578125
Epoch 1910, val loss: 0.9679396748542786
Epoch 1920, training loss: 891.8552856445312 = 0.9656554460525513 + 100.0 * 8.908896446228027
Epoch 1920, val loss: 0.967262327671051
Epoch 1930, training loss: 890.5668334960938 = 0.9649145007133484 + 100.0 * 8.896018981933594
Epoch 1930, val loss: 0.9665696024894714
Epoch 1940, training loss: 890.0702514648438 = 0.964162290096283 + 100.0 * 8.891060829162598
Epoch 1940, val loss: 0.9658434987068176
Epoch 1950, training loss: 890.0184936523438 = 0.9634591937065125 + 100.0 * 8.89055061340332
Epoch 1950, val loss: 0.9651790857315063
Epoch 1960, training loss: 890.3572387695312 = 0.9627918601036072 + 100.0 * 8.89394474029541
Epoch 1960, val loss: 0.96451735496521
Epoch 1970, training loss: 891.0595092773438 = 0.9621265530586243 + 100.0 * 8.90097427368164
Epoch 1970, val loss: 0.9638941884040833
Epoch 1980, training loss: 891.63818359375 = 0.9614729881286621 + 100.0 * 8.906766891479492
Epoch 1980, val loss: 0.9632711410522461
Epoch 1990, training loss: 892.3604125976562 = 0.9608277678489685 + 100.0 * 8.913995742797852
Epoch 1990, val loss: 0.9626381397247314
Epoch 2000, training loss: 892.8507690429688 = 0.9601687788963318 + 100.0 * 8.918906211853027
Epoch 2000, val loss: 0.9619990587234497
Epoch 2010, training loss: 893.1428833007812 = 0.9594934582710266 + 100.0 * 8.921833992004395
Epoch 2010, val loss: 0.961350679397583
Epoch 2020, training loss: 893.0722045898438 = 0.9588230848312378 + 100.0 * 8.921133995056152
Epoch 2020, val loss: 0.9607046246528625
Epoch 2030, training loss: 893.2808227539062 = 0.9581612348556519 + 100.0 * 8.923226356506348
Epoch 2030, val loss: 0.9600653052330017
Epoch 2040, training loss: 892.980712890625 = 0.9574857950210571 + 100.0 * 8.920232772827148
Epoch 2040, val loss: 0.9594061374664307
Epoch 2050, training loss: 893.0865478515625 = 0.9568357467651367 + 100.0 * 8.921297073364258
Epoch 2050, val loss: 0.9587826728820801
Epoch 2060, training loss: 893.7190551757812 = 0.9561960101127625 + 100.0 * 8.927628517150879
Epoch 2060, val loss: 0.9581674337387085
Epoch 2070, training loss: 894.088134765625 = 0.9555420279502869 + 100.0 * 8.931325912475586
Epoch 2070, val loss: 0.9575389623641968
Epoch 2080, training loss: 894.0455932617188 = 0.9548770189285278 + 100.0 * 8.930907249450684
Epoch 2080, val loss: 0.956903338432312
Epoch 2090, training loss: 894.0680541992188 = 0.9542180895805359 + 100.0 * 8.931138038635254
Epoch 2090, val loss: 0.956278383731842
Epoch 2100, training loss: 894.1076049804688 = 0.9535776376724243 + 100.0 * 8.931540489196777
Epoch 2100, val loss: 0.9556571841239929
Epoch 2110, training loss: 894.4410400390625 = 0.9529392123222351 + 100.0 * 8.934881210327148
Epoch 2110, val loss: 0.9550396800041199
Epoch 2120, training loss: 894.6580810546875 = 0.952301561832428 + 100.0 * 8.937057495117188
Epoch 2120, val loss: 0.9544299840927124
Epoch 2130, training loss: 894.6781005859375 = 0.9516640901565552 + 100.0 * 8.937264442443848
Epoch 2130, val loss: 0.9538237452507019
Epoch 2140, training loss: 894.8056640625 = 0.9510309100151062 + 100.0 * 8.938546180725098
Epoch 2140, val loss: 0.9532138109207153
Epoch 2150, training loss: 894.9989013671875 = 0.9503992199897766 + 100.0 * 8.940485000610352
Epoch 2150, val loss: 0.9526059031486511
Epoch 2160, training loss: 894.4098510742188 = 0.9497385621070862 + 100.0 * 8.934600830078125
Epoch 2160, val loss: 0.9519826769828796
Epoch 2170, training loss: 894.5650024414062 = 0.9491139650344849 + 100.0 * 8.936159133911133
Epoch 2170, val loss: 0.9513843655586243
Epoch 2180, training loss: 894.799072265625 = 0.9484895467758179 + 100.0 * 8.938506126403809
Epoch 2180, val loss: 0.9507964253425598
Epoch 2190, training loss: 895.2531127929688 = 0.9478702545166016 + 100.0 * 8.943052291870117
Epoch 2190, val loss: 0.9502013325691223
Epoch 2200, training loss: 895.4881591796875 = 0.9472483396530151 + 100.0 * 8.945408821105957
Epoch 2200, val loss: 0.9496127367019653
Epoch 2210, training loss: 895.5650024414062 = 0.9466230869293213 + 100.0 * 8.946184158325195
Epoch 2210, val loss: 0.9490131735801697
Epoch 2220, training loss: 893.6361083984375 = 0.9459512233734131 + 100.0 * 8.926901817321777
Epoch 2220, val loss: 0.9483314752578735
Epoch 2230, training loss: 895.7727661132812 = 0.945493757724762 + 100.0 * 8.948272705078125
Epoch 2230, val loss: 0.9479554891586304
Epoch 2240, training loss: 894.148681640625 = 0.9448316693305969 + 100.0 * 8.932038307189941
Epoch 2240, val loss: 0.9473565220832825
Epoch 2250, training loss: 894.7507934570312 = 0.9442345499992371 + 100.0 * 8.938065528869629
Epoch 2250, val loss: 0.9467625021934509
Epoch 2260, training loss: 895.5230102539062 = 0.9436126947402954 + 100.0 * 8.945794105529785
Epoch 2260, val loss: 0.9461578130722046
Epoch 2270, training loss: 895.7340087890625 = 0.9429956674575806 + 100.0 * 8.94791030883789
Epoch 2270, val loss: 0.9455702304840088
Epoch 2280, training loss: 896.3417358398438 = 0.9424139857292175 + 100.0 * 8.95399284362793
Epoch 2280, val loss: 0.9450355172157288
Epoch 2290, training loss: 896.644287109375 = 0.9418436884880066 + 100.0 * 8.957024574279785
Epoch 2290, val loss: 0.9444957971572876
Epoch 2300, training loss: 897.1591796875 = 0.9412633180618286 + 100.0 * 8.962179183959961
Epoch 2300, val loss: 0.9439525604248047
Epoch 2310, training loss: 897.4292602539062 = 0.940683901309967 + 100.0 * 8.964885711669922
Epoch 2310, val loss: 0.9434046745300293
Epoch 2320, training loss: 897.48388671875 = 0.940105676651001 + 100.0 * 8.965437889099121
Epoch 2320, val loss: 0.9428597092628479
Epoch 2330, training loss: 897.8363647460938 = 0.9395304322242737 + 100.0 * 8.968968391418457
Epoch 2330, val loss: 0.942317545413971
Epoch 2340, training loss: 898.150146484375 = 0.938952624797821 + 100.0 * 8.972111701965332
Epoch 2340, val loss: 0.9417662620544434
Epoch 2350, training loss: 897.8912963867188 = 0.9383690357208252 + 100.0 * 8.969529151916504
Epoch 2350, val loss: 0.9412121176719666
Epoch 2360, training loss: 898.286865234375 = 0.9378029108047485 + 100.0 * 8.973490715026855
Epoch 2360, val loss: 0.9406840205192566
Epoch 2370, training loss: 898.4871826171875 = 0.9372329115867615 + 100.0 * 8.975499153137207
Epoch 2370, val loss: 0.9401415586471558
Epoch 2380, training loss: 898.29150390625 = 0.9366527199745178 + 100.0 * 8.973548889160156
Epoch 2380, val loss: 0.9396028518676758
Epoch 2390, training loss: 898.4539794921875 = 0.9360875487327576 + 100.0 * 8.975178718566895
Epoch 2390, val loss: 0.9390767812728882
Epoch 2400, training loss: 898.7923583984375 = 0.9355307817459106 + 100.0 * 8.978568077087402
Epoch 2400, val loss: 0.9385562539100647
Epoch 2410, training loss: 898.7539672851562 = 0.9349585175514221 + 100.0 * 8.978190422058105
Epoch 2410, val loss: 0.9380344748497009
Epoch 2420, training loss: 898.9728393554688 = 0.9344038367271423 + 100.0 * 8.980384826660156
Epoch 2420, val loss: 0.9375255703926086
Epoch 2430, training loss: 898.698486328125 = 0.9338918924331665 + 100.0 * 8.977645874023438
Epoch 2430, val loss: 0.9370365738868713
Epoch 2440, training loss: 898.8482666015625 = 0.9333741664886475 + 100.0 * 8.979148864746094
Epoch 2440, val loss: 0.9365553855895996
Epoch 2450, training loss: 896.0406494140625 = 0.9327524900436401 + 100.0 * 8.951079368591309
Epoch 2450, val loss: 0.9360095858573914
Epoch 2460, training loss: 896.905517578125 = 0.9321954250335693 + 100.0 * 8.959733009338379
Epoch 2460, val loss: 0.935476541519165
Epoch 2470, training loss: 890.8099365234375 = 0.9316019415855408 + 100.0 * 8.898783683776855
Epoch 2470, val loss: 0.9350671768188477
Epoch 2480, training loss: 891.0977172851562 = 0.9311641454696655 + 100.0 * 8.901665687561035
Epoch 2480, val loss: 0.934593915939331
Epoch 2490, training loss: 892.6393432617188 = 0.9306669235229492 + 100.0 * 8.917086601257324
Epoch 2490, val loss: 0.9341239929199219
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.5095652173913043
0.8649568934289648
The final CL Acc:0.57058, 0.10215, The final GNN Acc:0.86351, 0.00128
Begin epxeriment: noisy_level: 0.2 cont_weight: 100 epoch: der1:0.3 der2:0.4 dfr1:0.4 dfr2:0
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=2500, cl_num_layers=2, cl_num_proj_hidden=64, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=100.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.4, drop_feat_rate_1=0.4, drop_feat_rate_2=0.0, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=32, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, noisy_level=0.2, num_hidden=64, num_proj_hidden=64, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=40, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 88648])
add edge: torch.Size([2, 106064])
remove edge: torch.Size([2, 70830])
updated graph: torch.Size([2, 88246])
=== Noisy graph ===
/home/project-robust-contrastive/Robust-Contrastive-Learning/model.py:227: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 1011.4771728515625 = 1.091472864151001 + 100.0 * 10.103857040405273
Epoch 0, val loss: 1.0924118757247925
Epoch 10, training loss: 964.134521484375 = 1.0913383960723877 + 100.0 * 9.63043212890625
Epoch 10, val loss: 1.0922926664352417
Epoch 20, training loss: 948.2362670898438 = 1.0911805629730225 + 100.0 * 9.471450805664062
Epoch 20, val loss: 1.0921682119369507
Epoch 30, training loss: 937.1967163085938 = 1.0910669565200806 + 100.0 * 9.361056327819824
Epoch 30, val loss: 1.092076301574707
Epoch 40, training loss: 928.5451049804688 = 1.0909473896026611 + 100.0 * 9.274541854858398
Epoch 40, val loss: 1.0919748544692993
Epoch 50, training loss: 921.42431640625 = 1.0908290147781372 + 100.0 * 9.20333480834961
Epoch 50, val loss: 1.0918784141540527
Epoch 60, training loss: 915.3475341796875 = 1.090714693069458 + 100.0 * 9.142568588256836
Epoch 60, val loss: 1.091784954071045
Epoch 70, training loss: 910.0481567382812 = 1.0905998945236206 + 100.0 * 9.08957576751709
Epoch 70, val loss: 1.0916922092437744
Epoch 80, training loss: 905.3184204101562 = 1.0904875993728638 + 100.0 * 9.042279243469238
Epoch 80, val loss: 1.0916004180908203
Epoch 90, training loss: 901.10595703125 = 1.090376615524292 + 100.0 * 9.000155448913574
Epoch 90, val loss: 1.091509461402893
Epoch 100, training loss: 897.3707275390625 = 1.0902645587921143 + 100.0 * 8.962804794311523
Epoch 100, val loss: 1.091417908668518
Epoch 110, training loss: 894.0404052734375 = 1.0901540517807007 + 100.0 * 8.929502487182617
Epoch 110, val loss: 1.0913282632827759
Epoch 120, training loss: 891.0701293945312 = 1.09004807472229 + 100.0 * 8.899801254272461
Epoch 120, val loss: 1.0912415981292725
Epoch 130, training loss: 888.4600830078125 = 1.0899409055709839 + 100.0 * 8.873701095581055
Epoch 130, val loss: 1.091154932975769
Epoch 140, training loss: 886.1182250976562 = 1.089836835861206 + 100.0 * 8.8502836227417
Epoch 140, val loss: 1.0910694599151611
Epoch 150, training loss: 884.0580444335938 = 1.0897374153137207 + 100.0 * 8.829683303833008
Epoch 150, val loss: 1.0909883975982666
Epoch 160, training loss: 882.21875 = 1.089638352394104 + 100.0 * 8.811290740966797
Epoch 160, val loss: 1.0909078121185303
Epoch 170, training loss: 880.5269775390625 = 1.0895408391952515 + 100.0 * 8.794374465942383
Epoch 170, val loss: 1.0908292531967163
Epoch 180, training loss: 879.0681762695312 = 1.0894474983215332 + 100.0 * 8.779787063598633
Epoch 180, val loss: 1.0907524824142456
Epoch 190, training loss: 877.8101806640625 = 1.0893516540527344 + 100.0 * 8.767208099365234
Epoch 190, val loss: 1.090675711631775
Epoch 200, training loss: 876.60791015625 = 1.0892605781555176 + 100.0 * 8.755187034606934
Epoch 200, val loss: 1.0906023979187012
Epoch 210, training loss: 875.5420532226562 = 1.089171290397644 + 100.0 * 8.744528770446777
Epoch 210, val loss: 1.0905311107635498
Epoch 220, training loss: 874.838623046875 = 1.0890811681747437 + 100.0 * 8.737495422363281
Epoch 220, val loss: 1.0904591083526611
Epoch 230, training loss: 873.9022827148438 = 1.0889973640441895 + 100.0 * 8.728133201599121
Epoch 230, val loss: 1.0903918743133545
Epoch 240, training loss: 873.038818359375 = 1.0889108180999756 + 100.0 * 8.719498634338379
Epoch 240, val loss: 1.0903221368789673
Epoch 250, training loss: 872.6063842773438 = 1.0888252258300781 + 100.0 * 8.71517562866211
Epoch 250, val loss: 1.0902527570724487
Epoch 260, training loss: 872.6924438476562 = 1.0887479782104492 + 100.0 * 8.716036796569824
Epoch 260, val loss: 1.0901923179626465
Epoch 270, training loss: 871.9345703125 = 1.0886622667312622 + 100.0 * 8.70845890045166
Epoch 270, val loss: 1.0901241302490234
Epoch 280, training loss: 871.24560546875 = 1.0885820388793945 + 100.0 * 8.701570510864258
Epoch 280, val loss: 1.0900607109069824
Epoch 290, training loss: 871.046875 = 1.0885043144226074 + 100.0 * 8.699584007263184
Epoch 290, val loss: 1.0899988412857056
Epoch 300, training loss: 870.5447387695312 = 1.0884253978729248 + 100.0 * 8.694562911987305
Epoch 300, val loss: 1.0899378061294556
Epoch 310, training loss: 870.312255859375 = 1.0883492231369019 + 100.0 * 8.692238807678223
Epoch 310, val loss: 1.0898784399032593
Epoch 320, training loss: 869.8540649414062 = 1.0882744789123535 + 100.0 * 8.687658309936523
Epoch 320, val loss: 1.0898195505142212
Epoch 330, training loss: 869.5294189453125 = 1.0881993770599365 + 100.0 * 8.684412002563477
Epoch 330, val loss: 1.089761734008789
Epoch 340, training loss: 869.3115844726562 = 1.0881266593933105 + 100.0 * 8.682234764099121
Epoch 340, val loss: 1.0897036790847778
Epoch 350, training loss: 869.1021118164062 = 1.0880553722381592 + 100.0 * 8.680140495300293
Epoch 350, val loss: 1.0896486043930054
Epoch 360, training loss: 868.6911010742188 = 1.0879838466644287 + 100.0 * 8.676031112670898
Epoch 360, val loss: 1.0895928144454956
Epoch 370, training loss: 868.6087646484375 = 1.087915062904358 + 100.0 * 8.675209045410156
Epoch 370, val loss: 1.0895402431488037
Epoch 380, training loss: 868.4945678710938 = 1.0878480672836304 + 100.0 * 8.674067497253418
Epoch 380, val loss: 1.0894880294799805
Epoch 390, training loss: 868.5137939453125 = 1.0877807140350342 + 100.0 * 8.674260139465332
Epoch 390, val loss: 1.0894373655319214
Epoch 400, training loss: 868.514892578125 = 1.0877126455307007 + 100.0 * 8.674271583557129
Epoch 400, val loss: 1.0893827676773071
Epoch 410, training loss: 868.6216430664062 = 1.0876514911651611 + 100.0 * 8.675339698791504
Epoch 410, val loss: 1.0893372297286987
Epoch 420, training loss: 868.4451293945312 = 1.0875861644744873 + 100.0 * 8.673575401306152
Epoch 420, val loss: 1.0892882347106934
Epoch 430, training loss: 868.26904296875 = 1.0875245332717896 + 100.0 * 8.671814918518066
Epoch 430, val loss: 1.0892425775527954
Epoch 440, training loss: 868.2742919921875 = 1.08746337890625 + 100.0 * 8.671868324279785
Epoch 440, val loss: 1.0891960859298706
Epoch 450, training loss: 868.0291137695312 = 1.0874009132385254 + 100.0 * 8.669417381286621
Epoch 450, val loss: 1.0891491174697876
Epoch 460, training loss: 868.08203125 = 1.0873404741287231 + 100.0 * 8.669946670532227
Epoch 460, val loss: 1.0891039371490479
Epoch 470, training loss: 868.1616821289062 = 1.0872831344604492 + 100.0 * 8.670743942260742
Epoch 470, val loss: 1.0890624523162842
Epoch 480, training loss: 868.3533935546875 = 1.0872249603271484 + 100.0 * 8.672661781311035
Epoch 480, val loss: 1.089018702507019
Epoch 490, training loss: 868.5084838867188 = 1.087166428565979 + 100.0 * 8.674213409423828
Epoch 490, val loss: 1.0889744758605957
Epoch 500, training loss: 868.4894409179688 = 1.0871129035949707 + 100.0 * 8.674023628234863
Epoch 500, val loss: 1.0889352560043335
Epoch 510, training loss: 868.676513671875 = 1.0870572328567505 + 100.0 * 8.675894737243652
Epoch 510, val loss: 1.0888947248458862
Epoch 520, training loss: 868.6459350585938 = 1.0870037078857422 + 100.0 * 8.675589561462402
Epoch 520, val loss: 1.0888556241989136
Epoch 530, training loss: 868.4430541992188 = 1.0869492292404175 + 100.0 * 8.673561096191406
Epoch 530, val loss: 1.088815689086914
Epoch 540, training loss: 868.8489990234375 = 1.0868985652923584 + 100.0 * 8.677620887756348
Epoch 540, val loss: 1.0887786149978638
Epoch 550, training loss: 868.7608032226562 = 1.0868455171585083 + 100.0 * 8.676739692687988
Epoch 550, val loss: 1.0887402296066284
Epoch 560, training loss: 868.6702880859375 = 1.0867928266525269 + 100.0 * 8.675834655761719
Epoch 560, val loss: 1.0887031555175781
Epoch 570, training loss: 869.2249755859375 = 1.086746096611023 + 100.0 * 8.681382179260254
Epoch 570, val loss: 1.0886691808700562
Epoch 580, training loss: 869.2426147460938 = 1.0866978168487549 + 100.0 * 8.681559562683105
Epoch 580, val loss: 1.088634729385376
Epoch 590, training loss: 870.9437255859375 = 1.0866122245788574 + 100.0 * 8.69857120513916
Epoch 590, val loss: 1.088550329208374
Epoch 600, training loss: 870.2837524414062 = 1.0865347385406494 + 100.0 * 8.691971778869629
Epoch 600, val loss: 1.0885100364685059
Epoch 610, training loss: 874.21044921875 = 1.0865432024002075 + 100.0 * 8.731239318847656
Epoch 610, val loss: 1.088519811630249
Epoch 620, training loss: 870.4246215820312 = 1.0864940881729126 + 100.0 * 8.693381309509277
Epoch 620, val loss: 1.0884852409362793
Epoch 630, training loss: 870.1867065429688 = 1.0864455699920654 + 100.0 * 8.69100284576416
Epoch 630, val loss: 1.0884531736373901
Epoch 640, training loss: 870.9902954101562 = 1.0864102840423584 + 100.0 * 8.6990385055542
Epoch 640, val loss: 1.0884287357330322
Epoch 650, training loss: 870.382080078125 = 1.0863673686981201 + 100.0 * 8.692956924438477
Epoch 650, val loss: 1.0884000062942505
Epoch 660, training loss: 871.0087280273438 = 1.0863276720046997 + 100.0 * 8.699224472045898
Epoch 660, val loss: 1.0883736610412598
Epoch 670, training loss: 871.33349609375 = 1.0862879753112793 + 100.0 * 8.702471733093262
Epoch 670, val loss: 1.0883468389511108
Epoch 680, training loss: 871.8783569335938 = 1.0862480401992798 + 100.0 * 8.707921028137207
Epoch 680, val loss: 1.0883196592330933
Epoch 690, training loss: 872.1527709960938 = 1.086208462715149 + 100.0 * 8.710665702819824
Epoch 690, val loss: 1.088293194770813
Epoch 700, training loss: 872.3272705078125 = 1.0861687660217285 + 100.0 * 8.712410926818848
Epoch 700, val loss: 1.0882660150527954
Epoch 710, training loss: 872.5315551757812 = 1.08613121509552 + 100.0 * 8.714454650878906
Epoch 710, val loss: 1.088240623474121
Epoch 720, training loss: 872.5897827148438 = 1.0860928297042847 + 100.0 * 8.715036392211914
Epoch 720, val loss: 1.088215947151184
Epoch 730, training loss: 872.6180419921875 = 1.086055040359497 + 100.0 * 8.715319633483887
Epoch 730, val loss: 1.0881909132003784
Epoch 740, training loss: 873.2636108398438 = 1.0860191583633423 + 100.0 * 8.721776008605957
Epoch 740, val loss: 1.0881681442260742
Epoch 750, training loss: 871.9456176757812 = 1.0859782695770264 + 100.0 * 8.708596229553223
Epoch 750, val loss: 1.0881397724151611
Epoch 760, training loss: 872.6249389648438 = 1.0859456062316895 + 100.0 * 8.7153902053833
Epoch 760, val loss: 1.0881192684173584
Epoch 770, training loss: 873.4885864257812 = 1.085912823677063 + 100.0 * 8.724026679992676
Epoch 770, val loss: 1.0880987644195557
Epoch 780, training loss: 874.13916015625 = 1.0858790874481201 + 100.0 * 8.7305326461792
Epoch 780, val loss: 1.0880779027938843
Epoch 790, training loss: 873.79833984375 = 1.0858440399169922 + 100.0 * 8.72712516784668
Epoch 790, val loss: 1.088054895401001
Epoch 800, training loss: 874.3617553710938 = 1.0858136415481567 + 100.0 * 8.732759475708008
Epoch 800, val loss: 1.0880359411239624
Epoch 810, training loss: 874.6845092773438 = 1.0857807397842407 + 100.0 * 8.735987663269043
Epoch 810, val loss: 1.0880160331726074
Epoch 820, training loss: 875.006591796875 = 1.0857490301132202 + 100.0 * 8.739208221435547
Epoch 820, val loss: 1.0879967212677002
Epoch 830, training loss: 875.3209228515625 = 1.0857194662094116 + 100.0 * 8.742352485656738
Epoch 830, val loss: 1.0879781246185303
Epoch 840, training loss: 875.541748046875 = 1.085688829421997 + 100.0 * 8.744560241699219
Epoch 840, val loss: 1.0879592895507812
Epoch 850, training loss: 875.3766479492188 = 1.085658073425293 + 100.0 * 8.742910385131836
Epoch 850, val loss: 1.0879395008087158
Epoch 860, training loss: 875.4177856445312 = 1.0856285095214844 + 100.0 * 8.743321418762207
Epoch 860, val loss: 1.087922215461731
Epoch 870, training loss: 875.6612548828125 = 1.0856002569198608 + 100.0 * 8.745757102966309
Epoch 870, val loss: 1.0879056453704834
Epoch 880, training loss: 875.4801635742188 = 1.085565447807312 + 100.0 * 8.743946075439453
Epoch 880, val loss: 1.087884783744812
Epoch 890, training loss: 875.0399780273438 = 1.0855380296707153 + 100.0 * 8.739544868469238
Epoch 890, val loss: 1.0878678560256958
Epoch 900, training loss: 875.6758422851562 = 1.0855154991149902 + 100.0 * 8.745903015136719
Epoch 900, val loss: 1.0878554582595825
Epoch 910, training loss: 876.1192016601562 = 1.0854909420013428 + 100.0 * 8.750336647033691
Epoch 910, val loss: 1.0878417491912842
Epoch 920, training loss: 876.978759765625 = 1.0854672193527222 + 100.0 * 8.758933067321777
Epoch 920, val loss: 1.087829351425171
Epoch 930, training loss: 876.8114624023438 = 1.085440993309021 + 100.0 * 8.7572603225708
Epoch 930, val loss: 1.0878138542175293
Epoch 940, training loss: 876.9839477539062 = 1.085416316986084 + 100.0 * 8.75898551940918
Epoch 940, val loss: 1.08780038356781
Epoch 950, training loss: 877.3101196289062 = 1.0853915214538574 + 100.0 * 8.762247085571289
Epoch 950, val loss: 1.0877872705459595
Epoch 960, training loss: 877.6915893554688 = 1.0853692293167114 + 100.0 * 8.766061782836914
Epoch 960, val loss: 1.0877752304077148
Epoch 970, training loss: 878.250244140625 = 1.085347294807434 + 100.0 * 8.771649360656738
Epoch 970, val loss: 1.0877641439437866
Epoch 980, training loss: 878.145751953125 = 1.0853238105773926 + 100.0 * 8.770604133605957
Epoch 980, val loss: 1.08774995803833
Epoch 990, training loss: 878.1761474609375 = 1.0852998495101929 + 100.0 * 8.77090835571289
Epoch 990, val loss: 1.087738037109375
Epoch 1000, training loss: 878.4799194335938 = 1.085280179977417 + 100.0 * 8.773946762084961
Epoch 1000, val loss: 1.0877281427383423
Epoch 1010, training loss: 878.4349975585938 = 1.0852569341659546 + 100.0 * 8.773497581481934
Epoch 1010, val loss: 1.087715983390808
Epoch 1020, training loss: 878.7156982421875 = 1.0852352380752563 + 100.0 * 8.776305198669434
Epoch 1020, val loss: 1.0877046585083008
Epoch 1030, training loss: 878.7269897460938 = 1.0852140188217163 + 100.0 * 8.77641773223877
Epoch 1030, val loss: 1.087694525718689
Epoch 1040, training loss: 879.173583984375 = 1.0851970911026 + 100.0 * 8.7808837890625
Epoch 1040, val loss: 1.087687373161316
Epoch 1050, training loss: 879.6272583007812 = 1.0851796865463257 + 100.0 * 8.785420417785645
Epoch 1050, val loss: 1.0876789093017578
Epoch 1060, training loss: 879.7568969726562 = 1.0851600170135498 + 100.0 * 8.786717414855957
Epoch 1060, val loss: 1.0876673460006714
Epoch 1070, training loss: 879.6316528320312 = 1.0851401090621948 + 100.0 * 8.785465240478516
Epoch 1070, val loss: 1.0876580476760864
Epoch 1080, training loss: 880.0130615234375 = 1.085123062133789 + 100.0 * 8.789278984069824
Epoch 1080, val loss: 1.0876514911651611
Epoch 1090, training loss: 880.4091186523438 = 1.085106372833252 + 100.0 * 8.793240547180176
Epoch 1090, val loss: 1.087644338607788
Epoch 1100, training loss: 880.5657958984375 = 1.0850894451141357 + 100.0 * 8.794807434082031
Epoch 1100, val loss: 1.087636947631836
Epoch 1110, training loss: 880.7044067382812 = 1.0850725173950195 + 100.0 * 8.79619312286377
Epoch 1110, val loss: 1.0876295566558838
Epoch 1120, training loss: 881.2041015625 = 1.0850574970245361 + 100.0 * 8.801190376281738
Epoch 1120, val loss: 1.0876225233078003
Epoch 1130, training loss: 881.0916137695312 = 1.0850403308868408 + 100.0 * 8.800065994262695
Epoch 1130, val loss: 1.087615728378296
Epoch 1140, training loss: 881.2069091796875 = 1.0850248336791992 + 100.0 * 8.80121898651123
Epoch 1140, val loss: 1.0876097679138184
Epoch 1150, training loss: 881.5203857421875 = 1.0850093364715576 + 100.0 * 8.804353713989258
Epoch 1150, val loss: 1.0876028537750244
Epoch 1160, training loss: 881.5463256835938 = 1.0849956274032593 + 100.0 * 8.80461311340332
Epoch 1160, val loss: 1.0875970125198364
Epoch 1170, training loss: 881.7683715820312 = 1.0849809646606445 + 100.0 * 8.80683422088623
Epoch 1170, val loss: 1.0875922441482544
Epoch 1180, training loss: 882.0682373046875 = 1.084967851638794 + 100.0 * 8.809832572937012
Epoch 1180, val loss: 1.0875868797302246
Epoch 1190, training loss: 881.881103515625 = 1.0849536657333374 + 100.0 * 8.807961463928223
Epoch 1190, val loss: 1.087581753730774
Epoch 1200, training loss: 882.3634033203125 = 1.0849409103393555 + 100.0 * 8.812784194946289
Epoch 1200, val loss: 1.0875788927078247
Epoch 1210, training loss: 882.4276123046875 = 1.0849273204803467 + 100.0 * 8.813426971435547
Epoch 1210, val loss: 1.0875728130340576
Epoch 1220, training loss: 882.3799438476562 = 1.0849148035049438 + 100.0 * 8.812950134277344
Epoch 1220, val loss: 1.0875691175460815
Epoch 1230, training loss: 882.7909545898438 = 1.0849028825759888 + 100.0 * 8.817060470581055
Epoch 1230, val loss: 1.0875650644302368
Epoch 1240, training loss: 883.3560180664062 = 1.0848917961120605 + 100.0 * 8.822710990905762
Epoch 1240, val loss: 1.0875617265701294
Epoch 1250, training loss: 883.3629760742188 = 1.0848788022994995 + 100.0 * 8.82278060913086
Epoch 1250, val loss: 1.0875567197799683
Epoch 1260, training loss: 882.8949584960938 = 1.084865689277649 + 100.0 * 8.818100929260254
Epoch 1260, val loss: 1.0875513553619385
Epoch 1270, training loss: 882.5941162109375 = 1.0848549604415894 + 100.0 * 8.815093040466309
Epoch 1270, val loss: 1.0875500440597534
Epoch 1280, training loss: 882.8577270507812 = 1.0848402976989746 + 100.0 * 8.817728996276855
Epoch 1280, val loss: 1.0875413417816162
Epoch 1290, training loss: 883.0292358398438 = 1.0848312377929688 + 100.0 * 8.819443702697754
Epoch 1290, val loss: 1.0875436067581177
Epoch 1300, training loss: 883.453125 = 1.084823727607727 + 100.0 * 8.82368278503418
Epoch 1300, val loss: 1.0875426530838013
Epoch 1310, training loss: 884.072021484375 = 1.0848156213760376 + 100.0 * 8.829872131347656
Epoch 1310, val loss: 1.087543249130249
Epoch 1320, training loss: 884.330078125 = 1.0848039388656616 + 100.0 * 8.832452774047852
Epoch 1320, val loss: 1.0875385999679565
Epoch 1330, training loss: 883.9744262695312 = 1.0847960710525513 + 100.0 * 8.828896522521973
Epoch 1330, val loss: 1.0875381231307983
Epoch 1340, training loss: 884.5510864257812 = 1.0847880840301514 + 100.0 * 8.834663391113281
Epoch 1340, val loss: 1.0875378847122192
Epoch 1350, training loss: 885.1776733398438 = 1.0847814083099365 + 100.0 * 8.84092903137207
Epoch 1350, val loss: 1.0875376462936401
Epoch 1360, training loss: 884.7595825195312 = 1.084770917892456 + 100.0 * 8.836748123168945
Epoch 1360, val loss: 1.0875345468521118
Epoch 1370, training loss: 885.3634643554688 = 1.084764003753662 + 100.0 * 8.84278678894043
Epoch 1370, val loss: 1.0875357389450073
Epoch 1380, training loss: 885.947021484375 = 1.0847572088241577 + 100.0 * 8.84862232208252
Epoch 1380, val loss: 1.0875352621078491
Epoch 1390, training loss: 886.1392822265625 = 1.0847493410110474 + 100.0 * 8.850544929504395
Epoch 1390, val loss: 1.0875321626663208
Epoch 1400, training loss: 885.5936279296875 = 1.0847398042678833 + 100.0 * 8.845088958740234
Epoch 1400, val loss: 1.0875297784805298
Epoch 1410, training loss: 885.7354125976562 = 1.084732174873352 + 100.0 * 8.84650707244873
Epoch 1410, val loss: 1.0875294208526611
Epoch 1420, training loss: 886.2437133789062 = 1.0847253799438477 + 100.0 * 8.851590156555176
Epoch 1420, val loss: 1.087530493736267
Epoch 1430, training loss: 886.7210693359375 = 1.084720253944397 + 100.0 * 8.856363296508789
Epoch 1430, val loss: 1.0875324010849
Epoch 1440, training loss: 886.5680541992188 = 1.0847123861312866 + 100.0 * 8.854833602905273
Epoch 1440, val loss: 1.0875314474105835
Epoch 1450, training loss: 886.7962646484375 = 1.0847067832946777 + 100.0 * 8.857115745544434
Epoch 1450, val loss: 1.0875318050384521
Epoch 1460, training loss: 887.535400390625 = 1.0847015380859375 + 100.0 * 8.864506721496582
Epoch 1460, val loss: 1.0875325202941895
Epoch 1470, training loss: 887.7796020507812 = 1.084695816040039 + 100.0 * 8.866949081420898
Epoch 1470, val loss: 1.0875329971313477
Epoch 1480, training loss: 887.522216796875 = 1.0846872329711914 + 100.0 * 8.864375114440918
Epoch 1480, val loss: 1.0875309705734253
Epoch 1490, training loss: 887.3488159179688 = 1.0846805572509766 + 100.0 * 8.862641334533691
Epoch 1490, val loss: 1.087530493736267
Epoch 1500, training loss: 887.9994506835938 = 1.0846771001815796 + 100.0 * 8.869148254394531
Epoch 1500, val loss: 1.087532877922058
Epoch 1510, training loss: 888.2898559570312 = 1.084670901298523 + 100.0 * 8.872052192687988
Epoch 1510, val loss: 1.0875329971313477
Epoch 1520, training loss: 888.2446899414062 = 1.0846664905548096 + 100.0 * 8.871600151062012
Epoch 1520, val loss: 1.0875344276428223
Epoch 1530, training loss: 888.7301025390625 = 1.084661602973938 + 100.0 * 8.87645435333252
Epoch 1530, val loss: 1.0875355005264282
Epoch 1540, training loss: 889.025146484375 = 1.084656834602356 + 100.0 * 8.87940502166748
Epoch 1540, val loss: 1.0875362157821655
Epoch 1550, training loss: 889.0466918945312 = 1.084652066230774 + 100.0 * 8.879620552062988
Epoch 1550, val loss: 1.0875365734100342
Epoch 1560, training loss: 889.1822509765625 = 1.0846471786499023 + 100.0 * 8.880975723266602
Epoch 1560, val loss: 1.0875376462936401
Epoch 1570, training loss: 889.3219604492188 = 1.0846431255340576 + 100.0 * 8.882372856140137
Epoch 1570, val loss: 1.0875393152236938
Epoch 1580, training loss: 889.2286987304688 = 1.0846378803253174 + 100.0 * 8.881440162658691
Epoch 1580, val loss: 1.087539553642273
Epoch 1590, training loss: 889.4207153320312 = 1.084633469581604 + 100.0 * 8.883360862731934
Epoch 1590, val loss: 1.087540864944458
Epoch 1600, training loss: 889.8429565429688 = 1.0846295356750488 + 100.0 * 8.887582778930664
Epoch 1600, val loss: 1.0875412225723267
Epoch 1610, training loss: 888.7825927734375 = 1.0846223831176758 + 100.0 * 8.87697982788086
Epoch 1610, val loss: 1.0875385999679565
Epoch 1620, training loss: 887.5059204101562 = 1.084607481956482 + 100.0 * 8.864212989807129
Epoch 1620, val loss: 1.0875306129455566
Epoch 1630, training loss: 888.798095703125 = 1.0846086740493774 + 100.0 * 8.877135276794434
Epoch 1630, val loss: 1.0875355005264282
Epoch 1640, training loss: 888.636962890625 = 1.084607481956482 + 100.0 * 8.875523567199707
Epoch 1640, val loss: 1.0875403881072998
Epoch 1650, training loss: 889.2708740234375 = 1.0846071243286133 + 100.0 * 8.88186264038086
Epoch 1650, val loss: 1.0875444412231445
Epoch 1660, training loss: 890.1718139648438 = 1.0846067667007446 + 100.0 * 8.89087200164795
Epoch 1660, val loss: 1.0875493288040161
Epoch 1670, training loss: 890.1961059570312 = 1.0846027135849 + 100.0 * 8.891115188598633
Epoch 1670, val loss: 1.0875500440597534
Epoch 1680, training loss: 890.5419921875 = 1.0846010446548462 + 100.0 * 8.894574165344238
Epoch 1680, val loss: 1.08755362033844
Epoch 1690, training loss: 890.806640625 = 1.0845993757247925 + 100.0 * 8.897220611572266
Epoch 1690, val loss: 1.0875557661056519
Epoch 1700, training loss: 890.6387939453125 = 1.0845952033996582 + 100.0 * 8.89554214477539
Epoch 1700, val loss: 1.0875569581985474
Epoch 1710, training loss: 891.1058959960938 = 1.084593415260315 + 100.0 * 8.900213241577148
Epoch 1710, val loss: 1.0875598192214966
Epoch 1720, training loss: 891.4539184570312 = 1.0845916271209717 + 100.0 * 8.903693199157715
Epoch 1720, val loss: 1.0875622034072876
Epoch 1730, training loss: 891.56689453125 = 1.0845881700515747 + 100.0 * 8.904823303222656
Epoch 1730, val loss: 1.0875636339187622
Epoch 1740, training loss: 891.570068359375 = 1.0845850706100464 + 100.0 * 8.904854774475098
Epoch 1740, val loss: 1.087564468383789
Epoch 1750, training loss: 891.7989501953125 = 1.0845835208892822 + 100.0 * 8.907143592834473
Epoch 1750, val loss: 1.0875649452209473
Epoch 1760, training loss: 891.699951171875 = 1.0845792293548584 + 100.0 * 8.906153678894043
Epoch 1760, val loss: 1.0875669717788696
Epoch 1770, training loss: 891.7435302734375 = 1.0845770835876465 + 100.0 * 8.90658950805664
Epoch 1770, val loss: 1.0875685214996338
Epoch 1780, training loss: 892.3414916992188 = 1.084576964378357 + 100.0 * 8.912569046020508
Epoch 1780, val loss: 1.0875720977783203
Epoch 1790, training loss: 892.5310668945312 = 1.084574818611145 + 100.0 * 8.914464950561523
Epoch 1790, val loss: 1.0875742435455322
Epoch 1800, training loss: 892.2883911132812 = 1.0845723152160645 + 100.0 * 8.91203784942627
Epoch 1800, val loss: 1.087575078010559
Epoch 1810, training loss: 892.4954223632812 = 1.0845699310302734 + 100.0 * 8.914108276367188
Epoch 1810, val loss: 1.0875768661499023
Epoch 1820, training loss: 892.9144897460938 = 1.084566354751587 + 100.0 * 8.918298721313477
Epoch 1820, val loss: 1.0875773429870605
Epoch 1830, training loss: 893.0971069335938 = 1.0845651626586914 + 100.0 * 8.920125007629395
Epoch 1830, val loss: 1.0875799655914307
Epoch 1840, training loss: 893.1593017578125 = 1.084563136100769 + 100.0 * 8.920747756958008
Epoch 1840, val loss: 1.0875824689865112
Epoch 1850, training loss: 892.53759765625 = 1.0845590829849243 + 100.0 * 8.914530754089355
Epoch 1850, val loss: 1.087581992149353
Epoch 1860, training loss: 893.2316284179688 = 1.0845547914505005 + 100.0 * 8.921470642089844
Epoch 1860, val loss: 1.0875823497772217
Epoch 1870, training loss: 891.812255859375 = 1.0845515727996826 + 100.0 * 8.90727710723877
Epoch 1870, val loss: 1.087581753730774
Epoch 1880, training loss: 892.3158569335938 = 1.0845527648925781 + 100.0 * 8.912313461303711
Epoch 1880, val loss: 1.0875861644744873
Epoch 1890, training loss: 893.2586059570312 = 1.084553837776184 + 100.0 * 8.921740531921387
Epoch 1890, val loss: 1.0875905752182007
Epoch 1900, training loss: 893.8507080078125 = 1.0845550298690796 + 100.0 * 8.927661895751953
Epoch 1900, val loss: 1.0875946283340454
Epoch 1910, training loss: 894.1904296875 = 1.0845528841018677 + 100.0 * 8.931058883666992
Epoch 1910, val loss: 1.0875954627990723
Epoch 1920, training loss: 894.4827270507812 = 1.0845526456832886 + 100.0 * 8.933981895446777
Epoch 1920, val loss: 1.087598204612732
Epoch 1930, training loss: 894.439697265625 = 1.0845507383346558 + 100.0 * 8.933551788330078
Epoch 1930, val loss: 1.0875991582870483
Epoch 1940, training loss: 894.3319702148438 = 1.0845487117767334 + 100.0 * 8.932474136352539
Epoch 1940, val loss: 1.0876003503799438
Epoch 1950, training loss: 895.0919189453125 = 1.0845497846603394 + 100.0 * 8.94007396697998
Epoch 1950, val loss: 1.0876041650772095
Epoch 1960, training loss: 894.7730102539062 = 1.0845468044281006 + 100.0 * 8.936884880065918
Epoch 1960, val loss: 1.087604284286499
Epoch 1970, training loss: 895.2307739257812 = 1.0845460891723633 + 100.0 * 8.941462516784668
Epoch 1970, val loss: 1.08760666847229
Epoch 1980, training loss: 895.7139282226562 = 1.084546685218811 + 100.0 * 8.946293830871582
Epoch 1980, val loss: 1.0876094102859497
Epoch 1990, training loss: 895.3521118164062 = 1.0845447778701782 + 100.0 * 8.942675590515137
Epoch 1990, val loss: 1.0876092910766602
Epoch 2000, training loss: 895.1212768554688 = 1.0845402479171753 + 100.0 * 8.940367698669434
Epoch 2000, val loss: 1.0876061916351318
Epoch 2010, training loss: 895.055419921875 = 1.0845390558242798 + 100.0 * 8.939708709716797
Epoch 2010, val loss: 1.0876094102859497
Epoch 2020, training loss: 895.808349609375 = 1.0845403671264648 + 100.0 * 8.947237968444824
Epoch 2020, val loss: 1.0876142978668213
Epoch 2030, training loss: 896.770751953125 = 1.08454167842865 + 100.0 * 8.956862449645996
Epoch 2030, val loss: 1.087617039680481
Epoch 2040, training loss: 896.5824584960938 = 1.0845390558242798 + 100.0 * 8.954978942871094
Epoch 2040, val loss: 1.08761727809906
Epoch 2050, training loss: 896.5013427734375 = 1.084538221359253 + 100.0 * 8.954168319702148
Epoch 2050, val loss: 1.0876182317733765
Epoch 2060, training loss: 896.7846069335938 = 1.0845383405685425 + 100.0 * 8.957000732421875
Epoch 2060, val loss: 1.0876213312149048
Epoch 2070, training loss: 897.2781372070312 = 1.0845378637313843 + 100.0 * 8.961935997009277
Epoch 2070, val loss: 1.0876233577728271
Epoch 2080, training loss: 893.670654296875 = 1.0845149755477905 + 100.0 * 8.925861358642578
Epoch 2080, val loss: 1.087601900100708
Epoch 2090, training loss: 896.76904296875 = 1.0845235586166382 + 100.0 * 8.9568452835083
Epoch 2090, val loss: 1.0876134634017944
Epoch 2100, training loss: 896.4193725585938 = 1.0845208168029785 + 100.0 * 8.953348159790039
Epoch 2100, val loss: 1.0876153707504272
Epoch 2110, training loss: 896.7462768554688 = 1.0845235586166382 + 100.0 * 8.95661735534668
Epoch 2110, val loss: 1.0876208543777466
Epoch 2120, training loss: 896.9115600585938 = 1.08452308177948 + 100.0 * 8.958270072937012
Epoch 2120, val loss: 1.0876224040985107
Epoch 2130, training loss: 895.8630981445312 = 1.0845134258270264 + 100.0 * 8.947785377502441
Epoch 2130, val loss: 1.0876160860061646
Epoch 2140, training loss: 895.1361694335938 = 1.08451509475708 + 100.0 * 8.940516471862793
Epoch 2140, val loss: 1.0876165628433228
Epoch 2150, training loss: 895.7213745117188 = 1.0845158100128174 + 100.0 * 8.946368217468262
Epoch 2150, val loss: 1.0876184701919556
Epoch 2160, training loss: 896.306884765625 = 1.0845190286636353 + 100.0 * 8.952223777770996
Epoch 2160, val loss: 1.0876246690750122
Epoch 2170, training loss: 897.0663452148438 = 1.0845214128494263 + 100.0 * 8.959817886352539
Epoch 2170, val loss: 1.0876294374465942
Epoch 2180, training loss: 897.7789306640625 = 1.0845232009887695 + 100.0 * 8.966943740844727
Epoch 2180, val loss: 1.0876332521438599
Epoch 2190, training loss: 898.2734985351562 = 1.084524393081665 + 100.0 * 8.97188949584961
Epoch 2190, val loss: 1.0876349210739136
Epoch 2200, training loss: 898.3911743164062 = 1.0845245122909546 + 100.0 * 8.973066329956055
Epoch 2200, val loss: 1.0876370668411255
Epoch 2210, training loss: 898.5115356445312 = 1.0845245122909546 + 100.0 * 8.97426986694336
Epoch 2210, val loss: 1.0876389741897583
Epoch 2220, training loss: 898.8899536132812 = 1.0845248699188232 + 100.0 * 8.97805404663086
Epoch 2220, val loss: 1.0876411199569702
Epoch 2230, training loss: 899.1914672851562 = 1.0845255851745605 + 100.0 * 8.981069564819336
Epoch 2230, val loss: 1.0876426696777344
Epoch 2240, training loss: 899.3174438476562 = 1.084524393081665 + 100.0 * 8.982329368591309
Epoch 2240, val loss: 1.0876433849334717
Epoch 2250, training loss: 899.3599853515625 = 1.0845240354537964 + 100.0 * 8.982754707336426
Epoch 2250, val loss: 1.0876445770263672
Epoch 2260, training loss: 899.5147094726562 = 1.0845237970352173 + 100.0 * 8.984301567077637
Epoch 2260, val loss: 1.0876460075378418
Epoch 2270, training loss: 899.6539916992188 = 1.0845224857330322 + 100.0 * 8.985694885253906
Epoch 2270, val loss: 1.0876466035842896
Epoch 2280, training loss: 899.4189453125 = 1.0845223665237427 + 100.0 * 8.983344078063965
Epoch 2280, val loss: 1.0876473188400269
Epoch 2290, training loss: 899.3540649414062 = 1.084520936012268 + 100.0 * 8.982695579528809
Epoch 2290, val loss: 1.0876474380493164
Epoch 2300, training loss: 899.633056640625 = 1.084520936012268 + 100.0 * 8.985485076904297
Epoch 2300, val loss: 1.0876485109329224
Epoch 2310, training loss: 900.189697265625 = 1.084522008895874 + 100.0 * 8.99105167388916
Epoch 2310, val loss: 1.0876500606536865
Epoch 2320, training loss: 899.9578857421875 = 1.0845210552215576 + 100.0 * 8.988733291625977
Epoch 2320, val loss: 1.0876506567001343
Epoch 2330, training loss: 899.7474975585938 = 1.0845192670822144 + 100.0 * 8.986629486083984
Epoch 2330, val loss: 1.0876493453979492
Epoch 2340, training loss: 899.8055419921875 = 1.0845184326171875 + 100.0 * 8.987210273742676
Epoch 2340, val loss: 1.0876505374908447
Epoch 2350, training loss: 900.352294921875 = 1.0845191478729248 + 100.0 * 8.992677688598633
Epoch 2350, val loss: 1.0876528024673462
Epoch 2360, training loss: 900.759521484375 = 1.0845199823379517 + 100.0 * 8.996749877929688
Epoch 2360, val loss: 1.087654948234558
Epoch 2370, training loss: 900.4341430664062 = 1.0845180749893188 + 100.0 * 8.99349594116211
Epoch 2370, val loss: 1.087653398513794
Epoch 2380, training loss: 899.8553466796875 = 1.0845165252685547 + 100.0 * 8.98770809173584
Epoch 2380, val loss: 1.0876539945602417
Epoch 2390, training loss: 900.4651489257812 = 1.0845168828964233 + 100.0 * 8.993805885314941
Epoch 2390, val loss: 1.0876554250717163
Epoch 2400, training loss: 901.2413940429688 = 1.0845186710357666 + 100.0 * 9.001568794250488
Epoch 2400, val loss: 1.0876582860946655
Epoch 2410, training loss: 901.7124633789062 = 1.0845197439193726 + 100.0 * 9.006278991699219
Epoch 2410, val loss: 1.0876597166061401
Epoch 2420, training loss: 900.6277465820312 = 1.0845142602920532 + 100.0 * 8.995431900024414
Epoch 2420, val loss: 1.0876531600952148
Epoch 2430, training loss: 898.9996337890625 = 1.0844489336013794 + 100.0 * 8.979151725769043
Epoch 2430, val loss: 1.0875746011734009
Epoch 2440, training loss: 885.2706298828125 = 1.084358811378479 + 100.0 * 8.841862678527832
Epoch 2440, val loss: 1.087512493133545
Epoch 2450, training loss: 891.2314453125 = 1.0844330787658691 + 100.0 * 8.901470184326172
Epoch 2450, val loss: 1.0875892639160156
Epoch 2460, training loss: 892.8104248046875 = 1.084441065788269 + 100.0 * 8.91726016998291
Epoch 2460, val loss: 1.0875816345214844
Epoch 2470, training loss: 891.7318725585938 = 1.0844528675079346 + 100.0 * 8.906474113464355
Epoch 2470, val loss: 1.0876080989837646
Epoch 2480, training loss: 895.2435913085938 = 1.0844769477844238 + 100.0 * 8.941591262817383
Epoch 2480, val loss: 1.0876235961914062
Epoch 2490, training loss: 895.3154296875 = 1.0844796895980835 + 100.0 * 8.942309379577637
Epoch 2490, val loss: 1.087629795074463
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.39666666666666667
0.8175758892994277
=== training gcn model ===
Epoch 0, training loss: 1003.9202880859375 = 1.09722101688385 + 100.0 * 10.028230667114258
Epoch 0, val loss: 1.0969024896621704
Epoch 10, training loss: 958.2564697265625 = 1.0971735715866089 + 100.0 * 9.571593284606934
Epoch 10, val loss: 1.0968254804611206
Epoch 20, training loss: 941.8677368164062 = 1.0970866680145264 + 100.0 * 9.407706260681152
Epoch 20, val loss: 1.096731424331665
Epoch 30, training loss: 931.348388671875 = 1.0970251560211182 + 100.0 * 9.302513122558594
Epoch 30, val loss: 1.0966485738754272
Epoch 40, training loss: 923.5366821289062 = 1.0969454050064087 + 100.0 * 9.224397659301758
Epoch 40, val loss: 1.0965465307235718
Epoch 50, training loss: 917.1487426757812 = 1.0968563556671143 + 100.0 * 9.160518646240234
Epoch 50, val loss: 1.096439003944397
Epoch 60, training loss: 911.6881713867188 = 1.0967599153518677 + 100.0 * 9.105914115905762
Epoch 60, val loss: 1.0963246822357178
Epoch 70, training loss: 906.94775390625 = 1.0966662168502808 + 100.0 * 9.058510780334473
Epoch 70, val loss: 1.096214771270752
Epoch 80, training loss: 902.7461547851562 = 1.096573829650879 + 100.0 * 9.016495704650879
Epoch 80, val loss: 1.0961074829101562
Epoch 90, training loss: 898.9678955078125 = 1.096483588218689 + 100.0 * 8.978713989257812
Epoch 90, val loss: 1.0959999561309814
Epoch 100, training loss: 895.70361328125 = 1.096390724182129 + 100.0 * 8.946072578430176
Epoch 100, val loss: 1.0958926677703857
Epoch 110, training loss: 892.8917846679688 = 1.0963057279586792 + 100.0 * 8.917954444885254
Epoch 110, val loss: 1.0957913398742676
Epoch 120, training loss: 890.275634765625 = 1.0962209701538086 + 100.0 * 8.891794204711914
Epoch 120, val loss: 1.0956913232803345
Epoch 130, training loss: 887.98388671875 = 1.09613835811615 + 100.0 * 8.868877410888672
Epoch 130, val loss: 1.0955933332443237
Epoch 140, training loss: 885.88427734375 = 1.0960590839385986 + 100.0 * 8.847882270812988
Epoch 140, val loss: 1.095497727394104
Epoch 150, training loss: 883.9658203125 = 1.0959818363189697 + 100.0 * 8.82869815826416
Epoch 150, val loss: 1.095405101776123
Epoch 160, training loss: 882.2132568359375 = 1.0959069728851318 + 100.0 * 8.811173439025879
Epoch 160, val loss: 1.0953136682510376
Epoch 170, training loss: 880.6954956054688 = 1.0958330631256104 + 100.0 * 8.79599666595459
Epoch 170, val loss: 1.095223307609558
Epoch 180, training loss: 879.2733154296875 = 1.0957608222961426 + 100.0 * 8.78177547454834
Epoch 180, val loss: 1.0951366424560547
Epoch 190, training loss: 878.1600952148438 = 1.0956895351409912 + 100.0 * 8.770644187927246
Epoch 190, val loss: 1.0950490236282349
Epoch 200, training loss: 876.9645385742188 = 1.09562087059021 + 100.0 * 8.758688926696777
Epoch 200, val loss: 1.0949652194976807
Epoch 210, training loss: 876.006591796875 = 1.0955530405044556 + 100.0 * 8.749110221862793
Epoch 210, val loss: 1.0948798656463623
Epoch 220, training loss: 875.2468872070312 = 1.095486044883728 + 100.0 * 8.741514205932617
Epoch 220, val loss: 1.0947964191436768
Epoch 230, training loss: 874.4732055664062 = 1.0954231023788452 + 100.0 * 8.73377799987793
Epoch 230, val loss: 1.0947171449661255
Epoch 240, training loss: 873.7716064453125 = 1.0953593254089355 + 100.0 * 8.726762771606445
Epoch 240, val loss: 1.0946376323699951
Epoch 250, training loss: 873.1009521484375 = 1.0952973365783691 + 100.0 * 8.720056533813477
Epoch 250, val loss: 1.0945594310760498
Epoch 260, training loss: 872.513427734375 = 1.0952376127243042 + 100.0 * 8.714181900024414
Epoch 260, val loss: 1.0944842100143433
Epoch 270, training loss: 871.8462524414062 = 1.0951792001724243 + 100.0 * 8.707510948181152
Epoch 270, val loss: 1.094409465789795
Epoch 280, training loss: 871.2224731445312 = 1.0951210260391235 + 100.0 * 8.701273918151855
Epoch 280, val loss: 1.0943361520767212
Epoch 290, training loss: 870.689453125 = 1.0950645208358765 + 100.0 * 8.695943832397461
Epoch 290, val loss: 1.094264268875122
Epoch 300, training loss: 870.3141479492188 = 1.095008373260498 + 100.0 * 8.692191123962402
Epoch 300, val loss: 1.0941929817199707
Epoch 310, training loss: 870.0706787109375 = 1.0949523448944092 + 100.0 * 8.689757347106934
Epoch 310, val loss: 1.094120740890503
Epoch 320, training loss: 869.5499267578125 = 1.0948971509933472 + 100.0 * 8.684550285339355
Epoch 320, val loss: 1.0940525531768799
Epoch 330, training loss: 869.5089721679688 = 1.0948481559753418 + 100.0 * 8.684141159057617
Epoch 330, val loss: 1.0939865112304688
Epoch 340, training loss: 869.7164306640625 = 1.0947959423065186 + 100.0 * 8.686216354370117
Epoch 340, val loss: 1.093920350074768
Epoch 350, training loss: 869.30224609375 = 1.094746470451355 + 100.0 * 8.682075500488281
Epoch 350, val loss: 1.0938565731048584
Epoch 360, training loss: 869.1867065429688 = 1.09469735622406 + 100.0 * 8.680919647216797
Epoch 360, val loss: 1.0937914848327637
Epoch 370, training loss: 869.022216796875 = 1.0946495532989502 + 100.0 * 8.679275512695312
Epoch 370, val loss: 1.0937288999557495
Epoch 380, training loss: 868.9058227539062 = 1.0946025848388672 + 100.0 * 8.678112030029297
Epoch 380, val loss: 1.09366774559021
Epoch 390, training loss: 868.8603515625 = 1.0945568084716797 + 100.0 * 8.677658081054688
Epoch 390, val loss: 1.0936065912246704
Epoch 400, training loss: 868.592529296875 = 1.0945122241973877 + 100.0 * 8.674980163574219
Epoch 400, val loss: 1.0935477018356323
Epoch 410, training loss: 868.4093017578125 = 1.0944665670394897 + 100.0 * 8.673148155212402
Epoch 410, val loss: 1.0934882164001465
Epoch 420, training loss: 868.2482299804688 = 1.0944257974624634 + 100.0 * 8.671538352966309
Epoch 420, val loss: 1.0934325456619263
Epoch 430, training loss: 868.2628173828125 = 1.0943835973739624 + 100.0 * 8.671684265136719
Epoch 430, val loss: 1.0933771133422852
Epoch 440, training loss: 868.2718505859375 = 1.0943427085876465 + 100.0 * 8.671774864196777
Epoch 440, val loss: 1.0933221578598022
Epoch 450, training loss: 868.1959838867188 = 1.0943031311035156 + 100.0 * 8.671016693115234
Epoch 450, val loss: 1.0932683944702148
Epoch 460, training loss: 868.4276733398438 = 1.094264030456543 + 100.0 * 8.673334121704102
Epoch 460, val loss: 1.0932161808013916
Epoch 470, training loss: 868.386474609375 = 1.0942254066467285 + 100.0 * 8.672922134399414
Epoch 470, val loss: 1.0931633710861206
Epoch 480, training loss: 868.094482421875 = 1.0941886901855469 + 100.0 * 8.670002937316895
Epoch 480, val loss: 1.0931130647659302
Epoch 490, training loss: 868.2930297851562 = 1.0941522121429443 + 100.0 * 8.671988487243652
Epoch 490, val loss: 1.0930637121200562
Epoch 500, training loss: 868.3109130859375 = 1.0941157341003418 + 100.0 * 8.672167778015137
Epoch 500, val loss: 1.0930148363113403
Epoch 510, training loss: 867.8342895507812 = 1.0940805673599243 + 100.0 * 8.667402267456055
Epoch 510, val loss: 1.092965841293335
Epoch 520, training loss: 867.80615234375 = 1.0940455198287964 + 100.0 * 8.667120933532715
Epoch 520, val loss: 1.0929187536239624
Epoch 530, training loss: 868.2451782226562 = 1.0940136909484863 + 100.0 * 8.67151165008545
Epoch 530, val loss: 1.0928741693496704
Epoch 540, training loss: 868.2181396484375 = 1.0939807891845703 + 100.0 * 8.671241760253906
Epoch 540, val loss: 1.0928282737731934
Epoch 550, training loss: 868.1636352539062 = 1.0939489603042603 + 100.0 * 8.670697212219238
Epoch 550, val loss: 1.0927846431732178
Epoch 560, training loss: 868.6996459960938 = 1.0939176082611084 + 100.0 * 8.676056861877441
Epoch 560, val loss: 1.092740535736084
Epoch 570, training loss: 868.00048828125 = 1.0938867330551147 + 100.0 * 8.669066429138184
Epoch 570, val loss: 1.092697262763977
Epoch 580, training loss: 868.0938110351562 = 1.0938572883605957 + 100.0 * 8.669999122619629
Epoch 580, val loss: 1.0926542282104492
Epoch 590, training loss: 868.1436767578125 = 1.0938291549682617 + 100.0 * 8.670498847961426
Epoch 590, val loss: 1.0926142930984497
Epoch 600, training loss: 868.1924438476562 = 1.0938007831573486 + 100.0 * 8.67098617553711
Epoch 600, val loss: 1.092573881149292
Epoch 610, training loss: 868.2523803710938 = 1.0937727689743042 + 100.0 * 8.671586036682129
Epoch 610, val loss: 1.0925343036651611
Epoch 620, training loss: 868.3038330078125 = 1.0937460660934448 + 100.0 * 8.672101020812988
Epoch 620, val loss: 1.0924955606460571
Epoch 630, training loss: 868.614013671875 = 1.0937199592590332 + 100.0 * 8.675202369689941
Epoch 630, val loss: 1.092457890510559
Epoch 640, training loss: 868.8515625 = 1.093695044517517 + 100.0 * 8.677578926086426
Epoch 640, val loss: 1.0924206972122192
Epoch 650, training loss: 868.8889770507812 = 1.0936695337295532 + 100.0 * 8.677952766418457
Epoch 650, val loss: 1.0923830270767212
Epoch 660, training loss: 875.8350219726562 = 1.0935574769973755 + 100.0 * 8.747414588928223
Epoch 660, val loss: 1.0922447443008423
Epoch 670, training loss: 874.1371459960938 = 1.0934886932373047 + 100.0 * 8.730436325073242
Epoch 670, val loss: 1.0921871662139893
Epoch 680, training loss: 872.0543823242188 = 1.0935386419296265 + 100.0 * 8.70960807800293
Epoch 680, val loss: 1.0922268629074097
Epoch 690, training loss: 872.4077758789062 = 1.0935406684875488 + 100.0 * 8.713142395019531
Epoch 690, val loss: 1.0922107696533203
Epoch 700, training loss: 869.8484497070312 = 1.0935156345367432 + 100.0 * 8.687549591064453
Epoch 700, val loss: 1.0921772718429565
Epoch 710, training loss: 870.3966064453125 = 1.0935049057006836 + 100.0 * 8.693031311035156
Epoch 710, val loss: 1.0921541452407837
Epoch 720, training loss: 870.4513549804688 = 1.0934851169586182 + 100.0 * 8.693578720092773
Epoch 720, val loss: 1.0921242237091064
Epoch 730, training loss: 870.7035522460938 = 1.0934687852859497 + 100.0 * 8.696101188659668
Epoch 730, val loss: 1.0920966863632202
Epoch 740, training loss: 871.3681030273438 = 1.0934523344039917 + 100.0 * 8.702746391296387
Epoch 740, val loss: 1.0920687913894653
Epoch 750, training loss: 871.7870483398438 = 1.0934340953826904 + 100.0 * 8.70693588256836
Epoch 750, val loss: 1.0920406579971313
Epoch 760, training loss: 872.0103759765625 = 1.09341561794281 + 100.0 * 8.709169387817383
Epoch 760, val loss: 1.0920120477676392
Epoch 770, training loss: 872.5284423828125 = 1.0933979749679565 + 100.0 * 8.714350700378418
Epoch 770, val loss: 1.0919835567474365
Epoch 780, training loss: 872.656982421875 = 1.0933806896209717 + 100.0 * 8.715636253356934
Epoch 780, val loss: 1.091956377029419
Epoch 790, training loss: 872.78662109375 = 1.0933631658554077 + 100.0 * 8.71693229675293
Epoch 790, val loss: 1.091929316520691
Epoch 800, training loss: 872.985107421875 = 1.0933470726013184 + 100.0 * 8.718917846679688
Epoch 800, val loss: 1.091902732849121
Epoch 810, training loss: 873.1085815429688 = 1.0933302640914917 + 100.0 * 8.720152854919434
Epoch 810, val loss: 1.0918762683868408
Epoch 820, training loss: 873.5286865234375 = 1.09331476688385 + 100.0 * 8.724353790283203
Epoch 820, val loss: 1.0918505191802979
Epoch 830, training loss: 873.5668334960938 = 1.0932992696762085 + 100.0 * 8.724735260009766
Epoch 830, val loss: 1.0918259620666504
Epoch 840, training loss: 873.7640991210938 = 1.0932835340499878 + 100.0 * 8.72670841217041
Epoch 840, val loss: 1.0918009281158447
Epoch 850, training loss: 874.0538330078125 = 1.0932698249816895 + 100.0 * 8.729605674743652
Epoch 850, val loss: 1.0917778015136719
Epoch 860, training loss: 874.326171875 = 1.093255639076233 + 100.0 * 8.732329368591309
Epoch 860, val loss: 1.0917537212371826
Epoch 870, training loss: 874.5892333984375 = 1.0932421684265137 + 100.0 * 8.734959602355957
Epoch 870, val loss: 1.091731071472168
Epoch 880, training loss: 874.7044677734375 = 1.0932285785675049 + 100.0 * 8.736112594604492
Epoch 880, val loss: 1.0917081832885742
Epoch 890, training loss: 875.09326171875 = 1.093216061592102 + 100.0 * 8.74000072479248
Epoch 890, val loss: 1.0916863679885864
Epoch 900, training loss: 875.1475219726562 = 1.093201994895935 + 100.0 * 8.740543365478516
Epoch 900, val loss: 1.0916645526885986
Epoch 910, training loss: 875.307861328125 = 1.0931907892227173 + 100.0 * 8.742146492004395
Epoch 910, val loss: 1.0916433334350586
Epoch 920, training loss: 875.3125 = 1.0931767225265503 + 100.0 * 8.742193222045898
Epoch 920, val loss: 1.0916208028793335
Epoch 930, training loss: 875.7708129882812 = 1.093168020248413 + 100.0 * 8.746776580810547
Epoch 930, val loss: 1.0916032791137695
Epoch 940, training loss: 875.81591796875 = 1.093156099319458 + 100.0 * 8.747227668762207
Epoch 940, val loss: 1.0915825366973877
Epoch 950, training loss: 875.793701171875 = 1.0931456089019775 + 100.0 * 8.747005462646484
Epoch 950, val loss: 1.0915632247924805
Epoch 960, training loss: 876.2567138671875 = 1.093135118484497 + 100.0 * 8.751635551452637
Epoch 960, val loss: 1.091545581817627
Epoch 970, training loss: 876.5034790039062 = 1.0931246280670166 + 100.0 * 8.754103660583496
Epoch 970, val loss: 1.0915265083312988
Epoch 980, training loss: 876.765869140625 = 1.0931153297424316 + 100.0 * 8.75672721862793
Epoch 980, val loss: 1.0915086269378662
Epoch 990, training loss: 877.018798828125 = 1.0931049585342407 + 100.0 * 8.759257316589355
Epoch 990, val loss: 1.0914909839630127
Epoch 1000, training loss: 876.9785766601562 = 1.0930957794189453 + 100.0 * 8.758854866027832
Epoch 1000, val loss: 1.0914738178253174
Epoch 1010, training loss: 877.16796875 = 1.093087077140808 + 100.0 * 8.760748863220215
Epoch 1010, val loss: 1.091456651687622
Epoch 1020, training loss: 877.2770385742188 = 1.0930778980255127 + 100.0 * 8.761839866638184
Epoch 1020, val loss: 1.0914400815963745
Epoch 1030, training loss: 877.5758666992188 = 1.0930695533752441 + 100.0 * 8.764827728271484
Epoch 1030, val loss: 1.0914244651794434
Epoch 1040, training loss: 877.7463989257812 = 1.0930614471435547 + 100.0 * 8.766532897949219
Epoch 1040, val loss: 1.09140944480896
Epoch 1050, training loss: 877.7959594726562 = 1.0930533409118652 + 100.0 * 8.76702880859375
Epoch 1050, val loss: 1.0913935899734497
Epoch 1060, training loss: 878.1134643554688 = 1.0930461883544922 + 100.0 * 8.770204544067383
Epoch 1060, val loss: 1.0913790464401245
Epoch 1070, training loss: 878.3045654296875 = 1.09303879737854 + 100.0 * 8.772115707397461
Epoch 1070, val loss: 1.0913645029067993
Epoch 1080, training loss: 878.762939453125 = 1.0930323600769043 + 100.0 * 8.77669906616211
Epoch 1080, val loss: 1.0913505554199219
Epoch 1090, training loss: 878.5281372070312 = 1.0930250883102417 + 100.0 * 8.774351119995117
Epoch 1090, val loss: 1.0913366079330444
Epoch 1100, training loss: 878.7443237304688 = 1.0930185317993164 + 100.0 * 8.77651309967041
Epoch 1100, val loss: 1.0913231372833252
Epoch 1110, training loss: 879.0509033203125 = 1.0930122137069702 + 100.0 * 8.779579162597656
Epoch 1110, val loss: 1.091309905052185
Epoch 1120, training loss: 879.0328369140625 = 1.0930044651031494 + 100.0 * 8.779397964477539
Epoch 1120, val loss: 1.091296672821045
Epoch 1130, training loss: 879.0944213867188 = 1.092998743057251 + 100.0 * 8.780014038085938
Epoch 1130, val loss: 1.0912835597991943
Epoch 1140, training loss: 879.1676635742188 = 1.0929925441741943 + 100.0 * 8.780746459960938
Epoch 1140, val loss: 1.0912714004516602
Epoch 1150, training loss: 879.2926025390625 = 1.0929862260818481 + 100.0 * 8.781996726989746
Epoch 1150, val loss: 1.0912595987319946
Epoch 1160, training loss: 879.69287109375 = 1.0929816961288452 + 100.0 * 8.785999298095703
Epoch 1160, val loss: 1.0912480354309082
Epoch 1170, training loss: 880.009033203125 = 1.0929770469665527 + 100.0 * 8.78916072845459
Epoch 1170, val loss: 1.0912362337112427
Epoch 1180, training loss: 879.9844360351562 = 1.0929698944091797 + 100.0 * 8.788914680480957
Epoch 1180, val loss: 1.0912234783172607
Epoch 1190, training loss: 879.9205322265625 = 1.0929654836654663 + 100.0 * 8.788275718688965
Epoch 1190, val loss: 1.0912134647369385
Epoch 1200, training loss: 880.1666870117188 = 1.0929605960845947 + 100.0 * 8.79073715209961
Epoch 1200, val loss: 1.091203212738037
Epoch 1210, training loss: 880.1702270507812 = 1.0929564237594604 + 100.0 * 8.790772438049316
Epoch 1210, val loss: 1.091193437576294
Epoch 1220, training loss: 880.6015625 = 1.0929524898529053 + 100.0 * 8.795085906982422
Epoch 1220, val loss: 1.0911834239959717
Epoch 1230, training loss: 880.6588745117188 = 1.092948079109192 + 100.0 * 8.795659065246582
Epoch 1230, val loss: 1.0911734104156494
Epoch 1240, training loss: 880.6979370117188 = 1.0929434299468994 + 100.0 * 8.796050071716309
Epoch 1240, val loss: 1.0911632776260376
Epoch 1250, training loss: 880.4097900390625 = 1.0929374694824219 + 100.0 * 8.793168067932129
Epoch 1250, val loss: 1.091152310371399
Epoch 1260, training loss: 880.908447265625 = 1.0929348468780518 + 100.0 * 8.798154830932617
Epoch 1260, val loss: 1.0911449193954468
Epoch 1270, training loss: 881.315185546875 = 1.0929330587387085 + 100.0 * 8.80222225189209
Epoch 1270, val loss: 1.091137409210205
Epoch 1280, training loss: 881.1967163085938 = 1.09292733669281 + 100.0 * 8.801037788391113
Epoch 1280, val loss: 1.0911273956298828
Epoch 1290, training loss: 881.2915649414062 = 1.0929245948791504 + 100.0 * 8.801986694335938
Epoch 1290, val loss: 1.0911191701889038
Epoch 1300, training loss: 881.5828247070312 = 1.0929216146469116 + 100.0 * 8.804899215698242
Epoch 1300, val loss: 1.0911105871200562
Epoch 1310, training loss: 881.9297485351562 = 1.0929187536239624 + 100.0 * 8.808368682861328
Epoch 1310, val loss: 1.091102957725525
Epoch 1320, training loss: 881.650634765625 = 1.092914342880249 + 100.0 * 8.805577278137207
Epoch 1320, val loss: 1.0910944938659668
Epoch 1330, training loss: 881.9746704101562 = 1.0929118394851685 + 100.0 * 8.808817863464355
Epoch 1330, val loss: 1.0910871028900146
Epoch 1340, training loss: 881.6156005859375 = 1.092901349067688 + 100.0 * 8.805227279663086
Epoch 1340, val loss: 1.0910700559616089
Epoch 1350, training loss: 881.6445922851562 = 1.0929005146026611 + 100.0 * 8.805517196655273
Epoch 1350, val loss: 1.0910654067993164
Epoch 1360, training loss: 881.9327392578125 = 1.092899203300476 + 100.0 * 8.808398246765137
Epoch 1360, val loss: 1.0910618305206299
Epoch 1370, training loss: 882.6099243164062 = 1.0928990840911865 + 100.0 * 8.815170288085938
Epoch 1370, val loss: 1.091057300567627
Epoch 1380, training loss: 882.728271484375 = 1.0928974151611328 + 100.0 * 8.816353797912598
Epoch 1380, val loss: 1.0910508632659912
Epoch 1390, training loss: 882.9360961914062 = 1.0928956270217896 + 100.0 * 8.818431854248047
Epoch 1390, val loss: 1.091044545173645
Epoch 1400, training loss: 883.3564453125 = 1.092894196510315 + 100.0 * 8.822635650634766
Epoch 1400, val loss: 1.0910383462905884
Epoch 1410, training loss: 883.5206909179688 = 1.092890977859497 + 100.0 * 8.824277877807617
Epoch 1410, val loss: 1.0910320281982422
Epoch 1420, training loss: 883.4854125976562 = 1.0928888320922852 + 100.0 * 8.823925018310547
Epoch 1420, val loss: 1.091025710105896
Epoch 1430, training loss: 883.9815673828125 = 1.0928863286972046 + 100.0 * 8.828886985778809
Epoch 1430, val loss: 1.091020107269287
Epoch 1440, training loss: 884.218017578125 = 1.0928843021392822 + 100.0 * 8.83125114440918
Epoch 1440, val loss: 1.0910143852233887
Epoch 1450, training loss: 883.9698486328125 = 1.0928804874420166 + 100.0 * 8.82876968383789
Epoch 1450, val loss: 1.0910067558288574
Epoch 1460, training loss: 884.2362060546875 = 1.0928796529769897 + 100.0 * 8.831433296203613
Epoch 1460, val loss: 1.0910035371780396
Epoch 1470, training loss: 884.6029052734375 = 1.092879295349121 + 100.0 * 8.835100173950195
Epoch 1470, val loss: 1.0909992456436157
Epoch 1480, training loss: 884.3573608398438 = 1.092875599861145 + 100.0 * 8.83264446258545
Epoch 1480, val loss: 1.090991735458374
Epoch 1490, training loss: 884.7340698242188 = 1.092874526977539 + 100.0 * 8.836411476135254
Epoch 1490, val loss: 1.0909875631332397
Epoch 1500, training loss: 885.0584106445312 = 1.092873454093933 + 100.0 * 8.839654922485352
Epoch 1500, val loss: 1.0909826755523682
Epoch 1510, training loss: 884.7977294921875 = 1.092871904373169 + 100.0 * 8.837048530578613
Epoch 1510, val loss: 1.0909781455993652
Epoch 1520, training loss: 884.8543090820312 = 1.092869520187378 + 100.0 * 8.837614059448242
Epoch 1520, val loss: 1.0909724235534668
Epoch 1530, training loss: 885.2752685546875 = 1.0928699970245361 + 100.0 * 8.84182357788086
Epoch 1530, val loss: 1.0909700393676758
Epoch 1540, training loss: 885.4766845703125 = 1.092868447303772 + 100.0 * 8.84383773803711
Epoch 1540, val loss: 1.0909639596939087
Epoch 1550, training loss: 885.5802612304688 = 1.0928670167922974 + 100.0 * 8.844873428344727
Epoch 1550, val loss: 1.0909603834152222
Epoch 1560, training loss: 885.760009765625 = 1.0928657054901123 + 100.0 * 8.846671104431152
Epoch 1560, val loss: 1.0909558534622192
Epoch 1570, training loss: 885.517822265625 = 1.0928627252578735 + 100.0 * 8.844249725341797
Epoch 1570, val loss: 1.090949535369873
Epoch 1580, training loss: 885.4130249023438 = 1.0928601026535034 + 100.0 * 8.843201637268066
Epoch 1580, val loss: 1.090946078300476
Epoch 1590, training loss: 886.2063598632812 = 1.0928623676300049 + 100.0 * 8.85113525390625
Epoch 1590, val loss: 1.0909446477890015
Epoch 1600, training loss: 886.3771362304688 = 1.0928608179092407 + 100.0 * 8.852843284606934
Epoch 1600, val loss: 1.0909405946731567
Epoch 1610, training loss: 886.4088134765625 = 1.092860221862793 + 100.0 * 8.85315990447998
Epoch 1610, val loss: 1.0909373760223389
Epoch 1620, training loss: 886.7685546875 = 1.0928598642349243 + 100.0 * 8.856757164001465
Epoch 1620, val loss: 1.0909345149993896
Epoch 1630, training loss: 886.5712280273438 = 1.0928571224212646 + 100.0 * 8.85478401184082
Epoch 1630, val loss: 1.0909301042556763
Epoch 1640, training loss: 886.7202758789062 = 1.09285569190979 + 100.0 * 8.856274604797363
Epoch 1640, val loss: 1.0909267663955688
Epoch 1650, training loss: 886.9949340820312 = 1.092855453491211 + 100.0 * 8.859021186828613
Epoch 1650, val loss: 1.0909234285354614
Epoch 1660, training loss: 887.0808715820312 = 1.0928547382354736 + 100.0 * 8.859880447387695
Epoch 1660, val loss: 1.0909199714660645
Epoch 1670, training loss: 887.245361328125 = 1.0928539037704468 + 100.0 * 8.861525535583496
Epoch 1670, val loss: 1.090916633605957
Epoch 1680, training loss: 887.3890380859375 = 1.0928536653518677 + 100.0 * 8.862961769104004
Epoch 1680, val loss: 1.0909147262573242
Epoch 1690, training loss: 887.49560546875 = 1.0928527116775513 + 100.0 * 8.864027976989746
Epoch 1690, val loss: 1.0909113883972168
Epoch 1700, training loss: 887.4104614257812 = 1.0928508043289185 + 100.0 * 8.863176345825195
Epoch 1700, val loss: 1.0909079313278198
Epoch 1710, training loss: 886.6088256835938 = 1.0928411483764648 + 100.0 * 8.855159759521484
Epoch 1710, val loss: 1.0909003019332886
Epoch 1720, training loss: 887.7775268554688 = 1.092847466468811 + 100.0 * 8.866847038269043
Epoch 1720, val loss: 1.0909031629562378
Epoch 1730, training loss: 887.82568359375 = 1.0928500890731812 + 100.0 * 8.867328643798828
Epoch 1730, val loss: 1.090903639793396
Epoch 1740, training loss: 888.5170288085938 = 1.0928525924682617 + 100.0 * 8.874241828918457
Epoch 1740, val loss: 1.090903401374817
Epoch 1750, training loss: 889.10302734375 = 1.0928541421890259 + 100.0 * 8.880102157592773
Epoch 1750, val loss: 1.090903639793396
Epoch 1760, training loss: 889.5237426757812 = 1.092855453491211 + 100.0 * 8.884308815002441
Epoch 1760, val loss: 1.0909016132354736
Epoch 1770, training loss: 889.4909057617188 = 1.0928540229797363 + 100.0 * 8.883980751037598
Epoch 1770, val loss: 1.0908993482589722
Epoch 1780, training loss: 889.7351684570312 = 1.0928540229797363 + 100.0 * 8.886423110961914
Epoch 1780, val loss: 1.0908976793289185
Epoch 1790, training loss: 889.859619140625 = 1.09285306930542 + 100.0 * 8.887667655944824
Epoch 1790, val loss: 1.090895414352417
Epoch 1800, training loss: 890.0869750976562 = 1.0928531885147095 + 100.0 * 8.889941215515137
Epoch 1800, val loss: 1.0908937454223633
Epoch 1810, training loss: 890.1465454101562 = 1.0928515195846558 + 100.0 * 8.89053726196289
Epoch 1810, val loss: 1.090890645980835
Epoch 1820, training loss: 890.3771362304688 = 1.0928508043289185 + 100.0 * 8.892843246459961
Epoch 1820, val loss: 1.09088933467865
Epoch 1830, training loss: 890.4329223632812 = 1.092850923538208 + 100.0 * 8.893401145935059
Epoch 1830, val loss: 1.090887427330017
Epoch 1840, training loss: 890.8995971679688 = 1.0928510427474976 + 100.0 * 8.898067474365234
Epoch 1840, val loss: 1.0908864736557007
Epoch 1850, training loss: 890.413330078125 = 1.0928478240966797 + 100.0 * 8.893204689025879
Epoch 1850, val loss: 1.0908812284469604
Epoch 1860, training loss: 890.8063354492188 = 1.0928469896316528 + 100.0 * 8.897134780883789
Epoch 1860, val loss: 1.090879201889038
Epoch 1870, training loss: 891.1976318359375 = 1.0928481817245483 + 100.0 * 8.901047706604004
Epoch 1870, val loss: 1.0908788442611694
Epoch 1880, training loss: 890.513427734375 = 1.0928441286087036 + 100.0 * 8.894206047058105
Epoch 1880, val loss: 1.0908749103546143
Epoch 1890, training loss: 891.0772705078125 = 1.0928455591201782 + 100.0 * 8.8998441696167
Epoch 1890, val loss: 1.090875267982483
Epoch 1900, training loss: 891.7000732421875 = 1.0928473472595215 + 100.0 * 8.906072616577148
Epoch 1900, val loss: 1.0908750295639038
Epoch 1910, training loss: 891.926513671875 = 1.0928479433059692 + 100.0 * 8.908336639404297
Epoch 1910, val loss: 1.0908739566802979
Epoch 1920, training loss: 891.9368286132812 = 1.0928460359573364 + 100.0 * 8.908439636230469
Epoch 1920, val loss: 1.0908719301223755
Epoch 1930, training loss: 892.1966552734375 = 1.092846393585205 + 100.0 * 8.911038398742676
Epoch 1930, val loss: 1.0908697843551636
Epoch 1940, training loss: 891.083251953125 = 1.0928415060043335 + 100.0 * 8.899904251098633
Epoch 1940, val loss: 1.0908645391464233
Epoch 1950, training loss: 891.296142578125 = 1.0928410291671753 + 100.0 * 8.902032852172852
Epoch 1950, val loss: 1.0908639430999756
Epoch 1960, training loss: 891.8948364257812 = 1.09284245967865 + 100.0 * 8.90802001953125
Epoch 1960, val loss: 1.0908647775650024
Epoch 1970, training loss: 892.2515258789062 = 1.0928434133529663 + 100.0 * 8.91158676147461
Epoch 1970, val loss: 1.0908644199371338
Epoch 1980, training loss: 892.5982666015625 = 1.0928442478179932 + 100.0 * 8.915054321289062
Epoch 1980, val loss: 1.0908639430999756
Epoch 1990, training loss: 892.5860595703125 = 1.092842936515808 + 100.0 * 8.914932250976562
Epoch 1990, val loss: 1.0908617973327637
Epoch 2000, training loss: 892.8099975585938 = 1.0928432941436768 + 100.0 * 8.917171478271484
Epoch 2000, val loss: 1.090861201286316
Epoch 2010, training loss: 892.6827392578125 = 1.0928415060043335 + 100.0 * 8.915899276733398
Epoch 2010, val loss: 1.090859293937683
Epoch 2020, training loss: 892.3280639648438 = 1.0928382873535156 + 100.0 * 8.912352561950684
Epoch 2020, val loss: 1.0908550024032593
Epoch 2030, training loss: 892.369384765625 = 1.0928372144699097 + 100.0 * 8.912765502929688
Epoch 2030, val loss: 1.0908540487289429
Epoch 2040, training loss: 893.0004272460938 = 1.0928391218185425 + 100.0 * 8.919075965881348
Epoch 2040, val loss: 1.0908550024032593
Epoch 2050, training loss: 893.2627563476562 = 1.0928395986557007 + 100.0 * 8.921699523925781
Epoch 2050, val loss: 1.0908552408218384
Epoch 2060, training loss: 893.330322265625 = 1.092839241027832 + 100.0 * 8.922374725341797
Epoch 2060, val loss: 1.090852975845337
Epoch 2070, training loss: 893.471923828125 = 1.092838168144226 + 100.0 * 8.92379093170166
Epoch 2070, val loss: 1.0908530950546265
Epoch 2080, training loss: 893.5328979492188 = 1.092839002609253 + 100.0 * 8.924400329589844
Epoch 2080, val loss: 1.0908523797988892
Epoch 2090, training loss: 893.6978759765625 = 1.0928380489349365 + 100.0 * 8.926050186157227
Epoch 2090, val loss: 1.0908511877059937
Epoch 2100, training loss: 893.7796630859375 = 1.0928378105163574 + 100.0 * 8.926868438720703
Epoch 2100, val loss: 1.0908502340316772
Epoch 2110, training loss: 893.8306884765625 = 1.0928367376327515 + 100.0 * 8.92737865447998
Epoch 2110, val loss: 1.0908483266830444
Epoch 2120, training loss: 893.9244384765625 = 1.0928360223770142 + 100.0 * 8.928316116333008
Epoch 2120, val loss: 1.0908465385437012
Epoch 2130, training loss: 894.2999877929688 = 1.0928364992141724 + 100.0 * 8.932071685791016
Epoch 2130, val loss: 1.0908467769622803
Epoch 2140, training loss: 894.407958984375 = 1.092836618423462 + 100.0 * 8.933151245117188
Epoch 2140, val loss: 1.0908457040786743
Epoch 2150, training loss: 894.1849975585938 = 1.0928338766098022 + 100.0 * 8.93092155456543
Epoch 2150, val loss: 1.0908442735671997
Epoch 2160, training loss: 894.3842163085938 = 1.092834234237671 + 100.0 * 8.932913780212402
Epoch 2160, val loss: 1.090843677520752
Epoch 2170, training loss: 894.5707397460938 = 1.0928341150283813 + 100.0 * 8.934779167175293
Epoch 2170, val loss: 1.0908430814743042
Epoch 2180, training loss: 894.5863647460938 = 1.092832326889038 + 100.0 * 8.934935569763184
Epoch 2180, val loss: 1.0908408164978027
Epoch 2190, training loss: 894.7551879882812 = 1.0928318500518799 + 100.0 * 8.936623573303223
Epoch 2190, val loss: 1.0908410549163818
Epoch 2200, training loss: 894.9661254882812 = 1.0928335189819336 + 100.0 * 8.938733100891113
Epoch 2200, val loss: 1.0908408164978027
Epoch 2210, training loss: 895.1849365234375 = 1.0928328037261963 + 100.0 * 8.94092082977295
Epoch 2210, val loss: 1.0908398628234863
Epoch 2220, training loss: 894.4609985351562 = 1.0928287506103516 + 100.0 * 8.93368148803711
Epoch 2220, val loss: 1.0908352136611938
Epoch 2230, training loss: 894.2410888671875 = 1.0928270816802979 + 100.0 * 8.931482315063477
Epoch 2230, val loss: 1.0908347368240356
Epoch 2240, training loss: 894.6663208007812 = 1.092828392982483 + 100.0 * 8.935734748840332
Epoch 2240, val loss: 1.0908361673355103
Epoch 2250, training loss: 895.3782958984375 = 1.092829942703247 + 100.0 * 8.942854881286621
Epoch 2250, val loss: 1.0908368825912476
Epoch 2260, training loss: 895.5206298828125 = 1.0928294658660889 + 100.0 * 8.9442777633667
Epoch 2260, val loss: 1.090835452079773
Epoch 2270, training loss: 895.1946411132812 = 1.0928274393081665 + 100.0 * 8.941018104553223
Epoch 2270, val loss: 1.0908339023590088
Epoch 2280, training loss: 894.439697265625 = 1.0928168296813965 + 100.0 * 8.93346881866455
Epoch 2280, val loss: 1.0908244848251343
Epoch 2290, training loss: 894.386474609375 = 1.0928165912628174 + 100.0 * 8.932936668395996
Epoch 2290, val loss: 1.0908230543136597
Epoch 2300, training loss: 894.4893798828125 = 1.0928188562393188 + 100.0 * 8.933965682983398
Epoch 2300, val loss: 1.0908255577087402
Epoch 2310, training loss: 895.1871337890625 = 1.0928219556808472 + 100.0 * 8.940942764282227
Epoch 2310, val loss: 1.0908279418945312
Epoch 2320, training loss: 895.8670043945312 = 1.0928231477737427 + 100.0 * 8.947741508483887
Epoch 2320, val loss: 1.0908290147781372
Epoch 2330, training loss: 896.3980102539062 = 1.0928246974945068 + 100.0 * 8.953051567077637
Epoch 2330, val loss: 1.090829849243164
Epoch 2340, training loss: 896.6676635742188 = 1.0928244590759277 + 100.0 * 8.955748558044434
Epoch 2340, val loss: 1.090829849243164
Epoch 2350, training loss: 896.8927612304688 = 1.092824935913086 + 100.0 * 8.957999229431152
Epoch 2350, val loss: 1.0908302068710327
Epoch 2360, training loss: 896.771728515625 = 1.0928230285644531 + 100.0 * 8.956789016723633
Epoch 2360, val loss: 1.0908288955688477
Epoch 2370, training loss: 896.8839721679688 = 1.0928236246109009 + 100.0 * 8.957911491394043
Epoch 2370, val loss: 1.0908273458480835
Epoch 2380, training loss: 897.0726928710938 = 1.0928237438201904 + 100.0 * 8.959798812866211
Epoch 2380, val loss: 1.0908278226852417
Epoch 2390, training loss: 897.4067993164062 = 1.0928242206573486 + 100.0 * 8.963139533996582
Epoch 2390, val loss: 1.090828537940979
Epoch 2400, training loss: 897.5671997070312 = 1.0928231477737427 + 100.0 * 8.964743614196777
Epoch 2400, val loss: 1.0908277034759521
Epoch 2410, training loss: 897.5634765625 = 1.0928223133087158 + 100.0 * 8.964706420898438
Epoch 2410, val loss: 1.0908268690109253
Epoch 2420, training loss: 897.5264892578125 = 1.092821478843689 + 100.0 * 8.964336395263672
Epoch 2420, val loss: 1.0908259153366089
Epoch 2430, training loss: 897.4600830078125 = 1.0928199291229248 + 100.0 * 8.963672637939453
Epoch 2430, val loss: 1.0908236503601074
Epoch 2440, training loss: 897.5105590820312 = 1.0928195714950562 + 100.0 * 8.964177131652832
Epoch 2440, val loss: 1.0908236503601074
Epoch 2450, training loss: 898.1534423828125 = 1.0928207635879517 + 100.0 * 8.970605850219727
Epoch 2450, val loss: 1.0908246040344238
Epoch 2460, training loss: 898.3262329101562 = 1.092820644378662 + 100.0 * 8.972333908081055
Epoch 2460, val loss: 1.0908247232437134
Epoch 2470, training loss: 898.2044677734375 = 1.0928195714950562 + 100.0 * 8.971116065979004
Epoch 2470, val loss: 1.0908235311508179
Epoch 2480, training loss: 898.3545532226562 = 1.0928187370300293 + 100.0 * 8.972617149353027
Epoch 2480, val loss: 1.0908228158950806
Epoch 2490, training loss: 898.4682006835938 = 1.0928179025650024 + 100.0 * 8.973753929138184
Epoch 2490, val loss: 1.0908219814300537
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.3973913043478261
0.8169238571325075
=== training gcn model ===
Epoch 0, training loss: 1010.155517578125 = 1.094273567199707 + 100.0 * 10.090612411499023
Epoch 0, val loss: 1.09318208694458
Epoch 10, training loss: 964.2140502929688 = 1.0941903591156006 + 100.0 * 9.63119888305664
Epoch 10, val loss: 1.093078374862671
Epoch 20, training loss: 949.219970703125 = 1.0941170454025269 + 100.0 * 9.481258392333984
Epoch 20, val loss: 1.0929951667785645
Epoch 30, training loss: 937.4635009765625 = 1.0940545797348022 + 100.0 * 9.363694190979004
Epoch 30, val loss: 1.092918872833252
Epoch 40, training loss: 928.3609619140625 = 1.0940004587173462 + 100.0 * 9.272669792175293
Epoch 40, val loss: 1.0928466320037842
Epoch 50, training loss: 921.0505981445312 = 1.093929409980774 + 100.0 * 9.199566841125488
Epoch 50, val loss: 1.0927588939666748
Epoch 60, training loss: 914.9080810546875 = 1.09378981590271 + 100.0 * 9.138142585754395
Epoch 60, val loss: 1.0925958156585693
Epoch 70, training loss: 909.5747680664062 = 1.0934163331985474 + 100.0 * 9.084813117980957
Epoch 70, val loss: 1.0922290086746216
Epoch 80, training loss: 904.8800659179688 = 1.0930119752883911 + 100.0 * 9.037870407104492
Epoch 80, val loss: 1.0918458700180054
Epoch 90, training loss: 900.669677734375 = 1.092651128768921 + 100.0 * 8.995770454406738
Epoch 90, val loss: 1.0915024280548096
Epoch 100, training loss: 896.9862670898438 = 1.0923142433166504 + 100.0 * 8.958939552307129
Epoch 100, val loss: 1.0911803245544434
Epoch 110, training loss: 893.7740478515625 = 1.0919965505599976 + 100.0 * 8.926820755004883
Epoch 110, val loss: 1.090874195098877
Epoch 120, training loss: 890.9327392578125 = 1.091686487197876 + 100.0 * 8.89841079711914
Epoch 120, val loss: 1.0905753374099731
Epoch 130, training loss: 888.3789672851562 = 1.0913918018341064 + 100.0 * 8.872876167297363
Epoch 130, val loss: 1.090287446975708
Epoch 140, training loss: 886.19580078125 = 1.091107964515686 + 100.0 * 8.851046562194824
Epoch 140, val loss: 1.0900121927261353
Epoch 150, training loss: 884.10546875 = 1.0908304452896118 + 100.0 * 8.830146789550781
Epoch 150, val loss: 1.0897400379180908
Epoch 160, training loss: 882.4906616210938 = 1.0905640125274658 + 100.0 * 8.814001083374023
Epoch 160, val loss: 1.0894825458526611
Epoch 170, training loss: 880.9473266601562 = 1.0902979373931885 + 100.0 * 8.79857063293457
Epoch 170, val loss: 1.0892212390899658
Epoch 180, training loss: 879.5427856445312 = 1.090043067932129 + 100.0 * 8.784527778625488
Epoch 180, val loss: 1.0889724493026733
Epoch 190, training loss: 878.3082275390625 = 1.089788556098938 + 100.0 * 8.772184371948242
Epoch 190, val loss: 1.088720679283142
Epoch 200, training loss: 877.2251586914062 = 1.089543342590332 + 100.0 * 8.761356353759766
Epoch 200, val loss: 1.088478446006775
Epoch 210, training loss: 876.2109985351562 = 1.089296817779541 + 100.0 * 8.751216888427734
Epoch 210, val loss: 1.0882365703582764
Epoch 220, training loss: 875.2749633789062 = 1.089057207107544 + 100.0 * 8.741859436035156
Epoch 220, val loss: 1.0880013704299927
Epoch 230, training loss: 874.4888305664062 = 1.0888220071792603 + 100.0 * 8.734000205993652
Epoch 230, val loss: 1.087767481803894
Epoch 240, training loss: 873.9439086914062 = 1.0885905027389526 + 100.0 * 8.728553771972656
Epoch 240, val loss: 1.0875409841537476
Epoch 250, training loss: 873.5169677734375 = 1.088356375694275 + 100.0 * 8.724286079406738
Epoch 250, val loss: 1.0873091220855713
Epoch 260, training loss: 873.1959228515625 = 1.0881319046020508 + 100.0 * 8.721077919006348
Epoch 260, val loss: 1.0870877504348755
Epoch 270, training loss: 872.7446899414062 = 1.0879080295562744 + 100.0 * 8.716567993164062
Epoch 270, val loss: 1.086866855621338
Epoch 280, training loss: 872.2029418945312 = 1.0876836776733398 + 100.0 * 8.711152076721191
Epoch 280, val loss: 1.0866469144821167
Epoch 290, training loss: 871.6861572265625 = 1.0874601602554321 + 100.0 * 8.705986976623535
Epoch 290, val loss: 1.0864239931106567
Epoch 300, training loss: 871.6165161132812 = 1.087241530418396 + 100.0 * 8.705292701721191
Epoch 300, val loss: 1.0862102508544922
Epoch 310, training loss: 870.991455078125 = 1.0870217084884644 + 100.0 * 8.699044227600098
Epoch 310, val loss: 1.0859957933425903
Epoch 320, training loss: 870.3887939453125 = 1.0868033170700073 + 100.0 * 8.69301986694336
Epoch 320, val loss: 1.085777759552002
Epoch 330, training loss: 870.1739501953125 = 1.0865941047668457 + 100.0 * 8.690873146057129
Epoch 330, val loss: 1.0855717658996582
Epoch 340, training loss: 869.94287109375 = 1.0863832235336304 + 100.0 * 8.688565254211426
Epoch 340, val loss: 1.0853649377822876
Epoch 350, training loss: 869.6630859375 = 1.0861777067184448 + 100.0 * 8.685769081115723
Epoch 350, val loss: 1.0851616859436035
Epoch 360, training loss: 870.049560546875 = 1.085965871810913 + 100.0 * 8.68963623046875
Epoch 360, val loss: 1.084951400756836
Epoch 370, training loss: 869.4451293945312 = 1.085756540298462 + 100.0 * 8.68359375
Epoch 370, val loss: 1.0847480297088623
Epoch 380, training loss: 869.1896362304688 = 1.0855523347854614 + 100.0 * 8.68104076385498
Epoch 380, val loss: 1.0845478773117065
Epoch 390, training loss: 868.9458618164062 = 1.0853527784347534 + 100.0 * 8.678605079650879
Epoch 390, val loss: 1.0843502283096313
Epoch 400, training loss: 869.44384765625 = 1.0851550102233887 + 100.0 * 8.683587074279785
Epoch 400, val loss: 1.0841519832611084
Epoch 410, training loss: 869.1036987304688 = 1.0849531888961792 + 100.0 * 8.680187225341797
Epoch 410, val loss: 1.0839558839797974
Epoch 420, training loss: 868.7144165039062 = 1.0847539901733398 + 100.0 * 8.67629623413086
Epoch 420, val loss: 1.0837600231170654
Epoch 430, training loss: 868.8172607421875 = 1.0845582485198975 + 100.0 * 8.677327156066895
Epoch 430, val loss: 1.0835672616958618
Epoch 440, training loss: 868.796875 = 1.0843653678894043 + 100.0 * 8.677124977111816
Epoch 440, val loss: 1.083377718925476
Epoch 450, training loss: 868.9896850585938 = 1.084172010421753 + 100.0 * 8.679055213928223
Epoch 450, val loss: 1.083187460899353
Epoch 460, training loss: 868.9534301757812 = 1.0839786529541016 + 100.0 * 8.678694725036621
Epoch 460, val loss: 1.0829981565475464
Epoch 470, training loss: 869.1082153320312 = 1.0837852954864502 + 100.0 * 8.680244445800781
Epoch 470, val loss: 1.082810401916504
Epoch 480, training loss: 869.2581787109375 = 1.0835984945297241 + 100.0 * 8.681745529174805
Epoch 480, val loss: 1.082623839378357
Epoch 490, training loss: 869.1891479492188 = 1.0834065675735474 + 100.0 * 8.68105697631836
Epoch 490, val loss: 1.0824360847473145
Epoch 500, training loss: 869.1823120117188 = 1.0832186937332153 + 100.0 * 8.680991172790527
Epoch 500, val loss: 1.0822523832321167
Epoch 510, training loss: 869.3827514648438 = 1.0830366611480713 + 100.0 * 8.682997703552246
Epoch 510, val loss: 1.0820742845535278
Epoch 520, training loss: 869.6348266601562 = 1.082851767539978 + 100.0 * 8.68552017211914
Epoch 520, val loss: 1.0818935632705688
Epoch 530, training loss: 869.7642822265625 = 1.0826588869094849 + 100.0 * 8.686816215515137
Epoch 530, val loss: 1.0817010402679443
Epoch 540, training loss: 869.8207397460938 = 1.0824848413467407 + 100.0 * 8.687382698059082
Epoch 540, val loss: 1.0815359354019165
Epoch 550, training loss: 869.5123291015625 = 1.0823012590408325 + 100.0 * 8.684300422668457
Epoch 550, val loss: 1.0813539028167725
Epoch 560, training loss: 869.9521484375 = 1.0821216106414795 + 100.0 * 8.688700675964355
Epoch 560, val loss: 1.081179141998291
Epoch 570, training loss: 869.8251953125 = 1.0819411277770996 + 100.0 * 8.687432289123535
Epoch 570, val loss: 1.0810027122497559
Epoch 580, training loss: 869.8687133789062 = 1.081764578819275 + 100.0 * 8.68786907196045
Epoch 580, val loss: 1.080829381942749
Epoch 590, training loss: 869.9757690429688 = 1.0815882682800293 + 100.0 * 8.688941955566406
Epoch 590, val loss: 1.0806571245193481
Epoch 600, training loss: 870.197265625 = 1.0814146995544434 + 100.0 * 8.691158294677734
Epoch 600, val loss: 1.0804862976074219
Epoch 610, training loss: 870.2083129882812 = 1.0812382698059082 + 100.0 * 8.69127082824707
Epoch 610, val loss: 1.0803147554397583
Epoch 620, training loss: 870.2039184570312 = 1.08106529712677 + 100.0 * 8.691228866577148
Epoch 620, val loss: 1.0801457166671753
Epoch 630, training loss: 869.9380493164062 = 1.0808905363082886 + 100.0 * 8.68857192993164
Epoch 630, val loss: 1.0799763202667236
Epoch 640, training loss: 870.1094360351562 = 1.08072030544281 + 100.0 * 8.690286636352539
Epoch 640, val loss: 1.0798094272613525
Epoch 650, training loss: 870.4258422851562 = 1.0805519819259644 + 100.0 * 8.693452835083008
Epoch 650, val loss: 1.0796458721160889
Epoch 660, training loss: 870.634521484375 = 1.0803817510604858 + 100.0 * 8.695541381835938
Epoch 660, val loss: 1.079480528831482
Epoch 670, training loss: 870.8184204101562 = 1.0802130699157715 + 100.0 * 8.697381973266602
Epoch 670, val loss: 1.079314112663269
Epoch 680, training loss: 870.375244140625 = 1.0800435543060303 + 100.0 * 8.692952156066895
Epoch 680, val loss: 1.0791501998901367
Epoch 690, training loss: 870.4129638671875 = 1.0798805952072144 + 100.0 * 8.693330764770508
Epoch 690, val loss: 1.0789908170700073
Epoch 700, training loss: 870.8643798828125 = 1.0797176361083984 + 100.0 * 8.697846412658691
Epoch 700, val loss: 1.0788328647613525
Epoch 710, training loss: 870.8204956054688 = 1.0795533657073975 + 100.0 * 8.697409629821777
Epoch 710, val loss: 1.0786727666854858
Epoch 720, training loss: 871.0097045898438 = 1.0793906450271606 + 100.0 * 8.699302673339844
Epoch 720, val loss: 1.078513503074646
Epoch 730, training loss: 871.3065795898438 = 1.079228401184082 + 100.0 * 8.70227336883545
Epoch 730, val loss: 1.0783562660217285
Epoch 740, training loss: 871.20654296875 = 1.0790685415267944 + 100.0 * 8.701274871826172
Epoch 740, val loss: 1.0782018899917603
Epoch 750, training loss: 871.5588989257812 = 1.078910231590271 + 100.0 * 8.70479965209961
Epoch 750, val loss: 1.0780484676361084
Epoch 760, training loss: 871.468505859375 = 1.0787509679794312 + 100.0 * 8.703897476196289
Epoch 760, val loss: 1.0778932571411133
Epoch 770, training loss: 871.671142578125 = 1.078592300415039 + 100.0 * 8.705924987792969
Epoch 770, val loss: 1.07774019241333
Epoch 780, training loss: 871.848388671875 = 1.0784391164779663 + 100.0 * 8.7076997756958
Epoch 780, val loss: 1.077589511871338
Epoch 790, training loss: 872.218505859375 = 1.078284740447998 + 100.0 * 8.71140193939209
Epoch 790, val loss: 1.0774389505386353
Epoch 800, training loss: 872.2620849609375 = 1.0781277418136597 + 100.0 * 8.71183967590332
Epoch 800, val loss: 1.0772877931594849
Epoch 810, training loss: 872.7952880859375 = 1.0779714584350586 + 100.0 * 8.71717357635498
Epoch 810, val loss: 1.0771362781524658
Epoch 820, training loss: 872.5945434570312 = 1.0778177976608276 + 100.0 * 8.715167045593262
Epoch 820, val loss: 1.0769877433776855
Epoch 830, training loss: 872.8573608398438 = 1.0776673555374146 + 100.0 * 8.717796325683594
Epoch 830, val loss: 1.076843023300171
Epoch 840, training loss: 873.5272216796875 = 1.077519178390503 + 100.0 * 8.724496841430664
Epoch 840, val loss: 1.0766963958740234
Epoch 850, training loss: 873.0708618164062 = 1.0773639678955078 + 100.0 * 8.719934463500977
Epoch 850, val loss: 1.0765459537506104
Epoch 860, training loss: 873.40283203125 = 1.077215552330017 + 100.0 * 8.72325611114502
Epoch 860, val loss: 1.0764031410217285
Epoch 870, training loss: 873.6029663085938 = 1.077068567276001 + 100.0 * 8.725258827209473
Epoch 870, val loss: 1.0762587785720825
Epoch 880, training loss: 873.3309936523438 = 1.0769178867340088 + 100.0 * 8.722540855407715
Epoch 880, val loss: 1.076115608215332
Epoch 890, training loss: 873.9327392578125 = 1.0767742395401 + 100.0 * 8.728559494018555
Epoch 890, val loss: 1.0759767293930054
Epoch 900, training loss: 873.5315551757812 = 1.076622724533081 + 100.0 * 8.724549293518066
Epoch 900, val loss: 1.0758295059204102
Epoch 910, training loss: 873.824462890625 = 1.0764777660369873 + 100.0 * 8.727479934692383
Epoch 910, val loss: 1.075690507888794
Epoch 920, training loss: 874.3372192382812 = 1.076338768005371 + 100.0 * 8.732608795166016
Epoch 920, val loss: 1.075554370880127
Epoch 930, training loss: 874.48095703125 = 1.0761964321136475 + 100.0 * 8.734047889709473
Epoch 930, val loss: 1.0754164457321167
Epoch 940, training loss: 874.5015258789062 = 1.0760515928268433 + 100.0 * 8.734254837036133
Epoch 940, val loss: 1.0752758979797363
Epoch 950, training loss: 874.7662353515625 = 1.075908899307251 + 100.0 * 8.736903190612793
Epoch 950, val loss: 1.0751372575759888
Epoch 960, training loss: 874.8160400390625 = 1.0757672786712646 + 100.0 * 8.73740291595459
Epoch 960, val loss: 1.0750023126602173
Epoch 970, training loss: 875.3201293945312 = 1.075629711151123 + 100.0 * 8.74244499206543
Epoch 970, val loss: 1.0748687982559204
Epoch 980, training loss: 875.1692504882812 = 1.075487732887268 + 100.0 * 8.740937232971191
Epoch 980, val loss: 1.0747321844100952
Epoch 990, training loss: 875.4273681640625 = 1.07534921169281 + 100.0 * 8.74351978302002
Epoch 990, val loss: 1.0745980739593506
Epoch 1000, training loss: 875.65234375 = 1.0752124786376953 + 100.0 * 8.745771408081055
Epoch 1000, val loss: 1.0744659900665283
Epoch 1010, training loss: 874.9385986328125 = 1.0750670433044434 + 100.0 * 8.738635063171387
Epoch 1010, val loss: 1.0743266344070435
Epoch 1020, training loss: 875.1336669921875 = 1.0749253034591675 + 100.0 * 8.74058723449707
Epoch 1020, val loss: 1.0741889476776123
Epoch 1030, training loss: 874.9725341796875 = 1.0747932195663452 + 100.0 * 8.738977432250977
Epoch 1030, val loss: 1.0740625858306885
Epoch 1040, training loss: 875.3464965820312 = 1.0746614933013916 + 100.0 * 8.742718696594238
Epoch 1040, val loss: 1.0739328861236572
Epoch 1050, training loss: 876.0716552734375 = 1.0745338201522827 + 100.0 * 8.749971389770508
Epoch 1050, val loss: 1.0738117694854736
Epoch 1060, training loss: 876.2844848632812 = 1.0743995904922485 + 100.0 * 8.752100944519043
Epoch 1060, val loss: 1.0736820697784424
Epoch 1070, training loss: 876.5028686523438 = 1.0742690563201904 + 100.0 * 8.75428581237793
Epoch 1070, val loss: 1.0735560655593872
Epoch 1080, training loss: 876.9639282226562 = 1.0741404294967651 + 100.0 * 8.75889778137207
Epoch 1080, val loss: 1.0734295845031738
Epoch 1090, training loss: 876.691650390625 = 1.0740060806274414 + 100.0 * 8.756175994873047
Epoch 1090, val loss: 1.0733006000518799
Epoch 1100, training loss: 876.6043701171875 = 1.0738730430603027 + 100.0 * 8.755305290222168
Epoch 1100, val loss: 1.0731741189956665
Epoch 1110, training loss: 876.8985595703125 = 1.0737440586090088 + 100.0 * 8.758248329162598
Epoch 1110, val loss: 1.0730500221252441
Epoch 1120, training loss: 877.2719116210938 = 1.0736186504364014 + 100.0 * 8.761982917785645
Epoch 1120, val loss: 1.072930932044983
Epoch 1130, training loss: 877.4037475585938 = 1.0734890699386597 + 100.0 * 8.76330280303955
Epoch 1130, val loss: 1.0728071928024292
Epoch 1140, training loss: 877.6148681640625 = 1.0733604431152344 + 100.0 * 8.76541519165039
Epoch 1140, val loss: 1.0726803541183472
Epoch 1150, training loss: 878.1282958984375 = 1.0732347965240479 + 100.0 * 8.770550727844238
Epoch 1150, val loss: 1.072558045387268
Epoch 1160, training loss: 878.0484619140625 = 1.0731077194213867 + 100.0 * 8.769753456115723
Epoch 1160, val loss: 1.0724375247955322
Epoch 1170, training loss: 878.5234375 = 1.0729846954345703 + 100.0 * 8.774504661560059
Epoch 1170, val loss: 1.0723204612731934
Epoch 1180, training loss: 878.1551513671875 = 1.072856068611145 + 100.0 * 8.770822525024414
Epoch 1180, val loss: 1.0721977949142456
Epoch 1190, training loss: 878.4063110351562 = 1.0727343559265137 + 100.0 * 8.773335456848145
Epoch 1190, val loss: 1.0720785856246948
Epoch 1200, training loss: 879.0809936523438 = 1.07261323928833 + 100.0 * 8.780083656311035
Epoch 1200, val loss: 1.071964144706726
Epoch 1210, training loss: 879.132568359375 = 1.072489619255066 + 100.0 * 8.780600547790527
Epoch 1210, val loss: 1.0718451738357544
Epoch 1220, training loss: 879.1253051757812 = 1.0723627805709839 + 100.0 * 8.780529022216797
Epoch 1220, val loss: 1.07172429561615
Epoch 1230, training loss: 879.3078002929688 = 1.0722460746765137 + 100.0 * 8.782355308532715
Epoch 1230, val loss: 1.0716116428375244
Epoch 1240, training loss: 879.9188842773438 = 1.0721272230148315 + 100.0 * 8.788467407226562
Epoch 1240, val loss: 1.0714977979660034
Epoch 1250, training loss: 880.026611328125 = 1.0720065832138062 + 100.0 * 8.789546012878418
Epoch 1250, val loss: 1.071380376815796
Epoch 1260, training loss: 879.6904296875 = 1.0718797445297241 + 100.0 * 8.786185264587402
Epoch 1260, val loss: 1.0712616443634033
Epoch 1270, training loss: 878.8394165039062 = 1.0717475414276123 + 100.0 * 8.777676582336426
Epoch 1270, val loss: 1.071131944656372
Epoch 1280, training loss: 879.0896606445312 = 1.071632742881775 + 100.0 * 8.780179977416992
Epoch 1280, val loss: 1.071024775505066
Epoch 1290, training loss: 879.3584594726562 = 1.0715231895446777 + 100.0 * 8.782869338989258
Epoch 1290, val loss: 1.0709198713302612
Epoch 1300, training loss: 880.1989135742188 = 1.071414589881897 + 100.0 * 8.791275024414062
Epoch 1300, val loss: 1.0708154439926147
Epoch 1310, training loss: 880.5101928710938 = 1.071302056312561 + 100.0 * 8.794388771057129
Epoch 1310, val loss: 1.0707060098648071
Epoch 1320, training loss: 880.6582641601562 = 1.0711877346038818 + 100.0 * 8.795870780944824
Epoch 1320, val loss: 1.0705970525741577
Epoch 1330, training loss: 880.9702758789062 = 1.0710721015930176 + 100.0 * 8.798992156982422
Epoch 1330, val loss: 1.0704869031906128
Epoch 1340, training loss: 881.0325927734375 = 1.0709612369537354 + 100.0 * 8.799615859985352
Epoch 1340, val loss: 1.0703821182250977
Epoch 1350, training loss: 881.4642944335938 = 1.070848822593689 + 100.0 * 8.803934097290039
Epoch 1350, val loss: 1.0702741146087646
Epoch 1360, training loss: 881.445068359375 = 1.0707356929779053 + 100.0 * 8.803743362426758
Epoch 1360, val loss: 1.0701658725738525
Epoch 1370, training loss: 881.1475830078125 = 1.0706216096878052 + 100.0 * 8.800769805908203
Epoch 1370, val loss: 1.0700559616088867
Epoch 1380, training loss: 881.3965454101562 = 1.0705108642578125 + 100.0 * 8.803260803222656
Epoch 1380, val loss: 1.0699515342712402
Epoch 1390, training loss: 882.1072387695312 = 1.0704026222229004 + 100.0 * 8.810368537902832
Epoch 1390, val loss: 1.0698485374450684
Epoch 1400, training loss: 882.2760620117188 = 1.0702929496765137 + 100.0 * 8.812057495117188
Epoch 1400, val loss: 1.0697429180145264
Epoch 1410, training loss: 882.2230224609375 = 1.0701844692230225 + 100.0 * 8.811528205871582
Epoch 1410, val loss: 1.0696392059326172
Epoch 1420, training loss: 882.573486328125 = 1.0700759887695312 + 100.0 * 8.815033912658691
Epoch 1420, val loss: 1.0695360898971558
Epoch 1430, training loss: 882.86962890625 = 1.0699695348739624 + 100.0 * 8.817996978759766
Epoch 1430, val loss: 1.0694345235824585
Epoch 1440, training loss: 882.9678955078125 = 1.069862961769104 + 100.0 * 8.81898021697998
Epoch 1440, val loss: 1.0693325996398926
Epoch 1450, training loss: 882.8341064453125 = 1.0697505474090576 + 100.0 * 8.817643165588379
Epoch 1450, val loss: 1.0692225694656372
Epoch 1460, training loss: 882.3140869140625 = 1.0696351528167725 + 100.0 * 8.812444686889648
Epoch 1460, val loss: 1.069111943244934
Epoch 1470, training loss: 882.393310546875 = 1.069530725479126 + 100.0 * 8.813238143920898
Epoch 1470, val loss: 1.0690149068832397
Epoch 1480, training loss: 883.0961303710938 = 1.0694328546524048 + 100.0 * 8.820266723632812
Epoch 1480, val loss: 1.0689246654510498
Epoch 1490, training loss: 883.9407958984375 = 1.0693345069885254 + 100.0 * 8.828714370727539
Epoch 1490, val loss: 1.0688304901123047
Epoch 1500, training loss: 883.8604736328125 = 1.0692306756973267 + 100.0 * 8.827912330627441
Epoch 1500, val loss: 1.0687309503555298
Epoch 1510, training loss: 883.6785278320312 = 1.0691254138946533 + 100.0 * 8.826093673706055
Epoch 1510, val loss: 1.0686326026916504
Epoch 1520, training loss: 884.1419067382812 = 1.069026231765747 + 100.0 * 8.830728530883789
Epoch 1520, val loss: 1.0685371160507202
Epoch 1530, training loss: 884.5653686523438 = 1.068926215171814 + 100.0 * 8.834964752197266
Epoch 1530, val loss: 1.068442702293396
Epoch 1540, training loss: 884.4615478515625 = 1.068821668624878 + 100.0 * 8.833927154541016
Epoch 1540, val loss: 1.0683425664901733
Epoch 1550, training loss: 884.6612548828125 = 1.0687227249145508 + 100.0 * 8.835925102233887
Epoch 1550, val loss: 1.0682491064071655
Epoch 1560, training loss: 885.0791015625 = 1.0686256885528564 + 100.0 * 8.840105056762695
Epoch 1560, val loss: 1.0681569576263428
Epoch 1570, training loss: 884.7355346679688 = 1.0685232877731323 + 100.0 * 8.836669921875
Epoch 1570, val loss: 1.068061351776123
Epoch 1580, training loss: 884.8704223632812 = 1.0684252977371216 + 100.0 * 8.838020324707031
Epoch 1580, val loss: 1.0679644346237183
Epoch 1590, training loss: 885.2474365234375 = 1.0683261156082153 + 100.0 * 8.841791152954102
Epoch 1590, val loss: 1.0678727626800537
Epoch 1600, training loss: 885.7367553710938 = 1.0682320594787598 + 100.0 * 8.846685409545898
Epoch 1600, val loss: 1.067783236503601
Epoch 1610, training loss: 885.5610961914062 = 1.0681325197219849 + 100.0 * 8.844929695129395
Epoch 1610, val loss: 1.067688226699829
Epoch 1620, training loss: 885.7376708984375 = 1.068035364151001 + 100.0 * 8.846695899963379
Epoch 1620, val loss: 1.0675976276397705
Epoch 1630, training loss: 885.879150390625 = 1.0679413080215454 + 100.0 * 8.848112106323242
Epoch 1630, val loss: 1.0675078630447388
Epoch 1640, training loss: 886.3126220703125 = 1.067848801612854 + 100.0 * 8.852447509765625
Epoch 1640, val loss: 1.0674206018447876
Epoch 1650, training loss: 886.5447387695312 = 1.0677545070648193 + 100.0 * 8.854769706726074
Epoch 1650, val loss: 1.0673298835754395
Epoch 1660, training loss: 886.174072265625 = 1.0676578283309937 + 100.0 * 8.85106372833252
Epoch 1660, val loss: 1.0672410726547241
Epoch 1670, training loss: 886.767578125 = 1.0675677061080933 + 100.0 * 8.857000350952148
Epoch 1670, val loss: 1.067155361175537
Epoch 1680, training loss: 886.752685546875 = 1.0674724578857422 + 100.0 * 8.856852531433105
Epoch 1680, val loss: 1.0670663118362427
Epoch 1690, training loss: 886.8126220703125 = 1.0673798322677612 + 100.0 * 8.857452392578125
Epoch 1690, val loss: 1.0669770240783691
Epoch 1700, training loss: 887.066650390625 = 1.0672892332077026 + 100.0 * 8.859993934631348
Epoch 1700, val loss: 1.0668926239013672
Epoch 1710, training loss: 887.2406616210938 = 1.0671988725662231 + 100.0 * 8.861734390258789
Epoch 1710, val loss: 1.0668060779571533
Epoch 1720, training loss: 887.541259765625 = 1.0671082735061646 + 100.0 * 8.864741325378418
Epoch 1720, val loss: 1.066719889640808
Epoch 1730, training loss: 887.2962646484375 = 1.0669957399368286 + 100.0 * 8.862292289733887
Epoch 1730, val loss: 1.0666130781173706
Epoch 1740, training loss: 887.1259155273438 = 1.0669187307357788 + 100.0 * 8.860589981079102
Epoch 1740, val loss: 1.0665416717529297
Epoch 1750, training loss: 887.2998657226562 = 1.0668294429779053 + 100.0 * 8.862330436706543
Epoch 1750, val loss: 1.0664591789245605
Epoch 1760, training loss: 887.9990234375 = 1.0667479038238525 + 100.0 * 8.869322776794434
Epoch 1760, val loss: 1.0663810968399048
Epoch 1770, training loss: 887.977294921875 = 1.0666580200195312 + 100.0 * 8.86910629272461
Epoch 1770, val loss: 1.0662975311279297
Epoch 1780, training loss: 887.8855590820312 = 1.066570520401001 + 100.0 * 8.868189811706543
Epoch 1780, val loss: 1.0662137269973755
Epoch 1790, training loss: 888.0693359375 = 1.066485047340393 + 100.0 * 8.870028495788574
Epoch 1790, val loss: 1.0661331415176392
Epoch 1800, training loss: 888.3941040039062 = 1.0664012432098389 + 100.0 * 8.873276710510254
Epoch 1800, val loss: 1.0660550594329834
Epoch 1810, training loss: 888.6984252929688 = 1.0663164854049683 + 100.0 * 8.876320838928223
Epoch 1810, val loss: 1.0659747123718262
Epoch 1820, training loss: 888.0872802734375 = 1.066226840019226 + 100.0 * 8.870210647583008
Epoch 1820, val loss: 1.0658903121948242
Epoch 1830, training loss: 888.7308349609375 = 1.0661437511444092 + 100.0 * 8.876646995544434
Epoch 1830, val loss: 1.065812349319458
Epoch 1840, training loss: 888.82763671875 = 1.0660618543624878 + 100.0 * 8.877615928649902
Epoch 1840, val loss: 1.065735101699829
Epoch 1850, training loss: 889.1548461914062 = 1.065978765487671 + 100.0 * 8.880888938903809
Epoch 1850, val loss: 1.065657138824463
Epoch 1860, training loss: 889.0165405273438 = 1.0658944845199585 + 100.0 * 8.87950611114502
Epoch 1860, val loss: 1.065576195716858
Epoch 1870, training loss: 889.0744018554688 = 1.0658118724822998 + 100.0 * 8.880085945129395
Epoch 1870, val loss: 1.065498948097229
Epoch 1880, training loss: 889.3868408203125 = 1.065730094909668 + 100.0 * 8.883211135864258
Epoch 1880, val loss: 1.0654234886169434
Epoch 1890, training loss: 889.4097900390625 = 1.0656484365463257 + 100.0 * 8.883440971374512
Epoch 1890, val loss: 1.0653444528579712
Epoch 1900, training loss: 889.4390258789062 = 1.0655659437179565 + 100.0 * 8.883734703063965
Epoch 1900, val loss: 1.0652692317962646
Epoch 1910, training loss: 889.6580200195312 = 1.0654857158660889 + 100.0 * 8.88592529296875
Epoch 1910, val loss: 1.0651942491531372
Epoch 1920, training loss: 890.0205688476562 = 1.065407156944275 + 100.0 * 8.889551162719727
Epoch 1920, val loss: 1.065119743347168
Epoch 1930, training loss: 890.0773315429688 = 1.0653269290924072 + 100.0 * 8.890120506286621
Epoch 1930, val loss: 1.0650442838668823
Epoch 1940, training loss: 890.3253784179688 = 1.0652492046356201 + 100.0 * 8.892601013183594
Epoch 1940, val loss: 1.064971923828125
Epoch 1950, training loss: 889.5692138671875 = 1.0651596784591675 + 100.0 * 8.885040283203125
Epoch 1950, val loss: 1.0648894309997559
Epoch 1960, training loss: 889.7289428710938 = 1.065085768699646 + 100.0 * 8.886638641357422
Epoch 1960, val loss: 1.064819097518921
Epoch 1970, training loss: 890.3850708007812 = 1.065011739730835 + 100.0 * 8.893200874328613
Epoch 1970, val loss: 1.064748764038086
Epoch 1980, training loss: 890.9639282226562 = 1.0649375915527344 + 100.0 * 8.8989896774292
Epoch 1980, val loss: 1.0646789073944092
Epoch 1990, training loss: 890.486328125 = 1.064858078956604 + 100.0 * 8.894214630126953
Epoch 1990, val loss: 1.0646029710769653
Epoch 2000, training loss: 890.8311767578125 = 1.0647838115692139 + 100.0 * 8.897664070129395
Epoch 2000, val loss: 1.0645357370376587
Epoch 2010, training loss: 891.255859375 = 1.0647096633911133 + 100.0 * 8.901911735534668
Epoch 2010, val loss: 1.0644670724868774
Epoch 2020, training loss: 891.0926513671875 = 1.0646337270736694 + 100.0 * 8.900279998779297
Epoch 2020, val loss: 1.0643928050994873
Epoch 2030, training loss: 891.2546997070312 = 1.0645579099655151 + 100.0 * 8.901901245117188
Epoch 2030, val loss: 1.0643237829208374
Epoch 2040, training loss: 891.6103515625 = 1.064485788345337 + 100.0 * 8.905458450317383
Epoch 2040, val loss: 1.0642554759979248
Epoch 2050, training loss: 891.8740234375 = 1.0644001960754395 + 100.0 * 8.908096313476562
Epoch 2050, val loss: 1.0641701221466064
Epoch 2060, training loss: 891.5546875 = 1.064325213432312 + 100.0 * 8.904903411865234
Epoch 2060, val loss: 1.0641013383865356
Epoch 2070, training loss: 891.6143798828125 = 1.064253568649292 + 100.0 * 8.905501365661621
Epoch 2070, val loss: 1.064037561416626
Epoch 2080, training loss: 891.9923706054688 = 1.0641865730285645 + 100.0 * 8.909281730651855
Epoch 2080, val loss: 1.0639747381210327
Epoch 2090, training loss: 892.6129760742188 = 1.0641188621520996 + 100.0 * 8.915488243103027
Epoch 2090, val loss: 1.063910961151123
Epoch 2100, training loss: 892.6151123046875 = 1.064048409461975 + 100.0 * 8.915511131286621
Epoch 2100, val loss: 1.0638452768325806
Epoch 2110, training loss: 892.2584228515625 = 1.063973069190979 + 100.0 * 8.911944389343262
Epoch 2110, val loss: 1.063775897026062
Epoch 2120, training loss: 892.7111206054688 = 1.0639053583145142 + 100.0 * 8.916472434997559
Epoch 2120, val loss: 1.0637121200561523
Epoch 2130, training loss: 893.12939453125 = 1.0638395547866821 + 100.0 * 8.920655250549316
Epoch 2130, val loss: 1.0636500120162964
Epoch 2140, training loss: 892.0899047851562 = 1.0637571811676025 + 100.0 * 8.910261154174805
Epoch 2140, val loss: 1.0635747909545898
Epoch 2150, training loss: 892.45849609375 = 1.063690185546875 + 100.0 * 8.913948059082031
Epoch 2150, val loss: 1.0635111331939697
Epoch 2160, training loss: 892.9221801757812 = 1.0636262893676758 + 100.0 * 8.918585777282715
Epoch 2160, val loss: 1.0634515285491943
Epoch 2170, training loss: 893.4063110351562 = 1.0635617971420288 + 100.0 * 8.92342758178711
Epoch 2170, val loss: 1.063390851020813
Epoch 2180, training loss: 893.5288696289062 = 1.0634912252426147 + 100.0 * 8.924654006958008
Epoch 2180, val loss: 1.0633236169815063
Epoch 2190, training loss: 893.238525390625 = 1.0634210109710693 + 100.0 * 8.921751022338867
Epoch 2190, val loss: 1.0632613897323608
Epoch 2200, training loss: 893.7538452148438 = 1.0633584260940552 + 100.0 * 8.926904678344727
Epoch 2200, val loss: 1.0632033348083496
Epoch 2210, training loss: 894.2290649414062 = 1.0632954835891724 + 100.0 * 8.931657791137695
Epoch 2210, val loss: 1.0631440877914429
Epoch 2220, training loss: 893.96875 = 1.063225507736206 + 100.0 * 8.929055213928223
Epoch 2220, val loss: 1.0630764961242676
Epoch 2230, training loss: 893.6879272460938 = 1.0631569623947144 + 100.0 * 8.926247596740723
Epoch 2230, val loss: 1.063016653060913
Epoch 2240, training loss: 894.0125122070312 = 1.0630929470062256 + 100.0 * 8.92949390411377
Epoch 2240, val loss: 1.0629562139511108
Epoch 2250, training loss: 894.6229858398438 = 1.0630332231521606 + 100.0 * 8.935599327087402
Epoch 2250, val loss: 1.062901258468628
Epoch 2260, training loss: 894.4321899414062 = 1.0629647970199585 + 100.0 * 8.93369197845459
Epoch 2260, val loss: 1.062837839126587
Epoch 2270, training loss: 894.17041015625 = 1.0628970861434937 + 100.0 * 8.931075096130371
Epoch 2270, val loss: 1.0627760887145996
Epoch 2280, training loss: 894.529052734375 = 1.0628368854522705 + 100.0 * 8.934661865234375
Epoch 2280, val loss: 1.0627185106277466
Epoch 2290, training loss: 895.1329345703125 = 1.0627778768539429 + 100.0 * 8.940701484680176
Epoch 2290, val loss: 1.0626636743545532
Epoch 2300, training loss: 895.452392578125 = 1.0627167224884033 + 100.0 * 8.943896293640137
Epoch 2300, val loss: 1.062606930732727
Epoch 2310, training loss: 894.8988037109375 = 1.0626500844955444 + 100.0 * 8.938362121582031
Epoch 2310, val loss: 1.062544822692871
Epoch 2320, training loss: 895.19677734375 = 1.0625892877578735 + 100.0 * 8.9413423538208
Epoch 2320, val loss: 1.0624891519546509
Epoch 2330, training loss: 895.6200561523438 = 1.0625301599502563 + 100.0 * 8.945575714111328
Epoch 2330, val loss: 1.0624346733093262
Epoch 2340, training loss: 895.9031372070312 = 1.0624700784683228 + 100.0 * 8.948406219482422
Epoch 2340, val loss: 1.0623784065246582
Epoch 2350, training loss: 895.5620727539062 = 1.062406063079834 + 100.0 * 8.94499683380127
Epoch 2350, val loss: 1.062319278717041
Epoch 2360, training loss: 895.8995971679688 = 1.0623455047607422 + 100.0 * 8.948372840881348
Epoch 2360, val loss: 1.0622638463974
Epoch 2370, training loss: 895.9052734375 = 1.062286376953125 + 100.0 * 8.948430061340332
Epoch 2370, val loss: 1.0622093677520752
Epoch 2380, training loss: 896.2855224609375 = 1.0622293949127197 + 100.0 * 8.952232360839844
Epoch 2380, val loss: 1.0621562004089355
Epoch 2390, training loss: 895.7308349609375 = 1.0621618032455444 + 100.0 * 8.946686744689941
Epoch 2390, val loss: 1.0620816946029663
Epoch 2400, training loss: 894.4974365234375 = 1.0620921850204468 + 100.0 * 8.934353828430176
Epoch 2400, val loss: 1.0620286464691162
Epoch 2410, training loss: 893.427490234375 = 1.0620160102844238 + 100.0 * 8.923654556274414
Epoch 2410, val loss: 1.0619632005691528
Epoch 2420, training loss: 893.6740112304688 = 1.0619564056396484 + 100.0 * 8.92612075805664
Epoch 2420, val loss: 1.061906337738037
Epoch 2430, training loss: 894.0928344726562 = 1.0619076490402222 + 100.0 * 8.930309295654297
Epoch 2430, val loss: 1.0618572235107422
Epoch 2440, training loss: 894.66162109375 = 1.0618587732315063 + 100.0 * 8.93599796295166
Epoch 2440, val loss: 1.061814546585083
Epoch 2450, training loss: 895.2701416015625 = 1.0618088245391846 + 100.0 * 8.942083358764648
Epoch 2450, val loss: 1.0617669820785522
Epoch 2460, training loss: 895.8836669921875 = 1.0617568492889404 + 100.0 * 8.948219299316406
Epoch 2460, val loss: 1.0617209672927856
Epoch 2470, training loss: 896.325927734375 = 1.0617053508758545 + 100.0 * 8.952642440795898
Epoch 2470, val loss: 1.0616726875305176
Epoch 2480, training loss: 896.73046875 = 1.0616523027420044 + 100.0 * 8.956687927246094
Epoch 2480, val loss: 1.0616213083267212
Epoch 2490, training loss: 896.2679443359375 = 1.0615941286087036 + 100.0 * 8.95206356048584
Epoch 2490, val loss: 1.061569333076477
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.4013043478260869
0.816561617039774
The final CL Acc:0.39845, 0.00204, The final GNN Acc:0.81702, 0.00042
raw graph: torch.Size([2, 88648])
add edge: torch.Size([2, 110534])
remove edge: torch.Size([2, 66486])
updated graph: torch.Size([2, 88372])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 1014.0007934570312 = 1.0978425741195679 + 100.0 * 10.129029273986816
Epoch 0, val loss: 1.096845269203186
Epoch 10, training loss: 965.686279296875 = 1.0974892377853394 + 100.0 * 9.645888328552246
Epoch 10, val loss: 1.09645676612854
Epoch 20, training loss: 944.0990600585938 = 1.09711754322052 + 100.0 * 9.43001937866211
Epoch 20, val loss: 1.0960698127746582
Epoch 30, training loss: 930.10693359375 = 1.0967624187469482 + 100.0 * 9.290102005004883
Epoch 30, val loss: 1.0957008600234985
Epoch 40, training loss: 919.5056762695312 = 1.096428632736206 + 100.0 * 9.18409252166748
Epoch 40, val loss: 1.0953508615493774
Epoch 50, training loss: 911.0278930664062 = 1.0961112976074219 + 100.0 * 9.09931755065918
Epoch 50, val loss: 1.0950202941894531
Epoch 60, training loss: 904.0960693359375 = 1.0958038568496704 + 100.0 * 9.03000259399414
Epoch 60, val loss: 1.094694972038269
Epoch 70, training loss: 898.16259765625 = 1.0955030918121338 + 100.0 * 8.970670700073242
Epoch 70, val loss: 1.0943773984909058
Epoch 80, training loss: 892.968505859375 = 1.09520423412323 + 100.0 * 8.918732643127441
Epoch 80, val loss: 1.0940630435943604
Epoch 90, training loss: 888.6279296875 = 1.0949211120605469 + 100.0 * 8.875329971313477
Epoch 90, val loss: 1.0937618017196655
Epoch 100, training loss: 884.9247436523438 = 1.094637155532837 + 100.0 * 8.838300704956055
Epoch 100, val loss: 1.0934638977050781
Epoch 110, training loss: 881.7518310546875 = 1.0943577289581299 + 100.0 * 8.806574821472168
Epoch 110, val loss: 1.0931659936904907
Epoch 120, training loss: 879.3079223632812 = 1.0940754413604736 + 100.0 * 8.78213882446289
Epoch 120, val loss: 1.0928702354431152
Epoch 130, training loss: 876.7628173828125 = 1.0938149690628052 + 100.0 * 8.75669002532959
Epoch 130, val loss: 1.0925970077514648
Epoch 140, training loss: 874.5789184570312 = 1.0937358140945435 + 100.0 * 8.734851837158203
Epoch 140, val loss: 1.0925178527832031
Epoch 150, training loss: 872.6896362304688 = 1.0937010049819946 + 100.0 * 8.715959548950195
Epoch 150, val loss: 1.0924612283706665
Epoch 160, training loss: 871.1593017578125 = 1.0936470031738281 + 100.0 * 8.70065689086914
Epoch 160, val loss: 1.0923999547958374
Epoch 170, training loss: 869.760009765625 = 1.0936156511306763 + 100.0 * 8.686663627624512
Epoch 170, val loss: 1.0923517942428589
Epoch 180, training loss: 868.685546875 = 1.0935758352279663 + 100.0 * 8.675919532775879
Epoch 180, val loss: 1.0922919511795044
Epoch 190, training loss: 867.690185546875 = 1.0935487747192383 + 100.0 * 8.665966033935547
Epoch 190, val loss: 1.0922510623931885
Epoch 200, training loss: 866.47119140625 = 1.0935155153274536 + 100.0 * 8.653777122497559
Epoch 200, val loss: 1.0922008752822876
Epoch 210, training loss: 865.6426391601562 = 1.0934879779815674 + 100.0 * 8.645491600036621
Epoch 210, val loss: 1.0921580791473389
Epoch 220, training loss: 864.9276123046875 = 1.0934559106826782 + 100.0 * 8.638341903686523
Epoch 220, val loss: 1.0921133756637573
Epoch 230, training loss: 864.244873046875 = 1.0934345722198486 + 100.0 * 8.631514549255371
Epoch 230, val loss: 1.0920753479003906
Epoch 240, training loss: 863.5372924804688 = 1.0934059619903564 + 100.0 * 8.624439239501953
Epoch 240, val loss: 1.0920319557189941
Epoch 250, training loss: 862.9213256835938 = 1.0933789014816284 + 100.0 * 8.618279457092285
Epoch 250, val loss: 1.0919909477233887
Epoch 260, training loss: 862.3671264648438 = 1.0933542251586914 + 100.0 * 8.612737655639648
Epoch 260, val loss: 1.0919522047042847
Epoch 270, training loss: 861.9838256835938 = 1.0933302640914917 + 100.0 * 8.608904838562012
Epoch 270, val loss: 1.0919108390808105
Epoch 280, training loss: 861.4741821289062 = 1.093305230140686 + 100.0 * 8.603808403015137
Epoch 280, val loss: 1.0918731689453125
Epoch 290, training loss: 861.1162109375 = 1.0932749509811401 + 100.0 * 8.600229263305664
Epoch 290, val loss: 1.0918296575546265
Epoch 300, training loss: 861.3388671875 = 1.093260645866394 + 100.0 * 8.602456092834473
Epoch 300, val loss: 1.091800570487976
Epoch 310, training loss: 860.5342407226562 = 1.0932327508926392 + 100.0 * 8.594409942626953
Epoch 310, val loss: 1.0917609930038452
Epoch 320, training loss: 860.458740234375 = 1.0932161808013916 + 100.0 * 8.593655586242676
Epoch 320, val loss: 1.0917288064956665
Epoch 330, training loss: 860.291748046875 = 1.0931955575942993 + 100.0 * 8.591985702514648
Epoch 330, val loss: 1.0916944742202759
Epoch 340, training loss: 860.0696411132812 = 1.0931768417358398 + 100.0 * 8.589764595031738
Epoch 340, val loss: 1.0916626453399658
Epoch 350, training loss: 859.9854736328125 = 1.093157172203064 + 100.0 * 8.588923454284668
Epoch 350, val loss: 1.0916296243667603
Epoch 360, training loss: 860.4852905273438 = 1.0931384563446045 + 100.0 * 8.593921661376953
Epoch 360, val loss: 1.0915967226028442
Epoch 370, training loss: 860.3272094726562 = 1.0931216478347778 + 100.0 * 8.592340469360352
Epoch 370, val loss: 1.0915706157684326
Epoch 380, training loss: 859.5167846679688 = 1.0931028127670288 + 100.0 * 8.584237098693848
Epoch 380, val loss: 1.0915367603302002
Epoch 390, training loss: 859.8502807617188 = 1.093090295791626 + 100.0 * 8.58757209777832
Epoch 390, val loss: 1.091511845588684
Epoch 400, training loss: 859.8126831054688 = 1.0930734872817993 + 100.0 * 8.587196350097656
Epoch 400, val loss: 1.0914863348007202
Epoch 410, training loss: 860.040283203125 = 1.0930593013763428 + 100.0 * 8.589471817016602
Epoch 410, val loss: 1.0914593935012817
Epoch 420, training loss: 860.1253662109375 = 1.0930458307266235 + 100.0 * 8.590323448181152
Epoch 420, val loss: 1.0914337635040283
Epoch 430, training loss: 860.0347900390625 = 1.093032956123352 + 100.0 * 8.589417457580566
Epoch 430, val loss: 1.0914078950881958
Epoch 440, training loss: 860.02197265625 = 1.093018889427185 + 100.0 * 8.589289665222168
Epoch 440, val loss: 1.091383934020996
Epoch 450, training loss: 859.9468383789062 = 1.0930043458938599 + 100.0 * 8.58853816986084
Epoch 450, val loss: 1.0913599729537964
Epoch 460, training loss: 860.1256713867188 = 1.0929932594299316 + 100.0 * 8.590326309204102
Epoch 460, val loss: 1.09133780002594
Epoch 470, training loss: 860.8767700195312 = 1.0929663181304932 + 100.0 * 8.597838401794434
Epoch 470, val loss: 1.0913031101226807
Epoch 480, training loss: 860.417724609375 = 1.092965841293335 + 100.0 * 8.593247413635254
Epoch 480, val loss: 1.0912882089614868
Epoch 490, training loss: 859.8014526367188 = 1.0929551124572754 + 100.0 * 8.587084770202637
Epoch 490, val loss: 1.0912692546844482
Epoch 500, training loss: 859.8432006835938 = 1.0929467678070068 + 100.0 * 8.587502479553223
Epoch 500, val loss: 1.0912507772445679
Epoch 510, training loss: 859.819580078125 = 1.0929371118545532 + 100.0 * 8.587265968322754
Epoch 510, val loss: 1.0912322998046875
Epoch 520, training loss: 859.9615478515625 = 1.0929276943206787 + 100.0 * 8.588685989379883
Epoch 520, val loss: 1.0912144184112549
Epoch 530, training loss: 859.9537963867188 = 1.0929168462753296 + 100.0 * 8.588608741760254
Epoch 530, val loss: 1.091194987297058
Epoch 540, training loss: 860.2574462890625 = 1.0929102897644043 + 100.0 * 8.591645240783691
Epoch 540, val loss: 1.0911785364151
Epoch 550, training loss: 860.4149780273438 = 1.0929025411605835 + 100.0 * 8.593220710754395
Epoch 550, val loss: 1.0911616086959839
Epoch 560, training loss: 860.471923828125 = 1.0928949117660522 + 100.0 * 8.593790054321289
Epoch 560, val loss: 1.0911449193954468
Epoch 570, training loss: 860.4494018554688 = 1.0928864479064941 + 100.0 * 8.593564987182617
Epoch 570, val loss: 1.0911282300949097
Epoch 580, training loss: 860.390869140625 = 1.0928767919540405 + 100.0 * 8.592979431152344
Epoch 580, val loss: 1.0911086797714233
Epoch 590, training loss: 860.40869140625 = 1.0928692817687988 + 100.0 * 8.593157768249512
Epoch 590, val loss: 1.0910941362380981
Epoch 600, training loss: 860.6349487304688 = 1.0928641557693481 + 100.0 * 8.595420837402344
Epoch 600, val loss: 1.0910803079605103
Epoch 610, training loss: 861.0321044921875 = 1.0928590297698975 + 100.0 * 8.599392890930176
Epoch 610, val loss: 1.091068148612976
Epoch 620, training loss: 861.2406616210938 = 1.092853307723999 + 100.0 * 8.601478576660156
Epoch 620, val loss: 1.0910553932189941
Epoch 630, training loss: 861.2766723632812 = 1.0928467512130737 + 100.0 * 8.601838111877441
Epoch 630, val loss: 1.091040849685669
Epoch 640, training loss: 861.58837890625 = 1.0928446054458618 + 100.0 * 8.604955673217773
Epoch 640, val loss: 1.091031789779663
Epoch 650, training loss: 861.5413818359375 = 1.0928322076797485 + 100.0 * 8.604485511779785
Epoch 650, val loss: 1.0910089015960693
Epoch 660, training loss: 861.4364013671875 = 1.0928288698196411 + 100.0 * 8.603435516357422
Epoch 660, val loss: 1.091001272201538
Epoch 670, training loss: 861.712646484375 = 1.0928281545639038 + 100.0 * 8.60619831085205
Epoch 670, val loss: 1.090994954109192
Epoch 680, training loss: 861.9566040039062 = 1.0928245782852173 + 100.0 * 8.608637809753418
Epoch 680, val loss: 1.0909842252731323
Epoch 690, training loss: 861.989013671875 = 1.0928192138671875 + 100.0 * 8.608962059020996
Epoch 690, val loss: 1.090970516204834
Epoch 700, training loss: 861.9164428710938 = 1.092814564704895 + 100.0 * 8.608236312866211
Epoch 700, val loss: 1.090961217880249
Epoch 710, training loss: 862.2429809570312 = 1.0928120613098145 + 100.0 * 8.611501693725586
Epoch 710, val loss: 1.0909518003463745
Epoch 720, training loss: 862.3685913085938 = 1.0928070545196533 + 100.0 * 8.612757682800293
Epoch 720, val loss: 1.0909392833709717
Epoch 730, training loss: 862.31201171875 = 1.092801809310913 + 100.0 * 8.612192153930664
Epoch 730, val loss: 1.0909308195114136
Epoch 740, training loss: 862.7185668945312 = 1.0927932262420654 + 100.0 * 8.616257667541504
Epoch 740, val loss: 1.0909197330474854
Epoch 750, training loss: 862.6026611328125 = 1.0927923917770386 + 100.0 * 8.61509895324707
Epoch 750, val loss: 1.09091055393219
Epoch 760, training loss: 862.6773681640625 = 1.0927923917770386 + 100.0 * 8.615845680236816
Epoch 760, val loss: 1.0909069776535034
Epoch 770, training loss: 863.1657104492188 = 1.0927915573120117 + 100.0 * 8.620729446411133
Epoch 770, val loss: 1.0909003019332886
Epoch 780, training loss: 863.21875 = 1.092787504196167 + 100.0 * 8.621259689331055
Epoch 780, val loss: 1.0908924341201782
Epoch 790, training loss: 863.2364501953125 = 1.0927835702896118 + 100.0 * 8.621437072753906
Epoch 790, val loss: 1.090885043144226
Epoch 800, training loss: 863.729248046875 = 1.0927836894989014 + 100.0 * 8.626364707946777
Epoch 800, val loss: 1.0908796787261963
Epoch 810, training loss: 863.9896850585938 = 1.092782974243164 + 100.0 * 8.628969192504883
Epoch 810, val loss: 1.0908704996109009
Epoch 820, training loss: 864.0750732421875 = 1.0927786827087402 + 100.0 * 8.629822731018066
Epoch 820, val loss: 1.090863585472107
Epoch 830, training loss: 864.0426025390625 = 1.0927765369415283 + 100.0 * 8.629498481750488
Epoch 830, val loss: 1.0908581018447876
Epoch 840, training loss: 864.2611083984375 = 1.0927741527557373 + 100.0 * 8.631683349609375
Epoch 840, val loss: 1.0908491611480713
Epoch 850, training loss: 864.2061767578125 = 1.0927691459655762 + 100.0 * 8.631134033203125
Epoch 850, val loss: 1.0908434391021729
Epoch 860, training loss: 864.4396362304688 = 1.0927702188491821 + 100.0 * 8.633468627929688
Epoch 860, val loss: 1.090839147567749
Epoch 870, training loss: 864.3701171875 = 1.0927650928497314 + 100.0 * 8.632773399353027
Epoch 870, val loss: 1.090830683708191
Epoch 880, training loss: 864.5354614257812 = 1.0927642583847046 + 100.0 * 8.634427070617676
Epoch 880, val loss: 1.0908266305923462
Epoch 890, training loss: 864.4003295898438 = 1.0927616357803345 + 100.0 * 8.633075714111328
Epoch 890, val loss: 1.0908164978027344
Epoch 900, training loss: 864.5921020507812 = 1.0927603244781494 + 100.0 * 8.634993553161621
Epoch 900, val loss: 1.090813159942627
Epoch 910, training loss: 865.0003051757812 = 1.0927602052688599 + 100.0 * 8.63907527923584
Epoch 910, val loss: 1.0908106565475464
Epoch 920, training loss: 865.0901489257812 = 1.0927594900131226 + 100.0 * 8.639973640441895
Epoch 920, val loss: 1.0908074378967285
Epoch 930, training loss: 864.8770141601562 = 1.0927537679672241 + 100.0 * 8.637842178344727
Epoch 930, val loss: 1.09079909324646
Epoch 940, training loss: 865.1905517578125 = 1.0927557945251465 + 100.0 * 8.64097785949707
Epoch 940, val loss: 1.090797781944275
Epoch 950, training loss: 865.4640502929688 = 1.0927544832229614 + 100.0 * 8.643712997436523
Epoch 950, val loss: 1.0907927751541138
Epoch 960, training loss: 865.6103515625 = 1.092754602432251 + 100.0 * 8.64517593383789
Epoch 960, val loss: 1.0907902717590332
Epoch 970, training loss: 865.7095336914062 = 1.0927515029907227 + 100.0 * 8.646167755126953
Epoch 970, val loss: 1.0907835960388184
Epoch 980, training loss: 865.788330078125 = 1.0927505493164062 + 100.0 * 8.646955490112305
Epoch 980, val loss: 1.090781331062317
Epoch 990, training loss: 865.75048828125 = 1.092747449874878 + 100.0 * 8.646576881408691
Epoch 990, val loss: 1.0907758474349976
Epoch 1000, training loss: 866.0364990234375 = 1.0927503108978271 + 100.0 * 8.649436950683594
Epoch 1000, val loss: 1.090775728225708
Epoch 1010, training loss: 866.142822265625 = 1.0927485227584839 + 100.0 * 8.650500297546387
Epoch 1010, val loss: 1.0907727479934692
Epoch 1020, training loss: 866.0908203125 = 1.0927486419677734 + 100.0 * 8.649980545043945
Epoch 1020, val loss: 1.0907665491104126
Epoch 1030, training loss: 865.9820556640625 = 1.0927428007125854 + 100.0 * 8.648893356323242
Epoch 1030, val loss: 1.0907617807388306
Epoch 1040, training loss: 866.4058227539062 = 1.0927447080612183 + 100.0 * 8.653130531311035
Epoch 1040, val loss: 1.0907602310180664
Epoch 1050, training loss: 866.8394775390625 = 1.0927460193634033 + 100.0 * 8.657466888427734
Epoch 1050, val loss: 1.0907617807388306
Epoch 1060, training loss: 866.813720703125 = 1.09274423122406 + 100.0 * 8.657209396362305
Epoch 1060, val loss: 1.0907572507858276
Epoch 1070, training loss: 866.8405151367188 = 1.0927428007125854 + 100.0 * 8.657478332519531
Epoch 1070, val loss: 1.0907541513442993
Epoch 1080, training loss: 867.3684692382812 = 1.0927441120147705 + 100.0 * 8.66275691986084
Epoch 1080, val loss: 1.0907529592514038
Epoch 1090, training loss: 867.3656616210938 = 1.092740774154663 + 100.0 * 8.662729263305664
Epoch 1090, val loss: 1.09074866771698
Epoch 1100, training loss: 867.3241577148438 = 1.0927387475967407 + 100.0 * 8.662314414978027
Epoch 1100, val loss: 1.0907448530197144
Epoch 1110, training loss: 867.1956787109375 = 1.0927375555038452 + 100.0 * 8.661029815673828
Epoch 1110, val loss: 1.0907403230667114
Epoch 1120, training loss: 867.3991088867188 = 1.0927386283874512 + 100.0 * 8.663064002990723
Epoch 1120, val loss: 1.090741515159607
Epoch 1130, training loss: 868.0226440429688 = 1.0927414894104004 + 100.0 * 8.669299125671387
Epoch 1130, val loss: 1.0907424688339233
Epoch 1140, training loss: 868.0401611328125 = 1.0927410125732422 + 100.0 * 8.669474601745605
Epoch 1140, val loss: 1.0907400846481323
Epoch 1150, training loss: 868.1287841796875 = 1.092740535736084 + 100.0 * 8.670360565185547
Epoch 1150, val loss: 1.0907385349273682
Epoch 1160, training loss: 867.246826171875 = 1.092726230621338 + 100.0 * 8.661540985107422
Epoch 1160, val loss: 1.0907243490219116
Epoch 1170, training loss: 867.2205810546875 = 1.092728614807129 + 100.0 * 8.66127872467041
Epoch 1170, val loss: 1.0907248258590698
Epoch 1180, training loss: 867.4691162109375 = 1.0927278995513916 + 100.0 * 8.663763999938965
Epoch 1180, val loss: 1.0907233953475952
Epoch 1190, training loss: 867.4881591796875 = 1.0927265882492065 + 100.0 * 8.663954734802246
Epoch 1190, val loss: 1.0907200574874878
Epoch 1200, training loss: 867.7997436523438 = 1.0927317142486572 + 100.0 * 8.667070388793945
Epoch 1200, val loss: 1.0907232761383057
Epoch 1210, training loss: 868.6058959960938 = 1.0927342176437378 + 100.0 * 8.675131797790527
Epoch 1210, val loss: 1.090725064277649
Epoch 1220, training loss: 869.0875244140625 = 1.092737078666687 + 100.0 * 8.679947853088379
Epoch 1220, val loss: 1.0907275676727295
Epoch 1230, training loss: 869.1682739257812 = 1.0927374362945557 + 100.0 * 8.680755615234375
Epoch 1230, val loss: 1.0907257795333862
Epoch 1240, training loss: 869.6631469726562 = 1.0927379131317139 + 100.0 * 8.685704231262207
Epoch 1240, val loss: 1.0907269716262817
Epoch 1250, training loss: 869.5673828125 = 1.092736840248108 + 100.0 * 8.684746742248535
Epoch 1250, val loss: 1.0907241106033325
Epoch 1260, training loss: 869.8640747070312 = 1.0927366018295288 + 100.0 * 8.687713623046875
Epoch 1260, val loss: 1.0907237529754639
Epoch 1270, training loss: 870.1029663085938 = 1.0927379131317139 + 100.0 * 8.690102577209473
Epoch 1270, val loss: 1.090722680091858
Epoch 1280, training loss: 870.1596069335938 = 1.0927358865737915 + 100.0 * 8.690669059753418
Epoch 1280, val loss: 1.0907191038131714
Epoch 1290, training loss: 870.4993286132812 = 1.0927292108535767 + 100.0 * 8.694066047668457
Epoch 1290, val loss: 1.0907142162322998
Epoch 1300, training loss: 869.5609130859375 = 1.0927284955978394 + 100.0 * 8.68468189239502
Epoch 1300, val loss: 1.0907127857208252
Epoch 1310, training loss: 869.9585571289062 = 1.0927294492721558 + 100.0 * 8.688658714294434
Epoch 1310, val loss: 1.0907111167907715
Epoch 1320, training loss: 870.3121948242188 = 1.0927298069000244 + 100.0 * 8.692194938659668
Epoch 1320, val loss: 1.090712070465088
Epoch 1330, training loss: 870.4927368164062 = 1.092731237411499 + 100.0 * 8.694000244140625
Epoch 1330, val loss: 1.0907127857208252
Epoch 1340, training loss: 870.8084106445312 = 1.0927321910858154 + 100.0 * 8.69715690612793
Epoch 1340, val loss: 1.0907137393951416
Epoch 1350, training loss: 870.9447021484375 = 1.0927321910858154 + 100.0 * 8.698519706726074
Epoch 1350, val loss: 1.0907131433486938
Epoch 1360, training loss: 871.0753173828125 = 1.09273099899292 + 100.0 * 8.69982624053955
Epoch 1360, val loss: 1.0907111167907715
Epoch 1370, training loss: 871.1795043945312 = 1.0927306413650513 + 100.0 * 8.700867652893066
Epoch 1370, val loss: 1.090710163116455
Epoch 1380, training loss: 871.4050903320312 = 1.0927324295043945 + 100.0 * 8.703124046325684
Epoch 1380, val loss: 1.090710997581482
Epoch 1390, training loss: 871.7127685546875 = 1.0927300453186035 + 100.0 * 8.70620059967041
Epoch 1390, val loss: 1.0907082557678223
Epoch 1400, training loss: 870.9005126953125 = 1.0927146673202515 + 100.0 * 8.698078155517578
Epoch 1400, val loss: 1.0906966924667358
Epoch 1410, training loss: 869.410400390625 = 1.0927050113677979 + 100.0 * 8.68317699432373
Epoch 1410, val loss: 1.0906840562820435
Epoch 1420, training loss: 869.6552124023438 = 1.0927104949951172 + 100.0 * 8.685625076293945
Epoch 1420, val loss: 1.090687870979309
Epoch 1430, training loss: 870.34375 = 1.0927175283432007 + 100.0 * 8.692510604858398
Epoch 1430, val loss: 1.0906939506530762
Epoch 1440, training loss: 871.1256103515625 = 1.0927211046218872 + 100.0 * 8.700328826904297
Epoch 1440, val loss: 1.0906977653503418
Epoch 1450, training loss: 871.4111328125 = 1.0927242040634155 + 100.0 * 8.703184127807617
Epoch 1450, val loss: 1.0907005071640015
Epoch 1460, training loss: 871.710693359375 = 1.0927246809005737 + 100.0 * 8.70617961883545
Epoch 1460, val loss: 1.0907002687454224
Epoch 1470, training loss: 871.8670043945312 = 1.0927238464355469 + 100.0 * 8.707742691040039
Epoch 1470, val loss: 1.0906996726989746
Epoch 1480, training loss: 872.0166015625 = 1.0927244424819946 + 100.0 * 8.70923900604248
Epoch 1480, val loss: 1.0906988382339478
Epoch 1490, training loss: 872.178955078125 = 1.0927238464355469 + 100.0 * 8.710862159729004
Epoch 1490, val loss: 1.0906977653503418
Epoch 1500, training loss: 872.3585205078125 = 1.0927226543426514 + 100.0 * 8.712657928466797
Epoch 1500, val loss: 1.0906976461410522
Epoch 1510, training loss: 872.4030151367188 = 1.0927214622497559 + 100.0 * 8.713103294372559
Epoch 1510, val loss: 1.0906968116760254
Epoch 1520, training loss: 872.5743408203125 = 1.0927199125289917 + 100.0 * 8.714816093444824
Epoch 1520, val loss: 1.0906949043273926
Epoch 1530, training loss: 872.7671508789062 = 1.0927215814590454 + 100.0 * 8.716744422912598
Epoch 1530, val loss: 1.090696096420288
Epoch 1540, training loss: 872.9539794921875 = 1.0927205085754395 + 100.0 * 8.718612670898438
Epoch 1540, val loss: 1.0906944274902344
Epoch 1550, training loss: 873.0505981445312 = 1.0927200317382812 + 100.0 * 8.719578742980957
Epoch 1550, val loss: 1.0906925201416016
Epoch 1560, training loss: 873.2376098632812 = 1.092719316482544 + 100.0 * 8.72144889831543
Epoch 1560, val loss: 1.0906914472579956
Epoch 1570, training loss: 873.2827758789062 = 1.092718482017517 + 100.0 * 8.721900939941406
Epoch 1570, val loss: 1.0906918048858643
Epoch 1580, training loss: 873.223388671875 = 1.092716932296753 + 100.0 * 8.721306800842285
Epoch 1580, val loss: 1.0906896591186523
Epoch 1590, training loss: 873.5505981445312 = 1.0927162170410156 + 100.0 * 8.724578857421875
Epoch 1590, val loss: 1.090688943862915
Epoch 1600, training loss: 873.0865478515625 = 1.0927138328552246 + 100.0 * 8.719938278198242
Epoch 1600, val loss: 1.0906867980957031
Epoch 1610, training loss: 873.5763549804688 = 1.0927149057388306 + 100.0 * 8.724836349487305
Epoch 1610, val loss: 1.0906872749328613
Epoch 1620, training loss: 874.0679931640625 = 1.0927165746688843 + 100.0 * 8.729752540588379
Epoch 1620, val loss: 1.090687870979309
Epoch 1630, training loss: 874.2219848632812 = 1.092715859413147 + 100.0 * 8.731292724609375
Epoch 1630, val loss: 1.0906879901885986
Epoch 1640, training loss: 874.3538818359375 = 1.0927149057388306 + 100.0 * 8.732611656188965
Epoch 1640, val loss: 1.0906869173049927
Epoch 1650, training loss: 873.8274536132812 = 1.0927096605300903 + 100.0 * 8.727347373962402
Epoch 1650, val loss: 1.090682029724121
Epoch 1660, training loss: 874.21142578125 = 1.0927109718322754 + 100.0 * 8.731186866760254
Epoch 1660, val loss: 1.090682029724121
Epoch 1670, training loss: 874.5916137695312 = 1.0927119255065918 + 100.0 * 8.734989166259766
Epoch 1670, val loss: 1.090683102607727
Epoch 1680, training loss: 874.8128662109375 = 1.092713475227356 + 100.0 * 8.737201690673828
Epoch 1680, val loss: 1.0906829833984375
Epoch 1690, training loss: 874.9586791992188 = 1.0927116870880127 + 100.0 * 8.738659858703613
Epoch 1690, val loss: 1.0906809568405151
Epoch 1700, training loss: 874.5586547851562 = 1.092708706855774 + 100.0 * 8.734659194946289
Epoch 1700, val loss: 1.0906788110733032
Epoch 1710, training loss: 874.9196166992188 = 1.0927090644836426 + 100.0 * 8.738268852233887
Epoch 1710, val loss: 1.0906805992126465
Epoch 1720, training loss: 875.082763671875 = 1.0927081108093262 + 100.0 * 8.739900588989258
Epoch 1720, val loss: 1.0906801223754883
Epoch 1730, training loss: 875.5328979492188 = 1.0927093029022217 + 100.0 * 8.744401931762695
Epoch 1730, val loss: 1.090680480003357
Epoch 1740, training loss: 875.4698486328125 = 1.0927085876464844 + 100.0 * 8.74377155303955
Epoch 1740, val loss: 1.0906792879104614
Epoch 1750, training loss: 875.6289672851562 = 1.0927083492279053 + 100.0 * 8.745362281799316
Epoch 1750, val loss: 1.0906786918640137
Epoch 1760, training loss: 875.881591796875 = 1.0927081108093262 + 100.0 * 8.747888565063477
Epoch 1760, val loss: 1.0906784534454346
Epoch 1770, training loss: 875.9463500976562 = 1.092707633972168 + 100.0 * 8.748536109924316
Epoch 1770, val loss: 1.0906776189804077
Epoch 1780, training loss: 876.2724609375 = 1.0927084684371948 + 100.0 * 8.751797676086426
Epoch 1780, val loss: 1.0906778573989868
Epoch 1790, training loss: 876.375 = 1.0927085876464844 + 100.0 * 8.752822875976562
Epoch 1790, val loss: 1.0906741619110107
Epoch 1800, training loss: 876.0625 = 1.0927037000656128 + 100.0 * 8.7496976852417
Epoch 1800, val loss: 1.0906729698181152
Epoch 1810, training loss: 876.3797607421875 = 1.0927048921585083 + 100.0 * 8.752870559692383
Epoch 1810, val loss: 1.0906744003295898
Epoch 1820, training loss: 876.9385986328125 = 1.0927056074142456 + 100.0 * 8.758459091186523
Epoch 1820, val loss: 1.0906745195388794
Epoch 1830, training loss: 876.607421875 = 1.0927058458328247 + 100.0 * 8.755146980285645
Epoch 1830, val loss: 1.0906729698181152
Epoch 1840, training loss: 876.6599731445312 = 1.0927010774612427 + 100.0 * 8.755672454833984
Epoch 1840, val loss: 1.0906692743301392
Epoch 1850, training loss: 876.7667846679688 = 1.0926990509033203 + 100.0 * 8.75674057006836
Epoch 1850, val loss: 1.0906680822372437
Epoch 1860, training loss: 875.8792114257812 = 1.0926799774169922 + 100.0 * 8.747865676879883
Epoch 1860, val loss: 1.0906445980072021
Epoch 1870, training loss: 874.4708251953125 = 1.0926620960235596 + 100.0 * 8.733781814575195
Epoch 1870, val loss: 1.0906238555908203
Epoch 1880, training loss: 874.9423217773438 = 1.0926774740219116 + 100.0 * 8.738496780395508
Epoch 1880, val loss: 1.0906397104263306
Epoch 1890, training loss: 873.126708984375 = 1.092657446861267 + 100.0 * 8.720340728759766
Epoch 1890, val loss: 1.090628981590271
Epoch 1900, training loss: 874.0834350585938 = 1.0926698446273804 + 100.0 * 8.729907989501953
Epoch 1900, val loss: 1.0906356573104858
Epoch 1910, training loss: 875.0721435546875 = 1.0926775932312012 + 100.0 * 8.739794731140137
Epoch 1910, val loss: 1.0906444787979126
Epoch 1920, training loss: 875.5213012695312 = 1.0926811695098877 + 100.0 * 8.74428653717041
Epoch 1920, val loss: 1.0906494855880737
Epoch 1930, training loss: 876.1741333007812 = 1.0926854610443115 + 100.0 * 8.750814437866211
Epoch 1930, val loss: 1.090653419494629
Epoch 1940, training loss: 876.6786499023438 = 1.0926883220672607 + 100.0 * 8.755859375
Epoch 1940, val loss: 1.0906563997268677
Epoch 1950, training loss: 876.5296630859375 = 1.092686414718628 + 100.0 * 8.754369735717773
Epoch 1950, val loss: 1.0906544923782349
Epoch 1960, training loss: 876.7487182617188 = 1.0926874876022339 + 100.0 * 8.756560325622559
Epoch 1960, val loss: 1.0906555652618408
Epoch 1970, training loss: 877.1964111328125 = 1.092689037322998 + 100.0 * 8.76103687286377
Epoch 1970, val loss: 1.0906567573547363
Epoch 1980, training loss: 877.3135986328125 = 1.0926899909973145 + 100.0 * 8.762208938598633
Epoch 1980, val loss: 1.0906575918197632
Epoch 1990, training loss: 877.5469970703125 = 1.0926904678344727 + 100.0 * 8.764542579650879
Epoch 1990, val loss: 1.0906577110290527
Epoch 2000, training loss: 877.4905395507812 = 1.0926884412765503 + 100.0 * 8.763978004455566
Epoch 2000, val loss: 1.0906562805175781
Epoch 2010, training loss: 877.761474609375 = 1.0926896333694458 + 100.0 * 8.766687393188477
Epoch 2010, val loss: 1.0906561613082886
Epoch 2020, training loss: 877.9517822265625 = 1.0926897525787354 + 100.0 * 8.768590927124023
Epoch 2020, val loss: 1.0906567573547363
Epoch 2030, training loss: 877.8897094726562 = 1.0926891565322876 + 100.0 * 8.767970085144043
Epoch 2030, val loss: 1.0906548500061035
Epoch 2040, training loss: 877.8362426757812 = 1.0926868915557861 + 100.0 * 8.767435073852539
Epoch 2040, val loss: 1.090653419494629
Epoch 2050, training loss: 878.0142822265625 = 1.0926867723464966 + 100.0 * 8.76921558380127
Epoch 2050, val loss: 1.0906535387039185
Epoch 2060, training loss: 878.3788452148438 = 1.0926885604858398 + 100.0 * 8.77286148071289
Epoch 2060, val loss: 1.0906541347503662
Epoch 2070, training loss: 877.8779907226562 = 1.0926828384399414 + 100.0 * 8.767852783203125
Epoch 2070, val loss: 1.0906496047973633
Epoch 2080, training loss: 878.362060546875 = 1.0926852226257324 + 100.0 * 8.772693634033203
Epoch 2080, val loss: 1.0906507968902588
Epoch 2090, training loss: 878.5029296875 = 1.0926860570907593 + 100.0 * 8.774102210998535
Epoch 2090, val loss: 1.0906521081924438
Epoch 2100, training loss: 878.5756225585938 = 1.0926848649978638 + 100.0 * 8.774828910827637
Epoch 2100, val loss: 1.0906509160995483
Epoch 2110, training loss: 878.6135864257812 = 1.092684030532837 + 100.0 * 8.775208473205566
Epoch 2110, val loss: 1.090650200843811
Epoch 2120, training loss: 878.7083740234375 = 1.0926841497421265 + 100.0 * 8.776156425476074
Epoch 2120, val loss: 1.0906503200531006
Epoch 2130, training loss: 878.945556640625 = 1.0926845073699951 + 100.0 * 8.778528213500977
Epoch 2130, val loss: 1.0906504392623901
Epoch 2140, training loss: 878.9011840820312 = 1.0926822423934937 + 100.0 * 8.778084754943848
Epoch 2140, val loss: 1.0906480550765991
Epoch 2150, training loss: 878.93701171875 = 1.0926811695098877 + 100.0 * 8.778443336486816
Epoch 2150, val loss: 1.0906476974487305
Epoch 2160, training loss: 879.2662353515625 = 1.0926827192306519 + 100.0 * 8.78173542022705
Epoch 2160, val loss: 1.0906480550765991
Epoch 2170, training loss: 879.3839111328125 = 1.092682123184204 + 100.0 * 8.782912254333496
Epoch 2170, val loss: 1.090646743774414
Epoch 2180, training loss: 879.34326171875 = 1.0926802158355713 + 100.0 * 8.782505989074707
Epoch 2180, val loss: 1.0906457901000977
Epoch 2190, training loss: 879.4575805664062 = 1.0926786661148071 + 100.0 * 8.783649444580078
Epoch 2190, val loss: 1.0906449556350708
Epoch 2200, training loss: 879.39990234375 = 1.0926780700683594 + 100.0 * 8.783072471618652
Epoch 2200, val loss: 1.0906434059143066
Epoch 2210, training loss: 879.75048828125 = 1.0926802158355713 + 100.0 * 8.786578178405762
Epoch 2210, val loss: 1.0906445980072021
Epoch 2220, training loss: 879.991455078125 = 1.0926803350448608 + 100.0 * 8.78898811340332
Epoch 2220, val loss: 1.0906449556350708
Epoch 2230, training loss: 879.938720703125 = 1.0926778316497803 + 100.0 * 8.788460731506348
Epoch 2230, val loss: 1.0906422138214111
Epoch 2240, training loss: 879.5531616210938 = 1.0926728248596191 + 100.0 * 8.784605026245117
Epoch 2240, val loss: 1.0906343460083008
Epoch 2250, training loss: 876.5164794921875 = 1.0926382541656494 + 100.0 * 8.75423812866211
Epoch 2250, val loss: 1.0905917882919312
Epoch 2260, training loss: 876.7239990234375 = 1.0926376581192017 + 100.0 * 8.75631332397461
Epoch 2260, val loss: 1.090599536895752
Epoch 2270, training loss: 877.1436157226562 = 1.0926433801651 + 100.0 * 8.760509490966797
Epoch 2270, val loss: 1.090608835220337
Epoch 2280, training loss: 877.04345703125 = 1.0926456451416016 + 100.0 * 8.75950813293457
Epoch 2280, val loss: 1.090613603591919
Epoch 2290, training loss: 877.8367309570312 = 1.0926543474197388 + 100.0 * 8.767440795898438
Epoch 2290, val loss: 1.0906195640563965
Epoch 2300, training loss: 878.5179443359375 = 1.0926604270935059 + 100.0 * 8.774252891540527
Epoch 2300, val loss: 1.0906260013580322
Epoch 2310, training loss: 879.1918334960938 = 1.0926647186279297 + 100.0 * 8.780991554260254
Epoch 2310, val loss: 1.0906305313110352
Epoch 2320, training loss: 879.3531494140625 = 1.0926657915115356 + 100.0 * 8.782605171203613
Epoch 2320, val loss: 1.0906317234039307
Epoch 2330, training loss: 879.6910400390625 = 1.0926671028137207 + 100.0 * 8.78598403930664
Epoch 2330, val loss: 1.090632438659668
Epoch 2340, training loss: 879.556884765625 = 1.092665672302246 + 100.0 * 8.784642219543457
Epoch 2340, val loss: 1.0906310081481934
Epoch 2350, training loss: 879.8778076171875 = 1.0926668643951416 + 100.0 * 8.787851333618164
Epoch 2350, val loss: 1.0906312465667725
Epoch 2360, training loss: 880.0010986328125 = 1.0926666259765625 + 100.0 * 8.789084434509277
Epoch 2360, val loss: 1.0906312465667725
Epoch 2370, training loss: 880.2830810546875 = 1.0926676988601685 + 100.0 * 8.79190444946289
Epoch 2370, val loss: 1.0906312465667725
Epoch 2380, training loss: 880.1742553710938 = 1.092665672302246 + 100.0 * 8.790816307067871
Epoch 2380, val loss: 1.0906306505203247
Epoch 2390, training loss: 880.443603515625 = 1.092665672302246 + 100.0 * 8.793509483337402
Epoch 2390, val loss: 1.0906298160552979
Epoch 2400, training loss: 880.5431518554688 = 1.0926653146743774 + 100.0 * 8.79450511932373
Epoch 2400, val loss: 1.0906306505203247
Epoch 2410, training loss: 880.501220703125 = 1.0926634073257446 + 100.0 * 8.794085502624512
Epoch 2410, val loss: 1.090627908706665
Epoch 2420, training loss: 880.6516723632812 = 1.0926626920700073 + 100.0 * 8.7955904006958
Epoch 2420, val loss: 1.0906267166137695
Epoch 2430, training loss: 880.9110717773438 = 1.0926631689071655 + 100.0 * 8.798184394836426
Epoch 2430, val loss: 1.0906277894973755
Epoch 2440, training loss: 881.147705078125 = 1.0926626920700073 + 100.0 * 8.80055046081543
Epoch 2440, val loss: 1.0906274318695068
Epoch 2450, training loss: 881.1488647460938 = 1.0926622152328491 + 100.0 * 8.800561904907227
Epoch 2450, val loss: 1.0906257629394531
Epoch 2460, training loss: 880.7471313476562 = 1.0926579236984253 + 100.0 * 8.796545028686523
Epoch 2460, val loss: 1.0906213521957397
Epoch 2470, training loss: 880.9857788085938 = 1.0926593542099 + 100.0 * 8.798931121826172
Epoch 2470, val loss: 1.090623140335083
Epoch 2480, training loss: 881.6083374023438 = 1.0926607847213745 + 100.0 * 8.805156707763672
Epoch 2480, val loss: 1.0906246900558472
Epoch 2490, training loss: 881.56103515625 = 1.0926599502563477 + 100.0 * 8.804683685302734
Epoch 2490, val loss: 1.090623140335083
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.3973913043478261
0.8631456929652975
=== training gcn model ===
Epoch 0, training loss: 1006.7242431640625 = 1.0937496423721313 + 100.0 * 10.056304931640625
Epoch 0, val loss: 1.0943427085876465
Epoch 10, training loss: 959.2711181640625 = 1.0935547351837158 + 100.0 * 9.581775665283203
Epoch 10, val loss: 1.0941969156265259
Epoch 20, training loss: 940.7371215820312 = 1.0934481620788574 + 100.0 * 9.39643669128418
Epoch 20, val loss: 1.0941040515899658
Epoch 30, training loss: 928.5818481445312 = 1.0933265686035156 + 100.0 * 9.274885177612305
Epoch 30, val loss: 1.0939987897872925
Epoch 40, training loss: 919.0650024414062 = 1.093212604522705 + 100.0 * 9.179718017578125
Epoch 40, val loss: 1.093903660774231
Epoch 50, training loss: 911.6566162109375 = 1.0930753946304321 + 100.0 * 9.105635643005371
Epoch 50, val loss: 1.0937825441360474
Epoch 60, training loss: 905.459716796875 = 1.0929368734359741 + 100.0 * 9.043667793273926
Epoch 60, val loss: 1.0936639308929443
Epoch 70, training loss: 900.0955810546875 = 1.0928070545196533 + 100.0 * 8.99002742767334
Epoch 70, val loss: 1.0935524702072144
Epoch 80, training loss: 895.4369506835938 = 1.0926837921142578 + 100.0 * 8.943442344665527
Epoch 80, val loss: 1.0934475660324097
Epoch 90, training loss: 891.4100341796875 = 1.092557668685913 + 100.0 * 8.903175354003906
Epoch 90, val loss: 1.0933400392532349
Epoch 100, training loss: 887.8410034179688 = 1.0924334526062012 + 100.0 * 8.867486000061035
Epoch 100, val loss: 1.093233585357666
Epoch 110, training loss: 884.6090698242188 = 1.092313528060913 + 100.0 * 8.83516788482666
Epoch 110, val loss: 1.0931323766708374
Epoch 120, training loss: 881.79541015625 = 1.0921927690505981 + 100.0 * 8.807032585144043
Epoch 120, val loss: 1.0930310487747192
Epoch 130, training loss: 879.2137451171875 = 1.0920816659927368 + 100.0 * 8.781216621398926
Epoch 130, val loss: 1.092936635017395
Epoch 140, training loss: 876.9548950195312 = 1.0919666290283203 + 100.0 * 8.758628845214844
Epoch 140, val loss: 1.0928400754928589
Epoch 150, training loss: 875.2030639648438 = 1.0918563604354858 + 100.0 * 8.741111755371094
Epoch 150, val loss: 1.0927469730377197
Epoch 160, training loss: 873.3819580078125 = 1.0917409658432007 + 100.0 * 8.722902297973633
Epoch 160, val loss: 1.0926505327224731
Epoch 170, training loss: 871.8547973632812 = 1.0916316509246826 + 100.0 * 8.707632064819336
Epoch 170, val loss: 1.0925590991973877
Epoch 180, training loss: 870.3282470703125 = 1.0915184020996094 + 100.0 * 8.692367553710938
Epoch 180, val loss: 1.0924650430679321
Epoch 190, training loss: 869.2413940429688 = 1.091407060623169 + 100.0 * 8.681499481201172
Epoch 190, val loss: 1.0923715829849243
Epoch 200, training loss: 868.4293212890625 = 1.0912995338439941 + 100.0 * 8.673379898071289
Epoch 200, val loss: 1.092281699180603
Epoch 210, training loss: 867.4950561523438 = 1.0911931991577148 + 100.0 * 8.66403865814209
Epoch 210, val loss: 1.0921937227249146
Epoch 220, training loss: 866.631103515625 = 1.091086983680725 + 100.0 * 8.655400276184082
Epoch 220, val loss: 1.092106819152832
Epoch 230, training loss: 865.908447265625 = 1.090981364250183 + 100.0 * 8.648174285888672
Epoch 230, val loss: 1.0920190811157227
Epoch 240, training loss: 865.1978149414062 = 1.090877652168274 + 100.0 * 8.641069412231445
Epoch 240, val loss: 1.0919342041015625
Epoch 250, training loss: 864.8748779296875 = 1.0907776355743408 + 100.0 * 8.63784122467041
Epoch 250, val loss: 1.091850996017456
Epoch 260, training loss: 864.24072265625 = 1.0906747579574585 + 100.0 * 8.631500244140625
Epoch 260, val loss: 1.09176766872406
Epoch 270, training loss: 863.7506713867188 = 1.0905754566192627 + 100.0 * 8.626601219177246
Epoch 270, val loss: 1.091685175895691
Epoch 280, training loss: 863.2214965820312 = 1.0904765129089355 + 100.0 * 8.621310234069824
Epoch 280, val loss: 1.0916030406951904
Epoch 290, training loss: 862.8565673828125 = 1.0903772115707397 + 100.0 * 8.617661476135254
Epoch 290, val loss: 1.0915234088897705
Epoch 300, training loss: 862.5250244140625 = 1.0902818441390991 + 100.0 * 8.614347457885742
Epoch 300, val loss: 1.091444730758667
Epoch 310, training loss: 862.1302490234375 = 1.0901827812194824 + 100.0 * 8.610400199890137
Epoch 310, val loss: 1.091363787651062
Epoch 320, training loss: 862.0056762695312 = 1.0900934934616089 + 100.0 * 8.609155654907227
Epoch 320, val loss: 1.0912902355194092
Epoch 330, training loss: 861.6320190429688 = 1.0899996757507324 + 100.0 * 8.605420112609863
Epoch 330, val loss: 1.0912134647369385
Epoch 340, training loss: 861.4429931640625 = 1.0899089574813843 + 100.0 * 8.603530883789062
Epoch 340, val loss: 1.091139793395996
Epoch 350, training loss: 861.3695068359375 = 1.0898200273513794 + 100.0 * 8.60279655456543
Epoch 350, val loss: 1.0910671949386597
Epoch 360, training loss: 861.3056640625 = 1.0897318124771118 + 100.0 * 8.60215950012207
Epoch 360, val loss: 1.0909948348999023
Epoch 370, training loss: 861.21484375 = 1.0896433591842651 + 100.0 * 8.601251602172852
Epoch 370, val loss: 1.0909243822097778
Epoch 380, training loss: 861.1665649414062 = 1.0895570516586304 + 100.0 * 8.600769996643066
Epoch 380, val loss: 1.0908536911010742
Epoch 390, training loss: 860.9200439453125 = 1.089470624923706 + 100.0 * 8.598305702209473
Epoch 390, val loss: 1.0907844305038452
Epoch 400, training loss: 861.0552978515625 = 1.0893869400024414 + 100.0 * 8.599658966064453
Epoch 400, val loss: 1.0907163619995117
Epoch 410, training loss: 860.9304809570312 = 1.0893038511276245 + 100.0 * 8.598411560058594
Epoch 410, val loss: 1.0906498432159424
Epoch 420, training loss: 861.4253540039062 = 1.0892207622528076 + 100.0 * 8.603361129760742
Epoch 420, val loss: 1.090580701828003
Epoch 430, training loss: 861.0880737304688 = 1.089141607284546 + 100.0 * 8.599989891052246
Epoch 430, val loss: 1.0905228853225708
Epoch 440, training loss: 860.1924438476562 = 1.0890551805496216 + 100.0 * 8.591033935546875
Epoch 440, val loss: 1.090451955795288
Epoch 450, training loss: 860.337890625 = 1.0889781713485718 + 100.0 * 8.592489242553711
Epoch 450, val loss: 1.0903888940811157
Epoch 460, training loss: 860.5281982421875 = 1.0888996124267578 + 100.0 * 8.594392776489258
Epoch 460, val loss: 1.0903267860412598
Epoch 470, training loss: 860.3591918945312 = 1.0888221263885498 + 100.0 * 8.592703819274902
Epoch 470, val loss: 1.0902657508850098
Epoch 480, training loss: 860.3423461914062 = 1.0887480974197388 + 100.0 * 8.592535972595215
Epoch 480, val loss: 1.0902084112167358
Epoch 490, training loss: 860.6575927734375 = 1.0886746644973755 + 100.0 * 8.595688819885254
Epoch 490, val loss: 1.0901496410369873
Epoch 500, training loss: 860.9322509765625 = 1.088600993156433 + 100.0 * 8.59843635559082
Epoch 500, val loss: 1.0900923013687134
Epoch 510, training loss: 861.324951171875 = 1.088529348373413 + 100.0 * 8.602364540100098
Epoch 510, val loss: 1.0900357961654663
Epoch 520, training loss: 860.7665405273438 = 1.088455319404602 + 100.0 * 8.596780776977539
Epoch 520, val loss: 1.0899769067764282
Epoch 530, training loss: 860.974853515625 = 1.088383436203003 + 100.0 * 8.598864555358887
Epoch 530, val loss: 1.0899215936660767
Epoch 540, training loss: 861.4456787109375 = 1.0883162021636963 + 100.0 * 8.6035737991333
Epoch 540, val loss: 1.0898696184158325
Epoch 550, training loss: 861.6018676757812 = 1.088248372077942 + 100.0 * 8.605135917663574
Epoch 550, val loss: 1.0898158550262451
Epoch 560, training loss: 862.5022583007812 = 1.0881799459457397 + 100.0 * 8.614140510559082
Epoch 560, val loss: 1.0897642374038696
Epoch 570, training loss: 861.4486694335938 = 1.088111400604248 + 100.0 * 8.603605270385742
Epoch 570, val loss: 1.089711308479309
Epoch 580, training loss: 861.7400512695312 = 1.0880467891693115 + 100.0 * 8.60651969909668
Epoch 580, val loss: 1.089660406112671
Epoch 590, training loss: 862.0687255859375 = 1.0879833698272705 + 100.0 * 8.609807014465332
Epoch 590, val loss: 1.089612364768982
Epoch 600, training loss: 862.0283203125 = 1.0879195928573608 + 100.0 * 8.609404563903809
Epoch 600, val loss: 1.0895637273788452
Epoch 610, training loss: 862.3059692382812 = 1.087856650352478 + 100.0 * 8.612181663513184
Epoch 610, val loss: 1.0895154476165771
Epoch 620, training loss: 862.6029052734375 = 1.0877922773361206 + 100.0 * 8.615151405334473
Epoch 620, val loss: 1.0894640684127808
Epoch 630, training loss: 862.5828247070312 = 1.0877315998077393 + 100.0 * 8.614951133728027
Epoch 630, val loss: 1.0894185304641724
Epoch 640, training loss: 862.754638671875 = 1.0876708030700684 + 100.0 * 8.616669654846191
Epoch 640, val loss: 1.0893741846084595
Epoch 650, training loss: 862.3563842773438 = 1.0876100063323975 + 100.0 * 8.612688064575195
Epoch 650, val loss: 1.0893281698226929
Epoch 660, training loss: 862.6845092773438 = 1.0875511169433594 + 100.0 * 8.61596965789795
Epoch 660, val loss: 1.089284062385559
Epoch 670, training loss: 863.0670776367188 = 1.0874948501586914 + 100.0 * 8.619795799255371
Epoch 670, val loss: 1.0892423391342163
Epoch 680, training loss: 863.3304443359375 = 1.0874388217926025 + 100.0 * 8.622429847717285
Epoch 680, val loss: 1.089200496673584
Epoch 690, training loss: 863.609619140625 = 1.087381362915039 + 100.0 * 8.625222206115723
Epoch 690, val loss: 1.0891581773757935
Epoch 700, training loss: 863.6940307617188 = 1.087327241897583 + 100.0 * 8.626067161560059
Epoch 700, val loss: 1.0891177654266357
Epoch 710, training loss: 864.0540161132812 = 1.0872725248336792 + 100.0 * 8.629667282104492
Epoch 710, val loss: 1.0890769958496094
Epoch 720, training loss: 864.3624877929688 = 1.08721923828125 + 100.0 * 8.632752418518066
Epoch 720, val loss: 1.0890371799468994
Epoch 730, training loss: 864.6445922851562 = 1.0871671438217163 + 100.0 * 8.635574340820312
Epoch 730, val loss: 1.0889991521835327
Epoch 740, training loss: 864.5087280273438 = 1.0871148109436035 + 100.0 * 8.63421630859375
Epoch 740, val loss: 1.088959813117981
Epoch 750, training loss: 864.7234497070312 = 1.0870622396469116 + 100.0 * 8.636363983154297
Epoch 750, val loss: 1.0889209508895874
Epoch 760, training loss: 864.6824340820312 = 1.0870110988616943 + 100.0 * 8.635953903198242
Epoch 760, val loss: 1.0888839960098267
Epoch 770, training loss: 865.036865234375 = 1.086962103843689 + 100.0 * 8.639498710632324
Epoch 770, val loss: 1.0888491868972778
Epoch 780, training loss: 865.3860473632812 = 1.0869134664535522 + 100.0 * 8.642991065979004
Epoch 780, val loss: 1.08881413936615
Epoch 790, training loss: 865.648193359375 = 1.0868651866912842 + 100.0 * 8.645613670349121
Epoch 790, val loss: 1.088779091835022
Epoch 800, training loss: 865.931884765625 = 1.0868180990219116 + 100.0 * 8.64845085144043
Epoch 800, val loss: 1.0887442827224731
Epoch 810, training loss: 865.8471069335938 = 1.0867702960968018 + 100.0 * 8.647603034973145
Epoch 810, val loss: 1.088711142539978
Epoch 820, training loss: 866.0735473632812 = 1.0867254734039307 + 100.0 * 8.64986801147461
Epoch 820, val loss: 1.0886787176132202
Epoch 830, training loss: 866.3482055664062 = 1.0866806507110596 + 100.0 * 8.652615547180176
Epoch 830, val loss: 1.0886476039886475
Epoch 840, training loss: 866.41796875 = 1.086634635925293 + 100.0 * 8.653313636779785
Epoch 840, val loss: 1.0886142253875732
Epoch 850, training loss: 866.5319213867188 = 1.0865919589996338 + 100.0 * 8.65445327758789
Epoch 850, val loss: 1.0885847806930542
Epoch 860, training loss: 863.8513793945312 = 1.0865331888198853 + 100.0 * 8.62764835357666
Epoch 860, val loss: 1.088542103767395
Epoch 870, training loss: 865.6472778320312 = 1.086496353149414 + 100.0 * 8.645607948303223
Epoch 870, val loss: 1.0885143280029297
Epoch 880, training loss: 865.7598876953125 = 1.0864574909210205 + 100.0 * 8.646734237670898
Epoch 880, val loss: 1.088487982749939
Epoch 890, training loss: 866.214111328125 = 1.0864174365997314 + 100.0 * 8.651276588439941
Epoch 890, val loss: 1.0884606838226318
Epoch 900, training loss: 866.804443359375 = 1.0863802433013916 + 100.0 * 8.657180786132812
Epoch 900, val loss: 1.0884361267089844
Epoch 910, training loss: 867.406982421875 = 1.0863425731658936 + 100.0 * 8.663206100463867
Epoch 910, val loss: 1.0884102582931519
Epoch 920, training loss: 867.5774536132812 = 1.0863035917282104 + 100.0 * 8.664911270141602
Epoch 920, val loss: 1.0883837938308716
Epoch 930, training loss: 867.8286743164062 = 1.086265206336975 + 100.0 * 8.667424201965332
Epoch 930, val loss: 1.08835768699646
Epoch 940, training loss: 867.923828125 = 1.0862265825271606 + 100.0 * 8.668375968933105
Epoch 940, val loss: 1.088330864906311
Epoch 950, training loss: 867.9359130859375 = 1.0861890316009521 + 100.0 * 8.668497085571289
Epoch 950, val loss: 1.0883065462112427
Epoch 960, training loss: 868.341552734375 = 1.0861536264419556 + 100.0 * 8.672554016113281
Epoch 960, val loss: 1.0882835388183594
Epoch 970, training loss: 868.5704956054688 = 1.0861173868179321 + 100.0 * 8.674843788146973
Epoch 970, val loss: 1.088258981704712
Epoch 980, training loss: 869.0302734375 = 1.0860823392868042 + 100.0 * 8.679442405700684
Epoch 980, val loss: 1.0882357358932495
Epoch 990, training loss: 869.097900390625 = 1.0860469341278076 + 100.0 * 8.680118560791016
Epoch 990, val loss: 1.0882128477096558
Epoch 1000, training loss: 869.5252075195312 = 1.086013913154602 + 100.0 * 8.684391975402832
Epoch 1000, val loss: 1.0881909132003784
Epoch 1010, training loss: 869.8463745117188 = 1.0859801769256592 + 100.0 * 8.687603950500488
Epoch 1010, val loss: 1.088169813156128
Epoch 1020, training loss: 869.9756469726562 = 1.085947871208191 + 100.0 * 8.688897132873535
Epoch 1020, val loss: 1.088148593902588
Epoch 1030, training loss: 870.3739013671875 = 1.0859160423278809 + 100.0 * 8.692879676818848
Epoch 1030, val loss: 1.0881279706954956
Epoch 1040, training loss: 870.59375 = 1.0858761072158813 + 100.0 * 8.69507884979248
Epoch 1040, val loss: 1.0880967378616333
Epoch 1050, training loss: 872.0619506835938 = 1.0858550071716309 + 100.0 * 8.709760665893555
Epoch 1050, val loss: 1.088088870048523
Epoch 1060, training loss: 870.0448608398438 = 1.0858163833618164 + 100.0 * 8.689590454101562
Epoch 1060, val loss: 1.08806574344635
Epoch 1070, training loss: 870.3491821289062 = 1.0857884883880615 + 100.0 * 8.692633628845215
Epoch 1070, val loss: 1.0880475044250488
Epoch 1080, training loss: 870.7901000976562 = 1.0857584476470947 + 100.0 * 8.697043418884277
Epoch 1080, val loss: 1.088029146194458
Epoch 1090, training loss: 871.8602294921875 = 1.0857328176498413 + 100.0 * 8.707744598388672
Epoch 1090, val loss: 1.088013768196106
Epoch 1100, training loss: 872.4337158203125 = 1.085706353187561 + 100.0 * 8.713479995727539
Epoch 1100, val loss: 1.0879971981048584
Epoch 1110, training loss: 872.7806396484375 = 1.0856781005859375 + 100.0 * 8.716949462890625
Epoch 1110, val loss: 1.0879802703857422
Epoch 1120, training loss: 872.9888305664062 = 1.0856510400772095 + 100.0 * 8.719032287597656
Epoch 1120, val loss: 1.0879645347595215
Epoch 1130, training loss: 872.9169311523438 = 1.0856235027313232 + 100.0 * 8.718313217163086
Epoch 1130, val loss: 1.0879474878311157
Epoch 1140, training loss: 873.2918701171875 = 1.0855978727340698 + 100.0 * 8.722063064575195
Epoch 1140, val loss: 1.08793306350708
Epoch 1150, training loss: 873.6305541992188 = 1.0855724811553955 + 100.0 * 8.725449562072754
Epoch 1150, val loss: 1.0879172086715698
Epoch 1160, training loss: 873.48583984375 = 1.0855450630187988 + 100.0 * 8.724002838134766
Epoch 1160, val loss: 1.08790123462677
Epoch 1170, training loss: 873.9931640625 = 1.0855214595794678 + 100.0 * 8.729076385498047
Epoch 1170, val loss: 1.0878885984420776
Epoch 1180, training loss: 874.2083129882812 = 1.0854978561401367 + 100.0 * 8.73122787475586
Epoch 1180, val loss: 1.0878750085830688
Epoch 1190, training loss: 874.70263671875 = 1.085474967956543 + 100.0 * 8.73617172241211
Epoch 1190, val loss: 1.087862253189087
Epoch 1200, training loss: 874.940673828125 = 1.0854524374008179 + 100.0 * 8.73855209350586
Epoch 1200, val loss: 1.0878496170043945
Epoch 1210, training loss: 874.9417114257812 = 1.0854277610778809 + 100.0 * 8.73856258392334
Epoch 1210, val loss: 1.0878359079360962
Epoch 1220, training loss: 875.4236450195312 = 1.0854068994522095 + 100.0 * 8.743382453918457
Epoch 1220, val loss: 1.0878247022628784
Epoch 1230, training loss: 875.4254760742188 = 1.085384726524353 + 100.0 * 8.743400573730469
Epoch 1230, val loss: 1.0878117084503174
Epoch 1240, training loss: 875.4000854492188 = 1.0853607654571533 + 100.0 * 8.743146896362305
Epoch 1240, val loss: 1.0877989530563354
Epoch 1250, training loss: 876.028564453125 = 1.0853408575057983 + 100.0 * 8.749432563781738
Epoch 1250, val loss: 1.087788462638855
Epoch 1260, training loss: 875.9974975585938 = 1.0853203535079956 + 100.0 * 8.74912166595459
Epoch 1260, val loss: 1.087777853012085
Epoch 1270, training loss: 876.466796875 = 1.085300326347351 + 100.0 * 8.753814697265625
Epoch 1270, val loss: 1.0877671241760254
Epoch 1280, training loss: 876.6253051757812 = 1.085279941558838 + 100.0 * 8.755400657653809
Epoch 1280, val loss: 1.0877571105957031
Epoch 1290, training loss: 876.6998901367188 = 1.0852603912353516 + 100.0 * 8.756146430969238
Epoch 1290, val loss: 1.0877466201782227
Epoch 1300, training loss: 877.0885620117188 = 1.085241436958313 + 100.0 * 8.760032653808594
Epoch 1300, val loss: 1.0877370834350586
Epoch 1310, training loss: 877.1766967773438 = 1.0852222442626953 + 100.0 * 8.76091480255127
Epoch 1310, val loss: 1.0877279043197632
Epoch 1320, training loss: 877.7463989257812 = 1.0852051973342896 + 100.0 * 8.76661205291748
Epoch 1320, val loss: 1.0877190828323364
Epoch 1330, training loss: 877.6069946289062 = 1.08518648147583 + 100.0 * 8.765217781066895
Epoch 1330, val loss: 1.0877106189727783
Epoch 1340, training loss: 877.8761596679688 = 1.0851690769195557 + 100.0 * 8.76791000366211
Epoch 1340, val loss: 1.0877021551132202
Epoch 1350, training loss: 877.8095092773438 = 1.0851502418518066 + 100.0 * 8.767243385314941
Epoch 1350, val loss: 1.0876920223236084
Epoch 1360, training loss: 878.618408203125 = 1.0851337909698486 + 100.0 * 8.7753324508667
Epoch 1360, val loss: 1.087684988975525
Epoch 1370, training loss: 875.6459350585938 = 1.085105538368225 + 100.0 * 8.74560832977295
Epoch 1370, val loss: 1.0876659154891968
Epoch 1380, training loss: 874.3672485351562 = 1.0850845575332642 + 100.0 * 8.732821464538574
Epoch 1380, val loss: 1.087656021118164
Epoch 1390, training loss: 876.4703369140625 = 1.0850746631622314 + 100.0 * 8.753852844238281
Epoch 1390, val loss: 1.0876539945602417
Epoch 1400, training loss: 875.8175048828125 = 1.0850580930709839 + 100.0 * 8.747323989868164
Epoch 1400, val loss: 1.0876474380493164
Epoch 1410, training loss: 876.8109741210938 = 1.0850480794906616 + 100.0 * 8.757259368896484
Epoch 1410, val loss: 1.0876442193984985
Epoch 1420, training loss: 877.56396484375 = 1.0850356817245483 + 100.0 * 8.764789581298828
Epoch 1420, val loss: 1.0876402854919434
Epoch 1430, training loss: 877.8458862304688 = 1.0850229263305664 + 100.0 * 8.767608642578125
Epoch 1430, val loss: 1.0876356363296509
Epoch 1440, training loss: 878.3304443359375 = 1.0850101709365845 + 100.0 * 8.772454261779785
Epoch 1440, val loss: 1.087631344795227
Epoch 1450, training loss: 878.50830078125 = 1.0849963426589966 + 100.0 * 8.774232864379883
Epoch 1450, val loss: 1.087625503540039
Epoch 1460, training loss: 878.671875 = 1.0849825143814087 + 100.0 * 8.775869369506836
Epoch 1460, val loss: 1.087620735168457
Epoch 1470, training loss: 879.1512451171875 = 1.0849707126617432 + 100.0 * 8.780662536621094
Epoch 1470, val loss: 1.0876166820526123
Epoch 1480, training loss: 879.2713012695312 = 1.0849581956863403 + 100.0 * 8.78186321258545
Epoch 1480, val loss: 1.087612271308899
Epoch 1490, training loss: 879.2525024414062 = 1.0849437713623047 + 100.0 * 8.781675338745117
Epoch 1490, val loss: 1.0876058340072632
Epoch 1500, training loss: 879.3508911132812 = 1.0849319696426392 + 100.0 * 8.782659530639648
Epoch 1500, val loss: 1.0876026153564453
Epoch 1510, training loss: 879.5623779296875 = 1.0849207639694214 + 100.0 * 8.784774780273438
Epoch 1510, val loss: 1.0875989198684692
Epoch 1520, training loss: 880.1358032226562 = 1.0849096775054932 + 100.0 * 8.790509223937988
Epoch 1520, val loss: 1.0875953435897827
Epoch 1530, training loss: 880.1235961914062 = 1.0848971605300903 + 100.0 * 8.790387153625488
Epoch 1530, val loss: 1.087591528892517
Epoch 1540, training loss: 880.2654418945312 = 1.0848865509033203 + 100.0 * 8.791805267333984
Epoch 1540, val loss: 1.0875879526138306
Epoch 1550, training loss: 880.7014770507812 = 1.084876537322998 + 100.0 * 8.796165466308594
Epoch 1550, val loss: 1.0875850915908813
Epoch 1560, training loss: 880.7599487304688 = 1.084865689277649 + 100.0 * 8.796751022338867
Epoch 1560, val loss: 1.0875818729400635
Epoch 1570, training loss: 880.7174072265625 = 1.0848543643951416 + 100.0 * 8.79632568359375
Epoch 1570, val loss: 1.0875780582427979
Epoch 1580, training loss: 881.1514282226562 = 1.0848445892333984 + 100.0 * 8.800665855407715
Epoch 1580, val loss: 1.0875757932662964
Epoch 1590, training loss: 881.47802734375 = 1.0848346948623657 + 100.0 * 8.803932189941406
Epoch 1590, val loss: 1.0875736474990845
Epoch 1600, training loss: 881.3640747070312 = 1.0848249197006226 + 100.0 * 8.8027925491333
Epoch 1600, val loss: 1.0875705480575562
Epoch 1610, training loss: 881.6561889648438 = 1.0848160982131958 + 100.0 * 8.805713653564453
Epoch 1610, val loss: 1.0875682830810547
Epoch 1620, training loss: 882.11083984375 = 1.08480703830719 + 100.0 * 8.810260772705078
Epoch 1620, val loss: 1.0875662565231323
Epoch 1630, training loss: 881.48193359375 = 1.0847941637039185 + 100.0 * 8.803971290588379
Epoch 1630, val loss: 1.0875613689422607
Epoch 1640, training loss: 881.418701171875 = 1.0847859382629395 + 100.0 * 8.803339004516602
Epoch 1640, val loss: 1.087561011314392
Epoch 1650, training loss: 882.0765380859375 = 1.084779143333435 + 100.0 * 8.809917449951172
Epoch 1650, val loss: 1.0875605344772339
Epoch 1660, training loss: 882.1598510742188 = 1.0847721099853516 + 100.0 * 8.810750961303711
Epoch 1660, val loss: 1.0875591039657593
Epoch 1670, training loss: 882.1798706054688 = 1.0847628116607666 + 100.0 * 8.810951232910156
Epoch 1670, val loss: 1.087557315826416
Epoch 1680, training loss: 882.8744506835938 = 1.0847554206848145 + 100.0 * 8.817896842956543
Epoch 1680, val loss: 1.0875569581985474
Epoch 1690, training loss: 883.261962890625 = 1.0847485065460205 + 100.0 * 8.821771621704102
Epoch 1690, val loss: 1.0875557661056519
Epoch 1700, training loss: 883.334716796875 = 1.0847402811050415 + 100.0 * 8.822500228881836
Epoch 1700, val loss: 1.0875543355941772
Epoch 1710, training loss: 883.1324462890625 = 1.0847324132919312 + 100.0 * 8.820477485656738
Epoch 1710, val loss: 1.0875526666641235
Epoch 1720, training loss: 883.563720703125 = 1.0847268104553223 + 100.0 * 8.824790000915527
Epoch 1720, val loss: 1.0875535011291504
Epoch 1730, training loss: 883.2178955078125 = 1.0847162008285522 + 100.0 * 8.821331977844238
Epoch 1730, val loss: 1.0875482559204102
Epoch 1740, training loss: 881.8441162109375 = 1.0847054719924927 + 100.0 * 8.807594299316406
Epoch 1740, val loss: 1.0875450372695923
Epoch 1750, training loss: 881.2962646484375 = 1.0846952199935913 + 100.0 * 8.802115440368652
Epoch 1750, val loss: 1.0875439643859863
Epoch 1760, training loss: 880.81103515625 = 1.084688425064087 + 100.0 * 8.797263145446777
Epoch 1760, val loss: 1.0875420570373535
Epoch 1770, training loss: 881.849365234375 = 1.084686279296875 + 100.0 * 8.807646751403809
Epoch 1770, val loss: 1.087545394897461
Epoch 1780, training loss: 883.0297241210938 = 1.0846831798553467 + 100.0 * 8.819450378417969
Epoch 1780, val loss: 1.0875468254089355
Epoch 1790, training loss: 883.5572509765625 = 1.0846787691116333 + 100.0 * 8.824726104736328
Epoch 1790, val loss: 1.0875483751296997
Epoch 1800, training loss: 883.95703125 = 1.0846741199493408 + 100.0 * 8.828723907470703
Epoch 1800, val loss: 1.087548851966858
Epoch 1810, training loss: 884.0753784179688 = 1.084668755531311 + 100.0 * 8.829907417297363
Epoch 1810, val loss: 1.087549090385437
Epoch 1820, training loss: 884.2322998046875 = 1.0846630334854126 + 100.0 * 8.831476211547852
Epoch 1820, val loss: 1.0875492095947266
Epoch 1830, training loss: 884.3809814453125 = 1.084657907485962 + 100.0 * 8.832962989807129
Epoch 1830, val loss: 1.0875494480133057
Epoch 1840, training loss: 884.5684204101562 = 1.0846529006958008 + 100.0 * 8.834837913513184
Epoch 1840, val loss: 1.0875500440597534
Epoch 1850, training loss: 885.1234741210938 = 1.0846484899520874 + 100.0 * 8.840388298034668
Epoch 1850, val loss: 1.0875505208969116
Epoch 1860, training loss: 884.977783203125 = 1.0846422910690308 + 100.0 * 8.8389310836792
Epoch 1860, val loss: 1.0875499248504639
Epoch 1870, training loss: 884.969970703125 = 1.0846383571624756 + 100.0 * 8.838852882385254
Epoch 1870, val loss: 1.087551236152649
Epoch 1880, training loss: 885.49169921875 = 1.0846346616744995 + 100.0 * 8.844070434570312
Epoch 1880, val loss: 1.087552547454834
Epoch 1890, training loss: 885.6507568359375 = 1.084630012512207 + 100.0 * 8.845661163330078
Epoch 1890, val loss: 1.0875535011291504
Epoch 1900, training loss: 885.487060546875 = 1.0846247673034668 + 100.0 * 8.844024658203125
Epoch 1900, val loss: 1.0875533819198608
Epoch 1910, training loss: 885.5533447265625 = 1.0846197605133057 + 100.0 * 8.844687461853027
Epoch 1910, val loss: 1.0875530242919922
Epoch 1920, training loss: 885.335205078125 = 1.0846145153045654 + 100.0 * 8.842506408691406
Epoch 1920, val loss: 1.0875530242919922
Epoch 1930, training loss: 885.8551025390625 = 1.0846120119094849 + 100.0 * 8.847704887390137
Epoch 1930, val loss: 1.0875554084777832
Epoch 1940, training loss: 885.99853515625 = 1.0846092700958252 + 100.0 * 8.849139213562012
Epoch 1940, val loss: 1.0875568389892578
Epoch 1950, training loss: 886.198486328125 = 1.0846049785614014 + 100.0 * 8.851139068603516
Epoch 1950, val loss: 1.087558388710022
Epoch 1960, training loss: 886.3017578125 = 1.0846014022827148 + 100.0 * 8.852171897888184
Epoch 1960, val loss: 1.0875587463378906
Epoch 1970, training loss: 886.2494506835938 = 1.0845977067947388 + 100.0 * 8.851648330688477
Epoch 1970, val loss: 1.0875592231750488
Epoch 1980, training loss: 885.0445556640625 = 1.0845885276794434 + 100.0 * 8.839599609375
Epoch 1980, val loss: 1.0875558853149414
Epoch 1990, training loss: 885.3056030273438 = 1.084586262702942 + 100.0 * 8.842209815979004
Epoch 1990, val loss: 1.0875581502914429
Epoch 2000, training loss: 885.854248046875 = 1.0845845937728882 + 100.0 * 8.847696304321289
Epoch 2000, val loss: 1.0875606536865234
Epoch 2010, training loss: 886.5222778320312 = 1.0845836400985718 + 100.0 * 8.854376792907715
Epoch 2010, val loss: 1.0875638723373413
Epoch 2020, training loss: 886.4573364257812 = 1.084580659866333 + 100.0 * 8.853727340698242
Epoch 2020, val loss: 1.087565541267395
Epoch 2030, training loss: 886.7255249023438 = 1.0845777988433838 + 100.0 * 8.856409072875977
Epoch 2030, val loss: 1.08756685256958
Epoch 2040, training loss: 887.01513671875 = 1.0845755338668823 + 100.0 * 8.859305381774902
Epoch 2040, val loss: 1.087567925453186
Epoch 2050, training loss: 886.918212890625 = 1.0845714807510376 + 100.0 * 8.858336448669434
Epoch 2050, val loss: 1.0875685214996338
Epoch 2060, training loss: 886.965087890625 = 1.0845694541931152 + 100.0 * 8.858804702758789
Epoch 2060, val loss: 1.0875705480575562
Epoch 2070, training loss: 887.2716674804688 = 1.084567904472351 + 100.0 * 8.861870765686035
Epoch 2070, val loss: 1.0875730514526367
Epoch 2080, training loss: 887.590576171875 = 1.0845656394958496 + 100.0 * 8.865059852600098
Epoch 2080, val loss: 1.0875744819641113
Epoch 2090, training loss: 887.7681884765625 = 1.0845632553100586 + 100.0 * 8.866836547851562
Epoch 2090, val loss: 1.0875755548477173
Epoch 2100, training loss: 887.3417358398438 = 1.0845593214035034 + 100.0 * 8.862571716308594
Epoch 2100, val loss: 1.0875756740570068
Epoch 2110, training loss: 887.73779296875 = 1.084557294845581 + 100.0 * 8.866532325744629
Epoch 2110, val loss: 1.0875777006149292
Epoch 2120, training loss: 887.9109497070312 = 1.0845550298690796 + 100.0 * 8.868264198303223
Epoch 2120, val loss: 1.0875794887542725
Epoch 2130, training loss: 888.0045166015625 = 1.0845530033111572 + 100.0 * 8.869199752807617
Epoch 2130, val loss: 1.087579369544983
Epoch 2140, training loss: 887.2621459960938 = 1.0845447778701782 + 100.0 * 8.861776351928711
Epoch 2140, val loss: 1.087571382522583
Epoch 2150, training loss: 884.0614624023438 = 1.084530234336853 + 100.0 * 8.829769134521484
Epoch 2150, val loss: 1.0875701904296875
Epoch 2160, training loss: 882.4759521484375 = 1.0845210552215576 + 100.0 * 8.81391429901123
Epoch 2160, val loss: 1.0875664949417114
Epoch 2170, training loss: 884.2666015625 = 1.0845245122909546 + 100.0 * 8.831820487976074
Epoch 2170, val loss: 1.0875705480575562
Epoch 2180, training loss: 885.54052734375 = 1.0845279693603516 + 100.0 * 8.844559669494629
Epoch 2180, val loss: 1.0875765085220337
Epoch 2190, training loss: 886.6900634765625 = 1.0845332145690918 + 100.0 * 8.85605525970459
Epoch 2190, val loss: 1.0875831842422485
Epoch 2200, training loss: 887.2709350585938 = 1.084533929824829 + 100.0 * 8.86186408996582
Epoch 2200, val loss: 1.0875877141952515
Epoch 2210, training loss: 887.97705078125 = 1.0845351219177246 + 100.0 * 8.868925094604492
Epoch 2210, val loss: 1.0875905752182007
Epoch 2220, training loss: 888.5905151367188 = 1.0845369100570679 + 100.0 * 8.875060081481934
Epoch 2220, val loss: 1.0875946283340454
Epoch 2230, training loss: 889.03955078125 = 1.0845369100570679 + 100.0 * 8.879549980163574
Epoch 2230, val loss: 1.0875970125198364
Epoch 2240, training loss: 889.0619506835938 = 1.084534764289856 + 100.0 * 8.87977409362793
Epoch 2240, val loss: 1.087598204612732
Epoch 2250, training loss: 889.3135986328125 = 1.0845340490341187 + 100.0 * 8.882290840148926
Epoch 2250, val loss: 1.0876009464263916
Epoch 2260, training loss: 889.7476196289062 = 1.0845342874526978 + 100.0 * 8.88663101196289
Epoch 2260, val loss: 1.0876026153564453
Epoch 2270, training loss: 890.0214233398438 = 1.0845332145690918 + 100.0 * 8.889369010925293
Epoch 2270, val loss: 1.0876049995422363
Epoch 2280, training loss: 889.8279418945312 = 1.0845311880111694 + 100.0 * 8.887434005737305
Epoch 2280, val loss: 1.087605357170105
Epoch 2290, training loss: 890.2781372070312 = 1.0845307111740112 + 100.0 * 8.891936302185059
Epoch 2290, val loss: 1.0876080989837646
Epoch 2300, training loss: 890.5869750976562 = 1.0845303535461426 + 100.0 * 8.895024299621582
Epoch 2300, val loss: 1.087610125541687
Epoch 2310, training loss: 890.7437744140625 = 1.0845295190811157 + 100.0 * 8.896592140197754
Epoch 2310, val loss: 1.087611436843872
Epoch 2320, training loss: 890.4340209960938 = 1.0845273733139038 + 100.0 * 8.893494606018066
Epoch 2320, val loss: 1.0876120328903198
Epoch 2330, training loss: 890.8326416015625 = 1.084526777267456 + 100.0 * 8.897480964660645
Epoch 2330, val loss: 1.0876140594482422
Epoch 2340, training loss: 890.9774780273438 = 1.0845260620117188 + 100.0 * 8.898929595947266
Epoch 2340, val loss: 1.0876153707504272
Epoch 2350, training loss: 891.3170776367188 = 1.0845251083374023 + 100.0 * 8.902325630187988
Epoch 2350, val loss: 1.087617039680481
Epoch 2360, training loss: 891.26123046875 = 1.084523320198059 + 100.0 * 8.901766777038574
Epoch 2360, val loss: 1.0876178741455078
Epoch 2370, training loss: 891.306396484375 = 1.0845222473144531 + 100.0 * 8.90221881866455
Epoch 2370, val loss: 1.0876182317733765
Epoch 2380, training loss: 891.7423706054688 = 1.0845215320587158 + 100.0 * 8.906578063964844
Epoch 2380, val loss: 1.0876208543777466
Epoch 2390, training loss: 892.1409912109375 = 1.084522008895874 + 100.0 * 8.910564422607422
Epoch 2390, val loss: 1.0876232385635376
Epoch 2400, training loss: 892.0134887695312 = 1.0845195055007935 + 100.0 * 8.909289360046387
Epoch 2400, val loss: 1.0876225233078003
Epoch 2410, training loss: 891.9611206054688 = 1.0845180749893188 + 100.0 * 8.90876579284668
Epoch 2410, val loss: 1.087624192237854
Epoch 2420, training loss: 892.18359375 = 1.0845175981521606 + 100.0 * 8.910990715026855
Epoch 2420, val loss: 1.0876256227493286
Epoch 2430, training loss: 892.207275390625 = 1.0845167636871338 + 100.0 * 8.911227226257324
Epoch 2430, val loss: 1.0876271724700928
Epoch 2440, training loss: 891.7667846679688 = 1.0845110416412354 + 100.0 * 8.906822204589844
Epoch 2440, val loss: 1.0876240730285645
Epoch 2450, training loss: 889.9583740234375 = 1.0845063924789429 + 100.0 * 8.888738632202148
Epoch 2450, val loss: 1.0876209735870361
Epoch 2460, training loss: 890.5654907226562 = 1.0845052003860474 + 100.0 * 8.89480972290039
Epoch 2460, val loss: 1.0876221656799316
Epoch 2470, training loss: 891.19580078125 = 1.0845071077346802 + 100.0 * 8.90111255645752
Epoch 2470, val loss: 1.0876264572143555
Epoch 2480, training loss: 892.2161254882812 = 1.084510326385498 + 100.0 * 8.91131591796875
Epoch 2480, val loss: 1.0876309871673584
Epoch 2490, training loss: 892.6741943359375 = 1.0845109224319458 + 100.0 * 8.91589641571045
Epoch 2490, val loss: 1.0876331329345703
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.39666666666666667
0.8645222053176846
=== training gcn model ===
Epoch 0, training loss: 1003.8060913085938 = 1.0979431867599487 + 100.0 * 10.027081489562988
Epoch 0, val loss: 1.0977754592895508
Epoch 10, training loss: 958.5032348632812 = 1.0977908372879028 + 100.0 * 9.574054718017578
Epoch 10, val loss: 1.0976139307022095
Epoch 20, training loss: 940.828369140625 = 1.097695231437683 + 100.0 * 9.397306442260742
Epoch 20, val loss: 1.0975044965744019
Epoch 30, training loss: 927.8074340820312 = 1.097599744796753 + 100.0 * 9.267098426818848
Epoch 30, val loss: 1.0973920822143555
Epoch 40, training loss: 917.7210083007812 = 1.0975037813186646 + 100.0 * 9.166234970092773
Epoch 40, val loss: 1.09727942943573
Epoch 50, training loss: 909.4963989257812 = 1.0974061489105225 + 100.0 * 9.083990097045898
Epoch 50, val loss: 1.0971629619598389
Epoch 60, training loss: 902.5926513671875 = 1.0973126888275146 + 100.0 * 9.01495361328125
Epoch 60, val loss: 1.0970518589019775
Epoch 70, training loss: 896.767822265625 = 1.0972274541854858 + 100.0 * 8.956706047058105
Epoch 70, val loss: 1.0969465970993042
Epoch 80, training loss: 891.7581176757812 = 1.0971381664276123 + 100.0 * 8.906609535217285
Epoch 80, val loss: 1.0968356132507324
Epoch 90, training loss: 887.5134887695312 = 1.0970414876937866 + 100.0 * 8.864164352416992
Epoch 90, val loss: 1.09671950340271
Epoch 100, training loss: 883.8629150390625 = 1.0969538688659668 + 100.0 * 8.827659606933594
Epoch 100, val loss: 1.0966126918792725
Epoch 110, training loss: 880.761962890625 = 1.096862554550171 + 100.0 * 8.796650886535645
Epoch 110, val loss: 1.0965044498443604
Epoch 120, training loss: 878.1280517578125 = 1.0967745780944824 + 100.0 * 8.770312309265137
Epoch 120, val loss: 1.0964007377624512
Epoch 130, training loss: 875.874755859375 = 1.0966875553131104 + 100.0 * 8.747780799865723
Epoch 130, val loss: 1.0962940454483032
Epoch 140, training loss: 873.947509765625 = 1.0966027975082397 + 100.0 * 8.728508949279785
Epoch 140, val loss: 1.096192717552185
Epoch 150, training loss: 872.2882080078125 = 1.096518635749817 + 100.0 * 8.71191692352295
Epoch 150, val loss: 1.0960910320281982
Epoch 160, training loss: 870.8555908203125 = 1.0964360237121582 + 100.0 * 8.697591781616211
Epoch 160, val loss: 1.0959923267364502
Epoch 170, training loss: 869.5875244140625 = 1.096354603767395 + 100.0 * 8.684911727905273
Epoch 170, val loss: 1.0958939790725708
Epoch 180, training loss: 868.3627319335938 = 1.0962753295898438 + 100.0 * 8.672664642333984
Epoch 180, val loss: 1.0957980155944824
Epoch 190, training loss: 867.5513916015625 = 1.096199631690979 + 100.0 * 8.664551734924316
Epoch 190, val loss: 1.0957067012786865
Epoch 200, training loss: 866.7778930664062 = 1.096124529838562 + 100.0 * 8.656817436218262
Epoch 200, val loss: 1.095613718032837
Epoch 210, training loss: 866.0839233398438 = 1.0960510969161987 + 100.0 * 8.64987850189209
Epoch 210, val loss: 1.0955240726470947
Epoch 220, training loss: 865.5324096679688 = 1.0959720611572266 + 100.0 * 8.644364356994629
Epoch 220, val loss: 1.0954314470291138
Epoch 230, training loss: 864.8831787109375 = 1.0959060192108154 + 100.0 * 8.637872695922852
Epoch 230, val loss: 1.095346212387085
Epoch 240, training loss: 863.895263671875 = 1.0958331823349 + 100.0 * 8.627994537353516
Epoch 240, val loss: 1.0952578783035278
Epoch 250, training loss: 863.5557250976562 = 1.0957672595977783 + 100.0 * 8.62459945678711
Epoch 250, val loss: 1.095174789428711
Epoch 260, training loss: 862.9473876953125 = 1.0956990718841553 + 100.0 * 8.61851692199707
Epoch 260, val loss: 1.0950907468795776
Epoch 270, training loss: 862.5192260742188 = 1.095631718635559 + 100.0 * 8.614235877990723
Epoch 270, val loss: 1.095008134841919
Epoch 280, training loss: 862.5091552734375 = 1.0955699682235718 + 100.0 * 8.6141357421875
Epoch 280, val loss: 1.0949304103851318
Epoch 290, training loss: 862.1767578125 = 1.0955040454864502 + 100.0 * 8.610812187194824
Epoch 290, val loss: 1.094848871231079
Epoch 300, training loss: 861.7059326171875 = 1.0954415798187256 + 100.0 * 8.606104850769043
Epoch 300, val loss: 1.094770908355713
Epoch 310, training loss: 861.4808349609375 = 1.0953787565231323 + 100.0 * 8.603854179382324
Epoch 310, val loss: 1.0946924686431885
Epoch 320, training loss: 861.3639526367188 = 1.0953171253204346 + 100.0 * 8.602685928344727
Epoch 320, val loss: 1.0946162939071655
Epoch 330, training loss: 861.0798950195312 = 1.0952585935592651 + 100.0 * 8.599845886230469
Epoch 330, val loss: 1.094539761543274
Epoch 340, training loss: 861.0540771484375 = 1.0952025651931763 + 100.0 * 8.599588394165039
Epoch 340, val loss: 1.094470739364624
Epoch 350, training loss: 861.2445678710938 = 1.0951414108276367 + 100.0 * 8.601493835449219
Epoch 350, val loss: 1.0943922996520996
Epoch 360, training loss: 861.66162109375 = 1.0950924158096313 + 100.0 * 8.60566520690918
Epoch 360, val loss: 1.094329595565796
Epoch 370, training loss: 860.6744995117188 = 1.0950356721878052 + 100.0 * 8.595794677734375
Epoch 370, val loss: 1.0942578315734863
Epoch 380, training loss: 860.76123046875 = 1.0949797630310059 + 100.0 * 8.596662521362305
Epoch 380, val loss: 1.09418523311615
Epoch 390, training loss: 860.9275512695312 = 1.0949294567108154 + 100.0 * 8.598326683044434
Epoch 390, val loss: 1.0941201448440552
Epoch 400, training loss: 860.9769287109375 = 1.0948773622512817 + 100.0 * 8.598820686340332
Epoch 400, val loss: 1.094053864479065
Epoch 410, training loss: 860.7644653320312 = 1.0948243141174316 + 100.0 * 8.596695899963379
Epoch 410, val loss: 1.0939865112304688
Epoch 420, training loss: 861.2451171875 = 1.094778299331665 + 100.0 * 8.601503372192383
Epoch 420, val loss: 1.093928337097168
Epoch 430, training loss: 861.4490356445312 = 1.0947315692901611 + 100.0 * 8.603543281555176
Epoch 430, val loss: 1.0938626527786255
Epoch 440, training loss: 861.2310791015625 = 1.0946823358535767 + 100.0 * 8.601364135742188
Epoch 440, val loss: 1.093801736831665
Epoch 450, training loss: 861.0499877929688 = 1.0946345329284668 + 100.0 * 8.599554061889648
Epoch 450, val loss: 1.0937385559082031
Epoch 460, training loss: 861.1182861328125 = 1.094589352607727 + 100.0 * 8.600236892700195
Epoch 460, val loss: 1.0936781167984009
Epoch 470, training loss: 861.128173828125 = 1.09454345703125 + 100.0 * 8.600336074829102
Epoch 470, val loss: 1.093618631362915
Epoch 480, training loss: 861.00537109375 = 1.0944981575012207 + 100.0 * 8.599108695983887
Epoch 480, val loss: 1.093559980392456
Epoch 490, training loss: 860.9480590820312 = 1.0944511890411377 + 100.0 * 8.598536491394043
Epoch 490, val loss: 1.093498706817627
Epoch 500, training loss: 861.1270751953125 = 1.0944105386734009 + 100.0 * 8.600326538085938
Epoch 500, val loss: 1.0934441089630127
Epoch 510, training loss: 861.3222045898438 = 1.0943708419799805 + 100.0 * 8.602278709411621
Epoch 510, val loss: 1.0933891534805298
Epoch 520, training loss: 864.4593505859375 = 1.0943188667297363 + 100.0 * 8.633650779724121
Epoch 520, val loss: 1.0933201313018799
Epoch 530, training loss: 861.7378540039062 = 1.0942871570587158 + 100.0 * 8.606435775756836
Epoch 530, val loss: 1.0932838916778564
Epoch 540, training loss: 861.2973022460938 = 1.0942471027374268 + 100.0 * 8.602030754089355
Epoch 540, val loss: 1.093221664428711
Epoch 550, training loss: 861.4921264648438 = 1.094212532043457 + 100.0 * 8.603979110717773
Epoch 550, val loss: 1.0931764841079712
Epoch 560, training loss: 861.6555786132812 = 1.094172477722168 + 100.0 * 8.605613708496094
Epoch 560, val loss: 1.093124270439148
Epoch 570, training loss: 861.8575439453125 = 1.0941375494003296 + 100.0 * 8.607634544372559
Epoch 570, val loss: 1.0930758714675903
Epoch 580, training loss: 862.1181030273438 = 1.0941030979156494 + 100.0 * 8.61023998260498
Epoch 580, val loss: 1.0930289030075073
Epoch 590, training loss: 862.518310546875 = 1.0940693616867065 + 100.0 * 8.614242553710938
Epoch 590, val loss: 1.092981219291687
Epoch 600, training loss: 862.7274780273438 = 1.0940333604812622 + 100.0 * 8.616333961486816
Epoch 600, val loss: 1.0929327011108398
Epoch 610, training loss: 862.6625366210938 = 1.0939979553222656 + 100.0 * 8.61568546295166
Epoch 610, val loss: 1.0928853750228882
Epoch 620, training loss: 862.9324951171875 = 1.0939663648605347 + 100.0 * 8.618385314941406
Epoch 620, val loss: 1.0928411483764648
Epoch 630, training loss: 863.1605834960938 = 1.0939345359802246 + 100.0 * 8.62066650390625
Epoch 630, val loss: 1.092797040939331
Epoch 640, training loss: 863.4160766601562 = 1.0939027070999146 + 100.0 * 8.623221397399902
Epoch 640, val loss: 1.0927519798278809
Epoch 650, training loss: 863.1619262695312 = 1.0938706398010254 + 100.0 * 8.620680809020996
Epoch 650, val loss: 1.0927085876464844
Epoch 660, training loss: 863.5209350585938 = 1.0938408374786377 + 100.0 * 8.624271392822266
Epoch 660, val loss: 1.0926659107208252
Epoch 670, training loss: 863.9944458007812 = 1.093812108039856 + 100.0 * 8.629006385803223
Epoch 670, val loss: 1.0926251411437988
Epoch 680, training loss: 863.912841796875 = 1.0937825441360474 + 100.0 * 8.628190040588379
Epoch 680, val loss: 1.0925822257995605
Epoch 690, training loss: 864.1448974609375 = 1.0937553644180298 + 100.0 * 8.630511283874512
Epoch 690, val loss: 1.0925438404083252
Epoch 700, training loss: 864.3034057617188 = 1.0937273502349854 + 100.0 * 8.632096290588379
Epoch 700, val loss: 1.0925043821334839
Epoch 710, training loss: 864.5595703125 = 1.093700647354126 + 100.0 * 8.634658813476562
Epoch 710, val loss: 1.0924663543701172
Epoch 720, training loss: 863.724365234375 = 1.0936665534973145 + 100.0 * 8.626306533813477
Epoch 720, val loss: 1.092421054840088
Epoch 730, training loss: 864.5147094726562 = 1.093644380569458 + 100.0 * 8.634210586547852
Epoch 730, val loss: 1.0923854112625122
Epoch 740, training loss: 864.8463745117188 = 1.0936213731765747 + 100.0 * 8.637527465820312
Epoch 740, val loss: 1.092352032661438
Epoch 750, training loss: 865.267822265625 = 1.09359872341156 + 100.0 * 8.641741752624512
Epoch 750, val loss: 1.092319369316101
Epoch 760, training loss: 865.5332641601562 = 1.0935744047164917 + 100.0 * 8.644396781921387
Epoch 760, val loss: 1.0922843217849731
Epoch 770, training loss: 865.817626953125 = 1.093551754951477 + 100.0 * 8.64724063873291
Epoch 770, val loss: 1.0922492742538452
Epoch 780, training loss: 865.8550415039062 = 1.0935282707214355 + 100.0 * 8.647615432739258
Epoch 780, val loss: 1.092215657234192
Epoch 790, training loss: 866.1392211914062 = 1.0935063362121582 + 100.0 * 8.650457382202148
Epoch 790, val loss: 1.0921815633773804
Epoch 800, training loss: 866.3153686523438 = 1.0934844017028809 + 100.0 * 8.65221881866455
Epoch 800, val loss: 1.092147946357727
Epoch 810, training loss: 866.1962280273438 = 1.0934627056121826 + 100.0 * 8.65102767944336
Epoch 810, val loss: 1.092117428779602
Epoch 820, training loss: 866.8228759765625 = 1.0934419631958008 + 100.0 * 8.657294273376465
Epoch 820, val loss: 1.0920861959457397
Epoch 830, training loss: 866.9302368164062 = 1.0934216976165771 + 100.0 * 8.658368110656738
Epoch 830, val loss: 1.0920547246932983
Epoch 840, training loss: 867.082763671875 = 1.0934029817581177 + 100.0 * 8.659893989562988
Epoch 840, val loss: 1.0920259952545166
Epoch 850, training loss: 866.8440551757812 = 1.0933821201324463 + 100.0 * 8.657506942749023
Epoch 850, val loss: 1.091993808746338
Epoch 860, training loss: 867.1510620117188 = 1.0933642387390137 + 100.0 * 8.660576820373535
Epoch 860, val loss: 1.0919666290283203
Epoch 870, training loss: 865.2628784179688 = 1.0932891368865967 + 100.0 * 8.641695976257324
Epoch 870, val loss: 1.0918806791305542
Epoch 880, training loss: 865.5572509765625 = 1.093314290046692 + 100.0 * 8.644639015197754
Epoch 880, val loss: 1.0919090509414673
Epoch 890, training loss: 865.94775390625 = 1.0933008193969727 + 100.0 * 8.648544311523438
Epoch 890, val loss: 1.0918773412704468
Epoch 900, training loss: 866.1898193359375 = 1.0932897329330444 + 100.0 * 8.650965690612793
Epoch 900, val loss: 1.0918588638305664
Epoch 910, training loss: 865.7175903320312 = 1.093272089958191 + 100.0 * 8.64624309539795
Epoch 910, val loss: 1.0918309688568115
Epoch 920, training loss: 866.6158447265625 = 1.0932621955871582 + 100.0 * 8.65522575378418
Epoch 920, val loss: 1.0918108224868774
Epoch 930, training loss: 867.1571655273438 = 1.093246340751648 + 100.0 * 8.660638809204102
Epoch 930, val loss: 1.0917863845825195
Epoch 940, training loss: 867.7075805664062 = 1.093233346939087 + 100.0 * 8.666143417358398
Epoch 940, val loss: 1.0917637348175049
Epoch 950, training loss: 868.26953125 = 1.0932211875915527 + 100.0 * 8.67176342010498
Epoch 950, val loss: 1.0917413234710693
Epoch 960, training loss: 868.7530517578125 = 1.0932068824768066 + 100.0 * 8.67659854888916
Epoch 960, val loss: 1.0917181968688965
Epoch 970, training loss: 869.2716674804688 = 1.0931954383850098 + 100.0 * 8.681784629821777
Epoch 970, val loss: 1.091697335243225
Epoch 980, training loss: 869.0066528320312 = 1.093180775642395 + 100.0 * 8.679134368896484
Epoch 980, val loss: 1.0916739702224731
Epoch 990, training loss: 869.5057373046875 = 1.0931689739227295 + 100.0 * 8.684125900268555
Epoch 990, val loss: 1.0916528701782227
Epoch 1000, training loss: 869.4009399414062 = 1.0931543111801147 + 100.0 * 8.683077812194824
Epoch 1000, val loss: 1.0916295051574707
Epoch 1010, training loss: 869.6778564453125 = 1.0931422710418701 + 100.0 * 8.685847282409668
Epoch 1010, val loss: 1.0916088819503784
Epoch 1020, training loss: 869.5851440429688 = 1.09312903881073 + 100.0 * 8.684920310974121
Epoch 1020, val loss: 1.0915864706039429
Epoch 1030, training loss: 870.206298828125 = 1.0931191444396973 + 100.0 * 8.691131591796875
Epoch 1030, val loss: 1.0915684700012207
Epoch 1040, training loss: 870.330810546875 = 1.0931087732315063 + 100.0 * 8.692377090454102
Epoch 1040, val loss: 1.0915497541427612
Epoch 1050, training loss: 870.7678833007812 = 1.0930981636047363 + 100.0 * 8.696747779846191
Epoch 1050, val loss: 1.0915310382843018
Epoch 1060, training loss: 871.0752563476562 = 1.0930874347686768 + 100.0 * 8.699821472167969
Epoch 1060, val loss: 1.0915113687515259
Epoch 1070, training loss: 871.3546752929688 = 1.0930776596069336 + 100.0 * 8.702615737915039
Epoch 1070, val loss: 1.0914934873580933
Epoch 1080, training loss: 871.502197265625 = 1.0930659770965576 + 100.0 * 8.70409107208252
Epoch 1080, val loss: 1.0914746522903442
Epoch 1090, training loss: 871.6390380859375 = 1.093056321144104 + 100.0 * 8.705459594726562
Epoch 1090, val loss: 1.0914573669433594
Epoch 1100, training loss: 871.7400512695312 = 1.0930466651916504 + 100.0 * 8.706470489501953
Epoch 1100, val loss: 1.0914390087127686
Epoch 1110, training loss: 870.8567504882812 = 1.0930315256118774 + 100.0 * 8.697637557983398
Epoch 1110, val loss: 1.0914174318313599
Epoch 1120, training loss: 871.3058471679688 = 1.0930225849151611 + 100.0 * 8.702128410339355
Epoch 1120, val loss: 1.09140145778656
Epoch 1130, training loss: 871.7789916992188 = 1.0930168628692627 + 100.0 * 8.706859588623047
Epoch 1130, val loss: 1.0913876295089722
Epoch 1140, training loss: 872.16455078125 = 1.0930105447769165 + 100.0 * 8.710715293884277
Epoch 1140, val loss: 1.0913736820220947
Epoch 1150, training loss: 872.2632446289062 = 1.0930017232894897 + 100.0 * 8.711702346801758
Epoch 1150, val loss: 1.0913575887680054
Epoch 1160, training loss: 872.8118896484375 = 1.09299635887146 + 100.0 * 8.717188835144043
Epoch 1160, val loss: 1.0913444757461548
Epoch 1170, training loss: 872.3959350585938 = 1.0929863452911377 + 100.0 * 8.713029861450195
Epoch 1170, val loss: 1.0913273096084595
Epoch 1180, training loss: 873.1412353515625 = 1.0929808616638184 + 100.0 * 8.72048282623291
Epoch 1180, val loss: 1.0913151502609253
Epoch 1190, training loss: 873.3624877929688 = 1.0929741859436035 + 100.0 * 8.722695350646973
Epoch 1190, val loss: 1.0913012027740479
Epoch 1200, training loss: 873.3739624023438 = 1.0929663181304932 + 100.0 * 8.722809791564941
Epoch 1200, val loss: 1.0912864208221436
Epoch 1210, training loss: 873.5512084960938 = 1.0929594039916992 + 100.0 * 8.72458267211914
Epoch 1210, val loss: 1.0912734270095825
Epoch 1220, training loss: 873.49951171875 = 1.092951774597168 + 100.0 * 8.724065780639648
Epoch 1220, val loss: 1.091259241104126
Epoch 1230, training loss: 873.6888427734375 = 1.092946171760559 + 100.0 * 8.725958824157715
Epoch 1230, val loss: 1.0912469625473022
Epoch 1240, training loss: 874.1782836914062 = 1.092941403388977 + 100.0 * 8.730853080749512
Epoch 1240, val loss: 1.0912339687347412
Epoch 1250, training loss: 873.82275390625 = 1.0929334163665771 + 100.0 * 8.72729778289795
Epoch 1250, val loss: 1.0912197828292847
Epoch 1260, training loss: 873.954833984375 = 1.0929275751113892 + 100.0 * 8.728618621826172
Epoch 1260, val loss: 1.0912078619003296
Epoch 1270, training loss: 874.4462890625 = 1.0929234027862549 + 100.0 * 8.73353385925293
Epoch 1270, val loss: 1.0911973714828491
Epoch 1280, training loss: 874.5372314453125 = 1.0929173231124878 + 100.0 * 8.734443664550781
Epoch 1280, val loss: 1.0911864042282104
Epoch 1290, training loss: 874.7489013671875 = 1.0929120779037476 + 100.0 * 8.736559867858887
Epoch 1290, val loss: 1.0911754369735718
Epoch 1300, training loss: 874.9926147460938 = 1.0929080247879028 + 100.0 * 8.738997459411621
Epoch 1300, val loss: 1.0911650657653809
Epoch 1310, training loss: 875.57373046875 = 1.0929030179977417 + 100.0 * 8.744808197021484
Epoch 1310, val loss: 1.0911529064178467
Epoch 1320, training loss: 875.1064453125 = 1.092896819114685 + 100.0 * 8.740135192871094
Epoch 1320, val loss: 1.0911412239074707
Epoch 1330, training loss: 874.9357299804688 = 1.0928912162780762 + 100.0 * 8.738428115844727
Epoch 1330, val loss: 1.0911303758621216
Epoch 1340, training loss: 875.2667236328125 = 1.0928887128829956 + 100.0 * 8.741738319396973
Epoch 1340, val loss: 1.0911229848861694
Epoch 1350, training loss: 875.710205078125 = 1.0928858518600464 + 100.0 * 8.746172904968262
Epoch 1350, val loss: 1.0911147594451904
Epoch 1360, training loss: 875.9072265625 = 1.0928822755813599 + 100.0 * 8.748143196105957
Epoch 1360, val loss: 1.0911046266555786
Epoch 1370, training loss: 876.0930786132812 = 1.0928786993026733 + 100.0 * 8.750001907348633
Epoch 1370, val loss: 1.0910959243774414
Epoch 1380, training loss: 876.3886108398438 = 1.0928754806518555 + 100.0 * 8.752957344055176
Epoch 1380, val loss: 1.0910872220993042
Epoch 1390, training loss: 876.6260986328125 = 1.0928720235824585 + 100.0 * 8.755331993103027
Epoch 1390, val loss: 1.0910789966583252
Epoch 1400, training loss: 876.9601440429688 = 1.0928694009780884 + 100.0 * 8.758672714233398
Epoch 1400, val loss: 1.0910706520080566
Epoch 1410, training loss: 877.0506591796875 = 1.0928654670715332 + 100.0 * 8.759577751159668
Epoch 1410, val loss: 1.0910618305206299
Epoch 1420, training loss: 876.9956665039062 = 1.0928573608398438 + 100.0 * 8.759028434753418
Epoch 1420, val loss: 1.0910457372665405
Epoch 1430, training loss: 877.1014404296875 = 1.0928583145141602 + 100.0 * 8.760086059570312
Epoch 1430, val loss: 1.0910451412200928
Epoch 1440, training loss: 876.1361083984375 = 1.0928473472595215 + 100.0 * 8.750432968139648
Epoch 1440, val loss: 1.0910296440124512
Epoch 1450, training loss: 876.7106323242188 = 1.0928486585617065 + 100.0 * 8.75617790222168
Epoch 1450, val loss: 1.0910272598266602
Epoch 1460, training loss: 877.0814208984375 = 1.092846393585205 + 100.0 * 8.759885787963867
Epoch 1460, val loss: 1.091019868850708
Epoch 1470, training loss: 877.19384765625 = 1.0928428173065186 + 100.0 * 8.76101016998291
Epoch 1470, val loss: 1.0910117626190186
Epoch 1480, training loss: 877.4136962890625 = 1.092840313911438 + 100.0 * 8.763208389282227
Epoch 1480, val loss: 1.0910040140151978
Epoch 1490, training loss: 877.5133056640625 = 1.0928382873535156 + 100.0 * 8.764204978942871
Epoch 1490, val loss: 1.0909969806671143
Epoch 1500, training loss: 877.8189697265625 = 1.092838168144226 + 100.0 * 8.767261505126953
Epoch 1500, val loss: 1.09099280834198
Epoch 1510, training loss: 878.1136474609375 = 1.0928367376327515 + 100.0 * 8.770208358764648
Epoch 1510, val loss: 1.090987205505371
Epoch 1520, training loss: 878.3074340820312 = 1.0928343534469604 + 100.0 * 8.772146224975586
Epoch 1520, val loss: 1.0909818410873413
Epoch 1530, training loss: 878.407958984375 = 1.0928326845169067 + 100.0 * 8.773151397705078
Epoch 1530, val loss: 1.0909756422042847
Epoch 1540, training loss: 879.0511474609375 = 1.0928306579589844 + 100.0 * 8.779582977294922
Epoch 1540, val loss: 1.090967059135437
Epoch 1550, training loss: 878.164794921875 = 1.0928232669830322 + 100.0 * 8.770719528198242
Epoch 1550, val loss: 1.0909570455551147
Epoch 1560, training loss: 878.51171875 = 1.092822551727295 + 100.0 * 8.774188995361328
Epoch 1560, val loss: 1.0909545421600342
Epoch 1570, training loss: 879.010986328125 = 1.0928223133087158 + 100.0 * 8.779181480407715
Epoch 1570, val loss: 1.0909512042999268
Epoch 1580, training loss: 879.1707153320312 = 1.0928210020065308 + 100.0 * 8.780778884887695
Epoch 1580, val loss: 1.0909451246261597
Epoch 1590, training loss: 879.1622314453125 = 1.0928194522857666 + 100.0 * 8.780694007873535
Epoch 1590, val loss: 1.0909405946731567
Epoch 1600, training loss: 879.527099609375 = 1.092819333076477 + 100.0 * 8.784342765808105
Epoch 1600, val loss: 1.0909366607666016
Epoch 1610, training loss: 879.6799926757812 = 1.092817783355713 + 100.0 * 8.785871505737305
Epoch 1610, val loss: 1.0909310579299927
Epoch 1620, training loss: 879.5922241210938 = 1.0928151607513428 + 100.0 * 8.784994125366211
Epoch 1620, val loss: 1.0909258127212524
Epoch 1630, training loss: 878.8487548828125 = 1.092808485031128 + 100.0 * 8.777559280395508
Epoch 1630, val loss: 1.0909167528152466
Epoch 1640, training loss: 879.3053588867188 = 1.0928093194961548 + 100.0 * 8.782125473022461
Epoch 1640, val loss: 1.090914011001587
Epoch 1650, training loss: 879.974853515625 = 1.0928096771240234 + 100.0 * 8.788820266723633
Epoch 1650, val loss: 1.0909109115600586
Epoch 1660, training loss: 880.5850219726562 = 1.0928103923797607 + 100.0 * 8.794921875
Epoch 1660, val loss: 1.0909080505371094
Epoch 1670, training loss: 880.318359375 = 1.092806339263916 + 100.0 * 8.792255401611328
Epoch 1670, val loss: 1.0909020900726318
Epoch 1680, training loss: 880.2789916992188 = 1.092806100845337 + 100.0 * 8.791861534118652
Epoch 1680, val loss: 1.0908983945846558
Epoch 1690, training loss: 880.9297485351562 = 1.0928066968917847 + 100.0 * 8.798369407653809
Epoch 1690, val loss: 1.0908957719802856
Epoch 1700, training loss: 881.0804443359375 = 1.0928059816360474 + 100.0 * 8.79987621307373
Epoch 1700, val loss: 1.090891718864441
Epoch 1710, training loss: 880.9727783203125 = 1.092804193496704 + 100.0 * 8.798799514770508
Epoch 1710, val loss: 1.090887188911438
Epoch 1720, training loss: 881.2714233398438 = 1.0928027629852295 + 100.0 * 8.801786422729492
Epoch 1720, val loss: 1.0908838510513306
Epoch 1730, training loss: 881.0089721679688 = 1.0927995443344116 + 100.0 * 8.799161911010742
Epoch 1730, val loss: 1.0908782482147217
Epoch 1740, training loss: 881.3798217773438 = 1.0927997827529907 + 100.0 * 8.802870750427246
Epoch 1740, val loss: 1.0908759832382202
Epoch 1750, training loss: 881.840576171875 = 1.0928007364273071 + 100.0 * 8.807477951049805
Epoch 1750, val loss: 1.090873122215271
Epoch 1760, training loss: 881.7966918945312 = 1.0927987098693848 + 100.0 * 8.807039260864258
Epoch 1760, val loss: 1.0908684730529785
Epoch 1770, training loss: 881.7883911132812 = 1.0927979946136475 + 100.0 * 8.80695629119873
Epoch 1770, val loss: 1.0908653736114502
Epoch 1780, training loss: 881.90869140625 = 1.0927976369857788 + 100.0 * 8.808158874511719
Epoch 1780, val loss: 1.0908626317977905
Epoch 1790, training loss: 881.9514770507812 = 1.0927958488464355 + 100.0 * 8.808587074279785
Epoch 1790, val loss: 1.0908582210540771
Epoch 1800, training loss: 882.3092041015625 = 1.092795491218567 + 100.0 * 8.812164306640625
Epoch 1800, val loss: 1.0908558368682861
Epoch 1810, training loss: 882.5806884765625 = 1.0927966833114624 + 100.0 * 8.814879417419434
Epoch 1810, val loss: 1.090854287147522
Epoch 1820, training loss: 882.0518188476562 = 1.0927914381027222 + 100.0 * 8.809590339660645
Epoch 1820, val loss: 1.0908466577529907
Epoch 1830, training loss: 882.2822875976562 = 1.0927908420562744 + 100.0 * 8.811895370483398
Epoch 1830, val loss: 1.0908448696136475
Epoch 1840, training loss: 882.5062255859375 = 1.0927915573120117 + 100.0 * 8.81413459777832
Epoch 1840, val loss: 1.0908427238464355
Epoch 1850, training loss: 883.0926513671875 = 1.0927929878234863 + 100.0 * 8.819998741149902
Epoch 1850, val loss: 1.0908422470092773
Epoch 1860, training loss: 882.9407348632812 = 1.0927839279174805 + 100.0 * 8.818479537963867
Epoch 1860, val loss: 1.090822696685791
Epoch 1870, training loss: 881.2493896484375 = 1.0927730798721313 + 100.0 * 8.801566123962402
Epoch 1870, val loss: 1.0908230543136597
Epoch 1880, training loss: 878.9153442382812 = 1.0927612781524658 + 100.0 * 8.778225898742676
Epoch 1880, val loss: 1.0908058881759644
Epoch 1890, training loss: 880.5889892578125 = 1.0927754640579224 + 100.0 * 8.794961929321289
Epoch 1890, val loss: 1.0908172130584717
Epoch 1900, training loss: 881.474365234375 = 1.0927793979644775 + 100.0 * 8.803815841674805
Epoch 1900, val loss: 1.0908198356628418
Epoch 1910, training loss: 881.8442993164062 = 1.0927820205688477 + 100.0 * 8.807515144348145
Epoch 1910, val loss: 1.090820074081421
Epoch 1920, training loss: 882.45751953125 = 1.0927841663360596 + 100.0 * 8.813647270202637
Epoch 1920, val loss: 1.090820074081421
Epoch 1930, training loss: 883.0158081054688 = 1.0927860736846924 + 100.0 * 8.819230079650879
Epoch 1930, val loss: 1.0908201932907104
Epoch 1940, training loss: 883.2354125976562 = 1.0927865505218506 + 100.0 * 8.821426391601562
Epoch 1940, val loss: 1.0908186435699463
Epoch 1950, training loss: 882.209228515625 = 1.0921437740325928 + 100.0 * 8.81117057800293
Epoch 1950, val loss: 1.0901213884353638
Epoch 1960, training loss: 882.7271118164062 = 1.0909150838851929 + 100.0 * 8.816361427307129
Epoch 1960, val loss: 1.0890032052993774
Epoch 1970, training loss: 883.2760009765625 = 1.089784860610962 + 100.0 * 8.82186222076416
Epoch 1970, val loss: 1.0879877805709839
Epoch 1980, training loss: 883.6454467773438 = 1.0888055562973022 + 100.0 * 8.825566291809082
Epoch 1980, val loss: 1.0871094465255737
Epoch 1990, training loss: 884.1949462890625 = 1.0879510641098022 + 100.0 * 8.831069946289062
Epoch 1990, val loss: 1.0863398313522339
Epoch 2000, training loss: 883.860595703125 = 1.0871926546096802 + 100.0 * 8.827733993530273
Epoch 2000, val loss: 1.0856565237045288
Epoch 2010, training loss: 884.3143310546875 = 1.0865113735198975 + 100.0 * 8.83227825164795
Epoch 2010, val loss: 1.0850396156311035
Epoch 2020, training loss: 884.6276245117188 = 1.085888385772705 + 100.0 * 8.835417747497559
Epoch 2020, val loss: 1.0844744443893433
Epoch 2030, training loss: 884.5101928710938 = 1.0853118896484375 + 100.0 * 8.834248542785645
Epoch 2030, val loss: 1.0839496850967407
Epoch 2040, training loss: 884.8079223632812 = 1.0847749710083008 + 100.0 * 8.837231636047363
Epoch 2040, val loss: 1.0834600925445557
Epoch 2050, training loss: 884.8638916015625 = 1.0842719078063965 + 100.0 * 8.837796211242676
Epoch 2050, val loss: 1.082999587059021
Epoch 2060, training loss: 884.997314453125 = 1.0837955474853516 + 100.0 * 8.83913516998291
Epoch 2060, val loss: 1.082563042640686
Epoch 2070, training loss: 884.77490234375 = 1.0833419561386108 + 100.0 * 8.836915969848633
Epoch 2070, val loss: 1.0821467638015747
Epoch 2080, training loss: 885.2345581054688 = 1.0829124450683594 + 100.0 * 8.841516494750977
Epoch 2080, val loss: 1.0817513465881348
Epoch 2090, training loss: 885.2579956054688 = 1.0824999809265137 + 100.0 * 8.841754913330078
Epoch 2090, val loss: 1.0813685655593872
Epoch 2100, training loss: 885.3575439453125 = 1.082105040550232 + 100.0 * 8.842754364013672
Epoch 2100, val loss: 1.0810045003890991
Epoch 2110, training loss: 885.5272827148438 = 1.0817252397537231 + 100.0 * 8.84445571899414
Epoch 2110, val loss: 1.0806528329849243
Epoch 2120, training loss: 885.7028198242188 = 1.081357479095459 + 100.0 * 8.846214294433594
Epoch 2120, val loss: 1.080312728881836
Epoch 2130, training loss: 885.6141357421875 = 1.0810014009475708 + 100.0 * 8.845331192016602
Epoch 2130, val loss: 1.0799800157546997
Epoch 2140, training loss: 885.7509765625 = 1.0806599855422974 + 100.0 * 8.846702575683594
Epoch 2140, val loss: 1.0796618461608887
Epoch 2150, training loss: 886.0317993164062 = 1.0803284645080566 + 100.0 * 8.849514961242676
Epoch 2150, val loss: 1.0793534517288208
Epoch 2160, training loss: 885.8232421875 = 1.080003261566162 + 100.0 * 8.847432136535645
Epoch 2160, val loss: 1.0790472030639648
Epoch 2170, training loss: 884.6678466796875 = 1.0796695947647095 + 100.0 * 8.835882186889648
Epoch 2170, val loss: 1.0787369012832642
Epoch 2180, training loss: 884.0196533203125 = 1.0793626308441162 + 100.0 * 8.829402923583984
Epoch 2180, val loss: 1.0784488916397095
Epoch 2190, training loss: 883.8159790039062 = 1.0790632963180542 + 100.0 * 8.827369689941406
Epoch 2190, val loss: 1.0781699419021606
Epoch 2200, training loss: 884.4112548828125 = 1.0787802934646606 + 100.0 * 8.833324432373047
Epoch 2200, val loss: 1.077901840209961
Epoch 2210, training loss: 885.0303344726562 = 1.0785022974014282 + 100.0 * 8.839518547058105
Epoch 2210, val loss: 1.0776398181915283
Epoch 2220, training loss: 885.8196411132812 = 1.0782301425933838 + 100.0 * 8.847414016723633
Epoch 2220, val loss: 1.0773831605911255
Epoch 2230, training loss: 886.178955078125 = 1.077960729598999 + 100.0 * 8.8510103225708
Epoch 2230, val loss: 1.0771288871765137
Epoch 2240, training loss: 886.2467041015625 = 1.0776954889297485 + 100.0 * 8.851690292358398
Epoch 2240, val loss: 1.0768780708312988
Epoch 2250, training loss: 886.4015502929688 = 1.0774366855621338 + 100.0 * 8.853240966796875
Epoch 2250, val loss: 1.0766345262527466
Epoch 2260, training loss: 886.540283203125 = 1.0771830081939697 + 100.0 * 8.854630470275879
Epoch 2260, val loss: 1.076393961906433
Epoch 2270, training loss: 886.7113037109375 = 1.0769346952438354 + 100.0 * 8.856344223022461
Epoch 2270, val loss: 1.076158046722412
Epoch 2280, training loss: 886.7025146484375 = 1.0766892433166504 + 100.0 * 8.856258392333984
Epoch 2280, val loss: 1.0759257078170776
Epoch 2290, training loss: 886.6680908203125 = 1.0764483213424683 + 100.0 * 8.855916023254395
Epoch 2290, val loss: 1.0756971836090088
Epoch 2300, training loss: 887.0038452148438 = 1.0762150287628174 + 100.0 * 8.859275817871094
Epoch 2300, val loss: 1.0754764080047607
Epoch 2310, training loss: 887.1421508789062 = 1.0759836435317993 + 100.0 * 8.860661506652832
Epoch 2310, val loss: 1.0752568244934082
Epoch 2320, training loss: 887.0089721679688 = 1.075754165649414 + 100.0 * 8.859332084655762
Epoch 2320, val loss: 1.0750384330749512
Epoch 2330, training loss: 887.2282104492188 = 1.0755327939987183 + 100.0 * 8.861526489257812
Epoch 2330, val loss: 1.074827790260315
Epoch 2340, training loss: 887.6145629882812 = 1.075315237045288 + 100.0 * 8.865392684936523
Epoch 2340, val loss: 1.0746212005615234
Epoch 2350, training loss: 887.6498413085938 = 1.075098991394043 + 100.0 * 8.865747451782227
Epoch 2350, val loss: 1.0744158029556274
Epoch 2360, training loss: 887.6283569335938 = 1.074885368347168 + 100.0 * 8.865534782409668
Epoch 2360, val loss: 1.0742107629776
Epoch 2370, training loss: 887.3878784179688 = 1.0746747255325317 + 100.0 * 8.86313247680664
Epoch 2370, val loss: 1.0740115642547607
Epoch 2380, training loss: 887.6343994140625 = 1.0744701623916626 + 100.0 * 8.865599632263184
Epoch 2380, val loss: 1.0738166570663452
Epoch 2390, training loss: 887.8949584960938 = 1.0742685794830322 + 100.0 * 8.868206977844238
Epoch 2390, val loss: 1.073623776435852
Epoch 2400, training loss: 888.2529296875 = 1.0740700960159302 + 100.0 * 8.871788024902344
Epoch 2400, val loss: 1.073434591293335
Epoch 2410, training loss: 888.2293090820312 = 1.0738739967346191 + 100.0 * 8.871554374694824
Epoch 2410, val loss: 1.0732463598251343
Epoch 2420, training loss: 888.097900390625 = 1.073678970336914 + 100.0 * 8.87024211883545
Epoch 2420, val loss: 1.0730608701705933
Epoch 2430, training loss: 888.2603759765625 = 1.0734879970550537 + 100.0 * 8.871869087219238
Epoch 2430, val loss: 1.0728795528411865
Epoch 2440, training loss: 888.525146484375 = 1.0733004808425903 + 100.0 * 8.874518394470215
Epoch 2440, val loss: 1.072701334953308
Epoch 2450, training loss: 888.56103515625 = 1.0731148719787598 + 100.0 * 8.874878883361816
Epoch 2450, val loss: 1.072523593902588
Epoch 2460, training loss: 887.4541015625 = 1.0729221105575562 + 100.0 * 8.863811492919922
Epoch 2460, val loss: 1.0723367929458618
Epoch 2470, training loss: 886.6480712890625 = 1.0727307796478271 + 100.0 * 8.855752944946289
Epoch 2470, val loss: 1.0721569061279297
Epoch 2480, training loss: 886.5692138671875 = 1.0725458860397339 + 100.0 * 8.854966163635254
Epoch 2480, val loss: 1.0719780921936035
Epoch 2490, training loss: 886.5945434570312 = 1.0723752975463867 + 100.0 * 8.85522174835205
Epoch 2490, val loss: 1.0718142986297607
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.3973913043478261
0.8634354850394842
The final CL Acc:0.39715, 0.00034, The final GNN Acc:0.86370, 0.00059
Begin epxeriment: noisy_level: 0.2 cont_weight: 100 epoch: der1:0.3 der2:0.4 dfr1:0.4 dfr2:0.4
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=2500, cl_num_layers=2, cl_num_proj_hidden=64, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=100.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.4, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=32, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, noisy_level=0.2, num_hidden=64, num_proj_hidden=64, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=40, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 88648])
add edge: torch.Size([2, 106228])
remove edge: torch.Size([2, 71144])
updated graph: torch.Size([2, 88724])
=== Noisy graph ===
/home/project-robust-contrastive/Robust-Contrastive-Learning/model.py:227: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 1025.728271484375 = 1.094292402267456 + 100.0 * 10.24634075164795
Epoch 0, val loss: 1.0941431522369385
Epoch 10, training loss: 987.3038330078125 = 1.093890905380249 + 100.0 * 9.862099647521973
Epoch 10, val loss: 1.0937365293502808
Epoch 20, training loss: 968.3220825195312 = 1.093580961227417 + 100.0 * 9.672285079956055
Epoch 20, val loss: 1.0934290885925293
Epoch 30, training loss: 954.8086547851562 = 1.0932552814483643 + 100.0 * 9.537154197692871
Epoch 30, val loss: 1.0931113958358765
Epoch 40, training loss: 944.039306640625 = 1.0929282903671265 + 100.0 * 9.429463386535645
Epoch 40, val loss: 1.092787265777588
Epoch 50, training loss: 935.2338256835938 = 1.092611312866211 + 100.0 * 9.341412544250488
Epoch 50, val loss: 1.0924731492996216
Epoch 60, training loss: 927.8231811523438 = 1.0922877788543701 + 100.0 * 9.267309188842773
Epoch 60, val loss: 1.092151165008545
Epoch 70, training loss: 921.5543212890625 = 1.0919491052627563 + 100.0 * 9.20462417602539
Epoch 70, val loss: 1.0918171405792236
Epoch 80, training loss: 916.2149658203125 = 1.091620683670044 + 100.0 * 9.151233673095703
Epoch 80, val loss: 1.0914944410324097
Epoch 90, training loss: 911.5582885742188 = 1.0912836790084839 + 100.0 * 9.104669570922852
Epoch 90, val loss: 1.091159701347351
Epoch 100, training loss: 907.485595703125 = 1.090951919555664 + 100.0 * 9.063946723937988
Epoch 100, val loss: 1.0908334255218506
Epoch 110, training loss: 903.9395751953125 = 1.090599775314331 + 100.0 * 9.02849006652832
Epoch 110, val loss: 1.0904868841171265
Epoch 120, training loss: 900.890869140625 = 1.0902553796768188 + 100.0 * 8.998005867004395
Epoch 120, val loss: 1.0901436805725098
Epoch 130, training loss: 898.2620849609375 = 1.0898979902267456 + 100.0 * 8.971721649169922
Epoch 130, val loss: 1.0897912979125977
Epoch 140, training loss: 895.9593505859375 = 1.0895313024520874 + 100.0 * 8.948698043823242
Epoch 140, val loss: 1.0894283056259155
Epoch 150, training loss: 893.9945068359375 = 1.0891454219818115 + 100.0 * 8.92905330657959
Epoch 150, val loss: 1.0890485048294067
Epoch 160, training loss: 892.79443359375 = 1.088787317276001 + 100.0 * 8.9170560836792
Epoch 160, val loss: 1.0886927843093872
Epoch 170, training loss: 890.91259765625 = 1.0883837938308716 + 100.0 * 8.898241996765137
Epoch 170, val loss: 1.0882976055145264
Epoch 180, training loss: 889.4296264648438 = 1.08800208568573 + 100.0 * 8.883416175842285
Epoch 180, val loss: 1.0879184007644653
Epoch 190, training loss: 888.210205078125 = 1.087595820426941 + 100.0 * 8.87122631072998
Epoch 190, val loss: 1.0875240564346313
Epoch 200, training loss: 887.202880859375 = 1.0871872901916504 + 100.0 * 8.861157417297363
Epoch 200, val loss: 1.08712637424469
Epoch 210, training loss: 886.1177978515625 = 1.0867714881896973 + 100.0 * 8.850310325622559
Epoch 210, val loss: 1.0867153406143188
Epoch 220, training loss: 885.5817260742188 = 1.0863561630249023 + 100.0 * 8.844953536987305
Epoch 220, val loss: 1.0863077640533447
Epoch 230, training loss: 885.071533203125 = 1.0859386920928955 + 100.0 * 8.839856147766113
Epoch 230, val loss: 1.0858957767486572
Epoch 240, training loss: 884.4493408203125 = 1.0854822397232056 + 100.0 * 8.833638191223145
Epoch 240, val loss: 1.0854369401931763
Epoch 250, training loss: 883.9658813476562 = 1.085071325302124 + 100.0 * 8.828807830810547
Epoch 250, val loss: 1.0850497484207153
Epoch 260, training loss: 883.4883422851562 = 1.0846003293991089 + 100.0 * 8.824037551879883
Epoch 260, val loss: 1.0845842361450195
Epoch 270, training loss: 882.9552001953125 = 1.0841788053512573 + 100.0 * 8.818710327148438
Epoch 270, val loss: 1.0841740369796753
Epoch 280, training loss: 882.3518676757812 = 1.0837241411209106 + 100.0 * 8.812681198120117
Epoch 280, val loss: 1.083722472190857
Epoch 290, training loss: 881.9534912109375 = 1.0832699537277222 + 100.0 * 8.80870246887207
Epoch 290, val loss: 1.083275556564331
Epoch 300, training loss: 881.79736328125 = 1.0828015804290771 + 100.0 * 8.807145118713379
Epoch 300, val loss: 1.0828173160552979
Epoch 310, training loss: 881.5478515625 = 1.0823367834091187 + 100.0 * 8.804655075073242
Epoch 310, val loss: 1.08235502243042
Epoch 320, training loss: 881.3190307617188 = 1.0818469524383545 + 100.0 * 8.802371978759766
Epoch 320, val loss: 1.0818760395050049
Epoch 330, training loss: 881.2225952148438 = 1.081344485282898 + 100.0 * 8.801412582397461
Epoch 330, val loss: 1.0813785791397095
Epoch 340, training loss: 881.1183471679688 = 1.080844521522522 + 100.0 * 8.800374984741211
Epoch 340, val loss: 1.0809102058410645
Epoch 350, training loss: 880.7431030273438 = 1.0803296566009521 + 100.0 * 8.79662799835205
Epoch 350, val loss: 1.080399751663208
Epoch 360, training loss: 880.591796875 = 1.0797951221466064 + 100.0 * 8.795120239257812
Epoch 360, val loss: 1.079875111579895
Epoch 370, training loss: 880.6367797851562 = 1.0792709589004517 + 100.0 * 8.795575141906738
Epoch 370, val loss: 1.0793578624725342
Epoch 380, training loss: 881.0885620117188 = 1.0787277221679688 + 100.0 * 8.800098419189453
Epoch 380, val loss: 1.0788230895996094
Epoch 390, training loss: 880.9404907226562 = 1.0781625509262085 + 100.0 * 8.798623085021973
Epoch 390, val loss: 1.0782713890075684
Epoch 400, training loss: 880.8349609375 = 1.0775922536849976 + 100.0 * 8.797574043273926
Epoch 400, val loss: 1.0777201652526855
Epoch 410, training loss: 880.9078369140625 = 1.0770037174224854 + 100.0 * 8.798308372497559
Epoch 410, val loss: 1.077134370803833
Epoch 420, training loss: 880.7998657226562 = 1.076427936553955 + 100.0 * 8.797234535217285
Epoch 420, val loss: 1.0765641927719116
Epoch 430, training loss: 881.5286254882812 = 1.075830340385437 + 100.0 * 8.80452823638916
Epoch 430, val loss: 1.0759788751602173
Epoch 440, training loss: 881.2565307617188 = 1.0752235651016235 + 100.0 * 8.801813125610352
Epoch 440, val loss: 1.0753912925720215
Epoch 450, training loss: 881.1837768554688 = 1.074596643447876 + 100.0 * 8.801092147827148
Epoch 450, val loss: 1.0747809410095215
Epoch 460, training loss: 881.5822143554688 = 1.0739555358886719 + 100.0 * 8.805082321166992
Epoch 460, val loss: 1.0741463899612427
Epoch 470, training loss: 881.6165161132812 = 1.0733003616333008 + 100.0 * 8.805432319641113
Epoch 470, val loss: 1.0735033750534058
Epoch 480, training loss: 881.5897216796875 = 1.0726314783096313 + 100.0 * 8.805171012878418
Epoch 480, val loss: 1.0728380680084229
Epoch 490, training loss: 881.552978515625 = 1.07191002368927 + 100.0 * 8.804810523986816
Epoch 490, val loss: 1.0721378326416016
Epoch 500, training loss: 881.4613647460938 = 1.0712405443191528 + 100.0 * 8.803901672363281
Epoch 500, val loss: 1.0714702606201172
Epoch 510, training loss: 881.619384765625 = 1.0705361366271973 + 100.0 * 8.805488586425781
Epoch 510, val loss: 1.0707687139511108
Epoch 520, training loss: 881.7159423828125 = 1.0697965621948242 + 100.0 * 8.806461334228516
Epoch 520, val loss: 1.0700485706329346
Epoch 530, training loss: 882.3629760742188 = 1.0690745115280151 + 100.0 * 8.812938690185547
Epoch 530, val loss: 1.0693364143371582
Epoch 540, training loss: 882.5136108398438 = 1.0683180093765259 + 100.0 * 8.814453125
Epoch 540, val loss: 1.0685969591140747
Epoch 550, training loss: 882.7609252929688 = 1.067543387413025 + 100.0 * 8.816933631896973
Epoch 550, val loss: 1.0678383111953735
Epoch 560, training loss: 883.0647583007812 = 1.0667729377746582 + 100.0 * 8.819979667663574
Epoch 560, val loss: 1.0670803785324097
Epoch 570, training loss: 883.1570434570312 = 1.0660037994384766 + 100.0 * 8.820910453796387
Epoch 570, val loss: 1.0663268566131592
Epoch 580, training loss: 883.2958374023438 = 1.0652225017547607 + 100.0 * 8.822305679321289
Epoch 580, val loss: 1.065566897392273
Epoch 590, training loss: 883.8004760742188 = 1.064462423324585 + 100.0 * 8.827360153198242
Epoch 590, val loss: 1.0648154020309448
Epoch 600, training loss: 883.760498046875 = 1.0635968446731567 + 100.0 * 8.826969146728516
Epoch 600, val loss: 1.0639609098434448
Epoch 610, training loss: 884.1776733398438 = 1.062908411026001 + 100.0 * 8.831147193908691
Epoch 610, val loss: 1.063294768333435
Epoch 620, training loss: 884.1011352539062 = 1.0620980262756348 + 100.0 * 8.830390930175781
Epoch 620, val loss: 1.062500238418579
Epoch 630, training loss: 884.2440185546875 = 1.0613030195236206 + 100.0 * 8.831827163696289
Epoch 630, val loss: 1.0617094039916992
Epoch 640, training loss: 884.9000244140625 = 1.060495138168335 + 100.0 * 8.838395118713379
Epoch 640, val loss: 1.0609439611434937
Epoch 650, training loss: 883.8627319335938 = 1.0596797466278076 + 100.0 * 8.828030586242676
Epoch 650, val loss: 1.0601325035095215
Epoch 660, training loss: 883.8369750976562 = 1.0589017868041992 + 100.0 * 8.827780723571777
Epoch 660, val loss: 1.0593581199645996
Epoch 670, training loss: 884.4214477539062 = 1.058120608329773 + 100.0 * 8.833633422851562
Epoch 670, val loss: 1.0585906505584717
Epoch 680, training loss: 885.0657958984375 = 1.0573042631149292 + 100.0 * 8.84008502960205
Epoch 680, val loss: 1.0577611923217773
Epoch 690, training loss: 884.513916015625 = 1.0564916133880615 + 100.0 * 8.834573745727539
Epoch 690, val loss: 1.0570133924484253
Epoch 700, training loss: 884.260498046875 = 1.0556894540786743 + 100.0 * 8.832048416137695
Epoch 700, val loss: 1.0561926364898682
Epoch 710, training loss: 884.4595336914062 = 1.0548810958862305 + 100.0 * 8.834046363830566
Epoch 710, val loss: 1.0554014444351196
Epoch 720, training loss: 885.1004638671875 = 1.0540999174118042 + 100.0 * 8.840463638305664
Epoch 720, val loss: 1.0546339750289917
Epoch 730, training loss: 885.46044921875 = 1.0532788038253784 + 100.0 * 8.844071388244629
Epoch 730, val loss: 1.0538229942321777
Epoch 740, training loss: 885.2736206054688 = 1.0524507761001587 + 100.0 * 8.842211723327637
Epoch 740, val loss: 1.053009033203125
Epoch 750, training loss: 885.0126953125 = 1.051512360572815 + 100.0 * 8.839612007141113
Epoch 750, val loss: 1.0520901679992676
Epoch 760, training loss: 886.7353515625 = 1.0506694316864014 + 100.0 * 8.856846809387207
Epoch 760, val loss: 1.051255464553833
Epoch 770, training loss: 887.9999389648438 = 1.0500065088272095 + 100.0 * 8.869499206542969
Epoch 770, val loss: 1.0506136417388916
Epoch 780, training loss: 886.1314697265625 = 1.0491514205932617 + 100.0 * 8.850823402404785
Epoch 780, val loss: 1.0497715473175049
Epoch 790, training loss: 886.1405029296875 = 1.0483380556106567 + 100.0 * 8.850921630859375
Epoch 790, val loss: 1.048959493637085
Epoch 800, training loss: 886.7830200195312 = 1.047493815422058 + 100.0 * 8.857355117797852
Epoch 800, val loss: 1.0481219291687012
Epoch 810, training loss: 887.1685791015625 = 1.0466574430465698 + 100.0 * 8.86121940612793
Epoch 810, val loss: 1.047307014465332
Epoch 820, training loss: 887.511474609375 = 1.0458122491836548 + 100.0 * 8.864656448364258
Epoch 820, val loss: 1.046469807624817
Epoch 830, training loss: 887.1774291992188 = 1.0449411869049072 + 100.0 * 8.86132526397705
Epoch 830, val loss: 1.045601487159729
Epoch 840, training loss: 887.6038208007812 = 1.0441207885742188 + 100.0 * 8.865596771240234
Epoch 840, val loss: 1.044791579246521
Epoch 850, training loss: 888.1395263671875 = 1.0433067083358765 + 100.0 * 8.870962142944336
Epoch 850, val loss: 1.0439949035644531
Epoch 860, training loss: 888.6513061523438 = 1.0424765348434448 + 100.0 * 8.87608814239502
Epoch 860, val loss: 1.0431767702102661
Epoch 870, training loss: 888.572509765625 = 1.0416189432144165 + 100.0 * 8.875308990478516
Epoch 870, val loss: 1.0423412322998047
Epoch 880, training loss: 888.9373779296875 = 1.040786862373352 + 100.0 * 8.878966331481934
Epoch 880, val loss: 1.0415127277374268
Epoch 890, training loss: 889.1448974609375 = 1.0399237871170044 + 100.0 * 8.881050109863281
Epoch 890, val loss: 1.0406633615493774
Epoch 900, training loss: 889.3878173828125 = 1.039075493812561 + 100.0 * 8.883487701416016
Epoch 900, val loss: 1.0398476123809814
Epoch 910, training loss: 889.705322265625 = 1.0382153987884521 + 100.0 * 8.88667106628418
Epoch 910, val loss: 1.039007544517517
Epoch 920, training loss: 889.831787109375 = 1.0373666286468506 + 100.0 * 8.887944221496582
Epoch 920, val loss: 1.0381439924240112
Epoch 930, training loss: 890.0116577148438 = 1.0364969968795776 + 100.0 * 8.889751434326172
Epoch 930, val loss: 1.037327527999878
Epoch 940, training loss: 889.7295532226562 = 1.03562593460083 + 100.0 * 8.88693904876709
Epoch 940, val loss: 1.0364649295806885
Epoch 950, training loss: 890.0096435546875 = 1.0347719192504883 + 100.0 * 8.889748573303223
Epoch 950, val loss: 1.035646915435791
Epoch 960, training loss: 890.68896484375 = 1.0339328050613403 + 100.0 * 8.896550178527832
Epoch 960, val loss: 1.0348215103149414
Epoch 970, training loss: 890.70947265625 = 1.0330785512924194 + 100.0 * 8.896763801574707
Epoch 970, val loss: 1.033974528312683
Epoch 980, training loss: 890.837158203125 = 1.0322388410568237 + 100.0 * 8.898049354553223
Epoch 980, val loss: 1.033129334449768
Epoch 990, training loss: 890.8572998046875 = 1.0313334465026855 + 100.0 * 8.898260116577148
Epoch 990, val loss: 1.0322670936584473
Epoch 1000, training loss: 891.3703002929688 = 1.030492901802063 + 100.0 * 8.903397560119629
Epoch 1000, val loss: 1.031434178352356
Epoch 1010, training loss: 891.1438598632812 = 1.0295603275299072 + 100.0 * 8.901143074035645
Epoch 1010, val loss: 1.0305286645889282
Epoch 1020, training loss: 890.91650390625 = 1.0286825895309448 + 100.0 * 8.89887809753418
Epoch 1020, val loss: 1.029662013053894
Epoch 1030, training loss: 891.4214477539062 = 1.027816653251648 + 100.0 * 8.903936386108398
Epoch 1030, val loss: 1.0287986993789673
Epoch 1040, training loss: 891.8408813476562 = 1.0269688367843628 + 100.0 * 8.9081392288208
Epoch 1040, val loss: 1.0279710292816162
Epoch 1050, training loss: 892.0842895507812 = 1.026059627532959 + 100.0 * 8.910582542419434
Epoch 1050, val loss: 1.027074933052063
Epoch 1060, training loss: 892.0916137695312 = 1.025141954421997 + 100.0 * 8.910664558410645
Epoch 1060, val loss: 1.02617609500885
Epoch 1070, training loss: 891.9898681640625 = 1.024253487586975 + 100.0 * 8.909656524658203
Epoch 1070, val loss: 1.0252857208251953
Epoch 1080, training loss: 892.2299194335938 = 1.0233628749847412 + 100.0 * 8.912065505981445
Epoch 1080, val loss: 1.0244321823120117
Epoch 1090, training loss: 892.525390625 = 1.0224946737289429 + 100.0 * 8.91502857208252
Epoch 1090, val loss: 1.0235792398452759
Epoch 1100, training loss: 892.40966796875 = 1.021551251411438 + 100.0 * 8.913881301879883
Epoch 1100, val loss: 1.0226495265960693
Epoch 1110, training loss: 892.536376953125 = 1.0206469297409058 + 100.0 * 8.915157318115234
Epoch 1110, val loss: 1.0217797756195068
Epoch 1120, training loss: 893.0770874023438 = 1.0197687149047852 + 100.0 * 8.920573234558105
Epoch 1120, val loss: 1.0208996534347534
Epoch 1130, training loss: 893.1316528320312 = 1.0188263654708862 + 100.0 * 8.921128273010254
Epoch 1130, val loss: 1.0199847221374512
Epoch 1140, training loss: 892.5302124023438 = 1.017867088317871 + 100.0 * 8.915122985839844
Epoch 1140, val loss: 1.0190106630325317
Epoch 1150, training loss: 892.767578125 = 1.0169404745101929 + 100.0 * 8.917506217956543
Epoch 1150, val loss: 1.0181081295013428
Epoch 1160, training loss: 893.0149536132812 = 1.0159931182861328 + 100.0 * 8.919989585876465
Epoch 1160, val loss: 1.0172008275985718
Epoch 1170, training loss: 893.8760986328125 = 1.0151063203811646 + 100.0 * 8.928609848022461
Epoch 1170, val loss: 1.0163419246673584
Epoch 1180, training loss: 893.2555541992188 = 1.014132022857666 + 100.0 * 8.92241382598877
Epoch 1180, val loss: 1.0153900384902954
Epoch 1190, training loss: 893.7591552734375 = 1.0131911039352417 + 100.0 * 8.927459716796875
Epoch 1190, val loss: 1.0144561529159546
Epoch 1200, training loss: 894.1229248046875 = 1.0122450590133667 + 100.0 * 8.931106567382812
Epoch 1200, val loss: 1.0135375261306763
Epoch 1210, training loss: 893.8871459960938 = 1.0112773180007935 + 100.0 * 8.92875862121582
Epoch 1210, val loss: 1.012555480003357
Epoch 1220, training loss: 894.212646484375 = 1.010337233543396 + 100.0 * 8.932023048400879
Epoch 1220, val loss: 1.0116655826568604
Epoch 1230, training loss: 894.9653930664062 = 1.0094103813171387 + 100.0 * 8.939559936523438
Epoch 1230, val loss: 1.0107394456863403
Epoch 1240, training loss: 894.8792724609375 = 1.0084511041641235 + 100.0 * 8.938708305358887
Epoch 1240, val loss: 1.0097821950912476
Epoch 1250, training loss: 894.9346923828125 = 1.0074959993362427 + 100.0 * 8.939271926879883
Epoch 1250, val loss: 1.0088516473770142
Epoch 1260, training loss: 895.2467651367188 = 1.0065617561340332 + 100.0 * 8.942401885986328
Epoch 1260, val loss: 1.0079351663589478
Epoch 1270, training loss: 894.78857421875 = 1.0055489540100098 + 100.0 * 8.937829971313477
Epoch 1270, val loss: 1.0069324970245361
Epoch 1280, training loss: 895.3446655273438 = 1.0046167373657227 + 100.0 * 8.943400382995605
Epoch 1280, val loss: 1.0060490369796753
Epoch 1290, training loss: 895.4268798828125 = 1.0036863088607788 + 100.0 * 8.944231986999512
Epoch 1290, val loss: 1.0051161050796509
Epoch 1300, training loss: 895.5596313476562 = 1.002736210823059 + 100.0 * 8.945569038391113
Epoch 1300, val loss: 1.004186749458313
Epoch 1310, training loss: 895.9981689453125 = 1.001810908317566 + 100.0 * 8.949963569641113
Epoch 1310, val loss: 1.0032600164413452
Epoch 1320, training loss: 896.5245971679688 = 1.0008665323257446 + 100.0 * 8.95523738861084
Epoch 1320, val loss: 1.002317190170288
Epoch 1330, training loss: 896.4151611328125 = 0.9998844265937805 + 100.0 * 8.954153060913086
Epoch 1330, val loss: 1.0013757944107056
Epoch 1340, training loss: 896.7739868164062 = 0.9989320039749146 + 100.0 * 8.95775032043457
Epoch 1340, val loss: 1.0004340410232544
Epoch 1350, training loss: 896.8562622070312 = 0.997970461845398 + 100.0 * 8.958582878112793
Epoch 1350, val loss: 0.9994678497314453
Epoch 1360, training loss: 897.3026123046875 = 0.9970472455024719 + 100.0 * 8.963055610656738
Epoch 1360, val loss: 0.9985504150390625
Epoch 1370, training loss: 897.2598876953125 = 0.9960925579071045 + 100.0 * 8.962637901306152
Epoch 1370, val loss: 0.9976381659507751
Epoch 1380, training loss: 897.3816528320312 = 0.9951443076133728 + 100.0 * 8.963865280151367
Epoch 1380, val loss: 0.9966946840286255
Epoch 1390, training loss: 895.6160278320312 = 0.9940863251686096 + 100.0 * 8.946219444274902
Epoch 1390, val loss: 0.9956433773040771
Epoch 1400, training loss: 896.4743041992188 = 0.9931206703186035 + 100.0 * 8.954812049865723
Epoch 1400, val loss: 0.9947249889373779
Epoch 1410, training loss: 897.12939453125 = 0.9922121167182922 + 100.0 * 8.961372375488281
Epoch 1410, val loss: 0.993824303150177
Epoch 1420, training loss: 897.9937744140625 = 0.9913038015365601 + 100.0 * 8.970024108886719
Epoch 1420, val loss: 0.992939293384552
Epoch 1430, training loss: 898.131103515625 = 0.9903244376182556 + 100.0 * 8.971407890319824
Epoch 1430, val loss: 0.9919478893280029
Epoch 1440, training loss: 898.2498779296875 = 0.9893640279769897 + 100.0 * 8.972604751586914
Epoch 1440, val loss: 0.990997314453125
Epoch 1450, training loss: 898.327392578125 = 0.9884117245674133 + 100.0 * 8.973389625549316
Epoch 1450, val loss: 0.9900870323181152
Epoch 1460, training loss: 898.753662109375 = 0.9874561429023743 + 100.0 * 8.977662086486816
Epoch 1460, val loss: 0.9891371726989746
Epoch 1470, training loss: 899.0974731445312 = 0.9865010380744934 + 100.0 * 8.981109619140625
Epoch 1470, val loss: 0.9881956577301025
Epoch 1480, training loss: 899.0181274414062 = 0.985518753528595 + 100.0 * 8.980325698852539
Epoch 1480, val loss: 0.9872541427612305
Epoch 1490, training loss: 899.1814575195312 = 0.9845730662345886 + 100.0 * 8.981968879699707
Epoch 1490, val loss: 0.9863283634185791
Epoch 1500, training loss: 898.9697875976562 = 0.9835653901100159 + 100.0 * 8.979862213134766
Epoch 1500, val loss: 0.985348105430603
Epoch 1510, training loss: 899.140625 = 0.9825489521026611 + 100.0 * 8.98158073425293
Epoch 1510, val loss: 0.984320342540741
Epoch 1520, training loss: 899.6087646484375 = 0.9815967082977295 + 100.0 * 8.986271858215332
Epoch 1520, val loss: 0.9833746552467346
Epoch 1530, training loss: 899.7855224609375 = 0.9806138277053833 + 100.0 * 8.988049507141113
Epoch 1530, val loss: 0.982413113117218
Epoch 1540, training loss: 899.5393676757812 = 0.9796257019042969 + 100.0 * 8.985597610473633
Epoch 1540, val loss: 0.9814246296882629
Epoch 1550, training loss: 900.1654663085938 = 0.97866290807724 + 100.0 * 8.991868019104004
Epoch 1550, val loss: 0.9804964065551758
Epoch 1560, training loss: 900.0675659179688 = 0.9776725769042969 + 100.0 * 8.990899085998535
Epoch 1560, val loss: 0.9795364737510681
Epoch 1570, training loss: 900.1144409179688 = 0.9767159223556519 + 100.0 * 8.991376876831055
Epoch 1570, val loss: 0.9785914421081543
Epoch 1580, training loss: 900.6117553710938 = 0.975739061832428 + 100.0 * 8.996359825134277
Epoch 1580, val loss: 0.9776358008384705
Epoch 1590, training loss: 900.46826171875 = 0.9747284650802612 + 100.0 * 8.994935035705566
Epoch 1590, val loss: 0.9766375422477722
Epoch 1600, training loss: 899.9747314453125 = 0.9736800193786621 + 100.0 * 8.990010261535645
Epoch 1600, val loss: 0.9755874276161194
Epoch 1610, training loss: 900.3554077148438 = 0.972693145275116 + 100.0 * 8.993826866149902
Epoch 1610, val loss: 0.9746298789978027
Epoch 1620, training loss: 900.9425659179688 = 0.9717531800270081 + 100.0 * 8.99970817565918
Epoch 1620, val loss: 0.9736940264701843
Epoch 1630, training loss: 899.7784423828125 = 0.970674991607666 + 100.0 * 8.988077163696289
Epoch 1630, val loss: 0.972630500793457
Epoch 1640, training loss: 900.3890991210938 = 0.9697678089141846 + 100.0 * 8.994193077087402
Epoch 1640, val loss: 0.9717293977737427
Epoch 1650, training loss: 901.1637573242188 = 0.9688089489936829 + 100.0 * 9.001949310302734
Epoch 1650, val loss: 0.9707891941070557
Epoch 1660, training loss: 901.9784545898438 = 0.9678479433059692 + 100.0 * 9.010106086730957
Epoch 1660, val loss: 0.9698458313941956
Epoch 1670, training loss: 901.5707397460938 = 0.966835081577301 + 100.0 * 9.006038665771484
Epoch 1670, val loss: 0.968841552734375
Epoch 1680, training loss: 902.3056640625 = 0.965843141078949 + 100.0 * 9.013398170471191
Epoch 1680, val loss: 0.9678822159767151
Epoch 1690, training loss: 902.6430053710938 = 0.9648654460906982 + 100.0 * 9.0167818069458
Epoch 1690, val loss: 0.9668826460838318
Epoch 1700, training loss: 902.6739501953125 = 0.9638290405273438 + 100.0 * 9.017101287841797
Epoch 1700, val loss: 0.9658775329589844
Epoch 1710, training loss: 903.0042724609375 = 0.9628555178642273 + 100.0 * 9.020414352416992
Epoch 1710, val loss: 0.9649296402931213
Epoch 1720, training loss: 903.087890625 = 0.9618762135505676 + 100.0 * 9.021260261535645
Epoch 1720, val loss: 0.9639375805854797
Epoch 1730, training loss: 902.2706298828125 = 0.9608495235443115 + 100.0 * 9.013097763061523
Epoch 1730, val loss: 0.9628667235374451
Epoch 1740, training loss: 901.3280029296875 = 0.9597048759460449 + 100.0 * 9.003683090209961
Epoch 1740, val loss: 0.9617707133293152
Epoch 1750, training loss: 901.939697265625 = 0.958696186542511 + 100.0 * 9.009810447692871
Epoch 1750, val loss: 0.9608249664306641
Epoch 1760, training loss: 902.3396606445312 = 0.957775354385376 + 100.0 * 9.013818740844727
Epoch 1760, val loss: 0.9599257707595825
Epoch 1770, training loss: 902.3748779296875 = 0.9568274021148682 + 100.0 * 9.014180183410645
Epoch 1770, val loss: 0.958989679813385
Epoch 1780, training loss: 903.1932373046875 = 0.9559117555618286 + 100.0 * 9.02237319946289
Epoch 1780, val loss: 0.9580776691436768
Epoch 1790, training loss: 902.8673706054688 = 0.9549111723899841 + 100.0 * 9.019124984741211
Epoch 1790, val loss: 0.9570856094360352
Epoch 1800, training loss: 903.494384765625 = 0.9539259076118469 + 100.0 * 9.025404930114746
Epoch 1800, val loss: 0.9561195969581604
Epoch 1810, training loss: 904.0082397460938 = 0.9529450535774231 + 100.0 * 9.030552864074707
Epoch 1810, val loss: 0.9551609754562378
Epoch 1820, training loss: 904.0611572265625 = 0.9519487023353577 + 100.0 * 9.031091690063477
Epoch 1820, val loss: 0.954169511795044
Epoch 1830, training loss: 904.2659912109375 = 0.9509468078613281 + 100.0 * 9.033150672912598
Epoch 1830, val loss: 0.953167736530304
Epoch 1840, training loss: 904.5430908203125 = 0.9499285221099854 + 100.0 * 9.035931587219238
Epoch 1840, val loss: 0.9521589875221252
Epoch 1850, training loss: 904.6498413085938 = 0.9489202499389648 + 100.0 * 9.037009239196777
Epoch 1850, val loss: 0.9511786699295044
Epoch 1860, training loss: 904.5462646484375 = 0.9479064345359802 + 100.0 * 9.03598403930664
Epoch 1860, val loss: 0.9501878619194031
Epoch 1870, training loss: 904.96240234375 = 0.946946918964386 + 100.0 * 9.040154457092285
Epoch 1870, val loss: 0.9492090940475464
Epoch 1880, training loss: 904.9839477539062 = 0.9459282755851746 + 100.0 * 9.040380477905273
Epoch 1880, val loss: 0.9482256770133972
Epoch 1890, training loss: 904.91455078125 = 0.9449309706687927 + 100.0 * 9.039695739746094
Epoch 1890, val loss: 0.9472389817237854
Epoch 1900, training loss: 905.2525024414062 = 0.9439477324485779 + 100.0 * 9.043085098266602
Epoch 1900, val loss: 0.9462491869926453
Epoch 1910, training loss: 905.561279296875 = 0.9429363012313843 + 100.0 * 9.046183586120605
Epoch 1910, val loss: 0.9452639222145081
Epoch 1920, training loss: 905.5910034179688 = 0.9419576525688171 + 100.0 * 9.046490669250488
Epoch 1920, val loss: 0.9442920684814453
Epoch 1930, training loss: 905.7877197265625 = 0.9409725666046143 + 100.0 * 9.048467636108398
Epoch 1930, val loss: 0.9433051347732544
Epoch 1940, training loss: 905.66845703125 = 0.9399489164352417 + 100.0 * 9.047285079956055
Epoch 1940, val loss: 0.9423426985740662
Epoch 1950, training loss: 904.9530029296875 = 0.9389636516571045 + 100.0 * 9.040140151977539
Epoch 1950, val loss: 0.9413459897041321
Epoch 1960, training loss: 904.9271240234375 = 0.9379575848579407 + 100.0 * 9.039892196655273
Epoch 1960, val loss: 0.9403179883956909
Epoch 1970, training loss: 905.4208984375 = 0.9369818568229675 + 100.0 * 9.044838905334473
Epoch 1970, val loss: 0.9393772482872009
Epoch 1980, training loss: 905.6469116210938 = 0.9360038638114929 + 100.0 * 9.047109603881836
Epoch 1980, val loss: 0.938433051109314
Epoch 1990, training loss: 906.1069946289062 = 0.9350206255912781 + 100.0 * 9.051719665527344
Epoch 1990, val loss: 0.9374567270278931
Epoch 2000, training loss: 906.1436767578125 = 0.9340122938156128 + 100.0 * 9.052096366882324
Epoch 2000, val loss: 0.9364611506462097
Epoch 2010, training loss: 906.5606689453125 = 0.9330289363861084 + 100.0 * 9.056276321411133
Epoch 2010, val loss: 0.93550044298172
Epoch 2020, training loss: 905.3847045898438 = 0.9319883584976196 + 100.0 * 9.044527053833008
Epoch 2020, val loss: 0.9344534277915955
Epoch 2030, training loss: 905.2173461914062 = 0.9310272932052612 + 100.0 * 9.042862892150879
Epoch 2030, val loss: 0.9335244297981262
Epoch 2040, training loss: 905.4163208007812 = 0.9300472140312195 + 100.0 * 9.044862747192383
Epoch 2040, val loss: 0.9325650334358215
Epoch 2050, training loss: 906.1250610351562 = 0.9290608167648315 + 100.0 * 9.051959991455078
Epoch 2050, val loss: 0.9316003918647766
Epoch 2060, training loss: 906.7000122070312 = 0.9280843734741211 + 100.0 * 9.057719230651855
Epoch 2060, val loss: 0.93064284324646
Epoch 2070, training loss: 906.9814453125 = 0.9271054863929749 + 100.0 * 9.060543060302734
Epoch 2070, val loss: 0.9296779036521912
Epoch 2080, training loss: 907.1265258789062 = 0.9261045455932617 + 100.0 * 9.062004089355469
Epoch 2080, val loss: 0.9287081360816956
Epoch 2090, training loss: 907.4266967773438 = 0.9251115322113037 + 100.0 * 9.06501579284668
Epoch 2090, val loss: 0.9277483820915222
Epoch 2100, training loss: 907.3301391601562 = 0.9241340756416321 + 100.0 * 9.06406021118164
Epoch 2100, val loss: 0.9267666935920715
Epoch 2110, training loss: 907.4981689453125 = 0.9231340885162354 + 100.0 * 9.065750122070312
Epoch 2110, val loss: 0.9257916808128357
Epoch 2120, training loss: 907.832763671875 = 0.922153651714325 + 100.0 * 9.069106101989746
Epoch 2120, val loss: 0.9248354434967041
Epoch 2130, training loss: 908.0822143554688 = 0.9211705327033997 + 100.0 * 9.071610450744629
Epoch 2130, val loss: 0.9238948822021484
Epoch 2140, training loss: 907.8253173828125 = 0.9201682209968567 + 100.0 * 9.069051742553711
Epoch 2140, val loss: 0.9228886365890503
Epoch 2150, training loss: 908.3948364257812 = 0.9192079901695251 + 100.0 * 9.074756622314453
Epoch 2150, val loss: 0.9219691753387451
Epoch 2160, training loss: 908.5538330078125 = 0.9182170629501343 + 100.0 * 9.076355934143066
Epoch 2160, val loss: 0.9209892749786377
Epoch 2170, training loss: 908.1622314453125 = 0.9172220826148987 + 100.0 * 9.072449684143066
Epoch 2170, val loss: 0.9200040102005005
Epoch 2180, training loss: 908.232421875 = 0.9162266850471497 + 100.0 * 9.073162078857422
Epoch 2180, val loss: 0.9190492630004883
Epoch 2190, training loss: 908.94140625 = 0.9152752161026001 + 100.0 * 9.08026123046875
Epoch 2190, val loss: 0.9181091785430908
Epoch 2200, training loss: 908.7230834960938 = 0.914292573928833 + 100.0 * 9.07808780670166
Epoch 2200, val loss: 0.9171409010887146
Epoch 2210, training loss: 909.02001953125 = 0.9133478403091431 + 100.0 * 9.081067085266113
Epoch 2210, val loss: 0.9162154793739319
Epoch 2220, training loss: 909.3516845703125 = 0.9123883843421936 + 100.0 * 9.084392547607422
Epoch 2220, val loss: 0.9152665138244629
Epoch 2230, training loss: 909.2150268554688 = 0.9113457202911377 + 100.0 * 9.083037376403809
Epoch 2230, val loss: 0.9142470359802246
Epoch 2240, training loss: 907.1989135742188 = 0.910456120967865 + 100.0 * 9.062884330749512
Epoch 2240, val loss: 0.9133011102676392
Epoch 2250, training loss: 906.5425415039062 = 0.9093780517578125 + 100.0 * 9.056331634521484
Epoch 2250, val loss: 0.9123230576515198
Epoch 2260, training loss: 906.7097778320312 = 0.9084948301315308 + 100.0 * 9.058012962341309
Epoch 2260, val loss: 0.9114659428596497
Epoch 2270, training loss: 907.6182861328125 = 0.9076306819915771 + 100.0 * 9.067106246948242
Epoch 2270, val loss: 0.9106236100196838
Epoch 2280, training loss: 908.5052490234375 = 0.9067441821098328 + 100.0 * 9.075984954833984
Epoch 2280, val loss: 0.909765899181366
Epoch 2290, training loss: 908.3226318359375 = 0.9058425426483154 + 100.0 * 9.07416820526123
Epoch 2290, val loss: 0.9088460206985474
Epoch 2300, training loss: 908.382080078125 = 0.9049115777015686 + 100.0 * 9.074771881103516
Epoch 2300, val loss: 0.9079716205596924
Epoch 2310, training loss: 909.1615600585938 = 0.9040060639381409 + 100.0 * 9.082575798034668
Epoch 2310, val loss: 0.9070842266082764
Epoch 2320, training loss: 909.5178833007812 = 0.9030668139457703 + 100.0 * 9.086148262023926
Epoch 2320, val loss: 0.9061543941497803
Epoch 2330, training loss: 909.611572265625 = 0.9021458029747009 + 100.0 * 9.0870943069458
Epoch 2330, val loss: 0.9052665829658508
Epoch 2340, training loss: 909.9495849609375 = 0.9012226462364197 + 100.0 * 9.090483665466309
Epoch 2340, val loss: 0.9043872952461243
Epoch 2350, training loss: 910.0731811523438 = 0.9003040790557861 + 100.0 * 9.091728210449219
Epoch 2350, val loss: 0.9034804701805115
Epoch 2360, training loss: 910.0726318359375 = 0.8993662595748901 + 100.0 * 9.0917329788208
Epoch 2360, val loss: 0.9025661945343018
Epoch 2370, training loss: 909.7192993164062 = 0.8983930945396423 + 100.0 * 9.08820915222168
Epoch 2370, val loss: 0.9016032814979553
Epoch 2380, training loss: 910.3443603515625 = 0.8974797129631042 + 100.0 * 9.09446907043457
Epoch 2380, val loss: 0.9007477164268494
Epoch 2390, training loss: 910.7605590820312 = 0.8965952396392822 + 100.0 * 9.098639488220215
Epoch 2390, val loss: 0.899874210357666
Epoch 2400, training loss: 911.0149536132812 = 0.8956923484802246 + 100.0 * 9.101192474365234
Epoch 2400, val loss: 0.8989893794059753
Epoch 2410, training loss: 910.5700073242188 = 0.8947628736495972 + 100.0 * 9.096752166748047
Epoch 2410, val loss: 0.8980602025985718
Epoch 2420, training loss: 910.7745971679688 = 0.8938515782356262 + 100.0 * 9.098807334899902
Epoch 2420, val loss: 0.8971789479255676
Epoch 2430, training loss: 910.9778442382812 = 0.892959713935852 + 100.0 * 9.100849151611328
Epoch 2430, val loss: 0.8963212370872498
Epoch 2440, training loss: 911.0656127929688 = 0.8920626640319824 + 100.0 * 9.10173511505127
Epoch 2440, val loss: 0.8954402804374695
Epoch 2450, training loss: 911.3800659179688 = 0.8911803364753723 + 100.0 * 9.104888916015625
Epoch 2450, val loss: 0.8945850729942322
Epoch 2460, training loss: 911.3642578125 = 0.8902810215950012 + 100.0 * 9.104740142822266
Epoch 2460, val loss: 0.8937056064605713
Epoch 2470, training loss: 911.6475219726562 = 0.889405369758606 + 100.0 * 9.10758113861084
Epoch 2470, val loss: 0.8928539752960205
Epoch 2480, training loss: 912.0167236328125 = 0.8885191679000854 + 100.0 * 9.111282348632812
Epoch 2480, val loss: 0.8919893503189087
Epoch 2490, training loss: 911.4544067382812 = 0.8875966668128967 + 100.0 * 9.105668067932129
Epoch 2490, val loss: 0.8910646438598633
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.6273913043478261
0.8135912482793596
=== training gcn model ===
Epoch 0, training loss: 1019.1428833007812 = 1.0949909687042236 + 100.0 * 10.180479049682617
Epoch 0, val loss: 1.0940712690353394
Epoch 10, training loss: 983.68310546875 = 1.094380497932434 + 100.0 * 9.825887680053711
Epoch 10, val loss: 1.0935004949569702
Epoch 20, training loss: 967.2330932617188 = 1.0938446521759033 + 100.0 * 9.661392211914062
Epoch 20, val loss: 1.0929683446884155
Epoch 30, training loss: 954.7420654296875 = 1.0933889150619507 + 100.0 * 9.536486625671387
Epoch 30, val loss: 1.0925260782241821
Epoch 40, training loss: 944.823486328125 = 1.0931164026260376 + 100.0 * 9.43730354309082
Epoch 40, val loss: 1.092253565788269
Epoch 50, training loss: 936.5702514648438 = 1.0928459167480469 + 100.0 * 9.354774475097656
Epoch 50, val loss: 1.091984748840332
Epoch 60, training loss: 929.6000366210938 = 1.0925719738006592 + 100.0 * 9.285074234008789
Epoch 60, val loss: 1.091709852218628
Epoch 70, training loss: 923.515869140625 = 1.092292070388794 + 100.0 * 9.224235534667969
Epoch 70, val loss: 1.091429591178894
Epoch 80, training loss: 918.2652587890625 = 1.0920041799545288 + 100.0 * 9.171732902526855
Epoch 80, val loss: 1.091140866279602
Epoch 90, training loss: 913.6687622070312 = 1.0917134284973145 + 100.0 * 9.125770568847656
Epoch 90, val loss: 1.0908468961715698
Epoch 100, training loss: 909.583251953125 = 1.0914126634597778 + 100.0 * 9.084918022155762
Epoch 100, val loss: 1.0905463695526123
Epoch 110, training loss: 905.8923950195312 = 1.0911005735397339 + 100.0 * 9.048012733459473
Epoch 110, val loss: 1.090234398841858
Epoch 120, training loss: 902.7633056640625 = 1.0907822847366333 + 100.0 * 9.016725540161133
Epoch 120, val loss: 1.0899133682250977
Epoch 130, training loss: 900.0034790039062 = 1.0904539823532104 + 100.0 * 8.989130020141602
Epoch 130, val loss: 1.0895878076553345
Epoch 140, training loss: 897.5367431640625 = 1.0901238918304443 + 100.0 * 8.964466094970703
Epoch 140, val loss: 1.0892552137374878
Epoch 150, training loss: 895.3397827148438 = 1.0897778272628784 + 100.0 * 8.942500114440918
Epoch 150, val loss: 1.0889091491699219
Epoch 160, training loss: 893.5059814453125 = 1.089422345161438 + 100.0 * 8.924165725708008
Epoch 160, val loss: 1.0885530710220337
Epoch 170, training loss: 891.7710571289062 = 1.0890454053878784 + 100.0 * 8.906820297241211
Epoch 170, val loss: 1.0881770849227905
Epoch 180, training loss: 890.5277099609375 = 1.0886585712432861 + 100.0 * 8.894390106201172
Epoch 180, val loss: 1.087789535522461
Epoch 190, training loss: 889.1818237304688 = 1.088258147239685 + 100.0 * 8.880935668945312
Epoch 190, val loss: 1.087386965751648
Epoch 200, training loss: 888.1258544921875 = 1.0878427028656006 + 100.0 * 8.870380401611328
Epoch 200, val loss: 1.0869643688201904
Epoch 210, training loss: 887.0578002929688 = 1.0874181985855103 + 100.0 * 8.85970401763916
Epoch 210, val loss: 1.0865377187728882
Epoch 220, training loss: 886.7266235351562 = 1.086989402770996 + 100.0 * 8.856396675109863
Epoch 220, val loss: 1.0861114263534546
Epoch 230, training loss: 885.905029296875 = 1.0865398645401 + 100.0 * 8.848184585571289
Epoch 230, val loss: 1.085658073425293
Epoch 240, training loss: 885.0877075195312 = 1.0860815048217773 + 100.0 * 8.84001636505127
Epoch 240, val loss: 1.0852046012878418
Epoch 250, training loss: 884.433837890625 = 1.0856010913848877 + 100.0 * 8.83348274230957
Epoch 250, val loss: 1.084725022315979
Epoch 260, training loss: 883.8933715820312 = 1.0851205587387085 + 100.0 * 8.828082084655762
Epoch 260, val loss: 1.0842435359954834
Epoch 270, training loss: 883.8134155273438 = 1.0846335887908936 + 100.0 * 8.827287673950195
Epoch 270, val loss: 1.0837552547454834
Epoch 280, training loss: 883.3611450195312 = 1.0841208696365356 + 100.0 * 8.822770118713379
Epoch 280, val loss: 1.0832464694976807
Epoch 290, training loss: 883.183349609375 = 1.0835973024368286 + 100.0 * 8.82099723815918
Epoch 290, val loss: 1.0827178955078125
Epoch 300, training loss: 883.8626098632812 = 1.0829951763153076 + 100.0 * 8.82779598236084
Epoch 300, val loss: 1.0821510553359985
Epoch 310, training loss: 883.5262451171875 = 1.0824803113937378 + 100.0 * 8.824438095092773
Epoch 310, val loss: 1.0816066265106201
Epoch 320, training loss: 882.8035278320312 = 1.0819356441497803 + 100.0 * 8.817215919494629
Epoch 320, val loss: 1.081071138381958
Epoch 330, training loss: 882.9656372070312 = 1.081357479095459 + 100.0 * 8.818842887878418
Epoch 330, val loss: 1.0804851055145264
Epoch 340, training loss: 881.8106079101562 = 1.0807210206985474 + 100.0 * 8.80729866027832
Epoch 340, val loss: 1.0798697471618652
Epoch 350, training loss: 882.2860717773438 = 1.0801178216934204 + 100.0 * 8.81205940246582
Epoch 350, val loss: 1.0792789459228516
Epoch 360, training loss: 882.4177856445312 = 1.0794953107833862 + 100.0 * 8.813383102416992
Epoch 360, val loss: 1.0786526203155518
Epoch 370, training loss: 882.1450805664062 = 1.0788449048995972 + 100.0 * 8.810662269592285
Epoch 370, val loss: 1.0780103206634521
Epoch 380, training loss: 882.2947998046875 = 1.0781978368759155 + 100.0 * 8.812166213989258
Epoch 380, val loss: 1.0773741006851196
Epoch 390, training loss: 882.6085815429688 = 1.0775368213653564 + 100.0 * 8.81531047821045
Epoch 390, val loss: 1.0767115354537964
Epoch 400, training loss: 882.6599731445312 = 1.0768611431121826 + 100.0 * 8.815831184387207
Epoch 400, val loss: 1.0760509967803955
Epoch 410, training loss: 882.857666015625 = 1.0761774778366089 + 100.0 * 8.817814826965332
Epoch 410, val loss: 1.0753644704818726
Epoch 420, training loss: 883.0674438476562 = 1.0754871368408203 + 100.0 * 8.81991958618164
Epoch 420, val loss: 1.0746824741363525
Epoch 430, training loss: 883.1187133789062 = 1.0747705698013306 + 100.0 * 8.820439338684082
Epoch 430, val loss: 1.073972225189209
Epoch 440, training loss: 883.3776245117188 = 1.074070692062378 + 100.0 * 8.82303524017334
Epoch 440, val loss: 1.0732905864715576
Epoch 450, training loss: 883.1224365234375 = 1.0733402967453003 + 100.0 * 8.820490837097168
Epoch 450, val loss: 1.0725746154785156
Epoch 460, training loss: 883.0444946289062 = 1.0725940465927124 + 100.0 * 8.819719314575195
Epoch 460, val loss: 1.0718319416046143
Epoch 470, training loss: 883.4013061523438 = 1.0718415975570679 + 100.0 * 8.823294639587402
Epoch 470, val loss: 1.071086049079895
Epoch 480, training loss: 883.6565551757812 = 1.071093201637268 + 100.0 * 8.825854301452637
Epoch 480, val loss: 1.0703437328338623
Epoch 490, training loss: 883.714599609375 = 1.0703089237213135 + 100.0 * 8.82644271850586
Epoch 490, val loss: 1.0695792436599731
Epoch 500, training loss: 884.1941528320312 = 1.06951105594635 + 100.0 * 8.831246376037598
Epoch 500, val loss: 1.0687932968139648
Epoch 510, training loss: 884.21142578125 = 1.0686850547790527 + 100.0 * 8.831427574157715
Epoch 510, val loss: 1.0679900646209717
Epoch 520, training loss: 884.3761596679688 = 1.0678575038909912 + 100.0 * 8.833083152770996
Epoch 520, val loss: 1.067196249961853
Epoch 530, training loss: 884.2989501953125 = 1.067003846168518 + 100.0 * 8.832319259643555
Epoch 530, val loss: 1.0663446187973022
Epoch 540, training loss: 884.6234741210938 = 1.0661523342132568 + 100.0 * 8.835573196411133
Epoch 540, val loss: 1.065515160560608
Epoch 550, training loss: 884.6251831054688 = 1.0652979612350464 + 100.0 * 8.835598945617676
Epoch 550, val loss: 1.0646802186965942
Epoch 560, training loss: 884.8546142578125 = 1.0644139051437378 + 100.0 * 8.837902069091797
Epoch 560, val loss: 1.063800573348999
Epoch 570, training loss: 885.1639404296875 = 1.0635877847671509 + 100.0 * 8.84100341796875
Epoch 570, val loss: 1.0629870891571045
Epoch 580, training loss: 884.821533203125 = 1.0626966953277588 + 100.0 * 8.8375883102417
Epoch 580, val loss: 1.0621362924575806
Epoch 590, training loss: 884.46728515625 = 1.0617793798446655 + 100.0 * 8.834054946899414
Epoch 590, val loss: 1.0612514019012451
Epoch 600, training loss: 884.57275390625 = 1.0608952045440674 + 100.0 * 8.835118293762207
Epoch 600, val loss: 1.0603702068328857
Epoch 610, training loss: 884.9609985351562 = 1.0600076913833618 + 100.0 * 8.839010238647461
Epoch 610, val loss: 1.0594955682754517
Epoch 620, training loss: 885.7466430664062 = 1.0591291189193726 + 100.0 * 8.846875190734863
Epoch 620, val loss: 1.0586329698562622
Epoch 630, training loss: 885.9844360351562 = 1.0582267045974731 + 100.0 * 8.849262237548828
Epoch 630, val loss: 1.0577565431594849
Epoch 640, training loss: 886.3832397460938 = 1.057318925857544 + 100.0 * 8.853259086608887
Epoch 640, val loss: 1.0568666458129883
Epoch 650, training loss: 886.0359497070312 = 1.0563663244247437 + 100.0 * 8.8497953414917
Epoch 650, val loss: 1.0559581518173218
Epoch 660, training loss: 886.2620849609375 = 1.055463433265686 + 100.0 * 8.852066040039062
Epoch 660, val loss: 1.0550484657287598
Epoch 670, training loss: 886.81982421875 = 1.0545417070388794 + 100.0 * 8.85765266418457
Epoch 670, val loss: 1.054145097732544
Epoch 680, training loss: 887.1860961914062 = 1.0536240339279175 + 100.0 * 8.861324310302734
Epoch 680, val loss: 1.053247094154358
Epoch 690, training loss: 887.4560546875 = 1.0526896715164185 + 100.0 * 8.864033699035645
Epoch 690, val loss: 1.0523170232772827
Epoch 700, training loss: 887.508544921875 = 1.0517377853393555 + 100.0 * 8.864567756652832
Epoch 700, val loss: 1.051381230354309
Epoch 710, training loss: 888.015869140625 = 1.0507962703704834 + 100.0 * 8.869650840759277
Epoch 710, val loss: 1.0504616498947144
Epoch 720, training loss: 888.1681518554688 = 1.0498127937316895 + 100.0 * 8.871183395385742
Epoch 720, val loss: 1.0495094060897827
Epoch 730, training loss: 888.2567138671875 = 1.0488370656967163 + 100.0 * 8.872078895568848
Epoch 730, val loss: 1.0485413074493408
Epoch 740, training loss: 888.6471557617188 = 1.047869324684143 + 100.0 * 8.875992774963379
Epoch 740, val loss: 1.0475949048995972
Epoch 750, training loss: 888.9944458007812 = 1.0469030141830444 + 100.0 * 8.879475593566895
Epoch 750, val loss: 1.0466233491897583
Epoch 760, training loss: 889.2242431640625 = 1.0459145307540894 + 100.0 * 8.881783485412598
Epoch 760, val loss: 1.0456665754318237
Epoch 770, training loss: 889.414306640625 = 1.0449146032333374 + 100.0 * 8.88369369506836
Epoch 770, val loss: 1.0446982383728027
Epoch 780, training loss: 889.7302856445312 = 1.0439016819000244 + 100.0 * 8.886863708496094
Epoch 780, val loss: 1.0437067747116089
Epoch 790, training loss: 890.1198120117188 = 1.0428930521011353 + 100.0 * 8.890769004821777
Epoch 790, val loss: 1.042701005935669
Epoch 800, training loss: 889.9193725585938 = 1.0418645143508911 + 100.0 * 8.888774871826172
Epoch 800, val loss: 1.0416829586029053
Epoch 810, training loss: 890.3245849609375 = 1.0408556461334229 + 100.0 * 8.892837524414062
Epoch 810, val loss: 1.04070246219635
Epoch 820, training loss: 890.4911499023438 = 1.0398310422897339 + 100.0 * 8.894513130187988
Epoch 820, val loss: 1.0397073030471802
Epoch 830, training loss: 890.3970947265625 = 1.0387874841690063 + 100.0 * 8.893583297729492
Epoch 830, val loss: 1.0386797189712524
Epoch 840, training loss: 890.758544921875 = 1.0377072095870972 + 100.0 * 8.897208213806152
Epoch 840, val loss: 1.0376406908035278
Epoch 850, training loss: 890.3818969726562 = 1.0366463661193848 + 100.0 * 8.893452644348145
Epoch 850, val loss: 1.0365879535675049
Epoch 860, training loss: 890.6007690429688 = 1.03559148311615 + 100.0 * 8.895651817321777
Epoch 860, val loss: 1.035536527633667
Epoch 870, training loss: 890.9959716796875 = 1.0345556735992432 + 100.0 * 8.899614334106445
Epoch 870, val loss: 1.0344982147216797
Epoch 880, training loss: 891.999755859375 = 1.0334978103637695 + 100.0 * 8.909662246704102
Epoch 880, val loss: 1.0335079431533813
Epoch 890, training loss: 892.5438232421875 = 1.0324761867523193 + 100.0 * 8.91511344909668
Epoch 890, val loss: 1.032454013824463
Epoch 900, training loss: 891.916015625 = 1.0314127206802368 + 100.0 * 8.908845901489258
Epoch 900, val loss: 1.0314412117004395
Epoch 910, training loss: 891.395263671875 = 1.0303335189819336 + 100.0 * 8.90364933013916
Epoch 910, val loss: 1.0303776264190674
Epoch 920, training loss: 891.5128784179688 = 1.0292837619781494 + 100.0 * 8.90483570098877
Epoch 920, val loss: 1.0293480157852173
Epoch 930, training loss: 892.0263671875 = 1.0281915664672852 + 100.0 * 8.909981727600098
Epoch 930, val loss: 1.0282922983169556
Epoch 940, training loss: 892.1780395507812 = 1.027147889137268 + 100.0 * 8.911508560180664
Epoch 940, val loss: 1.0272544622421265
Epoch 950, training loss: 892.937744140625 = 1.0260891914367676 + 100.0 * 8.919116973876953
Epoch 950, val loss: 1.0262064933776855
Epoch 960, training loss: 893.149169921875 = 1.02501380443573 + 100.0 * 8.921241760253906
Epoch 960, val loss: 1.025151014328003
Epoch 970, training loss: 892.5674438476562 = 1.0238921642303467 + 100.0 * 8.915435791015625
Epoch 970, val loss: 1.0240445137023926
Epoch 980, training loss: 893.4467163085938 = 1.0228195190429688 + 100.0 * 8.924239158630371
Epoch 980, val loss: 1.0230169296264648
Epoch 990, training loss: 893.8114624023438 = 1.0217183828353882 + 100.0 * 8.927897453308105
Epoch 990, val loss: 1.0219380855560303
Epoch 1000, training loss: 894.2288208007812 = 1.0206146240234375 + 100.0 * 8.932082176208496
Epoch 1000, val loss: 1.0208545923233032
Epoch 1010, training loss: 894.477294921875 = 1.0195173025131226 + 100.0 * 8.934577941894531
Epoch 1010, val loss: 1.019763469696045
Epoch 1020, training loss: 894.7020874023438 = 1.018381953239441 + 100.0 * 8.936837196350098
Epoch 1020, val loss: 1.0186532735824585
Epoch 1030, training loss: 894.96728515625 = 1.01726496219635 + 100.0 * 8.939499855041504
Epoch 1030, val loss: 1.0175589323043823
Epoch 1040, training loss: 894.551025390625 = 1.0160807371139526 + 100.0 * 8.935349464416504
Epoch 1040, val loss: 1.0164031982421875
Epoch 1050, training loss: 894.7610473632812 = 1.014970064163208 + 100.0 * 8.937460899353027
Epoch 1050, val loss: 1.0153100490570068
Epoch 1060, training loss: 895.4431762695312 = 1.013861894607544 + 100.0 * 8.944293022155762
Epoch 1060, val loss: 1.0142148733139038
Epoch 1070, training loss: 895.8328857421875 = 1.0126959085464478 + 100.0 * 8.948202133178711
Epoch 1070, val loss: 1.013116717338562
Epoch 1080, training loss: 895.254150390625 = 1.011470079421997 + 100.0 * 8.942426681518555
Epoch 1080, val loss: 1.0119050741195679
Epoch 1090, training loss: 893.5809936523438 = 1.0101702213287354 + 100.0 * 8.925707817077637
Epoch 1090, val loss: 1.010647177696228
Epoch 1100, training loss: 894.4371337890625 = 1.0091184377670288 + 100.0 * 8.934280395507812
Epoch 1100, val loss: 1.0095858573913574
Epoch 1110, training loss: 895.4080810546875 = 1.0080435276031494 + 100.0 * 8.944000244140625
Epoch 1110, val loss: 1.008495807647705
Epoch 1120, training loss: 895.6988525390625 = 1.0068832635879517 + 100.0 * 8.946919441223145
Epoch 1120, val loss: 1.0073599815368652
Epoch 1130, training loss: 896.0557861328125 = 1.0057134628295898 + 100.0 * 8.95050048828125
Epoch 1130, val loss: 1.0061970949172974
Epoch 1140, training loss: 896.6199951171875 = 1.0045384168624878 + 100.0 * 8.956154823303223
Epoch 1140, val loss: 1.0050551891326904
Epoch 1150, training loss: 896.9703369140625 = 1.0033400058746338 + 100.0 * 8.959670066833496
Epoch 1150, val loss: 1.0038946866989136
Epoch 1160, training loss: 897.5043334960938 = 1.0021711587905884 + 100.0 * 8.965021133422852
Epoch 1160, val loss: 1.0027512311935425
Epoch 1170, training loss: 897.5910034179688 = 1.0009742975234985 + 100.0 * 8.965900421142578
Epoch 1170, val loss: 1.0015859603881836
Epoch 1180, training loss: 897.984375 = 0.9997955560684204 + 100.0 * 8.96984577178955
Epoch 1180, val loss: 1.0004339218139648
Epoch 1190, training loss: 898.1181030273438 = 0.9985992908477783 + 100.0 * 8.971195220947266
Epoch 1190, val loss: 0.9992635250091553
Epoch 1200, training loss: 898.46240234375 = 0.9973689317703247 + 100.0 * 8.974650382995605
Epoch 1200, val loss: 0.9980535507202148
Epoch 1210, training loss: 898.5478515625 = 0.9961224794387817 + 100.0 * 8.975517272949219
Epoch 1210, val loss: 0.9968333840370178
Epoch 1220, training loss: 899.1412963867188 = 0.9949250221252441 + 100.0 * 8.981463432312012
Epoch 1220, val loss: 0.9956535696983337
Epoch 1230, training loss: 898.976806640625 = 0.9936714172363281 + 100.0 * 8.97983169555664
Epoch 1230, val loss: 0.9944308996200562
Epoch 1240, training loss: 899.2609252929688 = 0.9924293160438538 + 100.0 * 8.982685089111328
Epoch 1240, val loss: 0.9932169914245605
Epoch 1250, training loss: 899.4044189453125 = 0.9911950826644897 + 100.0 * 8.984131813049316
Epoch 1250, val loss: 0.9920018315315247
Epoch 1260, training loss: 899.7034912109375 = 0.989940881729126 + 100.0 * 8.987135887145996
Epoch 1260, val loss: 0.9908210039138794
Epoch 1270, training loss: 899.6452026367188 = 0.9886253476142883 + 100.0 * 8.986565589904785
Epoch 1270, val loss: 0.9895085692405701
Epoch 1280, training loss: 899.5701293945312 = 0.9873502850532532 + 100.0 * 8.985827445983887
Epoch 1280, val loss: 0.9882743954658508
Epoch 1290, training loss: 898.7078857421875 = 0.9860376715660095 + 100.0 * 8.977218627929688
Epoch 1290, val loss: 0.9870034456253052
Epoch 1300, training loss: 899.0524291992188 = 0.9848415851593018 + 100.0 * 8.98067569732666
Epoch 1300, val loss: 0.9858130812644958
Epoch 1310, training loss: 899.7715454101562 = 0.9836662411689758 + 100.0 * 8.987878799438477
Epoch 1310, val loss: 0.9846466779708862
Epoch 1320, training loss: 899.7322998046875 = 0.9823634028434753 + 100.0 * 8.987499237060547
Epoch 1320, val loss: 0.9833624958992004
Epoch 1330, training loss: 900.4646606445312 = 0.9811245799064636 + 100.0 * 8.994834899902344
Epoch 1330, val loss: 0.9821562170982361
Epoch 1340, training loss: 900.6327514648438 = 0.9798153638839722 + 100.0 * 8.996529579162598
Epoch 1340, val loss: 0.9809021949768066
Epoch 1350, training loss: 900.862060546875 = 0.9785309433937073 + 100.0 * 8.998835563659668
Epoch 1350, val loss: 0.9796503782272339
Epoch 1360, training loss: 901.0787353515625 = 0.9772744178771973 + 100.0 * 9.001014709472656
Epoch 1360, val loss: 0.9784206748008728
Epoch 1370, training loss: 901.431884765625 = 0.975989580154419 + 100.0 * 9.004558563232422
Epoch 1370, val loss: 0.9771825075149536
Epoch 1380, training loss: 901.6034545898438 = 0.9746889472007751 + 100.0 * 9.006287574768066
Epoch 1380, val loss: 0.9758866429328918
Epoch 1390, training loss: 901.29052734375 = 0.9733314514160156 + 100.0 * 9.003171920776367
Epoch 1390, val loss: 0.9745568633079529
Epoch 1400, training loss: 901.5888061523438 = 0.9720345735549927 + 100.0 * 9.0061674118042
Epoch 1400, val loss: 0.9733152389526367
Epoch 1410, training loss: 902.3545532226562 = 0.9707419872283936 + 100.0 * 9.013837814331055
Epoch 1410, val loss: 0.9720529317855835
Epoch 1420, training loss: 902.6183471679688 = 0.9694410562515259 + 100.0 * 9.016489028930664
Epoch 1420, val loss: 0.9707504510879517
Epoch 1430, training loss: 902.72119140625 = 0.968073844909668 + 100.0 * 9.017531394958496
Epoch 1430, val loss: 0.9694559574127197
Epoch 1440, training loss: 903.087890625 = 0.9667567014694214 + 100.0 * 9.021211624145508
Epoch 1440, val loss: 0.9681540131568909
Epoch 1450, training loss: 902.4620361328125 = 0.9653432965278625 + 100.0 * 9.01496696472168
Epoch 1450, val loss: 0.9668005108833313
Epoch 1460, training loss: 902.8211059570312 = 0.9640106558799744 + 100.0 * 9.018570899963379
Epoch 1460, val loss: 0.9654808640480042
Epoch 1470, training loss: 903.4193115234375 = 0.9626548886299133 + 100.0 * 9.024566650390625
Epoch 1470, val loss: 0.9641814231872559
Epoch 1480, training loss: 903.7288818359375 = 0.9612978100776672 + 100.0 * 9.02767562866211
Epoch 1480, val loss: 0.9628600478172302
Epoch 1490, training loss: 903.1664428710938 = 0.9599156975746155 + 100.0 * 9.022065162658691
Epoch 1490, val loss: 0.9614912271499634
Epoch 1500, training loss: 903.0667724609375 = 0.9585072994232178 + 100.0 * 9.021082878112793
Epoch 1500, val loss: 0.9601569175720215
Epoch 1510, training loss: 903.0116577148438 = 0.9571064710617065 + 100.0 * 9.020545959472656
Epoch 1510, val loss: 0.9587354063987732
Epoch 1520, training loss: 903.2945556640625 = 0.9557326436042786 + 100.0 * 9.023387908935547
Epoch 1520, val loss: 0.9574266076087952
Epoch 1530, training loss: 904.1216430664062 = 0.9544045329093933 + 100.0 * 9.031672477722168
Epoch 1530, val loss: 0.9561046957969666
Epoch 1540, training loss: 903.7384033203125 = 0.9530048370361328 + 100.0 * 9.027853965759277
Epoch 1540, val loss: 0.9547668099403381
Epoch 1550, training loss: 903.8807373046875 = 0.95162034034729 + 100.0 * 9.029291152954102
Epoch 1550, val loss: 0.9534127712249756
Epoch 1560, training loss: 904.323486328125 = 0.9502617716789246 + 100.0 * 9.033732414245605
Epoch 1560, val loss: 0.9520905017852783
Epoch 1570, training loss: 904.6464233398438 = 0.9488966464996338 + 100.0 * 9.036974906921387
Epoch 1570, val loss: 0.9507285952568054
Epoch 1580, training loss: 904.879638671875 = 0.9474783539772034 + 100.0 * 9.039321899414062
Epoch 1580, val loss: 0.9493554830551147
Epoch 1590, training loss: 904.96826171875 = 0.9460288286209106 + 100.0 * 9.04022216796875
Epoch 1590, val loss: 0.9479793906211853
Epoch 1600, training loss: 904.5814819335938 = 0.9445410370826721 + 100.0 * 9.036369323730469
Epoch 1600, val loss: 0.9465001225471497
Epoch 1610, training loss: 904.3515625 = 0.9430698156356812 + 100.0 * 9.034085273742676
Epoch 1610, val loss: 0.945045530796051
Epoch 1620, training loss: 904.5138549804688 = 0.9415925145149231 + 100.0 * 9.035722732543945
Epoch 1620, val loss: 0.9436379671096802
Epoch 1630, training loss: 905.2398681640625 = 0.9402371048927307 + 100.0 * 9.042996406555176
Epoch 1630, val loss: 0.9423139691352844
Epoch 1640, training loss: 905.61181640625 = 0.9388138651847839 + 100.0 * 9.046730041503906
Epoch 1640, val loss: 0.9409335851669312
Epoch 1650, training loss: 905.8242797851562 = 0.9374004006385803 + 100.0 * 9.048869132995605
Epoch 1650, val loss: 0.9395074248313904
Epoch 1660, training loss: 905.6634521484375 = 0.9358946681022644 + 100.0 * 9.04727554321289
Epoch 1660, val loss: 0.9380701780319214
Epoch 1670, training loss: 905.61865234375 = 0.9343945980072021 + 100.0 * 9.046842575073242
Epoch 1670, val loss: 0.9366053342819214
Epoch 1680, training loss: 905.7467651367188 = 0.932916522026062 + 100.0 * 9.048138618469238
Epoch 1680, val loss: 0.9351797699928284
Epoch 1690, training loss: 906.0004272460938 = 0.9314452409744263 + 100.0 * 9.050689697265625
Epoch 1690, val loss: 0.9337382316589355
Epoch 1700, training loss: 906.8873291015625 = 0.9300152659416199 + 100.0 * 9.05957317352295
Epoch 1700, val loss: 0.9323366284370422
Epoch 1710, training loss: 907.2154541015625 = 0.9285571575164795 + 100.0 * 9.06286907196045
Epoch 1710, val loss: 0.930910050868988
Epoch 1720, training loss: 906.070068359375 = 0.9268996119499207 + 100.0 * 9.051431655883789
Epoch 1720, val loss: 0.929324746131897
Epoch 1730, training loss: 905.5991821289062 = 0.925288200378418 + 100.0 * 9.046738624572754
Epoch 1730, val loss: 0.9277365803718567
Epoch 1740, training loss: 906.0009155273438 = 0.9237763285636902 + 100.0 * 9.050771713256836
Epoch 1740, val loss: 0.9262651801109314
Epoch 1750, training loss: 906.82470703125 = 0.922238290309906 + 100.0 * 9.059024810791016
Epoch 1750, val loss: 0.9247821569442749
Epoch 1760, training loss: 907.61962890625 = 0.9206796288490295 + 100.0 * 9.06698989868164
Epoch 1760, val loss: 0.9232627749443054
Epoch 1770, training loss: 907.5377807617188 = 0.9190293550491333 + 100.0 * 9.066187858581543
Epoch 1770, val loss: 0.9216734170913696
Epoch 1780, training loss: 907.9608154296875 = 0.917352557182312 + 100.0 * 9.0704345703125
Epoch 1780, val loss: 0.9200441241264343
Epoch 1790, training loss: 908.0662231445312 = 0.9156610369682312 + 100.0 * 9.071505546569824
Epoch 1790, val loss: 0.9184108972549438
Epoch 1800, training loss: 908.2317504882812 = 0.9139952063560486 + 100.0 * 9.073177337646484
Epoch 1800, val loss: 0.9168042540550232
Epoch 1810, training loss: 907.8524780273438 = 0.9122620224952698 + 100.0 * 9.069401741027832
Epoch 1810, val loss: 0.915121853351593
Epoch 1820, training loss: 908.4556274414062 = 0.9105285406112671 + 100.0 * 9.075450897216797
Epoch 1820, val loss: 0.9134588241577148
Epoch 1830, training loss: 908.2002563476562 = 0.9087519645690918 + 100.0 * 9.072915077209473
Epoch 1830, val loss: 0.9117425680160522
Epoch 1840, training loss: 908.511474609375 = 0.9069979786872864 + 100.0 * 9.076045036315918
Epoch 1840, val loss: 0.9100471138954163
Epoch 1850, training loss: 909.0634765625 = 0.9052689075469971 + 100.0 * 9.081582069396973
Epoch 1850, val loss: 0.9083830118179321
Epoch 1860, training loss: 909.3956909179688 = 0.9034985899925232 + 100.0 * 9.084921836853027
Epoch 1860, val loss: 0.9067564010620117
Epoch 1870, training loss: 907.301513671875 = 0.9018294215202332 + 100.0 * 9.063996315002441
Epoch 1870, val loss: 0.9050083756446838
Epoch 1880, training loss: 907.6390380859375 = 0.9000852704048157 + 100.0 * 9.067389488220215
Epoch 1880, val loss: 0.9033039212226868
Epoch 1890, training loss: 908.0231323242188 = 0.8982884287834167 + 100.0 * 9.071248054504395
Epoch 1890, val loss: 0.9016778469085693
Epoch 1900, training loss: 908.3384399414062 = 0.896582841873169 + 100.0 * 9.074418067932129
Epoch 1900, val loss: 0.9000558853149414
Epoch 1910, training loss: 908.8276977539062 = 0.8948462009429932 + 100.0 * 9.079328536987305
Epoch 1910, val loss: 0.8983871340751648
Epoch 1920, training loss: 909.0706176757812 = 0.8931190371513367 + 100.0 * 9.081774711608887
Epoch 1920, val loss: 0.8967236876487732
Epoch 1930, training loss: 909.0581665039062 = 0.8913528323173523 + 100.0 * 9.08166790008545
Epoch 1930, val loss: 0.8950127959251404
Epoch 1940, training loss: 909.5542602539062 = 0.8896292448043823 + 100.0 * 9.08664608001709
Epoch 1940, val loss: 0.8933261632919312
Epoch 1950, training loss: 909.4990234375 = 0.887810468673706 + 100.0 * 9.086112022399902
Epoch 1950, val loss: 0.8915984034538269
Epoch 1960, training loss: 909.9218139648438 = 0.8860862851142883 + 100.0 * 9.090356826782227
Epoch 1960, val loss: 0.8899243474006653
Epoch 1970, training loss: 909.5502319335938 = 0.8843061327934265 + 100.0 * 9.08665943145752
Epoch 1970, val loss: 0.8882231712341309
Epoch 1980, training loss: 910.001708984375 = 0.8825254440307617 + 100.0 * 9.091192245483398
Epoch 1980, val loss: 0.8865033388137817
Epoch 1990, training loss: 910.02880859375 = 0.8807052969932556 + 100.0 * 9.09148120880127
Epoch 1990, val loss: 0.8847424387931824
Epoch 2000, training loss: 909.3915405273438 = 0.8788747787475586 + 100.0 * 9.085126876831055
Epoch 2000, val loss: 0.882934033870697
Epoch 2010, training loss: 909.7886962890625 = 0.8770967721939087 + 100.0 * 9.089116096496582
Epoch 2010, val loss: 0.8812570571899414
Epoch 2020, training loss: 910.3115844726562 = 0.8753514885902405 + 100.0 * 9.094362258911133
Epoch 2020, val loss: 0.8795570731163025
Epoch 2030, training loss: 910.6784057617188 = 0.8735710978507996 + 100.0 * 9.098048210144043
Epoch 2030, val loss: 0.8778374195098877
Epoch 2040, training loss: 910.8994750976562 = 0.871760368347168 + 100.0 * 9.100276947021484
Epoch 2040, val loss: 0.8761002421379089
Epoch 2050, training loss: 910.7219848632812 = 0.869891881942749 + 100.0 * 9.09852123260498
Epoch 2050, val loss: 0.8742720484733582
Epoch 2060, training loss: 910.7081298828125 = 0.868071436882019 + 100.0 * 9.098401069641113
Epoch 2060, val loss: 0.8724982142448425
Epoch 2070, training loss: 911.03466796875 = 0.86632239818573 + 100.0 * 9.101683616638184
Epoch 2070, val loss: 0.8708147406578064
Epoch 2080, training loss: 911.6577758789062 = 0.864578366279602 + 100.0 * 9.107932090759277
Epoch 2080, val loss: 0.8691211938858032
Epoch 2090, training loss: 911.9298706054688 = 0.8627737760543823 + 100.0 * 9.110671043395996
Epoch 2090, val loss: 0.8673766851425171
Epoch 2100, training loss: 911.2647094726562 = 0.8609135746955872 + 100.0 * 9.10403823852539
Epoch 2100, val loss: 0.8655802607536316
Epoch 2110, training loss: 911.69775390625 = 0.8590768575668335 + 100.0 * 9.108386993408203
Epoch 2110, val loss: 0.8637996912002563
Epoch 2120, training loss: 912.4721069335938 = 0.8572565317153931 + 100.0 * 9.116148948669434
Epoch 2120, val loss: 0.8620644211769104
Epoch 2130, training loss: 912.64599609375 = 0.8554369807243347 + 100.0 * 9.117905616760254
Epoch 2130, val loss: 0.8602705597877502
Epoch 2140, training loss: 912.2991333007812 = 0.853630006313324 + 100.0 * 9.114455223083496
Epoch 2140, val loss: 0.8585965037345886
Epoch 2150, training loss: 912.6107177734375 = 0.8519635200500488 + 100.0 * 9.117587089538574
Epoch 2150, val loss: 0.8569530248641968
Epoch 2160, training loss: 912.843994140625 = 0.8501193523406982 + 100.0 * 9.119938850402832
Epoch 2160, val loss: 0.8552009463310242
Epoch 2170, training loss: 912.8182373046875 = 0.84820556640625 + 100.0 * 9.11970043182373
Epoch 2170, val loss: 0.853358805179596
Epoch 2180, training loss: 912.3173217773438 = 0.8462666273117065 + 100.0 * 9.114710807800293
Epoch 2180, val loss: 0.8514281511306763
Epoch 2190, training loss: 912.23583984375 = 0.8442825675010681 + 100.0 * 9.11391544342041
Epoch 2190, val loss: 0.849530816078186
Epoch 2200, training loss: 911.8601684570312 = 0.8423455357551575 + 100.0 * 9.110177993774414
Epoch 2200, val loss: 0.8476880192756653
Epoch 2210, training loss: 912.6875610351562 = 0.8404901623725891 + 100.0 * 9.118470191955566
Epoch 2210, val loss: 0.8459030985832214
Epoch 2220, training loss: 913.3535766601562 = 0.8386649489402771 + 100.0 * 9.12514877319336
Epoch 2220, val loss: 0.8441411256790161
Epoch 2230, training loss: 913.60302734375 = 0.8368160128593445 + 100.0 * 9.127662658691406
Epoch 2230, val loss: 0.8423405289649963
Epoch 2240, training loss: 913.3150634765625 = 0.8349065184593201 + 100.0 * 9.124801635742188
Epoch 2240, val loss: 0.8405241370201111
Epoch 2250, training loss: 913.2431640625 = 0.83302241563797 + 100.0 * 9.124101638793945
Epoch 2250, val loss: 0.8387199640274048
Epoch 2260, training loss: 913.3062133789062 = 0.8311504125595093 + 100.0 * 9.124750137329102
Epoch 2260, val loss: 0.8369219303131104
Epoch 2270, training loss: 913.8128051757812 = 0.8292479515075684 + 100.0 * 9.129836082458496
Epoch 2270, val loss: 0.8351137042045593
Epoch 2280, training loss: 914.287109375 = 0.8274860978126526 + 100.0 * 9.13459587097168
Epoch 2280, val loss: 0.833373486995697
Epoch 2290, training loss: 913.954345703125 = 0.8255734443664551 + 100.0 * 9.131287574768066
Epoch 2290, val loss: 0.8315247893333435
Epoch 2300, training loss: 913.0126342773438 = 0.823631763458252 + 100.0 * 9.1218900680542
Epoch 2300, val loss: 0.8296617865562439
Epoch 2310, training loss: 913.627685546875 = 0.8217965960502625 + 100.0 * 9.128059387207031
Epoch 2310, val loss: 0.8279011845588684
Epoch 2320, training loss: 914.0218505859375 = 0.819972574710846 + 100.0 * 9.13201904296875
Epoch 2320, val loss: 0.8261335492134094
Epoch 2330, training loss: 914.45068359375 = 0.8181512951850891 + 100.0 * 9.136324882507324
Epoch 2330, val loss: 0.8244005441665649
Epoch 2340, training loss: 914.3615112304688 = 0.816295862197876 + 100.0 * 9.135452270507812
Epoch 2340, val loss: 0.8225855231285095
Epoch 2350, training loss: 914.6344604492188 = 0.8144589066505432 + 100.0 * 9.138199806213379
Epoch 2350, val loss: 0.8208065032958984
Epoch 2360, training loss: 914.9386596679688 = 0.8126471042633057 + 100.0 * 9.141260147094727
Epoch 2360, val loss: 0.8190467357635498
Epoch 2370, training loss: 912.8009643554688 = 0.8108094930648804 + 100.0 * 9.119901657104492
Epoch 2370, val loss: 0.8173184990882874
Epoch 2380, training loss: 912.6676025390625 = 0.8091914653778076 + 100.0 * 9.118583679199219
Epoch 2380, val loss: 0.8157157897949219
Epoch 2390, training loss: 912.6350708007812 = 0.807416558265686 + 100.0 * 9.118276596069336
Epoch 2390, val loss: 0.8140032887458801
Epoch 2400, training loss: 913.3153686523438 = 0.8056637048721313 + 100.0 * 9.125097274780273
Epoch 2400, val loss: 0.8123215436935425
Epoch 2410, training loss: 914.1242065429688 = 0.8039312362670898 + 100.0 * 9.13320255279541
Epoch 2410, val loss: 0.8106642961502075
Epoch 2420, training loss: 914.6195678710938 = 0.8021834492683411 + 100.0 * 9.138174057006836
Epoch 2420, val loss: 0.8089707493782043
Epoch 2430, training loss: 915.1068725585938 = 0.8004217147827148 + 100.0 * 9.143064498901367
Epoch 2430, val loss: 0.807253897190094
Epoch 2440, training loss: 914.3517456054688 = 0.7985759973526001 + 100.0 * 9.135531425476074
Epoch 2440, val loss: 0.8054639101028442
Epoch 2450, training loss: 914.4931640625 = 0.7968645095825195 + 100.0 * 9.136962890625
Epoch 2450, val loss: 0.803816556930542
Epoch 2460, training loss: 914.9940185546875 = 0.7951818704605103 + 100.0 * 9.141988754272461
Epoch 2460, val loss: 0.8021796345710754
Epoch 2470, training loss: 915.4398803710938 = 0.7934700846672058 + 100.0 * 9.146464347839355
Epoch 2470, val loss: 0.8005425930023193
Epoch 2480, training loss: 914.591552734375 = 0.7916876077651978 + 100.0 * 9.137998580932617
Epoch 2480, val loss: 0.7987756729125977
Epoch 2490, training loss: 914.8198852539062 = 0.7899750471115112 + 100.0 * 9.140298843383789
Epoch 2490, val loss: 0.7971566915512085
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.72
0.8147504165761067
=== training gcn model ===
Epoch 0, training loss: 1023.070068359375 = 1.0892977714538574 + 100.0 * 10.219807624816895
Epoch 0, val loss: 1.089498519897461
Epoch 10, training loss: 985.5400390625 = 1.0890741348266602 + 100.0 * 9.844510078430176
Epoch 10, val loss: 1.0893100500106812
Epoch 20, training loss: 967.9168090820312 = 1.0888420343399048 + 100.0 * 9.668279647827148
Epoch 20, val loss: 1.0890823602676392
Epoch 30, training loss: 955.1470947265625 = 1.088562250137329 + 100.0 * 9.5405855178833
Epoch 30, val loss: 1.088806390762329
Epoch 40, training loss: 944.912841796875 = 1.0882844924926758 + 100.0 * 9.43824577331543
Epoch 40, val loss: 1.0885272026062012
Epoch 50, training loss: 936.4989013671875 = 1.0879900455474854 + 100.0 * 9.354108810424805
Epoch 50, val loss: 1.0882344245910645
Epoch 60, training loss: 929.3743896484375 = 1.0876715183258057 + 100.0 * 9.282867431640625
Epoch 60, val loss: 1.087918996810913
Epoch 70, training loss: 923.2639770507812 = 1.0873364210128784 + 100.0 * 9.221766471862793
Epoch 70, val loss: 1.087587594985962
Epoch 80, training loss: 917.8824462890625 = 1.0869860649108887 + 100.0 * 9.167954444885254
Epoch 80, val loss: 1.087243676185608
Epoch 90, training loss: 913.2610473632812 = 1.0866304636001587 + 100.0 * 9.121744155883789
Epoch 90, val loss: 1.0868890285491943
Epoch 100, training loss: 909.1635131835938 = 1.0862644910812378 + 100.0 * 9.080772399902344
Epoch 100, val loss: 1.0865222215652466
Epoch 110, training loss: 905.5449829101562 = 1.0858969688415527 + 100.0 * 9.044590950012207
Epoch 110, val loss: 1.0861592292785645
Epoch 120, training loss: 902.421142578125 = 1.085517168045044 + 100.0 * 9.01335620880127
Epoch 120, val loss: 1.0857797861099243
Epoch 130, training loss: 899.8760986328125 = 1.0851073265075684 + 100.0 * 8.987910270690918
Epoch 130, val loss: 1.0853681564331055
Epoch 140, training loss: 897.4949951171875 = 1.0847163200378418 + 100.0 * 8.964102745056152
Epoch 140, val loss: 1.0849825143814087
Epoch 150, training loss: 895.70458984375 = 1.0843137502670288 + 100.0 * 8.946203231811523
Epoch 150, val loss: 1.0845869779586792
Epoch 160, training loss: 893.9904174804688 = 1.0839128494262695 + 100.0 * 8.929064750671387
Epoch 160, val loss: 1.0841822624206543
Epoch 170, training loss: 892.1371459960938 = 1.0834835767745972 + 100.0 * 8.910536766052246
Epoch 170, val loss: 1.083748459815979
Epoch 180, training loss: 891.0718994140625 = 1.0830438137054443 + 100.0 * 8.899888038635254
Epoch 180, val loss: 1.083308219909668
Epoch 190, training loss: 890.2040405273438 = 1.0826071500778198 + 100.0 * 8.891214370727539
Epoch 190, val loss: 1.0828862190246582
Epoch 200, training loss: 889.0275268554688 = 1.0821634531021118 + 100.0 * 8.879453659057617
Epoch 200, val loss: 1.082417368888855
Epoch 210, training loss: 888.0751953125 = 1.081687092781067 + 100.0 * 8.869935035705566
Epoch 210, val loss: 1.0819692611694336
Epoch 220, training loss: 887.3516845703125 = 1.0812439918518066 + 100.0 * 8.862704277038574
Epoch 220, val loss: 1.0815256834030151
Epoch 230, training loss: 886.71337890625 = 1.0807617902755737 + 100.0 * 8.85632610321045
Epoch 230, val loss: 1.0810433626174927
Epoch 240, training loss: 886.3833618164062 = 1.0802937746047974 + 100.0 * 8.85303020477295
Epoch 240, val loss: 1.080567479133606
Epoch 250, training loss: 885.962646484375 = 1.0798064470291138 + 100.0 * 8.848828315734863
Epoch 250, val loss: 1.0800821781158447
Epoch 260, training loss: 885.8108520507812 = 1.079305648803711 + 100.0 * 8.847315788269043
Epoch 260, val loss: 1.0795838832855225
Epoch 270, training loss: 885.4132690429688 = 1.0787914991378784 + 100.0 * 8.843344688415527
Epoch 270, val loss: 1.0790737867355347
Epoch 280, training loss: 884.919921875 = 1.0782548189163208 + 100.0 * 8.838417053222656
Epoch 280, val loss: 1.078537940979004
Epoch 290, training loss: 884.7499389648438 = 1.0777249336242676 + 100.0 * 8.836722373962402
Epoch 290, val loss: 1.0780094861984253
Epoch 300, training loss: 884.69775390625 = 1.077190637588501 + 100.0 * 8.83620548248291
Epoch 300, val loss: 1.0774688720703125
Epoch 310, training loss: 884.68212890625 = 1.076636791229248 + 100.0 * 8.836054801940918
Epoch 310, val loss: 1.0769236087799072
Epoch 320, training loss: 884.8671875 = 1.0760574340820312 + 100.0 * 8.837911605834961
Epoch 320, val loss: 1.0763715505599976
Epoch 330, training loss: 884.0134887695312 = 1.0755126476287842 + 100.0 * 8.82938003540039
Epoch 330, val loss: 1.0758025646209717
Epoch 340, training loss: 884.2926635742188 = 1.0749558210372925 + 100.0 * 8.83217716217041
Epoch 340, val loss: 1.0752383470535278
Epoch 350, training loss: 883.5774536132812 = 1.0743801593780518 + 100.0 * 8.825030326843262
Epoch 350, val loss: 1.074662446975708
Epoch 360, training loss: 884.372314453125 = 1.0738301277160645 + 100.0 * 8.832984924316406
Epoch 360, val loss: 1.0741183757781982
Epoch 370, training loss: 884.2304077148438 = 1.073233962059021 + 100.0 * 8.831571578979492
Epoch 370, val loss: 1.073530912399292
Epoch 380, training loss: 884.5772094726562 = 1.07264244556427 + 100.0 * 8.83504581451416
Epoch 380, val loss: 1.0729376077651978
Epoch 390, training loss: 884.5997924804688 = 1.0720385313034058 + 100.0 * 8.835277557373047
Epoch 390, val loss: 1.072339415550232
Epoch 400, training loss: 884.9490966796875 = 1.0714242458343506 + 100.0 * 8.838776588439941
Epoch 400, val loss: 1.0717284679412842
Epoch 410, training loss: 884.7047119140625 = 1.0707865953445435 + 100.0 * 8.836338996887207
Epoch 410, val loss: 1.071080207824707
Epoch 420, training loss: 884.8828735351562 = 1.0701416730880737 + 100.0 * 8.838127136230469
Epoch 420, val loss: 1.0704452991485596
Epoch 430, training loss: 885.3439331054688 = 1.0695112943649292 + 100.0 * 8.842743873596191
Epoch 430, val loss: 1.0698137283325195
Epoch 440, training loss: 885.3236083984375 = 1.0688773393630981 + 100.0 * 8.842547416687012
Epoch 440, val loss: 1.0691819190979004
Epoch 450, training loss: 885.6685180664062 = 1.0682257413864136 + 100.0 * 8.846002578735352
Epoch 450, val loss: 1.0685368776321411
Epoch 460, training loss: 885.5877075195312 = 1.0675454139709473 + 100.0 * 8.84520149230957
Epoch 460, val loss: 1.0678565502166748
Epoch 470, training loss: 885.970458984375 = 1.0668615102767944 + 100.0 * 8.84903621673584
Epoch 470, val loss: 1.0671905279159546
Epoch 480, training loss: 885.5379028320312 = 1.0662027597427368 + 100.0 * 8.844717025756836
Epoch 480, val loss: 1.0665310621261597
Epoch 490, training loss: 885.8069458007812 = 1.0655359029769897 + 100.0 * 8.847414016723633
Epoch 490, val loss: 1.0658618211746216
Epoch 500, training loss: 886.3389892578125 = 1.0648620128631592 + 100.0 * 8.852741241455078
Epoch 500, val loss: 1.06520414352417
Epoch 510, training loss: 886.805908203125 = 1.064182162284851 + 100.0 * 8.857417106628418
Epoch 510, val loss: 1.0645191669464111
Epoch 520, training loss: 886.7771606445312 = 1.0634822845458984 + 100.0 * 8.857136726379395
Epoch 520, val loss: 1.0638295412063599
Epoch 530, training loss: 887.0420532226562 = 1.062800407409668 + 100.0 * 8.859792709350586
Epoch 530, val loss: 1.0631600618362427
Epoch 540, training loss: 887.4445190429688 = 1.0621060132980347 + 100.0 * 8.863823890686035
Epoch 540, val loss: 1.0624781847000122
Epoch 550, training loss: 887.7240600585938 = 1.0613945722579956 + 100.0 * 8.866626739501953
Epoch 550, val loss: 1.0617777109146118
Epoch 560, training loss: 887.984375 = 1.0606826543807983 + 100.0 * 8.869236946105957
Epoch 560, val loss: 1.06105375289917
Epoch 570, training loss: 888.145263671875 = 1.0599642992019653 + 100.0 * 8.870853424072266
Epoch 570, val loss: 1.0603774785995483
Epoch 580, training loss: 888.6118774414062 = 1.0592918395996094 + 100.0 * 8.875526428222656
Epoch 580, val loss: 1.059700846672058
Epoch 590, training loss: 888.7794799804688 = 1.0585542917251587 + 100.0 * 8.877209663391113
Epoch 590, val loss: 1.058983564376831
Epoch 600, training loss: 889.2493896484375 = 1.0578562021255493 + 100.0 * 8.881915092468262
Epoch 600, val loss: 1.0582847595214844
Epoch 610, training loss: 889.6581420898438 = 1.0571448802947998 + 100.0 * 8.88601016998291
Epoch 610, val loss: 1.0575928688049316
Epoch 620, training loss: 890.0760498046875 = 1.056427001953125 + 100.0 * 8.890196800231934
Epoch 620, val loss: 1.0568796396255493
Epoch 630, training loss: 890.41796875 = 1.0556870698928833 + 100.0 * 8.893623352050781
Epoch 630, val loss: 1.0561624765396118
Epoch 640, training loss: 890.94873046875 = 1.0549671649932861 + 100.0 * 8.898937225341797
Epoch 640, val loss: 1.0554498434066772
Epoch 650, training loss: 891.2809448242188 = 1.054195523262024 + 100.0 * 8.902267456054688
Epoch 650, val loss: 1.0546857118606567
Epoch 660, training loss: 890.8622436523438 = 1.053464651107788 + 100.0 * 8.898087501525879
Epoch 660, val loss: 1.053966760635376
Epoch 670, training loss: 891.0023803710938 = 1.052701711654663 + 100.0 * 8.899497032165527
Epoch 670, val loss: 1.0532240867614746
Epoch 680, training loss: 891.919921875 = 1.0519819259643555 + 100.0 * 8.908679008483887
Epoch 680, val loss: 1.0525118112564087
Epoch 690, training loss: 891.3815307617188 = 1.0511596202850342 + 100.0 * 8.903304100036621
Epoch 690, val loss: 1.051728367805481
Epoch 700, training loss: 892.0033569335938 = 1.0504270792007446 + 100.0 * 8.909529685974121
Epoch 700, val loss: 1.0510003566741943
Epoch 710, training loss: 892.1676635742188 = 1.0496615171432495 + 100.0 * 8.911179542541504
Epoch 710, val loss: 1.050245761871338
Epoch 720, training loss: 892.4354248046875 = 1.0488851070404053 + 100.0 * 8.913865089416504
Epoch 720, val loss: 1.0494849681854248
Epoch 730, training loss: 892.8726196289062 = 1.0481261014938354 + 100.0 * 8.918245315551758
Epoch 730, val loss: 1.048715353012085
Epoch 740, training loss: 892.1375122070312 = 1.0472962856292725 + 100.0 * 8.91090202331543
Epoch 740, val loss: 1.0479341745376587
Epoch 750, training loss: 892.5302124023438 = 1.0465015172958374 + 100.0 * 8.914836883544922
Epoch 750, val loss: 1.0471477508544922
Epoch 760, training loss: 892.7054443359375 = 1.0457671880722046 + 100.0 * 8.916596412658691
Epoch 760, val loss: 1.046424388885498
Epoch 770, training loss: 893.4695434570312 = 1.044983148574829 + 100.0 * 8.924245834350586
Epoch 770, val loss: 1.0456554889678955
Epoch 780, training loss: 893.7103881835938 = 1.0441566705703735 + 100.0 * 8.92666244506836
Epoch 780, val loss: 1.0448617935180664
Epoch 790, training loss: 893.5601196289062 = 1.043330430984497 + 100.0 * 8.92516803741455
Epoch 790, val loss: 1.0440572500228882
Epoch 800, training loss: 893.9201049804688 = 1.0425121784210205 + 100.0 * 8.928775787353516
Epoch 800, val loss: 1.0432356595993042
Epoch 810, training loss: 894.4990234375 = 1.0417163372039795 + 100.0 * 8.93457317352295
Epoch 810, val loss: 1.0424529314041138
Epoch 820, training loss: 894.9918212890625 = 1.0408858060836792 + 100.0 * 8.939509391784668
Epoch 820, val loss: 1.0416253805160522
Epoch 830, training loss: 894.8671264648438 = 1.0400041341781616 + 100.0 * 8.938271522521973
Epoch 830, val loss: 1.0407898426055908
Epoch 840, training loss: 894.9998779296875 = 1.03921377658844 + 100.0 * 8.939606666564941
Epoch 840, val loss: 1.0399980545043945
Epoch 850, training loss: 895.36767578125 = 1.0383809804916382 + 100.0 * 8.943292617797852
Epoch 850, val loss: 1.0391732454299927
Epoch 860, training loss: 895.2846069335938 = 1.037544846534729 + 100.0 * 8.94247055053711
Epoch 860, val loss: 1.0383481979370117
Epoch 870, training loss: 895.6536865234375 = 1.0366699695587158 + 100.0 * 8.94616985321045
Epoch 870, val loss: 1.0374901294708252
Epoch 880, training loss: 896.326416015625 = 1.035819411277771 + 100.0 * 8.952905654907227
Epoch 880, val loss: 1.0366711616516113
Epoch 890, training loss: 896.35009765625 = 1.0349231958389282 + 100.0 * 8.95315170288086
Epoch 890, val loss: 1.0358041524887085
Epoch 900, training loss: 896.5086059570312 = 1.0340449810028076 + 100.0 * 8.954745292663574
Epoch 900, val loss: 1.0349338054656982
Epoch 910, training loss: 897.0360107421875 = 1.033150315284729 + 100.0 * 8.960028648376465
Epoch 910, val loss: 1.0340595245361328
Epoch 920, training loss: 897.474853515625 = 1.032239556312561 + 100.0 * 8.964426040649414
Epoch 920, val loss: 1.0331870317459106
Epoch 930, training loss: 897.155517578125 = 1.0312726497650146 + 100.0 * 8.96124267578125
Epoch 930, val loss: 1.0322163105010986
Epoch 940, training loss: 897.5734252929688 = 1.0303419828414917 + 100.0 * 8.965431213378906
Epoch 940, val loss: 1.031325340270996
Epoch 950, training loss: 898.380859375 = 1.0294160842895508 + 100.0 * 8.973514556884766
Epoch 950, val loss: 1.0304124355316162
Epoch 960, training loss: 898.95263671875 = 1.028475284576416 + 100.0 * 8.979241371154785
Epoch 960, val loss: 1.0294690132141113
Epoch 970, training loss: 898.6114501953125 = 1.0274699926376343 + 100.0 * 8.975839614868164
Epoch 970, val loss: 1.028537392616272
Epoch 980, training loss: 898.1937866210938 = 1.0264827013015747 + 100.0 * 8.971673011779785
Epoch 980, val loss: 1.0275627374649048
Epoch 990, training loss: 898.6915893554688 = 1.0254770517349243 + 100.0 * 8.976661682128906
Epoch 990, val loss: 1.0265414714813232
Epoch 1000, training loss: 899.2968139648438 = 1.0244816541671753 + 100.0 * 8.982723236083984
Epoch 1000, val loss: 1.025600790977478
Epoch 1010, training loss: 899.4981079101562 = 1.0234661102294922 + 100.0 * 8.984746932983398
Epoch 1010, val loss: 1.0246007442474365
Epoch 1020, training loss: 899.5252075195312 = 1.022467851638794 + 100.0 * 8.985027313232422
Epoch 1020, val loss: 1.023655891418457
Epoch 1030, training loss: 899.0391845703125 = 1.0214097499847412 + 100.0 * 8.980177879333496
Epoch 1030, val loss: 1.0225836038589478
Epoch 1040, training loss: 899.718994140625 = 1.020430326461792 + 100.0 * 8.986985206604004
Epoch 1040, val loss: 1.021636962890625
Epoch 1050, training loss: 900.56787109375 = 1.0194034576416016 + 100.0 * 8.995484352111816
Epoch 1050, val loss: 1.0206295251846313
Epoch 1060, training loss: 899.699462890625 = 1.0182969570159912 + 100.0 * 8.986811637878418
Epoch 1060, val loss: 1.0195491313934326
Epoch 1070, training loss: 899.9487915039062 = 1.0172646045684814 + 100.0 * 8.989315032958984
Epoch 1070, val loss: 1.0185341835021973
Epoch 1080, training loss: 899.6787109375 = 1.0161402225494385 + 100.0 * 8.986625671386719
Epoch 1080, val loss: 1.017447829246521
Epoch 1090, training loss: 900.7466430664062 = 1.015083909034729 + 100.0 * 8.997315406799316
Epoch 1090, val loss: 1.016383409500122
Epoch 1100, training loss: 901.2744140625 = 1.0140184164047241 + 100.0 * 9.002603530883789
Epoch 1100, val loss: 1.0153648853302002
Epoch 1110, training loss: 901.6557006835938 = 1.0129393339157104 + 100.0 * 9.006427764892578
Epoch 1110, val loss: 1.0142699480056763
Epoch 1120, training loss: 901.711181640625 = 1.011812448501587 + 100.0 * 9.006993293762207
Epoch 1120, val loss: 1.013192057609558
Epoch 1130, training loss: 902.038330078125 = 1.0107182264328003 + 100.0 * 9.010275840759277
Epoch 1130, val loss: 1.012142539024353
Epoch 1140, training loss: 902.155029296875 = 1.0095806121826172 + 100.0 * 9.011454582214355
Epoch 1140, val loss: 1.0110337734222412
Epoch 1150, training loss: 902.28759765625 = 1.0084363222122192 + 100.0 * 9.012791633605957
Epoch 1150, val loss: 1.0098917484283447
Epoch 1160, training loss: 902.6107788085938 = 1.0072581768035889 + 100.0 * 9.016035079956055
Epoch 1160, val loss: 1.0087412595748901
Epoch 1170, training loss: 902.4058227539062 = 1.0061167478561401 + 100.0 * 9.013997077941895
Epoch 1170, val loss: 1.0076087713241577
Epoch 1180, training loss: 902.7758178710938 = 1.0049601793289185 + 100.0 * 9.017708778381348
Epoch 1180, val loss: 1.006474256515503
Epoch 1190, training loss: 903.1990356445312 = 1.0038646459579468 + 100.0 * 9.021951675415039
Epoch 1190, val loss: 1.005398154258728
Epoch 1200, training loss: 903.466796875 = 1.0026952028274536 + 100.0 * 9.024641036987305
Epoch 1200, val loss: 1.0042473077774048
Epoch 1210, training loss: 903.495849609375 = 1.0015006065368652 + 100.0 * 9.024943351745605
Epoch 1210, val loss: 1.0031177997589111
Epoch 1220, training loss: 903.7229614257812 = 1.0002562999725342 + 100.0 * 9.027227401733398
Epoch 1220, val loss: 1.0018551349639893
Epoch 1230, training loss: 903.8651733398438 = 0.9990888237953186 + 100.0 * 9.028660774230957
Epoch 1230, val loss: 1.0006897449493408
Epoch 1240, training loss: 902.0087280273438 = 0.9977249503135681 + 100.0 * 9.010109901428223
Epoch 1240, val loss: 0.9993226528167725
Epoch 1250, training loss: 902.6746826171875 = 0.9963855743408203 + 100.0 * 9.016782760620117
Epoch 1250, val loss: 0.9980165362358093
Epoch 1260, training loss: 903.3919677734375 = 0.9951857328414917 + 100.0 * 9.023967742919922
Epoch 1260, val loss: 0.9968617558479309
Epoch 1270, training loss: 903.6664428710938 = 0.9939650297164917 + 100.0 * 9.026724815368652
Epoch 1270, val loss: 0.9956518411636353
Epoch 1280, training loss: 904.3754272460938 = 0.992725670337677 + 100.0 * 9.03382682800293
Epoch 1280, val loss: 0.9944366216659546
Epoch 1290, training loss: 904.8090209960938 = 0.9914631843566895 + 100.0 * 9.038175582885742
Epoch 1290, val loss: 0.9931773543357849
Epoch 1300, training loss: 904.8199462890625 = 0.9901582598686218 + 100.0 * 9.038297653198242
Epoch 1300, val loss: 0.9918991923332214
Epoch 1310, training loss: 905.550048828125 = 0.9888542890548706 + 100.0 * 9.045612335205078
Epoch 1310, val loss: 0.9906060099601746
Epoch 1320, training loss: 905.5927124023438 = 0.98750239610672 + 100.0 * 9.046051979064941
Epoch 1320, val loss: 0.9892696142196655
Epoch 1330, training loss: 905.7621459960938 = 0.9861660003662109 + 100.0 * 9.047760009765625
Epoch 1330, val loss: 0.9880003929138184
Epoch 1340, training loss: 906.2213745117188 = 0.9852010607719421 + 100.0 * 9.052361488342285
Epoch 1340, val loss: 0.9870047569274902
Epoch 1350, training loss: 906.12255859375 = 0.98390793800354 + 100.0 * 9.051386833190918
Epoch 1350, val loss: 0.985713005065918
Epoch 1360, training loss: 906.25 = 0.9824783802032471 + 100.0 * 9.052675247192383
Epoch 1360, val loss: 0.9843035936355591
Epoch 1370, training loss: 906.80078125 = 0.9810973405838013 + 100.0 * 9.058197021484375
Epoch 1370, val loss: 0.9829358458518982
Epoch 1380, training loss: 906.8441772460938 = 0.9796950221061707 + 100.0 * 9.058645248413086
Epoch 1380, val loss: 0.9815458059310913
Epoch 1390, training loss: 906.933837890625 = 0.978315532207489 + 100.0 * 9.059555053710938
Epoch 1390, val loss: 0.980172336101532
Epoch 1400, training loss: 907.2352905273438 = 0.9768967032432556 + 100.0 * 9.062583923339844
Epoch 1400, val loss: 0.9787909388542175
Epoch 1410, training loss: 907.3529052734375 = 0.9754775166511536 + 100.0 * 9.063774108886719
Epoch 1410, val loss: 0.977365255355835
Epoch 1420, training loss: 907.6548461914062 = 0.9740776419639587 + 100.0 * 9.066807746887207
Epoch 1420, val loss: 0.975984513759613
Epoch 1430, training loss: 906.7675170898438 = 0.9725908041000366 + 100.0 * 9.05794906616211
Epoch 1430, val loss: 0.9745299220085144
Epoch 1440, training loss: 907.2698364257812 = 0.9711977243423462 + 100.0 * 9.062986373901367
Epoch 1440, val loss: 0.9731550216674805
Epoch 1450, training loss: 908.2373657226562 = 0.9697936773300171 + 100.0 * 9.072675704956055
Epoch 1450, val loss: 0.9717791080474854
Epoch 1460, training loss: 907.8399047851562 = 0.9683433771133423 + 100.0 * 9.068716049194336
Epoch 1460, val loss: 0.9703382849693298
Epoch 1470, training loss: 907.894775390625 = 0.9669238328933716 + 100.0 * 9.069278717041016
Epoch 1470, val loss: 0.9689227342605591
Epoch 1480, training loss: 908.4723510742188 = 0.9654991030693054 + 100.0 * 9.075068473815918
Epoch 1480, val loss: 0.9675011038780212
Epoch 1490, training loss: 908.8640747070312 = 0.9640809297561646 + 100.0 * 9.078999519348145
Epoch 1490, val loss: 0.9661070704460144
Epoch 1500, training loss: 908.7703857421875 = 0.9626140594482422 + 100.0 * 9.078078269958496
Epoch 1500, val loss: 0.9646692276000977
Epoch 1510, training loss: 909.3333740234375 = 0.9612081050872803 + 100.0 * 9.083722114562988
Epoch 1510, val loss: 0.9632571935653687
Epoch 1520, training loss: 908.692138671875 = 0.9597145915031433 + 100.0 * 9.077323913574219
Epoch 1520, val loss: 0.9617793560028076
Epoch 1530, training loss: 909.33642578125 = 0.9582963585853577 + 100.0 * 9.083781242370605
Epoch 1530, val loss: 0.9603835344314575
Epoch 1540, training loss: 909.62939453125 = 0.9568396806716919 + 100.0 * 9.086725234985352
Epoch 1540, val loss: 0.9589472413063049
Epoch 1550, training loss: 909.7604370117188 = 0.9553807973861694 + 100.0 * 9.088050842285156
Epoch 1550, val loss: 0.9574764966964722
Epoch 1560, training loss: 909.9120483398438 = 0.9538969993591309 + 100.0 * 9.089581489562988
Epoch 1560, val loss: 0.9560317397117615
Epoch 1570, training loss: 910.2976684570312 = 0.9524447917938232 + 100.0 * 9.093452453613281
Epoch 1570, val loss: 0.9545769691467285
Epoch 1580, training loss: 910.4766235351562 = 0.9509382843971252 + 100.0 * 9.095256805419922
Epoch 1580, val loss: 0.9530813097953796
Epoch 1590, training loss: 910.2173461914062 = 0.9494383931159973 + 100.0 * 9.092679023742676
Epoch 1590, val loss: 0.9515862464904785
Epoch 1600, training loss: 910.566650390625 = 0.9479996562004089 + 100.0 * 9.096186637878418
Epoch 1600, val loss: 0.9501350522041321
Epoch 1610, training loss: 910.7708740234375 = 0.9465032815933228 + 100.0 * 9.098243713378906
Epoch 1610, val loss: 0.9486579895019531
Epoch 1620, training loss: 910.7813720703125 = 0.9450356960296631 + 100.0 * 9.098363876342773
Epoch 1620, val loss: 0.9471877813339233
Epoch 1630, training loss: 910.5316772460938 = 0.9435787200927734 + 100.0 * 9.095880508422852
Epoch 1630, val loss: 0.9457380175590515
Epoch 1640, training loss: 910.781494140625 = 0.942075252532959 + 100.0 * 9.098394393920898
Epoch 1640, val loss: 0.9442505836486816
Epoch 1650, training loss: 910.5557861328125 = 0.9405328631401062 + 100.0 * 9.096152305603027
Epoch 1650, val loss: 0.9427253007888794
Epoch 1660, training loss: 911.2450561523438 = 0.9390615820884705 + 100.0 * 9.103059768676758
Epoch 1660, val loss: 0.9412807822227478
Epoch 1670, training loss: 911.2996826171875 = 0.9375354647636414 + 100.0 * 9.103621482849121
Epoch 1670, val loss: 0.9397494792938232
Epoch 1680, training loss: 911.76318359375 = 0.9360510110855103 + 100.0 * 9.108271598815918
Epoch 1680, val loss: 0.9382762908935547
Epoch 1690, training loss: 911.7131958007812 = 0.9345462918281555 + 100.0 * 9.107786178588867
Epoch 1690, val loss: 0.9367658495903015
Epoch 1700, training loss: 911.829833984375 = 0.9330063462257385 + 100.0 * 9.108968734741211
Epoch 1700, val loss: 0.9352552890777588
Epoch 1710, training loss: 912.0104370117188 = 0.9314982891082764 + 100.0 * 9.11078929901123
Epoch 1710, val loss: 0.9337345361709595
Epoch 1720, training loss: 911.8133544921875 = 0.9299578666687012 + 100.0 * 9.108834266662598
Epoch 1720, val loss: 0.9322164058685303
Epoch 1730, training loss: 912.167724609375 = 0.9284382462501526 + 100.0 * 9.11239242553711
Epoch 1730, val loss: 0.9307125210762024
Epoch 1740, training loss: 912.5015258789062 = 0.9269289374351501 + 100.0 * 9.115745544433594
Epoch 1740, val loss: 0.9291907548904419
Epoch 1750, training loss: 912.0635986328125 = 0.9253605008125305 + 100.0 * 9.111382484436035
Epoch 1750, val loss: 0.9276100993156433
Epoch 1760, training loss: 911.936279296875 = 0.9238027334213257 + 100.0 * 9.110124588012695
Epoch 1760, val loss: 0.9260650873184204
Epoch 1770, training loss: 912.1548461914062 = 0.9222918748855591 + 100.0 * 9.112325668334961
Epoch 1770, val loss: 0.9245631098747253
Epoch 1780, training loss: 912.8773193359375 = 0.9208219051361084 + 100.0 * 9.1195650100708
Epoch 1780, val loss: 0.9230844378471375
Epoch 1790, training loss: 912.8970947265625 = 0.9192872047424316 + 100.0 * 9.11977767944336
Epoch 1790, val loss: 0.9215695261955261
Epoch 1800, training loss: 913.1260375976562 = 0.9177687168121338 + 100.0 * 9.122082710266113
Epoch 1800, val loss: 0.9200418591499329
Epoch 1810, training loss: 913.247314453125 = 0.9162178635597229 + 100.0 * 9.123311042785645
Epoch 1810, val loss: 0.9184931516647339
Epoch 1820, training loss: 913.307373046875 = 0.9146628379821777 + 100.0 * 9.123927116394043
Epoch 1820, val loss: 0.9169387817382812
Epoch 1830, training loss: 913.3450317382812 = 0.9131312370300293 + 100.0 * 9.124319076538086
Epoch 1830, val loss: 0.9154151082038879
Epoch 1840, training loss: 913.5224609375 = 0.9115726947784424 + 100.0 * 9.12610912322998
Epoch 1840, val loss: 0.9138697385787964
Epoch 1850, training loss: 913.5477905273438 = 0.9100843071937561 + 100.0 * 9.12637710571289
Epoch 1850, val loss: 0.9123541712760925
Epoch 1860, training loss: 913.6318359375 = 0.9085578918457031 + 100.0 * 9.127232551574707
Epoch 1860, val loss: 0.910821259021759
Epoch 1870, training loss: 913.715087890625 = 0.9069801568984985 + 100.0 * 9.128081321716309
Epoch 1870, val loss: 0.909277081489563
Epoch 1880, training loss: 913.9564208984375 = 0.9054867625236511 + 100.0 * 9.130509376525879
Epoch 1880, val loss: 0.9077581763267517
Epoch 1890, training loss: 914.3784790039062 = 0.9039696455001831 + 100.0 * 9.134744644165039
Epoch 1890, val loss: 0.9062477350234985
Epoch 1900, training loss: 913.930908203125 = 0.9023787975311279 + 100.0 * 9.130285263061523
Epoch 1900, val loss: 0.9046341776847839
Epoch 1910, training loss: 913.4683837890625 = 0.9008682370185852 + 100.0 * 9.125675201416016
Epoch 1910, val loss: 0.9031365513801575
Epoch 1920, training loss: 913.8984375 = 0.8993450403213501 + 100.0 * 9.129990577697754
Epoch 1920, val loss: 0.9016424417495728
Epoch 1930, training loss: 914.4758911132812 = 0.8978695273399353 + 100.0 * 9.135780334472656
Epoch 1930, val loss: 0.9001482725143433
Epoch 1940, training loss: 914.6046752929688 = 0.8963638544082642 + 100.0 * 9.137083053588867
Epoch 1940, val loss: 0.8986519575119019
Epoch 1950, training loss: 914.8773803710938 = 0.894872784614563 + 100.0 * 9.139824867248535
Epoch 1950, val loss: 0.8971397876739502
Epoch 1960, training loss: 915.0725708007812 = 0.893369734287262 + 100.0 * 9.141792297363281
Epoch 1960, val loss: 0.8956328630447388
Epoch 1970, training loss: 914.7465209960938 = 0.8918187022209167 + 100.0 * 9.13854694366455
Epoch 1970, val loss: 0.8941057324409485
Epoch 1980, training loss: 914.9775390625 = 0.8903311491012573 + 100.0 * 9.14087200164795
Epoch 1980, val loss: 0.8926087021827698
Epoch 1990, training loss: 915.0924682617188 = 0.8888378739356995 + 100.0 * 9.142036437988281
Epoch 1990, val loss: 0.8911022543907166
Epoch 2000, training loss: 915.570556640625 = 0.8873494267463684 + 100.0 * 9.146832466125488
Epoch 2000, val loss: 0.889604389667511
Epoch 2010, training loss: 915.2418212890625 = 0.8858039379119873 + 100.0 * 9.143560409545898
Epoch 2010, val loss: 0.8881005644798279
Epoch 2020, training loss: 914.95361328125 = 0.884303867816925 + 100.0 * 9.140693664550781
Epoch 2020, val loss: 0.8865557909011841
Epoch 2030, training loss: 915.04833984375 = 0.8827968835830688 + 100.0 * 9.141654968261719
Epoch 2030, val loss: 0.8850833773612976
Epoch 2040, training loss: 915.67529296875 = 0.8813576102256775 + 100.0 * 9.147939682006836
Epoch 2040, val loss: 0.8836125731468201
Epoch 2050, training loss: 916.2685546875 = 0.8798757791519165 + 100.0 * 9.153886795043945
Epoch 2050, val loss: 0.8821280002593994
Epoch 2060, training loss: 915.94287109375 = 0.8783761858940125 + 100.0 * 9.15064525604248
Epoch 2060, val loss: 0.8806208968162537
Epoch 2070, training loss: 916.1319580078125 = 0.8768760561943054 + 100.0 * 9.15255069732666
Epoch 2070, val loss: 0.8791311979293823
Epoch 2080, training loss: 916.1903076171875 = 0.8753852844238281 + 100.0 * 9.153149604797363
Epoch 2080, val loss: 0.8776146769523621
Epoch 2090, training loss: 915.3147583007812 = 0.8738307952880859 + 100.0 * 9.1444091796875
Epoch 2090, val loss: 0.8760634660720825
Epoch 2100, training loss: 915.6240844726562 = 0.8723475337028503 + 100.0 * 9.147517204284668
Epoch 2100, val loss: 0.8745793700218201
Epoch 2110, training loss: 916.2299194335938 = 0.8709423542022705 + 100.0 * 9.153589248657227
Epoch 2110, val loss: 0.8731815218925476
Epoch 2120, training loss: 916.8115844726562 = 0.8695133328437805 + 100.0 * 9.15942096710205
Epoch 2120, val loss: 0.871752917766571
Epoch 2130, training loss: 916.6126098632812 = 0.8680244088172913 + 100.0 * 9.157445907592773
Epoch 2130, val loss: 0.8702505826950073
Epoch 2140, training loss: 916.8486328125 = 0.8665742874145508 + 100.0 * 9.159820556640625
Epoch 2140, val loss: 0.8688057661056519
Epoch 2150, training loss: 917.0206298828125 = 0.8651198148727417 + 100.0 * 9.161555290222168
Epoch 2150, val loss: 0.8673444390296936
Epoch 2160, training loss: 917.262939453125 = 0.8636724948883057 + 100.0 * 9.163992881774902
Epoch 2160, val loss: 0.8658924698829651
Epoch 2170, training loss: 917.3759155273438 = 0.8621975779533386 + 100.0 * 9.16513729095459
Epoch 2170, val loss: 0.8644441962242126
Epoch 2180, training loss: 917.189208984375 = 0.8607827425003052 + 100.0 * 9.163284301757812
Epoch 2180, val loss: 0.862960934638977
Epoch 2190, training loss: 916.8317260742188 = 0.859290599822998 + 100.0 * 9.159724235534668
Epoch 2190, val loss: 0.8615043759346008
Epoch 2200, training loss: 916.1063842773438 = 0.857806384563446 + 100.0 * 9.152485847473145
Epoch 2200, val loss: 0.8600500822067261
Epoch 2210, training loss: 916.258544921875 = 0.8564339876174927 + 100.0 * 9.154021263122559
Epoch 2210, val loss: 0.8586539626121521
Epoch 2220, training loss: 917.0974731445312 = 0.8550558686256409 + 100.0 * 9.162424087524414
Epoch 2220, val loss: 0.85727459192276
Epoch 2230, training loss: 917.6646118164062 = 0.8536581993103027 + 100.0 * 9.168109893798828
Epoch 2230, val loss: 0.8558744192123413
Epoch 2240, training loss: 917.61865234375 = 0.8521974086761475 + 100.0 * 9.167664527893066
Epoch 2240, val loss: 0.854425311088562
Epoch 2250, training loss: 917.4445190429688 = 0.8507533669471741 + 100.0 * 9.165937423706055
Epoch 2250, val loss: 0.8529921770095825
Epoch 2260, training loss: 917.6194458007812 = 0.8493707776069641 + 100.0 * 9.16770076751709
Epoch 2260, val loss: 0.8515843749046326
Epoch 2270, training loss: 917.8834228515625 = 0.8479565978050232 + 100.0 * 9.170354843139648
Epoch 2270, val loss: 0.8501790761947632
Epoch 2280, training loss: 917.3657836914062 = 0.8465179204940796 + 100.0 * 9.165192604064941
Epoch 2280, val loss: 0.8487375378608704
Epoch 2290, training loss: 917.5684814453125 = 0.8451341986656189 + 100.0 * 9.16723346710205
Epoch 2290, val loss: 0.8473469614982605
Epoch 2300, training loss: 917.8671875 = 0.8437708616256714 + 100.0 * 9.170234680175781
Epoch 2300, val loss: 0.8459895253181458
Epoch 2310, training loss: 918.172607421875 = 0.8423911333084106 + 100.0 * 9.173301696777344
Epoch 2310, val loss: 0.8446158766746521
Epoch 2320, training loss: 918.0858154296875 = 0.840977132320404 + 100.0 * 9.17244815826416
Epoch 2320, val loss: 0.8431915640830994
Epoch 2330, training loss: 918.2879028320312 = 0.8396252393722534 + 100.0 * 9.174483299255371
Epoch 2330, val loss: 0.8418396711349487
Epoch 2340, training loss: 918.7269897460938 = 0.8382769823074341 + 100.0 * 9.178887367248535
Epoch 2340, val loss: 0.8404761552810669
Epoch 2350, training loss: 918.5674438476562 = 0.8368703126907349 + 100.0 * 9.177306175231934
Epoch 2350, val loss: 0.8391079902648926
Epoch 2360, training loss: 918.527099609375 = 0.8355280160903931 + 100.0 * 9.176916122436523
Epoch 2360, val loss: 0.8377339243888855
Epoch 2370, training loss: 918.704833984375 = 0.8342024683952332 + 100.0 * 9.178706169128418
Epoch 2370, val loss: 0.8363779187202454
Epoch 2380, training loss: 918.945556640625 = 0.832832396030426 + 100.0 * 9.181127548217773
Epoch 2380, val loss: 0.8350555896759033
Epoch 2390, training loss: 918.5745239257812 = 0.8314638137817383 + 100.0 * 9.177430152893066
Epoch 2390, val loss: 0.8336547613143921
Epoch 2400, training loss: 916.41796875 = 0.8299027681350708 + 100.0 * 9.15588092803955
Epoch 2400, val loss: 0.8320351243019104
Epoch 2410, training loss: 916.4043579101562 = 0.8286733031272888 + 100.0 * 9.155756950378418
Epoch 2410, val loss: 0.8308476209640503
Epoch 2420, training loss: 915.9854736328125 = 0.8273692727088928 + 100.0 * 9.151580810546875
Epoch 2420, val loss: 0.8295952677726746
Epoch 2430, training loss: 916.6671142578125 = 0.8261777758598328 + 100.0 * 9.158409118652344
Epoch 2430, val loss: 0.828454852104187
Epoch 2440, training loss: 917.6720581054688 = 0.8249241709709167 + 100.0 * 9.168471336364746
Epoch 2440, val loss: 0.8272045850753784
Epoch 2450, training loss: 918.3550415039062 = 0.8236411809921265 + 100.0 * 9.175313949584961
Epoch 2450, val loss: 0.8259214162826538
Epoch 2460, training loss: 918.9705200195312 = 0.8222598433494568 + 100.0 * 9.181482315063477
Epoch 2460, val loss: 0.8245296478271484
Epoch 2470, training loss: 919.585205078125 = 0.8207848072052002 + 100.0 * 9.187644004821777
Epoch 2470, val loss: 0.8230457305908203
Epoch 2480, training loss: 919.3349609375 = 0.8190926313400269 + 100.0 * 9.185158729553223
Epoch 2480, val loss: 0.8213087320327759
Epoch 2490, training loss: 919.4944458007812 = 0.8173137903213501 + 100.0 * 9.186771392822266
Epoch 2490, val loss: 0.8194844722747803
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.6478260869565218
0.8122871839455191
The final CL Acc:0.66507, 0.03973, The final GNN Acc:0.81354, 0.00101
raw graph: torch.Size([2, 88648])
add edge: torch.Size([2, 110796])
remove edge: torch.Size([2, 66816])
updated graph: torch.Size([2, 88964])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 1017.3600463867188 = 1.0986114740371704 + 100.0 * 10.162613868713379
Epoch 0, val loss: 1.098612904548645
Epoch 10, training loss: 976.3333129882812 = 1.0986114740371704 + 100.0 * 9.752346992492676
Epoch 10, val loss: 1.098612904548645
Epoch 20, training loss: 956.41650390625 = 1.0986114740371704 + 100.0 * 9.553178787231445
Epoch 20, val loss: 1.098612904548645
Epoch 30, training loss: 941.7434692382812 = 1.0986114740371704 + 100.0 * 9.406448364257812
Epoch 30, val loss: 1.098612904548645
Epoch 40, training loss: 930.498291015625 = 1.0986114740371704 + 100.0 * 9.293996810913086
Epoch 40, val loss: 1.098612904548645
Epoch 50, training loss: 921.2926635742188 = 1.0986114740371704 + 100.0 * 9.201940536499023
Epoch 50, val loss: 1.098612904548645
Epoch 60, training loss: 913.6720581054688 = 1.0986114740371704 + 100.0 * 9.125734329223633
Epoch 60, val loss: 1.098612904548645
Epoch 70, training loss: 907.0728149414062 = 1.0986114740371704 + 100.0 * 9.059741973876953
Epoch 70, val loss: 1.098612904548645
Epoch 80, training loss: 901.653564453125 = 1.0986114740371704 + 100.0 * 9.005549430847168
Epoch 80, val loss: 1.098612904548645
Epoch 90, training loss: 897.1416625976562 = 1.0986114740371704 + 100.0 * 8.960430145263672
Epoch 90, val loss: 1.098612904548645
Epoch 100, training loss: 893.323486328125 = 1.0986114740371704 + 100.0 * 8.922248840332031
Epoch 100, val loss: 1.098612904548645
Epoch 110, training loss: 890.1932983398438 = 1.0986114740371704 + 100.0 * 8.890946388244629
Epoch 110, val loss: 1.098612904548645
Epoch 120, training loss: 887.5081176757812 = 1.0986114740371704 + 100.0 * 8.864094734191895
Epoch 120, val loss: 1.098612904548645
Epoch 130, training loss: 885.206787109375 = 1.0986114740371704 + 100.0 * 8.841081619262695
Epoch 130, val loss: 1.098612904548645
Epoch 140, training loss: 883.1404418945312 = 1.0986114740371704 + 100.0 * 8.820418357849121
Epoch 140, val loss: 1.098612904548645
Epoch 150, training loss: 881.5457153320312 = 1.0986114740371704 + 100.0 * 8.804471015930176
Epoch 150, val loss: 1.098612904548645
Epoch 160, training loss: 880.2326049804688 = 1.0986114740371704 + 100.0 * 8.791339874267578
Epoch 160, val loss: 1.098612904548645
Epoch 170, training loss: 878.4930419921875 = 1.0986114740371704 + 100.0 * 8.773943901062012
Epoch 170, val loss: 1.098612904548645
Epoch 180, training loss: 877.3422241210938 = 1.0986114740371704 + 100.0 * 8.762435913085938
Epoch 180, val loss: 1.098612904548645
Epoch 190, training loss: 876.3245849609375 = 1.0986114740371704 + 100.0 * 8.752259254455566
Epoch 190, val loss: 1.098612904548645
Epoch 200, training loss: 875.5818481445312 = 1.0986114740371704 + 100.0 * 8.744832038879395
Epoch 200, val loss: 1.098612904548645
Epoch 210, training loss: 875.1380615234375 = 1.0986114740371704 + 100.0 * 8.740394592285156
Epoch 210, val loss: 1.098612904548645
Epoch 220, training loss: 873.6112670898438 = 1.0986114740371704 + 100.0 * 8.725126266479492
Epoch 220, val loss: 1.098612904548645
Epoch 230, training loss: 873.460205078125 = 1.0986114740371704 + 100.0 * 8.723615646362305
Epoch 230, val loss: 1.098612904548645
Epoch 240, training loss: 872.9494018554688 = 1.0986114740371704 + 100.0 * 8.718507766723633
Epoch 240, val loss: 1.098612904548645
Epoch 250, training loss: 873.309326171875 = 1.0986114740371704 + 100.0 * 8.72210693359375
Epoch 250, val loss: 1.098612904548645
Epoch 260, training loss: 873.09033203125 = 1.0986114740371704 + 100.0 * 8.719917297363281
Epoch 260, val loss: 1.098612904548645
Epoch 270, training loss: 871.3359985351562 = 1.0986114740371704 + 100.0 * 8.702373504638672
Epoch 270, val loss: 1.098612904548645
Epoch 280, training loss: 871.5716552734375 = 1.0986114740371704 + 100.0 * 8.704730033874512
Epoch 280, val loss: 1.098612904548645
Epoch 290, training loss: 870.834228515625 = 1.0986114740371704 + 100.0 * 8.697356224060059
Epoch 290, val loss: 1.098612904548645
Epoch 300, training loss: 870.5828857421875 = 1.0986114740371704 + 100.0 * 8.694842338562012
Epoch 300, val loss: 1.098612904548645
Epoch 310, training loss: 870.659423828125 = 1.0986114740371704 + 100.0 * 8.695608139038086
Epoch 310, val loss: 1.098612904548645
Epoch 320, training loss: 870.53564453125 = 1.0986114740371704 + 100.0 * 8.69437026977539
Epoch 320, val loss: 1.098612904548645
Epoch 330, training loss: 870.2825317382812 = 1.0986114740371704 + 100.0 * 8.691839218139648
Epoch 330, val loss: 1.098612904548645
Epoch 340, training loss: 870.1348266601562 = 1.0986114740371704 + 100.0 * 8.690361976623535
Epoch 340, val loss: 1.098612904548645
Epoch 350, training loss: 870.3558959960938 = 1.0986114740371704 + 100.0 * 8.692572593688965
Epoch 350, val loss: 1.098612904548645
Epoch 360, training loss: 870.5350952148438 = 1.0986114740371704 + 100.0 * 8.694364547729492
Epoch 360, val loss: 1.0986127853393555
Epoch 370, training loss: 870.697509765625 = 1.0986114740371704 + 100.0 * 8.695988655090332
Epoch 370, val loss: 1.0986127853393555
Epoch 380, training loss: 870.70361328125 = 1.0986114740371704 + 100.0 * 8.696049690246582
Epoch 380, val loss: 1.098612904548645
Epoch 390, training loss: 870.6094970703125 = 1.0986114740371704 + 100.0 * 8.695108413696289
Epoch 390, val loss: 1.098612904548645
Epoch 400, training loss: 870.5066528320312 = 1.0986114740371704 + 100.0 * 8.694080352783203
Epoch 400, val loss: 1.098612904548645
Epoch 410, training loss: 870.8428344726562 = 1.0986114740371704 + 100.0 * 8.697442054748535
Epoch 410, val loss: 1.098612904548645
Epoch 420, training loss: 870.8089599609375 = 1.0986114740371704 + 100.0 * 8.697103500366211
Epoch 420, val loss: 1.098612904548645
Epoch 430, training loss: 870.3023071289062 = 1.0986114740371704 + 100.0 * 8.692036628723145
Epoch 430, val loss: 1.098612904548645
Epoch 440, training loss: 870.71875 = 1.0986114740371704 + 100.0 * 8.69620132446289
Epoch 440, val loss: 1.098612904548645
Epoch 450, training loss: 870.7255249023438 = 1.0986114740371704 + 100.0 * 8.696269035339355
Epoch 450, val loss: 1.098612904548645
Epoch 460, training loss: 870.8289794921875 = 1.0986114740371704 + 100.0 * 8.697303771972656
Epoch 460, val loss: 1.098612904548645
Epoch 470, training loss: 871.0130004882812 = 1.0986114740371704 + 100.0 * 8.699143409729004
Epoch 470, val loss: 1.098612904548645
Epoch 480, training loss: 871.4014892578125 = 1.0986114740371704 + 100.0 * 8.703028678894043
Epoch 480, val loss: 1.098612904548645
Epoch 490, training loss: 871.1256713867188 = 1.0986114740371704 + 100.0 * 8.700270652770996
Epoch 490, val loss: 1.098612904548645
Epoch 500, training loss: 871.4620361328125 = 1.0986114740371704 + 100.0 * 8.703634262084961
Epoch 500, val loss: 1.098612904548645
Epoch 510, training loss: 871.6638793945312 = 1.0986114740371704 + 100.0 * 8.705652236938477
Epoch 510, val loss: 1.098612904548645
Epoch 520, training loss: 871.4287109375 = 1.0986114740371704 + 100.0 * 8.703300476074219
Epoch 520, val loss: 1.098612904548645
Epoch 530, training loss: 871.4892578125 = 1.0986114740371704 + 100.0 * 8.703906059265137
Epoch 530, val loss: 1.098612904548645
Epoch 540, training loss: 871.7000732421875 = 1.0986114740371704 + 100.0 * 8.706014633178711
Epoch 540, val loss: 1.098612904548645
Epoch 550, training loss: 872.1521606445312 = 1.0986114740371704 + 100.0 * 8.710535049438477
Epoch 550, val loss: 1.098612904548645
Epoch 560, training loss: 871.9754028320312 = 1.0986114740371704 + 100.0 * 8.708767890930176
Epoch 560, val loss: 1.098612904548645
Epoch 570, training loss: 872.1067504882812 = 1.0986114740371704 + 100.0 * 8.710081100463867
Epoch 570, val loss: 1.098612904548645
Epoch 580, training loss: 872.262939453125 = 1.0986114740371704 + 100.0 * 8.71164321899414
Epoch 580, val loss: 1.098612904548645
Epoch 590, training loss: 872.6417236328125 = 1.0986114740371704 + 100.0 * 8.715431213378906
Epoch 590, val loss: 1.098612904548645
Epoch 600, training loss: 872.6345825195312 = 1.0986114740371704 + 100.0 * 8.715359687805176
Epoch 600, val loss: 1.098612904548645
Epoch 610, training loss: 872.8175048828125 = 1.0986114740371704 + 100.0 * 8.717188835144043
Epoch 610, val loss: 1.098612904548645
Epoch 620, training loss: 872.950927734375 = 1.0986114740371704 + 100.0 * 8.718523025512695
Epoch 620, val loss: 1.098612904548645
Epoch 630, training loss: 873.2796630859375 = 1.0986114740371704 + 100.0 * 8.721810340881348
Epoch 630, val loss: 1.098612904548645
Epoch 640, training loss: 872.9608764648438 = 1.0986114740371704 + 100.0 * 8.718622207641602
Epoch 640, val loss: 1.098612904548645
Epoch 650, training loss: 873.0698852539062 = 1.0986114740371704 + 100.0 * 8.719712257385254
Epoch 650, val loss: 1.098612904548645
Epoch 660, training loss: 873.5042724609375 = 1.0986114740371704 + 100.0 * 8.724056243896484
Epoch 660, val loss: 1.098612904548645
Epoch 670, training loss: 873.7708740234375 = 1.0986114740371704 + 100.0 * 8.726722717285156
Epoch 670, val loss: 1.098612904548645
Epoch 680, training loss: 873.693115234375 = 1.0986114740371704 + 100.0 * 8.725944519042969
Epoch 680, val loss: 1.098612904548645
Epoch 690, training loss: 874.2979125976562 = 1.0986114740371704 + 100.0 * 8.731992721557617
Epoch 690, val loss: 1.098612904548645
Epoch 700, training loss: 874.00634765625 = 1.0986114740371704 + 100.0 * 8.729077339172363
Epoch 700, val loss: 1.098612904548645
Epoch 710, training loss: 874.577880859375 = 1.0986114740371704 + 100.0 * 8.734792709350586
Epoch 710, val loss: 1.098612904548645
Epoch 720, training loss: 874.8628540039062 = 1.0986114740371704 + 100.0 * 8.737642288208008
Epoch 720, val loss: 1.098612904548645
Epoch 730, training loss: 874.5895385742188 = 1.0986114740371704 + 100.0 * 8.734909057617188
Epoch 730, val loss: 1.098612904548645
Epoch 740, training loss: 874.8021850585938 = 1.0986114740371704 + 100.0 * 8.737035751342773
Epoch 740, val loss: 1.098612904548645
Epoch 750, training loss: 875.2037353515625 = 1.0986114740371704 + 100.0 * 8.741050720214844
Epoch 750, val loss: 1.098612904548645
Epoch 760, training loss: 874.95458984375 = 1.0986114740371704 + 100.0 * 8.73855972290039
Epoch 760, val loss: 1.098612904548645
Epoch 770, training loss: 875.3546752929688 = 1.0986114740371704 + 100.0 * 8.742560386657715
Epoch 770, val loss: 1.098612904548645
Epoch 780, training loss: 875.6902465820312 = 1.0986114740371704 + 100.0 * 8.745916366577148
Epoch 780, val loss: 1.098612904548645
Epoch 790, training loss: 875.7442626953125 = 1.0986114740371704 + 100.0 * 8.746456146240234
Epoch 790, val loss: 1.098612904548645
Epoch 800, training loss: 876.0540161132812 = 1.0986114740371704 + 100.0 * 8.749553680419922
Epoch 800, val loss: 1.098612904548645
Epoch 810, training loss: 876.3203735351562 = 1.0986114740371704 + 100.0 * 8.752217292785645
Epoch 810, val loss: 1.098612904548645
Epoch 820, training loss: 876.6619873046875 = 1.0986114740371704 + 100.0 * 8.755633354187012
Epoch 820, val loss: 1.098612904548645
Epoch 830, training loss: 876.60009765625 = 1.0986114740371704 + 100.0 * 8.755014419555664
Epoch 830, val loss: 1.098612904548645
Epoch 840, training loss: 876.9254760742188 = 1.0986114740371704 + 100.0 * 8.758268356323242
Epoch 840, val loss: 1.098612904548645
Epoch 850, training loss: 877.1748657226562 = 1.0986114740371704 + 100.0 * 8.760762214660645
Epoch 850, val loss: 1.098612904548645
Epoch 860, training loss: 876.85498046875 = 1.0986114740371704 + 100.0 * 8.757563591003418
Epoch 860, val loss: 1.098612904548645
Epoch 870, training loss: 877.3164672851562 = 1.0986114740371704 + 100.0 * 8.762178421020508
Epoch 870, val loss: 1.098612904548645
Epoch 880, training loss: 877.5965576171875 = 1.0986114740371704 + 100.0 * 8.764979362487793
Epoch 880, val loss: 1.098612904548645
Epoch 890, training loss: 878.0313720703125 = 1.0986114740371704 + 100.0 * 8.769327163696289
Epoch 890, val loss: 1.098612904548645
Epoch 900, training loss: 877.7754516601562 = 1.0986114740371704 + 100.0 * 8.766768455505371
Epoch 900, val loss: 1.098612904548645
Epoch 910, training loss: 877.9296875 = 1.0986114740371704 + 100.0 * 8.768310546875
Epoch 910, val loss: 1.098612904548645
Epoch 920, training loss: 878.234375 = 1.0986114740371704 + 100.0 * 8.771357536315918
Epoch 920, val loss: 1.098612904548645
Epoch 930, training loss: 878.6566772460938 = 1.0986114740371704 + 100.0 * 8.775580406188965
Epoch 930, val loss: 1.098612904548645
Epoch 940, training loss: 878.7716674804688 = 1.0986114740371704 + 100.0 * 8.77673053741455
Epoch 940, val loss: 1.098612904548645
Epoch 950, training loss: 879.135009765625 = 1.0986114740371704 + 100.0 * 8.780364036560059
Epoch 950, val loss: 1.098612904548645
Epoch 960, training loss: 879.1826171875 = 1.0986114740371704 + 100.0 * 8.780839920043945
Epoch 960, val loss: 1.098612904548645
Epoch 970, training loss: 879.4097900390625 = 1.0986114740371704 + 100.0 * 8.783111572265625
Epoch 970, val loss: 1.098612904548645
Epoch 980, training loss: 879.5617065429688 = 1.0986114740371704 + 100.0 * 8.78463077545166
Epoch 980, val loss: 1.098612904548645
Epoch 990, training loss: 879.7766723632812 = 1.0986114740371704 + 100.0 * 8.78678035736084
Epoch 990, val loss: 1.098612904548645
Epoch 1000, training loss: 880.0128784179688 = 1.0986114740371704 + 100.0 * 8.789142608642578
Epoch 1000, val loss: 1.098612904548645
Epoch 1010, training loss: 880.1920166015625 = 1.0986114740371704 + 100.0 * 8.790933609008789
Epoch 1010, val loss: 1.098612904548645
Epoch 1020, training loss: 880.4736328125 = 1.0986114740371704 + 100.0 * 8.793749809265137
Epoch 1020, val loss: 1.098612904548645
Epoch 1030, training loss: 880.6793212890625 = 1.0986114740371704 + 100.0 * 8.795806884765625
Epoch 1030, val loss: 1.098612904548645
Epoch 1040, training loss: 880.5046997070312 = 1.0986114740371704 + 100.0 * 8.794060707092285
Epoch 1040, val loss: 1.098612904548645
Epoch 1050, training loss: 881.5353393554688 = 1.0986114740371704 + 100.0 * 8.804367065429688
Epoch 1050, val loss: 1.098612904548645
Epoch 1060, training loss: 879.8985595703125 = 1.0986114740371704 + 100.0 * 8.787999153137207
Epoch 1060, val loss: 1.098612904548645
Epoch 1070, training loss: 880.3425903320312 = 1.0986114740371704 + 100.0 * 8.792439460754395
Epoch 1070, val loss: 1.098612904548645
Epoch 1080, training loss: 880.8358154296875 = 1.0986114740371704 + 100.0 * 8.797371864318848
Epoch 1080, val loss: 1.098612904548645
Epoch 1090, training loss: 881.2376708984375 = 1.0986114740371704 + 100.0 * 8.801390647888184
Epoch 1090, val loss: 1.098612904548645
Epoch 1100, training loss: 881.893310546875 = 1.0983445644378662 + 100.0 * 8.807950019836426
Epoch 1100, val loss: 1.098308801651001
Epoch 1110, training loss: 881.9306030273438 = 1.097516417503357 + 100.0 * 8.808330535888672
Epoch 1110, val loss: 1.097571611404419
Epoch 1120, training loss: 882.46337890625 = 1.0967178344726562 + 100.0 * 8.813666343688965
Epoch 1120, val loss: 1.096876621246338
Epoch 1130, training loss: 881.8062133789062 = 1.09599769115448 + 100.0 * 8.80710220336914
Epoch 1130, val loss: 1.0962498188018799
Epoch 1140, training loss: 882.410400390625 = 1.0953253507614136 + 100.0 * 8.813150405883789
Epoch 1140, val loss: 1.0956568717956543
Epoch 1150, training loss: 882.8829345703125 = 1.094664216041565 + 100.0 * 8.817882537841797
Epoch 1150, val loss: 1.0950690507888794
Epoch 1160, training loss: 882.7517700195312 = 1.093990683555603 + 100.0 * 8.816577911376953
Epoch 1160, val loss: 1.0944629907608032
Epoch 1170, training loss: 882.9153442382812 = 1.0932977199554443 + 100.0 * 8.818220138549805
Epoch 1170, val loss: 1.0938411951065063
Epoch 1180, training loss: 882.5240478515625 = 1.092584252357483 + 100.0 * 8.814314842224121
Epoch 1180, val loss: 1.0931953191757202
Epoch 1190, training loss: 882.9346313476562 = 1.0918564796447754 + 100.0 * 8.818428039550781
Epoch 1190, val loss: 1.092529296875
Epoch 1200, training loss: 883.8531494140625 = 1.0911084413528442 + 100.0 * 8.827620506286621
Epoch 1200, val loss: 1.0918422937393188
Epoch 1210, training loss: 884.0943603515625 = 1.0903369188308716 + 100.0 * 8.830039978027344
Epoch 1210, val loss: 1.091134786605835
Epoch 1220, training loss: 883.4225463867188 = 1.0895428657531738 + 100.0 * 8.82332992553711
Epoch 1220, val loss: 1.0904126167297363
Epoch 1230, training loss: 883.2020263671875 = 1.088718056678772 + 100.0 * 8.82113265991211
Epoch 1230, val loss: 1.0896530151367188
Epoch 1240, training loss: 883.6962890625 = 1.0878900289535522 + 100.0 * 8.82608413696289
Epoch 1240, val loss: 1.0888868570327759
Epoch 1250, training loss: 884.4014282226562 = 1.0870438814163208 + 100.0 * 8.833144187927246
Epoch 1250, val loss: 1.0881116390228271
Epoch 1260, training loss: 884.4503173828125 = 1.086174726486206 + 100.0 * 8.833641052246094
Epoch 1260, val loss: 1.087313175201416
Epoch 1270, training loss: 884.4804077148438 = 1.0853010416030884 + 100.0 * 8.833950996398926
Epoch 1270, val loss: 1.086509346961975
Epoch 1280, training loss: 884.7932739257812 = 1.0844203233718872 + 100.0 * 8.837088584899902
Epoch 1280, val loss: 1.0857046842575073
Epoch 1290, training loss: 885.3112182617188 = 1.0835390090942383 + 100.0 * 8.842276573181152
Epoch 1290, val loss: 1.084896445274353
Epoch 1300, training loss: 885.696533203125 = 1.0826239585876465 + 100.0 * 8.846138954162598
Epoch 1300, val loss: 1.0840457677841187
Epoch 1310, training loss: 884.9888916015625 = 1.0817734003067017 + 100.0 * 8.839071273803711
Epoch 1310, val loss: 1.0832908153533936
Epoch 1320, training loss: 882.9842529296875 = 1.0808535814285278 + 100.0 * 8.8190336227417
Epoch 1320, val loss: 1.082444429397583
Epoch 1330, training loss: 883.595458984375 = 1.0799975395202637 + 100.0 * 8.825154304504395
Epoch 1330, val loss: 1.0816658735275269
Epoch 1340, training loss: 884.0255737304688 = 1.0791473388671875 + 100.0 * 8.829463958740234
Epoch 1340, val loss: 1.0808799266815186
Epoch 1350, training loss: 884.8812255859375 = 1.0783107280731201 + 100.0 * 8.838028907775879
Epoch 1350, val loss: 1.0801109075546265
Epoch 1360, training loss: 885.5402221679688 = 1.0774691104888916 + 100.0 * 8.844627380371094
Epoch 1360, val loss: 1.079344391822815
Epoch 1370, training loss: 886.0684204101562 = 1.076640009880066 + 100.0 * 8.8499174118042
Epoch 1370, val loss: 1.078582763671875
Epoch 1380, training loss: 885.7174682617188 = 1.075798511505127 + 100.0 * 8.846416473388672
Epoch 1380, val loss: 1.0778124332427979
Epoch 1390, training loss: 886.2317504882812 = 1.074968695640564 + 100.0 * 8.851568222045898
Epoch 1390, val loss: 1.0770518779754639
Epoch 1400, training loss: 886.9152221679688 = 1.0741510391235352 + 100.0 * 8.858410835266113
Epoch 1400, val loss: 1.0763025283813477
Epoch 1410, training loss: 886.8563842773438 = 1.0733271837234497 + 100.0 * 8.857831001281738
Epoch 1410, val loss: 1.075540542602539
Epoch 1420, training loss: 887.0912475585938 = 1.0725029706954956 + 100.0 * 8.860187530517578
Epoch 1420, val loss: 1.0747944116592407
Epoch 1430, training loss: 887.1441650390625 = 1.071729302406311 + 100.0 * 8.860724449157715
Epoch 1430, val loss: 1.0740302801132202
Epoch 1440, training loss: 887.3975219726562 = 1.0707751512527466 + 100.0 * 8.863266944885254
Epoch 1440, val loss: 1.0732126235961914
Epoch 1450, training loss: 882.5281982421875 = 1.0696929693222046 + 100.0 * 8.814584732055664
Epoch 1450, val loss: 1.0722442865371704
Epoch 1460, training loss: 885.8232421875 = 1.0688306093215942 + 100.0 * 8.847543716430664
Epoch 1460, val loss: 1.0714490413665771
Epoch 1470, training loss: 885.5015869140625 = 1.0678914785385132 + 100.0 * 8.844337463378906
Epoch 1470, val loss: 1.0706042051315308
Epoch 1480, training loss: 885.6293334960938 = 1.066968560218811 + 100.0 * 8.845623970031738
Epoch 1480, val loss: 1.069771409034729
Epoch 1490, training loss: 886.2400512695312 = 1.0660593509674072 + 100.0 * 8.851739883422852
Epoch 1490, val loss: 1.0689433813095093
Epoch 1500, training loss: 887.2921752929688 = 1.0651541948318481 + 100.0 * 8.86227035522461
Epoch 1500, val loss: 1.0681166648864746
Epoch 1510, training loss: 887.7017211914062 = 1.0642309188842773 + 100.0 * 8.866374969482422
Epoch 1510, val loss: 1.0672754049301147
Epoch 1520, training loss: 887.9261474609375 = 1.0632855892181396 + 100.0 * 8.86862850189209
Epoch 1520, val loss: 1.0664042234420776
Epoch 1530, training loss: 888.2562866210938 = 1.0623286962509155 + 100.0 * 8.871939659118652
Epoch 1530, val loss: 1.0655381679534912
Epoch 1540, training loss: 887.9268798828125 = 1.0613504648208618 + 100.0 * 8.86865520477295
Epoch 1540, val loss: 1.0646570920944214
Epoch 1550, training loss: 888.3428344726562 = 1.0603679418563843 + 100.0 * 8.872824668884277
Epoch 1550, val loss: 1.0637683868408203
Epoch 1560, training loss: 888.8745727539062 = 1.0593810081481934 + 100.0 * 8.878151893615723
Epoch 1560, val loss: 1.062886118888855
Epoch 1570, training loss: 888.8121337890625 = 1.0583767890930176 + 100.0 * 8.877537727355957
Epoch 1570, val loss: 1.0619853734970093
Epoch 1580, training loss: 889.1995849609375 = 1.0573680400848389 + 100.0 * 8.88142204284668
Epoch 1580, val loss: 1.0610721111297607
Epoch 1590, training loss: 889.2716064453125 = 1.0563372373580933 + 100.0 * 8.882152557373047
Epoch 1590, val loss: 1.0601483583450317
Epoch 1600, training loss: 889.4002685546875 = 1.0553200244903564 + 100.0 * 8.88344955444336
Epoch 1600, val loss: 1.0592299699783325
Epoch 1610, training loss: 889.781494140625 = 1.054290771484375 + 100.0 * 8.887271881103516
Epoch 1610, val loss: 1.0582952499389648
Epoch 1620, training loss: 889.6886596679688 = 1.0532467365264893 + 100.0 * 8.886354446411133
Epoch 1620, val loss: 1.0573582649230957
Epoch 1630, training loss: 890.140625 = 1.0522024631500244 + 100.0 * 8.890884399414062
Epoch 1630, val loss: 1.056419849395752
Epoch 1640, training loss: 890.3794555664062 = 1.0511555671691895 + 100.0 * 8.893282890319824
Epoch 1640, val loss: 1.0554732084274292
Epoch 1650, training loss: 890.5614013671875 = 1.05009925365448 + 100.0 * 8.895112991333008
Epoch 1650, val loss: 1.054530143737793
Epoch 1660, training loss: 890.7406616210938 = 1.0490390062332153 + 100.0 * 8.896916389465332
Epoch 1660, val loss: 1.0535796880722046
Epoch 1670, training loss: 890.635498046875 = 1.0479629039764404 + 100.0 * 8.895874977111816
Epoch 1670, val loss: 1.0526092052459717
Epoch 1680, training loss: 890.5342407226562 = 1.046883463859558 + 100.0 * 8.89487361907959
Epoch 1680, val loss: 1.0516448020935059
Epoch 1690, training loss: 891.0939331054688 = 1.0458195209503174 + 100.0 * 8.900481224060059
Epoch 1690, val loss: 1.0506900548934937
Epoch 1700, training loss: 891.3615112304688 = 1.0447487831115723 + 100.0 * 8.903167724609375
Epoch 1700, val loss: 1.049727201461792
Epoch 1710, training loss: 891.1891479492188 = 1.0436575412750244 + 100.0 * 8.90145492553711
Epoch 1710, val loss: 1.0487337112426758
Epoch 1720, training loss: 890.7056884765625 = 1.0425583124160767 + 100.0 * 8.896631240844727
Epoch 1720, val loss: 1.0477291345596313
Epoch 1730, training loss: 889.500244140625 = 1.04141366481781 + 100.0 * 8.884588241577148
Epoch 1730, val loss: 1.0467004776000977
Epoch 1740, training loss: 886.6819458007812 = 1.0402612686157227 + 100.0 * 8.856416702270508
Epoch 1740, val loss: 1.0456613302230835
Epoch 1750, training loss: 888.7453002929688 = 1.0391680002212524 + 100.0 * 8.877060890197754
Epoch 1750, val loss: 1.0446969270706177
Epoch 1760, training loss: 890.1041870117188 = 1.0381107330322266 + 100.0 * 8.890661239624023
Epoch 1760, val loss: 1.0437403917312622
Epoch 1770, training loss: 890.144775390625 = 1.0370116233825684 + 100.0 * 8.891077995300293
Epoch 1770, val loss: 1.0427602529525757
Epoch 1780, training loss: 890.4037475585938 = 1.0359158515930176 + 100.0 * 8.893678665161133
Epoch 1780, val loss: 1.0417728424072266
Epoch 1790, training loss: 891.0874633789062 = 1.0348166227340698 + 100.0 * 8.900527000427246
Epoch 1790, val loss: 1.040773868560791
Epoch 1800, training loss: 891.4000244140625 = 1.033690333366394 + 100.0 * 8.903663635253906
Epoch 1800, val loss: 1.0397634506225586
Epoch 1810, training loss: 891.72119140625 = 1.0325778722763062 + 100.0 * 8.906886100769043
Epoch 1810, val loss: 1.038745403289795
Epoch 1820, training loss: 891.89892578125 = 1.031455159187317 + 100.0 * 8.908675193786621
Epoch 1820, val loss: 1.0377432107925415
Epoch 1830, training loss: 892.36181640625 = 1.0303363800048828 + 100.0 * 8.913314819335938
Epoch 1830, val loss: 1.0367332696914673
Epoch 1840, training loss: 892.0908813476562 = 1.0292032957077026 + 100.0 * 8.910616874694824
Epoch 1840, val loss: 1.0356990098953247
Epoch 1850, training loss: 892.0634765625 = 1.0281094312667847 + 100.0 * 8.910353660583496
Epoch 1850, val loss: 1.0347260236740112
Epoch 1860, training loss: 892.2555541992188 = 1.026993751525879 + 100.0 * 8.912285804748535
Epoch 1860, val loss: 1.0337092876434326
Epoch 1870, training loss: 892.5161743164062 = 1.0258547067642212 + 100.0 * 8.914902687072754
Epoch 1870, val loss: 1.032692551612854
Epoch 1880, training loss: 893.3348999023438 = 1.0247511863708496 + 100.0 * 8.923101425170898
Epoch 1880, val loss: 1.0316768884658813
Epoch 1890, training loss: 893.9933471679688 = 1.023636817932129 + 100.0 * 8.929697036743164
Epoch 1890, val loss: 1.030673623085022
Epoch 1900, training loss: 894.2089233398438 = 1.022508978843689 + 100.0 * 8.931863784790039
Epoch 1900, val loss: 1.0296547412872314
Epoch 1910, training loss: 894.3909301757812 = 1.0213775634765625 + 100.0 * 8.933695793151855
Epoch 1910, val loss: 1.0286290645599365
Epoch 1920, training loss: 894.520751953125 = 1.0202438831329346 + 100.0 * 8.935005187988281
Epoch 1920, val loss: 1.027604341506958
Epoch 1930, training loss: 894.5665283203125 = 1.0190943479537964 + 100.0 * 8.935474395751953
Epoch 1930, val loss: 1.0265663862228394
Epoch 1940, training loss: 894.3427124023438 = 1.0179650783538818 + 100.0 * 8.933247566223145
Epoch 1940, val loss: 1.025529384613037
Epoch 1950, training loss: 891.3643188476562 = 1.0166712999343872 + 100.0 * 8.90347671508789
Epoch 1950, val loss: 1.0243589878082275
Epoch 1960, training loss: 891.4567260742188 = 1.015559196472168 + 100.0 * 8.904411315917969
Epoch 1960, val loss: 1.0233458280563354
Epoch 1970, training loss: 892.5101318359375 = 1.0143842697143555 + 100.0 * 8.914957046508789
Epoch 1970, val loss: 1.0222986936569214
Epoch 1980, training loss: 893.599365234375 = 1.013247013092041 + 100.0 * 8.925861358642578
Epoch 1980, val loss: 1.0212781429290771
Epoch 1990, training loss: 894.35205078125 = 1.012147068977356 + 100.0 * 8.933399200439453
Epoch 1990, val loss: 1.0202692747116089
Epoch 2000, training loss: 895.058349609375 = 1.0110094547271729 + 100.0 * 8.940473556518555
Epoch 2000, val loss: 1.0192478895187378
Epoch 2010, training loss: 895.461669921875 = 1.0098649263381958 + 100.0 * 8.944518089294434
Epoch 2010, val loss: 1.0182104110717773
Epoch 2020, training loss: 895.7374877929688 = 1.0087002515792847 + 100.0 * 8.947287559509277
Epoch 2020, val loss: 1.0171644687652588
Epoch 2030, training loss: 895.5752563476562 = 1.0075459480285645 + 100.0 * 8.945676803588867
Epoch 2030, val loss: 1.0161162614822388
Epoch 2040, training loss: 895.7665405273438 = 1.0063927173614502 + 100.0 * 8.947601318359375
Epoch 2040, val loss: 1.0150765180587769
Epoch 2050, training loss: 896.3165893554688 = 1.0052509307861328 + 100.0 * 8.953113555908203
Epoch 2050, val loss: 1.0140490531921387
Epoch 2060, training loss: 896.3877563476562 = 1.0040723085403442 + 100.0 * 8.953836441040039
Epoch 2060, val loss: 1.012997031211853
Epoch 2070, training loss: 896.5182495117188 = 1.0029011964797974 + 100.0 * 8.955153465270996
Epoch 2070, val loss: 1.0119304656982422
Epoch 2080, training loss: 895.5658569335938 = 1.0017333030700684 + 100.0 * 8.94564151763916
Epoch 2080, val loss: 1.0108503103256226
Epoch 2090, training loss: 895.8765258789062 = 1.0005584955215454 + 100.0 * 8.948760032653809
Epoch 2090, val loss: 1.009799599647522
Epoch 2100, training loss: 896.6436767578125 = 0.9993842840194702 + 100.0 * 8.956442832946777
Epoch 2100, val loss: 1.0087478160858154
Epoch 2110, training loss: 897.1319580078125 = 0.9982053637504578 + 100.0 * 8.961337089538574
Epoch 2110, val loss: 1.0076825618743896
Epoch 2120, training loss: 897.2465209960938 = 0.9970186948776245 + 100.0 * 8.962494850158691
Epoch 2120, val loss: 1.0066189765930176
Epoch 2130, training loss: 896.977294921875 = 0.9958065748214722 + 100.0 * 8.95981502532959
Epoch 2130, val loss: 1.005521535873413
Epoch 2140, training loss: 897.1290283203125 = 0.9946303367614746 + 100.0 * 8.961343765258789
Epoch 2140, val loss: 1.004451036453247
Epoch 2150, training loss: 897.1762084960938 = 0.9934408664703369 + 100.0 * 8.961827278137207
Epoch 2150, val loss: 1.0033721923828125
Epoch 2160, training loss: 897.7522583007812 = 0.9922565817832947 + 100.0 * 8.967599868774414
Epoch 2160, val loss: 1.0023162364959717
Epoch 2170, training loss: 897.9369506835938 = 0.9910714626312256 + 100.0 * 8.96945858001709
Epoch 2170, val loss: 1.0012435913085938
Epoch 2180, training loss: 897.661376953125 = 0.9898699522018433 + 100.0 * 8.966714859008789
Epoch 2180, val loss: 1.0001554489135742
Epoch 2190, training loss: 898.1005249023438 = 0.9886895418167114 + 100.0 * 8.971117973327637
Epoch 2190, val loss: 0.9990933537483215
Epoch 2200, training loss: 895.6806640625 = 0.9875277876853943 + 100.0 * 8.946930885314941
Epoch 2200, val loss: 0.9979951977729797
Epoch 2210, training loss: 895.0752563476562 = 0.9862357378005981 + 100.0 * 8.940890312194824
Epoch 2210, val loss: 0.9968547224998474
Epoch 2220, training loss: 896.4197387695312 = 0.9850724339485168 + 100.0 * 8.954346656799316
Epoch 2220, val loss: 0.9958395957946777
Epoch 2230, training loss: 897.1197509765625 = 0.9838499426841736 + 100.0 * 8.961359024047852
Epoch 2230, val loss: 0.994735836982727
Epoch 2240, training loss: 897.9395751953125 = 0.9827032089233398 + 100.0 * 8.969568252563477
Epoch 2240, val loss: 0.9936915040016174
Epoch 2250, training loss: 898.6740112304688 = 0.9815281629562378 + 100.0 * 8.976924896240234
Epoch 2250, val loss: 0.9926341772079468
Epoch 2260, training loss: 899.2376708984375 = 0.9803677797317505 + 100.0 * 8.982573509216309
Epoch 2260, val loss: 0.9915894269943237
Epoch 2270, training loss: 898.8292236328125 = 0.9791697859764099 + 100.0 * 8.978500366210938
Epoch 2270, val loss: 0.9905121922492981
Epoch 2280, training loss: 899.4293823242188 = 0.9779933094978333 + 100.0 * 8.984514236450195
Epoch 2280, val loss: 0.9894528388977051
Epoch 2290, training loss: 899.9871826171875 = 0.9768034815788269 + 100.0 * 8.990103721618652
Epoch 2290, val loss: 0.988395631313324
Epoch 2300, training loss: 900.1730346679688 = 0.975626528263092 + 100.0 * 8.991973876953125
Epoch 2300, val loss: 0.9873367547988892
Epoch 2310, training loss: 900.155517578125 = 0.9744182825088501 + 100.0 * 8.99181079864502
Epoch 2310, val loss: 0.986257016658783
Epoch 2320, training loss: 900.2914428710938 = 0.9732272028923035 + 100.0 * 8.993182182312012
Epoch 2320, val loss: 0.985176682472229
Epoch 2330, training loss: 899.8484497070312 = 0.9720158576965332 + 100.0 * 8.988763809204102
Epoch 2330, val loss: 0.9840932488441467
Epoch 2340, training loss: 900.2496337890625 = 0.9708218574523926 + 100.0 * 8.992788314819336
Epoch 2340, val loss: 0.9830182790756226
Epoch 2350, training loss: 900.7213134765625 = 0.9696382880210876 + 100.0 * 8.997516632080078
Epoch 2350, val loss: 0.9819416999816895
Epoch 2360, training loss: 901.1788330078125 = 0.9684404134750366 + 100.0 * 9.002103805541992
Epoch 2360, val loss: 0.9808685779571533
Epoch 2370, training loss: 900.8223876953125 = 0.9672132730484009 + 100.0 * 8.998551368713379
Epoch 2370, val loss: 0.9797642827033997
Epoch 2380, training loss: 900.9768676757812 = 0.9660072922706604 + 100.0 * 9.00010871887207
Epoch 2380, val loss: 0.9786791801452637
Epoch 2390, training loss: 901.2135009765625 = 0.964810848236084 + 100.0 * 9.002487182617188
Epoch 2390, val loss: 0.9776085019111633
Epoch 2400, training loss: 901.25732421875 = 0.9636197686195374 + 100.0 * 9.002937316894531
Epoch 2400, val loss: 0.9765243530273438
Epoch 2410, training loss: 901.3069458007812 = 0.9624021649360657 + 100.0 * 9.003445625305176
Epoch 2410, val loss: 0.9754405617713928
Epoch 2420, training loss: 901.3455810546875 = 0.9611969590187073 + 100.0 * 9.003844261169434
Epoch 2420, val loss: 0.9743552803993225
Epoch 2430, training loss: 901.4389038085938 = 0.9599905610084534 + 100.0 * 9.004789352416992
Epoch 2430, val loss: 0.973266065120697
Epoch 2440, training loss: 901.8507690429688 = 0.958778977394104 + 100.0 * 9.008919715881348
Epoch 2440, val loss: 0.9721813201904297
Epoch 2450, training loss: 902.0248413085938 = 0.9575636982917786 + 100.0 * 9.010672569274902
Epoch 2450, val loss: 0.9710912108421326
Epoch 2460, training loss: 901.8011474609375 = 0.9563511610031128 + 100.0 * 9.008447647094727
Epoch 2460, val loss: 0.9700039029121399
Epoch 2470, training loss: 901.8088989257812 = 0.9550669193267822 + 100.0 * 9.008538246154785
Epoch 2470, val loss: 0.9688940048217773
Epoch 2480, training loss: 901.4752807617188 = 0.9539511203765869 + 100.0 * 9.005212783813477
Epoch 2480, val loss: 0.967835545539856
Epoch 2490, training loss: 902.1406860351562 = 0.9527264833450317 + 100.0 * 9.011879920959473
Epoch 2490, val loss: 0.9667693972587585
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.5269565217391304
0.8649568934289648
=== training gcn model ===
Epoch 0, training loss: 1016.8930053710938 = 1.1083252429962158 + 100.0 * 10.157846450805664
Epoch 0, val loss: 1.107387900352478
Epoch 10, training loss: 975.1543579101562 = 1.1076796054840088 + 100.0 * 9.740467071533203
Epoch 10, val loss: 1.1067615747451782
Epoch 20, training loss: 954.1441040039062 = 1.1070433855056763 + 100.0 * 9.530370712280273
Epoch 20, val loss: 1.1061418056488037
Epoch 30, training loss: 939.0079956054688 = 1.106360673904419 + 100.0 * 9.379015922546387
Epoch 30, val loss: 1.1054582595825195
Epoch 40, training loss: 927.4463500976562 = 1.1056488752365112 + 100.0 * 9.263406753540039
Epoch 40, val loss: 1.104763388633728
Epoch 50, training loss: 918.2308349609375 = 1.1049420833587646 + 100.0 * 9.171258926391602
Epoch 50, val loss: 1.104073405265808
Epoch 60, training loss: 910.5167236328125 = 1.1042349338531494 + 100.0 * 9.094124794006348
Epoch 60, val loss: 1.103373408317566
Epoch 70, training loss: 904.0477294921875 = 1.1035195589065552 + 100.0 * 9.029441833496094
Epoch 70, val loss: 1.1026729345321655
Epoch 80, training loss: 898.7471313476562 = 1.1027827262878418 + 100.0 * 8.97644329071045
Epoch 80, val loss: 1.1019386053085327
Epoch 90, training loss: 894.22021484375 = 1.102036952972412 + 100.0 * 8.931181907653809
Epoch 90, val loss: 1.1012012958526611
Epoch 100, training loss: 890.5377197265625 = 1.1012535095214844 + 100.0 * 8.894364356994629
Epoch 100, val loss: 1.1004308462142944
Epoch 110, training loss: 887.3509521484375 = 1.1004849672317505 + 100.0 * 8.862504959106445
Epoch 110, val loss: 1.099664330482483
Epoch 120, training loss: 884.7010498046875 = 1.0996862649917603 + 100.0 * 8.836013793945312
Epoch 120, val loss: 1.0988743305206299
Epoch 130, training loss: 882.248046875 = 1.0988925695419312 + 100.0 * 8.811491966247559
Epoch 130, val loss: 1.0980825424194336
Epoch 140, training loss: 880.170166015625 = 1.0980743169784546 + 100.0 * 8.79072093963623
Epoch 140, val loss: 1.0972747802734375
Epoch 150, training loss: 878.3824462890625 = 1.097254753112793 + 100.0 * 8.772851943969727
Epoch 150, val loss: 1.0964603424072266
Epoch 160, training loss: 877.159912109375 = 1.0964001417160034 + 100.0 * 8.760635375976562
Epoch 160, val loss: 1.0956312417984009
Epoch 170, training loss: 875.4368896484375 = 1.0955582857131958 + 100.0 * 8.743412971496582
Epoch 170, val loss: 1.0947707891464233
Epoch 180, training loss: 874.18994140625 = 1.0946980714797974 + 100.0 * 8.730952262878418
Epoch 180, val loss: 1.0939282178878784
Epoch 190, training loss: 873.1734619140625 = 1.0938125848770142 + 100.0 * 8.720796585083008
Epoch 190, val loss: 1.0930513143539429
Epoch 200, training loss: 872.317138671875 = 1.0929168462753296 + 100.0 * 8.712242126464844
Epoch 200, val loss: 1.0921682119369507
Epoch 210, training loss: 871.5097045898438 = 1.0920175313949585 + 100.0 * 8.704176902770996
Epoch 210, val loss: 1.0912781953811646
Epoch 220, training loss: 871.1417846679688 = 1.0910894870758057 + 100.0 * 8.700507164001465
Epoch 220, val loss: 1.0903544425964355
Epoch 230, training loss: 870.4728393554688 = 1.090122103691101 + 100.0 * 8.693826675415039
Epoch 230, val loss: 1.0894187688827515
Epoch 240, training loss: 869.6095581054688 = 1.089143991470337 + 100.0 * 8.685203552246094
Epoch 240, val loss: 1.0884466171264648
Epoch 250, training loss: 869.4639892578125 = 1.0881999731063843 + 100.0 * 8.683757781982422
Epoch 250, val loss: 1.0875059366226196
Epoch 260, training loss: 868.99169921875 = 1.0872135162353516 + 100.0 * 8.679044723510742
Epoch 260, val loss: 1.086532711982727
Epoch 270, training loss: 868.7638549804688 = 1.0861971378326416 + 100.0 * 8.676776885986328
Epoch 270, val loss: 1.0854966640472412
Epoch 280, training loss: 868.6525268554688 = 1.0849192142486572 + 100.0 * 8.675676345825195
Epoch 280, val loss: 1.0842827558517456
Epoch 290, training loss: 868.240966796875 = 1.0841199159622192 + 100.0 * 8.671568870544434
Epoch 290, val loss: 1.0834457874298096
Epoch 300, training loss: 868.5099487304688 = 1.0830528736114502 + 100.0 * 8.67426872253418
Epoch 300, val loss: 1.082444667816162
Epoch 310, training loss: 867.65625 = 1.0819828510284424 + 100.0 * 8.665742874145508
Epoch 310, val loss: 1.0813610553741455
Epoch 320, training loss: 867.4970092773438 = 1.0808757543563843 + 100.0 * 8.664161682128906
Epoch 320, val loss: 1.0802754163742065
Epoch 330, training loss: 867.5299682617188 = 1.0797656774520874 + 100.0 * 8.664502143859863
Epoch 330, val loss: 1.079180121421814
Epoch 340, training loss: 867.56103515625 = 1.0786036252975464 + 100.0 * 8.664824485778809
Epoch 340, val loss: 1.0780290365219116
Epoch 350, training loss: 867.5132446289062 = 1.0774353742599487 + 100.0 * 8.664358139038086
Epoch 350, val loss: 1.076879620552063
Epoch 360, training loss: 867.6381225585938 = 1.076254963874817 + 100.0 * 8.665618896484375
Epoch 360, val loss: 1.075713038444519
Epoch 370, training loss: 867.6843872070312 = 1.075042486190796 + 100.0 * 8.666093826293945
Epoch 370, val loss: 1.0745129585266113
Epoch 380, training loss: 867.64892578125 = 1.0738177299499512 + 100.0 * 8.665751457214355
Epoch 380, val loss: 1.0733121633529663
Epoch 390, training loss: 867.3898315429688 = 1.072588562965393 + 100.0 * 8.663172721862793
Epoch 390, val loss: 1.0721184015274048
Epoch 400, training loss: 867.8323974609375 = 1.0714107751846313 + 100.0 * 8.667610168457031
Epoch 400, val loss: 1.0709302425384521
Epoch 410, training loss: 868.0277709960938 = 1.0701595544815063 + 100.0 * 8.669576644897461
Epoch 410, val loss: 1.069717526435852
Epoch 420, training loss: 867.7335815429688 = 1.0689677000045776 + 100.0 * 8.666646003723145
Epoch 420, val loss: 1.0685491561889648
Epoch 430, training loss: 868.0425415039062 = 1.067792296409607 + 100.0 * 8.669747352600098
Epoch 430, val loss: 1.0673856735229492
Epoch 440, training loss: 868.50927734375 = 1.0665761232376099 + 100.0 * 8.674427032470703
Epoch 440, val loss: 1.0662258863449097
Epoch 450, training loss: 868.6439819335938 = 1.0653730630874634 + 100.0 * 8.675786018371582
Epoch 450, val loss: 1.0650696754455566
Epoch 460, training loss: 868.598876953125 = 1.06412935256958 + 100.0 * 8.675347328186035
Epoch 460, val loss: 1.0638313293457031
Epoch 470, training loss: 869.0552368164062 = 1.063088297843933 + 100.0 * 8.67992115020752
Epoch 470, val loss: 1.0628252029418945
Epoch 480, training loss: 868.8317260742188 = 1.0618417263031006 + 100.0 * 8.677699089050293
Epoch 480, val loss: 1.0616346597671509
Epoch 490, training loss: 868.9436645507812 = 1.06072998046875 + 100.0 * 8.678829193115234
Epoch 490, val loss: 1.0605475902557373
Epoch 500, training loss: 869.407470703125 = 1.0596072673797607 + 100.0 * 8.683478355407715
Epoch 500, val loss: 1.0594364404678345
Epoch 510, training loss: 869.4207153320312 = 1.0584527254104614 + 100.0 * 8.683622360229492
Epoch 510, val loss: 1.0583171844482422
Epoch 520, training loss: 869.3197631835938 = 1.0573135614395142 + 100.0 * 8.682624816894531
Epoch 520, val loss: 1.0572220087051392
Epoch 530, training loss: 870.1344604492188 = 1.05618417263031 + 100.0 * 8.69078254699707
Epoch 530, val loss: 1.0561243295669556
Epoch 540, training loss: 870.2100830078125 = 1.0550395250320435 + 100.0 * 8.691550254821777
Epoch 540, val loss: 1.0550082921981812
Epoch 550, training loss: 870.6286010742188 = 1.0538885593414307 + 100.0 * 8.695747375488281
Epoch 550, val loss: 1.0539062023162842
Epoch 560, training loss: 870.8408203125 = 1.0527020692825317 + 100.0 * 8.697881698608398
Epoch 560, val loss: 1.052737832069397
Epoch 570, training loss: 872.23046875 = 1.0515811443328857 + 100.0 * 8.71178913116455
Epoch 570, val loss: 1.0516425371170044
Epoch 580, training loss: 870.748291015625 = 1.0502240657806396 + 100.0 * 8.696980476379395
Epoch 580, val loss: 1.0503273010253906
Epoch 590, training loss: 870.6537475585938 = 1.0491020679473877 + 100.0 * 8.696046829223633
Epoch 590, val loss: 1.0492192506790161
Epoch 600, training loss: 870.654052734375 = 1.0480531454086304 + 100.0 * 8.696060180664062
Epoch 600, val loss: 1.0481852293014526
Epoch 610, training loss: 871.3217163085938 = 1.0468498468399048 + 100.0 * 8.70274829864502
Epoch 610, val loss: 1.047016978263855
Epoch 620, training loss: 871.3707275390625 = 1.0455355644226074 + 100.0 * 8.703251838684082
Epoch 620, val loss: 1.0457788705825806
Epoch 630, training loss: 871.9693603515625 = 1.0443015098571777 + 100.0 * 8.709250450134277
Epoch 630, val loss: 1.0445741415023804
Epoch 640, training loss: 872.95703125 = 1.0430909395217896 + 100.0 * 8.719139099121094
Epoch 640, val loss: 1.0433844327926636
Epoch 650, training loss: 873.1767578125 = 1.0418204069137573 + 100.0 * 8.721349716186523
Epoch 650, val loss: 1.0421538352966309
Epoch 660, training loss: 873.6597900390625 = 1.0405526161193848 + 100.0 * 8.726192474365234
Epoch 660, val loss: 1.0409399271011353
Epoch 670, training loss: 872.9148559570312 = 1.0391451120376587 + 100.0 * 8.718757629394531
Epoch 670, val loss: 1.0395431518554688
Epoch 680, training loss: 874.8139038085938 = 1.037999153137207 + 100.0 * 8.73775863647461
Epoch 680, val loss: 1.0384315252304077
Epoch 690, training loss: 875.028564453125 = 1.0366735458374023 + 100.0 * 8.73991870880127
Epoch 690, val loss: 1.0371721982955933
Epoch 700, training loss: 875.1517333984375 = 1.035354495048523 + 100.0 * 8.741164207458496
Epoch 700, val loss: 1.0358946323394775
Epoch 710, training loss: 875.4962158203125 = 1.0340697765350342 + 100.0 * 8.744621276855469
Epoch 710, val loss: 1.0346450805664062
Epoch 720, training loss: 875.7611694335938 = 1.0327599048614502 + 100.0 * 8.747283935546875
Epoch 720, val loss: 1.0333726406097412
Epoch 730, training loss: 876.2506713867188 = 1.0314280986785889 + 100.0 * 8.752192497253418
Epoch 730, val loss: 1.0320731401443481
Epoch 740, training loss: 876.28271484375 = 1.0300909280776978 + 100.0 * 8.75252628326416
Epoch 740, val loss: 1.030775547027588
Epoch 750, training loss: 876.7113647460938 = 1.0287699699401855 + 100.0 * 8.756826400756836
Epoch 750, val loss: 1.0295122861862183
Epoch 760, training loss: 876.8539428710938 = 1.027394413948059 + 100.0 * 8.758265495300293
Epoch 760, val loss: 1.028163194656372
Epoch 770, training loss: 876.8123168945312 = 1.0259919166564941 + 100.0 * 8.75786304473877
Epoch 770, val loss: 1.0268259048461914
Epoch 780, training loss: 876.9806518554688 = 1.0246052742004395 + 100.0 * 8.759560585021973
Epoch 780, val loss: 1.025459885597229
Epoch 790, training loss: 877.9560546875 = 1.0232532024383545 + 100.0 * 8.769328117370605
Epoch 790, val loss: 1.0241564512252808
Epoch 800, training loss: 878.2973022460938 = 1.0218807458877563 + 100.0 * 8.772754669189453
Epoch 800, val loss: 1.0228193998336792
Epoch 810, training loss: 878.0740966796875 = 1.0204637050628662 + 100.0 * 8.770536422729492
Epoch 810, val loss: 1.0214647054672241
Epoch 820, training loss: 878.629150390625 = 1.0190562009811401 + 100.0 * 8.776101112365723
Epoch 820, val loss: 1.020090937614441
Epoch 830, training loss: 878.9986572265625 = 1.0176444053649902 + 100.0 * 8.779809951782227
Epoch 830, val loss: 1.0187183618545532
Epoch 840, training loss: 879.0367431640625 = 1.0161921977996826 + 100.0 * 8.780205726623535
Epoch 840, val loss: 1.0173227787017822
Epoch 850, training loss: 879.497802734375 = 1.0147989988327026 + 100.0 * 8.784830093383789
Epoch 850, val loss: 1.0159443616867065
Epoch 860, training loss: 879.5447387695312 = 1.0133517980575562 + 100.0 * 8.785313606262207
Epoch 860, val loss: 1.0145632028579712
Epoch 870, training loss: 879.6578979492188 = 1.011879324913025 + 100.0 * 8.786459922790527
Epoch 870, val loss: 1.0131464004516602
Epoch 880, training loss: 879.1385498046875 = 1.0103814601898193 + 100.0 * 8.781281471252441
Epoch 880, val loss: 1.0117100477218628
Epoch 890, training loss: 878.7409057617188 = 1.0088926553726196 + 100.0 * 8.77731990814209
Epoch 890, val loss: 1.0102310180664062
Epoch 900, training loss: 879.6854858398438 = 1.0073922872543335 + 100.0 * 8.786781311035156
Epoch 900, val loss: 1.0088427066802979
Epoch 910, training loss: 879.2882690429688 = 1.0059564113616943 + 100.0 * 8.782822608947754
Epoch 910, val loss: 1.0073933601379395
Epoch 920, training loss: 879.8264770507812 = 1.0044732093811035 + 100.0 * 8.788220405578613
Epoch 920, val loss: 1.0059523582458496
Epoch 930, training loss: 880.5907592773438 = 1.0029546022415161 + 100.0 * 8.795878410339355
Epoch 930, val loss: 1.004494547843933
Epoch 940, training loss: 880.8734741210938 = 1.0014703273773193 + 100.0 * 8.798720359802246
Epoch 940, val loss: 1.0030502080917358
Epoch 950, training loss: 881.2992553710938 = 0.9999704360961914 + 100.0 * 8.802992820739746
Epoch 950, val loss: 1.0015841722488403
Epoch 960, training loss: 881.68994140625 = 0.9984422922134399 + 100.0 * 8.806915283203125
Epoch 960, val loss: 1.0001131296157837
Epoch 970, training loss: 881.8930053710938 = 0.9968904852867126 + 100.0 * 8.808960914611816
Epoch 970, val loss: 0.998599648475647
Epoch 980, training loss: 882.0299682617188 = 0.9953303337097168 + 100.0 * 8.810346603393555
Epoch 980, val loss: 0.9970998764038086
Epoch 990, training loss: 882.0523681640625 = 0.9937827587127686 + 100.0 * 8.810585975646973
Epoch 990, val loss: 0.9956050515174866
Epoch 1000, training loss: 882.51708984375 = 0.9922400116920471 + 100.0 * 8.815248489379883
Epoch 1000, val loss: 0.9941062927246094
Epoch 1010, training loss: 882.5570068359375 = 0.9906337857246399 + 100.0 * 8.81566333770752
Epoch 1010, val loss: 0.992572009563446
Epoch 1020, training loss: 883.8816528320312 = 0.9890716671943665 + 100.0 * 8.828926086425781
Epoch 1020, val loss: 0.9911184310913086
Epoch 1030, training loss: 881.6395874023438 = 0.9874303936958313 + 100.0 * 8.80652141571045
Epoch 1030, val loss: 0.9894855618476868
Epoch 1040, training loss: 882.4396362304688 = 0.9859268665313721 + 100.0 * 8.814537048339844
Epoch 1040, val loss: 0.9879691004753113
Epoch 1050, training loss: 883.0986938476562 = 0.9843478798866272 + 100.0 * 8.82114315032959
Epoch 1050, val loss: 0.9864528179168701
Epoch 1060, training loss: 883.4134521484375 = 0.9827825427055359 + 100.0 * 8.82430648803711
Epoch 1060, val loss: 0.9849222302436829
Epoch 1070, training loss: 884.0971069335938 = 0.9811655879020691 + 100.0 * 8.831159591674805
Epoch 1070, val loss: 0.9833678603172302
Epoch 1080, training loss: 883.3425903320312 = 0.979424774646759 + 100.0 * 8.823631286621094
Epoch 1080, val loss: 0.9816398024559021
Epoch 1090, training loss: 884.2547607421875 = 0.9778615236282349 + 100.0 * 8.832769393920898
Epoch 1090, val loss: 0.9801521301269531
Epoch 1100, training loss: 885.0804443359375 = 0.9763473868370056 + 100.0 * 8.841041564941406
Epoch 1100, val loss: 0.9786904454231262
Epoch 1110, training loss: 885.1660766601562 = 0.9746953845024109 + 100.0 * 8.841914176940918
Epoch 1110, val loss: 0.9770925641059875
Epoch 1120, training loss: 885.491943359375 = 0.9730508923530579 + 100.0 * 8.845189094543457
Epoch 1120, val loss: 0.9755154848098755
Epoch 1130, training loss: 885.8358154296875 = 0.9713802337646484 + 100.0 * 8.848644256591797
Epoch 1130, val loss: 0.9738765954971313
Epoch 1140, training loss: 885.9042358398438 = 0.9697265625 + 100.0 * 8.849345207214355
Epoch 1140, val loss: 0.9722727537155151
Epoch 1150, training loss: 886.4286499023438 = 0.9680572152137756 + 100.0 * 8.854605674743652
Epoch 1150, val loss: 0.9706646203994751
Epoch 1160, training loss: 886.558837890625 = 0.9663628339767456 + 100.0 * 8.855924606323242
Epoch 1160, val loss: 0.9690120816230774
Epoch 1170, training loss: 886.45068359375 = 0.964661180973053 + 100.0 * 8.854860305786133
Epoch 1170, val loss: 0.9673503041267395
Epoch 1180, training loss: 885.8309326171875 = 0.9628943204879761 + 100.0 * 8.84868049621582
Epoch 1180, val loss: 0.9656496644020081
Epoch 1190, training loss: 883.8894653320312 = 0.9608676433563232 + 100.0 * 8.829285621643066
Epoch 1190, val loss: 0.963701069355011
Epoch 1200, training loss: 884.92041015625 = 0.9592589139938354 + 100.0 * 8.839612007141113
Epoch 1200, val loss: 0.9620574116706848
Epoch 1210, training loss: 886.18115234375 = 0.95759117603302 + 100.0 * 8.852235794067383
Epoch 1210, val loss: 0.9604629874229431
Epoch 1220, training loss: 885.4268188476562 = 0.955845832824707 + 100.0 * 8.844709396362305
Epoch 1220, val loss: 0.9587891101837158
Epoch 1230, training loss: 886.1354370117188 = 0.954222559928894 + 100.0 * 8.851812362670898
Epoch 1230, val loss: 0.9572013020515442
Epoch 1240, training loss: 886.4102172851562 = 0.9525277614593506 + 100.0 * 8.85457706451416
Epoch 1240, val loss: 0.9555299878120422
Epoch 1250, training loss: 887.0537109375 = 0.9507734775543213 + 100.0 * 8.861029624938965
Epoch 1250, val loss: 0.9538350701332092
Epoch 1260, training loss: 887.2141723632812 = 0.9489949345588684 + 100.0 * 8.862651824951172
Epoch 1260, val loss: 0.9520887732505798
Epoch 1270, training loss: 887.3641357421875 = 0.9472282528877258 + 100.0 * 8.864169120788574
Epoch 1270, val loss: 0.9503791928291321
Epoch 1280, training loss: 887.6840209960938 = 0.9454300999641418 + 100.0 * 8.867385864257812
Epoch 1280, val loss: 0.9486215114593506
Epoch 1290, training loss: 888.0916137695312 = 0.9436318874359131 + 100.0 * 8.871479988098145
Epoch 1290, val loss: 0.9468499422073364
Epoch 1300, training loss: 888.2285766601562 = 0.9418308734893799 + 100.0 * 8.872867584228516
Epoch 1300, val loss: 0.9451091289520264
Epoch 1310, training loss: 888.2550048828125 = 0.939964771270752 + 100.0 * 8.873150825500488
Epoch 1310, val loss: 0.9432787299156189
Epoch 1320, training loss: 888.307861328125 = 0.9381102323532104 + 100.0 * 8.873697280883789
Epoch 1320, val loss: 0.9415010809898376
Epoch 1330, training loss: 888.6864013671875 = 0.9362598061561584 + 100.0 * 8.877501487731934
Epoch 1330, val loss: 0.9396945238113403
Epoch 1340, training loss: 888.9468994140625 = 0.9343984723091125 + 100.0 * 8.880125045776367
Epoch 1340, val loss: 0.9378525018692017
Epoch 1350, training loss: 888.898681640625 = 0.9325064420700073 + 100.0 * 8.879661560058594
Epoch 1350, val loss: 0.9360161423683167
Epoch 1360, training loss: 889.409912109375 = 0.9306334853172302 + 100.0 * 8.884793281555176
Epoch 1360, val loss: 0.934187650680542
Epoch 1370, training loss: 889.8565673828125 = 0.9287337064743042 + 100.0 * 8.889278411865234
Epoch 1370, val loss: 0.9323239326477051
Epoch 1380, training loss: 889.5809936523438 = 0.9267374873161316 + 100.0 * 8.886542320251465
Epoch 1380, val loss: 0.9304385781288147
Epoch 1390, training loss: 889.5154418945312 = 0.9248331785202026 + 100.0 * 8.885906219482422
Epoch 1390, val loss: 0.9285428524017334
Epoch 1400, training loss: 890.1257934570312 = 0.9229061007499695 + 100.0 * 8.89202880859375
Epoch 1400, val loss: 0.9267070293426514
Epoch 1410, training loss: 890.1785888671875 = 0.9209346175193787 + 100.0 * 8.892576217651367
Epoch 1410, val loss: 0.9247557520866394
Epoch 1420, training loss: 890.2750854492188 = 0.9189417362213135 + 100.0 * 8.893561363220215
Epoch 1420, val loss: 0.922812283039093
Epoch 1430, training loss: 890.67138671875 = 0.9169625043869019 + 100.0 * 8.897543907165527
Epoch 1430, val loss: 0.9208880066871643
Epoch 1440, training loss: 890.8435668945312 = 0.9149202704429626 + 100.0 * 8.899286270141602
Epoch 1440, val loss: 0.9189174771308899
Epoch 1450, training loss: 891.10693359375 = 0.9128944873809814 + 100.0 * 8.90194034576416
Epoch 1450, val loss: 0.9169239401817322
Epoch 1460, training loss: 890.9796752929688 = 0.91081702709198 + 100.0 * 8.900688171386719
Epoch 1460, val loss: 0.9149222373962402
Epoch 1470, training loss: 891.5157470703125 = 0.9087584018707275 + 100.0 * 8.9060697555542
Epoch 1470, val loss: 0.9129084348678589
Epoch 1480, training loss: 891.2202758789062 = 0.9066786766052246 + 100.0 * 8.903136253356934
Epoch 1480, val loss: 0.9108500480651855
Epoch 1490, training loss: 891.5971069335938 = 0.9045028686523438 + 100.0 * 8.906926155090332
Epoch 1490, val loss: 0.9087602496147156
Epoch 1500, training loss: 891.6085815429688 = 0.9023679494857788 + 100.0 * 8.907062530517578
Epoch 1500, val loss: 0.906646192073822
Epoch 1510, training loss: 892.1077270507812 = 0.9002285599708557 + 100.0 * 8.91207504272461
Epoch 1510, val loss: 0.9045729637145996
Epoch 1520, training loss: 892.086181640625 = 0.898025631904602 + 100.0 * 8.911881446838379
Epoch 1520, val loss: 0.9023937582969666
Epoch 1530, training loss: 891.9192504882812 = 0.8957881927490234 + 100.0 * 8.910234451293945
Epoch 1530, val loss: 0.90023273229599
Epoch 1540, training loss: 892.36669921875 = 0.8935955166816711 + 100.0 * 8.9147310256958
Epoch 1540, val loss: 0.8981050252914429
Epoch 1550, training loss: 892.8412475585938 = 0.8914073705673218 + 100.0 * 8.919498443603516
Epoch 1550, val loss: 0.8959053158760071
Epoch 1560, training loss: 892.4887084960938 = 0.8891810774803162 + 100.0 * 8.915995597839355
Epoch 1560, val loss: 0.8937705159187317
Epoch 1570, training loss: 892.7139892578125 = 0.8869152665138245 + 100.0 * 8.9182710647583
Epoch 1570, val loss: 0.8916164040565491
Epoch 1580, training loss: 892.8263549804688 = 0.8847193717956543 + 100.0 * 8.919416427612305
Epoch 1580, val loss: 0.8894343376159668
Epoch 1590, training loss: 893.0215454101562 = 0.8824800848960876 + 100.0 * 8.921390533447266
Epoch 1590, val loss: 0.8872880935668945
Epoch 1600, training loss: 893.5839233398438 = 0.8802908062934875 + 100.0 * 8.92703628540039
Epoch 1600, val loss: 0.8851832151412964
Epoch 1610, training loss: 893.6051635742188 = 0.8780498504638672 + 100.0 * 8.927270889282227
Epoch 1610, val loss: 0.8830112814903259
Epoch 1620, training loss: 891.870361328125 = 0.8756304979324341 + 100.0 * 8.909947395324707
Epoch 1620, val loss: 0.8804964423179626
Epoch 1630, training loss: 894.5081787109375 = 0.8736239671707153 + 100.0 * 8.936346054077148
Epoch 1630, val loss: 0.8786789774894714
Epoch 1640, training loss: 890.923583984375 = 0.8711490035057068 + 100.0 * 8.900524139404297
Epoch 1640, val loss: 0.8763837814331055
Epoch 1650, training loss: 891.3549194335938 = 0.8688885569572449 + 100.0 * 8.904860496520996
Epoch 1650, val loss: 0.8741438984870911
Epoch 1660, training loss: 892.7950439453125 = 0.8667635321617126 + 100.0 * 8.919282913208008
Epoch 1660, val loss: 0.8720992207527161
Epoch 1670, training loss: 893.2210693359375 = 0.8645502328872681 + 100.0 * 8.923564910888672
Epoch 1670, val loss: 0.8699461817741394
Epoch 1680, training loss: 893.39892578125 = 0.8623194098472595 + 100.0 * 8.925366401672363
Epoch 1680, val loss: 0.8677704334259033
Epoch 1690, training loss: 893.602783203125 = 0.8600581884384155 + 100.0 * 8.927427291870117
Epoch 1690, val loss: 0.8655751347541809
Epoch 1700, training loss: 894.0551147460938 = 0.8578763604164124 + 100.0 * 8.93197250366211
Epoch 1700, val loss: 0.8634750247001648
Epoch 1710, training loss: 894.6734619140625 = 0.8556766510009766 + 100.0 * 8.938178062438965
Epoch 1710, val loss: 0.8613461256027222
Epoch 1720, training loss: 894.9032592773438 = 0.8534372448921204 + 100.0 * 8.940498352050781
Epoch 1720, val loss: 0.8591710925102234
Epoch 1730, training loss: 895.0968017578125 = 0.8512037396430969 + 100.0 * 8.942456245422363
Epoch 1730, val loss: 0.8570023775100708
Epoch 1740, training loss: 895.0704956054688 = 0.8489696979522705 + 100.0 * 8.942214965820312
Epoch 1740, val loss: 0.8548555374145508
Epoch 1750, training loss: 895.179443359375 = 0.8467610478401184 + 100.0 * 8.943326950073242
Epoch 1750, val loss: 0.8526819944381714
Epoch 1760, training loss: 894.9544067382812 = 0.8444356918334961 + 100.0 * 8.941100120544434
Epoch 1760, val loss: 0.850450336933136
Epoch 1770, training loss: 895.0431518554688 = 0.8422393798828125 + 100.0 * 8.942008972167969
Epoch 1770, val loss: 0.8483580350875854
Epoch 1780, training loss: 895.5642700195312 = 0.8400956392288208 + 100.0 * 8.94724178314209
Epoch 1780, val loss: 0.846272349357605
Epoch 1790, training loss: 895.4191284179688 = 0.8378584980964661 + 100.0 * 8.945813179016113
Epoch 1790, val loss: 0.8440980315208435
Epoch 1800, training loss: 895.7548217773438 = 0.83564692735672 + 100.0 * 8.94919204711914
Epoch 1800, val loss: 0.8419750928878784
Epoch 1810, training loss: 896.0317993164062 = 0.8334249258041382 + 100.0 * 8.951983451843262
Epoch 1810, val loss: 0.8398244976997375
Epoch 1820, training loss: 895.8676147460938 = 0.8311383724212646 + 100.0 * 8.95036506652832
Epoch 1820, val loss: 0.8376424908638
Epoch 1830, training loss: 894.7655639648438 = 0.8288846015930176 + 100.0 * 8.939367294311523
Epoch 1830, val loss: 0.8355183005332947
Epoch 1840, training loss: 895.1065063476562 = 0.8267379403114319 + 100.0 * 8.942797660827637
Epoch 1840, val loss: 0.8333582878112793
Epoch 1850, training loss: 895.1978149414062 = 0.8245086073875427 + 100.0 * 8.943733215332031
Epoch 1850, val loss: 0.8312012553215027
Epoch 1860, training loss: 895.1814575195312 = 0.822331428527832 + 100.0 * 8.943591117858887
Epoch 1860, val loss: 0.8291529417037964
Epoch 1870, training loss: 896.0617065429688 = 0.8201331496238708 + 100.0 * 8.952415466308594
Epoch 1870, val loss: 0.8270472288131714
Epoch 1880, training loss: 896.4884033203125 = 0.817964494228363 + 100.0 * 8.956704139709473
Epoch 1880, val loss: 0.8249377608299255
Epoch 1890, training loss: 896.6325073242188 = 0.8157173991203308 + 100.0 * 8.958168029785156
Epoch 1890, val loss: 0.8227733969688416
Epoch 1900, training loss: 897.0037231445312 = 0.8134823441505432 + 100.0 * 8.961902618408203
Epoch 1900, val loss: 0.8206304907798767
Epoch 1910, training loss: 897.3287353515625 = 0.8112632036209106 + 100.0 * 8.965174674987793
Epoch 1910, val loss: 0.8184897899627686
Epoch 1920, training loss: 897.3900146484375 = 0.8090000748634338 + 100.0 * 8.96580982208252
Epoch 1920, val loss: 0.8163111805915833
Epoch 1930, training loss: 897.7378540039062 = 0.8067765235900879 + 100.0 * 8.969310760498047
Epoch 1930, val loss: 0.8141763210296631
Epoch 1940, training loss: 897.8341674804688 = 0.804538905620575 + 100.0 * 8.970295906066895
Epoch 1940, val loss: 0.8120390176773071
Epoch 1950, training loss: 898.0469360351562 = 0.8022958040237427 + 100.0 * 8.97244644165039
Epoch 1950, val loss: 0.8098721504211426
Epoch 1960, training loss: 897.7435302734375 = 0.8000457286834717 + 100.0 * 8.96943473815918
Epoch 1960, val loss: 0.8078092336654663
Epoch 1970, training loss: 893.2135620117188 = 0.79752117395401 + 100.0 * 8.92416000366211
Epoch 1970, val loss: 0.8052608966827393
Epoch 1980, training loss: 897.3489990234375 = 0.7957759499549866 + 100.0 * 8.965532302856445
Epoch 1980, val loss: 0.8036591410636902
Epoch 1990, training loss: 892.7020263671875 = 0.7936001420021057 + 100.0 * 8.919084548950195
Epoch 1990, val loss: 0.8016554713249207
Epoch 2000, training loss: 895.5032958984375 = 0.791979968547821 + 100.0 * 8.947113037109375
Epoch 2000, val loss: 0.8000945448875427
Epoch 2010, training loss: 895.124267578125 = 0.7898332476615906 + 100.0 * 8.943344116210938
Epoch 2010, val loss: 0.7979736924171448
Epoch 2020, training loss: 895.451904296875 = 0.7877035140991211 + 100.0 * 8.94664192199707
Epoch 2020, val loss: 0.7959283590316772
Epoch 2030, training loss: 897.175048828125 = 0.785477876663208 + 100.0 * 8.963895797729492
Epoch 2030, val loss: 0.7938099503517151
Epoch 2040, training loss: 897.382080078125 = 0.7833629250526428 + 100.0 * 8.965987205505371
Epoch 2040, val loss: 0.7917793989181519
Epoch 2050, training loss: 898.1500854492188 = 0.7812615036964417 + 100.0 * 8.973688125610352
Epoch 2050, val loss: 0.7897674441337585
Epoch 2060, training loss: 898.7095336914062 = 0.7791319489479065 + 100.0 * 8.979304313659668
Epoch 2060, val loss: 0.7877265214920044
Epoch 2070, training loss: 899.1356201171875 = 0.7769482731819153 + 100.0 * 8.983586311340332
Epoch 2070, val loss: 0.7856562733650208
Epoch 2080, training loss: 898.988037109375 = 0.7747176289558411 + 100.0 * 8.982132911682129
Epoch 2080, val loss: 0.7835412621498108
Epoch 2090, training loss: 899.5007934570312 = 0.7725410461425781 + 100.0 * 8.987282752990723
Epoch 2090, val loss: 0.7814759612083435
Epoch 2100, training loss: 899.7339477539062 = 0.7703654170036316 + 100.0 * 8.989635467529297
Epoch 2100, val loss: 0.7793881893157959
Epoch 2110, training loss: 899.8387451171875 = 0.768156886100769 + 100.0 * 8.990706443786621
Epoch 2110, val loss: 0.7773006558418274
Epoch 2120, training loss: 900.112548828125 = 0.7659918665885925 + 100.0 * 8.993465423583984
Epoch 2120, val loss: 0.7752469182014465
Epoch 2130, training loss: 899.8404541015625 = 0.7637909650802612 + 100.0 * 8.990766525268555
Epoch 2130, val loss: 0.7731457352638245
Epoch 2140, training loss: 900.3026123046875 = 0.7615953683853149 + 100.0 * 8.995409965515137
Epoch 2140, val loss: 0.771067202091217
Epoch 2150, training loss: 900.7655029296875 = 0.7594062089920044 + 100.0 * 9.00006103515625
Epoch 2150, val loss: 0.7689998149871826
Epoch 2160, training loss: 900.7689208984375 = 0.7572323679924011 + 100.0 * 9.000117301940918
Epoch 2160, val loss: 0.7669256925582886
Epoch 2170, training loss: 900.9683837890625 = 0.7550739645957947 + 100.0 * 9.0021333694458
Epoch 2170, val loss: 0.7648536562919617
Epoch 2180, training loss: 900.6610107421875 = 0.7528745532035828 + 100.0 * 8.9990816116333
Epoch 2180, val loss: 0.762783408164978
Epoch 2190, training loss: 901.0694580078125 = 0.75074702501297 + 100.0 * 9.00318717956543
Epoch 2190, val loss: 0.7607663869857788
Epoch 2200, training loss: 901.6276245117188 = 0.7486006617546082 + 100.0 * 9.008790016174316
Epoch 2200, val loss: 0.7587288022041321
Epoch 2210, training loss: 901.5103759765625 = 0.7464286088943481 + 100.0 * 9.00763988494873
Epoch 2210, val loss: 0.7566678524017334
Epoch 2220, training loss: 901.5181884765625 = 0.744278609752655 + 100.0 * 9.007739067077637
Epoch 2220, val loss: 0.7546448111534119
Epoch 2230, training loss: 901.7315673828125 = 0.7421461343765259 + 100.0 * 9.009894371032715
Epoch 2230, val loss: 0.7526111006736755
Epoch 2240, training loss: 901.2247924804688 = 0.7399433255195618 + 100.0 * 9.00484848022461
Epoch 2240, val loss: 0.7505236864089966
Epoch 2250, training loss: 901.750732421875 = 0.7378393411636353 + 100.0 * 9.01012897491455
Epoch 2250, val loss: 0.7485589981079102
Epoch 2260, training loss: 900.1058959960938 = 0.7355115413665771 + 100.0 * 8.993703842163086
Epoch 2260, val loss: 0.746365487575531
Epoch 2270, training loss: 899.4637451171875 = 0.733609676361084 + 100.0 * 8.98730182647705
Epoch 2270, val loss: 0.7444430589675903
Epoch 2280, training loss: 901.0940551757812 = 0.7318758368492126 + 100.0 * 9.003622055053711
Epoch 2280, val loss: 0.7428563833236694
Epoch 2290, training loss: 900.5743408203125 = 0.7298148274421692 + 100.0 * 8.998445510864258
Epoch 2290, val loss: 0.7409676909446716
Epoch 2300, training loss: 901.1738891601562 = 0.7278528809547424 + 100.0 * 9.004460334777832
Epoch 2300, val loss: 0.739098846912384
Epoch 2310, training loss: 902.41552734375 = 0.7258803844451904 + 100.0 * 9.01689624786377
Epoch 2310, val loss: 0.7372121810913086
Epoch 2320, training loss: 903.3759765625 = 0.7238628268241882 + 100.0 * 9.026520729064941
Epoch 2320, val loss: 0.7353014945983887
Epoch 2330, training loss: 904.0352783203125 = 0.7217925786972046 + 100.0 * 9.033134460449219
Epoch 2330, val loss: 0.7333504557609558
Epoch 2340, training loss: 904.4686889648438 = 0.7197149991989136 + 100.0 * 9.037489891052246
Epoch 2340, val loss: 0.7313737273216248
Epoch 2350, training loss: 904.5582885742188 = 0.7176212072372437 + 100.0 * 9.038406372070312
Epoch 2350, val loss: 0.7293937802314758
Epoch 2360, training loss: 904.720458984375 = 0.7155454158782959 + 100.0 * 9.04004955291748
Epoch 2360, val loss: 0.7274416089057922
Epoch 2370, training loss: 905.0416259765625 = 0.7134760022163391 + 100.0 * 9.043281555175781
Epoch 2370, val loss: 0.7254956364631653
Epoch 2380, training loss: 904.6121215820312 = 0.7114114165306091 + 100.0 * 9.039007186889648
Epoch 2380, val loss: 0.7235421538352966
Epoch 2390, training loss: 905.1456298828125 = 0.7093818187713623 + 100.0 * 9.04436206817627
Epoch 2390, val loss: 0.7216303944587708
Epoch 2400, training loss: 905.5383911132812 = 0.7073253989219666 + 100.0 * 9.048310279846191
Epoch 2400, val loss: 0.7196908593177795
Epoch 2410, training loss: 905.16943359375 = 0.7052456736564636 + 100.0 * 9.044641494750977
Epoch 2410, val loss: 0.7177394032478333
Epoch 2420, training loss: 905.4080810546875 = 0.7032197713851929 + 100.0 * 9.047048568725586
Epoch 2420, val loss: 0.7158326506614685
Epoch 2430, training loss: 905.8124389648438 = 0.7012202739715576 + 100.0 * 9.051112174987793
Epoch 2430, val loss: 0.7139683961868286
Epoch 2440, training loss: 905.8363647460938 = 0.6992060542106628 + 100.0 * 9.051371574401855
Epoch 2440, val loss: 0.7120644450187683
Epoch 2450, training loss: 905.6951293945312 = 0.6971997618675232 + 100.0 * 9.049979209899902
Epoch 2450, val loss: 0.7101659178733826
Epoch 2460, training loss: 906.1062622070312 = 0.6952188611030579 + 100.0 * 9.054110527038574
Epoch 2460, val loss: 0.7083147764205933
Epoch 2470, training loss: 906.14013671875 = 0.6932200193405151 + 100.0 * 9.054469108581543
Epoch 2470, val loss: 0.7064308524131775
Epoch 2480, training loss: 906.3726196289062 = 0.6912431120872498 + 100.0 * 9.056814193725586
Epoch 2480, val loss: 0.7045823335647583
Epoch 2490, training loss: 906.3765869140625 = 0.6892811059951782 + 100.0 * 9.056873321533203
Epoch 2490, val loss: 0.7027280330657959
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.797536231884058
0.8640150691878578
=== training gcn model ===
Epoch 0, training loss: 1026.284912109375 = 1.0929038524627686 + 100.0 * 10.251919746398926
Epoch 0, val loss: 1.093613624572754
Epoch 10, training loss: 986.0278930664062 = 1.0927211046218872 + 100.0 * 9.84935188293457
Epoch 10, val loss: 1.0934613943099976
Epoch 20, training loss: 965.0845947265625 = 1.09259831905365 + 100.0 * 9.639920234680176
Epoch 20, val loss: 1.0933557748794556
Epoch 30, training loss: 949.59912109375 = 1.0924433469772339 + 100.0 * 9.485066413879395
Epoch 30, val loss: 1.093213438987732
Epoch 40, training loss: 937.2566528320312 = 1.092268943786621 + 100.0 * 9.36164379119873
Epoch 40, val loss: 1.0930566787719727
Epoch 50, training loss: 927.3786010742188 = 1.0921090841293335 + 100.0 * 9.26286506652832
Epoch 50, val loss: 1.0929160118103027
Epoch 60, training loss: 919.2526245117188 = 1.0919429063796997 + 100.0 * 9.181607246398926
Epoch 60, val loss: 1.092767357826233
Epoch 70, training loss: 912.4598999023438 = 1.0917667150497437 + 100.0 * 9.113680839538574
Epoch 70, val loss: 1.092607855796814
Epoch 80, training loss: 906.7796020507812 = 1.0915902853012085 + 100.0 * 9.056879997253418
Epoch 80, val loss: 1.0924488306045532
Epoch 90, training loss: 902.0022583007812 = 1.0914015769958496 + 100.0 * 9.009108543395996
Epoch 90, val loss: 1.092281699180603
Epoch 100, training loss: 897.9717407226562 = 1.0912152528762817 + 100.0 * 8.968805313110352
Epoch 100, val loss: 1.0921130180358887
Epoch 110, training loss: 894.4550170898438 = 1.0909165143966675 + 100.0 * 8.933640480041504
Epoch 110, val loss: 1.091788411140442
Epoch 120, training loss: 891.3314208984375 = 1.0903849601745605 + 100.0 * 8.902410507202148
Epoch 120, val loss: 1.0912364721298218
Epoch 130, training loss: 888.648193359375 = 1.0898475646972656 + 100.0 * 8.87558364868164
Epoch 130, val loss: 1.0906850099563599
Epoch 140, training loss: 886.2319946289062 = 1.0893218517303467 + 100.0 * 8.85142707824707
Epoch 140, val loss: 1.0901477336883545
Epoch 150, training loss: 884.1586303710938 = 1.0888028144836426 + 100.0 * 8.830698013305664
Epoch 150, val loss: 1.0896183252334595
Epoch 160, training loss: 882.2999877929688 = 1.0882946252822876 + 100.0 * 8.812116622924805
Epoch 160, val loss: 1.089102029800415
Epoch 170, training loss: 880.8734741210938 = 1.0877842903137207 + 100.0 * 8.797857284545898
Epoch 170, val loss: 1.0885825157165527
Epoch 180, training loss: 879.2660522460938 = 1.0872656106948853 + 100.0 * 8.781787872314453
Epoch 180, val loss: 1.0880590677261353
Epoch 190, training loss: 878.4380493164062 = 1.086763620376587 + 100.0 * 8.773512840270996
Epoch 190, val loss: 1.0875483751296997
Epoch 200, training loss: 877.6848754882812 = 1.086240530014038 + 100.0 * 8.765986442565918
Epoch 200, val loss: 1.087018370628357
Epoch 210, training loss: 876.8972778320312 = 1.0856971740722656 + 100.0 * 8.758115768432617
Epoch 210, val loss: 1.0864661931991577
Epoch 220, training loss: 876.1756591796875 = 1.085141897201538 + 100.0 * 8.75090503692627
Epoch 220, val loss: 1.0859099626541138
Epoch 230, training loss: 875.2482299804688 = 1.0845718383789062 + 100.0 * 8.741636276245117
Epoch 230, val loss: 1.0853468179702759
Epoch 240, training loss: 874.8212890625 = 1.0839965343475342 + 100.0 * 8.737373352050781
Epoch 240, val loss: 1.084754467010498
Epoch 250, training loss: 874.76806640625 = 1.0833970308303833 + 100.0 * 8.736846923828125
Epoch 250, val loss: 1.084163784980774
Epoch 260, training loss: 874.7109375 = 1.082801103591919 + 100.0 * 8.736281394958496
Epoch 260, val loss: 1.0835654735565186
Epoch 270, training loss: 874.5339965820312 = 1.082186222076416 + 100.0 * 8.734518051147461
Epoch 270, val loss: 1.0829510688781738
Epoch 280, training loss: 874.382080078125 = 1.0815638303756714 + 100.0 * 8.73300552368164
Epoch 280, val loss: 1.0823243856430054
Epoch 290, training loss: 874.3590087890625 = 1.0809260606765747 + 100.0 * 8.732780456542969
Epoch 290, val loss: 1.0816807746887207
Epoch 300, training loss: 874.3392333984375 = 1.080283284187317 + 100.0 * 8.732589721679688
Epoch 300, val loss: 1.081028699874878
Epoch 310, training loss: 874.3679809570312 = 1.0796445608139038 + 100.0 * 8.73288345336914
Epoch 310, val loss: 1.080398440361023
Epoch 320, training loss: 874.0929565429688 = 1.0789607763290405 + 100.0 * 8.73013973236084
Epoch 320, val loss: 1.079736590385437
Epoch 330, training loss: 874.4412231445312 = 1.078301191329956 + 100.0 * 8.73362922668457
Epoch 330, val loss: 1.0790647268295288
Epoch 340, training loss: 874.3447875976562 = 1.077617883682251 + 100.0 * 8.732671737670898
Epoch 340, val loss: 1.0783954858779907
Epoch 350, training loss: 874.4166259765625 = 1.0769134759902954 + 100.0 * 8.733397483825684
Epoch 350, val loss: 1.077699065208435
Epoch 360, training loss: 874.4216918945312 = 1.0762232542037964 + 100.0 * 8.733454704284668
Epoch 360, val loss: 1.0769985914230347
Epoch 370, training loss: 874.4812622070312 = 1.0755165815353394 + 100.0 * 8.734057426452637
Epoch 370, val loss: 1.0763062238693237
Epoch 380, training loss: 874.6328735351562 = 1.0748034715652466 + 100.0 * 8.735580444335938
Epoch 380, val loss: 1.0756001472473145
Epoch 390, training loss: 874.8392944335938 = 1.0740787982940674 + 100.0 * 8.737651824951172
Epoch 390, val loss: 1.0748865604400635
Epoch 400, training loss: 875.0435180664062 = 1.0733518600463867 + 100.0 * 8.739701271057129
Epoch 400, val loss: 1.074174404144287
Epoch 410, training loss: 874.8560180664062 = 1.0726089477539062 + 100.0 * 8.737833976745605
Epoch 410, val loss: 1.073402762413025
Epoch 420, training loss: 875.0459594726562 = 1.0718506574630737 + 100.0 * 8.739741325378418
Epoch 420, val loss: 1.0726944208145142
Epoch 430, training loss: 875.3250732421875 = 1.071118712425232 + 100.0 * 8.742539405822754
Epoch 430, val loss: 1.071967363357544
Epoch 440, training loss: 875.6595458984375 = 1.0703672170639038 + 100.0 * 8.745891571044922
Epoch 440, val loss: 1.0712288618087769
Epoch 450, training loss: 875.6339111328125 = 1.069593071937561 + 100.0 * 8.745643615722656
Epoch 450, val loss: 1.0704660415649414
Epoch 460, training loss: 875.3516235351562 = 1.0688023567199707 + 100.0 * 8.742828369140625
Epoch 460, val loss: 1.0696908235549927
Epoch 470, training loss: 875.6437377929688 = 1.068036437034607 + 100.0 * 8.745757102966309
Epoch 470, val loss: 1.0689220428466797
Epoch 480, training loss: 876.1620483398438 = 1.0672578811645508 + 100.0 * 8.750947952270508
Epoch 480, val loss: 1.0681663751602173
Epoch 490, training loss: 876.4771118164062 = 1.0664763450622559 + 100.0 * 8.754106521606445
Epoch 490, val loss: 1.0673604011535645
Epoch 500, training loss: 876.6239624023438 = 1.0656722784042358 + 100.0 * 8.755582809448242
Epoch 500, val loss: 1.0665963888168335
Epoch 510, training loss: 877.1609497070312 = 1.0648326873779297 + 100.0 * 8.760961532592773
Epoch 510, val loss: 1.065767765045166
Epoch 520, training loss: 877.1341552734375 = 1.0640478134155273 + 100.0 * 8.760701179504395
Epoch 520, val loss: 1.064997673034668
Epoch 530, training loss: 877.1614379882812 = 1.063187599182129 + 100.0 * 8.760982513427734
Epoch 530, val loss: 1.0641683340072632
Epoch 540, training loss: 877.4410400390625 = 1.062363862991333 + 100.0 * 8.763786315917969
Epoch 540, val loss: 1.063342809677124
Epoch 550, training loss: 877.7222290039062 = 1.0615421533584595 + 100.0 * 8.766607284545898
Epoch 550, val loss: 1.0625386238098145
Epoch 560, training loss: 877.8746337890625 = 1.06069815158844 + 100.0 * 8.768139839172363
Epoch 560, val loss: 1.0616964101791382
Epoch 570, training loss: 877.9783935546875 = 1.05985689163208 + 100.0 * 8.769185066223145
Epoch 570, val loss: 1.0608664751052856
Epoch 580, training loss: 878.31884765625 = 1.0590208768844604 + 100.0 * 8.772598266601562
Epoch 580, val loss: 1.0600790977478027
Epoch 590, training loss: 878.3143920898438 = 1.0581610202789307 + 100.0 * 8.772562026977539
Epoch 590, val loss: 1.0592093467712402
Epoch 600, training loss: 878.9346313476562 = 1.0572978258132935 + 100.0 * 8.778773307800293
Epoch 600, val loss: 1.0583584308624268
Epoch 610, training loss: 879.4929809570312 = 1.056416630744934 + 100.0 * 8.7843656539917
Epoch 610, val loss: 1.057507872581482
Epoch 620, training loss: 879.5035400390625 = 1.0555307865142822 + 100.0 * 8.784480094909668
Epoch 620, val loss: 1.056633710861206
Epoch 630, training loss: 879.4207763671875 = 1.0544992685317993 + 100.0 * 8.783662796020508
Epoch 630, val loss: 1.0556092262268066
Epoch 640, training loss: 880.4390258789062 = 1.0538216829299927 + 100.0 * 8.793851852416992
Epoch 640, val loss: 1.0549434423446655
Epoch 650, training loss: 877.216064453125 = 1.0526424646377563 + 100.0 * 8.761634826660156
Epoch 650, val loss: 1.0538437366485596
Epoch 660, training loss: 879.01220703125 = 1.051898717880249 + 100.0 * 8.779603004455566
Epoch 660, val loss: 1.0530638694763184
Epoch 670, training loss: 879.5907592773438 = 1.0510444641113281 + 100.0 * 8.78539752960205
Epoch 670, val loss: 1.0522732734680176
Epoch 680, training loss: 879.4808959960938 = 1.0500649213790894 + 100.0 * 8.784308433532715
Epoch 680, val loss: 1.0513046979904175
Epoch 690, training loss: 880.48388671875 = 1.0491803884506226 + 100.0 * 8.794346809387207
Epoch 690, val loss: 1.0504412651062012
Epoch 700, training loss: 880.7920532226562 = 1.0482429265975952 + 100.0 * 8.797438621520996
Epoch 700, val loss: 1.0495250225067139
Epoch 710, training loss: 881.4241943359375 = 1.0473078489303589 + 100.0 * 8.8037691116333
Epoch 710, val loss: 1.048606276512146
Epoch 720, training loss: 881.3754272460938 = 1.0463426113128662 + 100.0 * 8.803291320800781
Epoch 720, val loss: 1.0476471185684204
Epoch 730, training loss: 881.8858642578125 = 1.0453681945800781 + 100.0 * 8.808404922485352
Epoch 730, val loss: 1.046708106994629
Epoch 740, training loss: 882.0992431640625 = 1.0443880558013916 + 100.0 * 8.810548782348633
Epoch 740, val loss: 1.0457499027252197
Epoch 750, training loss: 882.4672241210938 = 1.0434582233428955 + 100.0 * 8.814237594604492
Epoch 750, val loss: 1.0448392629623413
Epoch 760, training loss: 882.5208129882812 = 1.0424141883850098 + 100.0 * 8.814784049987793
Epoch 760, val loss: 1.0438309907913208
Epoch 770, training loss: 882.6484375 = 1.041408896446228 + 100.0 * 8.816070556640625
Epoch 770, val loss: 1.0428518056869507
Epoch 780, training loss: 883.0284423828125 = 1.0404421091079712 + 100.0 * 8.819879531860352
Epoch 780, val loss: 1.0419058799743652
Epoch 790, training loss: 883.0491333007812 = 1.0394573211669922 + 100.0 * 8.820096969604492
Epoch 790, val loss: 1.0409494638442993
Epoch 800, training loss: 883.6040649414062 = 1.0384774208068848 + 100.0 * 8.825655937194824
Epoch 800, val loss: 1.0399888753890991
Epoch 810, training loss: 883.798828125 = 1.0374681949615479 + 100.0 * 8.827613830566406
Epoch 810, val loss: 1.0389971733093262
Epoch 820, training loss: 883.4470825195312 = 1.036423683166504 + 100.0 * 8.824106216430664
Epoch 820, val loss: 1.0379911661148071
Epoch 830, training loss: 883.5439453125 = 1.0354218482971191 + 100.0 * 8.825085639953613
Epoch 830, val loss: 1.0370173454284668
Epoch 840, training loss: 884.5750732421875 = 1.0344403982162476 + 100.0 * 8.835406303405762
Epoch 840, val loss: 1.0360430479049683
Epoch 850, training loss: 884.7210083007812 = 1.0334135293960571 + 100.0 * 8.836875915527344
Epoch 850, val loss: 1.0350362062454224
Epoch 860, training loss: 884.9235229492188 = 1.0323808193206787 + 100.0 * 8.838911056518555
Epoch 860, val loss: 1.0340293645858765
Epoch 870, training loss: 885.3778686523438 = 1.0313677787780762 + 100.0 * 8.843464851379395
Epoch 870, val loss: 1.0330371856689453
Epoch 880, training loss: 885.8822631835938 = 1.030328392982483 + 100.0 * 8.848519325256348
Epoch 880, val loss: 1.0320279598236084
Epoch 890, training loss: 886.1560668945312 = 1.0293006896972656 + 100.0 * 8.85126781463623
Epoch 890, val loss: 1.0310052633285522
Epoch 900, training loss: 885.8033447265625 = 1.0282063484191895 + 100.0 * 8.84775161743164
Epoch 900, val loss: 1.0299750566482544
Epoch 910, training loss: 886.1971435546875 = 1.0271812677383423 + 100.0 * 8.851699829101562
Epoch 910, val loss: 1.0289260149002075
Epoch 920, training loss: 886.7494506835938 = 1.0261173248291016 + 100.0 * 8.857233047485352
Epoch 920, val loss: 1.0279042720794678
Epoch 930, training loss: 887.19677734375 = 1.0250420570373535 + 100.0 * 8.861717224121094
Epoch 930, val loss: 1.0268670320510864
Epoch 940, training loss: 886.9085693359375 = 1.0239311456680298 + 100.0 * 8.858846664428711
Epoch 940, val loss: 1.0257601737976074
Epoch 950, training loss: 886.7530517578125 = 1.022861361503601 + 100.0 * 8.857301712036133
Epoch 950, val loss: 1.0247156620025635
Epoch 960, training loss: 887.5409545898438 = 1.021811842918396 + 100.0 * 8.865191459655762
Epoch 960, val loss: 1.0236811637878418
Epoch 970, training loss: 887.9573974609375 = 1.020737886428833 + 100.0 * 8.869366645812988
Epoch 970, val loss: 1.0226248502731323
Epoch 980, training loss: 888.2357177734375 = 1.0196540355682373 + 100.0 * 8.872160911560059
Epoch 980, val loss: 1.0215562582015991
Epoch 990, training loss: 888.1348876953125 = 1.0185303688049316 + 100.0 * 8.871163368225098
Epoch 990, val loss: 1.0204817056655884
Epoch 1000, training loss: 888.6153564453125 = 1.0174154043197632 + 100.0 * 8.87597942352295
Epoch 1000, val loss: 1.019392490386963
Epoch 1010, training loss: 888.7261962890625 = 1.0162891149520874 + 100.0 * 8.87709903717041
Epoch 1010, val loss: 1.0182698965072632
Epoch 1020, training loss: 889.00537109375 = 1.0151692628860474 + 100.0 * 8.879901885986328
Epoch 1020, val loss: 1.0171858072280884
Epoch 1030, training loss: 889.0087280273438 = 1.0140258073806763 + 100.0 * 8.8799467086792
Epoch 1030, val loss: 1.0160465240478516
Epoch 1040, training loss: 889.1740112304688 = 1.0128023624420166 + 100.0 * 8.881611824035645
Epoch 1040, val loss: 1.014853835105896
Epoch 1050, training loss: 888.3569946289062 = 1.011622428894043 + 100.0 * 8.873454093933105
Epoch 1050, val loss: 1.0137333869934082
Epoch 1060, training loss: 889.0542602539062 = 1.010494589805603 + 100.0 * 8.880437850952148
Epoch 1060, val loss: 1.0126278400421143
Epoch 1070, training loss: 889.69921875 = 1.0093579292297363 + 100.0 * 8.8868989944458
Epoch 1070, val loss: 1.0114976167678833
Epoch 1080, training loss: 889.340576171875 = 1.008148193359375 + 100.0 * 8.883323669433594
Epoch 1080, val loss: 1.0103195905685425
Epoch 1090, training loss: 888.6893310546875 = 1.006927490234375 + 100.0 * 8.876824378967285
Epoch 1090, val loss: 1.0091241598129272
Epoch 1100, training loss: 889.4517822265625 = 1.0057657957077026 + 100.0 * 8.88446044921875
Epoch 1100, val loss: 1.0079981088638306
Epoch 1110, training loss: 889.7257690429688 = 1.0046008825302124 + 100.0 * 8.887211799621582
Epoch 1110, val loss: 1.0068563222885132
Epoch 1120, training loss: 890.0452270507812 = 1.0034267902374268 + 100.0 * 8.89041805267334
Epoch 1120, val loss: 1.005713939666748
Epoch 1130, training loss: 890.279541015625 = 1.0022238492965698 + 100.0 * 8.892773628234863
Epoch 1130, val loss: 1.004549264907837
Epoch 1140, training loss: 890.4021606445312 = 1.0010205507278442 + 100.0 * 8.894011497497559
Epoch 1140, val loss: 1.0033622980117798
Epoch 1150, training loss: 890.7496948242188 = 0.9998083114624023 + 100.0 * 8.897499084472656
Epoch 1150, val loss: 1.0021793842315674
Epoch 1160, training loss: 890.8629760742188 = 0.9985575675964355 + 100.0 * 8.89864444732666
Epoch 1160, val loss: 1.0009870529174805
Epoch 1170, training loss: 891.141845703125 = 0.9973151683807373 + 100.0 * 8.901445388793945
Epoch 1170, val loss: 0.9997525811195374
Epoch 1180, training loss: 891.199951171875 = 0.9960702061653137 + 100.0 * 8.90203857421875
Epoch 1180, val loss: 0.9985244870185852
Epoch 1190, training loss: 891.3278198242188 = 0.9948164820671082 + 100.0 * 8.903329849243164
Epoch 1190, val loss: 0.9972918629646301
Epoch 1200, training loss: 890.6329956054688 = 0.9935084581375122 + 100.0 * 8.896394729614258
Epoch 1200, val loss: 0.9960253238677979
Epoch 1210, training loss: 890.031005859375 = 0.9922063946723938 + 100.0 * 8.890388488769531
Epoch 1210, val loss: 0.9947880506515503
Epoch 1220, training loss: 890.4243774414062 = 0.9909517765045166 + 100.0 * 8.894333839416504
Epoch 1220, val loss: 0.9935471415519714
Epoch 1230, training loss: 891.2169799804688 = 0.9897371530532837 + 100.0 * 8.90227222442627
Epoch 1230, val loss: 0.9923649430274963
Epoch 1240, training loss: 891.81298828125 = 0.9884849786758423 + 100.0 * 8.908245086669922
Epoch 1240, val loss: 0.9911333322525024
Epoch 1250, training loss: 891.7886962890625 = 0.9871881008148193 + 100.0 * 8.908015251159668
Epoch 1250, val loss: 0.9898708462715149
Epoch 1260, training loss: 891.97216796875 = 0.9858638644218445 + 100.0 * 8.909863471984863
Epoch 1260, val loss: 0.9885901212692261
Epoch 1270, training loss: 892.3548583984375 = 0.9845775961875916 + 100.0 * 8.913702964782715
Epoch 1270, val loss: 0.9873326420783997
Epoch 1280, training loss: 892.47021484375 = 0.983272910118103 + 100.0 * 8.91486930847168
Epoch 1280, val loss: 0.9860407710075378
Epoch 1290, training loss: 892.7184448242188 = 0.9819273352622986 + 100.0 * 8.917365074157715
Epoch 1290, val loss: 0.9847558736801147
Epoch 1300, training loss: 892.9287109375 = 0.9806161522865295 + 100.0 * 8.91948127746582
Epoch 1300, val loss: 0.9834680557250977
Epoch 1310, training loss: 893.1690063476562 = 0.9792802333831787 + 100.0 * 8.921896934509277
Epoch 1310, val loss: 0.9821574687957764
Epoch 1320, training loss: 893.1504516601562 = 0.9779285192489624 + 100.0 * 8.921725273132324
Epoch 1320, val loss: 0.9808298945426941
Epoch 1330, training loss: 893.4217529296875 = 0.9765536189079285 + 100.0 * 8.92445182800293
Epoch 1330, val loss: 0.9795145988464355
Epoch 1340, training loss: 893.5189208984375 = 0.9751900434494019 + 100.0 * 8.925436973571777
Epoch 1340, val loss: 0.978193461894989
Epoch 1350, training loss: 893.6017456054688 = 0.9738050103187561 + 100.0 * 8.926279067993164
Epoch 1350, val loss: 0.976848840713501
Epoch 1360, training loss: 893.3294677734375 = 0.9724082350730896 + 100.0 * 8.92357063293457
Epoch 1360, val loss: 0.9754419326782227
Epoch 1370, training loss: 893.515869140625 = 0.9709857702255249 + 100.0 * 8.925448417663574
Epoch 1370, val loss: 0.9740928411483765
Epoch 1380, training loss: 893.2300415039062 = 0.9696486592292786 + 100.0 * 8.922603607177734
Epoch 1380, val loss: 0.9728230834007263
Epoch 1390, training loss: 893.71044921875 = 0.9682782888412476 + 100.0 * 8.927421569824219
Epoch 1390, val loss: 0.9714754819869995
Epoch 1400, training loss: 893.9454956054688 = 0.9668307304382324 + 100.0 * 8.929786682128906
Epoch 1400, val loss: 0.9700725078582764
Epoch 1410, training loss: 894.3591918945312 = 0.9654124975204468 + 100.0 * 8.933938026428223
Epoch 1410, val loss: 0.9686901569366455
Epoch 1420, training loss: 894.3514404296875 = 0.9639309644699097 + 100.0 * 8.93387508392334
Epoch 1420, val loss: 0.9672400951385498
Epoch 1430, training loss: 894.2979736328125 = 0.9624882936477661 + 100.0 * 8.933355331420898
Epoch 1430, val loss: 0.9658508896827698
Epoch 1440, training loss: 894.5262451171875 = 0.9610388875007629 + 100.0 * 8.935651779174805
Epoch 1440, val loss: 0.9644410014152527
Epoch 1450, training loss: 894.8156127929688 = 0.9595544338226318 + 100.0 * 8.938560485839844
Epoch 1450, val loss: 0.9629952311515808
Epoch 1460, training loss: 894.851806640625 = 0.9580693244934082 + 100.0 * 8.938937187194824
Epoch 1460, val loss: 0.9615599513053894
Epoch 1470, training loss: 894.8729858398438 = 0.9565448760986328 + 100.0 * 8.939164161682129
Epoch 1470, val loss: 0.9600570797920227
Epoch 1480, training loss: 895.34521484375 = 0.9548616409301758 + 100.0 * 8.943903923034668
Epoch 1480, val loss: 0.9583727121353149
Epoch 1490, training loss: 895.08984375 = 0.9527469873428345 + 100.0 * 8.941370964050293
Epoch 1490, val loss: 0.9562307000160217
Epoch 1500, training loss: 894.067138671875 = 0.9504832029342651 + 100.0 * 8.931166648864746
Epoch 1500, val loss: 0.9540884494781494
Epoch 1510, training loss: 894.3119506835938 = 0.9483643770217896 + 100.0 * 8.933635711669922
Epoch 1510, val loss: 0.9520729780197144
Epoch 1520, training loss: 895.3436279296875 = 0.9463719725608826 + 100.0 * 8.94397258758545
Epoch 1520, val loss: 0.9500781297683716
Epoch 1530, training loss: 893.3350830078125 = 0.9442667365074158 + 100.0 * 8.923908233642578
Epoch 1530, val loss: 0.9480422139167786
Epoch 1540, training loss: 893.4973754882812 = 0.9421492218971252 + 100.0 * 8.925552368164062
Epoch 1540, val loss: 0.9459589123725891
Epoch 1550, training loss: 894.3082275390625 = 0.9401717782020569 + 100.0 * 8.933680534362793
Epoch 1550, val loss: 0.9440505504608154
Epoch 1560, training loss: 895.2278442382812 = 0.9381696581840515 + 100.0 * 8.942896842956543
Epoch 1560, val loss: 0.9421108365058899
Epoch 1570, training loss: 895.584228515625 = 0.9361447691917419 + 100.0 * 8.946480751037598
Epoch 1570, val loss: 0.9401552081108093
Epoch 1580, training loss: 896.0593872070312 = 0.934099018573761 + 100.0 * 8.951252937316895
Epoch 1580, val loss: 0.9381526708602905
Epoch 1590, training loss: 896.069580078125 = 0.932047963142395 + 100.0 * 8.951375007629395
Epoch 1590, val loss: 0.9361661672592163
Epoch 1600, training loss: 896.1843872070312 = 0.9299766421318054 + 100.0 * 8.952544212341309
Epoch 1600, val loss: 0.934149980545044
Epoch 1610, training loss: 896.1790161132812 = 0.9278863668441772 + 100.0 * 8.95251178741455
Epoch 1610, val loss: 0.9321451783180237
Epoch 1620, training loss: 896.8317260742188 = 0.9258231520652771 + 100.0 * 8.95905876159668
Epoch 1620, val loss: 0.9301350712776184
Epoch 1630, training loss: 896.8102416992188 = 0.9237239360809326 + 100.0 * 8.95886516571045
Epoch 1630, val loss: 0.9281008243560791
Epoch 1640, training loss: 896.7627563476562 = 0.9216191172599792 + 100.0 * 8.95841121673584
Epoch 1640, val loss: 0.9260630011558533
Epoch 1650, training loss: 897.3211669921875 = 0.9195415377616882 + 100.0 * 8.96401596069336
Epoch 1650, val loss: 0.9240652322769165
Epoch 1660, training loss: 897.6439208984375 = 0.9174385666847229 + 100.0 * 8.967265129089355
Epoch 1660, val loss: 0.9220747947692871
Epoch 1670, training loss: 897.069091796875 = 0.9153373837471008 + 100.0 * 8.96153736114502
Epoch 1670, val loss: 0.9200354218482971
Epoch 1680, training loss: 897.8195190429688 = 0.9132391810417175 + 100.0 * 8.969062805175781
Epoch 1680, val loss: 0.9180041551589966
Epoch 1690, training loss: 897.3792114257812 = 0.9110990166664124 + 100.0 * 8.964681625366211
Epoch 1690, val loss: 0.9159334897994995
Epoch 1700, training loss: 897.358154296875 = 0.9089857339859009 + 100.0 * 8.964491844177246
Epoch 1700, val loss: 0.9139052033424377
Epoch 1710, training loss: 897.7854614257812 = 0.9068766832351685 + 100.0 * 8.968786239624023
Epoch 1710, val loss: 0.9118970036506653
Epoch 1720, training loss: 898.1878662109375 = 0.904798150062561 + 100.0 * 8.972830772399902
Epoch 1720, val loss: 0.9099001288414001
Epoch 1730, training loss: 897.9505615234375 = 0.9026767611503601 + 100.0 * 8.970479011535645
Epoch 1730, val loss: 0.9078670740127563
Epoch 1740, training loss: 898.053955078125 = 0.9005737900733948 + 100.0 * 8.97153377532959
Epoch 1740, val loss: 0.9059135913848877
Epoch 1750, training loss: 895.70556640625 = 0.8986116647720337 + 100.0 * 8.94806957244873
Epoch 1750, val loss: 0.9040066599845886
Epoch 1760, training loss: 895.5431518554688 = 0.8966496586799622 + 100.0 * 8.946464538574219
Epoch 1760, val loss: 0.9020543098449707
Epoch 1770, training loss: 896.64892578125 = 0.8946029543876648 + 100.0 * 8.95754337310791
Epoch 1770, val loss: 0.9000931978225708
Epoch 1780, training loss: 897.6640014648438 = 0.8926092982292175 + 100.0 * 8.967713356018066
Epoch 1780, val loss: 0.8981868028640747
Epoch 1790, training loss: 898.4378662109375 = 0.8905206918716431 + 100.0 * 8.975473403930664
Epoch 1790, val loss: 0.89618319272995
Epoch 1800, training loss: 898.5997314453125 = 0.8884097337722778 + 100.0 * 8.977112770080566
Epoch 1800, val loss: 0.8941712975502014
Epoch 1810, training loss: 898.246826171875 = 0.8862499594688416 + 100.0 * 8.97360610961914
Epoch 1810, val loss: 0.8920993208885193
Epoch 1820, training loss: 898.8525390625 = 0.8841763138771057 + 100.0 * 8.979683876037598
Epoch 1820, val loss: 0.890116810798645
Epoch 1830, training loss: 899.1632690429688 = 0.8820669651031494 + 100.0 * 8.98281192779541
Epoch 1830, val loss: 0.8881032466888428
Epoch 1840, training loss: 899.0953369140625 = 0.879956841468811 + 100.0 * 8.98215389251709
Epoch 1840, val loss: 0.8860729932785034
Epoch 1850, training loss: 899.3241577148438 = 0.8778471946716309 + 100.0 * 8.98446273803711
Epoch 1850, val loss: 0.8840664029121399
Epoch 1860, training loss: 899.3489990234375 = 0.8757477402687073 + 100.0 * 8.984732627868652
Epoch 1860, val loss: 0.8820711970329285
Epoch 1870, training loss: 899.4581909179688 = 0.8736516833305359 + 100.0 * 8.985845565795898
Epoch 1870, val loss: 0.8800804615020752
Epoch 1880, training loss: 899.53759765625 = 0.871562659740448 + 100.0 * 8.98666000366211
Epoch 1880, val loss: 0.878083348274231
Epoch 1890, training loss: 899.1640014648438 = 0.8694531917572021 + 100.0 * 8.982945442199707
Epoch 1890, val loss: 0.8760825395584106
Epoch 1900, training loss: 899.7847900390625 = 0.8674100041389465 + 100.0 * 8.989173889160156
Epoch 1900, val loss: 0.8741135001182556
Epoch 1910, training loss: 900.202392578125 = 0.8653411865234375 + 100.0 * 8.993370056152344
Epoch 1910, val loss: 0.8721383810043335
Epoch 1920, training loss: 899.9683227539062 = 0.8632310628890991 + 100.0 * 8.991050720214844
Epoch 1920, val loss: 0.8701543807983398
Epoch 1930, training loss: 900.00634765625 = 0.8611759543418884 + 100.0 * 8.991451263427734
Epoch 1930, val loss: 0.868179976940155
Epoch 1940, training loss: 899.6644897460938 = 0.859076201915741 + 100.0 * 8.988054275512695
Epoch 1940, val loss: 0.8662182688713074
Epoch 1950, training loss: 899.5381469726562 = 0.8570592999458313 + 100.0 * 8.986810684204102
Epoch 1950, val loss: 0.8642643094062805
Epoch 1960, training loss: 899.8655395507812 = 0.8550010919570923 + 100.0 * 8.990105628967285
Epoch 1960, val loss: 0.8623356819152832
Epoch 1970, training loss: 900.601318359375 = 0.8529701828956604 + 100.0 * 8.997483253479004
Epoch 1970, val loss: 0.8603871464729309
Epoch 1980, training loss: 900.6226806640625 = 0.85089510679245 + 100.0 * 8.99771785736084
Epoch 1980, val loss: 0.8584339022636414
Epoch 1990, training loss: 900.8641357421875 = 0.8488786816596985 + 100.0 * 9.000152587890625
Epoch 1990, val loss: 0.8565298318862915
Epoch 2000, training loss: 900.5902099609375 = 0.8468390107154846 + 100.0 * 8.99743366241455
Epoch 2000, val loss: 0.8545948266983032
Epoch 2010, training loss: 901.0607299804688 = 0.8448142409324646 + 100.0 * 9.002159118652344
Epoch 2010, val loss: 0.8526962995529175
Epoch 2020, training loss: 901.3900756835938 = 0.842808187007904 + 100.0 * 9.005472183227539
Epoch 2020, val loss: 0.8507663011550903
Epoch 2030, training loss: 901.0462646484375 = 0.8407921195030212 + 100.0 * 9.002054214477539
Epoch 2030, val loss: 0.8488559126853943
Epoch 2040, training loss: 901.3978271484375 = 0.838778018951416 + 100.0 * 9.005590438842773
Epoch 2040, val loss: 0.8469629287719727
Epoch 2050, training loss: 901.8167724609375 = 0.8367818593978882 + 100.0 * 9.00979995727539
Epoch 2050, val loss: 0.845089852809906
Epoch 2060, training loss: 901.5418090820312 = 0.834793746471405 + 100.0 * 9.007070541381836
Epoch 2060, val loss: 0.8431922197341919
Epoch 2070, training loss: 901.76123046875 = 0.8328099250793457 + 100.0 * 9.009284019470215
Epoch 2070, val loss: 0.8413496017456055
Epoch 2080, training loss: 900.6554565429688 = 0.8308188319206238 + 100.0 * 8.998246192932129
Epoch 2080, val loss: 0.8394258618354797
Epoch 2090, training loss: 901.1592407226562 = 0.8289008736610413 + 100.0 * 9.003303527832031
Epoch 2090, val loss: 0.8376368284225464
Epoch 2100, training loss: 901.6779174804688 = 0.826947808265686 + 100.0 * 9.008509635925293
Epoch 2100, val loss: 0.8358043432235718
Epoch 2110, training loss: 902.0177001953125 = 0.8250030875205994 + 100.0 * 9.011926651000977
Epoch 2110, val loss: 0.8339657187461853
Epoch 2120, training loss: 901.6787109375 = 0.8230272531509399 + 100.0 * 9.008557319641113
Epoch 2120, val loss: 0.8321169018745422
Epoch 2130, training loss: 901.4639282226562 = 0.8210599422454834 + 100.0 * 9.006428718566895
Epoch 2130, val loss: 0.830262303352356
Epoch 2140, training loss: 902.0250854492188 = 0.8191757798194885 + 100.0 * 9.012059211730957
Epoch 2140, val loss: 0.8284708857536316
Epoch 2150, training loss: 902.540771484375 = 0.8172594308853149 + 100.0 * 9.017234802246094
Epoch 2150, val loss: 0.8266757726669312
Epoch 2160, training loss: 902.7510375976562 = 0.8153382539749146 + 100.0 * 9.019356727600098
Epoch 2160, val loss: 0.8248674273490906
Epoch 2170, training loss: 902.6978759765625 = 0.8134152293205261 + 100.0 * 9.018844604492188
Epoch 2170, val loss: 0.8230458498001099
Epoch 2180, training loss: 902.5101318359375 = 0.8115179538726807 + 100.0 * 9.016985893249512
Epoch 2180, val loss: 0.8212807774543762
Epoch 2190, training loss: 902.9603881835938 = 0.8096243739128113 + 100.0 * 9.021507263183594
Epoch 2190, val loss: 0.819501519203186
Epoch 2200, training loss: 903.2012939453125 = 0.8077479004859924 + 100.0 * 9.023935317993164
Epoch 2200, val loss: 0.8177258372306824
Epoch 2210, training loss: 903.4028930664062 = 0.8058688640594482 + 100.0 * 9.025970458984375
Epoch 2210, val loss: 0.8159781694412231
Epoch 2220, training loss: 902.9657592773438 = 0.803960382938385 + 100.0 * 9.021617889404297
Epoch 2220, val loss: 0.814225435256958
Epoch 2230, training loss: 902.6023559570312 = 0.8021392822265625 + 100.0 * 9.0180025100708
Epoch 2230, val loss: 0.8124897480010986
Epoch 2240, training loss: 902.4024658203125 = 0.8002973198890686 + 100.0 * 9.016021728515625
Epoch 2240, val loss: 0.8107756972312927
Epoch 2250, training loss: 902.9702758789062 = 0.7984519600868225 + 100.0 * 9.02171802520752
Epoch 2250, val loss: 0.8090554475784302
Epoch 2260, training loss: 903.1978149414062 = 0.7966392040252686 + 100.0 * 9.024011611938477
Epoch 2260, val loss: 0.8073616027832031
Epoch 2270, training loss: 903.5040893554688 = 0.7948137521743774 + 100.0 * 9.027092933654785
Epoch 2270, val loss: 0.8056654334068298
Epoch 2280, training loss: 902.434814453125 = 0.7929736375808716 + 100.0 * 9.01641845703125
Epoch 2280, val loss: 0.8039407134056091
Epoch 2290, training loss: 902.8350219726562 = 0.7911950945854187 + 100.0 * 9.020438194274902
Epoch 2290, val loss: 0.8022870421409607
Epoch 2300, training loss: 903.5889282226562 = 0.7894410490989685 + 100.0 * 9.027995109558105
Epoch 2300, val loss: 0.8006501793861389
Epoch 2310, training loss: 901.7200317382812 = 0.787556529045105 + 100.0 * 9.00932502746582
Epoch 2310, val loss: 0.7989795804023743
Epoch 2320, training loss: 900.8175659179688 = 0.7857998013496399 + 100.0 * 9.000317573547363
Epoch 2320, val loss: 0.7972946166992188
Epoch 2330, training loss: 901.4009399414062 = 0.7840949892997742 + 100.0 * 9.006168365478516
Epoch 2330, val loss: 0.7956826090812683
Epoch 2340, training loss: 902.4783935546875 = 0.7823678851127625 + 100.0 * 9.016960144042969
Epoch 2340, val loss: 0.7940762042999268
Epoch 2350, training loss: 903.0660400390625 = 0.7806473970413208 + 100.0 * 9.02285385131836
Epoch 2350, val loss: 0.7924521565437317
Epoch 2360, training loss: 903.5512084960938 = 0.7788918614387512 + 100.0 * 9.02772331237793
Epoch 2360, val loss: 0.7908138036727905
Epoch 2370, training loss: 904.0368041992188 = 0.7771623134613037 + 100.0 * 9.032596588134766
Epoch 2370, val loss: 0.7892004251480103
Epoch 2380, training loss: 903.8469848632812 = 0.775408148765564 + 100.0 * 9.030715942382812
Epoch 2380, val loss: 0.787581741809845
Epoch 2390, training loss: 903.74658203125 = 0.7736643552780151 + 100.0 * 9.029728889465332
Epoch 2390, val loss: 0.7859764099121094
Epoch 2400, training loss: 903.9456176757812 = 0.7719617486000061 + 100.0 * 9.031736373901367
Epoch 2400, val loss: 0.784386396408081
Epoch 2410, training loss: 904.4299926757812 = 0.7702672481536865 + 100.0 * 9.03659725189209
Epoch 2410, val loss: 0.7828192710876465
Epoch 2420, training loss: 904.2974243164062 = 0.7685645818710327 + 100.0 * 9.03528881072998
Epoch 2420, val loss: 0.7812589406967163
Epoch 2430, training loss: 903.9694213867188 = 0.7668774127960205 + 100.0 * 9.032025337219238
Epoch 2430, val loss: 0.7797187566757202
Epoch 2440, training loss: 904.1840209960938 = 0.7652133703231812 + 100.0 * 9.034188270568848
Epoch 2440, val loss: 0.7782036662101746
Epoch 2450, training loss: 904.4946899414062 = 0.7635564804077148 + 100.0 * 9.037311553955078
Epoch 2450, val loss: 0.7766794562339783
Epoch 2460, training loss: 904.5692749023438 = 0.7619244456291199 + 100.0 * 9.038073539733887
Epoch 2460, val loss: 0.7751727104187012
Epoch 2470, training loss: 904.4782104492188 = 0.7602936029434204 + 100.0 * 9.037178993225098
Epoch 2470, val loss: 0.7737032175064087
Epoch 2480, training loss: 904.6768188476562 = 0.7586851716041565 + 100.0 * 9.03918170928955
Epoch 2480, val loss: 0.7722172737121582
Epoch 2490, training loss: 905.0782470703125 = 0.7570851445198059 + 100.0 * 9.043211936950684
Epoch 2490, val loss: 0.770729660987854
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.6952173913043478
0.8624212127798305
The final CL Acc:0.67324, 0.11155, The final GNN Acc:0.86380, 0.00105
Begin epxeriment: noisy_level: 0.2 cont_weight: 100 epoch: der1:0.3 der2:0.4 dfr1:0.4 dfr2:0.3
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=2500, cl_num_layers=2, cl_num_proj_hidden=64, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=100.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.4, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=32, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, noisy_level=0.2, num_hidden=64, num_proj_hidden=64, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=40, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 88648])
add edge: torch.Size([2, 106526])
remove edge: torch.Size([2, 70736])
updated graph: torch.Size([2, 88614])
=== Noisy graph ===
/home/project-robust-contrastive/Robust-Contrastive-Learning/model.py:227: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 1022.1771240234375 = 1.099199652671814 + 100.0 * 10.210779190063477
Epoch 0, val loss: 1.098366141319275
Epoch 10, training loss: 985.1725463867188 = 1.0988199710845947 + 100.0 * 9.840737342834473
Epoch 10, val loss: 1.0979925394058228
Epoch 20, training loss: 968.7915649414062 = 1.0984622240066528 + 100.0 * 9.676931381225586
Epoch 20, val loss: 1.097604513168335
Epoch 30, training loss: 955.942138671875 = 1.0980489253997803 + 100.0 * 9.548440933227539
Epoch 30, val loss: 1.0971935987472534
Epoch 40, training loss: 945.3087768554688 = 1.0976680517196655 + 100.0 * 9.442111015319824
Epoch 40, val loss: 1.096799612045288
Epoch 50, training loss: 936.2760620117188 = 1.0972895622253418 + 100.0 * 9.351787567138672
Epoch 50, val loss: 1.0964088439941406
Epoch 60, training loss: 928.6361083984375 = 1.0968888998031616 + 100.0 * 9.275392532348633
Epoch 60, val loss: 1.095994472503662
Epoch 70, training loss: 922.0352783203125 = 1.0964794158935547 + 100.0 * 9.20938777923584
Epoch 70, val loss: 1.0955731868743896
Epoch 80, training loss: 916.3138427734375 = 1.0960613489151 + 100.0 * 9.152177810668945
Epoch 80, val loss: 1.095143437385559
Epoch 90, training loss: 911.3416137695312 = 1.0956321954727173 + 100.0 * 9.102459907531738
Epoch 90, val loss: 1.0947034358978271
Epoch 100, training loss: 907.0520629882812 = 1.0951969623565674 + 100.0 * 9.059568405151367
Epoch 100, val loss: 1.094259262084961
Epoch 110, training loss: 903.2589721679688 = 1.0947558879852295 + 100.0 * 9.021642684936523
Epoch 110, val loss: 1.0938022136688232
Epoch 120, training loss: 900.0322875976562 = 1.0943032503128052 + 100.0 * 8.9893798828125
Epoch 120, val loss: 1.0933476686477661
Epoch 130, training loss: 897.2786254882812 = 1.0938842296600342 + 100.0 * 8.961847305297852
Epoch 130, val loss: 1.0929173231124878
Epoch 140, training loss: 894.7945556640625 = 1.0935276746749878 + 100.0 * 8.937010765075684
Epoch 140, val loss: 1.0925627946853638
Epoch 150, training loss: 892.7172241210938 = 1.0932921171188354 + 100.0 * 8.916239738464355
Epoch 150, val loss: 1.0923268795013428
Epoch 160, training loss: 890.8755493164062 = 1.0931382179260254 + 100.0 * 8.89782428741455
Epoch 160, val loss: 1.0921690464019775
Epoch 170, training loss: 889.185546875 = 1.09299635887146 + 100.0 * 8.880925178527832
Epoch 170, val loss: 1.0920186042785645
Epoch 180, training loss: 887.8233032226562 = 1.092850923538208 + 100.0 * 8.867304801940918
Epoch 180, val loss: 1.091867208480835
Epoch 190, training loss: 886.7908325195312 = 1.092694878578186 + 100.0 * 8.85698127746582
Epoch 190, val loss: 1.0917085409164429
Epoch 200, training loss: 885.7488403320312 = 1.0925538539886475 + 100.0 * 8.846563339233398
Epoch 200, val loss: 1.0915485620498657
Epoch 210, training loss: 885.1598510742188 = 1.0924009084701538 + 100.0 * 8.84067440032959
Epoch 210, val loss: 1.0913951396942139
Epoch 220, training loss: 884.2791137695312 = 1.0922434329986572 + 100.0 * 8.831869125366211
Epoch 220, val loss: 1.0912222862243652
Epoch 230, training loss: 883.8101806640625 = 1.0920777320861816 + 100.0 * 8.827180862426758
Epoch 230, val loss: 1.0910452604293823
Epoch 240, training loss: 883.14111328125 = 1.0919092893600464 + 100.0 * 8.820491790771484
Epoch 240, val loss: 1.0908827781677246
Epoch 250, training loss: 882.9055786132812 = 1.0917328596115112 + 100.0 * 8.818138122558594
Epoch 250, val loss: 1.0906966924667358
Epoch 260, training loss: 882.318359375 = 1.091554045677185 + 100.0 * 8.812268257141113
Epoch 260, val loss: 1.0905144214630127
Epoch 270, training loss: 882.5916137695312 = 1.0913679599761963 + 100.0 * 8.81500244140625
Epoch 270, val loss: 1.0903325080871582
Epoch 280, training loss: 881.864990234375 = 1.0911593437194824 + 100.0 * 8.807738304138184
Epoch 280, val loss: 1.0901193618774414
Epoch 290, training loss: 881.2796630859375 = 1.0909565687179565 + 100.0 * 8.801887512207031
Epoch 290, val loss: 1.089911699295044
Epoch 300, training loss: 881.6549682617188 = 1.090772032737732 + 100.0 * 8.805642127990723
Epoch 300, val loss: 1.089719295501709
Epoch 310, training loss: 881.3432006835938 = 1.0905463695526123 + 100.0 * 8.802526473999023
Epoch 310, val loss: 1.0895013809204102
Epoch 320, training loss: 881.4983520507812 = 1.0903382301330566 + 100.0 * 8.80408000946045
Epoch 320, val loss: 1.0892913341522217
Epoch 330, training loss: 881.4094848632812 = 1.0901037454605103 + 100.0 * 8.803194046020508
Epoch 330, val loss: 1.0890663862228394
Epoch 340, training loss: 881.2711791992188 = 1.0898722410202026 + 100.0 * 8.801813125610352
Epoch 340, val loss: 1.0888372659683228
Epoch 350, training loss: 881.4882202148438 = 1.0896321535110474 + 100.0 * 8.803985595703125
Epoch 350, val loss: 1.0886051654815674
Epoch 360, training loss: 881.5011596679688 = 1.0893810987472534 + 100.0 * 8.804118156433105
Epoch 360, val loss: 1.0883591175079346
Epoch 370, training loss: 881.0738525390625 = 1.0891170501708984 + 100.0 * 8.799847602844238
Epoch 370, val loss: 1.088098168373108
Epoch 380, training loss: 881.521728515625 = 1.0888577699661255 + 100.0 * 8.804328918457031
Epoch 380, val loss: 1.087844967842102
Epoch 390, training loss: 881.2800903320312 = 1.0885868072509766 + 100.0 * 8.801915168762207
Epoch 390, val loss: 1.0875866413116455
Epoch 400, training loss: 881.3026123046875 = 1.088301420211792 + 100.0 * 8.802143096923828
Epoch 400, val loss: 1.08730947971344
Epoch 410, training loss: 881.70703125 = 1.0880118608474731 + 100.0 * 8.806190490722656
Epoch 410, val loss: 1.087031364440918
Epoch 420, training loss: 881.8134765625 = 1.0877048969268799 + 100.0 * 8.807257652282715
Epoch 420, val loss: 1.0867353677749634
Epoch 430, training loss: 882.0442504882812 = 1.087393045425415 + 100.0 * 8.809568405151367
Epoch 430, val loss: 1.0864286422729492
Epoch 440, training loss: 882.0128173828125 = 1.0870660543441772 + 100.0 * 8.809257507324219
Epoch 440, val loss: 1.0861152410507202
Epoch 450, training loss: 882.1541137695312 = 1.0867277383804321 + 100.0 * 8.810673713684082
Epoch 450, val loss: 1.0857861042022705
Epoch 460, training loss: 882.5474853515625 = 1.0863838195800781 + 100.0 * 8.814611434936523
Epoch 460, val loss: 1.0854649543762207
Epoch 470, training loss: 882.6920776367188 = 1.0860434770584106 + 100.0 * 8.816060066223145
Epoch 470, val loss: 1.0851266384124756
Epoch 480, training loss: 882.6220703125 = 1.0856796503067017 + 100.0 * 8.815363883972168
Epoch 480, val loss: 1.0847939252853394
Epoch 490, training loss: 883.0203247070312 = 1.0853303670883179 + 100.0 * 8.819350242614746
Epoch 490, val loss: 1.084457278251648
Epoch 500, training loss: 883.0914306640625 = 1.0849703550338745 + 100.0 * 8.820064544677734
Epoch 500, val loss: 1.084114909172058
Epoch 510, training loss: 883.4381103515625 = 1.0846190452575684 + 100.0 * 8.823534965515137
Epoch 510, val loss: 1.0837733745574951
Epoch 520, training loss: 881.9796142578125 = 1.0841844081878662 + 100.0 * 8.808954238891602
Epoch 520, val loss: 1.0833879709243774
Epoch 530, training loss: 884.7866821289062 = 1.0838980674743652 + 100.0 * 8.837027549743652
Epoch 530, val loss: 1.0831156969070435
Epoch 540, training loss: 882.9781494140625 = 1.0834887027740479 + 100.0 * 8.818946838378906
Epoch 540, val loss: 1.082720398902893
Epoch 550, training loss: 882.7442626953125 = 1.0831414461135864 + 100.0 * 8.816611289978027
Epoch 550, val loss: 1.0823838710784912
Epoch 560, training loss: 882.8575439453125 = 1.0827643871307373 + 100.0 * 8.817748069763184
Epoch 560, val loss: 1.0820170640945435
Epoch 570, training loss: 883.4320068359375 = 1.0824153423309326 + 100.0 * 8.823495864868164
Epoch 570, val loss: 1.0816880464553833
Epoch 580, training loss: 883.9561157226562 = 1.0820316076278687 + 100.0 * 8.828741073608398
Epoch 580, val loss: 1.0813297033309937
Epoch 590, training loss: 884.5352172851562 = 1.0816634893417358 + 100.0 * 8.834535598754883
Epoch 590, val loss: 1.0809863805770874
Epoch 600, training loss: 885.4688110351562 = 1.0812808275222778 + 100.0 * 8.84387493133545
Epoch 600, val loss: 1.0806094408035278
Epoch 610, training loss: 885.085205078125 = 1.0808625221252441 + 100.0 * 8.840043067932129
Epoch 610, val loss: 1.0802335739135742
Epoch 620, training loss: 885.6646728515625 = 1.080467700958252 + 100.0 * 8.845842361450195
Epoch 620, val loss: 1.0798441171646118
Epoch 630, training loss: 886.3350219726562 = 1.0800930261611938 + 100.0 * 8.85254955291748
Epoch 630, val loss: 1.079501986503601
Epoch 640, training loss: 885.5803833007812 = 1.0796849727630615 + 100.0 * 8.845006942749023
Epoch 640, val loss: 1.0791146755218506
Epoch 650, training loss: 886.915771484375 = 1.079307198524475 + 100.0 * 8.858365058898926
Epoch 650, val loss: 1.0787668228149414
Epoch 660, training loss: 887.4379272460938 = 1.078892707824707 + 100.0 * 8.863590240478516
Epoch 660, val loss: 1.0783698558807373
Epoch 670, training loss: 887.2366943359375 = 1.07846999168396 + 100.0 * 8.861581802368164
Epoch 670, val loss: 1.0779860019683838
Epoch 680, training loss: 887.779052734375 = 1.078059196472168 + 100.0 * 8.867010116577148
Epoch 680, val loss: 1.0775911808013916
Epoch 690, training loss: 888.3158569335938 = 1.0776370763778687 + 100.0 * 8.872382164001465
Epoch 690, val loss: 1.0772004127502441
Epoch 700, training loss: 888.2086181640625 = 1.0771936178207397 + 100.0 * 8.87131404876709
Epoch 700, val loss: 1.0767762660980225
Epoch 710, training loss: 888.470458984375 = 1.076761245727539 + 100.0 * 8.873936653137207
Epoch 710, val loss: 1.0763561725616455
Epoch 720, training loss: 888.8040771484375 = 1.0763251781463623 + 100.0 * 8.877277374267578
Epoch 720, val loss: 1.0759671926498413
Epoch 730, training loss: 888.9583129882812 = 1.075881004333496 + 100.0 * 8.878824234008789
Epoch 730, val loss: 1.075546383857727
Epoch 740, training loss: 889.5230102539062 = 1.0754432678222656 + 100.0 * 8.884475708007812
Epoch 740, val loss: 1.0751314163208008
Epoch 750, training loss: 890.3499755859375 = 1.0749878883361816 + 100.0 * 8.892749786376953
Epoch 750, val loss: 1.0747076272964478
Epoch 760, training loss: 890.152587890625 = 1.074522614479065 + 100.0 * 8.890780448913574
Epoch 760, val loss: 1.0742835998535156
Epoch 770, training loss: 890.510986328125 = 1.0740779638290405 + 100.0 * 8.894369125366211
Epoch 770, val loss: 1.0738632678985596
Epoch 780, training loss: 890.9534301757812 = 1.073618769645691 + 100.0 * 8.898797988891602
Epoch 780, val loss: 1.0734398365020752
Epoch 790, training loss: 891.2902221679688 = 1.0731477737426758 + 100.0 * 8.90217113494873
Epoch 790, val loss: 1.0730098485946655
Epoch 800, training loss: 891.3931884765625 = 1.0726799964904785 + 100.0 * 8.903204917907715
Epoch 800, val loss: 1.072570562362671
Epoch 810, training loss: 891.7506103515625 = 1.072218656539917 + 100.0 * 8.906784057617188
Epoch 810, val loss: 1.0721310377120972
Epoch 820, training loss: 891.7457275390625 = 1.0717363357543945 + 100.0 * 8.906740188598633
Epoch 820, val loss: 1.071686029434204
Epoch 830, training loss: 891.9368896484375 = 1.0712600946426392 + 100.0 * 8.908656120300293
Epoch 830, val loss: 1.0712356567382812
Epoch 840, training loss: 892.0596923828125 = 1.0707775354385376 + 100.0 * 8.909889221191406
Epoch 840, val loss: 1.0707895755767822
Epoch 850, training loss: 892.1190795898438 = 1.0702811479568481 + 100.0 * 8.91048812866211
Epoch 850, val loss: 1.0703178644180298
Epoch 860, training loss: 891.8607788085938 = 1.0697879791259766 + 100.0 * 8.907910346984863
Epoch 860, val loss: 1.0698747634887695
Epoch 870, training loss: 892.5430297851562 = 1.069297432899475 + 100.0 * 8.914737701416016
Epoch 870, val loss: 1.0694161653518677
Epoch 880, training loss: 892.8468627929688 = 1.0687803030014038 + 100.0 * 8.917780876159668
Epoch 880, val loss: 1.0689384937286377
Epoch 890, training loss: 892.8612670898438 = 1.0682899951934814 + 100.0 * 8.917929649353027
Epoch 890, val loss: 1.068485975265503
Epoch 900, training loss: 893.1018676757812 = 1.067786455154419 + 100.0 * 8.920340538024902
Epoch 900, val loss: 1.0680111646652222
Epoch 910, training loss: 893.50927734375 = 1.0672732591629028 + 100.0 * 8.924420356750488
Epoch 910, val loss: 1.0675286054611206
Epoch 920, training loss: 893.5499267578125 = 1.0667519569396973 + 100.0 * 8.92483139038086
Epoch 920, val loss: 1.0670490264892578
Epoch 930, training loss: 893.833251953125 = 1.066221833229065 + 100.0 * 8.9276704788208
Epoch 930, val loss: 1.0665613412857056
Epoch 940, training loss: 894.1973876953125 = 1.0656731128692627 + 100.0 * 8.931317329406738
Epoch 940, val loss: 1.0660383701324463
Epoch 950, training loss: 894.3651733398438 = 1.065154790878296 + 100.0 * 8.933000564575195
Epoch 950, val loss: 1.0655508041381836
Epoch 960, training loss: 893.8167724609375 = 1.0645912885665894 + 100.0 * 8.927521705627441
Epoch 960, val loss: 1.065025806427002
Epoch 970, training loss: 894.2280883789062 = 1.0640348196029663 + 100.0 * 8.931640625
Epoch 970, val loss: 1.064509391784668
Epoch 980, training loss: 893.9657592773438 = 1.063485860824585 + 100.0 * 8.929022789001465
Epoch 980, val loss: 1.0639888048171997
Epoch 990, training loss: 894.1295166015625 = 1.0629254579544067 + 100.0 * 8.930665969848633
Epoch 990, val loss: 1.0634649991989136
Epoch 1000, training loss: 894.760498046875 = 1.0623751878738403 + 100.0 * 8.936981201171875
Epoch 1000, val loss: 1.0629572868347168
Epoch 1010, training loss: 895.2424926757812 = 1.0618456602096558 + 100.0 * 8.94180679321289
Epoch 1010, val loss: 1.0624558925628662
Epoch 1020, training loss: 895.360107421875 = 1.0612766742706299 + 100.0 * 8.942988395690918
Epoch 1020, val loss: 1.0619319677352905
Epoch 1030, training loss: 895.7066040039062 = 1.0607279539108276 + 100.0 * 8.94645881652832
Epoch 1030, val loss: 1.0614042282104492
Epoch 1040, training loss: 895.1976928710938 = 1.060136079788208 + 100.0 * 8.941375732421875
Epoch 1040, val loss: 1.0608527660369873
Epoch 1050, training loss: 895.7328491210938 = 1.059587001800537 + 100.0 * 8.946732521057129
Epoch 1050, val loss: 1.0603526830673218
Epoch 1060, training loss: 896.2291259765625 = 1.059027075767517 + 100.0 * 8.951701164245605
Epoch 1060, val loss: 1.0598214864730835
Epoch 1070, training loss: 896.0093383789062 = 1.0584486722946167 + 100.0 * 8.949508666992188
Epoch 1070, val loss: 1.0592987537384033
Epoch 1080, training loss: 896.5872802734375 = 1.0579012632369995 + 100.0 * 8.955293655395508
Epoch 1080, val loss: 1.0587682723999023
Epoch 1090, training loss: 896.9225463867188 = 1.0573294162750244 + 100.0 * 8.95865249633789
Epoch 1090, val loss: 1.0582506656646729
Epoch 1100, training loss: 897.08203125 = 1.0567611455917358 + 100.0 * 8.96025276184082
Epoch 1100, val loss: 1.0577157735824585
Epoch 1110, training loss: 896.223388671875 = 1.0562034845352173 + 100.0 * 8.951671600341797
Epoch 1110, val loss: 1.0571874380111694
Epoch 1120, training loss: 895.2039794921875 = 1.055562973022461 + 100.0 * 8.941484451293945
Epoch 1120, val loss: 1.056627869606018
Epoch 1130, training loss: 895.4400024414062 = 1.0550488233566284 + 100.0 * 8.943849563598633
Epoch 1130, val loss: 1.0561161041259766
Epoch 1140, training loss: 896.5755004882812 = 1.0545017719268799 + 100.0 * 8.955209732055664
Epoch 1140, val loss: 1.0556033849716187
Epoch 1150, training loss: 897.1829223632812 = 1.053931713104248 + 100.0 * 8.961289405822754
Epoch 1150, val loss: 1.0550732612609863
Epoch 1160, training loss: 897.4628295898438 = 1.0533696413040161 + 100.0 * 8.964095115661621
Epoch 1160, val loss: 1.0545512437820435
Epoch 1170, training loss: 897.6858520507812 = 1.052778720855713 + 100.0 * 8.966330528259277
Epoch 1170, val loss: 1.0540000200271606
Epoch 1180, training loss: 897.4675903320312 = 1.0521706342697144 + 100.0 * 8.964154243469238
Epoch 1180, val loss: 1.0534392595291138
Epoch 1190, training loss: 897.8377685546875 = 1.0516067743301392 + 100.0 * 8.96786117553711
Epoch 1190, val loss: 1.0529142618179321
Epoch 1200, training loss: 898.5543212890625 = 1.0510499477386475 + 100.0 * 8.975032806396484
Epoch 1200, val loss: 1.0523988008499146
Epoch 1210, training loss: 898.6845092773438 = 1.0504859685897827 + 100.0 * 8.976340293884277
Epoch 1210, val loss: 1.0518704652786255
Epoch 1220, training loss: 898.7647705078125 = 1.0498926639556885 + 100.0 * 8.97714900970459
Epoch 1220, val loss: 1.0513144731521606
Epoch 1230, training loss: 899.1102294921875 = 1.0493067502975464 + 100.0 * 8.980608940124512
Epoch 1230, val loss: 1.0507667064666748
Epoch 1240, training loss: 899.437255859375 = 1.0487464666366577 + 100.0 * 8.983884811401367
Epoch 1240, val loss: 1.0502277612686157
Epoch 1250, training loss: 899.8208618164062 = 1.048154354095459 + 100.0 * 8.987727165222168
Epoch 1250, val loss: 1.0496892929077148
Epoch 1260, training loss: 899.912841796875 = 1.047574758529663 + 100.0 * 8.988653182983398
Epoch 1260, val loss: 1.0491400957107544
Epoch 1270, training loss: 900.2804565429688 = 1.0469887256622314 + 100.0 * 8.992334365844727
Epoch 1270, val loss: 1.048589825630188
Epoch 1280, training loss: 900.2227783203125 = 1.0464016199111938 + 100.0 * 8.991764068603516
Epoch 1280, val loss: 1.0480304956436157
Epoch 1290, training loss: 899.833251953125 = 1.0458046197891235 + 100.0 * 8.987874984741211
Epoch 1290, val loss: 1.0474804639816284
Epoch 1300, training loss: 900.3933715820312 = 1.045211672782898 + 100.0 * 8.993481636047363
Epoch 1300, val loss: 1.0469422340393066
Epoch 1310, training loss: 900.8402099609375 = 1.0446540117263794 + 100.0 * 8.997955322265625
Epoch 1310, val loss: 1.046403408050537
Epoch 1320, training loss: 900.897216796875 = 1.0440747737884521 + 100.0 * 8.998531341552734
Epoch 1320, val loss: 1.0458587408065796
Epoch 1330, training loss: 900.53466796875 = 1.0434486865997314 + 100.0 * 8.994912147521973
Epoch 1330, val loss: 1.045291543006897
Epoch 1340, training loss: 900.784912109375 = 1.0428805351257324 + 100.0 * 8.997420310974121
Epoch 1340, val loss: 1.0447392463684082
Epoch 1350, training loss: 901.2396240234375 = 1.0422996282577515 + 100.0 * 9.001973152160645
Epoch 1350, val loss: 1.0442079305648804
Epoch 1360, training loss: 902.0213623046875 = 1.0417417287826538 + 100.0 * 9.009796142578125
Epoch 1360, val loss: 1.0436755418777466
Epoch 1370, training loss: 901.8861694335938 = 1.041154146194458 + 100.0 * 9.008450508117676
Epoch 1370, val loss: 1.043123722076416
Epoch 1380, training loss: 902.2745361328125 = 1.0405755043029785 + 100.0 * 9.01233959197998
Epoch 1380, val loss: 1.0425808429718018
Epoch 1390, training loss: 892.9905395507812 = 1.039434552192688 + 100.0 * 8.919510841369629
Epoch 1390, val loss: 1.0413857698440552
Epoch 1400, training loss: 894.1017456054688 = 1.0388944149017334 + 100.0 * 8.930628776550293
Epoch 1400, val loss: 1.0410059690475464
Epoch 1410, training loss: 896.4996337890625 = 1.038641095161438 + 100.0 * 8.954609870910645
Epoch 1410, val loss: 1.0407947301864624
Epoch 1420, training loss: 896.3898315429688 = 1.0381866693496704 + 100.0 * 8.953516006469727
Epoch 1420, val loss: 1.0403770208358765
Epoch 1430, training loss: 896.9666748046875 = 1.0377023220062256 + 100.0 * 8.95928955078125
Epoch 1430, val loss: 1.039923906326294
Epoch 1440, training loss: 898.4130249023438 = 1.0371474027633667 + 100.0 * 8.973758697509766
Epoch 1440, val loss: 1.0394152402877808
Epoch 1450, training loss: 899.680908203125 = 1.0366270542144775 + 100.0 * 8.986442565917969
Epoch 1450, val loss: 1.0389348268508911
Epoch 1460, training loss: 900.2880249023438 = 1.0360620021820068 + 100.0 * 8.99251937866211
Epoch 1460, val loss: 1.0383981466293335
Epoch 1470, training loss: 900.8820190429688 = 1.0354727506637573 + 100.0 * 8.998465538024902
Epoch 1470, val loss: 1.0378501415252686
Epoch 1480, training loss: 901.3209228515625 = 1.034892201423645 + 100.0 * 9.002860069274902
Epoch 1480, val loss: 1.0373088121414185
Epoch 1490, training loss: 901.673095703125 = 1.0343140363693237 + 100.0 * 9.006387710571289
Epoch 1490, val loss: 1.0367711782455444
Epoch 1500, training loss: 901.6925659179688 = 1.033720850944519 + 100.0 * 9.00658893585205
Epoch 1500, val loss: 1.0362215042114258
Epoch 1510, training loss: 901.998046875 = 1.0331333875656128 + 100.0 * 9.009649276733398
Epoch 1510, val loss: 1.035675048828125
Epoch 1520, training loss: 902.0565795898438 = 1.0325534343719482 + 100.0 * 9.01024055480957
Epoch 1520, val loss: 1.0351306200027466
Epoch 1530, training loss: 902.2442016601562 = 1.0319520235061646 + 100.0 * 9.01212215423584
Epoch 1530, val loss: 1.0345628261566162
Epoch 1540, training loss: 902.6253662109375 = 1.0313746929168701 + 100.0 * 9.015939712524414
Epoch 1540, val loss: 1.0340195894241333
Epoch 1550, training loss: 903.06591796875 = 1.0307904481887817 + 100.0 * 9.02035140991211
Epoch 1550, val loss: 1.0334826707839966
Epoch 1560, training loss: 903.2142333984375 = 1.030203104019165 + 100.0 * 9.02184009552002
Epoch 1560, val loss: 1.0329054594039917
Epoch 1570, training loss: 903.0750122070312 = 1.02959144115448 + 100.0 * 9.020454406738281
Epoch 1570, val loss: 1.0323677062988281
Epoch 1580, training loss: 903.3504028320312 = 1.0290167331695557 + 100.0 * 9.023214340209961
Epoch 1580, val loss: 1.031816005706787
Epoch 1590, training loss: 903.5144653320312 = 1.0284132957458496 + 100.0 * 9.024860382080078
Epoch 1590, val loss: 1.0312496423721313
Epoch 1600, training loss: 903.30029296875 = 1.0277844667434692 + 100.0 * 9.022725105285645
Epoch 1600, val loss: 1.0306611061096191
Epoch 1610, training loss: 903.7554321289062 = 1.0271522998809814 + 100.0 * 9.02728271484375
Epoch 1610, val loss: 1.0300686359405518
Epoch 1620, training loss: 904.318359375 = 1.026471734046936 + 100.0 * 9.032918930053711
Epoch 1620, val loss: 1.0294417142868042
Epoch 1630, training loss: 904.6265258789062 = 1.0257697105407715 + 100.0 * 9.03600788116455
Epoch 1630, val loss: 1.0287798643112183
Epoch 1640, training loss: 904.8633422851562 = 1.024953842163086 + 100.0 * 9.038383483886719
Epoch 1640, val loss: 1.0280498266220093
Epoch 1650, training loss: 904.2706298828125 = 1.0241271257400513 + 100.0 * 9.032464981079102
Epoch 1650, val loss: 1.0272912979125977
Epoch 1660, training loss: 904.65673828125 = 1.0233012437820435 + 100.0 * 9.036334037780762
Epoch 1660, val loss: 1.0265674591064453
Epoch 1670, training loss: 905.1173095703125 = 1.0225716829299927 + 100.0 * 9.040946960449219
Epoch 1670, val loss: 1.0258779525756836
Epoch 1680, training loss: 905.4161376953125 = 1.0217677354812622 + 100.0 * 9.043943405151367
Epoch 1680, val loss: 1.0251518487930298
Epoch 1690, training loss: 905.1859741210938 = 1.0210044384002686 + 100.0 * 9.04164981842041
Epoch 1690, val loss: 1.024436116218567
Epoch 1700, training loss: 905.1632690429688 = 1.0201971530914307 + 100.0 * 9.041430473327637
Epoch 1700, val loss: 1.02372407913208
Epoch 1710, training loss: 905.7149658203125 = 1.0194562673568726 + 100.0 * 9.046955108642578
Epoch 1710, val loss: 1.0230236053466797
Epoch 1720, training loss: 906.2244873046875 = 1.0187053680419922 + 100.0 * 9.052058219909668
Epoch 1720, val loss: 1.0223280191421509
Epoch 1730, training loss: 905.890625 = 1.017933964729309 + 100.0 * 9.048727035522461
Epoch 1730, val loss: 1.0216124057769775
Epoch 1740, training loss: 906.0125732421875 = 1.0171763896942139 + 100.0 * 9.049954414367676
Epoch 1740, val loss: 1.020930290222168
Epoch 1750, training loss: 906.4121704101562 = 1.0165140628814697 + 100.0 * 9.053956031799316
Epoch 1750, val loss: 1.0203173160552979
Epoch 1760, training loss: 906.8611450195312 = 1.0158798694610596 + 100.0 * 9.058452606201172
Epoch 1760, val loss: 1.0197066068649292
Epoch 1770, training loss: 907.0414428710938 = 1.0151852369308472 + 100.0 * 9.060262680053711
Epoch 1770, val loss: 1.0190601348876953
Epoch 1780, training loss: 906.977783203125 = 1.014480710029602 + 100.0 * 9.059633255004883
Epoch 1780, val loss: 1.0184276103973389
Epoch 1790, training loss: 903.59716796875 = 1.0137614011764526 + 100.0 * 9.025834083557129
Epoch 1790, val loss: 1.0177665948867798
Epoch 1800, training loss: 903.4132080078125 = 1.0129979848861694 + 100.0 * 9.024002075195312
Epoch 1800, val loss: 1.0171092748641968
Epoch 1810, training loss: 904.4530029296875 = 1.0123169422149658 + 100.0 * 9.034406661987305
Epoch 1810, val loss: 1.0164841413497925
Epoch 1820, training loss: 904.8580932617188 = 1.0117151737213135 + 100.0 * 9.038463592529297
Epoch 1820, val loss: 1.015924334526062
Epoch 1830, training loss: 906.1617431640625 = 1.011056900024414 + 100.0 * 9.051506996154785
Epoch 1830, val loss: 1.0153355598449707
Epoch 1840, training loss: 906.1766967773438 = 1.0103596448898315 + 100.0 * 9.051663398742676
Epoch 1840, val loss: 1.0147260427474976
Epoch 1850, training loss: 906.6324462890625 = 1.009722352027893 + 100.0 * 9.056227684020996
Epoch 1850, val loss: 1.014154076576233
Epoch 1860, training loss: 907.1249389648438 = 1.0090627670288086 + 100.0 * 9.061159133911133
Epoch 1860, val loss: 1.0135672092437744
Epoch 1870, training loss: 907.7738647460938 = 1.0084177255630493 + 100.0 * 9.067654609680176
Epoch 1870, val loss: 1.0129692554473877
Epoch 1880, training loss: 907.4183349609375 = 1.007727861404419 + 100.0 * 9.064105987548828
Epoch 1880, val loss: 1.012346625328064
Epoch 1890, training loss: 907.8875732421875 = 1.0070561170578003 + 100.0 * 9.068804740905762
Epoch 1890, val loss: 1.0117522478103638
Epoch 1900, training loss: 908.3157348632812 = 1.0063930749893188 + 100.0 * 9.07309341430664
Epoch 1900, val loss: 1.0111396312713623
Epoch 1910, training loss: 908.3848266601562 = 1.0057061910629272 + 100.0 * 9.07379150390625
Epoch 1910, val loss: 1.0105223655700684
Epoch 1920, training loss: 908.3722534179688 = 1.0050209760665894 + 100.0 * 9.0736722946167
Epoch 1920, val loss: 1.0099163055419922
Epoch 1930, training loss: 908.2750244140625 = 1.004341721534729 + 100.0 * 9.072707176208496
Epoch 1930, val loss: 1.009303331375122
Epoch 1940, training loss: 908.632080078125 = 1.003683090209961 + 100.0 * 9.076284408569336
Epoch 1940, val loss: 1.0087175369262695
Epoch 1950, training loss: 908.7604370117188 = 1.003010630607605 + 100.0 * 9.077574729919434
Epoch 1950, val loss: 1.0081027746200562
Epoch 1960, training loss: 908.0415649414062 = 1.0022985935211182 + 100.0 * 9.070392608642578
Epoch 1960, val loss: 1.0075093507766724
Epoch 1970, training loss: 908.0116577148438 = 1.001660943031311 + 100.0 * 9.070099830627441
Epoch 1970, val loss: 1.0069212913513184
Epoch 1980, training loss: 909.052978515625 = 1.0010333061218262 + 100.0 * 9.080519676208496
Epoch 1980, val loss: 1.0063393115997314
Epoch 1990, training loss: 909.8428955078125 = 1.0004255771636963 + 100.0 * 9.088424682617188
Epoch 1990, val loss: 1.0058073997497559
Epoch 2000, training loss: 910.2183837890625 = 0.9997778534889221 + 100.0 * 9.092185974121094
Epoch 2000, val loss: 1.0052307844161987
Epoch 2010, training loss: 910.5133666992188 = 0.9991302490234375 + 100.0 * 9.095142364501953
Epoch 2010, val loss: 1.0046303272247314
Epoch 2020, training loss: 910.4431762695312 = 0.9984496235847473 + 100.0 * 9.094447135925293
Epoch 2020, val loss: 1.0040372610092163
Epoch 2030, training loss: 910.9451293945312 = 0.9977957010269165 + 100.0 * 9.099472999572754
Epoch 2030, val loss: 1.0034602880477905
Epoch 2040, training loss: 911.0660400390625 = 0.99713134765625 + 100.0 * 9.100688934326172
Epoch 2040, val loss: 1.0028659105300903
Epoch 2050, training loss: 910.9583740234375 = 0.9964513778686523 + 100.0 * 9.099618911743164
Epoch 2050, val loss: 1.0022627115249634
Epoch 2060, training loss: 911.2158203125 = 0.9957806468009949 + 100.0 * 9.102200508117676
Epoch 2060, val loss: 1.0016621351242065
Epoch 2070, training loss: 910.2089233398438 = 0.9950941801071167 + 100.0 * 9.092138290405273
Epoch 2070, val loss: 1.0010499954223633
Epoch 2080, training loss: 910.3349609375 = 0.9944090247154236 + 100.0 * 9.093405723571777
Epoch 2080, val loss: 1.00045645236969
Epoch 2090, training loss: 910.8931274414062 = 0.9937171339988708 + 100.0 * 9.098994255065918
Epoch 2090, val loss: 0.9998455047607422
Epoch 2100, training loss: 911.5948486328125 = 0.9930416941642761 + 100.0 * 9.10601806640625
Epoch 2100, val loss: 0.9992513656616211
Epoch 2110, training loss: 911.6390991210938 = 0.9923482537269592 + 100.0 * 9.106467247009277
Epoch 2110, val loss: 0.9986375570297241
Epoch 2120, training loss: 911.9727783203125 = 0.9915457963943481 + 100.0 * 9.10981273651123
Epoch 2120, val loss: 0.9979066848754883
Epoch 2130, training loss: 911.9183959960938 = 0.9909248948097229 + 100.0 * 9.109274864196777
Epoch 2130, val loss: 0.997387707233429
Epoch 2140, training loss: 912.1344604492188 = 0.99015212059021 + 100.0 * 9.111442565917969
Epoch 2140, val loss: 0.9967082142829895
Epoch 2150, training loss: 912.2050170898438 = 0.9892820119857788 + 100.0 * 9.112157821655273
Epoch 2150, val loss: 0.9959600567817688
Epoch 2160, training loss: 912.56982421875 = 0.9885053038597107 + 100.0 * 9.115813255310059
Epoch 2160, val loss: 0.9952667951583862
Epoch 2170, training loss: 912.697509765625 = 0.9877423644065857 + 100.0 * 9.117097854614258
Epoch 2170, val loss: 0.9946112632751465
Epoch 2180, training loss: 912.8361206054688 = 0.9869877099990845 + 100.0 * 9.118491172790527
Epoch 2180, val loss: 0.9939209818840027
Epoch 2190, training loss: 913.110107421875 = 0.9862101674079895 + 100.0 * 9.121238708496094
Epoch 2190, val loss: 0.9932374358177185
Epoch 2200, training loss: 913.3422241210938 = 0.9854506850242615 + 100.0 * 9.123567581176758
Epoch 2200, val loss: 0.9925642013549805
Epoch 2210, training loss: 913.1966552734375 = 0.9846595525741577 + 100.0 * 9.122119903564453
Epoch 2210, val loss: 0.9918733239173889
Epoch 2220, training loss: 913.52099609375 = 0.9838910698890686 + 100.0 * 9.125370979309082
Epoch 2220, val loss: 0.9911694526672363
Epoch 2230, training loss: 913.9044799804688 = 0.9831280708312988 + 100.0 * 9.129213333129883
Epoch 2230, val loss: 0.9904942512512207
Epoch 2240, training loss: 913.779052734375 = 0.9823456406593323 + 100.0 * 9.12796688079834
Epoch 2240, val loss: 0.9898000955581665
Epoch 2250, training loss: 914.1485595703125 = 0.981586754322052 + 100.0 * 9.131669998168945
Epoch 2250, val loss: 0.9891231060028076
Epoch 2260, training loss: 914.0447387695312 = 0.9808107018470764 + 100.0 * 9.13063907623291
Epoch 2260, val loss: 0.9884209036827087
Epoch 2270, training loss: 914.3812866210938 = 0.9800319075584412 + 100.0 * 9.134012222290039
Epoch 2270, val loss: 0.9877426028251648
Epoch 2280, training loss: 912.4127807617188 = 0.979219913482666 + 100.0 * 9.114335060119629
Epoch 2280, val loss: 0.9869744777679443
Epoch 2290, training loss: 911.3174438476562 = 0.9784417152404785 + 100.0 * 9.103389739990234
Epoch 2290, val loss: 0.9863297343254089
Epoch 2300, training loss: 912.5650024414062 = 0.9777546525001526 + 100.0 * 9.115872383117676
Epoch 2300, val loss: 0.9856720566749573
Epoch 2310, training loss: 910.4158325195312 = 0.9769440293312073 + 100.0 * 9.094388961791992
Epoch 2310, val loss: 0.9849485158920288
Epoch 2320, training loss: 911.5560302734375 = 0.976131796836853 + 100.0 * 9.105798721313477
Epoch 2320, val loss: 0.9842381477355957
Epoch 2330, training loss: 912.6533813476562 = 0.9753246307373047 + 100.0 * 9.116780281066895
Epoch 2330, val loss: 0.9835509061813354
Epoch 2340, training loss: 913.3921508789062 = 0.974537193775177 + 100.0 * 9.124176025390625
Epoch 2340, val loss: 0.9828671216964722
Epoch 2350, training loss: 914.1007690429688 = 0.9737722873687744 + 100.0 * 9.131270408630371
Epoch 2350, val loss: 0.9821996092796326
Epoch 2360, training loss: 914.2384643554688 = 0.9729922413825989 + 100.0 * 9.132654190063477
Epoch 2360, val loss: 0.9815228581428528
Epoch 2370, training loss: 914.3492431640625 = 0.9721893072128296 + 100.0 * 9.133770942687988
Epoch 2370, val loss: 0.9807992577552795
Epoch 2380, training loss: 914.8457641601562 = 0.9714280962944031 + 100.0 * 9.13874340057373
Epoch 2380, val loss: 0.9801458120346069
Epoch 2390, training loss: 914.938232421875 = 0.970626950263977 + 100.0 * 9.139676094055176
Epoch 2390, val loss: 0.9794175028800964
Epoch 2400, training loss: 914.9744262695312 = 0.9698067307472229 + 100.0 * 9.140046119689941
Epoch 2400, val loss: 0.978714644908905
Epoch 2410, training loss: 915.160400390625 = 0.9689995646476746 + 100.0 * 9.141914367675781
Epoch 2410, val loss: 0.977992057800293
Epoch 2420, training loss: 915.253662109375 = 0.9681732058525085 + 100.0 * 9.142854690551758
Epoch 2420, val loss: 0.9772712588310242
Epoch 2430, training loss: 915.3330078125 = 0.9673570394515991 + 100.0 * 9.143656730651855
Epoch 2430, val loss: 0.9765570759773254
Epoch 2440, training loss: 915.7650146484375 = 0.9665539860725403 + 100.0 * 9.147984504699707
Epoch 2440, val loss: 0.9758535623550415
Epoch 2450, training loss: 915.7122192382812 = 0.965741753578186 + 100.0 * 9.147464752197266
Epoch 2450, val loss: 0.9751326441764832
Epoch 2460, training loss: 914.091064453125 = 0.9648813605308533 + 100.0 * 9.131261825561523
Epoch 2460, val loss: 0.974378228187561
Epoch 2470, training loss: 913.34619140625 = 0.9640263319015503 + 100.0 * 9.123821258544922
Epoch 2470, val loss: 0.9735999703407288
Epoch 2480, training loss: 913.3501586914062 = 0.9632763266563416 + 100.0 * 9.123868942260742
Epoch 2480, val loss: 0.9730021953582764
Epoch 2490, training loss: 914.1302490234375 = 0.9626008868217468 + 100.0 * 9.13167667388916
Epoch 2490, val loss: 0.9724445343017578
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.53
0.8170687531696009
=== training gcn model ===
Epoch 0, training loss: 1012.0844116210938 = 1.1095590591430664 + 100.0 * 10.109748840332031
Epoch 0, val loss: 1.1101953983306885
Epoch 10, training loss: 973.5381469726562 = 1.1090943813323975 + 100.0 * 9.72429084777832
Epoch 10, val loss: 1.109719157218933
Epoch 20, training loss: 956.79736328125 = 1.1085710525512695 + 100.0 * 9.55688762664795
Epoch 20, val loss: 1.109222412109375
Epoch 30, training loss: 944.1561279296875 = 1.1080541610717773 + 100.0 * 9.43048095703125
Epoch 30, val loss: 1.1087249517440796
Epoch 40, training loss: 934.2991333007812 = 1.1075307130813599 + 100.0 * 9.331915855407715
Epoch 40, val loss: 1.108220100402832
Epoch 50, training loss: 926.39794921875 = 1.1070290803909302 + 100.0 * 9.252908706665039
Epoch 50, val loss: 1.1077361106872559
Epoch 60, training loss: 919.8236694335938 = 1.1065213680267334 + 100.0 * 9.187171936035156
Epoch 60, val loss: 1.1072479486465454
Epoch 70, training loss: 914.3251953125 = 1.1060138940811157 + 100.0 * 9.13219165802002
Epoch 70, val loss: 1.1067614555358887
Epoch 80, training loss: 909.6300048828125 = 1.105498194694519 + 100.0 * 9.085245132446289
Epoch 80, val loss: 1.1062642335891724
Epoch 90, training loss: 905.5447387695312 = 1.1049940586090088 + 100.0 * 9.044397354125977
Epoch 90, val loss: 1.105782151222229
Epoch 100, training loss: 902.0035400390625 = 1.104494333267212 + 100.0 * 9.008990287780762
Epoch 100, val loss: 1.1053048372268677
Epoch 110, training loss: 898.710693359375 = 1.1039981842041016 + 100.0 * 8.976066589355469
Epoch 110, val loss: 1.1048314571380615
Epoch 120, training loss: 896.0377807617188 = 1.1035083532333374 + 100.0 * 8.949342727661133
Epoch 120, val loss: 1.104358434677124
Epoch 130, training loss: 893.478271484375 = 1.1029913425445557 + 100.0 * 8.923752784729004
Epoch 130, val loss: 1.1038756370544434
Epoch 140, training loss: 891.9307250976562 = 1.1024659872055054 + 100.0 * 8.908282279968262
Epoch 140, val loss: 1.1033707857131958
Epoch 150, training loss: 889.9230346679688 = 1.1019688844680786 + 100.0 * 8.88821029663086
Epoch 150, val loss: 1.1029012203216553
Epoch 160, training loss: 888.24169921875 = 1.101453185081482 + 100.0 * 8.871402740478516
Epoch 160, val loss: 1.1024118661880493
Epoch 170, training loss: 886.6322021484375 = 1.1009458303451538 + 100.0 * 8.85531234741211
Epoch 170, val loss: 1.1019376516342163
Epoch 180, training loss: 885.4879760742188 = 1.100443720817566 + 100.0 * 8.84387493133545
Epoch 180, val loss: 1.1014602184295654
Epoch 190, training loss: 885.0956420898438 = 1.099927544593811 + 100.0 * 8.839957237243652
Epoch 190, val loss: 1.1009777784347534
Epoch 200, training loss: 883.3624877929688 = 1.0993973016738892 + 100.0 * 8.822630882263184
Epoch 200, val loss: 1.1004711389541626
Epoch 210, training loss: 882.816162109375 = 1.0989049673080444 + 100.0 * 8.81717300415039
Epoch 210, val loss: 1.100000023841858
Epoch 220, training loss: 881.8036499023438 = 1.0983797311782837 + 100.0 * 8.807052612304688
Epoch 220, val loss: 1.09950590133667
Epoch 230, training loss: 881.3099365234375 = 1.0978657007217407 + 100.0 * 8.80212116241455
Epoch 230, val loss: 1.0990169048309326
Epoch 240, training loss: 880.8343505859375 = 1.0973376035690308 + 100.0 * 8.797369956970215
Epoch 240, val loss: 1.0985208749771118
Epoch 250, training loss: 880.2539672851562 = 1.0968178510665894 + 100.0 * 8.791571617126465
Epoch 250, val loss: 1.0980244874954224
Epoch 260, training loss: 879.8553466796875 = 1.0962955951690674 + 100.0 * 8.787590026855469
Epoch 260, val loss: 1.0975297689437866
Epoch 270, training loss: 879.37060546875 = 1.0957633256912231 + 100.0 * 8.782748222351074
Epoch 270, val loss: 1.0970262289047241
Epoch 280, training loss: 879.4226684570312 = 1.0952351093292236 + 100.0 * 8.78327465057373
Epoch 280, val loss: 1.0965336561203003
Epoch 290, training loss: 878.685546875 = 1.0947223901748657 + 100.0 * 8.775908470153809
Epoch 290, val loss: 1.096055507659912
Epoch 300, training loss: 877.9945678710938 = 1.0941863059997559 + 100.0 * 8.769003868103027
Epoch 300, val loss: 1.0955346822738647
Epoch 310, training loss: 877.8275146484375 = 1.0936568975448608 + 100.0 * 8.767338752746582
Epoch 310, val loss: 1.0950368642807007
Epoch 320, training loss: 877.8885498046875 = 1.093137502670288 + 100.0 * 8.767953872680664
Epoch 320, val loss: 1.0945435762405396
Epoch 330, training loss: 877.55419921875 = 1.09260094165802 + 100.0 * 8.764616012573242
Epoch 330, val loss: 1.0940312147140503
Epoch 340, training loss: 877.56201171875 = 1.0920624732971191 + 100.0 * 8.764699935913086
Epoch 340, val loss: 1.0935351848602295
Epoch 350, training loss: 877.5233764648438 = 1.091543436050415 + 100.0 * 8.764318466186523
Epoch 350, val loss: 1.0930317640304565
Epoch 360, training loss: 877.8563232421875 = 1.0909807682037354 + 100.0 * 8.767653465270996
Epoch 360, val loss: 1.0925180912017822
Epoch 370, training loss: 877.8974609375 = 1.0904582738876343 + 100.0 * 8.768070220947266
Epoch 370, val loss: 1.091995120048523
Epoch 380, training loss: 878.444580078125 = 1.0899580717086792 + 100.0 * 8.77354621887207
Epoch 380, val loss: 1.0915337800979614
Epoch 390, training loss: 876.91455078125 = 1.089423656463623 + 100.0 * 8.758251190185547
Epoch 390, val loss: 1.0910344123840332
Epoch 400, training loss: 877.2048950195312 = 1.088915228843689 + 100.0 * 8.761159896850586
Epoch 400, val loss: 1.0905520915985107
Epoch 410, training loss: 877.4501953125 = 1.0884015560150146 + 100.0 * 8.763618469238281
Epoch 410, val loss: 1.0900700092315674
Epoch 420, training loss: 877.6842651367188 = 1.0878868103027344 + 100.0 * 8.765963554382324
Epoch 420, val loss: 1.0895817279815674
Epoch 430, training loss: 877.616455078125 = 1.0873695611953735 + 100.0 * 8.765291213989258
Epoch 430, val loss: 1.089098572731018
Epoch 440, training loss: 877.6060791015625 = 1.0868481397628784 + 100.0 * 8.765192031860352
Epoch 440, val loss: 1.0886117219924927
Epoch 450, training loss: 878.091064453125 = 1.0863399505615234 + 100.0 * 8.770047187805176
Epoch 450, val loss: 1.088111162185669
Epoch 460, training loss: 878.44384765625 = 1.0859065055847168 + 100.0 * 8.773579597473145
Epoch 460, val loss: 1.0877259969711304
Epoch 470, training loss: 877.3403930664062 = 1.0854194164276123 + 100.0 * 8.76254940032959
Epoch 470, val loss: 1.087265968322754
Epoch 480, training loss: 877.255615234375 = 1.084991455078125 + 100.0 * 8.761706352233887
Epoch 480, val loss: 1.0868499279022217
Epoch 490, training loss: 877.7579956054688 = 1.084596037864685 + 100.0 * 8.76673412322998
Epoch 490, val loss: 1.0864802598953247
Epoch 500, training loss: 877.9216918945312 = 1.0842071771621704 + 100.0 * 8.7683744430542
Epoch 500, val loss: 1.0861221551895142
Epoch 510, training loss: 877.7691040039062 = 1.0838404893875122 + 100.0 * 8.766852378845215
Epoch 510, val loss: 1.08578622341156
Epoch 520, training loss: 877.0848388671875 = 1.0834710597991943 + 100.0 * 8.760013580322266
Epoch 520, val loss: 1.0854483842849731
Epoch 530, training loss: 878.1282348632812 = 1.0831831693649292 + 100.0 * 8.770450592041016
Epoch 530, val loss: 1.085174798965454
Epoch 540, training loss: 877.79541015625 = 1.0828906297683716 + 100.0 * 8.767125129699707
Epoch 540, val loss: 1.0849037170410156
Epoch 550, training loss: 878.1917114257812 = 1.082607388496399 + 100.0 * 8.77109146118164
Epoch 550, val loss: 1.0846428871154785
Epoch 560, training loss: 878.5542602539062 = 1.0823222398757935 + 100.0 * 8.77471923828125
Epoch 560, val loss: 1.0843894481658936
Epoch 570, training loss: 878.744873046875 = 1.0820541381835938 + 100.0 * 8.776628494262695
Epoch 570, val loss: 1.0841419696807861
Epoch 580, training loss: 879.038818359375 = 1.0817878246307373 + 100.0 * 8.779570579528809
Epoch 580, val loss: 1.0838910341262817
Epoch 590, training loss: 879.115966796875 = 1.0815116167068481 + 100.0 * 8.78034496307373
Epoch 590, val loss: 1.0836366415023804
Epoch 600, training loss: 879.0232543945312 = 1.0812492370605469 + 100.0 * 8.779419898986816
Epoch 600, val loss: 1.083398461341858
Epoch 610, training loss: 879.4916381835938 = 1.0809910297393799 + 100.0 * 8.784106254577637
Epoch 610, val loss: 1.083165168762207
Epoch 620, training loss: 879.2974243164062 = 1.0807251930236816 + 100.0 * 8.782166481018066
Epoch 620, val loss: 1.0829209089279175
Epoch 630, training loss: 879.5118408203125 = 1.080460548400879 + 100.0 * 8.784314155578613
Epoch 630, val loss: 1.0826727151870728
Epoch 640, training loss: 879.680908203125 = 1.0802079439163208 + 100.0 * 8.786006927490234
Epoch 640, val loss: 1.082445740699768
Epoch 650, training loss: 880.1090087890625 = 1.0799471139907837 + 100.0 * 8.790290832519531
Epoch 650, val loss: 1.08220374584198
Epoch 660, training loss: 880.1533813476562 = 1.0796759128570557 + 100.0 * 8.79073715209961
Epoch 660, val loss: 1.0819534063339233
Epoch 670, training loss: 880.4137573242188 = 1.0794132947921753 + 100.0 * 8.793343544006348
Epoch 670, val loss: 1.081710696220398
Epoch 680, training loss: 880.6978149414062 = 1.0791455507278442 + 100.0 * 8.796186447143555
Epoch 680, val loss: 1.081456184387207
Epoch 690, training loss: 881.6869506835938 = 1.0788850784301758 + 100.0 * 8.80608081817627
Epoch 690, val loss: 1.081228256225586
Epoch 700, training loss: 880.8920288085938 = 1.0785709619522095 + 100.0 * 8.798134803771973
Epoch 700, val loss: 1.0809450149536133
Epoch 710, training loss: 879.4830322265625 = 1.0782896280288696 + 100.0 * 8.78404712677002
Epoch 710, val loss: 1.0806976556777954
Epoch 720, training loss: 880.5742797851562 = 1.0780298709869385 + 100.0 * 8.794962882995605
Epoch 720, val loss: 1.0804493427276611
Epoch 730, training loss: 881.3591918945312 = 1.0777769088745117 + 100.0 * 8.802814483642578
Epoch 730, val loss: 1.0802181959152222
Epoch 740, training loss: 881.810791015625 = 1.0775136947631836 + 100.0 * 8.807332992553711
Epoch 740, val loss: 1.0799797773361206
Epoch 750, training loss: 881.7809448242188 = 1.0772215127944946 + 100.0 * 8.807037353515625
Epoch 750, val loss: 1.0797080993652344
Epoch 760, training loss: 881.9799194335938 = 1.0769379138946533 + 100.0 * 8.809029579162598
Epoch 760, val loss: 1.07944655418396
Epoch 770, training loss: 880.5817260742188 = 1.076614260673523 + 100.0 * 8.795051574707031
Epoch 770, val loss: 1.0791572332382202
Epoch 780, training loss: 882.0404663085938 = 1.0763641595840454 + 100.0 * 8.809640884399414
Epoch 780, val loss: 1.0789250135421753
Epoch 790, training loss: 882.0625 = 1.0760831832885742 + 100.0 * 8.809864044189453
Epoch 790, val loss: 1.078680157661438
Epoch 800, training loss: 882.3173217773438 = 1.0757925510406494 + 100.0 * 8.81241512298584
Epoch 800, val loss: 1.078420877456665
Epoch 810, training loss: 882.8154296875 = 1.075514793395996 + 100.0 * 8.817399024963379
Epoch 810, val loss: 1.0781643390655518
Epoch 820, training loss: 883.3052368164062 = 1.0752291679382324 + 100.0 * 8.82229995727539
Epoch 820, val loss: 1.0779080390930176
Epoch 830, training loss: 883.204345703125 = 1.0749305486679077 + 100.0 * 8.821293830871582
Epoch 830, val loss: 1.0776443481445312
Epoch 840, training loss: 883.621826171875 = 1.0746450424194336 + 100.0 * 8.825471878051758
Epoch 840, val loss: 1.0773742198944092
Epoch 850, training loss: 884.4918212890625 = 1.0743465423583984 + 100.0 * 8.834175109863281
Epoch 850, val loss: 1.0771018266677856
Epoch 860, training loss: 883.9651489257812 = 1.0740516185760498 + 100.0 * 8.828910827636719
Epoch 860, val loss: 1.0768355131149292
Epoch 870, training loss: 883.7987670898438 = 1.073750376701355 + 100.0 * 8.827250480651855
Epoch 870, val loss: 1.0765750408172607
Epoch 880, training loss: 883.8196411132812 = 1.0734469890594482 + 100.0 * 8.827462196350098
Epoch 880, val loss: 1.0762979984283447
Epoch 890, training loss: 884.4093627929688 = 1.073154091835022 + 100.0 * 8.833361625671387
Epoch 890, val loss: 1.076033115386963
Epoch 900, training loss: 884.868408203125 = 1.07285475730896 + 100.0 * 8.837955474853516
Epoch 900, val loss: 1.0757554769515991
Epoch 910, training loss: 885.1760864257812 = 1.0725492238998413 + 100.0 * 8.841034889221191
Epoch 910, val loss: 1.075477123260498
Epoch 920, training loss: 885.7271728515625 = 1.0722440481185913 + 100.0 * 8.846549034118652
Epoch 920, val loss: 1.0752053260803223
Epoch 930, training loss: 885.877685546875 = 1.071927547454834 + 100.0 * 8.848057746887207
Epoch 930, val loss: 1.0749160051345825
Epoch 940, training loss: 886.1585083007812 = 1.071622371673584 + 100.0 * 8.850869178771973
Epoch 940, val loss: 1.074643611907959
Epoch 950, training loss: 886.4541015625 = 1.0713104009628296 + 100.0 * 8.853828430175781
Epoch 950, val loss: 1.0743504762649536
Epoch 960, training loss: 886.4401245117188 = 1.0709857940673828 + 100.0 * 8.853691101074219
Epoch 960, val loss: 1.0740580558776855
Epoch 970, training loss: 886.9171142578125 = 1.0706795454025269 + 100.0 * 8.858464241027832
Epoch 970, val loss: 1.0737810134887695
Epoch 980, training loss: 887.3818359375 = 1.0703741312026978 + 100.0 * 8.863114356994629
Epoch 980, val loss: 1.0735034942626953
Epoch 990, training loss: 887.100830078125 = 1.0700455904006958 + 100.0 * 8.860307693481445
Epoch 990, val loss: 1.0731992721557617
Epoch 1000, training loss: 887.0862426757812 = 1.0697050094604492 + 100.0 * 8.8601655960083
Epoch 1000, val loss: 1.0728793144226074
Epoch 1010, training loss: 886.7369995117188 = 1.0693769454956055 + 100.0 * 8.85667610168457
Epoch 1010, val loss: 1.0725849866867065
Epoch 1020, training loss: 887.5804443359375 = 1.0690574645996094 + 100.0 * 8.865114212036133
Epoch 1020, val loss: 1.072304368019104
Epoch 1030, training loss: 888.097412109375 = 1.0687522888183594 + 100.0 * 8.87028694152832
Epoch 1030, val loss: 1.072031021118164
Epoch 1040, training loss: 888.3143920898438 = 1.06840980052948 + 100.0 * 8.872459411621094
Epoch 1040, val loss: 1.071729302406311
Epoch 1050, training loss: 888.3079833984375 = 1.0680803060531616 + 100.0 * 8.87239933013916
Epoch 1050, val loss: 1.0714167356491089
Epoch 1060, training loss: 888.7750854492188 = 1.067745566368103 + 100.0 * 8.877073287963867
Epoch 1060, val loss: 1.071105718612671
Epoch 1070, training loss: 889.4053344726562 = 1.0674220323562622 + 100.0 * 8.883378982543945
Epoch 1070, val loss: 1.0708131790161133
Epoch 1080, training loss: 889.562255859375 = 1.0670838356018066 + 100.0 * 8.8849515914917
Epoch 1080, val loss: 1.0705057382583618
Epoch 1090, training loss: 889.5270385742188 = 1.0667402744293213 + 100.0 * 8.884603500366211
Epoch 1090, val loss: 1.070193886756897
Epoch 1100, training loss: 889.1160888671875 = 1.0663999319076538 + 100.0 * 8.880496978759766
Epoch 1100, val loss: 1.069879412651062
Epoch 1110, training loss: 889.6484375 = 1.0660812854766846 + 100.0 * 8.885823249816895
Epoch 1110, val loss: 1.069589614868164
Epoch 1120, training loss: 890.05126953125 = 1.0657436847686768 + 100.0 * 8.88985538482666
Epoch 1120, val loss: 1.0692665576934814
Epoch 1130, training loss: 890.376953125 = 1.0654090642929077 + 100.0 * 8.893115043640137
Epoch 1130, val loss: 1.0689659118652344
Epoch 1140, training loss: 890.6388549804688 = 1.0650261640548706 + 100.0 * 8.89573860168457
Epoch 1140, val loss: 1.0686033964157104
Epoch 1150, training loss: 890.966552734375 = 1.0647138357162476 + 100.0 * 8.899018287658691
Epoch 1150, val loss: 1.068338394165039
Epoch 1160, training loss: 890.1912231445312 = 1.0643304586410522 + 100.0 * 8.891268730163574
Epoch 1160, val loss: 1.067983865737915
Epoch 1170, training loss: 890.7094116210938 = 1.064002513885498 + 100.0 * 8.896453857421875
Epoch 1170, val loss: 1.0676815509796143
Epoch 1180, training loss: 891.3524169921875 = 1.063661813735962 + 100.0 * 8.902887344360352
Epoch 1180, val loss: 1.0673755407333374
Epoch 1190, training loss: 891.3590698242188 = 1.0633167028427124 + 100.0 * 8.902957916259766
Epoch 1190, val loss: 1.067047357559204
Epoch 1200, training loss: 892.04541015625 = 1.0629769563674927 + 100.0 * 8.90982437133789
Epoch 1200, val loss: 1.066742181777954
Epoch 1210, training loss: 892.2446899414062 = 1.0626295804977417 + 100.0 * 8.911820411682129
Epoch 1210, val loss: 1.066420078277588
Epoch 1220, training loss: 892.4017333984375 = 1.0622625350952148 + 100.0 * 8.913394927978516
Epoch 1220, val loss: 1.0660877227783203
Epoch 1230, training loss: 891.4790649414062 = 1.06187105178833 + 100.0 * 8.90417194366455
Epoch 1230, val loss: 1.0657336711883545
Epoch 1240, training loss: 891.7516479492188 = 1.0615105628967285 + 100.0 * 8.906901359558105
Epoch 1240, val loss: 1.065393090248108
Epoch 1250, training loss: 892.4398193359375 = 1.0611594915390015 + 100.0 * 8.913786888122559
Epoch 1250, val loss: 1.0650678873062134
Epoch 1260, training loss: 893.2708129882812 = 1.0608123540878296 + 100.0 * 8.922100067138672
Epoch 1260, val loss: 1.0647635459899902
Epoch 1270, training loss: 893.7708129882812 = 1.0604714155197144 + 100.0 * 8.927103042602539
Epoch 1270, val loss: 1.064435601234436
Epoch 1280, training loss: 893.9754638671875 = 1.06010103225708 + 100.0 * 8.929153442382812
Epoch 1280, val loss: 1.0641101598739624
Epoch 1290, training loss: 893.875244140625 = 1.059735894203186 + 100.0 * 8.928154945373535
Epoch 1290, val loss: 1.0637753009796143
Epoch 1300, training loss: 893.7716064453125 = 1.0593713521957397 + 100.0 * 8.927122116088867
Epoch 1300, val loss: 1.0634398460388184
Epoch 1310, training loss: 894.4029541015625 = 1.0590063333511353 + 100.0 * 8.933439254760742
Epoch 1310, val loss: 1.0630990266799927
Epoch 1320, training loss: 894.6204223632812 = 1.0586435794830322 + 100.0 * 8.935617446899414
Epoch 1320, val loss: 1.0627623796463013
Epoch 1330, training loss: 894.903564453125 = 1.0582726001739502 + 100.0 * 8.93845272064209
Epoch 1330, val loss: 1.0624297857284546
Epoch 1340, training loss: 895.1301879882812 = 1.057908058166504 + 100.0 * 8.940722465515137
Epoch 1340, val loss: 1.0620959997177124
Epoch 1350, training loss: 895.1536254882812 = 1.0575425624847412 + 100.0 * 8.940960884094238
Epoch 1350, val loss: 1.0617510080337524
Epoch 1360, training loss: 895.4776000976562 = 1.0571600198745728 + 100.0 * 8.944204330444336
Epoch 1360, val loss: 1.0614038705825806
Epoch 1370, training loss: 895.6556396484375 = 1.0567917823791504 + 100.0 * 8.945988655090332
Epoch 1370, val loss: 1.061067819595337
Epoch 1380, training loss: 895.7799072265625 = 1.056414008140564 + 100.0 * 8.947235107421875
Epoch 1380, val loss: 1.0607298612594604
Epoch 1390, training loss: 896.1007080078125 = 1.0560381412506104 + 100.0 * 8.950447082519531
Epoch 1390, val loss: 1.0603747367858887
Epoch 1400, training loss: 896.0037841796875 = 1.0556422472000122 + 100.0 * 8.949481010437012
Epoch 1400, val loss: 1.0600273609161377
Epoch 1410, training loss: 896.2589721679688 = 1.0552865266799927 + 100.0 * 8.95203685760498
Epoch 1410, val loss: 1.0596952438354492
Epoch 1420, training loss: 896.757568359375 = 1.054911494255066 + 100.0 * 8.957026481628418
Epoch 1420, val loss: 1.0593595504760742
Epoch 1430, training loss: 897.1261596679688 = 1.0545238256454468 + 100.0 * 8.960716247558594
Epoch 1430, val loss: 1.0590060949325562
Epoch 1440, training loss: 896.8848876953125 = 1.0541425943374634 + 100.0 * 8.958307266235352
Epoch 1440, val loss: 1.0586310625076294
Epoch 1450, training loss: 896.9784545898438 = 1.0537595748901367 + 100.0 * 8.959246635437012
Epoch 1450, val loss: 1.0583014488220215
Epoch 1460, training loss: 897.7076416015625 = 1.053381085395813 + 100.0 * 8.96654224395752
Epoch 1460, val loss: 1.0579633712768555
Epoch 1470, training loss: 896.4332885742188 = 1.0529208183288574 + 100.0 * 8.953804016113281
Epoch 1470, val loss: 1.0575369596481323
Epoch 1480, training loss: 895.3865356445312 = 1.05247163772583 + 100.0 * 8.943340301513672
Epoch 1480, val loss: 1.057115912437439
Epoch 1490, training loss: 895.6056518554688 = 1.0520734786987305 + 100.0 * 8.945535659790039
Epoch 1490, val loss: 1.056753396987915
Epoch 1500, training loss: 896.5230712890625 = 1.0517302751541138 + 100.0 * 8.954712867736816
Epoch 1500, val loss: 1.0564298629760742
Epoch 1510, training loss: 897.4893798828125 = 1.0513607263565063 + 100.0 * 8.964380264282227
Epoch 1510, val loss: 1.0561048984527588
Epoch 1520, training loss: 897.9485473632812 = 1.0510053634643555 + 100.0 * 8.968975067138672
Epoch 1520, val loss: 1.0557832717895508
Epoch 1530, training loss: 897.7222900390625 = 1.0506187677383423 + 100.0 * 8.966716766357422
Epoch 1530, val loss: 1.0554226636886597
Epoch 1540, training loss: 898.14697265625 = 1.0502315759658813 + 100.0 * 8.970967292785645
Epoch 1540, val loss: 1.0550811290740967
Epoch 1550, training loss: 898.5181274414062 = 1.0498526096343994 + 100.0 * 8.974682807922363
Epoch 1550, val loss: 1.0547512769699097
Epoch 1560, training loss: 898.5369873046875 = 1.0494623184204102 + 100.0 * 8.974875450134277
Epoch 1560, val loss: 1.054382085800171
Epoch 1570, training loss: 898.7440185546875 = 1.049070119857788 + 100.0 * 8.976949691772461
Epoch 1570, val loss: 1.054034948348999
Epoch 1580, training loss: 898.208984375 = 1.0486453771591187 + 100.0 * 8.971603393554688
Epoch 1580, val loss: 1.0536590814590454
Epoch 1590, training loss: 898.5990600585938 = 1.0482655763626099 + 100.0 * 8.975507736206055
Epoch 1590, val loss: 1.0533123016357422
Epoch 1600, training loss: 898.9209594726562 = 1.0478894710540771 + 100.0 * 8.978730201721191
Epoch 1600, val loss: 1.0529597997665405
Epoch 1610, training loss: 899.22119140625 = 1.0474921464920044 + 100.0 * 8.98173713684082
Epoch 1610, val loss: 1.0526065826416016
Epoch 1620, training loss: 898.947998046875 = 1.0470882654190063 + 100.0 * 8.979009628295898
Epoch 1620, val loss: 1.0522395372390747
Epoch 1630, training loss: 898.9443359375 = 1.0466842651367188 + 100.0 * 8.978976249694824
Epoch 1630, val loss: 1.0518651008605957
Epoch 1640, training loss: 898.9537963867188 = 1.0462626218795776 + 100.0 * 8.97907543182373
Epoch 1640, val loss: 1.0514891147613525
Epoch 1650, training loss: 899.3045043945312 = 1.0458171367645264 + 100.0 * 8.982586860656738
Epoch 1650, val loss: 1.0510972738265991
Epoch 1660, training loss: 899.7767944335938 = 1.045350193977356 + 100.0 * 8.987314224243164
Epoch 1660, val loss: 1.0506726503372192
Epoch 1670, training loss: 899.3975830078125 = 1.0447779893875122 + 100.0 * 8.983528137207031
Epoch 1670, val loss: 1.050155520439148
Epoch 1680, training loss: 899.804443359375 = 1.0441563129425049 + 100.0 * 8.987603187561035
Epoch 1680, val loss: 1.0495928525924683
Epoch 1690, training loss: 900.3216552734375 = 1.043556571006775 + 100.0 * 8.992780685424805
Epoch 1690, val loss: 1.0490299463272095
Epoch 1700, training loss: 900.427734375 = 1.0428509712219238 + 100.0 * 8.99384880065918
Epoch 1700, val loss: 1.0484012365341187
Epoch 1710, training loss: 900.7156982421875 = 1.0422276258468628 + 100.0 * 8.996734619140625
Epoch 1710, val loss: 1.0478417873382568
Epoch 1720, training loss: 900.61181640625 = 1.0416185855865479 + 100.0 * 8.995701789855957
Epoch 1720, val loss: 1.047310709953308
Epoch 1730, training loss: 900.4861450195312 = 1.0410265922546387 + 100.0 * 8.994451522827148
Epoch 1730, val loss: 1.0467617511749268
Epoch 1740, training loss: 900.6993408203125 = 1.0404441356658936 + 100.0 * 8.996588706970215
Epoch 1740, val loss: 1.0462350845336914
Epoch 1750, training loss: 901.2485961914062 = 1.0398590564727783 + 100.0 * 9.002087593078613
Epoch 1750, val loss: 1.0457096099853516
Epoch 1760, training loss: 901.1636962890625 = 1.0392717123031616 + 100.0 * 9.00124454498291
Epoch 1760, val loss: 1.0451724529266357
Epoch 1770, training loss: 901.3493041992188 = 1.0386731624603271 + 100.0 * 9.003106117248535
Epoch 1770, val loss: 1.0446255207061768
Epoch 1780, training loss: 900.5663452148438 = 1.0380462408065796 + 100.0 * 8.995283126831055
Epoch 1780, val loss: 1.0440459251403809
Epoch 1790, training loss: 900.0772094726562 = 1.037383794784546 + 100.0 * 8.990398406982422
Epoch 1790, val loss: 1.0434821844100952
Epoch 1800, training loss: 901.1001586914062 = 1.036767601966858 + 100.0 * 9.00063419342041
Epoch 1800, val loss: 1.0429315567016602
Epoch 1810, training loss: 901.4666137695312 = 1.0361815690994263 + 100.0 * 9.004303932189941
Epoch 1810, val loss: 1.042400598526001
Epoch 1820, training loss: 901.9531860351562 = 1.0356186628341675 + 100.0 * 9.009175300598145
Epoch 1820, val loss: 1.0418940782546997
Epoch 1830, training loss: 902.24560546875 = 1.0350086688995361 + 100.0 * 9.012105941772461
Epoch 1830, val loss: 1.0413620471954346
Epoch 1840, training loss: 901.8621215820312 = 1.0344114303588867 + 100.0 * 9.00827693939209
Epoch 1840, val loss: 1.0408213138580322
Epoch 1850, training loss: 902.1340942382812 = 1.0338002443313599 + 100.0 * 9.011002540588379
Epoch 1850, val loss: 1.0402724742889404
Epoch 1860, training loss: 902.5360107421875 = 1.0332077741622925 + 100.0 * 9.01502799987793
Epoch 1860, val loss: 1.039726972579956
Epoch 1870, training loss: 902.5617065429688 = 1.0325838327407837 + 100.0 * 9.015291213989258
Epoch 1870, val loss: 1.0391651391983032
Epoch 1880, training loss: 902.51513671875 = 1.0319688320159912 + 100.0 * 9.01483154296875
Epoch 1880, val loss: 1.0385985374450684
Epoch 1890, training loss: 903.078369140625 = 1.031355381011963 + 100.0 * 9.020469665527344
Epoch 1890, val loss: 1.0380561351776123
Epoch 1900, training loss: 902.8202514648438 = 1.0307332277297974 + 100.0 * 9.017894744873047
Epoch 1900, val loss: 1.0375009775161743
Epoch 1910, training loss: 903.0672607421875 = 1.030095100402832 + 100.0 * 9.020371437072754
Epoch 1910, val loss: 1.0369212627410889
Epoch 1920, training loss: 903.1913452148438 = 1.0294718742370605 + 100.0 * 9.021618843078613
Epoch 1920, val loss: 1.0363742113113403
Epoch 1930, training loss: 903.5486450195312 = 1.0288617610931396 + 100.0 * 9.025197982788086
Epoch 1930, val loss: 1.0358105897903442
Epoch 1940, training loss: 903.7236328125 = 1.0282268524169922 + 100.0 * 9.026954650878906
Epoch 1940, val loss: 1.0352610349655151
Epoch 1950, training loss: 903.5858764648438 = 1.0276269912719727 + 100.0 * 9.025582313537598
Epoch 1950, val loss: 1.0346988439559937
Epoch 1960, training loss: 903.8396606445312 = 1.0269908905029297 + 100.0 * 9.02812671661377
Epoch 1960, val loss: 1.0341315269470215
Epoch 1970, training loss: 904.0623779296875 = 1.026381254196167 + 100.0 * 9.030360221862793
Epoch 1970, val loss: 1.033597469329834
Epoch 1980, training loss: 903.6630249023438 = 1.0257641077041626 + 100.0 * 9.026372909545898
Epoch 1980, val loss: 1.0330356359481812
Epoch 1990, training loss: 904.0888061523438 = 1.0251461267471313 + 100.0 * 9.03063678741455
Epoch 1990, val loss: 1.0324807167053223
Epoch 2000, training loss: 904.17724609375 = 1.0245022773742676 + 100.0 * 9.031527519226074
Epoch 2000, val loss: 1.0319206714630127
Epoch 2010, training loss: 904.3052368164062 = 1.023906946182251 + 100.0 * 9.03281307220459
Epoch 2010, val loss: 1.0313798189163208
Epoch 2020, training loss: 904.8423461914062 = 1.023236870765686 + 100.0 * 9.038190841674805
Epoch 2020, val loss: 1.030766487121582
Epoch 2030, training loss: 904.2571411132812 = 1.0224897861480713 + 100.0 * 9.032346725463867
Epoch 2030, val loss: 1.03013014793396
Epoch 2040, training loss: 904.6806640625 = 1.0219029188156128 + 100.0 * 9.036587715148926
Epoch 2040, val loss: 1.0296111106872559
Epoch 2050, training loss: 905.5103149414062 = 1.0213351249694824 + 100.0 * 9.044889450073242
Epoch 2050, val loss: 1.029097318649292
Epoch 2060, training loss: 906.4240112304688 = 1.0207546949386597 + 100.0 * 9.054032325744629
Epoch 2060, val loss: 1.0285817384719849
Epoch 2070, training loss: 906.9116821289062 = 1.020147442817688 + 100.0 * 9.058915138244629
Epoch 2070, val loss: 1.0280392169952393
Epoch 2080, training loss: 906.8569946289062 = 1.0195220708847046 + 100.0 * 9.058374404907227
Epoch 2080, val loss: 1.02747642993927
Epoch 2090, training loss: 907.052734375 = 1.0189063549041748 + 100.0 * 9.060338020324707
Epoch 2090, val loss: 1.0269371271133423
Epoch 2100, training loss: 907.3192138671875 = 1.0182905197143555 + 100.0 * 9.063009262084961
Epoch 2100, val loss: 1.0263574123382568
Epoch 2110, training loss: 907.344482421875 = 1.017655849456787 + 100.0 * 9.063268661499023
Epoch 2110, val loss: 1.0258067846298218
Epoch 2120, training loss: 907.5978393554688 = 1.0170190334320068 + 100.0 * 9.065808296203613
Epoch 2120, val loss: 1.0252423286437988
Epoch 2130, training loss: 907.2788696289062 = 1.0163671970367432 + 100.0 * 9.06262493133545
Epoch 2130, val loss: 1.0246609449386597
Epoch 2140, training loss: 907.46142578125 = 1.0157514810562134 + 100.0 * 9.064456939697266
Epoch 2140, val loss: 1.0241070985794067
Epoch 2150, training loss: 907.9747924804688 = 1.0151329040527344 + 100.0 * 9.069596290588379
Epoch 2150, val loss: 1.0235605239868164
Epoch 2160, training loss: 908.3536987304688 = 1.0145143270492554 + 100.0 * 9.073391914367676
Epoch 2160, val loss: 1.0230242013931274
Epoch 2170, training loss: 908.16259765625 = 1.0138564109802246 + 100.0 * 9.071487426757812
Epoch 2170, val loss: 1.0224472284317017
Epoch 2180, training loss: 908.4297485351562 = 1.0132373571395874 + 100.0 * 9.074165344238281
Epoch 2180, val loss: 1.021881103515625
Epoch 2190, training loss: 908.604248046875 = 1.012607455253601 + 100.0 * 9.075916290283203
Epoch 2190, val loss: 1.0213152170181274
Epoch 2200, training loss: 908.1384887695312 = 1.0119489431381226 + 100.0 * 9.07126522064209
Epoch 2200, val loss: 1.0207293033599854
Epoch 2210, training loss: 908.0591430664062 = 1.0113143920898438 + 100.0 * 9.070478439331055
Epoch 2210, val loss: 1.0201597213745117
Epoch 2220, training loss: 908.2737426757812 = 1.0107007026672363 + 100.0 * 9.072630882263184
Epoch 2220, val loss: 1.0196229219436646
Epoch 2230, training loss: 908.8400268554688 = 1.0100971460342407 + 100.0 * 9.078299522399902
Epoch 2230, val loss: 1.0190823078155518
Epoch 2240, training loss: 909.0781860351562 = 1.0094637870788574 + 100.0 * 9.080687522888184
Epoch 2240, val loss: 1.0185316801071167
Epoch 2250, training loss: 909.114013671875 = 1.0088438987731934 + 100.0 * 9.08105182647705
Epoch 2250, val loss: 1.0179567337036133
Epoch 2260, training loss: 909.0626220703125 = 1.008182406425476 + 100.0 * 9.080544471740723
Epoch 2260, val loss: 1.0173708200454712
Epoch 2270, training loss: 909.4349975585938 = 1.0075552463531494 + 100.0 * 9.084274291992188
Epoch 2270, val loss: 1.0168300867080688
Epoch 2280, training loss: 909.7055053710938 = 1.006946325302124 + 100.0 * 9.08698558807373
Epoch 2280, val loss: 1.0162709951400757
Epoch 2290, training loss: 909.4925537109375 = 1.0062940120697021 + 100.0 * 9.08486270904541
Epoch 2290, val loss: 1.015714406967163
Epoch 2300, training loss: 909.3037109375 = 1.0056403875350952 + 100.0 * 9.08298110961914
Epoch 2300, val loss: 1.0151268243789673
Epoch 2310, training loss: 909.1466064453125 = 1.0050220489501953 + 100.0 * 9.081416130065918
Epoch 2310, val loss: 1.0145918130874634
Epoch 2320, training loss: 909.5108032226562 = 1.004386305809021 + 100.0 * 9.085063934326172
Epoch 2320, val loss: 1.014015555381775
Epoch 2330, training loss: 910.1114501953125 = 1.003791093826294 + 100.0 * 9.091076850891113
Epoch 2330, val loss: 1.0134907960891724
Epoch 2340, training loss: 910.2483520507812 = 1.0031648874282837 + 100.0 * 9.092452049255371
Epoch 2340, val loss: 1.0129207372665405
Epoch 2350, training loss: 909.89599609375 = 1.0025266408920288 + 100.0 * 9.088934898376465
Epoch 2350, val loss: 1.0123658180236816
Epoch 2360, training loss: 910.3550415039062 = 1.0019166469573975 + 100.0 * 9.093531608581543
Epoch 2360, val loss: 1.0118294954299927
Epoch 2370, training loss: 910.61328125 = 1.001302719116211 + 100.0 * 9.09611988067627
Epoch 2370, val loss: 1.0112943649291992
Epoch 2380, training loss: 910.1514892578125 = 1.0006827116012573 + 100.0 * 9.091507911682129
Epoch 2380, val loss: 1.010758399963379
Epoch 2390, training loss: 910.297119140625 = 1.000063180923462 + 100.0 * 9.092970848083496
Epoch 2390, val loss: 1.0102182626724243
Epoch 2400, training loss: 910.8512573242188 = 0.9994673132896423 + 100.0 * 9.098518371582031
Epoch 2400, val loss: 1.0096851587295532
Epoch 2410, training loss: 910.7901000976562 = 0.9988575577735901 + 100.0 * 9.097912788391113
Epoch 2410, val loss: 1.0091263055801392
Epoch 2420, training loss: 910.603271484375 = 0.9982145428657532 + 100.0 * 9.096050262451172
Epoch 2420, val loss: 1.008578896522522
Epoch 2430, training loss: 910.7066040039062 = 0.9975818395614624 + 100.0 * 9.097090721130371
Epoch 2430, val loss: 1.0080302953720093
Epoch 2440, training loss: 911.02880859375 = 0.9969757199287415 + 100.0 * 9.100318908691406
Epoch 2440, val loss: 1.0075089931488037
Epoch 2450, training loss: 911.60595703125 = 0.996399462223053 + 100.0 * 9.106095314025879
Epoch 2450, val loss: 1.0069972276687622
Epoch 2460, training loss: 910.6177978515625 = 0.9957332015037537 + 100.0 * 9.096220970153809
Epoch 2460, val loss: 1.0064220428466797
Epoch 2470, training loss: 910.7149658203125 = 0.99513179063797 + 100.0 * 9.097198486328125
Epoch 2470, val loss: 1.005904197692871
Epoch 2480, training loss: 911.0288696289062 = 0.9945534467697144 + 100.0 * 9.100342750549316
Epoch 2480, val loss: 1.0054110288619995
Epoch 2490, training loss: 911.8552856445312 = 0.9939836263656616 + 100.0 * 9.108613014221191
Epoch 2490, val loss: 1.0049211978912354
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.46942028985507245
0.8179381293921612
=== training gcn model ===
Epoch 0, training loss: 1034.433837890625 = 1.1114994287490845 + 100.0 * 10.333224296569824
Epoch 0, val loss: 1.1114813089370728
Epoch 10, training loss: 992.9701538085938 = 1.1111081838607788 + 100.0 * 9.918590545654297
Epoch 10, val loss: 1.1110666990280151
Epoch 20, training loss: 972.5161743164062 = 1.1107854843139648 + 100.0 * 9.714054107666016
Epoch 20, val loss: 1.110750675201416
Epoch 30, training loss: 958.559814453125 = 1.1104782819747925 + 100.0 * 9.574493408203125
Epoch 30, val loss: 1.110443353652954
Epoch 40, training loss: 947.7200317382812 = 1.1101661920547485 + 100.0 * 9.46609878540039
Epoch 40, val loss: 1.1101306676864624
Epoch 50, training loss: 939.0677490234375 = 1.1098413467407227 + 100.0 * 9.379578590393066
Epoch 50, val loss: 1.1098060607910156
Epoch 60, training loss: 931.7374267578125 = 1.109511375427246 + 100.0 * 9.306279182434082
Epoch 60, val loss: 1.1094714403152466
Epoch 70, training loss: 925.4030151367188 = 1.109177589416504 + 100.0 * 9.242938041687012
Epoch 70, val loss: 1.1091364622116089
Epoch 80, training loss: 919.8648071289062 = 1.1088377237319946 + 100.0 * 9.187560081481934
Epoch 80, val loss: 1.108796238899231
Epoch 90, training loss: 915.1219482421875 = 1.1084868907928467 + 100.0 * 9.140134811401367
Epoch 90, val loss: 1.1084426641464233
Epoch 100, training loss: 910.8366088867188 = 1.108137845993042 + 100.0 * 9.097284317016602
Epoch 100, val loss: 1.1080976724624634
Epoch 110, training loss: 907.0398559570312 = 1.1077834367752075 + 100.0 * 9.059320449829102
Epoch 110, val loss: 1.1077423095703125
Epoch 120, training loss: 903.8209838867188 = 1.1074292659759521 + 100.0 * 9.027135848999023
Epoch 120, val loss: 1.107389211654663
Epoch 130, training loss: 901.1651611328125 = 1.1070747375488281 + 100.0 * 9.000580787658691
Epoch 130, val loss: 1.1070383787155151
Epoch 140, training loss: 898.458251953125 = 1.1066981554031372 + 100.0 * 8.973515510559082
Epoch 140, val loss: 1.106658935546875
Epoch 150, training loss: 896.2046508789062 = 1.1063400506973267 + 100.0 * 8.950983047485352
Epoch 150, val loss: 1.1063035726547241
Epoch 160, training loss: 894.1809692382812 = 1.1059544086456299 + 100.0 * 8.930749893188477
Epoch 160, val loss: 1.1059197187423706
Epoch 170, training loss: 892.4799194335938 = 1.1055766344070435 + 100.0 * 8.913743019104004
Epoch 170, val loss: 1.1055450439453125
Epoch 180, training loss: 890.9992065429688 = 1.105196475982666 + 100.0 * 8.898940086364746
Epoch 180, val loss: 1.105169415473938
Epoch 190, training loss: 889.7383422851562 = 1.1048015356063843 + 100.0 * 8.886335372924805
Epoch 190, val loss: 1.1047749519348145
Epoch 200, training loss: 888.6001586914062 = 1.1044095754623413 + 100.0 * 8.874957084655762
Epoch 200, val loss: 1.1043869256973267
Epoch 210, training loss: 887.5216674804688 = 1.104008436203003 + 100.0 * 8.864176750183105
Epoch 210, val loss: 1.1039854288101196
Epoch 220, training loss: 886.6220092773438 = 1.1036076545715332 + 100.0 * 8.855183601379395
Epoch 220, val loss: 1.1035807132720947
Epoch 230, training loss: 885.6399536132812 = 1.1031943559646606 + 100.0 * 8.845367431640625
Epoch 230, val loss: 1.1031733751296997
Epoch 240, training loss: 884.7548217773438 = 1.1027847528457642 + 100.0 * 8.836520195007324
Epoch 240, val loss: 1.1027677059173584
Epoch 250, training loss: 884.1091918945312 = 1.1023657321929932 + 100.0 * 8.830068588256836
Epoch 250, val loss: 1.1023468971252441
Epoch 260, training loss: 883.6806030273438 = 1.1019357442855835 + 100.0 * 8.825786590576172
Epoch 260, val loss: 1.1019290685653687
Epoch 270, training loss: 883.1422119140625 = 1.1015156507492065 + 100.0 * 8.820406913757324
Epoch 270, val loss: 1.1015058755874634
Epoch 280, training loss: 882.9979248046875 = 1.101082682609558 + 100.0 * 8.818968772888184
Epoch 280, val loss: 1.1010781526565552
Epoch 290, training loss: 882.5593872070312 = 1.1006567478179932 + 100.0 * 8.814587593078613
Epoch 290, val loss: 1.1006511449813843
Epoch 300, training loss: 882.1689453125 = 1.1002188920974731 + 100.0 * 8.810687065124512
Epoch 300, val loss: 1.1002181768417358
Epoch 310, training loss: 881.7745971679688 = 1.0998233556747437 + 100.0 * 8.806747436523438
Epoch 310, val loss: 1.0998249053955078
Epoch 320, training loss: 881.4586181640625 = 1.0994187593460083 + 100.0 * 8.80359172821045
Epoch 320, val loss: 1.0994306802749634
Epoch 330, training loss: 881.0752563476562 = 1.0990420579910278 + 100.0 * 8.799761772155762
Epoch 330, val loss: 1.0990674495697021
Epoch 340, training loss: 880.9900512695312 = 1.0987255573272705 + 100.0 * 8.79891300201416
Epoch 340, val loss: 1.0987614393234253
Epoch 350, training loss: 880.8129272460938 = 1.0984525680541992 + 100.0 * 8.797144889831543
Epoch 350, val loss: 1.098502516746521
Epoch 360, training loss: 880.7006225585938 = 1.0982362031936646 + 100.0 * 8.79602336883545
Epoch 360, val loss: 1.0983045101165771
Epoch 370, training loss: 880.6725463867188 = 1.098066806793213 + 100.0 * 8.795744895935059
Epoch 370, val loss: 1.0981496572494507
Epoch 380, training loss: 880.6409912109375 = 1.097951054573059 + 100.0 * 8.795430183410645
Epoch 380, val loss: 1.0980416536331177
Epoch 390, training loss: 880.7219848632812 = 1.09785795211792 + 100.0 * 8.796241760253906
Epoch 390, val loss: 1.0979589223861694
Epoch 400, training loss: 880.4271850585938 = 1.0977863073349 + 100.0 * 8.793293952941895
Epoch 400, val loss: 1.0978947877883911
Epoch 410, training loss: 880.78271484375 = 1.0977272987365723 + 100.0 * 8.796850204467773
Epoch 410, val loss: 1.0978397130966187
Epoch 420, training loss: 880.8826904296875 = 1.0976730585098267 + 100.0 * 8.797850608825684
Epoch 420, val loss: 1.0977885723114014
Epoch 430, training loss: 880.8379516601562 = 1.0976181030273438 + 100.0 * 8.797403335571289
Epoch 430, val loss: 1.097739577293396
Epoch 440, training loss: 880.5818481445312 = 1.0975656509399414 + 100.0 * 8.794842720031738
Epoch 440, val loss: 1.0976908206939697
Epoch 450, training loss: 880.7731323242188 = 1.0975127220153809 + 100.0 * 8.79675579071045
Epoch 450, val loss: 1.097643256187439
Epoch 460, training loss: 881.3322143554688 = 1.09746515750885 + 100.0 * 8.802347183227539
Epoch 460, val loss: 1.097599983215332
Epoch 470, training loss: 881.0831298828125 = 1.097414493560791 + 100.0 * 8.799857139587402
Epoch 470, val loss: 1.09755539894104
Epoch 480, training loss: 880.8201293945312 = 1.0973669290542603 + 100.0 * 8.79722785949707
Epoch 480, val loss: 1.0975137948989868
Epoch 490, training loss: 881.16259765625 = 1.097322940826416 + 100.0 * 8.800652503967285
Epoch 490, val loss: 1.0974708795547485
Epoch 500, training loss: 881.7676391601562 = 1.0972754955291748 + 100.0 * 8.806703567504883
Epoch 500, val loss: 1.0974268913269043
Epoch 510, training loss: 881.3666381835938 = 1.0972254276275635 + 100.0 * 8.802694320678711
Epoch 510, val loss: 1.0973796844482422
Epoch 520, training loss: 881.4644775390625 = 1.097181797027588 + 100.0 * 8.803672790527344
Epoch 520, val loss: 1.0973421335220337
Epoch 530, training loss: 881.523681640625 = 1.0971379280090332 + 100.0 * 8.804265022277832
Epoch 530, val loss: 1.0972986221313477
Epoch 540, training loss: 881.95458984375 = 1.0970863103866577 + 100.0 * 8.808574676513672
Epoch 540, val loss: 1.097253441810608
Epoch 550, training loss: 882.2010498046875 = 1.0970377922058105 + 100.0 * 8.811039924621582
Epoch 550, val loss: 1.0972079038619995
Epoch 560, training loss: 882.3809204101562 = 1.096988320350647 + 100.0 * 8.81283950805664
Epoch 560, val loss: 1.0971652269363403
Epoch 570, training loss: 882.984375 = 1.0969390869140625 + 100.0 * 8.81887435913086
Epoch 570, val loss: 1.0971200466156006
Epoch 580, training loss: 882.78466796875 = 1.0968878269195557 + 100.0 * 8.816878318786621
Epoch 580, val loss: 1.0970767736434937
Epoch 590, training loss: 883.6472778320312 = 1.0968362092971802 + 100.0 * 8.825504302978516
Epoch 590, val loss: 1.0970293283462524
Epoch 600, training loss: 882.29150390625 = 1.096781849861145 + 100.0 * 8.811946868896484
Epoch 600, val loss: 1.0969791412353516
Epoch 610, training loss: 882.5464477539062 = 1.096731424331665 + 100.0 * 8.814496994018555
Epoch 610, val loss: 1.0969363451004028
Epoch 620, training loss: 882.9957275390625 = 1.096684217453003 + 100.0 * 8.818990707397461
Epoch 620, val loss: 1.09689199924469
Epoch 630, training loss: 883.6898803710938 = 1.0966320037841797 + 100.0 * 8.825932502746582
Epoch 630, val loss: 1.0968481302261353
Epoch 640, training loss: 883.9620971679688 = 1.0965797901153564 + 100.0 * 8.828655242919922
Epoch 640, val loss: 1.0967981815338135
Epoch 650, training loss: 884.070068359375 = 1.0965228080749512 + 100.0 * 8.82973575592041
Epoch 650, val loss: 1.0967493057250977
Epoch 660, training loss: 883.5408935546875 = 1.0964637994766235 + 100.0 * 8.824444770812988
Epoch 660, val loss: 1.0966981649398804
Epoch 670, training loss: 883.503173828125 = 1.0964075326919556 + 100.0 * 8.824067115783691
Epoch 670, val loss: 1.0966483354568481
Epoch 680, training loss: 884.0555419921875 = 1.0963510274887085 + 100.0 * 8.829591751098633
Epoch 680, val loss: 1.0965989828109741
Epoch 690, training loss: 884.639892578125 = 1.0962964296340942 + 100.0 * 8.83543586730957
Epoch 690, val loss: 1.0965516567230225
Epoch 700, training loss: 885.1710205078125 = 1.0962388515472412 + 100.0 * 8.840747833251953
Epoch 700, val loss: 1.0964994430541992
Epoch 710, training loss: 884.97607421875 = 1.0961730480194092 + 100.0 * 8.838798522949219
Epoch 710, val loss: 1.0964446067810059
Epoch 720, training loss: 885.3378295898438 = 1.09611177444458 + 100.0 * 8.842416763305664
Epoch 720, val loss: 1.0963876247406006
Epoch 730, training loss: 885.5236206054688 = 1.0960469245910645 + 100.0 * 8.84427547454834
Epoch 730, val loss: 1.0963321924209595
Epoch 740, training loss: 885.7094116210938 = 1.0959807634353638 + 100.0 * 8.846134185791016
Epoch 740, val loss: 1.0962724685668945
Epoch 750, training loss: 885.7665405273438 = 1.0959163904190063 + 100.0 * 8.84670639038086
Epoch 750, val loss: 1.0962163209915161
Epoch 760, training loss: 886.7175903320312 = 1.0958497524261475 + 100.0 * 8.856217384338379
Epoch 760, val loss: 1.0961582660675049
Epoch 770, training loss: 886.366943359375 = 1.0957825183868408 + 100.0 * 8.85271167755127
Epoch 770, val loss: 1.0960991382598877
Epoch 780, training loss: 886.653564453125 = 1.09571373462677 + 100.0 * 8.855578422546387
Epoch 780, val loss: 1.0960360765457153
Epoch 790, training loss: 886.82763671875 = 1.095642328262329 + 100.0 * 8.857319831848145
Epoch 790, val loss: 1.0959739685058594
Epoch 800, training loss: 886.896240234375 = 1.095566749572754 + 100.0 * 8.858006477355957
Epoch 800, val loss: 1.0959086418151855
Epoch 810, training loss: 887.5065307617188 = 1.0954939126968384 + 100.0 * 8.864109992980957
Epoch 810, val loss: 1.095844030380249
Epoch 820, training loss: 887.3861694335938 = 1.095413327217102 + 100.0 * 8.862907409667969
Epoch 820, val loss: 1.095773458480835
Epoch 830, training loss: 887.8839721679688 = 1.095339298248291 + 100.0 * 8.867886543273926
Epoch 830, val loss: 1.0957086086273193
Epoch 840, training loss: 888.1290893554688 = 1.0952630043029785 + 100.0 * 8.870338439941406
Epoch 840, val loss: 1.095637321472168
Epoch 850, training loss: 888.2347412109375 = 1.0951833724975586 + 100.0 * 8.8713960647583
Epoch 850, val loss: 1.0955687761306763
Epoch 860, training loss: 887.9774780273438 = 1.0951069593429565 + 100.0 * 8.868824005126953
Epoch 860, val loss: 1.0955004692077637
Epoch 870, training loss: 887.6793212890625 = 1.0950236320495605 + 100.0 * 8.865842819213867
Epoch 870, val loss: 1.0954266786575317
Epoch 880, training loss: 888.5628051757812 = 1.0949440002441406 + 100.0 * 8.874678611755371
Epoch 880, val loss: 1.0953593254089355
Epoch 890, training loss: 888.79638671875 = 1.0948635339736938 + 100.0 * 8.877015113830566
Epoch 890, val loss: 1.0952848196029663
Epoch 900, training loss: 888.8590698242188 = 1.0947765111923218 + 100.0 * 8.877642631530762
Epoch 900, val loss: 1.0952092409133911
Epoch 910, training loss: 889.1302490234375 = 1.0946935415267944 + 100.0 * 8.880355834960938
Epoch 910, val loss: 1.0951319932937622
Epoch 920, training loss: 889.068115234375 = 1.094599723815918 + 100.0 * 8.879734992980957
Epoch 920, val loss: 1.095052719116211
Epoch 930, training loss: 889.7561645507812 = 1.094511866569519 + 100.0 * 8.886616706848145
Epoch 930, val loss: 1.0949745178222656
Epoch 940, training loss: 889.8856201171875 = 1.0944244861602783 + 100.0 * 8.887911796569824
Epoch 940, val loss: 1.094895601272583
Epoch 950, training loss: 889.5773315429688 = 1.0943372249603271 + 100.0 * 8.8848295211792
Epoch 950, val loss: 1.0948224067687988
Epoch 960, training loss: 889.96142578125 = 1.094250202178955 + 100.0 * 8.888671875
Epoch 960, val loss: 1.0947397947311401
Epoch 970, training loss: 890.4608764648438 = 1.0941559076309204 + 100.0 * 8.893667221069336
Epoch 970, val loss: 1.0946599245071411
Epoch 980, training loss: 890.4454956054688 = 1.0940601825714111 + 100.0 * 8.893514633178711
Epoch 980, val loss: 1.094577431678772
Epoch 990, training loss: 890.1637573242188 = 1.0939627885818481 + 100.0 * 8.890698432922363
Epoch 990, val loss: 1.0944918394088745
Epoch 1000, training loss: 890.37646484375 = 1.0938761234283447 + 100.0 * 8.892826080322266
Epoch 1000, val loss: 1.0944138765335083
Epoch 1010, training loss: 891.3885498046875 = 1.0937832593917847 + 100.0 * 8.902947425842285
Epoch 1010, val loss: 1.0943320989608765
Epoch 1020, training loss: 891.5682983398438 = 1.0936882495880127 + 100.0 * 8.904746055603027
Epoch 1020, val loss: 1.0942438840866089
Epoch 1030, training loss: 891.685546875 = 1.09358811378479 + 100.0 * 8.905920028686523
Epoch 1030, val loss: 1.0941592454910278
Epoch 1040, training loss: 892.0927124023438 = 1.0934888124465942 + 100.0 * 8.909992218017578
Epoch 1040, val loss: 1.0940728187561035
Epoch 1050, training loss: 890.90576171875 = 1.0933868885040283 + 100.0 * 8.898123741149902
Epoch 1050, val loss: 1.0939812660217285
Epoch 1060, training loss: 891.4861450195312 = 1.0932862758636475 + 100.0 * 8.903928756713867
Epoch 1060, val loss: 1.093895435333252
Epoch 1070, training loss: 891.575439453125 = 1.093180775642395 + 100.0 * 8.90482234954834
Epoch 1070, val loss: 1.093802571296692
Epoch 1080, training loss: 892.2500610351562 = 1.0930818319320679 + 100.0 * 8.911569595336914
Epoch 1080, val loss: 1.0937161445617676
Epoch 1090, training loss: 892.7048950195312 = 1.0929778814315796 + 100.0 * 8.916119575500488
Epoch 1090, val loss: 1.0936262607574463
Epoch 1100, training loss: 892.4620361328125 = 1.0928720235824585 + 100.0 * 8.913691520690918
Epoch 1100, val loss: 1.0935331583023071
Epoch 1110, training loss: 892.5037841796875 = 1.0927671194076538 + 100.0 * 8.91411018371582
Epoch 1110, val loss: 1.093442440032959
Epoch 1120, training loss: 893.2197875976562 = 1.092665195465088 + 100.0 * 8.921271324157715
Epoch 1120, val loss: 1.0933516025543213
Epoch 1130, training loss: 893.5048217773438 = 1.092553973197937 + 100.0 * 8.92412281036377
Epoch 1130, val loss: 1.0932528972625732
Epoch 1140, training loss: 893.3516845703125 = 1.0924437046051025 + 100.0 * 8.922592163085938
Epoch 1140, val loss: 1.0931580066680908
Epoch 1150, training loss: 893.7569580078125 = 1.0923293828964233 + 100.0 * 8.92664623260498
Epoch 1150, val loss: 1.093061923980713
Epoch 1160, training loss: 893.845458984375 = 1.0922205448150635 + 100.0 * 8.927532196044922
Epoch 1160, val loss: 1.0929673910140991
Epoch 1170, training loss: 893.997802734375 = 1.0921123027801514 + 100.0 * 8.929057121276855
Epoch 1170, val loss: 1.0928668975830078
Epoch 1180, training loss: 893.7392578125 = 1.0919885635375977 + 100.0 * 8.926472663879395
Epoch 1180, val loss: 1.0927625894546509
Epoch 1190, training loss: 893.3419189453125 = 1.0918793678283691 + 100.0 * 8.922500610351562
Epoch 1190, val loss: 1.092667579650879
Epoch 1200, training loss: 893.9573974609375 = 1.0917619466781616 + 100.0 * 8.928656578063965
Epoch 1200, val loss: 1.0925689935684204
Epoch 1210, training loss: 894.3582763671875 = 1.091644525527954 + 100.0 * 8.932665824890137
Epoch 1210, val loss: 1.0924662351608276
Epoch 1220, training loss: 894.8745727539062 = 1.091526985168457 + 100.0 * 8.937829971313477
Epoch 1220, val loss: 1.0923683643341064
Epoch 1230, training loss: 894.9226684570312 = 1.0914093255996704 + 100.0 * 8.938312530517578
Epoch 1230, val loss: 1.09226393699646
Epoch 1240, training loss: 895.2321166992188 = 1.0912892818450928 + 100.0 * 8.941408157348633
Epoch 1240, val loss: 1.0921623706817627
Epoch 1250, training loss: 895.18310546875 = 1.0911692380905151 + 100.0 * 8.940918922424316
Epoch 1250, val loss: 1.092058777809143
Epoch 1260, training loss: 895.5973510742188 = 1.0910483598709106 + 100.0 * 8.945062637329102
Epoch 1260, val loss: 1.091954231262207
Epoch 1270, training loss: 895.5640258789062 = 1.090936303138733 + 100.0 * 8.944730758666992
Epoch 1270, val loss: 1.09186851978302
Epoch 1280, training loss: 894.315185546875 = 1.0907821655273438 + 100.0 * 8.932244300842285
Epoch 1280, val loss: 1.091733455657959
Epoch 1290, training loss: 894.9407958984375 = 1.0906596183776855 + 100.0 * 8.938501358032227
Epoch 1290, val loss: 1.0916173458099365
Epoch 1300, training loss: 895.2806396484375 = 1.0905430316925049 + 100.0 * 8.941901206970215
Epoch 1300, val loss: 1.0915242433547974
Epoch 1310, training loss: 896.11669921875 = 1.0904213190078735 + 100.0 * 8.950263023376465
Epoch 1310, val loss: 1.0914226770401
Epoch 1320, training loss: 896.7625122070312 = 1.090293526649475 + 100.0 * 8.956722259521484
Epoch 1320, val loss: 1.0913106203079224
Epoch 1330, training loss: 896.5294189453125 = 1.090160846710205 + 100.0 * 8.954392433166504
Epoch 1330, val loss: 1.0911980867385864
Epoch 1340, training loss: 896.754150390625 = 1.0900295972824097 + 100.0 * 8.95664119720459
Epoch 1340, val loss: 1.0910859107971191
Epoch 1350, training loss: 894.6827392578125 = 1.0898853540420532 + 100.0 * 8.935928344726562
Epoch 1350, val loss: 1.090964436531067
Epoch 1360, training loss: 895.0863037109375 = 1.0897600650787354 + 100.0 * 8.93996524810791
Epoch 1360, val loss: 1.0908575057983398
Epoch 1370, training loss: 896.0011596679688 = 1.0896307229995728 + 100.0 * 8.949114799499512
Epoch 1370, val loss: 1.0907477140426636
Epoch 1380, training loss: 897.1748046875 = 1.089502215385437 + 100.0 * 8.960853576660156
Epoch 1380, val loss: 1.090638518333435
Epoch 1390, training loss: 897.9131469726562 = 1.089370846748352 + 100.0 * 8.96823787689209
Epoch 1390, val loss: 1.0905243158340454
Epoch 1400, training loss: 897.831787109375 = 1.089231252670288 + 100.0 * 8.967425346374512
Epoch 1400, val loss: 1.0904030799865723
Epoch 1410, training loss: 897.5809326171875 = 1.0890940427780151 + 100.0 * 8.96491813659668
Epoch 1410, val loss: 1.0902862548828125
Epoch 1420, training loss: 898.2216796875 = 1.088959813117981 + 100.0 * 8.97132682800293
Epoch 1420, val loss: 1.0901747941970825
Epoch 1430, training loss: 897.6060180664062 = 1.0888242721557617 + 100.0 * 8.965171813964844
Epoch 1430, val loss: 1.09005606174469
Epoch 1440, training loss: 897.9613037109375 = 1.0886878967285156 + 100.0 * 8.96872615814209
Epoch 1440, val loss: 1.0899418592453003
Epoch 1450, training loss: 898.5990600585938 = 1.0885603427886963 + 100.0 * 8.975105285644531
Epoch 1450, val loss: 1.089829921722412
Epoch 1460, training loss: 898.70263671875 = 1.0883938074111938 + 100.0 * 8.976142883300781
Epoch 1460, val loss: 1.0896323919296265
Epoch 1470, training loss: 898.7476806640625 = 1.0875104665756226 + 100.0 * 8.976601600646973
Epoch 1470, val loss: 1.0886461734771729
Epoch 1480, training loss: 899.3711547851562 = 1.0865306854248047 + 100.0 * 8.9828462600708
Epoch 1480, val loss: 1.0876070261001587
Epoch 1490, training loss: 899.255615234375 = 1.0856484174728394 + 100.0 * 8.98169994354248
Epoch 1490, val loss: 1.086678385734558
Epoch 1500, training loss: 899.68115234375 = 1.0848569869995117 + 100.0 * 8.985962867736816
Epoch 1500, val loss: 1.085847020149231
Epoch 1510, training loss: 899.6463012695312 = 1.0841376781463623 + 100.0 * 8.985621452331543
Epoch 1510, val loss: 1.0850986242294312
Epoch 1520, training loss: 899.6389770507812 = 1.0834723711013794 + 100.0 * 8.985554695129395
Epoch 1520, val loss: 1.084405541419983
Epoch 1530, training loss: 899.97705078125 = 1.082841157913208 + 100.0 * 8.98894214630127
Epoch 1530, val loss: 1.083757996559143
Epoch 1540, training loss: 900.3187255859375 = 1.0822392702102661 + 100.0 * 8.992364883422852
Epoch 1540, val loss: 1.0831382274627686
Epoch 1550, training loss: 899.7977294921875 = 1.0816519260406494 + 100.0 * 8.987160682678223
Epoch 1550, val loss: 1.0825361013412476
Epoch 1560, training loss: 900.4794921875 = 1.0810825824737549 + 100.0 * 8.99398422241211
Epoch 1560, val loss: 1.0819610357284546
Epoch 1570, training loss: 899.891845703125 = 1.0805189609527588 + 100.0 * 8.988113403320312
Epoch 1570, val loss: 1.081392765045166
Epoch 1580, training loss: 900.6630249023438 = 1.0799777507781982 + 100.0 * 8.995830535888672
Epoch 1580, val loss: 1.0808460712432861
Epoch 1590, training loss: 901.0628051757812 = 1.0794364213943481 + 100.0 * 8.999834060668945
Epoch 1590, val loss: 1.0803064107894897
Epoch 1600, training loss: 900.4048461914062 = 1.0789037942886353 + 100.0 * 8.99325942993164
Epoch 1600, val loss: 1.079753041267395
Epoch 1610, training loss: 899.707763671875 = 1.0783485174179077 + 100.0 * 8.98629379272461
Epoch 1610, val loss: 1.0792113542556763
Epoch 1620, training loss: 899.9866333007812 = 1.0778156518936157 + 100.0 * 8.98908805847168
Epoch 1620, val loss: 1.0786930322647095
Epoch 1630, training loss: 900.3933715820312 = 1.0772969722747803 + 100.0 * 8.99316120147705
Epoch 1630, val loss: 1.0781835317611694
Epoch 1640, training loss: 901.1954956054688 = 1.0767755508422852 + 100.0 * 9.001187324523926
Epoch 1640, val loss: 1.0776726007461548
Epoch 1650, training loss: 901.5344848632812 = 1.076250433921814 + 100.0 * 9.004582405090332
Epoch 1650, val loss: 1.0771547555923462
Epoch 1660, training loss: 901.3336791992188 = 1.0757108926773071 + 100.0 * 9.002579689025879
Epoch 1660, val loss: 1.0766215324401855
Epoch 1670, training loss: 901.4896850585938 = 1.075166940689087 + 100.0 * 9.004144668579102
Epoch 1670, val loss: 1.076092004776001
Epoch 1680, training loss: 901.80908203125 = 1.0746111869812012 + 100.0 * 9.007345199584961
Epoch 1680, val loss: 1.075554370880127
Epoch 1690, training loss: 901.9144287109375 = 1.0740437507629395 + 100.0 * 9.008403778076172
Epoch 1690, val loss: 1.0749999284744263
Epoch 1700, training loss: 901.7952880859375 = 1.073452353477478 + 100.0 * 9.007218360900879
Epoch 1700, val loss: 1.0744143724441528
Epoch 1710, training loss: 901.7153930664062 = 1.0728036165237427 + 100.0 * 9.006425857543945
Epoch 1710, val loss: 1.0737875699996948
Epoch 1720, training loss: 901.9210205078125 = 1.0721251964569092 + 100.0 * 9.008488655090332
Epoch 1720, val loss: 1.0731182098388672
Epoch 1730, training loss: 902.0328979492188 = 1.0714471340179443 + 100.0 * 9.009613990783691
Epoch 1730, val loss: 1.0724692344665527
Epoch 1740, training loss: 902.3057250976562 = 1.0707662105560303 + 100.0 * 9.012350082397461
Epoch 1740, val loss: 1.07180917263031
Epoch 1750, training loss: 902.481689453125 = 1.0700918436050415 + 100.0 * 9.014116287231445
Epoch 1750, val loss: 1.0711615085601807
Epoch 1760, training loss: 902.3346557617188 = 1.0694104433059692 + 100.0 * 9.012652397155762
Epoch 1760, val loss: 1.0705087184906006
Epoch 1770, training loss: 902.2041625976562 = 1.068726658821106 + 100.0 * 9.011354446411133
Epoch 1770, val loss: 1.0698645114898682
Epoch 1780, training loss: 902.6734008789062 = 1.068041205406189 + 100.0 * 9.016053199768066
Epoch 1780, val loss: 1.069211721420288
Epoch 1790, training loss: 902.9000854492188 = 1.0673576593399048 + 100.0 * 9.018326759338379
Epoch 1790, val loss: 1.0685595273971558
Epoch 1800, training loss: 902.9599609375 = 1.066652774810791 + 100.0 * 9.018933296203613
Epoch 1800, val loss: 1.0678988695144653
Epoch 1810, training loss: 903.0752563476562 = 1.065954327583313 + 100.0 * 9.020092964172363
Epoch 1810, val loss: 1.0672308206558228
Epoch 1820, training loss: 903.2379760742188 = 1.0652490854263306 + 100.0 * 9.021727561950684
Epoch 1820, val loss: 1.0665656328201294
Epoch 1830, training loss: 903.1956176757812 = 1.064525842666626 + 100.0 * 9.021310806274414
Epoch 1830, val loss: 1.0658752918243408
Epoch 1840, training loss: 903.2969360351562 = 1.0638017654418945 + 100.0 * 9.022331237792969
Epoch 1840, val loss: 1.0651897192001343
Epoch 1850, training loss: 903.5185546875 = 1.0630642175674438 + 100.0 * 9.024555206298828
Epoch 1850, val loss: 1.0645051002502441
Epoch 1860, training loss: 902.7634887695312 = 1.06228506565094 + 100.0 * 9.017012596130371
Epoch 1860, val loss: 1.0637694597244263
Epoch 1870, training loss: 902.0988159179688 = 1.0615442991256714 + 100.0 * 9.01037311553955
Epoch 1870, val loss: 1.0630601644515991
Epoch 1880, training loss: 902.2930297851562 = 1.0607880353927612 + 100.0 * 9.012322425842285
Epoch 1880, val loss: 1.0623422861099243
Epoch 1890, training loss: 903.2239379882812 = 1.0600370168685913 + 100.0 * 9.021638870239258
Epoch 1890, val loss: 1.0616384744644165
Epoch 1900, training loss: 903.7972412109375 = 1.0592865943908691 + 100.0 * 9.027379989624023
Epoch 1900, val loss: 1.0609259605407715
Epoch 1910, training loss: 903.8150024414062 = 1.0585100650787354 + 100.0 * 9.027565002441406
Epoch 1910, val loss: 1.0602025985717773
Epoch 1920, training loss: 903.307373046875 = 1.0577138662338257 + 100.0 * 9.022496223449707
Epoch 1920, val loss: 1.0594475269317627
Epoch 1930, training loss: 903.9348754882812 = 1.056922435760498 + 100.0 * 9.028779029846191
Epoch 1930, val loss: 1.0587111711502075
Epoch 1940, training loss: 904.5177001953125 = 1.0561294555664062 + 100.0 * 9.034615516662598
Epoch 1940, val loss: 1.0579599142074585
Epoch 1950, training loss: 904.4570922851562 = 1.0553309917449951 + 100.0 * 9.034017562866211
Epoch 1950, val loss: 1.057213544845581
Epoch 1960, training loss: 904.0999755859375 = 1.0545095205307007 + 100.0 * 9.030454635620117
Epoch 1960, val loss: 1.0564546585083008
Epoch 1970, training loss: 904.3717041015625 = 1.0536919832229614 + 100.0 * 9.033180236816406
Epoch 1970, val loss: 1.055680751800537
Epoch 1980, training loss: 904.9119873046875 = 1.0528686046600342 + 100.0 * 9.038591384887695
Epoch 1980, val loss: 1.0549153089523315
Epoch 1990, training loss: 904.6923217773438 = 1.0520507097244263 + 100.0 * 9.036402702331543
Epoch 1990, val loss: 1.0541452169418335
Epoch 2000, training loss: 904.6143188476562 = 1.051212191581726 + 100.0 * 9.03563117980957
Epoch 2000, val loss: 1.053360939025879
Epoch 2010, training loss: 904.9281616210938 = 1.0503820180892944 + 100.0 * 9.038778305053711
Epoch 2010, val loss: 1.0525853633880615
Epoch 2020, training loss: 905.3754272460938 = 1.0495485067367554 + 100.0 * 9.043258666992188
Epoch 2020, val loss: 1.0518062114715576
Epoch 2030, training loss: 904.8068237304688 = 1.0487112998962402 + 100.0 * 9.037581443786621
Epoch 2030, val loss: 1.0510231256484985
Epoch 2040, training loss: 904.6312866210938 = 1.0478812456130981 + 100.0 * 9.035834312438965
Epoch 2040, val loss: 1.0502625703811646
Epoch 2050, training loss: 904.1007690429688 = 1.0470901727676392 + 100.0 * 9.030536651611328
Epoch 2050, val loss: 1.0495089292526245
Epoch 2060, training loss: 903.6969604492188 = 1.0462185144424438 + 100.0 * 9.026507377624512
Epoch 2060, val loss: 1.048708200454712
Epoch 2070, training loss: 903.7726440429688 = 1.0453236103057861 + 100.0 * 9.027273178100586
Epoch 2070, val loss: 1.0479036569595337
Epoch 2080, training loss: 904.0187377929688 = 1.0444906949996948 + 100.0 * 9.029742240905762
Epoch 2080, val loss: 1.0471062660217285
Epoch 2090, training loss: 904.5256958007812 = 1.043745994567871 + 100.0 * 9.034819602966309
Epoch 2090, val loss: 1.0464528799057007
Epoch 2100, training loss: 905.24658203125 = 1.0429346561431885 + 100.0 * 9.042037010192871
Epoch 2100, val loss: 1.0456933975219727
Epoch 2110, training loss: 905.1300659179688 = 1.0420809984207153 + 100.0 * 9.04088020324707
Epoch 2110, val loss: 1.0448873043060303
Epoch 2120, training loss: 905.434326171875 = 1.0412273406982422 + 100.0 * 9.043931007385254
Epoch 2120, val loss: 1.0440795421600342
Epoch 2130, training loss: 905.9136962890625 = 1.040371298789978 + 100.0 * 9.048733711242676
Epoch 2130, val loss: 1.0432815551757812
Epoch 2140, training loss: 905.5882568359375 = 1.039502501487732 + 100.0 * 9.045487403869629
Epoch 2140, val loss: 1.0424824953079224
Epoch 2150, training loss: 904.5562744140625 = 1.0386463403701782 + 100.0 * 9.035176277160645
Epoch 2150, val loss: 1.0417126417160034
Epoch 2160, training loss: 905.5765380859375 = 1.0378139019012451 + 100.0 * 9.045387268066406
Epoch 2160, val loss: 1.040937066078186
Epoch 2170, training loss: 906.1683349609375 = 1.036981225013733 + 100.0 * 9.051313400268555
Epoch 2170, val loss: 1.040169596672058
Epoch 2180, training loss: 906.4339599609375 = 1.0361342430114746 + 100.0 * 9.053977966308594
Epoch 2180, val loss: 1.0393931865692139
Epoch 2190, training loss: 906.315673828125 = 1.0352890491485596 + 100.0 * 9.052803993225098
Epoch 2190, val loss: 1.038605809211731
Epoch 2200, training loss: 906.5989990234375 = 1.034430742263794 + 100.0 * 9.055645942687988
Epoch 2200, val loss: 1.0378377437591553
Epoch 2210, training loss: 906.7387084960938 = 1.0335832834243774 + 100.0 * 9.057051658630371
Epoch 2210, val loss: 1.0370607376098633
Epoch 2220, training loss: 907.1387329101562 = 1.0327367782592773 + 100.0 * 9.061059951782227
Epoch 2220, val loss: 1.0362803936004639
Epoch 2230, training loss: 906.8621215820312 = 1.0318779945373535 + 100.0 * 9.058302879333496
Epoch 2230, val loss: 1.0354886054992676
Epoch 2240, training loss: 907.0022583007812 = 1.0310251712799072 + 100.0 * 9.059712409973145
Epoch 2240, val loss: 1.0347117185592651
Epoch 2250, training loss: 907.3469848632812 = 1.0302135944366455 + 100.0 * 9.063167572021484
Epoch 2250, val loss: 1.0339585542678833
Epoch 2260, training loss: 907.154541015625 = 1.0293638706207275 + 100.0 * 9.061251640319824
Epoch 2260, val loss: 1.0331687927246094
Epoch 2270, training loss: 907.7493286132812 = 1.0284115076065063 + 100.0 * 9.067209243774414
Epoch 2270, val loss: 1.0322787761688232
Epoch 2280, training loss: 906.6001586914062 = 1.0275541543960571 + 100.0 * 9.055726051330566
Epoch 2280, val loss: 1.0314784049987793
Epoch 2290, training loss: 906.6715698242188 = 1.0266456604003906 + 100.0 * 9.056448936462402
Epoch 2290, val loss: 1.03069007396698
Epoch 2300, training loss: 907.4008178710938 = 1.0257983207702637 + 100.0 * 9.063750267028809
Epoch 2300, val loss: 1.0299397706985474
Epoch 2310, training loss: 907.1188354492188 = 1.0249561071395874 + 100.0 * 9.060938835144043
Epoch 2310, val loss: 1.0291723012924194
Epoch 2320, training loss: 907.5489501953125 = 1.024088740348816 + 100.0 * 9.065248489379883
Epoch 2320, val loss: 1.0283730030059814
Epoch 2330, training loss: 908.3720092773438 = 1.0232043266296387 + 100.0 * 9.073488235473633
Epoch 2330, val loss: 1.0275663137435913
Epoch 2340, training loss: 908.9236450195312 = 1.0223004817962646 + 100.0 * 9.07901382446289
Epoch 2340, val loss: 1.0267398357391357
Epoch 2350, training loss: 908.915283203125 = 1.0213896036148071 + 100.0 * 9.078939437866211
Epoch 2350, val loss: 1.0259203910827637
Epoch 2360, training loss: 908.830078125 = 1.0204936265945435 + 100.0 * 9.078095436096191
Epoch 2360, val loss: 1.0250903367996216
Epoch 2370, training loss: 909.3370361328125 = 1.01961350440979 + 100.0 * 9.083174705505371
Epoch 2370, val loss: 1.0243072509765625
Epoch 2380, training loss: 909.7254028320312 = 1.018742322921753 + 100.0 * 9.087066650390625
Epoch 2380, val loss: 1.0235247611999512
Epoch 2390, training loss: 909.3878173828125 = 1.0178468227386475 + 100.0 * 9.083700180053711
Epoch 2390, val loss: 1.0227001905441284
Epoch 2400, training loss: 909.6629028320312 = 1.0169733762741089 + 100.0 * 9.086459159851074
Epoch 2400, val loss: 1.0219208002090454
Epoch 2410, training loss: 909.9828491210938 = 1.0161060094833374 + 100.0 * 9.089667320251465
Epoch 2410, val loss: 1.0211395025253296
Epoch 2420, training loss: 910.24609375 = 1.0152307748794556 + 100.0 * 9.092308044433594
Epoch 2420, val loss: 1.0203707218170166
Epoch 2430, training loss: 909.4808959960938 = 1.0143215656280518 + 100.0 * 9.084665298461914
Epoch 2430, val loss: 1.0195568799972534
Epoch 2440, training loss: 909.831298828125 = 1.0134471654891968 + 100.0 * 9.088178634643555
Epoch 2440, val loss: 1.0187640190124512
Epoch 2450, training loss: 910.12548828125 = 1.012574553489685 + 100.0 * 9.091129302978516
Epoch 2450, val loss: 1.0179873704910278
Epoch 2460, training loss: 910.2022705078125 = 1.011696219444275 + 100.0 * 9.09190559387207
Epoch 2460, val loss: 1.0171977281570435
Epoch 2470, training loss: 910.2503662109375 = 1.0108416080474854 + 100.0 * 9.092394828796387
Epoch 2470, val loss: 1.0164408683776855
Epoch 2480, training loss: 910.2486572265625 = 1.009951114654541 + 100.0 * 9.092387199401855
Epoch 2480, val loss: 1.0156527757644653
Epoch 2490, training loss: 910.3638916015625 = 1.0090972185134888 + 100.0 * 9.093547821044922
Epoch 2490, val loss: 1.0148788690567017
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.5092753623188405
0.8164167210026807
The final CL Acc:0.50290, 0.02514, The final GNN Acc:0.81714, 0.00062
raw graph: torch.Size([2, 88648])
add edge: torch.Size([2, 110778])
remove edge: torch.Size([2, 66334])
updated graph: torch.Size([2, 88464])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 1011.3350219726562 = 1.1084222793579102 + 100.0 * 10.102266311645508
Epoch 0, val loss: 1.107256293296814
Epoch 10, training loss: 970.2508544921875 = 1.1081750392913818 + 100.0 * 9.691427230834961
Epoch 10, val loss: 1.1069858074188232
Epoch 20, training loss: 949.6146240234375 = 1.1077693700790405 + 100.0 * 9.485068321228027
Epoch 20, val loss: 1.1065622568130493
Epoch 30, training loss: 934.6052856445312 = 1.1073827743530273 + 100.0 * 9.334979057312012
Epoch 30, val loss: 1.1061606407165527
Epoch 40, training loss: 923.3180541992188 = 1.1069945096969604 + 100.0 * 9.222110748291016
Epoch 40, val loss: 1.1057542562484741
Epoch 50, training loss: 914.4327392578125 = 1.1065881252288818 + 100.0 * 9.133261680603027
Epoch 50, val loss: 1.1053341627120972
Epoch 60, training loss: 907.2173461914062 = 1.1061817407608032 + 100.0 * 9.061111450195312
Epoch 60, val loss: 1.1049118041992188
Epoch 70, training loss: 901.279052734375 = 1.105777621269226 + 100.0 * 9.00173282623291
Epoch 70, val loss: 1.104492425918579
Epoch 80, training loss: 896.2180786132812 = 1.105366826057434 + 100.0 * 8.951127052307129
Epoch 80, val loss: 1.104064702987671
Epoch 90, training loss: 891.7496337890625 = 1.1049647331237793 + 100.0 * 8.90644645690918
Epoch 90, val loss: 1.1036486625671387
Epoch 100, training loss: 887.9537963867188 = 1.104562759399414 + 100.0 * 8.868492126464844
Epoch 100, val loss: 1.1032311916351318
Epoch 110, training loss: 884.5680541992188 = 1.1041474342346191 + 100.0 * 8.834639549255371
Epoch 110, val loss: 1.102803349494934
Epoch 120, training loss: 881.682373046875 = 1.1037424802780151 + 100.0 * 8.8057861328125
Epoch 120, val loss: 1.1023849248886108
Epoch 130, training loss: 879.09912109375 = 1.1033196449279785 + 100.0 * 8.77995777130127
Epoch 130, val loss: 1.101948857307434
Epoch 140, training loss: 876.953857421875 = 1.1028926372528076 + 100.0 * 8.758509635925293
Epoch 140, val loss: 1.1015090942382812
Epoch 150, training loss: 875.1478271484375 = 1.102460503578186 + 100.0 * 8.740453720092773
Epoch 150, val loss: 1.1010624170303345
Epoch 160, training loss: 873.5346069335938 = 1.1020134687423706 + 100.0 * 8.724326133728027
Epoch 160, val loss: 1.100602149963379
Epoch 170, training loss: 872.1249389648438 = 1.1015487909317017 + 100.0 * 8.710233688354492
Epoch 170, val loss: 1.1001224517822266
Epoch 180, training loss: 870.9778442382812 = 1.1010684967041016 + 100.0 * 8.69876766204834
Epoch 180, val loss: 1.09963858127594
Epoch 190, training loss: 869.728515625 = 1.1005668640136719 + 100.0 * 8.686279296875
Epoch 190, val loss: 1.0991262197494507
Epoch 200, training loss: 869.310546875 = 1.1000516414642334 + 100.0 * 8.68210506439209
Epoch 200, val loss: 1.0985920429229736
Epoch 210, training loss: 868.2656860351562 = 1.0995299816131592 + 100.0 * 8.671661376953125
Epoch 210, val loss: 1.0980710983276367
Epoch 220, training loss: 867.7344360351562 = 1.0990079641342163 + 100.0 * 8.666354179382324
Epoch 220, val loss: 1.0975351333618164
Epoch 230, training loss: 867.2369995117188 = 1.0984634160995483 + 100.0 * 8.661385536193848
Epoch 230, val loss: 1.0969877243041992
Epoch 240, training loss: 866.6148681640625 = 1.0979002714157104 + 100.0 * 8.655169486999512
Epoch 240, val loss: 1.0964194536209106
Epoch 250, training loss: 866.0896606445312 = 1.097326636314392 + 100.0 * 8.649923324584961
Epoch 250, val loss: 1.0958361625671387
Epoch 260, training loss: 865.5996704101562 = 1.0967565774917603 + 100.0 * 8.645029067993164
Epoch 260, val loss: 1.0952587127685547
Epoch 270, training loss: 865.5443115234375 = 1.096160888671875 + 100.0 * 8.644481658935547
Epoch 270, val loss: 1.0946670770645142
Epoch 280, training loss: 865.0760498046875 = 1.095551609992981 + 100.0 * 8.63980484008789
Epoch 280, val loss: 1.0940380096435547
Epoch 290, training loss: 864.761474609375 = 1.0949714183807373 + 100.0 * 8.636665344238281
Epoch 290, val loss: 1.0934579372406006
Epoch 300, training loss: 864.50732421875 = 1.0943455696105957 + 100.0 * 8.634129524230957
Epoch 300, val loss: 1.0928359031677246
Epoch 310, training loss: 864.7679443359375 = 1.0937302112579346 + 100.0 * 8.636741638183594
Epoch 310, val loss: 1.0922142267227173
Epoch 320, training loss: 864.290283203125 = 1.0930976867675781 + 100.0 * 8.631972312927246
Epoch 320, val loss: 1.0915765762329102
Epoch 330, training loss: 864.4923706054688 = 1.092466115951538 + 100.0 * 8.63399887084961
Epoch 330, val loss: 1.0909380912780762
Epoch 340, training loss: 864.2651977539062 = 1.0918093919754028 + 100.0 * 8.631733894348145
Epoch 340, val loss: 1.0902942419052124
Epoch 350, training loss: 864.3402709960938 = 1.091161847114563 + 100.0 * 8.632491111755371
Epoch 350, val loss: 1.0896245241165161
Epoch 360, training loss: 864.5050048828125 = 1.090507984161377 + 100.0 * 8.63414478302002
Epoch 360, val loss: 1.0889838933944702
Epoch 370, training loss: 864.437255859375 = 1.0898447036743164 + 100.0 * 8.633474349975586
Epoch 370, val loss: 1.088319182395935
Epoch 380, training loss: 864.3561401367188 = 1.0892021656036377 + 100.0 * 8.632669448852539
Epoch 380, val loss: 1.0876761674880981
Epoch 390, training loss: 864.665283203125 = 1.088558554649353 + 100.0 * 8.635766983032227
Epoch 390, val loss: 1.0870286226272583
Epoch 400, training loss: 864.6362915039062 = 1.0879079103469849 + 100.0 * 8.635483741760254
Epoch 400, val loss: 1.086399793624878
Epoch 410, training loss: 864.7333374023438 = 1.0872958898544312 + 100.0 * 8.636460304260254
Epoch 410, val loss: 1.085808515548706
Epoch 420, training loss: 864.9052734375 = 1.0867058038711548 + 100.0 * 8.638185501098633
Epoch 420, val loss: 1.0852254629135132
Epoch 430, training loss: 865.3175659179688 = 1.0861585140228271 + 100.0 * 8.642313957214355
Epoch 430, val loss: 1.0846842527389526
Epoch 440, training loss: 865.4403686523438 = 1.0856292247772217 + 100.0 * 8.643547058105469
Epoch 440, val loss: 1.0841790437698364
Epoch 450, training loss: 865.587646484375 = 1.0851411819458008 + 100.0 * 8.645025253295898
Epoch 450, val loss: 1.0836952924728394
Epoch 460, training loss: 865.66455078125 = 1.0846712589263916 + 100.0 * 8.645798683166504
Epoch 460, val loss: 1.0832452774047852
Epoch 470, training loss: 866.0747680664062 = 1.0842324495315552 + 100.0 * 8.64990520477295
Epoch 470, val loss: 1.0828121900558472
Epoch 480, training loss: 865.9938354492188 = 1.0837923288345337 + 100.0 * 8.649100303649902
Epoch 480, val loss: 1.0823943614959717
Epoch 490, training loss: 866.1217651367188 = 1.083362340927124 + 100.0 * 8.650383949279785
Epoch 490, val loss: 1.0819741487503052
Epoch 500, training loss: 866.2225341796875 = 1.0829436779022217 + 100.0 * 8.651395797729492
Epoch 500, val loss: 1.0815632343292236
Epoch 510, training loss: 866.4830932617188 = 1.0825469493865967 + 100.0 * 8.654006004333496
Epoch 510, val loss: 1.0811675786972046
Epoch 520, training loss: 866.5562133789062 = 1.0821372270584106 + 100.0 * 8.654740333557129
Epoch 520, val loss: 1.0807782411575317
Epoch 530, training loss: 866.7199096679688 = 1.0816816091537476 + 100.0 * 8.65638256072998
Epoch 530, val loss: 1.0803325176239014
Epoch 540, training loss: 866.62158203125 = 1.0813488960266113 + 100.0 * 8.655402183532715
Epoch 540, val loss: 1.0800071954727173
Epoch 550, training loss: 866.525390625 = 1.080931305885315 + 100.0 * 8.654444694519043
Epoch 550, val loss: 1.0796157121658325
Epoch 560, training loss: 866.6171875 = 1.0805010795593262 + 100.0 * 8.655366897583008
Epoch 560, val loss: 1.0791767835617065
Epoch 570, training loss: 867.0438842773438 = 1.0801149606704712 + 100.0 * 8.659637451171875
Epoch 570, val loss: 1.0788154602050781
Epoch 580, training loss: 867.1521606445312 = 1.0797089338302612 + 100.0 * 8.660724639892578
Epoch 580, val loss: 1.0784181356430054
Epoch 590, training loss: 867.15087890625 = 1.0792768001556396 + 100.0 * 8.66071605682373
Epoch 590, val loss: 1.078001856803894
Epoch 600, training loss: 867.064453125 = 1.0788559913635254 + 100.0 * 8.659855842590332
Epoch 600, val loss: 1.0776058435440063
Epoch 610, training loss: 867.3939819335938 = 1.0784218311309814 + 100.0 * 8.663155555725098
Epoch 610, val loss: 1.0771925449371338
Epoch 620, training loss: 867.7285766601562 = 1.0780099630355835 + 100.0 * 8.666505813598633
Epoch 620, val loss: 1.076782464981079
Epoch 630, training loss: 867.7819213867188 = 1.0775740146636963 + 100.0 * 8.667043685913086
Epoch 630, val loss: 1.0763461589813232
Epoch 640, training loss: 867.7695922851562 = 1.0771394968032837 + 100.0 * 8.666924476623535
Epoch 640, val loss: 1.0759536027908325
Epoch 650, training loss: 868.1903076171875 = 1.076715350151062 + 100.0 * 8.671135902404785
Epoch 650, val loss: 1.07554292678833
Epoch 660, training loss: 868.1625366210938 = 1.0762535333633423 + 100.0 * 8.670863151550293
Epoch 660, val loss: 1.075087070465088
Epoch 670, training loss: 867.8240356445312 = 1.075804591178894 + 100.0 * 8.667482376098633
Epoch 670, val loss: 1.0746608972549438
Epoch 680, training loss: 868.3927612304688 = 1.0753926038742065 + 100.0 * 8.673173904418945
Epoch 680, val loss: 1.0742638111114502
Epoch 690, training loss: 868.730712890625 = 1.0749692916870117 + 100.0 * 8.676557540893555
Epoch 690, val loss: 1.0738695859909058
Epoch 700, training loss: 868.793212890625 = 1.0745418071746826 + 100.0 * 8.677186965942383
Epoch 700, val loss: 1.0734421014785767
Epoch 710, training loss: 869.39697265625 = 1.0741029977798462 + 100.0 * 8.683228492736816
Epoch 710, val loss: 1.073027491569519
Epoch 720, training loss: 869.649169921875 = 1.073664903640747 + 100.0 * 8.685754776000977
Epoch 720, val loss: 1.072601556777954
Epoch 730, training loss: 869.5261840820312 = 1.0732110738754272 + 100.0 * 8.684530258178711
Epoch 730, val loss: 1.0721617937088013
Epoch 740, training loss: 870.0143432617188 = 1.0727673768997192 + 100.0 * 8.68941593170166
Epoch 740, val loss: 1.0717358589172363
Epoch 750, training loss: 870.413330078125 = 1.0723345279693604 + 100.0 * 8.69340991973877
Epoch 750, val loss: 1.071320652961731
Epoch 760, training loss: 870.1810302734375 = 1.0718718767166138 + 100.0 * 8.691091537475586
Epoch 760, val loss: 1.070873737335205
Epoch 770, training loss: 870.2109985351562 = 1.0714284181594849 + 100.0 * 8.69139575958252
Epoch 770, val loss: 1.0704402923583984
Epoch 780, training loss: 870.8255615234375 = 1.071018099784851 + 100.0 * 8.697545051574707
Epoch 780, val loss: 1.0700310468673706
Epoch 790, training loss: 870.9129028320312 = 1.070572018623352 + 100.0 * 8.698423385620117
Epoch 790, val loss: 1.0695947408676147
Epoch 800, training loss: 871.2843017578125 = 1.070123553276062 + 100.0 * 8.702141761779785
Epoch 800, val loss: 1.0691654682159424
Epoch 810, training loss: 871.3523559570312 = 1.0696802139282227 + 100.0 * 8.702826499938965
Epoch 810, val loss: 1.0687179565429688
Epoch 820, training loss: 871.8728637695312 = 1.06923508644104 + 100.0 * 8.708036422729492
Epoch 820, val loss: 1.0682979822158813
Epoch 830, training loss: 872.0139770507812 = 1.0687907934188843 + 100.0 * 8.709451675415039
Epoch 830, val loss: 1.0678694248199463
Epoch 840, training loss: 872.4199829101562 = 1.0683472156524658 + 100.0 * 8.713516235351562
Epoch 840, val loss: 1.067423939704895
Epoch 850, training loss: 871.3184204101562 = 1.0678308010101318 + 100.0 * 8.702506065368652
Epoch 850, val loss: 1.0669448375701904
Epoch 860, training loss: 871.3258056640625 = 1.0673699378967285 + 100.0 * 8.702584266662598
Epoch 860, val loss: 1.0664974451065063
Epoch 870, training loss: 871.8629760742188 = 1.0669459104537964 + 100.0 * 8.70796012878418
Epoch 870, val loss: 1.0660662651062012
Epoch 880, training loss: 872.5321044921875 = 1.0665134191513062 + 100.0 * 8.714655876159668
Epoch 880, val loss: 1.0656497478485107
Epoch 890, training loss: 872.8711547851562 = 1.0660535097122192 + 100.0 * 8.718050956726074
Epoch 890, val loss: 1.0652053356170654
Epoch 900, training loss: 872.9598999023438 = 1.065579891204834 + 100.0 * 8.71894359588623
Epoch 900, val loss: 1.0647519826889038
Epoch 910, training loss: 873.5665893554688 = 1.0651344060897827 + 100.0 * 8.725014686584473
Epoch 910, val loss: 1.064318299293518
Epoch 920, training loss: 873.5199584960938 = 1.0646437406539917 + 100.0 * 8.724553108215332
Epoch 920, val loss: 1.063854455947876
Epoch 930, training loss: 873.528076171875 = 1.0641978979110718 + 100.0 * 8.724638938903809
Epoch 930, val loss: 1.063421368598938
Epoch 940, training loss: 873.6981201171875 = 1.063739538192749 + 100.0 * 8.726344108581543
Epoch 940, val loss: 1.062978744506836
Epoch 950, training loss: 873.897705078125 = 1.0632802248001099 + 100.0 * 8.728343963623047
Epoch 950, val loss: 1.0625152587890625
Epoch 960, training loss: 874.1359252929688 = 1.0628055334091187 + 100.0 * 8.730731010437012
Epoch 960, val loss: 1.06207275390625
Epoch 970, training loss: 874.5352783203125 = 1.0623581409454346 + 100.0 * 8.734728813171387
Epoch 970, val loss: 1.0616371631622314
Epoch 980, training loss: 873.7706298828125 = 1.0617761611938477 + 100.0 * 8.727088928222656
Epoch 980, val loss: 1.0610579252243042
Epoch 990, training loss: 875.52587890625 = 1.0614477396011353 + 100.0 * 8.744644165039062
Epoch 990, val loss: 1.0607171058654785
Epoch 1000, training loss: 873.2108764648438 = 1.060862421989441 + 100.0 * 8.721500396728516
Epoch 1000, val loss: 1.060225486755371
Epoch 1010, training loss: 874.5599975585938 = 1.0605067014694214 + 100.0 * 8.734994888305664
Epoch 1010, val loss: 1.0598340034484863
Epoch 1020, training loss: 874.129150390625 = 1.0599586963653564 + 100.0 * 8.730691909790039
Epoch 1020, val loss: 1.0593197345733643
Epoch 1030, training loss: 874.6463623046875 = 1.0595027208328247 + 100.0 * 8.735868453979492
Epoch 1030, val loss: 1.0588620901107788
Epoch 1040, training loss: 875.0907592773438 = 1.0591412782669067 + 100.0 * 8.740316390991211
Epoch 1040, val loss: 1.0585136413574219
Epoch 1050, training loss: 875.4898071289062 = 1.0586762428283691 + 100.0 * 8.744311332702637
Epoch 1050, val loss: 1.0580507516860962
Epoch 1060, training loss: 875.7740478515625 = 1.058161735534668 + 100.0 * 8.747159004211426
Epoch 1060, val loss: 1.057561993598938
Epoch 1070, training loss: 875.9276733398438 = 1.0576742887496948 + 100.0 * 8.748700141906738
Epoch 1070, val loss: 1.057090163230896
Epoch 1080, training loss: 876.2269287109375 = 1.0571985244750977 + 100.0 * 8.751697540283203
Epoch 1080, val loss: 1.0566381216049194
Epoch 1090, training loss: 876.2186279296875 = 1.0567119121551514 + 100.0 * 8.751619338989258
Epoch 1090, val loss: 1.0561747550964355
Epoch 1100, training loss: 876.83349609375 = 1.0562714338302612 + 100.0 * 8.757772445678711
Epoch 1100, val loss: 1.0557235479354858
Epoch 1110, training loss: 876.2483520507812 = 1.0557678937911987 + 100.0 * 8.751925468444824
Epoch 1110, val loss: 1.0552494525909424
Epoch 1120, training loss: 876.8140258789062 = 1.0553004741668701 + 100.0 * 8.757587432861328
Epoch 1120, val loss: 1.0547856092453003
Epoch 1130, training loss: 877.3834838867188 = 1.0548217296600342 + 100.0 * 8.763286590576172
Epoch 1130, val loss: 1.0543302297592163
Epoch 1140, training loss: 877.5652465820312 = 1.0543290376663208 + 100.0 * 8.765109062194824
Epoch 1140, val loss: 1.0538558959960938
Epoch 1150, training loss: 877.9283447265625 = 1.0538434982299805 + 100.0 * 8.768745422363281
Epoch 1150, val loss: 1.0533761978149414
Epoch 1160, training loss: 877.5099487304688 = 1.0533053874969482 + 100.0 * 8.764566421508789
Epoch 1160, val loss: 1.0528666973114014
Epoch 1170, training loss: 878.049560546875 = 1.052832841873169 + 100.0 * 8.769967079162598
Epoch 1170, val loss: 1.0524147748947144
Epoch 1180, training loss: 878.58447265625 = 1.0523381233215332 + 100.0 * 8.775321006774902
Epoch 1180, val loss: 1.051935076713562
Epoch 1190, training loss: 878.28466796875 = 1.051833987236023 + 100.0 * 8.77232837677002
Epoch 1190, val loss: 1.0514425039291382
Epoch 1200, training loss: 878.762451171875 = 1.0513359308242798 + 100.0 * 8.777111053466797
Epoch 1200, val loss: 1.050971508026123
Epoch 1210, training loss: 879.1657104492188 = 1.0508413314819336 + 100.0 * 8.781148910522461
Epoch 1210, val loss: 1.05048668384552
Epoch 1220, training loss: 879.0177612304688 = 1.050315499305725 + 100.0 * 8.779674530029297
Epoch 1220, val loss: 1.0499662160873413
Epoch 1230, training loss: 879.1611938476562 = 1.0497992038726807 + 100.0 * 8.781113624572754
Epoch 1230, val loss: 1.049487590789795
Epoch 1240, training loss: 879.5367431640625 = 1.0493040084838867 + 100.0 * 8.784873962402344
Epoch 1240, val loss: 1.0490193367004395
Epoch 1250, training loss: 879.2614135742188 = 1.04878830909729 + 100.0 * 8.782126426696777
Epoch 1250, val loss: 1.0485141277313232
Epoch 1260, training loss: 879.37744140625 = 1.0482860803604126 + 100.0 * 8.783291816711426
Epoch 1260, val loss: 1.0480303764343262
Epoch 1270, training loss: 879.0239868164062 = 1.0477726459503174 + 100.0 * 8.779762268066406
Epoch 1270, val loss: 1.0475153923034668
Epoch 1280, training loss: 879.017333984375 = 1.0472451448440552 + 100.0 * 8.779701232910156
Epoch 1280, val loss: 1.0469857454299927
Epoch 1290, training loss: 879.3097534179688 = 1.0467381477355957 + 100.0 * 8.78262996673584
Epoch 1290, val loss: 1.046555757522583
Epoch 1300, training loss: 880.1161499023438 = 1.0462511777877808 + 100.0 * 8.790699005126953
Epoch 1300, val loss: 1.0460407733917236
Epoch 1310, training loss: 879.038818359375 = 1.045698642730713 + 100.0 * 8.77993106842041
Epoch 1310, val loss: 1.045533299446106
Epoch 1320, training loss: 879.2252807617188 = 1.0451873540878296 + 100.0 * 8.781801223754883
Epoch 1320, val loss: 1.045048713684082
Epoch 1330, training loss: 879.9634399414062 = 1.0446979999542236 + 100.0 * 8.78918743133545
Epoch 1330, val loss: 1.0445786714553833
Epoch 1340, training loss: 880.3381958007812 = 1.044189214706421 + 100.0 * 8.792940139770508
Epoch 1340, val loss: 1.0440939664840698
Epoch 1350, training loss: 880.5643920898438 = 1.0436714887619019 + 100.0 * 8.795207023620605
Epoch 1350, val loss: 1.0435869693756104
Epoch 1360, training loss: 880.7216796875 = 1.0431487560272217 + 100.0 * 8.796785354614258
Epoch 1360, val loss: 1.0430837869644165
Epoch 1370, training loss: 880.923095703125 = 1.0426267385482788 + 100.0 * 8.798805236816406
Epoch 1370, val loss: 1.0425910949707031
Epoch 1380, training loss: 880.7669677734375 = 1.042098879814148 + 100.0 * 8.797248840332031
Epoch 1380, val loss: 1.0420676469802856
Epoch 1390, training loss: 881.109375 = 1.0415757894515991 + 100.0 * 8.800678253173828
Epoch 1390, val loss: 1.0415889024734497
Epoch 1400, training loss: 881.7146606445312 = 1.0410654544830322 + 100.0 * 8.80673599243164
Epoch 1400, val loss: 1.0410858392715454
Epoch 1410, training loss: 881.4298095703125 = 1.0405210256576538 + 100.0 * 8.803893089294434
Epoch 1410, val loss: 1.0405718088150024
Epoch 1420, training loss: 881.9563598632812 = 1.039995789527893 + 100.0 * 8.809164047241211
Epoch 1420, val loss: 1.0400807857513428
Epoch 1430, training loss: 882.0579223632812 = 1.0394620895385742 + 100.0 * 8.810184478759766
Epoch 1430, val loss: 1.0395684242248535
Epoch 1440, training loss: 882.4205322265625 = 1.038925290107727 + 100.0 * 8.81381607055664
Epoch 1440, val loss: 1.0390616655349731
Epoch 1450, training loss: 882.28271484375 = 1.038439154624939 + 100.0 * 8.812442779541016
Epoch 1450, val loss: 1.0385781526565552
Epoch 1460, training loss: 882.084228515625 = 1.037826657295227 + 100.0 * 8.810463905334473
Epoch 1460, val loss: 1.0380091667175293
Epoch 1470, training loss: 882.9148559570312 = 1.0373296737670898 + 100.0 * 8.818775177001953
Epoch 1470, val loss: 1.0375161170959473
Epoch 1480, training loss: 883.1236572265625 = 1.036777138710022 + 100.0 * 8.820868492126465
Epoch 1480, val loss: 1.036995768547058
Epoch 1490, training loss: 884.4557495117188 = 1.0362505912780762 + 100.0 * 8.834195137023926
Epoch 1490, val loss: 1.036409616470337
Epoch 1500, training loss: 880.8759765625 = 1.035522699356079 + 100.0 * 8.798404693603516
Epoch 1500, val loss: 1.0358201265335083
Epoch 1510, training loss: 880.0528564453125 = 1.0349220037460327 + 100.0 * 8.790179252624512
Epoch 1510, val loss: 1.0351988077163696
Epoch 1520, training loss: 881.5639038085938 = 1.0344123840332031 + 100.0 * 8.80529499053955
Epoch 1520, val loss: 1.034749984741211
Epoch 1530, training loss: 880.678955078125 = 1.0338183641433716 + 100.0 * 8.796451568603516
Epoch 1530, val loss: 1.0341792106628418
Epoch 1540, training loss: 881.190185546875 = 1.033387303352356 + 100.0 * 8.801568031311035
Epoch 1540, val loss: 1.0337636470794678
Epoch 1550, training loss: 876.3240966796875 = 1.0323072671890259 + 100.0 * 8.752918243408203
Epoch 1550, val loss: 1.0327305793762207
Epoch 1560, training loss: 884.1990356445312 = 1.0322136878967285 + 100.0 * 8.83166790008545
Epoch 1560, val loss: 1.0326786041259766
Epoch 1570, training loss: 879.2515258789062 = 1.0315357446670532 + 100.0 * 8.78219985961914
Epoch 1570, val loss: 1.0320405960083008
Epoch 1580, training loss: 882.1581420898438 = 1.0311905145645142 + 100.0 * 8.811269760131836
Epoch 1580, val loss: 1.03167724609375
Epoch 1590, training loss: 882.7760620117188 = 1.030706763267517 + 100.0 * 8.817453384399414
Epoch 1590, val loss: 1.0312206745147705
Epoch 1600, training loss: 883.76708984375 = 1.0302339792251587 + 100.0 * 8.82736873626709
Epoch 1600, val loss: 1.0307751893997192
Epoch 1610, training loss: 884.855712890625 = 1.0297428369522095 + 100.0 * 8.83825969696045
Epoch 1610, val loss: 1.030301809310913
Epoch 1620, training loss: 885.3690185546875 = 1.0292085409164429 + 100.0 * 8.843398094177246
Epoch 1620, val loss: 1.0297877788543701
Epoch 1630, training loss: 885.9882202148438 = 1.0286716222763062 + 100.0 * 8.849595069885254
Epoch 1630, val loss: 1.0292824506759644
Epoch 1640, training loss: 886.539794921875 = 1.0281307697296143 + 100.0 * 8.855116844177246
Epoch 1640, val loss: 1.0287755727767944
Epoch 1650, training loss: 886.7161865234375 = 1.0275757312774658 + 100.0 * 8.85688591003418
Epoch 1650, val loss: 1.0282472372055054
Epoch 1660, training loss: 887.3164672851562 = 1.027036190032959 + 100.0 * 8.862894058227539
Epoch 1660, val loss: 1.027734398841858
Epoch 1670, training loss: 886.5122680664062 = 1.026450276374817 + 100.0 * 8.8548583984375
Epoch 1670, val loss: 1.0271620750427246
Epoch 1680, training loss: 885.425048828125 = 1.0257965326309204 + 100.0 * 8.843992233276367
Epoch 1680, val loss: 1.0265798568725586
Epoch 1690, training loss: 886.65185546875 = 1.0253214836120605 + 100.0 * 8.8562650680542
Epoch 1690, val loss: 1.026116132736206
Epoch 1700, training loss: 888.0302734375 = 1.0247865915298462 + 100.0 * 8.870055198669434
Epoch 1700, val loss: 1.02560293674469
Epoch 1710, training loss: 888.156005859375 = 1.0242162942886353 + 100.0 * 8.871317863464355
Epoch 1710, val loss: 1.0250663757324219
Epoch 1720, training loss: 888.4261474609375 = 1.0236505270004272 + 100.0 * 8.874025344848633
Epoch 1720, val loss: 1.0245190858840942
Epoch 1730, training loss: 889.0042724609375 = 1.0230793952941895 + 100.0 * 8.879812240600586
Epoch 1730, val loss: 1.0239816904067993
Epoch 1740, training loss: 889.6318359375 = 1.0225257873535156 + 100.0 * 8.886093139648438
Epoch 1740, val loss: 1.0234594345092773
Epoch 1750, training loss: 889.7620239257812 = 1.0219544172286987 + 100.0 * 8.88740062713623
Epoch 1750, val loss: 1.022918939590454
Epoch 1760, training loss: 889.9817504882812 = 1.0213773250579834 + 100.0 * 8.889603614807129
Epoch 1760, val loss: 1.0223766565322876
Epoch 1770, training loss: 890.02685546875 = 1.0208015441894531 + 100.0 * 8.890060424804688
Epoch 1770, val loss: 1.021817684173584
Epoch 1780, training loss: 890.3143310546875 = 1.0202351808547974 + 100.0 * 8.892940521240234
Epoch 1780, val loss: 1.021291732788086
Epoch 1790, training loss: 890.591796875 = 1.0196598768234253 + 100.0 * 8.895721435546875
Epoch 1790, val loss: 1.0207386016845703
Epoch 1800, training loss: 890.1351318359375 = 1.0190600156784058 + 100.0 * 8.89116096496582
Epoch 1800, val loss: 1.0201733112335205
Epoch 1810, training loss: 890.6582641601562 = 1.018480658531189 + 100.0 * 8.896397590637207
Epoch 1810, val loss: 1.019620656967163
Epoch 1820, training loss: 890.9297485351562 = 1.0178990364074707 + 100.0 * 8.899118423461914
Epoch 1820, val loss: 1.019076943397522
Epoch 1830, training loss: 891.305419921875 = 1.01732337474823 + 100.0 * 8.902880668640137
Epoch 1830, val loss: 1.0185377597808838
Epoch 1840, training loss: 890.6924438476562 = 1.0166820287704468 + 100.0 * 8.896758079528809
Epoch 1840, val loss: 1.0179235935211182
Epoch 1850, training loss: 890.7769775390625 = 1.0161265134811401 + 100.0 * 8.897608757019043
Epoch 1850, val loss: 1.0174098014831543
Epoch 1860, training loss: 891.4774780273438 = 1.0155689716339111 + 100.0 * 8.904619216918945
Epoch 1860, val loss: 1.0168797969818115
Epoch 1870, training loss: 892.06982421875 = 1.0149942636489868 + 100.0 * 8.910548210144043
Epoch 1870, val loss: 1.0163328647613525
Epoch 1880, training loss: 892.2716674804688 = 1.0144098997116089 + 100.0 * 8.912572860717773
Epoch 1880, val loss: 1.0157867670059204
Epoch 1890, training loss: 892.2771606445312 = 1.0138356685638428 + 100.0 * 8.912632942199707
Epoch 1890, val loss: 1.0152267217636108
Epoch 1900, training loss: 892.3261108398438 = 1.0132453441619873 + 100.0 * 8.913128852844238
Epoch 1900, val loss: 1.014665961265564
Epoch 1910, training loss: 892.5554809570312 = 1.0126628875732422 + 100.0 * 8.915428161621094
Epoch 1910, val loss: 1.0141233205795288
Epoch 1920, training loss: 893.0109252929688 = 1.012081265449524 + 100.0 * 8.919988632202148
Epoch 1920, val loss: 1.013574242591858
Epoch 1930, training loss: 889.2427978515625 = 1.0113627910614014 + 100.0 * 8.882314682006836
Epoch 1930, val loss: 1.0129011869430542
Epoch 1940, training loss: 890.3792724609375 = 1.0109386444091797 + 100.0 * 8.893683433532715
Epoch 1940, val loss: 1.0124726295471191
Epoch 1950, training loss: 890.3414306640625 = 1.010345697402954 + 100.0 * 8.893310546875
Epoch 1950, val loss: 1.0119719505310059
Epoch 1960, training loss: 889.8553466796875 = 1.0097155570983887 + 100.0 * 8.888456344604492
Epoch 1960, val loss: 1.0113784074783325
Epoch 1970, training loss: 891.0447387695312 = 1.0091989040374756 + 100.0 * 8.900355339050293
Epoch 1970, val loss: 1.0108728408813477
Epoch 1980, training loss: 892.310302734375 = 1.0086560249328613 + 100.0 * 8.913016319274902
Epoch 1980, val loss: 1.0103703737258911
Epoch 1990, training loss: 892.8240966796875 = 1.0080654621124268 + 100.0 * 8.918160438537598
Epoch 1990, val loss: 1.0098103284835815
Epoch 2000, training loss: 893.3404541015625 = 1.0074976682662964 + 100.0 * 8.92332935333252
Epoch 2000, val loss: 1.0092757940292358
Epoch 2010, training loss: 893.459228515625 = 1.0068981647491455 + 100.0 * 8.92452335357666
Epoch 2010, val loss: 1.00871741771698
Epoch 2020, training loss: 893.75927734375 = 1.0063177347183228 + 100.0 * 8.927529335021973
Epoch 2020, val loss: 1.0081740617752075
Epoch 2030, training loss: 894.06591796875 = 1.0057318210601807 + 100.0 * 8.930602073669434
Epoch 2030, val loss: 1.0076195001602173
Epoch 2040, training loss: 894.2805786132812 = 1.0051411390304565 + 100.0 * 8.932754516601562
Epoch 2040, val loss: 1.0070635080337524
Epoch 2050, training loss: 894.286865234375 = 1.0045323371887207 + 100.0 * 8.932823181152344
Epoch 2050, val loss: 1.0064947605133057
Epoch 2060, training loss: 894.53662109375 = 1.003944993019104 + 100.0 * 8.93532657623291
Epoch 2060, val loss: 1.0059362649917603
Epoch 2070, training loss: 894.5173950195312 = 1.0033355951309204 + 100.0 * 8.935140609741211
Epoch 2070, val loss: 1.005367636680603
Epoch 2080, training loss: 894.8441772460938 = 1.0027480125427246 + 100.0 * 8.938414573669434
Epoch 2080, val loss: 1.0048177242279053
Epoch 2090, training loss: 894.9049072265625 = 1.0021449327468872 + 100.0 * 8.939027786254883
Epoch 2090, val loss: 1.0042446851730347
Epoch 2100, training loss: 894.8551635742188 = 1.001524567604065 + 100.0 * 8.938536643981934
Epoch 2100, val loss: 1.0036689043045044
Epoch 2110, training loss: 895.2593994140625 = 1.0009419918060303 + 100.0 * 8.942584991455078
Epoch 2110, val loss: 1.0031218528747559
Epoch 2120, training loss: 895.4188232421875 = 1.000335693359375 + 100.0 * 8.944185256958008
Epoch 2120, val loss: 1.002549409866333
Epoch 2130, training loss: 895.0772705078125 = 0.9997052550315857 + 100.0 * 8.940775871276855
Epoch 2130, val loss: 1.0019679069519043
Epoch 2140, training loss: 895.2918090820312 = 0.9991110563278198 + 100.0 * 8.942927360534668
Epoch 2140, val loss: 1.0014077425003052
Epoch 2150, training loss: 895.7169189453125 = 0.9985182285308838 + 100.0 * 8.947183609008789
Epoch 2150, val loss: 1.0008283853530884
Epoch 2160, training loss: 896.1044311523438 = 0.9979271292686462 + 100.0 * 8.951065063476562
Epoch 2160, val loss: 1.0002681016921997
Epoch 2170, training loss: 896.1311645507812 = 0.9973139762878418 + 100.0 * 8.951338768005371
Epoch 2170, val loss: 0.9996858239173889
Epoch 2180, training loss: 896.2474365234375 = 0.9966962337493896 + 100.0 * 8.952507019042969
Epoch 2180, val loss: 0.999110221862793
Epoch 2190, training loss: 896.005615234375 = 0.9960929155349731 + 100.0 * 8.950095176696777
Epoch 2190, val loss: 0.9985384345054626
Epoch 2200, training loss: 895.8286743164062 = 0.9954781532287598 + 100.0 * 8.948331832885742
Epoch 2200, val loss: 0.9979625940322876
Epoch 2210, training loss: 896.391845703125 = 0.9948753714561462 + 100.0 * 8.953969955444336
Epoch 2210, val loss: 0.997397780418396
Epoch 2220, training loss: 896.6743774414062 = 0.9942772388458252 + 100.0 * 8.956801414489746
Epoch 2220, val loss: 0.9968224167823792
Epoch 2230, training loss: 896.5553588867188 = 0.9936134815216064 + 100.0 * 8.955617904663086
Epoch 2230, val loss: 0.9961724877357483
Epoch 2240, training loss: 895.0704345703125 = 0.9929410815238953 + 100.0 * 8.940774917602539
Epoch 2240, val loss: 0.9955601096153259
Epoch 2250, training loss: 893.6975708007812 = 0.9923244714736938 + 100.0 * 8.92705249786377
Epoch 2250, val loss: 0.9949583411216736
Epoch 2260, training loss: 894.5465698242188 = 0.9917715787887573 + 100.0 * 8.935547828674316
Epoch 2260, val loss: 0.9944302439689636
Epoch 2270, training loss: 894.8911743164062 = 0.9911751747131348 + 100.0 * 8.939000129699707
Epoch 2270, val loss: 0.9938986301422119
Epoch 2280, training loss: 891.8358154296875 = 0.9905199408531189 + 100.0 * 8.908452987670898
Epoch 2280, val loss: 0.9932624697685242
Epoch 2290, training loss: 894.739990234375 = 0.9899851083755493 + 100.0 * 8.9375
Epoch 2290, val loss: 0.9927508234977722
Epoch 2300, training loss: 894.4319458007812 = 0.9894275665283203 + 100.0 * 8.934425354003906
Epoch 2300, val loss: 0.9922510981559753
Epoch 2310, training loss: 895.4644775390625 = 0.9888674020767212 + 100.0 * 8.944755554199219
Epoch 2310, val loss: 0.991719663143158
Epoch 2320, training loss: 895.80078125 = 0.9882900714874268 + 100.0 * 8.948124885559082
Epoch 2320, val loss: 0.991177499294281
Epoch 2330, training loss: 896.4266967773438 = 0.9877105951309204 + 100.0 * 8.954389572143555
Epoch 2330, val loss: 0.990629255771637
Epoch 2340, training loss: 897.0940551757812 = 0.9871299266815186 + 100.0 * 8.961069107055664
Epoch 2340, val loss: 0.9900792837142944
Epoch 2350, training loss: 897.3663940429688 = 0.9865370988845825 + 100.0 * 8.963798522949219
Epoch 2350, val loss: 0.9895251393318176
Epoch 2360, training loss: 897.464111328125 = 0.9859365820884705 + 100.0 * 8.964781761169434
Epoch 2360, val loss: 0.9889614582061768
Epoch 2370, training loss: 897.5354614257812 = 0.9853360056877136 + 100.0 * 8.965500831604004
Epoch 2370, val loss: 0.9883903861045837
Epoch 2380, training loss: 897.3931274414062 = 0.9847168922424316 + 100.0 * 8.964083671569824
Epoch 2380, val loss: 0.9878200888633728
Epoch 2390, training loss: 897.7174072265625 = 0.9841234087944031 + 100.0 * 8.96733283996582
Epoch 2390, val loss: 0.9872507452964783
Epoch 2400, training loss: 897.881103515625 = 0.9835215210914612 + 100.0 * 8.968976020812988
Epoch 2400, val loss: 0.9866908192634583
Epoch 2410, training loss: 898.04931640625 = 0.9829207062721252 + 100.0 * 8.970664024353027
Epoch 2410, val loss: 0.9861258268356323
Epoch 2420, training loss: 897.8233642578125 = 0.9822891354560852 + 100.0 * 8.96841049194336
Epoch 2420, val loss: 0.9855465888977051
Epoch 2430, training loss: 897.8848266601562 = 0.9816900491714478 + 100.0 * 8.96903133392334
Epoch 2430, val loss: 0.9849761128425598
Epoch 2440, training loss: 898.2644653320312 = 0.9810866713523865 + 100.0 * 8.972833633422852
Epoch 2440, val loss: 0.9844136834144592
Epoch 2450, training loss: 898.328369140625 = 0.9804736971855164 + 100.0 * 8.973479270935059
Epoch 2450, val loss: 0.983837366104126
Epoch 2460, training loss: 898.18359375 = 0.9798632264137268 + 100.0 * 8.972037315368652
Epoch 2460, val loss: 0.9832707643508911
Epoch 2470, training loss: 898.554443359375 = 0.9792628884315491 + 100.0 * 8.975751876831055
Epoch 2470, val loss: 0.9826940298080444
Epoch 2480, training loss: 898.4033203125 = 0.9786490797996521 + 100.0 * 8.974246978759766
Epoch 2480, val loss: 0.9821316599845886
Epoch 2490, training loss: 897.564697265625 = 0.9779928922653198 + 100.0 * 8.965867042541504
Epoch 2490, val loss: 0.9815175533294678
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.5334782608695652
0.8645946533362313
=== training gcn model ===
Epoch 0, training loss: 1016.6201782226562 = 1.1090543270111084 + 100.0 * 10.155111312866211
Epoch 0, val loss: 1.1084622144699097
Epoch 10, training loss: 974.810302734375 = 1.108534336090088 + 100.0 * 9.737017631530762
Epoch 10, val loss: 1.107942819595337
Epoch 20, training loss: 954.2133178710938 = 1.1081407070159912 + 100.0 * 9.531051635742188
Epoch 20, val loss: 1.1075174808502197
Epoch 30, training loss: 939.7112426757812 = 1.107639193534851 + 100.0 * 9.386035919189453
Epoch 30, val loss: 1.1069931983947754
Epoch 40, training loss: 928.2869873046875 = 1.1071619987487793 + 100.0 * 9.271798133850098
Epoch 40, val loss: 1.1065019369125366
Epoch 50, training loss: 918.828125 = 1.1066863536834717 + 100.0 * 9.177214622497559
Epoch 50, val loss: 1.106013298034668
Epoch 60, training loss: 910.9574584960938 = 1.1062219142913818 + 100.0 * 9.098512649536133
Epoch 60, val loss: 1.1055266857147217
Epoch 70, training loss: 904.3021240234375 = 1.105762243270874 + 100.0 * 9.031963348388672
Epoch 70, val loss: 1.1050488948822021
Epoch 80, training loss: 898.6065673828125 = 1.1052991151809692 + 100.0 * 8.97501277923584
Epoch 80, val loss: 1.1045732498168945
Epoch 90, training loss: 893.79541015625 = 1.1048469543457031 + 100.0 * 8.926905632019043
Epoch 90, val loss: 1.1041022539138794
Epoch 100, training loss: 889.7388916015625 = 1.1043859720230103 + 100.0 * 8.886344909667969
Epoch 100, val loss: 1.1036258935928345
Epoch 110, training loss: 886.2982177734375 = 1.103926420211792 + 100.0 * 8.851943016052246
Epoch 110, val loss: 1.103150725364685
Epoch 120, training loss: 883.4139404296875 = 1.1034637689590454 + 100.0 * 8.823104858398438
Epoch 120, val loss: 1.1026736497879028
Epoch 130, training loss: 880.965087890625 = 1.1029881238937378 + 100.0 * 8.79862117767334
Epoch 130, val loss: 1.1021804809570312
Epoch 140, training loss: 878.9468383789062 = 1.1024982929229736 + 100.0 * 8.778443336486816
Epoch 140, val loss: 1.1016744375228882
Epoch 150, training loss: 877.0051879882812 = 1.102012038230896 + 100.0 * 8.759032249450684
Epoch 150, val loss: 1.101171851158142
Epoch 160, training loss: 875.5026245117188 = 1.1015043258666992 + 100.0 * 8.744010925292969
Epoch 160, val loss: 1.1006495952606201
Epoch 170, training loss: 874.196533203125 = 1.1009914875030518 + 100.0 * 8.730955123901367
Epoch 170, val loss: 1.1001437902450562
Epoch 180, training loss: 873.01416015625 = 1.1004674434661865 + 100.0 * 8.719137191772461
Epoch 180, val loss: 1.0995935201644897
Epoch 190, training loss: 872.2026977539062 = 1.0999470949172974 + 100.0 * 8.711027145385742
Epoch 190, val loss: 1.0990582704544067
Epoch 200, training loss: 871.27001953125 = 1.0994031429290771 + 100.0 * 8.701705932617188
Epoch 200, val loss: 1.0985125303268433
Epoch 210, training loss: 870.5158081054688 = 1.0988500118255615 + 100.0 * 8.694169044494629
Epoch 210, val loss: 1.0979571342468262
Epoch 220, training loss: 869.8738403320312 = 1.0982881784439087 + 100.0 * 8.687755584716797
Epoch 220, val loss: 1.0973844528198242
Epoch 230, training loss: 869.408935546875 = 1.097705602645874 + 100.0 * 8.683112144470215
Epoch 230, val loss: 1.0967899560928345
Epoch 240, training loss: 868.9467163085938 = 1.0971109867095947 + 100.0 * 8.678496360778809
Epoch 240, val loss: 1.0961982011795044
Epoch 250, training loss: 868.5416870117188 = 1.096508502960205 + 100.0 * 8.67445182800293
Epoch 250, val loss: 1.0955994129180908
Epoch 260, training loss: 868.2169799804688 = 1.095926284790039 + 100.0 * 8.671210289001465
Epoch 260, val loss: 1.095009446144104
Epoch 270, training loss: 867.803955078125 = 1.095300555229187 + 100.0 * 8.667086601257324
Epoch 270, val loss: 1.094376802444458
Epoch 280, training loss: 867.5886840820312 = 1.09467613697052 + 100.0 * 8.664939880371094
Epoch 280, val loss: 1.0937520265579224
Epoch 290, training loss: 867.1937866210938 = 1.094075322151184 + 100.0 * 8.66099739074707
Epoch 290, val loss: 1.0931450128555298
Epoch 300, training loss: 867.2171020507812 = 1.0934706926345825 + 100.0 * 8.661236763000488
Epoch 300, val loss: 1.0925334692001343
Epoch 310, training loss: 867.2276611328125 = 1.0928505659103394 + 100.0 * 8.661348342895508
Epoch 310, val loss: 1.0919173955917358
Epoch 320, training loss: 866.9035034179688 = 1.0921841859817505 + 100.0 * 8.658113479614258
Epoch 320, val loss: 1.0912898778915405
Epoch 330, training loss: 866.615478515625 = 1.0915607213974 + 100.0 * 8.65523910522461
Epoch 330, val loss: 1.0906130075454712
Epoch 340, training loss: 867.1842651367188 = 1.0909558534622192 + 100.0 * 8.660933494567871
Epoch 340, val loss: 1.0900150537490845
Epoch 350, training loss: 866.7919921875 = 1.0903152227401733 + 100.0 * 8.65701675415039
Epoch 350, val loss: 1.0893903970718384
Epoch 360, training loss: 866.881591796875 = 1.0897196531295776 + 100.0 * 8.657918930053711
Epoch 360, val loss: 1.0887888669967651
Epoch 370, training loss: 867.21142578125 = 1.0891510248184204 + 100.0 * 8.661222457885742
Epoch 370, val loss: 1.0882291793823242
Epoch 380, training loss: 867.5969848632812 = 1.0886058807373047 + 100.0 * 8.665083885192871
Epoch 380, val loss: 1.0876879692077637
Epoch 390, training loss: 867.519775390625 = 1.0880626440048218 + 100.0 * 8.66431713104248
Epoch 390, val loss: 1.087144374847412
Epoch 400, training loss: 867.6223754882812 = 1.087587594985962 + 100.0 * 8.665348052978516
Epoch 400, val loss: 1.086660623550415
Epoch 410, training loss: 867.7630004882812 = 1.0871504545211792 + 100.0 * 8.66675853729248
Epoch 410, val loss: 1.0862230062484741
Epoch 420, training loss: 867.8905639648438 = 1.0867384672164917 + 100.0 * 8.668038368225098
Epoch 420, val loss: 1.085819959640503
Epoch 430, training loss: 867.78564453125 = 1.0863287448883057 + 100.0 * 8.666993141174316
Epoch 430, val loss: 1.0854216814041138
Epoch 440, training loss: 867.9265747070312 = 1.0859652757644653 + 100.0 * 8.66840648651123
Epoch 440, val loss: 1.085062861442566
Epoch 450, training loss: 868.1612548828125 = 1.0856029987335205 + 100.0 * 8.670756340026855
Epoch 450, val loss: 1.0847069025039673
Epoch 460, training loss: 868.474365234375 = 1.0852460861206055 + 100.0 * 8.673891067504883
Epoch 460, val loss: 1.0843520164489746
Epoch 470, training loss: 868.3836669921875 = 1.0848925113677979 + 100.0 * 8.672987937927246
Epoch 470, val loss: 1.0840041637420654
Epoch 480, training loss: 868.5343627929688 = 1.084514856338501 + 100.0 * 8.674498558044434
Epoch 480, val loss: 1.0836454629898071
Epoch 490, training loss: 868.4832153320312 = 1.0841798782348633 + 100.0 * 8.673990249633789
Epoch 490, val loss: 1.0833193063735962
Epoch 500, training loss: 868.3751831054688 = 1.0838173627853394 + 100.0 * 8.672913551330566
Epoch 500, val loss: 1.082964539527893
Epoch 510, training loss: 868.782470703125 = 1.0834596157073975 + 100.0 * 8.676990509033203
Epoch 510, val loss: 1.082633137702942
Epoch 520, training loss: 869.156982421875 = 1.0831139087677002 + 100.0 * 8.68073844909668
Epoch 520, val loss: 1.0822978019714355
Epoch 530, training loss: 869.5042114257812 = 1.0827683210372925 + 100.0 * 8.68421459197998
Epoch 530, val loss: 1.0819661617279053
Epoch 540, training loss: 869.59619140625 = 1.082419753074646 + 100.0 * 8.685137748718262
Epoch 540, val loss: 1.0816303491592407
Epoch 550, training loss: 869.5145263671875 = 1.082039475440979 + 100.0 * 8.684325218200684
Epoch 550, val loss: 1.081264853477478
Epoch 560, training loss: 869.400146484375 = 1.0816450119018555 + 100.0 * 8.683184623718262
Epoch 560, val loss: 1.080886960029602
Epoch 570, training loss: 870.03369140625 = 1.0813251733779907 + 100.0 * 8.689523696899414
Epoch 570, val loss: 1.080556035041809
Epoch 580, training loss: 870.405029296875 = 1.0809471607208252 + 100.0 * 8.693241119384766
Epoch 580, val loss: 1.0802053213119507
Epoch 590, training loss: 870.3108520507812 = 1.0805633068084717 + 100.0 * 8.692302703857422
Epoch 590, val loss: 1.0798500776290894
Epoch 600, training loss: 871.4893798828125 = 1.080169916152954 + 100.0 * 8.704092025756836
Epoch 600, val loss: 1.0794788599014282
Epoch 610, training loss: 869.4041748046875 = 1.0796829462051392 + 100.0 * 8.683244705200195
Epoch 610, val loss: 1.078994631767273
Epoch 620, training loss: 871.899169921875 = 1.0795118808746338 + 100.0 * 8.708196640014648
Epoch 620, val loss: 1.0788084268569946
Epoch 630, training loss: 869.7412109375 = 1.0790210962295532 + 100.0 * 8.68662166595459
Epoch 630, val loss: 1.0783494710922241
Epoch 640, training loss: 870.612060546875 = 1.0786371231079102 + 100.0 * 8.695334434509277
Epoch 640, val loss: 1.077997088432312
Epoch 650, training loss: 871.190673828125 = 1.0782634019851685 + 100.0 * 8.70112419128418
Epoch 650, val loss: 1.0776395797729492
Epoch 660, training loss: 871.5833740234375 = 1.0778621435165405 + 100.0 * 8.705055236816406
Epoch 660, val loss: 1.0772632360458374
Epoch 670, training loss: 871.6873168945312 = 1.0774394273757935 + 100.0 * 8.706098556518555
Epoch 670, val loss: 1.076846718788147
Epoch 680, training loss: 872.3218383789062 = 1.077024221420288 + 100.0 * 8.712448120117188
Epoch 680, val loss: 1.0764551162719727
Epoch 690, training loss: 872.5848388671875 = 1.0765793323516846 + 100.0 * 8.715082168579102
Epoch 690, val loss: 1.0760418176651
Epoch 700, training loss: 872.4447021484375 = 1.0761624574661255 + 100.0 * 8.713685035705566
Epoch 700, val loss: 1.0756285190582275
Epoch 710, training loss: 871.3043823242188 = 1.0756995677947998 + 100.0 * 8.702286720275879
Epoch 710, val loss: 1.0752016305923462
Epoch 720, training loss: 871.8925170898438 = 1.0752748250961304 + 100.0 * 8.708172798156738
Epoch 720, val loss: 1.0747755765914917
Epoch 730, training loss: 873.186767578125 = 1.0748655796051025 + 100.0 * 8.721118927001953
Epoch 730, val loss: 1.0743789672851562
Epoch 740, training loss: 873.9271850585938 = 1.0744496583938599 + 100.0 * 8.728527069091797
Epoch 740, val loss: 1.0739960670471191
Epoch 750, training loss: 874.4241333007812 = 1.0740002393722534 + 100.0 * 8.733501434326172
Epoch 750, val loss: 1.073575735092163
Epoch 760, training loss: 874.256103515625 = 1.073570966720581 + 100.0 * 8.731825828552246
Epoch 760, val loss: 1.07315993309021
Epoch 770, training loss: 874.4972534179688 = 1.0731241703033447 + 100.0 * 8.734241485595703
Epoch 770, val loss: 1.0727242231369019
Epoch 780, training loss: 874.6914672851562 = 1.0726646184921265 + 100.0 * 8.736187934875488
Epoch 780, val loss: 1.072284460067749
Epoch 790, training loss: 875.0946655273438 = 1.0722178220748901 + 100.0 * 8.740224838256836
Epoch 790, val loss: 1.0718638896942139
Epoch 800, training loss: 875.4712524414062 = 1.0717710256576538 + 100.0 * 8.74399471282959
Epoch 800, val loss: 1.0714349746704102
Epoch 810, training loss: 875.638916015625 = 1.0712882280349731 + 100.0 * 8.745676040649414
Epoch 810, val loss: 1.0709707736968994
Epoch 820, training loss: 876.045166015625 = 1.0708049535751343 + 100.0 * 8.749743461608887
Epoch 820, val loss: 1.0705231428146362
Epoch 830, training loss: 876.0448608398438 = 1.0703409910202026 + 100.0 * 8.74974536895752
Epoch 830, val loss: 1.0700833797454834
Epoch 840, training loss: 876.5371704101562 = 1.0698798894882202 + 100.0 * 8.75467300415039
Epoch 840, val loss: 1.0696394443511963
Epoch 850, training loss: 876.2135009765625 = 1.0693976879119873 + 100.0 * 8.75144100189209
Epoch 850, val loss: 1.0691797733306885
Epoch 860, training loss: 876.7161865234375 = 1.0689164400100708 + 100.0 * 8.75647258758545
Epoch 860, val loss: 1.068719506263733
Epoch 870, training loss: 876.9502563476562 = 1.0684338808059692 + 100.0 * 8.758818626403809
Epoch 870, val loss: 1.0682681798934937
Epoch 880, training loss: 877.2327270507812 = 1.0679467916488647 + 100.0 * 8.761648178100586
Epoch 880, val loss: 1.067810297012329
Epoch 890, training loss: 877.3052978515625 = 1.0674426555633545 + 100.0 * 8.762378692626953
Epoch 890, val loss: 1.06732976436615
Epoch 900, training loss: 877.2955932617188 = 1.0669445991516113 + 100.0 * 8.762286186218262
Epoch 900, val loss: 1.0668569803237915
Epoch 910, training loss: 877.8367919921875 = 1.0664217472076416 + 100.0 * 8.767704010009766
Epoch 910, val loss: 1.0663655996322632
Epoch 920, training loss: 877.7867431640625 = 1.065935730934143 + 100.0 * 8.767208099365234
Epoch 920, val loss: 1.0659055709838867
Epoch 930, training loss: 877.7730712890625 = 1.0654443502426147 + 100.0 * 8.76707649230957
Epoch 930, val loss: 1.0654315948486328
Epoch 940, training loss: 878.2352905273438 = 1.0649323463439941 + 100.0 * 8.771703720092773
Epoch 940, val loss: 1.0649542808532715
Epoch 950, training loss: 877.2194213867188 = 1.0643411874771118 + 100.0 * 8.761550903320312
Epoch 950, val loss: 1.0644066333770752
Epoch 960, training loss: 876.9453735351562 = 1.0637953281402588 + 100.0 * 8.75881576538086
Epoch 960, val loss: 1.0638864040374756
Epoch 970, training loss: 877.9097900390625 = 1.0632976293563843 + 100.0 * 8.768465042114258
Epoch 970, val loss: 1.0634067058563232
Epoch 980, training loss: 878.6273193359375 = 1.0628312826156616 + 100.0 * 8.77564525604248
Epoch 980, val loss: 1.0629777908325195
Epoch 990, training loss: 879.0247192382812 = 1.0623220205307007 + 100.0 * 8.779623985290527
Epoch 990, val loss: 1.0625033378601074
Epoch 1000, training loss: 879.0604858398438 = 1.0617965459823608 + 100.0 * 8.779987335205078
Epoch 1000, val loss: 1.0620143413543701
Epoch 1010, training loss: 879.6480712890625 = 1.061281681060791 + 100.0 * 8.785867691040039
Epoch 1010, val loss: 1.0615220069885254
Epoch 1020, training loss: 879.81005859375 = 1.0606927871704102 + 100.0 * 8.787493705749512
Epoch 1020, val loss: 1.0608892440795898
Epoch 1030, training loss: 879.9818725585938 = 1.0590075254440308 + 100.0 * 8.789228439331055
Epoch 1030, val loss: 1.0592913627624512
Epoch 1040, training loss: 880.194091796875 = 1.057224154472351 + 100.0 * 8.79136848449707
Epoch 1040, val loss: 1.0576640367507935
Epoch 1050, training loss: 880.1339111328125 = 1.0555250644683838 + 100.0 * 8.790783882141113
Epoch 1050, val loss: 1.0561144351959229
Epoch 1060, training loss: 880.4605102539062 = 1.0539599657058716 + 100.0 * 8.794065475463867
Epoch 1060, val loss: 1.054673433303833
Epoch 1070, training loss: 880.7752685546875 = 1.052519679069519 + 100.0 * 8.79722785949707
Epoch 1070, val loss: 1.0533452033996582
Epoch 1080, training loss: 880.9755249023438 = 1.0511358976364136 + 100.0 * 8.799243927001953
Epoch 1080, val loss: 1.0520763397216797
Epoch 1090, training loss: 881.3195190429688 = 1.0498099327087402 + 100.0 * 8.80269718170166
Epoch 1090, val loss: 1.0508564710617065
Epoch 1100, training loss: 881.4781494140625 = 1.0484989881515503 + 100.0 * 8.804296493530273
Epoch 1100, val loss: 1.0496511459350586
Epoch 1110, training loss: 882.213623046875 = 1.047215461730957 + 100.0 * 8.811663627624512
Epoch 1110, val loss: 1.0484440326690674
Epoch 1120, training loss: 880.7820434570312 = 1.0458958148956299 + 100.0 * 8.797361373901367
Epoch 1120, val loss: 1.0472650527954102
Epoch 1130, training loss: 880.6869506835938 = 1.0446951389312744 + 100.0 * 8.796422958374023
Epoch 1130, val loss: 1.046121597290039
Epoch 1140, training loss: 881.3351440429688 = 1.0434945821762085 + 100.0 * 8.802916526794434
Epoch 1140, val loss: 1.0450174808502197
Epoch 1150, training loss: 881.8035278320312 = 1.0423227548599243 + 100.0 * 8.807612419128418
Epoch 1150, val loss: 1.0439239740371704
Epoch 1160, training loss: 882.4185791015625 = 1.0411429405212402 + 100.0 * 8.813774108886719
Epoch 1160, val loss: 1.0428365468978882
Epoch 1170, training loss: 882.9254150390625 = 1.0399881601333618 + 100.0 * 8.818854331970215
Epoch 1170, val loss: 1.0417511463165283
Epoch 1180, training loss: 883.0582275390625 = 1.038794994354248 + 100.0 * 8.820194244384766
Epoch 1180, val loss: 1.0406434535980225
Epoch 1190, training loss: 883.22216796875 = 1.0376174449920654 + 100.0 * 8.821846008300781
Epoch 1190, val loss: 1.039557933807373
Epoch 1200, training loss: 883.1800537109375 = 1.036438226699829 + 100.0 * 8.821435928344727
Epoch 1200, val loss: 1.038472056388855
Epoch 1210, training loss: 883.3606567382812 = 1.035275936126709 + 100.0 * 8.823253631591797
Epoch 1210, val loss: 1.0373963117599487
Epoch 1220, training loss: 883.9017333984375 = 1.0341112613677979 + 100.0 * 8.828676223754883
Epoch 1220, val loss: 1.0363118648529053
Epoch 1230, training loss: 884.0562133789062 = 1.032958745956421 + 100.0 * 8.830232620239258
Epoch 1230, val loss: 1.035241961479187
Epoch 1240, training loss: 884.3363647460938 = 1.0317918062210083 + 100.0 * 8.833045959472656
Epoch 1240, val loss: 1.034177541732788
Epoch 1250, training loss: 884.67041015625 = 1.0306087732315063 + 100.0 * 8.836398124694824
Epoch 1250, val loss: 1.0330830812454224
Epoch 1260, training loss: 884.8356323242188 = 1.0294365882873535 + 100.0 * 8.838062286376953
Epoch 1260, val loss: 1.0319846868515015
Epoch 1270, training loss: 884.0755004882812 = 1.0282074213027954 + 100.0 * 8.830472946166992
Epoch 1270, val loss: 1.0308500528335571
Epoch 1280, training loss: 884.5866088867188 = 1.027003526687622 + 100.0 * 8.835596084594727
Epoch 1280, val loss: 1.0297552347183228
Epoch 1290, training loss: 885.1911010742188 = 1.025842547416687 + 100.0 * 8.841652870178223
Epoch 1290, val loss: 1.028660535812378
Epoch 1300, training loss: 885.2462158203125 = 1.0246341228485107 + 100.0 * 8.842215538024902
Epoch 1300, val loss: 1.027563452720642
Epoch 1310, training loss: 885.495361328125 = 1.023446798324585 + 100.0 * 8.844718933105469
Epoch 1310, val loss: 1.0264521837234497
Epoch 1320, training loss: 886.046630859375 = 1.0222498178482056 + 100.0 * 8.85024356842041
Epoch 1320, val loss: 1.0253455638885498
Epoch 1330, training loss: 885.9556274414062 = 1.0209944248199463 + 100.0 * 8.849346160888672
Epoch 1330, val loss: 1.0241988897323608
Epoch 1340, training loss: 885.9862670898438 = 1.0197471380233765 + 100.0 * 8.849664688110352
Epoch 1340, val loss: 1.023051381111145
Epoch 1350, training loss: 886.3910522460938 = 1.0185173749923706 + 100.0 * 8.85372543334961
Epoch 1350, val loss: 1.021929144859314
Epoch 1360, training loss: 885.8298950195312 = 1.017170786857605 + 100.0 * 8.848127365112305
Epoch 1360, val loss: 1.0206801891326904
Epoch 1370, training loss: 885.65380859375 = 1.0158960819244385 + 100.0 * 8.846379280090332
Epoch 1370, val loss: 1.0194942951202393
Epoch 1380, training loss: 885.7769775390625 = 1.0146080255508423 + 100.0 * 8.847623825073242
Epoch 1380, val loss: 1.0183123350143433
Epoch 1390, training loss: 886.2426147460938 = 1.0133140087127686 + 100.0 * 8.852293014526367
Epoch 1390, val loss: 1.0171531438827515
Epoch 1400, training loss: 886.6932373046875 = 1.0120762586593628 + 100.0 * 8.8568115234375
Epoch 1400, val loss: 1.016013503074646
Epoch 1410, training loss: 886.7188720703125 = 1.0107481479644775 + 100.0 * 8.857081413269043
Epoch 1410, val loss: 1.0148075819015503
Epoch 1420, training loss: 887.0276489257812 = 1.0094282627105713 + 100.0 * 8.860182762145996
Epoch 1420, val loss: 1.0136046409606934
Epoch 1430, training loss: 887.4688110351562 = 1.0081051588058472 + 100.0 * 8.864606857299805
Epoch 1430, val loss: 1.0123933553695679
Epoch 1440, training loss: 887.4579467773438 = 1.0067490339279175 + 100.0 * 8.864511489868164
Epoch 1440, val loss: 1.0111565589904785
Epoch 1450, training loss: 887.7442626953125 = 1.005397915840149 + 100.0 * 8.867388725280762
Epoch 1450, val loss: 1.0099196434020996
Epoch 1460, training loss: 887.80810546875 = 1.0040309429168701 + 100.0 * 8.868041038513184
Epoch 1460, val loss: 1.0086721181869507
Epoch 1470, training loss: 887.917724609375 = 1.0026925802230835 + 100.0 * 8.869150161743164
Epoch 1470, val loss: 1.0074342489242554
Epoch 1480, training loss: 888.3197631835938 = 1.001352310180664 + 100.0 * 8.873184204101562
Epoch 1480, val loss: 1.0062185525894165
Epoch 1490, training loss: 888.3013916015625 = 0.9999622106552124 + 100.0 * 8.873014450073242
Epoch 1490, val loss: 1.0049192905426025
Epoch 1500, training loss: 888.4236450195312 = 0.9985957145690918 + 100.0 * 8.874250411987305
Epoch 1500, val loss: 1.0037040710449219
Epoch 1510, training loss: 888.8569946289062 = 0.997242271900177 + 100.0 * 8.878597259521484
Epoch 1510, val loss: 1.002442479133606
Epoch 1520, training loss: 889.0231323242188 = 0.9958491325378418 + 100.0 * 8.88027286529541
Epoch 1520, val loss: 1.0011823177337646
Epoch 1530, training loss: 887.91943359375 = 0.9943919777870178 + 100.0 * 8.869250297546387
Epoch 1530, val loss: 0.9998525977134705
Epoch 1540, training loss: 887.9086303710938 = 0.993162214756012 + 100.0 * 8.869154930114746
Epoch 1540, val loss: 0.9986839294433594
Epoch 1550, training loss: 887.2044677734375 = 0.9916241765022278 + 100.0 * 8.862128257751465
Epoch 1550, val loss: 0.9972944855690002
Epoch 1560, training loss: 887.8841552734375 = 0.9902807474136353 + 100.0 * 8.868938446044922
Epoch 1560, val loss: 0.9960742592811584
Epoch 1570, training loss: 888.4845581054688 = 0.9889508485794067 + 100.0 * 8.874956130981445
Epoch 1570, val loss: 0.9948334097862244
Epoch 1580, training loss: 889.4085083007812 = 0.9876287579536438 + 100.0 * 8.884208679199219
Epoch 1580, val loss: 0.9936237931251526
Epoch 1590, training loss: 889.9295654296875 = 0.9862534999847412 + 100.0 * 8.889432907104492
Epoch 1590, val loss: 0.9923658967018127
Epoch 1600, training loss: 890.2094116210938 = 0.9848488569259644 + 100.0 * 8.892245292663574
Epoch 1600, val loss: 0.9910635948181152
Epoch 1610, training loss: 890.2122192382812 = 0.983410120010376 + 100.0 * 8.892288208007812
Epoch 1610, val loss: 0.9897677302360535
Epoch 1620, training loss: 890.2677612304688 = 0.982013463973999 + 100.0 * 8.892857551574707
Epoch 1620, val loss: 0.9884653687477112
Epoch 1630, training loss: 890.25537109375 = 0.9805715084075928 + 100.0 * 8.89274787902832
Epoch 1630, val loss: 0.9871302247047424
Epoch 1640, training loss: 890.7286376953125 = 0.9791522026062012 + 100.0 * 8.89749526977539
Epoch 1640, val loss: 0.985831081867218
Epoch 1650, training loss: 890.9580688476562 = 0.9777151346206665 + 100.0 * 8.899803161621094
Epoch 1650, val loss: 0.9845364093780518
Epoch 1660, training loss: 891.2755126953125 = 0.9763058423995972 + 100.0 * 8.902992248535156
Epoch 1660, val loss: 0.9832088947296143
Epoch 1670, training loss: 891.2493896484375 = 0.9748547077178955 + 100.0 * 8.902745246887207
Epoch 1670, val loss: 0.9818693399429321
Epoch 1680, training loss: 891.1407470703125 = 0.9733945727348328 + 100.0 * 8.901673316955566
Epoch 1680, val loss: 0.9805181622505188
Epoch 1690, training loss: 891.1652221679688 = 0.9719343781471252 + 100.0 * 8.901932716369629
Epoch 1690, val loss: 0.9791807532310486
Epoch 1700, training loss: 891.3438110351562 = 0.9704511165618896 + 100.0 * 8.903733253479004
Epoch 1700, val loss: 0.9778191447257996
Epoch 1710, training loss: 891.83203125 = 0.9690206050872803 + 100.0 * 8.90863037109375
Epoch 1710, val loss: 0.9764885902404785
Epoch 1720, training loss: 891.9854736328125 = 0.9675467014312744 + 100.0 * 8.910179138183594
Epoch 1720, val loss: 0.9751351475715637
Epoch 1730, training loss: 892.2161865234375 = 0.9660815000534058 + 100.0 * 8.912501335144043
Epoch 1730, val loss: 0.9737855792045593
Epoch 1740, training loss: 892.4027709960938 = 0.9646130204200745 + 100.0 * 8.914381980895996
Epoch 1740, val loss: 0.9724081158638
Epoch 1750, training loss: 891.3582153320312 = 0.9630855321884155 + 100.0 * 8.903951644897461
Epoch 1750, val loss: 0.9709530472755432
Epoch 1760, training loss: 889.9761962890625 = 0.9615390300750732 + 100.0 * 8.890146255493164
Epoch 1760, val loss: 0.9695973992347717
Epoch 1770, training loss: 891.006591796875 = 0.9601116180419922 + 100.0 * 8.90046501159668
Epoch 1770, val loss: 0.9682092070579529
Epoch 1780, training loss: 891.3811645507812 = 0.9585750102996826 + 100.0 * 8.904226303100586
Epoch 1780, val loss: 0.9668087959289551
Epoch 1790, training loss: 891.7716674804688 = 0.9570773243904114 + 100.0 * 8.908145904541016
Epoch 1790, val loss: 0.9654342532157898
Epoch 1800, training loss: 892.7670288085938 = 0.9556105136871338 + 100.0 * 8.918113708496094
Epoch 1800, val loss: 0.9640747308731079
Epoch 1810, training loss: 893.233642578125 = 0.9541114568710327 + 100.0 * 8.922795295715332
Epoch 1810, val loss: 0.9626874327659607
Epoch 1820, training loss: 893.17822265625 = 0.9525893330574036 + 100.0 * 8.922256469726562
Epoch 1820, val loss: 0.9612546563148499
Epoch 1830, training loss: 893.3213500976562 = 0.951061487197876 + 100.0 * 8.92370319366455
Epoch 1830, val loss: 0.9598451256752014
Epoch 1840, training loss: 893.6318969726562 = 0.9495486617088318 + 100.0 * 8.926823616027832
Epoch 1840, val loss: 0.9584347605705261
Epoch 1850, training loss: 893.2572631835938 = 0.9479691386222839 + 100.0 * 8.92309284210205
Epoch 1850, val loss: 0.956958532333374
Epoch 1860, training loss: 893.3157958984375 = 0.946439266204834 + 100.0 * 8.923693656921387
Epoch 1860, val loss: 0.9555774927139282
Epoch 1870, training loss: 893.1744384765625 = 0.9448647499084473 + 100.0 * 8.922295570373535
Epoch 1870, val loss: 0.9541386961936951
Epoch 1880, training loss: 893.1692504882812 = 0.9432993531227112 + 100.0 * 8.922259330749512
Epoch 1880, val loss: 0.9526734352111816
Epoch 1890, training loss: 894.1318969726562 = 0.9418012499809265 + 100.0 * 8.931900978088379
Epoch 1890, val loss: 0.9512264728546143
Epoch 1900, training loss: 893.0231323242188 = 0.9402576684951782 + 100.0 * 8.920828819274902
Epoch 1900, val loss: 0.9498220086097717
Epoch 1910, training loss: 891.0195922851562 = 0.938527524471283 + 100.0 * 8.900810241699219
Epoch 1910, val loss: 0.9482915997505188
Epoch 1920, training loss: 891.107421875 = 0.9369785189628601 + 100.0 * 8.901704788208008
Epoch 1920, val loss: 0.9468335509300232
Epoch 1930, training loss: 891.8837890625 = 0.9354453682899475 + 100.0 * 8.909483909606934
Epoch 1930, val loss: 0.9454243779182434
Epoch 1940, training loss: 892.963134765625 = 0.9340048432350159 + 100.0 * 8.92029094696045
Epoch 1940, val loss: 0.9440850615501404
Epoch 1950, training loss: 893.37548828125 = 0.9324938654899597 + 100.0 * 8.924429893493652
Epoch 1950, val loss: 0.9426895380020142
Epoch 1960, training loss: 893.7084350585938 = 0.9309224486351013 + 100.0 * 8.927775382995605
Epoch 1960, val loss: 0.9412363171577454
Epoch 1970, training loss: 894.2108764648438 = 0.9293514490127563 + 100.0 * 8.932815551757812
Epoch 1970, val loss: 0.9397853016853333
Epoch 1980, training loss: 894.644775390625 = 0.9277732372283936 + 100.0 * 8.937170028686523
Epoch 1980, val loss: 0.938326895236969
Epoch 1990, training loss: 894.6009521484375 = 0.9261872172355652 + 100.0 * 8.936747550964355
Epoch 1990, val loss: 0.9368374943733215
Epoch 2000, training loss: 894.8578491210938 = 0.9245978593826294 + 100.0 * 8.939332008361816
Epoch 2000, val loss: 0.9353640675544739
Epoch 2010, training loss: 895.0648803710938 = 0.9230050444602966 + 100.0 * 8.941418647766113
Epoch 2010, val loss: 0.9339146018028259
Epoch 2020, training loss: 894.8024291992188 = 0.921403706073761 + 100.0 * 8.938810348510742
Epoch 2020, val loss: 0.9324007034301758
Epoch 2030, training loss: 894.9613647460938 = 0.9197826385498047 + 100.0 * 8.940415382385254
Epoch 2030, val loss: 0.9309062361717224
Epoch 2040, training loss: 895.389892578125 = 0.9181587100028992 + 100.0 * 8.944717407226562
Epoch 2040, val loss: 0.9294154644012451
Epoch 2050, training loss: 895.5219116210938 = 0.916538655757904 + 100.0 * 8.946053504943848
Epoch 2050, val loss: 0.9279302358627319
Epoch 2060, training loss: 895.6246948242188 = 0.9149312376976013 + 100.0 * 8.947097778320312
Epoch 2060, val loss: 0.9264028072357178
Epoch 2070, training loss: 894.8653564453125 = 0.9132506847381592 + 100.0 * 8.939520835876465
Epoch 2070, val loss: 0.9248829483985901
Epoch 2080, training loss: 895.471923828125 = 0.9116573333740234 + 100.0 * 8.945602416992188
Epoch 2080, val loss: 0.9233826398849487
Epoch 2090, training loss: 895.8880615234375 = 0.9100408554077148 + 100.0 * 8.949780464172363
Epoch 2090, val loss: 0.9218814969062805
Epoch 2100, training loss: 896.0965576171875 = 0.9084230661392212 + 100.0 * 8.951881408691406
Epoch 2100, val loss: 0.9203648567199707
Epoch 2110, training loss: 895.9852294921875 = 0.9067602157592773 + 100.0 * 8.950784683227539
Epoch 2110, val loss: 0.9188381433486938
Epoch 2120, training loss: 896.1357421875 = 0.9051034450531006 + 100.0 * 8.952306747436523
Epoch 2120, val loss: 0.9172930121421814
Epoch 2130, training loss: 896.514892578125 = 0.903480589389801 + 100.0 * 8.956113815307617
Epoch 2130, val loss: 0.915773332118988
Epoch 2140, training loss: 896.7520141601562 = 0.9018363356590271 + 100.0 * 8.958501815795898
Epoch 2140, val loss: 0.914271354675293
Epoch 2150, training loss: 896.9105834960938 = 0.9001972079277039 + 100.0 * 8.960103988647461
Epoch 2150, val loss: 0.912715494632721
Epoch 2160, training loss: 896.401123046875 = 0.8984618186950684 + 100.0 * 8.955026626586914
Epoch 2160, val loss: 0.9111318588256836
Epoch 2170, training loss: 896.4736328125 = 0.896845281124115 + 100.0 * 8.955767631530762
Epoch 2170, val loss: 0.9096023440361023
Epoch 2180, training loss: 896.8973388671875 = 0.8951680660247803 + 100.0 * 8.96002197265625
Epoch 2180, val loss: 0.9080753922462463
Epoch 2190, training loss: 897.5504760742188 = 0.8935409188270569 + 100.0 * 8.966568946838379
Epoch 2190, val loss: 0.9065579175949097
Epoch 2200, training loss: 897.3025512695312 = 0.8918850421905518 + 100.0 * 8.964106559753418
Epoch 2200, val loss: 0.9050312638282776
Epoch 2210, training loss: 897.5358276367188 = 0.8902117013931274 + 100.0 * 8.966456413269043
Epoch 2210, val loss: 0.9034798741340637
Epoch 2220, training loss: 897.498779296875 = 0.8885424733161926 + 100.0 * 8.966102600097656
Epoch 2220, val loss: 0.9019352793693542
Epoch 2230, training loss: 897.2453002929688 = 0.8868745565414429 + 100.0 * 8.963583946228027
Epoch 2230, val loss: 0.9003673195838928
Epoch 2240, training loss: 897.542236328125 = 0.8852469325065613 + 100.0 * 8.966569900512695
Epoch 2240, val loss: 0.898917555809021
Epoch 2250, training loss: 897.8116455078125 = 0.8839045763015747 + 100.0 * 8.969277381896973
Epoch 2250, val loss: 0.8976237177848816
Epoch 2260, training loss: 897.8935546875 = 0.8822646737098694 + 100.0 * 8.970112800598145
Epoch 2260, val loss: 0.896089494228363
Epoch 2270, training loss: 894.37744140625 = 0.8802063465118408 + 100.0 * 8.934972763061523
Epoch 2270, val loss: 0.894221842288971
Epoch 2280, training loss: 895.2192993164062 = 0.8785787224769592 + 100.0 * 8.94340705871582
Epoch 2280, val loss: 0.8927631378173828
Epoch 2290, training loss: 896.56591796875 = 0.8770473003387451 + 100.0 * 8.956888198852539
Epoch 2290, val loss: 0.8913037180900574
Epoch 2300, training loss: 896.4133911132812 = 0.8754403591156006 + 100.0 * 8.955379486083984
Epoch 2300, val loss: 0.8898105621337891
Epoch 2310, training loss: 897.339599609375 = 0.8738415837287903 + 100.0 * 8.9646577835083
Epoch 2310, val loss: 0.8883455991744995
Epoch 2320, training loss: 898.1395263671875 = 0.8722253441810608 + 100.0 * 8.972672462463379
Epoch 2320, val loss: 0.8868629336357117
Epoch 2330, training loss: 898.7105102539062 = 0.8706005215644836 + 100.0 * 8.978399276733398
Epoch 2330, val loss: 0.88536137342453
Epoch 2340, training loss: 899.0179443359375 = 0.8689388632774353 + 100.0 * 8.981490135192871
Epoch 2340, val loss: 0.8838330507278442
Epoch 2350, training loss: 899.08642578125 = 0.8672477602958679 + 100.0 * 8.982192039489746
Epoch 2350, val loss: 0.8822643756866455
Epoch 2360, training loss: 899.1804809570312 = 0.8655506372451782 + 100.0 * 8.983149528503418
Epoch 2360, val loss: 0.8807122707366943
Epoch 2370, training loss: 899.3828125 = 0.8638636469841003 + 100.0 * 8.985189437866211
Epoch 2370, val loss: 0.8791477680206299
Epoch 2380, training loss: 899.6094360351562 = 0.8621898889541626 + 100.0 * 8.987472534179688
Epoch 2380, val loss: 0.8776074647903442
Epoch 2390, training loss: 899.674560546875 = 0.8605048060417175 + 100.0 * 8.988140106201172
Epoch 2390, val loss: 0.8760389685630798
Epoch 2400, training loss: 899.7142944335938 = 0.8588471412658691 + 100.0 * 8.988554954528809
Epoch 2400, val loss: 0.8745092153549194
Epoch 2410, training loss: 899.7377319335938 = 0.8571679592132568 + 100.0 * 8.988805770874023
Epoch 2410, val loss: 0.8729681372642517
Epoch 2420, training loss: 899.9918823242188 = 0.8555101752281189 + 100.0 * 8.991363525390625
Epoch 2420, val loss: 0.8714258074760437
Epoch 2430, training loss: 900.1656494140625 = 0.8538345694541931 + 100.0 * 8.993118286132812
Epoch 2430, val loss: 0.86985844373703
Epoch 2440, training loss: 899.9827270507812 = 0.8521408438682556 + 100.0 * 8.99130630493164
Epoch 2440, val loss: 0.8683308959007263
Epoch 2450, training loss: 900.4864501953125 = 0.8504725098609924 + 100.0 * 8.996359825134277
Epoch 2450, val loss: 0.8667926788330078
Epoch 2460, training loss: 900.6485595703125 = 0.8488291501998901 + 100.0 * 8.997997283935547
Epoch 2460, val loss: 0.8652374148368835
Epoch 2470, training loss: 900.6229248046875 = 0.8471123576164246 + 100.0 * 8.997757911682129
Epoch 2470, val loss: 0.8636872172355652
Epoch 2480, training loss: 900.5885620117188 = 0.8454350829124451 + 100.0 * 8.997430801391602
Epoch 2480, val loss: 0.8621401190757751
Epoch 2490, training loss: 900.6651000976562 = 0.8437545895576477 + 100.0 * 8.998213768005371
Epoch 2490, val loss: 0.8605763912200928
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7575362318840579
0.8647395493733248
=== training gcn model ===
Epoch 0, training loss: 1012.7313842773438 = 1.093021035194397 + 100.0 * 10.11638355255127
Epoch 0, val loss: 1.0937358140945435
Epoch 10, training loss: 969.1360473632812 = 1.0928875207901 + 100.0 * 9.680431365966797
Epoch 10, val loss: 1.093637228012085
Epoch 20, training loss: 949.549072265625 = 1.092781901359558 + 100.0 * 9.484562873840332
Epoch 20, val loss: 1.0935399532318115
Epoch 30, training loss: 935.2548828125 = 1.0926481485366821 + 100.0 * 9.341622352600098
Epoch 30, val loss: 1.0934276580810547
Epoch 40, training loss: 923.8438110351562 = 1.0924961566925049 + 100.0 * 9.227513313293457
Epoch 40, val loss: 1.093292236328125
Epoch 50, training loss: 914.565673828125 = 1.0923415422439575 + 100.0 * 9.134733200073242
Epoch 50, val loss: 1.0931594371795654
Epoch 60, training loss: 907.0071411132812 = 1.0921835899353027 + 100.0 * 9.059149742126465
Epoch 60, val loss: 1.0930202007293701
Epoch 70, training loss: 900.7520751953125 = 1.092035174369812 + 100.0 * 8.996600151062012
Epoch 70, val loss: 1.0928915739059448
Epoch 80, training loss: 895.7282104492188 = 1.0918809175491333 + 100.0 * 8.94636344909668
Epoch 80, val loss: 1.0927554368972778
Epoch 90, training loss: 891.4102172851562 = 1.091727375984192 + 100.0 * 8.90318489074707
Epoch 90, val loss: 1.0926213264465332
Epoch 100, training loss: 887.9094848632812 = 1.0915637016296387 + 100.0 * 8.868179321289062
Epoch 100, val loss: 1.0924761295318604
Epoch 110, training loss: 884.9116821289062 = 1.0914040803909302 + 100.0 * 8.838202476501465
Epoch 110, val loss: 1.092334508895874
Epoch 120, training loss: 882.311279296875 = 1.0912295579910278 + 100.0 * 8.812200546264648
Epoch 120, val loss: 1.0921791791915894
Epoch 130, training loss: 880.0741577148438 = 1.0910555124282837 + 100.0 * 8.789831161499023
Epoch 130, val loss: 1.09202241897583
Epoch 140, training loss: 878.94287109375 = 1.0908491611480713 + 100.0 * 8.778520584106445
Epoch 140, val loss: 1.0918370485305786
Epoch 150, training loss: 877.0743408203125 = 1.090684413909912 + 100.0 * 8.759836196899414
Epoch 150, val loss: 1.0916882753372192
Epoch 160, training loss: 875.0621948242188 = 1.0904741287231445 + 100.0 * 8.739717483520508
Epoch 160, val loss: 1.0915066003799438
Epoch 170, training loss: 874.134765625 = 1.0902771949768066 + 100.0 * 8.73044490814209
Epoch 170, val loss: 1.0913293361663818
Epoch 180, training loss: 872.730224609375 = 1.090064287185669 + 100.0 * 8.716401100158691
Epoch 180, val loss: 1.0911366939544678
Epoch 190, training loss: 871.8920288085938 = 1.0898478031158447 + 100.0 * 8.708022117614746
Epoch 190, val loss: 1.0909411907196045
Epoch 200, training loss: 871.0343627929688 = 1.0896286964416504 + 100.0 * 8.699447631835938
Epoch 200, val loss: 1.0907437801361084
Epoch 210, training loss: 870.1595458984375 = 1.0894027948379517 + 100.0 * 8.690701484680176
Epoch 210, val loss: 1.0905407667160034
Epoch 220, training loss: 869.5545043945312 = 1.0891740322113037 + 100.0 * 8.684653282165527
Epoch 220, val loss: 1.0903366804122925
Epoch 230, training loss: 868.8536987304688 = 1.0889290571212769 + 100.0 * 8.677647590637207
Epoch 230, val loss: 1.0901191234588623
Epoch 240, training loss: 868.3355712890625 = 1.0886995792388916 + 100.0 * 8.672469139099121
Epoch 240, val loss: 1.089909553527832
Epoch 250, training loss: 867.6668090820312 = 1.088444471359253 + 100.0 * 8.665783882141113
Epoch 250, val loss: 1.0896782875061035
Epoch 260, training loss: 867.0980224609375 = 1.088194727897644 + 100.0 * 8.6600980758667
Epoch 260, val loss: 1.0894590616226196
Epoch 270, training loss: 866.6505126953125 = 1.087934136390686 + 100.0 * 8.655625343322754
Epoch 270, val loss: 1.0892236232757568
Epoch 280, training loss: 866.328857421875 = 1.087679386138916 + 100.0 * 8.652411460876465
Epoch 280, val loss: 1.0889875888824463
Epoch 290, training loss: 866.1212158203125 = 1.0874239206314087 + 100.0 * 8.650338172912598
Epoch 290, val loss: 1.0887547731399536
Epoch 300, training loss: 866.5427856445312 = 1.0871729850769043 + 100.0 * 8.654556274414062
Epoch 300, val loss: 1.0885298252105713
Epoch 310, training loss: 865.5791625976562 = 1.086894154548645 + 100.0 * 8.644922256469727
Epoch 310, val loss: 1.088279128074646
Epoch 320, training loss: 865.3972778320312 = 1.0866248607635498 + 100.0 * 8.643106460571289
Epoch 320, val loss: 1.0880297422409058
Epoch 330, training loss: 865.5154418945312 = 1.0863581895828247 + 100.0 * 8.644290924072266
Epoch 330, val loss: 1.0877976417541504
Epoch 340, training loss: 865.5626220703125 = 1.08607816696167 + 100.0 * 8.644765853881836
Epoch 340, val loss: 1.08754301071167
Epoch 350, training loss: 865.5267333984375 = 1.0858057737350464 + 100.0 * 8.6444091796875
Epoch 350, val loss: 1.0872998237609863
Epoch 360, training loss: 865.3700561523438 = 1.0855258703231812 + 100.0 * 8.642845153808594
Epoch 360, val loss: 1.0870436429977417
Epoch 370, training loss: 865.2605590820312 = 1.085249423980713 + 100.0 * 8.641753196716309
Epoch 370, val loss: 1.0867873430252075
Epoch 380, training loss: 865.2514038085938 = 1.0849609375 + 100.0 * 8.641664505004883
Epoch 380, val loss: 1.086531162261963
Epoch 390, training loss: 865.2168579101562 = 1.0846855640411377 + 100.0 * 8.641322135925293
Epoch 390, val loss: 1.0862791538238525
Epoch 400, training loss: 864.9573364257812 = 1.0843884944915771 + 100.0 * 8.638729095458984
Epoch 400, val loss: 1.086006760597229
Epoch 410, training loss: 865.3325805664062 = 1.084110975265503 + 100.0 * 8.642484664916992
Epoch 410, val loss: 1.0857561826705933
Epoch 420, training loss: 865.3297729492188 = 1.0838215351104736 + 100.0 * 8.642459869384766
Epoch 420, val loss: 1.0854853391647339
Epoch 430, training loss: 865.262939453125 = 1.0835185050964355 + 100.0 * 8.641794204711914
Epoch 430, val loss: 1.085211157798767
Epoch 440, training loss: 865.1538696289062 = 1.0832107067108154 + 100.0 * 8.640707015991211
Epoch 440, val loss: 1.084935188293457
Epoch 450, training loss: 865.0120849609375 = 1.0829087495803833 + 100.0 * 8.639291763305664
Epoch 450, val loss: 1.0846563577651978
Epoch 460, training loss: 865.336181640625 = 1.082602620124817 + 100.0 * 8.642536163330078
Epoch 460, val loss: 1.084383249282837
Epoch 470, training loss: 865.2821044921875 = 1.082299828529358 + 100.0 * 8.641998291015625
Epoch 470, val loss: 1.0841008424758911
Epoch 480, training loss: 865.3157348632812 = 1.0819891691207886 + 100.0 * 8.642337799072266
Epoch 480, val loss: 1.0838112831115723
Epoch 490, training loss: 865.1962280273438 = 1.0816774368286133 + 100.0 * 8.641145706176758
Epoch 490, val loss: 1.0835256576538086
Epoch 500, training loss: 865.5429077148438 = 1.0813649892807007 + 100.0 * 8.644615173339844
Epoch 500, val loss: 1.0832406282424927
Epoch 510, training loss: 865.8359375 = 1.081066370010376 + 100.0 * 8.64754867553711
Epoch 510, val loss: 1.0829613208770752
Epoch 520, training loss: 866.0594482421875 = 1.0807448625564575 + 100.0 * 8.649786949157715
Epoch 520, val loss: 1.0826712846755981
Epoch 530, training loss: 866.2734985351562 = 1.0804312229156494 + 100.0 * 8.651930809020996
Epoch 530, val loss: 1.0823882818222046
Epoch 540, training loss: 866.3681640625 = 1.0801079273223877 + 100.0 * 8.652880668640137
Epoch 540, val loss: 1.082093358039856
Epoch 550, training loss: 866.4058837890625 = 1.0797643661499023 + 100.0 * 8.653261184692383
Epoch 550, val loss: 1.0817878246307373
Epoch 560, training loss: 866.70751953125 = 1.0794483423233032 + 100.0 * 8.656280517578125
Epoch 560, val loss: 1.0814841985702515
Epoch 570, training loss: 866.5615234375 = 1.0791120529174805 + 100.0 * 8.654824256896973
Epoch 570, val loss: 1.081185221672058
Epoch 580, training loss: 866.8163452148438 = 1.078779697418213 + 100.0 * 8.65737533569336
Epoch 580, val loss: 1.0808717012405396
Epoch 590, training loss: 866.746337890625 = 1.0784474611282349 + 100.0 * 8.656679153442383
Epoch 590, val loss: 1.0805681943893433
Epoch 600, training loss: 867.19140625 = 1.078109622001648 + 100.0 * 8.6611328125
Epoch 600, val loss: 1.080257534980774
Epoch 610, training loss: 867.408203125 = 1.0777639150619507 + 100.0 * 8.663304328918457
Epoch 610, val loss: 1.079932451248169
Epoch 620, training loss: 867.5693969726562 = 1.0774216651916504 + 100.0 * 8.66491985321045
Epoch 620, val loss: 1.0796289443969727
Epoch 630, training loss: 867.2791748046875 = 1.0770618915557861 + 100.0 * 8.662020683288574
Epoch 630, val loss: 1.0792992115020752
Epoch 640, training loss: 867.1965942382812 = 1.076712965965271 + 100.0 * 8.661198616027832
Epoch 640, val loss: 1.0789768695831299
Epoch 650, training loss: 867.5698852539062 = 1.0763715505599976 + 100.0 * 8.664935111999512
Epoch 650, val loss: 1.0786523818969727
Epoch 660, training loss: 867.4439086914062 = 1.076014757156372 + 100.0 * 8.663679122924805
Epoch 660, val loss: 1.0783262252807617
Epoch 670, training loss: 868.033935546875 = 1.075668215751648 + 100.0 * 8.66958236694336
Epoch 670, val loss: 1.0780034065246582
Epoch 680, training loss: 868.1897583007812 = 1.0752984285354614 + 100.0 * 8.671144485473633
Epoch 680, val loss: 1.0776735544204712
Epoch 690, training loss: 868.1600952148438 = 1.0749521255493164 + 100.0 * 8.670851707458496
Epoch 690, val loss: 1.0773481130599976
Epoch 700, training loss: 868.7855834960938 = 1.0745925903320312 + 100.0 * 8.677109718322754
Epoch 700, val loss: 1.077020287513733
Epoch 710, training loss: 868.8458251953125 = 1.0742278099060059 + 100.0 * 8.677716255187988
Epoch 710, val loss: 1.0766788721084595
Epoch 720, training loss: 868.854248046875 = 1.0738438367843628 + 100.0 * 8.677803993225098
Epoch 720, val loss: 1.0763306617736816
Epoch 730, training loss: 869.0327758789062 = 1.0734668970108032 + 100.0 * 8.679593086242676
Epoch 730, val loss: 1.0759838819503784
Epoch 740, training loss: 868.2963256835938 = 1.0730770826339722 + 100.0 * 8.672232627868652
Epoch 740, val loss: 1.0756125450134277
Epoch 750, training loss: 868.6607055664062 = 1.072699785232544 + 100.0 * 8.675880432128906
Epoch 750, val loss: 1.0752558708190918
Epoch 760, training loss: 869.4926147460938 = 1.072340488433838 + 100.0 * 8.684203147888184
Epoch 760, val loss: 1.0749280452728271
Epoch 770, training loss: 870.0247192382812 = 1.0719534158706665 + 100.0 * 8.68952751159668
Epoch 770, val loss: 1.0745830535888672
Epoch 780, training loss: 870.1690063476562 = 1.0715676546096802 + 100.0 * 8.690974235534668
Epoch 780, val loss: 1.0742179155349731
Epoch 790, training loss: 870.4638671875 = 1.0711787939071655 + 100.0 * 8.693926811218262
Epoch 790, val loss: 1.0738662481307983
Epoch 800, training loss: 870.4378051757812 = 1.0707883834838867 + 100.0 * 8.693670272827148
Epoch 800, val loss: 1.0734901428222656
Epoch 810, training loss: 870.550048828125 = 1.0703870058059692 + 100.0 * 8.694796562194824
Epoch 810, val loss: 1.0731292963027954
Epoch 820, training loss: 870.2467651367188 = 1.069986343383789 + 100.0 * 8.691767692565918
Epoch 820, val loss: 1.0727643966674805
Epoch 830, training loss: 870.6554565429688 = 1.0695878267288208 + 100.0 * 8.6958589553833
Epoch 830, val loss: 1.0723949670791626
Epoch 840, training loss: 871.162109375 = 1.069198489189148 + 100.0 * 8.700928688049316
Epoch 840, val loss: 1.0720285177230835
Epoch 850, training loss: 871.3004150390625 = 1.0687910318374634 + 100.0 * 8.702316284179688
Epoch 850, val loss: 1.0716506242752075
Epoch 860, training loss: 871.5833129882812 = 1.0683939456939697 + 100.0 * 8.705148696899414
Epoch 860, val loss: 1.071279764175415
Epoch 870, training loss: 871.4375 = 1.067965030670166 + 100.0 * 8.703695297241211
Epoch 870, val loss: 1.070867896080017
Epoch 880, training loss: 871.4736328125 = 1.0675415992736816 + 100.0 * 8.704060554504395
Epoch 880, val loss: 1.0704982280731201
Epoch 890, training loss: 871.73095703125 = 1.0671380758285522 + 100.0 * 8.70663833618164
Epoch 890, val loss: 1.070116639137268
Epoch 900, training loss: 872.3305053710938 = 1.066734790802002 + 100.0 * 8.712637901306152
Epoch 900, val loss: 1.0697429180145264
Epoch 910, training loss: 872.5415649414062 = 1.0663208961486816 + 100.0 * 8.714752197265625
Epoch 910, val loss: 1.0693557262420654
Epoch 920, training loss: 872.4785766601562 = 1.0659018754959106 + 100.0 * 8.714126586914062
Epoch 920, val loss: 1.0689516067504883
Epoch 930, training loss: 872.6589965820312 = 1.0654499530792236 + 100.0 * 8.715935707092285
Epoch 930, val loss: 1.068540096282959
Epoch 940, training loss: 873.2548828125 = 1.0650352239608765 + 100.0 * 8.721898078918457
Epoch 940, val loss: 1.0681620836257935
Epoch 950, training loss: 873.3272705078125 = 1.064582109451294 + 100.0 * 8.722626686096191
Epoch 950, val loss: 1.0677300691604614
Epoch 960, training loss: 873.5228881835938 = 1.0641518831253052 + 100.0 * 8.724587440490723
Epoch 960, val loss: 1.0673130750656128
Epoch 970, training loss: 873.8903198242188 = 1.0636945962905884 + 100.0 * 8.728265762329102
Epoch 970, val loss: 1.0668898820877075
Epoch 980, training loss: 873.937744140625 = 1.063265323638916 + 100.0 * 8.728744506835938
Epoch 980, val loss: 1.0664910078048706
Epoch 990, training loss: 874.1873779296875 = 1.0628294944763184 + 100.0 * 8.731245994567871
Epoch 990, val loss: 1.0660910606384277
Epoch 1000, training loss: 874.2999877929688 = 1.0623798370361328 + 100.0 * 8.732376098632812
Epoch 1000, val loss: 1.0656700134277344
Epoch 1010, training loss: 874.4970703125 = 1.0619168281555176 + 100.0 * 8.734352111816406
Epoch 1010, val loss: 1.0652319192886353
Epoch 1020, training loss: 874.5900268554688 = 1.061480164527893 + 100.0 * 8.735285758972168
Epoch 1020, val loss: 1.0648208856582642
Epoch 1030, training loss: 874.803466796875 = 1.060997724533081 + 100.0 * 8.737424850463867
Epoch 1030, val loss: 1.0643720626831055
Epoch 1040, training loss: 873.9039306640625 = 1.0604987144470215 + 100.0 * 8.728434562683105
Epoch 1040, val loss: 1.06391441822052
Epoch 1050, training loss: 874.1348876953125 = 1.0600475072860718 + 100.0 * 8.730748176574707
Epoch 1050, val loss: 1.0634828805923462
Epoch 1060, training loss: 874.6641235351562 = 1.0596187114715576 + 100.0 * 8.736044883728027
Epoch 1060, val loss: 1.0630862712860107
Epoch 1070, training loss: 875.2311401367188 = 1.0591646432876587 + 100.0 * 8.741720199584961
Epoch 1070, val loss: 1.062658429145813
Epoch 1080, training loss: 875.3475341796875 = 1.0586915016174316 + 100.0 * 8.742888450622559
Epoch 1080, val loss: 1.062213659286499
Epoch 1090, training loss: 875.5347290039062 = 1.0582351684570312 + 100.0 * 8.744765281677246
Epoch 1090, val loss: 1.06178879737854
Epoch 1100, training loss: 876.0422973632812 = 1.0577658414840698 + 100.0 * 8.749845504760742
Epoch 1100, val loss: 1.061357855796814
Epoch 1110, training loss: 875.6792602539062 = 1.05729079246521 + 100.0 * 8.746219635009766
Epoch 1110, val loss: 1.060899019241333
Epoch 1120, training loss: 876.0850219726562 = 1.0568243265151978 + 100.0 * 8.750282287597656
Epoch 1120, val loss: 1.0604523420333862
Epoch 1130, training loss: 876.4354858398438 = 1.0563668012619019 + 100.0 * 8.753790855407715
Epoch 1130, val loss: 1.0600351095199585
Epoch 1140, training loss: 876.8995971679688 = 1.0559091567993164 + 100.0 * 8.758437156677246
Epoch 1140, val loss: 1.059589147567749
Epoch 1150, training loss: 876.8900756835938 = 1.0554285049438477 + 100.0 * 8.758346557617188
Epoch 1150, val loss: 1.0591444969177246
Epoch 1160, training loss: 877.423828125 = 1.054960012435913 + 100.0 * 8.763689041137695
Epoch 1160, val loss: 1.0587093830108643
Epoch 1170, training loss: 877.525634765625 = 1.0544800758361816 + 100.0 * 8.764711380004883
Epoch 1170, val loss: 1.058237910270691
Epoch 1180, training loss: 877.3545532226562 = 1.0539716482162476 + 100.0 * 8.763006210327148
Epoch 1180, val loss: 1.057777762413025
Epoch 1190, training loss: 878.020751953125 = 1.053542137145996 + 100.0 * 8.769672393798828
Epoch 1190, val loss: 1.0573625564575195
Epoch 1200, training loss: 878.337158203125 = 1.0530339479446411 + 100.0 * 8.772841453552246
Epoch 1200, val loss: 1.056892991065979
Epoch 1210, training loss: 878.4442749023438 = 1.0525562763214111 + 100.0 * 8.773917198181152
Epoch 1210, val loss: 1.0564357042312622
Epoch 1220, training loss: 878.1920166015625 = 1.0520727634429932 + 100.0 * 8.77139949798584
Epoch 1220, val loss: 1.0559672117233276
Epoch 1230, training loss: 878.1832275390625 = 1.0515693426132202 + 100.0 * 8.771316528320312
Epoch 1230, val loss: 1.0554702281951904
Epoch 1240, training loss: 876.7466430664062 = 1.051002860069275 + 100.0 * 8.756956100463867
Epoch 1240, val loss: 1.0550224781036377
Epoch 1250, training loss: 877.0054321289062 = 1.0505441427230835 + 100.0 * 8.759549140930176
Epoch 1250, val loss: 1.054525375366211
Epoch 1260, training loss: 877.7888793945312 = 1.0501056909561157 + 100.0 * 8.767387390136719
Epoch 1260, val loss: 1.0541372299194336
Epoch 1270, training loss: 878.2013549804688 = 1.0496821403503418 + 100.0 * 8.771516799926758
Epoch 1270, val loss: 1.053741455078125
Epoch 1280, training loss: 878.5262451171875 = 1.0491960048675537 + 100.0 * 8.774770736694336
Epoch 1280, val loss: 1.0532890558242798
Epoch 1290, training loss: 878.8136596679688 = 1.0487459897994995 + 100.0 * 8.77764892578125
Epoch 1290, val loss: 1.0528466701507568
Epoch 1300, training loss: 878.7966918945312 = 1.0482529401779175 + 100.0 * 8.777483940124512
Epoch 1300, val loss: 1.05239999294281
Epoch 1310, training loss: 879.388916015625 = 1.0477900505065918 + 100.0 * 8.783411026000977
Epoch 1310, val loss: 1.0519620180130005
Epoch 1320, training loss: 879.3506469726562 = 1.0473064184188843 + 100.0 * 8.78303337097168
Epoch 1320, val loss: 1.0515116453170776
Epoch 1330, training loss: 879.9285278320312 = 1.0468300580978394 + 100.0 * 8.788817405700684
Epoch 1330, val loss: 1.0510509014129639
Epoch 1340, training loss: 880.0933227539062 = 1.046347737312317 + 100.0 * 8.790470123291016
Epoch 1340, val loss: 1.050612211227417
Epoch 1350, training loss: 880.0987548828125 = 1.0458656549453735 + 100.0 * 8.790529251098633
Epoch 1350, val loss: 1.0501439571380615
Epoch 1360, training loss: 880.5399780273438 = 1.0453914403915405 + 100.0 * 8.79494571685791
Epoch 1360, val loss: 1.0496904850006104
Epoch 1370, training loss: 880.2274780273438 = 1.0448901653289795 + 100.0 * 8.791826248168945
Epoch 1370, val loss: 1.0492377281188965
Epoch 1380, training loss: 880.7059936523438 = 1.0444165468215942 + 100.0 * 8.796615600585938
Epoch 1380, val loss: 1.048783779144287
Epoch 1390, training loss: 880.8902587890625 = 1.0439220666885376 + 100.0 * 8.798462867736816
Epoch 1390, val loss: 1.0483170747756958
Epoch 1400, training loss: 881.0659790039062 = 1.0434480905532837 + 100.0 * 8.800225257873535
Epoch 1400, val loss: 1.0478596687316895
Epoch 1410, training loss: 881.3116455078125 = 1.042966604232788 + 100.0 * 8.80268669128418
Epoch 1410, val loss: 1.0474128723144531
Epoch 1420, training loss: 881.4271240234375 = 1.042481780052185 + 100.0 * 8.80384635925293
Epoch 1420, val loss: 1.0468868017196655
Epoch 1430, training loss: 880.3284301757812 = 1.0419400930404663 + 100.0 * 8.792864799499512
Epoch 1430, val loss: 1.0464181900024414
Epoch 1440, training loss: 880.7210083007812 = 1.0414644479751587 + 100.0 * 8.796795845031738
Epoch 1440, val loss: 1.0459821224212646
Epoch 1450, training loss: 881.3695068359375 = 1.04099440574646 + 100.0 * 8.803284645080566
Epoch 1450, val loss: 1.04555082321167
Epoch 1460, training loss: 882.1812133789062 = 1.040544033050537 + 100.0 * 8.811407089233398
Epoch 1460, val loss: 1.045121669769287
Epoch 1470, training loss: 882.2766723632812 = 1.0400402545928955 + 100.0 * 8.812366485595703
Epoch 1470, val loss: 1.0446466207504272
Epoch 1480, training loss: 882.5645751953125 = 1.0395656824111938 + 100.0 * 8.815250396728516
Epoch 1480, val loss: 1.0441972017288208
Epoch 1490, training loss: 882.8143310546875 = 1.0390793085098267 + 100.0 * 8.817752838134766
Epoch 1490, val loss: 1.043744444847107
Epoch 1500, training loss: 881.908447265625 = 1.0385559797286987 + 100.0 * 8.808698654174805
Epoch 1500, val loss: 1.0432360172271729
Epoch 1510, training loss: 881.8243408203125 = 1.0380840301513672 + 100.0 * 8.807862281799316
Epoch 1510, val loss: 1.0427850484848022
Epoch 1520, training loss: 881.9739379882812 = 1.037606954574585 + 100.0 * 8.80936336517334
Epoch 1520, val loss: 1.042336344718933
Epoch 1530, training loss: 882.5 = 1.0371347665786743 + 100.0 * 8.814628601074219
Epoch 1530, val loss: 1.0419002771377563
Epoch 1540, training loss: 882.9960327148438 = 1.0366556644439697 + 100.0 * 8.81959342956543
Epoch 1540, val loss: 1.0414475202560425
Epoch 1550, training loss: 883.137451171875 = 1.0361671447753906 + 100.0 * 8.821012496948242
Epoch 1550, val loss: 1.0409972667694092
Epoch 1560, training loss: 883.43798828125 = 1.035673975944519 + 100.0 * 8.824023246765137
Epoch 1560, val loss: 1.0405349731445312
Epoch 1570, training loss: 882.4299926757812 = 1.0351321697235107 + 100.0 * 8.813948631286621
Epoch 1570, val loss: 1.03999924659729
Epoch 1580, training loss: 882.8629760742188 = 1.034654140472412 + 100.0 * 8.818283081054688
Epoch 1580, val loss: 1.0395655632019043
Epoch 1590, training loss: 883.4384765625 = 1.0341893434524536 + 100.0 * 8.824043273925781
Epoch 1590, val loss: 1.039131999015808
Epoch 1600, training loss: 884.0866088867188 = 1.0337250232696533 + 100.0 * 8.830528259277344
Epoch 1600, val loss: 1.038684606552124
Epoch 1610, training loss: 884.2220458984375 = 1.0332293510437012 + 100.0 * 8.831888198852539
Epoch 1610, val loss: 1.0382182598114014
Epoch 1620, training loss: 884.3694458007812 = 1.0327264070510864 + 100.0 * 8.833367347717285
Epoch 1620, val loss: 1.0377453565597534
Epoch 1630, training loss: 884.6825561523438 = 1.03224778175354 + 100.0 * 8.836503028869629
Epoch 1630, val loss: 1.0372966527938843
Epoch 1640, training loss: 884.4732666015625 = 1.0317493677139282 + 100.0 * 8.834415435791016
Epoch 1640, val loss: 1.0367960929870605
Epoch 1650, training loss: 884.6775512695312 = 1.0312411785125732 + 100.0 * 8.83646297454834
Epoch 1650, val loss: 1.036333680152893
Epoch 1660, training loss: 885.0277099609375 = 1.030757188796997 + 100.0 * 8.839969635009766
Epoch 1660, val loss: 1.0358928442001343
Epoch 1670, training loss: 884.8523559570312 = 1.030266284942627 + 100.0 * 8.838220596313477
Epoch 1670, val loss: 1.0354108810424805
Epoch 1680, training loss: 885.0008544921875 = 1.0297513008117676 + 100.0 * 8.83971118927002
Epoch 1680, val loss: 1.0349518060684204
Epoch 1690, training loss: 885.3884887695312 = 1.0292754173278809 + 100.0 * 8.843591690063477
Epoch 1690, val loss: 1.0345027446746826
Epoch 1700, training loss: 885.5944213867188 = 1.0287843942642212 + 100.0 * 8.845656394958496
Epoch 1700, val loss: 1.034037470817566
Epoch 1710, training loss: 885.2786865234375 = 1.0282515287399292 + 100.0 * 8.842504501342773
Epoch 1710, val loss: 1.0335242748260498
Epoch 1720, training loss: 885.6177368164062 = 1.027747631072998 + 100.0 * 8.84589958190918
Epoch 1720, val loss: 1.0330673456192017
Epoch 1730, training loss: 885.7630615234375 = 1.0272777080535889 + 100.0 * 8.847357749938965
Epoch 1730, val loss: 1.0326204299926758
Epoch 1740, training loss: 885.7639770507812 = 1.026789665222168 + 100.0 * 8.847372055053711
Epoch 1740, val loss: 1.032144546508789
Epoch 1750, training loss: 886.0191040039062 = 1.026301622390747 + 100.0 * 8.84992790222168
Epoch 1750, val loss: 1.0317004919052124
Epoch 1760, training loss: 886.2146606445312 = 1.0258195400238037 + 100.0 * 8.851888656616211
Epoch 1760, val loss: 1.0312410593032837
Epoch 1770, training loss: 886.3621215820312 = 1.0252385139465332 + 100.0 * 8.853368759155273
Epoch 1770, val loss: 1.0306411981582642
Epoch 1780, training loss: 886.0651245117188 = 1.0247182846069336 + 100.0 * 8.850403785705566
Epoch 1780, val loss: 1.030195951461792
Epoch 1790, training loss: 884.2144165039062 = 1.024153709411621 + 100.0 * 8.831902503967285
Epoch 1790, val loss: 1.0296852588653564
Epoch 1800, training loss: 884.4336547851562 = 1.0237088203430176 + 100.0 * 8.834099769592285
Epoch 1800, val loss: 1.029285192489624
Epoch 1810, training loss: 885.5503540039062 = 1.023285984992981 + 100.0 * 8.845270156860352
Epoch 1810, val loss: 1.0288742780685425
Epoch 1820, training loss: 886.701171875 = 1.02291738986969 + 100.0 * 8.856782913208008
Epoch 1820, val loss: 1.0285687446594238
Epoch 1830, training loss: 887.285400390625 = 1.0225971937179565 + 100.0 * 8.862627983093262
Epoch 1830, val loss: 1.028220295906067
Epoch 1840, training loss: 886.5651245117188 = 1.022033929824829 + 100.0 * 8.855430603027344
Epoch 1840, val loss: 1.0277096033096313
Epoch 1850, training loss: 886.92138671875 = 1.021557331085205 + 100.0 * 8.85899829864502
Epoch 1850, val loss: 1.0272612571716309
Epoch 1860, training loss: 887.2074584960938 = 1.0210751295089722 + 100.0 * 8.86186408996582
Epoch 1860, val loss: 1.0268158912658691
Epoch 1870, training loss: 887.2680053710938 = 1.020564079284668 + 100.0 * 8.86247444152832
Epoch 1870, val loss: 1.0263341665267944
Epoch 1880, training loss: 887.3572387695312 = 1.020068883895874 + 100.0 * 8.863371849060059
Epoch 1880, val loss: 1.0258722305297852
Epoch 1890, training loss: 887.2281494140625 = 1.0195581912994385 + 100.0 * 8.862086296081543
Epoch 1890, val loss: 1.0253816843032837
Epoch 1900, training loss: 887.3636474609375 = 1.0190579891204834 + 100.0 * 8.863446235656738
Epoch 1900, val loss: 1.0249403715133667
Epoch 1910, training loss: 888.0765380859375 = 1.0185415744781494 + 100.0 * 8.870579719543457
Epoch 1910, val loss: 1.02443528175354
Epoch 1920, training loss: 884.3675537109375 = 1.0178316831588745 + 100.0 * 8.833497047424316
Epoch 1920, val loss: 1.0237548351287842
Epoch 1930, training loss: 884.9468383789062 = 1.0173779726028442 + 100.0 * 8.83929443359375
Epoch 1930, val loss: 1.0233330726623535
Epoch 1940, training loss: 886.0989990234375 = 1.0169785022735596 + 100.0 * 8.850820541381836
Epoch 1940, val loss: 1.0229605436325073
Epoch 1950, training loss: 886.59716796875 = 1.0165305137634277 + 100.0 * 8.855806350708008
Epoch 1950, val loss: 1.022580623626709
Epoch 1960, training loss: 887.1698608398438 = 1.0160620212554932 + 100.0 * 8.86153793334961
Epoch 1960, val loss: 1.0221467018127441
Epoch 1970, training loss: 887.7142333984375 = 1.015605092048645 + 100.0 * 8.866986274719238
Epoch 1970, val loss: 1.0217293500900269
Epoch 1980, training loss: 888.1411743164062 = 1.0151485204696655 + 100.0 * 8.871260643005371
Epoch 1980, val loss: 1.0212966203689575
Epoch 1990, training loss: 888.3189697265625 = 1.0146605968475342 + 100.0 * 8.873043060302734
Epoch 1990, val loss: 1.020850419998169
Epoch 2000, training loss: 888.2780151367188 = 1.0141510963439941 + 100.0 * 8.872638702392578
Epoch 2000, val loss: 1.020379900932312
Epoch 2010, training loss: 886.8829345703125 = 1.0135853290557861 + 100.0 * 8.85869312286377
Epoch 2010, val loss: 1.0198402404785156
Epoch 2020, training loss: 888.1876220703125 = 1.0131480693817139 + 100.0 * 8.871745109558105
Epoch 2020, val loss: 1.019432783126831
Epoch 2030, training loss: 888.6541748046875 = 1.0126534700393677 + 100.0 * 8.876415252685547
Epoch 2030, val loss: 1.0189882516860962
Epoch 2040, training loss: 889.2747802734375 = 1.012178897857666 + 100.0 * 8.882625579833984
Epoch 2040, val loss: 1.0185428857803345
Epoch 2050, training loss: 889.8670043945312 = 1.0117055177688599 + 100.0 * 8.88855266571045
Epoch 2050, val loss: 1.0181035995483398
Epoch 2060, training loss: 890.0612182617188 = 1.0112196207046509 + 100.0 * 8.89050006866455
Epoch 2060, val loss: 1.0176422595977783
Epoch 2070, training loss: 890.1576538085938 = 1.0107240676879883 + 100.0 * 8.89146900177002
Epoch 2070, val loss: 1.0171972513198853
Epoch 2080, training loss: 890.2545166015625 = 1.0102375745773315 + 100.0 * 8.89244270324707
Epoch 2080, val loss: 1.0167462825775146
Epoch 2090, training loss: 890.389892578125 = 1.0097256898880005 + 100.0 * 8.89380168914795
Epoch 2090, val loss: 1.0161981582641602
Epoch 2100, training loss: 889.1829833984375 = 1.0090439319610596 + 100.0 * 8.881739616394043
Epoch 2100, val loss: 1.0156521797180176
Epoch 2110, training loss: 887.9942016601562 = 1.0085889101028442 + 100.0 * 8.869855880737305
Epoch 2110, val loss: 1.0152099132537842
Epoch 2120, training loss: 887.7672729492188 = 1.0081062316894531 + 100.0 * 8.867591857910156
Epoch 2120, val loss: 1.0147373676300049
Epoch 2130, training loss: 889.221435546875 = 1.007678747177124 + 100.0 * 8.882137298583984
Epoch 2130, val loss: 1.0143399238586426
Epoch 2140, training loss: 890.0236206054688 = 1.0072441101074219 + 100.0 * 8.89016342163086
Epoch 2140, val loss: 1.0139507055282593
Epoch 2150, training loss: 890.6528930664062 = 1.0067964792251587 + 100.0 * 8.896461486816406
Epoch 2150, val loss: 1.0135341882705688
Epoch 2160, training loss: 890.8270263671875 = 1.0063233375549316 + 100.0 * 8.89820671081543
Epoch 2160, val loss: 1.0130892992019653
Epoch 2170, training loss: 891.0259399414062 = 1.0058488845825195 + 100.0 * 8.900200843811035
Epoch 2170, val loss: 1.0126484632492065
Epoch 2180, training loss: 890.927490234375 = 1.0053514242172241 + 100.0 * 8.899221420288086
Epoch 2180, val loss: 1.012181043624878
Epoch 2190, training loss: 891.4155883789062 = 1.0048831701278687 + 100.0 * 8.904107093811035
Epoch 2190, val loss: 1.0117511749267578
Epoch 2200, training loss: 891.8001708984375 = 1.0044149160385132 + 100.0 * 8.907958030700684
Epoch 2200, val loss: 1.0113054513931274
Epoch 2210, training loss: 891.5838012695312 = 1.003903865814209 + 100.0 * 8.90579891204834
Epoch 2210, val loss: 1.0108312368392944
Epoch 2220, training loss: 891.7459106445312 = 1.003434658050537 + 100.0 * 8.907424926757812
Epoch 2220, val loss: 1.0103888511657715
Epoch 2230, training loss: 892.0271606445312 = 1.002956748008728 + 100.0 * 8.910242080688477
Epoch 2230, val loss: 1.0099411010742188
Epoch 2240, training loss: 891.9779663085938 = 1.0024737119674683 + 100.0 * 8.909754753112793
Epoch 2240, val loss: 1.0094879865646362
Epoch 2250, training loss: 892.351806640625 = 1.0019936561584473 + 100.0 * 8.913497924804688
Epoch 2250, val loss: 1.009055495262146
Epoch 2260, training loss: 892.4058837890625 = 1.001531958580017 + 100.0 * 8.914043426513672
Epoch 2260, val loss: 1.0085997581481934
Epoch 2270, training loss: 892.7236328125 = 1.0010508298873901 + 100.0 * 8.91722583770752
Epoch 2270, val loss: 1.0081546306610107
Epoch 2280, training loss: 893.0107421875 = 1.0005860328674316 + 100.0 * 8.920101165771484
Epoch 2280, val loss: 1.0077053308486938
Epoch 2290, training loss: 892.4603881835938 = 1.0000804662704468 + 100.0 * 8.914603233337402
Epoch 2290, val loss: 1.00724458694458
Epoch 2300, training loss: 892.7156372070312 = 0.9996104836463928 + 100.0 * 8.917160034179688
Epoch 2300, val loss: 1.0068068504333496
Epoch 2310, training loss: 893.1707153320312 = 0.9991431832313538 + 100.0 * 8.92171573638916
Epoch 2310, val loss: 1.0063799619674683
Epoch 2320, training loss: 893.142578125 = 0.9986700415611267 + 100.0 * 8.921439170837402
Epoch 2320, val loss: 1.0059287548065186
Epoch 2330, training loss: 890.5613403320312 = 0.9980489611625671 + 100.0 * 8.89563274383545
Epoch 2330, val loss: 1.0053097009658813
Epoch 2340, training loss: 891.3240966796875 = 0.9974706768989563 + 100.0 * 8.903265953063965
Epoch 2340, val loss: 1.00480055809021
Epoch 2350, training loss: 891.5545654296875 = 0.9970557689666748 + 100.0 * 8.905574798583984
Epoch 2350, val loss: 1.0044225454330444
Epoch 2360, training loss: 892.3001098632812 = 0.996607780456543 + 100.0 * 8.91303539276123
Epoch 2360, val loss: 1.0040028095245361
Epoch 2370, training loss: 893.0585327148438 = 0.9961747527122498 + 100.0 * 8.920623779296875
Epoch 2370, val loss: 1.0036017894744873
Epoch 2380, training loss: 893.4019775390625 = 0.9957283735275269 + 100.0 * 8.924062728881836
Epoch 2380, val loss: 1.0031888484954834
Epoch 2390, training loss: 893.6809692382812 = 0.9952774047851562 + 100.0 * 8.926856994628906
Epoch 2390, val loss: 1.0027639865875244
Epoch 2400, training loss: 894.0552978515625 = 0.9948192238807678 + 100.0 * 8.930604934692383
Epoch 2400, val loss: 1.0023329257965088
Epoch 2410, training loss: 894.2203979492188 = 0.9943495988845825 + 100.0 * 8.932260513305664
Epoch 2410, val loss: 1.0018951892852783
Epoch 2420, training loss: 893.7630004882812 = 0.9938586354255676 + 100.0 * 8.927691459655762
Epoch 2420, val loss: 1.0014432668685913
Epoch 2430, training loss: 894.006103515625 = 0.993394136428833 + 100.0 * 8.930127143859863
Epoch 2430, val loss: 1.0010011196136475
Epoch 2440, training loss: 894.45166015625 = 0.9929393529891968 + 100.0 * 8.934587478637695
Epoch 2440, val loss: 1.00058114528656
Epoch 2450, training loss: 894.4838256835938 = 0.9924765229225159 + 100.0 * 8.934913635253906
Epoch 2450, val loss: 1.0001453161239624
Epoch 2460, training loss: 893.540283203125 = 0.9919819831848145 + 100.0 * 8.925482749938965
Epoch 2460, val loss: 0.9996716976165771
Epoch 2470, training loss: 893.675048828125 = 0.9914984703063965 + 100.0 * 8.926835060119629
Epoch 2470, val loss: 0.9992190003395081
Epoch 2480, training loss: 893.9008178710938 = 0.9910454154014587 + 100.0 * 8.929098129272461
Epoch 2480, val loss: 0.9988093376159668
Epoch 2490, training loss: 894.027099609375 = 0.9906377196311951 + 100.0 * 8.930364608764648
Epoch 2490, val loss: 0.9984145164489746
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.4489855072463768
0.8640875172064045
The final CL Acc:0.58000, 0.13019, The final GNN Acc:0.86447, 0.00028
Begin epxeriment: noisy_level: 0.2 cont_weight: 100 epoch: der1:0.3 der2:0.4 dfr1:0.4 dfr2:0.5
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=2500, cl_num_layers=2, cl_num_proj_hidden=64, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=100.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.4, drop_feat_rate_1=0.4, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=32, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, noisy_level=0.2, num_hidden=64, num_proj_hidden=64, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=40, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 88648])
add edge: torch.Size([2, 106494])
remove edge: torch.Size([2, 71008])
updated graph: torch.Size([2, 88854])
=== Noisy graph ===
/home/project-robust-contrastive/Robust-Contrastive-Learning/model.py:227: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 1026.1824951171875 = 1.0945478677749634 + 100.0 * 10.25087833404541
Epoch 0, val loss: 1.0935460329055786
Epoch 10, training loss: 986.7030029296875 = 1.0943708419799805 + 100.0 * 9.856086730957031
Epoch 10, val loss: 1.093359112739563
Epoch 20, training loss: 969.21533203125 = 1.0942710638046265 + 100.0 * 9.6812105178833
Epoch 20, val loss: 1.0932395458221436
Epoch 30, training loss: 956.42236328125 = 1.094160795211792 + 100.0 * 9.553281784057617
Epoch 30, val loss: 1.0931153297424316
Epoch 40, training loss: 946.1746215820312 = 1.0940492153167725 + 100.0 * 9.4508056640625
Epoch 40, val loss: 1.0929893255233765
Epoch 50, training loss: 937.6047973632812 = 1.0939432382583618 + 100.0 * 9.365108489990234
Epoch 50, val loss: 1.0928676128387451
Epoch 60, training loss: 930.3001098632812 = 1.093825340270996 + 100.0 * 9.292062759399414
Epoch 60, val loss: 1.0927355289459229
Epoch 70, training loss: 923.9365844726562 = 1.0936946868896484 + 100.0 * 9.228428840637207
Epoch 70, val loss: 1.0925908088684082
Epoch 80, training loss: 918.3544311523438 = 1.0935554504394531 + 100.0 * 9.172608375549316
Epoch 80, val loss: 1.09243905544281
Epoch 90, training loss: 913.5167846679688 = 1.0933984518051147 + 100.0 * 9.124234199523926
Epoch 90, val loss: 1.092271089553833
Epoch 100, training loss: 909.261962890625 = 1.0932469367980957 + 100.0 * 9.081686973571777
Epoch 100, val loss: 1.0921061038970947
Epoch 110, training loss: 905.6198120117188 = 1.0930674076080322 + 100.0 * 9.045267105102539
Epoch 110, val loss: 1.091922640800476
Epoch 120, training loss: 902.3549194335938 = 1.0928789377212524 + 100.0 * 9.012619972229004
Epoch 120, val loss: 1.0917243957519531
Epoch 130, training loss: 899.6329345703125 = 1.0926494598388672 + 100.0 * 8.985403060913086
Epoch 130, val loss: 1.0914762020111084
Epoch 140, training loss: 897.246826171875 = 1.0923540592193604 + 100.0 * 8.96154499053955
Epoch 140, val loss: 1.0911767482757568
Epoch 150, training loss: 895.2823486328125 = 1.0920628309249878 + 100.0 * 8.941903114318848
Epoch 150, val loss: 1.0908781290054321
Epoch 160, training loss: 893.1923828125 = 1.0917582511901855 + 100.0 * 8.921006202697754
Epoch 160, val loss: 1.0905709266662598
Epoch 170, training loss: 891.8388061523438 = 1.0914418697357178 + 100.0 * 8.90747356414795
Epoch 170, val loss: 1.090247631072998
Epoch 180, training loss: 890.5303955078125 = 1.0911327600479126 + 100.0 * 8.894392967224121
Epoch 180, val loss: 1.0899358987808228
Epoch 190, training loss: 889.1397705078125 = 1.0908013582229614 + 100.0 * 8.880489349365234
Epoch 190, val loss: 1.0895999670028687
Epoch 200, training loss: 887.88232421875 = 1.0904664993286133 + 100.0 * 8.867918968200684
Epoch 200, val loss: 1.0892614126205444
Epoch 210, training loss: 887.5385131835938 = 1.0901246070861816 + 100.0 * 8.864483833312988
Epoch 210, val loss: 1.0889250040054321
Epoch 220, training loss: 886.0028686523438 = 1.0897525548934937 + 100.0 * 8.849130630493164
Epoch 220, val loss: 1.088546872138977
Epoch 230, training loss: 885.5382080078125 = 1.089409589767456 + 100.0 * 8.844488143920898
Epoch 230, val loss: 1.0881967544555664
Epoch 240, training loss: 885.201416015625 = 1.0890476703643799 + 100.0 * 8.841123580932617
Epoch 240, val loss: 1.0878429412841797
Epoch 250, training loss: 884.4526977539062 = 1.0886658430099487 + 100.0 * 8.833640098571777
Epoch 250, val loss: 1.08745539188385
Epoch 260, training loss: 884.0488891601562 = 1.088258147239685 + 100.0 * 8.829606056213379
Epoch 260, val loss: 1.0870541334152222
Epoch 270, training loss: 883.822265625 = 1.0878633260726929 + 100.0 * 8.827343940734863
Epoch 270, val loss: 1.0866564512252808
Epoch 280, training loss: 882.7293701171875 = 1.0873959064483643 + 100.0 * 8.81641960144043
Epoch 280, val loss: 1.0861937999725342
Epoch 290, training loss: 883.232666015625 = 1.0869921445846558 + 100.0 * 8.821456909179688
Epoch 290, val loss: 1.0857964754104614
Epoch 300, training loss: 883.0586547851562 = 1.086525559425354 + 100.0 * 8.819721221923828
Epoch 300, val loss: 1.085344672203064
Epoch 310, training loss: 882.8250122070312 = 1.0860804319381714 + 100.0 * 8.817389488220215
Epoch 310, val loss: 1.0848972797393799
Epoch 320, training loss: 882.5946655273438 = 1.0856095552444458 + 100.0 * 8.81509017944336
Epoch 320, val loss: 1.0844323635101318
Epoch 330, training loss: 882.5018310546875 = 1.0851287841796875 + 100.0 * 8.814167022705078
Epoch 330, val loss: 1.0839650630950928
Epoch 340, training loss: 882.6848754882812 = 1.0846494436264038 + 100.0 * 8.816001892089844
Epoch 340, val loss: 1.083495855331421
Epoch 350, training loss: 882.7020263671875 = 1.0841354131698608 + 100.0 * 8.816179275512695
Epoch 350, val loss: 1.0829929113388062
Epoch 360, training loss: 882.7459716796875 = 1.0836281776428223 + 100.0 * 8.81662368774414
Epoch 360, val loss: 1.0825064182281494
Epoch 370, training loss: 882.5941772460938 = 1.0831117630004883 + 100.0 * 8.815110206604004
Epoch 370, val loss: 1.082000970840454
Epoch 380, training loss: 882.2232055664062 = 1.0825775861740112 + 100.0 * 8.811406135559082
Epoch 380, val loss: 1.0814871788024902
Epoch 390, training loss: 882.6062622070312 = 1.0819898843765259 + 100.0 * 8.815242767333984
Epoch 390, val loss: 1.080904483795166
Epoch 400, training loss: 882.9169311523438 = 1.0815271139144897 + 100.0 * 8.818353652954102
Epoch 400, val loss: 1.0804626941680908
Epoch 410, training loss: 881.0386352539062 = 1.0809146165847778 + 100.0 * 8.799576759338379
Epoch 410, val loss: 1.0798882246017456
Epoch 420, training loss: 881.8259887695312 = 1.080370545387268 + 100.0 * 8.807456016540527
Epoch 420, val loss: 1.0793579816818237
Epoch 430, training loss: 881.610595703125 = 1.079811930656433 + 100.0 * 8.805307388305664
Epoch 430, val loss: 1.0788148641586304
Epoch 440, training loss: 881.673095703125 = 1.079244613647461 + 100.0 * 8.805938720703125
Epoch 440, val loss: 1.0782592296600342
Epoch 450, training loss: 881.9696044921875 = 1.0786625146865845 + 100.0 * 8.80890941619873
Epoch 450, val loss: 1.0777051448822021
Epoch 460, training loss: 882.668212890625 = 1.078039288520813 + 100.0 * 8.815901756286621
Epoch 460, val loss: 1.0770996809005737
Epoch 470, training loss: 882.6180419921875 = 1.0774565935134888 + 100.0 * 8.81540584564209
Epoch 470, val loss: 1.0765494108200073
Epoch 480, training loss: 882.4197387695312 = 1.0768386125564575 + 100.0 * 8.81342887878418
Epoch 480, val loss: 1.0759567022323608
Epoch 490, training loss: 882.3994750976562 = 1.076257586479187 + 100.0 * 8.813232421875
Epoch 490, val loss: 1.0753978490829468
Epoch 500, training loss: 882.8833618164062 = 1.0756382942199707 + 100.0 * 8.818077087402344
Epoch 500, val loss: 1.0748051404953003
Epoch 510, training loss: 883.6342163085938 = 1.0750205516815186 + 100.0 * 8.825592041015625
Epoch 510, val loss: 1.0742319822311401
Epoch 520, training loss: 883.1703491210938 = 1.0743408203125 + 100.0 * 8.82096004486084
Epoch 520, val loss: 1.0735828876495361
Epoch 530, training loss: 882.570068359375 = 1.0737208127975464 + 100.0 * 8.814963340759277
Epoch 530, val loss: 1.0729920864105225
Epoch 540, training loss: 882.9193725585938 = 1.0730940103530884 + 100.0 * 8.818462371826172
Epoch 540, val loss: 1.0723875761032104
Epoch 550, training loss: 883.5593872070312 = 1.0724611282348633 + 100.0 * 8.824869155883789
Epoch 550, val loss: 1.0717802047729492
Epoch 560, training loss: 883.690673828125 = 1.0718042850494385 + 100.0 * 8.826189041137695
Epoch 560, val loss: 1.0711557865142822
Epoch 570, training loss: 883.361572265625 = 1.0710554122924805 + 100.0 * 8.822905540466309
Epoch 570, val loss: 1.0704152584075928
Epoch 580, training loss: 884.1151733398438 = 1.0703938007354736 + 100.0 * 8.830448150634766
Epoch 580, val loss: 1.0698161125183105
Epoch 590, training loss: 883.1699829101562 = 1.0697249174118042 + 100.0 * 8.821002960205078
Epoch 590, val loss: 1.0691826343536377
Epoch 600, training loss: 882.748046875 = 1.0690616369247437 + 100.0 * 8.816789627075195
Epoch 600, val loss: 1.0685501098632812
Epoch 610, training loss: 883.0786743164062 = 1.0684051513671875 + 100.0 * 8.82010269165039
Epoch 610, val loss: 1.0679237842559814
Epoch 620, training loss: 883.7212524414062 = 1.067754864692688 + 100.0 * 8.82653522491455
Epoch 620, val loss: 1.0673044919967651
Epoch 630, training loss: 883.783447265625 = 1.0670746564865112 + 100.0 * 8.827163696289062
Epoch 630, val loss: 1.066663384437561
Epoch 640, training loss: 884.0206298828125 = 1.0663952827453613 + 100.0 * 8.82954216003418
Epoch 640, val loss: 1.0660223960876465
Epoch 650, training loss: 884.6064453125 = 1.0657193660736084 + 100.0 * 8.835407257080078
Epoch 650, val loss: 1.0653752088546753
Epoch 660, training loss: 884.5317993164062 = 1.065021276473999 + 100.0 * 8.834668159484863
Epoch 660, val loss: 1.0646928548812866
Epoch 670, training loss: 884.4188232421875 = 1.0642873048782349 + 100.0 * 8.833545684814453
Epoch 670, val loss: 1.063996434211731
Epoch 680, training loss: 884.962646484375 = 1.0635805130004883 + 100.0 * 8.838990211486816
Epoch 680, val loss: 1.0633465051651
Epoch 690, training loss: 885.20654296875 = 1.062898874282837 + 100.0 * 8.841436386108398
Epoch 690, val loss: 1.0626929998397827
Epoch 700, training loss: 885.6515502929688 = 1.0622376203536987 + 100.0 * 8.845892906188965
Epoch 700, val loss: 1.0620677471160889
Epoch 710, training loss: 885.64892578125 = 1.0615004301071167 + 100.0 * 8.845873832702637
Epoch 710, val loss: 1.061376929283142
Epoch 720, training loss: 885.7911376953125 = 1.0607976913452148 + 100.0 * 8.84730339050293
Epoch 720, val loss: 1.060706615447998
Epoch 730, training loss: 886.0703125 = 1.0600870847702026 + 100.0 * 8.850102424621582
Epoch 730, val loss: 1.0600242614746094
Epoch 740, training loss: 886.7109985351562 = 1.059374451637268 + 100.0 * 8.856515884399414
Epoch 740, val loss: 1.0593509674072266
Epoch 750, training loss: 886.8113403320312 = 1.0586745738983154 + 100.0 * 8.857526779174805
Epoch 750, val loss: 1.0586769580841064
Epoch 760, training loss: 886.970458984375 = 1.0579297542572021 + 100.0 * 8.859125137329102
Epoch 760, val loss: 1.0579841136932373
Epoch 770, training loss: 886.4392700195312 = 1.0571939945220947 + 100.0 * 8.85382080078125
Epoch 770, val loss: 1.0572693347930908
Epoch 780, training loss: 887.0167236328125 = 1.056459903717041 + 100.0 * 8.859602928161621
Epoch 780, val loss: 1.056569218635559
Epoch 790, training loss: 887.0950927734375 = 1.0557310581207275 + 100.0 * 8.860393524169922
Epoch 790, val loss: 1.0558784008026123
Epoch 800, training loss: 887.189697265625 = 1.0550127029418945 + 100.0 * 8.861347198486328
Epoch 800, val loss: 1.0551908016204834
Epoch 810, training loss: 887.18505859375 = 1.0542826652526855 + 100.0 * 8.861308097839355
Epoch 810, val loss: 1.054483413696289
Epoch 820, training loss: 888.0758666992188 = 1.053576946258545 + 100.0 * 8.870223045349121
Epoch 820, val loss: 1.0538004636764526
Epoch 830, training loss: 887.7276000976562 = 1.0527915954589844 + 100.0 * 8.866747856140137
Epoch 830, val loss: 1.053058385848999
Epoch 840, training loss: 888.1898803710938 = 1.0520685911178589 + 100.0 * 8.871377944946289
Epoch 840, val loss: 1.0523639917373657
Epoch 850, training loss: 888.5103149414062 = 1.051362156867981 + 100.0 * 8.874588966369629
Epoch 850, val loss: 1.0516786575317383
Epoch 860, training loss: 888.5121459960938 = 1.0506260395050049 + 100.0 * 8.874615669250488
Epoch 860, val loss: 1.0509756803512573
Epoch 870, training loss: 888.5985107421875 = 1.0498697757720947 + 100.0 * 8.875486373901367
Epoch 870, val loss: 1.0502527952194214
Epoch 880, training loss: 888.922119140625 = 1.0491271018981934 + 100.0 * 8.878729820251465
Epoch 880, val loss: 1.04954195022583
Epoch 890, training loss: 889.4188232421875 = 1.0483922958374023 + 100.0 * 8.88370418548584
Epoch 890, val loss: 1.0488146543502808
Epoch 900, training loss: 889.3944702148438 = 1.0476317405700684 + 100.0 * 8.883468627929688
Epoch 900, val loss: 1.0480865240097046
Epoch 910, training loss: 889.8194580078125 = 1.0468757152557373 + 100.0 * 8.887725830078125
Epoch 910, val loss: 1.0473594665527344
Epoch 920, training loss: 890.2327270507812 = 1.0461052656173706 + 100.0 * 8.891866683959961
Epoch 920, val loss: 1.046639323234558
Epoch 930, training loss: 890.1889038085938 = 1.0453547239303589 + 100.0 * 8.891435623168945
Epoch 930, val loss: 1.0459173917770386
Epoch 940, training loss: 889.3184204101562 = 1.044517993927002 + 100.0 * 8.882739067077637
Epoch 940, val loss: 1.045135498046875
Epoch 950, training loss: 889.50439453125 = 1.0437839031219482 + 100.0 * 8.88460636138916
Epoch 950, val loss: 1.0443962812423706
Epoch 960, training loss: 888.1241455078125 = 1.0428866147994995 + 100.0 * 8.87081241607666
Epoch 960, val loss: 1.0436139106750488
Epoch 970, training loss: 889.7342529296875 = 1.042357087135315 + 100.0 * 8.886919021606445
Epoch 970, val loss: 1.0430172681808472
Epoch 980, training loss: 889.354736328125 = 1.0416183471679688 + 100.0 * 8.88313102722168
Epoch 980, val loss: 1.0423167943954468
Epoch 990, training loss: 889.6119995117188 = 1.0409539937973022 + 100.0 * 8.885710716247559
Epoch 990, val loss: 1.0416755676269531
Epoch 1000, training loss: 890.1342163085938 = 1.0402350425720215 + 100.0 * 8.890939712524414
Epoch 1000, val loss: 1.0409951210021973
Epoch 1010, training loss: 890.8463134765625 = 1.0395034551620483 + 100.0 * 8.89806842803955
Epoch 1010, val loss: 1.0402799844741821
Epoch 1020, training loss: 891.09814453125 = 1.0387142896652222 + 100.0 * 8.900594711303711
Epoch 1020, val loss: 1.0394983291625977
Epoch 1030, training loss: 891.4869384765625 = 1.0379278659820557 + 100.0 * 8.90449047088623
Epoch 1030, val loss: 1.0387639999389648
Epoch 1040, training loss: 892.8902587890625 = 1.0370630025863647 + 100.0 * 8.918532371520996
Epoch 1040, val loss: 1.037901520729065
Epoch 1050, training loss: 892.4043579101562 = 1.0363558530807495 + 100.0 * 8.913680076599121
Epoch 1050, val loss: 1.0372587442398071
Epoch 1060, training loss: 890.6187133789062 = 1.0355889797210693 + 100.0 * 8.895831108093262
Epoch 1060, val loss: 1.0365334749221802
Epoch 1070, training loss: 891.307373046875 = 1.0348252058029175 + 100.0 * 8.902725219726562
Epoch 1070, val loss: 1.0357894897460938
Epoch 1080, training loss: 891.7943115234375 = 1.0340780019760132 + 100.0 * 8.907602310180664
Epoch 1080, val loss: 1.0350772142410278
Epoch 1090, training loss: 892.608642578125 = 1.0333517789840698 + 100.0 * 8.915753364562988
Epoch 1090, val loss: 1.034377932548523
Epoch 1100, training loss: 893.197021484375 = 1.0325952768325806 + 100.0 * 8.92164421081543
Epoch 1100, val loss: 1.0336469411849976
Epoch 1110, training loss: 893.3198852539062 = 1.0318206548690796 + 100.0 * 8.922881126403809
Epoch 1110, val loss: 1.0328699350357056
Epoch 1120, training loss: 893.8662109375 = 1.031017780303955 + 100.0 * 8.928352355957031
Epoch 1120, val loss: 1.0321216583251953
Epoch 1130, training loss: 893.7972412109375 = 1.0302348136901855 + 100.0 * 8.9276704788208
Epoch 1130, val loss: 1.031359076499939
Epoch 1140, training loss: 894.205078125 = 1.0294513702392578 + 100.0 * 8.931756019592285
Epoch 1140, val loss: 1.0306264162063599
Epoch 1150, training loss: 894.1967163085938 = 1.0286390781402588 + 100.0 * 8.931680679321289
Epoch 1150, val loss: 1.0298500061035156
Epoch 1160, training loss: 894.9235229492188 = 1.0278908014297485 + 100.0 * 8.938956260681152
Epoch 1160, val loss: 1.0291348695755005
Epoch 1170, training loss: 895.0390625 = 1.0270744562149048 + 100.0 * 8.940119743347168
Epoch 1170, val loss: 1.0283589363098145
Epoch 1180, training loss: 894.5904541015625 = 1.0262607336044312 + 100.0 * 8.93564224243164
Epoch 1180, val loss: 1.027557373046875
Epoch 1190, training loss: 895.7893676757812 = 1.0254924297332764 + 100.0 * 8.947638511657715
Epoch 1190, val loss: 1.0268243551254272
Epoch 1200, training loss: 896.3759155273438 = 1.0247383117675781 + 100.0 * 8.953512191772461
Epoch 1200, val loss: 1.0261062383651733
Epoch 1210, training loss: 896.1653442382812 = 1.0239430665969849 + 100.0 * 8.951414108276367
Epoch 1210, val loss: 1.0253324508666992
Epoch 1220, training loss: 896.136962890625 = 1.023137092590332 + 100.0 * 8.951138496398926
Epoch 1220, val loss: 1.0245667695999146
Epoch 1230, training loss: 896.46728515625 = 1.0223461389541626 + 100.0 * 8.954449653625488
Epoch 1230, val loss: 1.023818850517273
Epoch 1240, training loss: 892.5596923828125 = 1.0212403535842896 + 100.0 * 8.915384292602539
Epoch 1240, val loss: 1.022720456123352
Epoch 1250, training loss: 895.3875122070312 = 1.0207377672195435 + 100.0 * 8.9436674118042
Epoch 1250, val loss: 1.022202968597412
Epoch 1260, training loss: 895.7681884765625 = 1.019961953163147 + 100.0 * 8.947482109069824
Epoch 1260, val loss: 1.0215259790420532
Epoch 1270, training loss: 895.0457153320312 = 1.0191569328308105 + 100.0 * 8.940265655517578
Epoch 1270, val loss: 1.0207406282424927
Epoch 1280, training loss: 896.441650390625 = 1.0184088945388794 + 100.0 * 8.954232215881348
Epoch 1280, val loss: 1.0200140476226807
Epoch 1290, training loss: 896.796875 = 1.0176470279693604 + 100.0 * 8.957792282104492
Epoch 1290, val loss: 1.0192996263504028
Epoch 1300, training loss: 897.248046875 = 1.0168694257736206 + 100.0 * 8.962311744689941
Epoch 1300, val loss: 1.018563151359558
Epoch 1310, training loss: 896.8994140625 = 1.0160695314407349 + 100.0 * 8.958833694458008
Epoch 1310, val loss: 1.017778992652893
Epoch 1320, training loss: 897.3881225585938 = 1.0152696371078491 + 100.0 * 8.963728904724121
Epoch 1320, val loss: 1.0170258283615112
Epoch 1330, training loss: 898.1746215820312 = 1.0144846439361572 + 100.0 * 8.971601486206055
Epoch 1330, val loss: 1.016273021697998
Epoch 1340, training loss: 898.337890625 = 1.0137031078338623 + 100.0 * 8.973241806030273
Epoch 1340, val loss: 1.0155268907546997
Epoch 1350, training loss: 898.236572265625 = 1.0129156112670898 + 100.0 * 8.972236633300781
Epoch 1350, val loss: 1.0147584676742554
Epoch 1360, training loss: 898.6710205078125 = 1.012109398841858 + 100.0 * 8.97658920288086
Epoch 1360, val loss: 1.0139930248260498
Epoch 1370, training loss: 898.5743408203125 = 1.0113073587417603 + 100.0 * 8.975630760192871
Epoch 1370, val loss: 1.0132352113723755
Epoch 1380, training loss: 899.1107788085938 = 1.010522723197937 + 100.0 * 8.981002807617188
Epoch 1380, val loss: 1.012471318244934
Epoch 1390, training loss: 898.7844848632812 = 1.0096949338912964 + 100.0 * 8.977747917175293
Epoch 1390, val loss: 1.0117055177688599
Epoch 1400, training loss: 898.2404174804688 = 1.0088355541229248 + 100.0 * 8.972315788269043
Epoch 1400, val loss: 1.0109034776687622
Epoch 1410, training loss: 898.7372436523438 = 1.008056640625 + 100.0 * 8.97729206085205
Epoch 1410, val loss: 1.0101155042648315
Epoch 1420, training loss: 899.6878662109375 = 1.0072600841522217 + 100.0 * 8.98680591583252
Epoch 1420, val loss: 1.0093574523925781
Epoch 1430, training loss: 899.7882690429688 = 1.0064759254455566 + 100.0 * 8.987817764282227
Epoch 1430, val loss: 1.0085883140563965
Epoch 1440, training loss: 899.9546508789062 = 1.0056360960006714 + 100.0 * 8.989490509033203
Epoch 1440, val loss: 1.007818341255188
Epoch 1450, training loss: 900.2176513671875 = 1.0048296451568604 + 100.0 * 8.992128372192383
Epoch 1450, val loss: 1.0070223808288574
Epoch 1460, training loss: 900.5215454101562 = 1.0040451288223267 + 100.0 * 8.9951753616333
Epoch 1460, val loss: 1.006266713142395
Epoch 1470, training loss: 900.7945556640625 = 1.0032739639282227 + 100.0 * 8.997912406921387
Epoch 1470, val loss: 1.005537748336792
Epoch 1480, training loss: 901.338623046875 = 1.0024594068527222 + 100.0 * 9.003361701965332
Epoch 1480, val loss: 1.004759430885315
Epoch 1490, training loss: 900.769287109375 = 1.00160551071167 + 100.0 * 8.997676849365234
Epoch 1490, val loss: 1.0039196014404297
Epoch 1500, training loss: 901.0591430664062 = 1.0007874965667725 + 100.0 * 9.00058364868164
Epoch 1500, val loss: 1.0031582117080688
Epoch 1510, training loss: 901.609619140625 = 0.999977707862854 + 100.0 * 9.006095886230469
Epoch 1510, val loss: 1.0023868083953857
Epoch 1520, training loss: 901.7610473632812 = 0.9991550445556641 + 100.0 * 9.00761890411377
Epoch 1520, val loss: 1.001592993736267
Epoch 1530, training loss: 901.9131469726562 = 0.998313307762146 + 100.0 * 9.009148597717285
Epoch 1530, val loss: 1.000799536705017
Epoch 1540, training loss: 901.539794921875 = 0.9974497556686401 + 100.0 * 9.005423545837402
Epoch 1540, val loss: 1.0000029802322388
Epoch 1550, training loss: 902.12255859375 = 0.9966707229614258 + 100.0 * 9.011259078979492
Epoch 1550, val loss: 0.9992424845695496
Epoch 1560, training loss: 902.858154296875 = 0.9958674907684326 + 100.0 * 9.018623352050781
Epoch 1560, val loss: 0.9984670281410217
Epoch 1570, training loss: 903.2756958007812 = 0.9950446486473083 + 100.0 * 9.022806167602539
Epoch 1570, val loss: 0.9976808428764343
Epoch 1580, training loss: 902.6033935546875 = 0.9941700100898743 + 100.0 * 9.016092300415039
Epoch 1580, val loss: 0.9968681335449219
Epoch 1590, training loss: 903.485595703125 = 0.9933774471282959 + 100.0 * 9.024922370910645
Epoch 1590, val loss: 0.9961123466491699
Epoch 1600, training loss: 903.2937622070312 = 0.9925251007080078 + 100.0 * 9.023012161254883
Epoch 1600, val loss: 0.9952898621559143
Epoch 1610, training loss: 902.9026489257812 = 0.9916950464248657 + 100.0 * 9.019109725952148
Epoch 1610, val loss: 0.9944893717765808
Epoch 1620, training loss: 903.358154296875 = 0.9908387064933777 + 100.0 * 9.023673057556152
Epoch 1620, val loss: 0.9936743378639221
Epoch 1630, training loss: 903.873291015625 = 0.990027129650116 + 100.0 * 9.02883243560791
Epoch 1630, val loss: 0.992874801158905
Epoch 1640, training loss: 902.765625 = 0.9890280961990356 + 100.0 * 9.017765998840332
Epoch 1640, val loss: 0.9919792413711548
Epoch 1650, training loss: 902.9852905273438 = 0.9882059693336487 + 100.0 * 9.019970893859863
Epoch 1650, val loss: 0.9912093877792358
Epoch 1660, training loss: 903.890869140625 = 0.9874493479728699 + 100.0 * 9.029034614562988
Epoch 1660, val loss: 0.9904817938804626
Epoch 1670, training loss: 904.7374267578125 = 0.9866786003112793 + 100.0 * 9.037507057189941
Epoch 1670, val loss: 0.9897534847259521
Epoch 1680, training loss: 904.8588256835938 = 0.9858490228652954 + 100.0 * 9.038729667663574
Epoch 1680, val loss: 0.9889516830444336
Epoch 1690, training loss: 904.385986328125 = 0.9849986433982849 + 100.0 * 9.03400993347168
Epoch 1690, val loss: 0.9881426095962524
Epoch 1700, training loss: 904.3660888671875 = 0.9841806292533875 + 100.0 * 9.033819198608398
Epoch 1700, val loss: 0.9873780012130737
Epoch 1710, training loss: 905.2958984375 = 0.9833714962005615 + 100.0 * 9.04312515258789
Epoch 1710, val loss: 0.9866034388542175
Epoch 1720, training loss: 905.4063720703125 = 0.9825268983840942 + 100.0 * 9.044238090515137
Epoch 1720, val loss: 0.9858239889144897
Epoch 1730, training loss: 906.0161743164062 = 0.9817344546318054 + 100.0 * 9.050344467163086
Epoch 1730, val loss: 0.9850693941116333
Epoch 1740, training loss: 906.2158813476562 = 0.9808989763259888 + 100.0 * 9.052350044250488
Epoch 1740, val loss: 0.9842603206634521
Epoch 1750, training loss: 904.5001831054688 = 0.979965090751648 + 100.0 * 9.035202026367188
Epoch 1750, val loss: 0.9834099411964417
Epoch 1760, training loss: 904.78173828125 = 0.9790809154510498 + 100.0 * 9.038026809692383
Epoch 1760, val loss: 0.9825553297996521
Epoch 1770, training loss: 905.168701171875 = 0.9782659411430359 + 100.0 * 9.04190444946289
Epoch 1770, val loss: 0.9817968010902405
Epoch 1780, training loss: 906.0739135742188 = 0.9775017499923706 + 100.0 * 9.05096435546875
Epoch 1780, val loss: 0.9810696840286255
Epoch 1790, training loss: 906.9002685546875 = 0.9767006039619446 + 100.0 * 9.059235572814941
Epoch 1790, val loss: 0.9803136587142944
Epoch 1800, training loss: 906.6734008789062 = 0.9758782386779785 + 100.0 * 9.056975364685059
Epoch 1800, val loss: 0.9795005321502686
Epoch 1810, training loss: 906.3565063476562 = 0.9750368595123291 + 100.0 * 9.053814888000488
Epoch 1810, val loss: 0.9787216186523438
Epoch 1820, training loss: 906.9300537109375 = 0.9742293953895569 + 100.0 * 9.059557914733887
Epoch 1820, val loss: 0.9779576063156128
Epoch 1830, training loss: 907.600830078125 = 0.9734287858009338 + 100.0 * 9.06627368927002
Epoch 1830, val loss: 0.9771979451179504
Epoch 1840, training loss: 907.5166015625 = 0.9725961089134216 + 100.0 * 9.06544017791748
Epoch 1840, val loss: 0.9763684272766113
Epoch 1850, training loss: 907.7203979492188 = 0.9717660546302795 + 100.0 * 9.067486763000488
Epoch 1850, val loss: 0.9755945205688477
Epoch 1860, training loss: 907.8758544921875 = 0.9709344506263733 + 100.0 * 9.069048881530762
Epoch 1860, val loss: 0.9748159050941467
Epoch 1870, training loss: 907.5257568359375 = 0.9700555801391602 + 100.0 * 9.065557479858398
Epoch 1870, val loss: 0.9739753007888794
Epoch 1880, training loss: 907.960693359375 = 0.96921306848526 + 100.0 * 9.069914817810059
Epoch 1880, val loss: 0.9731804132461548
Epoch 1890, training loss: 908.38232421875 = 0.9684189558029175 + 100.0 * 9.074138641357422
Epoch 1890, val loss: 0.9724269509315491
Epoch 1900, training loss: 908.5445556640625 = 0.9676030874252319 + 100.0 * 9.075769424438477
Epoch 1900, val loss: 0.9716306924819946
Epoch 1910, training loss: 908.6907348632812 = 0.966745913028717 + 100.0 * 9.077239990234375
Epoch 1910, val loss: 0.970839262008667
Epoch 1920, training loss: 908.81298828125 = 0.9659289121627808 + 100.0 * 9.078470230102539
Epoch 1920, val loss: 0.9700665473937988
Epoch 1930, training loss: 909.361083984375 = 0.9651432633399963 + 100.0 * 9.083959579467773
Epoch 1930, val loss: 0.9693061709403992
Epoch 1940, training loss: 907.0091552734375 = 0.9640817046165466 + 100.0 * 9.060450553894043
Epoch 1940, val loss: 0.9682357907295227
Epoch 1950, training loss: 906.7514038085938 = 0.9633530974388123 + 100.0 * 9.057880401611328
Epoch 1950, val loss: 0.9676448106765747
Epoch 1960, training loss: 907.4432373046875 = 0.9628554582595825 + 100.0 * 9.064804077148438
Epoch 1960, val loss: 0.967124342918396
Epoch 1970, training loss: 908.0836791992188 = 0.9620863795280457 + 100.0 * 9.071215629577637
Epoch 1970, val loss: 0.9663859009742737
Epoch 1980, training loss: 908.8685913085938 = 0.9613618850708008 + 100.0 * 9.079071998596191
Epoch 1980, val loss: 0.965705156326294
Epoch 1990, training loss: 909.401611328125 = 0.9606189727783203 + 100.0 * 9.084409713745117
Epoch 1990, val loss: 0.9649965167045593
Epoch 2000, training loss: 909.8377685546875 = 0.9598398804664612 + 100.0 * 9.08877944946289
Epoch 2000, val loss: 0.9642575979232788
Epoch 2010, training loss: 908.9505004882812 = 0.9589978456497192 + 100.0 * 9.079915046691895
Epoch 2010, val loss: 0.963357150554657
Epoch 2020, training loss: 908.3348999023438 = 0.9581195116043091 + 100.0 * 9.07376766204834
Epoch 2020, val loss: 0.9625806212425232
Epoch 2030, training loss: 908.533447265625 = 0.9573397636413574 + 100.0 * 9.075760841369629
Epoch 2030, val loss: 0.961868941783905
Epoch 2040, training loss: 909.4060668945312 = 0.956599235534668 + 100.0 * 9.084494590759277
Epoch 2040, val loss: 0.9611726999282837
Epoch 2050, training loss: 910.0994873046875 = 0.9558605551719666 + 100.0 * 9.091436386108398
Epoch 2050, val loss: 0.9604671597480774
Epoch 2060, training loss: 910.1976318359375 = 0.9550502896308899 + 100.0 * 9.092425346374512
Epoch 2060, val loss: 0.9596875309944153
Epoch 2070, training loss: 910.2526245117188 = 0.9542393088340759 + 100.0 * 9.092984199523926
Epoch 2070, val loss: 0.9589335918426514
Epoch 2080, training loss: 910.6881103515625 = 0.9534697532653809 + 100.0 * 9.097346305847168
Epoch 2080, val loss: 0.9581947922706604
Epoch 2090, training loss: 910.7808227539062 = 0.9526515603065491 + 100.0 * 9.098281860351562
Epoch 2090, val loss: 0.9574022889137268
Epoch 2100, training loss: 910.572021484375 = 0.9518303871154785 + 100.0 * 9.09620189666748
Epoch 2100, val loss: 0.9566318988800049
Epoch 2110, training loss: 910.8153076171875 = 0.9510405659675598 + 100.0 * 9.098642349243164
Epoch 2110, val loss: 0.9558712840080261
Epoch 2120, training loss: 911.0321044921875 = 0.9502376914024353 + 100.0 * 9.100818634033203
Epoch 2120, val loss: 0.9551172256469727
Epoch 2130, training loss: 911.4142456054688 = 0.9494496583938599 + 100.0 * 9.104647636413574
Epoch 2130, val loss: 0.9543536901473999
Epoch 2140, training loss: 910.8597412109375 = 0.9486056566238403 + 100.0 * 9.099111557006836
Epoch 2140, val loss: 0.9535390138626099
Epoch 2150, training loss: 910.9100952148438 = 0.9477884769439697 + 100.0 * 9.09962272644043
Epoch 2150, val loss: 0.9527872800827026
Epoch 2160, training loss: 911.4549560546875 = 0.9470186829566956 + 100.0 * 9.105079650878906
Epoch 2160, val loss: 0.9520390033721924
Epoch 2170, training loss: 911.66650390625 = 0.9462398886680603 + 100.0 * 9.107202529907227
Epoch 2170, val loss: 0.9512927532196045
Epoch 2180, training loss: 911.7985229492188 = 0.945415735244751 + 100.0 * 9.10853099822998
Epoch 2180, val loss: 0.9505169987678528
Epoch 2190, training loss: 911.8350219726562 = 0.9446277022361755 + 100.0 * 9.108903884887695
Epoch 2190, val loss: 0.9497755765914917
Epoch 2200, training loss: 912.164794921875 = 0.9438756704330444 + 100.0 * 9.11220932006836
Epoch 2200, val loss: 0.949029266834259
Epoch 2210, training loss: 912.2357177734375 = 0.9430789947509766 + 100.0 * 9.112926483154297
Epoch 2210, val loss: 0.9482715725898743
Epoch 2220, training loss: 911.9271850585938 = 0.9422618746757507 + 100.0 * 9.109848976135254
Epoch 2220, val loss: 0.947510302066803
Epoch 2230, training loss: 911.5364990234375 = 0.941317081451416 + 100.0 * 9.105951309204102
Epoch 2230, val loss: 0.9466369152069092
Epoch 2240, training loss: 911.8631591796875 = 0.9405555129051208 + 100.0 * 9.10922622680664
Epoch 2240, val loss: 0.9458792209625244
Epoch 2250, training loss: 912.4425048828125 = 0.9397850036621094 + 100.0 * 9.11502742767334
Epoch 2250, val loss: 0.9451479315757751
Epoch 2260, training loss: 913.5189819335938 = 0.9390668272972107 + 100.0 * 9.125799179077148
Epoch 2260, val loss: 0.9444589614868164
Epoch 2270, training loss: 914.3247680664062 = 0.938335120677948 + 100.0 * 9.133864402770996
Epoch 2270, val loss: 0.9437664747238159
Epoch 2280, training loss: 914.048583984375 = 0.937531054019928 + 100.0 * 9.131110191345215
Epoch 2280, val loss: 0.9430012702941895
Epoch 2290, training loss: 914.114013671875 = 0.9367352724075317 + 100.0 * 9.131772994995117
Epoch 2290, val loss: 0.9422398209571838
Epoch 2300, training loss: 914.7317504882812 = 0.9359740614891052 + 100.0 * 9.137957572937012
Epoch 2300, val loss: 0.941514790058136
Epoch 2310, training loss: 914.524169921875 = 0.9351688027381897 + 100.0 * 9.135890007019043
Epoch 2310, val loss: 0.9407527446746826
Epoch 2320, training loss: 914.85107421875 = 0.9344024658203125 + 100.0 * 9.139166831970215
Epoch 2320, val loss: 0.9400280117988586
Epoch 2330, training loss: 914.7081909179688 = 0.9336071014404297 + 100.0 * 9.13774585723877
Epoch 2330, val loss: 0.9392828345298767
Epoch 2340, training loss: 915.00341796875 = 0.9328244924545288 + 100.0 * 9.140706062316895
Epoch 2340, val loss: 0.9385485649108887
Epoch 2350, training loss: 915.3442993164062 = 0.93206787109375 + 100.0 * 9.144122123718262
Epoch 2350, val loss: 0.9378482699394226
Epoch 2360, training loss: 915.7418212890625 = 0.9313008785247803 + 100.0 * 9.14810562133789
Epoch 2360, val loss: 0.9371135830879211
Epoch 2370, training loss: 915.5450439453125 = 0.9304813146591187 + 100.0 * 9.146145820617676
Epoch 2370, val loss: 0.9363584518432617
Epoch 2380, training loss: 915.4274291992188 = 0.929701030254364 + 100.0 * 9.144977569580078
Epoch 2380, val loss: 0.9356158971786499
Epoch 2390, training loss: 916.0060424804688 = 0.9289435744285583 + 100.0 * 9.150771141052246
Epoch 2390, val loss: 0.9349068403244019
Epoch 2400, training loss: 915.6790161132812 = 0.9281446933746338 + 100.0 * 9.14750862121582
Epoch 2400, val loss: 0.9341744184494019
Epoch 2410, training loss: 915.9576416015625 = 0.92740398645401 + 100.0 * 9.150301933288574
Epoch 2410, val loss: 0.9334678649902344
Epoch 2420, training loss: 916.626953125 = 0.926659882068634 + 100.0 * 9.157003402709961
Epoch 2420, val loss: 0.9327731728553772
Epoch 2430, training loss: 916.6008911132812 = 0.9259213805198669 + 100.0 * 9.156749725341797
Epoch 2430, val loss: 0.9320569038391113
Epoch 2440, training loss: 916.1542358398438 = 0.9251177310943604 + 100.0 * 9.152291297912598
Epoch 2440, val loss: 0.9313318133354187
Epoch 2450, training loss: 916.1740112304688 = 0.9243699908256531 + 100.0 * 9.152496337890625
Epoch 2450, val loss: 0.9306129217147827
Epoch 2460, training loss: 916.1317138671875 = 0.9235939383506775 + 100.0 * 9.152081489562988
Epoch 2460, val loss: 0.9299068450927734
Epoch 2470, training loss: 916.717529296875 = 0.9228662848472595 + 100.0 * 9.157946586608887
Epoch 2470, val loss: 0.9292294383049011
Epoch 2480, training loss: 917.18505859375 = 0.9221369624137878 + 100.0 * 9.162629127502441
Epoch 2480, val loss: 0.928548276424408
Epoch 2490, training loss: 916.8485107421875 = 0.9213904142379761 + 100.0 * 9.159271240234375
Epoch 2490, val loss: 0.9278407096862793
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.5391304347826087
0.8136636962979064
=== training gcn model ===
Epoch 0, training loss: 1029.645263671875 = 1.1006832122802734 + 100.0 * 10.285445213317871
Epoch 0, val loss: 1.1007119417190552
Epoch 10, training loss: 994.6367797851562 = 1.1005690097808838 + 100.0 * 9.935361862182617
Epoch 10, val loss: 1.1005610227584839
Epoch 20, training loss: 977.1478881835938 = 1.1002733707427979 + 100.0 * 9.760476112365723
Epoch 20, val loss: 1.1002612113952637
Epoch 30, training loss: 963.5466918945312 = 1.0999517440795898 + 100.0 * 9.624466896057129
Epoch 30, val loss: 1.0999428033828735
Epoch 40, training loss: 952.5114135742188 = 1.0996177196502686 + 100.0 * 9.514118194580078
Epoch 40, val loss: 1.0996135473251343
Epoch 50, training loss: 943.3480224609375 = 1.0992743968963623 + 100.0 * 9.422487258911133
Epoch 50, val loss: 1.0992692708969116
Epoch 60, training loss: 935.5631713867188 = 1.0989432334899902 + 100.0 * 9.344642639160156
Epoch 60, val loss: 1.0989359617233276
Epoch 70, training loss: 928.9068603515625 = 1.0986747741699219 + 100.0 * 9.278081893920898
Epoch 70, val loss: 1.0986768007278442
Epoch 80, training loss: 923.0856323242188 = 1.0985636711120605 + 100.0 * 9.219870567321777
Epoch 80, val loss: 1.0985827445983887
Epoch 90, training loss: 918.0271606445312 = 1.0985420942306519 + 100.0 * 9.169285774230957
Epoch 90, val loss: 1.0985645055770874
Epoch 100, training loss: 913.46044921875 = 1.0985230207443237 + 100.0 * 9.123619079589844
Epoch 100, val loss: 1.0985441207885742
Epoch 110, training loss: 909.5363159179688 = 1.09850013256073 + 100.0 * 9.084378242492676
Epoch 110, val loss: 1.098520040512085
Epoch 120, training loss: 906.0389404296875 = 1.0984752178192139 + 100.0 * 9.049405097961426
Epoch 120, val loss: 1.0984934568405151
Epoch 130, training loss: 919.8696899414062 = 1.0984431505203247 + 100.0 * 9.187712669372559
Epoch 130, val loss: 1.098460078239441
Epoch 140, training loss: 908.2884521484375 = 1.0984065532684326 + 100.0 * 9.071900367736816
Epoch 140, val loss: 1.0984373092651367
Epoch 150, training loss: 903.5027465820312 = 1.098435640335083 + 100.0 * 9.024043083190918
Epoch 150, val loss: 1.0984508991241455
Epoch 160, training loss: 900.9463500976562 = 1.0983670949935913 + 100.0 * 8.998479843139648
Epoch 160, val loss: 1.0983818769454956
Epoch 170, training loss: 898.6765747070312 = 1.0983283519744873 + 100.0 * 8.97578239440918
Epoch 170, val loss: 1.098345160484314
Epoch 180, training loss: 897.1962890625 = 1.0982866287231445 + 100.0 * 8.960980415344238
Epoch 180, val loss: 1.0983010530471802
Epoch 190, training loss: 895.6832275390625 = 1.0982404947280884 + 100.0 * 8.945849418640137
Epoch 190, val loss: 1.0982557535171509
Epoch 200, training loss: 894.5455322265625 = 1.0981963872909546 + 100.0 * 8.934473037719727
Epoch 200, val loss: 1.0982133150100708
Epoch 210, training loss: 893.4871826171875 = 1.098146915435791 + 100.0 * 8.923890113830566
Epoch 210, val loss: 1.098165512084961
Epoch 220, training loss: 892.3880615234375 = 1.098093867301941 + 100.0 * 8.9128999710083
Epoch 220, val loss: 1.098114013671875
Epoch 230, training loss: 891.6131591796875 = 1.0980411767959595 + 100.0 * 8.9051513671875
Epoch 230, val loss: 1.0980632305145264
Epoch 240, training loss: 890.7552490234375 = 1.0979852676391602 + 100.0 * 8.896573066711426
Epoch 240, val loss: 1.098009467124939
Epoch 250, training loss: 890.1141357421875 = 1.097926139831543 + 100.0 * 8.890162467956543
Epoch 250, val loss: 1.0979539155960083
Epoch 260, training loss: 889.4785766601562 = 1.0978622436523438 + 100.0 * 8.883807182312012
Epoch 260, val loss: 1.097892165184021
Epoch 270, training loss: 888.9080200195312 = 1.097795009613037 + 100.0 * 8.87810230255127
Epoch 270, val loss: 1.0978304147720337
Epoch 280, training loss: 888.3374633789062 = 1.0977271795272827 + 100.0 * 8.872397422790527
Epoch 280, val loss: 1.0977622270584106
Epoch 290, training loss: 887.9435424804688 = 1.097650408744812 + 100.0 * 8.86845874786377
Epoch 290, val loss: 1.0976903438568115
Epoch 300, training loss: 887.5323486328125 = 1.0975713729858398 + 100.0 * 8.864347457885742
Epoch 300, val loss: 1.0976135730743408
Epoch 310, training loss: 887.1488647460938 = 1.0974856615066528 + 100.0 * 8.860513687133789
Epoch 310, val loss: 1.0975351333618164
Epoch 320, training loss: 886.5618896484375 = 1.0973975658416748 + 100.0 * 8.854644775390625
Epoch 320, val loss: 1.0974504947662354
Epoch 330, training loss: 886.4430541992188 = 1.0973076820373535 + 100.0 * 8.8534574508667
Epoch 330, val loss: 1.097367525100708
Epoch 340, training loss: 885.9703369140625 = 1.0972181558609009 + 100.0 * 8.84873104095459
Epoch 340, val loss: 1.097281813621521
Epoch 350, training loss: 885.6473999023438 = 1.0971211194992065 + 100.0 * 8.845502853393555
Epoch 350, val loss: 1.0971920490264893
Epoch 360, training loss: 885.603759765625 = 1.0970271825790405 + 100.0 * 8.845067024230957
Epoch 360, val loss: 1.0971044301986694
Epoch 370, training loss: 885.5127563476562 = 1.0969294309616089 + 100.0 * 8.844158172607422
Epoch 370, val loss: 1.0970110893249512
Epoch 380, training loss: 885.1632080078125 = 1.0968260765075684 + 100.0 * 8.84066390991211
Epoch 380, val loss: 1.0969144105911255
Epoch 390, training loss: 885.2842407226562 = 1.0967168807983398 + 100.0 * 8.841875076293945
Epoch 390, val loss: 1.0968137979507446
Epoch 400, training loss: 884.7344970703125 = 1.096611499786377 + 100.0 * 8.836379051208496
Epoch 400, val loss: 1.096716284751892
Epoch 410, training loss: 884.7158813476562 = 1.0965009927749634 + 100.0 * 8.836194038391113
Epoch 410, val loss: 1.0966142416000366
Epoch 420, training loss: 884.3595581054688 = 1.09638249874115 + 100.0 * 8.832632064819336
Epoch 420, val loss: 1.0965051651000977
Epoch 430, training loss: 884.0706176757812 = 1.096266269683838 + 100.0 * 8.829743385314941
Epoch 430, val loss: 1.0963952541351318
Epoch 440, training loss: 884.044921875 = 1.0961463451385498 + 100.0 * 8.829487800598145
Epoch 440, val loss: 1.0962870121002197
Epoch 450, training loss: 884.0409545898438 = 1.096021056175232 + 100.0 * 8.829449653625488
Epoch 450, val loss: 1.0961706638336182
Epoch 460, training loss: 884.3466186523438 = 1.0958943367004395 + 100.0 * 8.832507133483887
Epoch 460, val loss: 1.0960525274276733
Epoch 470, training loss: 883.9400024414062 = 1.0957657098770142 + 100.0 * 8.828442573547363
Epoch 470, val loss: 1.0959341526031494
Epoch 480, training loss: 884.0223999023438 = 1.0956311225891113 + 100.0 * 8.829267501831055
Epoch 480, val loss: 1.095808744430542
Epoch 490, training loss: 884.232666015625 = 1.095494270324707 + 100.0 * 8.831371307373047
Epoch 490, val loss: 1.095681071281433
Epoch 500, training loss: 883.9616088867188 = 1.095353126525879 + 100.0 * 8.828662872314453
Epoch 500, val loss: 1.095551609992981
Epoch 510, training loss: 884.07861328125 = 1.0952039957046509 + 100.0 * 8.829833984375
Epoch 510, val loss: 1.0954108238220215
Epoch 520, training loss: 883.9222412109375 = 1.095057487487793 + 100.0 * 8.828271865844727
Epoch 520, val loss: 1.095279574394226
Epoch 530, training loss: 884.0234985351562 = 1.094905138015747 + 100.0 * 8.829285621643066
Epoch 530, val loss: 1.0951366424560547
Epoch 540, training loss: 884.2434692382812 = 1.0947504043579102 + 100.0 * 8.831487655639648
Epoch 540, val loss: 1.0949922800064087
Epoch 550, training loss: 884.3646850585938 = 1.0945866107940674 + 100.0 * 8.832700729370117
Epoch 550, val loss: 1.0948365926742554
Epoch 560, training loss: 884.1697387695312 = 1.0944206714630127 + 100.0 * 8.830753326416016
Epoch 560, val loss: 1.0946824550628662
Epoch 570, training loss: 883.685302734375 = 1.0942292213439941 + 100.0 * 8.825910568237305
Epoch 570, val loss: 1.09450101852417
Epoch 580, training loss: 884.9090576171875 = 1.094079613685608 + 100.0 * 8.838150024414062
Epoch 580, val loss: 1.094356656074524
Epoch 590, training loss: 883.6854248046875 = 1.0938897132873535 + 100.0 * 8.825915336608887
Epoch 590, val loss: 1.094185471534729
Epoch 600, training loss: 883.918212890625 = 1.0937141180038452 + 100.0 * 8.828245162963867
Epoch 600, val loss: 1.0940182209014893
Epoch 610, training loss: 884.0736694335938 = 1.0935336351394653 + 100.0 * 8.829801559448242
Epoch 610, val loss: 1.093851923942566
Epoch 620, training loss: 884.1659545898438 = 1.0933504104614258 + 100.0 * 8.830726623535156
Epoch 620, val loss: 1.0936800241470337
Epoch 630, training loss: 884.8248901367188 = 1.0931620597839355 + 100.0 * 8.83731746673584
Epoch 630, val loss: 1.0935007333755493
Epoch 640, training loss: 884.305908203125 = 1.0929673910140991 + 100.0 * 8.83212947845459
Epoch 640, val loss: 1.0933247804641724
Epoch 650, training loss: 884.4415283203125 = 1.0927790403366089 + 100.0 * 8.833487510681152
Epoch 650, val loss: 1.0931496620178223
Epoch 660, training loss: 884.5257568359375 = 1.0925838947296143 + 100.0 * 8.834331512451172
Epoch 660, val loss: 1.0929687023162842
Epoch 670, training loss: 884.5879516601562 = 1.092378854751587 + 100.0 * 8.834955215454102
Epoch 670, val loss: 1.0927740335464478
Epoch 680, training loss: 884.8668212890625 = 1.0921727418899536 + 100.0 * 8.837746620178223
Epoch 680, val loss: 1.0925827026367188
Epoch 690, training loss: 885.0084838867188 = 1.0919638872146606 + 100.0 * 8.839164733886719
Epoch 690, val loss: 1.0923899412155151
Epoch 700, training loss: 885.1439819335938 = 1.0917493104934692 + 100.0 * 8.840522766113281
Epoch 700, val loss: 1.0921895503997803
Epoch 710, training loss: 885.5831909179688 = 1.0915285348892212 + 100.0 * 8.844916343688965
Epoch 710, val loss: 1.0919873714447021
Epoch 720, training loss: 885.4679565429688 = 1.0913108587265015 + 100.0 * 8.843766212463379
Epoch 720, val loss: 1.0917872190475464
Epoch 730, training loss: 885.7577514648438 = 1.0911022424697876 + 100.0 * 8.84666633605957
Epoch 730, val loss: 1.0915881395339966
Epoch 740, training loss: 885.9911499023438 = 1.0908833742141724 + 100.0 * 8.849002838134766
Epoch 740, val loss: 1.0913890600204468
Epoch 750, training loss: 885.8667602539062 = 1.0906646251678467 + 100.0 * 8.847761154174805
Epoch 750, val loss: 1.0911872386932373
Epoch 760, training loss: 885.7996215820312 = 1.090437412261963 + 100.0 * 8.847091674804688
Epoch 760, val loss: 1.0909810066223145
Epoch 770, training loss: 885.9319458007812 = 1.0902055501937866 + 100.0 * 8.848417282104492
Epoch 770, val loss: 1.0907607078552246
Epoch 780, training loss: 886.34375 = 1.0899745225906372 + 100.0 * 8.852538108825684
Epoch 780, val loss: 1.0905474424362183
Epoch 790, training loss: 886.6480712890625 = 1.0897506475448608 + 100.0 * 8.855583190917969
Epoch 790, val loss: 1.090341567993164
Epoch 800, training loss: 887.7109985351562 = 1.0895143747329712 + 100.0 * 8.866214752197266
Epoch 800, val loss: 1.0901315212249756
Epoch 810, training loss: 886.49462890625 = 1.0892665386199951 + 100.0 * 8.854053497314453
Epoch 810, val loss: 1.0898914337158203
Epoch 820, training loss: 886.85595703125 = 1.089036464691162 + 100.0 * 8.85766887664795
Epoch 820, val loss: 1.0896790027618408
Epoch 830, training loss: 887.1312866210938 = 1.088792324066162 + 100.0 * 8.860424995422363
Epoch 830, val loss: 1.089452862739563
Epoch 840, training loss: 886.9930419921875 = 1.088545799255371 + 100.0 * 8.859045028686523
Epoch 840, val loss: 1.089226245880127
Epoch 850, training loss: 887.3561401367188 = 1.0882970094680786 + 100.0 * 8.862678527832031
Epoch 850, val loss: 1.0889960527420044
Epoch 860, training loss: 887.497802734375 = 1.0880427360534668 + 100.0 * 8.864097595214844
Epoch 860, val loss: 1.0887635946273804
Epoch 870, training loss: 886.99951171875 = 1.0877790451049805 + 100.0 * 8.85911750793457
Epoch 870, val loss: 1.0885224342346191
Epoch 880, training loss: 887.1425170898438 = 1.0875402688980103 + 100.0 * 8.860549926757812
Epoch 880, val loss: 1.0883009433746338
Epoch 890, training loss: 887.690185546875 = 1.0872899293899536 + 100.0 * 8.866028785705566
Epoch 890, val loss: 1.0880738496780396
Epoch 900, training loss: 887.9375610351562 = 1.087035059928894 + 100.0 * 8.868505477905273
Epoch 900, val loss: 1.0878419876098633
Epoch 910, training loss: 887.9442749023438 = 1.0867717266082764 + 100.0 * 8.868575096130371
Epoch 910, val loss: 1.0876001119613647
Epoch 920, training loss: 887.935791015625 = 1.086512804031372 + 100.0 * 8.86849308013916
Epoch 920, val loss: 1.0873637199401855
Epoch 930, training loss: 888.4763793945312 = 1.0862549543380737 + 100.0 * 8.8739013671875
Epoch 930, val loss: 1.0871241092681885
Epoch 940, training loss: 888.599853515625 = 1.085991382598877 + 100.0 * 8.875138282775879
Epoch 940, val loss: 1.0868840217590332
Epoch 950, training loss: 888.487060546875 = 1.085718035697937 + 100.0 * 8.874013900756836
Epoch 950, val loss: 1.0866395235061646
Epoch 960, training loss: 888.7760009765625 = 1.0854474306106567 + 100.0 * 8.87690544128418
Epoch 960, val loss: 1.0863913297653198
Epoch 970, training loss: 888.908203125 = 1.0851759910583496 + 100.0 * 8.878230094909668
Epoch 970, val loss: 1.0861486196517944
Epoch 980, training loss: 888.9734497070312 = 1.0849032402038574 + 100.0 * 8.878885269165039
Epoch 980, val loss: 1.0859004259109497
Epoch 990, training loss: 889.2653198242188 = 1.0846329927444458 + 100.0 * 8.881806373596191
Epoch 990, val loss: 1.0856523513793945
Epoch 1000, training loss: 889.362548828125 = 1.0843592882156372 + 100.0 * 8.882781982421875
Epoch 1000, val loss: 1.0853999853134155
Epoch 1010, training loss: 889.7085571289062 = 1.084080457687378 + 100.0 * 8.886244773864746
Epoch 1010, val loss: 1.0851513147354126
Epoch 1020, training loss: 889.4309692382812 = 1.0837955474853516 + 100.0 * 8.883471488952637
Epoch 1020, val loss: 1.084890604019165
Epoch 1030, training loss: 889.79638671875 = 1.0835202932357788 + 100.0 * 8.887128829956055
Epoch 1030, val loss: 1.0846439599990845
Epoch 1040, training loss: 890.174560546875 = 1.0832419395446777 + 100.0 * 8.890913009643555
Epoch 1040, val loss: 1.0843937397003174
Epoch 1050, training loss: 890.3411254882812 = 1.0829577445983887 + 100.0 * 8.892581939697266
Epoch 1050, val loss: 1.0841379165649414
Epoch 1060, training loss: 890.2777099609375 = 1.0826549530029297 + 100.0 * 8.891950607299805
Epoch 1060, val loss: 1.0838658809661865
Epoch 1070, training loss: 890.410888671875 = 1.0823780298233032 + 100.0 * 8.893284797668457
Epoch 1070, val loss: 1.0836191177368164
Epoch 1080, training loss: 890.6038818359375 = 1.0820930004119873 + 100.0 * 8.895217895507812
Epoch 1080, val loss: 1.0833508968353271
Epoch 1090, training loss: 890.542236328125 = 1.0817925930023193 + 100.0 * 8.894604682922363
Epoch 1090, val loss: 1.083093285560608
Epoch 1100, training loss: 891.0284423828125 = 1.081506609916687 + 100.0 * 8.899469375610352
Epoch 1100, val loss: 1.0828299522399902
Epoch 1110, training loss: 891.057373046875 = 1.0812151432037354 + 100.0 * 8.899761199951172
Epoch 1110, val loss: 1.0825709104537964
Epoch 1120, training loss: 891.4564208984375 = 1.0809272527694702 + 100.0 * 8.903755187988281
Epoch 1120, val loss: 1.0823103189468384
Epoch 1130, training loss: 891.6064453125 = 1.080636978149414 + 100.0 * 8.905258178710938
Epoch 1130, val loss: 1.0820420980453491
Epoch 1140, training loss: 891.7525024414062 = 1.0803371667861938 + 100.0 * 8.906722068786621
Epoch 1140, val loss: 1.0817668437957764
Epoch 1150, training loss: 891.8311157226562 = 1.0800371170043945 + 100.0 * 8.907510757446289
Epoch 1150, val loss: 1.0814988613128662
Epoch 1160, training loss: 892.299072265625 = 1.0797383785247803 + 100.0 * 8.912193298339844
Epoch 1160, val loss: 1.0812323093414307
Epoch 1170, training loss: 892.1934204101562 = 1.0794405937194824 + 100.0 * 8.911139488220215
Epoch 1170, val loss: 1.0809577703475952
Epoch 1180, training loss: 892.3033447265625 = 1.0791374444961548 + 100.0 * 8.91224193572998
Epoch 1180, val loss: 1.0806853771209717
Epoch 1190, training loss: 892.5634765625 = 1.0788366794586182 + 100.0 * 8.914846420288086
Epoch 1190, val loss: 1.0804139375686646
Epoch 1200, training loss: 892.8396606445312 = 1.0785346031188965 + 100.0 * 8.917611122131348
Epoch 1200, val loss: 1.0801396369934082
Epoch 1210, training loss: 892.8793334960938 = 1.0782251358032227 + 100.0 * 8.918010711669922
Epoch 1210, val loss: 1.0798618793487549
Epoch 1220, training loss: 892.818359375 = 1.077921986579895 + 100.0 * 8.917404174804688
Epoch 1220, val loss: 1.0795820951461792
Epoch 1230, training loss: 893.1981811523438 = 1.0776193141937256 + 100.0 * 8.921205520629883
Epoch 1230, val loss: 1.0793108940124512
Epoch 1240, training loss: 893.5711669921875 = 1.0773162841796875 + 100.0 * 8.924938201904297
Epoch 1240, val loss: 1.0790339708328247
Epoch 1250, training loss: 893.703125 = 1.0770028829574585 + 100.0 * 8.926260948181152
Epoch 1250, val loss: 1.078751802444458
Epoch 1260, training loss: 893.6006469726562 = 1.0766929388046265 + 100.0 * 8.925239562988281
Epoch 1260, val loss: 1.0784722566604614
Epoch 1270, training loss: 894.1180419921875 = 1.076392650604248 + 100.0 * 8.930416107177734
Epoch 1270, val loss: 1.0781972408294678
Epoch 1280, training loss: 893.7850952148438 = 1.076064109802246 + 100.0 * 8.927090644836426
Epoch 1280, val loss: 1.0778971910476685
Epoch 1290, training loss: 894.1807861328125 = 1.075761318206787 + 100.0 * 8.931050300598145
Epoch 1290, val loss: 1.0776312351226807
Epoch 1300, training loss: 894.6287841796875 = 1.0754591226577759 + 100.0 * 8.93553352355957
Epoch 1300, val loss: 1.077351450920105
Epoch 1310, training loss: 894.7105102539062 = 1.075147271156311 + 100.0 * 8.93635368347168
Epoch 1310, val loss: 1.077066421508789
Epoch 1320, training loss: 894.6953125 = 1.0748265981674194 + 100.0 * 8.93620491027832
Epoch 1320, val loss: 1.0767866373062134
Epoch 1330, training loss: 894.8472290039062 = 1.0745230913162231 + 100.0 * 8.937726974487305
Epoch 1330, val loss: 1.076503038406372
Epoch 1340, training loss: 895.0678100585938 = 1.0742127895355225 + 100.0 * 8.939935684204102
Epoch 1340, val loss: 1.0762200355529785
Epoch 1350, training loss: 895.2216186523438 = 1.0739001035690308 + 100.0 * 8.941476821899414
Epoch 1350, val loss: 1.075935959815979
Epoch 1360, training loss: 901.2224731445312 = 1.0735816955566406 + 100.0 * 9.00148868560791
Epoch 1360, val loss: 1.0755947828292847
Epoch 1370, training loss: 889.6841430664062 = 1.0731614828109741 + 100.0 * 8.886109352111816
Epoch 1370, val loss: 1.0751794576644897
Epoch 1380, training loss: 896.2354736328125 = 1.0730429887771606 + 100.0 * 8.951623916625977
Epoch 1380, val loss: 1.0751242637634277
Epoch 1390, training loss: 894.9738159179688 = 1.0727310180664062 + 100.0 * 8.939010620117188
Epoch 1390, val loss: 1.0748538970947266
Epoch 1400, training loss: 895.8944702148438 = 1.072461724281311 + 100.0 * 8.948220252990723
Epoch 1400, val loss: 1.0746277570724487
Epoch 1410, training loss: 895.4317626953125 = 1.0721542835235596 + 100.0 * 8.943595886230469
Epoch 1410, val loss: 1.0743529796600342
Epoch 1420, training loss: 896.274658203125 = 1.0718629360198975 + 100.0 * 8.952028274536133
Epoch 1420, val loss: 1.0740852355957031
Epoch 1430, training loss: 897.0841674804688 = 1.0715761184692383 + 100.0 * 8.960125923156738
Epoch 1430, val loss: 1.0738345384597778
Epoch 1440, training loss: 897.4554443359375 = 1.0712783336639404 + 100.0 * 8.963841438293457
Epoch 1440, val loss: 1.0735700130462646
Epoch 1450, training loss: 898.1856689453125 = 1.0709835290908813 + 100.0 * 8.971146583557129
Epoch 1450, val loss: 1.073301911354065
Epoch 1460, training loss: 898.5115966796875 = 1.0706807374954224 + 100.0 * 8.974409103393555
Epoch 1460, val loss: 1.0730327367782593
Epoch 1470, training loss: 899.0772705078125 = 1.0703777074813843 + 100.0 * 8.980069160461426
Epoch 1470, val loss: 1.0727639198303223
Epoch 1480, training loss: 899.0762329101562 = 1.0700690746307373 + 100.0 * 8.980061531066895
Epoch 1480, val loss: 1.0724929571151733
Epoch 1490, training loss: 899.2396850585938 = 1.0697633028030396 + 100.0 * 8.981698989868164
Epoch 1490, val loss: 1.07221519947052
Epoch 1500, training loss: 899.2669067382812 = 1.069458246231079 + 100.0 * 8.981974601745605
Epoch 1500, val loss: 1.071946144104004
Epoch 1510, training loss: 900.0708618164062 = 1.0691527128219604 + 100.0 * 8.99001693725586
Epoch 1510, val loss: 1.071675419807434
Epoch 1520, training loss: 900.3689575195312 = 1.0688496828079224 + 100.0 * 8.993000984191895
Epoch 1520, val loss: 1.0714067220687866
Epoch 1530, training loss: 900.6416015625 = 1.0685425996780396 + 100.0 * 8.99573040008545
Epoch 1530, val loss: 1.0711331367492676
Epoch 1540, training loss: 900.5230102539062 = 1.0682286024093628 + 100.0 * 8.994547843933105
Epoch 1540, val loss: 1.0708545446395874
Epoch 1550, training loss: 900.9380493164062 = 1.0679230690002441 + 100.0 * 8.998701095581055
Epoch 1550, val loss: 1.0705749988555908
Epoch 1560, training loss: 901.120361328125 = 1.0676079988479614 + 100.0 * 9.000527381896973
Epoch 1560, val loss: 1.0703016519546509
Epoch 1570, training loss: 900.8281860351562 = 1.0672894716262817 + 100.0 * 8.99760913848877
Epoch 1570, val loss: 1.070007562637329
Epoch 1580, training loss: 901.1629638671875 = 1.0669785737991333 + 100.0 * 9.000960350036621
Epoch 1580, val loss: 1.069735050201416
Epoch 1590, training loss: 901.8550415039062 = 1.0666756629943848 + 100.0 * 9.00788402557373
Epoch 1590, val loss: 1.0694668292999268
Epoch 1600, training loss: 902.063232421875 = 1.0663645267486572 + 100.0 * 9.009968757629395
Epoch 1600, val loss: 1.069191813468933
Epoch 1610, training loss: 902.15234375 = 1.0660569667816162 + 100.0 * 9.010863304138184
Epoch 1610, val loss: 1.0689131021499634
Epoch 1620, training loss: 902.0814208984375 = 1.0657391548156738 + 100.0 * 9.010156631469727
Epoch 1620, val loss: 1.0686331987380981
Epoch 1630, training loss: 902.3521118164062 = 1.0654306411743164 + 100.0 * 9.012866973876953
Epoch 1630, val loss: 1.0683622360229492
Epoch 1640, training loss: 902.7966918945312 = 1.0651220083236694 + 100.0 * 9.017315864562988
Epoch 1640, val loss: 1.0680872201919556
Epoch 1650, training loss: 902.8185424804688 = 1.0648090839385986 + 100.0 * 9.017537117004395
Epoch 1650, val loss: 1.0678023099899292
Epoch 1660, training loss: 903.1275024414062 = 1.06449556350708 + 100.0 * 9.0206298828125
Epoch 1660, val loss: 1.067530632019043
Epoch 1670, training loss: 903.3720092773438 = 1.0641846656799316 + 100.0 * 9.023077964782715
Epoch 1670, val loss: 1.0672564506530762
Epoch 1680, training loss: 903.458984375 = 1.0638688802719116 + 100.0 * 9.023951530456543
Epoch 1680, val loss: 1.0669798851013184
Epoch 1690, training loss: 903.6075439453125 = 1.0635534524917603 + 100.0 * 9.025440216064453
Epoch 1690, val loss: 1.0667041540145874
Epoch 1700, training loss: 903.7346801757812 = 1.0632504224777222 + 100.0 * 9.026714324951172
Epoch 1700, val loss: 1.066431999206543
Epoch 1710, training loss: 903.9660034179688 = 1.062942624092102 + 100.0 * 9.029030799865723
Epoch 1710, val loss: 1.0661615133285522
Epoch 1720, training loss: 904.3190307617188 = 1.062638759613037 + 100.0 * 9.032564163208008
Epoch 1720, val loss: 1.0658975839614868
Epoch 1730, training loss: 904.0806884765625 = 1.0623189210891724 + 100.0 * 9.030183792114258
Epoch 1730, val loss: 1.06561279296875
Epoch 1740, training loss: 903.5914306640625 = 1.061998724937439 + 100.0 * 9.025294303894043
Epoch 1740, val loss: 1.0653386116027832
Epoch 1750, training loss: 904.2415161132812 = 1.0617036819458008 + 100.0 * 9.031798362731934
Epoch 1750, val loss: 1.0650746822357178
Epoch 1760, training loss: 904.3391723632812 = 1.0613915920257568 + 100.0 * 9.032777786254883
Epoch 1760, val loss: 1.0648043155670166
Epoch 1770, training loss: 905.1277465820312 = 1.0610967874526978 + 100.0 * 9.040666580200195
Epoch 1770, val loss: 1.064548134803772
Epoch 1780, training loss: 905.2214965820312 = 1.0607900619506836 + 100.0 * 9.041606903076172
Epoch 1780, val loss: 1.0642731189727783
Epoch 1790, training loss: 905.0581665039062 = 1.0604792833328247 + 100.0 * 9.039977073669434
Epoch 1790, val loss: 1.0640121698379517
Epoch 1800, training loss: 905.6359252929688 = 1.0601890087127686 + 100.0 * 9.045757293701172
Epoch 1800, val loss: 1.063759684562683
Epoch 1810, training loss: 905.7926635742188 = 1.059894323348999 + 100.0 * 9.047327995300293
Epoch 1810, val loss: 1.0635040998458862
Epoch 1820, training loss: 905.6341552734375 = 1.0595893859863281 + 100.0 * 9.045745849609375
Epoch 1820, val loss: 1.0632262229919434
Epoch 1830, training loss: 906.2490234375 = 1.059327483177185 + 100.0 * 9.051897048950195
Epoch 1830, val loss: 1.0630097389221191
Epoch 1840, training loss: 906.470947265625 = 1.0590206384658813 + 100.0 * 9.054119110107422
Epoch 1840, val loss: 1.0627447366714478
Epoch 1850, training loss: 906.3847045898438 = 1.0587183237075806 + 100.0 * 9.05325984954834
Epoch 1850, val loss: 1.0624802112579346
Epoch 1860, training loss: 906.4354858398438 = 1.0584169626235962 + 100.0 * 9.053771018981934
Epoch 1860, val loss: 1.0622237920761108
Epoch 1870, training loss: 906.5059814453125 = 1.0581330060958862 + 100.0 * 9.054478645324707
Epoch 1870, val loss: 1.0619618892669678
Epoch 1880, training loss: 906.5888061523438 = 1.0578372478485107 + 100.0 * 9.055309295654297
Epoch 1880, val loss: 1.0617094039916992
Epoch 1890, training loss: 907.1298828125 = 1.0575556755065918 + 100.0 * 9.060723304748535
Epoch 1890, val loss: 1.061464548110962
Epoch 1900, training loss: 906.9249267578125 = 1.0572574138641357 + 100.0 * 9.058676719665527
Epoch 1900, val loss: 1.0612086057662964
Epoch 1910, training loss: 907.3338623046875 = 1.0569617748260498 + 100.0 * 9.062768936157227
Epoch 1910, val loss: 1.0609560012817383
Epoch 1920, training loss: 907.1464233398438 = 1.0566657781600952 + 100.0 * 9.060897827148438
Epoch 1920, val loss: 1.060686469078064
Epoch 1930, training loss: 906.889892578125 = 1.0563658475875854 + 100.0 * 9.058335304260254
Epoch 1930, val loss: 1.0604251623153687
Epoch 1940, training loss: 907.2290649414062 = 1.0560818910598755 + 100.0 * 9.061729431152344
Epoch 1940, val loss: 1.0601887702941895
Epoch 1950, training loss: 907.869384765625 = 1.0558141469955444 + 100.0 * 9.068136215209961
Epoch 1950, val loss: 1.0599550008773804
Epoch 1960, training loss: 908.0814208984375 = 1.055537223815918 + 100.0 * 9.070259094238281
Epoch 1960, val loss: 1.0597081184387207
Epoch 1970, training loss: 908.091796875 = 1.0552679300308228 + 100.0 * 9.070364952087402
Epoch 1970, val loss: 1.0594823360443115
Epoch 1980, training loss: 908.4488525390625 = 1.0549856424331665 + 100.0 * 9.073938369750977
Epoch 1980, val loss: 1.0592423677444458
Epoch 1990, training loss: 908.8426513671875 = 1.0547163486480713 + 100.0 * 9.077879905700684
Epoch 1990, val loss: 1.0590085983276367
Epoch 2000, training loss: 908.785888671875 = 1.0544315576553345 + 100.0 * 9.077314376831055
Epoch 2000, val loss: 1.0587654113769531
Epoch 2010, training loss: 908.48681640625 = 1.0541428327560425 + 100.0 * 9.074326515197754
Epoch 2010, val loss: 1.0585108995437622
Epoch 2020, training loss: 908.391845703125 = 1.053864598274231 + 100.0 * 9.073379516601562
Epoch 2020, val loss: 1.0582726001739502
Epoch 2030, training loss: 909.2493896484375 = 1.0535997152328491 + 100.0 * 9.081957817077637
Epoch 2030, val loss: 1.0580452680587769
Epoch 2040, training loss: 909.650634765625 = 1.0533273220062256 + 100.0 * 9.085972785949707
Epoch 2040, val loss: 1.057803750038147
Epoch 2050, training loss: 909.6981811523438 = 1.053052544593811 + 100.0 * 9.086451530456543
Epoch 2050, val loss: 1.0575587749481201
Epoch 2060, training loss: 909.8632202148438 = 1.0527821779251099 + 100.0 * 9.088104248046875
Epoch 2060, val loss: 1.0573291778564453
Epoch 2070, training loss: 909.6914672851562 = 1.0525113344192505 + 100.0 * 9.086389541625977
Epoch 2070, val loss: 1.0570870637893677
Epoch 2080, training loss: 909.7736206054688 = 1.0522414445877075 + 100.0 * 9.087213516235352
Epoch 2080, val loss: 1.0568559169769287
Epoch 2090, training loss: 909.8341674804688 = 1.05197012424469 + 100.0 * 9.087821960449219
Epoch 2090, val loss: 1.056620717048645
Epoch 2100, training loss: 910.1932373046875 = 1.0517041683197021 + 100.0 * 9.091415405273438
Epoch 2100, val loss: 1.0563970804214478
Epoch 2110, training loss: 910.16064453125 = 1.0514405965805054 + 100.0 * 9.091092109680176
Epoch 2110, val loss: 1.05617094039917
Epoch 2120, training loss: 909.9078979492188 = 1.0511730909347534 + 100.0 * 9.088567733764648
Epoch 2120, val loss: 1.0559370517730713
Epoch 2130, training loss: 910.2088623046875 = 1.0509047508239746 + 100.0 * 9.09157943725586
Epoch 2130, val loss: 1.0557035207748413
Epoch 2140, training loss: 910.7711791992188 = 1.0506689548492432 + 100.0 * 9.09720516204834
Epoch 2140, val loss: 1.0555062294006348
Epoch 2150, training loss: 911.1256103515625 = 1.0504237413406372 + 100.0 * 9.100751876831055
Epoch 2150, val loss: 1.0552878379821777
Epoch 2160, training loss: 910.6929931640625 = 1.0501554012298584 + 100.0 * 9.096427917480469
Epoch 2160, val loss: 1.055068850517273
Epoch 2170, training loss: 910.822998046875 = 1.0498939752578735 + 100.0 * 9.097731590270996
Epoch 2170, val loss: 1.0548458099365234
Epoch 2180, training loss: 911.0676879882812 = 1.0496419668197632 + 100.0 * 9.100180625915527
Epoch 2180, val loss: 1.0546303987503052
Epoch 2190, training loss: 910.6581420898438 = 1.0493885278701782 + 100.0 * 9.096087455749512
Epoch 2190, val loss: 1.0544086694717407
Epoch 2200, training loss: 911.08447265625 = 1.049146056175232 + 100.0 * 9.100353240966797
Epoch 2200, val loss: 1.054200530052185
Epoch 2210, training loss: 911.4064331054688 = 1.0489062070846558 + 100.0 * 9.103575706481934
Epoch 2210, val loss: 1.0539910793304443
Epoch 2220, training loss: 911.59619140625 = 1.0486501455307007 + 100.0 * 9.105475425720215
Epoch 2220, val loss: 1.0537749528884888
Epoch 2230, training loss: 911.6746215820312 = 1.0484036207199097 + 100.0 * 9.10626220703125
Epoch 2230, val loss: 1.0535593032836914
Epoch 2240, training loss: 910.4639892578125 = 1.048109769821167 + 100.0 * 9.094159126281738
Epoch 2240, val loss: 1.053302526473999
Epoch 2250, training loss: 910.1820678710938 = 1.04784095287323 + 100.0 * 9.091341972351074
Epoch 2250, val loss: 1.0530638694763184
Epoch 2260, training loss: 910.1810302734375 = 1.0476038455963135 + 100.0 * 9.091334342956543
Epoch 2260, val loss: 1.052865743637085
Epoch 2270, training loss: 910.724609375 = 1.0473722219467163 + 100.0 * 9.096772193908691
Epoch 2270, val loss: 1.052669644355774
Epoch 2280, training loss: 911.3853149414062 = 1.0471365451812744 + 100.0 * 9.103382110595703
Epoch 2280, val loss: 1.052470088005066
Epoch 2290, training loss: 912.1141357421875 = 1.0469071865081787 + 100.0 * 9.110671997070312
Epoch 2290, val loss: 1.052271842956543
Epoch 2300, training loss: 912.0208740234375 = 1.046658992767334 + 100.0 * 9.109742164611816
Epoch 2300, val loss: 1.052067756652832
Epoch 2310, training loss: 912.3189697265625 = 1.04641854763031 + 100.0 * 9.112725257873535
Epoch 2310, val loss: 1.051862359046936
Epoch 2320, training loss: 912.4202270507812 = 1.046186923980713 + 100.0 * 9.113739967346191
Epoch 2320, val loss: 1.0516623258590698
Epoch 2330, training loss: 912.26611328125 = 1.045944333076477 + 100.0 * 9.112201690673828
Epoch 2330, val loss: 1.051459550857544
Epoch 2340, training loss: 912.5512084960938 = 1.0457127094268799 + 100.0 * 9.115055084228516
Epoch 2340, val loss: 1.051265001296997
Epoch 2350, training loss: 913.0263671875 = 1.0454825162887573 + 100.0 * 9.1198091506958
Epoch 2350, val loss: 1.0510623455047607
Epoch 2360, training loss: 912.42578125 = 1.045250415802002 + 100.0 * 9.113805770874023
Epoch 2360, val loss: 1.0508697032928467
Epoch 2370, training loss: 912.7896118164062 = 1.0450180768966675 + 100.0 * 9.117445945739746
Epoch 2370, val loss: 1.0506765842437744
Epoch 2380, training loss: 913.2591552734375 = 1.044797658920288 + 100.0 * 9.122143745422363
Epoch 2380, val loss: 1.0504896640777588
Epoch 2390, training loss: 913.5111694335938 = 1.0445704460144043 + 100.0 * 9.124666213989258
Epoch 2390, val loss: 1.0503021478652954
Epoch 2400, training loss: 913.2769775390625 = 1.0443347692489624 + 100.0 * 9.122326850891113
Epoch 2400, val loss: 1.0501052141189575
Epoch 2410, training loss: 913.40185546875 = 1.0441129207611084 + 100.0 * 9.123577117919922
Epoch 2410, val loss: 1.049918532371521
Epoch 2420, training loss: 913.7936401367188 = 1.043897032737732 + 100.0 * 9.127497673034668
Epoch 2420, val loss: 1.049737811088562
Epoch 2430, training loss: 914.1242065429688 = 1.0436739921569824 + 100.0 * 9.130805015563965
Epoch 2430, val loss: 1.0495595932006836
Epoch 2440, training loss: 913.8450927734375 = 1.0434409379959106 + 100.0 * 9.128016471862793
Epoch 2440, val loss: 1.0493613481521606
Epoch 2450, training loss: 914.2037353515625 = 1.0432208776474 + 100.0 * 9.13160514831543
Epoch 2450, val loss: 1.049183964729309
Epoch 2460, training loss: 914.1268310546875 = 1.0429956912994385 + 100.0 * 9.130838394165039
Epoch 2460, val loss: 1.0489944219589233
Epoch 2470, training loss: 913.5963134765625 = 1.0427732467651367 + 100.0 * 9.125535011291504
Epoch 2470, val loss: 1.0488115549087524
Epoch 2480, training loss: 913.7416381835938 = 1.0425547361373901 + 100.0 * 9.126991271972656
Epoch 2480, val loss: 1.0486348867416382
Epoch 2490, training loss: 914.40380859375 = 1.0423520803451538 + 100.0 * 9.133614540100098
Epoch 2490, val loss: 1.048460841178894
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.20594202898550723
0.8127218720567994
=== training gcn model ===
Epoch 0, training loss: 1033.5352783203125 = 1.111130952835083 + 100.0 * 10.324241638183594
Epoch 0, val loss: 1.1105602979660034
Epoch 10, training loss: 997.7532958984375 = 1.1105443239212036 + 100.0 * 9.96642780303955
Epoch 10, val loss: 1.1099928617477417
Epoch 20, training loss: 978.6893310546875 = 1.110120415687561 + 100.0 * 9.775792121887207
Epoch 20, val loss: 1.1095415353775024
Epoch 30, training loss: 965.1072387695312 = 1.1095513105392456 + 100.0 * 9.639976501464844
Epoch 30, val loss: 1.1089524030685425
Epoch 40, training loss: 954.200439453125 = 1.1089948415756226 + 100.0 * 9.530914306640625
Epoch 40, val loss: 1.108388066291809
Epoch 50, training loss: 944.8306274414062 = 1.1083929538726807 + 100.0 * 9.437222480773926
Epoch 50, val loss: 1.107765793800354
Epoch 60, training loss: 936.748291015625 = 1.1077748537063599 + 100.0 * 9.356405258178711
Epoch 60, val loss: 1.1071285009384155
Epoch 70, training loss: 929.7691650390625 = 1.1071587800979614 + 100.0 * 9.286620140075684
Epoch 70, val loss: 1.106498122215271
Epoch 80, training loss: 923.767333984375 = 1.1065027713775635 + 100.0 * 9.226608276367188
Epoch 80, val loss: 1.1058269739151
Epoch 90, training loss: 918.5274658203125 = 1.1058361530303955 + 100.0 * 9.174216270446777
Epoch 90, val loss: 1.1051392555236816
Epoch 100, training loss: 914.1123046875 = 1.1051573753356934 + 100.0 * 9.130071640014648
Epoch 100, val loss: 1.1044485569000244
Epoch 110, training loss: 910.2235107421875 = 1.1044692993164062 + 100.0 * 9.091190338134766
Epoch 110, val loss: 1.1037484407424927
Epoch 120, training loss: 906.7596435546875 = 1.1037567853927612 + 100.0 * 9.056558609008789
Epoch 120, val loss: 1.103010654449463
Epoch 130, training loss: 903.7557983398438 = 1.1030601263046265 + 100.0 * 9.026527404785156
Epoch 130, val loss: 1.1022937297821045
Epoch 140, training loss: 900.9880981445312 = 1.1023398637771606 + 100.0 * 8.998857498168945
Epoch 140, val loss: 1.1015623807907104
Epoch 150, training loss: 898.5203247070312 = 1.101589560508728 + 100.0 * 8.974187850952148
Epoch 150, val loss: 1.1007969379425049
Epoch 160, training loss: 896.3841552734375 = 1.1008542776107788 + 100.0 * 8.95283317565918
Epoch 160, val loss: 1.1000440120697021
Epoch 170, training loss: 894.5468139648438 = 1.1000893115997314 + 100.0 * 8.934467315673828
Epoch 170, val loss: 1.099261999130249
Epoch 180, training loss: 892.9131469726562 = 1.0993157625198364 + 100.0 * 8.91813850402832
Epoch 180, val loss: 1.0984662771224976
Epoch 190, training loss: 891.47119140625 = 1.0984829664230347 + 100.0 * 8.903726577758789
Epoch 190, val loss: 1.097646713256836
Epoch 200, training loss: 890.1275024414062 = 1.09771728515625 + 100.0 * 8.890297889709473
Epoch 200, val loss: 1.0968433618545532
Epoch 210, training loss: 889.1957397460938 = 1.0969252586364746 + 100.0 * 8.880988121032715
Epoch 210, val loss: 1.0960465669631958
Epoch 220, training loss: 888.36962890625 = 1.096134901046753 + 100.0 * 8.872735023498535
Epoch 220, val loss: 1.0952296257019043
Epoch 230, training loss: 887.4885864257812 = 1.0953131914138794 + 100.0 * 8.863932609558105
Epoch 230, val loss: 1.0944066047668457
Epoch 240, training loss: 886.6873168945312 = 1.0944956541061401 + 100.0 * 8.855928421020508
Epoch 240, val loss: 1.0935841798782349
Epoch 250, training loss: 885.97998046875 = 1.093678593635559 + 100.0 * 8.848862648010254
Epoch 250, val loss: 1.0927575826644897
Epoch 260, training loss: 886.7621459960938 = 1.092889666557312 + 100.0 * 8.85669231414795
Epoch 260, val loss: 1.0919232368469238
Epoch 270, training loss: 885.5057373046875 = 1.092115044593811 + 100.0 * 8.844136238098145
Epoch 270, val loss: 1.0911648273468018
Epoch 280, training loss: 884.9439086914062 = 1.091288447380066 + 100.0 * 8.838525772094727
Epoch 280, val loss: 1.0903632640838623
Epoch 290, training loss: 883.9127807617188 = 1.0905547142028809 + 100.0 * 8.828222274780273
Epoch 290, val loss: 1.0895984172821045
Epoch 300, training loss: 884.3648071289062 = 1.089859962463379 + 100.0 * 8.832749366760254
Epoch 300, val loss: 1.088903546333313
Epoch 310, training loss: 884.0126953125 = 1.0891844034194946 + 100.0 * 8.829235076904297
Epoch 310, val loss: 1.0882279872894287
Epoch 320, training loss: 883.7189331054688 = 1.0885310173034668 + 100.0 * 8.82630443572998
Epoch 320, val loss: 1.0875712633132935
Epoch 330, training loss: 884.4249267578125 = 1.0878791809082031 + 100.0 * 8.833370208740234
Epoch 330, val loss: 1.0869051218032837
Epoch 340, training loss: 884.5977172851562 = 1.0873548984527588 + 100.0 * 8.835103988647461
Epoch 340, val loss: 1.0863938331604004
Epoch 350, training loss: 883.3402099609375 = 1.0867544412612915 + 100.0 * 8.822534561157227
Epoch 350, val loss: 1.0857614278793335
Epoch 360, training loss: 883.8994750976562 = 1.0862488746643066 + 100.0 * 8.828132629394531
Epoch 360, val loss: 1.0852701663970947
Epoch 370, training loss: 883.4599609375 = 1.0857504606246948 + 100.0 * 8.823741912841797
Epoch 370, val loss: 1.0847700834274292
Epoch 380, training loss: 883.468994140625 = 1.0852606296539307 + 100.0 * 8.823837280273438
Epoch 380, val loss: 1.0842758417129517
Epoch 390, training loss: 883.90673828125 = 1.084808111190796 + 100.0 * 8.828219413757324
Epoch 390, val loss: 1.0838218927383423
Epoch 400, training loss: 884.3090209960938 = 1.0843496322631836 + 100.0 * 8.832246780395508
Epoch 400, val loss: 1.0833561420440674
Epoch 410, training loss: 884.2062377929688 = 1.083859920501709 + 100.0 * 8.831223487854004
Epoch 410, val loss: 1.0828896760940552
Epoch 420, training loss: 884.395263671875 = 1.0834159851074219 + 100.0 * 8.833118438720703
Epoch 420, val loss: 1.0824310779571533
Epoch 430, training loss: 884.5087280273438 = 1.082953929901123 + 100.0 * 8.834258079528809
Epoch 430, val loss: 1.0819807052612305
Epoch 440, training loss: 884.7222900390625 = 1.0824840068817139 + 100.0 * 8.836398124694824
Epoch 440, val loss: 1.0815119743347168
Epoch 450, training loss: 884.7606811523438 = 1.082006573677063 + 100.0 * 8.836786270141602
Epoch 450, val loss: 1.081047773361206
Epoch 460, training loss: 885.1515502929688 = 1.0815280675888062 + 100.0 * 8.840700149536133
Epoch 460, val loss: 1.0805727243423462
Epoch 470, training loss: 885.7240600585938 = 1.0810304880142212 + 100.0 * 8.846429824829102
Epoch 470, val loss: 1.080078125
Epoch 480, training loss: 885.651123046875 = 1.080534815788269 + 100.0 * 8.84570598602295
Epoch 480, val loss: 1.079611897468567
Epoch 490, training loss: 884.93212890625 = 1.0800319910049438 + 100.0 * 8.838521003723145
Epoch 490, val loss: 1.0790832042694092
Epoch 500, training loss: 885.9642333984375 = 1.0795625448226929 + 100.0 * 8.848846435546875
Epoch 500, val loss: 1.0786106586456299
Epoch 510, training loss: 886.6318359375 = 1.0790526866912842 + 100.0 * 8.855527877807617
Epoch 510, val loss: 1.0781108140945435
Epoch 520, training loss: 886.8035278320312 = 1.0785034894943237 + 100.0 * 8.857250213623047
Epoch 520, val loss: 1.0775697231292725
Epoch 530, training loss: 886.9849243164062 = 1.0779448747634888 + 100.0 * 8.85906982421875
Epoch 530, val loss: 1.077022910118103
Epoch 540, training loss: 887.41552734375 = 1.0773667097091675 + 100.0 * 8.863381385803223
Epoch 540, val loss: 1.0764435529708862
Epoch 550, training loss: 887.6480102539062 = 1.0767781734466553 + 100.0 * 8.86571216583252
Epoch 550, val loss: 1.0758767127990723
Epoch 560, training loss: 887.8333740234375 = 1.0761586427688599 + 100.0 * 8.867571830749512
Epoch 560, val loss: 1.0752755403518677
Epoch 570, training loss: 888.3027954101562 = 1.0755401849746704 + 100.0 * 8.872272491455078
Epoch 570, val loss: 1.0746686458587646
Epoch 580, training loss: 888.0240478515625 = 1.0748883485794067 + 100.0 * 8.869491577148438
Epoch 580, val loss: 1.074045181274414
Epoch 590, training loss: 887.5985107421875 = 1.0742487907409668 + 100.0 * 8.865242958068848
Epoch 590, val loss: 1.0734295845031738
Epoch 600, training loss: 888.1131591796875 = 1.073616862297058 + 100.0 * 8.87039566040039
Epoch 600, val loss: 1.0728033781051636
Epoch 610, training loss: 888.6762084960938 = 1.0729684829711914 + 100.0 * 8.876031875610352
Epoch 610, val loss: 1.0721768140792847
Epoch 620, training loss: 888.9312744140625 = 1.0723079442977905 + 100.0 * 8.878589630126953
Epoch 620, val loss: 1.0715279579162598
Epoch 630, training loss: 888.9308471679688 = 1.0716214179992676 + 100.0 * 8.878592491149902
Epoch 630, val loss: 1.0708706378936768
Epoch 640, training loss: 889.2535400390625 = 1.070958137512207 + 100.0 * 8.88182544708252
Epoch 640, val loss: 1.0702104568481445
Epoch 650, training loss: 890.1932373046875 = 1.0702459812164307 + 100.0 * 8.891229629516602
Epoch 650, val loss: 1.0694698095321655
Epoch 660, training loss: 891.6184692382812 = 1.0696687698364258 + 100.0 * 8.905488014221191
Epoch 660, val loss: 1.0689733028411865
Epoch 670, training loss: 888.6264038085938 = 1.0688886642456055 + 100.0 * 8.875575065612793
Epoch 670, val loss: 1.068256139755249
Epoch 680, training loss: 888.91064453125 = 1.0683009624481201 + 100.0 * 8.878423690795898
Epoch 680, val loss: 1.0676177740097046
Epoch 690, training loss: 890.31396484375 = 1.067629098892212 + 100.0 * 8.892463684082031
Epoch 690, val loss: 1.0670013427734375
Epoch 700, training loss: 889.3302001953125 = 1.0668740272521973 + 100.0 * 8.882633209228516
Epoch 700, val loss: 1.0662859678268433
Epoch 710, training loss: 889.3043212890625 = 1.0661835670471191 + 100.0 * 8.882381439208984
Epoch 710, val loss: 1.065612554550171
Epoch 720, training loss: 890.7499389648438 = 1.0655499696731567 + 100.0 * 8.896843910217285
Epoch 720, val loss: 1.064992070198059
Epoch 730, training loss: 891.5630493164062 = 1.0648488998413086 + 100.0 * 8.904982566833496
Epoch 730, val loss: 1.0643221139907837
Epoch 740, training loss: 891.9970703125 = 1.0641180276870728 + 100.0 * 8.909329414367676
Epoch 740, val loss: 1.0636075735092163
Epoch 750, training loss: 892.2108764648438 = 1.063380479812622 + 100.0 * 8.91147518157959
Epoch 750, val loss: 1.0628873109817505
Epoch 760, training loss: 891.6394653320312 = 1.0626296997070312 + 100.0 * 8.905768394470215
Epoch 760, val loss: 1.0621590614318848
Epoch 770, training loss: 892.4016723632812 = 1.061907172203064 + 100.0 * 8.913397789001465
Epoch 770, val loss: 1.0614672899246216
Epoch 780, training loss: 892.8326416015625 = 1.061151385307312 + 100.0 * 8.917715072631836
Epoch 780, val loss: 1.060752034187317
Epoch 790, training loss: 892.8219604492188 = 1.0603910684585571 + 100.0 * 8.91761589050293
Epoch 790, val loss: 1.0600110292434692
Epoch 800, training loss: 893.45556640625 = 1.0596731901168823 + 100.0 * 8.923958778381348
Epoch 800, val loss: 1.0593048334121704
Epoch 810, training loss: 893.3262329101562 = 1.0589020252227783 + 100.0 * 8.922673225402832
Epoch 810, val loss: 1.0585424900054932
Epoch 820, training loss: 893.857666015625 = 1.0581282377243042 + 100.0 * 8.927995681762695
Epoch 820, val loss: 1.0578025579452515
Epoch 830, training loss: 894.25634765625 = 1.0573536157608032 + 100.0 * 8.931989669799805
Epoch 830, val loss: 1.0570664405822754
Epoch 840, training loss: 894.968017578125 = 1.0565779209136963 + 100.0 * 8.939114570617676
Epoch 840, val loss: 1.0563050508499146
Epoch 850, training loss: 894.8502807617188 = 1.0557385683059692 + 100.0 * 8.937945365905762
Epoch 850, val loss: 1.0554877519607544
Epoch 860, training loss: 894.437255859375 = 1.0549678802490234 + 100.0 * 8.933822631835938
Epoch 860, val loss: 1.0547500848770142
Epoch 870, training loss: 895.0634155273438 = 1.0541616678237915 + 100.0 * 8.940093040466309
Epoch 870, val loss: 1.0539872646331787
Epoch 880, training loss: 895.6646728515625 = 1.0533876419067383 + 100.0 * 8.946112632751465
Epoch 880, val loss: 1.0532422065734863
Epoch 890, training loss: 896.0532836914062 = 1.0525959730148315 + 100.0 * 8.950006484985352
Epoch 890, val loss: 1.0524758100509644
Epoch 900, training loss: 896.2347412109375 = 1.0517761707305908 + 100.0 * 8.95182991027832
Epoch 900, val loss: 1.0516796112060547
Epoch 910, training loss: 896.48876953125 = 1.0509592294692993 + 100.0 * 8.954378128051758
Epoch 910, val loss: 1.0508962869644165
Epoch 920, training loss: 897.6348266601562 = 1.0501370429992676 + 100.0 * 8.96584701538086
Epoch 920, val loss: 1.0501084327697754
Epoch 930, training loss: 897.1140747070312 = 1.0493149757385254 + 100.0 * 8.960647583007812
Epoch 930, val loss: 1.0493019819259644
Epoch 940, training loss: 897.3175659179688 = 1.0484776496887207 + 100.0 * 8.962691307067871
Epoch 940, val loss: 1.0485010147094727
Epoch 950, training loss: 898.132080078125 = 1.0476677417755127 + 100.0 * 8.970844268798828
Epoch 950, val loss: 1.0477169752120972
Epoch 960, training loss: 897.9358520507812 = 1.046794056892395 + 100.0 * 8.968890190124512
Epoch 960, val loss: 1.0468624830245972
Epoch 970, training loss: 898.0374755859375 = 1.0459741353988647 + 100.0 * 8.969915390014648
Epoch 970, val loss: 1.0460658073425293
Epoch 980, training loss: 898.4705810546875 = 1.0451152324676514 + 100.0 * 8.974254608154297
Epoch 980, val loss: 1.0452548265457153
Epoch 990, training loss: 898.5966186523438 = 1.0442789793014526 + 100.0 * 8.975523948669434
Epoch 990, val loss: 1.044450044631958
Epoch 1000, training loss: 898.9314575195312 = 1.0434465408325195 + 100.0 * 8.978879928588867
Epoch 1000, val loss: 1.0436333417892456
Epoch 1010, training loss: 899.1098022460938 = 1.042586326599121 + 100.0 * 8.980671882629395
Epoch 1010, val loss: 1.0428041219711304
Epoch 1020, training loss: 899.0089111328125 = 1.0417165756225586 + 100.0 * 8.9796724319458
Epoch 1020, val loss: 1.041980266571045
Epoch 1030, training loss: 899.1565551757812 = 1.0408515930175781 + 100.0 * 8.981157302856445
Epoch 1030, val loss: 1.041155219078064
Epoch 1040, training loss: 899.4954833984375 = 1.0400270223617554 + 100.0 * 8.984554290771484
Epoch 1040, val loss: 1.0403367280960083
Epoch 1050, training loss: 899.8927612304688 = 1.0391641855239868 + 100.0 * 8.98853588104248
Epoch 1050, val loss: 1.039463758468628
Epoch 1060, training loss: 901.6845703125 = 1.0383338928222656 + 100.0 * 9.006462097167969
Epoch 1060, val loss: 1.0387252569198608
Epoch 1070, training loss: 900.7064819335938 = 1.0374349355697632 + 100.0 * 8.99669075012207
Epoch 1070, val loss: 1.0378379821777344
Epoch 1080, training loss: 900.634765625 = 1.0365467071533203 + 100.0 * 8.99598217010498
Epoch 1080, val loss: 1.0369735956192017
Epoch 1090, training loss: 900.986328125 = 1.035695195198059 + 100.0 * 8.999505996704102
Epoch 1090, val loss: 1.0361603498458862
Epoch 1100, training loss: 901.5282592773438 = 1.034821629524231 + 100.0 * 9.004934310913086
Epoch 1100, val loss: 1.035308837890625
Epoch 1110, training loss: 900.9586181640625 = 1.0338813066482544 + 100.0 * 8.999247550964355
Epoch 1110, val loss: 1.0344054698944092
Epoch 1120, training loss: 900.1111450195312 = 1.0329782962799072 + 100.0 * 8.990781784057617
Epoch 1120, val loss: 1.0335414409637451
Epoch 1130, training loss: 900.6489868164062 = 1.0321013927459717 + 100.0 * 8.996169090270996
Epoch 1130, val loss: 1.0327043533325195
Epoch 1140, training loss: 901.2674560546875 = 1.0312201976776123 + 100.0 * 9.002362251281738
Epoch 1140, val loss: 1.0318495035171509
Epoch 1150, training loss: 901.70068359375 = 1.0303099155426025 + 100.0 * 9.00670337677002
Epoch 1150, val loss: 1.030975341796875
Epoch 1160, training loss: 901.9790649414062 = 1.0294119119644165 + 100.0 * 9.009496688842773
Epoch 1160, val loss: 1.0300906896591187
Epoch 1170, training loss: 902.3828125 = 1.0285027027130127 + 100.0 * 9.013543128967285
Epoch 1170, val loss: 1.02922523021698
Epoch 1180, training loss: 901.3560791015625 = 1.0274395942687988 + 100.0 * 9.003286361694336
Epoch 1180, val loss: 1.0282211303710938
Epoch 1190, training loss: 901.3402709960938 = 1.0265871286392212 + 100.0 * 9.00313663482666
Epoch 1190, val loss: 1.0273972749710083
Epoch 1200, training loss: 902.3145751953125 = 1.0257326364517212 + 100.0 * 9.012887954711914
Epoch 1200, val loss: 1.0265657901763916
Epoch 1210, training loss: 903.3440551757812 = 1.024835467338562 + 100.0 * 9.023192405700684
Epoch 1210, val loss: 1.0257079601287842
Epoch 1220, training loss: 903.5125732421875 = 1.0238879919052124 + 100.0 * 9.024887084960938
Epoch 1220, val loss: 1.0248053073883057
Epoch 1230, training loss: 903.6959228515625 = 1.0229569673538208 + 100.0 * 9.026729583740234
Epoch 1230, val loss: 1.0239261388778687
Epoch 1240, training loss: 904.0659790039062 = 1.0220139026641846 + 100.0 * 9.030439376831055
Epoch 1240, val loss: 1.0230191946029663
Epoch 1250, training loss: 904.0820922851562 = 1.021035075187683 + 100.0 * 9.030610084533691
Epoch 1250, val loss: 1.0220929384231567
Epoch 1260, training loss: 904.446533203125 = 1.0200809240341187 + 100.0 * 9.03426456451416
Epoch 1260, val loss: 1.0211842060089111
Epoch 1270, training loss: 904.0635375976562 = 1.0190352201461792 + 100.0 * 9.030445098876953
Epoch 1270, val loss: 1.0202125310897827
Epoch 1280, training loss: 904.0657348632812 = 1.018078088760376 + 100.0 * 9.030476570129395
Epoch 1280, val loss: 1.019270896911621
Epoch 1290, training loss: 904.6187133789062 = 1.0171139240264893 + 100.0 * 9.036016464233398
Epoch 1290, val loss: 1.0183440446853638
Epoch 1300, training loss: 905.4873046875 = 1.0161429643630981 + 100.0 * 9.04471206665039
Epoch 1300, val loss: 1.017424464225769
Epoch 1310, training loss: 902.1954345703125 = 1.014944076538086 + 100.0 * 9.011804580688477
Epoch 1310, val loss: 1.0162748098373413
Epoch 1320, training loss: 902.2775268554688 = 1.0139716863632202 + 100.0 * 9.012635231018066
Epoch 1320, val loss: 1.01532781124115
Epoch 1330, training loss: 903.902587890625 = 1.0130374431610107 + 100.0 * 9.028895378112793
Epoch 1330, val loss: 1.0144413709640503
Epoch 1340, training loss: 905.344482421875 = 1.012042760848999 + 100.0 * 9.04332447052002
Epoch 1340, val loss: 1.0134916305541992
Epoch 1350, training loss: 905.3280029296875 = 1.0109914541244507 + 100.0 * 9.043169975280762
Epoch 1350, val loss: 1.0125095844268799
Epoch 1360, training loss: 906.0204467773438 = 1.009986162185669 + 100.0 * 9.050104141235352
Epoch 1360, val loss: 1.0115395784378052
Epoch 1370, training loss: 906.1243896484375 = 1.0089203119277954 + 100.0 * 9.051155090332031
Epoch 1370, val loss: 1.0105462074279785
Epoch 1380, training loss: 906.4547729492188 = 1.0078703165054321 + 100.0 * 9.054469108581543
Epoch 1380, val loss: 1.0095360279083252
Epoch 1390, training loss: 906.78076171875 = 1.0068186521530151 + 100.0 * 9.0577392578125
Epoch 1390, val loss: 1.0085426568984985
Epoch 1400, training loss: 906.5626220703125 = 1.0057052373886108 + 100.0 * 9.055569648742676
Epoch 1400, val loss: 1.007493257522583
Epoch 1410, training loss: 906.6971435546875 = 1.0046268701553345 + 100.0 * 9.056924819946289
Epoch 1410, val loss: 1.0064371824264526
Epoch 1420, training loss: 907.0029907226562 = 1.0035204887390137 + 100.0 * 9.0599946975708
Epoch 1420, val loss: 1.0054078102111816
Epoch 1430, training loss: 907.1622314453125 = 1.002380132675171 + 100.0 * 9.061598777770996
Epoch 1430, val loss: 1.00434410572052
Epoch 1440, training loss: 907.2338256835938 = 1.0012961626052856 + 100.0 * 9.062325477600098
Epoch 1440, val loss: 1.0033105611801147
Epoch 1450, training loss: 907.2056274414062 = 1.0001988410949707 + 100.0 * 9.062054634094238
Epoch 1450, val loss: 1.0022826194763184
Epoch 1460, training loss: 907.04736328125 = 0.9991128444671631 + 100.0 * 9.0604829788208
Epoch 1460, val loss: 1.0012434720993042
Epoch 1470, training loss: 907.5240478515625 = 0.9980286359786987 + 100.0 * 9.06525993347168
Epoch 1470, val loss: 1.0002295970916748
Epoch 1480, training loss: 908.1613159179688 = 0.9969351291656494 + 100.0 * 9.071643829345703
Epoch 1480, val loss: 0.9991933703422546
Epoch 1490, training loss: 907.2357177734375 = 0.9957379102706909 + 100.0 * 9.062399864196777
Epoch 1490, val loss: 0.9980527758598328
Epoch 1500, training loss: 907.6011352539062 = 0.9946466088294983 + 100.0 * 9.066064834594727
Epoch 1500, val loss: 0.9970417618751526
Epoch 1510, training loss: 908.2268676757812 = 0.993529200553894 + 100.0 * 9.072333335876465
Epoch 1510, val loss: 0.9959942102432251
Epoch 1520, training loss: 908.496337890625 = 0.9924024343490601 + 100.0 * 9.07503890991211
Epoch 1520, val loss: 0.994962751865387
Epoch 1530, training loss: 909.057861328125 = 0.9913113117218018 + 100.0 * 9.080665588378906
Epoch 1530, val loss: 0.9939154386520386
Epoch 1540, training loss: 908.8510131835938 = 0.9901587963104248 + 100.0 * 9.078608512878418
Epoch 1540, val loss: 0.9928353428840637
Epoch 1550, training loss: 908.733642578125 = 0.9890158772468567 + 100.0 * 9.077445983886719
Epoch 1550, val loss: 0.9917464256286621
Epoch 1560, training loss: 908.7788696289062 = 0.9878355860710144 + 100.0 * 9.077910423278809
Epoch 1560, val loss: 0.9906677603721619
Epoch 1570, training loss: 909.3953247070312 = 0.9867619872093201 + 100.0 * 9.084085464477539
Epoch 1570, val loss: 0.9896183013916016
Epoch 1580, training loss: 908.7826538085938 = 0.9855919480323792 + 100.0 * 9.077970504760742
Epoch 1580, val loss: 0.9885181784629822
Epoch 1590, training loss: 908.833251953125 = 0.9844469428062439 + 100.0 * 9.07848834991455
Epoch 1590, val loss: 0.9874483346939087
Epoch 1600, training loss: 909.51025390625 = 0.9833450317382812 + 100.0 * 9.0852689743042
Epoch 1600, val loss: 0.9864113926887512
Epoch 1610, training loss: 910.12939453125 = 0.9822562336921692 + 100.0 * 9.091471672058105
Epoch 1610, val loss: 0.9853883981704712
Epoch 1620, training loss: 910.0277099609375 = 0.9810929894447327 + 100.0 * 9.090466499328613
Epoch 1620, val loss: 0.984291136264801
Epoch 1630, training loss: 910.2958374023438 = 0.9799564480781555 + 100.0 * 9.093158721923828
Epoch 1630, val loss: 0.9832351803779602
Epoch 1640, training loss: 910.8960571289062 = 0.9788312315940857 + 100.0 * 9.099172592163086
Epoch 1640, val loss: 0.982173502445221
Epoch 1650, training loss: 910.786865234375 = 0.9776626229286194 + 100.0 * 9.098092079162598
Epoch 1650, val loss: 0.9810623526573181
Epoch 1660, training loss: 910.8076171875 = 0.9764804840087891 + 100.0 * 9.098311424255371
Epoch 1660, val loss: 0.9799478054046631
Epoch 1670, training loss: 910.95703125 = 0.9753060936927795 + 100.0 * 9.099817276000977
Epoch 1670, val loss: 0.9788557887077332
Epoch 1680, training loss: 911.3319091796875 = 0.9740638136863708 + 100.0 * 9.103578567504883
Epoch 1680, val loss: 0.977558434009552
Epoch 1690, training loss: 895.423583984375 = 0.9717733263969421 + 100.0 * 8.944518089294434
Epoch 1690, val loss: 0.9756166934967041
Epoch 1700, training loss: 904.4407958984375 = 0.9719419479370117 + 100.0 * 9.034688949584961
Epoch 1700, val loss: 0.9758819341659546
Epoch 1710, training loss: 905.5296630859375 = 0.9710976481437683 + 100.0 * 9.045585632324219
Epoch 1710, val loss: 0.974900484085083
Epoch 1720, training loss: 907.3977661132812 = 0.9701108932495117 + 100.0 * 9.064276695251465
Epoch 1720, val loss: 0.9739320278167725
Epoch 1730, training loss: 908.6249389648438 = 0.9690569639205933 + 100.0 * 9.076559066772461
Epoch 1730, val loss: 0.9729963541030884
Epoch 1740, training loss: 909.18212890625 = 0.9679647088050842 + 100.0 * 9.082141876220703
Epoch 1740, val loss: 0.9719940423965454
Epoch 1750, training loss: 910.5672607421875 = 0.9668949246406555 + 100.0 * 9.096003532409668
Epoch 1750, val loss: 0.9709861874580383
Epoch 1760, training loss: 911.5618896484375 = 0.9658145308494568 + 100.0 * 9.105960845947266
Epoch 1760, val loss: 0.9699712991714478
Epoch 1770, training loss: 912.2658081054688 = 0.9646651148796082 + 100.0 * 9.113011360168457
Epoch 1770, val loss: 0.9688988327980042
Epoch 1780, training loss: 912.9678344726562 = 0.9635326266288757 + 100.0 * 9.12004280090332
Epoch 1780, val loss: 0.9678418040275574
Epoch 1790, training loss: 913.251953125 = 0.962356448173523 + 100.0 * 9.122896194458008
Epoch 1790, val loss: 0.9667505025863647
Epoch 1800, training loss: 913.3283081054688 = 0.961193859577179 + 100.0 * 9.123671531677246
Epoch 1800, val loss: 0.9656646251678467
Epoch 1810, training loss: 913.9329223632812 = 0.9600547552108765 + 100.0 * 9.129728317260742
Epoch 1810, val loss: 0.9645858407020569
Epoch 1820, training loss: 914.5848388671875 = 0.9589031338691711 + 100.0 * 9.136259078979492
Epoch 1820, val loss: 0.9635216593742371
Epoch 1830, training loss: 914.7435302734375 = 0.9577438831329346 + 100.0 * 9.137857437133789
Epoch 1830, val loss: 0.9624508619308472
Epoch 1840, training loss: 914.9574584960938 = 0.9565732479095459 + 100.0 * 9.140008926391602
Epoch 1840, val loss: 0.9613497257232666
Epoch 1850, training loss: 915.0928344726562 = 0.955398678779602 + 100.0 * 9.141374588012695
Epoch 1850, val loss: 0.9602512121200562
Epoch 1860, training loss: 915.638671875 = 0.954277753829956 + 100.0 * 9.146843910217285
Epoch 1860, val loss: 0.9591943621635437
Epoch 1870, training loss: 915.885498046875 = 0.9531177282333374 + 100.0 * 9.149323463439941
Epoch 1870, val loss: 0.9581155776977539
Epoch 1880, training loss: 915.8768920898438 = 0.951936662197113 + 100.0 * 9.149249076843262
Epoch 1880, val loss: 0.9570321440696716
Epoch 1890, training loss: 916.3504638671875 = 0.9507930278778076 + 100.0 * 9.153996467590332
Epoch 1890, val loss: 0.9559620022773743
Epoch 1900, training loss: 916.4359741210938 = 0.9496378898620605 + 100.0 * 9.154863357543945
Epoch 1900, val loss: 0.9548833966255188
Epoch 1910, training loss: 915.7806396484375 = 0.9484301805496216 + 100.0 * 9.148322105407715
Epoch 1910, val loss: 0.9537624716758728
Epoch 1920, training loss: 916.1162109375 = 0.9472930431365967 + 100.0 * 9.151689529418945
Epoch 1920, val loss: 0.9527045488357544
Epoch 1930, training loss: 916.8485107421875 = 0.9461509585380554 + 100.0 * 9.15902328491211
Epoch 1930, val loss: 0.9516658186912537
Epoch 1940, training loss: 917.2427978515625 = 0.945023238658905 + 100.0 * 9.162978172302246
Epoch 1940, val loss: 0.9506037831306458
Epoch 1950, training loss: 916.2852783203125 = 0.9437961578369141 + 100.0 * 9.153414726257324
Epoch 1950, val loss: 0.9494344592094421
Epoch 1960, training loss: 916.7955322265625 = 0.9426589608192444 + 100.0 * 9.158528327941895
Epoch 1960, val loss: 0.9484038352966309
Epoch 1970, training loss: 917.4017333984375 = 0.9415274858474731 + 100.0 * 9.164602279663086
Epoch 1970, val loss: 0.9473717212677002
Epoch 1980, training loss: 917.3565063476562 = 0.9403769373893738 + 100.0 * 9.164161682128906
Epoch 1980, val loss: 0.9463047981262207
Epoch 1990, training loss: 917.930908203125 = 0.9392342567443848 + 100.0 * 9.169917106628418
Epoch 1990, val loss: 0.9452596306800842
Epoch 2000, training loss: 918.2215576171875 = 0.9381016492843628 + 100.0 * 9.172834396362305
Epoch 2000, val loss: 0.9442009925842285
Epoch 2010, training loss: 918.090576171875 = 0.9369440078735352 + 100.0 * 9.171536445617676
Epoch 2010, val loss: 0.9431168437004089
Epoch 2020, training loss: 917.3694458007812 = 0.935745120048523 + 100.0 * 9.164337158203125
Epoch 2020, val loss: 0.9420188069343567
Epoch 2030, training loss: 917.2678833007812 = 0.9345788955688477 + 100.0 * 9.16333293914795
Epoch 2030, val loss: 0.9409216046333313
Epoch 2040, training loss: 917.6024780273438 = 0.9334865808486938 + 100.0 * 9.1666898727417
Epoch 2040, val loss: 0.9399060010910034
Epoch 2050, training loss: 918.4811401367188 = 0.9323695302009583 + 100.0 * 9.175487518310547
Epoch 2050, val loss: 0.9389001727104187
Epoch 2060, training loss: 918.0060424804688 = 0.93125319480896 + 100.0 * 9.170747756958008
Epoch 2060, val loss: 0.937842607498169
Epoch 2070, training loss: 918.5968627929688 = 0.9301416277885437 + 100.0 * 9.176667213439941
Epoch 2070, val loss: 0.9368314146995544
Epoch 2080, training loss: 918.5274658203125 = 0.9289982914924622 + 100.0 * 9.175984382629395
Epoch 2080, val loss: 0.9357580542564392
Epoch 2090, training loss: 917.6197509765625 = 0.9278628826141357 + 100.0 * 9.166918754577637
Epoch 2090, val loss: 0.9346906542778015
Epoch 2100, training loss: 918.3349609375 = 0.926781952381134 + 100.0 * 9.174081802368164
Epoch 2100, val loss: 0.933707058429718
Epoch 2110, training loss: 918.3098754882812 = 0.9256790280342102 + 100.0 * 9.173842430114746
Epoch 2110, val loss: 0.9326966404914856
Epoch 2120, training loss: 918.0406494140625 = 0.9245843887329102 + 100.0 * 9.171160697937012
Epoch 2120, val loss: 0.9316582679748535
Epoch 2130, training loss: 918.510986328125 = 0.9235076904296875 + 100.0 * 9.175874710083008
Epoch 2130, val loss: 0.9307072758674622
Epoch 2140, training loss: 914.1532592773438 = 0.9222033023834229 + 100.0 * 9.13231086730957
Epoch 2140, val loss: 0.9293486475944519
Epoch 2150, training loss: 919.0030517578125 = 0.9212951064109802 + 100.0 * 9.180817604064941
Epoch 2150, val loss: 0.9286390542984009
Epoch 2160, training loss: 916.7103881835938 = 0.9200559258460999 + 100.0 * 9.157903671264648
Epoch 2160, val loss: 0.9275506138801575
Epoch 2170, training loss: 916.9205932617188 = 0.9190132021903992 + 100.0 * 9.160016059875488
Epoch 2170, val loss: 0.926565945148468
Epoch 2180, training loss: 918.9082641601562 = 0.9180147051811218 + 100.0 * 9.179902076721191
Epoch 2180, val loss: 0.9256548881530762
Epoch 2190, training loss: 919.7908325195312 = 0.9170272350311279 + 100.0 * 9.188737869262695
Epoch 2190, val loss: 0.9247251152992249
Epoch 2200, training loss: 920.3831787109375 = 0.91599440574646 + 100.0 * 9.194671630859375
Epoch 2200, val loss: 0.923800528049469
Epoch 2210, training loss: 920.8902587890625 = 0.9149796366691589 + 100.0 * 9.199752807617188
Epoch 2210, val loss: 0.9228717088699341
Epoch 2220, training loss: 921.4463500976562 = 0.913942813873291 + 100.0 * 9.205324172973633
Epoch 2220, val loss: 0.921948254108429
Epoch 2230, training loss: 921.68212890625 = 0.9128971695899963 + 100.0 * 9.20769214630127
Epoch 2230, val loss: 0.9209914803504944
Epoch 2240, training loss: 922.0040893554688 = 0.9118443727493286 + 100.0 * 9.210922241210938
Epoch 2240, val loss: 0.9200477600097656
Epoch 2250, training loss: 922.2278442382812 = 0.9108026027679443 + 100.0 * 9.213170051574707
Epoch 2250, val loss: 0.919104278087616
Epoch 2260, training loss: 922.1692504882812 = 0.9097527265548706 + 100.0 * 9.212594985961914
Epoch 2260, val loss: 0.9181565046310425
Epoch 2270, training loss: 922.5904541015625 = 0.9087381362915039 + 100.0 * 9.216816902160645
Epoch 2270, val loss: 0.917230486869812
Epoch 2280, training loss: 922.8240356445312 = 0.9076945185661316 + 100.0 * 9.219162940979004
Epoch 2280, val loss: 0.9163026809692383
Epoch 2290, training loss: 922.81494140625 = 0.9066674709320068 + 100.0 * 9.219082832336426
Epoch 2290, val loss: 0.9153595566749573
Epoch 2300, training loss: 923.2684326171875 = 0.9056566953659058 + 100.0 * 9.223628044128418
Epoch 2300, val loss: 0.9144635796546936
Epoch 2310, training loss: 923.6824340820312 = 0.9046494364738464 + 100.0 * 9.227777481079102
Epoch 2310, val loss: 0.9135520458221436
Epoch 2320, training loss: 923.5088500976562 = 0.9036335349082947 + 100.0 * 9.226052284240723
Epoch 2320, val loss: 0.9126322865486145
Epoch 2330, training loss: 924.078857421875 = 0.9026402831077576 + 100.0 * 9.231761932373047
Epoch 2330, val loss: 0.9117512106895447
Epoch 2340, training loss: 923.9204711914062 = 0.9016352295875549 + 100.0 * 9.230188369750977
Epoch 2340, val loss: 0.9108437895774841
Epoch 2350, training loss: 923.3802490234375 = 0.9006150960922241 + 100.0 * 9.224796295166016
Epoch 2350, val loss: 0.9099228382110596
Epoch 2360, training loss: 924.0616455078125 = 0.8996344208717346 + 100.0 * 9.231619834899902
Epoch 2360, val loss: 0.9090538620948792
Epoch 2370, training loss: 924.113037109375 = 0.898645281791687 + 100.0 * 9.232144355773926
Epoch 2370, val loss: 0.9081665873527527
Epoch 2380, training loss: 924.5723876953125 = 0.8976810574531555 + 100.0 * 9.236746788024902
Epoch 2380, val loss: 0.9073039889335632
Epoch 2390, training loss: 924.34814453125 = 0.8966997265815735 + 100.0 * 9.234514236450195
Epoch 2390, val loss: 0.9064379930496216
Epoch 2400, training loss: 924.5148315429688 = 0.8957384824752808 + 100.0 * 9.236190795898438
Epoch 2400, val loss: 0.905576765537262
Epoch 2410, training loss: 925.0993041992188 = 0.8947916030883789 + 100.0 * 9.242045402526855
Epoch 2410, val loss: 0.9047128558158875
Epoch 2420, training loss: 925.3533935546875 = 0.8938345313072205 + 100.0 * 9.244595527648926
Epoch 2420, val loss: 0.9038780927658081
Epoch 2430, training loss: 925.3897094726562 = 0.8928879499435425 + 100.0 * 9.24496841430664
Epoch 2430, val loss: 0.9030152559280396
Epoch 2440, training loss: 924.9419555664062 = 0.8919144868850708 + 100.0 * 9.240500450134277
Epoch 2440, val loss: 0.9021494388580322
Epoch 2450, training loss: 923.9598388671875 = 0.8909227848052979 + 100.0 * 9.23068904876709
Epoch 2450, val loss: 0.9012781381607056
Epoch 2460, training loss: 922.7485961914062 = 0.8899126052856445 + 100.0 * 9.218586921691895
Epoch 2460, val loss: 0.9003974795341492
Epoch 2470, training loss: 922.9771728515625 = 0.8890107870101929 + 100.0 * 9.220881462097168
Epoch 2470, val loss: 0.8996025919914246
Epoch 2480, training loss: 923.9706420898438 = 0.8881725072860718 + 100.0 * 9.23082447052002
Epoch 2480, val loss: 0.898867130279541
Epoch 2490, training loss: 925.0394287109375 = 0.88731849193573 + 100.0 * 9.241520881652832
Epoch 2490, val loss: 0.8981015086174011
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.5392753623188405
0.8164891690212274
The final CL Acc:0.42812, 0.15710, The final GNN Acc:0.81429, 0.00160
raw graph: torch.Size([2, 88648])
add edge: torch.Size([2, 110722])
remove edge: torch.Size([2, 66718])
updated graph: torch.Size([2, 88792])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 1028.150390625 = 1.0944076776504517 + 100.0 * 10.270560264587402
Epoch 0, val loss: 1.0933729410171509
Epoch 10, training loss: 987.8297119140625 = 1.0943207740783691 + 100.0 * 9.867354393005371
Epoch 10, val loss: 1.0932737588882446
Epoch 20, training loss: 966.796875 = 1.094249963760376 + 100.0 * 9.657026290893555
Epoch 20, val loss: 1.09319007396698
Epoch 30, training loss: 950.44140625 = 1.0941598415374756 + 100.0 * 9.4934720993042
Epoch 30, val loss: 1.0930753946304321
Epoch 40, training loss: 937.6895751953125 = 1.094057559967041 + 100.0 * 9.365955352783203
Epoch 40, val loss: 1.0929597616195679
Epoch 50, training loss: 927.4977416992188 = 1.0939416885375977 + 100.0 * 9.2640380859375
Epoch 50, val loss: 1.09282648563385
Epoch 60, training loss: 919.1434326171875 = 1.0938153266906738 + 100.0 * 9.180496215820312
Epoch 60, val loss: 1.092685341835022
Epoch 70, training loss: 912.1895141601562 = 1.09366774559021 + 100.0 * 9.110958099365234
Epoch 70, val loss: 1.0925236940383911
Epoch 80, training loss: 906.3859252929688 = 1.09348464012146 + 100.0 * 9.052924156188965
Epoch 80, val loss: 1.0923268795013428
Epoch 90, training loss: 901.514892578125 = 1.093285083770752 + 100.0 * 9.004216194152832
Epoch 90, val loss: 1.0921120643615723
Epoch 100, training loss: 897.4055786132812 = 1.0930694341659546 + 100.0 * 8.963125228881836
Epoch 100, val loss: 1.0918854475021362
Epoch 110, training loss: 893.9795532226562 = 1.092843770980835 + 100.0 * 8.92886734008789
Epoch 110, val loss: 1.091644525527954
Epoch 120, training loss: 890.8928833007812 = 1.0925965309143066 + 100.0 * 8.898002624511719
Epoch 120, val loss: 1.0913728475570679
Epoch 130, training loss: 888.6055908203125 = 1.0921823978424072 + 100.0 * 8.875134468078613
Epoch 130, val loss: 1.0909600257873535
Epoch 140, training loss: 886.3111572265625 = 1.0918359756469727 + 100.0 * 8.852192878723145
Epoch 140, val loss: 1.0905944108963013
Epoch 150, training loss: 884.3828735351562 = 1.0914597511291504 + 100.0 * 8.832914352416992
Epoch 150, val loss: 1.0902183055877686
Epoch 160, training loss: 882.5859375 = 1.091074824333191 + 100.0 * 8.814949035644531
Epoch 160, val loss: 1.0898261070251465
Epoch 170, training loss: 881.2924194335938 = 1.090697169303894 + 100.0 * 8.802017211914062
Epoch 170, val loss: 1.089442491531372
Epoch 180, training loss: 879.9339599609375 = 1.090290904045105 + 100.0 * 8.788436889648438
Epoch 180, val loss: 1.089030385017395
Epoch 190, training loss: 878.9600830078125 = 1.0898762941360474 + 100.0 * 8.778701782226562
Epoch 190, val loss: 1.0886112451553345
Epoch 200, training loss: 878.1712646484375 = 1.089436411857605 + 100.0 * 8.770818710327148
Epoch 200, val loss: 1.0881630182266235
Epoch 210, training loss: 877.4324340820312 = 1.0889766216278076 + 100.0 * 8.763434410095215
Epoch 210, val loss: 1.0877007246017456
Epoch 220, training loss: 876.4774169921875 = 1.0885008573532104 + 100.0 * 8.753889083862305
Epoch 220, val loss: 1.0872222185134888
Epoch 230, training loss: 876.1910400390625 = 1.088009238243103 + 100.0 * 8.751029968261719
Epoch 230, val loss: 1.086729884147644
Epoch 240, training loss: 875.1644897460938 = 1.0874977111816406 + 100.0 * 8.740769386291504
Epoch 240, val loss: 1.0862177610397339
Epoch 250, training loss: 874.7396240234375 = 1.0869896411895752 + 100.0 * 8.736526489257812
Epoch 250, val loss: 1.08571457862854
Epoch 260, training loss: 874.3428344726562 = 1.086437702178955 + 100.0 * 8.732563972473145
Epoch 260, val loss: 1.0851620435714722
Epoch 270, training loss: 874.4066772460938 = 1.0858935117721558 + 100.0 * 8.733207702636719
Epoch 270, val loss: 1.0846192836761475
Epoch 280, training loss: 874.0421752929688 = 1.0853201150894165 + 100.0 * 8.729568481445312
Epoch 280, val loss: 1.0840506553649902
Epoch 290, training loss: 873.7759399414062 = 1.084736704826355 + 100.0 * 8.726912498474121
Epoch 290, val loss: 1.0834728479385376
Epoch 300, training loss: 873.7557373046875 = 1.0841243267059326 + 100.0 * 8.726716041564941
Epoch 300, val loss: 1.082864761352539
Epoch 310, training loss: 873.3875732421875 = 1.0834934711456299 + 100.0 * 8.723040580749512
Epoch 310, val loss: 1.0822508335113525
Epoch 320, training loss: 873.4732666015625 = 1.0828524827957153 + 100.0 * 8.723904609680176
Epoch 320, val loss: 1.0816137790679932
Epoch 330, training loss: 873.5072021484375 = 1.0822011232376099 + 100.0 * 8.724249839782715
Epoch 330, val loss: 1.0809669494628906
Epoch 340, training loss: 873.2630615234375 = 1.0815426111221313 + 100.0 * 8.72181510925293
Epoch 340, val loss: 1.0803265571594238
Epoch 350, training loss: 873.3235473632812 = 1.0808030366897583 + 100.0 * 8.722427368164062
Epoch 350, val loss: 1.0796114206314087
Epoch 360, training loss: 873.0516357421875 = 1.0801340341567993 + 100.0 * 8.719715118408203
Epoch 360, val loss: 1.078934907913208
Epoch 370, training loss: 873.294189453125 = 1.0793941020965576 + 100.0 * 8.722147941589355
Epoch 370, val loss: 1.0782285928726196
Epoch 380, training loss: 873.1697998046875 = 1.0786620378494263 + 100.0 * 8.720911026000977
Epoch 380, val loss: 1.077523112297058
Epoch 390, training loss: 872.7421875 = 1.0779039859771729 + 100.0 * 8.716643333435059
Epoch 390, val loss: 1.0767712593078613
Epoch 400, training loss: 872.9641723632812 = 1.0771642923355103 + 100.0 * 8.718870162963867
Epoch 400, val loss: 1.0760464668273926
Epoch 410, training loss: 873.6207275390625 = 1.0763956308364868 + 100.0 * 8.725442886352539
Epoch 410, val loss: 1.0752882957458496
Epoch 420, training loss: 873.7014770507812 = 1.0756045579910278 + 100.0 * 8.726258277893066
Epoch 420, val loss: 1.0745291709899902
Epoch 430, training loss: 873.8656616210938 = 1.074774146080017 + 100.0 * 8.727909088134766
Epoch 430, val loss: 1.0737208127975464
Epoch 440, training loss: 874.0394897460938 = 1.0739660263061523 + 100.0 * 8.729655265808105
Epoch 440, val loss: 1.0729360580444336
Epoch 450, training loss: 873.8894653320312 = 1.0731383562088013 + 100.0 * 8.728163719177246
Epoch 450, val loss: 1.0721330642700195
Epoch 460, training loss: 874.184814453125 = 1.072302222251892 + 100.0 * 8.731124877929688
Epoch 460, val loss: 1.071334958076477
Epoch 470, training loss: 874.65966796875 = 1.071468472480774 + 100.0 * 8.735881805419922
Epoch 470, val loss: 1.0705187320709229
Epoch 480, training loss: 874.3489379882812 = 1.0706055164337158 + 100.0 * 8.732783317565918
Epoch 480, val loss: 1.0696871280670166
Epoch 490, training loss: 874.563720703125 = 1.06975257396698 + 100.0 * 8.734939575195312
Epoch 490, val loss: 1.068859577178955
Epoch 500, training loss: 874.7687377929688 = 1.068885326385498 + 100.0 * 8.736998558044434
Epoch 500, val loss: 1.0680274963378906
Epoch 510, training loss: 874.7745361328125 = 1.0680011510849 + 100.0 * 8.737065315246582
Epoch 510, val loss: 1.0671809911727905
Epoch 520, training loss: 875.1810913085938 = 1.067121982574463 + 100.0 * 8.74113941192627
Epoch 520, val loss: 1.0663267374038696
Epoch 530, training loss: 875.3478393554688 = 1.0662251710891724 + 100.0 * 8.742815971374512
Epoch 530, val loss: 1.0654608011245728
Epoch 540, training loss: 875.6304931640625 = 1.0652707815170288 + 100.0 * 8.745652198791504
Epoch 540, val loss: 1.064549207687378
Epoch 550, training loss: 875.6827392578125 = 1.064363718032837 + 100.0 * 8.746183395385742
Epoch 550, val loss: 1.0636892318725586
Epoch 560, training loss: 875.7431030273438 = 1.0634294748306274 + 100.0 * 8.746796607971191
Epoch 560, val loss: 1.0627845525741577
Epoch 570, training loss: 875.7811279296875 = 1.0624525547027588 + 100.0 * 8.747186660766602
Epoch 570, val loss: 1.0618535280227661
Epoch 580, training loss: 876.0015869140625 = 1.0614432096481323 + 100.0 * 8.749401092529297
Epoch 580, val loss: 1.060901165008545
Epoch 590, training loss: 876.1425170898438 = 1.0604259967803955 + 100.0 * 8.750821113586426
Epoch 590, val loss: 1.0599250793457031
Epoch 600, training loss: 876.78076171875 = 1.0593998432159424 + 100.0 * 8.757213592529297
Epoch 600, val loss: 1.0589529275894165
Epoch 610, training loss: 877.079833984375 = 1.0583477020263672 + 100.0 * 8.760214805603027
Epoch 610, val loss: 1.0579463243484497
Epoch 620, training loss: 876.8544921875 = 1.0572634935379028 + 100.0 * 8.757972717285156
Epoch 620, val loss: 1.0569114685058594
Epoch 630, training loss: 877.3062744140625 = 1.0562000274658203 + 100.0 * 8.762500762939453
Epoch 630, val loss: 1.055894374847412
Epoch 640, training loss: 877.132568359375 = 1.055120825767517 + 100.0 * 8.760774612426758
Epoch 640, val loss: 1.054862141609192
Epoch 650, training loss: 877.8123779296875 = 1.054003119468689 + 100.0 * 8.767583847045898
Epoch 650, val loss: 1.0538055896759033
Epoch 660, training loss: 878.1500244140625 = 1.0528979301452637 + 100.0 * 8.770971298217773
Epoch 660, val loss: 1.0527360439300537
Epoch 670, training loss: 878.3201904296875 = 1.0517691373825073 + 100.0 * 8.772684097290039
Epoch 670, val loss: 1.051654577255249
Epoch 680, training loss: 878.3377075195312 = 1.050594687461853 + 100.0 * 8.772871017456055
Epoch 680, val loss: 1.050528883934021
Epoch 690, training loss: 878.336669921875 = 1.0493905544281006 + 100.0 * 8.772872924804688
Epoch 690, val loss: 1.0493443012237549
Epoch 700, training loss: 878.1468505859375 = 1.0482171773910522 + 100.0 * 8.770986557006836
Epoch 700, val loss: 1.0482617616653442
Epoch 710, training loss: 878.4924926757812 = 1.047066569328308 + 100.0 * 8.774454116821289
Epoch 710, val loss: 1.047148585319519
Epoch 720, training loss: 878.79150390625 = 1.0459281206130981 + 100.0 * 8.777456283569336
Epoch 720, val loss: 1.0460302829742432
Epoch 730, training loss: 878.9635620117188 = 1.04473078250885 + 100.0 * 8.77918815612793
Epoch 730, val loss: 1.0448857545852661
Epoch 740, training loss: 879.0946044921875 = 1.0435190200805664 + 100.0 * 8.780510902404785
Epoch 740, val loss: 1.0437253713607788
Epoch 750, training loss: 879.2984008789062 = 1.0423176288604736 + 100.0 * 8.782561302185059
Epoch 750, val loss: 1.0425714254379272
Epoch 760, training loss: 879.4210815429688 = 1.0411072969436646 + 100.0 * 8.783799171447754
Epoch 760, val loss: 1.0414249897003174
Epoch 770, training loss: 879.9879150390625 = 1.039947271347046 + 100.0 * 8.789480209350586
Epoch 770, val loss: 1.0402929782867432
Epoch 780, training loss: 880.606689453125 = 1.0387803316116333 + 100.0 * 8.795679092407227
Epoch 780, val loss: 1.0391621589660645
Epoch 790, training loss: 881.0746459960938 = 1.0375956296920776 + 100.0 * 8.800370216369629
Epoch 790, val loss: 1.0380094051361084
Epoch 800, training loss: 880.7876586914062 = 1.0363779067993164 + 100.0 * 8.797513008117676
Epoch 800, val loss: 1.0368342399597168
Epoch 810, training loss: 880.7977294921875 = 1.0351816415786743 + 100.0 * 8.797625541687012
Epoch 810, val loss: 1.0357160568237305
Epoch 820, training loss: 881.6017456054688 = 1.034014105796814 + 100.0 * 8.80567741394043
Epoch 820, val loss: 1.034566044807434
Epoch 830, training loss: 881.9575805664062 = 1.0327630043029785 + 100.0 * 8.809247970581055
Epoch 830, val loss: 1.0333497524261475
Epoch 840, training loss: 881.6990356445312 = 1.0315815210342407 + 100.0 * 8.80667495727539
Epoch 840, val loss: 1.0322136878967285
Epoch 850, training loss: 881.7843017578125 = 1.0303962230682373 + 100.0 * 8.807538986206055
Epoch 850, val loss: 1.0310637950897217
Epoch 860, training loss: 882.520263671875 = 1.029198169708252 + 100.0 * 8.814910888671875
Epoch 860, val loss: 1.0299404859542847
Epoch 870, training loss: 882.6642456054688 = 1.0280288457870483 + 100.0 * 8.816362380981445
Epoch 870, val loss: 1.0288013219833374
Epoch 880, training loss: 880.6246337890625 = 1.02670156955719 + 100.0 * 8.795979499816895
Epoch 880, val loss: 1.0275578498840332
Epoch 890, training loss: 880.4580078125 = 1.0254453420639038 + 100.0 * 8.794325828552246
Epoch 890, val loss: 1.026302695274353
Epoch 900, training loss: 886.1651611328125 = 1.0245649814605713 + 100.0 * 8.85140609741211
Epoch 900, val loss: 1.0254889726638794
Epoch 910, training loss: 880.9442138671875 = 1.023114800453186 + 100.0 * 8.799210548400879
Epoch 910, val loss: 1.0241104364395142
Epoch 920, training loss: 883.01171875 = 1.022035002708435 + 100.0 * 8.819896697998047
Epoch 920, val loss: 1.0230724811553955
Epoch 930, training loss: 882.55224609375 = 1.0208258628845215 + 100.0 * 8.815314292907715
Epoch 930, val loss: 1.0219250917434692
Epoch 940, training loss: 883.5470581054688 = 1.0197209119796753 + 100.0 * 8.825273513793945
Epoch 940, val loss: 1.0208396911621094
Epoch 950, training loss: 884.1377563476562 = 1.0185292959213257 + 100.0 * 8.831192016601562
Epoch 950, val loss: 1.0197064876556396
Epoch 960, training loss: 883.5679931640625 = 1.0172321796417236 + 100.0 * 8.825508117675781
Epoch 960, val loss: 1.0184688568115234
Epoch 970, training loss: 884.2282104492188 = 1.0160527229309082 + 100.0 * 8.832121849060059
Epoch 970, val loss: 1.0173380374908447
Epoch 980, training loss: 885.0453491210938 = 1.014874815940857 + 100.0 * 8.840304374694824
Epoch 980, val loss: 1.0161869525909424
Epoch 990, training loss: 884.8411865234375 = 1.013633370399475 + 100.0 * 8.838275909423828
Epoch 990, val loss: 1.0150115489959717
Epoch 1000, training loss: 885.668701171875 = 1.012448787689209 + 100.0 * 8.846562385559082
Epoch 1000, val loss: 1.0138529539108276
Epoch 1010, training loss: 886.1001586914062 = 1.0112029314041138 + 100.0 * 8.850889205932617
Epoch 1010, val loss: 1.0126678943634033
Epoch 1020, training loss: 886.1151733398438 = 1.0099434852600098 + 100.0 * 8.851052284240723
Epoch 1020, val loss: 1.0114555358886719
Epoch 1030, training loss: 886.4603881835938 = 1.0086373090744019 + 100.0 * 8.854516983032227
Epoch 1030, val loss: 1.010197639465332
Epoch 1040, training loss: 886.9948120117188 = 1.0072853565216064 + 100.0 * 8.859875679016113
Epoch 1040, val loss: 1.0088633298873901
Epoch 1050, training loss: 886.6429443359375 = 1.005867838859558 + 100.0 * 8.85637092590332
Epoch 1050, val loss: 1.0075085163116455
Epoch 1060, training loss: 887.0941772460938 = 1.004392147064209 + 100.0 * 8.8608980178833
Epoch 1060, val loss: 1.0061042308807373
Epoch 1070, training loss: 887.3309936523438 = 1.0028927326202393 + 100.0 * 8.86328125
Epoch 1070, val loss: 1.00462007522583
Epoch 1080, training loss: 887.6137084960938 = 1.001373529434204 + 100.0 * 8.86612319946289
Epoch 1080, val loss: 1.003171682357788
Epoch 1090, training loss: 888.38623046875 = 0.9998324513435364 + 100.0 * 8.87386417388916
Epoch 1090, val loss: 1.0016798973083496
Epoch 1100, training loss: 888.2026977539062 = 0.9982315897941589 + 100.0 * 8.872044563293457
Epoch 1100, val loss: 1.000114917755127
Epoch 1110, training loss: 888.2376098632812 = 0.9967180490493774 + 100.0 * 8.872408866882324
Epoch 1110, val loss: 0.998688817024231
Epoch 1120, training loss: 888.8670654296875 = 0.9952166676521301 + 100.0 * 8.878718376159668
Epoch 1120, val loss: 0.9972327351570129
Epoch 1130, training loss: 888.9885864257812 = 0.9936689734458923 + 100.0 * 8.879949569702148
Epoch 1130, val loss: 0.9957557916641235
Epoch 1140, training loss: 889.2767333984375 = 0.9921315908432007 + 100.0 * 8.882845878601074
Epoch 1140, val loss: 0.9942625164985657
Epoch 1150, training loss: 889.2105102539062 = 0.9905714392662048 + 100.0 * 8.88219928741455
Epoch 1150, val loss: 0.9927734136581421
Epoch 1160, training loss: 889.5698852539062 = 0.9890056848526001 + 100.0 * 8.885808944702148
Epoch 1160, val loss: 0.9912633895874023
Epoch 1170, training loss: 889.8563842773438 = 0.9874600172042847 + 100.0 * 8.888689041137695
Epoch 1170, val loss: 0.9897869229316711
Epoch 1180, training loss: 890.0880737304688 = 0.9859941005706787 + 100.0 * 8.891020774841309
Epoch 1180, val loss: 0.9883225560188293
Epoch 1190, training loss: 889.3145751953125 = 0.9841811656951904 + 100.0 * 8.88330364227295
Epoch 1190, val loss: 0.9866116642951965
Epoch 1200, training loss: 886.4634399414062 = 0.9827194213867188 + 100.0 * 8.854806900024414
Epoch 1200, val loss: 0.9852132201194763
Epoch 1210, training loss: 887.8147583007812 = 0.9812315106391907 + 100.0 * 8.868335723876953
Epoch 1210, val loss: 0.9837977290153503
Epoch 1220, training loss: 887.92919921875 = 0.9796575307846069 + 100.0 * 8.869495391845703
Epoch 1220, val loss: 0.9822659492492676
Epoch 1230, training loss: 889.1272583007812 = 0.9780780673027039 + 100.0 * 8.881491661071777
Epoch 1230, val loss: 0.980749249458313
Epoch 1240, training loss: 890.225830078125 = 0.9764871597290039 + 100.0 * 8.89249324798584
Epoch 1240, val loss: 0.9792105555534363
Epoch 1250, training loss: 890.4387817382812 = 0.9748712778091431 + 100.0 * 8.894639015197754
Epoch 1250, val loss: 0.9776497483253479
Epoch 1260, training loss: 890.8672485351562 = 0.973242461681366 + 100.0 * 8.898940086364746
Epoch 1260, val loss: 0.9760869741439819
Epoch 1270, training loss: 890.7508544921875 = 0.9715846180915833 + 100.0 * 8.89779281616211
Epoch 1270, val loss: 0.9744776487350464
Epoch 1280, training loss: 891.3417358398438 = 0.9700124859809875 + 100.0 * 8.903717041015625
Epoch 1280, val loss: 0.9729647636413574
Epoch 1290, training loss: 891.8059692382812 = 0.9683768153190613 + 100.0 * 8.90837574005127
Epoch 1290, val loss: 0.9713820219039917
Epoch 1300, training loss: 892.1640625 = 0.9667356610298157 + 100.0 * 8.911972999572754
Epoch 1300, val loss: 0.9697908163070679
Epoch 1310, training loss: 891.9635620117188 = 0.9650326371192932 + 100.0 * 8.909985542297363
Epoch 1310, val loss: 0.9681380391120911
Epoch 1320, training loss: 892.0521240234375 = 0.963350236415863 + 100.0 * 8.910887718200684
Epoch 1320, val loss: 0.9665258526802063
Epoch 1330, training loss: 892.4613647460938 = 0.9617139101028442 + 100.0 * 8.914996147155762
Epoch 1330, val loss: 0.9649521708488464
Epoch 1340, training loss: 892.4010009765625 = 0.9600276350975037 + 100.0 * 8.914409637451172
Epoch 1340, val loss: 0.9633331298828125
Epoch 1350, training loss: 892.7579956054688 = 0.9583538174629211 + 100.0 * 8.917996406555176
Epoch 1350, val loss: 0.9617235064506531
Epoch 1360, training loss: 892.9298706054688 = 0.9567055106163025 + 100.0 * 8.919731140136719
Epoch 1360, val loss: 0.9601290225982666
Epoch 1370, training loss: 893.0595703125 = 0.9550264477729797 + 100.0 * 8.921045303344727
Epoch 1370, val loss: 0.9585182666778564
Epoch 1380, training loss: 893.1365966796875 = 0.9533000588417053 + 100.0 * 8.921833038330078
Epoch 1380, val loss: 0.9568734765052795
Epoch 1390, training loss: 892.5515747070312 = 0.9515646696090698 + 100.0 * 8.916000366210938
Epoch 1390, val loss: 0.9551832675933838
Epoch 1400, training loss: 892.9064331054688 = 0.9498615860939026 + 100.0 * 8.919565200805664
Epoch 1400, val loss: 0.9535291790962219
Epoch 1410, training loss: 893.4959716796875 = 0.9481778144836426 + 100.0 * 8.925477981567383
Epoch 1410, val loss: 0.9519408345222473
Epoch 1420, training loss: 894.3062133789062 = 0.9464951753616333 + 100.0 * 8.933597564697266
Epoch 1420, val loss: 0.9502955079078674
Epoch 1430, training loss: 893.958984375 = 0.944744884967804 + 100.0 * 8.930142402648926
Epoch 1430, val loss: 0.9486181735992432
Epoch 1440, training loss: 894.5835571289062 = 0.9430283904075623 + 100.0 * 8.936405181884766
Epoch 1440, val loss: 0.9469623565673828
Epoch 1450, training loss: 894.7753295898438 = 0.9412755966186523 + 100.0 * 8.938340187072754
Epoch 1450, val loss: 0.9452850818634033
Epoch 1460, training loss: 894.869873046875 = 0.9395126104354858 + 100.0 * 8.939303398132324
Epoch 1460, val loss: 0.9435834884643555
Epoch 1470, training loss: 894.6415405273438 = 0.9378001093864441 + 100.0 * 8.937037467956543
Epoch 1470, val loss: 0.9419637322425842
Epoch 1480, training loss: 894.8328247070312 = 0.9360278248786926 + 100.0 * 8.93896770477295
Epoch 1480, val loss: 0.9402701258659363
Epoch 1490, training loss: 893.6552734375 = 0.9342872500419617 + 100.0 * 8.927209854125977
Epoch 1490, val loss: 0.9386149048805237
Epoch 1500, training loss: 894.3988037109375 = 0.9326027631759644 + 100.0 * 8.934661865234375
Epoch 1500, val loss: 0.9369930624961853
Epoch 1510, training loss: 895.358154296875 = 0.9309694766998291 + 100.0 * 8.9442720413208
Epoch 1510, val loss: 0.9354346394538879
Epoch 1520, training loss: 896.0880126953125 = 0.9292692542076111 + 100.0 * 8.951587677001953
Epoch 1520, val loss: 0.9337880611419678
Epoch 1530, training loss: 896.088623046875 = 0.9275501370429993 + 100.0 * 8.951610565185547
Epoch 1530, val loss: 0.9321540594100952
Epoch 1540, training loss: 896.220703125 = 0.9258276224136353 + 100.0 * 8.952948570251465
Epoch 1540, val loss: 0.9304894804954529
Epoch 1550, training loss: 896.2434692382812 = 0.9241336584091187 + 100.0 * 8.953193664550781
Epoch 1550, val loss: 0.9288791418075562
Epoch 1560, training loss: 897.0374755859375 = 0.9224758744239807 + 100.0 * 8.961150169372559
Epoch 1560, val loss: 0.9272982478141785
Epoch 1570, training loss: 897.1810302734375 = 0.9207529425621033 + 100.0 * 8.962602615356445
Epoch 1570, val loss: 0.9256469011306763
Epoch 1580, training loss: 897.2191772460938 = 0.9190385937690735 + 100.0 * 8.963001251220703
Epoch 1580, val loss: 0.9240261316299438
Epoch 1590, training loss: 897.5234375 = 0.9173541069030762 + 100.0 * 8.966060638427734
Epoch 1590, val loss: 0.9224168658256531
Epoch 1600, training loss: 897.3260498046875 = 0.9156429171562195 + 100.0 * 8.964103698730469
Epoch 1600, val loss: 0.920799970626831
Epoch 1610, training loss: 897.4907836914062 = 0.9140053987503052 + 100.0 * 8.965767860412598
Epoch 1610, val loss: 0.9192400574684143
Epoch 1620, training loss: 897.9442138671875 = 0.912356972694397 + 100.0 * 8.970318794250488
Epoch 1620, val loss: 0.917686939239502
Epoch 1630, training loss: 898.2529296875 = 0.9106656312942505 + 100.0 * 8.97342300415039
Epoch 1630, val loss: 0.9160631895065308
Epoch 1640, training loss: 898.4484252929688 = 0.90897136926651 + 100.0 * 8.975394248962402
Epoch 1640, val loss: 0.9144547581672668
Epoch 1650, training loss: 896.947509765625 = 0.9072821736335754 + 100.0 * 8.960402488708496
Epoch 1650, val loss: 0.9128170609474182
Epoch 1660, training loss: 897.2051391601562 = 0.9055819511413574 + 100.0 * 8.962995529174805
Epoch 1660, val loss: 0.9112420678138733
Epoch 1670, training loss: 897.9014892578125 = 0.9039573669433594 + 100.0 * 8.969975471496582
Epoch 1670, val loss: 0.9096946716308594
Epoch 1680, training loss: 898.7214965820312 = 0.9023450613021851 + 100.0 * 8.978191375732422
Epoch 1680, val loss: 0.9081507921218872
Epoch 1690, training loss: 899.11474609375 = 0.900728166103363 + 100.0 * 8.982139587402344
Epoch 1690, val loss: 0.9066212773323059
Epoch 1700, training loss: 898.6056518554688 = 0.8991430997848511 + 100.0 * 8.977065086364746
Epoch 1700, val loss: 0.9051370620727539
Epoch 1710, training loss: 899.0789184570312 = 0.8975551724433899 + 100.0 * 8.981813430786133
Epoch 1710, val loss: 0.9036102890968323
Epoch 1720, training loss: 899.712646484375 = 0.8959658145904541 + 100.0 * 8.988166809082031
Epoch 1720, val loss: 0.9021170735359192
Epoch 1730, training loss: 899.7687377929688 = 0.8943765759468079 + 100.0 * 8.988743782043457
Epoch 1730, val loss: 0.9006143808364868
Epoch 1740, training loss: 899.5260009765625 = 0.8928104043006897 + 100.0 * 8.986331939697266
Epoch 1740, val loss: 0.8991241455078125
Epoch 1750, training loss: 899.8846435546875 = 0.8912150859832764 + 100.0 * 8.989933967590332
Epoch 1750, val loss: 0.8976343274116516
Epoch 1760, training loss: 900.5027465820312 = 0.8896583914756775 + 100.0 * 8.99613094329834
Epoch 1760, val loss: 0.8961686491966248
Epoch 1770, training loss: 900.8233032226562 = 0.8880925178527832 + 100.0 * 8.999351501464844
Epoch 1770, val loss: 0.8946847915649414
Epoch 1780, training loss: 900.6182861328125 = 0.8865150213241577 + 100.0 * 8.99731731414795
Epoch 1780, val loss: 0.89317387342453
Epoch 1790, training loss: 900.36767578125 = 0.8849557042121887 + 100.0 * 8.994827270507812
Epoch 1790, val loss: 0.8916862607002258
Epoch 1800, training loss: 900.431884765625 = 0.8833621740341187 + 100.0 * 8.995485305786133
Epoch 1800, val loss: 0.8901849389076233
Epoch 1810, training loss: 897.9398193359375 = 0.8818235397338867 + 100.0 * 8.970580101013184
Epoch 1810, val loss: 0.8887210488319397
Epoch 1820, training loss: 898.3812255859375 = 0.8803303241729736 + 100.0 * 8.975008964538574
Epoch 1820, val loss: 0.8873496055603027
Epoch 1830, training loss: 898.536376953125 = 0.8788918256759644 + 100.0 * 8.976574897766113
Epoch 1830, val loss: 0.8860136866569519
Epoch 1840, training loss: 899.2831420898438 = 0.8774034976959229 + 100.0 * 8.984057426452637
Epoch 1840, val loss: 0.8845760822296143
Epoch 1850, training loss: 900.2621459960938 = 0.8759145140647888 + 100.0 * 8.99386215209961
Epoch 1850, val loss: 0.8832013010978699
Epoch 1860, training loss: 901.361083984375 = 0.8744645714759827 + 100.0 * 9.004866600036621
Epoch 1860, val loss: 0.8818427920341492
Epoch 1870, training loss: 901.637939453125 = 0.8729561567306519 + 100.0 * 9.007649421691895
Epoch 1870, val loss: 0.880440890789032
Epoch 1880, training loss: 902.182861328125 = 0.8714832067489624 + 100.0 * 9.013113975524902
Epoch 1880, val loss: 0.8790647387504578
Epoch 1890, training loss: 902.0479736328125 = 0.8699996471405029 + 100.0 * 9.01177978515625
Epoch 1890, val loss: 0.8776733875274658
Epoch 1900, training loss: 902.4227294921875 = 0.8685321807861328 + 100.0 * 9.015542030334473
Epoch 1900, val loss: 0.8762919902801514
Epoch 1910, training loss: 902.8658447265625 = 0.8670632839202881 + 100.0 * 9.019988059997559
Epoch 1910, val loss: 0.8749110698699951
Epoch 1920, training loss: 901.664794921875 = 0.8655704855918884 + 100.0 * 9.007991790771484
Epoch 1920, val loss: 0.87351393699646
Epoch 1930, training loss: 901.1408081054688 = 0.8640863299369812 + 100.0 * 9.002767562866211
Epoch 1930, val loss: 0.872157096862793
Epoch 1940, training loss: 901.6498413085938 = 0.8626482486724854 + 100.0 * 9.007871627807617
Epoch 1940, val loss: 0.8708155751228333
Epoch 1950, training loss: 902.656982421875 = 0.8612158894538879 + 100.0 * 9.01795768737793
Epoch 1950, val loss: 0.8694986701011658
Epoch 1960, training loss: 903.2218627929688 = 0.8597917556762695 + 100.0 * 9.02362060546875
Epoch 1960, val loss: 0.8681795001029968
Epoch 1970, training loss: 903.064208984375 = 0.8583593964576721 + 100.0 * 9.022058486938477
Epoch 1970, val loss: 0.8668613433837891
Epoch 1980, training loss: 903.75732421875 = 0.8569832444190979 + 100.0 * 9.029003143310547
Epoch 1980, val loss: 0.865570604801178
Epoch 1990, training loss: 904.150634765625 = 0.8555900454521179 + 100.0 * 9.032950401306152
Epoch 1990, val loss: 0.8642796874046326
Epoch 2000, training loss: 904.3043823242188 = 0.8542160391807556 + 100.0 * 9.034502029418945
Epoch 2000, val loss: 0.8630143404006958
Epoch 2010, training loss: 902.7938842773438 = 0.8527752757072449 + 100.0 * 9.019411087036133
Epoch 2010, val loss: 0.8616657257080078
Epoch 2020, training loss: 903.6322631835938 = 0.8514079451560974 + 100.0 * 9.027809143066406
Epoch 2020, val loss: 0.8604035973548889
Epoch 2030, training loss: 904.14697265625 = 0.8500623106956482 + 100.0 * 9.03296947479248
Epoch 2030, val loss: 0.8591625094413757
Epoch 2040, training loss: 904.90087890625 = 0.8487206697463989 + 100.0 * 9.040521621704102
Epoch 2040, val loss: 0.8579117059707642
Epoch 2050, training loss: 905.0562744140625 = 0.8473788499832153 + 100.0 * 9.042089462280273
Epoch 2050, val loss: 0.8566765785217285
Epoch 2060, training loss: 904.9287719726562 = 0.8460553288459778 + 100.0 * 9.040826797485352
Epoch 2060, val loss: 0.8554422855377197
Epoch 2070, training loss: 904.6222534179688 = 0.8447245955467224 + 100.0 * 9.037775039672852
Epoch 2070, val loss: 0.8542214632034302
Epoch 2080, training loss: 905.0352172851562 = 0.8434218764305115 + 100.0 * 9.04191780090332
Epoch 2080, val loss: 0.8530304431915283
Epoch 2090, training loss: 905.43798828125 = 0.8421353101730347 + 100.0 * 9.045958518981934
Epoch 2090, val loss: 0.851851761341095
Epoch 2100, training loss: 905.3577270507812 = 0.840848982334137 + 100.0 * 9.04516887664795
Epoch 2100, val loss: 0.8506737351417542
Epoch 2110, training loss: 905.1220092773438 = 0.8395654559135437 + 100.0 * 9.042824745178223
Epoch 2110, val loss: 0.8495035171508789
Epoch 2120, training loss: 898.5861206054688 = 0.8380378484725952 + 100.0 * 8.9774808883667
Epoch 2120, val loss: 0.8480893969535828
Epoch 2130, training loss: 898.0529174804688 = 0.8368720412254333 + 100.0 * 8.972160339355469
Epoch 2130, val loss: 0.8470496535301208
Epoch 2140, training loss: 897.5409545898438 = 0.8358871340751648 + 100.0 * 8.967050552368164
Epoch 2140, val loss: 0.8461341857910156
Epoch 2150, training loss: 898.7083129882812 = 0.8348518013954163 + 100.0 * 8.978734970092773
Epoch 2150, val loss: 0.845167338848114
Epoch 2160, training loss: 898.8447875976562 = 0.8337022066116333 + 100.0 * 8.980111122131348
Epoch 2160, val loss: 0.8440964818000793
Epoch 2170, training loss: 900.8975219726562 = 0.83255934715271 + 100.0 * 9.000649452209473
Epoch 2170, val loss: 0.8430854082107544
Epoch 2180, training loss: 901.7044067382812 = 0.8314079642295837 + 100.0 * 9.008729934692383
Epoch 2180, val loss: 0.8420523405075073
Epoch 2190, training loss: 902.805419921875 = 0.8302539587020874 + 100.0 * 9.01975154876709
Epoch 2190, val loss: 0.8409998416900635
Epoch 2200, training loss: 903.2586669921875 = 0.8290916681289673 + 100.0 * 9.024295806884766
Epoch 2200, val loss: 0.8399385213851929
Epoch 2210, training loss: 903.9871826171875 = 0.8279368877410889 + 100.0 * 9.03159236907959
Epoch 2210, val loss: 0.8388863801956177
Epoch 2220, training loss: 904.5106201171875 = 0.8267924189567566 + 100.0 * 9.03683853149414
Epoch 2220, val loss: 0.8378438949584961
Epoch 2230, training loss: 904.62060546875 = 0.8256366848945618 + 100.0 * 9.037949562072754
Epoch 2230, val loss: 0.8367923498153687
Epoch 2240, training loss: 904.8729858398438 = 0.8245089650154114 + 100.0 * 9.040484428405762
Epoch 2240, val loss: 0.8357676267623901
Epoch 2250, training loss: 905.204345703125 = 0.8233838677406311 + 100.0 * 9.04380989074707
Epoch 2250, val loss: 0.8347363471984863
Epoch 2260, training loss: 904.8363647460938 = 0.8222481608390808 + 100.0 * 9.040141105651855
Epoch 2260, val loss: 0.8337079286575317
Epoch 2270, training loss: 904.76904296875 = 0.8211363554000854 + 100.0 * 9.03947925567627
Epoch 2270, val loss: 0.8326992988586426
Epoch 2280, training loss: 905.3007202148438 = 0.8200529217720032 + 100.0 * 9.044806480407715
Epoch 2280, val loss: 0.8317146301269531
Epoch 2290, training loss: 905.664794921875 = 0.8189783692359924 + 100.0 * 9.048458099365234
Epoch 2290, val loss: 0.830724835395813
Epoch 2300, training loss: 905.8848266601562 = 0.8179033398628235 + 100.0 * 9.050668716430664
Epoch 2300, val loss: 0.829735279083252
Epoch 2310, training loss: 905.5779418945312 = 0.8168392777442932 + 100.0 * 9.047611236572266
Epoch 2310, val loss: 0.8287487030029297
Epoch 2320, training loss: 904.9291381835938 = 0.8157551288604736 + 100.0 * 9.041133880615234
Epoch 2320, val loss: 0.8277721405029297
Epoch 2330, training loss: 904.3424072265625 = 0.8147112131118774 + 100.0 * 9.035277366638184
Epoch 2330, val loss: 0.8267850875854492
Epoch 2340, training loss: 904.9459838867188 = 0.8137155771255493 + 100.0 * 9.041322708129883
Epoch 2340, val loss: 0.825877845287323
Epoch 2350, training loss: 906.0902709960938 = 0.8127333521842957 + 100.0 * 9.052775382995605
Epoch 2350, val loss: 0.8249713778495789
Epoch 2360, training loss: 906.7157592773438 = 0.811745285987854 + 100.0 * 9.059040069580078
Epoch 2360, val loss: 0.8240631222724915
Epoch 2370, training loss: 906.830810546875 = 0.8107339143753052 + 100.0 * 9.060200691223145
Epoch 2370, val loss: 0.8231365084648132
Epoch 2380, training loss: 906.8978881835938 = 0.8097531199455261 + 100.0 * 9.060881614685059
Epoch 2380, val loss: 0.8222283124923706
Epoch 2390, training loss: 907.17919921875 = 0.8087775707244873 + 100.0 * 9.063704490661621
Epoch 2390, val loss: 0.8213257789611816
Epoch 2400, training loss: 907.2069702148438 = 0.8077878355979919 + 100.0 * 9.06399154663086
Epoch 2400, val loss: 0.8204177021980286
Epoch 2410, training loss: 907.3643188476562 = 0.8068323135375977 + 100.0 * 9.065574645996094
Epoch 2410, val loss: 0.819539487361908
Epoch 2420, training loss: 907.4217529296875 = 0.805882453918457 + 100.0 * 9.066158294677734
Epoch 2420, val loss: 0.8186769485473633
Epoch 2430, training loss: 907.6949462890625 = 0.8049380779266357 + 100.0 * 9.068900108337402
Epoch 2430, val loss: 0.817833423614502
Epoch 2440, training loss: 907.5836181640625 = 0.8039998412132263 + 100.0 * 9.067795753479004
Epoch 2440, val loss: 0.8169822096824646
Epoch 2450, training loss: 907.7673950195312 = 0.8030740022659302 + 100.0 * 9.069643020629883
Epoch 2450, val loss: 0.8161435723304749
Epoch 2460, training loss: 907.6455688476562 = 0.8021647930145264 + 100.0 * 9.06843376159668
Epoch 2460, val loss: 0.8153125047683716
Epoch 2470, training loss: 907.7338256835938 = 0.8012558221817017 + 100.0 * 9.06932544708252
Epoch 2470, val loss: 0.814502477645874
Epoch 2480, training loss: 908.1549072265625 = 0.8003773093223572 + 100.0 * 9.073545455932617
Epoch 2480, val loss: 0.8137242197990417
Epoch 2490, training loss: 908.090087890625 = 0.79953533411026 + 100.0 * 9.072905540466309
Epoch 2490, val loss: 0.8130362629890442
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.5547826086956522
0.8635803810765776
=== training gcn model ===
Epoch 0, training loss: 1028.72900390625 = 1.095624327659607 + 100.0 * 10.276334762573242
Epoch 0, val loss: 1.0960016250610352
Epoch 10, training loss: 987.5624389648438 = 1.0956122875213623 + 100.0 * 9.864667892456055
Epoch 10, val loss: 1.0959947109222412
Epoch 20, training loss: 965.77197265625 = 1.0954257249832153 + 100.0 * 9.64676570892334
Epoch 20, val loss: 1.0958337783813477
Epoch 30, training loss: 950.3820190429688 = 1.0953019857406616 + 100.0 * 9.492867469787598
Epoch 30, val loss: 1.0957305431365967
Epoch 40, training loss: 938.7243041992188 = 1.0951534509658813 + 100.0 * 9.376291275024414
Epoch 40, val loss: 1.0955963134765625
Epoch 50, training loss: 929.39013671875 = 1.0949758291244507 + 100.0 * 9.282951354980469
Epoch 50, val loss: 1.095438003540039
Epoch 60, training loss: 921.677001953125 = 1.0947978496551514 + 100.0 * 9.205821990966797
Epoch 60, val loss: 1.0952768325805664
Epoch 70, training loss: 915.1150512695312 = 1.0946173667907715 + 100.0 * 9.140204429626465
Epoch 70, val loss: 1.095112919807434
Epoch 80, training loss: 909.4393310546875 = 1.0944340229034424 + 100.0 * 9.083449363708496
Epoch 80, val loss: 1.0949475765228271
Epoch 90, training loss: 904.6578369140625 = 1.094246506690979 + 100.0 * 9.035635948181152
Epoch 90, val loss: 1.0947740077972412
Epoch 100, training loss: 900.5130004882812 = 1.0938749313354492 + 100.0 * 8.99419116973877
Epoch 100, val loss: 1.094380259513855
Epoch 110, training loss: 896.8020629882812 = 1.0933914184570312 + 100.0 * 8.957086563110352
Epoch 110, val loss: 1.0938810110092163
Epoch 120, training loss: 893.580810546875 = 1.0928986072540283 + 100.0 * 8.92487907409668
Epoch 120, val loss: 1.093381404876709
Epoch 130, training loss: 890.9788818359375 = 1.092418909072876 + 100.0 * 8.89886474609375
Epoch 130, val loss: 1.0928924083709717
Epoch 140, training loss: 888.5152587890625 = 1.0919440984725952 + 100.0 * 8.87423324584961
Epoch 140, val loss: 1.092413067817688
Epoch 150, training loss: 887.29931640625 = 1.0914349555969238 + 100.0 * 8.862078666687012
Epoch 150, val loss: 1.0919060707092285
Epoch 160, training loss: 885.3529052734375 = 1.0909985303878784 + 100.0 * 8.842618942260742
Epoch 160, val loss: 1.091463565826416
Epoch 170, training loss: 883.8034057617188 = 1.0904780626296997 + 100.0 * 8.827129364013672
Epoch 170, val loss: 1.0909439325332642
Epoch 180, training loss: 882.3301391601562 = 1.0899789333343506 + 100.0 * 8.81240177154541
Epoch 180, val loss: 1.090456247329712
Epoch 190, training loss: 880.764404296875 = 1.0894558429718018 + 100.0 * 8.796749114990234
Epoch 190, val loss: 1.089935302734375
Epoch 200, training loss: 879.7908935546875 = 1.0889190435409546 + 100.0 * 8.787019729614258
Epoch 200, val loss: 1.0894039869308472
Epoch 210, training loss: 878.9010620117188 = 1.088366150856018 + 100.0 * 8.77812671661377
Epoch 210, val loss: 1.0888550281524658
Epoch 220, training loss: 878.7814331054688 = 1.0877853631973267 + 100.0 * 8.776936531066895
Epoch 220, val loss: 1.08828604221344
Epoch 230, training loss: 877.6383056640625 = 1.087175726890564 + 100.0 * 8.765511512756348
Epoch 230, val loss: 1.087681770324707
Epoch 240, training loss: 876.7379760742188 = 1.0865473747253418 + 100.0 * 8.756514549255371
Epoch 240, val loss: 1.0870667695999146
Epoch 250, training loss: 876.2442016601562 = 1.0858973264694214 + 100.0 * 8.751583099365234
Epoch 250, val loss: 1.0864256620407104
Epoch 260, training loss: 875.70751953125 = 1.0852282047271729 + 100.0 * 8.746223449707031
Epoch 260, val loss: 1.0857685804367065
Epoch 270, training loss: 875.2318725585938 = 1.0845297574996948 + 100.0 * 8.741473197937012
Epoch 270, val loss: 1.0850855112075806
Epoch 280, training loss: 875.3687133789062 = 1.083797574043274 + 100.0 * 8.742849349975586
Epoch 280, val loss: 1.0843675136566162
Epoch 290, training loss: 874.5376586914062 = 1.0830429792404175 + 100.0 * 8.734545707702637
Epoch 290, val loss: 1.0836262702941895
Epoch 300, training loss: 874.3414306640625 = 1.082305908203125 + 100.0 * 8.732590675354004
Epoch 300, val loss: 1.0829018354415894
Epoch 310, training loss: 873.9306640625 = 1.0814990997314453 + 100.0 * 8.72849178314209
Epoch 310, val loss: 1.0821119546890259
Epoch 320, training loss: 873.8687133789062 = 1.0806870460510254 + 100.0 * 8.727880477905273
Epoch 320, val loss: 1.081315040588379
Epoch 330, training loss: 873.5948486328125 = 1.0798598527908325 + 100.0 * 8.725150108337402
Epoch 330, val loss: 1.0804946422576904
Epoch 340, training loss: 873.2318115234375 = 1.0789732933044434 + 100.0 * 8.721528053283691
Epoch 340, val loss: 1.0796345472335815
Epoch 350, training loss: 873.37353515625 = 1.0781128406524658 + 100.0 * 8.722953796386719
Epoch 350, val loss: 1.0787805318832397
Epoch 360, training loss: 873.462890625 = 1.0771955251693726 + 100.0 * 8.723856925964355
Epoch 360, val loss: 1.0778812170028687
Epoch 370, training loss: 873.5348510742188 = 1.0762536525726318 + 100.0 * 8.724586486816406
Epoch 370, val loss: 1.0769580602645874
Epoch 380, training loss: 873.3743896484375 = 1.0753018856048584 + 100.0 * 8.722990989685059
Epoch 380, val loss: 1.076033115386963
Epoch 390, training loss: 873.2454833984375 = 1.0743132829666138 + 100.0 * 8.721711158752441
Epoch 390, val loss: 1.0750592947006226
Epoch 400, training loss: 873.2559814453125 = 1.0732765197753906 + 100.0 * 8.721826553344727
Epoch 400, val loss: 1.0740493535995483
Epoch 410, training loss: 873.5679321289062 = 1.0722465515136719 + 100.0 * 8.724956512451172
Epoch 410, val loss: 1.0730373859405518
Epoch 420, training loss: 873.5315551757812 = 1.0711876153945923 + 100.0 * 8.724603652954102
Epoch 420, val loss: 1.0720053911209106
Epoch 430, training loss: 873.5225219726562 = 1.0700949430465698 + 100.0 * 8.72452449798584
Epoch 430, val loss: 1.0709418058395386
Epoch 440, training loss: 873.513916015625 = 1.0689729452133179 + 100.0 * 8.724449157714844
Epoch 440, val loss: 1.0698411464691162
Epoch 450, training loss: 873.3894653320312 = 1.0678865909576416 + 100.0 * 8.72321605682373
Epoch 450, val loss: 1.0687808990478516
Epoch 460, training loss: 873.1642456054688 = 1.0667489767074585 + 100.0 * 8.720974922180176
Epoch 460, val loss: 1.0676685571670532
Epoch 470, training loss: 873.6467895507812 = 1.0656538009643555 + 100.0 * 8.725811004638672
Epoch 470, val loss: 1.0666028261184692
Epoch 480, training loss: 873.8719482421875 = 1.0645338296890259 + 100.0 * 8.728074073791504
Epoch 480, val loss: 1.0655032396316528
Epoch 490, training loss: 873.697509765625 = 1.063361406326294 + 100.0 * 8.726341247558594
Epoch 490, val loss: 1.0643653869628906
Epoch 500, training loss: 873.728271484375 = 1.0622104406356812 + 100.0 * 8.72666072845459
Epoch 500, val loss: 1.0632438659667969
Epoch 510, training loss: 873.63134765625 = 1.0610425472259521 + 100.0 * 8.725703239440918
Epoch 510, val loss: 1.062097430229187
Epoch 520, training loss: 874.4265747070312 = 1.0598329305648804 + 100.0 * 8.733667373657227
Epoch 520, val loss: 1.0609208345413208
Epoch 530, training loss: 874.2854614257812 = 1.05865478515625 + 100.0 * 8.732268333435059
Epoch 530, val loss: 1.05977463722229
Epoch 540, training loss: 873.8508911132812 = 1.0574214458465576 + 100.0 * 8.727934837341309
Epoch 540, val loss: 1.058569073677063
Epoch 550, training loss: 874.3250122070312 = 1.0561968088150024 + 100.0 * 8.732687950134277
Epoch 550, val loss: 1.0573756694793701
Epoch 560, training loss: 874.8705444335938 = 1.0549551248550415 + 100.0 * 8.73815631866455
Epoch 560, val loss: 1.0561643838882446
Epoch 570, training loss: 874.7905883789062 = 1.0536561012268066 + 100.0 * 8.737369537353516
Epoch 570, val loss: 1.054900884628296
Epoch 580, training loss: 874.4376831054688 = 1.0523545742034912 + 100.0 * 8.733853340148926
Epoch 580, val loss: 1.0536198616027832
Epoch 590, training loss: 875.0762329101562 = 1.051064133644104 + 100.0 * 8.740251541137695
Epoch 590, val loss: 1.0523619651794434
Epoch 600, training loss: 875.1641845703125 = 1.0497174263000488 + 100.0 * 8.741144180297852
Epoch 600, val loss: 1.0510448217391968
Epoch 610, training loss: 875.479736328125 = 1.0483421087265015 + 100.0 * 8.744314193725586
Epoch 610, val loss: 1.0497097969055176
Epoch 620, training loss: 875.6142578125 = 1.046945333480835 + 100.0 * 8.745673179626465
Epoch 620, val loss: 1.048346757888794
Epoch 630, training loss: 875.5296630859375 = 1.04551362991333 + 100.0 * 8.744841575622559
Epoch 630, val loss: 1.0469409227371216
Epoch 640, training loss: 875.989990234375 = 1.0440534353256226 + 100.0 * 8.749459266662598
Epoch 640, val loss: 1.0455223321914673
Epoch 650, training loss: 875.9411010742188 = 1.0425630807876587 + 100.0 * 8.748985290527344
Epoch 650, val loss: 1.0440618991851807
Epoch 660, training loss: 876.2870483398438 = 1.041060447692871 + 100.0 * 8.752459526062012
Epoch 660, val loss: 1.042587161064148
Epoch 670, training loss: 876.667724609375 = 1.0395506620407104 + 100.0 * 8.756281852722168
Epoch 670, val loss: 1.0411031246185303
Epoch 680, training loss: 876.656494140625 = 1.0380295515060425 + 100.0 * 8.756184577941895
Epoch 680, val loss: 1.0396004915237427
Epoch 690, training loss: 876.8921508789062 = 1.036460280418396 + 100.0 * 8.758557319641113
Epoch 690, val loss: 1.0380291938781738
Epoch 700, training loss: 877.3922119140625 = 1.0348594188690186 + 100.0 * 8.76357364654541
Epoch 700, val loss: 1.0364511013031006
Epoch 710, training loss: 877.3150024414062 = 1.0332562923431396 + 100.0 * 8.7628173828125
Epoch 710, val loss: 1.0348851680755615
Epoch 720, training loss: 876.7454223632812 = 1.0316212177276611 + 100.0 * 8.7571382522583
Epoch 720, val loss: 1.03325355052948
Epoch 730, training loss: 876.5690307617188 = 1.0300326347351074 + 100.0 * 8.755390167236328
Epoch 730, val loss: 1.0316814184188843
Epoch 740, training loss: 877.2747192382812 = 1.0283852815628052 + 100.0 * 8.762463569641113
Epoch 740, val loss: 1.0300629138946533
Epoch 750, training loss: 877.9928588867188 = 1.0268466472625732 + 100.0 * 8.769659996032715
Epoch 750, val loss: 1.0285650491714478
Epoch 760, training loss: 878.0390625 = 1.0251120328903198 + 100.0 * 8.770139694213867
Epoch 760, val loss: 1.0268396139144897
Epoch 770, training loss: 878.0069580078125 = 1.0234373807907104 + 100.0 * 8.769835472106934
Epoch 770, val loss: 1.025209903717041
Epoch 780, training loss: 878.4035034179688 = 1.0217727422714233 + 100.0 * 8.77381706237793
Epoch 780, val loss: 1.0235283374786377
Epoch 790, training loss: 878.650634765625 = 1.0201176404953003 + 100.0 * 8.776305198669434
Epoch 790, val loss: 1.0219005346298218
Epoch 800, training loss: 878.6903686523438 = 1.0183696746826172 + 100.0 * 8.77672004699707
Epoch 800, val loss: 1.0202044248580933
Epoch 810, training loss: 878.6798095703125 = 1.0166785717010498 + 100.0 * 8.776631355285645
Epoch 810, val loss: 1.0185546875
Epoch 820, training loss: 879.0382080078125 = 1.0149402618408203 + 100.0 * 8.780232429504395
Epoch 820, val loss: 1.0168309211730957
Epoch 830, training loss: 879.3017578125 = 1.0131915807724 + 100.0 * 8.782885551452637
Epoch 830, val loss: 1.0151160955429077
Epoch 840, training loss: 879.891845703125 = 1.0114322900772095 + 100.0 * 8.788804054260254
Epoch 840, val loss: 1.0133633613586426
Epoch 850, training loss: 879.2696533203125 = 1.0093318223953247 + 100.0 * 8.78260326385498
Epoch 850, val loss: 1.011268138885498
Epoch 860, training loss: 878.404296875 = 1.0079461336135864 + 100.0 * 8.773963928222656
Epoch 860, val loss: 1.0099623203277588
Epoch 870, training loss: 877.0541381835938 = 1.0060216188430786 + 100.0 * 8.760480880737305
Epoch 870, val loss: 1.0080519914627075
Epoch 880, training loss: 878.1103515625 = 1.0042868852615356 + 100.0 * 8.771060943603516
Epoch 880, val loss: 1.0063552856445312
Epoch 890, training loss: 878.694580078125 = 1.0022591352462769 + 100.0 * 8.776923179626465
Epoch 890, val loss: 1.0043773651123047
Epoch 900, training loss: 878.5125122070312 = 1.0004533529281616 + 100.0 * 8.775120735168457
Epoch 900, val loss: 1.0025979280471802
Epoch 910, training loss: 878.6784057617188 = 0.9984711408615112 + 100.0 * 8.776799201965332
Epoch 910, val loss: 1.0006448030471802
Epoch 920, training loss: 879.6920166015625 = 0.9965258240699768 + 100.0 * 8.786954879760742
Epoch 920, val loss: 0.9987117648124695
Epoch 930, training loss: 879.918212890625 = 0.9945016503334045 + 100.0 * 8.789237022399902
Epoch 930, val loss: 0.9967164397239685
Epoch 940, training loss: 880.1798706054688 = 0.9924880266189575 + 100.0 * 8.791873931884766
Epoch 940, val loss: 0.9947497248649597
Epoch 950, training loss: 881.0709838867188 = 0.9904799461364746 + 100.0 * 8.80080509185791
Epoch 950, val loss: 0.9927665591239929
Epoch 960, training loss: 881.2792358398438 = 0.9884370565414429 + 100.0 * 8.802907943725586
Epoch 960, val loss: 0.9907568693161011
Epoch 970, training loss: 881.3233642578125 = 0.9863766431808472 + 100.0 * 8.803369522094727
Epoch 970, val loss: 0.9887248277664185
Epoch 980, training loss: 881.3076171875 = 0.9843041300773621 + 100.0 * 8.80323314666748
Epoch 980, val loss: 0.9866845607757568
Epoch 990, training loss: 881.5765991210938 = 0.9822110533714294 + 100.0 * 8.805943489074707
Epoch 990, val loss: 0.9846197962760925
Epoch 1000, training loss: 882.13720703125 = 0.9801384210586548 + 100.0 * 8.811570167541504
Epoch 1000, val loss: 0.9826052784919739
Epoch 1010, training loss: 882.6123046875 = 0.9780747294425964 + 100.0 * 8.8163423538208
Epoch 1010, val loss: 0.9805605411529541
Epoch 1020, training loss: 882.6945190429688 = 0.9759604930877686 + 100.0 * 8.817185401916504
Epoch 1020, val loss: 0.9785068035125732
Epoch 1030, training loss: 883.3445434570312 = 0.9738758206367493 + 100.0 * 8.82370662689209
Epoch 1030, val loss: 0.9764471650123596
Epoch 1040, training loss: 882.5775756835938 = 0.9717469811439514 + 100.0 * 8.816058158874512
Epoch 1040, val loss: 0.974359393119812
Epoch 1050, training loss: 882.9089965820312 = 0.9696812033653259 + 100.0 * 8.819393157958984
Epoch 1050, val loss: 0.9723111987113953
Epoch 1060, training loss: 883.5377807617188 = 0.9675337076187134 + 100.0 * 8.825702667236328
Epoch 1060, val loss: 0.9702315330505371
Epoch 1070, training loss: 883.780517578125 = 0.9654290676116943 + 100.0 * 8.828150749206543
Epoch 1070, val loss: 0.9681460857391357
Epoch 1080, training loss: 884.3532104492188 = 0.9632463455200195 + 100.0 * 8.83389949798584
Epoch 1080, val loss: 0.9660171270370483
Epoch 1090, training loss: 883.410400390625 = 0.961033821105957 + 100.0 * 8.824493408203125
Epoch 1090, val loss: 0.9638638496398926
Epoch 1100, training loss: 882.5325927734375 = 0.958776593208313 + 100.0 * 8.8157377243042
Epoch 1100, val loss: 0.9616109728813171
Epoch 1110, training loss: 882.8692016601562 = 0.9566664695739746 + 100.0 * 8.819125175476074
Epoch 1110, val loss: 0.9595561623573303
Epoch 1120, training loss: 883.7049560546875 = 0.9544735550880432 + 100.0 * 8.827505111694336
Epoch 1120, val loss: 0.9574039578437805
Epoch 1130, training loss: 884.2183227539062 = 0.9522907137870789 + 100.0 * 8.832660675048828
Epoch 1130, val loss: 0.9552749395370483
Epoch 1140, training loss: 884.7501831054688 = 0.9500986337661743 + 100.0 * 8.838001251220703
Epoch 1140, val loss: 0.9531020522117615
Epoch 1150, training loss: 884.56689453125 = 0.9478481411933899 + 100.0 * 8.836190223693848
Epoch 1150, val loss: 0.9509267210960388
Epoch 1160, training loss: 885.1392211914062 = 0.9456466436386108 + 100.0 * 8.841936111450195
Epoch 1160, val loss: 0.9487498998641968
Epoch 1170, training loss: 885.1712646484375 = 0.9433177709579468 + 100.0 * 8.842279434204102
Epoch 1170, val loss: 0.9464780688285828
Epoch 1180, training loss: 885.3607788085938 = 0.9410087466239929 + 100.0 * 8.844198226928711
Epoch 1180, val loss: 0.9442370533943176
Epoch 1190, training loss: 884.7096557617188 = 0.9387860894203186 + 100.0 * 8.837708473205566
Epoch 1190, val loss: 0.9420779943466187
Epoch 1200, training loss: 885.1820678710938 = 0.9365741610527039 + 100.0 * 8.84245491027832
Epoch 1200, val loss: 0.9398843050003052
Epoch 1210, training loss: 885.4581298828125 = 0.9342734217643738 + 100.0 * 8.84523868560791
Epoch 1210, val loss: 0.9376475811004639
Epoch 1220, training loss: 885.8104248046875 = 0.9320711493492126 + 100.0 * 8.848783493041992
Epoch 1220, val loss: 0.9354804754257202
Epoch 1230, training loss: 886.5662231445312 = 0.9297791123390198 + 100.0 * 8.856364250183105
Epoch 1230, val loss: 0.9332060217857361
Epoch 1240, training loss: 886.6651000976562 = 0.9276663064956665 + 100.0 * 8.85737419128418
Epoch 1240, val loss: 0.9311155080795288
Epoch 1250, training loss: 886.078857421875 = 0.9252882599830627 + 100.0 * 8.85153579711914
Epoch 1250, val loss: 0.9287904500961304
Epoch 1260, training loss: 886.5654296875 = 0.922971248626709 + 100.0 * 8.856424331665039
Epoch 1260, val loss: 0.9265294075012207
Epoch 1270, training loss: 886.3632202148438 = 0.9206268191337585 + 100.0 * 8.854425430297852
Epoch 1270, val loss: 0.9242423176765442
Epoch 1280, training loss: 887.1527099609375 = 0.9183644652366638 + 100.0 * 8.862343788146973
Epoch 1280, val loss: 0.9220395684242249
Epoch 1290, training loss: 887.4371948242188 = 0.916063666343689 + 100.0 * 8.865211486816406
Epoch 1290, val loss: 0.9197889566421509
Epoch 1300, training loss: 887.6842651367188 = 0.9137510657310486 + 100.0 * 8.867705345153809
Epoch 1300, val loss: 0.9175466299057007
Epoch 1310, training loss: 887.9429931640625 = 0.9114238023757935 + 100.0 * 8.870315551757812
Epoch 1310, val loss: 0.9152635931968689
Epoch 1320, training loss: 888.00439453125 = 0.909106433391571 + 100.0 * 8.870952606201172
Epoch 1320, val loss: 0.912999153137207
Epoch 1330, training loss: 888.5385131835938 = 0.906785786151886 + 100.0 * 8.876317024230957
Epoch 1330, val loss: 0.9107457399368286
Epoch 1340, training loss: 888.60546875 = 0.904444694519043 + 100.0 * 8.877010345458984
Epoch 1340, val loss: 0.9083980321884155
Epoch 1350, training loss: 888.3536376953125 = 0.9021512866020203 + 100.0 * 8.87451457977295
Epoch 1350, val loss: 0.9061875343322754
Epoch 1360, training loss: 888.3683471679688 = 0.8997397422790527 + 100.0 * 8.874686241149902
Epoch 1360, val loss: 0.9038416147232056
Epoch 1370, training loss: 888.4136962890625 = 0.897383451461792 + 100.0 * 8.875163078308105
Epoch 1370, val loss: 0.9015517830848694
Epoch 1380, training loss: 889.3490600585938 = 0.89508455991745 + 100.0 * 8.884539604187012
Epoch 1380, val loss: 0.899314284324646
Epoch 1390, training loss: 889.4173583984375 = 0.8927228450775146 + 100.0 * 8.885246276855469
Epoch 1390, val loss: 0.8970036506652832
Epoch 1400, training loss: 889.59716796875 = 0.8904056549072266 + 100.0 * 8.887067794799805
Epoch 1400, val loss: 0.8947374820709229
Epoch 1410, training loss: 890.0673828125 = 0.8880859613418579 + 100.0 * 8.891793251037598
Epoch 1410, val loss: 0.8924760818481445
Epoch 1420, training loss: 888.945556640625 = 0.8857141137123108 + 100.0 * 8.880598068237305
Epoch 1420, val loss: 0.8901839852333069
Epoch 1430, training loss: 889.40478515625 = 0.8834018111228943 + 100.0 * 8.885213851928711
Epoch 1430, val loss: 0.887887716293335
Epoch 1440, training loss: 890.2026977539062 = 0.8810948729515076 + 100.0 * 8.893216133117676
Epoch 1440, val loss: 0.8856511116027832
Epoch 1450, training loss: 890.6051635742188 = 0.8787649869918823 + 100.0 * 8.897263526916504
Epoch 1450, val loss: 0.883371114730835
Epoch 1460, training loss: 890.7034301757812 = 0.8764437437057495 + 100.0 * 8.898269653320312
Epoch 1460, val loss: 0.8810818791389465
Epoch 1470, training loss: 890.8207397460938 = 0.8741111159324646 + 100.0 * 8.899466514587402
Epoch 1470, val loss: 0.8788260221481323
Epoch 1480, training loss: 891.2338256835938 = 0.8718045949935913 + 100.0 * 8.903619766235352
Epoch 1480, val loss: 0.8765662312507629
Epoch 1490, training loss: 891.5661010742188 = 0.8694912195205688 + 100.0 * 8.906966209411621
Epoch 1490, val loss: 0.874302864074707
Epoch 1500, training loss: 891.2136840820312 = 0.8671222925186157 + 100.0 * 8.903465270996094
Epoch 1500, val loss: 0.8720120787620544
Epoch 1510, training loss: 890.3579711914062 = 0.8647801876068115 + 100.0 * 8.89493179321289
Epoch 1510, val loss: 0.8697625398635864
Epoch 1520, training loss: 890.331298828125 = 0.8625774383544922 + 100.0 * 8.89468765258789
Epoch 1520, val loss: 0.8675466775894165
Epoch 1530, training loss: 890.33447265625 = 0.8603006601333618 + 100.0 * 8.894742012023926
Epoch 1530, val loss: 0.8653185963630676
Epoch 1540, training loss: 889.5029296875 = 0.8578653931617737 + 100.0 * 8.88645076751709
Epoch 1540, val loss: 0.8629596829414368
Epoch 1550, training loss: 889.2877807617188 = 0.8555948734283447 + 100.0 * 8.884322166442871
Epoch 1550, val loss: 0.8607579469680786
Epoch 1560, training loss: 889.7398681640625 = 0.8533264398574829 + 100.0 * 8.88886547088623
Epoch 1560, val loss: 0.8585489988327026
Epoch 1570, training loss: 890.652587890625 = 0.8510388135910034 + 100.0 * 8.898015975952148
Epoch 1570, val loss: 0.8563222885131836
Epoch 1580, training loss: 891.44921875 = 0.8487755060195923 + 100.0 * 8.906004905700684
Epoch 1580, val loss: 0.854113757610321
Epoch 1590, training loss: 891.8102416992188 = 0.8465049266815186 + 100.0 * 8.909637451171875
Epoch 1590, val loss: 0.8519070744514465
Epoch 1600, training loss: 892.2255859375 = 0.8442384004592896 + 100.0 * 8.913813591003418
Epoch 1600, val loss: 0.8497076630592346
Epoch 1610, training loss: 892.1610717773438 = 0.8419716954231262 + 100.0 * 8.913190841674805
Epoch 1610, val loss: 0.8475027084350586
Epoch 1620, training loss: 892.078125 = 0.8396963477134705 + 100.0 * 8.912384033203125
Epoch 1620, val loss: 0.8452693819999695
Epoch 1630, training loss: 892.4072875976562 = 0.8374601602554321 + 100.0 * 8.915698051452637
Epoch 1630, val loss: 0.8431179523468018
Epoch 1640, training loss: 892.333984375 = 0.8351874351501465 + 100.0 * 8.914987564086914
Epoch 1640, val loss: 0.8409303426742554
Epoch 1650, training loss: 893.0179443359375 = 0.8329574465751648 + 100.0 * 8.921850204467773
Epoch 1650, val loss: 0.8387534022331238
Epoch 1660, training loss: 893.0611572265625 = 0.8307325839996338 + 100.0 * 8.922304153442383
Epoch 1660, val loss: 0.8366063833236694
Epoch 1670, training loss: 893.0471801757812 = 0.8284803032875061 + 100.0 * 8.922186851501465
Epoch 1670, val loss: 0.8344219923019409
Epoch 1680, training loss: 893.4649047851562 = 0.8262490034103394 + 100.0 * 8.926386833190918
Epoch 1680, val loss: 0.8322683572769165
Epoch 1690, training loss: 893.5831909179688 = 0.8240224719047546 + 100.0 * 8.927591323852539
Epoch 1690, val loss: 0.8301141858100891
Epoch 1700, training loss: 893.3917236328125 = 0.8217882513999939 + 100.0 * 8.925699234008789
Epoch 1700, val loss: 0.8279424905776978
Epoch 1710, training loss: 893.9290771484375 = 0.8195686936378479 + 100.0 * 8.931095123291016
Epoch 1710, val loss: 0.8258045315742493
Epoch 1720, training loss: 894.191650390625 = 0.8173631429672241 + 100.0 * 8.93374252319336
Epoch 1720, val loss: 0.8236688375473022
Epoch 1730, training loss: 893.7005004882812 = 0.8151599764823914 + 100.0 * 8.928853034973145
Epoch 1730, val loss: 0.821528434753418
Epoch 1740, training loss: 894.5066528320312 = 0.8129739165306091 + 100.0 * 8.936936378479004
Epoch 1740, val loss: 0.8194295167922974
Epoch 1750, training loss: 894.779541015625 = 0.8107820153236389 + 100.0 * 8.939687728881836
Epoch 1750, val loss: 0.8173109889030457
Epoch 1760, training loss: 894.0916748046875 = 0.8085819482803345 + 100.0 * 8.932830810546875
Epoch 1760, val loss: 0.8152087926864624
Epoch 1770, training loss: 894.6870727539062 = 0.8064091205596924 + 100.0 * 8.938806533813477
Epoch 1770, val loss: 0.8130887746810913
Epoch 1780, training loss: 893.7556762695312 = 0.8041833639144897 + 100.0 * 8.92951488494873
Epoch 1780, val loss: 0.8109440803527832
Epoch 1790, training loss: 893.8374633789062 = 0.802053689956665 + 100.0 * 8.930354118347168
Epoch 1790, val loss: 0.808883547782898
Epoch 1800, training loss: 893.9344482421875 = 0.7998529672622681 + 100.0 * 8.93134593963623
Epoch 1800, val loss: 0.806826651096344
Epoch 1810, training loss: 894.4360961914062 = 0.7976805567741394 + 100.0 * 8.936384201049805
Epoch 1810, val loss: 0.8047135472297668
Epoch 1820, training loss: 895.0873413085938 = 0.7955061793327332 + 100.0 * 8.942917823791504
Epoch 1820, val loss: 0.8026275038719177
Epoch 1830, training loss: 895.9808349609375 = 0.7933271527290344 + 100.0 * 8.951874732971191
Epoch 1830, val loss: 0.8005326986312866
Epoch 1840, training loss: 895.8388671875 = 0.791163980960846 + 100.0 * 8.950477600097656
Epoch 1840, val loss: 0.7984669208526611
Epoch 1850, training loss: 895.4581298828125 = 0.78901606798172 + 100.0 * 8.946691513061523
Epoch 1850, val loss: 0.7963952422142029
Epoch 1860, training loss: 894.374755859375 = 0.7867797613143921 + 100.0 * 8.935879707336426
Epoch 1860, val loss: 0.7942514419555664
Epoch 1870, training loss: 894.7830200195312 = 0.7846956253051758 + 100.0 * 8.939983367919922
Epoch 1870, val loss: 0.7922489643096924
Epoch 1880, training loss: 895.38330078125 = 0.7826203107833862 + 100.0 * 8.946006774902344
Epoch 1880, val loss: 0.79023677110672
Epoch 1890, training loss: 896.02001953125 = 0.7805517315864563 + 100.0 * 8.952394485473633
Epoch 1890, val loss: 0.788257360458374
Epoch 1900, training loss: 895.9100341796875 = 0.7784634828567505 + 100.0 * 8.951315879821777
Epoch 1900, val loss: 0.786248505115509
Epoch 1910, training loss: 896.3873901367188 = 0.7763907313346863 + 100.0 * 8.956110000610352
Epoch 1910, val loss: 0.7842662930488586
Epoch 1920, training loss: 896.4574584960938 = 0.7743538618087769 + 100.0 * 8.956830978393555
Epoch 1920, val loss: 0.7823483347892761
Epoch 1930, training loss: 896.8067016601562 = 0.7723304629325867 + 100.0 * 8.960343360900879
Epoch 1930, val loss: 0.7804170846939087
Epoch 1940, training loss: 897.0180053710938 = 0.770298957824707 + 100.0 * 8.96247673034668
Epoch 1940, val loss: 0.7784582376480103
Epoch 1950, training loss: 897.2418212890625 = 0.7682713270187378 + 100.0 * 8.964735984802246
Epoch 1950, val loss: 0.7765630483627319
Epoch 1960, training loss: 896.9285278320312 = 0.7662745714187622 + 100.0 * 8.96162223815918
Epoch 1960, val loss: 0.7746597528457642
Epoch 1970, training loss: 897.2439575195312 = 0.764296293258667 + 100.0 * 8.964797019958496
Epoch 1970, val loss: 0.7727708220481873
Epoch 1980, training loss: 897.4366455078125 = 0.7623395919799805 + 100.0 * 8.966743469238281
Epoch 1980, val loss: 0.7709372043609619
Epoch 1990, training loss: 897.6553955078125 = 0.7603673934936523 + 100.0 * 8.968950271606445
Epoch 1990, val loss: 0.7690797448158264
Epoch 2000, training loss: 897.8982543945312 = 0.7584072351455688 + 100.0 * 8.97139835357666
Epoch 2000, val loss: 0.7672275304794312
Epoch 2010, training loss: 895.1524658203125 = 0.7564663290977478 + 100.0 * 8.943960189819336
Epoch 2010, val loss: 0.7654201984405518
Epoch 2020, training loss: 895.150146484375 = 0.7545433044433594 + 100.0 * 8.94395637512207
Epoch 2020, val loss: 0.7636261582374573
Epoch 2030, training loss: 896.1112670898438 = 0.7526765465736389 + 100.0 * 8.953585624694824
Epoch 2030, val loss: 0.7618348598480225
Epoch 2040, training loss: 896.8682250976562 = 0.7507548928260803 + 100.0 * 8.961174964904785
Epoch 2040, val loss: 0.7600337862968445
Epoch 2050, training loss: 897.0996704101562 = 0.7488899230957031 + 100.0 * 8.963507652282715
Epoch 2050, val loss: 0.7582564949989319
Epoch 2060, training loss: 897.682373046875 = 0.7470363974571228 + 100.0 * 8.969353675842285
Epoch 2060, val loss: 0.7565195560455322
Epoch 2070, training loss: 897.6615600585938 = 0.7451770901679993 + 100.0 * 8.96916389465332
Epoch 2070, val loss: 0.7547643780708313
Epoch 2080, training loss: 897.7967529296875 = 0.7433186173439026 + 100.0 * 8.970534324645996
Epoch 2080, val loss: 0.7530278563499451
Epoch 2090, training loss: 897.8794555664062 = 0.741508424282074 + 100.0 * 8.971379280090332
Epoch 2090, val loss: 0.7513477206230164
Epoch 2100, training loss: 898.3836059570312 = 0.7397255301475525 + 100.0 * 8.976438522338867
Epoch 2100, val loss: 0.7496539354324341
Epoch 2110, training loss: 898.2587280273438 = 0.7378954291343689 + 100.0 * 8.975208282470703
Epoch 2110, val loss: 0.7479541897773743
Epoch 2120, training loss: 898.4921264648438 = 0.7361360788345337 + 100.0 * 8.977560043334961
Epoch 2120, val loss: 0.7463351488113403
Epoch 2130, training loss: 898.5889892578125 = 0.7343922257423401 + 100.0 * 8.978546142578125
Epoch 2130, val loss: 0.744692862033844
Epoch 2140, training loss: 898.6231689453125 = 0.7326475381851196 + 100.0 * 8.978904724121094
Epoch 2140, val loss: 0.7430723905563354
Epoch 2150, training loss: 899.0296020507812 = 0.7309521436691284 + 100.0 * 8.982986450195312
Epoch 2150, val loss: 0.7415059208869934
Epoch 2160, training loss: 899.1620483398438 = 0.729249119758606 + 100.0 * 8.984328269958496
Epoch 2160, val loss: 0.7399443984031677
Epoch 2170, training loss: 899.004150390625 = 0.727551281452179 + 100.0 * 8.982766151428223
Epoch 2170, val loss: 0.7383357286453247
Epoch 2180, training loss: 899.302734375 = 0.725877046585083 + 100.0 * 8.98576831817627
Epoch 2180, val loss: 0.7367961406707764
Epoch 2190, training loss: 899.458740234375 = 0.7242242693901062 + 100.0 * 8.987344741821289
Epoch 2190, val loss: 0.7352606058120728
Epoch 2200, training loss: 899.5350341796875 = 0.722561776638031 + 100.0 * 8.98812484741211
Epoch 2200, val loss: 0.7337393760681152
Epoch 2210, training loss: 898.9088745117188 = 0.7209393978118896 + 100.0 * 8.981879234313965
Epoch 2210, val loss: 0.7322457432746887
Epoch 2220, training loss: 899.1749877929688 = 0.7193363308906555 + 100.0 * 8.984556198120117
Epoch 2220, val loss: 0.7307848334312439
Epoch 2230, training loss: 899.6078491210938 = 0.7177278995513916 + 100.0 * 8.988901138305664
Epoch 2230, val loss: 0.7293274402618408
Epoch 2240, training loss: 898.8958740234375 = 0.7161328792572021 + 100.0 * 8.981797218322754
Epoch 2240, val loss: 0.7278271913528442
Epoch 2250, training loss: 898.8634643554688 = 0.7145341038703918 + 100.0 * 8.981489181518555
Epoch 2250, val loss: 0.7263997793197632
Epoch 2260, training loss: 899.4003295898438 = 0.7129476070404053 + 100.0 * 8.986873626708984
Epoch 2260, val loss: 0.724973738193512
Epoch 2270, training loss: 900.205322265625 = 0.7114173769950867 + 100.0 * 8.994938850402832
Epoch 2270, val loss: 0.7236177921295166
Epoch 2280, training loss: 900.405029296875 = 0.7098919153213501 + 100.0 * 8.99695110321045
Epoch 2280, val loss: 0.7222557663917542
Epoch 2290, training loss: 900.0856323242188 = 0.7083675265312195 + 100.0 * 8.993772506713867
Epoch 2290, val loss: 0.7208623886108398
Epoch 2300, training loss: 900.594970703125 = 0.706868588924408 + 100.0 * 8.998881340026855
Epoch 2300, val loss: 0.7194802761077881
Epoch 2310, training loss: 900.7860717773438 = 0.7053788304328918 + 100.0 * 9.00080680847168
Epoch 2310, val loss: 0.7181340456008911
Epoch 2320, training loss: 900.7579956054688 = 0.703909158706665 + 100.0 * 9.000540733337402
Epoch 2320, val loss: 0.7168222069740295
Epoch 2330, training loss: 899.7643432617188 = 0.7024282217025757 + 100.0 * 8.990618705749512
Epoch 2330, val loss: 0.7154902815818787
Epoch 2340, training loss: 900.0904541015625 = 0.7010069489479065 + 100.0 * 8.993894577026367
Epoch 2340, val loss: 0.7142571210861206
Epoch 2350, training loss: 900.6013793945312 = 0.6995837092399597 + 100.0 * 8.999017715454102
Epoch 2350, val loss: 0.7129926681518555
Epoch 2360, training loss: 901.20849609375 = 0.6981726288795471 + 100.0 * 9.00510311126709
Epoch 2360, val loss: 0.711759626865387
Epoch 2370, training loss: 901.3074951171875 = 0.6967703104019165 + 100.0 * 9.006107330322266
Epoch 2370, val loss: 0.7105274796485901
Epoch 2380, training loss: 901.2545166015625 = 0.695367693901062 + 100.0 * 9.00559139251709
Epoch 2380, val loss: 0.7092994451522827
Epoch 2390, training loss: 901.48046875 = 0.6939821839332581 + 100.0 * 9.007864952087402
Epoch 2390, val loss: 0.7081024646759033
Epoch 2400, training loss: 901.5757446289062 = 0.6926229000091553 + 100.0 * 9.008831024169922
Epoch 2400, val loss: 0.7069104313850403
Epoch 2410, training loss: 901.5255737304688 = 0.6912723779678345 + 100.0 * 9.008342742919922
Epoch 2410, val loss: 0.7057409286499023
Epoch 2420, training loss: 901.866943359375 = 0.6899223923683167 + 100.0 * 9.011770248413086
Epoch 2420, val loss: 0.7045810222625732
Epoch 2430, training loss: 901.7830200195312 = 0.6885944604873657 + 100.0 * 9.010944366455078
Epoch 2430, val loss: 0.7034077644348145
Epoch 2440, training loss: 902.0403442382812 = 0.687262773513794 + 100.0 * 9.013530731201172
Epoch 2440, val loss: 0.7022775411605835
Epoch 2450, training loss: 902.1228637695312 = 0.6859523057937622 + 100.0 * 9.014369010925293
Epoch 2450, val loss: 0.7011618614196777
Epoch 2460, training loss: 902.1740112304688 = 0.684646725654602 + 100.0 * 9.014893531799316
Epoch 2460, val loss: 0.700023353099823
Epoch 2470, training loss: 901.8779296875 = 0.6833540201187134 + 100.0 * 9.011945724487305
Epoch 2470, val loss: 0.6988840103149414
Epoch 2480, training loss: 902.3351440429688 = 0.6820598840713501 + 100.0 * 9.016530990600586
Epoch 2480, val loss: 0.697777509689331
Epoch 2490, training loss: 902.7015991210938 = 0.6807965040206909 + 100.0 * 9.020208358764648
Epoch 2490, val loss: 0.6966939568519592
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.702463768115942
0.862566108816924
=== training gcn model ===
Epoch 0, training loss: 1029.4471435546875 = 1.097020149230957 + 100.0 * 10.283500671386719
Epoch 0, val loss: 1.0966603755950928
Epoch 10, training loss: 989.0344848632812 = 1.0968987941741943 + 100.0 * 9.879375457763672
Epoch 10, val loss: 1.096532940864563
Epoch 20, training loss: 968.4915161132812 = 1.0967835187911987 + 100.0 * 9.67394733428955
Epoch 20, val loss: 1.096408486366272
Epoch 30, training loss: 953.1547241210938 = 1.0966516733169556 + 100.0 * 9.520580291748047
Epoch 30, val loss: 1.0962642431259155
Epoch 40, training loss: 940.7326049804688 = 1.0965150594711304 + 100.0 * 9.396361351013184
Epoch 40, val loss: 1.096117377281189
Epoch 50, training loss: 930.595703125 = 1.0963774919509888 + 100.0 * 9.29499340057373
Epoch 50, val loss: 1.0959676504135132
Epoch 60, training loss: 922.1505126953125 = 1.0962388515472412 + 100.0 * 9.210542678833008
Epoch 60, val loss: 1.0958150625228882
Epoch 70, training loss: 915.1100463867188 = 1.0960896015167236 + 100.0 * 9.14013957977295
Epoch 70, val loss: 1.0956491231918335
Epoch 80, training loss: 909.2415771484375 = 1.0959502458572388 + 100.0 * 9.081456184387207
Epoch 80, val loss: 1.0954970121383667
Epoch 90, training loss: 904.5175170898438 = 1.0957969427108765 + 100.0 * 9.03421688079834
Epoch 90, val loss: 1.0953270196914673
Epoch 100, training loss: 900.2144165039062 = 1.0956318378448486 + 100.0 * 8.991188049316406
Epoch 100, val loss: 1.0951491594314575
Epoch 110, training loss: 896.5423583984375 = 1.0954421758651733 + 100.0 * 8.954468727111816
Epoch 110, val loss: 1.0949498414993286
Epoch 120, training loss: 893.4413452148438 = 1.0952460765838623 + 100.0 * 8.923460960388184
Epoch 120, val loss: 1.0947387218475342
Epoch 130, training loss: 890.7219848632812 = 1.0950366258621216 + 100.0 * 8.896269798278809
Epoch 130, val loss: 1.0945134162902832
Epoch 140, training loss: 888.5308837890625 = 1.0948035717010498 + 100.0 * 8.874361038208008
Epoch 140, val loss: 1.094262957572937
Epoch 150, training loss: 886.676513671875 = 1.094563364982605 + 100.0 * 8.855819702148438
Epoch 150, val loss: 1.094011664390564
Epoch 160, training loss: 885.0128784179688 = 1.0942950248718262 + 100.0 * 8.83918571472168
Epoch 160, val loss: 1.0937358140945435
Epoch 170, training loss: 883.4849853515625 = 1.0940120220184326 + 100.0 * 8.823909759521484
Epoch 170, val loss: 1.0934360027313232
Epoch 180, training loss: 882.3768310546875 = 1.0937227010726929 + 100.0 * 8.812830924987793
Epoch 180, val loss: 1.0931345224380493
Epoch 190, training loss: 881.168212890625 = 1.0934244394302368 + 100.0 * 8.800747871398926
Epoch 190, val loss: 1.092828631401062
Epoch 200, training loss: 880.4334716796875 = 1.0931209325790405 + 100.0 * 8.793403625488281
Epoch 200, val loss: 1.0925054550170898
Epoch 210, training loss: 879.690185546875 = 1.092805027961731 + 100.0 * 8.78597354888916
Epoch 210, val loss: 1.0921868085861206
Epoch 220, training loss: 879.0611572265625 = 1.0924791097640991 + 100.0 * 8.77968692779541
Epoch 220, val loss: 1.0918513536453247
Epoch 230, training loss: 878.6414184570312 = 1.0921506881713867 + 100.0 * 8.775492668151855
Epoch 230, val loss: 1.0915124416351318
Epoch 240, training loss: 878.0772705078125 = 1.0918235778808594 + 100.0 * 8.769854545593262
Epoch 240, val loss: 1.0911799669265747
Epoch 250, training loss: 877.5359497070312 = 1.0914874076843262 + 100.0 * 8.764444351196289
Epoch 250, val loss: 1.0908303260803223
Epoch 260, training loss: 877.0772094726562 = 1.0911517143249512 + 100.0 * 8.75986099243164
Epoch 260, val loss: 1.0904918909072876
Epoch 270, training loss: 876.8197021484375 = 1.0907928943634033 + 100.0 * 8.757288932800293
Epoch 270, val loss: 1.0901261568069458
Epoch 280, training loss: 876.8760986328125 = 1.090447187423706 + 100.0 * 8.757856369018555
Epoch 280, val loss: 1.0897663831710815
Epoch 290, training loss: 876.5404052734375 = 1.0900828838348389 + 100.0 * 8.75450325012207
Epoch 290, val loss: 1.0893961191177368
Epoch 300, training loss: 876.782958984375 = 1.0897232294082642 + 100.0 * 8.756932258605957
Epoch 300, val loss: 1.089034080505371
Epoch 310, training loss: 876.1030883789062 = 1.0893312692642212 + 100.0 * 8.750137329101562
Epoch 310, val loss: 1.088646411895752
Epoch 320, training loss: 876.1492919921875 = 1.0889580249786377 + 100.0 * 8.750603675842285
Epoch 320, val loss: 1.0882548093795776
Epoch 330, training loss: 876.5283813476562 = 1.088588833808899 + 100.0 * 8.754398345947266
Epoch 330, val loss: 1.0878719091415405
Epoch 340, training loss: 876.2290649414062 = 1.0882034301757812 + 100.0 * 8.751408576965332
Epoch 340, val loss: 1.0874946117401123
Epoch 350, training loss: 876.6770629882812 = 1.0877972841262817 + 100.0 * 8.755892753601074
Epoch 350, val loss: 1.0870877504348755
Epoch 360, training loss: 876.545166015625 = 1.0873860120773315 + 100.0 * 8.75457763671875
Epoch 360, val loss: 1.086670160293579
Epoch 370, training loss: 876.9642333984375 = 1.0869728326797485 + 100.0 * 8.758772850036621
Epoch 370, val loss: 1.086244821548462
Epoch 380, training loss: 877.1431884765625 = 1.086542010307312 + 100.0 * 8.760566711425781
Epoch 380, val loss: 1.0858194828033447
Epoch 390, training loss: 877.4273681640625 = 1.0861207246780396 + 100.0 * 8.763412475585938
Epoch 390, val loss: 1.0854004621505737
Epoch 400, training loss: 877.42626953125 = 1.0856890678405762 + 100.0 * 8.763405799865723
Epoch 400, val loss: 1.0849721431732178
Epoch 410, training loss: 877.7734375 = 1.0852423906326294 + 100.0 * 8.766881942749023
Epoch 410, val loss: 1.084511637687683
Epoch 420, training loss: 877.738525390625 = 1.084795355796814 + 100.0 * 8.7665376663208
Epoch 420, val loss: 1.0840752124786377
Epoch 430, training loss: 877.77978515625 = 1.0843346118927002 + 100.0 * 8.76695442199707
Epoch 430, val loss: 1.0836137533187866
Epoch 440, training loss: 878.1519165039062 = 1.0838905572891235 + 100.0 * 8.77068042755127
Epoch 440, val loss: 1.0831714868545532
Epoch 450, training loss: 878.2213745117188 = 1.0834170579910278 + 100.0 * 8.771379470825195
Epoch 450, val loss: 1.0827138423919678
Epoch 460, training loss: 878.422607421875 = 1.0829534530639648 + 100.0 * 8.773396492004395
Epoch 460, val loss: 1.0822511911392212
Epoch 470, training loss: 878.5283203125 = 1.082488775253296 + 100.0 * 8.774458885192871
Epoch 470, val loss: 1.081788182258606
Epoch 480, training loss: 879.0220336914062 = 1.08201265335083 + 100.0 * 8.779399871826172
Epoch 480, val loss: 1.0813242197036743
Epoch 490, training loss: 879.0222778320312 = 1.0815324783325195 + 100.0 * 8.779407501220703
Epoch 490, val loss: 1.0808403491973877
Epoch 500, training loss: 879.59765625 = 1.0810465812683105 + 100.0 * 8.785165786743164
Epoch 500, val loss: 1.0803579092025757
Epoch 510, training loss: 880.0596313476562 = 1.0805174112319946 + 100.0 * 8.789791107177734
Epoch 510, val loss: 1.0798438787460327
Epoch 520, training loss: 879.3623046875 = 1.0800089836120605 + 100.0 * 8.782822608947754
Epoch 520, val loss: 1.0793253183364868
Epoch 530, training loss: 880.4306030273438 = 1.0795197486877441 + 100.0 * 8.793510437011719
Epoch 530, val loss: 1.078851342201233
Epoch 540, training loss: 880.9848022460938 = 1.0789927244186401 + 100.0 * 8.799057960510254
Epoch 540, val loss: 1.0783287286758423
Epoch 550, training loss: 881.5699462890625 = 1.0784786939620972 + 100.0 * 8.804914474487305
Epoch 550, val loss: 1.077820897102356
Epoch 560, training loss: 881.5322875976562 = 1.0779335498809814 + 100.0 * 8.804543495178223
Epoch 560, val loss: 1.0772814750671387
Epoch 570, training loss: 881.4111328125 = 1.0774019956588745 + 100.0 * 8.803337097167969
Epoch 570, val loss: 1.076762080192566
Epoch 580, training loss: 879.9337768554688 = 1.076845645904541 + 100.0 * 8.788569450378418
Epoch 580, val loss: 1.0761889219284058
Epoch 590, training loss: 882.6292114257812 = 1.0763435363769531 + 100.0 * 8.815528869628906
Epoch 590, val loss: 1.0757261514663696
Epoch 600, training loss: 881.7058715820312 = 1.075729489326477 + 100.0 * 8.80630111694336
Epoch 600, val loss: 1.075119972229004
Epoch 610, training loss: 882.3092651367188 = 1.0752053260803223 + 100.0 * 8.81234073638916
Epoch 610, val loss: 1.0745939016342163
Epoch 620, training loss: 882.9002075195312 = 1.0746656656265259 + 100.0 * 8.818255424499512
Epoch 620, val loss: 1.074066400527954
Epoch 630, training loss: 883.3277587890625 = 1.0741270780563354 + 100.0 * 8.82253646850586
Epoch 630, val loss: 1.0735398530960083
Epoch 640, training loss: 883.6949462890625 = 1.073562741279602 + 100.0 * 8.826213836669922
Epoch 640, val loss: 1.0729870796203613
Epoch 650, training loss: 883.980224609375 = 1.0730030536651611 + 100.0 * 8.829071998596191
Epoch 650, val loss: 1.0724247694015503
Epoch 660, training loss: 884.4243774414062 = 1.0724231004714966 + 100.0 * 8.833518981933594
Epoch 660, val loss: 1.0718746185302734
Epoch 670, training loss: 884.4158935546875 = 1.0718616247177124 + 100.0 * 8.833440780639648
Epoch 670, val loss: 1.0713248252868652
Epoch 680, training loss: 885.1199340820312 = 1.0712850093841553 + 100.0 * 8.840486526489258
Epoch 680, val loss: 1.0707533359527588
Epoch 690, training loss: 885.1683349609375 = 1.070704460144043 + 100.0 * 8.84097671508789
Epoch 690, val loss: 1.0701781511306763
Epoch 700, training loss: 885.724853515625 = 1.0701191425323486 + 100.0 * 8.84654712677002
Epoch 700, val loss: 1.06960928440094
Epoch 710, training loss: 886.0428466796875 = 1.0695282220840454 + 100.0 * 8.849733352661133
Epoch 710, val loss: 1.0690312385559082
Epoch 720, training loss: 885.082763671875 = 1.0688458681106567 + 100.0 * 8.840139389038086
Epoch 720, val loss: 1.0683635473251343
Epoch 730, training loss: 885.1815185546875 = 1.0683162212371826 + 100.0 * 8.841132164001465
Epoch 730, val loss: 1.0678339004516602
Epoch 740, training loss: 885.5582885742188 = 1.067665696144104 + 100.0 * 8.844905853271484
Epoch 740, val loss: 1.0672051906585693
Epoch 750, training loss: 886.696044921875 = 1.0670732259750366 + 100.0 * 8.856289863586426
Epoch 750, val loss: 1.0666288137435913
Epoch 760, training loss: 887.4254760742188 = 1.0664550065994263 + 100.0 * 8.863590240478516
Epoch 760, val loss: 1.0660306215286255
Epoch 770, training loss: 887.7711791992188 = 1.0658295154571533 + 100.0 * 8.867053031921387
Epoch 770, val loss: 1.0654017925262451
Epoch 780, training loss: 887.8456420898438 = 1.0651952028274536 + 100.0 * 8.867804527282715
Epoch 780, val loss: 1.0647835731506348
Epoch 790, training loss: 888.623046875 = 1.064559817314148 + 100.0 * 8.875584602355957
Epoch 790, val loss: 1.064176082611084
Epoch 800, training loss: 888.6815185546875 = 1.0639160871505737 + 100.0 * 8.876175880432129
Epoch 800, val loss: 1.0635508298873901
Epoch 810, training loss: 889.7730102539062 = 1.0632741451263428 + 100.0 * 8.887097358703613
Epoch 810, val loss: 1.0629258155822754
Epoch 820, training loss: 888.4408569335938 = 1.0626500844955444 + 100.0 * 8.87378215789795
Epoch 820, val loss: 1.0623035430908203
Epoch 830, training loss: 887.8651123046875 = 1.061893343925476 + 100.0 * 8.868032455444336
Epoch 830, val loss: 1.0615967512130737
Epoch 840, training loss: 888.4663696289062 = 1.0612893104553223 + 100.0 * 8.874051094055176
Epoch 840, val loss: 1.0610003471374512
Epoch 850, training loss: 889.46630859375 = 1.0606416463851929 + 100.0 * 8.884056091308594
Epoch 850, val loss: 1.0603704452514648
Epoch 860, training loss: 889.2694091796875 = 1.0599993467330933 + 100.0 * 8.882094383239746
Epoch 860, val loss: 1.0597641468048096
Epoch 870, training loss: 890.2341918945312 = 1.0593551397323608 + 100.0 * 8.891748428344727
Epoch 870, val loss: 1.0591336488723755
Epoch 880, training loss: 890.9904174804688 = 1.0586724281311035 + 100.0 * 8.899317741394043
Epoch 880, val loss: 1.0584795475006104
Epoch 890, training loss: 890.5740356445312 = 1.057962417602539 + 100.0 * 8.895160675048828
Epoch 890, val loss: 1.0578079223632812
Epoch 900, training loss: 891.1657104492188 = 1.0573245286941528 + 100.0 * 8.901083946228027
Epoch 900, val loss: 1.057181477546692
Epoch 910, training loss: 891.5097045898438 = 1.0566505193710327 + 100.0 * 8.90453052520752
Epoch 910, val loss: 1.0565338134765625
Epoch 920, training loss: 892.3209838867188 = 1.0559706687927246 + 100.0 * 8.912650108337402
Epoch 920, val loss: 1.0558862686157227
Epoch 930, training loss: 892.3991088867188 = 1.0553040504455566 + 100.0 * 8.913437843322754
Epoch 930, val loss: 1.0552246570587158
Epoch 940, training loss: 892.5335693359375 = 1.0546151399612427 + 100.0 * 8.914789199829102
Epoch 940, val loss: 1.054569959640503
Epoch 950, training loss: 892.7142333984375 = 1.053924560546875 + 100.0 * 8.916603088378906
Epoch 950, val loss: 1.053900122642517
Epoch 960, training loss: 891.623046875 = 1.0531820058822632 + 100.0 * 8.905698776245117
Epoch 960, val loss: 1.0531855821609497
Epoch 970, training loss: 891.09765625 = 1.0524693727493286 + 100.0 * 8.90045166015625
Epoch 970, val loss: 1.0525085926055908
Epoch 980, training loss: 892.9208984375 = 1.0518254041671753 + 100.0 * 8.91869068145752
Epoch 980, val loss: 1.0518810749053955
Epoch 990, training loss: 893.5673217773438 = 1.0510843992233276 + 100.0 * 8.925162315368652
Epoch 990, val loss: 1.0511704683303833
Epoch 1000, training loss: 894.0658569335938 = 1.0503778457641602 + 100.0 * 8.930154800415039
Epoch 1000, val loss: 1.050489068031311
Epoch 1010, training loss: 894.1043701171875 = 1.049666166305542 + 100.0 * 8.930546760559082
Epoch 1010, val loss: 1.0498182773590088
Epoch 1020, training loss: 894.2280883789062 = 1.0489658117294312 + 100.0 * 8.931791305541992
Epoch 1020, val loss: 1.0491365194320679
Epoch 1030, training loss: 894.5487060546875 = 1.048263669013977 + 100.0 * 8.935004234313965
Epoch 1030, val loss: 1.0484626293182373
Epoch 1040, training loss: 894.6173706054688 = 1.0475598573684692 + 100.0 * 8.935698509216309
Epoch 1040, val loss: 1.0477851629257202
Epoch 1050, training loss: 894.8699340820312 = 1.046847939491272 + 100.0 * 8.938230514526367
Epoch 1050, val loss: 1.0470879077911377
Epoch 1060, training loss: 895.0103149414062 = 1.04612135887146 + 100.0 * 8.939641952514648
Epoch 1060, val loss: 1.0463917255401611
Epoch 1070, training loss: 895.5881958007812 = 1.0454143285751343 + 100.0 * 8.945427894592285
Epoch 1070, val loss: 1.0457056760787964
Epoch 1080, training loss: 895.86572265625 = 1.0446813106536865 + 100.0 * 8.948210716247559
Epoch 1080, val loss: 1.0450068712234497
Epoch 1090, training loss: 896.324951171875 = 1.0439642667770386 + 100.0 * 8.952810287475586
Epoch 1090, val loss: 1.0443059206008911
Epoch 1100, training loss: 897.4385986328125 = 1.043236494064331 + 100.0 * 8.963953971862793
Epoch 1100, val loss: 1.043591022491455
Epoch 1110, training loss: 888.03466796875 = 1.042100429534912 + 100.0 * 8.869925498962402
Epoch 1110, val loss: 1.0425326824188232
Epoch 1120, training loss: 898.9419555664062 = 1.0419245958328247 + 100.0 * 8.979000091552734
Epoch 1120, val loss: 1.0423275232315063
Epoch 1130, training loss: 890.0525512695312 = 1.0409308671951294 + 100.0 * 8.890115737915039
Epoch 1130, val loss: 1.041417121887207
Epoch 1140, training loss: 892.7222290039062 = 1.0402965545654297 + 100.0 * 8.91681957244873
Epoch 1140, val loss: 1.040776014328003
Epoch 1150, training loss: 891.2694091796875 = 1.0396023988723755 + 100.0 * 8.902297973632812
Epoch 1150, val loss: 1.0401227474212646
Epoch 1160, training loss: 893.5042114257812 = 1.038866400718689 + 100.0 * 8.924653053283691
Epoch 1160, val loss: 1.0393887758255005
Epoch 1170, training loss: 893.40380859375 = 1.038151741027832 + 100.0 * 8.923656463623047
Epoch 1170, val loss: 1.0387135744094849
Epoch 1180, training loss: 892.1614379882812 = 1.0373202562332153 + 100.0 * 8.91124153137207
Epoch 1180, val loss: 1.037909746170044
Epoch 1190, training loss: 894.4222412109375 = 1.0366169214248657 + 100.0 * 8.933856010437012
Epoch 1190, val loss: 1.0372233390808105
Epoch 1200, training loss: 895.5062866210938 = 1.035870909690857 + 100.0 * 8.944704055786133
Epoch 1200, val loss: 1.0365055799484253
Epoch 1210, training loss: 895.8706665039062 = 1.035102128982544 + 100.0 * 8.948355674743652
Epoch 1210, val loss: 1.0357742309570312
Epoch 1220, training loss: 895.848876953125 = 1.0343339443206787 + 100.0 * 8.948144912719727
Epoch 1220, val loss: 1.0350263118743896
Epoch 1230, training loss: 896.2359008789062 = 1.0335659980773926 + 100.0 * 8.95202350616455
Epoch 1230, val loss: 1.0342921018600464
Epoch 1240, training loss: 896.6509399414062 = 1.0328047275543213 + 100.0 * 8.956181526184082
Epoch 1240, val loss: 1.0335532426834106
Epoch 1250, training loss: 897.2220458984375 = 1.0320467948913574 + 100.0 * 8.961899757385254
Epoch 1250, val loss: 1.032824993133545
Epoch 1260, training loss: 897.0758666992188 = 1.0312546491622925 + 100.0 * 8.96044635772705
Epoch 1260, val loss: 1.0320676565170288
Epoch 1270, training loss: 896.6217041015625 = 1.030462622642517 + 100.0 * 8.955912590026855
Epoch 1270, val loss: 1.0313128232955933
Epoch 1280, training loss: 896.1617431640625 = 1.0296807289123535 + 100.0 * 8.95132064819336
Epoch 1280, val loss: 1.0305531024932861
Epoch 1290, training loss: 897.1995849609375 = 1.0288678407669067 + 100.0 * 8.96170711517334
Epoch 1290, val loss: 1.0297578573226929
Epoch 1300, training loss: 898.2404174804688 = 1.028079628944397 + 100.0 * 8.972123146057129
Epoch 1300, val loss: 1.0290099382400513
Epoch 1310, training loss: 898.2020874023438 = 1.0272656679153442 + 100.0 * 8.971748352050781
Epoch 1310, val loss: 1.0282182693481445
Epoch 1320, training loss: 898.833740234375 = 1.0264496803283691 + 100.0 * 8.978073120117188
Epoch 1320, val loss: 1.027446985244751
Epoch 1330, training loss: 899.3941040039062 = 1.0256091356277466 + 100.0 * 8.983684539794922
Epoch 1330, val loss: 1.0266309976577759
Epoch 1340, training loss: 899.410888671875 = 1.024731993675232 + 100.0 * 8.983861923217773
Epoch 1340, val loss: 1.0257956981658936
Epoch 1350, training loss: 899.9552001953125 = 1.0238546133041382 + 100.0 * 8.989313125610352
Epoch 1350, val loss: 1.0249478816986084
Epoch 1360, training loss: 899.9097290039062 = 1.0228997468948364 + 100.0 * 8.988868713378906
Epoch 1360, val loss: 1.0240272283554077
Epoch 1370, training loss: 898.4273681640625 = 1.0218687057495117 + 100.0 * 8.974055290222168
Epoch 1370, val loss: 1.02303147315979
Epoch 1380, training loss: 898.5453491210938 = 1.0208539962768555 + 100.0 * 8.975244522094727
Epoch 1380, val loss: 1.022053837776184
Epoch 1390, training loss: 899.3464965820312 = 1.0198118686676025 + 100.0 * 8.983266830444336
Epoch 1390, val loss: 1.0210468769073486
Epoch 1400, training loss: 900.0809326171875 = 1.0187009572982788 + 100.0 * 8.990622520446777
Epoch 1400, val loss: 1.0199611186981201
Epoch 1410, training loss: 900.777587890625 = 1.0174885988235474 + 100.0 * 8.997600555419922
Epoch 1410, val loss: 1.0187854766845703
Epoch 1420, training loss: 901.1234130859375 = 1.0161354541778564 + 100.0 * 9.001072883605957
Epoch 1420, val loss: 1.017494797706604
Epoch 1430, training loss: 901.164306640625 = 1.0147948265075684 + 100.0 * 9.001495361328125
Epoch 1430, val loss: 1.0162322521209717
Epoch 1440, training loss: 900.0773315429688 = 1.0135080814361572 + 100.0 * 8.990638732910156
Epoch 1440, val loss: 1.0149400234222412
Epoch 1450, training loss: 899.8967895507812 = 1.0108405351638794 + 100.0 * 8.988859176635742
Epoch 1450, val loss: 1.0123544931411743
Epoch 1460, training loss: 900.4639892578125 = 1.0077358484268188 + 100.0 * 8.994562149047852
Epoch 1460, val loss: 1.009434461593628
Epoch 1470, training loss: 900.9820556640625 = 1.0048385858535767 + 100.0 * 8.999772071838379
Epoch 1470, val loss: 1.006757378578186
Epoch 1480, training loss: 901.7955322265625 = 1.0020689964294434 + 100.0 * 9.0079345703125
Epoch 1480, val loss: 1.004195213317871
Epoch 1490, training loss: 902.4462890625 = 0.9994515776634216 + 100.0 * 9.0144681930542
Epoch 1490, val loss: 1.0017668008804321
Epoch 1500, training loss: 902.5335083007812 = 0.9969244599342346 + 100.0 * 9.015365600585938
Epoch 1500, val loss: 0.9994157552719116
Epoch 1510, training loss: 902.534912109375 = 0.9944807887077332 + 100.0 * 9.015403747558594
Epoch 1510, val loss: 0.9971306920051575
Epoch 1520, training loss: 902.8018798828125 = 0.9921761155128479 + 100.0 * 9.018096923828125
Epoch 1520, val loss: 0.9949958324432373
Epoch 1530, training loss: 902.837890625 = 0.9898943305015564 + 100.0 * 9.01848030090332
Epoch 1530, val loss: 0.9928691387176514
Epoch 1540, training loss: 902.536376953125 = 0.9875876307487488 + 100.0 * 9.015487670898438
Epoch 1540, val loss: 0.9907177090644836
Epoch 1550, training loss: 903.1416625976562 = 0.9853114485740662 + 100.0 * 9.021563529968262
Epoch 1550, val loss: 0.9885934591293335
Epoch 1560, training loss: 903.8765258789062 = 0.9830721616744995 + 100.0 * 9.028934478759766
Epoch 1560, val loss: 0.9865082502365112
Epoch 1570, training loss: 903.7667236328125 = 0.9808529615402222 + 100.0 * 9.02785873413086
Epoch 1570, val loss: 0.9844207763671875
Epoch 1580, training loss: 903.91064453125 = 0.9786405563354492 + 100.0 * 9.029319763183594
Epoch 1580, val loss: 0.9823760986328125
Epoch 1590, training loss: 903.9347534179688 = 0.9764008522033691 + 100.0 * 9.029583930969238
Epoch 1590, val loss: 0.9802536368370056
Epoch 1600, training loss: 903.348388671875 = 0.9743143916130066 + 100.0 * 9.023740768432617
Epoch 1600, val loss: 0.9783287048339844
Epoch 1610, training loss: 899.779052734375 = 0.9721029996871948 + 100.0 * 8.988069534301758
Epoch 1610, val loss: 0.9762538075447083
Epoch 1620, training loss: 901.4146728515625 = 0.9698476791381836 + 100.0 * 9.004447937011719
Epoch 1620, val loss: 0.9741566777229309
Epoch 1630, training loss: 902.3451538085938 = 0.9676490426063538 + 100.0 * 9.013774871826172
Epoch 1630, val loss: 0.9721099734306335
Epoch 1640, training loss: 903.17431640625 = 0.965421199798584 + 100.0 * 9.022089004516602
Epoch 1640, val loss: 0.9700360298156738
Epoch 1650, training loss: 904.005859375 = 0.9632381796836853 + 100.0 * 9.030426025390625
Epoch 1650, val loss: 0.9679986238479614
Epoch 1660, training loss: 904.2330932617188 = 0.9610323309898376 + 100.0 * 9.032720565795898
Epoch 1660, val loss: 0.965946614742279
Epoch 1670, training loss: 904.3253784179688 = 0.9588147401809692 + 100.0 * 9.033665657043457
Epoch 1670, val loss: 0.963882565498352
Epoch 1680, training loss: 904.606201171875 = 0.956590473651886 + 100.0 * 9.03649616241455
Epoch 1680, val loss: 0.9618126749992371
Epoch 1690, training loss: 905.0989990234375 = 0.9543373584747314 + 100.0 * 9.041446685791016
Epoch 1690, val loss: 0.9597072601318359
Epoch 1700, training loss: 905.0870361328125 = 0.952064573764801 + 100.0 * 9.041349411010742
Epoch 1700, val loss: 0.9575981497764587
Epoch 1710, training loss: 904.9606323242188 = 0.949795126914978 + 100.0 * 9.040108680725098
Epoch 1710, val loss: 0.955481231212616
Epoch 1720, training loss: 905.4739990234375 = 0.9475073218345642 + 100.0 * 9.045265197753906
Epoch 1720, val loss: 0.9533352851867676
Epoch 1730, training loss: 905.4970092773438 = 0.9451910257339478 + 100.0 * 9.045517921447754
Epoch 1730, val loss: 0.9511802792549133
Epoch 1740, training loss: 904.7921142578125 = 0.9428545236587524 + 100.0 * 9.038492202758789
Epoch 1740, val loss: 0.9490148425102234
Epoch 1750, training loss: 905.4534912109375 = 0.940517783164978 + 100.0 * 9.045129776000977
Epoch 1750, val loss: 0.9468308091163635
Epoch 1760, training loss: 905.75390625 = 0.9382117986679077 + 100.0 * 9.04815673828125
Epoch 1760, val loss: 0.9446830153465271
Epoch 1770, training loss: 906.2867431640625 = 0.9358549118041992 + 100.0 * 9.053508758544922
Epoch 1770, val loss: 0.9424782991409302
Epoch 1780, training loss: 906.4129028320312 = 0.9334949851036072 + 100.0 * 9.054794311523438
Epoch 1780, val loss: 0.9402792453765869
Epoch 1790, training loss: 906.6298828125 = 0.9310976266860962 + 100.0 * 9.056987762451172
Epoch 1790, val loss: 0.9380428194999695
Epoch 1800, training loss: 905.9502563476562 = 0.9286903142929077 + 100.0 * 9.050215721130371
Epoch 1800, val loss: 0.9357868432998657
Epoch 1810, training loss: 906.334228515625 = 0.926304280757904 + 100.0 * 9.054079055786133
Epoch 1810, val loss: 0.9335715770721436
Epoch 1820, training loss: 907.1390991210938 = 0.9238795042037964 + 100.0 * 9.062151908874512
Epoch 1820, val loss: 0.9312944412231445
Epoch 1830, training loss: 906.7493286132812 = 0.9214172959327698 + 100.0 * 9.058279037475586
Epoch 1830, val loss: 0.9289946556091309
Epoch 1840, training loss: 906.735107421875 = 0.9189568161964417 + 100.0 * 9.058161735534668
Epoch 1840, val loss: 0.9266870021820068
Epoch 1850, training loss: 906.7857055664062 = 0.9164931774139404 + 100.0 * 9.05869197845459
Epoch 1850, val loss: 0.924376904964447
Epoch 1860, training loss: 907.4657592773438 = 0.9140188694000244 + 100.0 * 9.06551742553711
Epoch 1860, val loss: 0.9220576286315918
Epoch 1870, training loss: 907.513427734375 = 0.9115222096443176 + 100.0 * 9.066019058227539
Epoch 1870, val loss: 0.9197061657905579
Epoch 1880, training loss: 907.39453125 = 0.9089875817298889 + 100.0 * 9.064855575561523
Epoch 1880, val loss: 0.9173499941825867
Epoch 1890, training loss: 907.66748046875 = 0.9064640998840332 + 100.0 * 9.067609786987305
Epoch 1890, val loss: 0.9149965643882751
Epoch 1900, training loss: 908.26513671875 = 0.9039268493652344 + 100.0 * 9.073612213134766
Epoch 1900, val loss: 0.9126051664352417
Epoch 1910, training loss: 908.31201171875 = 0.901363730430603 + 100.0 * 9.074106216430664
Epoch 1910, val loss: 0.9102286696434021
Epoch 1920, training loss: 907.0162353515625 = 0.8987855911254883 + 100.0 * 9.061174392700195
Epoch 1920, val loss: 0.9077957272529602
Epoch 1930, training loss: 904.76904296875 = 0.8960163593292236 + 100.0 * 9.03873062133789
Epoch 1930, val loss: 0.9052371382713318
Epoch 1940, training loss: 902.5006103515625 = 0.8936923146247864 + 100.0 * 9.016069412231445
Epoch 1940, val loss: 0.9030030965805054
Epoch 1950, training loss: 904.0287475585938 = 0.8913414478302002 + 100.0 * 9.031373977661133
Epoch 1950, val loss: 0.9008156061172485
Epoch 1960, training loss: 903.5828857421875 = 0.8884831070899963 + 100.0 * 9.026944160461426
Epoch 1960, val loss: 0.8981605768203735
Epoch 1970, training loss: 905.149658203125 = 0.886044442653656 + 100.0 * 9.042635917663574
Epoch 1970, val loss: 0.8958820700645447
Epoch 1980, training loss: 903.6663208007812 = 0.8833927512168884 + 100.0 * 9.02782917022705
Epoch 1980, val loss: 0.8933913111686707
Epoch 1990, training loss: 904.7998657226562 = 0.8808869123458862 + 100.0 * 9.039190292358398
Epoch 1990, val loss: 0.8910379409790039
Epoch 2000, training loss: 906.3416137695312 = 0.8783080577850342 + 100.0 * 9.054633140563965
Epoch 2000, val loss: 0.8886142373085022
Epoch 2010, training loss: 907.121826171875 = 0.8757235407829285 + 100.0 * 9.062460899353027
Epoch 2010, val loss: 0.8861846923828125
Epoch 2020, training loss: 907.5362548828125 = 0.8731356263160706 + 100.0 * 9.066631317138672
Epoch 2020, val loss: 0.8837670683860779
Epoch 2030, training loss: 907.3880615234375 = 0.8705058693885803 + 100.0 * 9.065176010131836
Epoch 2030, val loss: 0.8813005089759827
Epoch 2040, training loss: 907.9263305664062 = 0.8679056167602539 + 100.0 * 9.070584297180176
Epoch 2040, val loss: 0.8788614273071289
Epoch 2050, training loss: 908.0867309570312 = 0.8652750253677368 + 100.0 * 9.072214126586914
Epoch 2050, val loss: 0.8764027953147888
Epoch 2060, training loss: 908.37744140625 = 0.8626500368118286 + 100.0 * 9.07514762878418
Epoch 2060, val loss: 0.8739479184150696
Epoch 2070, training loss: 908.0753784179688 = 0.8600068688392639 + 100.0 * 9.07215404510498
Epoch 2070, val loss: 0.8714776039123535
Epoch 2080, training loss: 908.5513916015625 = 0.8573718667030334 + 100.0 * 9.076940536499023
Epoch 2080, val loss: 0.8690153360366821
Epoch 2090, training loss: 908.7686157226562 = 0.8547390699386597 + 100.0 * 9.07913875579834
Epoch 2090, val loss: 0.8665315508842468
Epoch 2100, training loss: 908.5194702148438 = 0.8520748615264893 + 100.0 * 9.076674461364746
Epoch 2100, val loss: 0.8640538454055786
Epoch 2110, training loss: 908.9262084960938 = 0.8494501709938049 + 100.0 * 9.080767631530762
Epoch 2110, val loss: 0.8615795373916626
Epoch 2120, training loss: 909.1201171875 = 0.8467881679534912 + 100.0 * 9.082733154296875
Epoch 2120, val loss: 0.8590799570083618
Epoch 2130, training loss: 908.93603515625 = 0.8441473841667175 + 100.0 * 9.080918312072754
Epoch 2130, val loss: 0.8566160202026367
Epoch 2140, training loss: 909.0360107421875 = 0.8415457010269165 + 100.0 * 9.081944465637207
Epoch 2140, val loss: 0.8541797399520874
Epoch 2150, training loss: 909.2325439453125 = 0.8388908505439758 + 100.0 * 9.08393669128418
Epoch 2150, val loss: 0.8516924977302551
Epoch 2160, training loss: 909.8416137695312 = 0.8362018465995789 + 100.0 * 9.090054512023926
Epoch 2160, val loss: 0.8491941690444946
Epoch 2170, training loss: 908.8562622070312 = 0.8335316777229309 + 100.0 * 9.08022689819336
Epoch 2170, val loss: 0.8467243313789368
Epoch 2180, training loss: 909.5301513671875 = 0.8309409618377686 + 100.0 * 9.086992263793945
Epoch 2180, val loss: 0.8442764282226562
Epoch 2190, training loss: 909.8207397460938 = 0.8283419013023376 + 100.0 * 9.089923858642578
Epoch 2190, val loss: 0.8418578505516052
Epoch 2200, training loss: 910.1087646484375 = 0.8257120847702026 + 100.0 * 9.092830657958984
Epoch 2200, val loss: 0.8394031524658203
Epoch 2210, training loss: 910.3701171875 = 0.8230711221694946 + 100.0 * 9.095470428466797
Epoch 2210, val loss: 0.8369290232658386
Epoch 2220, training loss: 909.93701171875 = 0.8204250931739807 + 100.0 * 9.091165542602539
Epoch 2220, val loss: 0.8344720602035522
Epoch 2230, training loss: 910.259765625 = 0.8178042769432068 + 100.0 * 9.094419479370117
Epoch 2230, val loss: 0.8320344090461731
Epoch 2240, training loss: 910.7198486328125 = 0.8151712417602539 + 100.0 * 9.09904670715332
Epoch 2240, val loss: 0.8295770287513733
Epoch 2250, training loss: 909.8431396484375 = 0.8124435544013977 + 100.0 * 9.090307235717773
Epoch 2250, val loss: 0.8270508050918579
Epoch 2260, training loss: 910.0759887695312 = 0.8098470568656921 + 100.0 * 9.092660903930664
Epoch 2260, val loss: 0.8246131539344788
Epoch 2270, training loss: 910.6776733398438 = 0.8072497248649597 + 100.0 * 9.09870433807373
Epoch 2270, val loss: 0.8221883773803711
Epoch 2280, training loss: 910.6015014648438 = 0.8046326041221619 + 100.0 * 9.097969055175781
Epoch 2280, val loss: 0.8197638392448425
Epoch 2290, training loss: 910.8373413085938 = 0.8020203709602356 + 100.0 * 9.100353240966797
Epoch 2290, val loss: 0.8173329830169678
Epoch 2300, training loss: 909.314453125 = 0.7993379831314087 + 100.0 * 9.085151672363281
Epoch 2300, val loss: 0.8148442506790161
Epoch 2310, training loss: 908.3026123046875 = 0.7967190742492676 + 100.0 * 9.075058937072754
Epoch 2310, val loss: 0.8124589920043945
Epoch 2320, training loss: 908.9784545898438 = 0.7941959500312805 + 100.0 * 9.081842422485352
Epoch 2320, val loss: 0.8100885152816772
Epoch 2330, training loss: 909.5247192382812 = 0.79164719581604 + 100.0 * 9.08733081817627
Epoch 2330, val loss: 0.8076920509338379
Epoch 2340, training loss: 910.2636108398438 = 0.7890554070472717 + 100.0 * 9.094745635986328
Epoch 2340, val loss: 0.8052951693534851
Epoch 2350, training loss: 910.83203125 = 0.7864769697189331 + 100.0 * 9.100455284118652
Epoch 2350, val loss: 0.8029031753540039
Epoch 2360, training loss: 910.4953002929688 = 0.7838820219039917 + 100.0 * 9.097114562988281
Epoch 2360, val loss: 0.8004977703094482
Epoch 2370, training loss: 911.0032958984375 = 0.781332790851593 + 100.0 * 9.102219581604004
Epoch 2370, val loss: 0.7981417179107666
Epoch 2380, training loss: 911.4744873046875 = 0.778791606426239 + 100.0 * 9.106956481933594
Epoch 2380, val loss: 0.7957929372787476
Epoch 2390, training loss: 911.4465942382812 = 0.7762417197227478 + 100.0 * 9.106703758239746
Epoch 2390, val loss: 0.793440043926239
Epoch 2400, training loss: 911.385009765625 = 0.7737142443656921 + 100.0 * 9.106112480163574
Epoch 2400, val loss: 0.7911028265953064
Epoch 2410, training loss: 911.6015014648438 = 0.7712068557739258 + 100.0 * 9.10830307006836
Epoch 2410, val loss: 0.788804292678833
Epoch 2420, training loss: 911.7041625976562 = 0.7687254548072815 + 100.0 * 9.109354019165039
Epoch 2420, val loss: 0.7865126729011536
Epoch 2430, training loss: 911.8944702148438 = 0.7662519812583923 + 100.0 * 9.111282348632812
Epoch 2430, val loss: 0.7842420935630798
Epoch 2440, training loss: 911.73388671875 = 0.7638111710548401 + 100.0 * 9.109701156616211
Epoch 2440, val loss: 0.7819862961769104
Epoch 2450, training loss: 911.7771606445312 = 0.7613699436187744 + 100.0 * 9.11015796661377
Epoch 2450, val loss: 0.7797502875328064
Epoch 2460, training loss: 911.9400634765625 = 0.7589672803878784 + 100.0 * 9.111810684204102
Epoch 2460, val loss: 0.7775420546531677
Epoch 2470, training loss: 912.2868041992188 = 0.7565742135047913 + 100.0 * 9.115302085876465
Epoch 2470, val loss: 0.775347113609314
Epoch 2480, training loss: 912.5628662109375 = 0.7541921138763428 + 100.0 * 9.118086814880371
Epoch 2480, val loss: 0.7731620073318481
Epoch 2490, training loss: 912.3128662109375 = 0.751829206943512 + 100.0 * 9.115610122680664
Epoch 2490, val loss: 0.7709948420524597
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.6520289855072463
0.8631456929652975
The final CL Acc:0.63643, 0.06129, The final GNN Acc:0.86310, 0.00042
Begin epxeriment: noisy_level: 0.2 cont_weight: 100 epoch: der1:0.4 der2:0 dfr1:0.4 dfr2:0
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=2500, cl_num_layers=2, cl_num_proj_hidden=64, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=100.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.0, drop_feat_rate_1=0.4, drop_feat_rate_2=0.0, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=32, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, noisy_level=0.2, num_hidden=64, num_proj_hidden=64, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=40, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 88648])
add edge: torch.Size([2, 106248])
remove edge: torch.Size([2, 70854])
updated graph: torch.Size([2, 88454])
=== Noisy graph ===
/home/project-robust-contrastive/Robust-Contrastive-Learning/model.py:227: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 1060.39501953125 = 2.1694809198379517 + 100.0 * 10.582256317138672
Epoch 0, val loss: 2.168511152267456
Epoch 10, training loss: 1060.35498046875 = 2.1621618270874023 + 100.0 * 10.581928253173828
Epoch 10, val loss: 2.1617980003356934
Epoch 20, training loss: 1060.2327880859375 = 2.1584291458129883 + 100.0 * 10.580743789672852
Epoch 20, val loss: 2.1585633754730225
Epoch 30, training loss: 1059.73095703125 = 2.1575030088424683 + 100.0 * 10.575735092163086
Epoch 30, val loss: 2.157829523086548
Epoch 40, training loss: 1057.672607421875 = 2.158214569091797 + 100.0 * 10.555144309997559
Epoch 40, val loss: 2.1586062908172607
Epoch 50, training loss: 1050.1727294921875 = 2.1600635051727295 + 100.0 * 10.48012638092041
Epoch 50, val loss: 2.1604347229003906
Epoch 60, training loss: 1027.5284423828125 = 2.162365674972534 + 100.0 * 10.253661155700684
Epoch 60, val loss: 2.162632942199707
Epoch 70, training loss: 991.439453125 = 2.1647599935531616 + 100.0 * 9.892746925354004
Epoch 70, val loss: 2.1646361351013184
Epoch 80, training loss: 977.987548828125 = 2.1649320125579834 + 100.0 * 9.75822639465332
Epoch 80, val loss: 2.164515733718872
Epoch 90, training loss: 969.2283935546875 = 2.1639214754104614 + 100.0 * 9.670644760131836
Epoch 90, val loss: 2.1633825302124023
Epoch 100, training loss: 961.0562744140625 = 2.162792444229126 + 100.0 * 9.588934898376465
Epoch 100, val loss: 2.162313938140869
Epoch 110, training loss: 949.45654296875 = 2.1620850563049316 + 100.0 * 9.472944259643555
Epoch 110, val loss: 2.1617379188537598
Epoch 120, training loss: 933.7515869140625 = 2.1614606380462646 + 100.0 * 9.315901756286621
Epoch 120, val loss: 2.161245346069336
Epoch 130, training loss: 925.8645629882812 = 2.1608842611312866 + 100.0 * 9.23703670501709
Epoch 130, val loss: 2.1608293056488037
Epoch 140, training loss: 920.9883422851562 = 2.1602169275283813 + 100.0 * 9.188281059265137
Epoch 140, val loss: 2.160287618637085
Epoch 150, training loss: 912.6044921875 = 2.160039186477661 + 100.0 * 9.10444450378418
Epoch 150, val loss: 2.1601686477661133
Epoch 160, training loss: 903.1159057617188 = 2.1605440378189087 + 100.0 * 9.009553909301758
Epoch 160, val loss: 2.160694122314453
Epoch 170, training loss: 896.66943359375 = 2.1609503030776978 + 100.0 * 8.945084571838379
Epoch 170, val loss: 2.1610798835754395
Epoch 180, training loss: 890.9706420898438 = 2.1607251167297363 + 100.0 * 8.888099670410156
Epoch 180, val loss: 2.1608362197875977
Epoch 190, training loss: 887.2321166992188 = 2.160466432571411 + 100.0 * 8.850716590881348
Epoch 190, val loss: 2.1605634689331055
Epoch 200, training loss: 883.423583984375 = 2.1603935956954956 + 100.0 * 8.812631607055664
Epoch 200, val loss: 2.160471200942993
Epoch 210, training loss: 879.9596557617188 = 2.16034471988678 + 100.0 * 8.777993202209473
Epoch 210, val loss: 2.160388469696045
Epoch 220, training loss: 877.1229248046875 = 2.160238027572632 + 100.0 * 8.749627113342285
Epoch 220, val loss: 2.1602694988250732
Epoch 230, training loss: 874.4214477539062 = 2.1600152254104614 + 100.0 * 8.722614288330078
Epoch 230, val loss: 2.160031318664551
Epoch 240, training loss: 871.9387817382812 = 2.159790873527527 + 100.0 * 8.697790145874023
Epoch 240, val loss: 2.1598057746887207
Epoch 250, training loss: 870.0433959960938 = 2.1596028804779053 + 100.0 * 8.678837776184082
Epoch 250, val loss: 2.159621238708496
Epoch 260, training loss: 868.3463745117188 = 2.159293532371521 + 100.0 * 8.661870956420898
Epoch 260, val loss: 2.1593332290649414
Epoch 270, training loss: 866.8494873046875 = 2.1589685678482056 + 100.0 * 8.646904945373535
Epoch 270, val loss: 2.159000873565674
Epoch 280, training loss: 865.55517578125 = 2.1586241722106934 + 100.0 * 8.633965492248535
Epoch 280, val loss: 2.1586899757385254
Epoch 290, training loss: 864.3780517578125 = 2.158290982246399 + 100.0 * 8.622198104858398
Epoch 290, val loss: 2.1583995819091797
Epoch 300, training loss: 863.2113037109375 = 2.15803861618042 + 100.0 * 8.610532760620117
Epoch 300, val loss: 2.1581313610076904
Epoch 310, training loss: 862.4015502929688 = 2.1577521562576294 + 100.0 * 8.602437973022461
Epoch 310, val loss: 2.1578807830810547
Epoch 320, training loss: 861.385009765625 = 2.157458782196045 + 100.0 * 8.592275619506836
Epoch 320, val loss: 2.1575992107391357
Epoch 330, training loss: 860.5812377929688 = 2.157164454460144 + 100.0 * 8.584240913391113
Epoch 330, val loss: 2.1573219299316406
Epoch 340, training loss: 860.157958984375 = 2.1568959951400757 + 100.0 * 8.580010414123535
Epoch 340, val loss: 2.1570329666137695
Epoch 350, training loss: 859.2498168945312 = 2.1565661430358887 + 100.0 * 8.570932388305664
Epoch 350, val loss: 2.1567344665527344
Epoch 360, training loss: 858.5237426757812 = 2.1562618017196655 + 100.0 * 8.563674926757812
Epoch 360, val loss: 2.1564338207244873
Epoch 370, training loss: 857.879150390625 = 2.155948042869568 + 100.0 * 8.557231903076172
Epoch 370, val loss: 2.1561336517333984
Epoch 380, training loss: 857.2486572265625 = 2.155639410018921 + 100.0 * 8.55093002319336
Epoch 380, val loss: 2.15582275390625
Epoch 390, training loss: 856.7119750976562 = 2.1553313732147217 + 100.0 * 8.54556655883789
Epoch 390, val loss: 2.1555123329162598
Epoch 400, training loss: 856.21728515625 = 2.1550090312957764 + 100.0 * 8.54062271118164
Epoch 400, val loss: 2.155191421508789
Epoch 410, training loss: 855.5631713867188 = 2.154697060585022 + 100.0 * 8.53408432006836
Epoch 410, val loss: 2.154874324798584
Epoch 420, training loss: 855.0433959960938 = 2.1543772220611572 + 100.0 * 8.528890609741211
Epoch 420, val loss: 2.1545567512512207
Epoch 430, training loss: 854.5654296875 = 2.1540697813034058 + 100.0 * 8.524113655090332
Epoch 430, val loss: 2.15423321723938
Epoch 440, training loss: 854.2238159179688 = 2.1537482738494873 + 100.0 * 8.520700454711914
Epoch 440, val loss: 2.1539297103881836
Epoch 450, training loss: 853.9435424804688 = 2.15339195728302 + 100.0 * 8.517901420593262
Epoch 450, val loss: 2.153578996658325
Epoch 460, training loss: 853.4482421875 = 2.1530786752700806 + 100.0 * 8.512951850891113
Epoch 460, val loss: 2.1532421112060547
Epoch 470, training loss: 853.0801391601562 = 2.152737021446228 + 100.0 * 8.50927448272705
Epoch 470, val loss: 2.15291166305542
Epoch 480, training loss: 852.7902221679688 = 2.152390480041504 + 100.0 * 8.506378173828125
Epoch 480, val loss: 2.152571439743042
Epoch 490, training loss: 852.5621337890625 = 2.152043342590332 + 100.0 * 8.504100799560547
Epoch 490, val loss: 2.1522316932678223
Epoch 500, training loss: 852.5138549804688 = 2.151702404022217 + 100.0 * 8.503622055053711
Epoch 500, val loss: 2.1518774032592773
Epoch 510, training loss: 852.105712890625 = 2.15134334564209 + 100.0 * 8.499543190002441
Epoch 510, val loss: 2.1515347957611084
Epoch 520, training loss: 851.810546875 = 2.1509957313537598 + 100.0 * 8.49659538269043
Epoch 520, val loss: 2.1511898040771484
Epoch 530, training loss: 851.606689453125 = 2.150646448135376 + 100.0 * 8.494560241699219
Epoch 530, val loss: 2.1508450508117676
Epoch 540, training loss: 851.7350463867188 = 2.1502933502197266 + 100.0 * 8.495847702026367
Epoch 540, val loss: 2.150500535964966
Epoch 550, training loss: 851.21728515625 = 2.149951696395874 + 100.0 * 8.490673065185547
Epoch 550, val loss: 2.150153636932373
Epoch 560, training loss: 851.0397338867188 = 2.149606943130493 + 100.0 * 8.488901138305664
Epoch 560, val loss: 2.1498122215270996
Epoch 570, training loss: 850.8402709960938 = 2.149264693260193 + 100.0 * 8.486909866333008
Epoch 570, val loss: 2.149475574493408
Epoch 580, training loss: 850.6768188476562 = 2.1489193439483643 + 100.0 * 8.485279083251953
Epoch 580, val loss: 2.1491427421569824
Epoch 590, training loss: 850.5947265625 = 2.148579239845276 + 100.0 * 8.484461784362793
Epoch 590, val loss: 2.148792266845703
Epoch 600, training loss: 850.4099731445312 = 2.1482441425323486 + 100.0 * 8.482617378234863
Epoch 600, val loss: 2.148463010787964
Epoch 610, training loss: 850.1786499023438 = 2.147911548614502 + 100.0 * 8.480307579040527
Epoch 610, val loss: 2.148137092590332
Epoch 620, training loss: 849.983154296875 = 2.1475841999053955 + 100.0 * 8.478355407714844
Epoch 620, val loss: 2.1478142738342285
Epoch 630, training loss: 849.82421875 = 2.1472599506378174 + 100.0 * 8.47676944732666
Epoch 630, val loss: 2.147491455078125
Epoch 640, training loss: 849.6912231445312 = 2.146932601928711 + 100.0 * 8.475442886352539
Epoch 640, val loss: 2.1471714973449707
Epoch 650, training loss: 849.8749389648438 = 2.1465901136398315 + 100.0 * 8.477283477783203
Epoch 650, val loss: 2.1468467712402344
Epoch 660, training loss: 849.4785766601562 = 2.146278142929077 + 100.0 * 8.473322868347168
Epoch 660, val loss: 2.1465282440185547
Epoch 670, training loss: 849.2998657226562 = 2.145969271659851 + 100.0 * 8.471538543701172
Epoch 670, val loss: 2.1462087631225586
Epoch 680, training loss: 849.1810302734375 = 2.145647644996643 + 100.0 * 8.470354080200195
Epoch 680, val loss: 2.145895481109619
Epoch 690, training loss: 849.2278442382812 = 2.1453360319137573 + 100.0 * 8.4708251953125
Epoch 690, val loss: 2.145582675933838
Epoch 700, training loss: 849.1090698242188 = 2.145012378692627 + 100.0 * 8.469640731811523
Epoch 700, val loss: 2.145266532897949
Epoch 710, training loss: 848.8455810546875 = 2.1446950435638428 + 100.0 * 8.467008590698242
Epoch 710, val loss: 2.1449544429779053
Epoch 720, training loss: 848.76171875 = 2.144383430480957 + 100.0 * 8.46617317199707
Epoch 720, val loss: 2.1446471214294434
Epoch 730, training loss: 848.7081298828125 = 2.1440643072128296 + 100.0 * 8.465641021728516
Epoch 730, val loss: 2.144346237182617
Epoch 740, training loss: 848.5667114257812 = 2.143766164779663 + 100.0 * 8.464229583740234
Epoch 740, val loss: 2.144026756286621
Epoch 750, training loss: 848.4951782226562 = 2.143444061279297 + 100.0 * 8.463517189025879
Epoch 750, val loss: 2.1437246799468994
Epoch 760, training loss: 848.3706665039062 = 2.1431424617767334 + 100.0 * 8.462275505065918
Epoch 760, val loss: 2.143425464630127
Epoch 770, training loss: 848.2499389648438 = 2.142844557762146 + 100.0 * 8.461071014404297
Epoch 770, val loss: 2.1431281566619873
Epoch 780, training loss: 848.2667846679688 = 2.1425331830978394 + 100.0 * 8.46124267578125
Epoch 780, val loss: 2.142834186553955
Epoch 790, training loss: 848.1043090820312 = 2.142244577407837 + 100.0 * 8.459620475769043
Epoch 790, val loss: 2.1425228118896484
Epoch 800, training loss: 848.0513916015625 = 2.141929864883423 + 100.0 * 8.459095001220703
Epoch 800, val loss: 2.1422300338745117
Epoch 810, training loss: 847.8905639648438 = 2.141639828681946 + 100.0 * 8.457489013671875
Epoch 810, val loss: 2.1419425010681152
Epoch 820, training loss: 847.80126953125 = 2.141357421875 + 100.0 * 8.456599235534668
Epoch 820, val loss: 2.1416523456573486
Epoch 830, training loss: 847.7033081054688 = 2.141062021255493 + 100.0 * 8.455622673034668
Epoch 830, val loss: 2.141366720199585
Epoch 840, training loss: 847.6199340820312 = 2.1407766342163086 + 100.0 * 8.454792022705078
Epoch 840, val loss: 2.1410837173461914
Epoch 850, training loss: 848.5026245117188 = 2.140455961227417 + 100.0 * 8.463622093200684
Epoch 850, val loss: 2.140800952911377
Epoch 860, training loss: 847.725830078125 = 2.1401844024658203 + 100.0 * 8.455856323242188
Epoch 860, val loss: 2.1405043601989746
Epoch 870, training loss: 847.4015502929688 = 2.139902114868164 + 100.0 * 8.452616691589355
Epoch 870, val loss: 2.140221357345581
Epoch 880, training loss: 847.2930908203125 = 2.139625906944275 + 100.0 * 8.451534271240234
Epoch 880, val loss: 2.139946222305298
Epoch 890, training loss: 847.2145385742188 = 2.1393531560897827 + 100.0 * 8.450752258300781
Epoch 890, val loss: 2.139674186706543
Epoch 900, training loss: 847.1375732421875 = 2.1390808820724487 + 100.0 * 8.449984550476074
Epoch 900, val loss: 2.1394028663635254
Epoch 910, training loss: 847.4811401367188 = 2.1388280391693115 + 100.0 * 8.453422546386719
Epoch 910, val loss: 2.139129638671875
Epoch 920, training loss: 847.0459594726562 = 2.1385098695755005 + 100.0 * 8.449074745178223
Epoch 920, val loss: 2.138856887817383
Epoch 930, training loss: 846.8978881835938 = 2.138246774673462 + 100.0 * 8.447596549987793
Epoch 930, val loss: 2.1385927200317383
Epoch 940, training loss: 846.8272705078125 = 2.137988805770874 + 100.0 * 8.446892738342285
Epoch 940, val loss: 2.1383280754089355
Epoch 950, training loss: 846.9102172851562 = 2.1377038955688477 + 100.0 * 8.447725296020508
Epoch 950, val loss: 2.138070583343506
Epoch 960, training loss: 846.9072265625 = 2.1374536752700806 + 100.0 * 8.447697639465332
Epoch 960, val loss: 2.1377978324890137
Epoch 970, training loss: 846.6062622070312 = 2.1371887922286987 + 100.0 * 8.444690704345703
Epoch 970, val loss: 2.137533187866211
Epoch 980, training loss: 846.5151977539062 = 2.1369152069091797 + 100.0 * 8.443782806396484
Epoch 980, val loss: 2.1372790336608887
Epoch 990, training loss: 846.4421997070312 = 2.136658787727356 + 100.0 * 8.443055152893066
Epoch 990, val loss: 2.137026309967041
Epoch 1000, training loss: 846.3545532226562 = 2.1364060640335083 + 100.0 * 8.442181587219238
Epoch 1000, val loss: 2.1367766857147217
Epoch 1010, training loss: 846.3087158203125 = 2.1361483335494995 + 100.0 * 8.441725730895996
Epoch 1010, val loss: 2.1365292072296143
Epoch 1020, training loss: 846.5006103515625 = 2.1358699798583984 + 100.0 * 8.443647384643555
Epoch 1020, val loss: 2.136265754699707
Epoch 1030, training loss: 846.3292846679688 = 2.135616660118103 + 100.0 * 8.441936492919922
Epoch 1030, val loss: 2.1360130310058594
Epoch 1040, training loss: 846.1129150390625 = 2.1353718042373657 + 100.0 * 8.439775466918945
Epoch 1040, val loss: 2.135765790939331
Epoch 1050, training loss: 846.02734375 = 2.1351336240768433 + 100.0 * 8.438921928405762
Epoch 1050, val loss: 2.13552188873291
Epoch 1060, training loss: 845.9373168945312 = 2.1348859071731567 + 100.0 * 8.438024520874023
Epoch 1060, val loss: 2.1352810859680176
Epoch 1070, training loss: 845.8961181640625 = 2.1346421241760254 + 100.0 * 8.437614440917969
Epoch 1070, val loss: 2.135042667388916
Epoch 1080, training loss: 846.0653076171875 = 2.1344008445739746 + 100.0 * 8.439309120178223
Epoch 1080, val loss: 2.1348018646240234
Epoch 1090, training loss: 845.8619384765625 = 2.134147882461548 + 100.0 * 8.437277793884277
Epoch 1090, val loss: 2.1345527172088623
Epoch 1100, training loss: 845.6724853515625 = 2.133898973464966 + 100.0 * 8.435385704040527
Epoch 1100, val loss: 2.1343133449554443
Epoch 1110, training loss: 845.6207275390625 = 2.1336504220962524 + 100.0 * 8.434870719909668
Epoch 1110, val loss: 2.1340830326080322
Epoch 1120, training loss: 845.6806030273438 = 2.133405566215515 + 100.0 * 8.435471534729004
Epoch 1120, val loss: 2.1338562965393066
Epoch 1130, training loss: 845.5435791015625 = 2.133183002471924 + 100.0 * 8.434103965759277
Epoch 1130, val loss: 2.1336095333099365
Epoch 1140, training loss: 845.4832763671875 = 2.132930636405945 + 100.0 * 8.433503150939941
Epoch 1140, val loss: 2.133380174636841
Epoch 1150, training loss: 845.3605346679688 = 2.1326990127563477 + 100.0 * 8.432278633117676
Epoch 1150, val loss: 2.133153200149536
Epoch 1160, training loss: 845.3098754882812 = 2.1324702501296997 + 100.0 * 8.431774139404297
Epoch 1160, val loss: 2.1329240798950195
Epoch 1170, training loss: 845.5885009765625 = 2.132230520248413 + 100.0 * 8.434562683105469
Epoch 1170, val loss: 2.1326987743377686
Epoch 1180, training loss: 845.3375854492188 = 2.1319899559020996 + 100.0 * 8.432055473327637
Epoch 1180, val loss: 2.132467269897461
Epoch 1190, training loss: 845.1797485351562 = 2.1317559480667114 + 100.0 * 8.430480003356934
Epoch 1190, val loss: 2.132237195968628
Epoch 1200, training loss: 845.1378784179688 = 2.1315226554870605 + 100.0 * 8.430063247680664
Epoch 1200, val loss: 2.132014274597168
Epoch 1210, training loss: 845.2110595703125 = 2.1312813758850098 + 100.0 * 8.430797576904297
Epoch 1210, val loss: 2.131791114807129
Epoch 1220, training loss: 844.9805908203125 = 2.131072998046875 + 100.0 * 8.428495407104492
Epoch 1220, val loss: 2.131559371948242
Epoch 1230, training loss: 844.9654541015625 = 2.1308486461639404 + 100.0 * 8.428345680236816
Epoch 1230, val loss: 2.131338119506836
Epoch 1240, training loss: 844.88232421875 = 2.130612373352051 + 100.0 * 8.42751693725586
Epoch 1240, val loss: 2.1311192512512207
Epoch 1250, training loss: 844.941650390625 = 2.130373954772949 + 100.0 * 8.428112983703613
Epoch 1250, val loss: 2.13090181350708
Epoch 1260, training loss: 844.8399658203125 = 2.1301504373550415 + 100.0 * 8.427098274230957
Epoch 1260, val loss: 2.1306707859039307
Epoch 1270, training loss: 844.8699951171875 = 2.1299211978912354 + 100.0 * 8.427400588989258
Epoch 1270, val loss: 2.1304497718811035
Epoch 1280, training loss: 844.8057861328125 = 2.1297165155410767 + 100.0 * 8.42676067352295
Epoch 1280, val loss: 2.1302263736724854
Epoch 1290, training loss: 844.7719116210938 = 2.129486560821533 + 100.0 * 8.426424026489258
Epoch 1290, val loss: 2.1300058364868164
Epoch 1300, training loss: 844.656982421875 = 2.1292370557785034 + 100.0 * 8.425277709960938
Epoch 1300, val loss: 2.129791736602783
Epoch 1310, training loss: 844.5791015625 = 2.1290241479873657 + 100.0 * 8.424500465393066
Epoch 1310, val loss: 2.129577159881592
Epoch 1320, training loss: 844.6151733398438 = 2.1288024187088013 + 100.0 * 8.424863815307617
Epoch 1320, val loss: 2.129364490509033
Epoch 1330, training loss: 844.73828125 = 2.1285706758499146 + 100.0 * 8.42609691619873
Epoch 1330, val loss: 2.129145622253418
Epoch 1340, training loss: 844.4822998046875 = 2.1283615827560425 + 100.0 * 8.423539161682129
Epoch 1340, val loss: 2.1289260387420654
Epoch 1350, training loss: 844.3997192382812 = 2.1281373500823975 + 100.0 * 8.42271614074707
Epoch 1350, val loss: 2.1287131309509277
Epoch 1360, training loss: 844.3536987304688 = 2.127926826477051 + 100.0 * 8.422257423400879
Epoch 1360, val loss: 2.128505229949951
Epoch 1370, training loss: 844.3761596679688 = 2.127721667289734 + 100.0 * 8.422484397888184
Epoch 1370, val loss: 2.128293752670288
Epoch 1380, training loss: 844.5394287109375 = 2.127514123916626 + 100.0 * 8.424118995666504
Epoch 1380, val loss: 2.1280877590179443
Epoch 1390, training loss: 844.4111938476562 = 2.127275586128235 + 100.0 * 8.422839164733887
Epoch 1390, val loss: 2.127859592437744
Epoch 1400, training loss: 844.2438354492188 = 2.127050518989563 + 100.0 * 8.421167373657227
Epoch 1400, val loss: 2.127653121948242
Epoch 1410, training loss: 844.1571655273438 = 2.1268426179885864 + 100.0 * 8.420303344726562
Epoch 1410, val loss: 2.1274523735046387
Epoch 1420, training loss: 844.1245727539062 = 2.1266322135925293 + 100.0 * 8.419979095458984
Epoch 1420, val loss: 2.1272528171539307
Epoch 1430, training loss: 844.0827026367188 = 2.126426577568054 + 100.0 * 8.419563293457031
Epoch 1430, val loss: 2.1270532608032227
Epoch 1440, training loss: 844.126220703125 = 2.126217246055603 + 100.0 * 8.420000076293945
Epoch 1440, val loss: 2.1268534660339355
Epoch 1450, training loss: 844.1409301757812 = 2.1259982585906982 + 100.0 * 8.420149803161621
Epoch 1450, val loss: 2.126645565032959
Epoch 1460, training loss: 843.9971313476562 = 2.12579607963562 + 100.0 * 8.418713569641113
Epoch 1460, val loss: 2.126439094543457
Epoch 1470, training loss: 843.9899291992188 = 2.125592350959778 + 100.0 * 8.4186429977417
Epoch 1470, val loss: 2.126239776611328
Epoch 1480, training loss: 844.0728149414062 = 2.1253817081451416 + 100.0 * 8.419474601745605
Epoch 1480, val loss: 2.1260457038879395
Epoch 1490, training loss: 843.8851318359375 = 2.125179171562195 + 100.0 * 8.41759967803955
Epoch 1490, val loss: 2.125843048095703
Epoch 1500, training loss: 843.85595703125 = 2.1249839067459106 + 100.0 * 8.417309761047363
Epoch 1500, val loss: 2.125645637512207
Epoch 1510, training loss: 843.8096923828125 = 2.1247811317443848 + 100.0 * 8.416849136352539
Epoch 1510, val loss: 2.1254539489746094
Epoch 1520, training loss: 843.7664794921875 = 2.1245838403701782 + 100.0 * 8.41641902923584
Epoch 1520, val loss: 2.1252636909484863
Epoch 1530, training loss: 843.8130493164062 = 2.1243802309036255 + 100.0 * 8.416886329650879
Epoch 1530, val loss: 2.1250767707824707
Epoch 1540, training loss: 843.8623046875 = 2.124170660972595 + 100.0 * 8.417381286621094
Epoch 1540, val loss: 2.124876022338867
Epoch 1550, training loss: 843.7888793945312 = 2.1239686012268066 + 100.0 * 8.416648864746094
Epoch 1550, val loss: 2.124678611755371
Epoch 1560, training loss: 843.6365356445312 = 2.123787522315979 + 100.0 * 8.415127754211426
Epoch 1560, val loss: 2.1244869232177734
Epoch 1570, training loss: 843.6231079101562 = 2.1235989332199097 + 100.0 * 8.414995193481445
Epoch 1570, val loss: 2.124300956726074
Epoch 1580, training loss: 843.6162719726562 = 2.1234123706817627 + 100.0 * 8.414928436279297
Epoch 1580, val loss: 2.124114990234375
Epoch 1590, training loss: 843.8060302734375 = 2.1232320070266724 + 100.0 * 8.416828155517578
Epoch 1590, val loss: 2.1239285469055176
Epoch 1600, training loss: 843.6065063476562 = 2.122997999191284 + 100.0 * 8.414834976196289
Epoch 1600, val loss: 2.1237435340881348
Epoch 1610, training loss: 843.5140991210938 = 2.1228283643722534 + 100.0 * 8.413912773132324
Epoch 1610, val loss: 2.1235544681549072
Epoch 1620, training loss: 843.52294921875 = 2.1226316690444946 + 100.0 * 8.414003372192383
Epoch 1620, val loss: 2.1233761310577393
Epoch 1630, training loss: 843.5241088867188 = 2.1224414110183716 + 100.0 * 8.414016723632812
Epoch 1630, val loss: 2.1231908798217773
Epoch 1640, training loss: 843.4160766601562 = 2.1222639083862305 + 100.0 * 8.412938117980957
Epoch 1640, val loss: 2.1230082511901855
Epoch 1650, training loss: 843.3794555664062 = 2.1220688819885254 + 100.0 * 8.41257381439209
Epoch 1650, val loss: 2.1228318214416504
Epoch 1660, training loss: 843.4541625976562 = 2.12187659740448 + 100.0 * 8.413322448730469
Epoch 1660, val loss: 2.1226553916931152
Epoch 1670, training loss: 843.4058837890625 = 2.1216965913772583 + 100.0 * 8.412841796875
Epoch 1670, val loss: 2.122469663619995
Epoch 1680, training loss: 843.39990234375 = 2.1215269565582275 + 100.0 * 8.4127836227417
Epoch 1680, val loss: 2.1222875118255615
Epoch 1690, training loss: 843.7086181640625 = 2.121335029602051 + 100.0 * 8.415872573852539
Epoch 1690, val loss: 2.1221072673797607
Epoch 1700, training loss: 843.2853393554688 = 2.121126651763916 + 100.0 * 8.411642074584961
Epoch 1700, val loss: 2.1219234466552734
Epoch 1710, training loss: 843.2132568359375 = 2.120951771736145 + 100.0 * 8.41092300415039
Epoch 1710, val loss: 2.1217479705810547
Epoch 1720, training loss: 843.1904907226562 = 2.1207752227783203 + 100.0 * 8.410696983337402
Epoch 1720, val loss: 2.121577024459839
Epoch 1730, training loss: 843.1559448242188 = 2.1205978393554688 + 100.0 * 8.410353660583496
Epoch 1730, val loss: 2.1214077472686768
Epoch 1740, training loss: 843.1581420898438 = 2.1204283237457275 + 100.0 * 8.410377502441406
Epoch 1740, val loss: 2.1212360858917236
Epoch 1750, training loss: 843.7164306640625 = 2.120264768600464 + 100.0 * 8.415962219238281
Epoch 1750, val loss: 2.121060371398926
Epoch 1760, training loss: 843.0946655273438 = 2.120046615600586 + 100.0 * 8.409746170043945
Epoch 1760, val loss: 2.1208739280700684
Epoch 1770, training loss: 843.1152954101562 = 2.1198620796203613 + 100.0 * 8.409954071044922
Epoch 1770, val loss: 2.120704412460327
Epoch 1780, training loss: 843.0415649414062 = 2.1196982860565186 + 100.0 * 8.409218788146973
Epoch 1780, val loss: 2.1205379962921143
Epoch 1790, training loss: 843.01904296875 = 2.1195214986801147 + 100.0 * 8.408995628356934
Epoch 1790, val loss: 2.120372772216797
Epoch 1800, training loss: 843.432861328125 = 2.1193212270736694 + 100.0 * 8.413135528564453
Epoch 1800, val loss: 2.1202123165130615
Epoch 1810, training loss: 843.1448364257812 = 2.1191823482513428 + 100.0 * 8.410256385803223
Epoch 1810, val loss: 2.1200194358825684
Epoch 1820, training loss: 843.0250244140625 = 2.1189743280410767 + 100.0 * 8.40906047821045
Epoch 1820, val loss: 2.1198511123657227
Epoch 1830, training loss: 842.9313354492188 = 2.118814468383789 + 100.0 * 8.408124923706055
Epoch 1830, val loss: 2.1196889877319336
Epoch 1840, training loss: 842.9091186523438 = 2.1186509132385254 + 100.0 * 8.407904624938965
Epoch 1840, val loss: 2.119525909423828
Epoch 1850, training loss: 842.8870849609375 = 2.118477702140808 + 100.0 * 8.407686233520508
Epoch 1850, val loss: 2.11936616897583
Epoch 1860, training loss: 842.8682250976562 = 2.118314266204834 + 100.0 * 8.407499313354492
Epoch 1860, val loss: 2.1192054748535156
Epoch 1870, training loss: 842.9998168945312 = 2.1181453466415405 + 100.0 * 8.40881633758545
Epoch 1870, val loss: 2.1190457344055176
Epoch 1880, training loss: 842.871826171875 = 2.117964267730713 + 100.0 * 8.407538414001465
Epoch 1880, val loss: 2.118868350982666
Epoch 1890, training loss: 842.91748046875 = 2.1177929639816284 + 100.0 * 8.407997131347656
Epoch 1890, val loss: 2.118701457977295
Epoch 1900, training loss: 842.79345703125 = 2.1176196336746216 + 100.0 * 8.406758308410645
Epoch 1900, val loss: 2.118541717529297
Epoch 1910, training loss: 842.7858276367188 = 2.117457866668701 + 100.0 * 8.406683921813965
Epoch 1910, val loss: 2.11838436126709
Epoch 1920, training loss: 842.7587280273438 = 2.1172972917556763 + 100.0 * 8.406414031982422
Epoch 1920, val loss: 2.118229389190674
Epoch 1930, training loss: 842.789794921875 = 2.1171367168426514 + 100.0 * 8.406726837158203
Epoch 1930, val loss: 2.118072986602783
Epoch 1940, training loss: 843.1504516601562 = 2.1169753074645996 + 100.0 * 8.410334587097168
Epoch 1940, val loss: 2.1179065704345703
Epoch 1950, training loss: 842.722412109375 = 2.116789937019348 + 100.0 * 8.40605640411377
Epoch 1950, val loss: 2.117737293243408
Epoch 1960, training loss: 842.7330322265625 = 2.116620659828186 + 100.0 * 8.406164169311523
Epoch 1960, val loss: 2.1175804138183594
Epoch 1970, training loss: 842.692626953125 = 2.116463541984558 + 100.0 * 8.40576171875
Epoch 1970, val loss: 2.1174280643463135
Epoch 1980, training loss: 842.68994140625 = 2.1163079738616943 + 100.0 * 8.405735969543457
Epoch 1980, val loss: 2.117276191711426
Epoch 1990, training loss: 843.1328125 = 2.116161346435547 + 100.0 * 8.41016674041748
Epoch 1990, val loss: 2.1171183586120605
Epoch 2000, training loss: 842.8223266601562 = 2.115972876548767 + 100.0 * 8.407063484191895
Epoch 2000, val loss: 2.1169567108154297
Epoch 2010, training loss: 842.663330078125 = 2.1158167123794556 + 100.0 * 8.405474662780762
Epoch 2010, val loss: 2.1168012619018555
Epoch 2020, training loss: 842.5921630859375 = 2.1156527996063232 + 100.0 * 8.404765129089355
Epoch 2020, val loss: 2.116652011871338
Epoch 2030, training loss: 842.5729370117188 = 2.115498661994934 + 100.0 * 8.404574394226074
Epoch 2030, val loss: 2.1165060997009277
Epoch 2040, training loss: 842.5748291015625 = 2.1153509616851807 + 100.0 * 8.404594421386719
Epoch 2040, val loss: 2.1163582801818848
Epoch 2050, training loss: 843.1427001953125 = 2.115225315093994 + 100.0 * 8.410274505615234
Epoch 2050, val loss: 2.1162028312683105
Epoch 2060, training loss: 842.7206420898438 = 2.1149959564208984 + 100.0 * 8.40605640411377
Epoch 2060, val loss: 2.11604905128479
Epoch 2070, training loss: 842.522705078125 = 2.114864230155945 + 100.0 * 8.404078483581543
Epoch 2070, val loss: 2.1158976554870605
Epoch 2080, training loss: 842.4984741210938 = 2.114710807800293 + 100.0 * 8.403838157653809
Epoch 2080, val loss: 2.1157517433166504
Epoch 2090, training loss: 842.4813842773438 = 2.1145564317703247 + 100.0 * 8.403668403625488
Epoch 2090, val loss: 2.1156129837036133
Epoch 2100, training loss: 842.462890625 = 2.114414691925049 + 100.0 * 8.403484344482422
Epoch 2100, val loss: 2.1154706478118896
Epoch 2110, training loss: 842.6226196289062 = 2.1142537593841553 + 100.0 * 8.405083656311035
Epoch 2110, val loss: 2.1153321266174316
Epoch 2120, training loss: 842.4708862304688 = 2.1140981912612915 + 100.0 * 8.403568267822266
Epoch 2120, val loss: 2.1151692867279053
Epoch 2130, training loss: 842.5299072265625 = 2.1139332056045532 + 100.0 * 8.404159545898438
Epoch 2130, val loss: 2.115016460418701
Epoch 2140, training loss: 842.4142456054688 = 2.1137837171554565 + 100.0 * 8.40300464630127
Epoch 2140, val loss: 2.1148765087127686
Epoch 2150, training loss: 842.393310546875 = 2.1136502027511597 + 100.0 * 8.402796745300293
Epoch 2150, val loss: 2.114739418029785
Epoch 2160, training loss: 842.3616333007812 = 2.113502264022827 + 100.0 * 8.402481079101562
Epoch 2160, val loss: 2.11460542678833
Epoch 2170, training loss: 842.3443603515625 = 2.1133612394332886 + 100.0 * 8.402310371398926
Epoch 2170, val loss: 2.114471912384033
Epoch 2180, training loss: 842.334716796875 = 2.113218307495117 + 100.0 * 8.402215003967285
Epoch 2180, val loss: 2.114337921142578
Epoch 2190, training loss: 842.6436157226562 = 2.1130597591400146 + 100.0 * 8.405305862426758
Epoch 2190, val loss: 2.1142070293426514
Epoch 2200, training loss: 842.4893798828125 = 2.1129356622695923 + 100.0 * 8.403764724731445
Epoch 2200, val loss: 2.1140432357788086
Epoch 2210, training loss: 842.3854370117188 = 2.112758159637451 + 100.0 * 8.402727127075195
Epoch 2210, val loss: 2.113903284072876
Epoch 2220, training loss: 842.2802124023438 = 2.1126255989074707 + 100.0 * 8.401676177978516
Epoch 2220, val loss: 2.1137709617614746
Epoch 2230, training loss: 842.2785034179688 = 2.1124871969223022 + 100.0 * 8.401659965515137
Epoch 2230, val loss: 2.1136412620544434
Epoch 2240, training loss: 842.3287353515625 = 2.112349510192871 + 100.0 * 8.4021635055542
Epoch 2240, val loss: 2.113513946533203
Epoch 2250, training loss: 842.3739013671875 = 2.112210512161255 + 100.0 * 8.402617454528809
Epoch 2250, val loss: 2.1133780479431152
Epoch 2260, training loss: 842.6056518554688 = 2.1120901107788086 + 100.0 * 8.404935836791992
Epoch 2260, val loss: 2.1132373809814453
Epoch 2270, training loss: 842.2824096679688 = 2.111910820007324 + 100.0 * 8.401704788208008
Epoch 2270, val loss: 2.1131033897399902
Epoch 2280, training loss: 842.2063598632812 = 2.111789107322693 + 100.0 * 8.400945663452148
Epoch 2280, val loss: 2.112969398498535
Epoch 2290, training loss: 842.1617431640625 = 2.11164927482605 + 100.0 * 8.400501251220703
Epoch 2290, val loss: 2.1128482818603516
Epoch 2300, training loss: 842.1343383789062 = 2.1115232706069946 + 100.0 * 8.400228500366211
Epoch 2300, val loss: 2.112724781036377
Epoch 2310, training loss: 842.1197509765625 = 2.1113977432250977 + 100.0 * 8.400083541870117
Epoch 2310, val loss: 2.112602710723877
Epoch 2320, training loss: 842.1273193359375 = 2.1112656593322754 + 100.0 * 8.400160789489746
Epoch 2320, val loss: 2.1124815940856934
Epoch 2330, training loss: 842.810791015625 = 2.111122965812683 + 100.0 * 8.406996726989746
Epoch 2330, val loss: 2.112353801727295
Epoch 2340, training loss: 842.2688598632812 = 2.110978841781616 + 100.0 * 8.401578903198242
Epoch 2340, val loss: 2.1122097969055176
Epoch 2350, training loss: 842.0559692382812 = 2.110844373703003 + 100.0 * 8.39945125579834
Epoch 2350, val loss: 2.112081289291382
Epoch 2360, training loss: 842.0467529296875 = 2.110718846321106 + 100.0 * 8.399360656738281
Epoch 2360, val loss: 2.111964225769043
Epoch 2370, training loss: 842.040283203125 = 2.1105936765670776 + 100.0 * 8.399296760559082
Epoch 2370, val loss: 2.1118500232696533
Epoch 2380, training loss: 842.4116821289062 = 2.110453963279724 + 100.0 * 8.4030122756958
Epoch 2380, val loss: 2.11173677444458
Epoch 2390, training loss: 842.1163330078125 = 2.1103326082229614 + 100.0 * 8.400059700012207
Epoch 2390, val loss: 2.1115927696228027
Epoch 2400, training loss: 842.0430297851562 = 2.1102030277252197 + 100.0 * 8.399328231811523
Epoch 2400, val loss: 2.111473798751831
Epoch 2410, training loss: 841.9659423828125 = 2.1100807189941406 + 100.0 * 8.398558616638184
Epoch 2410, val loss: 2.111358642578125
Epoch 2420, training loss: 841.9677734375 = 2.109960675239563 + 100.0 * 8.398577690124512
Epoch 2420, val loss: 2.111246109008789
Epoch 2430, training loss: 842.4505615234375 = 2.1098517179489136 + 100.0 * 8.403407096862793
Epoch 2430, val loss: 2.111126184463501
Epoch 2440, training loss: 842.083984375 = 2.109699845314026 + 100.0 * 8.39974308013916
Epoch 2440, val loss: 2.110999584197998
Epoch 2450, training loss: 841.9503784179688 = 2.1095712184906006 + 100.0 * 8.398407936096191
Epoch 2450, val loss: 2.1108813285827637
Epoch 2460, training loss: 841.8806762695312 = 2.109457492828369 + 100.0 * 8.397712707519531
Epoch 2460, val loss: 2.110772132873535
Epoch 2470, training loss: 841.8560791015625 = 2.109344482421875 + 100.0 * 8.397467613220215
Epoch 2470, val loss: 2.1106648445129395
Epoch 2480, training loss: 841.9141845703125 = 2.109227180480957 + 100.0 * 8.398049354553223
Epoch 2480, val loss: 2.11055850982666
Epoch 2490, training loss: 842.0673828125 = 2.1090917587280273 + 100.0 * 8.399582862854004
Epoch 2490, val loss: 2.110443592071533
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.39608695652173914
0.8108382235745853
=== training gcn model ===
Epoch 0, training loss: 1060.41650390625 = 2.189197540283203 + 100.0 * 10.58227252960205
Epoch 0, val loss: 2.189603567123413
Epoch 10, training loss: 1060.38232421875 = 2.183274030685425 + 100.0 * 10.581991195678711
Epoch 10, val loss: 2.1848673820495605
Epoch 20, training loss: 1060.2757568359375 = 2.180642008781433 + 100.0 * 10.580950736999512
Epoch 20, val loss: 2.182921886444092
Epoch 30, training loss: 1059.845458984375 = 2.1800758838653564 + 100.0 * 10.576653480529785
Epoch 30, val loss: 2.1825923919677734
Epoch 40, training loss: 1058.1259765625 = 2.1807267665863037 + 100.0 * 10.559452056884766
Epoch 40, val loss: 2.183229923248291
Epoch 50, training loss: 1052.370361328125 = 2.1822363138198853 + 100.0 * 10.501880645751953
Epoch 50, val loss: 2.1845693588256836
Epoch 60, training loss: 1036.3546142578125 = 2.184829592704773 + 100.0 * 10.341697692871094
Epoch 60, val loss: 2.186814308166504
Epoch 70, training loss: 999.2133178710938 = 2.188121795654297 + 100.0 * 9.97025203704834
Epoch 70, val loss: 2.189551591873169
Epoch 80, training loss: 965.177734375 = 2.1912885904312134 + 100.0 * 9.629864692687988
Epoch 80, val loss: 2.1921141147613525
Epoch 90, training loss: 949.5745239257812 = 2.191247344017029 + 100.0 * 9.473833084106445
Epoch 90, val loss: 2.1916861534118652
Epoch 100, training loss: 936.5497436523438 = 2.188939094543457 + 100.0 * 9.343607902526855
Epoch 100, val loss: 2.1895642280578613
Epoch 110, training loss: 930.2491455078125 = 2.1868830919265747 + 100.0 * 9.280622482299805
Epoch 110, val loss: 2.1876635551452637
Epoch 120, training loss: 926.6653442382812 = 2.1851991415023804 + 100.0 * 9.24480152130127
Epoch 120, val loss: 2.1860055923461914
Epoch 130, training loss: 922.2584838867188 = 2.183553457260132 + 100.0 * 9.200749397277832
Epoch 130, val loss: 2.184438705444336
Epoch 140, training loss: 916.553466796875 = 2.1823525428771973 + 100.0 * 9.14371109008789
Epoch 140, val loss: 2.1834053993225098
Epoch 150, training loss: 909.872314453125 = 2.1818913221359253 + 100.0 * 9.076904296875
Epoch 150, val loss: 2.1831114292144775
Epoch 160, training loss: 904.17578125 = 2.1822246313095093 + 100.0 * 9.019935607910156
Epoch 160, val loss: 2.1835343837738037
Epoch 170, training loss: 899.9057006835938 = 2.1824328899383545 + 100.0 * 8.977232933044434
Epoch 170, val loss: 2.183749198913574
Epoch 180, training loss: 893.6519775390625 = 2.1823558807373047 + 100.0 * 8.914695739746094
Epoch 180, val loss: 2.1836795806884766
Epoch 190, training loss: 886.8674926757812 = 2.182427167892456 + 100.0 * 8.846850395202637
Epoch 190, val loss: 2.183762311935425
Epoch 200, training loss: 882.1071166992188 = 2.182274580001831 + 100.0 * 8.799248695373535
Epoch 200, val loss: 2.183692455291748
Epoch 210, training loss: 877.984130859375 = 2.1820541620254517 + 100.0 * 8.758020401000977
Epoch 210, val loss: 2.1835310459136963
Epoch 220, training loss: 875.3870849609375 = 2.1818472146987915 + 100.0 * 8.73205280303955
Epoch 220, val loss: 2.183380365371704
Epoch 230, training loss: 873.44091796875 = 2.1816149950027466 + 100.0 * 8.712593078613281
Epoch 230, val loss: 2.1832046508789062
Epoch 240, training loss: 871.5341796875 = 2.1814297437667847 + 100.0 * 8.693527221679688
Epoch 240, val loss: 2.1830685138702393
Epoch 250, training loss: 869.680419921875 = 2.181281805038452 + 100.0 * 8.674991607666016
Epoch 250, val loss: 2.1829705238342285
Epoch 260, training loss: 868.0038452148438 = 2.1811362504959106 + 100.0 * 8.65822696685791
Epoch 260, val loss: 2.182866334915161
Epoch 270, training loss: 866.6653442382812 = 2.180981159210205 + 100.0 * 8.644844055175781
Epoch 270, val loss: 2.182750701904297
Epoch 280, training loss: 865.5767822265625 = 2.180799126625061 + 100.0 * 8.633959770202637
Epoch 280, val loss: 2.1826040744781494
Epoch 290, training loss: 864.6680908203125 = 2.1806074380874634 + 100.0 * 8.62487506866455
Epoch 290, val loss: 2.1824498176574707
Epoch 300, training loss: 863.9154052734375 = 2.1804193258285522 + 100.0 * 8.617349624633789
Epoch 300, val loss: 2.1822924613952637
Epoch 310, training loss: 863.304443359375 = 2.1802064180374146 + 100.0 * 8.611242294311523
Epoch 310, val loss: 2.1821160316467285
Epoch 320, training loss: 862.7853393554688 = 2.1799609661102295 + 100.0 * 8.606054306030273
Epoch 320, val loss: 2.1819241046905518
Epoch 330, training loss: 862.1976318359375 = 2.1797311305999756 + 100.0 * 8.600178718566895
Epoch 330, val loss: 2.1817209720611572
Epoch 340, training loss: 861.6660766601562 = 2.179487943649292 + 100.0 * 8.594865798950195
Epoch 340, val loss: 2.1815245151519775
Epoch 350, training loss: 861.1231079101562 = 2.1792508363723755 + 100.0 * 8.589438438415527
Epoch 350, val loss: 2.181318759918213
Epoch 360, training loss: 860.7587280273438 = 2.1789870262145996 + 100.0 * 8.585797309875488
Epoch 360, val loss: 2.181124210357666
Epoch 370, training loss: 860.0173950195312 = 2.178768754005432 + 100.0 * 8.578386306762695
Epoch 370, val loss: 2.18093204498291
Epoch 380, training loss: 859.426513671875 = 2.178578019142151 + 100.0 * 8.572479248046875
Epoch 380, val loss: 2.1807632446289062
Epoch 390, training loss: 859.06201171875 = 2.178386926651001 + 100.0 * 8.568836212158203
Epoch 390, val loss: 2.1806020736694336
Epoch 400, training loss: 858.3583374023438 = 2.1781712770462036 + 100.0 * 8.56180191040039
Epoch 400, val loss: 2.1804380416870117
Epoch 410, training loss: 857.7807006835938 = 2.1779881715774536 + 100.0 * 8.55602741241455
Epoch 410, val loss: 2.180288076400757
Epoch 420, training loss: 857.4661865234375 = 2.177817940711975 + 100.0 * 8.552884101867676
Epoch 420, val loss: 2.1801390647888184
Epoch 430, training loss: 856.8369750976562 = 2.1776156425476074 + 100.0 * 8.54659366607666
Epoch 430, val loss: 2.179993152618408
Epoch 440, training loss: 856.3074340820312 = 2.177448034286499 + 100.0 * 8.541299819946289
Epoch 440, val loss: 2.1798529624938965
Epoch 450, training loss: 856.1029052734375 = 2.1772881746292114 + 100.0 * 8.53925609588623
Epoch 450, val loss: 2.1797194480895996
Epoch 460, training loss: 855.4191284179688 = 2.1771024465560913 + 100.0 * 8.53242015838623
Epoch 460, val loss: 2.1795787811279297
Epoch 470, training loss: 854.9652709960938 = 2.176932215690613 + 100.0 * 8.527883529663086
Epoch 470, val loss: 2.179448127746582
Epoch 480, training loss: 854.521728515625 = 2.1767778396606445 + 100.0 * 8.523449897766113
Epoch 480, val loss: 2.1793203353881836
Epoch 490, training loss: 854.2020263671875 = 2.176623821258545 + 100.0 * 8.520254135131836
Epoch 490, val loss: 2.1791884899139404
Epoch 500, training loss: 854.1605834960938 = 2.1764432191848755 + 100.0 * 8.519841194152832
Epoch 500, val loss: 2.179058074951172
Epoch 510, training loss: 853.6058959960938 = 2.1762659549713135 + 100.0 * 8.514296531677246
Epoch 510, val loss: 2.178920269012451
Epoch 520, training loss: 853.2595825195312 = 2.1761151552200317 + 100.0 * 8.510834693908691
Epoch 520, val loss: 2.1787896156311035
Epoch 530, training loss: 853.092529296875 = 2.1759512424468994 + 100.0 * 8.50916576385498
Epoch 530, val loss: 2.178656578063965
Epoch 540, training loss: 852.9130859375 = 2.1757712364196777 + 100.0 * 8.507372856140137
Epoch 540, val loss: 2.178518772125244
Epoch 550, training loss: 852.5068359375 = 2.1756101846694946 + 100.0 * 8.503312110900879
Epoch 550, val loss: 2.1783852577209473
Epoch 560, training loss: 852.2872314453125 = 2.1754499673843384 + 100.0 * 8.501117706298828
Epoch 560, val loss: 2.1782565116882324
Epoch 570, training loss: 852.1161499023438 = 2.1752859354019165 + 100.0 * 8.499408721923828
Epoch 570, val loss: 2.178129196166992
Epoch 580, training loss: 851.8619384765625 = 2.175124764442444 + 100.0 * 8.496868133544922
Epoch 580, val loss: 2.1779913902282715
Epoch 590, training loss: 851.7383422851562 = 2.1749558448791504 + 100.0 * 8.495634078979492
Epoch 590, val loss: 2.177865743637085
Epoch 600, training loss: 851.4901733398438 = 2.174809455871582 + 100.0 * 8.49315357208252
Epoch 600, val loss: 2.1777400970458984
Epoch 610, training loss: 851.3283081054688 = 2.174654483795166 + 100.0 * 8.491536140441895
Epoch 610, val loss: 2.1776132583618164
Epoch 620, training loss: 851.1589965820312 = 2.174489974975586 + 100.0 * 8.489845275878906
Epoch 620, val loss: 2.1774864196777344
Epoch 630, training loss: 850.972412109375 = 2.174341320991516 + 100.0 * 8.487980842590332
Epoch 630, val loss: 2.177363157272339
Epoch 640, training loss: 850.7949829101562 = 2.174195408821106 + 100.0 * 8.486207962036133
Epoch 640, val loss: 2.1772449016571045
Epoch 650, training loss: 850.8912963867188 = 2.174041986465454 + 100.0 * 8.48717212677002
Epoch 650, val loss: 2.1771292686462402
Epoch 660, training loss: 850.5529174804688 = 2.173903703689575 + 100.0 * 8.483790397644043
Epoch 660, val loss: 2.177006959915161
Epoch 670, training loss: 850.2337036132812 = 2.1737589836120605 + 100.0 * 8.480599403381348
Epoch 670, val loss: 2.176898241043091
Epoch 680, training loss: 850.0621948242188 = 2.1736167669296265 + 100.0 * 8.478885650634766
Epoch 680, val loss: 2.1767969131469727
Epoch 690, training loss: 850.0675659179688 = 2.1734756231307983 + 100.0 * 8.478940963745117
Epoch 690, val loss: 2.1766910552978516
Epoch 700, training loss: 849.7398681640625 = 2.173353672027588 + 100.0 * 8.475665092468262
Epoch 700, val loss: 2.1765854358673096
Epoch 710, training loss: 849.6824951171875 = 2.173227548599243 + 100.0 * 8.475092887878418
Epoch 710, val loss: 2.176478862762451
Epoch 720, training loss: 849.4109497070312 = 2.173103451728821 + 100.0 * 8.472378730773926
Epoch 720, val loss: 2.176380157470703
Epoch 730, training loss: 849.225341796875 = 2.1729670763015747 + 100.0 * 8.470523834228516
Epoch 730, val loss: 2.176285743713379
Epoch 740, training loss: 849.4647827148438 = 2.1728450059890747 + 100.0 * 8.472919464111328
Epoch 740, val loss: 2.1761906147003174
Epoch 750, training loss: 849.0206298828125 = 2.172711133956909 + 100.0 * 8.46847915649414
Epoch 750, val loss: 2.176083564758301
Epoch 760, training loss: 848.7770385742188 = 2.1725956201553345 + 100.0 * 8.466044425964355
Epoch 760, val loss: 2.17598295211792
Epoch 770, training loss: 848.6116333007812 = 2.1724624633789062 + 100.0 * 8.464391708374023
Epoch 770, val loss: 2.1758873462677
Epoch 780, training loss: 848.8079833984375 = 2.1723525524139404 + 100.0 * 8.46635627746582
Epoch 780, val loss: 2.175787925720215
Epoch 790, training loss: 848.5061645507812 = 2.172187566757202 + 100.0 * 8.463339805603027
Epoch 790, val loss: 2.1756832599639893
Epoch 800, training loss: 848.2340698242188 = 2.1720707416534424 + 100.0 * 8.460619926452637
Epoch 800, val loss: 2.1755852699279785
Epoch 810, training loss: 848.1039428710938 = 2.171946167945862 + 100.0 * 8.459320068359375
Epoch 810, val loss: 2.1754822731018066
Epoch 820, training loss: 848.1782836914062 = 2.171809434890747 + 100.0 * 8.460064888000488
Epoch 820, val loss: 2.1753833293914795
Epoch 830, training loss: 847.9335327148438 = 2.1716829538345337 + 100.0 * 8.457618713378906
Epoch 830, val loss: 2.175276756286621
Epoch 840, training loss: 847.798583984375 = 2.171546697616577 + 100.0 * 8.456270217895508
Epoch 840, val loss: 2.175175666809082
Epoch 850, training loss: 847.6947021484375 = 2.171419858932495 + 100.0 * 8.455232620239258
Epoch 850, val loss: 2.175074577331543
Epoch 860, training loss: 847.9605102539062 = 2.1712762117385864 + 100.0 * 8.457892417907715
Epoch 860, val loss: 2.174964189529419
Epoch 870, training loss: 847.6635131835938 = 2.17113721370697 + 100.0 * 8.454923629760742
Epoch 870, val loss: 2.1748578548431396
Epoch 880, training loss: 847.4192504882812 = 2.171012282371521 + 100.0 * 8.452482223510742
Epoch 880, val loss: 2.1747570037841797
Epoch 890, training loss: 847.290283203125 = 2.1708840131759644 + 100.0 * 8.451193809509277
Epoch 890, val loss: 2.174656867980957
Epoch 900, training loss: 847.2123413085938 = 2.170760154724121 + 100.0 * 8.45041561126709
Epoch 900, val loss: 2.1745588779449463
Epoch 910, training loss: 847.153564453125 = 2.170630693435669 + 100.0 * 8.4498291015625
Epoch 910, val loss: 2.174464464187622
Epoch 920, training loss: 847.2206420898438 = 2.17048716545105 + 100.0 * 8.450501441955566
Epoch 920, val loss: 2.174360752105713
Epoch 930, training loss: 847.0789794921875 = 2.170364737510681 + 100.0 * 8.44908618927002
Epoch 930, val loss: 2.174257755279541
Epoch 940, training loss: 846.9181518554688 = 2.170249581336975 + 100.0 * 8.447479248046875
Epoch 940, val loss: 2.1741580963134766
Epoch 950, training loss: 846.8439331054688 = 2.1701197624206543 + 100.0 * 8.446738243103027
Epoch 950, val loss: 2.174063205718994
Epoch 960, training loss: 846.7650146484375 = 2.1699975728988647 + 100.0 * 8.445950508117676
Epoch 960, val loss: 2.1739673614501953
Epoch 970, training loss: 846.8071899414062 = 2.1698713302612305 + 100.0 * 8.446372985839844
Epoch 970, val loss: 2.173872232437134
Epoch 980, training loss: 846.6520385742188 = 2.169752836227417 + 100.0 * 8.444823265075684
Epoch 980, val loss: 2.173772096633911
Epoch 990, training loss: 846.9212646484375 = 2.1696295738220215 + 100.0 * 8.447516441345215
Epoch 990, val loss: 2.173673152923584
Epoch 1000, training loss: 846.5487060546875 = 2.1694986820220947 + 100.0 * 8.443792343139648
Epoch 1000, val loss: 2.173576831817627
Epoch 1010, training loss: 846.4609375 = 2.1693782806396484 + 100.0 * 8.442915916442871
Epoch 1010, val loss: 2.1734843254089355
Epoch 1020, training loss: 846.3939208984375 = 2.169261336326599 + 100.0 * 8.442246437072754
Epoch 1020, val loss: 2.1733925342559814
Epoch 1030, training loss: 846.3248901367188 = 2.1691462993621826 + 100.0 * 8.441557884216309
Epoch 1030, val loss: 2.1733055114746094
Epoch 1040, training loss: 846.2677612304688 = 2.1690303087234497 + 100.0 * 8.440987586975098
Epoch 1040, val loss: 2.173214912414551
Epoch 1050, training loss: 846.4090576171875 = 2.168923258781433 + 100.0 * 8.442400932312012
Epoch 1050, val loss: 2.1731245517730713
Epoch 1060, training loss: 846.3914794921875 = 2.16878879070282 + 100.0 * 8.442227363586426
Epoch 1060, val loss: 2.173032283782959
Epoch 1070, training loss: 846.1675415039062 = 2.168670892715454 + 100.0 * 8.439988136291504
Epoch 1070, val loss: 2.172943592071533
Epoch 1080, training loss: 846.0418701171875 = 2.1685633659362793 + 100.0 * 8.438733100891113
Epoch 1080, val loss: 2.1728553771972656
Epoch 1090, training loss: 846.0262451171875 = 2.1684571504592896 + 100.0 * 8.438577651977539
Epoch 1090, val loss: 2.172769784927368
Epoch 1100, training loss: 846.1414794921875 = 2.16834819316864 + 100.0 * 8.43973159790039
Epoch 1100, val loss: 2.1726861000061035
Epoch 1110, training loss: 845.9859008789062 = 2.1682242155075073 + 100.0 * 8.438177108764648
Epoch 1110, val loss: 2.1726012229919434
Epoch 1120, training loss: 845.864013671875 = 2.1681201457977295 + 100.0 * 8.436959266662598
Epoch 1120, val loss: 2.172516345977783
Epoch 1130, training loss: 845.7645874023438 = 2.1680171489715576 + 100.0 * 8.435965538024902
Epoch 1130, val loss: 2.1724371910095215
Epoch 1140, training loss: 845.7354736328125 = 2.167915463447571 + 100.0 * 8.435675621032715
Epoch 1140, val loss: 2.1723601818084717
Epoch 1150, training loss: 845.7993774414062 = 2.1678203344345093 + 100.0 * 8.436315536499023
Epoch 1150, val loss: 2.1722817420959473
Epoch 1160, training loss: 845.6969604492188 = 2.1677085161209106 + 100.0 * 8.43529224395752
Epoch 1160, val loss: 2.172201156616211
Epoch 1170, training loss: 845.758056640625 = 2.1676019430160522 + 100.0 * 8.435904502868652
Epoch 1170, val loss: 2.1721174716949463
Epoch 1180, training loss: 845.584716796875 = 2.1674935817718506 + 100.0 * 8.434172630310059
Epoch 1180, val loss: 2.172041177749634
Epoch 1190, training loss: 845.4774169921875 = 2.1673983335494995 + 100.0 * 8.433099746704102
Epoch 1190, val loss: 2.1719698905944824
Epoch 1200, training loss: 845.4413452148438 = 2.167304277420044 + 100.0 * 8.432740211486816
Epoch 1200, val loss: 2.171898603439331
Epoch 1210, training loss: 845.6408081054688 = 2.1672027111053467 + 100.0 * 8.434736251831055
Epoch 1210, val loss: 2.171827793121338
Epoch 1220, training loss: 845.4430541992188 = 2.1671175956726074 + 100.0 * 8.432759284973145
Epoch 1220, val loss: 2.1717522144317627
Epoch 1230, training loss: 845.2994995117188 = 2.1670175790786743 + 100.0 * 8.43132495880127
Epoch 1230, val loss: 2.1716809272766113
Epoch 1240, training loss: 845.26416015625 = 2.166925072669983 + 100.0 * 8.4309720993042
Epoch 1240, val loss: 2.171614408493042
Epoch 1250, training loss: 845.4328002929688 = 2.1668351888656616 + 100.0 * 8.432660102844238
Epoch 1250, val loss: 2.171544075012207
Epoch 1260, training loss: 845.2110595703125 = 2.1667433977127075 + 100.0 * 8.430442810058594
Epoch 1260, val loss: 2.1714766025543213
Epoch 1270, training loss: 845.1616821289062 = 2.166653871536255 + 100.0 * 8.429950714111328
Epoch 1270, val loss: 2.1714091300964355
Epoch 1280, training loss: 845.1100463867188 = 2.1665656566619873 + 100.0 * 8.429434776306152
Epoch 1280, val loss: 2.1713438034057617
Epoch 1290, training loss: 845.034423828125 = 2.1664804220199585 + 100.0 * 8.428679466247559
Epoch 1290, val loss: 2.171280860900879
Epoch 1300, training loss: 845.0464477539062 = 2.1663949489593506 + 100.0 * 8.428800582885742
Epoch 1300, val loss: 2.1712183952331543
Epoch 1310, training loss: 844.9318237304688 = 2.166309952735901 + 100.0 * 8.427655220031738
Epoch 1310, val loss: 2.1711554527282715
Epoch 1320, training loss: 844.9223022460938 = 2.1662286520004272 + 100.0 * 8.427560806274414
Epoch 1320, val loss: 2.1710963249206543
Epoch 1330, training loss: 845.1987915039062 = 2.166151285171509 + 100.0 * 8.430326461791992
Epoch 1330, val loss: 2.1710317134857178
Epoch 1340, training loss: 844.826904296875 = 2.1660478115081787 + 100.0 * 8.426608085632324
Epoch 1340, val loss: 2.1709656715393066
Epoch 1350, training loss: 844.8212280273438 = 2.165970206260681 + 100.0 * 8.426552772521973
Epoch 1350, val loss: 2.1709022521972656
Epoch 1360, training loss: 845.0902709960938 = 2.1658869981765747 + 100.0 * 8.429244041442871
Epoch 1360, val loss: 2.1708407402038574
Epoch 1370, training loss: 844.771728515625 = 2.165805459022522 + 100.0 * 8.426058769226074
Epoch 1370, val loss: 2.1707849502563477
Epoch 1380, training loss: 844.6612548828125 = 2.1657274961471558 + 100.0 * 8.424955368041992
Epoch 1380, val loss: 2.170724391937256
Epoch 1390, training loss: 844.5963134765625 = 2.1656543016433716 + 100.0 * 8.424306869506836
Epoch 1390, val loss: 2.170672655105591
Epoch 1400, training loss: 844.557373046875 = 2.1655797958374023 + 100.0 * 8.423917770385742
Epoch 1400, val loss: 2.170619487762451
Epoch 1410, training loss: 844.7034301757812 = 2.165501594543457 + 100.0 * 8.425378799438477
Epoch 1410, val loss: 2.170565366744995
Epoch 1420, training loss: 844.5282592773438 = 2.1654317378997803 + 100.0 * 8.423628807067871
Epoch 1420, val loss: 2.1705071926116943
Epoch 1430, training loss: 844.5842895507812 = 2.1653507947921753 + 100.0 * 8.424189567565918
Epoch 1430, val loss: 2.1704511642456055
Epoch 1440, training loss: 844.4712524414062 = 2.165279984474182 + 100.0 * 8.423059463500977
Epoch 1440, val loss: 2.170393943786621
Epoch 1450, training loss: 844.3917236328125 = 2.1652051210403442 + 100.0 * 8.42226505279541
Epoch 1450, val loss: 2.1703453063964844
Epoch 1460, training loss: 844.3989868164062 = 2.165139079093933 + 100.0 * 8.422338485717773
Epoch 1460, val loss: 2.170295238494873
Epoch 1470, training loss: 844.4459228515625 = 2.165066957473755 + 100.0 * 8.422808647155762
Epoch 1470, val loss: 2.1702451705932617
Epoch 1480, training loss: 844.3648681640625 = 2.164993643760681 + 100.0 * 8.421998977661133
Epoch 1480, val loss: 2.170191526412964
Epoch 1490, training loss: 844.3347778320312 = 2.164923310279846 + 100.0 * 8.421698570251465
Epoch 1490, val loss: 2.1701371669769287
Epoch 1500, training loss: 844.3073120117188 = 2.1648584604263306 + 100.0 * 8.421424865722656
Epoch 1500, val loss: 2.1700873374938965
Epoch 1510, training loss: 844.2695922851562 = 2.164782166481018 + 100.0 * 8.421048164367676
Epoch 1510, val loss: 2.170036792755127
Epoch 1520, training loss: 844.14794921875 = 2.164719343185425 + 100.0 * 8.419832229614258
Epoch 1520, val loss: 2.1699886322021484
Epoch 1530, training loss: 844.131591796875 = 2.1646530628204346 + 100.0 * 8.419669151306152
Epoch 1530, val loss: 2.1699421405792236
Epoch 1540, training loss: 844.31103515625 = 2.164581537246704 + 100.0 * 8.421463966369629
Epoch 1540, val loss: 2.1698951721191406
Epoch 1550, training loss: 844.2050170898438 = 2.164520025253296 + 100.0 * 8.420405387878418
Epoch 1550, val loss: 2.16984224319458
Epoch 1560, training loss: 844.0827026367188 = 2.1644481420516968 + 100.0 * 8.419182777404785
Epoch 1560, val loss: 2.1697921752929688
Epoch 1570, training loss: 844.0609741210938 = 2.164386510848999 + 100.0 * 8.418966293334961
Epoch 1570, val loss: 2.1697449684143066
Epoch 1580, training loss: 844.0123901367188 = 2.1643195152282715 + 100.0 * 8.41848087310791
Epoch 1580, val loss: 2.1696996688842773
Epoch 1590, training loss: 843.9804077148438 = 2.16426157951355 + 100.0 * 8.418161392211914
Epoch 1590, val loss: 2.1696553230285645
Epoch 1600, training loss: 843.9554443359375 = 2.164198160171509 + 100.0 * 8.417912483215332
Epoch 1600, val loss: 2.1696112155914307
Epoch 1610, training loss: 844.0264892578125 = 2.1641346216201782 + 100.0 * 8.418623924255371
Epoch 1610, val loss: 2.169565200805664
Epoch 1620, training loss: 843.9520874023438 = 2.1640650033950806 + 100.0 * 8.417880058288574
Epoch 1620, val loss: 2.1695141792297363
Epoch 1630, training loss: 843.9052734375 = 2.1640042066574097 + 100.0 * 8.417412757873535
Epoch 1630, val loss: 2.169466972351074
Epoch 1640, training loss: 843.8909301757812 = 2.1639416217803955 + 100.0 * 8.417269706726074
Epoch 1640, val loss: 2.169421672821045
Epoch 1650, training loss: 843.7880249023438 = 2.163880467414856 + 100.0 * 8.416241645812988
Epoch 1650, val loss: 2.1693811416625977
Epoch 1660, training loss: 843.7584228515625 = 2.1638238430023193 + 100.0 * 8.415946006774902
Epoch 1660, val loss: 2.169339179992676
Epoch 1670, training loss: 844.0240478515625 = 2.1637567281723022 + 100.0 * 8.41860294342041
Epoch 1670, val loss: 2.169293165206909
Epoch 1680, training loss: 843.780029296875 = 2.1637046337127686 + 100.0 * 8.416163444519043
Epoch 1680, val loss: 2.1692497730255127
Epoch 1690, training loss: 843.7083740234375 = 2.163640260696411 + 100.0 * 8.415447235107422
Epoch 1690, val loss: 2.169206142425537
Epoch 1700, training loss: 843.8303833007812 = 2.1635886430740356 + 100.0 * 8.416667938232422
Epoch 1700, val loss: 2.1691646575927734
Epoch 1710, training loss: 843.64111328125 = 2.163521885871887 + 100.0 * 8.414775848388672
Epoch 1710, val loss: 2.1691207885742188
Epoch 1720, training loss: 843.67724609375 = 2.1634676456451416 + 100.0 * 8.415138244628906
Epoch 1720, val loss: 2.1690781116485596
Epoch 1730, training loss: 843.705810546875 = 2.1634069681167603 + 100.0 * 8.415424346923828
Epoch 1730, val loss: 2.169034481048584
Epoch 1740, training loss: 843.5746459960938 = 2.163346529006958 + 100.0 * 8.41411304473877
Epoch 1740, val loss: 2.1689939498901367
Epoch 1750, training loss: 843.5108642578125 = 2.1632953882217407 + 100.0 * 8.41347599029541
Epoch 1750, val loss: 2.1689562797546387
Epoch 1760, training loss: 843.49609375 = 2.1632453203201294 + 100.0 * 8.413328170776367
Epoch 1760, val loss: 2.1689202785491943
Epoch 1770, training loss: 843.7252807617188 = 2.1631879806518555 + 100.0 * 8.415620803833008
Epoch 1770, val loss: 2.1688790321350098
Epoch 1780, training loss: 843.614501953125 = 2.163125157356262 + 100.0 * 8.41451358795166
Epoch 1780, val loss: 2.1688356399536133
Epoch 1790, training loss: 843.5007934570312 = 2.163070321083069 + 100.0 * 8.413376808166504
Epoch 1790, val loss: 2.168792963027954
Epoch 1800, training loss: 843.4019165039062 = 2.1630165576934814 + 100.0 * 8.412388801574707
Epoch 1800, val loss: 2.168755531311035
Epoch 1810, training loss: 843.4064331054688 = 2.1629644632339478 + 100.0 * 8.412434577941895
Epoch 1810, val loss: 2.1687192916870117
Epoch 1820, training loss: 843.558349609375 = 2.162912368774414 + 100.0 * 8.413954734802246
Epoch 1820, val loss: 2.168679714202881
Epoch 1830, training loss: 843.3154907226562 = 2.1628546714782715 + 100.0 * 8.411526679992676
Epoch 1830, val loss: 2.168640613555908
Epoch 1840, training loss: 843.369140625 = 2.1628003120422363 + 100.0 * 8.412063598632812
Epoch 1840, val loss: 2.168602466583252
Epoch 1850, training loss: 843.4943237304688 = 2.1627472639083862 + 100.0 * 8.413315773010254
Epoch 1850, val loss: 2.168562889099121
Epoch 1860, training loss: 843.29296875 = 2.162695527076721 + 100.0 * 8.41130256652832
Epoch 1860, val loss: 2.168524742126465
Epoch 1870, training loss: 843.224365234375 = 2.1626464128494263 + 100.0 * 8.410616874694824
Epoch 1870, val loss: 2.168489694595337
Epoch 1880, training loss: 843.1976928710938 = 2.1625990867614746 + 100.0 * 8.410350799560547
Epoch 1880, val loss: 2.1684579849243164
Epoch 1890, training loss: 843.6640625 = 2.1625605821609497 + 100.0 * 8.41501522064209
Epoch 1890, val loss: 2.168428421020508
Epoch 1900, training loss: 843.4542846679688 = 2.162480115890503 + 100.0 * 8.412918090820312
Epoch 1900, val loss: 2.168367862701416
Epoch 1910, training loss: 843.1288452148438 = 2.162429094314575 + 100.0 * 8.409664154052734
Epoch 1910, val loss: 2.1683297157287598
Epoch 1920, training loss: 843.1370239257812 = 2.1623849868774414 + 100.0 * 8.409746170043945
Epoch 1920, val loss: 2.168299674987793
Epoch 1930, training loss: 843.0801391601562 = 2.1623374223709106 + 100.0 * 8.409177780151367
Epoch 1930, val loss: 2.168266773223877
Epoch 1940, training loss: 843.191650390625 = 2.162294387817383 + 100.0 * 8.410293579101562
Epoch 1940, val loss: 2.1682353019714355
Epoch 1950, training loss: 843.0829467773438 = 2.1622308492660522 + 100.0 * 8.409207344055176
Epoch 1950, val loss: 2.1681857109069824
Epoch 1960, training loss: 843.1198120117188 = 2.162176012992859 + 100.0 * 8.409576416015625
Epoch 1960, val loss: 2.168145179748535
Epoch 1970, training loss: 843.0216064453125 = 2.162129282951355 + 100.0 * 8.408595085144043
Epoch 1970, val loss: 2.16811466217041
Epoch 1980, training loss: 842.9716186523438 = 2.162084460258484 + 100.0 * 8.408095359802246
Epoch 1980, val loss: 2.1680831909179688
Epoch 1990, training loss: 842.9544067382812 = 2.162044048309326 + 100.0 * 8.407923698425293
Epoch 1990, val loss: 2.1680548191070557
Epoch 2000, training loss: 843.0009765625 = 2.1620001792907715 + 100.0 * 8.408390045166016
Epoch 2000, val loss: 2.168025016784668
Epoch 2010, training loss: 843.1477661132812 = 2.1619473695755005 + 100.0 * 8.409858703613281
Epoch 2010, val loss: 2.1679844856262207
Epoch 2020, training loss: 842.9451904296875 = 2.161895513534546 + 100.0 * 8.407833099365234
Epoch 2020, val loss: 2.1679461002349854
Epoch 2030, training loss: 842.8794555664062 = 2.161849021911621 + 100.0 * 8.40717601776123
Epoch 2030, val loss: 2.167912006378174
Epoch 2040, training loss: 842.8516235351562 = 2.161808967590332 + 100.0 * 8.406898498535156
Epoch 2040, val loss: 2.1678848266601562
Epoch 2050, training loss: 842.880126953125 = 2.1617677211761475 + 100.0 * 8.407183647155762
Epoch 2050, val loss: 2.1678547859191895
Epoch 2060, training loss: 843.0491943359375 = 2.1617188453674316 + 100.0 * 8.40887451171875
Epoch 2060, val loss: 2.167816400527954
Epoch 2070, training loss: 842.8458251953125 = 2.161665201187134 + 100.0 * 8.406841278076172
Epoch 2070, val loss: 2.167780876159668
Epoch 2080, training loss: 842.77490234375 = 2.161623954772949 + 100.0 * 8.406132698059082
Epoch 2080, val loss: 2.1677494049072266
Epoch 2090, training loss: 842.746337890625 = 2.161582350730896 + 100.0 * 8.405847549438477
Epoch 2090, val loss: 2.1677207946777344
Epoch 2100, training loss: 842.7460327148438 = 2.1615419387817383 + 100.0 * 8.405844688415527
Epoch 2100, val loss: 2.1676926612854004
Epoch 2110, training loss: 843.1155395507812 = 2.1614972352981567 + 100.0 * 8.409540176391602
Epoch 2110, val loss: 2.1676604747772217
Epoch 2120, training loss: 842.8894653320312 = 2.161441206932068 + 100.0 * 8.407279968261719
Epoch 2120, val loss: 2.167616367340088
Epoch 2130, training loss: 842.730224609375 = 2.161394238471985 + 100.0 * 8.405688285827637
Epoch 2130, val loss: 2.1675806045532227
Epoch 2140, training loss: 842.714111328125 = 2.1613528728485107 + 100.0 * 8.405527114868164
Epoch 2140, val loss: 2.1675500869750977
Epoch 2150, training loss: 842.6724243164062 = 2.1613082885742188 + 100.0 * 8.405111312866211
Epoch 2150, val loss: 2.167518377304077
Epoch 2160, training loss: 842.73779296875 = 2.1612669229507446 + 100.0 * 8.405765533447266
Epoch 2160, val loss: 2.167487859725952
Epoch 2170, training loss: 842.7015991210938 = 2.161221742630005 + 100.0 * 8.405404090881348
Epoch 2170, val loss: 2.1674537658691406
Epoch 2180, training loss: 842.6068115234375 = 2.161177635192871 + 100.0 * 8.40445613861084
Epoch 2180, val loss: 2.167422294616699
Epoch 2190, training loss: 842.6176147460938 = 2.1611368656158447 + 100.0 * 8.40456485748291
Epoch 2190, val loss: 2.1673927307128906
Epoch 2200, training loss: 842.7903442382812 = 2.1610957384109497 + 100.0 * 8.406292915344238
Epoch 2200, val loss: 2.1673598289489746
Epoch 2210, training loss: 842.57080078125 = 2.161045551300049 + 100.0 * 8.404097557067871
Epoch 2210, val loss: 2.167323589324951
Epoch 2220, training loss: 842.6871948242188 = 2.160996675491333 + 100.0 * 8.405261993408203
Epoch 2220, val loss: 2.1672849655151367
Epoch 2230, training loss: 842.5016479492188 = 2.1609545946121216 + 100.0 * 8.403407096862793
Epoch 2230, val loss: 2.1672539710998535
Epoch 2240, training loss: 842.4935913085938 = 2.160914897918701 + 100.0 * 8.403326988220215
Epoch 2240, val loss: 2.167224407196045
Epoch 2250, training loss: 842.4683837890625 = 2.1608740091323853 + 100.0 * 8.403075218200684
Epoch 2250, val loss: 2.1671946048736572
Epoch 2260, training loss: 842.5208129882812 = 2.160834789276123 + 100.0 * 8.403599739074707
Epoch 2260, val loss: 2.1671652793884277
Epoch 2270, training loss: 842.7862548828125 = 2.1607872247695923 + 100.0 * 8.406254768371582
Epoch 2270, val loss: 2.167128801345825
Epoch 2280, training loss: 842.4658203125 = 2.1607383489608765 + 100.0 * 8.403050422668457
Epoch 2280, val loss: 2.1670894622802734
Epoch 2290, training loss: 842.4164428710938 = 2.1606944799423218 + 100.0 * 8.402557373046875
Epoch 2290, val loss: 2.1670567989349365
Epoch 2300, training loss: 842.383544921875 = 2.1606552600860596 + 100.0 * 8.402229309082031
Epoch 2300, val loss: 2.1670265197753906
Epoch 2310, training loss: 842.3680419921875 = 2.1606205701828003 + 100.0 * 8.402073860168457
Epoch 2310, val loss: 2.1670022010803223
Epoch 2320, training loss: 842.3671875 = 2.1605842113494873 + 100.0 * 8.402066230773926
Epoch 2320, val loss: 2.1669750213623047
Epoch 2330, training loss: 842.64892578125 = 2.160549521446228 + 100.0 * 8.404884338378906
Epoch 2330, val loss: 2.166947603225708
Epoch 2340, training loss: 842.4487915039062 = 2.1604902744293213 + 100.0 * 8.402883529663086
Epoch 2340, val loss: 2.1668996810913086
Epoch 2350, training loss: 842.5089111328125 = 2.160447120666504 + 100.0 * 8.403484344482422
Epoch 2350, val loss: 2.1668648719787598
Epoch 2360, training loss: 842.3406982421875 = 2.1603928804397583 + 100.0 * 8.401803016662598
Epoch 2360, val loss: 2.1668224334716797
Epoch 2370, training loss: 842.3097534179688 = 2.16035532951355 + 100.0 * 8.401494026184082
Epoch 2370, val loss: 2.166794776916504
Epoch 2380, training loss: 842.249267578125 = 2.160317540168762 + 100.0 * 8.40088939666748
Epoch 2380, val loss: 2.16676664352417
Epoch 2390, training loss: 842.235107421875 = 2.160282015800476 + 100.0 * 8.400748252868652
Epoch 2390, val loss: 2.166740894317627
Epoch 2400, training loss: 842.2929077148438 = 2.160244345664978 + 100.0 * 8.401327133178711
Epoch 2400, val loss: 2.1667118072509766
Epoch 2410, training loss: 842.5382080078125 = 2.1601961851119995 + 100.0 * 8.403779983520508
Epoch 2410, val loss: 2.1666717529296875
Epoch 2420, training loss: 842.3907470703125 = 2.1601369380950928 + 100.0 * 8.402305603027344
Epoch 2420, val loss: 2.166623592376709
Epoch 2430, training loss: 842.2357788085938 = 2.1600922346115112 + 100.0 * 8.4007568359375
Epoch 2430, val loss: 2.1665873527526855
Epoch 2440, training loss: 842.1832885742188 = 2.1600555181503296 + 100.0 * 8.400232315063477
Epoch 2440, val loss: 2.1665596961975098
Epoch 2450, training loss: 842.155517578125 = 2.1600197553634644 + 100.0 * 8.399954795837402
Epoch 2450, val loss: 2.1665332317352295
Epoch 2460, training loss: 842.1415405273438 = 2.159984827041626 + 100.0 * 8.399815559387207
Epoch 2460, val loss: 2.166505813598633
Epoch 2470, training loss: 842.3338623046875 = 2.159947991371155 + 100.0 * 8.401739120483398
Epoch 2470, val loss: 2.1664750576019287
Epoch 2480, training loss: 842.1204833984375 = 2.159900426864624 + 100.0 * 8.399605751037598
Epoch 2480, val loss: 2.166438579559326
Epoch 2490, training loss: 842.0951538085938 = 2.1598576307296753 + 100.0 * 8.39935302734375
Epoch 2490, val loss: 2.1664040088653564
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.39666666666666667
0.8098239513149316
=== training gcn model ===
Epoch 0, training loss: 1060.4180908203125 = 2.192582845687866 + 100.0 * 10.582254409790039
Epoch 0, val loss: 2.192629337310791
Epoch 10, training loss: 1060.3739013671875 = 2.1875975131988525 + 100.0 * 10.581863403320312
Epoch 10, val loss: 2.188582420349121
Epoch 20, training loss: 1060.21630859375 = 2.1854889392852783 + 100.0 * 10.580307960510254
Epoch 20, val loss: 2.186981201171875
Epoch 30, training loss: 1059.571533203125 = 2.1851402521133423 + 100.0 * 10.573863983154297
Epoch 30, val loss: 2.186781883239746
Epoch 40, training loss: 1057.0191650390625 = 2.185780167579651 + 100.0 * 10.548334121704102
Epoch 40, val loss: 2.187389850616455
Epoch 50, training loss: 1048.1412353515625 = 2.187353014945984 + 100.0 * 10.459538459777832
Epoch 50, val loss: 2.1888155937194824
Epoch 60, training loss: 1021.7537231445312 = 2.1902952194213867 + 100.0 * 10.195633888244629
Epoch 60, val loss: 2.1914432048797607
Epoch 70, training loss: 980.5999145507812 = 2.1933112144470215 + 100.0 * 9.784066200256348
Epoch 70, val loss: 2.1939845085144043
Epoch 80, training loss: 970.5889282226562 = 2.19428551197052 + 100.0 * 9.68394660949707
Epoch 80, val loss: 2.1945886611938477
Epoch 90, training loss: 964.2545776367188 = 2.192345380783081 + 100.0 * 9.620622634887695
Epoch 90, val loss: 2.192538022994995
Epoch 100, training loss: 956.000732421875 = 2.1905194520950317 + 100.0 * 9.538102149963379
Epoch 100, val loss: 2.190807580947876
Epoch 110, training loss: 940.7463989257812 = 2.189471960067749 + 100.0 * 9.38556957244873
Epoch 110, val loss: 2.189716339111328
Epoch 120, training loss: 927.7960815429688 = 2.188243269920349 + 100.0 * 9.256078720092773
Epoch 120, val loss: 2.188570976257324
Epoch 130, training loss: 922.1156616210938 = 2.187356114387512 + 100.0 * 9.1992826461792
Epoch 130, val loss: 2.1876885890960693
Epoch 140, training loss: 914.2835083007812 = 2.1862281560897827 + 100.0 * 9.120972633361816
Epoch 140, val loss: 2.186527729034424
Epoch 150, training loss: 905.4827270507812 = 2.1853004693984985 + 100.0 * 9.032974243164062
Epoch 150, val loss: 2.18564510345459
Epoch 160, training loss: 900.16845703125 = 2.1846283674240112 + 100.0 * 8.979838371276855
Epoch 160, val loss: 2.1850533485412598
Epoch 170, training loss: 894.4786987304688 = 2.184053421020508 + 100.0 * 8.922945976257324
Epoch 170, val loss: 2.184539794921875
Epoch 180, training loss: 890.681396484375 = 2.183586001396179 + 100.0 * 8.884978294372559
Epoch 180, val loss: 2.184077262878418
Epoch 190, training loss: 887.83349609375 = 2.183004379272461 + 100.0 * 8.856505393981934
Epoch 190, val loss: 2.1835217475891113
Epoch 200, training loss: 884.5330810546875 = 2.182535171508789 + 100.0 * 8.823505401611328
Epoch 200, val loss: 2.1830806732177734
Epoch 210, training loss: 881.4298706054688 = 2.1822006702423096 + 100.0 * 8.792476654052734
Epoch 210, val loss: 2.1827712059020996
Epoch 220, training loss: 879.3677368164062 = 2.1817997694015503 + 100.0 * 8.771859169006348
Epoch 220, val loss: 2.1824004650115967
Epoch 230, training loss: 877.0545654296875 = 2.1812362670898438 + 100.0 * 8.748733520507812
Epoch 230, val loss: 2.1818642616271973
Epoch 240, training loss: 874.519775390625 = 2.1807628870010376 + 100.0 * 8.723389625549316
Epoch 240, val loss: 2.181361675262451
Epoch 250, training loss: 871.8534545898438 = 2.1803083419799805 + 100.0 * 8.696731567382812
Epoch 250, val loss: 2.1809582710266113
Epoch 260, training loss: 869.6550903320312 = 2.1798975467681885 + 100.0 * 8.674752235412598
Epoch 260, val loss: 2.1805551052093506
Epoch 270, training loss: 867.8624267578125 = 2.17944598197937 + 100.0 * 8.656829833984375
Epoch 270, val loss: 2.1801228523254395
Epoch 280, training loss: 866.547119140625 = 2.17897629737854 + 100.0 * 8.643681526184082
Epoch 280, val loss: 2.1796789169311523
Epoch 290, training loss: 865.62109375 = 2.178466796875 + 100.0 * 8.63442611694336
Epoch 290, val loss: 2.1791558265686035
Epoch 300, training loss: 864.8807983398438 = 2.1779541969299316 + 100.0 * 8.627028465270996
Epoch 300, val loss: 2.178638458251953
Epoch 310, training loss: 864.165283203125 = 2.1774479150772095 + 100.0 * 8.619878768920898
Epoch 310, val loss: 2.1781368255615234
Epoch 320, training loss: 863.8258056640625 = 2.177036762237549 + 100.0 * 8.616487503051758
Epoch 320, val loss: 2.177649974822998
Epoch 330, training loss: 862.8413696289062 = 2.176511764526367 + 100.0 * 8.606648445129395
Epoch 330, val loss: 2.1771981716156006
Epoch 340, training loss: 861.9735107421875 = 2.176126003265381 + 100.0 * 8.597973823547363
Epoch 340, val loss: 2.176799774169922
Epoch 350, training loss: 861.2286376953125 = 2.175742506980896 + 100.0 * 8.590529441833496
Epoch 350, val loss: 2.1763863563537598
Epoch 360, training loss: 860.720947265625 = 2.175356388092041 + 100.0 * 8.585455894470215
Epoch 360, val loss: 2.1759982109069824
Epoch 370, training loss: 859.8820190429688 = 2.1749777793884277 + 100.0 * 8.577070236206055
Epoch 370, val loss: 2.1756014823913574
Epoch 380, training loss: 859.16845703125 = 2.1745883226394653 + 100.0 * 8.569938659667969
Epoch 380, val loss: 2.175210952758789
Epoch 390, training loss: 858.572509765625 = 2.1742113828659058 + 100.0 * 8.563982963562012
Epoch 390, val loss: 2.1748270988464355
Epoch 400, training loss: 858.027099609375 = 2.173826575279236 + 100.0 * 8.55853271484375
Epoch 400, val loss: 2.1744375228881836
Epoch 410, training loss: 857.6538696289062 = 2.173462390899658 + 100.0 * 8.554803848266602
Epoch 410, val loss: 2.1740410327911377
Epoch 420, training loss: 856.9393310546875 = 2.1730105876922607 + 100.0 * 8.547662734985352
Epoch 420, val loss: 2.173628807067871
Epoch 430, training loss: 856.4661254882812 = 2.1726155281066895 + 100.0 * 8.542935371398926
Epoch 430, val loss: 2.173206329345703
Epoch 440, training loss: 855.9918212890625 = 2.1722272634506226 + 100.0 * 8.538195610046387
Epoch 440, val loss: 2.172795534133911
Epoch 450, training loss: 855.8193359375 = 2.171809673309326 + 100.0 * 8.53647518157959
Epoch 450, val loss: 2.1723732948303223
Epoch 460, training loss: 855.2156982421875 = 2.1713720560073853 + 100.0 * 8.53044319152832
Epoch 460, val loss: 2.171942949295044
Epoch 470, training loss: 854.8539428710938 = 2.1709567308425903 + 100.0 * 8.526829719543457
Epoch 470, val loss: 2.1715123653411865
Epoch 480, training loss: 854.5266723632812 = 2.1705331802368164 + 100.0 * 8.523561477661133
Epoch 480, val loss: 2.171081066131592
Epoch 490, training loss: 854.3179931640625 = 2.1700994968414307 + 100.0 * 8.521478652954102
Epoch 490, val loss: 2.1706464290618896
Epoch 500, training loss: 854.2655639648438 = 2.169677257537842 + 100.0 * 8.52095890045166
Epoch 500, val loss: 2.170182228088379
Epoch 510, training loss: 853.9237670898438 = 2.1692159175872803 + 100.0 * 8.517545700073242
Epoch 510, val loss: 2.1697216033935547
Epoch 520, training loss: 853.6556396484375 = 2.168756604194641 + 100.0 * 8.51486873626709
Epoch 520, val loss: 2.1692726612091064
Epoch 530, training loss: 853.4398803710938 = 2.168304204940796 + 100.0 * 8.512716293334961
Epoch 530, val loss: 2.168820381164551
Epoch 540, training loss: 853.2474365234375 = 2.1678526401519775 + 100.0 * 8.510795593261719
Epoch 540, val loss: 2.1683688163757324
Epoch 550, training loss: 853.1295166015625 = 2.1673868894577026 + 100.0 * 8.509621620178223
Epoch 550, val loss: 2.1679301261901855
Epoch 560, training loss: 853.1642456054688 = 2.166978597640991 + 100.0 * 8.50997257232666
Epoch 560, val loss: 2.1674633026123047
Epoch 570, training loss: 852.7785034179688 = 2.1665191650390625 + 100.0 * 8.506119728088379
Epoch 570, val loss: 2.1670212745666504
Epoch 580, training loss: 852.5465087890625 = 2.1660674810409546 + 100.0 * 8.503804206848145
Epoch 580, val loss: 2.1665849685668945
Epoch 590, training loss: 852.368896484375 = 2.1656299829483032 + 100.0 * 8.502032279968262
Epoch 590, val loss: 2.1661529541015625
Epoch 600, training loss: 852.2056274414062 = 2.1652002334594727 + 100.0 * 8.500404357910156
Epoch 600, val loss: 2.1657261848449707
Epoch 610, training loss: 852.250244140625 = 2.1647684574127197 + 100.0 * 8.5008544921875
Epoch 610, val loss: 2.1652915477752686
Epoch 620, training loss: 851.9510498046875 = 2.164357900619507 + 100.0 * 8.4978666305542
Epoch 620, val loss: 2.1648528575897217
Epoch 630, training loss: 851.7334594726562 = 2.1639251708984375 + 100.0 * 8.495695114135742
Epoch 630, val loss: 2.1644344329833984
Epoch 640, training loss: 851.5728149414062 = 2.1635018587112427 + 100.0 * 8.49409294128418
Epoch 640, val loss: 2.1640217304229736
Epoch 650, training loss: 851.6788940429688 = 2.1630735397338867 + 100.0 * 8.495158195495605
Epoch 650, val loss: 2.163618564605713
Epoch 660, training loss: 851.5487060546875 = 2.162668824195862 + 100.0 * 8.493860244750977
Epoch 660, val loss: 2.163179397583008
Epoch 670, training loss: 851.2103881835938 = 2.1622440814971924 + 100.0 * 8.49048137664795
Epoch 670, val loss: 2.1627650260925293
Epoch 680, training loss: 851.0285034179688 = 2.161840796470642 + 100.0 * 8.488666534423828
Epoch 680, val loss: 2.1623616218566895
Epoch 690, training loss: 850.881103515625 = 2.1614322662353516 + 100.0 * 8.487196922302246
Epoch 690, val loss: 2.1619632244110107
Epoch 700, training loss: 850.7429809570312 = 2.161040782928467 + 100.0 * 8.485819816589355
Epoch 700, val loss: 2.1615655422210693
Epoch 710, training loss: 850.6285400390625 = 2.160650134086609 + 100.0 * 8.484679222106934
Epoch 710, val loss: 2.1611647605895996
Epoch 720, training loss: 850.5552978515625 = 2.1602444648742676 + 100.0 * 8.4839506149292
Epoch 720, val loss: 2.1607813835144043
Epoch 730, training loss: 850.4180297851562 = 2.1598583459854126 + 100.0 * 8.482582092285156
Epoch 730, val loss: 2.160370349884033
Epoch 740, training loss: 850.2297973632812 = 2.159462332725525 + 100.0 * 8.480703353881836
Epoch 740, val loss: 2.159980535507202
Epoch 750, training loss: 850.0997924804688 = 2.1590769290924072 + 100.0 * 8.47940731048584
Epoch 750, val loss: 2.159599781036377
Epoch 760, training loss: 849.988525390625 = 2.1586803197860718 + 100.0 * 8.47829818725586
Epoch 760, val loss: 2.159222364425659
Epoch 770, training loss: 849.8770141601562 = 2.1583051681518555 + 100.0 * 8.477187156677246
Epoch 770, val loss: 2.1588375568389893
Epoch 780, training loss: 849.6534423828125 = 2.1579270362854004 + 100.0 * 8.474955558776855
Epoch 780, val loss: 2.158466339111328
Epoch 790, training loss: 849.5370483398438 = 2.15755558013916 + 100.0 * 8.473794937133789
Epoch 790, val loss: 2.1581082344055176
Epoch 800, training loss: 849.8104248046875 = 2.1572147607803345 + 100.0 * 8.476531982421875
Epoch 800, val loss: 2.15773868560791
Epoch 810, training loss: 849.3012084960938 = 2.156820774078369 + 100.0 * 8.471444129943848
Epoch 810, val loss: 2.1573681831359863
Epoch 820, training loss: 849.1856689453125 = 2.1564449071884155 + 100.0 * 8.470292091369629
Epoch 820, val loss: 2.1570115089416504
Epoch 830, training loss: 849.0478515625 = 2.1560925245285034 + 100.0 * 8.468917846679688
Epoch 830, val loss: 2.1566522121429443
Epoch 840, training loss: 849.3682861328125 = 2.155720829963684 + 100.0 * 8.472126007080078
Epoch 840, val loss: 2.156296730041504
Epoch 850, training loss: 849.0389404296875 = 2.155357599258423 + 100.0 * 8.468835830688477
Epoch 850, val loss: 2.1559133529663086
Epoch 860, training loss: 848.7393188476562 = 2.1549806594848633 + 100.0 * 8.465843200683594
Epoch 860, val loss: 2.155559539794922
Epoch 870, training loss: 848.6258544921875 = 2.154637932777405 + 100.0 * 8.464712142944336
Epoch 870, val loss: 2.155202627182007
Epoch 880, training loss: 848.5225830078125 = 2.154273748397827 + 100.0 * 8.463683128356934
Epoch 880, val loss: 2.1548514366149902
Epoch 890, training loss: 848.4479370117188 = 2.153925895690918 + 100.0 * 8.462940216064453
Epoch 890, val loss: 2.1544981002807617
Epoch 900, training loss: 848.5283813476562 = 2.153571128845215 + 100.0 * 8.46374797821045
Epoch 900, val loss: 2.154141902923584
Epoch 910, training loss: 848.4354858398438 = 2.153219699859619 + 100.0 * 8.462822914123535
Epoch 910, val loss: 2.153775215148926
Epoch 920, training loss: 848.2342529296875 = 2.1528306007385254 + 100.0 * 8.460814476013184
Epoch 920, val loss: 2.153430938720703
Epoch 930, training loss: 848.0772705078125 = 2.152499556541443 + 100.0 * 8.459247589111328
Epoch 930, val loss: 2.1530771255493164
Epoch 940, training loss: 848.0985717773438 = 2.1521633863449097 + 100.0 * 8.459464073181152
Epoch 940, val loss: 2.1527304649353027
Epoch 950, training loss: 847.9607543945312 = 2.1517754793167114 + 100.0 * 8.458089828491211
Epoch 950, val loss: 2.152402400970459
Epoch 960, training loss: 848.1195068359375 = 2.1514322757720947 + 100.0 * 8.459680557250977
Epoch 960, val loss: 2.152043104171753
Epoch 970, training loss: 847.7789916992188 = 2.1510974168777466 + 100.0 * 8.456278800964355
Epoch 970, val loss: 2.1516966819763184
Epoch 980, training loss: 847.6307373046875 = 2.150753378868103 + 100.0 * 8.45479965209961
Epoch 980, val loss: 2.151371955871582
Epoch 990, training loss: 847.5361938476562 = 2.1504307985305786 + 100.0 * 8.453857421875
Epoch 990, val loss: 2.151038885116577
Epoch 1000, training loss: 847.8369750976562 = 2.150075912475586 + 100.0 * 8.456869125366211
Epoch 1000, val loss: 2.150719165802002
Epoch 1010, training loss: 847.5571899414062 = 2.1497397422790527 + 100.0 * 8.45407485961914
Epoch 1010, val loss: 2.150364398956299
Epoch 1020, training loss: 847.3331909179688 = 2.149414539337158 + 100.0 * 8.451837539672852
Epoch 1020, val loss: 2.1500344276428223
Epoch 1030, training loss: 847.1802978515625 = 2.149078607559204 + 100.0 * 8.450311660766602
Epoch 1030, val loss: 2.1497161388397217
Epoch 1040, training loss: 847.1394653320312 = 2.1487631797790527 + 100.0 * 8.449907302856445
Epoch 1040, val loss: 2.1493911743164062
Epoch 1050, training loss: 847.1411743164062 = 2.1484427452087402 + 100.0 * 8.44992733001709
Epoch 1050, val loss: 2.1490626335144043
Epoch 1060, training loss: 847.1115112304688 = 2.14807391166687 + 100.0 * 8.449634552001953
Epoch 1060, val loss: 2.1487326622009277
Epoch 1070, training loss: 846.8764038085938 = 2.1477692127227783 + 100.0 * 8.447286605834961
Epoch 1070, val loss: 2.148409843444824
Epoch 1080, training loss: 846.7708129882812 = 2.147445797920227 + 100.0 * 8.446233749389648
Epoch 1080, val loss: 2.1480965614318848
Epoch 1090, training loss: 846.7061157226562 = 2.1471227407455444 + 100.0 * 8.445590019226074
Epoch 1090, val loss: 2.147784948348999
Epoch 1100, training loss: 846.8370361328125 = 2.1467857360839844 + 100.0 * 8.44690227508545
Epoch 1100, val loss: 2.147480010986328
Epoch 1110, training loss: 846.6051635742188 = 2.146499991416931 + 100.0 * 8.444586753845215
Epoch 1110, val loss: 2.147128105163574
Epoch 1120, training loss: 846.5485229492188 = 2.1461340188980103 + 100.0 * 8.444024085998535
Epoch 1120, val loss: 2.146829128265381
Epoch 1130, training loss: 846.46337890625 = 2.145849823951721 + 100.0 * 8.443175315856934
Epoch 1130, val loss: 2.1465060710906982
Epoch 1140, training loss: 846.3865356445312 = 2.1455157995224 + 100.0 * 8.442410469055176
Epoch 1140, val loss: 2.1462063789367676
Epoch 1150, training loss: 846.715576171875 = 2.1451953649520874 + 100.0 * 8.445703506469727
Epoch 1150, val loss: 2.145899772644043
Epoch 1160, training loss: 846.3509521484375 = 2.1448960304260254 + 100.0 * 8.442060470581055
Epoch 1160, val loss: 2.1455676555633545
Epoch 1170, training loss: 846.205322265625 = 2.1445647478103638 + 100.0 * 8.440607070922852
Epoch 1170, val loss: 2.1452550888061523
Epoch 1180, training loss: 846.2384643554688 = 2.144257068634033 + 100.0 * 8.44094181060791
Epoch 1180, val loss: 2.1449522972106934
Epoch 1190, training loss: 846.2689208984375 = 2.1439433097839355 + 100.0 * 8.44124984741211
Epoch 1190, val loss: 2.144637107849121
Epoch 1200, training loss: 846.0988159179688 = 2.143631935119629 + 100.0 * 8.439552307128906
Epoch 1200, val loss: 2.144327163696289
Epoch 1210, training loss: 846.0221557617188 = 2.1433067321777344 + 100.0 * 8.438788414001465
Epoch 1210, val loss: 2.144026517868042
Epoch 1220, training loss: 846.1664428710938 = 2.142996311187744 + 100.0 * 8.440234184265137
Epoch 1220, val loss: 2.1437220573425293
Epoch 1230, training loss: 845.973876953125 = 2.1427013874053955 + 100.0 * 8.438311576843262
Epoch 1230, val loss: 2.1434006690979004
Epoch 1240, training loss: 845.8665771484375 = 2.1423927545547485 + 100.0 * 8.437241554260254
Epoch 1240, val loss: 2.143099784851074
Epoch 1250, training loss: 845.862548828125 = 2.142100691795349 + 100.0 * 8.437204360961914
Epoch 1250, val loss: 2.142789840698242
Epoch 1260, training loss: 845.9530029296875 = 2.141801595687866 + 100.0 * 8.438112258911133
Epoch 1260, val loss: 2.142486572265625
Epoch 1270, training loss: 845.7694091796875 = 2.1414566040039062 + 100.0 * 8.436279296875
Epoch 1270, val loss: 2.142188549041748
Epoch 1280, training loss: 845.651611328125 = 2.1411712169647217 + 100.0 * 8.435104370117188
Epoch 1280, val loss: 2.1418797969818115
Epoch 1290, training loss: 845.6356811523438 = 2.140879511833191 + 100.0 * 8.434947967529297
Epoch 1290, val loss: 2.1415810585021973
Epoch 1300, training loss: 845.7581176757812 = 2.1405757665634155 + 100.0 * 8.436175346374512
Epoch 1300, val loss: 2.141286611557007
Epoch 1310, training loss: 845.678955078125 = 2.1402307748794556 + 100.0 * 8.435386657714844
Epoch 1310, val loss: 2.1409811973571777
Epoch 1320, training loss: 845.6296997070312 = 2.1399385929107666 + 100.0 * 8.434897422790527
Epoch 1320, val loss: 2.1406750679016113
Epoch 1330, training loss: 845.500244140625 = 2.1396437883377075 + 100.0 * 8.433606147766113
Epoch 1330, val loss: 2.140368938446045
Epoch 1340, training loss: 845.4376831054688 = 2.1393327713012695 + 100.0 * 8.4329833984375
Epoch 1340, val loss: 2.140078067779541
Epoch 1350, training loss: 845.5092163085938 = 2.139031171798706 + 100.0 * 8.433701515197754
Epoch 1350, val loss: 2.1397924423217773
Epoch 1360, training loss: 845.4661865234375 = 2.1387308835983276 + 100.0 * 8.433274269104004
Epoch 1360, val loss: 2.1394805908203125
Epoch 1370, training loss: 845.3695068359375 = 2.1384453773498535 + 100.0 * 8.432311058044434
Epoch 1370, val loss: 2.1391801834106445
Epoch 1380, training loss: 845.2991333007812 = 2.1381407976150513 + 100.0 * 8.431610107421875
Epoch 1380, val loss: 2.1388916969299316
Epoch 1390, training loss: 845.2677001953125 = 2.137857437133789 + 100.0 * 8.43129825592041
Epoch 1390, val loss: 2.1386008262634277
Epoch 1400, training loss: 845.2775268554688 = 2.1375675201416016 + 100.0 * 8.43139934539795
Epoch 1400, val loss: 2.1383113861083984
Epoch 1410, training loss: 845.4342041015625 = 2.137270450592041 + 100.0 * 8.432969093322754
Epoch 1410, val loss: 2.1380186080932617
Epoch 1420, training loss: 845.26708984375 = 2.136936068534851 + 100.0 * 8.43130111694336
Epoch 1420, val loss: 2.1377322673797607
Epoch 1430, training loss: 845.3417358398438 = 2.1366446018218994 + 100.0 * 8.432050704956055
Epoch 1430, val loss: 2.137444019317627
Epoch 1440, training loss: 845.1115112304688 = 2.136382818222046 + 100.0 * 8.4297513961792
Epoch 1440, val loss: 2.1371288299560547
Epoch 1450, training loss: 845.0823364257812 = 2.136084198951721 + 100.0 * 8.429462432861328
Epoch 1450, val loss: 2.1368579864501953
Epoch 1460, training loss: 845.0309448242188 = 2.13580322265625 + 100.0 * 8.428951263427734
Epoch 1460, val loss: 2.1365764141082764
Epoch 1470, training loss: 845.0437622070312 = 2.1355109214782715 + 100.0 * 8.429082870483398
Epoch 1470, val loss: 2.1363000869750977
Epoch 1480, training loss: 845.252197265625 = 2.1352343559265137 + 100.0 * 8.431169509887695
Epoch 1480, val loss: 2.136007785797119
Epoch 1490, training loss: 845.2492065429688 = 2.1349525451660156 + 100.0 * 8.431142807006836
Epoch 1490, val loss: 2.1357216835021973
Epoch 1500, training loss: 845.0025024414062 = 2.1346681118011475 + 100.0 * 8.428678512573242
Epoch 1500, val loss: 2.1354289054870605
Epoch 1510, training loss: 844.9100952148438 = 2.13435435295105 + 100.0 * 8.427757263183594
Epoch 1510, val loss: 2.1351590156555176
Epoch 1520, training loss: 844.8591918945312 = 2.134085536003113 + 100.0 * 8.427250862121582
Epoch 1520, val loss: 2.1348910331726074
Epoch 1530, training loss: 844.8656616210938 = 2.133812427520752 + 100.0 * 8.427318572998047
Epoch 1530, val loss: 2.1346206665039062
Epoch 1540, training loss: 845.0137939453125 = 2.1335160732269287 + 100.0 * 8.428802490234375
Epoch 1540, val loss: 2.1343531608581543
Epoch 1550, training loss: 844.8988647460938 = 2.133260130882263 + 100.0 * 8.427656173706055
Epoch 1550, val loss: 2.1340575218200684
Epoch 1560, training loss: 844.7341918945312 = 2.132977843284607 + 100.0 * 8.42601203918457
Epoch 1560, val loss: 2.133796215057373
Epoch 1570, training loss: 844.6974487304688 = 2.1327141523361206 + 100.0 * 8.425647735595703
Epoch 1570, val loss: 2.133525848388672
Epoch 1580, training loss: 844.7260131835938 = 2.132454514503479 + 100.0 * 8.425935745239258
Epoch 1580, val loss: 2.1332616806030273
Epoch 1590, training loss: 844.91552734375 = 2.1321834325790405 + 100.0 * 8.427833557128906
Epoch 1590, val loss: 2.1329939365386963
Epoch 1600, training loss: 845.0498657226562 = 2.131929039955139 + 100.0 * 8.429179191589355
Epoch 1600, val loss: 2.132716178894043
Epoch 1610, training loss: 844.7039794921875 = 2.131615400314331 + 100.0 * 8.425724029541016
Epoch 1610, val loss: 2.1324543952941895
Epoch 1620, training loss: 844.5521850585938 = 2.131360411643982 + 100.0 * 8.424208641052246
Epoch 1620, val loss: 2.132197380065918
Epoch 1630, training loss: 844.5035400390625 = 2.131109118461609 + 100.0 * 8.423724174499512
Epoch 1630, val loss: 2.13193678855896
Epoch 1640, training loss: 844.4872436523438 = 2.1308449506759644 + 100.0 * 8.423563957214355
Epoch 1640, val loss: 2.1316895484924316
Epoch 1650, training loss: 844.8212280273438 = 2.13058340549469 + 100.0 * 8.42690658569336
Epoch 1650, val loss: 2.131431818008423
Epoch 1660, training loss: 844.5523071289062 = 2.130335807800293 + 100.0 * 8.424220085144043
Epoch 1660, val loss: 2.131167411804199
Epoch 1670, training loss: 844.4776611328125 = 2.1300694942474365 + 100.0 * 8.423476219177246
Epoch 1670, val loss: 2.1309099197387695
Epoch 1680, training loss: 844.8189697265625 = 2.1298452615737915 + 100.0 * 8.426891326904297
Epoch 1680, val loss: 2.1306471824645996
Epoch 1690, training loss: 844.4497680664062 = 2.129531145095825 + 100.0 * 8.423202514648438
Epoch 1690, val loss: 2.130398988723755
Epoch 1700, training loss: 844.3369750976562 = 2.129290461540222 + 100.0 * 8.422077178955078
Epoch 1700, val loss: 2.1301510334014893
Epoch 1710, training loss: 844.2722778320312 = 2.1290459632873535 + 100.0 * 8.421432495117188
Epoch 1710, val loss: 2.1299140453338623
Epoch 1720, training loss: 844.2294311523438 = 2.128806948661804 + 100.0 * 8.421006202697754
Epoch 1720, val loss: 2.129669427871704
Epoch 1730, training loss: 844.2174072265625 = 2.128567337989807 + 100.0 * 8.420888900756836
Epoch 1730, val loss: 2.1294312477111816
Epoch 1740, training loss: 844.5494384765625 = 2.1283305883407593 + 100.0 * 8.424210548400879
Epoch 1740, val loss: 2.1291866302490234
Epoch 1750, training loss: 844.4072875976562 = 2.1280598640441895 + 100.0 * 8.422792434692383
Epoch 1750, val loss: 2.128936290740967
Epoch 1760, training loss: 844.2955932617188 = 2.1278237104415894 + 100.0 * 8.421677589416504
Epoch 1760, val loss: 2.128683090209961
Epoch 1770, training loss: 844.0870361328125 = 2.127563238143921 + 100.0 * 8.419594764709473
Epoch 1770, val loss: 2.128450393676758
Epoch 1780, training loss: 844.0760498046875 = 2.127319812774658 + 100.0 * 8.419486999511719
Epoch 1780, val loss: 2.1282238960266113
Epoch 1790, training loss: 844.2476196289062 = 2.1270787715911865 + 100.0 * 8.421205520629883
Epoch 1790, val loss: 2.1279940605163574
Epoch 1800, training loss: 844.0093383789062 = 2.1268447637557983 + 100.0 * 8.418825149536133
Epoch 1800, val loss: 2.127741813659668
Epoch 1810, training loss: 844.02783203125 = 2.126619815826416 + 100.0 * 8.419012069702148
Epoch 1810, val loss: 2.127507209777832
Epoch 1820, training loss: 843.9328002929688 = 2.1263843774795532 + 100.0 * 8.41806411743164
Epoch 1820, val loss: 2.1272811889648438
Epoch 1830, training loss: 844.0283203125 = 2.1261630058288574 + 100.0 * 8.419021606445312
Epoch 1830, val loss: 2.1270484924316406
Epoch 1840, training loss: 843.9921264648438 = 2.1259422302246094 + 100.0 * 8.418662071228027
Epoch 1840, val loss: 2.126814126968384
Epoch 1850, training loss: 843.9568481445312 = 2.125681161880493 + 100.0 * 8.418312072753906
Epoch 1850, val loss: 2.126586437225342
Epoch 1860, training loss: 843.8798828125 = 2.125446319580078 + 100.0 * 8.4175443649292
Epoch 1860, val loss: 2.1263608932495117
Epoch 1870, training loss: 843.9334106445312 = 2.1252121925354004 + 100.0 * 8.418082237243652
Epoch 1870, val loss: 2.1261377334594727
Epoch 1880, training loss: 843.78076171875 = 2.124983549118042 + 100.0 * 8.416557312011719
Epoch 1880, val loss: 2.1259124279022217
Epoch 1890, training loss: 843.7931518554688 = 2.1247512102127075 + 100.0 * 8.4166841506958
Epoch 1890, val loss: 2.125694751739502
Epoch 1900, training loss: 843.93017578125 = 2.124527931213379 + 100.0 * 8.41805648803711
Epoch 1900, val loss: 2.1254775524139404
Epoch 1910, training loss: 843.7796020507812 = 2.124294638633728 + 100.0 * 8.416553497314453
Epoch 1910, val loss: 2.1252379417419434
Epoch 1920, training loss: 843.7146606445312 = 2.1240822076797485 + 100.0 * 8.415905952453613
Epoch 1920, val loss: 2.1250104904174805
Epoch 1930, training loss: 843.686279296875 = 2.1238598823547363 + 100.0 * 8.415624618530273
Epoch 1930, val loss: 2.1247994899749756
Epoch 1940, training loss: 843.7147216796875 = 2.123647689819336 + 100.0 * 8.415910720825195
Epoch 1940, val loss: 2.1245827674865723
Epoch 1950, training loss: 843.627197265625 = 2.1234116554260254 + 100.0 * 8.415038108825684
Epoch 1950, val loss: 2.1243648529052734
Epoch 1960, training loss: 843.651611328125 = 2.1231882572174072 + 100.0 * 8.415284156799316
Epoch 1960, val loss: 2.1241562366485596
Epoch 1970, training loss: 843.788818359375 = 2.122957706451416 + 100.0 * 8.416658401489258
Epoch 1970, val loss: 2.123934507369995
Epoch 1980, training loss: 843.5833740234375 = 2.122765898704529 + 100.0 * 8.414606094360352
Epoch 1980, val loss: 2.123704195022583
Epoch 1990, training loss: 843.4907836914062 = 2.122535228729248 + 100.0 * 8.413681983947754
Epoch 1990, val loss: 2.123501777648926
Epoch 2000, training loss: 843.6097412109375 = 2.122312545776367 + 100.0 * 8.414874076843262
Epoch 2000, val loss: 2.1232998371124268
Epoch 2010, training loss: 843.468017578125 = 2.1221030950546265 + 100.0 * 8.413458824157715
Epoch 2010, val loss: 2.123074531555176
Epoch 2020, training loss: 843.5460815429688 = 2.12189781665802 + 100.0 * 8.414241790771484
Epoch 2020, val loss: 2.1228652000427246
Epoch 2030, training loss: 843.4368896484375 = 2.1216639280319214 + 100.0 * 8.413152694702148
Epoch 2030, val loss: 2.122650623321533
Epoch 2040, training loss: 843.3623657226562 = 2.1214730739593506 + 100.0 * 8.412408828735352
Epoch 2040, val loss: 2.122439384460449
Epoch 2050, training loss: 843.3370971679688 = 2.121267318725586 + 100.0 * 8.412158012390137
Epoch 2050, val loss: 2.1222381591796875
Epoch 2060, training loss: 843.3611450195312 = 2.1210609674453735 + 100.0 * 8.41240119934082
Epoch 2060, val loss: 2.12203311920166
Epoch 2070, training loss: 843.8187255859375 = 2.120876908302307 + 100.0 * 8.41697883605957
Epoch 2070, val loss: 2.121807336807251
Epoch 2080, training loss: 843.521484375 = 2.120613217353821 + 100.0 * 8.414009094238281
Epoch 2080, val loss: 2.1216111183166504
Epoch 2090, training loss: 843.330322265625 = 2.1204124689102173 + 100.0 * 8.41209888458252
Epoch 2090, val loss: 2.1213908195495605
Epoch 2100, training loss: 843.2269897460938 = 2.1202062368392944 + 100.0 * 8.411067962646484
Epoch 2100, val loss: 2.12119722366333
Epoch 2110, training loss: 843.1820678710938 = 2.1200140714645386 + 100.0 * 8.41062068939209
Epoch 2110, val loss: 2.1210029125213623
Epoch 2120, training loss: 843.1799926757812 = 2.119807243347168 + 100.0 * 8.410601615905762
Epoch 2120, val loss: 2.120814323425293
Epoch 2130, training loss: 843.697265625 = 2.1195961236953735 + 100.0 * 8.415777206420898
Epoch 2130, val loss: 2.1206154823303223
Epoch 2140, training loss: 843.2726440429688 = 2.1194030046463013 + 100.0 * 8.411532402038574
Epoch 2140, val loss: 2.120394229888916
Epoch 2150, training loss: 843.1309204101562 = 2.1191866397857666 + 100.0 * 8.410117149353027
Epoch 2150, val loss: 2.120190382003784
Epoch 2160, training loss: 843.086669921875 = 2.1189898252487183 + 100.0 * 8.409676551818848
Epoch 2160, val loss: 2.1199984550476074
Epoch 2170, training loss: 843.0584106445312 = 2.1188021898269653 + 100.0 * 8.409396171569824
Epoch 2170, val loss: 2.119809627532959
Epoch 2180, training loss: 843.1455688476562 = 2.1186070442199707 + 100.0 * 8.410269737243652
Epoch 2180, val loss: 2.1196160316467285
Epoch 2190, training loss: 843.1864624023438 = 2.1184180974960327 + 100.0 * 8.410680770874023
Epoch 2190, val loss: 2.1194143295288086
Epoch 2200, training loss: 843.0277099609375 = 2.118204951286316 + 100.0 * 8.40909481048584
Epoch 2200, val loss: 2.119203805923462
Epoch 2210, training loss: 843.0376586914062 = 2.117998957633972 + 100.0 * 8.409196853637695
Epoch 2210, val loss: 2.1190176010131836
Epoch 2220, training loss: 843.0568237304688 = 2.117825150489807 + 100.0 * 8.409390449523926
Epoch 2220, val loss: 2.118823528289795
Epoch 2230, training loss: 843.1221923828125 = 2.117628574371338 + 100.0 * 8.410045623779297
Epoch 2230, val loss: 2.1186299324035645
Epoch 2240, training loss: 842.9716186523438 = 2.117401599884033 + 100.0 * 8.408541679382324
Epoch 2240, val loss: 2.1184544563293457
Epoch 2250, training loss: 842.9612426757812 = 2.1172163486480713 + 100.0 * 8.408440589904785
Epoch 2250, val loss: 2.1182620525360107
Epoch 2260, training loss: 842.882568359375 = 2.117045521736145 + 100.0 * 8.407654762268066
Epoch 2260, val loss: 2.118070602416992
Epoch 2270, training loss: 842.910888671875 = 2.1168686151504517 + 100.0 * 8.407939910888672
Epoch 2270, val loss: 2.1178863048553467
Epoch 2280, training loss: 843.2506713867188 = 2.1166841983795166 + 100.0 * 8.41133975982666
Epoch 2280, val loss: 2.117697238922119
Epoch 2290, training loss: 842.8931884765625 = 2.1164456605911255 + 100.0 * 8.407767295837402
Epoch 2290, val loss: 2.1175105571746826
Epoch 2300, training loss: 842.8318481445312 = 2.116263508796692 + 100.0 * 8.407155990600586
Epoch 2300, val loss: 2.1173105239868164
Epoch 2310, training loss: 842.806884765625 = 2.1160889863967896 + 100.0 * 8.40690803527832
Epoch 2310, val loss: 2.1171326637268066
Epoch 2320, training loss: 842.7727661132812 = 2.1159093379974365 + 100.0 * 8.40656852722168
Epoch 2320, val loss: 2.1169567108154297
Epoch 2330, training loss: 842.8329467773438 = 2.11574125289917 + 100.0 * 8.407172203063965
Epoch 2330, val loss: 2.1167798042297363
Epoch 2340, training loss: 842.9217529296875 = 2.115549683570862 + 100.0 * 8.408061981201172
Epoch 2340, val loss: 2.1166000366210938
Epoch 2350, training loss: 842.8489990234375 = 2.1153347492218018 + 100.0 * 8.407336235046387
Epoch 2350, val loss: 2.1164207458496094
Epoch 2360, training loss: 842.7937622070312 = 2.115164041519165 + 100.0 * 8.40678596496582
Epoch 2360, val loss: 2.1162400245666504
Epoch 2370, training loss: 842.9140014648438 = 2.114956021308899 + 100.0 * 8.407990455627441
Epoch 2370, val loss: 2.116065502166748
Epoch 2380, training loss: 842.6643676757812 = 2.114801287651062 + 100.0 * 8.405495643615723
Epoch 2380, val loss: 2.115865468978882
Epoch 2390, training loss: 842.672607421875 = 2.114630341529846 + 100.0 * 8.405579566955566
Epoch 2390, val loss: 2.115689754486084
Epoch 2400, training loss: 842.635498046875 = 2.1144495010375977 + 100.0 * 8.405210494995117
Epoch 2400, val loss: 2.1155240535736084
Epoch 2410, training loss: 842.7042846679688 = 2.114284038543701 + 100.0 * 8.405900001525879
Epoch 2410, val loss: 2.1153509616851807
Epoch 2420, training loss: 842.9494018554688 = 2.11411190032959 + 100.0 * 8.408352851867676
Epoch 2420, val loss: 2.1151676177978516
Epoch 2430, training loss: 842.6568603515625 = 2.113905429840088 + 100.0 * 8.40542984008789
Epoch 2430, val loss: 2.1149933338165283
Epoch 2440, training loss: 842.59326171875 = 2.1137269735336304 + 100.0 * 8.40479564666748
Epoch 2440, val loss: 2.114823818206787
Epoch 2450, training loss: 842.6527709960938 = 2.113548994064331 + 100.0 * 8.40539264678955
Epoch 2450, val loss: 2.1146631240844727
Epoch 2460, training loss: 842.7803955078125 = 2.1133710145950317 + 100.0 * 8.406670570373535
Epoch 2460, val loss: 2.1144728660583496
Epoch 2470, training loss: 842.563720703125 = 2.1131956577301025 + 100.0 * 8.404504776000977
Epoch 2470, val loss: 2.1142921447753906
Epoch 2480, training loss: 842.498291015625 = 2.1130300760269165 + 100.0 * 8.403852462768555
Epoch 2480, val loss: 2.1141228675842285
Epoch 2490, training loss: 842.4683227539062 = 2.112860083580017 + 100.0 * 8.403554916381836
Epoch 2490, val loss: 2.1139659881591797
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.4013043478260869
0.8123596319640659
The final CL Acc:0.39802, 0.00233, The final GNN Acc:0.81101, 0.00104
raw graph: torch.Size([2, 88648])
add edge: torch.Size([2, 111034])
remove edge: torch.Size([2, 66804])
updated graph: torch.Size([2, 89190])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 1060.428955078125 = 2.20495343208313 + 100.0 * 10.582240104675293
Epoch 0, val loss: 2.203300952911377
Epoch 10, training loss: 1060.374755859375 = 2.1945663690567017 + 100.0 * 10.581801414489746
Epoch 10, val loss: 2.1938111782073975
Epoch 20, training loss: 1060.19970703125 = 2.192996025085449 + 100.0 * 10.580066680908203
Epoch 20, val loss: 2.192044734954834
Epoch 30, training loss: 1059.450439453125 = 2.192291259765625 + 100.0 * 10.572582244873047
Epoch 30, val loss: 2.1912810802459717
Epoch 40, training loss: 1056.5655517578125 = 2.1919878721237183 + 100.0 * 10.54373550415039
Epoch 40, val loss: 2.1910128593444824
Epoch 50, training loss: 1048.060546875 = 2.1917461156845093 + 100.0 * 10.458687782287598
Epoch 50, val loss: 2.1908135414123535
Epoch 60, training loss: 1028.760986328125 = 2.191077947616577 + 100.0 * 10.26569938659668
Epoch 60, val loss: 2.1902012825012207
Epoch 70, training loss: 992.0057373046875 = 2.1902999877929688 + 100.0 * 9.898154258728027
Epoch 70, val loss: 2.189500331878662
Epoch 80, training loss: 951.5440063476562 = 2.1891469955444336 + 100.0 * 9.493548393249512
Epoch 80, val loss: 2.1882433891296387
Epoch 90, training loss: 932.8555908203125 = 2.1874920129776 + 100.0 * 9.306680679321289
Epoch 90, val loss: 2.186444044113159
Epoch 100, training loss: 921.1062622070312 = 2.1860350370407104 + 100.0 * 9.189202308654785
Epoch 100, val loss: 2.1850037574768066
Epoch 110, training loss: 915.3380737304688 = 2.1846102476119995 + 100.0 * 9.131534576416016
Epoch 110, val loss: 2.1835508346557617
Epoch 120, training loss: 909.9254150390625 = 2.1838250160217285 + 100.0 * 9.077415466308594
Epoch 120, val loss: 2.1827120780944824
Epoch 130, training loss: 905.4541015625 = 2.182633638381958 + 100.0 * 9.03271484375
Epoch 130, val loss: 2.181485652923584
Epoch 140, training loss: 902.6442260742188 = 2.1807053089141846 + 100.0 * 9.004634857177734
Epoch 140, val loss: 2.179640293121338
Epoch 150, training loss: 900.3193969726562 = 2.178925395011902 + 100.0 * 8.981404304504395
Epoch 150, val loss: 2.1779274940490723
Epoch 160, training loss: 897.3228149414062 = 2.1774556636810303 + 100.0 * 8.951454162597656
Epoch 160, val loss: 2.176503896713257
Epoch 170, training loss: 893.3499145507812 = 2.1763408184051514 + 100.0 * 8.911735534667969
Epoch 170, val loss: 2.1754298210144043
Epoch 180, training loss: 888.7310791015625 = 2.175739049911499 + 100.0 * 8.865553855895996
Epoch 180, val loss: 2.1748180389404297
Epoch 190, training loss: 884.3897094726562 = 2.1755547523498535 + 100.0 * 8.822141647338867
Epoch 190, val loss: 2.174614429473877
Epoch 200, training loss: 880.2329711914062 = 2.1754955053329468 + 100.0 * 8.780574798583984
Epoch 200, val loss: 2.174528121948242
Epoch 210, training loss: 876.187255859375 = 2.1753766536712646 + 100.0 * 8.740118980407715
Epoch 210, val loss: 2.1744155883789062
Epoch 220, training loss: 873.306396484375 = 2.175038456916809 + 100.0 * 8.711313247680664
Epoch 220, val loss: 2.174067974090576
Epoch 230, training loss: 871.1500854492188 = 2.174562931060791 + 100.0 * 8.6897554397583
Epoch 230, val loss: 2.173574686050415
Epoch 240, training loss: 869.4305419921875 = 2.1741877794265747 + 100.0 * 8.672563552856445
Epoch 240, val loss: 2.173208713531494
Epoch 250, training loss: 867.9578857421875 = 2.173862934112549 + 100.0 * 8.65783977508545
Epoch 250, val loss: 2.172898769378662
Epoch 260, training loss: 866.3760986328125 = 2.1735626459121704 + 100.0 * 8.642024993896484
Epoch 260, val loss: 2.1725945472717285
Epoch 270, training loss: 864.7523803710938 = 2.1733354330062866 + 100.0 * 8.6257905960083
Epoch 270, val loss: 2.1723921298980713
Epoch 280, training loss: 863.5013427734375 = 2.1731127500534058 + 100.0 * 8.613282203674316
Epoch 280, val loss: 2.172205924987793
Epoch 290, training loss: 862.2146606445312 = 2.172842264175415 + 100.0 * 8.600418090820312
Epoch 290, val loss: 2.1719486713409424
Epoch 300, training loss: 861.10302734375 = 2.1725963354110718 + 100.0 * 8.589303970336914
Epoch 300, val loss: 2.171710968017578
Epoch 310, training loss: 859.96923828125 = 2.1723906993865967 + 100.0 * 8.57796859741211
Epoch 310, val loss: 2.1714887619018555
Epoch 320, training loss: 858.6405029296875 = 2.1722018718719482 + 100.0 * 8.564682960510254
Epoch 320, val loss: 2.1713008880615234
Epoch 330, training loss: 857.5032958984375 = 2.172047734260559 + 100.0 * 8.553312301635742
Epoch 330, val loss: 2.1711130142211914
Epoch 340, training loss: 856.5313720703125 = 2.1717984676361084 + 100.0 * 8.543595314025879
Epoch 340, val loss: 2.1708898544311523
Epoch 350, training loss: 855.533203125 = 2.1715738773345947 + 100.0 * 8.533616065979004
Epoch 350, val loss: 2.170616626739502
Epoch 360, training loss: 854.6566162109375 = 2.1712870597839355 + 100.0 * 8.524853706359863
Epoch 360, val loss: 2.1703529357910156
Epoch 370, training loss: 853.9526977539062 = 2.170969247817993 + 100.0 * 8.517817497253418
Epoch 370, val loss: 2.1700599193573
Epoch 380, training loss: 853.451904296875 = 2.1706454753875732 + 100.0 * 8.512812614440918
Epoch 380, val loss: 2.1696839332580566
Epoch 390, training loss: 852.8693237304688 = 2.1701951026916504 + 100.0 * 8.506991386413574
Epoch 390, val loss: 2.1693050861358643
Epoch 400, training loss: 852.332763671875 = 2.1697983741760254 + 100.0 * 8.501629829406738
Epoch 400, val loss: 2.1688849925994873
Epoch 410, training loss: 851.9564208984375 = 2.1693772077560425 + 100.0 * 8.497870445251465
Epoch 410, val loss: 2.168445587158203
Epoch 420, training loss: 851.4894409179688 = 2.1689373254776 + 100.0 * 8.493205070495605
Epoch 420, val loss: 2.168034553527832
Epoch 430, training loss: 851.0698852539062 = 2.1685125827789307 + 100.0 * 8.489013671875
Epoch 430, val loss: 2.167619466781616
Epoch 440, training loss: 850.9002075195312 = 2.16811740398407 + 100.0 * 8.487320899963379
Epoch 440, val loss: 2.167180299758911
Epoch 450, training loss: 850.4188232421875 = 2.1676641702651978 + 100.0 * 8.482511520385742
Epoch 450, val loss: 2.166759967803955
Epoch 460, training loss: 850.0221557617188 = 2.1672357320785522 + 100.0 * 8.478549003601074
Epoch 460, val loss: 2.1663706302642822
Epoch 470, training loss: 849.6542358398438 = 2.1668070554733276 + 100.0 * 8.474874496459961
Epoch 470, val loss: 2.1659669876098633
Epoch 480, training loss: 849.3162231445312 = 2.1664000749588013 + 100.0 * 8.471498489379883
Epoch 480, val loss: 2.1655678749084473
Epoch 490, training loss: 849.024658203125 = 2.165982961654663 + 100.0 * 8.468586921691895
Epoch 490, val loss: 2.165156841278076
Epoch 500, training loss: 848.66162109375 = 2.1655584573745728 + 100.0 * 8.464960098266602
Epoch 500, val loss: 2.164745807647705
Epoch 510, training loss: 848.3651733398438 = 2.165167450904846 + 100.0 * 8.461999893188477
Epoch 510, val loss: 2.1643600463867188
Epoch 520, training loss: 848.0546264648438 = 2.164772629737854 + 100.0 * 8.458898544311523
Epoch 520, val loss: 2.1639723777770996
Epoch 530, training loss: 848.1713256835938 = 2.164384365081787 + 100.0 * 8.46006965637207
Epoch 530, val loss: 2.1635818481445312
Epoch 540, training loss: 847.5318603515625 = 2.163967251777649 + 100.0 * 8.453679084777832
Epoch 540, val loss: 2.1631669998168945
Epoch 550, training loss: 847.26123046875 = 2.1635812520980835 + 100.0 * 8.450976371765137
Epoch 550, val loss: 2.1627769470214844
Epoch 560, training loss: 846.97900390625 = 2.1631821393966675 + 100.0 * 8.448158264160156
Epoch 560, val loss: 2.162391185760498
Epoch 570, training loss: 846.8812866210938 = 2.16281259059906 + 100.0 * 8.447184562683105
Epoch 570, val loss: 2.1619865894317627
Epoch 580, training loss: 846.555908203125 = 2.162359118461609 + 100.0 * 8.44393539428711
Epoch 580, val loss: 2.1616077423095703
Epoch 590, training loss: 846.3909301757812 = 2.1619848012924194 + 100.0 * 8.442289352416992
Epoch 590, val loss: 2.16121244430542
Epoch 600, training loss: 846.1314697265625 = 2.161567449569702 + 100.0 * 8.439699172973633
Epoch 600, val loss: 2.160829544067383
Epoch 610, training loss: 845.8563842773438 = 2.1611952781677246 + 100.0 * 8.436951637268066
Epoch 610, val loss: 2.160442352294922
Epoch 620, training loss: 845.659912109375 = 2.1608115434646606 + 100.0 * 8.434990882873535
Epoch 620, val loss: 2.1600558757781982
Epoch 630, training loss: 845.4714965820312 = 2.1604154109954834 + 100.0 * 8.433111190795898
Epoch 630, val loss: 2.1596763134002686
Epoch 640, training loss: 845.5260009765625 = 2.1600265502929688 + 100.0 * 8.433659553527832
Epoch 640, val loss: 2.159290313720703
Epoch 650, training loss: 845.29736328125 = 2.1596208810806274 + 100.0 * 8.431377410888672
Epoch 650, val loss: 2.158874988555908
Epoch 660, training loss: 845.0094604492188 = 2.159225344657898 + 100.0 * 8.428502082824707
Epoch 660, val loss: 2.1584925651550293
Epoch 670, training loss: 844.87353515625 = 2.1588287353515625 + 100.0 * 8.427146911621094
Epoch 670, val loss: 2.1581130027770996
Epoch 680, training loss: 844.729736328125 = 2.1584516763687134 + 100.0 * 8.425712585449219
Epoch 680, val loss: 2.1577329635620117
Epoch 690, training loss: 844.595703125 = 2.1580750942230225 + 100.0 * 8.424376487731934
Epoch 690, val loss: 2.1573562622070312
Epoch 700, training loss: 844.7573852539062 = 2.1577054262161255 + 100.0 * 8.425996780395508
Epoch 700, val loss: 2.1569786071777344
Epoch 710, training loss: 844.3699340820312 = 2.1572824716567993 + 100.0 * 8.422126770019531
Epoch 710, val loss: 2.156601905822754
Epoch 720, training loss: 844.3267822265625 = 2.1569091081619263 + 100.0 * 8.421698570251465
Epoch 720, val loss: 2.1562368869781494
Epoch 730, training loss: 844.107177734375 = 2.1565548181533813 + 100.0 * 8.419506072998047
Epoch 730, val loss: 2.1558594703674316
Epoch 740, training loss: 843.980712890625 = 2.1562029123306274 + 100.0 * 8.418245315551758
Epoch 740, val loss: 2.1554980278015137
Epoch 750, training loss: 843.9071044921875 = 2.155855655670166 + 100.0 * 8.417511940002441
Epoch 750, val loss: 2.1551308631896973
Epoch 760, training loss: 843.7488403320312 = 2.1554561853408813 + 100.0 * 8.415933609008789
Epoch 760, val loss: 2.154804229736328
Epoch 770, training loss: 843.7133178710938 = 2.1551328897476196 + 100.0 * 8.415581703186035
Epoch 770, val loss: 2.1544270515441895
Epoch 780, training loss: 843.4944458007812 = 2.154785394668579 + 100.0 * 8.413396835327148
Epoch 780, val loss: 2.154086112976074
Epoch 790, training loss: 843.375 = 2.154427647590637 + 100.0 * 8.412205696105957
Epoch 790, val loss: 2.1537671089172363
Epoch 800, training loss: 843.375244140625 = 2.1540884971618652 + 100.0 * 8.412211418151855
Epoch 800, val loss: 2.153439521789551
Epoch 810, training loss: 843.1487426757812 = 2.1537537574768066 + 100.0 * 8.409950256347656
Epoch 810, val loss: 2.1531081199645996
Epoch 820, training loss: 843.0135498046875 = 2.1534340381622314 + 100.0 * 8.408600807189941
Epoch 820, val loss: 2.1527953147888184
Epoch 830, training loss: 842.8539428710938 = 2.153107166290283 + 100.0 * 8.407008171081543
Epoch 830, val loss: 2.1524806022644043
Epoch 840, training loss: 842.7242431640625 = 2.152822256088257 + 100.0 * 8.40571403503418
Epoch 840, val loss: 2.152161121368408
Epoch 850, training loss: 842.9796752929688 = 2.152512311935425 + 100.0 * 8.408271789550781
Epoch 850, val loss: 2.151857376098633
Epoch 860, training loss: 842.5377197265625 = 2.1521397829055786 + 100.0 * 8.403855323791504
Epoch 860, val loss: 2.1515395641326904
Epoch 870, training loss: 842.36572265625 = 2.151852011680603 + 100.0 * 8.402138710021973
Epoch 870, val loss: 2.1512298583984375
Epoch 880, training loss: 842.2353515625 = 2.1515363454818726 + 100.0 * 8.400837898254395
Epoch 880, val loss: 2.1509313583374023
Epoch 890, training loss: 842.0885620117188 = 2.151233196258545 + 100.0 * 8.399373054504395
Epoch 890, val loss: 2.150635004043579
Epoch 900, training loss: 841.9859008789062 = 2.15092670917511 + 100.0 * 8.39834976196289
Epoch 900, val loss: 2.1503357887268066
Epoch 910, training loss: 842.7064208984375 = 2.1506205797195435 + 100.0 * 8.405557632446289
Epoch 910, val loss: 2.1500015258789062
Epoch 920, training loss: 841.9364013671875 = 2.150247097015381 + 100.0 * 8.39786148071289
Epoch 920, val loss: 2.1496806144714355
Epoch 930, training loss: 841.7592163085938 = 2.1499441862106323 + 100.0 * 8.396092414855957
Epoch 930, val loss: 2.1493682861328125
Epoch 940, training loss: 841.6328735351562 = 2.149611473083496 + 100.0 * 8.394832611083984
Epoch 940, val loss: 2.149057149887085
Epoch 950, training loss: 841.5318603515625 = 2.149294376373291 + 100.0 * 8.39382553100586
Epoch 950, val loss: 2.14874267578125
Epoch 960, training loss: 841.4932250976562 = 2.1489579677581787 + 100.0 * 8.393442153930664
Epoch 960, val loss: 2.148433208465576
Epoch 970, training loss: 841.386962890625 = 2.1486127376556396 + 100.0 * 8.392383575439453
Epoch 970, val loss: 2.1480793952941895
Epoch 980, training loss: 841.3492431640625 = 2.148269295692444 + 100.0 * 8.392009735107422
Epoch 980, val loss: 2.147756576538086
Epoch 990, training loss: 841.2682495117188 = 2.1479365825653076 + 100.0 * 8.391202926635742
Epoch 990, val loss: 2.147433280944824
Epoch 1000, training loss: 841.17529296875 = 2.1476060152053833 + 100.0 * 8.390276908874512
Epoch 1000, val loss: 2.1471099853515625
Epoch 1010, training loss: 841.1124877929688 = 2.147276282310486 + 100.0 * 8.389652252197266
Epoch 1010, val loss: 2.1467857360839844
Epoch 1020, training loss: 841.2207641601562 = 2.146907687187195 + 100.0 * 8.390738487243652
Epoch 1020, val loss: 2.1464786529541016
Epoch 1030, training loss: 841.2127075195312 = 2.1466249227523804 + 100.0 * 8.390661239624023
Epoch 1030, val loss: 2.14607310295105
Epoch 1040, training loss: 840.9354248046875 = 2.146247148513794 + 100.0 * 8.38789176940918
Epoch 1040, val loss: 2.1457643508911133
Epoch 1050, training loss: 840.8875732421875 = 2.1459027528762817 + 100.0 * 8.38741683959961
Epoch 1050, val loss: 2.1454529762268066
Epoch 1060, training loss: 840.834716796875 = 2.145574688911438 + 100.0 * 8.38689136505127
Epoch 1060, val loss: 2.1451358795166016
Epoch 1070, training loss: 840.838623046875 = 2.1452512741088867 + 100.0 * 8.386933326721191
Epoch 1070, val loss: 2.144813299179077
Epoch 1080, training loss: 840.7264404296875 = 2.144918918609619 + 100.0 * 8.385815620422363
Epoch 1080, val loss: 2.1444668769836426
Epoch 1090, training loss: 840.7235717773438 = 2.14458429813385 + 100.0 * 8.38578987121582
Epoch 1090, val loss: 2.144148826599121
Epoch 1100, training loss: 840.615478515625 = 2.144260287284851 + 100.0 * 8.384712219238281
Epoch 1100, val loss: 2.1438376903533936
Epoch 1110, training loss: 840.5648193359375 = 2.1439472436904907 + 100.0 * 8.384208679199219
Epoch 1110, val loss: 2.143522262573242
Epoch 1120, training loss: 840.5451049804688 = 2.1436184644699097 + 100.0 * 8.384015083312988
Epoch 1120, val loss: 2.1432175636291504
Epoch 1130, training loss: 840.6236572265625 = 2.1432886123657227 + 100.0 * 8.384803771972656
Epoch 1130, val loss: 2.1428873538970947
Epoch 1140, training loss: 840.5062866210938 = 2.1429649591445923 + 100.0 * 8.383633613586426
Epoch 1140, val loss: 2.1425726413726807
Epoch 1150, training loss: 840.3627319335938 = 2.142658829689026 + 100.0 * 8.382201194763184
Epoch 1150, val loss: 2.1422603130340576
Epoch 1160, training loss: 840.3289794921875 = 2.1423513889312744 + 100.0 * 8.381866455078125
Epoch 1160, val loss: 2.1419529914855957
Epoch 1170, training loss: 840.26123046875 = 2.1420446634292603 + 100.0 * 8.381192207336426
Epoch 1170, val loss: 2.1416573524475098
Epoch 1180, training loss: 840.21240234375 = 2.141741394996643 + 100.0 * 8.380706787109375
Epoch 1180, val loss: 2.1413583755493164
Epoch 1190, training loss: 840.319091796875 = 2.141451835632324 + 100.0 * 8.381775856018066
Epoch 1190, val loss: 2.141049861907959
Epoch 1200, training loss: 840.380859375 = 2.141115188598633 + 100.0 * 8.382397651672363
Epoch 1200, val loss: 2.140742778778076
Epoch 1210, training loss: 840.2612915039062 = 2.1407907009124756 + 100.0 * 8.381204605102539
Epoch 1210, val loss: 2.1404471397399902
Epoch 1220, training loss: 840.056396484375 = 2.140509605407715 + 100.0 * 8.379158973693848
Epoch 1220, val loss: 2.1401400566101074
Epoch 1230, training loss: 840.002197265625 = 2.1402058601379395 + 100.0 * 8.378620147705078
Epoch 1230, val loss: 2.139857769012451
Epoch 1240, training loss: 839.928955078125 = 2.1399213075637817 + 100.0 * 8.377890586853027
Epoch 1240, val loss: 2.139568328857422
Epoch 1250, training loss: 839.92041015625 = 2.1396297216415405 + 100.0 * 8.3778076171875
Epoch 1250, val loss: 2.1392855644226074
Epoch 1260, training loss: 840.0878295898438 = 2.1393251419067383 + 100.0 * 8.379485130310059
Epoch 1260, val loss: 2.138993501663208
Epoch 1270, training loss: 839.8759155273438 = 2.139034867286682 + 100.0 * 8.377368927001953
Epoch 1270, val loss: 2.138702154159546
Epoch 1280, training loss: 839.7716064453125 = 2.138740658760071 + 100.0 * 8.376328468322754
Epoch 1280, val loss: 2.138425350189209
Epoch 1290, training loss: 839.7288208007812 = 2.138447642326355 + 100.0 * 8.375904083251953
Epoch 1290, val loss: 2.1381492614746094
Epoch 1300, training loss: 839.9312133789062 = 2.138108015060425 + 100.0 * 8.377930641174316
Epoch 1300, val loss: 2.1378817558288574
Epoch 1310, training loss: 839.7201538085938 = 2.137883424758911 + 100.0 * 8.375823020935059
Epoch 1310, val loss: 2.1375603675842285
Epoch 1320, training loss: 839.5814208984375 = 2.1375839710235596 + 100.0 * 8.374438285827637
Epoch 1320, val loss: 2.137281894683838
Epoch 1330, training loss: 839.5354614257812 = 2.137299656867981 + 100.0 * 8.373981475830078
Epoch 1330, val loss: 2.1370129585266113
Epoch 1340, training loss: 839.586181640625 = 2.1370311975479126 + 100.0 * 8.374491691589355
Epoch 1340, val loss: 2.136725425720215
Epoch 1350, training loss: 839.5038452148438 = 2.1367121934890747 + 100.0 * 8.373671531677246
Epoch 1350, val loss: 2.136448383331299
Epoch 1360, training loss: 839.49462890625 = 2.136419892311096 + 100.0 * 8.373581886291504
Epoch 1360, val loss: 2.1361804008483887
Epoch 1370, training loss: 839.3606567382812 = 2.1361567974090576 + 100.0 * 8.372244834899902
Epoch 1370, val loss: 2.135889768600464
Epoch 1380, training loss: 839.343017578125 = 2.135886073112488 + 100.0 * 8.372071266174316
Epoch 1380, val loss: 2.1356201171875
Epoch 1390, training loss: 839.36962890625 = 2.1356087923049927 + 100.0 * 8.372340202331543
Epoch 1390, val loss: 2.1353530883789062
Epoch 1400, training loss: 839.387939453125 = 2.135287284851074 + 100.0 * 8.372526168823242
Epoch 1400, val loss: 2.135077953338623
Epoch 1410, training loss: 839.2189331054688 = 2.1350306272506714 + 100.0 * 8.37083911895752
Epoch 1410, val loss: 2.134799003601074
Epoch 1420, training loss: 839.160400390625 = 2.13476026058197 + 100.0 * 8.370256423950195
Epoch 1420, val loss: 2.134528636932373
Epoch 1430, training loss: 839.1265258789062 = 2.1344975233078003 + 100.0 * 8.369919776916504
Epoch 1430, val loss: 2.134265422821045
Epoch 1440, training loss: 839.2326049804688 = 2.1342501640319824 + 100.0 * 8.370983123779297
Epoch 1440, val loss: 2.1339831352233887
Epoch 1450, training loss: 839.0672607421875 = 2.133933663368225 + 100.0 * 8.369333267211914
Epoch 1450, val loss: 2.133735179901123
Epoch 1460, training loss: 838.9992065429688 = 2.1336618661880493 + 100.0 * 8.36865520477295
Epoch 1460, val loss: 2.133474826812744
Epoch 1470, training loss: 838.9627075195312 = 2.1333941221237183 + 100.0 * 8.368292808532715
Epoch 1470, val loss: 2.1332240104675293
Epoch 1480, training loss: 839.1395874023438 = 2.133123755455017 + 100.0 * 8.370064735412598
Epoch 1480, val loss: 2.132960319519043
Epoch 1490, training loss: 839.198486328125 = 2.13286292552948 + 100.0 * 8.37065601348877
Epoch 1490, val loss: 2.132669448852539
Epoch 1500, training loss: 838.8699340820312 = 2.132556200027466 + 100.0 * 8.3673734664917
Epoch 1500, val loss: 2.132416248321533
Epoch 1510, training loss: 838.8356323242188 = 2.132300019264221 + 100.0 * 8.367033004760742
Epoch 1510, val loss: 2.1321728229522705
Epoch 1520, training loss: 838.7506713867188 = 2.1320526599884033 + 100.0 * 8.366186141967773
Epoch 1520, val loss: 2.1319217681884766
Epoch 1530, training loss: 838.71435546875 = 2.131804347038269 + 100.0 * 8.365825653076172
Epoch 1530, val loss: 2.1316771507263184
Epoch 1540, training loss: 838.7600708007812 = 2.1315423250198364 + 100.0 * 8.36628532409668
Epoch 1540, val loss: 2.1314404010772705
Epoch 1550, training loss: 838.6663818359375 = 2.1312721967697144 + 100.0 * 8.365350723266602
Epoch 1550, val loss: 2.131164073944092
Epoch 1560, training loss: 838.642333984375 = 2.13100802898407 + 100.0 * 8.365113258361816
Epoch 1560, val loss: 2.130927085876465
Epoch 1570, training loss: 838.733642578125 = 2.1307411193847656 + 100.0 * 8.366028785705566
Epoch 1570, val loss: 2.1306800842285156
Epoch 1580, training loss: 838.5045776367188 = 2.130496382713318 + 100.0 * 8.363740921020508
Epoch 1580, val loss: 2.1304256916046143
Epoch 1590, training loss: 838.4791870117188 = 2.130259156227112 + 100.0 * 8.363489151000977
Epoch 1590, val loss: 2.1301889419555664
Epoch 1600, training loss: 838.4447021484375 = 2.130012035369873 + 100.0 * 8.363146781921387
Epoch 1600, val loss: 2.129952907562256
Epoch 1610, training loss: 838.5801391601562 = 2.1297892332077026 + 100.0 * 8.364503860473633
Epoch 1610, val loss: 2.129699230194092
Epoch 1620, training loss: 838.4140014648438 = 2.129462957382202 + 100.0 * 8.362845420837402
Epoch 1620, val loss: 2.129483222961426
Epoch 1630, training loss: 838.4262084960938 = 2.129252314567566 + 100.0 * 8.362969398498535
Epoch 1630, val loss: 2.129202127456665
Epoch 1640, training loss: 838.2767944335938 = 2.128998637199402 + 100.0 * 8.361477851867676
Epoch 1640, val loss: 2.1289825439453125
Epoch 1650, training loss: 838.2500610351562 = 2.128770589828491 + 100.0 * 8.361212730407715
Epoch 1650, val loss: 2.128756046295166
Epoch 1660, training loss: 838.3576049804688 = 2.128565549850464 + 100.0 * 8.362290382385254
Epoch 1660, val loss: 2.128502368927002
Epoch 1670, training loss: 838.1785278320312 = 2.1282676458358765 + 100.0 * 8.360502243041992
Epoch 1670, val loss: 2.128283977508545
Epoch 1680, training loss: 838.216064453125 = 2.1280258893966675 + 100.0 * 8.360879898071289
Epoch 1680, val loss: 2.1280503273010254
Epoch 1690, training loss: 838.1375122070312 = 2.127798914909363 + 100.0 * 8.36009693145752
Epoch 1690, val loss: 2.127809524536133
Epoch 1700, training loss: 838.0673828125 = 2.1275432109832764 + 100.0 * 8.359397888183594
Epoch 1700, val loss: 2.127591371536255
Epoch 1710, training loss: 838.0313720703125 = 2.127321720123291 + 100.0 * 8.359040260314941
Epoch 1710, val loss: 2.1273603439331055
Epoch 1720, training loss: 837.9999389648438 = 2.127076745033264 + 100.0 * 8.358728408813477
Epoch 1720, val loss: 2.1271400451660156
Epoch 1730, training loss: 838.409912109375 = 2.1268423795700073 + 100.0 * 8.362831115722656
Epoch 1730, val loss: 2.1268982887268066
Epoch 1740, training loss: 838.0646362304688 = 2.1265875101089478 + 100.0 * 8.359380722045898
Epoch 1740, val loss: 2.1266536712646484
Epoch 1750, training loss: 837.8780517578125 = 2.1263457536697388 + 100.0 * 8.35751724243164
Epoch 1750, val loss: 2.1264278888702393
Epoch 1760, training loss: 837.8345336914062 = 2.1261156797409058 + 100.0 * 8.357084274291992
Epoch 1760, val loss: 2.1262075901031494
Epoch 1770, training loss: 837.8065185546875 = 2.125894784927368 + 100.0 * 8.356805801391602
Epoch 1770, val loss: 2.125988245010376
Epoch 1780, training loss: 837.9667358398438 = 2.125655174255371 + 100.0 * 8.358410835266113
Epoch 1780, val loss: 2.1257729530334473
Epoch 1790, training loss: 837.8035888671875 = 2.125390887260437 + 100.0 * 8.356781959533691
Epoch 1790, val loss: 2.125520706176758
Epoch 1800, training loss: 837.7868041992188 = 2.1251598596572876 + 100.0 * 8.356616020202637
Epoch 1800, val loss: 2.125303268432617
Epoch 1810, training loss: 837.6878051757812 = 2.1249499320983887 + 100.0 * 8.355628967285156
Epoch 1810, val loss: 2.125077486038208
Epoch 1820, training loss: 837.6569213867188 = 2.1247278451919556 + 100.0 * 8.355321884155273
Epoch 1820, val loss: 2.1248652935028076
Epoch 1830, training loss: 837.9175415039062 = 2.1244863271713257 + 100.0 * 8.357930183410645
Epoch 1830, val loss: 2.1246628761291504
Epoch 1840, training loss: 837.8787231445312 = 2.1242074966430664 + 100.0 * 8.357544898986816
Epoch 1840, val loss: 2.124413013458252
Epoch 1850, training loss: 837.6243896484375 = 2.124027967453003 + 100.0 * 8.355003356933594
Epoch 1850, val loss: 2.1241695880889893
Epoch 1860, training loss: 837.52294921875 = 2.123790740966797 + 100.0 * 8.353991508483887
Epoch 1860, val loss: 2.123958110809326
Epoch 1870, training loss: 837.4884033203125 = 2.123566508293152 + 100.0 * 8.35364818572998
Epoch 1870, val loss: 2.1237528324127197
Epoch 1880, training loss: 837.4998779296875 = 2.1233547925949097 + 100.0 * 8.353765487670898
Epoch 1880, val loss: 2.123534679412842
Epoch 1890, training loss: 837.8392333984375 = 2.1231446266174316 + 100.0 * 8.357160568237305
Epoch 1890, val loss: 2.123289108276367
Epoch 1900, training loss: 837.4671630859375 = 2.122871994972229 + 100.0 * 8.353443145751953
Epoch 1900, val loss: 2.1230673789978027
Epoch 1910, training loss: 837.3966674804688 = 2.122646689414978 + 100.0 * 8.352740287780762
Epoch 1910, val loss: 2.1228673458099365
Epoch 1920, training loss: 837.3704223632812 = 2.122426390647888 + 100.0 * 8.352479934692383
Epoch 1920, val loss: 2.1226553916931152
Epoch 1930, training loss: 837.3759765625 = 2.1221933364868164 + 100.0 * 8.352538108825684
Epoch 1930, val loss: 2.1224594116210938
Epoch 1940, training loss: 837.5256958007812 = 2.1219385862350464 + 100.0 * 8.354037284851074
Epoch 1940, val loss: 2.1222445964813232
Epoch 1950, training loss: 837.3333129882812 = 2.1217827796936035 + 100.0 * 8.352115631103516
Epoch 1950, val loss: 2.1219818592071533
Epoch 1960, training loss: 837.2574462890625 = 2.1215301752090454 + 100.0 * 8.351359367370605
Epoch 1960, val loss: 2.1217994689941406
Epoch 1970, training loss: 837.2330322265625 = 2.1213269233703613 + 100.0 * 8.351117134094238
Epoch 1970, val loss: 2.121588706970215
Epoch 1980, training loss: 837.2942504882812 = 2.1211211681365967 + 100.0 * 8.351731300354004
Epoch 1980, val loss: 2.1213836669921875
Epoch 1990, training loss: 837.3336791992188 = 2.120869994163513 + 100.0 * 8.352128028869629
Epoch 1990, val loss: 2.121152400970459
Epoch 2000, training loss: 837.179443359375 = 2.1206562519073486 + 100.0 * 8.350587844848633
Epoch 2000, val loss: 2.120931625366211
Epoch 2010, training loss: 837.1534423828125 = 2.1204371452331543 + 100.0 * 8.350330352783203
Epoch 2010, val loss: 2.1207313537597656
Epoch 2020, training loss: 837.1534423828125 = 2.1202151775360107 + 100.0 * 8.350332260131836
Epoch 2020, val loss: 2.120535373687744
Epoch 2030, training loss: 837.2421264648438 = 2.1199991703033447 + 100.0 * 8.351221084594727
Epoch 2030, val loss: 2.1203255653381348
Epoch 2040, training loss: 837.1317749023438 = 2.1198068857192993 + 100.0 * 8.350119590759277
Epoch 2040, val loss: 2.1200954914093018
Epoch 2050, training loss: 837.1788940429688 = 2.119596242904663 + 100.0 * 8.350593566894531
Epoch 2050, val loss: 2.119877576828003
Epoch 2060, training loss: 837.053466796875 = 2.1193495988845825 + 100.0 * 8.34934139251709
Epoch 2060, val loss: 2.1197009086608887
Epoch 2070, training loss: 837.0244750976562 = 2.1191658973693848 + 100.0 * 8.349053382873535
Epoch 2070, val loss: 2.1194963455200195
Epoch 2080, training loss: 836.9953002929688 = 2.1189552545547485 + 100.0 * 8.348763465881348
Epoch 2080, val loss: 2.1193065643310547
Epoch 2090, training loss: 837.0944213867188 = 2.118769645690918 + 100.0 * 8.349756240844727
Epoch 2090, val loss: 2.1190967559814453
Epoch 2100, training loss: 837.010986328125 = 2.118540406227112 + 100.0 * 8.34892463684082
Epoch 2100, val loss: 2.118887424468994
Epoch 2110, training loss: 837.1460571289062 = 2.118326187133789 + 100.0 * 8.350276947021484
Epoch 2110, val loss: 2.118673086166382
Epoch 2120, training loss: 836.95361328125 = 2.118107557296753 + 100.0 * 8.348355293273926
Epoch 2120, val loss: 2.1184802055358887
Epoch 2130, training loss: 836.8881225585938 = 2.1178932189941406 + 100.0 * 8.347702026367188
Epoch 2130, val loss: 2.1182992458343506
Epoch 2140, training loss: 836.8370361328125 = 2.1177061796188354 + 100.0 * 8.347192764282227
Epoch 2140, val loss: 2.1181063652038574
Epoch 2150, training loss: 836.808837890625 = 2.1175132989883423 + 100.0 * 8.34691333770752
Epoch 2150, val loss: 2.117919921875
Epoch 2160, training loss: 836.9829711914062 = 2.1173003911972046 + 100.0 * 8.34865665435791
Epoch 2160, val loss: 2.1177453994750977
Epoch 2170, training loss: 836.8148803710938 = 2.1170650720596313 + 100.0 * 8.346978187561035
Epoch 2170, val loss: 2.117506980895996
Epoch 2180, training loss: 836.8311157226562 = 2.116881251335144 + 100.0 * 8.347142219543457
Epoch 2180, val loss: 2.117321014404297
Epoch 2190, training loss: 836.7412719726562 = 2.1166930198669434 + 100.0 * 8.346245765686035
Epoch 2190, val loss: 2.117126703262329
Epoch 2200, training loss: 836.7032470703125 = 2.1165043115615845 + 100.0 * 8.345867156982422
Epoch 2200, val loss: 2.116947650909424
Epoch 2210, training loss: 836.8116455078125 = 2.1163220405578613 + 100.0 * 8.346953392028809
Epoch 2210, val loss: 2.116755485534668
Epoch 2220, training loss: 836.772216796875 = 2.1161017417907715 + 100.0 * 8.346561431884766
Epoch 2220, val loss: 2.1165459156036377
Epoch 2230, training loss: 836.7911987304688 = 2.115881323814392 + 100.0 * 8.346753120422363
Epoch 2230, val loss: 2.1163554191589355
Epoch 2240, training loss: 836.6292114257812 = 2.1156853437423706 + 100.0 * 8.345135688781738
Epoch 2240, val loss: 2.1161842346191406
Epoch 2250, training loss: 836.6044921875 = 2.1155091524124146 + 100.0 * 8.344889640808105
Epoch 2250, val loss: 2.115996837615967
Epoch 2260, training loss: 836.5784301757812 = 2.115320086479187 + 100.0 * 8.34463119506836
Epoch 2260, val loss: 2.115823745727539
Epoch 2270, training loss: 836.5840454101562 = 2.1151349544525146 + 100.0 * 8.34468936920166
Epoch 2270, val loss: 2.115647792816162
Epoch 2280, training loss: 837.0596923828125 = 2.114898204803467 + 100.0 * 8.349448204040527
Epoch 2280, val loss: 2.1154870986938477
Epoch 2290, training loss: 836.7276611328125 = 2.1147267818450928 + 100.0 * 8.346129417419434
Epoch 2290, val loss: 2.1152360439300537
Epoch 2300, training loss: 836.51123046875 = 2.1145302057266235 + 100.0 * 8.34396743774414
Epoch 2300, val loss: 2.11506986618042
Epoch 2310, training loss: 836.4857788085938 = 2.114344596862793 + 100.0 * 8.343714714050293
Epoch 2310, val loss: 2.1148953437805176
Epoch 2320, training loss: 836.4603881835938 = 2.114182233810425 + 100.0 * 8.343461990356445
Epoch 2320, val loss: 2.1147267818450928
Epoch 2330, training loss: 836.43017578125 = 2.1139955520629883 + 100.0 * 8.343161582946777
Epoch 2330, val loss: 2.114562511444092
Epoch 2340, training loss: 836.4512329101562 = 2.1138209104537964 + 100.0 * 8.343374252319336
Epoch 2340, val loss: 2.114396572113037
Epoch 2350, training loss: 836.7252807617188 = 2.113620400428772 + 100.0 * 8.346116065979004
Epoch 2350, val loss: 2.1142148971557617
Epoch 2360, training loss: 836.431640625 = 2.113432288169861 + 100.0 * 8.343182563781738
Epoch 2360, val loss: 2.114001750946045
Epoch 2370, training loss: 836.3712768554688 = 2.1132454872131348 + 100.0 * 8.342580795288086
Epoch 2370, val loss: 2.1138455867767334
Epoch 2380, training loss: 836.3529052734375 = 2.1130592823028564 + 100.0 * 8.342398643493652
Epoch 2380, val loss: 2.1136856079101562
Epoch 2390, training loss: 836.4159545898438 = 2.1128755807876587 + 100.0 * 8.34303092956543
Epoch 2390, val loss: 2.113527297973633
Epoch 2400, training loss: 836.430908203125 = 2.11268150806427 + 100.0 * 8.343182563781738
Epoch 2400, val loss: 2.113333225250244
Epoch 2410, training loss: 836.3485717773438 = 2.1125357151031494 + 100.0 * 8.342360496520996
Epoch 2410, val loss: 2.1131386756896973
Epoch 2420, training loss: 836.3099365234375 = 2.1123275756835938 + 100.0 * 8.341976165771484
Epoch 2420, val loss: 2.112997055053711
Epoch 2430, training loss: 836.5502319335938 = 2.1121509075164795 + 100.0 * 8.344381332397461
Epoch 2430, val loss: 2.1128058433532715
Epoch 2440, training loss: 836.3244018554688 = 2.1119887828826904 + 100.0 * 8.342123985290527
Epoch 2440, val loss: 2.112636089324951
Epoch 2450, training loss: 836.2341918945312 = 2.1118032932281494 + 100.0 * 8.34122371673584
Epoch 2450, val loss: 2.1124701499938965
Epoch 2460, training loss: 836.2362060546875 = 2.111639976501465 + 100.0 * 8.341245651245117
Epoch 2460, val loss: 2.1123058795928955
Epoch 2470, training loss: 836.4226684570312 = 2.111464262008667 + 100.0 * 8.343111991882324
Epoch 2470, val loss: 2.112116813659668
Epoch 2480, training loss: 836.2351684570312 = 2.111253499984741 + 100.0 * 8.341238975524902
Epoch 2480, val loss: 2.111971855163574
Epoch 2490, training loss: 836.3290405273438 = 2.111081600189209 + 100.0 * 8.342179298400879
Epoch 2490, val loss: 2.1117935180664062
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.4315942028985507
0.8648119973918714
=== training gcn model ===
Epoch 0, training loss: 1060.425048828125 = 2.200334906578064 + 100.0 * 10.582246780395508
Epoch 0, val loss: 2.1998305320739746
Epoch 10, training loss: 1060.3682861328125 = 2.1860134601593018 + 100.0 * 10.581822395324707
Epoch 10, val loss: 2.1870229244232178
Epoch 20, training loss: 1060.1859130859375 = 2.1835871934890747 + 100.0 * 10.580022811889648
Epoch 20, val loss: 2.185309886932373
Epoch 30, training loss: 1059.3929443359375 = 2.1831552982330322 + 100.0 * 10.572098731994629
Epoch 30, val loss: 2.1850361824035645
Epoch 40, training loss: 1056.32958984375 = 2.1838319301605225 + 100.0 * 10.541457176208496
Epoch 40, val loss: 2.1856327056884766
Epoch 50, training loss: 1046.8841552734375 = 2.1849982738494873 + 100.0 * 10.446990966796875
Epoch 50, val loss: 2.1865944862365723
Epoch 60, training loss: 1022.21484375 = 2.1865307092666626 + 100.0 * 10.20028305053711
Epoch 60, val loss: 2.187896728515625
Epoch 70, training loss: 980.6218872070312 = 2.1893216371536255 + 100.0 * 9.78432559967041
Epoch 70, val loss: 2.1903109550476074
Epoch 80, training loss: 959.1703491210938 = 2.1914607286453247 + 100.0 * 9.569788932800293
Epoch 80, val loss: 2.1916344165802
Epoch 90, training loss: 945.3370971679688 = 2.189130663871765 + 100.0 * 9.431479454040527
Epoch 90, val loss: 2.1892547607421875
Epoch 100, training loss: 935.97412109375 = 2.1855305433273315 + 100.0 * 9.337885856628418
Epoch 100, val loss: 2.186112403869629
Epoch 110, training loss: 930.3325805664062 = 2.184380292892456 + 100.0 * 9.281481742858887
Epoch 110, val loss: 2.185206174850464
Epoch 120, training loss: 926.3214721679688 = 2.184388756752014 + 100.0 * 9.241371154785156
Epoch 120, val loss: 2.1852426528930664
Epoch 130, training loss: 923.5838623046875 = 2.184241771697998 + 100.0 * 9.213995933532715
Epoch 130, val loss: 2.185128688812256
Epoch 140, training loss: 920.4611206054688 = 2.18381929397583 + 100.0 * 9.182772636413574
Epoch 140, val loss: 2.1847310066223145
Epoch 150, training loss: 915.8560791015625 = 2.1835321187973022 + 100.0 * 9.136725425720215
Epoch 150, val loss: 2.1844937801361084
Epoch 160, training loss: 909.2553100585938 = 2.183821439743042 + 100.0 * 9.070714950561523
Epoch 160, val loss: 2.1848220825195312
Epoch 170, training loss: 902.2741088867188 = 2.184649705886841 + 100.0 * 9.000894546508789
Epoch 170, val loss: 2.1856536865234375
Epoch 180, training loss: 898.6925048828125 = 2.1852129697799683 + 100.0 * 8.965072631835938
Epoch 180, val loss: 2.1861648559570312
Epoch 190, training loss: 895.3240966796875 = 2.1850253343582153 + 100.0 * 8.931390762329102
Epoch 190, val loss: 2.1859803199768066
Epoch 200, training loss: 890.489013671875 = 2.1850082874298096 + 100.0 * 8.883040428161621
Epoch 200, val loss: 2.18599796295166
Epoch 210, training loss: 884.406982421875 = 2.185327887535095 + 100.0 * 8.822216987609863
Epoch 210, val loss: 2.186356544494629
Epoch 220, training loss: 878.9813232421875 = 2.1855236291885376 + 100.0 * 8.76795768737793
Epoch 220, val loss: 2.186593532562256
Epoch 230, training loss: 874.8016967773438 = 2.1855911016464233 + 100.0 * 8.726161003112793
Epoch 230, val loss: 2.1866705417633057
Epoch 240, training loss: 871.5145874023438 = 2.185587167739868 + 100.0 * 8.693289756774902
Epoch 240, val loss: 2.1867055892944336
Epoch 250, training loss: 869.310546875 = 2.185512900352478 + 100.0 * 8.671250343322754
Epoch 250, val loss: 2.186659336090088
Epoch 260, training loss: 867.5073852539062 = 2.185341954231262 + 100.0 * 8.653220176696777
Epoch 260, val loss: 2.1865272521972656
Epoch 270, training loss: 865.967529296875 = 2.1852431297302246 + 100.0 * 8.637823104858398
Epoch 270, val loss: 2.1864013671875
Epoch 280, training loss: 864.2509765625 = 2.1850602626800537 + 100.0 * 8.620658874511719
Epoch 280, val loss: 2.186298370361328
Epoch 290, training loss: 862.7418212890625 = 2.184937000274658 + 100.0 * 8.605568885803223
Epoch 290, val loss: 2.1861765384674072
Epoch 300, training loss: 861.4188842773438 = 2.1848167181015015 + 100.0 * 8.592340469360352
Epoch 300, val loss: 2.1861021518707275
Epoch 310, training loss: 860.251708984375 = 2.184728503227234 + 100.0 * 8.580669403076172
Epoch 310, val loss: 2.1860151290893555
Epoch 320, training loss: 859.3011474609375 = 2.184579372406006 + 100.0 * 8.571166038513184
Epoch 320, val loss: 2.1859188079833984
Epoch 330, training loss: 858.5966796875 = 2.1844441890716553 + 100.0 * 8.564122200012207
Epoch 330, val loss: 2.185798168182373
Epoch 340, training loss: 857.5828247070312 = 2.18428897857666 + 100.0 * 8.553985595703125
Epoch 340, val loss: 2.185673713684082
Epoch 350, training loss: 856.6371459960938 = 2.1841436624526978 + 100.0 * 8.544529914855957
Epoch 350, val loss: 2.185560703277588
Epoch 360, training loss: 855.8593139648438 = 2.1840102672576904 + 100.0 * 8.536752700805664
Epoch 360, val loss: 2.1854422092437744
Epoch 370, training loss: 854.8492431640625 = 2.1838661432266235 + 100.0 * 8.526654243469238
Epoch 370, val loss: 2.185328722000122
Epoch 380, training loss: 854.012451171875 = 2.1837306022644043 + 100.0 * 8.518287658691406
Epoch 380, val loss: 2.1852245330810547
Epoch 390, training loss: 853.301025390625 = 2.183582901954651 + 100.0 * 8.511174201965332
Epoch 390, val loss: 2.1851115226745605
Epoch 400, training loss: 852.6771240234375 = 2.183432936668396 + 100.0 * 8.504937171936035
Epoch 400, val loss: 2.1849894523620605
Epoch 410, training loss: 852.2880859375 = 2.1832778453826904 + 100.0 * 8.50104808807373
Epoch 410, val loss: 2.1848556995391846
Epoch 420, training loss: 851.6502685546875 = 2.1830962896347046 + 100.0 * 8.494671821594238
Epoch 420, val loss: 2.1847238540649414
Epoch 430, training loss: 851.2077026367188 = 2.182928681373596 + 100.0 * 8.49024772644043
Epoch 430, val loss: 2.184581995010376
Epoch 440, training loss: 850.8829956054688 = 2.1827592849731445 + 100.0 * 8.4870023727417
Epoch 440, val loss: 2.1844372749328613
Epoch 450, training loss: 850.5267944335938 = 2.1825649738311768 + 100.0 * 8.483442306518555
Epoch 450, val loss: 2.184295654296875
Epoch 460, training loss: 850.0790405273438 = 2.1823949813842773 + 100.0 * 8.47896671295166
Epoch 460, val loss: 2.1841468811035156
Epoch 470, training loss: 849.680908203125 = 2.1822162866592407 + 100.0 * 8.474987030029297
Epoch 470, val loss: 2.183994770050049
Epoch 480, training loss: 849.3282470703125 = 2.182024121284485 + 100.0 * 8.47146224975586
Epoch 480, val loss: 2.1838436126708984
Epoch 490, training loss: 849.3177490234375 = 2.18182635307312 + 100.0 * 8.471359252929688
Epoch 490, val loss: 2.183682441711426
Epoch 500, training loss: 848.71826171875 = 2.1816455125808716 + 100.0 * 8.46536636352539
Epoch 500, val loss: 2.183518409729004
Epoch 510, training loss: 848.45068359375 = 2.181455135345459 + 100.0 * 8.462692260742188
Epoch 510, val loss: 2.1833581924438477
Epoch 520, training loss: 848.1700439453125 = 2.1812433004379272 + 100.0 * 8.459888458251953
Epoch 520, val loss: 2.183187961578369
Epoch 530, training loss: 848.1513671875 = 2.1810277700424194 + 100.0 * 8.45970344543457
Epoch 530, val loss: 2.1830224990844727
Epoch 540, training loss: 847.7339477539062 = 2.1808356046676636 + 100.0 * 8.455531120300293
Epoch 540, val loss: 2.1828432083129883
Epoch 550, training loss: 847.4794921875 = 2.180629014968872 + 100.0 * 8.452988624572754
Epoch 550, val loss: 2.1826725006103516
Epoch 560, training loss: 847.2562255859375 = 2.1804243326187134 + 100.0 * 8.45075798034668
Epoch 560, val loss: 2.182504653930664
Epoch 570, training loss: 847.0463256835938 = 2.1802207231521606 + 100.0 * 8.448660850524902
Epoch 570, val loss: 2.1823339462280273
Epoch 580, training loss: 846.8939819335938 = 2.1800166368484497 + 100.0 * 8.447139739990234
Epoch 580, val loss: 2.1821720600128174
Epoch 590, training loss: 846.898193359375 = 2.179836630821228 + 100.0 * 8.447183609008789
Epoch 590, val loss: 2.1820011138916016
Epoch 600, training loss: 846.4860229492188 = 2.1796289682388306 + 100.0 * 8.443063735961914
Epoch 600, val loss: 2.1818408966064453
Epoch 610, training loss: 846.2593383789062 = 2.179441452026367 + 100.0 * 8.44079875946045
Epoch 610, val loss: 2.1816842555999756
Epoch 620, training loss: 846.0472412109375 = 2.1792526245117188 + 100.0 * 8.438679695129395
Epoch 620, val loss: 2.1815316677093506
Epoch 630, training loss: 845.9707641601562 = 2.17907452583313 + 100.0 * 8.43791675567627
Epoch 630, val loss: 2.181385040283203
Epoch 640, training loss: 845.6910400390625 = 2.1788830757141113 + 100.0 * 8.435121536254883
Epoch 640, val loss: 2.181232452392578
Epoch 650, training loss: 845.454345703125 = 2.1787161827087402 + 100.0 * 8.432756423950195
Epoch 650, val loss: 2.181096076965332
Epoch 660, training loss: 845.3595581054688 = 2.1785465478897095 + 100.0 * 8.43181037902832
Epoch 660, val loss: 2.1809568405151367
Epoch 670, training loss: 845.1691284179688 = 2.178372383117676 + 100.0 * 8.42990779876709
Epoch 670, val loss: 2.180820941925049
Epoch 680, training loss: 844.7949829101562 = 2.1782113313674927 + 100.0 * 8.426167488098145
Epoch 680, val loss: 2.180691719055176
Epoch 690, training loss: 844.5425415039062 = 2.178053617477417 + 100.0 * 8.42364501953125
Epoch 690, val loss: 2.1805667877197266
Epoch 700, training loss: 844.6806030273438 = 2.177894353866577 + 100.0 * 8.425026893615723
Epoch 700, val loss: 2.1804354190826416
Epoch 710, training loss: 844.197509765625 = 2.1777323484420776 + 100.0 * 8.420197486877441
Epoch 710, val loss: 2.1803035736083984
Epoch 720, training loss: 843.9749145507812 = 2.17758047580719 + 100.0 * 8.417973518371582
Epoch 720, val loss: 2.180182933807373
Epoch 730, training loss: 843.8359985351562 = 2.1774269342422485 + 100.0 * 8.416585922241211
Epoch 730, val loss: 2.1800596714019775
Epoch 740, training loss: 843.7172241210938 = 2.177270531654358 + 100.0 * 8.415399551391602
Epoch 740, val loss: 2.179932117462158
Epoch 750, training loss: 843.477294921875 = 2.177114248275757 + 100.0 * 8.413002014160156
Epoch 750, val loss: 2.179809331893921
Epoch 760, training loss: 843.3181762695312 = 2.1769543886184692 + 100.0 * 8.411412239074707
Epoch 760, val loss: 2.1796875
Epoch 770, training loss: 843.2051391601562 = 2.1768022775650024 + 100.0 * 8.410283088684082
Epoch 770, val loss: 2.1795668601989746
Epoch 780, training loss: 843.1339111328125 = 2.176643490791321 + 100.0 * 8.40957260131836
Epoch 780, val loss: 2.179438591003418
Epoch 790, training loss: 842.9921264648438 = 2.1764883995056152 + 100.0 * 8.408156394958496
Epoch 790, val loss: 2.179320812225342
Epoch 800, training loss: 842.8052368164062 = 2.1763488054275513 + 100.0 * 8.406289100646973
Epoch 800, val loss: 2.179197072982788
Epoch 810, training loss: 842.7101440429688 = 2.176195740699768 + 100.0 * 8.405339241027832
Epoch 810, val loss: 2.179076671600342
Epoch 820, training loss: 842.9103393554688 = 2.176032066345215 + 100.0 * 8.407342910766602
Epoch 820, val loss: 2.178957462310791
Epoch 830, training loss: 842.54638671875 = 2.1758824586868286 + 100.0 * 8.403704643249512
Epoch 830, val loss: 2.1788330078125
Epoch 840, training loss: 842.4432983398438 = 2.1757357120513916 + 100.0 * 8.40267562866211
Epoch 840, val loss: 2.178711414337158
Epoch 850, training loss: 842.2982177734375 = 2.175588011741638 + 100.0 * 8.401226043701172
Epoch 850, val loss: 2.1785926818847656
Epoch 860, training loss: 842.21875 = 2.1754409074783325 + 100.0 * 8.400433540344238
Epoch 860, val loss: 2.178476333618164
Epoch 870, training loss: 842.4430541992188 = 2.175297737121582 + 100.0 * 8.402677536010742
Epoch 870, val loss: 2.1783599853515625
Epoch 880, training loss: 842.1650390625 = 2.175140857696533 + 100.0 * 8.399898529052734
Epoch 880, val loss: 2.1782350540161133
Epoch 890, training loss: 841.9490966796875 = 2.1750006675720215 + 100.0 * 8.397741317749023
Epoch 890, val loss: 2.178126811981201
Epoch 900, training loss: 841.8489379882812 = 2.1748563051223755 + 100.0 * 8.396740913391113
Epoch 900, val loss: 2.178013324737549
Epoch 910, training loss: 841.8984985351562 = 2.174716830253601 + 100.0 * 8.397237777709961
Epoch 910, val loss: 2.1779017448425293
Epoch 920, training loss: 841.6928100585938 = 2.1745704412460327 + 100.0 * 8.395182609558105
Epoch 920, val loss: 2.1777868270874023
Epoch 930, training loss: 841.6076049804688 = 2.174436926841736 + 100.0 * 8.394331932067871
Epoch 930, val loss: 2.1776795387268066
Epoch 940, training loss: 841.5761108398438 = 2.174301266670227 + 100.0 * 8.394018173217773
Epoch 940, val loss: 2.177574634552002
Epoch 950, training loss: 841.600341796875 = 2.174166679382324 + 100.0 * 8.394261360168457
Epoch 950, val loss: 2.1774649620056152
Epoch 960, training loss: 841.4050903320312 = 2.174022674560547 + 100.0 * 8.392311096191406
Epoch 960, val loss: 2.1773557662963867
Epoch 970, training loss: 841.2723388671875 = 2.173892378807068 + 100.0 * 8.390984535217285
Epoch 970, val loss: 2.1772546768188477
Epoch 980, training loss: 841.17333984375 = 2.17376446723938 + 100.0 * 8.389995574951172
Epoch 980, val loss: 2.177154064178467
Epoch 990, training loss: 841.2536010742188 = 2.1736433506011963 + 100.0 * 8.390799522399902
Epoch 990, val loss: 2.1770527362823486
Epoch 1000, training loss: 841.1144409179688 = 2.173496723175049 + 100.0 * 8.389409065246582
Epoch 1000, val loss: 2.1769461631774902
Epoch 1010, training loss: 840.9949340820312 = 2.1733778715133667 + 100.0 * 8.388215065002441
Epoch 1010, val loss: 2.1768484115600586
Epoch 1020, training loss: 840.8779907226562 = 2.1732523441314697 + 100.0 * 8.387046813964844
Epoch 1020, val loss: 2.176750421524048
Epoch 1030, training loss: 840.80224609375 = 2.1731250286102295 + 100.0 * 8.38629150390625
Epoch 1030, val loss: 2.1766555309295654
Epoch 1040, training loss: 840.7599487304688 = 2.1730101108551025 + 100.0 * 8.385869026184082
Epoch 1040, val loss: 2.1765613555908203
Epoch 1050, training loss: 841.0499877929688 = 2.172887444496155 + 100.0 * 8.388771057128906
Epoch 1050, val loss: 2.1764657497406006
Epoch 1060, training loss: 840.8577880859375 = 2.1727579832077026 + 100.0 * 8.386850357055664
Epoch 1060, val loss: 2.1763577461242676
Epoch 1070, training loss: 840.6255493164062 = 2.172639489173889 + 100.0 * 8.384529113769531
Epoch 1070, val loss: 2.1762704849243164
Epoch 1080, training loss: 840.504150390625 = 2.172518014907837 + 100.0 * 8.383316040039062
Epoch 1080, val loss: 2.176180362701416
Epoch 1090, training loss: 840.4298095703125 = 2.1724008321762085 + 100.0 * 8.382574081420898
Epoch 1090, val loss: 2.1760916709899902
Epoch 1100, training loss: 840.3701782226562 = 2.1722865104675293 + 100.0 * 8.381978988647461
Epoch 1100, val loss: 2.176004409790039
Epoch 1110, training loss: 840.3157348632812 = 2.172173261642456 + 100.0 * 8.38143539428711
Epoch 1110, val loss: 2.175917387008667
Epoch 1120, training loss: 840.262939453125 = 2.1720603704452515 + 100.0 * 8.380908966064453
Epoch 1120, val loss: 2.175830841064453
Epoch 1130, training loss: 840.8070068359375 = 2.171954393386841 + 100.0 * 8.386350631713867
Epoch 1130, val loss: 2.1757395267486572
Epoch 1140, training loss: 840.2167358398438 = 2.1718251705169678 + 100.0 * 8.380449295043945
Epoch 1140, val loss: 2.1756443977355957
Epoch 1150, training loss: 840.1234741210938 = 2.171717047691345 + 100.0 * 8.379517555236816
Epoch 1150, val loss: 2.175563097000122
Epoch 1160, training loss: 840.0665283203125 = 2.1716039180755615 + 100.0 * 8.378949165344238
Epoch 1160, val loss: 2.1754791736602783
Epoch 1170, training loss: 840.0584106445312 = 2.1714991331100464 + 100.0 * 8.37886905670166
Epoch 1170, val loss: 2.175398349761963
Epoch 1180, training loss: 840.2999267578125 = 2.17139208316803 + 100.0 * 8.381285667419434
Epoch 1180, val loss: 2.1753134727478027
Epoch 1190, training loss: 839.9271240234375 = 2.171276569366455 + 100.0 * 8.377558708190918
Epoch 1190, val loss: 2.175227165222168
Epoch 1200, training loss: 839.9044799804688 = 2.1711708307266235 + 100.0 * 8.377333641052246
Epoch 1200, val loss: 2.1751489639282227
Epoch 1210, training loss: 839.8462524414062 = 2.1710708141326904 + 100.0 * 8.376751899719238
Epoch 1210, val loss: 2.1750712394714355
Epoch 1220, training loss: 839.7864379882812 = 2.170968174934387 + 100.0 * 8.376154899597168
Epoch 1220, val loss: 2.1749958992004395
Epoch 1230, training loss: 839.7457885742188 = 2.170870780944824 + 100.0 * 8.375748634338379
Epoch 1230, val loss: 2.174919605255127
Epoch 1240, training loss: 840.3407592773438 = 2.1707773208618164 + 100.0 * 8.381699562072754
Epoch 1240, val loss: 2.1748406887054443
Epoch 1250, training loss: 839.6603393554688 = 2.1706576347351074 + 100.0 * 8.374897003173828
Epoch 1250, val loss: 2.1747608184814453
Epoch 1260, training loss: 839.6552124023438 = 2.17056405544281 + 100.0 * 8.374846458435059
Epoch 1260, val loss: 2.174692392349243
Epoch 1270, training loss: 839.5595703125 = 2.170470118522644 + 100.0 * 8.37389087677002
Epoch 1270, val loss: 2.174621105194092
Epoch 1280, training loss: 839.5052490234375 = 2.170379638671875 + 100.0 * 8.373348236083984
Epoch 1280, val loss: 2.1745524406433105
Epoch 1290, training loss: 839.4566650390625 = 2.1702873706817627 + 100.0 * 8.37286376953125
Epoch 1290, val loss: 2.174485206604004
Epoch 1300, training loss: 839.697998046875 = 2.1701927185058594 + 100.0 * 8.37527847290039
Epoch 1300, val loss: 2.17441725730896
Epoch 1310, training loss: 839.6239624023438 = 2.170097231864929 + 100.0 * 8.37453842163086
Epoch 1310, val loss: 2.1743407249450684
Epoch 1320, training loss: 839.337890625 = 2.1700092554092407 + 100.0 * 8.371679306030273
Epoch 1320, val loss: 2.1742770671844482
Epoch 1330, training loss: 839.3222045898438 = 2.169921636581421 + 100.0 * 8.371522903442383
Epoch 1330, val loss: 2.174213171005249
Epoch 1340, training loss: 839.2293701171875 = 2.1698371171951294 + 100.0 * 8.37059497833252
Epoch 1340, val loss: 2.174154043197632
Epoch 1350, training loss: 839.1941528320312 = 2.169757127761841 + 100.0 * 8.370244026184082
Epoch 1350, val loss: 2.174095630645752
Epoch 1360, training loss: 839.6554565429688 = 2.1696696281433105 + 100.0 * 8.374857902526855
Epoch 1360, val loss: 2.1740336418151855
Epoch 1370, training loss: 839.2457275390625 = 2.169582724571228 + 100.0 * 8.37076187133789
Epoch 1370, val loss: 2.1739654541015625
Epoch 1380, training loss: 839.1182861328125 = 2.169506072998047 + 100.0 * 8.369487762451172
Epoch 1380, val loss: 2.173910617828369
Epoch 1390, training loss: 839.00732421875 = 2.1694236993789673 + 100.0 * 8.368378639221191
Epoch 1390, val loss: 2.173853874206543
Epoch 1400, training loss: 838.9759521484375 = 2.1693469285964966 + 100.0 * 8.36806583404541
Epoch 1400, val loss: 2.173799514770508
Epoch 1410, training loss: 838.9417114257812 = 2.1692700386047363 + 100.0 * 8.367724418640137
Epoch 1410, val loss: 2.173746109008789
Epoch 1420, training loss: 839.3904418945312 = 2.1691815853118896 + 100.0 * 8.372212409973145
Epoch 1420, val loss: 2.1736903190612793
Epoch 1430, training loss: 838.9268188476562 = 2.169116258621216 + 100.0 * 8.367576599121094
Epoch 1430, val loss: 2.173631429672241
Epoch 1440, training loss: 838.8223266601562 = 2.169042706489563 + 100.0 * 8.366532325744629
Epoch 1440, val loss: 2.1735777854919434
Epoch 1450, training loss: 838.7744140625 = 2.1689655780792236 + 100.0 * 8.36605453491211
Epoch 1450, val loss: 2.173530340194702
Epoch 1460, training loss: 838.7440795898438 = 2.16890025138855 + 100.0 * 8.365752220153809
Epoch 1460, val loss: 2.1734795570373535
Epoch 1470, training loss: 839.0922241210938 = 2.168829560279846 + 100.0 * 8.369234085083008
Epoch 1470, val loss: 2.173424005508423
Epoch 1480, training loss: 838.7352294921875 = 2.1687432527542114 + 100.0 * 8.3656644821167
Epoch 1480, val loss: 2.1733760833740234
Epoch 1490, training loss: 838.6000366210938 = 2.168683886528015 + 100.0 * 8.364313125610352
Epoch 1490, val loss: 2.1733288764953613
Epoch 1500, training loss: 838.6137084960938 = 2.1686158180236816 + 100.0 * 8.364450454711914
Epoch 1500, val loss: 2.173281192779541
Epoch 1510, training loss: 838.7794799804688 = 2.168544888496399 + 100.0 * 8.366109848022461
Epoch 1510, val loss: 2.1732306480407715
Epoch 1520, training loss: 838.5067749023438 = 2.168471574783325 + 100.0 * 8.363383293151855
Epoch 1520, val loss: 2.173180103302002
Epoch 1530, training loss: 838.456298828125 = 2.1684072017669678 + 100.0 * 8.362878799438477
Epoch 1530, val loss: 2.1731390953063965
Epoch 1540, training loss: 838.4155883789062 = 2.168345332145691 + 100.0 * 8.362472534179688
Epoch 1540, val loss: 2.1730947494506836
Epoch 1550, training loss: 838.3678588867188 = 2.168283462524414 + 100.0 * 8.361995697021484
Epoch 1550, val loss: 2.1730542182922363
Epoch 1560, training loss: 838.4175415039062 = 2.1682251691818237 + 100.0 * 8.362493515014648
Epoch 1560, val loss: 2.1730129718780518
Epoch 1570, training loss: 838.4052734375 = 2.168154001235962 + 100.0 * 8.362371444702148
Epoch 1570, val loss: 2.1729626655578613
Epoch 1580, training loss: 838.2457885742188 = 2.168087124824524 + 100.0 * 8.360776901245117
Epoch 1580, val loss: 2.172919273376465
Epoch 1590, training loss: 838.2188110351562 = 2.1680264472961426 + 100.0 * 8.36050796508789
Epoch 1590, val loss: 2.172881603240967
Epoch 1600, training loss: 838.179443359375 = 2.1679699420928955 + 100.0 * 8.360115051269531
Epoch 1600, val loss: 2.1728410720825195
Epoch 1610, training loss: 838.2683715820312 = 2.167907238006592 + 100.0 * 8.361004829406738
Epoch 1610, val loss: 2.1728038787841797
Epoch 1620, training loss: 838.12451171875 = 2.167843222618103 + 100.0 * 8.359566688537598
Epoch 1620, val loss: 2.1727514266967773
Epoch 1630, training loss: 838.2247924804688 = 2.1677836179733276 + 100.0 * 8.360569953918457
Epoch 1630, val loss: 2.1727144718170166
Epoch 1640, training loss: 838.0375366210938 = 2.1677249670028687 + 100.0 * 8.358697891235352
Epoch 1640, val loss: 2.172677516937256
Epoch 1650, training loss: 837.9982299804688 = 2.167670726776123 + 100.0 * 8.358305931091309
Epoch 1650, val loss: 2.1726412773132324
Epoch 1660, training loss: 837.989990234375 = 2.1676156520843506 + 100.0 * 8.358223915100098
Epoch 1660, val loss: 2.1726064682006836
Epoch 1670, training loss: 838.3606567382812 = 2.1675528287887573 + 100.0 * 8.361930847167969
Epoch 1670, val loss: 2.1725685596466064
Epoch 1680, training loss: 837.9859619140625 = 2.1675013303756714 + 100.0 * 8.358184814453125
Epoch 1680, val loss: 2.1725268363952637
Epoch 1690, training loss: 837.8878784179688 = 2.1674435138702393 + 100.0 * 8.35720443725586
Epoch 1690, val loss: 2.1724929809570312
Epoch 1700, training loss: 837.9550170898438 = 2.167387843132019 + 100.0 * 8.357876777648926
Epoch 1700, val loss: 2.1724565029144287
Epoch 1710, training loss: 837.8339233398438 = 2.167339324951172 + 100.0 * 8.35666561126709
Epoch 1710, val loss: 2.172421455383301
Epoch 1720, training loss: 838.0371704101562 = 2.167288899421692 + 100.0 * 8.358698844909668
Epoch 1720, val loss: 2.172386646270752
Epoch 1730, training loss: 837.7532348632812 = 2.167221426963806 + 100.0 * 8.355859756469727
Epoch 1730, val loss: 2.1723461151123047
Epoch 1740, training loss: 837.7135620117188 = 2.1671758890151978 + 100.0 * 8.355463981628418
Epoch 1740, val loss: 2.1723155975341797
Epoch 1750, training loss: 837.6637573242188 = 2.167121171951294 + 100.0 * 8.354966163635254
Epoch 1750, val loss: 2.1722817420959473
Epoch 1760, training loss: 837.6301879882812 = 2.16707444190979 + 100.0 * 8.354631423950195
Epoch 1760, val loss: 2.172250747680664
Epoch 1770, training loss: 837.6243896484375 = 2.167025923728943 + 100.0 * 8.354573249816895
Epoch 1770, val loss: 2.1722195148468018
Epoch 1780, training loss: 837.934326171875 = 2.1669795513153076 + 100.0 * 8.357673645019531
Epoch 1780, val loss: 2.1721837520599365
Epoch 1790, training loss: 837.7051391601562 = 2.1669139862060547 + 100.0 * 8.355381965637207
Epoch 1790, val loss: 2.172147750854492
Epoch 1800, training loss: 837.52197265625 = 2.1668665409088135 + 100.0 * 8.353550910949707
Epoch 1800, val loss: 2.1721158027648926
Epoch 1810, training loss: 837.4923095703125 = 2.1668161153793335 + 100.0 * 8.353255271911621
Epoch 1810, val loss: 2.172085762023926
Epoch 1820, training loss: 837.4735717773438 = 2.1667686700820923 + 100.0 * 8.353068351745605
Epoch 1820, val loss: 2.172055959701538
Epoch 1830, training loss: 837.8301391601562 = 2.166720986366272 + 100.0 * 8.356634140014648
Epoch 1830, val loss: 2.172024965286255
Epoch 1840, training loss: 837.6356201171875 = 2.166667103767395 + 100.0 * 8.354689598083496
Epoch 1840, val loss: 2.1719837188720703
Epoch 1850, training loss: 837.4217529296875 = 2.1666171550750732 + 100.0 * 8.352551460266113
Epoch 1850, val loss: 2.171955108642578
Epoch 1860, training loss: 837.3523559570312 = 2.1665674448013306 + 100.0 * 8.351858139038086
Epoch 1860, val loss: 2.171923875808716
Epoch 1870, training loss: 837.3743286132812 = 2.166520833969116 + 100.0 * 8.352078437805176
Epoch 1870, val loss: 2.171894073486328
Epoch 1880, training loss: 837.4247436523438 = 2.166471242904663 + 100.0 * 8.352582931518555
Epoch 1880, val loss: 2.1718626022338867
Epoch 1890, training loss: 837.2813110351562 = 2.166429042816162 + 100.0 * 8.35114860534668
Epoch 1890, val loss: 2.171834707260132
Epoch 1900, training loss: 837.30859375 = 2.1663787364959717 + 100.0 * 8.351422309875488
Epoch 1900, val loss: 2.171807289123535
Epoch 1910, training loss: 837.4768676757812 = 2.1663200855255127 + 100.0 * 8.353105545043945
Epoch 1910, val loss: 2.1717653274536133
Epoch 1920, training loss: 837.2214965820312 = 2.1662806272506714 + 100.0 * 8.350552558898926
Epoch 1920, val loss: 2.1717395782470703
Epoch 1930, training loss: 837.1959838867188 = 2.1662367582321167 + 100.0 * 8.350296974182129
Epoch 1930, val loss: 2.1717076301574707
Epoch 1940, training loss: 837.1260986328125 = 2.1661934852600098 + 100.0 * 8.34959888458252
Epoch 1940, val loss: 2.1716833114624023
Epoch 1950, training loss: 837.102294921875 = 2.166152238845825 + 100.0 * 8.349361419677734
Epoch 1950, val loss: 2.1716578006744385
Epoch 1960, training loss: 837.5316162109375 = 2.166112184524536 + 100.0 * 8.353654861450195
Epoch 1960, val loss: 2.1716277599334717
Epoch 1970, training loss: 837.2346801757812 = 2.166053891181946 + 100.0 * 8.350686073303223
Epoch 1970, val loss: 2.1715965270996094
Epoch 1980, training loss: 837.0648803710938 = 2.1660176515579224 + 100.0 * 8.34898853302002
Epoch 1980, val loss: 2.171570062637329
Epoch 1990, training loss: 836.9910888671875 = 2.165974259376526 + 100.0 * 8.348251342773438
Epoch 1990, val loss: 2.171543836593628
Epoch 2000, training loss: 836.9609375 = 2.1659364700317383 + 100.0 * 8.347949981689453
Epoch 2000, val loss: 2.171523094177246
Epoch 2010, training loss: 836.952392578125 = 2.165897011756897 + 100.0 * 8.347865104675293
Epoch 2010, val loss: 2.171499490737915
Epoch 2020, training loss: 837.6593017578125 = 2.1658467054367065 + 100.0 * 8.354934692382812
Epoch 2020, val loss: 2.171468734741211
Epoch 2030, training loss: 837.0765380859375 = 2.165810227394104 + 100.0 * 8.349106788635254
Epoch 2030, val loss: 2.1714391708374023
Epoch 2040, training loss: 836.8628540039062 = 2.165767550468445 + 100.0 * 8.346970558166504
Epoch 2040, val loss: 2.171415328979492
Epoch 2050, training loss: 836.8565673828125 = 2.165726900100708 + 100.0 * 8.346908569335938
Epoch 2050, val loss: 2.1713905334472656
Epoch 2060, training loss: 836.8154907226562 = 2.1656943559646606 + 100.0 * 8.346497535705566
Epoch 2060, val loss: 2.1713707447052
Epoch 2070, training loss: 836.8666381835938 = 2.1656577587127686 + 100.0 * 8.347009658813477
Epoch 2070, val loss: 2.171349048614502
Epoch 2080, training loss: 836.8572998046875 = 2.1656076908111572 + 100.0 * 8.346917152404785
Epoch 2080, val loss: 2.1713144779205322
Epoch 2090, training loss: 836.8751220703125 = 2.165568232536316 + 100.0 * 8.347095489501953
Epoch 2090, val loss: 2.1712894439697266
Epoch 2100, training loss: 836.7141723632812 = 2.1655319929122925 + 100.0 * 8.345486640930176
Epoch 2100, val loss: 2.171269178390503
Epoch 2110, training loss: 836.699462890625 = 2.165495276451111 + 100.0 * 8.34533977508545
Epoch 2110, val loss: 2.1712472438812256
Epoch 2120, training loss: 836.6554565429688 = 2.1654629707336426 + 100.0 * 8.344900131225586
Epoch 2120, val loss: 2.1712284088134766
Epoch 2130, training loss: 836.74658203125 = 2.165428400039673 + 100.0 * 8.34581184387207
Epoch 2130, val loss: 2.171208381652832
Epoch 2140, training loss: 836.660400390625 = 2.1653857231140137 + 100.0 * 8.344949722290039
Epoch 2140, val loss: 2.171180009841919
Epoch 2150, training loss: 836.6534423828125 = 2.165351986885071 + 100.0 * 8.344881057739258
Epoch 2150, val loss: 2.171157121658325
Epoch 2160, training loss: 836.64697265625 = 2.1653164625167847 + 100.0 * 8.344816207885742
Epoch 2160, val loss: 2.171135902404785
Epoch 2170, training loss: 836.5941772460938 = 2.165280342102051 + 100.0 * 8.34428882598877
Epoch 2170, val loss: 2.171114921569824
Epoch 2180, training loss: 836.5123901367188 = 2.1652469635009766 + 100.0 * 8.34347152709961
Epoch 2180, val loss: 2.171095371246338
Epoch 2190, training loss: 836.5184936523438 = 2.165215015411377 + 100.0 * 8.34353256225586
Epoch 2190, val loss: 2.1710760593414307
Epoch 2200, training loss: 836.7403564453125 = 2.165181517601013 + 100.0 * 8.345751762390137
Epoch 2200, val loss: 2.171053409576416
Epoch 2210, training loss: 836.6304931640625 = 2.165141463279724 + 100.0 * 8.344653129577637
Epoch 2210, val loss: 2.1710317134857178
Epoch 2220, training loss: 836.50146484375 = 2.1651066541671753 + 100.0 * 8.343363761901855
Epoch 2220, val loss: 2.1710071563720703
Epoch 2230, training loss: 836.41650390625 = 2.165071487426758 + 100.0 * 8.342514038085938
Epoch 2230, val loss: 2.1709883213043213
Epoch 2240, training loss: 836.3703002929688 = 2.1650426387786865 + 100.0 * 8.342052459716797
Epoch 2240, val loss: 2.1709704399108887
Epoch 2250, training loss: 836.3931884765625 = 2.165012001991272 + 100.0 * 8.342281341552734
Epoch 2250, val loss: 2.1709537506103516
Epoch 2260, training loss: 836.5196533203125 = 2.1649765968322754 + 100.0 * 8.343546867370605
Epoch 2260, val loss: 2.170931339263916
Epoch 2270, training loss: 836.3917236328125 = 2.1649465560913086 + 100.0 * 8.342267990112305
Epoch 2270, val loss: 2.1709136962890625
Epoch 2280, training loss: 836.4263305664062 = 2.164908766746521 + 100.0 * 8.34261417388916
Epoch 2280, val loss: 2.1708896160125732
Epoch 2290, training loss: 836.2391357421875 = 2.164882183074951 + 100.0 * 8.340743064880371
Epoch 2290, val loss: 2.1708731651306152
Epoch 2300, training loss: 836.2421875 = 2.1648541688919067 + 100.0 * 8.340773582458496
Epoch 2300, val loss: 2.1708579063415527
Epoch 2310, training loss: 836.1922607421875 = 2.164824962615967 + 100.0 * 8.340274810791016
Epoch 2310, val loss: 2.170840263366699
Epoch 2320, training loss: 836.2941284179688 = 2.1647974252700806 + 100.0 * 8.341293334960938
Epoch 2320, val loss: 2.1708247661590576
Epoch 2330, training loss: 836.20166015625 = 2.164755344390869 + 100.0 * 8.34036922454834
Epoch 2330, val loss: 2.1707944869995117
Epoch 2340, training loss: 836.1719970703125 = 2.1647262573242188 + 100.0 * 8.340072631835938
Epoch 2340, val loss: 2.170778751373291
Epoch 2350, training loss: 836.1375732421875 = 2.1646958589553833 + 100.0 * 8.339729309082031
Epoch 2350, val loss: 2.170759916305542
Epoch 2360, training loss: 836.1246337890625 = 2.164668560028076 + 100.0 * 8.339599609375
Epoch 2360, val loss: 2.1707444190979004
Epoch 2370, training loss: 836.39111328125 = 2.164634108543396 + 100.0 * 8.342265129089355
Epoch 2370, val loss: 2.170722246170044
Epoch 2380, training loss: 836.1315307617188 = 2.1646082401275635 + 100.0 * 8.339669227600098
Epoch 2380, val loss: 2.170706272125244
Epoch 2390, training loss: 836.0195922851562 = 2.1645781993865967 + 100.0 * 8.338550567626953
Epoch 2390, val loss: 2.1706881523132324
Epoch 2400, training loss: 835.9683837890625 = 2.1645519733428955 + 100.0 * 8.338038444519043
Epoch 2400, val loss: 2.1706724166870117
Epoch 2410, training loss: 835.9857788085938 = 2.164527416229248 + 100.0 * 8.338212013244629
Epoch 2410, val loss: 2.170658588409424
Epoch 2420, training loss: 836.3887939453125 = 2.164495348930359 + 100.0 * 8.342243194580078
Epoch 2420, val loss: 2.1706366539001465
Epoch 2430, training loss: 836.1064453125 = 2.164458394050598 + 100.0 * 8.339420318603516
Epoch 2430, val loss: 2.17061185836792
Epoch 2440, training loss: 836.0142211914062 = 2.1644307374954224 + 100.0 * 8.33849811553955
Epoch 2440, val loss: 2.1705942153930664
Epoch 2450, training loss: 835.8768310546875 = 2.164401888847351 + 100.0 * 8.33712387084961
Epoch 2450, val loss: 2.170577049255371
Epoch 2460, training loss: 835.8790283203125 = 2.1643779277801514 + 100.0 * 8.337146759033203
Epoch 2460, val loss: 2.170562267303467
Epoch 2470, training loss: 835.9813842773438 = 2.1643489599227905 + 100.0 * 8.338170051574707
Epoch 2470, val loss: 2.170543909072876
Epoch 2480, training loss: 835.8690795898438 = 2.16431987285614 + 100.0 * 8.337047576904297
Epoch 2480, val loss: 2.170525550842285
Epoch 2490, training loss: 835.9388427734375 = 2.164290428161621 + 100.0 * 8.337745666503906
Epoch 2490, val loss: 2.1705055236816406
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.39666666666666667
0.8648119973918714
=== training gcn model ===
Epoch 0, training loss: 1060.396728515625 = 2.1760566234588623 + 100.0 * 10.582206726074219
Epoch 0, val loss: 2.1749281883239746
Epoch 10, training loss: 1060.3216552734375 = 2.1709752082824707 + 100.0 * 10.58150577545166
Epoch 10, val loss: 2.1705591678619385
Epoch 20, training loss: 1060.0186767578125 = 2.170375347137451 + 100.0 * 10.578482627868652
Epoch 20, val loss: 2.1702585220336914
Epoch 30, training loss: 1058.660400390625 = 2.1728988885879517 + 100.0 * 10.564875602722168
Epoch 30, val loss: 2.17279314994812
Epoch 40, training loss: 1053.0096435546875 = 2.1771280765533447 + 100.0 * 10.50832462310791
Epoch 40, val loss: 2.176917552947998
Epoch 50, training loss: 1033.103759765625 = 2.18163526058197 + 100.0 * 10.309221267700195
Epoch 50, val loss: 2.181415557861328
Epoch 60, training loss: 990.5357055664062 = 2.186362862586975 + 100.0 * 9.883493423461914
Epoch 60, val loss: 2.186143398284912
Epoch 70, training loss: 962.1065063476562 = 2.1882972717285156 + 100.0 * 9.59918212890625
Epoch 70, val loss: 2.187762975692749
Epoch 80, training loss: 949.2781982421875 = 2.1863430738449097 + 100.0 * 9.470918655395508
Epoch 80, val loss: 2.185751438140869
Epoch 90, training loss: 937.8980712890625 = 2.182863235473633 + 100.0 * 9.357151985168457
Epoch 90, val loss: 2.182340145111084
Epoch 100, training loss: 928.4417724609375 = 2.1799877882003784 + 100.0 * 9.262618064880371
Epoch 100, val loss: 2.1796774864196777
Epoch 110, training loss: 923.49365234375 = 2.1782952547073364 + 100.0 * 9.213153839111328
Epoch 110, val loss: 2.1780097484588623
Epoch 120, training loss: 918.3311157226562 = 2.17721164226532 + 100.0 * 9.161539077758789
Epoch 120, val loss: 2.1769943237304688
Epoch 130, training loss: 908.793212890625 = 2.1763522624969482 + 100.0 * 9.066168785095215
Epoch 130, val loss: 2.176154136657715
Epoch 140, training loss: 897.2811279296875 = 2.1758298873901367 + 100.0 * 8.95105266571045
Epoch 140, val loss: 2.175663471221924
Epoch 150, training loss: 889.7244873046875 = 2.1758660078048706 + 100.0 * 8.875486373901367
Epoch 150, val loss: 2.175637722015381
Epoch 160, training loss: 884.7083129882812 = 2.175825595855713 + 100.0 * 8.825325012207031
Epoch 160, val loss: 2.1755871772766113
Epoch 170, training loss: 880.7474975585938 = 2.1754337549209595 + 100.0 * 8.785720825195312
Epoch 170, val loss: 2.1751797199249268
Epoch 180, training loss: 876.7759399414062 = 2.1752214431762695 + 100.0 * 8.746006965637207
Epoch 180, val loss: 2.174898386001587
Epoch 190, training loss: 873.1802368164062 = 2.1749913692474365 + 100.0 * 8.710052490234375
Epoch 190, val loss: 2.1747524738311768
Epoch 200, training loss: 870.17041015625 = 2.1747097969055176 + 100.0 * 8.679957389831543
Epoch 200, val loss: 2.1744205951690674
Epoch 210, training loss: 867.3291625976562 = 2.174302339553833 + 100.0 * 8.651548385620117
Epoch 210, val loss: 2.174015998840332
Epoch 220, training loss: 865.1660766601562 = 2.1739020347595215 + 100.0 * 8.629921913146973
Epoch 220, val loss: 2.1736042499542236
Epoch 230, training loss: 863.4807739257812 = 2.1734771728515625 + 100.0 * 8.613073348999023
Epoch 230, val loss: 2.1731536388397217
Epoch 240, training loss: 862.2495727539062 = 2.172991156578064 + 100.0 * 8.6007661819458
Epoch 240, val loss: 2.1726531982421875
Epoch 250, training loss: 861.2568359375 = 2.172470211982727 + 100.0 * 8.590843200683594
Epoch 250, val loss: 2.172151565551758
Epoch 260, training loss: 860.5275268554688 = 2.172012209892273 + 100.0 * 8.583555221557617
Epoch 260, val loss: 2.171659231185913
Epoch 270, training loss: 859.588134765625 = 2.171530246734619 + 100.0 * 8.574166297912598
Epoch 270, val loss: 2.1711819171905518
Epoch 280, training loss: 858.8412475585938 = 2.171060562133789 + 100.0 * 8.566701889038086
Epoch 280, val loss: 2.170722484588623
Epoch 290, training loss: 858.0523681640625 = 2.1706255674362183 + 100.0 * 8.558816909790039
Epoch 290, val loss: 2.170261859893799
Epoch 300, training loss: 857.3682861328125 = 2.1701526641845703 + 100.0 * 8.551980972290039
Epoch 300, val loss: 2.1698384284973145
Epoch 310, training loss: 856.616455078125 = 2.169709801673889 + 100.0 * 8.544466972351074
Epoch 310, val loss: 2.169358968734741
Epoch 320, training loss: 855.9029541015625 = 2.169287323951721 + 100.0 * 8.537336349487305
Epoch 320, val loss: 2.1689376831054688
Epoch 330, training loss: 855.4319458007812 = 2.1688703298568726 + 100.0 * 8.532630920410156
Epoch 330, val loss: 2.168485641479492
Epoch 340, training loss: 854.6799926757812 = 2.16841983795166 + 100.0 * 8.525115966796875
Epoch 340, val loss: 2.168043375015259
Epoch 350, training loss: 854.0720825195312 = 2.167975664138794 + 100.0 * 8.519041061401367
Epoch 350, val loss: 2.1676273345947266
Epoch 360, training loss: 853.5840454101562 = 2.1675312519073486 + 100.0 * 8.514164924621582
Epoch 360, val loss: 2.1672134399414062
Epoch 370, training loss: 852.9649047851562 = 2.167156219482422 + 100.0 * 8.507977485656738
Epoch 370, val loss: 2.16676664352417
Epoch 380, training loss: 852.3653564453125 = 2.166702628135681 + 100.0 * 8.501986503601074
Epoch 380, val loss: 2.1663718223571777
Epoch 390, training loss: 851.9560546875 = 2.1662986278533936 + 100.0 * 8.497897148132324
Epoch 390, val loss: 2.1659655570983887
Epoch 400, training loss: 851.51318359375 = 2.1658843755722046 + 100.0 * 8.493473052978516
Epoch 400, val loss: 2.1655421257019043
Epoch 410, training loss: 850.996337890625 = 2.1654661893844604 + 100.0 * 8.488308906555176
Epoch 410, val loss: 2.165139675140381
Epoch 420, training loss: 850.7794189453125 = 2.1650876998901367 + 100.0 * 8.486143112182617
Epoch 420, val loss: 2.164727210998535
Epoch 430, training loss: 850.2161865234375 = 2.1646236181259155 + 100.0 * 8.480515480041504
Epoch 430, val loss: 2.164332389831543
Epoch 440, training loss: 849.7861938476562 = 2.1642403602600098 + 100.0 * 8.476219177246094
Epoch 440, val loss: 2.163933515548706
Epoch 450, training loss: 849.3956909179688 = 2.1638394594192505 + 100.0 * 8.472318649291992
Epoch 450, val loss: 2.1635279655456543
Epoch 460, training loss: 849.479248046875 = 2.1634421348571777 + 100.0 * 8.47315788269043
Epoch 460, val loss: 2.1631250381469727
Epoch 470, training loss: 848.7638549804688 = 2.1630090475082397 + 100.0 * 8.466008186340332
Epoch 470, val loss: 2.1627087593078613
Epoch 480, training loss: 848.4906005859375 = 2.1626131534576416 + 100.0 * 8.463279724121094
Epoch 480, val loss: 2.162313461303711
Epoch 490, training loss: 848.1895141601562 = 2.162213087081909 + 100.0 * 8.460272789001465
Epoch 490, val loss: 2.161912202835083
Epoch 500, training loss: 848.3729248046875 = 2.1618491411209106 + 100.0 * 8.46211051940918
Epoch 500, val loss: 2.161497116088867
Epoch 510, training loss: 847.6884765625 = 2.161392569541931 + 100.0 * 8.455270767211914
Epoch 510, val loss: 2.161093235015869
Epoch 520, training loss: 847.380859375 = 2.1609798669815063 + 100.0 * 8.45219898223877
Epoch 520, val loss: 2.1607069969177246
Epoch 530, training loss: 847.1114501953125 = 2.1605782508850098 + 100.0 * 8.449508666992188
Epoch 530, val loss: 2.1603217124938965
Epoch 540, training loss: 847.0820922851562 = 2.1602033376693726 + 100.0 * 8.44921875
Epoch 540, val loss: 2.159926414489746
Epoch 550, training loss: 846.7181396484375 = 2.159779906272888 + 100.0 * 8.44558334350586
Epoch 550, val loss: 2.1595354080200195
Epoch 560, training loss: 846.4630126953125 = 2.159411668777466 + 100.0 * 8.443036079406738
Epoch 560, val loss: 2.159149169921875
Epoch 570, training loss: 846.1707153320312 = 2.1590181589126587 + 100.0 * 8.440116882324219
Epoch 570, val loss: 2.1587677001953125
Epoch 580, training loss: 846.0693969726562 = 2.1586239337921143 + 100.0 * 8.439107894897461
Epoch 580, val loss: 2.1583895683288574
Epoch 590, training loss: 845.8538818359375 = 2.1582536697387695 + 100.0 * 8.436956405639648
Epoch 590, val loss: 2.1579887866973877
Epoch 600, training loss: 845.604736328125 = 2.1578261852264404 + 100.0 * 8.434469223022461
Epoch 600, val loss: 2.1576240062713623
Epoch 610, training loss: 845.4193115234375 = 2.1574572324752808 + 100.0 * 8.432618141174316
Epoch 610, val loss: 2.1572413444519043
Epoch 620, training loss: 845.3634643554688 = 2.1570504903793335 + 100.0 * 8.432064056396484
Epoch 620, val loss: 2.156848430633545
Epoch 630, training loss: 845.103515625 = 2.156654119491577 + 100.0 * 8.429468154907227
Epoch 630, val loss: 2.1564691066741943
Epoch 640, training loss: 844.8899536132812 = 2.156284213066101 + 100.0 * 8.427336692810059
Epoch 640, val loss: 2.1560885906219482
Epoch 650, training loss: 844.83056640625 = 2.1559300422668457 + 100.0 * 8.426746368408203
Epoch 650, val loss: 2.155698299407959
Epoch 660, training loss: 844.6417236328125 = 2.155494213104248 + 100.0 * 8.424861907958984
Epoch 660, val loss: 2.1553292274475098
Epoch 670, training loss: 844.4969482421875 = 2.1551443338394165 + 100.0 * 8.423418045043945
Epoch 670, val loss: 2.154939651489258
Epoch 680, training loss: 844.33056640625 = 2.1547545194625854 + 100.0 * 8.421757698059082
Epoch 680, val loss: 2.1545701026916504
Epoch 690, training loss: 844.187255859375 = 2.1543662548065186 + 100.0 * 8.420329093933105
Epoch 690, val loss: 2.1542086601257324
Epoch 700, training loss: 844.3598022460938 = 2.153995394706726 + 100.0 * 8.42205810546875
Epoch 700, val loss: 2.153839349746704
Epoch 710, training loss: 844.09423828125 = 2.1536147594451904 + 100.0 * 8.419405937194824
Epoch 710, val loss: 2.153451919555664
Epoch 720, training loss: 843.8869018554688 = 2.1532381772994995 + 100.0 * 8.417336463928223
Epoch 720, val loss: 2.1530871391296387
Epoch 730, training loss: 843.7342529296875 = 2.1528706550598145 + 100.0 * 8.415813446044922
Epoch 730, val loss: 2.1527225971221924
Epoch 740, training loss: 843.7592163085938 = 2.152501344680786 + 100.0 * 8.416067123413086
Epoch 740, val loss: 2.1523613929748535
Epoch 750, training loss: 843.7207641601562 = 2.1521180868148804 + 100.0 * 8.41568660736084
Epoch 750, val loss: 2.1520020961761475
Epoch 760, training loss: 843.4595947265625 = 2.1517528295516968 + 100.0 * 8.413078308105469
Epoch 760, val loss: 2.151632308959961
Epoch 770, training loss: 843.2952270507812 = 2.1514058113098145 + 100.0 * 8.41143798828125
Epoch 770, val loss: 2.151275157928467
Epoch 780, training loss: 843.2109375 = 2.1510506868362427 + 100.0 * 8.410598754882812
Epoch 780, val loss: 2.1509206295013428
Epoch 790, training loss: 843.2313842773438 = 2.1507015228271484 + 100.0 * 8.410806655883789
Epoch 790, val loss: 2.1505661010742188
Epoch 800, training loss: 843.1586303710938 = 2.150311231613159 + 100.0 * 8.410082817077637
Epoch 800, val loss: 2.1502225399017334
Epoch 810, training loss: 843.2488403320312 = 2.149939775466919 + 100.0 * 8.410988807678223
Epoch 810, val loss: 2.149874687194824
Epoch 820, training loss: 842.8297119140625 = 2.1496236324310303 + 100.0 * 8.406801223754883
Epoch 820, val loss: 2.1495065689086914
Epoch 830, training loss: 842.70849609375 = 2.149261951446533 + 100.0 * 8.40559196472168
Epoch 830, val loss: 2.149174213409424
Epoch 840, training loss: 842.6015014648438 = 2.1489239931106567 + 100.0 * 8.404525756835938
Epoch 840, val loss: 2.1488311290740967
Epoch 850, training loss: 842.5006103515625 = 2.148595094680786 + 100.0 * 8.403519630432129
Epoch 850, val loss: 2.1485002040863037
Epoch 860, training loss: 842.8413696289062 = 2.148271918296814 + 100.0 * 8.406930923461914
Epoch 860, val loss: 2.148155450820923
Epoch 870, training loss: 842.5505981445312 = 2.147907853126526 + 100.0 * 8.404026985168457
Epoch 870, val loss: 2.14780855178833
Epoch 880, training loss: 842.3472290039062 = 2.147557497024536 + 100.0 * 8.401996612548828
Epoch 880, val loss: 2.1474852561950684
Epoch 890, training loss: 842.1338500976562 = 2.1472235918045044 + 100.0 * 8.399866104125977
Epoch 890, val loss: 2.1471502780914307
Epoch 900, training loss: 842.0659790039062 = 2.1468942165374756 + 100.0 * 8.399190902709961
Epoch 900, val loss: 2.1468262672424316
Epoch 910, training loss: 842.22412109375 = 2.1465368270874023 + 100.0 * 8.400775909423828
Epoch 910, val loss: 2.1465086936950684
Epoch 920, training loss: 842.171875 = 2.1462098360061646 + 100.0 * 8.400256156921387
Epoch 920, val loss: 2.1461682319641113
Epoch 930, training loss: 841.8648071289062 = 2.145894765853882 + 100.0 * 8.397189140319824
Epoch 930, val loss: 2.145829439163208
Epoch 940, training loss: 841.7088623046875 = 2.145565390586853 + 100.0 * 8.39563274383545
Epoch 940, val loss: 2.1455140113830566
Epoch 950, training loss: 841.7138671875 = 2.1452343463897705 + 100.0 * 8.395686149597168
Epoch 950, val loss: 2.1451990604400635
Epoch 960, training loss: 841.5628051757812 = 2.144914150238037 + 100.0 * 8.394179344177246
Epoch 960, val loss: 2.144864797592163
Epoch 970, training loss: 841.5737915039062 = 2.1445977687835693 + 100.0 * 8.394291877746582
Epoch 970, val loss: 2.144540548324585
Epoch 980, training loss: 841.39599609375 = 2.1442697048187256 + 100.0 * 8.39251708984375
Epoch 980, val loss: 2.1442174911499023
Epoch 990, training loss: 841.4906005859375 = 2.143960952758789 + 100.0 * 8.393465995788574
Epoch 990, val loss: 2.143897533416748
Epoch 1000, training loss: 841.2636108398438 = 2.1436123847961426 + 100.0 * 8.391200065612793
Epoch 1000, val loss: 2.1435837745666504
Epoch 1010, training loss: 841.1522827148438 = 2.1432993412017822 + 100.0 * 8.390089988708496
Epoch 1010, val loss: 2.1432676315307617
Epoch 1020, training loss: 841.221923828125 = 2.143001914024353 + 100.0 * 8.390789031982422
Epoch 1020, val loss: 2.142951011657715
Epoch 1030, training loss: 841.0790405273438 = 2.1426498889923096 + 100.0 * 8.389364242553711
Epoch 1030, val loss: 2.1426265239715576
Epoch 1040, training loss: 841.0057373046875 = 2.142340064048767 + 100.0 * 8.388633728027344
Epoch 1040, val loss: 2.1423158645629883
Epoch 1050, training loss: 840.8616943359375 = 2.1420204639434814 + 100.0 * 8.38719654083252
Epoch 1050, val loss: 2.142009973526001
Epoch 1060, training loss: 840.8131103515625 = 2.141707181930542 + 100.0 * 8.386713981628418
Epoch 1060, val loss: 2.1417150497436523
Epoch 1070, training loss: 841.1925048828125 = 2.1413806676864624 + 100.0 * 8.390511512756348
Epoch 1070, val loss: 2.1414027214050293
Epoch 1080, training loss: 840.6726684570312 = 2.141080379486084 + 100.0 * 8.385315895080566
Epoch 1080, val loss: 2.141083240509033
Epoch 1090, training loss: 840.6246337890625 = 2.1407811641693115 + 100.0 * 8.384838104248047
Epoch 1090, val loss: 2.1407742500305176
Epoch 1100, training loss: 840.56103515625 = 2.1404649019241333 + 100.0 * 8.38420581817627
Epoch 1100, val loss: 2.1404809951782227
Epoch 1110, training loss: 840.6047973632812 = 2.140148162841797 + 100.0 * 8.38464641571045
Epoch 1110, val loss: 2.1401748657226562
Epoch 1120, training loss: 840.3700561523438 = 2.1398597955703735 + 100.0 * 8.382302284240723
Epoch 1120, val loss: 2.1398680210113525
Epoch 1130, training loss: 840.3304443359375 = 2.1395606994628906 + 100.0 * 8.381908416748047
Epoch 1130, val loss: 2.1395773887634277
Epoch 1140, training loss: 840.5359497070312 = 2.1392462253570557 + 100.0 * 8.383967399597168
Epoch 1140, val loss: 2.1392927169799805
Epoch 1150, training loss: 840.342529296875 = 2.1389507055282593 + 100.0 * 8.382035255432129
Epoch 1150, val loss: 2.1389617919921875
Epoch 1160, training loss: 840.2080078125 = 2.1386446952819824 + 100.0 * 8.380693435668945
Epoch 1160, val loss: 2.1386778354644775
Epoch 1170, training loss: 840.0502319335938 = 2.1383577585220337 + 100.0 * 8.379118919372559
Epoch 1170, val loss: 2.138388156890869
Epoch 1180, training loss: 839.9904174804688 = 2.138075351715088 + 100.0 * 8.378523826599121
Epoch 1180, val loss: 2.1381027698516846
Epoch 1190, training loss: 839.9364013671875 = 2.1377872228622437 + 100.0 * 8.377985954284668
Epoch 1190, val loss: 2.137817859649658
Epoch 1200, training loss: 840.3899536132812 = 2.1375123262405396 + 100.0 * 8.382524490356445
Epoch 1200, val loss: 2.137521743774414
Epoch 1210, training loss: 840.0469360351562 = 2.1371623277664185 + 100.0 * 8.379097938537598
Epoch 1210, val loss: 2.137225866317749
Epoch 1220, training loss: 839.7869262695312 = 2.136890411376953 + 100.0 * 8.376500129699707
Epoch 1220, val loss: 2.1369428634643555
Epoch 1230, training loss: 839.7236938476562 = 2.1366069316864014 + 100.0 * 8.375870704650879
Epoch 1230, val loss: 2.136657476425171
Epoch 1240, training loss: 839.6995239257812 = 2.1363162994384766 + 100.0 * 8.375632286071777
Epoch 1240, val loss: 2.1363892555236816
Epoch 1250, training loss: 839.8121337890625 = 2.1360068321228027 + 100.0 * 8.376761436462402
Epoch 1250, val loss: 2.1360924243927
Epoch 1260, training loss: 839.5479125976562 = 2.1357501745224 + 100.0 * 8.37412166595459
Epoch 1260, val loss: 2.1358091831207275
Epoch 1270, training loss: 839.4812622070312 = 2.135468363761902 + 100.0 * 8.373457908630371
Epoch 1270, val loss: 2.1355276107788086
Epoch 1280, training loss: 839.5352172851562 = 2.1351850032806396 + 100.0 * 8.374000549316406
Epoch 1280, val loss: 2.1352639198303223
Epoch 1290, training loss: 839.4385375976562 = 2.1349068880081177 + 100.0 * 8.37303638458252
Epoch 1290, val loss: 2.134963035583496
Epoch 1300, training loss: 839.319091796875 = 2.134614944458008 + 100.0 * 8.371844291687012
Epoch 1300, val loss: 2.134697914123535
Epoch 1310, training loss: 839.2802124023438 = 2.1343528032302856 + 100.0 * 8.371459007263184
Epoch 1310, val loss: 2.1344213485717773
Epoch 1320, training loss: 839.5792846679688 = 2.1340960264205933 + 100.0 * 8.374451637268066
Epoch 1320, val loss: 2.1341443061828613
Epoch 1330, training loss: 839.3145751953125 = 2.1337770223617554 + 100.0 * 8.371808052062988
Epoch 1330, val loss: 2.133878707885742
Epoch 1340, training loss: 839.1614990234375 = 2.1335246562957764 + 100.0 * 8.370279312133789
Epoch 1340, val loss: 2.1336028575897217
Epoch 1350, training loss: 839.0714111328125 = 2.1332414150238037 + 100.0 * 8.36938190460205
Epoch 1350, val loss: 2.1333470344543457
Epoch 1360, training loss: 839.0992431640625 = 2.132975459098816 + 100.0 * 8.369662284851074
Epoch 1360, val loss: 2.133085250854492
Epoch 1370, training loss: 839.1511840820312 = 2.1326780319213867 + 100.0 * 8.370184898376465
Epoch 1370, val loss: 2.132805347442627
Epoch 1380, training loss: 838.955810546875 = 2.1324363946914673 + 100.0 * 8.368233680725098
Epoch 1380, val loss: 2.1325325965881348
Epoch 1390, training loss: 838.8840942382812 = 2.1321576833724976 + 100.0 * 8.36751937866211
Epoch 1390, val loss: 2.132272720336914
Epoch 1400, training loss: 838.8384399414062 = 2.131907343864441 + 100.0 * 8.3670654296875
Epoch 1400, val loss: 2.1320128440856934
Epoch 1410, training loss: 839.2103271484375 = 2.1316529512405396 + 100.0 * 8.370786666870117
Epoch 1410, val loss: 2.131742000579834
Epoch 1420, training loss: 838.9439697265625 = 2.131334900856018 + 100.0 * 8.368125915527344
Epoch 1420, val loss: 2.1314845085144043
Epoch 1430, training loss: 838.7620849609375 = 2.1310821771621704 + 100.0 * 8.366310119628906
Epoch 1430, val loss: 2.131209373474121
Epoch 1440, training loss: 838.7559204101562 = 2.130810499191284 + 100.0 * 8.366250991821289
Epoch 1440, val loss: 2.1309561729431152
Epoch 1450, training loss: 838.69873046875 = 2.1305506229400635 + 100.0 * 8.365681648254395
Epoch 1450, val loss: 2.1306920051574707
Epoch 1460, training loss: 838.6049194335938 = 2.130303740501404 + 100.0 * 8.36474609375
Epoch 1460, val loss: 2.1304354667663574
Epoch 1470, training loss: 838.5372314453125 = 2.1300450563430786 + 100.0 * 8.3640718460083
Epoch 1470, val loss: 2.130185127258301
Epoch 1480, training loss: 838.604248046875 = 2.129793643951416 + 100.0 * 8.364744186401367
Epoch 1480, val loss: 2.129930019378662
Epoch 1490, training loss: 838.7351684570312 = 2.129544496536255 + 100.0 * 8.366056442260742
Epoch 1490, val loss: 2.1296567916870117
Epoch 1500, training loss: 838.60400390625 = 2.129229426383972 + 100.0 * 8.364748001098633
Epoch 1500, val loss: 2.1293911933898926
Epoch 1510, training loss: 838.3953857421875 = 2.1289883852005005 + 100.0 * 8.362664222717285
Epoch 1510, val loss: 2.1291427612304688
Epoch 1520, training loss: 838.365966796875 = 2.128743886947632 + 100.0 * 8.362372398376465
Epoch 1520, val loss: 2.1289005279541016
Epoch 1530, training loss: 838.3016357421875 = 2.12849497795105 + 100.0 * 8.36173152923584
Epoch 1530, val loss: 2.128659725189209
Epoch 1540, training loss: 838.2818603515625 = 2.1282516717910767 + 100.0 * 8.361536026000977
Epoch 1540, val loss: 2.128418445587158
Epoch 1550, training loss: 838.7230224609375 = 2.127997875213623 + 100.0 * 8.365950584411621
Epoch 1550, val loss: 2.128174304962158
Epoch 1560, training loss: 838.3880615234375 = 2.1277343034744263 + 100.0 * 8.362603187561035
Epoch 1560, val loss: 2.1279051303863525
Epoch 1570, training loss: 838.2342529296875 = 2.1274983882904053 + 100.0 * 8.361067771911621
Epoch 1570, val loss: 2.127659320831299
Epoch 1580, training loss: 838.3441162109375 = 2.1272659301757812 + 100.0 * 8.362168312072754
Epoch 1580, val loss: 2.1274094581604004
Epoch 1590, training loss: 838.137939453125 = 2.1269880533218384 + 100.0 * 8.360109329223633
Epoch 1590, val loss: 2.127171039581299
Epoch 1600, training loss: 838.19482421875 = 2.1267534494400024 + 100.0 * 8.36068058013916
Epoch 1600, val loss: 2.1269280910491943
Epoch 1610, training loss: 838.123046875 = 2.1265175342559814 + 100.0 * 8.359965324401855
Epoch 1610, val loss: 2.1266841888427734
Epoch 1620, training loss: 838.032958984375 = 2.1262513399124146 + 100.0 * 8.3590669631958
Epoch 1620, val loss: 2.1264595985412598
Epoch 1630, training loss: 837.9561767578125 = 2.1260321140289307 + 100.0 * 8.358301162719727
Epoch 1630, val loss: 2.1262221336364746
Epoch 1640, training loss: 838.07275390625 = 2.125800848007202 + 100.0 * 8.359469413757324
Epoch 1640, val loss: 2.1259894371032715
Epoch 1650, training loss: 838.0374755859375 = 2.1255249977111816 + 100.0 * 8.359119415283203
Epoch 1650, val loss: 2.125749349594116
Epoch 1660, training loss: 837.8773193359375 = 2.1253063678741455 + 100.0 * 8.35752010345459
Epoch 1660, val loss: 2.1255035400390625
Epoch 1670, training loss: 837.843017578125 = 2.1250847578048706 + 100.0 * 8.357179641723633
Epoch 1670, val loss: 2.1252779960632324
Epoch 1680, training loss: 837.8211669921875 = 2.124847888946533 + 100.0 * 8.356963157653809
Epoch 1680, val loss: 2.125056743621826
Epoch 1690, training loss: 838.4051513671875 = 2.124627113342285 + 100.0 * 8.362805366516113
Epoch 1690, val loss: 2.12481689453125
Epoch 1700, training loss: 837.9462280273438 = 2.1243566274642944 + 100.0 * 8.358219146728516
Epoch 1700, val loss: 2.124575138092041
Epoch 1710, training loss: 837.7252807617188 = 2.1241352558135986 + 100.0 * 8.356011390686035
Epoch 1710, val loss: 2.124351978302002
Epoch 1720, training loss: 837.6679077148438 = 2.1239086389541626 + 100.0 * 8.355440139770508
Epoch 1720, val loss: 2.1241331100463867
Epoch 1730, training loss: 837.69091796875 = 2.12368106842041 + 100.0 * 8.355672836303711
Epoch 1730, val loss: 2.1239256858825684
Epoch 1740, training loss: 838.062744140625 = 2.1234241724014282 + 100.0 * 8.359393119812012
Epoch 1740, val loss: 2.1236934661865234
Epoch 1750, training loss: 837.6912841796875 = 2.123242735862732 + 100.0 * 8.355680465698242
Epoch 1750, val loss: 2.123445510864258
Epoch 1760, training loss: 837.5662841796875 = 2.1229969263076782 + 100.0 * 8.354433059692383
Epoch 1760, val loss: 2.123236656188965
Epoch 1770, training loss: 837.5165405273438 = 2.122794032096863 + 100.0 * 8.353937149047852
Epoch 1770, val loss: 2.123020648956299
Epoch 1780, training loss: 837.4959716796875 = 2.122573733329773 + 100.0 * 8.353734016418457
Epoch 1780, val loss: 2.1228153705596924
Epoch 1790, training loss: 837.9237670898438 = 2.122362494468689 + 100.0 * 8.358014106750488
Epoch 1790, val loss: 2.1225996017456055
Epoch 1800, training loss: 837.7549438476562 = 2.1221102476119995 + 100.0 * 8.356328010559082
Epoch 1800, val loss: 2.1223554611206055
Epoch 1810, training loss: 837.4319458007812 = 2.121900796890259 + 100.0 * 8.353100776672363
Epoch 1810, val loss: 2.1221392154693604
Epoch 1820, training loss: 837.3654174804688 = 2.1216893196105957 + 100.0 * 8.352437019348145
Epoch 1820, val loss: 2.121938467025757
Epoch 1830, training loss: 837.3462524414062 = 2.121482729911804 + 100.0 * 8.352248191833496
Epoch 1830, val loss: 2.121732711791992
Epoch 1840, training loss: 837.3167114257812 = 2.1212793588638306 + 100.0 * 8.351954460144043
Epoch 1840, val loss: 2.121530771255493
Epoch 1850, training loss: 837.5104370117188 = 2.1210721731185913 + 100.0 * 8.353893280029297
Epoch 1850, val loss: 2.1213245391845703
Epoch 1860, training loss: 837.4721069335938 = 2.120844602584839 + 100.0 * 8.35351276397705
Epoch 1860, val loss: 2.1210851669311523
Epoch 1870, training loss: 837.3314819335938 = 2.120611786842346 + 100.0 * 8.3521089553833
Epoch 1870, val loss: 2.1208724975585938
Epoch 1880, training loss: 837.2575073242188 = 2.120408296585083 + 100.0 * 8.351370811462402
Epoch 1880, val loss: 2.1206655502319336
Epoch 1890, training loss: 837.1776123046875 = 2.1202073097229004 + 100.0 * 8.350574493408203
Epoch 1890, val loss: 2.1204686164855957
Epoch 1900, training loss: 837.1531982421875 = 2.120018720626831 + 100.0 * 8.350332260131836
Epoch 1900, val loss: 2.120272159576416
Epoch 1910, training loss: 837.540771484375 = 2.119846224784851 + 100.0 * 8.354208946228027
Epoch 1910, val loss: 2.120056629180908
Epoch 1920, training loss: 837.3350219726562 = 2.1195603609085083 + 100.0 * 8.352154731750488
Epoch 1920, val loss: 2.119866371154785
Epoch 1930, training loss: 837.16064453125 = 2.119387149810791 + 100.0 * 8.350412368774414
Epoch 1930, val loss: 2.119636058807373
Epoch 1940, training loss: 837.0538940429688 = 2.119171380996704 + 100.0 * 8.349347114562988
Epoch 1940, val loss: 2.1194543838500977
Epoch 1950, training loss: 837.0181884765625 = 2.118989586830139 + 100.0 * 8.348991394042969
Epoch 1950, val loss: 2.1192636489868164
Epoch 1960, training loss: 837.0621337890625 = 2.1187920570373535 + 100.0 * 8.349433898925781
Epoch 1960, val loss: 2.1190710067749023
Epoch 1970, training loss: 837.0985107421875 = 2.118587613105774 + 100.0 * 8.349799156188965
Epoch 1970, val loss: 2.1188712120056152
Epoch 1980, training loss: 837.0324096679688 = 2.1183873414993286 + 100.0 * 8.349140167236328
Epoch 1980, val loss: 2.1186795234680176
Epoch 1990, training loss: 837.0270385742188 = 2.1181834936141968 + 100.0 * 8.349088668823242
Epoch 1990, val loss: 2.1184778213500977
Epoch 2000, training loss: 836.8939208984375 = 2.1179949045181274 + 100.0 * 8.347759246826172
Epoch 2000, val loss: 2.1182847023010254
Epoch 2010, training loss: 836.8695068359375 = 2.1178148984909058 + 100.0 * 8.347517013549805
Epoch 2010, val loss: 2.118098258972168
Epoch 2020, training loss: 836.8611450195312 = 2.1176215410232544 + 100.0 * 8.347434997558594
Epoch 2020, val loss: 2.1179118156433105
Epoch 2030, training loss: 836.9771728515625 = 2.117427349090576 + 100.0 * 8.348597526550293
Epoch 2030, val loss: 2.1177196502685547
Epoch 2040, training loss: 837.1597290039062 = 2.117223858833313 + 100.0 * 8.350424766540527
Epoch 2040, val loss: 2.1175241470336914
Epoch 2050, training loss: 836.8439331054688 = 2.11700975894928 + 100.0 * 8.347269058227539
Epoch 2050, val loss: 2.1173174381256104
Epoch 2060, training loss: 836.764404296875 = 2.1168293952941895 + 100.0 * 8.346475601196289
Epoch 2060, val loss: 2.117133617401123
Epoch 2070, training loss: 836.707275390625 = 2.1166480779647827 + 100.0 * 8.345906257629395
Epoch 2070, val loss: 2.116953134536743
Epoch 2080, training loss: 836.6859130859375 = 2.116475462913513 + 100.0 * 8.345694541931152
Epoch 2080, val loss: 2.116774082183838
Epoch 2090, training loss: 837.1416625976562 = 2.116312861442566 + 100.0 * 8.350253105163574
Epoch 2090, val loss: 2.1165719032287598
Epoch 2100, training loss: 836.8265991210938 = 2.1160460710525513 + 100.0 * 8.347105979919434
Epoch 2100, val loss: 2.116382122039795
Epoch 2110, training loss: 836.7241821289062 = 2.115884780883789 + 100.0 * 8.34608268737793
Epoch 2110, val loss: 2.116182565689087
Epoch 2120, training loss: 836.5958251953125 = 2.1156787872314453 + 100.0 * 8.344801902770996
Epoch 2120, val loss: 2.1160120964050293
Epoch 2130, training loss: 836.5436401367188 = 2.115514397621155 + 100.0 * 8.344281196594238
Epoch 2130, val loss: 2.115830659866333
Epoch 2140, training loss: 836.52001953125 = 2.115327835083008 + 100.0 * 8.344046592712402
Epoch 2140, val loss: 2.115658760070801
Epoch 2150, training loss: 836.7930297851562 = 2.115147352218628 + 100.0 * 8.346778869628906
Epoch 2150, val loss: 2.115480422973633
Epoch 2160, training loss: 836.596923828125 = 2.114930748939514 + 100.0 * 8.344820022583008
Epoch 2160, val loss: 2.1152586936950684
Epoch 2170, training loss: 836.5914306640625 = 2.1147468090057373 + 100.0 * 8.344766616821289
Epoch 2170, val loss: 2.115072727203369
Epoch 2180, training loss: 836.444091796875 = 2.1145596504211426 + 100.0 * 8.343295097351074
Epoch 2180, val loss: 2.114896059036255
Epoch 2190, training loss: 836.4090576171875 = 2.1143826246261597 + 100.0 * 8.342947006225586
Epoch 2190, val loss: 2.1147215366363525
Epoch 2200, training loss: 836.4303588867188 = 2.1142029762268066 + 100.0 * 8.343161582946777
Epoch 2200, val loss: 2.1145472526550293
Epoch 2210, training loss: 836.6685791015625 = 2.11401104927063 + 100.0 * 8.345545768737793
Epoch 2210, val loss: 2.1143617630004883
Epoch 2220, training loss: 836.4844360351562 = 2.1138269901275635 + 100.0 * 8.343706130981445
Epoch 2220, val loss: 2.1141676902770996
Epoch 2230, training loss: 836.4347534179688 = 2.113647699356079 + 100.0 * 8.34321117401123
Epoch 2230, val loss: 2.1139774322509766
Epoch 2240, training loss: 836.4758911132812 = 2.113456606864929 + 100.0 * 8.343624114990234
Epoch 2240, val loss: 2.11379337310791
Epoch 2250, training loss: 836.3477783203125 = 2.1132534742355347 + 100.0 * 8.342345237731934
Epoch 2250, val loss: 2.1136274337768555
Epoch 2260, training loss: 836.2766723632812 = 2.1130768060684204 + 100.0 * 8.341635704040527
Epoch 2260, val loss: 2.113443374633789
Epoch 2270, training loss: 836.2817993164062 = 2.1129039525985718 + 100.0 * 8.341689109802246
Epoch 2270, val loss: 2.1132664680480957
Epoch 2280, training loss: 836.3557739257812 = 2.1127136945724487 + 100.0 * 8.342430114746094
Epoch 2280, val loss: 2.1130928993225098
Epoch 2290, training loss: 836.2735595703125 = 2.112518310546875 + 100.0 * 8.3416109085083
Epoch 2290, val loss: 2.112912893295288
Epoch 2300, training loss: 836.2780151367188 = 2.1123383045196533 + 100.0 * 8.341656684875488
Epoch 2300, val loss: 2.1127285957336426
Epoch 2310, training loss: 836.1730346679688 = 2.112172484397888 + 100.0 * 8.340608596801758
Epoch 2310, val loss: 2.112542152404785
Epoch 2320, training loss: 836.2638549804688 = 2.111998438835144 + 100.0 * 8.34151840209961
Epoch 2320, val loss: 2.1123690605163574
Epoch 2330, training loss: 836.3145141601562 = 2.1117799282073975 + 100.0 * 8.34202766418457
Epoch 2330, val loss: 2.112169027328491
Epoch 2340, training loss: 836.1395263671875 = 2.1115920543670654 + 100.0 * 8.340279579162598
Epoch 2340, val loss: 2.111985683441162
Epoch 2350, training loss: 836.0905151367188 = 2.1114190816879272 + 100.0 * 8.339791297912598
Epoch 2350, val loss: 2.1118111610412598
Epoch 2360, training loss: 836.0582885742188 = 2.1112449169158936 + 100.0 * 8.339469909667969
Epoch 2360, val loss: 2.111645221710205
Epoch 2370, training loss: 836.0303955078125 = 2.1110743284225464 + 100.0 * 8.339193344116211
Epoch 2370, val loss: 2.1114773750305176
Epoch 2380, training loss: 836.11865234375 = 2.1108953952789307 + 100.0 * 8.34007740020752
Epoch 2380, val loss: 2.1113100051879883
Epoch 2390, training loss: 836.2657470703125 = 2.110661268234253 + 100.0 * 8.341550827026367
Epoch 2390, val loss: 2.111097812652588
Epoch 2400, training loss: 836.0619506835938 = 2.110486149787903 + 100.0 * 8.33951473236084
Epoch 2400, val loss: 2.110903739929199
Epoch 2410, training loss: 836.0142822265625 = 2.1103150844573975 + 100.0 * 8.33903980255127
Epoch 2410, val loss: 2.1107330322265625
Epoch 2420, training loss: 835.9293823242188 = 2.1101396083831787 + 100.0 * 8.338191986083984
Epoch 2420, val loss: 2.1105704307556152
Epoch 2430, training loss: 835.9185180664062 = 2.1099729537963867 + 100.0 * 8.338085174560547
Epoch 2430, val loss: 2.1104087829589844
Epoch 2440, training loss: 835.9075317382812 = 2.109804391860962 + 100.0 * 8.337977409362793
Epoch 2440, val loss: 2.1102466583251953
Epoch 2450, training loss: 836.1240234375 = 2.1096194982528687 + 100.0 * 8.340144157409668
Epoch 2450, val loss: 2.110086441040039
Epoch 2460, training loss: 835.9305419921875 = 2.1094367504119873 + 100.0 * 8.338211059570312
Epoch 2460, val loss: 2.109879970550537
Epoch 2470, training loss: 835.9486694335938 = 2.109261393547058 + 100.0 * 8.338394165039062
Epoch 2470, val loss: 2.1097021102905273
Epoch 2480, training loss: 835.944580078125 = 2.1090924739837646 + 100.0 * 8.33835506439209
Epoch 2480, val loss: 2.109524726867676
Epoch 2490, training loss: 835.8283081054688 = 2.1089011430740356 + 100.0 * 8.337194442749023
Epoch 2490, val loss: 2.109372615814209
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.39666666666666667
0.8653191335216983
The final CL Acc:0.40831, 0.01646, The final GNN Acc:0.86498, 0.00024
Begin epxeriment: noisy_level: 0.2 cont_weight: 100 epoch: der1:0.4 der2:0 dfr1:0.4 dfr2:0.4
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=2500, cl_num_layers=2, cl_num_proj_hidden=64, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=100.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.0, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=32, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, noisy_level=0.2, num_hidden=64, num_proj_hidden=64, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=40, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 88648])
add edge: torch.Size([2, 106264])
remove edge: torch.Size([2, 70812])
updated graph: torch.Size([2, 88428])
=== Noisy graph ===
/home/project-robust-contrastive/Robust-Contrastive-Learning/model.py:227: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 1059.35009765625 = 1.1197221279144287 + 100.0 * 10.582304000854492
Epoch 0, val loss: 1.119221806526184
Epoch 10, training loss: 1059.319580078125 = 1.1168148517608643 + 100.0 * 10.582027435302734
Epoch 10, val loss: 1.1163463592529297
Epoch 20, training loss: 1059.193359375 = 1.113763689994812 + 100.0 * 10.580795288085938
Epoch 20, val loss: 1.1133246421813965
Epoch 30, training loss: 1058.626708984375 = 1.1104499101638794 + 100.0 * 10.575162887573242
Epoch 30, val loss: 1.1100330352783203
Epoch 40, training loss: 1056.3970947265625 = 1.1066452264785767 + 100.0 * 10.55290412902832
Epoch 40, val loss: 1.1062290668487549
Epoch 50, training loss: 1050.1055908203125 = 1.1019399166107178 + 100.0 * 10.490036010742188
Epoch 50, val loss: 1.1015416383743286
Epoch 60, training loss: 1036.214599609375 = 1.0964529514312744 + 100.0 * 10.351181030273438
Epoch 60, val loss: 1.0961285829544067
Epoch 70, training loss: 1013.75537109375 = 1.0902292728424072 + 100.0 * 10.126651763916016
Epoch 70, val loss: 1.089948296546936
Epoch 80, training loss: 987.0616455078125 = 1.0839020013809204 + 100.0 * 9.859777450561523
Epoch 80, val loss: 1.0838463306427002
Epoch 90, training loss: 955.439208984375 = 1.0789735317230225 + 100.0 * 9.543601989746094
Epoch 90, val loss: 1.0791467428207397
Epoch 100, training loss: 938.4295654296875 = 1.0749967098236084 + 100.0 * 9.37354564666748
Epoch 100, val loss: 1.075287938117981
Epoch 110, training loss: 933.5484619140625 = 1.071114420890808 + 100.0 * 9.324773788452148
Epoch 110, val loss: 1.071449875831604
Epoch 120, training loss: 931.2246704101562 = 1.066813588142395 + 100.0 * 9.301578521728516
Epoch 120, val loss: 1.067211627960205
Epoch 130, training loss: 928.3727416992188 = 1.0626814365386963 + 100.0 * 9.273100852966309
Epoch 130, val loss: 1.0632128715515137
Epoch 140, training loss: 924.7221069335938 = 1.0589593648910522 + 100.0 * 9.236631393432617
Epoch 140, val loss: 1.0596181154251099
Epoch 150, training loss: 918.9251098632812 = 1.0558345317840576 + 100.0 * 9.178692817687988
Epoch 150, val loss: 1.0566633939743042
Epoch 160, training loss: 911.2243041992188 = 1.0534918308258057 + 100.0 * 9.10170841217041
Epoch 160, val loss: 1.0544376373291016
Epoch 170, training loss: 906.2791137695312 = 1.0512484312057495 + 100.0 * 9.052278518676758
Epoch 170, val loss: 1.052211880683899
Epoch 180, training loss: 901.7135620117188 = 1.0486220121383667 + 100.0 * 9.006649017333984
Epoch 180, val loss: 1.0495963096618652
Epoch 190, training loss: 896.3336791992188 = 1.046077847480774 + 100.0 * 8.952876091003418
Epoch 190, val loss: 1.0471320152282715
Epoch 200, training loss: 892.849853515625 = 1.0436336994171143 + 100.0 * 8.918062210083008
Epoch 200, val loss: 1.0446991920471191
Epoch 210, training loss: 890.0564575195312 = 1.0408848524093628 + 100.0 * 8.890155792236328
Epoch 210, val loss: 1.0419889688491821
Epoch 220, training loss: 886.655029296875 = 1.038258671760559 + 100.0 * 8.856167793273926
Epoch 220, val loss: 1.0394634008407593
Epoch 230, training loss: 883.4091796875 = 1.0360363721847534 + 100.0 * 8.823731422424316
Epoch 230, val loss: 1.0373239517211914
Epoch 240, training loss: 880.8978881835938 = 1.0337473154067993 + 100.0 * 8.798641204833984
Epoch 240, val loss: 1.0350761413574219
Epoch 250, training loss: 878.9992065429688 = 1.0310683250427246 + 100.0 * 8.779681205749512
Epoch 250, val loss: 1.0324620008468628
Epoch 260, training loss: 877.4368286132812 = 1.028237223625183 + 100.0 * 8.76408576965332
Epoch 260, val loss: 1.0296955108642578
Epoch 270, training loss: 876.1473999023438 = 1.0253409147262573 + 100.0 * 8.751220703125
Epoch 270, val loss: 1.0268768072128296
Epoch 280, training loss: 875.0458984375 = 1.0223060846328735 + 100.0 * 8.740236282348633
Epoch 280, val loss: 1.0239195823669434
Epoch 290, training loss: 873.9664916992188 = 1.0191689729690552 + 100.0 * 8.729473114013672
Epoch 290, val loss: 1.0208600759506226
Epoch 300, training loss: 872.9109497070312 = 1.0159112215042114 + 100.0 * 8.718950271606445
Epoch 300, val loss: 1.0176881551742554
Epoch 310, training loss: 871.9829711914062 = 1.012446641921997 + 100.0 * 8.709705352783203
Epoch 310, val loss: 1.0142905712127686
Epoch 320, training loss: 870.646484375 = 1.0087940692901611 + 100.0 * 8.69637680053711
Epoch 320, val loss: 1.010746955871582
Epoch 330, training loss: 869.6254272460938 = 1.0049527883529663 + 100.0 * 8.68620491027832
Epoch 330, val loss: 1.0069905519485474
Epoch 340, training loss: 868.613525390625 = 1.000879168510437 + 100.0 * 8.676126480102539
Epoch 340, val loss: 1.0030194520950317
Epoch 350, training loss: 867.6364135742188 = 0.9965735077857971 + 100.0 * 8.666398048400879
Epoch 350, val loss: 0.998815655708313
Epoch 360, training loss: 867.013427734375 = 0.9920279383659363 + 100.0 * 8.6602144241333
Epoch 360, val loss: 0.9943769574165344
Epoch 370, training loss: 865.8694458007812 = 0.9872130751609802 + 100.0 * 8.648822784423828
Epoch 370, val loss: 0.989669919013977
Epoch 380, training loss: 864.9273681640625 = 0.982257604598999 + 100.0 * 8.639451026916504
Epoch 380, val loss: 0.9848321676254272
Epoch 390, training loss: 864.0950927734375 = 0.9771352410316467 + 100.0 * 8.631179809570312
Epoch 390, val loss: 0.9798299074172974
Epoch 400, training loss: 863.3905639648438 = 0.9717966914176941 + 100.0 * 8.624187469482422
Epoch 400, val loss: 0.9745922684669495
Epoch 410, training loss: 862.72607421875 = 0.9662330746650696 + 100.0 * 8.617598533630371
Epoch 410, val loss: 0.9691565036773682
Epoch 420, training loss: 862.1083374023438 = 0.9603604674339294 + 100.0 * 8.611479759216309
Epoch 420, val loss: 0.9634088277816772
Epoch 430, training loss: 861.5875244140625 = 0.9541902542114258 + 100.0 * 8.60633373260498
Epoch 430, val loss: 0.9573601484298706
Epoch 440, training loss: 861.177978515625 = 0.9477539658546448 + 100.0 * 8.602302551269531
Epoch 440, val loss: 0.9510427713394165
Epoch 450, training loss: 860.8151245117188 = 0.9409842491149902 + 100.0 * 8.59874153137207
Epoch 450, val loss: 0.9444290399551392
Epoch 460, training loss: 860.3421630859375 = 0.9339746236801147 + 100.0 * 8.59408187866211
Epoch 460, val loss: 0.9375655651092529
Epoch 470, training loss: 859.9891967773438 = 0.9267202019691467 + 100.0 * 8.590624809265137
Epoch 470, val loss: 0.9304596781730652
Epoch 480, training loss: 859.8969116210938 = 0.9192067980766296 + 100.0 * 8.589776992797852
Epoch 480, val loss: 0.9231252670288086
Epoch 490, training loss: 859.287109375 = 0.9114648699760437 + 100.0 * 8.583756446838379
Epoch 490, val loss: 0.91555255651474
Epoch 500, training loss: 858.9365234375 = 0.9036091566085815 + 100.0 * 8.580328941345215
Epoch 500, val loss: 0.9078876972198486
Epoch 510, training loss: 858.5901489257812 = 0.8956087827682495 + 100.0 * 8.576945304870605
Epoch 510, val loss: 0.9000812768936157
Epoch 520, training loss: 858.6224365234375 = 0.8874614238739014 + 100.0 * 8.577349662780762
Epoch 520, val loss: 0.8921157717704773
Epoch 530, training loss: 858.1741333007812 = 0.8791152238845825 + 100.0 * 8.57295036315918
Epoch 530, val loss: 0.8839877247810364
Epoch 540, training loss: 857.63427734375 = 0.8707423806190491 + 100.0 * 8.567635536193848
Epoch 540, val loss: 0.8758484125137329
Epoch 550, training loss: 857.33642578125 = 0.8623319268226624 + 100.0 * 8.564741134643555
Epoch 550, val loss: 0.8676672577857971
Epoch 560, training loss: 857.0145263671875 = 0.8538861870765686 + 100.0 * 8.561606407165527
Epoch 560, val loss: 0.8594548106193542
Epoch 570, training loss: 856.7432250976562 = 0.845413327217102 + 100.0 * 8.558978080749512
Epoch 570, val loss: 0.8512170314788818
Epoch 580, training loss: 856.9470825195312 = 0.8369078040122986 + 100.0 * 8.561101913452148
Epoch 580, val loss: 0.8429588675498962
Epoch 590, training loss: 856.3085327148438 = 0.8283313512802124 + 100.0 * 8.554801940917969
Epoch 590, val loss: 0.8346459269523621
Epoch 600, training loss: 856.0911865234375 = 0.8198531866073608 + 100.0 * 8.552713394165039
Epoch 600, val loss: 0.8264500498771667
Epoch 610, training loss: 855.8236083984375 = 0.8114113807678223 + 100.0 * 8.550122261047363
Epoch 610, val loss: 0.8182789087295532
Epoch 620, training loss: 855.5941162109375 = 0.8030198812484741 + 100.0 * 8.547910690307617
Epoch 620, val loss: 0.8101652264595032
Epoch 630, training loss: 855.7664794921875 = 0.7946574687957764 + 100.0 * 8.549717903137207
Epoch 630, val loss: 0.8021333813667297
Epoch 640, training loss: 855.254638671875 = 0.7863657474517822 + 100.0 * 8.544682502746582
Epoch 640, val loss: 0.7941117286682129
Epoch 650, training loss: 855.011962890625 = 0.7782065868377686 + 100.0 * 8.542337417602539
Epoch 650, val loss: 0.7862621545791626
Epoch 660, training loss: 854.8076782226562 = 0.7701959609985352 + 100.0 * 8.540374755859375
Epoch 660, val loss: 0.7785578370094299
Epoch 670, training loss: 854.603759765625 = 0.7623209953308105 + 100.0 * 8.538414001464844
Epoch 670, val loss: 0.7709986567497253
Epoch 680, training loss: 854.8203735351562 = 0.7545682191848755 + 100.0 * 8.540657997131348
Epoch 680, val loss: 0.7635564208030701
Epoch 690, training loss: 854.3460693359375 = 0.7468985319137573 + 100.0 * 8.535991668701172
Epoch 690, val loss: 0.7562065124511719
Epoch 700, training loss: 854.0853881835938 = 0.739421010017395 + 100.0 * 8.533459663391113
Epoch 700, val loss: 0.7490658760070801
Epoch 710, training loss: 853.8246459960938 = 0.7321510910987854 + 100.0 * 8.530924797058105
Epoch 710, val loss: 0.7420890927314758
Epoch 720, training loss: 853.6731567382812 = 0.7250286340713501 + 100.0 * 8.529480934143066
Epoch 720, val loss: 0.7352677583694458
Epoch 730, training loss: 853.597900390625 = 0.7180532217025757 + 100.0 * 8.52879810333252
Epoch 730, val loss: 0.728583037853241
Epoch 740, training loss: 853.2443237304688 = 0.7112439870834351 + 100.0 * 8.525330543518066
Epoch 740, val loss: 0.7221016883850098
Epoch 750, training loss: 853.0677490234375 = 0.7046573758125305 + 100.0 * 8.52363109588623
Epoch 750, val loss: 0.7158421277999878
Epoch 760, training loss: 852.9840087890625 = 0.6982582807540894 + 100.0 * 8.522857666015625
Epoch 760, val loss: 0.7097737789154053
Epoch 770, training loss: 852.7922973632812 = 0.6920430660247803 + 100.0 * 8.521002769470215
Epoch 770, val loss: 0.7038347721099854
Epoch 780, training loss: 852.6278686523438 = 0.6860000491142273 + 100.0 * 8.519418716430664
Epoch 780, val loss: 0.6981683969497681
Epoch 790, training loss: 852.420166015625 = 0.6801855564117432 + 100.0 * 8.517399787902832
Epoch 790, val loss: 0.692679762840271
Epoch 800, training loss: 852.388671875 = 0.6745673418045044 + 100.0 * 8.517141342163086
Epoch 800, val loss: 0.6873936057090759
Epoch 810, training loss: 852.6339111328125 = 0.6691007018089294 + 100.0 * 8.519647598266602
Epoch 810, val loss: 0.6822352409362793
Epoch 820, training loss: 852.0850219726562 = 0.6637870669364929 + 100.0 * 8.514212608337402
Epoch 820, val loss: 0.6772756576538086
Epoch 830, training loss: 851.88427734375 = 0.6587194800376892 + 100.0 * 8.512255668640137
Epoch 830, val loss: 0.6725568175315857
Epoch 840, training loss: 851.7374877929688 = 0.6538286209106445 + 100.0 * 8.510836601257324
Epoch 840, val loss: 0.6680302619934082
Epoch 850, training loss: 851.5919799804688 = 0.6491357684135437 + 100.0 * 8.509428977966309
Epoch 850, val loss: 0.6636963486671448
Epoch 860, training loss: 851.484375 = 0.644595205783844 + 100.0 * 8.508398056030273
Epoch 860, val loss: 0.6595125794410706
Epoch 870, training loss: 851.8670043945312 = 0.6401954889297485 + 100.0 * 8.51226806640625
Epoch 870, val loss: 0.6554623246192932
Epoch 880, training loss: 851.2823486328125 = 0.635877788066864 + 100.0 * 8.506464958190918
Epoch 880, val loss: 0.651525616645813
Epoch 890, training loss: 851.1875610351562 = 0.6317854523658752 + 100.0 * 8.505558013916016
Epoch 890, val loss: 0.6477774381637573
Epoch 900, training loss: 851.0140380859375 = 0.6278479099273682 + 100.0 * 8.503861427307129
Epoch 900, val loss: 0.6442168354988098
Epoch 910, training loss: 850.9130249023438 = 0.6240646839141846 + 100.0 * 8.502889633178711
Epoch 910, val loss: 0.6407999396324158
Epoch 920, training loss: 851.2788696289062 = 0.6204157471656799 + 100.0 * 8.506584167480469
Epoch 920, val loss: 0.6374771595001221
Epoch 930, training loss: 850.8589477539062 = 0.6168293356895447 + 100.0 * 8.502421379089355
Epoch 930, val loss: 0.6343380808830261
Epoch 940, training loss: 850.6199340820312 = 0.6134359836578369 + 100.0 * 8.500064849853516
Epoch 940, val loss: 0.6313403248786926
Epoch 950, training loss: 850.6000366210938 = 0.6101978421211243 + 100.0 * 8.499898910522461
Epoch 950, val loss: 0.6284803152084351
Epoch 960, training loss: 850.51171875 = 0.6070643663406372 + 100.0 * 8.499046325683594
Epoch 960, val loss: 0.6257230639457703
Epoch 970, training loss: 850.3319702148438 = 0.6040436625480652 + 100.0 * 8.497279167175293
Epoch 970, val loss: 0.6231213212013245
Epoch 980, training loss: 850.2105712890625 = 0.6011676788330078 + 100.0 * 8.49609375
Epoch 980, val loss: 0.6206329464912415
Epoch 990, training loss: 850.1195678710938 = 0.5984076857566833 + 100.0 * 8.495211601257324
Epoch 990, val loss: 0.618267834186554
Epoch 1000, training loss: 850.0243530273438 = 0.595747709274292 + 100.0 * 8.494285583496094
Epoch 1000, val loss: 0.616028368473053
Epoch 1010, training loss: 850.8565673828125 = 0.5931578278541565 + 100.0 * 8.502634048461914
Epoch 1010, val loss: 0.6138402819633484
Epoch 1020, training loss: 850.0530395507812 = 0.5906189680099487 + 100.0 * 8.494624137878418
Epoch 1020, val loss: 0.6117530465126038
Epoch 1030, training loss: 849.8170166015625 = 0.5882353186607361 + 100.0 * 8.492287635803223
Epoch 1030, val loss: 0.6098019480705261
Epoch 1040, training loss: 849.68115234375 = 0.5859693884849548 + 100.0 * 8.490951538085938
Epoch 1040, val loss: 0.6079354882240295
Epoch 1050, training loss: 849.5718383789062 = 0.5837901830673218 + 100.0 * 8.489880561828613
Epoch 1050, val loss: 0.6061811447143555
Epoch 1060, training loss: 849.4810180664062 = 0.5816876888275146 + 100.0 * 8.488993644714355
Epoch 1060, val loss: 0.6045083999633789
Epoch 1070, training loss: 849.6822509765625 = 0.579651415348053 + 100.0 * 8.491025924682617
Epoch 1070, val loss: 0.6028640866279602
Epoch 1080, training loss: 849.441162109375 = 0.5776427388191223 + 100.0 * 8.488635063171387
Epoch 1080, val loss: 0.6013302803039551
Epoch 1090, training loss: 849.4006958007812 = 0.5757231116294861 + 100.0 * 8.488249778747559
Epoch 1090, val loss: 0.5998572111129761
Epoch 1100, training loss: 849.1742553710938 = 0.5738314986228943 + 100.0 * 8.486003875732422
Epoch 1100, val loss: 0.5983766913414001
Epoch 1110, training loss: 849.112548828125 = 0.5720083117485046 + 100.0 * 8.485404968261719
Epoch 1110, val loss: 0.5970538258552551
Epoch 1120, training loss: 848.9846801757812 = 0.5702534914016724 + 100.0 * 8.48414421081543
Epoch 1120, val loss: 0.5957493185997009
Epoch 1130, training loss: 849.0971069335938 = 0.5685487389564514 + 100.0 * 8.485285758972168
Epoch 1130, val loss: 0.5944772958755493
Epoch 1140, training loss: 848.89208984375 = 0.5668352842330933 + 100.0 * 8.48325252532959
Epoch 1140, val loss: 0.593298077583313
Epoch 1150, training loss: 848.774658203125 = 0.5651875138282776 + 100.0 * 8.482094764709473
Epoch 1150, val loss: 0.5920870304107666
Epoch 1160, training loss: 848.6238403320312 = 0.5636085867881775 + 100.0 * 8.480602264404297
Epoch 1160, val loss: 0.5909779071807861
Epoch 1170, training loss: 848.5427856445312 = 0.5620776414871216 + 100.0 * 8.479806900024414
Epoch 1170, val loss: 0.5899118185043335
Epoch 1180, training loss: 848.4583129882812 = 0.5605828166007996 + 100.0 * 8.47897720336914
Epoch 1180, val loss: 0.588883101940155
Epoch 1190, training loss: 848.5401000976562 = 0.5591160655021667 + 100.0 * 8.479809761047363
Epoch 1190, val loss: 0.5878939628601074
Epoch 1200, training loss: 848.5306396484375 = 0.5576719045639038 + 100.0 * 8.479729652404785
Epoch 1200, val loss: 0.5868687033653259
Epoch 1210, training loss: 848.2725219726562 = 0.5562370419502258 + 100.0 * 8.477163314819336
Epoch 1210, val loss: 0.5859390497207642
Epoch 1220, training loss: 848.1959838867188 = 0.554867148399353 + 100.0 * 8.476410865783691
Epoch 1220, val loss: 0.5849984288215637
Epoch 1230, training loss: 848.1044921875 = 0.5535354018211365 + 100.0 * 8.475509643554688
Epoch 1230, val loss: 0.584114134311676
Epoch 1240, training loss: 848.0562744140625 = 0.5522220730781555 + 100.0 * 8.475040435791016
Epoch 1240, val loss: 0.583257794380188
Epoch 1250, training loss: 848.339599609375 = 0.5509152412414551 + 100.0 * 8.477887153625488
Epoch 1250, val loss: 0.5824042558670044
Epoch 1260, training loss: 848.1658935546875 = 0.5496140718460083 + 100.0 * 8.476162910461426
Epoch 1260, val loss: 0.5816107392311096
Epoch 1270, training loss: 847.8905639648438 = 0.5483514070510864 + 100.0 * 8.473422050476074
Epoch 1270, val loss: 0.5807405114173889
Epoch 1280, training loss: 847.7861328125 = 0.5471184849739075 + 100.0 * 8.472390174865723
Epoch 1280, val loss: 0.5799593925476074
Epoch 1290, training loss: 847.7784423828125 = 0.5459141135215759 + 100.0 * 8.472325325012207
Epoch 1290, val loss: 0.5791711807250977
Epoch 1300, training loss: 847.6862182617188 = 0.5446885824203491 + 100.0 * 8.471415519714355
Epoch 1300, val loss: 0.5784391164779663
Epoch 1310, training loss: 847.7599487304688 = 0.5434908866882324 + 100.0 * 8.472164154052734
Epoch 1310, val loss: 0.5777308940887451
Epoch 1320, training loss: 847.6253662109375 = 0.5422996282577515 + 100.0 * 8.470830917358398
Epoch 1320, val loss: 0.576949954032898
Epoch 1330, training loss: 847.5228271484375 = 0.5411226153373718 + 100.0 * 8.469817161560059
Epoch 1330, val loss: 0.5762383341789246
Epoch 1340, training loss: 847.4223022460938 = 0.5399727821350098 + 100.0 * 8.468823432922363
Epoch 1340, val loss: 0.5755500793457031
Epoch 1350, training loss: 847.4658203125 = 0.5388409495353699 + 100.0 * 8.469269752502441
Epoch 1350, val loss: 0.5749106407165527
Epoch 1360, training loss: 847.3605346679688 = 0.5377070903778076 + 100.0 * 8.468228340148926
Epoch 1360, val loss: 0.5741474628448486
Epoch 1370, training loss: 847.3751831054688 = 0.5365927219390869 + 100.0 * 8.468385696411133
Epoch 1370, val loss: 0.5734919905662537
Epoch 1380, training loss: 847.4825439453125 = 0.5354900360107422 + 100.0 * 8.469470977783203
Epoch 1380, val loss: 0.5728567242622375
Epoch 1390, training loss: 847.2254028320312 = 0.5343781113624573 + 100.0 * 8.466910362243652
Epoch 1390, val loss: 0.5721425414085388
Epoch 1400, training loss: 847.1404418945312 = 0.5333027839660645 + 100.0 * 8.466071128845215
Epoch 1400, val loss: 0.5715506672859192
Epoch 1410, training loss: 847.0625 = 0.5322521924972534 + 100.0 * 8.465302467346191
Epoch 1410, val loss: 0.570885181427002
Epoch 1420, training loss: 847.0164184570312 = 0.5312120318412781 + 100.0 * 8.464852333068848
Epoch 1420, val loss: 0.5703112483024597
Epoch 1430, training loss: 846.9777221679688 = 0.5301780700683594 + 100.0 * 8.464475631713867
Epoch 1430, val loss: 0.5697214603424072
Epoch 1440, training loss: 847.1190185546875 = 0.5291498303413391 + 100.0 * 8.465898513793945
Epoch 1440, val loss: 0.5690708756446838
Epoch 1450, training loss: 847.0433349609375 = 0.528096079826355 + 100.0 * 8.465152740478516
Epoch 1450, val loss: 0.5686084628105164
Epoch 1460, training loss: 846.9578857421875 = 0.5270496606826782 + 100.0 * 8.464308738708496
Epoch 1460, val loss: 0.5679255127906799
Epoch 1470, training loss: 846.8099975585938 = 0.5260435342788696 + 100.0 * 8.462839126586914
Epoch 1470, val loss: 0.5673690438270569
Epoch 1480, training loss: 846.7947387695312 = 0.5250517129898071 + 100.0 * 8.46269702911377
Epoch 1480, val loss: 0.566858172416687
Epoch 1490, training loss: 846.8483276367188 = 0.5240617394447327 + 100.0 * 8.463242530822754
Epoch 1490, val loss: 0.5662561655044556
Epoch 1500, training loss: 846.762451171875 = 0.5230575203895569 + 100.0 * 8.462393760681152
Epoch 1500, val loss: 0.5656923651695251
Epoch 1510, training loss: 846.9393920898438 = 0.5220581293106079 + 100.0 * 8.464173316955566
Epoch 1510, val loss: 0.56514573097229
Epoch 1520, training loss: 846.683837890625 = 0.5210602283477783 + 100.0 * 8.461627960205078
Epoch 1520, val loss: 0.5646053552627563
Epoch 1530, training loss: 846.57568359375 = 0.5200879573822021 + 100.0 * 8.460556030273438
Epoch 1530, val loss: 0.5640427470207214
Epoch 1540, training loss: 846.5112915039062 = 0.5191299915313721 + 100.0 * 8.459921836853027
Epoch 1540, val loss: 0.5634934902191162
Epoch 1550, training loss: 846.4820556640625 = 0.5181741714477539 + 100.0 * 8.459638595581055
Epoch 1550, val loss: 0.5629529356956482
Epoch 1560, training loss: 846.9708862304688 = 0.5172178149223328 + 100.0 * 8.464536666870117
Epoch 1560, val loss: 0.5624426603317261
Epoch 1570, training loss: 846.7301635742188 = 0.5162126421928406 + 100.0 * 8.462139129638672
Epoch 1570, val loss: 0.5617659687995911
Epoch 1580, training loss: 846.3839111328125 = 0.5152344703674316 + 100.0 * 8.458686828613281
Epoch 1580, val loss: 0.5612053275108337
Epoch 1590, training loss: 846.371337890625 = 0.5142956972122192 + 100.0 * 8.45857048034668
Epoch 1590, val loss: 0.560664176940918
Epoch 1600, training loss: 846.3223266601562 = 0.5133594274520874 + 100.0 * 8.458089828491211
Epoch 1600, val loss: 0.5601119995117188
Epoch 1610, training loss: 846.4363403320312 = 0.5124225616455078 + 100.0 * 8.45923900604248
Epoch 1610, val loss: 0.5595635771751404
Epoch 1620, training loss: 846.2383422851562 = 0.5114673972129822 + 100.0 * 8.457268714904785
Epoch 1620, val loss: 0.5589931607246399
Epoch 1630, training loss: 846.2338256835938 = 0.5105217099189758 + 100.0 * 8.457233428955078
Epoch 1630, val loss: 0.558456301689148
Epoch 1640, training loss: 846.1589965820312 = 0.5095822215080261 + 100.0 * 8.456494331359863
Epoch 1640, val loss: 0.5578669905662537
Epoch 1650, training loss: 846.6580810546875 = 0.5086472034454346 + 100.0 * 8.461494445800781
Epoch 1650, val loss: 0.5572265982627869
Epoch 1660, training loss: 846.2399291992188 = 0.5076746344566345 + 100.0 * 8.457322120666504
Epoch 1660, val loss: 0.5567548274993896
Epoch 1670, training loss: 846.0487670898438 = 0.5067241191864014 + 100.0 * 8.45542049407959
Epoch 1670, val loss: 0.5561519265174866
Epoch 1680, training loss: 846.0247802734375 = 0.5058037042617798 + 100.0 * 8.45518970489502
Epoch 1680, val loss: 0.555637776851654
Epoch 1690, training loss: 846.015625 = 0.5048739910125732 + 100.0 * 8.455107688903809
Epoch 1690, val loss: 0.5550956130027771
Epoch 1700, training loss: 846.5453491210938 = 0.5039381980895996 + 100.0 * 8.460413932800293
Epoch 1700, val loss: 0.5545604228973389
Epoch 1710, training loss: 846.101806640625 = 0.5029720067977905 + 100.0 * 8.455987930297852
Epoch 1710, val loss: 0.5539057850837708
Epoch 1720, training loss: 845.925048828125 = 0.5020376443862915 + 100.0 * 8.454230308532715
Epoch 1720, val loss: 0.5533947348594666
Epoch 1730, training loss: 845.8522338867188 = 0.501124918460846 + 100.0 * 8.453511238098145
Epoch 1730, val loss: 0.5528257489204407
Epoch 1740, training loss: 845.8099365234375 = 0.5002124309539795 + 100.0 * 8.453097343444824
Epoch 1740, val loss: 0.5523077845573425
Epoch 1750, training loss: 845.8221435546875 = 0.4992976486682892 + 100.0 * 8.453228950500488
Epoch 1750, val loss: 0.5517547130584717
Epoch 1760, training loss: 846.2095947265625 = 0.4983663558959961 + 100.0 * 8.457112312316895
Epoch 1760, val loss: 0.5512822270393372
Epoch 1770, training loss: 845.8595581054688 = 0.49738410115242004 + 100.0 * 8.453621864318848
Epoch 1770, val loss: 0.5505682826042175
Epoch 1780, training loss: 845.7068481445312 = 0.496468186378479 + 100.0 * 8.452103614807129
Epoch 1780, val loss: 0.5499905347824097
Epoch 1790, training loss: 845.6598510742188 = 0.49557214975357056 + 100.0 * 8.451642990112305
Epoch 1790, val loss: 0.549511194229126
Epoch 1800, training loss: 845.6148071289062 = 0.4946756064891815 + 100.0 * 8.451201438903809
Epoch 1800, val loss: 0.5489954352378845
Epoch 1810, training loss: 845.6133422851562 = 0.49377045035362244 + 100.0 * 8.45119571685791
Epoch 1810, val loss: 0.5484746694564819
Epoch 1820, training loss: 846.0121459960938 = 0.4928559362888336 + 100.0 * 8.455192565917969
Epoch 1820, val loss: 0.5479415655136108
Epoch 1830, training loss: 845.6170043945312 = 0.4919123649597168 + 100.0 * 8.451251029968262
Epoch 1830, val loss: 0.5472967028617859
Epoch 1840, training loss: 845.5055541992188 = 0.4909864366054535 + 100.0 * 8.450145721435547
Epoch 1840, val loss: 0.546801745891571
Epoch 1850, training loss: 845.4686279296875 = 0.4900857210159302 + 100.0 * 8.449785232543945
Epoch 1850, val loss: 0.5462941527366638
Epoch 1860, training loss: 845.4754638671875 = 0.489189088344574 + 100.0 * 8.449862480163574
Epoch 1860, val loss: 0.5457344055175781
