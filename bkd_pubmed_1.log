nohup: ignoring input
Namespace(config='config.yaml', cuda=True, dataset='Pubmed', debug=True, defense_mode='prune', device_id=0, dis_weight=1, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=400, use_vs_number=True, vs_number=40, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 88648])
add edge: torch.Size([2, 115198])
remove edge: torch.Size([2, 62260])
updated graph: torch.Size([2, 88810])
#Attach Nodes:80
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 27.0025634765625 = 1.105156421661377 + 2.5 * 10.358963012695312
Epoch 0, val loss: 1.1028974056243896
Epoch 10, training loss: 26.818517684936523 = 1.0676904916763306 + 2.5 * 10.300331115722656
Epoch 10, val loss: 1.062506914138794
Epoch 20, training loss: 25.165817260742188 = 1.026414155960083 + 2.5 * 9.655760765075684
Epoch 20, val loss: 1.0200766324996948
Epoch 30, training loss: 24.539241790771484 = 0.9700003266334534 + 2.5 * 9.427696228027344
Epoch 30, val loss: 0.9633303284645081
Epoch 40, training loss: 24.431659698486328 = 0.8910279273986816 + 2.5 * 9.416253089904785
Epoch 40, val loss: 0.885136067867279
Epoch 50, training loss: 24.221172332763672 = 0.7923099398612976 + 2.5 * 9.37154483795166
Epoch 50, val loss: 0.7915804982185364
Epoch 60, training loss: 23.983671188354492 = 0.6888794898986816 + 2.5 * 9.317916870117188
Epoch 60, val loss: 0.6945345997810364
Epoch 70, training loss: 23.81452178955078 = 0.5837104320526123 + 2.5 * 9.292325019836426
Epoch 70, val loss: 0.5982099175453186
Epoch 80, training loss: 23.675708770751953 = 0.49419867992401123 + 2.5 * 9.272603988647461
Epoch 80, val loss: 0.5204784274101257
Epoch 90, training loss: 23.582393646240234 = 0.4338277578353882 + 2.5 * 9.25942611694336
Epoch 90, val loss: 0.46984943747520447
Epoch 100, training loss: 23.528579711914062 = 0.3944653272628784 + 2.5 * 9.253645896911621
Epoch 100, val loss: 0.44131049513816833
Epoch 110, training loss: 23.471420288085938 = 0.3702218532562256 + 2.5 * 9.240479469299316
Epoch 110, val loss: 0.4270208179950714
Epoch 120, training loss: 23.411230087280273 = 0.35287994146347046 + 2.5 * 9.223340034484863
Epoch 120, val loss: 0.4148181676864624
Epoch 130, training loss: 23.455419540405273 = 0.33929261565208435 + 2.5 * 9.246450424194336
Epoch 130, val loss: 0.409614235162735
Epoch 140, training loss: 23.352519989013672 = 0.32808226346969604 + 2.5 * 9.2097749710083
Epoch 140, val loss: 0.4012105166912079
Epoch 150, training loss: 23.291196823120117 = 0.3203505575656891 + 2.5 * 9.188338279724121
Epoch 150, val loss: 0.4016714096069336
Epoch 160, training loss: 23.298181533813477 = 0.3097486197948456 + 2.5 * 9.19537353515625
Epoch 160, val loss: 0.39266237616539
Epoch 170, training loss: 23.375839233398438 = 0.30115652084350586 + 2.5 * 9.229872703552246
Epoch 170, val loss: 0.39031782746315
Epoch 180, training loss: 23.28201675415039 = 0.2946031093597412 + 2.5 * 9.194965362548828
Epoch 180, val loss: 0.38873958587646484
Epoch 190, training loss: 23.24126434326172 = 0.28967756032943726 + 2.5 * 9.180635452270508
Epoch 190, val loss: 0.3906932473182678
Epoch 200, training loss: 23.23470115661621 = 0.2829338014125824 + 2.5 * 9.180706977844238
Epoch 200, val loss: 0.39264699816703796
Epoch 210, training loss: 23.21824073791504 = 0.2784615755081177 + 2.5 * 9.175911903381348
Epoch 210, val loss: 0.391205757856369
Epoch 220, training loss: 23.20479393005371 = 0.2720690965652466 + 2.5 * 9.173089981079102
Epoch 220, val loss: 0.3867327570915222
Epoch 230, training loss: 23.191747665405273 = 0.267191618680954 + 2.5 * 9.169822692871094
Epoch 230, val loss: 0.38157743215560913
Epoch 240, training loss: 23.202939987182617 = 0.26198065280914307 + 2.5 * 9.176383972167969
Epoch 240, val loss: 0.3842813968658447
Epoch 250, training loss: 23.18218421936035 = 0.25691327452659607 + 2.5 * 9.1701078414917
Epoch 250, val loss: 0.39189043641090393
Epoch 260, training loss: 23.189680099487305 = 0.25255805253982544 + 2.5 * 9.174848556518555
Epoch 260, val loss: 0.3855104446411133
Epoch 270, training loss: 23.225034713745117 = 0.2476472556591034 + 2.5 * 9.19095516204834
Epoch 270, val loss: 0.38810640573501587
Epoch 280, training loss: 23.174009323120117 = 0.24417969584465027 + 2.5 * 9.171932220458984
Epoch 280, val loss: 0.3943549692630768
Epoch 290, training loss: 23.17496681213379 = 0.24167844653129578 + 2.5 * 9.173315048217773
Epoch 290, val loss: 0.3863210082054138
Epoch 300, training loss: 23.161867141723633 = 0.23639187216758728 + 2.5 * 9.170190811157227
Epoch 300, val loss: 0.3875803053379059
Epoch 310, training loss: 23.160411834716797 = 0.23543435335159302 + 2.5 * 9.169991493225098
Epoch 310, val loss: 0.3909286558628082
Epoch 320, training loss: 23.121564865112305 = 0.22820447385311127 + 2.5 * 9.157343864440918
Epoch 320, val loss: 0.39306020736694336
Epoch 330, training loss: 23.12973976135254 = 0.22489656507968903 + 2.5 * 9.16193675994873
Epoch 330, val loss: 0.40968966484069824
Epoch 340, training loss: 23.124513626098633 = 0.21937546133995056 + 2.5 * 9.162055015563965
Epoch 340, val loss: 0.398800790309906
Epoch 350, training loss: 23.14063835144043 = 0.21615956723690033 + 2.5 * 9.169791221618652
Epoch 350, val loss: 0.4100514054298401
Epoch 360, training loss: 23.136959075927734 = 0.21578052639961243 + 2.5 * 9.168471336364746
Epoch 360, val loss: 0.4107983112335205
Epoch 370, training loss: 23.09657096862793 = 0.20866340398788452 + 2.5 * 9.155162811279297
Epoch 370, val loss: 0.4053577780723572
Epoch 380, training loss: 23.103443145751953 = 0.20529913902282715 + 2.5 * 9.159257888793945
Epoch 380, val loss: 0.4038081169128418
Epoch 390, training loss: 23.083324432373047 = 0.20061734318733215 + 2.5 * 9.153082847595215
Epoch 390, val loss: 0.4177962839603424
Epoch 400, training loss: 23.051042556762695 = 0.19750429689884186 + 2.5 * 9.1414155960083
Epoch 400, val loss: 0.41495585441589355
Epoch 410, training loss: 23.056304931640625 = 0.19407765567302704 + 2.5 * 9.144890785217285
Epoch 410, val loss: 0.4138084352016449
Epoch 420, training loss: 23.09463882446289 = 0.18955586850643158 + 2.5 * 9.162033081054688
Epoch 420, val loss: 0.4230981171131134
Epoch 430, training loss: 23.088300704956055 = 0.18608257174491882 + 2.5 * 9.160886764526367
Epoch 430, val loss: 0.42818382382392883
Epoch 440, training loss: 23.07132339477539 = 0.18231786787509918 + 2.5 * 9.15560245513916
Epoch 440, val loss: 0.4272588789463043
Epoch 450, training loss: 23.06036376953125 = 0.17859265208244324 + 2.5 * 9.152708053588867
Epoch 450, val loss: 0.4275404214859009
Epoch 460, training loss: 23.05596160888672 = 0.17528603971004486 + 2.5 * 9.152270317077637
Epoch 460, val loss: 0.43560686707496643
Epoch 470, training loss: 23.058252334594727 = 0.17142058908939362 + 2.5 * 9.154732704162598
Epoch 470, val loss: 0.44054368138313293
Epoch 480, training loss: 23.041015625 = 0.16815625131130219 + 2.5 * 9.149144172668457
Epoch 480, val loss: 0.44730672240257263
Epoch 490, training loss: 23.020645141601562 = 0.16404902935028076 + 2.5 * 9.142638206481934
Epoch 490, val loss: 0.4486840069293976
Epoch 500, training loss: 23.01719093322754 = 0.160654217004776 + 2.5 * 9.142614364624023
Epoch 500, val loss: 0.45622822642326355
Epoch 510, training loss: 23.000362396240234 = 0.15762804448604584 + 2.5 * 9.137094497680664
Epoch 510, val loss: 0.45370179414749146
Epoch 520, training loss: 22.99231719970703 = 0.15323896706104279 + 2.5 * 9.135631561279297
Epoch 520, val loss: 0.4611586928367615
Epoch 530, training loss: 23.013229370117188 = 0.15039455890655518 + 2.5 * 9.145133972167969
Epoch 530, val loss: 0.46639469265937805
Epoch 540, training loss: 23.00998878479004 = 0.14690497517585754 + 2.5 * 9.145233154296875
Epoch 540, val loss: 0.47077828645706177
Epoch 550, training loss: 22.988868713378906 = 0.14601975679397583 + 2.5 * 9.137140274047852
Epoch 550, val loss: 0.4698377549648285
Epoch 560, training loss: 22.987070083618164 = 0.13928160071372986 + 2.5 * 9.139115333557129
Epoch 560, val loss: 0.496717631816864
Epoch 570, training loss: 22.98154640197754 = 0.13688410818576813 + 2.5 * 9.13786506652832
Epoch 570, val loss: 0.4871917963027954
Epoch 580, training loss: 22.970170974731445 = 0.13239042460918427 + 2.5 * 9.135111808776855
Epoch 580, val loss: 0.49187523126602173
Epoch 590, training loss: 22.953767776489258 = 0.12934988737106323 + 2.5 * 9.129766464233398
Epoch 590, val loss: 0.5020459294319153
Epoch 600, training loss: 22.977888107299805 = 0.12610533833503723 + 2.5 * 9.14071273803711
Epoch 600, val loss: 0.52588951587677
Epoch 610, training loss: 22.969768524169922 = 0.12213970720767975 + 2.5 * 9.13905143737793
Epoch 610, val loss: 0.5187674760818481
Epoch 620, training loss: 22.956323623657227 = 0.11962824314832687 + 2.5 * 9.13467788696289
Epoch 620, val loss: 0.5319290161132812
Epoch 630, training loss: 22.93590545654297 = 0.11632570624351501 + 2.5 * 9.127832412719727
Epoch 630, val loss: 0.5281587243080139
Epoch 640, training loss: 22.959991455078125 = 0.1116173267364502 + 2.5 * 9.139348983764648
Epoch 640, val loss: 0.5454067587852478
Epoch 650, training loss: 22.946008682250977 = 0.10989375412464142 + 2.5 * 9.134446144104004
Epoch 650, val loss: 0.5449755787849426
Epoch 660, training loss: 22.92923355102539 = 0.10531111061573029 + 2.5 * 9.129569053649902
Epoch 660, val loss: 0.5565308332443237
Epoch 670, training loss: 22.923633575439453 = 0.10213829576969147 + 2.5 * 9.1285982131958
Epoch 670, val loss: 0.5711596608161926
Epoch 680, training loss: 22.905183792114258 = 0.09859198331832886 + 2.5 * 9.122636795043945
Epoch 680, val loss: 0.5735831260681152
Epoch 690, training loss: 22.91706657409668 = 0.09597478806972504 + 2.5 * 9.128437042236328
Epoch 690, val loss: 0.5901793241500854
Epoch 700, training loss: 22.902868270874023 = 0.09285242855548859 + 2.5 * 9.124006271362305
Epoch 700, val loss: 0.6007259488105774
Epoch 710, training loss: 22.9126033782959 = 0.08955971896648407 + 2.5 * 9.129217147827148
Epoch 710, val loss: 0.6004219055175781
Epoch 720, training loss: 22.910701751708984 = 0.08615029603242874 + 2.5 * 9.129819869995117
Epoch 720, val loss: 0.6119133830070496
Epoch 730, training loss: 22.886545181274414 = 0.08295371383428574 + 2.5 * 9.12143611907959
Epoch 730, val loss: 0.6196277141571045
Epoch 740, training loss: 22.896974563598633 = 0.08066892623901367 + 2.5 * 9.126522064208984
Epoch 740, val loss: 0.638845682144165
Epoch 750, training loss: 22.923511505126953 = 0.0775199681520462 + 2.5 * 9.138396263122559
Epoch 750, val loss: 0.6419944167137146
Epoch 760, training loss: 22.874500274658203 = 0.0748835951089859 + 2.5 * 9.11984634399414
Epoch 760, val loss: 0.6509260535240173
Epoch 770, training loss: 22.90265464782715 = 0.0720931738615036 + 2.5 * 9.132224082946777
Epoch 770, val loss: 0.6554540395736694
Epoch 780, training loss: 22.89174461364746 = 0.07015523314476013 + 2.5 * 9.12863540649414
Epoch 780, val loss: 0.6605015397071838
Epoch 790, training loss: 22.866235733032227 = 0.06964641064405441 + 2.5 * 9.118635177612305
Epoch 790, val loss: 0.6868060827255249
Epoch 800, training loss: 22.87799072265625 = 0.0657387524843216 + 2.5 * 9.124900817871094
Epoch 800, val loss: 0.6907725930213928
Epoch 810, training loss: 22.867769241333008 = 0.06147681549191475 + 2.5 * 9.122516632080078
Epoch 810, val loss: 0.6976315379142761
Epoch 820, training loss: 22.878124237060547 = 0.05904051288962364 + 2.5 * 9.127634048461914
Epoch 820, val loss: 0.7298571467399597
Epoch 830, training loss: 22.896785736083984 = 0.057916417717933655 + 2.5 * 9.135547637939453
Epoch 830, val loss: 0.7197479009628296
Epoch 840, training loss: 22.870756149291992 = 0.054507747292518616 + 2.5 * 9.12649917602539
Epoch 840, val loss: 0.7270722389221191
Epoch 850, training loss: 22.873123168945312 = 0.05235239863395691 + 2.5 * 9.128308296203613
Epoch 850, val loss: 0.7389447689056396
Epoch 860, training loss: 22.87624740600586 = 0.04984396696090698 + 2.5 * 9.130560874938965
Epoch 860, val loss: 0.7570912837982178
Epoch 870, training loss: 22.83759117126465 = 0.04889897257089615 + 2.5 * 9.115476608276367
Epoch 870, val loss: 0.7631728649139404
Epoch 880, training loss: 22.875032424926758 = 0.04585057497024536 + 2.5 * 9.131672859191895
Epoch 880, val loss: 0.7751831412315369
Epoch 890, training loss: 22.890336990356445 = 0.044065725058317184 + 2.5 * 9.138508796691895
Epoch 890, val loss: 0.7806158661842346
Epoch 900, training loss: 22.84873390197754 = 0.042477622628211975 + 2.5 * 9.122502326965332
Epoch 900, val loss: 0.7953932881355286
Epoch 910, training loss: 22.85399627685547 = 0.040809497237205505 + 2.5 * 9.125274658203125
Epoch 910, val loss: 0.8093997836112976
Epoch 920, training loss: 22.878475189208984 = 0.04104503616690636 + 2.5 * 9.13497257232666
Epoch 920, val loss: 0.8271390199661255
Epoch 930, training loss: 22.837265014648438 = 0.03745769336819649 + 2.5 * 9.119922637939453
Epoch 930, val loss: 0.8189627528190613
Epoch 940, training loss: 22.82721710205078 = 0.03604885935783386 + 2.5 * 9.116467475891113
Epoch 940, val loss: 0.8447896838188171
Epoch 950, training loss: 22.826635360717773 = 0.03503081947565079 + 2.5 * 9.116641998291016
Epoch 950, val loss: 0.8459259271621704
Epoch 960, training loss: 22.825132369995117 = 0.03286992385983467 + 2.5 * 9.116905212402344
Epoch 960, val loss: 0.8714819550514221
Epoch 970, training loss: 22.845701217651367 = 0.032210469245910645 + 2.5 * 9.125395774841309
Epoch 970, val loss: 0.8707855939865112
Epoch 980, training loss: 22.835777282714844 = 0.030533134937286377 + 2.5 * 9.122097969055176
Epoch 980, val loss: 0.8734299540519714
Epoch 990, training loss: 22.825687408447266 = 0.02946082316339016 + 2.5 * 9.118490219116211
Epoch 990, val loss: 0.897277295589447
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8498
Overall ASR: 0.7160
Flip ASR: 0.6467/1554 nodes
./selected_nodes/Pubmed/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 26.98845863342285 = 1.0912407636642456 + 2.5 * 10.358887672424316
Epoch 0, val loss: 1.087862253189087
Epoch 10, training loss: 26.715883255004883 = 1.064501404762268 + 2.5 * 10.260553359985352
Epoch 10, val loss: 1.0600978136062622
Epoch 20, training loss: 24.98727035522461 = 1.0309492349624634 + 2.5 * 9.582529067993164
Epoch 20, val loss: 1.0247771739959717
Epoch 30, training loss: 24.548324584960938 = 0.97179114818573 + 2.5 * 9.43061351776123
Epoch 30, val loss: 0.9665609002113342
Epoch 40, training loss: 24.391801834106445 = 0.8899499773979187 + 2.5 * 9.400740623474121
Epoch 40, val loss: 0.8881655335426331
Epoch 50, training loss: 24.184505462646484 = 0.7951586246490479 + 2.5 * 9.355738639831543
Epoch 50, val loss: 0.7980120182037354
Epoch 60, training loss: 23.898738861083984 = 0.6906918883323669 + 2.5 * 9.283219337463379
Epoch 60, val loss: 0.6978491544723511
Epoch 70, training loss: 23.745250701904297 = 0.5782746076583862 + 2.5 * 9.266790390014648
Epoch 70, val loss: 0.594856858253479
Epoch 80, training loss: 23.591793060302734 = 0.48677846789360046 + 2.5 * 9.242006301879883
Epoch 80, val loss: 0.5121553540229797
Epoch 90, training loss: 23.51323890686035 = 0.4263753592967987 + 2.5 * 9.234745979309082
Epoch 90, val loss: 0.46520134806632996
Epoch 100, training loss: 23.44140625 = 0.3891811668872833 + 2.5 * 9.220890045166016
Epoch 100, val loss: 0.4386775493621826
Epoch 110, training loss: 23.356475830078125 = 0.36507588624954224 + 2.5 * 9.19655990600586
Epoch 110, val loss: 0.42735472321510315
Epoch 120, training loss: 23.34693717956543 = 0.34821850061416626 + 2.5 * 9.199487686157227
Epoch 120, val loss: 0.41475582122802734
Epoch 130, training loss: 23.29154396057129 = 0.33493179082870483 + 2.5 * 9.182644844055176
Epoch 130, val loss: 0.4082021117210388
Epoch 140, training loss: 23.305992126464844 = 0.3237561285495758 + 2.5 * 9.19289493560791
Epoch 140, val loss: 0.40369218587875366
Epoch 150, training loss: 23.36625862121582 = 0.3145027756690979 + 2.5 * 9.220702171325684
Epoch 150, val loss: 0.39840131998062134
Epoch 160, training loss: 23.27599334716797 = 0.3077322244644165 + 2.5 * 9.187304496765137
Epoch 160, val loss: 0.39449363946914673
Epoch 170, training loss: 23.314992904663086 = 0.2994016706943512 + 2.5 * 9.206235885620117
Epoch 170, val loss: 0.39574113488197327
Epoch 180, training loss: 23.215591430664062 = 0.292515367269516 + 2.5 * 9.169230461120605
Epoch 180, val loss: 0.3929458558559418
Epoch 190, training loss: 23.222150802612305 = 0.2861231863498688 + 2.5 * 9.174410820007324
Epoch 190, val loss: 0.3899475634098053
Epoch 200, training loss: 23.198265075683594 = 0.27996018528938293 + 2.5 * 9.167322158813477
Epoch 200, val loss: 0.3871186077594757
Epoch 210, training loss: 23.24303436279297 = 0.27417370676994324 + 2.5 * 9.187543869018555
Epoch 210, val loss: 0.39062508940696716
Epoch 220, training loss: 23.22854232788086 = 0.2695876955986023 + 2.5 * 9.183581352233887
Epoch 220, val loss: 0.3941688537597656
Epoch 230, training loss: 23.175453186035156 = 0.26544108986854553 + 2.5 * 9.1640043258667
Epoch 230, val loss: 0.3911222219467163
Epoch 240, training loss: 23.203569412231445 = 0.2589021921157837 + 2.5 * 9.17786693572998
Epoch 240, val loss: 0.3855600953102112
Epoch 250, training loss: 23.15685272216797 = 0.2559494078159332 + 2.5 * 9.160361289978027
Epoch 250, val loss: 0.386491596698761
Epoch 260, training loss: 23.176170349121094 = 0.2501024901866913 + 2.5 * 9.170427322387695
Epoch 260, val loss: 0.3965330421924591
Epoch 270, training loss: 23.14553451538086 = 0.24478092789649963 + 2.5 * 9.160301208496094
Epoch 270, val loss: 0.38744834065437317
Epoch 280, training loss: 23.152360916137695 = 0.24379485845565796 + 2.5 * 9.163426399230957
Epoch 280, val loss: 0.3894229531288147
Epoch 290, training loss: 23.19821548461914 = 0.2386016547679901 + 2.5 * 9.183845520019531
Epoch 290, val loss: 0.41016706824302673
Epoch 300, training loss: 23.171422958374023 = 0.23284517228603363 + 2.5 * 9.175431251525879
Epoch 300, val loss: 0.3925532400608063
Epoch 310, training loss: 23.154993057250977 = 0.22872085869312286 + 2.5 * 9.17050838470459
Epoch 310, val loss: 0.395146906375885
Epoch 320, training loss: 23.13517189025879 = 0.22468402981758118 + 2.5 * 9.16419506072998
Epoch 320, val loss: 0.3985518515110016
Epoch 330, training loss: 23.127227783203125 = 0.22117997705936432 + 2.5 * 9.162419319152832
Epoch 330, val loss: 0.4039042592048645
Epoch 340, training loss: 23.18063735961914 = 0.22005921602249146 + 2.5 * 9.184231758117676
Epoch 340, val loss: 0.4082191288471222
Epoch 350, training loss: 23.107406616210938 = 0.2162116914987564 + 2.5 * 9.156477928161621
Epoch 350, val loss: 0.4039572775363922
Epoch 360, training loss: 23.109203338623047 = 0.21075785160064697 + 2.5 * 9.159378051757812
Epoch 360, val loss: 0.4033755958080292
Epoch 370, training loss: 23.118370056152344 = 0.20696212351322174 + 2.5 * 9.164563179016113
Epoch 370, val loss: 0.41238081455230713
Epoch 380, training loss: 23.070894241333008 = 0.2032303363084793 + 2.5 * 9.147066116333008
Epoch 380, val loss: 0.4126830995082855
Epoch 390, training loss: 23.091197967529297 = 0.20235863327980042 + 2.5 * 9.155535697937012
Epoch 390, val loss: 0.41876670718193054
Epoch 400, training loss: 23.09938621520996 = 0.1952846497297287 + 2.5 * 9.161641120910645
Epoch 400, val loss: 0.41854584217071533
Epoch 410, training loss: 23.08197021484375 = 0.19332937896251678 + 2.5 * 9.15545654296875
Epoch 410, val loss: 0.42208313941955566
Epoch 420, training loss: 23.1008358001709 = 0.19095411896705627 + 2.5 * 9.163952827453613
Epoch 420, val loss: 0.4231627881526947
Epoch 430, training loss: 23.042898178100586 = 0.1853318065404892 + 2.5 * 9.143026351928711
Epoch 430, val loss: 0.42301908135414124
Epoch 440, training loss: 23.08051872253418 = 0.18177610635757446 + 2.5 * 9.159497261047363
Epoch 440, val loss: 0.43454962968826294
Epoch 450, training loss: 23.052936553955078 = 0.1809113621711731 + 2.5 * 9.148809432983398
Epoch 450, val loss: 0.43198394775390625
Epoch 460, training loss: 23.064916610717773 = 0.17486514151096344 + 2.5 * 9.156020164489746
Epoch 460, val loss: 0.43886321783065796
Epoch 470, training loss: 23.02942657470703 = 0.17139936983585358 + 2.5 * 9.143210411071777
Epoch 470, val loss: 0.44217392802238464
Epoch 480, training loss: 23.05461311340332 = 0.16958490014076233 + 2.5 * 9.154011726379395
Epoch 480, val loss: 0.4491785168647766
Epoch 490, training loss: 23.0150203704834 = 0.16645410656929016 + 2.5 * 9.139426231384277
Epoch 490, val loss: 0.4573897421360016
Epoch 500, training loss: 23.002687454223633 = 0.16028708219528198 + 2.5 * 9.13696002960205
Epoch 500, val loss: 0.4558383524417877
Epoch 510, training loss: 22.98732566833496 = 0.15723267197608948 + 2.5 * 9.132037162780762
Epoch 510, val loss: 0.4699620306491852
Epoch 520, training loss: 23.04534149169922 = 0.15380237996578217 + 2.5 * 9.156615257263184
Epoch 520, val loss: 0.4651707410812378
Epoch 530, training loss: 23.0147762298584 = 0.15047946572303772 + 2.5 * 9.145718574523926
Epoch 530, val loss: 0.4813222587108612
Epoch 540, training loss: 23.001157760620117 = 0.14721807837486267 + 2.5 * 9.141575813293457
Epoch 540, val loss: 0.4882766306400299
Epoch 550, training loss: 22.971532821655273 = 0.14484132826328278 + 2.5 * 9.13067626953125
Epoch 550, val loss: 0.4816916286945343
Epoch 560, training loss: 22.969511032104492 = 0.1411648839712143 + 2.5 * 9.131338119506836
Epoch 560, val loss: 0.4903199076652527
Epoch 570, training loss: 22.956050872802734 = 0.13613156974315643 + 2.5 * 9.127967834472656
Epoch 570, val loss: 0.5068740844726562
Epoch 580, training loss: 22.99637794494629 = 0.13534751534461975 + 2.5 * 9.14441204071045
Epoch 580, val loss: 0.5114774703979492
Epoch 590, training loss: 22.9976863861084 = 0.13068540394306183 + 2.5 * 9.14680004119873
Epoch 590, val loss: 0.522192656993866
Epoch 600, training loss: 22.988037109375 = 0.1275259107351303 + 2.5 * 9.144205093383789
Epoch 600, val loss: 0.5149064064025879
Epoch 610, training loss: 23.026567459106445 = 0.1227637454867363 + 2.5 * 9.161520957946777
Epoch 610, val loss: 0.526497483253479
Epoch 620, training loss: 22.960325241088867 = 0.11931319534778595 + 2.5 * 9.136404991149902
Epoch 620, val loss: 0.5355024933815002
Epoch 630, training loss: 22.962921142578125 = 0.11632959544658661 + 2.5 * 9.138636589050293
Epoch 630, val loss: 0.5398857593536377
Epoch 640, training loss: 22.94915008544922 = 0.11319112777709961 + 2.5 * 9.134383201599121
Epoch 640, val loss: 0.5576425790786743
Epoch 650, training loss: 22.94487190246582 = 0.10922002792358398 + 2.5 * 9.134260177612305
Epoch 650, val loss: 0.5524885058403015
Epoch 660, training loss: 22.92281723022461 = 0.10605231672525406 + 2.5 * 9.12670612335205
Epoch 660, val loss: 0.5734943151473999
Epoch 670, training loss: 22.94130516052246 = 0.10244246572256088 + 2.5 * 9.13554573059082
Epoch 670, val loss: 0.5764234066009521
Epoch 680, training loss: 22.919771194458008 = 0.10190307348966599 + 2.5 * 9.12714672088623
Epoch 680, val loss: 0.5795615911483765
Epoch 690, training loss: 22.915267944335938 = 0.10310734063386917 + 2.5 * 9.12486457824707
Epoch 690, val loss: 0.6060987114906311
Epoch 700, training loss: 22.922208786010742 = 0.09561175853013992 + 2.5 * 9.13063907623291
Epoch 700, val loss: 0.5978791117668152
Epoch 710, training loss: 22.916641235351562 = 0.09226714819669724 + 2.5 * 9.129749298095703
Epoch 710, val loss: 0.6104367971420288
Epoch 720, training loss: 22.903640747070312 = 0.0872725173830986 + 2.5 * 9.126546859741211
Epoch 720, val loss: 0.6164088845252991
Epoch 730, training loss: 22.911109924316406 = 0.08375222980976105 + 2.5 * 9.130943298339844
Epoch 730, val loss: 0.6238378882408142
Epoch 740, training loss: 22.904726028442383 = 0.08198478072881699 + 2.5 * 9.129096031188965
Epoch 740, val loss: 0.6362385153770447
Epoch 750, training loss: 22.885175704956055 = 0.07788543403148651 + 2.5 * 9.122916221618652
Epoch 750, val loss: 0.6448172926902771
Epoch 760, training loss: 22.900955200195312 = 0.07636095583438873 + 2.5 * 9.129837989807129
Epoch 760, val loss: 0.6583468914031982
Epoch 770, training loss: 22.895998001098633 = 0.07343824207782745 + 2.5 * 9.129023551940918
Epoch 770, val loss: 0.6718558073043823
Epoch 780, training loss: 22.88289451599121 = 0.07049736380577087 + 2.5 * 9.124958992004395
Epoch 780, val loss: 0.6741026043891907
Epoch 790, training loss: 22.926725387573242 = 0.06743752211332321 + 2.5 * 9.143714904785156
Epoch 790, val loss: 0.690588653087616
Epoch 800, training loss: 22.890348434448242 = 0.06596236675977707 + 2.5 * 9.129755020141602
Epoch 800, val loss: 0.6911003589630127
Epoch 810, training loss: 22.901445388793945 = 0.06285072863101959 + 2.5 * 9.135437965393066
Epoch 810, val loss: 0.7022456526756287
Epoch 820, training loss: 22.896440505981445 = 0.06036720797419548 + 2.5 * 9.134428977966309
Epoch 820, val loss: 0.7233338356018066
Epoch 830, training loss: 22.879959106445312 = 0.05799750238656998 + 2.5 * 9.128785133361816
Epoch 830, val loss: 0.7338650822639465
Epoch 840, training loss: 22.850473403930664 = 0.05685940012335777 + 2.5 * 9.11744499206543
Epoch 840, val loss: 0.7515125870704651
Epoch 850, training loss: 22.85536003112793 = 0.053405363112688065 + 2.5 * 9.120781898498535
Epoch 850, val loss: 0.7409763336181641
Epoch 860, training loss: 22.8768310546875 = 0.0545671209692955 + 2.5 * 9.128905296325684
Epoch 860, val loss: 0.7544527053833008
Epoch 870, training loss: 22.829755783081055 = 0.05017608031630516 + 2.5 * 9.111831665039062
Epoch 870, val loss: 0.7659867405891418
Epoch 880, training loss: 22.861989974975586 = 0.04699859768152237 + 2.5 * 9.125996589660645
Epoch 880, val loss: 0.7830705642700195
Epoch 890, training loss: 22.82911491394043 = 0.04603737220168114 + 2.5 * 9.11323070526123
Epoch 890, val loss: 0.795866847038269
Epoch 900, training loss: 22.861053466796875 = 0.043280258774757385 + 2.5 * 9.12710952758789
Epoch 900, val loss: 0.8048615455627441
Epoch 910, training loss: 22.827655792236328 = 0.04233646020293236 + 2.5 * 9.114128112792969
Epoch 910, val loss: 0.8287935853004456
Epoch 920, training loss: 22.85297966003418 = 0.039819613099098206 + 2.5 * 9.125264167785645
Epoch 920, val loss: 0.8269854784011841
Epoch 930, training loss: 22.83656120300293 = 0.038934897631406784 + 2.5 * 9.119050979614258
Epoch 930, val loss: 0.8329334855079651
Epoch 940, training loss: 22.830259323120117 = 0.03696996718645096 + 2.5 * 9.117315292358398
Epoch 940, val loss: 0.8477078080177307
Epoch 950, training loss: 22.848417282104492 = 0.03585103154182434 + 2.5 * 9.12502670288086
Epoch 950, val loss: 0.8538938760757446
Epoch 960, training loss: 22.84429168701172 = 0.03408525511622429 + 2.5 * 9.124082565307617
Epoch 960, val loss: 0.863890528678894
Epoch 970, training loss: 22.816165924072266 = 0.03395555540919304 + 2.5 * 9.112884521484375
Epoch 970, val loss: 0.8676764369010925
Epoch 980, training loss: 22.810895919799805 = 0.03175827115774155 + 2.5 * 9.111655235290527
Epoch 980, val loss: 0.8960229158401489
Epoch 990, training loss: 22.84541130065918 = 0.03150687366724014 + 2.5 * 9.125561714172363
Epoch 990, val loss: 0.8941459655761719
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8493
Overall ASR: 0.7738
Flip ASR: 0.7194/1554 nodes
./selected_nodes/Pubmed/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 26.987445831298828 = 1.0903476476669312 + 2.5 * 10.35883903503418
Epoch 0, val loss: 1.0876764059066772
Epoch 10, training loss: 26.564437866210938 = 1.0635713338851929 + 2.5 * 10.200345993041992
Epoch 10, val loss: 1.059435486793518
Epoch 20, training loss: 24.93077278137207 = 1.032426357269287 + 2.5 * 9.559338569641113
Epoch 20, val loss: 1.0257378816604614
Epoch 30, training loss: 24.578933715820312 = 0.9751582145690918 + 2.5 * 9.441510200500488
Epoch 30, val loss: 0.969369113445282
Epoch 40, training loss: 24.422983169555664 = 0.8954330086708069 + 2.5 * 9.411020278930664
Epoch 40, val loss: 0.8936290740966797
Epoch 50, training loss: 24.192026138305664 = 0.8059711456298828 + 2.5 * 9.354421615600586
Epoch 50, val loss: 0.8099758625030518
Epoch 60, training loss: 23.979103088378906 = 0.7053699493408203 + 2.5 * 9.309493064880371
Epoch 60, val loss: 0.7126243114471436
Epoch 70, training loss: 23.789873123168945 = 0.595986545085907 + 2.5 * 9.277554512023926
Epoch 70, val loss: 0.6116693019866943
Epoch 80, training loss: 23.602062225341797 = 0.5045356750488281 + 2.5 * 9.23901081085205
Epoch 80, val loss: 0.5280237793922424
Epoch 90, training loss: 23.53656005859375 = 0.4409242868423462 + 2.5 * 9.23825454711914
Epoch 90, val loss: 0.4772202968597412
Epoch 100, training loss: 23.426971435546875 = 0.39860033988952637 + 2.5 * 9.211348533630371
Epoch 100, val loss: 0.443825364112854
Epoch 110, training loss: 23.39034080505371 = 0.3729989230632782 + 2.5 * 9.206936836242676
Epoch 110, val loss: 0.42623087763786316
Epoch 120, training loss: 23.311309814453125 = 0.35537955164909363 + 2.5 * 9.182372093200684
Epoch 120, val loss: 0.42147690057754517
Epoch 130, training loss: 23.325721740722656 = 0.3428340256214142 + 2.5 * 9.193155288696289
Epoch 130, val loss: 0.4122450649738312
Epoch 140, training loss: 23.299144744873047 = 0.3307943642139435 + 2.5 * 9.187339782714844
Epoch 140, val loss: 0.40379711985588074
Epoch 150, training loss: 23.311668395996094 = 0.32039326429367065 + 2.5 * 9.196510314941406
Epoch 150, val loss: 0.3953302502632141
Epoch 160, training loss: 23.3077392578125 = 0.3116595447063446 + 2.5 * 9.198431968688965
Epoch 160, val loss: 0.3947288691997528
Epoch 170, training loss: 23.271169662475586 = 0.30361446738243103 + 2.5 * 9.18702220916748
Epoch 170, val loss: 0.39102703332901
Epoch 180, training loss: 23.269939422607422 = 0.2966521382331848 + 2.5 * 9.189314842224121
Epoch 180, val loss: 0.3887965977191925
Epoch 190, training loss: 23.2401123046875 = 0.28991779685020447 + 2.5 * 9.180078506469727
Epoch 190, val loss: 0.391988605260849
Epoch 200, training loss: 23.214019775390625 = 0.28366491198539734 + 2.5 * 9.172142028808594
Epoch 200, val loss: 0.38810452818870544
Epoch 210, training loss: 23.23052978515625 = 0.27743998169898987 + 2.5 * 9.181236267089844
Epoch 210, val loss: 0.38257724046707153
Epoch 220, training loss: 23.221115112304688 = 0.27260521054267883 + 2.5 * 9.179403305053711
Epoch 220, val loss: 0.38409149646759033
Epoch 230, training loss: 23.20952033996582 = 0.2664414644241333 + 2.5 * 9.177231788635254
Epoch 230, val loss: 0.38289323449134827
Epoch 240, training loss: 23.231182098388672 = 0.262594997882843 + 2.5 * 9.187435150146484
Epoch 240, val loss: 0.3855065405368805
Epoch 250, training loss: 23.183570861816406 = 0.2567916810512543 + 2.5 * 9.170711517333984
Epoch 250, val loss: 0.39105451107025146
Epoch 260, training loss: 23.154264450073242 = 0.2522978186607361 + 2.5 * 9.160786628723145
Epoch 260, val loss: 0.38637107610702515
Epoch 270, training loss: 23.151674270629883 = 0.24849972128868103 + 2.5 * 9.161270141601562
Epoch 270, val loss: 0.39166268706321716
Epoch 280, training loss: 23.1514892578125 = 0.24350592494010925 + 2.5 * 9.163193702697754
Epoch 280, val loss: 0.39043349027633667
Epoch 290, training loss: 23.135223388671875 = 0.23887260258197784 + 2.5 * 9.158540725708008
Epoch 290, val loss: 0.38969528675079346
Epoch 300, training loss: 23.184823989868164 = 0.23723582923412323 + 2.5 * 9.179035186767578
Epoch 300, val loss: 0.3890620172023773
Epoch 310, training loss: 23.152616500854492 = 0.2311696708202362 + 2.5 * 9.1685791015625
Epoch 310, val loss: 0.3966875374317169
Epoch 320, training loss: 23.102609634399414 = 0.22699984908103943 + 2.5 * 9.150243759155273
Epoch 320, val loss: 0.3972187340259552
Epoch 330, training loss: 23.117027282714844 = 0.22621439397335052 + 2.5 * 9.156325340270996
Epoch 330, val loss: 0.3957432508468628
Epoch 340, training loss: 23.096773147583008 = 0.21842017769813538 + 2.5 * 9.151341438293457
Epoch 340, val loss: 0.41161882877349854
Epoch 350, training loss: 23.124086380004883 = 0.21711555123329163 + 2.5 * 9.162788391113281
Epoch 350, val loss: 0.4036553204059601
Epoch 360, training loss: 23.15163803100586 = 0.21142388880252838 + 2.5 * 9.176085472106934
Epoch 360, val loss: 0.40017932653427124
Epoch 370, training loss: 23.08297348022461 = 0.20745524764060974 + 2.5 * 9.15020751953125
Epoch 370, val loss: 0.4106546938419342
Epoch 380, training loss: 23.091846466064453 = 0.20358839631080627 + 2.5 * 9.155303001403809
Epoch 380, val loss: 0.41295722126960754
Epoch 390, training loss: 23.110931396484375 = 0.20334728062152863 + 2.5 * 9.163033485412598
Epoch 390, val loss: 0.40738335251808167
Epoch 400, training loss: 23.078773498535156 = 0.19847525656223297 + 2.5 * 9.152119636535645
Epoch 400, val loss: 0.4112963080406189
Epoch 410, training loss: 23.07353401184082 = 0.19372139871120453 + 2.5 * 9.151925086975098
Epoch 410, val loss: 0.43282222747802734
Epoch 420, training loss: 23.06516456604004 = 0.187643900513649 + 2.5 * 9.151008605957031
Epoch 420, val loss: 0.41941896080970764
Epoch 430, training loss: 23.07410430908203 = 0.18890570104122162 + 2.5 * 9.15407943725586
Epoch 430, val loss: 0.4223951995372772
Epoch 440, training loss: 23.0458984375 = 0.18039746582508087 + 2.5 * 9.146200180053711
Epoch 440, val loss: 0.43152227997779846
Epoch 450, training loss: 23.06427001953125 = 0.17737950384616852 + 2.5 * 9.154756546020508
Epoch 450, val loss: 0.43782103061676025
Epoch 460, training loss: 23.043376922607422 = 0.17334279417991638 + 2.5 * 9.1480131149292
Epoch 460, val loss: 0.4307693839073181
Epoch 470, training loss: 23.0396728515625 = 0.1688944399356842 + 2.5 * 9.148311614990234
Epoch 470, val loss: 0.443624347448349
Epoch 480, training loss: 23.026046752929688 = 0.16721326112747192 + 2.5 * 9.143533706665039
Epoch 480, val loss: 0.45539170503616333
Epoch 490, training loss: 23.029611587524414 = 0.16372781991958618 + 2.5 * 9.146352767944336
Epoch 490, val loss: 0.45712485909461975
Epoch 500, training loss: 23.036033630371094 = 0.15887488424777985 + 2.5 * 9.150863647460938
Epoch 500, val loss: 0.4569234549999237
Epoch 510, training loss: 22.98271369934082 = 0.15415790677070618 + 2.5 * 9.13142204284668
Epoch 510, val loss: 0.45684531331062317
Epoch 520, training loss: 23.00217628479004 = 0.15047615766525269 + 2.5 * 9.140680313110352
Epoch 520, val loss: 0.46364232897758484
Epoch 530, training loss: 22.98692512512207 = 0.14645667374134064 + 2.5 * 9.136187553405762
Epoch 530, val loss: 0.46957796812057495
Epoch 540, training loss: 22.9744873046875 = 0.14283503592014313 + 2.5 * 9.132660865783691
Epoch 540, val loss: 0.4737686514854431
Epoch 550, training loss: 23.01169776916504 = 0.13990812003612518 + 2.5 * 9.14871597290039
Epoch 550, val loss: 0.4812997579574585
Epoch 560, training loss: 23.000560760498047 = 0.13552115857601166 + 2.5 * 9.146016120910645
Epoch 560, val loss: 0.4824177622795105
Epoch 570, training loss: 22.95572280883789 = 0.1318393051624298 + 2.5 * 9.129552841186523
Epoch 570, val loss: 0.49001744389533997
Epoch 580, training loss: 22.962356567382812 = 0.12811557948589325 + 2.5 * 9.133696556091309
Epoch 580, val loss: 0.4974888265132904
Epoch 590, training loss: 23.01357078552246 = 0.12433736026287079 + 2.5 * 9.155693054199219
Epoch 590, val loss: 0.5113592743873596
Epoch 600, training loss: 23.000654220581055 = 0.1208057776093483 + 2.5 * 9.151939392089844
Epoch 600, val loss: 0.5101212859153748
Epoch 610, training loss: 23.018632888793945 = 0.11734849214553833 + 2.5 * 9.160513877868652
Epoch 610, val loss: 0.5183870792388916
Epoch 620, training loss: 22.94717025756836 = 0.11444272100925446 + 2.5 * 9.13309097290039
Epoch 620, val loss: 0.5281305909156799
Epoch 630, training loss: 22.941913604736328 = 0.11030331999063492 + 2.5 * 9.132643699645996
Epoch 630, val loss: 0.5336811542510986
Epoch 640, training loss: 22.942310333251953 = 0.1100383847951889 + 2.5 * 9.132908821105957
Epoch 640, val loss: 0.5633320212364197
Epoch 650, training loss: 22.981117248535156 = 0.10576996952295303 + 2.5 * 9.150138854980469
Epoch 650, val loss: 0.5452038049697876
Epoch 660, training loss: 22.932329177856445 = 0.10057441145181656 + 2.5 * 9.132701873779297
Epoch 660, val loss: 0.5627390146255493
Epoch 670, training loss: 22.912979125976562 = 0.09870992600917816 + 2.5 * 9.125707626342773
Epoch 670, val loss: 0.5736172795295715
Epoch 680, training loss: 22.911407470703125 = 0.09369131922721863 + 2.5 * 9.127086639404297
Epoch 680, val loss: 0.5784465670585632
Epoch 690, training loss: 22.905248641967773 = 0.09035391360521317 + 2.5 * 9.125958442687988
Epoch 690, val loss: 0.5842574834823608
Epoch 700, training loss: 22.928646087646484 = 0.08745910227298737 + 2.5 * 9.136474609375
Epoch 700, val loss: 0.5904171466827393
Epoch 710, training loss: 22.907184600830078 = 0.08411890268325806 + 2.5 * 9.129225730895996
Epoch 710, val loss: 0.6127519011497498
Epoch 720, training loss: 22.89479637145996 = 0.0816853791475296 + 2.5 * 9.125244140625
Epoch 720, val loss: 0.6070436239242554
Epoch 730, training loss: 22.904062271118164 = 0.0784013420343399 + 2.5 * 9.130264282226562
Epoch 730, val loss: 0.6184662580490112
Epoch 740, training loss: 22.903053283691406 = 0.07680366933345795 + 2.5 * 9.130499839782715
Epoch 740, val loss: 0.6309304237365723
Epoch 750, training loss: 22.922229766845703 = 0.07363937795162201 + 2.5 * 9.139436721801758
Epoch 750, val loss: 0.6383415460586548
Epoch 760, training loss: 22.89593505859375 = 0.07133053243160248 + 2.5 * 9.129841804504395
Epoch 760, val loss: 0.6494764089584351
Epoch 770, training loss: 22.883073806762695 = 0.067847840487957 + 2.5 * 9.126090049743652
Epoch 770, val loss: 0.6552050113677979
Epoch 780, training loss: 22.873281478881836 = 0.06483741849660873 + 2.5 * 9.123377799987793
Epoch 780, val loss: 0.6734509468078613
Epoch 790, training loss: 22.9334716796875 = 0.06230955943465233 + 2.5 * 9.148465156555176
Epoch 790, val loss: 0.6681271195411682
Epoch 800, training loss: 22.875600814819336 = 0.06091184914112091 + 2.5 * 9.125875473022461
Epoch 800, val loss: 0.6949061751365662
Epoch 810, training loss: 22.890689849853516 = 0.057714853435754776 + 2.5 * 9.133190155029297
Epoch 810, val loss: 0.7041233777999878
Epoch 820, training loss: 22.876985549926758 = 0.056559689342975616 + 2.5 * 9.128170013427734
Epoch 820, val loss: 0.7010387182235718
Epoch 830, training loss: 22.873172760009766 = 0.053366776555776596 + 2.5 * 9.127922058105469
Epoch 830, val loss: 0.7209354639053345
Epoch 840, training loss: 22.885629653930664 = 0.05181717127561569 + 2.5 * 9.133524894714355
Epoch 840, val loss: 0.7169415950775146
Epoch 850, training loss: 22.858991622924805 = 0.05111507698893547 + 2.5 * 9.123150825500488
Epoch 850, val loss: 0.7279186844825745
Epoch 860, training loss: 22.873231887817383 = 0.04717109352350235 + 2.5 * 9.130424499511719
Epoch 860, val loss: 0.7565529942512512
Epoch 870, training loss: 22.847105026245117 = 0.04513268172740936 + 2.5 * 9.12078857421875
Epoch 870, val loss: 0.7543760538101196
Epoch 880, training loss: 22.869543075561523 = 0.04468599334359169 + 2.5 * 9.129942893981934
Epoch 880, val loss: 0.7582860589027405
Epoch 890, training loss: 22.845285415649414 = 0.04305679351091385 + 2.5 * 9.120891571044922
Epoch 890, val loss: 0.7876747250556946
Epoch 900, training loss: 22.836278915405273 = 0.04028879851102829 + 2.5 * 9.118395805358887
Epoch 900, val loss: 0.7876814007759094
Epoch 910, training loss: 22.866025924682617 = 0.04076547175645828 + 2.5 * 9.130104064941406
Epoch 910, val loss: 0.8076326251029968
Epoch 920, training loss: 22.852291107177734 = 0.03700260818004608 + 2.5 * 9.126115798950195
Epoch 920, val loss: 0.8008798360824585
Epoch 930, training loss: 22.838111877441406 = 0.035748690366744995 + 2.5 * 9.12094497680664
Epoch 930, val loss: 0.8270537853240967
Epoch 940, training loss: 22.844762802124023 = 0.034649502485990524 + 2.5 * 9.124045372009277
Epoch 940, val loss: 0.830339789390564
Epoch 950, training loss: 22.828062057495117 = 0.03284519910812378 + 2.5 * 9.118086814880371
Epoch 950, val loss: 0.8320557475090027
Epoch 960, training loss: 22.827836990356445 = 0.03175518289208412 + 2.5 * 9.118432998657227
Epoch 960, val loss: 0.8359465599060059
Epoch 970, training loss: 22.84021759033203 = 0.031816646456718445 + 2.5 * 9.123360633850098
Epoch 970, val loss: 0.8577284812927246
run_preliminary.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
/home/mfl5681/project-contrastive/RobustCL/model.py:222: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
Epoch 980, training loss: 22.81816291809082 = 0.029124008491635323 + 2.5 * 9.115615844726562
Epoch 980, val loss: 0.8778612613677979
Epoch 990, training loss: 22.82082748413086 = 0.028098393231630325 + 2.5 * 9.117091178894043
Epoch 990, val loss: 0.8727931380271912
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8447
Overall ASR: 0.7733
Flip ASR: 0.7169/1554 nodes
The final ASR:0.75439, 0.02713, Accuracy:0.84796, 0.00228
#Attach Nodes:80
raw graph: torch.Size([2, 88648])
add edge: torch.Size([2, 97588])
remove edge: torch.Size([2, 79724])
updated graph: torch.Size([2, 88664])
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8513
Overall ASR: 0.8134
Flip ASR: 0.7683/1554 nodes
./selected_nodes/Pubmed/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8493
Overall ASR: 0.8463
Flip ASR: 0.8089/1554 nodes
./selected_nodes/Pubmed/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8534
Overall ASR: 0.8626
Flip ASR: 0.8288/1554 nodes
The final ASR:0.84077, 0.02046, Accuracy:0.85134, 0.00166
