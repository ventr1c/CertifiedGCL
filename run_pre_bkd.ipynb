{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* implement unifyGNN+Contrastive\n",
    "* try different noisy and augmentation\n",
    "* try against with/without unnoticeable "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1640224/2877102333.py:6: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  import imp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(add_edge_rate_1=0, add_edge_rate_2=0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=3, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=3, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.5, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=400, use_vs_number=True, vs_number=160, vs_ratio=0, weight_decay=0.0005)\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# In[1]: \n",
    "\n",
    "import imp\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from torch_geometric.datasets import Planetoid,Reddit2,Flickr,PPI\n",
    "\n",
    "\n",
    "# from torch_geometric.loader import DataLoader\n",
    "# from help_funcs import prune_unrelated_edge,prune_unrelated_edge_isolated\n",
    "import scipy.sparse as sp\n",
    "\n",
    "# Training settings\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--debug', action='store_true',\n",
    "        default=True, help='debug mode')\n",
    "parser.add_argument('--no-cuda', action='store_true', default=False,\n",
    "                    help='Disables CUDA training.')\n",
    "parser.add_argument('--seed', type=int, default=10, help='Random seed.')\n",
    "parser.add_argument('--model', type=str, default='GCN', help='model',\n",
    "                    choices=['GCN','GAT','GraphSage','GIN'])\n",
    "parser.add_argument('--dataset', type=str, default='Cora', \n",
    "                    help='Dataset',\n",
    "                    choices=['Cora','Citeseer','Pubmed','PPI','Flickr','ogbn-arxiv','Reddit','Reddit2','Yelp'])\n",
    "parser.add_argument('--train_lr', type=float, default=0.01,\n",
    "                    help='Initial learning rate.')\n",
    "parser.add_argument('--weight_decay', type=float, default=5e-4,\n",
    "                    help='Weight decay (L2 loss on parameters).')\n",
    "parser.add_argument('--hidden', type=int, default=128,\n",
    "                    help='Number of hidden units.')\n",
    "parser.add_argument('--proj_hidden', type=int, default=128,\n",
    "                    help='Number of hidden units in MLP.')\n",
    "parser.add_argument('--thrd', type=float, default=0.5)\n",
    "parser.add_argument('--target_class', type=int, default=0)\n",
    "parser.add_argument('--dropout', type=float, default=0.5,\n",
    "                    help='Dropout rate (1 - keep probability).')\n",
    "parser.add_argument('--epochs', type=int,  default=200, help='Number of epochs to train benign and backdoor model.')\n",
    "parser.add_argument('--trojan_epochs', type=int,  default=400, help='Number of epochs to train trigger generator.')\n",
    "parser.add_argument('--inner', type=int,  default=1, help='Number of inner')\n",
    "parser.add_argument('--temperature', type=float,  default=0.5, help='Temperature')\n",
    "# backdoor setting\n",
    "parser.add_argument('--lr', type=float, default=0.01,\n",
    "                    help='Initial learning rate.')\n",
    "parser.add_argument('--trigger_size', type=int, default=3,\n",
    "                    help='tirgger_size')\n",
    "parser.add_argument('--use_vs_number', action='store_true', default=True,\n",
    "                    help=\"if use detailed number to decide Vs\")\n",
    "parser.add_argument('--vs_ratio', type=float, default=0,\n",
    "                    help=\"ratio of poisoning nodes relative to the full graph\")\n",
    "parser.add_argument('--vs_number', type=int, default=160,\n",
    "                    help=\"number of poisoning nodes relative to the full graph\")\n",
    "# defense setting\n",
    "parser.add_argument('--defense_mode', type=str, default=\"none\",\n",
    "                    choices=['prune', 'isolate', 'none'],\n",
    "                    help=\"Mode of defense\")\n",
    "parser.add_argument('--prune_thr', type=float, default=0.2,\n",
    "                    help=\"Threshold of prunning edges\")\n",
    "parser.add_argument('--target_loss_weight', type=float, default=1,\n",
    "                    help=\"Weight of optimize outter trigger generator\")\n",
    "parser.add_argument('--homo_loss_weight', type=float, default=100,\n",
    "                    help=\"Weight of optimize similarity loss\")\n",
    "parser.add_argument('--homo_boost_thrd', type=float, default=0.5,\n",
    "                    help=\"Threshold of increase similarity\")\n",
    "# attack setting\n",
    "parser.add_argument('--dis_weight', type=float, default=1,\n",
    "                    help=\"Weight of cluster distance\")\n",
    "parser.add_argument('--selection_method', type=str, default='cluster_degree',\n",
    "                    choices=['loss','conf','cluster','none','cluster_degree'],\n",
    "                    help='Method to select idx_attach for training trojan model (none means randomly select)')\n",
    "parser.add_argument('--test_model', type=str, default='GCN',\n",
    "                    choices=['GCN','GAT','GraphSage','GIN'],\n",
    "                    help='Model used to attack')\n",
    "parser.add_argument('--evaluate_mode', type=str, default='1by1',\n",
    "                    choices=['overall','1by1'],\n",
    "                    help='Model used to attack')\n",
    "# GPU setting\n",
    "parser.add_argument('--device_id', type=int, default=3,\n",
    "                    help=\"Threshold of prunning edges\")\n",
    "# GRACE setting\n",
    "parser.add_argument('--gpu_id', type=int, default=0)\n",
    "parser.add_argument('--config', type=str, default=\"config.yaml\")\n",
    "## Contrasitve setting\n",
    "parser.add_argument('--cl_lr', type=float, default=0.0002)\n",
    "parser.add_argument('--cl_num_hidden', type=int, default=128)\n",
    "parser.add_argument('--cl_num_proj_hidden', type=int, default=128)\n",
    "parser.add_argument('--cl_num_layers', type=int, default=2)\n",
    "parser.add_argument('--cl_activation', type=str, default='relu')\n",
    "parser.add_argument('--cl_base_model', type=str, default='GCNConv')\n",
    "parser.add_argument('--cont_weight', type=float, default=3)\n",
    "parser.add_argument('--add_edge_rate_1', type=float, default=0)\n",
    "parser.add_argument('--add_edge_rate_2', type=float, default=0)\n",
    "parser.add_argument('--drop_edge_rate_1', type=float, default=0.3)\n",
    "parser.add_argument('--drop_edge_rate_2', type=float, default=0.5)\n",
    "parser.add_argument('--drop_feat_rate_1', type=float, default=0.4)\n",
    "parser.add_argument('--drop_feat_rate_2', type=float, default=0.4)\n",
    "parser.add_argument('--tau', type=float, default=0.4)\n",
    "parser.add_argument('--cl_num_epochs', type=int, default=1000)\n",
    "parser.add_argument('--cl_weight_decay', type=float, default=0.00001)\n",
    "parser.add_argument('--cont_batch_size', type=int, default=0)\n",
    "\n",
    "# args = parser.parse_args()\n",
    "args = parser.parse_known_args()[0]\n",
    "args.cuda =  not args.no_cuda and torch.cuda.is_available()\n",
    "device = torch.device(('cuda:{}' if torch.cuda.is_available() else 'cpu').format(args.device_id))\n",
    "\n",
    "np.random.seed(args.seed)\n",
    "# torch.manual_seed(args.seed)\n",
    "# torch.cuda.manual_seed(args.seed)\n",
    "print(args)\n",
    "#%%\n",
    "from torch_geometric.utils import to_undirected\n",
    "import torch_geometric.transforms as T\n",
    "transform = T.Compose([T.NormalizeFeatures()])\n",
    "\n",
    "if(args.dataset == 'Cora' or args.dataset == 'Citeseer' or args.dataset == 'Pubmed'):\n",
    "    dataset = Planetoid(root='./data/', \\\n",
    "                        name=args.dataset,\\\n",
    "                        transform=transform)\n",
    "elif(args.dataset == 'Flickr'):\n",
    "    dataset = Flickr(root='./data/Flickr/', \\\n",
    "                    transform=transform)\n",
    "elif(args.dataset == 'Reddit2'):\n",
    "    dataset = Reddit2(root='./data/Reddit2/', \\\n",
    "                    transform=transform)\n",
    "elif(args.dataset == 'ogbn-arxiv'):\n",
    "    from ogb.nodeproppred import PygNodePropPredDataset\n",
    "    # Download and process data at './dataset/ogbg_molhiv/'\n",
    "    dataset = PygNodePropPredDataset(name = 'ogbn-arxiv', root='./data/')\n",
    "    split_idx = dataset.get_idx_split() \n",
    "\n",
    "data = dataset[0].to(device)\n",
    "\n",
    "if(args.dataset == 'ogbn-arxiv'):\n",
    "    nNode = data.x.shape[0]\n",
    "    setattr(data,'train_mask',torch.zeros(nNode, dtype=torch.bool).to(device))\n",
    "    # dataset[0].train_mask = torch.zeros(nEdge, dtype=torch.bool).to(device)\n",
    "    data.val_mask = torch.zeros(nNode, dtype=torch.bool).to(device)\n",
    "    data.test_mask = torch.zeros(nNode, dtype=torch.bool).to(device)\n",
    "    data.y = data.y.squeeze(1)\n",
    "# we build our own train test split \n",
    "#%% \n",
    "from utils import get_split\n",
    "data, idx_train, idx_val, idx_clean_test, idx_atk = get_split(args,data,device)\n",
    "\n",
    "from torch_geometric.utils import to_undirected\n",
    "from utils import subgraph\n",
    "data.edge_index = to_undirected(data.edge_index)\n",
    "train_edge_index,_, edge_mask = subgraph(torch.bitwise_not(data.test_mask),data.edge_index,relabel_nodes=False)\n",
    "mask_edge_index = data.edge_index[:,torch.bitwise_not(edge_mask)]\n",
    "# filter out the unlabeled nodes except from training nodes and testing nodes, nonzero() is to get index, flatten is to get 1-d tensor\n",
    "unlabeled_idx = (torch.bitwise_not(data.test_mask)&torch.bitwise_not(data.train_mask)).nonzero().flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.utils import to_dense_adj, dense_to_sparse\n",
    "from torch_geometric.data import Data, InMemoryDataset\n",
    "\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "from scipy.linalg import expm\n",
    "\n",
    "import torch\n",
    "from torch_geometric.data import Data, InMemoryDataset\n",
    "from torch_geometric.datasets import Planetoid, Amazon, Coauthor\n",
    "\n",
    "# from seeds import development_seed\n",
    "DATA_PATH = './data/'\n",
    "\n",
    "def get_dataset(name: str, use_lcc: bool = True) -> InMemoryDataset:\n",
    "    path = os.path.join(DATA_PATH, name)\n",
    "    if name in ['Cora', 'Citeseer', 'Pubmed']:\n",
    "        dataset = Planetoid(path, name)\n",
    "    elif name in ['Computers', 'Photo']:\n",
    "        dataset = Amazon(path, name)\n",
    "    elif name == 'CoauthorCS':\n",
    "        dataset = Coauthor(path, 'CS')\n",
    "    else:\n",
    "        raise Exception('Unknown dataset.')\n",
    "\n",
    "    if use_lcc:\n",
    "        lcc = get_largest_connected_component(dataset)\n",
    "\n",
    "        x_new = dataset.data.x[lcc]\n",
    "        y_new = dataset.data.y[lcc]\n",
    "\n",
    "        row, col = dataset.data.edge_index.numpy()\n",
    "        edges = [[i, j] for i, j in zip(row, col) if i in lcc and j in lcc]\n",
    "        edges = remap_edges(edges, get_node_mapper(lcc))\n",
    "        \n",
    "        data = Data(\n",
    "            x=x_new,\n",
    "            edge_index=torch.LongTensor(edges),\n",
    "            y=y_new,\n",
    "            train_mask=torch.zeros(y_new.size()[0], dtype=torch.bool),\n",
    "            test_mask=torch.zeros(y_new.size()[0], dtype=torch.bool),\n",
    "            val_mask=torch.zeros(y_new.size()[0], dtype=torch.bool)\n",
    "        )\n",
    "        dataset.data = data\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def get_component(dataset: InMemoryDataset, start: int = 0) -> set:\n",
    "    visited_nodes = set()\n",
    "    queued_nodes = set([start])\n",
    "    row, col = dataset.data.edge_index.numpy()\n",
    "    while queued_nodes:\n",
    "        current_node = queued_nodes.pop()\n",
    "        visited_nodes.update([current_node])\n",
    "        neighbors = col[np.where(row == current_node)[0]]\n",
    "        neighbors = [n for n in neighbors if n not in visited_nodes and n not in queued_nodes]\n",
    "        queued_nodes.update(neighbors)\n",
    "    return visited_nodes\n",
    "\n",
    "\n",
    "def get_largest_connected_component(dataset: InMemoryDataset) -> np.ndarray:\n",
    "    remaining_nodes = set(range(dataset.data.x.shape[0]))\n",
    "    comps = []\n",
    "    while remaining_nodes:\n",
    "        start = min(remaining_nodes)\n",
    "        comp = get_component(dataset, start)\n",
    "        comps.append(comp)\n",
    "        remaining_nodes = remaining_nodes.difference(comp)\n",
    "    return np.array(list(comps[np.argmax(list(map(len, comps)))]))\n",
    "\n",
    "\n",
    "def get_node_mapper(lcc: np.ndarray) -> dict:\n",
    "    mapper = {}\n",
    "    counter = 0\n",
    "    for node in lcc:\n",
    "        mapper[node] = counter\n",
    "        counter += 1\n",
    "    return mapper\n",
    "\n",
    "\n",
    "def remap_edges(edges: list, mapper: dict) -> list:\n",
    "    row = [e[0] for e in edges]\n",
    "    col = [e[1] for e in edges]\n",
    "    row = list(map(lambda x: mapper[x], row))\n",
    "    col = list(map(lambda x: mapper[x], col))\n",
    "    return [row, col]\n",
    "\n",
    "\n",
    "def get_adj_matrix(data) -> np.ndarray:\n",
    "    num_nodes = data.x.shape[0]\n",
    "    adj_matrix = np.zeros(shape=(num_nodes, num_nodes))\n",
    "    for i, j in zip(data.edge_index[0], data.edge_index[1]):\n",
    "        adj_matrix[i, j] = 1.\n",
    "    return adj_matrix\n",
    "\n",
    "\n",
    "def get_ppr_matrix(\n",
    "        adj_matrix: np.ndarray,\n",
    "        alpha: float = 0.1) -> np.ndarray:\n",
    "    num_nodes = adj_matrix.shape[0]\n",
    "    A_tilde = adj_matrix + np.eye(num_nodes)\n",
    "    D_tilde = np.diag(1/np.sqrt(A_tilde.sum(axis=1)))\n",
    "    H = D_tilde @ A_tilde @ D_tilde\n",
    "    return alpha * np.linalg.inv(np.eye(num_nodes) - (1 - alpha) * H)\n",
    "\n",
    "\n",
    "def get_heat_matrix(\n",
    "        adj_matrix: np.ndarray,\n",
    "        t: float = 5.0) -> np.ndarray:\n",
    "    num_nodes = adj_matrix.shape[0]\n",
    "    A_tilde = adj_matrix + np.eye(num_nodes)\n",
    "    D_tilde = np.diag(1/np.sqrt(A_tilde.sum(axis=1)))\n",
    "    H = D_tilde @ A_tilde @ D_tilde\n",
    "    return expm(-t * (np.eye(num_nodes) - H))\n",
    "\n",
    "\n",
    "def get_top_k_matrix(A: np.ndarray, k: int = 128) -> np.ndarray:\n",
    "    num_nodes = A.shape[0]\n",
    "    row_idx = np.arange(num_nodes)\n",
    "    A[A.argsort(axis=0)[:num_nodes - k], row_idx] = 0.\n",
    "    norm = A.sum(axis=0)\n",
    "    norm[norm <= 0] = 1 # avoid dividing by zero\n",
    "    return A/norm\n",
    "\n",
    "\n",
    "def get_clipped_matrix(A: np.ndarray, eps: float = 0.01) -> np.ndarray:\n",
    "    num_nodes = A.shape[0]\n",
    "    A[A < eps] = 0.\n",
    "    norm = A.sum(axis=0)\n",
    "    norm[norm <= 0] = 1 # avoid dividing by zero\n",
    "    return A/norm\n",
    "\n",
    "\n",
    "def set_train_val_test_split(\n",
    "        seed: int,\n",
    "        data: Data,\n",
    "        num_development: int = 1500,\n",
    "        num_per_class: int = 20) -> Data:\n",
    "    rnd_state = np.random.RandomState(development_seed)\n",
    "    num_nodes = data.y.shape[0]\n",
    "    development_idx = rnd_state.choice(num_nodes, num_development, replace=False)\n",
    "    test_idx = [i for i in np.arange(num_nodes) if i not in development_idx]\n",
    "\n",
    "    train_idx = []\n",
    "    rnd_state = np.random.RandomState(seed)\n",
    "    for c in range(data.y.max() + 1):\n",
    "        class_idx = development_idx[np.where(data.y[development_idx].cpu() == c)[0]]\n",
    "        train_idx.extend(rnd_state.choice(class_idx, num_per_class, replace=False))\n",
    "\n",
    "    val_idx = [i for i in development_idx if i not in train_idx]\n",
    "\n",
    "    def get_mask(idx):\n",
    "        mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "        mask[idx] = 1\n",
    "        return mask\n",
    "\n",
    "    data.train_mask = get_mask(train_idx)\n",
    "    data.val_mask = get_mask(val_idx)\n",
    "    data.test_mask = get_mask(test_idx)\n",
    "\n",
    "    return data\n",
    "\n",
    "class PPRDataset(InMemoryDataset):\n",
    "    \"\"\"\n",
    "    Dataset preprocessed with GDC using PPR diffusion.\n",
    "    Note that this implementations is not scalable\n",
    "    since we directly invert the adjacency matrix.\n",
    "    \"\"\"\n",
    "    def __init__(self,noisy_data,\n",
    "                 name: str = 'Cora',\n",
    "                 use_lcc: bool = True,\n",
    "                 alpha: float = 0.1,\n",
    "                 k: int = 16,\n",
    "                 eps: float = None):\n",
    "        self.name = name\n",
    "        self.use_lcc = use_lcc\n",
    "        self.alpha = alpha\n",
    "        self.k = k\n",
    "        self.eps = eps\n",
    "        self.noisy_data = noisy_data\n",
    "\n",
    "        super(PPRDataset, self).__init__(DATA_PATH)\n",
    "        self.data, self.slices = torch.load(self.processed_paths[0])\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self) -> list:\n",
    "        return []\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self) -> list:\n",
    "        return [str(self) + '.pt']\n",
    "\n",
    "    def download(self):\n",
    "        pass\n",
    "\n",
    "    def process(self):\n",
    "        # base = get_dataset(name=self.name, use_lcc=self.use_lcc)\n",
    "        # generate adjacency matrix from sparse representation\n",
    "        adj_matrix = get_adj_matrix(self.noisy_data)\n",
    "        # obtain exact PPR matrix\n",
    "        ppr_matrix = get_ppr_matrix(adj_matrix,\n",
    "                                        alpha=self.alpha)\n",
    "\n",
    "        if self.k:\n",
    "            print(f'Selecting top {self.k} edges per node.')\n",
    "            ppr_matrix = get_top_k_matrix(ppr_matrix, k=self.k)\n",
    "        elif self.eps:\n",
    "            print(f'Selecting edges with weight greater than {self.eps}.')\n",
    "            ppr_matrix = get_clipped_matrix(ppr_matrix, eps=self.eps)\n",
    "        else:\n",
    "            raise ValueError\n",
    "\n",
    "        # create PyG Data object\n",
    "        edges_i = []\n",
    "        edges_j = []\n",
    "        edge_attr = []\n",
    "        for i, row in enumerate(ppr_matrix):\n",
    "            for j in np.where(row > 0)[0]:\n",
    "                edges_i.append(i)\n",
    "                edges_j.append(j)\n",
    "                edge_attr.append(ppr_matrix[i, j])\n",
    "        edge_index = [edges_i, edges_j]\n",
    "\n",
    "        data = Data(\n",
    "            x=self.noisy_data.x,\n",
    "            edge_index=torch.LongTensor(edge_index),\n",
    "            edge_attr=torch.FloatTensor(edge_attr),\n",
    "            y=self.noisy_data.y,\n",
    "            train_mask=torch.zeros(self.noisy_data.train_mask.size()[0], dtype=torch.bool),\n",
    "            test_mask=torch.zeros(self.noisy_data.test_mask.size()[0], dtype=torch.bool),\n",
    "            val_mask=torch.zeros(self.noisy_data.val_mask.size()[0], dtype=torch.bool)\n",
    "        )\n",
    "\n",
    "        data, slices = self.collate([data])\n",
    "        torch.save((data, slices), self.processed_paths[0])\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return f'{self.name}_ppr_alpha={self.alpha}_k={self.k}_eps={self.eps}_lcc={self.use_lcc}'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.utils import to_dense_adj, dense_to_sparse\n",
    "from torch_geometric.data import Data, InMemoryDataset\n",
    "\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "from scipy.linalg import expm\n",
    "\n",
    "import torch\n",
    "from torch_geometric.data import Data, InMemoryDataset\n",
    "from torch_geometric.datasets import Planetoid, Amazon, Coauthor\n",
    "\n",
    "# from seeds import development_seed\n",
    "DATA_PATH = './data/'\n",
    "\n",
    "def get_dataset(name: str, use_lcc: bool = True) -> InMemoryDataset:\n",
    "    path = os.path.join(DATA_PATH, name)\n",
    "    if name in ['Cora', 'Citeseer', 'Pubmed']:\n",
    "        dataset = Planetoid(path, name)\n",
    "    elif name in ['Computers', 'Photo']:\n",
    "        dataset = Amazon(path, name)\n",
    "    elif name == 'CoauthorCS':\n",
    "        dataset = Coauthor(path, 'CS')\n",
    "    else:\n",
    "        raise Exception('Unknown dataset.')\n",
    "\n",
    "    if use_lcc:\n",
    "        lcc = get_largest_connected_component(dataset)\n",
    "\n",
    "        x_new = dataset.data.x[lcc]\n",
    "        y_new = dataset.data.y[lcc]\n",
    "\n",
    "        row, col = dataset.data.edge_index.numpy()\n",
    "        edges = [[i, j] for i, j in zip(row, col) if i in lcc and j in lcc]\n",
    "        edges = remap_edges(edges, get_node_mapper(lcc))\n",
    "        \n",
    "        data = Data(\n",
    "            x=x_new,\n",
    "            edge_index=torch.LongTensor(edges),\n",
    "            y=y_new,\n",
    "            train_mask=torch.zeros(y_new.size()[0], dtype=torch.bool),\n",
    "            test_mask=torch.zeros(y_new.size()[0], dtype=torch.bool),\n",
    "            val_mask=torch.zeros(y_new.size()[0], dtype=torch.bool)\n",
    "        )\n",
    "        dataset.data = data\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def get_component(dataset: InMemoryDataset, start: int = 0) -> set:\n",
    "    visited_nodes = set()\n",
    "    queued_nodes = set([start])\n",
    "    row, col = dataset.data.edge_index.numpy()\n",
    "    while queued_nodes:\n",
    "        current_node = queued_nodes.pop()\n",
    "        visited_nodes.update([current_node])\n",
    "        neighbors = col[np.where(row == current_node)[0]]\n",
    "        neighbors = [n for n in neighbors if n not in visited_nodes and n not in queued_nodes]\n",
    "        queued_nodes.update(neighbors)\n",
    "    return visited_nodes\n",
    "\n",
    "\n",
    "def get_largest_connected_component(dataset: InMemoryDataset) -> np.ndarray:\n",
    "    remaining_nodes = set(range(dataset.data.x.shape[0]))\n",
    "    comps = []\n",
    "    while remaining_nodes:\n",
    "        start = min(remaining_nodes)\n",
    "        comp = get_component(dataset, start)\n",
    "        comps.append(comp)\n",
    "        remaining_nodes = remaining_nodes.difference(comp)\n",
    "    return np.array(list(comps[np.argmax(list(map(len, comps)))]))\n",
    "\n",
    "\n",
    "def get_node_mapper(lcc: np.ndarray) -> dict:\n",
    "    mapper = {}\n",
    "    counter = 0\n",
    "    for node in lcc:\n",
    "        mapper[node] = counter\n",
    "        counter += 1\n",
    "    return mapper\n",
    "\n",
    "\n",
    "def remap_edges(edges: list, mapper: dict) -> list:\n",
    "    row = [e[0] for e in edges]\n",
    "    col = [e[1] for e in edges]\n",
    "    row = list(map(lambda x: mapper[x], row))\n",
    "    col = list(map(lambda x: mapper[x], col))\n",
    "    return [row, col]\n",
    "\n",
    "\n",
    "def get_adj_matrix(data) -> np.ndarray:\n",
    "    num_nodes = data.x.shape[0]\n",
    "    adj_matrix = np.zeros(shape=(num_nodes, num_nodes))\n",
    "    for i, j in zip(data.edge_index[0], data.edge_index[1]):\n",
    "        adj_matrix[i, j] = 1.\n",
    "    return adj_matrix\n",
    "\n",
    "\n",
    "def get_ppr_matrix(\n",
    "        adj_matrix: np.ndarray,\n",
    "        alpha: float = 0.1) -> np.ndarray:\n",
    "    num_nodes = adj_matrix.shape[0]\n",
    "    A_tilde = adj_matrix + np.eye(num_nodes)\n",
    "    D_tilde = np.diag(1/np.sqrt(A_tilde.sum(axis=1)))\n",
    "    H = D_tilde @ A_tilde @ D_tilde\n",
    "    return alpha * np.linalg.inv(np.eye(num_nodes) - (1 - alpha) * H)\n",
    "\n",
    "\n",
    "def get_heat_matrix(\n",
    "        adj_matrix: np.ndarray,\n",
    "        t: float = 5.0) -> np.ndarray:\n",
    "    num_nodes = adj_matrix.shape[0]\n",
    "    A_tilde = adj_matrix + np.eye(num_nodes)\n",
    "    D_tilde = np.diag(1/np.sqrt(A_tilde.sum(axis=1)))\n",
    "    H = D_tilde @ A_tilde @ D_tilde\n",
    "    return expm(-t * (np.eye(num_nodes) - H))\n",
    "\n",
    "\n",
    "def get_top_k_matrix(A: np.ndarray, k: int = 128) -> np.ndarray:\n",
    "    num_nodes = A.shape[0]\n",
    "    row_idx = np.arange(num_nodes)\n",
    "    A[A.argsort(axis=0)[:num_nodes - k], row_idx] = 0.\n",
    "    norm = A.sum(axis=0)\n",
    "    norm[norm <= 0] = 1 # avoid dividing by zero\n",
    "    return A/norm\n",
    "\n",
    "\n",
    "def get_clipped_matrix(A: np.ndarray, eps: float = 0.01) -> np.ndarray:\n",
    "    num_nodes = A.shape[0]\n",
    "    A[A < eps] = 0.\n",
    "    norm = A.sum(axis=0)\n",
    "    norm[norm <= 0] = 1 # avoid dividing by zero\n",
    "    return A/norm\n",
    "\n",
    "\n",
    "def set_train_val_test_split(\n",
    "        seed: int,\n",
    "        data: Data,\n",
    "        num_development: int = 1500,\n",
    "        num_per_class: int = 20) -> Data:\n",
    "    rnd_state = np.random.RandomState(development_seed)\n",
    "    num_nodes = data.y.shape[0]\n",
    "    development_idx = rnd_state.choice(num_nodes, num_development, replace=False)\n",
    "    test_idx = [i for i in np.arange(num_nodes) if i not in development_idx]\n",
    "\n",
    "    train_idx = []\n",
    "    rnd_state = np.random.RandomState(seed)\n",
    "    for c in range(data.y.max() + 1):\n",
    "        class_idx = development_idx[np.where(data.y[development_idx].cpu() == c)[0]]\n",
    "        train_idx.extend(rnd_state.choice(class_idx, num_per_class, replace=False))\n",
    "\n",
    "    val_idx = [i for i in development_idx if i not in train_idx]\n",
    "\n",
    "    def get_mask(idx):\n",
    "        mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "        mask[idx] = 1\n",
    "        return mask\n",
    "\n",
    "    data.train_mask = get_mask(train_idx)\n",
    "    data.val_mask = get_mask(val_idx)\n",
    "    data.test_mask = get_mask(test_idx)\n",
    "\n",
    "    return data\n",
    "\n",
    "class PPRDataset(InMemoryDataset):\n",
    "    \"\"\"\n",
    "    Dataset preprocessed with GDC using PPR diffusion.\n",
    "    Note that this implementations is not scalable\n",
    "    since we directly invert the adjacency matrix.\n",
    "    \"\"\"\n",
    "    def __init__(self,noisy_data,\n",
    "                 name: str = 'Cora',\n",
    "                 use_lcc: bool = True,\n",
    "                 alpha: float = 0.1,\n",
    "                 k: int = 16,\n",
    "                 eps: float = None):\n",
    "        self.name = name\n",
    "        self.use_lcc = use_lcc\n",
    "        self.alpha = alpha\n",
    "        self.k = k\n",
    "        self.eps = eps\n",
    "        self.noisy_data = noisy_data\n",
    "\n",
    "        super(PPRDataset, self).__init__(DATA_PATH)\n",
    "        self.data, self.slices = torch.load(self.processed_paths[0])\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self) -> list:\n",
    "        return []\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self) -> list:\n",
    "        return [str(self) + '.pt']\n",
    "\n",
    "    def download(self):\n",
    "        pass\n",
    "\n",
    "    def process(self):\n",
    "        # base = get_dataset(name=self.name, use_lcc=self.use_lcc)\n",
    "        # generate adjacency matrix from sparse representation\n",
    "        adj_matrix = get_adj_matrix(self.noisy_data)\n",
    "        # obtain exact PPR matrix\n",
    "        ppr_matrix = get_ppr_matrix(adj_matrix,\n",
    "                                        alpha=self.alpha)\n",
    "\n",
    "        if self.k:\n",
    "            print(f'Selecting top {self.k} edges per node.')\n",
    "            ppr_matrix = get_top_k_matrix(ppr_matrix, k=self.k)\n",
    "        elif self.eps:\n",
    "            print(f'Selecting edges with weight greater than {self.eps}.')\n",
    "            ppr_matrix = get_clipped_matrix(ppr_matrix, eps=self.eps)\n",
    "        else:\n",
    "            raise ValueError\n",
    "\n",
    "        # create PyG Data object\n",
    "        edges_i = []\n",
    "        edges_j = []\n",
    "        edge_attr = []\n",
    "        for i, row in enumerate(ppr_matrix):\n",
    "            for j in np.where(row > 0)[0]:\n",
    "                edges_i.append(i)\n",
    "                edges_j.append(j)\n",
    "                edge_attr.append(ppr_matrix[i, j])\n",
    "        edge_index = [edges_i, edges_j]\n",
    "\n",
    "        data = Data(\n",
    "            x=self.noisy_data.x,\n",
    "            edge_index=torch.LongTensor(edge_index),\n",
    "            edge_attr=torch.FloatTensor(edge_attr),\n",
    "            y=self.noisy_data.y,\n",
    "            train_mask=torch.zeros(self.noisy_data.train_mask.size()[0], dtype=torch.bool),\n",
    "            test_mask=torch.zeros(self.noisy_data.test_mask.size()[0], dtype=torch.bool),\n",
    "            val_mask=torch.zeros(self.noisy_data.val_mask.size()[0], dtype=torch.bool)\n",
    "        )\n",
    "\n",
    "        data, slices = self.collate([data])\n",
    "        torch.save((data, slices), self.processed_paths[0])\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return f'{self.name}_ppr_alpha={self.alpha}_k={self.k}_eps={self.eps}_lcc={self.use_lcc}'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unify Contrastive GNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backdoor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GNN+CL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Attach Nodes:160\n",
      "./selected_nodes/Cora/Overall/seed265/nodes.txt\n",
      "precent of left attach nodes: 1.000\n",
      "=== training gcn model ===\n",
      "Epoch 0, training loss: 27.061141967773438 = 1.9564933776855469 + 3 * 8.368216514587402\n",
      "Epoch 0, val loss: 1.9341914653778076\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.labels = torch.tensor(labels, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, training loss: 26.821531295776367 = 1.8736979961395264 + 3 * 8.31594467163086\n",
      "Epoch 10, val loss: 1.7186591625213623\n",
      "Epoch 20, training loss: 26.673852920532227 = 1.7359130382537842 + 3 * 8.312646865844727\n",
      "Epoch 20, val loss: 1.3621045351028442\n",
      "Epoch 30, training loss: 26.531566619873047 = 1.6050291061401367 + 3 * 8.308845520019531\n",
      "Epoch 30, val loss: 1.0306048393249512\n",
      "Epoch 40, training loss: 26.47280502319336 = 1.5591599941253662 + 3 * 8.304548263549805\n",
      "Epoch 40, val loss: 0.9040172696113586\n",
      "Epoch 50, training loss: 26.43131446838379 = 1.5448170900344849 + 3 * 8.295498847961426\n",
      "Epoch 50, val loss: 0.8852729797363281\n",
      "Epoch 60, training loss: 26.32436180114746 = 1.526318907737732 + 3 * 8.266014099121094\n",
      "Epoch 60, val loss: 0.8926793932914734\n",
      "Epoch 70, training loss: 25.806407928466797 = 1.5122653245925903 + 3 * 8.098047256469727\n",
      "Epoch 70, val loss: 0.8970462679862976\n",
      "Epoch 80, training loss: 25.07903480529785 = 1.4985525608062744 + 3 * 7.860160827636719\n",
      "Epoch 80, val loss: 0.8869006037712097\n",
      "Epoch 90, training loss: 24.427364349365234 = 1.4868099689483643 + 3 * 7.646851062774658\n",
      "Epoch 90, val loss: 0.8813305497169495\n",
      "Epoch 100, training loss: 23.73305320739746 = 1.4760973453521729 + 3 * 7.418985366821289\n",
      "Epoch 100, val loss: 0.8796442747116089\n",
      "Epoch 110, training loss: 23.02136993408203 = 1.4654481410980225 + 3 * 7.185307025909424\n",
      "Epoch 110, val loss: 0.8737494349479675\n",
      "Epoch 120, training loss: 22.418703079223633 = 1.4545263051986694 + 3 * 6.988058567047119\n",
      "Epoch 120, val loss: 0.8681859374046326\n",
      "Epoch 130, training loss: 22.0009822845459 = 1.4431318044662476 + 3 * 6.852616310119629\n",
      "Epoch 130, val loss: 0.8653551936149597\n",
      "Epoch 140, training loss: 21.68600082397461 = 1.4318047761917114 + 3 * 6.75139856338501\n",
      "Epoch 140, val loss: 0.8588207364082336\n",
      "Epoch 150, training loss: 21.422603607177734 = 1.4200637340545654 + 3 * 6.667513370513916\n",
      "Epoch 150, val loss: 0.8450865745544434\n",
      "Epoch 160, training loss: 21.22770118713379 = 1.408482313156128 + 3 * 6.6064066886901855\n",
      "Epoch 160, val loss: 0.8382266759872437\n",
      "Epoch 170, training loss: 21.076475143432617 = 1.3971205949783325 + 3 * 6.559784412384033\n",
      "Epoch 170, val loss: 0.8308801651000977\n",
      "Epoch 180, training loss: 20.93767547607422 = 1.3865981101989746 + 3 * 6.517025470733643\n",
      "Epoch 180, val loss: 0.8234315514564514\n",
      "Epoch 190, training loss: 20.796112060546875 = 1.3773161172866821 + 3 * 6.4729323387146\n",
      "Epoch 190, val loss: 0.8170323967933655\n",
      "Epoch 200, training loss: 20.658504486083984 = 1.3683991432189941 + 3 * 6.43003511428833\n",
      "Epoch 200, val loss: 0.8119650483131409\n",
      "Epoch 210, training loss: 20.554519653320312 = 1.3597949743270874 + 3 * 6.398241996765137\n",
      "Epoch 210, val loss: 0.8091633915901184\n",
      "Epoch 220, training loss: 20.498289108276367 = 1.3513028621673584 + 3 * 6.382328510284424\n",
      "Epoch 220, val loss: 0.805967390537262\n",
      "Epoch 230, training loss: 20.42165756225586 = 1.3423700332641602 + 3 * 6.359763145446777\n",
      "Epoch 230, val loss: 0.8034688234329224\n",
      "Epoch 240, training loss: 20.35685920715332 = 1.3332685232162476 + 3 * 6.341196537017822\n",
      "Epoch 240, val loss: 0.8011107444763184\n",
      "Epoch 250, training loss: 20.31306266784668 = 1.3240246772766113 + 3 * 6.329679489135742\n",
      "Epoch 250, val loss: 0.7982982993125916\n",
      "Epoch 260, training loss: 20.314815521240234 = 1.3143465518951416 + 3 * 6.333489418029785\n",
      "Epoch 260, val loss: 0.7954611778259277\n",
      "Epoch 270, training loss: 20.36985206604004 = 1.303910493850708 + 3 * 6.355313777923584\n",
      "Epoch 270, val loss: 0.7926017642021179\n",
      "Epoch 280, training loss: 20.235607147216797 = 1.2928924560546875 + 3 * 6.31423807144165\n",
      "Epoch 280, val loss: 0.7897890210151672\n",
      "Epoch 290, training loss: 20.1728515625 = 1.280940294265747 + 3 * 6.297303199768066\n",
      "Epoch 290, val loss: 0.7855493426322937\n",
      "Epoch 300, training loss: 20.127798080444336 = 1.268480896949768 + 3 * 6.286438941955566\n",
      "Epoch 300, val loss: 0.7819949388504028\n",
      "Epoch 310, training loss: 20.08746337890625 = 1.255171775817871 + 3 * 6.277431011199951\n",
      "Epoch 310, val loss: 0.7771363854408264\n",
      "Epoch 320, training loss: 20.0501708984375 = 1.240946888923645 + 3 * 6.269741058349609\n",
      "Epoch 320, val loss: 0.772140383720398\n",
      "Epoch 330, training loss: 20.03946876525879 = 1.2257016897201538 + 3 * 6.2712554931640625\n",
      "Epoch 330, val loss: 0.7666245102882385\n",
      "Epoch 340, training loss: 20.038089752197266 = 1.2092816829681396 + 3 * 6.276269435882568\n",
      "Epoch 340, val loss: 0.7607452869415283\n",
      "Epoch 350, training loss: 19.980825424194336 = 1.1916491985321045 + 3 * 6.263058662414551\n",
      "Epoch 350, val loss: 0.7544645071029663\n",
      "Epoch 360, training loss: 19.928831100463867 = 1.1730141639709473 + 3 * 6.251939296722412\n",
      "Epoch 360, val loss: 0.7477882504463196\n",
      "Epoch 370, training loss: 19.88817596435547 = 1.1534827947616577 + 3 * 6.244897842407227\n",
      "Epoch 370, val loss: 0.7408434748649597\n",
      "Epoch 380, training loss: 19.84906578063965 = 1.1329811811447144 + 3 * 6.23869514465332\n",
      "Epoch 380, val loss: 0.7333641052246094\n",
      "Epoch 390, training loss: 19.816055297851562 = 1.111565113067627 + 3 * 6.234829902648926\n",
      "Epoch 390, val loss: 0.725809633731842\n",
      "Epoch 400, training loss: 19.785419464111328 = 1.0892059803009033 + 3 * 6.2320709228515625\n",
      "Epoch 400, val loss: 0.7179477214813232\n",
      "Epoch 410, training loss: 19.742708206176758 = 1.0659626722335815 + 3 * 6.225582122802734\n",
      "Epoch 410, val loss: 0.7099491357803345\n",
      "Epoch 420, training loss: 19.715179443359375 = 1.0419435501098633 + 3 * 6.224411964416504\n",
      "Epoch 420, val loss: 0.7017584443092346\n",
      "Epoch 430, training loss: 19.7408504486084 = 1.0171645879745483 + 3 * 6.2412285804748535\n",
      "Epoch 430, val loss: 0.693888247013092\n",
      "Epoch 440, training loss: 19.71713638305664 = 0.991938054561615 + 3 * 6.241733074188232\n",
      "Epoch 440, val loss: 0.6857870817184448\n",
      "Epoch 450, training loss: 19.630979537963867 = 0.9660921692848206 + 3 * 6.2216291427612305\n",
      "Epoch 450, val loss: 0.6777992248535156\n",
      "Epoch 460, training loss: 19.581201553344727 = 0.9402117729187012 + 3 * 6.213663101196289\n",
      "Epoch 460, val loss: 0.6696049571037292\n",
      "Epoch 470, training loss: 19.538372039794922 = 0.9141515493392944 + 3 * 6.208073139190674\n",
      "Epoch 470, val loss: 0.6617968082427979\n",
      "Epoch 480, training loss: 19.499711990356445 = 0.8878848552703857 + 3 * 6.20394229888916\n",
      "Epoch 480, val loss: 0.6544175744056702\n",
      "Epoch 490, training loss: 19.46024513244629 = 0.8612430691719055 + 3 * 6.199667453765869\n",
      "Epoch 490, val loss: 0.6469057202339172\n",
      "Epoch 500, training loss: 19.42345428466797 = 0.8342980742454529 + 3 * 6.196385860443115\n",
      "Epoch 500, val loss: 0.6394485235214233\n",
      "Epoch 510, training loss: 19.390878677368164 = 0.8076850771903992 + 3 * 6.194397449493408\n",
      "Epoch 510, val loss: 0.6328233480453491\n",
      "Epoch 520, training loss: 19.351930618286133 = 0.7812969088554382 + 3 * 6.190210819244385\n",
      "Epoch 520, val loss: 0.6261576414108276\n",
      "Epoch 530, training loss: 19.327077865600586 = 0.7552571892738342 + 3 * 6.190607070922852\n",
      "Epoch 530, val loss: 0.6194646954536438\n",
      "Epoch 540, training loss: 19.289623260498047 = 0.7296060919761658 + 3 * 6.186672210693359\n",
      "Epoch 540, val loss: 0.6132396459579468\n",
      "Epoch 550, training loss: 19.250808715820312 = 0.7045563459396362 + 3 * 6.182084083557129\n",
      "Epoch 550, val loss: 0.6074880957603455\n",
      "Epoch 560, training loss: 19.22288703918457 = 0.6801313161849976 + 3 * 6.180918216705322\n",
      "Epoch 560, val loss: 0.6015779972076416\n",
      "Epoch 570, training loss: 19.215633392333984 = 0.6563611030578613 + 3 * 6.186424255371094\n",
      "Epoch 570, val loss: 0.5962284803390503\n",
      "Epoch 580, training loss: 19.191768646240234 = 0.6333389282226562 + 3 * 6.186143398284912\n",
      "Epoch 580, val loss: 0.5910347700119019\n",
      "Epoch 590, training loss: 19.18157386779785 = 0.6115466952323914 + 3 * 6.190009117126465\n",
      "Epoch 590, val loss: 0.5856950879096985\n",
      "Epoch 600, training loss: 19.162628173828125 = 0.590901792049408 + 3 * 6.190575122833252\n",
      "Epoch 600, val loss: 0.5816018581390381\n",
      "Epoch 610, training loss: 19.10698890686035 = 0.5712196826934814 + 3 * 6.178589344024658\n",
      "Epoch 610, val loss: 0.5773090720176697\n",
      "Epoch 620, training loss: 19.084556579589844 = 0.5527455806732178 + 3 * 6.17726993560791\n",
      "Epoch 620, val loss: 0.5740824341773987\n",
      "Epoch 630, training loss: 19.036684036254883 = 0.5353612303733826 + 3 * 6.167108058929443\n",
      "Epoch 630, val loss: 0.571108341217041\n",
      "Epoch 640, training loss: 19.02151870727539 = 0.5189129114151001 + 3 * 6.167535305023193\n",
      "Epoch 640, val loss: 0.5683542490005493\n",
      "Epoch 650, training loss: 19.023746490478516 = 0.5033063888549805 + 3 * 6.173480033874512\n",
      "Epoch 650, val loss: 0.5657435059547424\n",
      "Epoch 660, training loss: 18.98935317993164 = 0.4884212911128998 + 3 * 6.166977405548096\n",
      "Epoch 660, val loss: 0.5625811815261841\n",
      "Epoch 670, training loss: 18.951269149780273 = 0.4742150902748108 + 3 * 6.159018516540527\n",
      "Epoch 670, val loss: 0.5602051615715027\n",
      "Epoch 680, training loss: 18.924036026000977 = 0.46067604422569275 + 3 * 6.154453277587891\n",
      "Epoch 680, val loss: 0.5574952960014343\n",
      "Epoch 690, training loss: 18.903457641601562 = 0.44776326417922974 + 3 * 6.15189790725708\n",
      "Epoch 690, val loss: 0.5551885366439819\n",
      "Epoch 700, training loss: 18.884078979492188 = 0.4354122579097748 + 3 * 6.149555683135986\n",
      "Epoch 700, val loss: 0.5535019636154175\n",
      "Epoch 710, training loss: 18.876846313476562 = 0.4235233962535858 + 3 * 6.1511077880859375\n",
      "Epoch 710, val loss: 0.5519437193870544\n",
      "Epoch 720, training loss: 18.850528717041016 = 0.4120093882083893 + 3 * 6.146173000335693\n",
      "Epoch 720, val loss: 0.5506641864776611\n",
      "Epoch 730, training loss: 18.83583641052246 = 0.40087929368019104 + 3 * 6.144985675811768\n",
      "Epoch 730, val loss: 0.5490038394927979\n",
      "Epoch 740, training loss: 18.827190399169922 = 0.39008456468582153 + 3 * 6.145701885223389\n",
      "Epoch 740, val loss: 0.5478379130363464\n",
      "Epoch 750, training loss: 18.821914672851562 = 0.3796022832393646 + 3 * 6.147437572479248\n",
      "Epoch 750, val loss: 0.5461266040802002\n",
      "Epoch 760, training loss: 18.787107467651367 = 0.3693425953388214 + 3 * 6.139255046844482\n",
      "Epoch 760, val loss: 0.5447463393211365\n",
      "Epoch 770, training loss: 18.77993392944336 = 0.3593263328075409 + 3 * 6.140202522277832\n",
      "Epoch 770, val loss: 0.543662965297699\n",
      "Epoch 780, training loss: 18.766075134277344 = 0.3495263457298279 + 3 * 6.138850212097168\n",
      "Epoch 780, val loss: 0.5422350764274597\n",
      "Epoch 790, training loss: 18.7459659576416 = 0.33986955881118774 + 3 * 6.135365009307861\n",
      "Epoch 790, val loss: 0.5408831834793091\n",
      "Epoch 800, training loss: 18.742874145507812 = 0.33037230372428894 + 3 * 6.137500762939453\n",
      "Epoch 800, val loss: 0.5398380756378174\n",
      "Epoch 810, training loss: 18.729154586791992 = 0.3210170567035675 + 3 * 6.136046409606934\n",
      "Epoch 810, val loss: 0.538105845451355\n",
      "Epoch 820, training loss: 18.709684371948242 = 0.3117706775665283 + 3 * 6.1326375007629395\n",
      "Epoch 820, val loss: 0.5370264053344727\n",
      "Epoch 830, training loss: 18.70212173461914 = 0.30268800258636475 + 3 * 6.133144378662109\n",
      "Epoch 830, val loss: 0.5360260009765625\n",
      "Epoch 840, training loss: 18.68592071533203 = 0.29370978474617004 + 3 * 6.130736827850342\n",
      "Epoch 840, val loss: 0.5348286628723145\n",
      "Epoch 850, training loss: 18.668298721313477 = 0.28482019901275635 + 3 * 6.12782621383667\n",
      "Epoch 850, val loss: 0.5333771705627441\n",
      "Epoch 860, training loss: 18.662315368652344 = 0.276094526052475 + 3 * 6.128740310668945\n",
      "Epoch 860, val loss: 0.5324627757072449\n",
      "Epoch 870, training loss: 18.64512825012207 = 0.2674861252307892 + 3 * 6.125880718231201\n",
      "Epoch 870, val loss: 0.5315027832984924\n",
      "Epoch 880, training loss: 18.62923240661621 = 0.2590079605579376 + 3 * 6.123408317565918\n",
      "Epoch 880, val loss: 0.5304581522941589\n",
      "Epoch 890, training loss: 18.623762130737305 = 0.2507001459598541 + 3 * 6.124353885650635\n",
      "Epoch 890, val loss: 0.5295284986495972\n",
      "Epoch 900, training loss: 18.613826751708984 = 0.24250182509422302 + 3 * 6.123775005340576\n",
      "Epoch 900, val loss: 0.5290609002113342\n",
      "Epoch 910, training loss: 18.59937858581543 = 0.23443888127803802 + 3 * 6.121646881103516\n",
      "Epoch 910, val loss: 0.5282008647918701\n",
      "Epoch 920, training loss: 18.604148864746094 = 0.22657138109207153 + 3 * 6.125858783721924\n",
      "Epoch 920, val loss: 0.5273561477661133\n",
      "Epoch 930, training loss: 18.575870513916016 = 0.21885374188423157 + 3 * 6.119006156921387\n",
      "Epoch 930, val loss: 0.5266168713569641\n",
      "Epoch 940, training loss: 18.558202743530273 = 0.21134765446186066 + 3 * 6.1156182289123535\n",
      "Epoch 940, val loss: 0.5260786414146423\n",
      "Epoch 950, training loss: 18.555437088012695 = 0.2040131688117981 + 3 * 6.117141246795654\n",
      "Epoch 950, val loss: 0.5257986187934875\n",
      "Epoch 960, training loss: 18.54181671142578 = 0.19686399400234222 + 3 * 6.114984512329102\n",
      "Epoch 960, val loss: 0.5260894298553467\n",
      "Epoch 970, training loss: 18.56247901916504 = 0.19000940024852753 + 3 * 6.124156475067139\n",
      "Epoch 970, val loss: 0.5265427231788635\n",
      "Epoch 980, training loss: 18.527177810668945 = 0.18326088786125183 + 3 * 6.1146392822265625\n",
      "Epoch 980, val loss: 0.5263941884040833\n",
      "Epoch 990, training loss: 18.526126861572266 = 0.17684145271778107 + 3 * 6.116428375244141\n",
      "Epoch 990, val loss: 0.5272791385650635\n",
      "=== picking the best model according to the performance on validation ===\n",
      "target class rate on Vs: 1.0000\n",
      "accuracy on clean test nodes: 0.7333\n",
      "Overall ASR: 0.9963\n",
      "Flip ASR: 0.9956/225 nodes\n",
      "./selected_nodes/Cora/Overall/seed125/nodes.txt\n",
      "precent of left attach nodes: 1.000\n",
      "=== training gcn model ===\n",
      "Epoch 0, training loss: 26.979589462280273 = 1.9345835447311401 + 3 * 8.348335266113281\n",
      "Epoch 0, val loss: 1.8214737176895142\n",
      "Epoch 10, training loss: 26.6838321685791 = 1.6771289110183716 + 3 * 8.335567474365234\n",
      "Epoch 10, val loss: 1.1936709880828857\n",
      "Epoch 20, training loss: 26.555776596069336 = 1.5515235662460327 + 3 * 8.33475112915039\n",
      "Epoch 20, val loss: 0.9055327773094177\n",
      "Epoch 30, training loss: 26.535720825195312 = 1.536177158355713 + 3 * 8.333181381225586\n",
      "Epoch 30, val loss: 0.8685043454170227\n",
      "Epoch 40, training loss: 26.5142879486084 = 1.522887110710144 + 3 * 8.330467224121094\n",
      "Epoch 40, val loss: 0.8703967332839966\n",
      "Epoch 50, training loss: 26.48870086669922 = 1.512514352798462 + 3 * 8.325395584106445\n",
      "Epoch 50, val loss: 0.8777446746826172\n",
      "Epoch 60, training loss: 26.433523178100586 = 1.4998528957366943 + 3 * 8.311223030090332\n",
      "Epoch 60, val loss: 0.8584627509117126\n",
      "Epoch 70, training loss: 26.197052001953125 = 1.4865052700042725 + 3 * 8.236848831176758\n",
      "Epoch 70, val loss: 0.8488666415214539\n",
      "Epoch 80, training loss: 25.020795822143555 = 1.4716562032699585 + 3 * 7.84971284866333\n",
      "Epoch 80, val loss: 0.8372722268104553\n",
      "Epoch 90, training loss: 24.468965530395508 = 1.4552117586135864 + 3 * 7.67125129699707\n",
      "Epoch 90, val loss: 0.8232586979866028\n",
      "Epoch 100, training loss: 24.166973114013672 = 1.4406636953353882 + 3 * 7.575436115264893\n",
      "Epoch 100, val loss: 0.8108053803443909\n",
      "Epoch 110, training loss: 23.85602378845215 = 1.4265215396881104 + 3 * 7.476500988006592\n",
      "Epoch 110, val loss: 0.8004850745201111\n",
      "Epoch 120, training loss: 23.56549072265625 = 1.4138342142105103 + 3 * 7.383885383605957\n",
      "Epoch 120, val loss: 0.7867892384529114\n",
      "Epoch 130, training loss: 23.201122283935547 = 1.4023525714874268 + 3 * 7.266256332397461\n",
      "Epoch 130, val loss: 0.7770424485206604\n",
      "Epoch 140, training loss: 22.75552749633789 = 1.3926862478256226 + 3 * 7.120946884155273\n",
      "Epoch 140, val loss: 0.7704348564147949\n",
      "Epoch 150, training loss: 22.292043685913086 = 1.3845245838165283 + 3 * 6.969172954559326\n",
      "Epoch 150, val loss: 0.7644675374031067\n",
      "Epoch 160, training loss: 21.92255401611328 = 1.3769794702529907 + 3 * 6.848524570465088\n",
      "Epoch 160, val loss: 0.7587281465530396\n",
      "Epoch 170, training loss: 21.665218353271484 = 1.369827389717102 + 3 * 6.765130519866943\n",
      "Epoch 170, val loss: 0.753017783164978\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/mfl5681/project-contrastive/RobustCL/run_pre_bkd.ipynb Cell 9\u001b[0m in \u001b[0;36m<cell line: 26>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bxzz89-02-mfl5681/home/mfl5681/project-contrastive/RobustCL/run_pre_bkd.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=57'>58</a>\u001b[0m test_model \u001b[39m=\u001b[39m UnifyModel(args, encoder, args\u001b[39m.\u001b[39mcl_num_hidden, args\u001b[39m.\u001b[39mcl_num_proj_hidden, num_class, args\u001b[39m.\u001b[39mtau, lr\u001b[39m=\u001b[39margs\u001b[39m.\u001b[39mcl_lr, weight_decay\u001b[39m=\u001b[39margs\u001b[39m.\u001b[39mcl_weight_decay, device\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m)\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bxzz89-02-mfl5681/home/mfl5681/project-contrastive/RobustCL/run_pre_bkd.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=58'>59</a>\u001b[0m \u001b[39m# test_model = UnifyModel(args, encoder, num_hidden, num_proj_hidden, num_class, tau, lr=learning_rate, weight_decay=weight_decay, device=device,data1=noisy_data,data2=diff_noisy_data).to(device)\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bxzz89-02-mfl5681/home/mfl5681/project-contrastive/RobustCL/run_pre_bkd.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=59'>60</a>\u001b[0m test_model\u001b[39m.\u001b[39;49mfit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val\u001b[39m=\u001b[39;49midx_val,train_iters\u001b[39m=\u001b[39;49margs\u001b[39m.\u001b[39;49mcl_num_epochs,cont_iters\u001b[39m=\u001b[39;49margs\u001b[39m.\u001b[39;49mcl_num_epochs,seen_node_idx\u001b[39m=\u001b[39;49mseen_node_idx)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bxzz89-02-mfl5681/home/mfl5681/project-contrastive/RobustCL/run_pre_bkd.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=60'>61</a>\u001b[0m test_embds \u001b[39m=\u001b[39m test_model(poison_x,poison_edge_index,poison_edge_weights)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bxzz89-02-mfl5681/home/mfl5681/project-contrastive/RobustCL/run_pre_bkd.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=61'>62</a>\u001b[0m output \u001b[39m=\u001b[39m test_model\u001b[39m.\u001b[39mclf_head(test_embds)\n",
      "File \u001b[0;32m~/project-contrastive/RobustCL/model.py:234\u001b[0m, in \u001b[0;36mUnifyModel.fit\u001b[0;34m(self, args, x, edge_index, edge_weight, labels, idx_train, idx_val, train_iters, cont_iters, seen_node_idx)\u001b[0m\n\u001b[1;32m    232\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_without_val(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlabels, idx_train, train_iters,verbose\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    233\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 234\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_train_with_val(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlabels, idx_train, idx_val, train_iters)\n",
      "File \u001b[0;32m~/project-contrastive/RobustCL/model.py:302\u001b[0m, in \u001b[0;36mUnifyModel._train_with_val\u001b[0;34m(self, labels, idx_train, idx_val, train_iters, verbose)\u001b[0m\n\u001b[1;32m    297\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m    298\u001b[0m \u001b[39m# output = self.forward(self.features, self.edge_index, self.edge_weight)\u001b[39;00m\n\u001b[1;32m    299\u001b[0m \u001b[39m# loss_train = F.nll_loss(output[idx_train], labels[idx_train])\u001b[39;00m\n\u001b[1;32m    300\u001b[0m \u001b[39m# loss_train.backward()\u001b[39;00m\n\u001b[1;32m    301\u001b[0m \u001b[39m# optimizer.step()\u001b[39;00m\n\u001b[0;32m--> 302\u001b[0m z1 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(x_1, edge_index_1)\n\u001b[1;32m    303\u001b[0m z2 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mforward(x_2, edge_index_2)\n\u001b[1;32m    304\u001b[0m \u001b[39m# h1 = self.projection(z1)\u001b[39;00m\n\u001b[1;32m    305\u001b[0m \u001b[39m# h2 = self.projection(z2)\u001b[39;00m\n",
      "File \u001b[0;32m~/project-contrastive/RobustCL/model.py:156\u001b[0m, in \u001b[0;36mUnifyModel.forward\u001b[0;34m(self, x, edge_index, edge_weights)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x: torch\u001b[39m.\u001b[39mTensor,\n\u001b[1;32m    155\u001b[0m             edge_index: torch\u001b[39m.\u001b[39mTensor, edge_weights\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m torch\u001b[39m.\u001b[39mTensor:\n\u001b[0;32m--> 156\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(x, edge_index, edge_weights)\n",
      "File \u001b[0;32m~/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/project-contrastive/RobustCL/model.py:47\u001b[0m, in \u001b[0;36mEncoder.forward\u001b[0;34m(self, x, edge_index, edge_weights)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x: torch\u001b[39m.\u001b[39mTensor, edge_index: torch\u001b[39m.\u001b[39mTensor, edge_weights\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m     46\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mk):\n\u001b[0;32m---> 47\u001b[0m         x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mactivation(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv[i](x, edge_index))\n\u001b[1;32m     48\u001b[0m     \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch_geometric/nn/conv/gcn_conv.py:197\u001b[0m, in \u001b[0;36mGCNConv.forward\u001b[0;34m(self, x, edge_index, edge_weight)\u001b[0m\n\u001b[1;32m    194\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlin(x)\n\u001b[1;32m    196\u001b[0m \u001b[39m# propagate_type: (x: Tensor, edge_weight: OptTensor)\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpropagate(edge_index, x\u001b[39m=\u001b[39;49mx, edge_weight\u001b[39m=\u001b[39;49medge_weight,\n\u001b[1;32m    198\u001b[0m                      size\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m)\n\u001b[1;32m    200\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    201\u001b[0m     out \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias\n",
      "File \u001b[0;32m~/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py:391\u001b[0m, in \u001b[0;36mMessagePassing.propagate\u001b[0;34m(self, edge_index, size, **kwargs)\u001b[0m\n\u001b[1;32m    388\u001b[0m     \u001b[39mif\u001b[39;00m res \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    389\u001b[0m         aggr_kwargs \u001b[39m=\u001b[39m res[\u001b[39m0\u001b[39m] \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(res, \u001b[39mtuple\u001b[39m) \u001b[39melse\u001b[39;00m res\n\u001b[0;32m--> 391\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49maggregate(out, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49maggr_kwargs)\n\u001b[1;32m    393\u001b[0m \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_aggregate_forward_hooks\u001b[39m.\u001b[39mvalues():\n\u001b[1;32m    394\u001b[0m     res \u001b[39m=\u001b[39m hook(\u001b[39mself\u001b[39m, (aggr_kwargs, ), out)\n",
      "File \u001b[0;32m~/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py:514\u001b[0m, in \u001b[0;36mMessagePassing.aggregate\u001b[0;34m(self, inputs, index, ptr, dim_size)\u001b[0m\n\u001b[1;32m    501\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39maggregate\u001b[39m(\u001b[39mself\u001b[39m, inputs: Tensor, index: Tensor,\n\u001b[1;32m    502\u001b[0m               ptr: Optional[Tensor] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    503\u001b[0m               dim_size: Optional[\u001b[39mint\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m    504\u001b[0m     \u001b[39mr\u001b[39m\u001b[39m\"\"\"Aggregates messages from neighbors as\u001b[39;00m\n\u001b[1;32m    505\u001b[0m \u001b[39m    :math:`\\square_{j \\in \\mathcal{N}(i)}`.\u001b[39;00m\n\u001b[1;32m    506\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[39m    as specified in :meth:`__init__` by the :obj:`aggr` argument.\u001b[39;00m\n\u001b[1;32m    513\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 514\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49maggr_module(inputs, index, ptr\u001b[39m=\u001b[39;49mptr, dim_size\u001b[39m=\u001b[39;49mdim_size,\n\u001b[1;32m    515\u001b[0m                             dim\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnode_dim)\n",
      "File \u001b[0;32m~/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch_geometric/nn/aggr/base.py:109\u001b[0m, in \u001b[0;36mAggregation.__call__\u001b[0;34m(self, x, index, ptr, dim_size, dim, **kwargs)\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[39mif\u001b[39;00m dim_size \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    108\u001b[0m         dim_size \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(index\u001b[39m.\u001b[39mmax()) \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m \u001b[39mif\u001b[39;00m index\u001b[39m.\u001b[39mnumel() \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m \u001b[39melse\u001b[39;00m \u001b[39m0\u001b[39m\n\u001b[0;32m--> 109\u001b[0m     \u001b[39melif\u001b[39;00m index\u001b[39m.\u001b[39mnumel() \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m dim_size \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39mint\u001b[39;49m(index\u001b[39m.\u001b[39;49mmax()):\n\u001b[1;32m    110\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEncountered invalid \u001b[39m\u001b[39m'\u001b[39m\u001b[39mdim_size\u001b[39m\u001b[39m'\u001b[39m\u001b[39m (got \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    111\u001b[0m                          \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mdim_size\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m but expected \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    112\u001b[0m                          \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m>= \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mint\u001b[39m(index\u001b[39m.\u001b[39mmax()) \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    114\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__call__\u001b[39m(x, index, ptr, dim_size, dim, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "''' Contrastive learning to backdoor in Contrastive learning'''\n",
    "from models.GTA import Backdoor\n",
    "import heuristic_selection as hs\n",
    "from models.GCN import GCN\n",
    "from models.construct import model_construct\n",
    "import copy \n",
    "from model import UnifyModel, Encoder\n",
    "from models.construct import model_construct\n",
    "data = data.to(device)\n",
    "# learning_rate = 0.0002\n",
    "# weight_decay = config['weight_decay']\n",
    "num_class = int(data.y.max()+1)\n",
    "\n",
    "\n",
    "size = args.vs_number #int((len(data.test_mask)-data.test_mask.sum())*args.vs_ratio)\n",
    "print(\"#Attach Nodes:{}\".format(size))\n",
    "from models.construct import model_construct\n",
    "result_asr = []\n",
    "result_acc = []\n",
    "data = data.to(device)\n",
    "\n",
    "seen_node_idx = (torch.bitwise_not(data.test_mask)).nonzero().flatten()\n",
    "rs = np.random.RandomState(args.seed)\n",
    "seeds = rs.randint(1000,size=3)\n",
    "# seeds = [args.seed]\n",
    "for seed in seeds:\n",
    "    np.random.seed(seed)\n",
    "    # torch.manual_seed(seed)\n",
    "    # torch.cuda.manual_seed(seed)\n",
    "    args.seed = seed\n",
    "    if(args.selection_method == 'none'):\n",
    "        idx_attach = hs.obtain_attach_nodes(args,unlabeled_idx,size)\n",
    "    elif(args.selection_method == 'cluster'):\n",
    "        idx_attach = hs.cluster_distance_selection(args,data,idx_train,idx_val,idx_clean_test,unlabeled_idx,train_edge_index,size,device)\n",
    "        idx_attach = torch.LongTensor(idx_attach).to(device)\n",
    "    elif(args.selection_method == 'cluster_degree'):\n",
    "        idx_attach = hs.cluster_degree_selection(args,data,idx_train,idx_val,idx_clean_test,unlabeled_idx,train_edge_index,size,device)\n",
    "        idx_attach = torch.LongTensor(idx_attach).to(device)\n",
    "    # train trigger generator \n",
    "    model = Backdoor(args,device)\n",
    "    model.fit(data.x, train_edge_index, None, data.y, idx_train,idx_attach, unlabeled_idx)\n",
    "    poison_x, poison_edge_index, poison_edge_weights, poison_labels = model.get_poisoned()\n",
    "    bkd_tn_nodes = torch.cat([idx_train,idx_attach]).to(device)\n",
    "    print(\"precent of left attach nodes: {:.3f}\"\\\n",
    "        .format(len(set(bkd_tn_nodes.tolist()) & set(idx_attach.tolist()))/len(idx_attach)))\n",
    "    #%%\n",
    "\n",
    "    # model = GCN(nfeat=data.x.shape[1],\\\n",
    "    #             nhid=args.hidden,\\\n",
    "    #             nclass= int(data.y.max()+1),\\\n",
    "    #             dropout=args.dropout,\\\n",
    "    #             lr=args.train_lr,\\\n",
    "    #             weight_decay=args.weight_decay,\\\n",
    "    #             device=device)\n",
    "    # test_model = model_construct(args,args.test_model,data,device).to(device) \n",
    "    encoder = Encoder(dataset.num_features, args.cl_num_hidden, args.cl_activation,\n",
    "                            base_model=args.cl_base_model, k=args.cl_num_layers).to(device)\n",
    "    test_model = UnifyModel(args, encoder, args.cl_num_hidden, args.cl_num_proj_hidden, num_class, args.tau, lr=args.cl_lr, weight_decay=args.cl_weight_decay, device=None).to(device)\n",
    "    # test_model = UnifyModel(args, encoder, num_hidden, num_proj_hidden, num_class, tau, lr=learning_rate, weight_decay=weight_decay, device=device,data1=noisy_data,data2=diff_noisy_data).to(device)\n",
    "    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)\n",
    "    test_embds = test_model(poison_x,poison_edge_index,poison_edge_weights)\n",
    "    output = test_model.clf_head(test_embds)\n",
    "    train_attach_rate = (output.argmax(dim=1)[idx_attach]==args.target_class).float().mean()\n",
    "    print(\"target class rate on Vs: {:.4f}\".format(train_attach_rate))\n",
    "    torch.cuda.empty_cache()\n",
    "    # test_model = GCN(nfeat=cont_poison_x.shape[1],\\\n",
    "    #             nhid=args.hidden,\\\n",
    "    #             nclass= int(data.y.max()+1),\\\n",
    "    #             dropout=args.dropout,\\\n",
    "    #             lr=args.train_lr,\\\n",
    "    #             weight_decay=args.weight_decay,\\\n",
    "    #             device=device).to(device) \n",
    "    # test_model.fit(cont_poison_x, poison_edge_index, poison_edge_weights, poison_labels, bkd_tn_nodes, idx_val,train_iters=args.epochs,verbose=False)\n",
    "\n",
    "    # output = test_model(cont_poison_x,poison_edge_index,poison_edge_weights)\n",
    "    # train_attach_rate = (output.argmax(dim=1)[idx_attach]==args.target_class).float().mean()\n",
    "    # print(\"target class rate on Vs: {:.4f}\".format(train_attach_rate))\n",
    "    # torch.cuda.empty_cache()\n",
    "    #%%\n",
    "    induct_edge_index = torch.cat([poison_edge_index,mask_edge_index],dim=1)\n",
    "    induct_edge_weights = torch.cat([poison_edge_weights,torch.ones([mask_edge_index.shape[1]],dtype=torch.float,device=device)])\n",
    "    clean_acc = test_model.test(poison_x,induct_edge_index,induct_edge_weights,data.y,idx_clean_test)\n",
    "    # test_model = test_model.cpu()\n",
    "\n",
    "    print(\"accuracy on clean test nodes: {:.4f}\".format(clean_acc))\n",
    "\n",
    "\n",
    "    if(args.evaluate_mode == '1by1'):\n",
    "        from torch_geometric.utils  import k_hop_subgraph\n",
    "        overall_induct_edge_index, overall_induct_edge_weights = induct_edge_index.clone(),induct_edge_weights.clone()\n",
    "        asr = 0\n",
    "        flip_asr = 0\n",
    "        flip_idx_atk = idx_atk[(data.y[idx_atk] != args.target_class).nonzero().flatten()]\n",
    "        for i, idx in enumerate(idx_atk):\n",
    "            idx=int(idx)\n",
    "            sub_induct_nodeset, sub_induct_edge_index, sub_mapping, sub_edge_mask  = k_hop_subgraph(node_idx = [idx], num_hops = 2, edge_index = overall_induct_edge_index, relabel_nodes=True) # sub_mapping means the index of [idx] in sub)nodeset\n",
    "            ori_node_idx = sub_induct_nodeset[sub_mapping]\n",
    "            relabeled_node_idx = sub_mapping\n",
    "            sub_induct_edge_weights = torch.ones([sub_induct_edge_index.shape[1]]).to(device)\n",
    "            # inject trigger on attack test nodes (idx_atk)'''\n",
    "            with torch.no_grad():\n",
    "                induct_x, induct_edge_index,induct_edge_weights = model.inject_trigger(relabeled_node_idx,poison_x[sub_induct_nodeset],sub_induct_edge_index,sub_induct_edge_weights,device)\n",
    "                induct_x, induct_edge_index,induct_edge_weights = induct_x.clone().detach(), induct_edge_index.clone().detach(),induct_edge_weights.clone().detach()\n",
    "                test_embeds = test_model(induct_x, induct_edge_index,induct_edge_weights).to(device)\n",
    "                output = test_model.clf_head(test_embeds)\n",
    "                # cont_induct_x = contrastive_model(induct_x, induct_edge_index,induct_edge_weights).detach().to(device)\n",
    "                # # do pruning in test datas'''\n",
    "                # if(args.defense_mode == 'prune' or args.defense_mode == 'isolate'):\n",
    "                #     induct_edge_index,induct_edge_weights = prune_unrelated_edge(args,induct_edge_index,induct_edge_weights,induct_x,device,large_graph=False)\n",
    "                # attack evaluation\n",
    "\n",
    "                # output = test_model(cont_induct_x,induct_edge_index,induct_edge_weights)\n",
    "                train_attach_rate = (output.argmax(dim=1)[relabeled_node_idx]==args.target_class).float().mean()\n",
    "                # print(\"Node {}: {}, Origin Label: {}\".format(i, idx, data.y[idx]))\n",
    "                # print(\"ASR: {:.4f}\".format(train_attach_rate))\n",
    "                asr += train_attach_rate\n",
    "                if(data.y[idx] != args.target_class):\n",
    "                    flip_asr += train_attach_rate\n",
    "        asr = asr/(idx_atk.shape[0])\n",
    "        flip_asr = flip_asr/(flip_idx_atk.shape[0])\n",
    "        print(\"Overall ASR: {:.4f}\".format(asr))\n",
    "        print(\"Flip ASR: {:.4f}/{} nodes\".format(flip_asr,flip_idx_atk.shape[0]))\n",
    "    elif(args.evaluate_mode == 'overall'):\n",
    "        # %% inject trigger on attack test nodes (idx_atk)'''\n",
    "        induct_x, induct_edge_index,induct_edge_weights = model.inject_trigger(idx_atk,poison_x,induct_edge_index,induct_edge_weights,device)\n",
    "        induct_x, induct_edge_index,induct_edge_weights = induct_x.clone().detach(), induct_edge_index.clone().detach(),induct_edge_weights.clone().detach()\n",
    "        # do pruning in test datas'''\n",
    "        if(args.defense_mode == 'prune' or args.defense_mode == 'isolate'):\n",
    "            induct_edge_index,induct_edge_weights = prune_unrelated_edge(args,induct_edge_index,induct_edge_weights,induct_x,device)\n",
    "        # attack evaluation\n",
    "\n",
    "        test_embeds = test_model(induct_x, induct_edge_index,induct_edge_weights).to(device)\n",
    "        output = test_model.clf_head(test_embeds)\n",
    "        # test_model = test_model.to(device)\n",
    "        # output = test_model(induct_x,induct_edge_index,induct_edge_weights)\n",
    "        train_attach_rate = (output.argmax(dim=1)[idx_atk]==args.target_class).float().mean()\n",
    "        print(\"ASR: {:.4f}\".format(train_attach_rate))\n",
    "        flip_idx_atk = idx_atk[(data.y[idx_atk] != args.target_class).nonzero().flatten()]\n",
    "        flip_asr = (output.argmax(dim=1)[flip_idx_atk]==args.target_class).float().mean()\n",
    "        print(\"Flip ASR: {:.4f}/{} nodes\".format(flip_asr,flip_idx_atk.shape[0]))\n",
    "        ca = test_model.test(induct_x,induct_edge_index,induct_edge_weights,data.y,idx_clean_test)\n",
    "        print(\"CA: {:.4f}\".format(ca))\n",
    "\n",
    "    result_asr.append(float(asr))\n",
    "    result_acc.append(float(clean_acc))\n",
    "\n",
    "print('The final ASR:{:.5f}, {:.5f}, Accuracy:{:.5f}, {:.5f}'\\\n",
    "            .format(np.average(result_asr),np.std(result_asr),np.average(result_acc),np.std(result_acc)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Attach Nodes:160\n",
      "raw graph: torch.Size([2, 10556])\n",
      "add edge: torch.Size([2, 11624])\n",
      "remove edge: torch.Size([2, 9508])\n",
      "updated graph: torch.Size([2, 10576])\n",
      "Length of training set: 541\n",
      "=== training gcn model ===\n",
      "Epoch 0, training loss: 1.9487913846969604\n",
      "acc_val: 0.3222\n",
      "Epoch 10, training loss: 1.7001640796661377\n",
      "acc_val: 0.3222\n",
      "Epoch 20, training loss: 1.251969814300537\n",
      "acc_val: 0.5481\n",
      "Epoch 30, training loss: 0.9154055118560791\n",
      "acc_val: 0.5778\n",
      "Epoch 40, training loss: 0.7175220251083374\n",
      "acc_val: 0.6778\n",
      "Epoch 50, training loss: 0.5528076887130737\n",
      "acc_val: 0.7556\n",
      "Epoch 60, training loss: 0.45035868883132935\n",
      "acc_val: 0.7667\n",
      "Epoch 70, training loss: 0.3029544949531555\n",
      "acc_val: 0.7815\n",
      "Epoch 80, training loss: 0.26119792461395264\n",
      "acc_val: 0.7852\n",
      "Epoch 90, training loss: 0.2227189987897873\n",
      "acc_val: 0.8185\n",
      "Epoch 100, training loss: 0.16095885634422302\n",
      "acc_val: 0.8074\n",
      "Epoch 110, training loss: 0.12907230854034424\n",
      "acc_val: 0.8148\n",
      "Epoch 120, training loss: 0.0998087078332901\n",
      "acc_val: 0.8296\n",
      "Epoch 130, training loss: 0.09762615710496902\n",
      "acc_val: 0.8259\n",
      "Epoch 140, training loss: 0.08657265454530716\n",
      "acc_val: 0.8148\n",
      "Epoch 150, training loss: 0.09258854389190674\n",
      "acc_val: 0.8111\n",
      "Epoch 160, training loss: 0.10084856301546097\n",
      "acc_val: 0.8259\n",
      "Epoch 170, training loss: 0.07240049540996552\n",
      "acc_val: 0.8148\n",
      "Epoch 180, training loss: 0.0827595442533493\n",
      "acc_val: 0.8074\n",
      "Epoch 190, training loss: 0.07424037158489227\n",
      "acc_val: 0.8074\n",
      "=== picking the best model according to the performance on validation ===\n",
      "Training encoder Finished!\n",
      "Total time elapsed: 1.2694s\n",
      "Encoder CA on clean test nodes: 0.7704\n",
      "[2 4 4 ... 6 2 2]\n",
      "precent of left attach nodes: 1.000\n",
      "target class rate on Vs: 1.0000\n",
      "accuracy on clean test nodes: 0.8037\n",
      "Overall ASR: 0.9594\n",
      "Flip ASR: 0.9511/225 nodes\n",
      "Length of training set: 541\n",
      "=== training gcn model ===\n",
      "Epoch 0, training loss: 1.9455997943878174\n",
      "acc_val: 0.1296\n",
      "Epoch 10, training loss: 1.722038745880127\n",
      "acc_val: 0.3222\n",
      "Epoch 20, training loss: 1.3054707050323486\n",
      "acc_val: 0.5296\n",
      "Epoch 30, training loss: 0.9448956847190857\n",
      "acc_val: 0.5778\n",
      "Epoch 40, training loss: 0.7089287042617798\n",
      "acc_val: 0.7222\n",
      "Epoch 50, training loss: 0.5311965346336365\n",
      "acc_val: 0.7556\n",
      "Epoch 60, training loss: 0.35577860474586487\n",
      "acc_val: 0.7741\n",
      "Epoch 70, training loss: 0.2966192364692688\n",
      "acc_val: 0.7815\n",
      "Epoch 80, training loss: 0.2487214207649231\n",
      "acc_val: 0.8148\n",
      "Epoch 90, training loss: 0.18961064517498016\n",
      "acc_val: 0.8148\n",
      "Epoch 100, training loss: 0.1612675040960312\n",
      "acc_val: 0.8296\n",
      "Epoch 110, training loss: 0.1248779147863388\n",
      "acc_val: 0.8148\n",
      "Epoch 120, training loss: 0.11717411130666733\n",
      "acc_val: 0.8222\n",
      "Epoch 130, training loss: 0.10370758920907974\n",
      "acc_val: 0.8148\n",
      "Epoch 140, training loss: 0.10872744023799896\n",
      "acc_val: 0.8074\n",
      "Epoch 150, training loss: 0.09167472273111343\n",
      "acc_val: 0.8148\n",
      "Epoch 160, training loss: 0.08959712833166122\n",
      "acc_val: 0.8074\n",
      "Epoch 170, training loss: 0.11034353077411652\n",
      "acc_val: 0.8259\n",
      "Epoch 180, training loss: 0.08823854476213455\n",
      "acc_val: 0.8148\n",
      "Epoch 190, training loss: 0.08149073272943497\n",
      "acc_val: 0.8037\n",
      "=== picking the best model according to the performance on validation ===\n",
      "Training encoder Finished!\n",
      "Total time elapsed: 1.2062s\n",
      "Encoder CA on clean test nodes: 0.7407\n",
      "[0 3 3 ... 5 0 0]\n",
      "precent of left attach nodes: 1.000\n",
      "target class rate on Vs: 1.0000\n",
      "accuracy on clean test nodes: 0.8222\n",
      "Overall ASR: 0.9815\n",
      "Flip ASR: 0.9778/225 nodes\n",
      "Length of training set: 541\n",
      "=== training gcn model ===\n",
      "Epoch 0, training loss: 1.9298548698425293\n",
      "acc_val: 0.3222\n",
      "Epoch 10, training loss: 1.6883423328399658\n",
      "acc_val: 0.3222\n",
      "Epoch 20, training loss: 1.2069751024246216\n",
      "acc_val: 0.5630\n",
      "Epoch 30, training loss: 0.815941572189331\n",
      "acc_val: 0.6556\n",
      "Epoch 40, training loss: 0.5853594541549683\n",
      "acc_val: 0.7852\n",
      "Epoch 50, training loss: 0.3796159625053406\n",
      "acc_val: 0.8148\n",
      "Epoch 60, training loss: 0.24937653541564941\n",
      "acc_val: 0.8259\n",
      "Epoch 70, training loss: 0.16019858419895172\n",
      "acc_val: 0.8148\n",
      "Epoch 80, training loss: 0.1436629295349121\n",
      "acc_val: 0.8296\n",
      "Epoch 90, training loss: 0.1204776018857956\n",
      "acc_val: 0.8259\n",
      "Epoch 100, training loss: 0.10674170404672623\n",
      "acc_val: 0.8111\n",
      "Epoch 110, training loss: 0.10395748168230057\n",
      "acc_val: 0.8222\n",
      "Epoch 120, training loss: 0.08133775740861893\n",
      "acc_val: 0.8296\n",
      "Epoch 130, training loss: 0.08778337389230728\n",
      "acc_val: 0.8185\n",
      "Epoch 140, training loss: 0.0891847088932991\n",
      "acc_val: 0.8296\n",
      "Epoch 150, training loss: 0.08757417649030685\n",
      "acc_val: 0.8296\n",
      "Epoch 160, training loss: 0.0776069313287735\n",
      "acc_val: 0.8370\n",
      "Epoch 170, training loss: 0.07304735481739044\n",
      "acc_val: 0.8185\n",
      "Epoch 180, training loss: 0.07583433389663696\n",
      "acc_val: 0.8296\n",
      "Epoch 190, training loss: 0.06716092675924301\n",
      "acc_val: 0.8370\n",
      "=== picking the best model according to the performance on validation ===\n",
      "Training encoder Finished!\n",
      "Total time elapsed: 1.2546s\n",
      "Encoder CA on clean test nodes: 0.7704\n",
      "[5 3 5 ... 2 5 5]\n",
      "precent of left attach nodes: 1.000\n",
      "target class rate on Vs: 1.0000\n",
      "accuracy on clean test nodes: 0.8111\n",
      "Overall ASR: 0.9336\n",
      "Flip ASR: 0.9200/225 nodes\n",
      "The final ASR:0.95818, 0.01960, Accuracy:0.81235, 0.00761\n"
     ]
    }
   ],
   "source": [
    "'''Backdoor attack to GNN classifier'''\n",
    "from models.GTA import Backdoor\n",
    "import heuristic_selection as hs\n",
    "from models.GCN import GCN\n",
    "from models.construct import model_construct\n",
    "from construct_graph import *\n",
    "# args.homo_loss_weight=config['homo_loss_weight']\n",
    "# args.vs_number=config['vs_number']\n",
    "# args.trojan_epochs = config['trojan_epochs']\n",
    "\n",
    "data = data.to(device)\n",
    "num_class = int(data.y.max()+1)\n",
    "# args.seed = config['seed']\n",
    "\n",
    "size = args.vs_number #int((len(data.test_mask)-data.test_mask.sum())*args.vs_ratio)\n",
    "print(\"#Attach Nodes:{}\".format(size))\n",
    "from models.construct import model_construct\n",
    "result_asr = []\n",
    "result_acc = []\n",
    "data = data.to(device)\n",
    "\n",
    "noisy_data = construct_noisy_graph(data,perturb_ratio=0.1,mode='random_noise')\n",
    "data = noisy_data.to(device)\n",
    "rs = np.random.RandomState(args.seed)\n",
    "seeds = rs.randint(1000,size=3)\n",
    "# seeds = [args.seed]\n",
    "for seed in seeds:\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    args.seed = seed\n",
    "    if(args.selection_method == 'none'):\n",
    "        idx_attach = hs.obtain_attach_nodes(args,unlabeled_idx,size)\n",
    "    elif(args.selection_method == 'cluster'):\n",
    "        idx_attach = hs.cluster_distance_selection(args,data,idx_train,idx_val,idx_clean_test,unlabeled_idx,train_edge_index,size,device)\n",
    "        idx_attach = torch.LongTensor(idx_attach).to(device)\n",
    "    elif(args.selection_method == 'cluster_degree'):\n",
    "        idx_attach = hs.cluster_degree_selection(args,data,idx_train,idx_val,idx_clean_test,unlabeled_idx,train_edge_index,size,device)\n",
    "        idx_attach = torch.LongTensor(idx_attach).to(device)\n",
    "    # train trigger generator \n",
    "    model = Backdoor(args,device)\n",
    "    model.fit(data.x, train_edge_index, None, data.y, idx_train,idx_attach, unlabeled_idx)\n",
    "    poison_x, poison_edge_index, poison_edge_weights, poison_labels = model.get_poisoned()\n",
    "    bkd_tn_nodes = torch.cat([idx_train,idx_attach]).to(device)\n",
    "    # # learn contrastive node representation\n",
    "    # encoder = Encoder(dataset.num_features, num_hidden, activation,\n",
    "    #                     base_model=base_model, k=num_layers).to(device)\n",
    "    # contrastive_model = Model(encoder, num_hidden, num_proj_hidden, tau).to(device)\n",
    "    # optimizer = torch.optim.Adam(\n",
    "    #     contrastive_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    \n",
    "    # start = t()\n",
    "    # prev = start\n",
    "    # for epoch in range(1, num_epochs + 1):\n",
    "    #     # loss = train_(model, data.x, data.edge_index)\n",
    "    #     loss = train_1(contrastive_model, poison_x, poison_edge_index, poison_edge_weights)\n",
    "\n",
    "    #     now = t()\n",
    "    #     if(epoch%10 == 0):\n",
    "    #         print(f'(T) | Epoch={epoch:03d}, loss={loss:.4f}, '\n",
    "    #                 f'this epoch {now - prev:.4f}, total {now - start:.4f}')\n",
    "    #     prev = now\n",
    "\n",
    "    # contrastive_model.eval()\n",
    "    # cont_poison_x = contrastive_model(poison_x, poison_edge_index, poison_edge_weights).detach().to(device)\n",
    "\n",
    "    print(\"precent of left attach nodes: {:.3f}\"\\\n",
    "        .format(len(set(bkd_tn_nodes.tolist()) & set(idx_attach.tolist()))/len(idx_attach)))\n",
    "    #%%\n",
    "\n",
    "    # model = GCN(nfeat=data.x.shape[1],\\\n",
    "    #             nhid=args.hidden,\\\n",
    "    #             nclass= int(data.y.max()+1),\\\n",
    "    #             dropout=args.dropout,\\\n",
    "    #             lr=args.train_lr,\\\n",
    "    #             weight_decay=args.weight_decay,\\\n",
    "    #             device=device)\n",
    "    test_model = model_construct(args,args.test_model,data,device).to(device) \n",
    "    # test_model = GCN(nfeat=cont_poison_x.shape[1],\\\n",
    "    #             nhid=args.hidden,\\\n",
    "    #             nclass= int(data.y.max()+1),\\\n",
    "    #             dropout=args.dropout,\\\n",
    "    #             lr=args.train_lr,\\\n",
    "    #             weight_decay=args.weight_decay,\\\n",
    "    #             device=device).to(device) \n",
    "    test_model.fit(poison_x, poison_edge_index, poison_edge_weights, poison_labels, bkd_tn_nodes, idx_val,train_iters=args.epochs,verbose=False)\n",
    "\n",
    "    output = test_model(poison_x,poison_edge_index,poison_edge_weights)\n",
    "    train_attach_rate = (output.argmax(dim=1)[idx_attach]==args.target_class).float().mean()\n",
    "    print(\"target class rate on Vs: {:.4f}\".format(train_attach_rate))\n",
    "    torch.cuda.empty_cache()\n",
    "    #%%\n",
    "    induct_edge_index = torch.cat([poison_edge_index,mask_edge_index],dim=1)\n",
    "    induct_edge_weights = torch.cat([poison_edge_weights,torch.ones([mask_edge_index.shape[1]],dtype=torch.float,device=device)])\n",
    "    clean_acc = test_model.test(poison_x,induct_edge_index,induct_edge_weights,data.y,idx_clean_test)\n",
    "    # test_model = test_model.cpu()\n",
    "\n",
    "    print(\"accuracy on clean test nodes: {:.4f}\".format(clean_acc))\n",
    "\n",
    "\n",
    "    if(args.evaluate_mode == '1by1'):\n",
    "        from torch_geometric.utils  import k_hop_subgraph\n",
    "        overall_induct_edge_index, overall_induct_edge_weights = induct_edge_index.clone(),induct_edge_weights.clone()\n",
    "        asr = 0\n",
    "        flip_asr = 0\n",
    "        flip_idx_atk = idx_atk[(data.y[idx_atk] != args.target_class).nonzero().flatten()]\n",
    "        for i, idx in enumerate(idx_atk):\n",
    "            idx=int(idx)\n",
    "            sub_induct_nodeset, sub_induct_edge_index, sub_mapping, sub_edge_mask  = k_hop_subgraph(node_idx = [idx], num_hops = 2, edge_index = overall_induct_edge_index, relabel_nodes=True) # sub_mapping means the index of [idx] in sub)nodeset\n",
    "            ori_node_idx = sub_induct_nodeset[sub_mapping]\n",
    "            relabeled_node_idx = sub_mapping\n",
    "            sub_induct_edge_weights = torch.ones([sub_induct_edge_index.shape[1]]).to(device)\n",
    "            # inject trigger on attack test nodes (idx_atk)'''\n",
    "            with torch.no_grad():\n",
    "                induct_x, induct_edge_index,induct_edge_weights = model.inject_trigger(relabeled_node_idx,poison_x[sub_induct_nodeset],sub_induct_edge_index,sub_induct_edge_weights,device)\n",
    "                induct_x, induct_edge_index,induct_edge_weights = induct_x.clone().detach(), induct_edge_index.clone().detach(),induct_edge_weights.clone().detach()\n",
    "                # cont_induct_x = contrastive_model(induct_x, induct_edge_index,induct_edge_weights).detach().to(device)\n",
    "                # # do pruning in test datas'''\n",
    "                # if(args.defense_mode == 'prune' or args.defense_mode == 'isolate'):\n",
    "                #     induct_edge_index,induct_edge_weights = prune_unrelated_edge(args,induct_edge_index,induct_edge_weights,induct_x,device,large_graph=False)\n",
    "                # attack evaluation\n",
    "\n",
    "                output = test_model(induct_x,induct_edge_index,induct_edge_weights)\n",
    "                train_attach_rate = (output.argmax(dim=1)[relabeled_node_idx]==args.target_class).float().mean()\n",
    "                # print(\"Node {}: {}, Origin Label: {}\".format(i, idx, data.y[idx]))\n",
    "                # print(\"ASR: {:.4f}\".format(train_attach_rate))\n",
    "                asr += train_attach_rate\n",
    "                if(data.y[idx] != args.target_class):\n",
    "                    flip_asr += train_attach_rate\n",
    "        asr = asr/(idx_atk.shape[0])\n",
    "        flip_asr = flip_asr/(flip_idx_atk.shape[0])\n",
    "        print(\"Overall ASR: {:.4f}\".format(asr))\n",
    "        print(\"Flip ASR: {:.4f}/{} nodes\".format(flip_asr,flip_idx_atk.shape[0]))\n",
    "    result_asr.append(float(asr))\n",
    "    result_acc.append(float(clean_acc))\n",
    "print('The final ASR:{:.5f}, {:.5f}, Accuracy:{:.5f}, {:.5f}'\\\n",
    "            .format(np.average(result_asr),np.std(result_asr),np.average(result_acc),np.std(result_acc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('py38_torch120': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4ab847dfc59cee10fa08e4e9fed31787c275fa5742f67664facc345e6fad65e7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
