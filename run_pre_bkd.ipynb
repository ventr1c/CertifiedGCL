{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* implement unifyGNN+Contrastive\n",
    "* try different noisy and augmentation\n",
    "* try against with/without unnoticeable "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(add_edge_rate_1=0, add_edge_rate_2=0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_layers=2, cl_num_proj_hidden=64, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=3, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=3, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.5, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=64, homo_boost_thrd=0.5, homo_loss_weight=100, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=64, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=400, use_vs_number=True, vs_number=160, vs_ratio=0, weight_decay=0.0005)\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# In[1]: \n",
    "\n",
    "import imp\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from torch_geometric.datasets import Planetoid,Reddit2,Flickr,PPI\n",
    "\n",
    "\n",
    "# from torch_geometric.loader import DataLoader\n",
    "# from help_funcs import prune_unrelated_edge,prune_unrelated_edge_isolated\n",
    "import scipy.sparse as sp\n",
    "\n",
    "# Training settings\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--debug', action='store_true',\n",
    "        default=True, help='debug mode')\n",
    "parser.add_argument('--no-cuda', action='store_true', default=False,\n",
    "                    help='Disables CUDA training.')\n",
    "parser.add_argument('--seed', type=int, default=10, help='Random seed.')\n",
    "parser.add_argument('--model', type=str, default='GCN', help='model',\n",
    "                    choices=['GCN','GAT','GraphSage','GIN'])\n",
    "parser.add_argument('--dataset', type=str, default='Pubmed', \n",
    "                    help='Dataset',\n",
    "                    choices=['Cora','Citeseer','Pubmed','PPI','Flickr','ogbn-arxiv','Reddit','Reddit2','Yelp'])\n",
    "parser.add_argument('--train_lr', type=float, default=0.01,\n",
    "                    help='Initial learning rate.')\n",
    "parser.add_argument('--weight_decay', type=float, default=5e-4,\n",
    "                    help='Weight decay (L2 loss on parameters).')\n",
    "parser.add_argument('--hidden', type=int, default=32,\n",
    "                    help='Number of hidden units.')\n",
    "parser.add_argument('--num_hidden', type=int, default=64,\n",
    "                    help='Number of hidden units.')\n",
    "parser.add_argument('--num_proj_hidden', type=int, default=64,\n",
    "                    help='Number of hidden units in MLP.')\n",
    "parser.add_argument('--thrd', type=float, default=0.5)\n",
    "parser.add_argument('--target_class', type=int, default=0)\n",
    "parser.add_argument('--dropout', type=float, default=0.5,\n",
    "                    help='Dropout rate (1 - keep probability).')\n",
    "parser.add_argument('--epochs', type=int,  default=200, help='Number of epochs to train benign and backdoor model.')\n",
    "parser.add_argument('--trojan_epochs', type=int,  default=400, help='Number of epochs to train trigger generator.')\n",
    "parser.add_argument('--inner', type=int,  default=1, help='Number of inner')\n",
    "parser.add_argument('--temperature', type=float,  default=0.5, help='Temperature')\n",
    "# backdoor setting\n",
    "parser.add_argument('--lr', type=float, default=0.01,\n",
    "                    help='Initial learning rate.')\n",
    "parser.add_argument('--trigger_size', type=int, default=3,\n",
    "                    help='tirgger_size')\n",
    "parser.add_argument('--use_vs_number', action='store_true', default=True,\n",
    "                    help=\"if use detailed number to decide Vs\")\n",
    "parser.add_argument('--vs_ratio', type=float, default=0,\n",
    "                    help=\"ratio of poisoning nodes relative to the full graph\")\n",
    "parser.add_argument('--vs_number', type=int, default=160,\n",
    "                    help=\"number of poisoning nodes relative to the full graph\")\n",
    "# defense setting\n",
    "parser.add_argument('--defense_mode', type=str, default=\"none\",\n",
    "                    choices=['prune', 'isolate', 'none'],\n",
    "                    help=\"Mode of defense\")\n",
    "parser.add_argument('--prune_thr', type=float, default=0.2,\n",
    "                    help=\"Threshold of prunning edges\")\n",
    "parser.add_argument('--target_loss_weight', type=float, default=1,\n",
    "                    help=\"Weight of optimize outter trigger generator\")\n",
    "parser.add_argument('--homo_loss_weight', type=float, default=100,\n",
    "                    help=\"Weight of optimize similarity loss\")\n",
    "parser.add_argument('--homo_boost_thrd', type=float, default=0.5,\n",
    "                    help=\"Threshold of increase similarity\")\n",
    "# attack setting\n",
    "parser.add_argument('--dis_weight', type=float, default=1,\n",
    "                    help=\"Weight of cluster distance\")\n",
    "parser.add_argument('--selection_method', type=str, default='cluster_degree',\n",
    "                    choices=['loss','conf','cluster','none','cluster_degree'],\n",
    "                    help='Method to select idx_attach for training trojan model (none means randomly select)')\n",
    "parser.add_argument('--test_model', type=str, default='GCN',\n",
    "                    choices=['GCN','GAT','GraphSage','GIN'],\n",
    "                    help='Model used to attack')\n",
    "parser.add_argument('--evaluate_mode', type=str, default='1by1',\n",
    "                    choices=['overall','1by1'],\n",
    "                    help='Model used to attack')\n",
    "# GPU setting\n",
    "parser.add_argument('--device_id', type=int, default=3,\n",
    "                    help=\"Threshold of prunning edges\")\n",
    "# GRACE setting\n",
    "parser.add_argument('--gpu_id', type=int, default=0)\n",
    "parser.add_argument('--config', type=str, default=\"config.yaml\")\n",
    "## Contrasitve setting\n",
    "parser.add_argument('--cl_lr', type=float, default=0.0002)\n",
    "# parser.add_argument('--cl_num_hidden', type=int, default=64)\n",
    "parser.add_argument('--cl_num_proj_hidden', type=int, default=64)\n",
    "parser.add_argument('--cl_num_layers', type=int, default=2)\n",
    "parser.add_argument('--cl_activation', type=str, default='relu')\n",
    "parser.add_argument('--cl_base_model', type=str, default='GCNConv')\n",
    "parser.add_argument('--cont_weight', type=float, default=3)\n",
    "parser.add_argument('--add_edge_rate_1', type=float, default=0)\n",
    "parser.add_argument('--add_edge_rate_2', type=float, default=0)\n",
    "parser.add_argument('--drop_edge_rate_1', type=float, default=0.3)\n",
    "parser.add_argument('--drop_edge_rate_2', type=float, default=0.5)\n",
    "parser.add_argument('--drop_feat_rate_1', type=float, default=0.4)\n",
    "parser.add_argument('--drop_feat_rate_2', type=float, default=0.4)\n",
    "parser.add_argument('--tau', type=float, default=0.4)\n",
    "parser.add_argument('--cl_num_epochs', type=int, default=1000)\n",
    "parser.add_argument('--cl_weight_decay', type=float, default=0.00001)\n",
    "parser.add_argument('--cont_batch_size', type=int, default=0)\n",
    "\n",
    "# args = parser.parse_args()\n",
    "args = parser.parse_known_args()[0]\n",
    "args.cuda =  not args.no_cuda and torch.cuda.is_available()\n",
    "device = torch.device(('cuda:{}' if torch.cuda.is_available() else 'cpu').format(args.device_id))\n",
    "\n",
    "np.random.seed(args.seed)\n",
    "# torch.manual_seed(args.seed)\n",
    "# torch.cuda.manual_seed(args.seed)\n",
    "print(args)\n",
    "#%%\n",
    "from torch_geometric.utils import to_undirected\n",
    "import torch_geometric.transforms as T\n",
    "transform = T.Compose([T.NormalizeFeatures()])\n",
    "\n",
    "if(args.dataset == 'Cora' or args.dataset == 'Citeseer' or args.dataset == 'Pubmed'):\n",
    "    dataset = Planetoid(root='./data/', \\\n",
    "                        name=args.dataset,\\\n",
    "                        transform=transform)\n",
    "elif(args.dataset == 'Flickr'):\n",
    "    dataset = Flickr(root='./data/Flickr/', \\\n",
    "                    transform=transform)\n",
    "elif(args.dataset == 'Reddit2'):\n",
    "    dataset = Reddit2(root='./data/Reddit2/', \\\n",
    "                    transform=transform)\n",
    "elif(args.dataset == 'ogbn-arxiv'):\n",
    "    from ogb.nodeproppred import PygNodePropPredDataset\n",
    "    # Download and process data at './dataset/ogbg_molhiv/'\n",
    "    dataset = PygNodePropPredDataset(name = 'ogbn-arxiv', root='./data/')\n",
    "    split_idx = dataset.get_idx_split() \n",
    "\n",
    "data = dataset[0].to(device)\n",
    "\n",
    "if(args.dataset == 'ogbn-arxiv'):\n",
    "    nNode = data.x.shape[0]\n",
    "    setattr(data,'train_mask',torch.zeros(nNode, dtype=torch.bool).to(device))\n",
    "    # dataset[0].train_mask = torch.zeros(nEdge, dtype=torch.bool).to(device)\n",
    "    data.val_mask = torch.zeros(nNode, dtype=torch.bool).to(device)\n",
    "    data.test_mask = torch.zeros(nNode, dtype=torch.bool).to(device)\n",
    "    data.y = data.y.squeeze(1)\n",
    "# we build our own train test split \n",
    "#%% \n",
    "from utils import get_split\n",
    "data, idx_train, idx_val, idx_clean_test, idx_atk = get_split(args,data,device)\n",
    "\n",
    "from torch_geometric.utils import to_undirected\n",
    "from utils import subgraph\n",
    "data.edge_index = to_undirected(data.edge_index)\n",
    "train_edge_index,_, edge_mask = subgraph(torch.bitwise_not(data.test_mask),data.edge_index,relabel_nodes=False)\n",
    "mask_edge_index = data.edge_index[:,torch.bitwise_not(edge_mask)]\n",
    "# filter out the unlabeled nodes except from training nodes and testing nodes, nonzero() is to get index, flatten is to get 1-d tensor\n",
    "unlabeled_idx = (torch.bitwise_not(data.test_mask)&torch.bitwise_not(data.train_mask)).nonzero().flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[19717, 500], edge_index=[2, 88648], y=[19717], train_mask=[19717], val_mask=[19717], test_mask=[19717])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data.val_mask.nonzero().flatten().shape\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.utils import to_dense_adj, dense_to_sparse\n",
    "from torch_geometric.data import Data, InMemoryDataset\n",
    "\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "from scipy.linalg import expm\n",
    "\n",
    "import torch\n",
    "from torch_geometric.data import Data, InMemoryDataset\n",
    "from torch_geometric.datasets import Planetoid, Amazon, Coauthor\n",
    "\n",
    "# from seeds import development_seed\n",
    "DATA_PATH = './data/'\n",
    "\n",
    "def get_dataset(name: str, use_lcc: bool = True) -> InMemoryDataset:\n",
    "    path = os.path.join(DATA_PATH, name)\n",
    "    if name in ['Cora', 'Citeseer', 'Pubmed']:\n",
    "        dataset = Planetoid(path, name)\n",
    "    elif name in ['Computers', 'Photo']:\n",
    "        dataset = Amazon(path, name)\n",
    "    elif name == 'CoauthorCS':\n",
    "        dataset = Coauthor(path, 'CS')\n",
    "    else:\n",
    "        raise Exception('Unknown dataset.')\n",
    "\n",
    "    if use_lcc:\n",
    "        lcc = get_largest_connected_component(dataset)\n",
    "\n",
    "        x_new = dataset.data.x[lcc]\n",
    "        y_new = dataset.data.y[lcc]\n",
    "\n",
    "        row, col = dataset.data.edge_index.numpy()\n",
    "        edges = [[i, j] for i, j in zip(row, col) if i in lcc and j in lcc]\n",
    "        edges = remap_edges(edges, get_node_mapper(lcc))\n",
    "        \n",
    "        data = Data(\n",
    "            x=x_new,\n",
    "            edge_index=torch.LongTensor(edges),\n",
    "            y=y_new,\n",
    "            train_mask=torch.zeros(y_new.size()[0], dtype=torch.bool),\n",
    "            test_mask=torch.zeros(y_new.size()[0], dtype=torch.bool),\n",
    "            val_mask=torch.zeros(y_new.size()[0], dtype=torch.bool)\n",
    "        )\n",
    "        dataset.data = data\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def get_component(dataset: InMemoryDataset, start: int = 0) -> set:\n",
    "    visited_nodes = set()\n",
    "    queued_nodes = set([start])\n",
    "    row, col = dataset.data.edge_index.numpy()\n",
    "    while queued_nodes:\n",
    "        current_node = queued_nodes.pop()\n",
    "        visited_nodes.update([current_node])\n",
    "        neighbors = col[np.where(row == current_node)[0]]\n",
    "        neighbors = [n for n in neighbors if n not in visited_nodes and n not in queued_nodes]\n",
    "        queued_nodes.update(neighbors)\n",
    "    return visited_nodes\n",
    "\n",
    "\n",
    "def get_largest_connected_component(dataset: InMemoryDataset) -> np.ndarray:\n",
    "    remaining_nodes = set(range(dataset.data.x.shape[0]))\n",
    "    comps = []\n",
    "    while remaining_nodes:\n",
    "        start = min(remaining_nodes)\n",
    "        comp = get_component(dataset, start)\n",
    "        comps.append(comp)\n",
    "        remaining_nodes = remaining_nodes.difference(comp)\n",
    "    return np.array(list(comps[np.argmax(list(map(len, comps)))]))\n",
    "\n",
    "\n",
    "def get_node_mapper(lcc: np.ndarray) -> dict:\n",
    "    mapper = {}\n",
    "    counter = 0\n",
    "    for node in lcc:\n",
    "        mapper[node] = counter\n",
    "        counter += 1\n",
    "    return mapper\n",
    "\n",
    "\n",
    "def remap_edges(edges: list, mapper: dict) -> list:\n",
    "    row = [e[0] for e in edges]\n",
    "    col = [e[1] for e in edges]\n",
    "    row = list(map(lambda x: mapper[x], row))\n",
    "    col = list(map(lambda x: mapper[x], col))\n",
    "    return [row, col]\n",
    "\n",
    "\n",
    "def get_adj_matrix(data) -> np.ndarray:\n",
    "    num_nodes = data.x.shape[0]\n",
    "    adj_matrix = np.zeros(shape=(num_nodes, num_nodes))\n",
    "    for i, j in zip(data.edge_index[0], data.edge_index[1]):\n",
    "        adj_matrix[i, j] = 1.\n",
    "    return adj_matrix\n",
    "\n",
    "\n",
    "def get_ppr_matrix(\n",
    "        adj_matrix: np.ndarray,\n",
    "        alpha: float = 0.1) -> np.ndarray:\n",
    "    num_nodes = adj_matrix.shape[0]\n",
    "    A_tilde = adj_matrix + np.eye(num_nodes)\n",
    "    D_tilde = np.diag(1/np.sqrt(A_tilde.sum(axis=1)))\n",
    "    H = D_tilde @ A_tilde @ D_tilde\n",
    "    return alpha * np.linalg.inv(np.eye(num_nodes) - (1 - alpha) * H)\n",
    "\n",
    "\n",
    "def get_heat_matrix(\n",
    "        adj_matrix: np.ndarray,\n",
    "        t: float = 5.0) -> np.ndarray:\n",
    "    num_nodes = adj_matrix.shape[0]\n",
    "    A_tilde = adj_matrix + np.eye(num_nodes)\n",
    "    D_tilde = np.diag(1/np.sqrt(A_tilde.sum(axis=1)))\n",
    "    H = D_tilde @ A_tilde @ D_tilde\n",
    "    return expm(-t * (np.eye(num_nodes) - H))\n",
    "\n",
    "\n",
    "def get_top_k_matrix(A: np.ndarray, k: int = 128) -> np.ndarray:\n",
    "    num_nodes = A.shape[0]\n",
    "    row_idx = np.arange(num_nodes)\n",
    "    A[A.argsort(axis=0)[:num_nodes - k], row_idx] = 0.\n",
    "    norm = A.sum(axis=0)\n",
    "    norm[norm <= 0] = 1 # avoid dividing by zero\n",
    "    return A/norm\n",
    "\n",
    "\n",
    "def get_clipped_matrix(A: np.ndarray, eps: float = 0.01) -> np.ndarray:\n",
    "    num_nodes = A.shape[0]\n",
    "    A[A < eps] = 0.\n",
    "    norm = A.sum(axis=0)\n",
    "    norm[norm <= 0] = 1 # avoid dividing by zero\n",
    "    return A/norm\n",
    "\n",
    "\n",
    "def set_train_val_test_split(\n",
    "        seed: int,\n",
    "        data: Data,\n",
    "        num_development: int = 1500,\n",
    "        num_per_class: int = 20) -> Data:\n",
    "    rnd_state = np.random.RandomState(development_seed)\n",
    "    num_nodes = data.y.shape[0]\n",
    "    development_idx = rnd_state.choice(num_nodes, num_development, replace=False)\n",
    "    test_idx = [i for i in np.arange(num_nodes) if i not in development_idx]\n",
    "\n",
    "    train_idx = []\n",
    "    rnd_state = np.random.RandomState(seed)\n",
    "    for c in range(data.y.max() + 1):\n",
    "        class_idx = development_idx[np.where(data.y[development_idx].cpu() == c)[0]]\n",
    "        train_idx.extend(rnd_state.choice(class_idx, num_per_class, replace=False))\n",
    "\n",
    "    val_idx = [i for i in development_idx if i not in train_idx]\n",
    "\n",
    "    def get_mask(idx):\n",
    "        mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "        mask[idx] = 1\n",
    "        return mask\n",
    "\n",
    "    data.train_mask = get_mask(train_idx)\n",
    "    data.val_mask = get_mask(val_idx)\n",
    "    data.test_mask = get_mask(test_idx)\n",
    "\n",
    "    return data\n",
    "\n",
    "class PPRDataset(InMemoryDataset):\n",
    "    \"\"\"\n",
    "    Dataset preprocessed with GDC using PPR diffusion.\n",
    "    Note that this implementations is not scalable\n",
    "    since we directly invert the adjacency matrix.\n",
    "    \"\"\"\n",
    "    def __init__(self,noisy_data,\n",
    "                 name: str = 'Cora',\n",
    "                 use_lcc: bool = True,\n",
    "                 alpha: float = 0.1,\n",
    "                 k: int = 16,\n",
    "                 eps: float = None):\n",
    "        self.name = name\n",
    "        self.use_lcc = use_lcc\n",
    "        self.alpha = alpha\n",
    "        self.k = k\n",
    "        self.eps = eps\n",
    "        self.noisy_data = noisy_data\n",
    "\n",
    "        super(PPRDataset, self).__init__(DATA_PATH)\n",
    "        self.data, self.slices = torch.load(self.processed_paths[0])\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self) -> list:\n",
    "        return []\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self) -> list:\n",
    "        return [str(self) + '.pt']\n",
    "\n",
    "    def download(self):\n",
    "        pass\n",
    "\n",
    "    def process(self):\n",
    "        # base = get_dataset(name=self.name, use_lcc=self.use_lcc)\n",
    "        # generate adjacency matrix from sparse representation\n",
    "        adj_matrix = get_adj_matrix(self.noisy_data)\n",
    "        # obtain exact PPR matrix\n",
    "        ppr_matrix = get_ppr_matrix(adj_matrix,\n",
    "                                        alpha=self.alpha)\n",
    "\n",
    "        if self.k:\n",
    "            print(f'Selecting top {self.k} edges per node.')\n",
    "            ppr_matrix = get_top_k_matrix(ppr_matrix, k=self.k)\n",
    "        elif self.eps:\n",
    "            print(f'Selecting edges with weight greater than {self.eps}.')\n",
    "            ppr_matrix = get_clipped_matrix(ppr_matrix, eps=self.eps)\n",
    "        else:\n",
    "            raise ValueError\n",
    "\n",
    "        # create PyG Data object\n",
    "        edges_i = []\n",
    "        edges_j = []\n",
    "        edge_attr = []\n",
    "        for i, row in enumerate(ppr_matrix):\n",
    "            for j in np.where(row > 0)[0]:\n",
    "                edges_i.append(i)\n",
    "                edges_j.append(j)\n",
    "                edge_attr.append(ppr_matrix[i, j])\n",
    "        edge_index = [edges_i, edges_j]\n",
    "\n",
    "        data = Data(\n",
    "            x=self.noisy_data.x,\n",
    "            edge_index=torch.LongTensor(edge_index),\n",
    "            edge_attr=torch.FloatTensor(edge_attr),\n",
    "            y=self.noisy_data.y,\n",
    "            train_mask=torch.zeros(self.noisy_data.train_mask.size()[0], dtype=torch.bool),\n",
    "            test_mask=torch.zeros(self.noisy_data.test_mask.size()[0], dtype=torch.bool),\n",
    "            val_mask=torch.zeros(self.noisy_data.val_mask.size()[0], dtype=torch.bool)\n",
    "        )\n",
    "\n",
    "        data, slices = self.collate([data])\n",
    "        torch.save((data, slices), self.processed_paths[0])\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return f'{self.name}_ppr_alpha={self.alpha}_k={self.k}_eps={self.eps}_lcc={self.use_lcc}'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.utils import to_dense_adj, dense_to_sparse\n",
    "from torch_geometric.data import Data, InMemoryDataset\n",
    "\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "from scipy.linalg import expm\n",
    "\n",
    "import torch\n",
    "from torch_geometric.data import Data, InMemoryDataset\n",
    "from torch_geometric.datasets import Planetoid, Amazon, Coauthor\n",
    "\n",
    "# from seeds import development_seed\n",
    "DATA_PATH = './data/'\n",
    "\n",
    "def get_dataset(name: str, use_lcc: bool = True) -> InMemoryDataset:\n",
    "    path = os.path.join(DATA_PATH, name)\n",
    "    if name in ['Cora', 'Citeseer', 'Pubmed']:\n",
    "        dataset = Planetoid(path, name)\n",
    "    elif name in ['Computers', 'Photo']:\n",
    "        dataset = Amazon(path, name)\n",
    "    elif name == 'CoauthorCS':\n",
    "        dataset = Coauthor(path, 'CS')\n",
    "    else:\n",
    "        raise Exception('Unknown dataset.')\n",
    "\n",
    "    if use_lcc:\n",
    "        lcc = get_largest_connected_component(dataset)\n",
    "\n",
    "        x_new = dataset.data.x[lcc]\n",
    "        y_new = dataset.data.y[lcc]\n",
    "\n",
    "        row, col = dataset.data.edge_index.numpy()\n",
    "        edges = [[i, j] for i, j in zip(row, col) if i in lcc and j in lcc]\n",
    "        edges = remap_edges(edges, get_node_mapper(lcc))\n",
    "        \n",
    "        data = Data(\n",
    "            x=x_new,\n",
    "            edge_index=torch.LongTensor(edges),\n",
    "            y=y_new,\n",
    "            train_mask=torch.zeros(y_new.size()[0], dtype=torch.bool),\n",
    "            test_mask=torch.zeros(y_new.size()[0], dtype=torch.bool),\n",
    "            val_mask=torch.zeros(y_new.size()[0], dtype=torch.bool)\n",
    "        )\n",
    "        dataset.data = data\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def get_component(dataset: InMemoryDataset, start: int = 0) -> set:\n",
    "    visited_nodes = set()\n",
    "    queued_nodes = set([start])\n",
    "    row, col = dataset.data.edge_index.numpy()\n",
    "    while queued_nodes:\n",
    "        current_node = queued_nodes.pop()\n",
    "        visited_nodes.update([current_node])\n",
    "        neighbors = col[np.where(row == current_node)[0]]\n",
    "        neighbors = [n for n in neighbors if n not in visited_nodes and n not in queued_nodes]\n",
    "        queued_nodes.update(neighbors)\n",
    "    return visited_nodes\n",
    "\n",
    "\n",
    "def get_largest_connected_component(dataset: InMemoryDataset) -> np.ndarray:\n",
    "    remaining_nodes = set(range(dataset.data.x.shape[0]))\n",
    "    comps = []\n",
    "    while remaining_nodes:\n",
    "        start = min(remaining_nodes)\n",
    "        comp = get_component(dataset, start)\n",
    "        comps.append(comp)\n",
    "        remaining_nodes = remaining_nodes.difference(comp)\n",
    "    return np.array(list(comps[np.argmax(list(map(len, comps)))]))\n",
    "\n",
    "\n",
    "def get_node_mapper(lcc: np.ndarray) -> dict:\n",
    "    mapper = {}\n",
    "    counter = 0\n",
    "    for node in lcc:\n",
    "        mapper[node] = counter\n",
    "        counter += 1\n",
    "    return mapper\n",
    "\n",
    "\n",
    "def remap_edges(edges: list, mapper: dict) -> list:\n",
    "    row = [e[0] for e in edges]\n",
    "    col = [e[1] for e in edges]\n",
    "    row = list(map(lambda x: mapper[x], row))\n",
    "    col = list(map(lambda x: mapper[x], col))\n",
    "    return [row, col]\n",
    "\n",
    "\n",
    "def get_adj_matrix(data) -> np.ndarray:\n",
    "    num_nodes = data.x.shape[0]\n",
    "    adj_matrix = np.zeros(shape=(num_nodes, num_nodes))\n",
    "    for i, j in zip(data.edge_index[0], data.edge_index[1]):\n",
    "        adj_matrix[i, j] = 1.\n",
    "    return adj_matrix\n",
    "\n",
    "\n",
    "def get_ppr_matrix(\n",
    "        adj_matrix: np.ndarray,\n",
    "        alpha: float = 0.1) -> np.ndarray:\n",
    "    num_nodes = adj_matrix.shape[0]\n",
    "    A_tilde = adj_matrix + np.eye(num_nodes)\n",
    "    D_tilde = np.diag(1/np.sqrt(A_tilde.sum(axis=1)))\n",
    "    H = D_tilde @ A_tilde @ D_tilde\n",
    "    return alpha * np.linalg.inv(np.eye(num_nodes) - (1 - alpha) * H)\n",
    "\n",
    "\n",
    "def get_heat_matrix(\n",
    "        adj_matrix: np.ndarray,\n",
    "        t: float = 5.0) -> np.ndarray:\n",
    "    num_nodes = adj_matrix.shape[0]\n",
    "    A_tilde = adj_matrix + np.eye(num_nodes)\n",
    "    D_tilde = np.diag(1/np.sqrt(A_tilde.sum(axis=1)))\n",
    "    H = D_tilde @ A_tilde @ D_tilde\n",
    "    return expm(-t * (np.eye(num_nodes) - H))\n",
    "\n",
    "\n",
    "def get_top_k_matrix(A: np.ndarray, k: int = 128) -> np.ndarray:\n",
    "    num_nodes = A.shape[0]\n",
    "    row_idx = np.arange(num_nodes)\n",
    "    A[A.argsort(axis=0)[:num_nodes - k], row_idx] = 0.\n",
    "    norm = A.sum(axis=0)\n",
    "    norm[norm <= 0] = 1 # avoid dividing by zero\n",
    "    return A/norm\n",
    "\n",
    "\n",
    "def get_clipped_matrix(A: np.ndarray, eps: float = 0.01) -> np.ndarray:\n",
    "    num_nodes = A.shape[0]\n",
    "    A[A < eps] = 0.\n",
    "    norm = A.sum(axis=0)\n",
    "    norm[norm <= 0] = 1 # avoid dividing by zero\n",
    "    return A/norm\n",
    "\n",
    "\n",
    "def set_train_val_test_split(\n",
    "        seed: int,\n",
    "        data: Data,\n",
    "        num_development: int = 1500,\n",
    "        num_per_class: int = 20) -> Data:\n",
    "    rnd_state = np.random.RandomState(development_seed)\n",
    "    num_nodes = data.y.shape[0]\n",
    "    development_idx = rnd_state.choice(num_nodes, num_development, replace=False)\n",
    "    test_idx = [i for i in np.arange(num_nodes) if i not in development_idx]\n",
    "\n",
    "    train_idx = []\n",
    "    rnd_state = np.random.RandomState(seed)\n",
    "    for c in range(data.y.max() + 1):\n",
    "        class_idx = development_idx[np.where(data.y[development_idx].cpu() == c)[0]]\n",
    "        train_idx.extend(rnd_state.choice(class_idx, num_per_class, replace=False))\n",
    "\n",
    "    val_idx = [i for i in development_idx if i not in train_idx]\n",
    "\n",
    "    def get_mask(idx):\n",
    "        mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "        mask[idx] = 1\n",
    "        return mask\n",
    "\n",
    "    data.train_mask = get_mask(train_idx)\n",
    "    data.val_mask = get_mask(val_idx)\n",
    "    data.test_mask = get_mask(test_idx)\n",
    "\n",
    "    return data\n",
    "\n",
    "class PPRDataset(InMemoryDataset):\n",
    "    \"\"\"\n",
    "    Dataset preprocessed with GDC using PPR diffusion.\n",
    "    Note that this implementations is not scalable\n",
    "    since we directly invert the adjacency matrix.\n",
    "    \"\"\"\n",
    "    def __init__(self,noisy_data,\n",
    "                 name: str = 'Cora',\n",
    "                 use_lcc: bool = True,\n",
    "                 alpha: float = 0.1,\n",
    "                 k: int = 16,\n",
    "                 eps: float = None):\n",
    "        self.name = name\n",
    "        self.use_lcc = use_lcc\n",
    "        self.alpha = alpha\n",
    "        self.k = k\n",
    "        self.eps = eps\n",
    "        self.noisy_data = noisy_data\n",
    "\n",
    "        super(PPRDataset, self).__init__(DATA_PATH)\n",
    "        self.data, self.slices = torch.load(self.processed_paths[0])\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self) -> list:\n",
    "        return []\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self) -> list:\n",
    "        return [str(self) + '.pt']\n",
    "\n",
    "    def download(self):\n",
    "        pass\n",
    "\n",
    "    def process(self):\n",
    "        # base = get_dataset(name=self.name, use_lcc=self.use_lcc)\n",
    "        # generate adjacency matrix from sparse representation\n",
    "        adj_matrix = get_adj_matrix(self.noisy_data)\n",
    "        # obtain exact PPR matrix\n",
    "        ppr_matrix = get_ppr_matrix(adj_matrix,\n",
    "                                        alpha=self.alpha)\n",
    "\n",
    "        if self.k:\n",
    "            print(f'Selecting top {self.k} edges per node.')\n",
    "            ppr_matrix = get_top_k_matrix(ppr_matrix, k=self.k)\n",
    "        elif self.eps:\n",
    "            print(f'Selecting edges with weight greater than {self.eps}.')\n",
    "            ppr_matrix = get_clipped_matrix(ppr_matrix, eps=self.eps)\n",
    "        else:\n",
    "            raise ValueError\n",
    "\n",
    "        # create PyG Data object\n",
    "        edges_i = []\n",
    "        edges_j = []\n",
    "        edge_attr = []\n",
    "        for i, row in enumerate(ppr_matrix):\n",
    "            for j in np.where(row > 0)[0]:\n",
    "                edges_i.append(i)\n",
    "                edges_j.append(j)\n",
    "                edge_attr.append(ppr_matrix[i, j])\n",
    "        edge_index = [edges_i, edges_j]\n",
    "\n",
    "        data = Data(\n",
    "            x=self.noisy_data.x,\n",
    "            edge_index=torch.LongTensor(edge_index),\n",
    "            edge_attr=torch.FloatTensor(edge_attr),\n",
    "            y=self.noisy_data.y,\n",
    "            train_mask=torch.zeros(self.noisy_data.train_mask.size()[0], dtype=torch.bool),\n",
    "            test_mask=torch.zeros(self.noisy_data.test_mask.size()[0], dtype=torch.bool),\n",
    "            val_mask=torch.zeros(self.noisy_data.val_mask.size()[0], dtype=torch.bool)\n",
    "        )\n",
    "\n",
    "        data, slices = self.collate([data])\n",
    "        torch.save((data, slices), self.processed_paths[0])\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return f'{self.name}_ppr_alpha={self.alpha}_k={self.k}_eps={self.eps}_lcc={self.use_lcc}'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unify Contrastive GNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backdoor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GNN+CL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Contrastive learning to backdoor in Contrastive learning'''\n",
    "from models.GTA import Backdoor\n",
    "import heuristic_selection as hs\n",
    "from models.GCN import GCN\n",
    "from models.construct import model_construct\n",
    "import copy \n",
    "from model import UnifyModel, Encoder\n",
    "from models.construct import model_construct\n",
    "data = data.to(device)\n",
    "# learning_rate = 0.0002\n",
    "# weight_decay = config['weight_decay']\n",
    "num_class = int(data.y.max()+1)\n",
    "\n",
    "\n",
    "size = args.vs_number #int((len(data.test_mask)-data.test_mask.sum())*args.vs_ratio)\n",
    "print(\"#Attach Nodes:{}\".format(size))\n",
    "from models.construct import model_construct\n",
    "result_asr = []\n",
    "result_acc = []\n",
    "data = data.to(device)\n",
    "\n",
    "seen_node_idx = (torch.bitwise_not(data.test_mask)).nonzero().flatten()\n",
    "rs = np.random.RandomState(args.seed)\n",
    "seeds = rs.randint(1000,size=3)\n",
    "# seeds = [args.seed]\n",
    "for seed in seeds:\n",
    "    np.random.seed(seed)\n",
    "    # torch.manual_seed(seed)\n",
    "    # torch.cuda.manual_seed(seed)\n",
    "    args.seed = seed\n",
    "    if(args.selection_method == 'none'):\n",
    "        idx_attach = hs.obtain_attach_nodes(args,unlabeled_idx,size)\n",
    "    elif(args.selection_method == 'cluster'):\n",
    "        idx_attach = hs.cluster_distance_selection(args,data,idx_train,idx_val,idx_clean_test,unlabeled_idx,train_edge_index,size,device)\n",
    "        idx_attach = torch.LongTensor(idx_attach).to(device)\n",
    "    elif(args.selection_method == 'cluster_degree'):\n",
    "        idx_attach = hs.cluster_degree_selection(args,data,idx_train,idx_val,idx_clean_test,unlabeled_idx,train_edge_index,size,device)\n",
    "        idx_attach = torch.LongTensor(idx_attach).to(device)\n",
    "    # train trigger generator \n",
    "    model = Backdoor(args,device)\n",
    "    model.fit(data.x, train_edge_index, None, data.y, idx_train,idx_attach, unlabeled_idx)\n",
    "    poison_x, poison_edge_index, poison_edge_weights, poison_labels = model.get_poisoned()\n",
    "    bkd_tn_nodes = torch.cat([idx_train,idx_attach]).to(device)\n",
    "    print(\"precent of left attach nodes: {:.3f}\"\\\n",
    "        .format(len(set(bkd_tn_nodes.tolist()) & set(idx_attach.tolist()))/len(idx_attach)))\n",
    "    #%%\n",
    "\n",
    "    # model = GCN(nfeat=data.x.shape[1],\\\n",
    "    #             nhid=args.hidden,\\\n",
    "    #             nclass= int(data.y.max()+1),\\\n",
    "    #             dropout=args.dropout,\\\n",
    "    #             lr=args.train_lr,\\\n",
    "    #             weight_decay=args.weight_decay,\\\n",
    "    #             device=device)\n",
    "    # test_model = model_construct(args,args.test_model,data,device).to(device) \n",
    "    encoder = Encoder(dataset.num_features, args.num_hidden, args.cl_activation,\n",
    "                            base_model=args.cl_base_model, k=args.cl_num_layers).to(device)\n",
    "    # test_model = UnifyModel(args, encoder, args.cl_num_hidden, args.cl_num_proj_hidden, num_class, args.tau, lr=args.cl_lr, weight_decay=args.cl_weight_decay, device=None).to(device)\n",
    "    test_model = UnifyModel(args, encoder, args.num_hidden, args.num_proj_hidden, args.cl_num_proj_hidden, num_class, args.tau, lr=args.cl_lr, weight_decay=args.cl_weight_decay, device=device).to(device)\n",
    "    # test_model = UnifyModel(args, encoder, num_hidden, num_proj_hidden, num_class, tau, lr=learning_rate, weight_decay=weight_decay, device=device,data1=noisy_data,data2=diff_noisy_data).to(device)\n",
    "    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)\n",
    "    test_embds = test_model(poison_x,poison_edge_index,poison_edge_weights)\n",
    "    output = test_model.clf_head(test_embds)\n",
    "    train_attach_rate = (output.argmax(dim=1)[idx_attach]==args.target_class).float().mean()\n",
    "    print(\"target class rate on Vs: {:.4f}\".format(train_attach_rate))\n",
    "    torch.cuda.empty_cache()\n",
    "    # test_model = GCN(nfeat=cont_poison_x.shape[1],\\\n",
    "    #             nhid=args.hidden,\\\n",
    "    #             nclass= int(data.y.max()+1),\\\n",
    "    #             dropout=args.dropout,\\\n",
    "    #             lr=args.train_lr,\\\n",
    "    #             weight_decay=args.weight_decay,\\\n",
    "    #             device=device).to(device) \n",
    "    # test_model.fit(cont_poison_x, poison_edge_index, poison_edge_weights, poison_labels, bkd_tn_nodes, idx_val,train_iters=args.epochs,verbose=False)\n",
    "\n",
    "    # output = test_model(cont_poison_x,poison_edge_index,poison_edge_weights)\n",
    "    # train_attach_rate = (output.argmax(dim=1)[idx_attach]==args.target_class).float().mean()\n",
    "    # print(\"target class rate on Vs: {:.4f}\".format(train_attach_rate))\n",
    "    # torch.cuda.empty_cache()\n",
    "    #%%\n",
    "    induct_edge_index = torch.cat([poison_edge_index,mask_edge_index],dim=1)\n",
    "    induct_edge_weights = torch.cat([poison_edge_weights,torch.ones([mask_edge_index.shape[1]],dtype=torch.float,device=device)])\n",
    "    clean_acc = test_model.test(poison_x,induct_edge_index,induct_edge_weights,data.y,idx_clean_test)\n",
    "    # test_model = test_model.cpu()\n",
    "\n",
    "    print(\"accuracy on clean test nodes: {:.4f}\".format(clean_acc))\n",
    "\n",
    "\n",
    "    if(args.evaluate_mode == '1by1'):\n",
    "        from torch_geometric.utils  import k_hop_subgraph\n",
    "        overall_induct_edge_index, overall_induct_edge_weights = induct_edge_index.clone(),induct_edge_weights.clone()\n",
    "        asr = 0\n",
    "        flip_asr = 0\n",
    "        flip_idx_atk = idx_atk[(data.y[idx_atk] != args.target_class).nonzero().flatten()]\n",
    "        for i, idx in enumerate(idx_atk):\n",
    "            idx=int(idx)\n",
    "            sub_induct_nodeset, sub_induct_edge_index, sub_mapping, sub_edge_mask  = k_hop_subgraph(node_idx = [idx], num_hops = 2, edge_index = overall_induct_edge_index, relabel_nodes=True) # sub_mapping means the index of [idx] in sub)nodeset\n",
    "            ori_node_idx = sub_induct_nodeset[sub_mapping]\n",
    "            relabeled_node_idx = sub_mapping\n",
    "            sub_induct_edge_weights = torch.ones([sub_induct_edge_index.shape[1]]).to(device)\n",
    "            # inject trigger on attack test nodes (idx_atk)'''\n",
    "            with torch.no_grad():\n",
    "                induct_x, induct_edge_index,induct_edge_weights = model.inject_trigger(relabeled_node_idx,poison_x[sub_induct_nodeset],sub_induct_edge_index,sub_induct_edge_weights,device)\n",
    "                induct_x, induct_edge_index,induct_edge_weights = induct_x.clone().detach(), induct_edge_index.clone().detach(),induct_edge_weights.clone().detach()\n",
    "                test_embeds = test_model(induct_x, induct_edge_index,induct_edge_weights).to(device)\n",
    "                output = test_model.clf_head(test_embeds)\n",
    "                # cont_induct_x = contrastive_model(induct_x, induct_edge_index,induct_edge_weights).detach().to(device)\n",
    "                # # do pruning in test datas'''\n",
    "                # if(args.defense_mode == 'prune' or args.defense_mode == 'isolate'):\n",
    "                #     induct_edge_index,induct_edge_weights = prune_unrelated_edge(args,induct_edge_index,induct_edge_weights,induct_x,device,large_graph=False)\n",
    "                # attack evaluation\n",
    "\n",
    "                # output = test_model(cont_induct_x,induct_edge_index,induct_edge_weights)\n",
    "                train_attach_rate = (output.argmax(dim=1)[relabeled_node_idx]==args.target_class).float().mean()\n",
    "                # print(\"Node {}: {}, Origin Label: {}\".format(i, idx, data.y[idx]))\n",
    "                # print(\"ASR: {:.4f}\".format(train_attach_rate))\n",
    "                asr += train_attach_rate\n",
    "                if(data.y[idx] != args.target_class):\n",
    "                    flip_asr += train_attach_rate\n",
    "        asr = asr/(idx_atk.shape[0])\n",
    "        flip_asr = flip_asr/(flip_idx_atk.shape[0])\n",
    "        print(\"Overall ASR: {:.4f}\".format(asr))\n",
    "        print(\"Flip ASR: {:.4f}/{} nodes\".format(flip_asr,flip_idx_atk.shape[0]))\n",
    "    elif(args.evaluate_mode == 'overall'):\n",
    "        # %% inject trigger on attack test nodes (idx_atk)'''\n",
    "        induct_x, induct_edge_index,induct_edge_weights = model.inject_trigger(idx_atk,poison_x,induct_edge_index,induct_edge_weights,device)\n",
    "        induct_x, induct_edge_index,induct_edge_weights = induct_x.clone().detach(), induct_edge_index.clone().detach(),induct_edge_weights.clone().detach()\n",
    "        # do pruning in test datas'''\n",
    "        if(args.defense_mode == 'prune' or args.defense_mode == 'isolate'):\n",
    "            induct_edge_index,induct_edge_weights = prune_unrelated_edge(args,induct_edge_index,induct_edge_weights,induct_x,device)\n",
    "        # attack evaluation\n",
    "\n",
    "        test_embeds = test_model(induct_x, induct_edge_index,induct_edge_weights).to(device)\n",
    "        output = test_model.clf_head(test_embeds)\n",
    "        # test_model = test_model.to(device)\n",
    "        # output = test_model(induct_x,induct_edge_index,induct_edge_weights)\n",
    "        train_attach_rate = (output.argmax(dim=1)[idx_atk]==args.target_class).float().mean()\n",
    "        print(\"ASR: {:.4f}\".format(train_attach_rate))\n",
    "        flip_idx_atk = idx_atk[(data.y[idx_atk] != args.target_class).nonzero().flatten()]\n",
    "        flip_asr = (output.argmax(dim=1)[flip_idx_atk]==args.target_class).float().mean()\n",
    "        print(\"Flip ASR: {:.4f}/{} nodes\".format(flip_asr,flip_idx_atk.shape[0]))\n",
    "        ca = test_model.test(induct_x,induct_edge_index,induct_edge_weights,data.y,idx_clean_test)\n",
    "        print(\"CA: {:.4f}\".format(ca))\n",
    "\n",
    "    result_asr.append(float(asr))\n",
    "    result_acc.append(float(clean_acc))\n",
    "\n",
    "print('The final ASR:{:.5f}, {:.5f}, Accuracy:{:.5f}, {:.5f}'\\\n",
    "            .format(np.average(result_asr),np.std(result_asr),np.average(result_acc),np.std(result_acc)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Attach Nodes:160\n",
      "raw graph: torch.Size([2, 10556])\n",
      "add edge: torch.Size([2, 11624])\n",
      "remove edge: torch.Size([2, 9508])\n",
      "updated graph: torch.Size([2, 10576])\n",
      "Length of training set: 541\n",
      "=== training gcn model ===\n",
      "Epoch 0, training loss: 1.9487913846969604\n",
      "acc_val: 0.3222\n",
      "Epoch 10, training loss: 1.7001640796661377\n",
      "acc_val: 0.3222\n",
      "Epoch 20, training loss: 1.251969814300537\n",
      "acc_val: 0.5481\n",
      "Epoch 30, training loss: 0.9154055118560791\n",
      "acc_val: 0.5778\n",
      "Epoch 40, training loss: 0.7175220251083374\n",
      "acc_val: 0.6778\n",
      "Epoch 50, training loss: 0.5528076887130737\n",
      "acc_val: 0.7556\n",
      "Epoch 60, training loss: 0.45035868883132935\n",
      "acc_val: 0.7667\n",
      "Epoch 70, training loss: 0.3029544949531555\n",
      "acc_val: 0.7815\n",
      "Epoch 80, training loss: 0.26119792461395264\n",
      "acc_val: 0.7852\n",
      "Epoch 90, training loss: 0.2227189987897873\n",
      "acc_val: 0.8185\n",
      "Epoch 100, training loss: 0.16095885634422302\n",
      "acc_val: 0.8074\n",
      "Epoch 110, training loss: 0.12907230854034424\n",
      "acc_val: 0.8148\n",
      "Epoch 120, training loss: 0.0998087078332901\n",
      "acc_val: 0.8296\n",
      "Epoch 130, training loss: 0.09762615710496902\n",
      "acc_val: 0.8259\n",
      "Epoch 140, training loss: 0.08657265454530716\n",
      "acc_val: 0.8148\n",
      "Epoch 150, training loss: 0.09258854389190674\n",
      "acc_val: 0.8111\n",
      "Epoch 160, training loss: 0.10084856301546097\n",
      "acc_val: 0.8259\n",
      "Epoch 170, training loss: 0.07240049540996552\n",
      "acc_val: 0.8148\n",
      "Epoch 180, training loss: 0.0827595442533493\n",
      "acc_val: 0.8074\n",
      "Epoch 190, training loss: 0.07424037158489227\n",
      "acc_val: 0.8074\n",
      "=== picking the best model according to the performance on validation ===\n",
      "Training encoder Finished!\n",
      "Total time elapsed: 1.2694s\n",
      "Encoder CA on clean test nodes: 0.7704\n",
      "[2 4 4 ... 6 2 2]\n",
      "precent of left attach nodes: 1.000\n",
      "target class rate on Vs: 1.0000\n",
      "accuracy on clean test nodes: 0.8037\n",
      "Overall ASR: 0.9594\n",
      "Flip ASR: 0.9511/225 nodes\n",
      "Length of training set: 541\n",
      "=== training gcn model ===\n",
      "Epoch 0, training loss: 1.9455997943878174\n",
      "acc_val: 0.1296\n",
      "Epoch 10, training loss: 1.722038745880127\n",
      "acc_val: 0.3222\n",
      "Epoch 20, training loss: 1.3054707050323486\n",
      "acc_val: 0.5296\n",
      "Epoch 30, training loss: 0.9448956847190857\n",
      "acc_val: 0.5778\n",
      "Epoch 40, training loss: 0.7089287042617798\n",
      "acc_val: 0.7222\n",
      "Epoch 50, training loss: 0.5311965346336365\n",
      "acc_val: 0.7556\n",
      "Epoch 60, training loss: 0.35577860474586487\n",
      "acc_val: 0.7741\n",
      "Epoch 70, training loss: 0.2966192364692688\n",
      "acc_val: 0.7815\n",
      "Epoch 80, training loss: 0.2487214207649231\n",
      "acc_val: 0.8148\n",
      "Epoch 90, training loss: 0.18961064517498016\n",
      "acc_val: 0.8148\n",
      "Epoch 100, training loss: 0.1612675040960312\n",
      "acc_val: 0.8296\n",
      "Epoch 110, training loss: 0.1248779147863388\n",
      "acc_val: 0.8148\n",
      "Epoch 120, training loss: 0.11717411130666733\n",
      "acc_val: 0.8222\n",
      "Epoch 130, training loss: 0.10370758920907974\n",
      "acc_val: 0.8148\n",
      "Epoch 140, training loss: 0.10872744023799896\n",
      "acc_val: 0.8074\n",
      "Epoch 150, training loss: 0.09167472273111343\n",
      "acc_val: 0.8148\n",
      "Epoch 160, training loss: 0.08959712833166122\n",
      "acc_val: 0.8074\n",
      "Epoch 170, training loss: 0.11034353077411652\n",
      "acc_val: 0.8259\n",
      "Epoch 180, training loss: 0.08823854476213455\n",
      "acc_val: 0.8148\n",
      "Epoch 190, training loss: 0.08149073272943497\n",
      "acc_val: 0.8037\n",
      "=== picking the best model according to the performance on validation ===\n",
      "Training encoder Finished!\n",
      "Total time elapsed: 1.2062s\n",
      "Encoder CA on clean test nodes: 0.7407\n",
      "[0 3 3 ... 5 0 0]\n",
      "precent of left attach nodes: 1.000\n",
      "target class rate on Vs: 1.0000\n",
      "accuracy on clean test nodes: 0.8222\n",
      "Overall ASR: 0.9815\n",
      "Flip ASR: 0.9778/225 nodes\n",
      "Length of training set: 541\n",
      "=== training gcn model ===\n",
      "Epoch 0, training loss: 1.9298548698425293\n",
      "acc_val: 0.3222\n",
      "Epoch 10, training loss: 1.6883423328399658\n",
      "acc_val: 0.3222\n",
      "Epoch 20, training loss: 1.2069751024246216\n",
      "acc_val: 0.5630\n",
      "Epoch 30, training loss: 0.815941572189331\n",
      "acc_val: 0.6556\n",
      "Epoch 40, training loss: 0.5853594541549683\n",
      "acc_val: 0.7852\n",
      "Epoch 50, training loss: 0.3796159625053406\n",
      "acc_val: 0.8148\n",
      "Epoch 60, training loss: 0.24937653541564941\n",
      "acc_val: 0.8259\n",
      "Epoch 70, training loss: 0.16019858419895172\n",
      "acc_val: 0.8148\n",
      "Epoch 80, training loss: 0.1436629295349121\n",
      "acc_val: 0.8296\n",
      "Epoch 90, training loss: 0.1204776018857956\n",
      "acc_val: 0.8259\n",
      "Epoch 100, training loss: 0.10674170404672623\n",
      "acc_val: 0.8111\n",
      "Epoch 110, training loss: 0.10395748168230057\n",
      "acc_val: 0.8222\n",
      "Epoch 120, training loss: 0.08133775740861893\n",
      "acc_val: 0.8296\n",
      "Epoch 130, training loss: 0.08778337389230728\n",
      "acc_val: 0.8185\n",
      "Epoch 140, training loss: 0.0891847088932991\n",
      "acc_val: 0.8296\n",
      "Epoch 150, training loss: 0.08757417649030685\n",
      "acc_val: 0.8296\n",
      "Epoch 160, training loss: 0.0776069313287735\n",
      "acc_val: 0.8370\n",
      "Epoch 170, training loss: 0.07304735481739044\n",
      "acc_val: 0.8185\n",
      "Epoch 180, training loss: 0.07583433389663696\n",
      "acc_val: 0.8296\n",
      "Epoch 190, training loss: 0.06716092675924301\n",
      "acc_val: 0.8370\n",
      "=== picking the best model according to the performance on validation ===\n",
      "Training encoder Finished!\n",
      "Total time elapsed: 1.2546s\n",
      "Encoder CA on clean test nodes: 0.7704\n",
      "[5 3 5 ... 2 5 5]\n",
      "precent of left attach nodes: 1.000\n",
      "target class rate on Vs: 1.0000\n",
      "accuracy on clean test nodes: 0.8111\n",
      "Overall ASR: 0.9336\n",
      "Flip ASR: 0.9200/225 nodes\n",
      "The final ASR:0.95818, 0.01960, Accuracy:0.81235, 0.00761\n"
     ]
    }
   ],
   "source": [
    "'''Backdoor attack to GNN classifier'''\n",
    "from models.GTA import Backdoor\n",
    "import heuristic_selection as hs\n",
    "from models.GCN import GCN\n",
    "from models.construct import model_construct\n",
    "from construct_graph import *\n",
    "# args.homo_loss_weight=config['homo_loss_weight']\n",
    "# args.vs_number=config['vs_number']\n",
    "# args.trojan_epochs = config['trojan_epochs']\n",
    "\n",
    "data = data.to(device)\n",
    "num_class = int(data.y.max()+1)\n",
    "# args.seed = config['seed']\n",
    "\n",
    "size = args.vs_number #int((len(data.test_mask)-data.test_mask.sum())*args.vs_ratio)\n",
    "print(\"#Attach Nodes:{}\".format(size))\n",
    "from models.construct import model_construct\n",
    "result_asr = []\n",
    "result_acc = []\n",
    "data = data.to(device)\n",
    "\n",
    "noisy_data = construct_noisy_graph(data,perturb_ratio=0.1,mode='random_noise')\n",
    "data = noisy_data.to(device)\n",
    "rs = np.random.RandomState(args.seed)\n",
    "seeds = rs.randint(1000,size=3)\n",
    "# seeds = [args.seed]\n",
    "for seed in seeds:\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    args.seed = seed\n",
    "    if(args.selection_method == 'none'):\n",
    "        idx_attach = hs.obtain_attach_nodes(args,unlabeled_idx,size)\n",
    "    elif(args.selection_method == 'cluster'):\n",
    "        idx_attach = hs.cluster_distance_selection(args,data,idx_train,idx_val,idx_clean_test,unlabeled_idx,train_edge_index,size,device)\n",
    "        idx_attach = torch.LongTensor(idx_attach).to(device)\n",
    "    elif(args.selection_method == 'cluster_degree'):\n",
    "        idx_attach = hs.cluster_degree_selection(args,data,idx_train,idx_val,idx_clean_test,unlabeled_idx,train_edge_index,size,device)\n",
    "        idx_attach = torch.LongTensor(idx_attach).to(device)\n",
    "    # train trigger generator \n",
    "    model = Backdoor(args,device)\n",
    "    model.fit(data.x, train_edge_index, None, data.y, idx_train,idx_attach, unlabeled_idx)\n",
    "    poison_x, poison_edge_index, poison_edge_weights, poison_labels = model.get_poisoned()\n",
    "    bkd_tn_nodes = torch.cat([idx_train,idx_attach]).to(device)\n",
    "    # # learn contrastive node representation\n",
    "    # encoder = Encoder(dataset.num_features, num_hidden, activation,\n",
    "    #                     base_model=base_model, k=num_layers).to(device)\n",
    "    # contrastive_model = Model(encoder, num_hidden, num_proj_hidden, tau).to(device)\n",
    "    # optimizer = torch.optim.Adam(\n",
    "    #     contrastive_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    \n",
    "    # start = t()\n",
    "    # prev = start\n",
    "    # for epoch in range(1, num_epochs + 1):\n",
    "    #     # loss = train_(model, data.x, data.edge_index)\n",
    "    #     loss = train_1(contrastive_model, poison_x, poison_edge_index, poison_edge_weights)\n",
    "\n",
    "    #     now = t()\n",
    "    #     if(epoch%10 == 0):\n",
    "    #         print(f'(T) | Epoch={epoch:03d}, loss={loss:.4f}, '\n",
    "    #                 f'this epoch {now - prev:.4f}, total {now - start:.4f}')\n",
    "    #     prev = now\n",
    "\n",
    "    # contrastive_model.eval()\n",
    "    # cont_poison_x = contrastive_model(poison_x, poison_edge_index, poison_edge_weights).detach().to(device)\n",
    "\n",
    "    print(\"precent of left attach nodes: {:.3f}\"\\\n",
    "        .format(len(set(bkd_tn_nodes.tolist()) & set(idx_attach.tolist()))/len(idx_attach)))\n",
    "    #%%\n",
    "\n",
    "    # model = GCN(nfeat=data.x.shape[1],\\\n",
    "    #             nhid=args.hidden,\\\n",
    "    #             nclass= int(data.y.max()+1),\\\n",
    "    #             dropout=args.dropout,\\\n",
    "    #             lr=args.train_lr,\\\n",
    "    #             weight_decay=args.weight_decay,\\\n",
    "    #             device=device)\n",
    "    test_model = model_construct(args,args.test_model,data,device).to(device) \n",
    "    # test_model = GCN(nfeat=cont_poison_x.shape[1],\\\n",
    "    #             nhid=args.hidden,\\\n",
    "    #             nclass= int(data.y.max()+1),\\\n",
    "    #             dropout=args.dropout,\\\n",
    "    #             lr=args.train_lr,\\\n",
    "    #             weight_decay=args.weight_decay,\\\n",
    "    #             device=device).to(device) \n",
    "    test_model.fit(poison_x, poison_edge_index, poison_edge_weights, poison_labels, bkd_tn_nodes, idx_val,train_iters=args.epochs,verbose=False)\n",
    "\n",
    "    output = test_model(poison_x,poison_edge_index,poison_edge_weights)\n",
    "    train_attach_rate = (output.argmax(dim=1)[idx_attach]==args.target_class).float().mean()\n",
    "    print(\"target class rate on Vs: {:.4f}\".format(train_attach_rate))\n",
    "    torch.cuda.empty_cache()\n",
    "    #%%\n",
    "    induct_edge_index = torch.cat([poison_edge_index,mask_edge_index],dim=1)\n",
    "    induct_edge_weights = torch.cat([poison_edge_weights,torch.ones([mask_edge_index.shape[1]],dtype=torch.float,device=device)])\n",
    "    clean_acc = test_model.test(poison_x,induct_edge_index,induct_edge_weights,data.y,idx_clean_test)\n",
    "    # test_model = test_model.cpu()\n",
    "\n",
    "    print(\"accuracy on clean test nodes: {:.4f}\".format(clean_acc))\n",
    "\n",
    "\n",
    "    if(args.evaluate_mode == '1by1'):\n",
    "        from torch_geometric.utils  import k_hop_subgraph\n",
    "        overall_induct_edge_index, overall_induct_edge_weights = induct_edge_index.clone(),induct_edge_weights.clone()\n",
    "        asr = 0\n",
    "        flip_asr = 0\n",
    "        flip_idx_atk = idx_atk[(data.y[idx_atk] != args.target_class).nonzero().flatten()]\n",
    "        for i, idx in enumerate(idx_atk):\n",
    "            idx=int(idx)\n",
    "            sub_induct_nodeset, sub_induct_edge_index, sub_mapping, sub_edge_mask  = k_hop_subgraph(node_idx = [idx], num_hops = 2, edge_index = overall_induct_edge_index, relabel_nodes=True) # sub_mapping means the index of [idx] in sub)nodeset\n",
    "            ori_node_idx = sub_induct_nodeset[sub_mapping]\n",
    "            relabeled_node_idx = sub_mapping\n",
    "            sub_induct_edge_weights = torch.ones([sub_induct_edge_index.shape[1]]).to(device)\n",
    "            # inject trigger on attack test nodes (idx_atk)'''\n",
    "            with torch.no_grad():\n",
    "                induct_x, induct_edge_index,induct_edge_weights = model.inject_trigger(relabeled_node_idx,poison_x[sub_induct_nodeset],sub_induct_edge_index,sub_induct_edge_weights,device)\n",
    "                induct_x, induct_edge_index,induct_edge_weights = induct_x.clone().detach(), induct_edge_index.clone().detach(),induct_edge_weights.clone().detach()\n",
    "                # cont_induct_x = contrastive_model(induct_x, induct_edge_index,induct_edge_weights).detach().to(device)\n",
    "                # # do pruning in test datas'''\n",
    "                # if(args.defense_mode == 'prune' or args.defense_mode == 'isolate'):\n",
    "                #     induct_edge_index,induct_edge_weights = prune_unrelated_edge(args,induct_edge_index,induct_edge_weights,induct_x,device,large_graph=False)\n",
    "                # attack evaluation\n",
    "\n",
    "                output = test_model(induct_x,induct_edge_index,induct_edge_weights)\n",
    "                train_attach_rate = (output.argmax(dim=1)[relabeled_node_idx]==args.target_class).float().mean()\n",
    "                # print(\"Node {}: {}, Origin Label: {}\".format(i, idx, data.y[idx]))\n",
    "                # print(\"ASR: {:.4f}\".format(train_attach_rate))\n",
    "                asr += train_attach_rate\n",
    "                if(data.y[idx] != args.target_class):\n",
    "                    flip_asr += train_attach_rate\n",
    "        asr = asr/(idx_atk.shape[0])\n",
    "        flip_asr = flip_asr/(flip_idx_atk.shape[0])\n",
    "        print(\"Overall ASR: {:.4f}\".format(asr))\n",
    "        print(\"Flip ASR: {:.4f}/{} nodes\".format(flip_asr,flip_idx_atk.shape[0]))\n",
    "    result_asr.append(float(asr))\n",
    "    result_acc.append(float(clean_acc))\n",
    "print('The final ASR:{:.5f}, {:.5f}, Accuracy:{:.5f}, {:.5f}'\\\n",
    "            .format(np.average(result_asr),np.std(result_asr),np.average(result_acc),np.std(result_acc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('py38_torch110')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c4260eba67904b42d68a3963bc583366103d86fb6c89846e20de6072b78e7707"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
