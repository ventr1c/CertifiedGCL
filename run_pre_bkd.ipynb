{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* implement unifyGNN+Contrastive\n",
    "* try different noisy and augmentation\n",
    "* try against with/without unnoticeable "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(add_edge_rate_1=0, add_edge_rate_2=0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.001, cl_num_epochs=700, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, clf_weight=1, config='config.yaml', cont_batch_size=0, cont_weight=3, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.2, drop_edge_rate_2=0.5, drop_feat_rate_1=0.0, drop_feat_rate_2=0, dropout=0.5, epochs=200, evaluate_mode='1by1', hidden=32, homo_boost_thrd=0.5, homo_loss_weight=100, inner=1, inv_weight=1, lr=0.01, model='GCN', no_cuda=False, num_hidden=128, num_proj_hidden=128, prune_thr=0.2, seed=10, select_thrh=0.8, selection_method='none', target_class=0, target_loss_weight=1, tau=0.1, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=400, use_vs_number=True, vs_number=10, vs_ratio=0, weight_decay=0.0005)\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# In[1]: \n",
    "\n",
    "import imp\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from torch_geometric.datasets import Planetoid,Reddit2,Flickr,PPI\n",
    "\n",
    "\n",
    "# from torch_geometric.loader import DataLoader\n",
    "# from help_funcs import prune_unrelated_edge,prune_unrelated_edge_isolated\n",
    "import scipy.sparse as sp\n",
    "\n",
    "# Training settings\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--debug', action='store_true',\n",
    "        default=True, help='debug mode')\n",
    "parser.add_argument('--no-cuda', action='store_true', default=False,\n",
    "                    help='Disables CUDA training.')\n",
    "parser.add_argument('--seed', type=int, default=10, help='Random seed.')\n",
    "parser.add_argument('--model', type=str, default='GCN', help='model',\n",
    "                    choices=['GCN','GAT','GraphSage','GIN'])\n",
    "parser.add_argument('--dataset', type=str, default='Cora', \n",
    "                    help='Dataset',\n",
    "                    choices=['Cora','Citeseer','Pubmed','PPI','Flickr','ogbn-arxiv','Reddit','Reddit2','Yelp'])\n",
    "parser.add_argument('--train_lr', type=float, default=0.01,\n",
    "                    help='Initial learning rate.')\n",
    "parser.add_argument('--weight_decay', type=float, default=5e-4,\n",
    "                    help='Weight decay (L2 loss on parameters).')\n",
    "parser.add_argument('--hidden', type=int, default=32,\n",
    "                    help='Number of hidden units.')\n",
    "parser.add_argument('--num_hidden', type=int, default=128,\n",
    "                    help='Number of hidden units.')\n",
    "parser.add_argument('--num_proj_hidden', type=int, default=128,\n",
    "                    help='Number of hidden units in MLP.')\n",
    "parser.add_argument('--thrd', type=float, default=0.5)\n",
    "parser.add_argument('--target_class', type=int, default=0)\n",
    "parser.add_argument('--dropout', type=float, default=0.5,\n",
    "                    help='Dropout rate (1 - keep probability).')\n",
    "parser.add_argument('--epochs', type=int,  default=200, help='Number of epochs to train benign and backdoor model.')\n",
    "parser.add_argument('--trojan_epochs', type=int,  default=400, help='Number of epochs to train trigger generator.')\n",
    "parser.add_argument('--inner', type=int,  default=1, help='Number of inner')\n",
    "parser.add_argument('--temperature', type=float,  default=0.5, help='Temperature')\n",
    "# backdoor setting\n",
    "parser.add_argument('--lr', type=float, default=0.01,\n",
    "                    help='Initial learning rate.')\n",
    "parser.add_argument('--trigger_size', type=int, default=3,\n",
    "                    help='tirgger_size')\n",
    "parser.add_argument('--use_vs_number', action='store_true', default=True,\n",
    "                    help=\"if use detailed number to decide Vs\")\n",
    "parser.add_argument('--vs_ratio', type=float, default=0,\n",
    "                    help=\"ratio of poisoning nodes relative to the full graph\")\n",
    "parser.add_argument('--vs_number', type=int, default=10,\n",
    "                    help=\"number of poisoning nodes relative to the full graph\")\n",
    "# defense setting\n",
    "parser.add_argument('--defense_mode', type=str, default=\"none\",\n",
    "                    choices=['prune', 'isolate', 'none'],\n",
    "                    help=\"Mode of defense\")\n",
    "parser.add_argument('--prune_thr', type=float, default=0.2,\n",
    "                    help=\"Threshold of prunning edges\")\n",
    "parser.add_argument('--target_loss_weight', type=float, default=1,\n",
    "                    help=\"Weight of optimize outter trigger generator\")\n",
    "parser.add_argument('--homo_loss_weight', type=float, default=100,\n",
    "                    help=\"Weight of optimize similarity loss\")\n",
    "parser.add_argument('--homo_boost_thrd', type=float, default=0.5,\n",
    "                    help=\"Threshold of increase similarity\")\n",
    "# attack setting\n",
    "parser.add_argument('--dis_weight', type=float, default=1,\n",
    "                    help=\"Weight of cluster distance\")\n",
    "parser.add_argument('--selection_method', type=str, default='none',\n",
    "                    choices=['loss','conf','cluster','none','cluster_degree'],\n",
    "                    help='Method to select idx_attach for training trojan model (none means randomly select)')\n",
    "parser.add_argument('--test_model', type=str, default='GCN',\n",
    "                    choices=['GCN','GAT','GraphSage','GIN'],\n",
    "                    help='Model used to attack')\n",
    "parser.add_argument('--evaluate_mode', type=str, default='1by1',\n",
    "                    choices=['overall','1by1'],\n",
    "                    help='Model used to attack')\n",
    "# GPU setting\n",
    "parser.add_argument('--device_id', type=int, default=2,\n",
    "                    help=\"Threshold of prunning edges\")\n",
    "# GRACE setting\n",
    "parser.add_argument('--config', type=str, default=\"config.yaml\")\n",
    "## Contrasitve setting\n",
    "parser.add_argument('--cl_lr', type=float, default=0.001)\n",
    "# parser.add_argument('--cl_num_hidden', type=int, default=64)\n",
    "parser.add_argument('--cl_num_proj_hidden', type=int, default=128)\n",
    "parser.add_argument('--cl_num_layers', type=int, default=2)\n",
    "parser.add_argument('--cl_activation', type=str, default='relu')\n",
    "parser.add_argument('--cl_base_model', type=str, default='GCNConv')\n",
    "parser.add_argument('--cont_weight', type=float, default=3)\n",
    "parser.add_argument('--add_edge_rate_1', type=float, default=0)\n",
    "parser.add_argument('--add_edge_rate_2', type=float, default=0)\n",
    "parser.add_argument('--drop_edge_rate_1', type=float, default=0.2)\n",
    "parser.add_argument('--drop_edge_rate_2', type=float, default=0.5)\n",
    "parser.add_argument('--drop_feat_rate_1', type=float, default=0.)\n",
    "parser.add_argument('--drop_feat_rate_2', type=float, default=0)\n",
    "parser.add_argument('--tau', type=float, default=0.1)\n",
    "parser.add_argument('--cl_num_epochs', type=int, default=700)\n",
    "parser.add_argument('--cl_weight_decay', type=float, default=1e-5)\n",
    "parser.add_argument('--cont_batch_size', type=int, default=0)\n",
    "parser.add_argument('--clf_weight', type=float, default=1)\n",
    "parser.add_argument('--inv_weight', type=float, default=1)\n",
    "parser.add_argument('--select_thrh', type=float, default=0.8)\n",
    "\n",
    "\n",
    "# args = parser.parse_args()\n",
    "args = parser.parse_known_args()[0]\n",
    "args.cuda =  not args.no_cuda and torch.cuda.is_available()\n",
    "device = torch.device(('cuda:{}' if torch.cuda.is_available() else 'cpu').format(args.device_id))\n",
    "\n",
    "np.random.seed(args.seed)\n",
    "# torch.manual_seed(args.seed)\n",
    "# torch.cuda.manual_seed(args.seed)\n",
    "print(args)\n",
    "#%%\n",
    "from torch_geometric.utils import to_undirected\n",
    "import torch_geometric.transforms as T\n",
    "transform = T.Compose([T.NormalizeFeatures()])\n",
    "\n",
    "if(args.dataset == 'Cora' or args.dataset == 'Citeseer' or args.dataset == 'Pubmed'):\n",
    "    dataset = Planetoid(root='./data/', \\\n",
    "                        name=args.dataset,\\\n",
    "                        transform=transform)\n",
    "elif(args.dataset == 'Flickr'):\n",
    "    dataset = Flickr(root='./data/Flickr/', \\\n",
    "                    transform=transform)\n",
    "elif(args.dataset == 'Reddit2'):\n",
    "    dataset = Reddit2(root='./data/Reddit2/', \\\n",
    "                    transform=transform)\n",
    "elif(args.dataset == 'ogbn-arxiv'):\n",
    "    from ogb.nodeproppred import PygNodePropPredDataset\n",
    "    # Download and process data at './dataset/ogbg_molhiv/'\n",
    "    dataset = PygNodePropPredDataset(name = 'ogbn-arxiv', root='./data/')\n",
    "    split_idx = dataset.get_idx_split() \n",
    "\n",
    "data = dataset[0].to(device)\n",
    "\n",
    "if(args.dataset == 'ogbn-arxiv'):\n",
    "    nNode = data.x.shape[0]\n",
    "    setattr(data,'train_mask',torch.zeros(nNode, dtype=torch.bool).to(device))\n",
    "    # dataset[0].train_mask = torch.zeros(nEdge, dtype=torch.bool).to(device)\n",
    "    data.val_mask = torch.zeros(nNode, dtype=torch.bool).to(device)\n",
    "    data.test_mask = torch.zeros(nNode, dtype=torch.bool).to(device)\n",
    "    data.y = data.y.squeeze(1)\n",
    "# we build our own train test split \n",
    "#%% \n",
    "from utils import get_split\n",
    "# data, idx_train, idx_val, idx_clean_test, idx_atk = get_split(args,data,device)\n",
    "idx_train = data.train_mask.nonzero().flatten()\n",
    "idx_val = data.val_mask.nonzero().flatten()\n",
    "idx_test = data.test_mask.nonzero().flatten()\n",
    "idx_clean_test = idx_test[:int(len(idx_test)/2)]\n",
    "idx_atk = idx_test[int(len(idx_test)/2):]\n",
    "\n",
    "from torch_geometric.utils import to_undirected\n",
    "from utils import subgraph\n",
    "data.edge_index = to_undirected(data.edge_index)\n",
    "train_edge_index,_, edge_mask = subgraph(torch.bitwise_not(data.test_mask),data.edge_index,relabel_nodes=False)\n",
    "mask_edge_index = data.edge_index[:,torch.bitwise_not(edge_mask)]\n",
    "# filter out the unlabeled nodes except from training nodes and testing nodes, nonzero() is to get index, flatten is to get 1-d tensor\n",
    "unlabeled_idx = (torch.bitwise_not(data.test_mask)&torch.bitwise_not(data.train_mask)).nonzero().flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch_geometric.utils import to_dense_adj, dense_to_sparse\n",
    "# from torch_geometric.data import Data, InMemoryDataset\n",
    "\n",
    "# import os\n",
    "\n",
    "# import numpy as np\n",
    "# from scipy.linalg import expm\n",
    "\n",
    "# import torch\n",
    "# from torch_geometric.data import Data, InMemoryDataset\n",
    "# from torch_geometric.datasets import Planetoid, Amazon, Coauthor\n",
    "\n",
    "# # from seeds import development_seed\n",
    "# DATA_PATH = './data/'\n",
    "\n",
    "# def get_dataset(name: str, use_lcc: bool = True) -> InMemoryDataset:\n",
    "#     path = os.path.join(DATA_PATH, name)\n",
    "#     if name in ['Cora', 'Citeseer', 'Pubmed']:\n",
    "#         dataset = Planetoid(path, name)\n",
    "#     elif name in ['Computers', 'Photo']:\n",
    "#         dataset = Amazon(path, name)\n",
    "#     elif name == 'CoauthorCS':\n",
    "#         dataset = Coauthor(path, 'CS')\n",
    "#     else:\n",
    "#         raise Exception('Unknown dataset.')\n",
    "\n",
    "#     if use_lcc:\n",
    "#         lcc = get_largest_connected_component(dataset)\n",
    "\n",
    "#         x_new = dataset.data.x[lcc]\n",
    "#         y_new = dataset.data.y[lcc]\n",
    "\n",
    "#         row, col = dataset.data.edge_index.numpy()\n",
    "#         edges = [[i, j] for i, j in zip(row, col) if i in lcc and j in lcc]\n",
    "#         edges = remap_edges(edges, get_node_mapper(lcc))\n",
    "        \n",
    "#         data = Data(\n",
    "#             x=x_new,\n",
    "#             edge_index=torch.LongTensor(edges),\n",
    "#             y=y_new,\n",
    "#             train_mask=torch.zeros(y_new.size()[0], dtype=torch.bool),\n",
    "#             test_mask=torch.zeros(y_new.size()[0], dtype=torch.bool),\n",
    "#             val_mask=torch.zeros(y_new.size()[0], dtype=torch.bool)\n",
    "#         )\n",
    "#         dataset.data = data\n",
    "\n",
    "#     return dataset\n",
    "\n",
    "\n",
    "# def get_component(dataset: InMemoryDataset, start: int = 0) -> set:\n",
    "#     visited_nodes = set()\n",
    "#     queued_nodes = set([start])\n",
    "#     row, col = dataset.data.edge_index.numpy()\n",
    "#     while queued_nodes:\n",
    "#         current_node = queued_nodes.pop()\n",
    "#         visited_nodes.update([current_node])\n",
    "#         neighbors = col[np.where(row == current_node)[0]]\n",
    "#         neighbors = [n for n in neighbors if n not in visited_nodes and n not in queued_nodes]\n",
    "#         queued_nodes.update(neighbors)\n",
    "#     return visited_nodes\n",
    "\n",
    "\n",
    "# def get_largest_connected_component(dataset: InMemoryDataset) -> np.ndarray:\n",
    "#     remaining_nodes = set(range(dataset.data.x.shape[0]))\n",
    "#     comps = []\n",
    "#     while remaining_nodes:\n",
    "#         start = min(remaining_nodes)\n",
    "#         comp = get_component(dataset, start)\n",
    "#         comps.append(comp)\n",
    "#         remaining_nodes = remaining_nodes.difference(comp)\n",
    "#     return np.array(list(comps[np.argmax(list(map(len, comps)))]))\n",
    "\n",
    "\n",
    "# def get_node_mapper(lcc: np.ndarray) -> dict:\n",
    "#     mapper = {}\n",
    "#     counter = 0\n",
    "#     for node in lcc:\n",
    "#         mapper[node] = counter\n",
    "#         counter += 1\n",
    "#     return mapper\n",
    "\n",
    "\n",
    "# def remap_edges(edges: list, mapper: dict) -> list:\n",
    "#     row = [e[0] for e in edges]\n",
    "#     col = [e[1] for e in edges]\n",
    "#     row = list(map(lambda x: mapper[x], row))\n",
    "#     col = list(map(lambda x: mapper[x], col))\n",
    "#     return [row, col]\n",
    "\n",
    "\n",
    "# def get_adj_matrix(data) -> np.ndarray:\n",
    "#     num_nodes = data.x.shape[0]\n",
    "#     adj_matrix = np.zeros(shape=(num_nodes, num_nodes))\n",
    "#     for i, j in zip(data.edge_index[0], data.edge_index[1]):\n",
    "#         adj_matrix[i, j] = 1.\n",
    "#     return adj_matrix\n",
    "\n",
    "\n",
    "# def get_ppr_matrix(\n",
    "#         adj_matrix: np.ndarray,\n",
    "#         alpha: float = 0.1) -> np.ndarray:\n",
    "#     num_nodes = adj_matrix.shape[0]\n",
    "#     A_tilde = adj_matrix + np.eye(num_nodes)\n",
    "#     D_tilde = np.diag(1/np.sqrt(A_tilde.sum(axis=1)))\n",
    "#     H = D_tilde @ A_tilde @ D_tilde\n",
    "#     return alpha * np.linalg.inv(np.eye(num_nodes) - (1 - alpha) * H)\n",
    "\n",
    "\n",
    "# def get_heat_matrix(\n",
    "#         adj_matrix: np.ndarray,\n",
    "#         t: float = 5.0) -> np.ndarray:\n",
    "#     num_nodes = adj_matrix.shape[0]\n",
    "#     A_tilde = adj_matrix + np.eye(num_nodes)\n",
    "#     D_tilde = np.diag(1/np.sqrt(A_tilde.sum(axis=1)))\n",
    "#     H = D_tilde @ A_tilde @ D_tilde\n",
    "#     return expm(-t * (np.eye(num_nodes) - H))\n",
    "\n",
    "\n",
    "# def get_top_k_matrix(A: np.ndarray, k: int = 128) -> np.ndarray:\n",
    "#     num_nodes = A.shape[0]\n",
    "#     row_idx = np.arange(num_nodes)\n",
    "#     A[A.argsort(axis=0)[:num_nodes - k], row_idx] = 0.\n",
    "#     norm = A.sum(axis=0)\n",
    "#     norm[norm <= 0] = 1 # avoid dividing by zero\n",
    "#     return A/norm\n",
    "\n",
    "\n",
    "# def get_clipped_matrix(A: np.ndarray, eps: float = 0.01) -> np.ndarray:\n",
    "#     num_nodes = A.shape[0]\n",
    "#     A[A < eps] = 0.\n",
    "#     norm = A.sum(axis=0)\n",
    "#     norm[norm <= 0] = 1 # avoid dividing by zero\n",
    "#     return A/norm\n",
    "\n",
    "\n",
    "# def set_train_val_test_split(\n",
    "#         seed: int,\n",
    "#         data: Data,\n",
    "#         num_development: int = 1500,\n",
    "#         num_per_class: int = 20) -> Data:\n",
    "#     rnd_state = np.random.RandomState(development_seed)\n",
    "#     num_nodes = data.y.shape[0]\n",
    "#     development_idx = rnd_state.choice(num_nodes, num_development, replace=False)\n",
    "#     test_idx = [i for i in np.arange(num_nodes) if i not in development_idx]\n",
    "\n",
    "#     train_idx = []\n",
    "#     rnd_state = np.random.RandomState(seed)\n",
    "#     for c in range(data.y.max() + 1):\n",
    "#         class_idx = development_idx[np.where(data.y[development_idx].cpu() == c)[0]]\n",
    "#         train_idx.extend(rnd_state.choice(class_idx, num_per_class, replace=False))\n",
    "\n",
    "#     val_idx = [i for i in development_idx if i not in train_idx]\n",
    "\n",
    "#     def get_mask(idx):\n",
    "#         mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "#         mask[idx] = 1\n",
    "#         return mask\n",
    "\n",
    "#     data.train_mask = get_mask(train_idx)\n",
    "#     data.val_mask = get_mask(val_idx)\n",
    "#     data.test_mask = get_mask(test_idx)\n",
    "\n",
    "#     return data\n",
    "\n",
    "# class PPRDataset(InMemoryDataset):\n",
    "#     \"\"\"\n",
    "#     Dataset preprocessed with GDC using PPR diffusion.\n",
    "#     Note that this implementations is not scalable\n",
    "#     since we directly invert the adjacency matrix.\n",
    "#     \"\"\"\n",
    "#     def __init__(self,noisy_data,\n",
    "#                  name: str = 'Cora',\n",
    "#                  use_lcc: bool = True,\n",
    "#                  alpha: float = 0.1,\n",
    "#                  k: int = 16,\n",
    "#                  eps: float = None):\n",
    "#         self.name = name\n",
    "#         self.use_lcc = use_lcc\n",
    "#         self.alpha = alpha\n",
    "#         self.k = k\n",
    "#         self.eps = eps\n",
    "#         self.noisy_data = noisy_data\n",
    "\n",
    "#         super(PPRDataset, self).__init__(DATA_PATH)\n",
    "#         self.data, self.slices = torch.load(self.processed_paths[0])\n",
    "\n",
    "#     @property\n",
    "#     def raw_file_names(self) -> list:\n",
    "#         return []\n",
    "\n",
    "#     @property\n",
    "#     def processed_file_names(self) -> list:\n",
    "#         return [str(self) + '.pt']\n",
    "\n",
    "#     def download(self):\n",
    "#         pass\n",
    "\n",
    "#     def process(self):\n",
    "#         # base = get_dataset(name=self.name, use_lcc=self.use_lcc)\n",
    "#         # generate adjacency matrix from sparse representation\n",
    "#         adj_matrix = get_adj_matrix(self.noisy_data)\n",
    "#         # obtain exact PPR matrix\n",
    "#         ppr_matrix = get_ppr_matrix(adj_matrix,\n",
    "#                                         alpha=self.alpha)\n",
    "\n",
    "#         if self.k:\n",
    "#             print(f'Selecting top {self.k} edges per node.')\n",
    "#             ppr_matrix = get_top_k_matrix(ppr_matrix, k=self.k)\n",
    "#         elif self.eps:\n",
    "#             print(f'Selecting edges with weight greater than {self.eps}.')\n",
    "#             ppr_matrix = get_clipped_matrix(ppr_matrix, eps=self.eps)\n",
    "#         else:\n",
    "#             raise ValueError\n",
    "\n",
    "#         # create PyG Data object\n",
    "#         edges_i = []\n",
    "#         edges_j = []\n",
    "#         edge_attr = []\n",
    "#         for i, row in enumerate(ppr_matrix):\n",
    "#             for j in np.where(row > 0)[0]:\n",
    "#                 edges_i.append(i)\n",
    "#                 edges_j.append(j)\n",
    "#                 edge_attr.append(ppr_matrix[i, j])\n",
    "#         edge_index = [edges_i, edges_j]\n",
    "\n",
    "#         data = Data(\n",
    "#             x=self.noisy_data.x,\n",
    "#             edge_index=torch.LongTensor(edge_index),\n",
    "#             edge_attr=torch.FloatTensor(edge_attr),\n",
    "#             y=self.noisy_data.y,\n",
    "#             train_mask=torch.zeros(self.noisy_data.train_mask.size()[0], dtype=torch.bool),\n",
    "#             test_mask=torch.zeros(self.noisy_data.test_mask.size()[0], dtype=torch.bool),\n",
    "#             val_mask=torch.zeros(self.noisy_data.val_mask.size()[0], dtype=torch.bool)\n",
    "#         )\n",
    "\n",
    "#         data, slices = self.collate([data])\n",
    "#         torch.save((data, slices), self.processed_paths[0])\n",
    "\n",
    "#     def __str__(self) -> str:\n",
    "#         return f'{self.name}_ppr_alpha={self.alpha}_k={self.k}_eps={self.eps}_lcc={self.use_lcc}'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch_geometric.utils import to_dense_adj, dense_to_sparse\n",
    "# from torch_geometric.data import Data, InMemoryDataset\n",
    "\n",
    "# import os\n",
    "\n",
    "# import numpy as np\n",
    "# from scipy.linalg import expm\n",
    "\n",
    "# import torch\n",
    "# from torch_geometric.data import Data, InMemoryDataset\n",
    "# from torch_geometric.datasets import Planetoid, Amazon, Coauthor\n",
    "\n",
    "# # from seeds import development_seed\n",
    "# DATA_PATH = './data/'\n",
    "\n",
    "# def get_dataset(name: str, use_lcc: bool = True) -> InMemoryDataset:\n",
    "#     path = os.path.join(DATA_PATH, name)\n",
    "#     if name in ['Cora', 'Citeseer', 'Pubmed']:\n",
    "#         dataset = Planetoid(path, name)\n",
    "#     elif name in ['Computers', 'Photo']:\n",
    "#         dataset = Amazon(path, name)\n",
    "#     elif name == 'CoauthorCS':\n",
    "#         dataset = Coauthor(path, 'CS')\n",
    "#     else:\n",
    "#         raise Exception('Unknown dataset.')\n",
    "\n",
    "#     if use_lcc:\n",
    "#         lcc = get_largest_connected_component(dataset)\n",
    "\n",
    "#         x_new = dataset.data.x[lcc]\n",
    "#         y_new = dataset.data.y[lcc]\n",
    "\n",
    "#         row, col = dataset.data.edge_index.numpy()\n",
    "#         edges = [[i, j] for i, j in zip(row, col) if i in lcc and j in lcc]\n",
    "#         edges = remap_edges(edges, get_node_mapper(lcc))\n",
    "        \n",
    "#         data = Data(\n",
    "#             x=x_new,\n",
    "#             edge_index=torch.LongTensor(edges),\n",
    "#             y=y_new,\n",
    "#             train_mask=torch.zeros(y_new.size()[0], dtype=torch.bool),\n",
    "#             test_mask=torch.zeros(y_new.size()[0], dtype=torch.bool),\n",
    "#             val_mask=torch.zeros(y_new.size()[0], dtype=torch.bool)\n",
    "#         )\n",
    "#         dataset.data = data\n",
    "\n",
    "#     return dataset\n",
    "\n",
    "\n",
    "# def get_component(dataset: InMemoryDataset, start: int = 0) -> set:\n",
    "#     visited_nodes = set()\n",
    "#     queued_nodes = set([start])\n",
    "#     row, col = dataset.data.edge_index.numpy()\n",
    "#     while queued_nodes:\n",
    "#         current_node = queued_nodes.pop()\n",
    "#         visited_nodes.update([current_node])\n",
    "#         neighbors = col[np.where(row == current_node)[0]]\n",
    "#         neighbors = [n for n in neighbors if n not in visited_nodes and n not in queued_nodes]\n",
    "#         queued_nodes.update(neighbors)\n",
    "#     return visited_nodes\n",
    "\n",
    "\n",
    "# def get_largest_connected_component(dataset: InMemoryDataset) -> np.ndarray:\n",
    "#     remaining_nodes = set(range(dataset.data.x.shape[0]))\n",
    "#     comps = []\n",
    "#     while remaining_nodes:\n",
    "#         start = min(remaining_nodes)\n",
    "#         comp = get_component(dataset, start)\n",
    "#         comps.append(comp)\n",
    "#         remaining_nodes = remaining_nodes.difference(comp)\n",
    "#     return np.array(list(comps[np.argmax(list(map(len, comps)))]))\n",
    "\n",
    "\n",
    "# def get_node_mapper(lcc: np.ndarray) -> dict:\n",
    "#     mapper = {}\n",
    "#     counter = 0\n",
    "#     for node in lcc:\n",
    "#         mapper[node] = counter\n",
    "#         counter += 1\n",
    "#     return mapper\n",
    "\n",
    "\n",
    "# def remap_edges(edges: list, mapper: dict) -> list:\n",
    "#     row = [e[0] for e in edges]\n",
    "#     col = [e[1] for e in edges]\n",
    "#     row = list(map(lambda x: mapper[x], row))\n",
    "#     col = list(map(lambda x: mapper[x], col))\n",
    "#     return [row, col]\n",
    "\n",
    "\n",
    "# def get_adj_matrix(data) -> np.ndarray:\n",
    "#     num_nodes = data.x.shape[0]\n",
    "#     adj_matrix = np.zeros(shape=(num_nodes, num_nodes))\n",
    "#     for i, j in zip(data.edge_index[0], data.edge_index[1]):\n",
    "#         adj_matrix[i, j] = 1.\n",
    "#     return adj_matrix\n",
    "\n",
    "\n",
    "# def get_ppr_matrix(\n",
    "#         adj_matrix: np.ndarray,\n",
    "#         alpha: float = 0.1) -> np.ndarray:\n",
    "#     num_nodes = adj_matrix.shape[0]\n",
    "#     A_tilde = adj_matrix + np.eye(num_nodes)\n",
    "#     D_tilde = np.diag(1/np.sqrt(A_tilde.sum(axis=1)))\n",
    "#     H = D_tilde @ A_tilde @ D_tilde\n",
    "#     return alpha * np.linalg.inv(np.eye(num_nodes) - (1 - alpha) * H)\n",
    "\n",
    "\n",
    "# def get_heat_matrix(\n",
    "#         adj_matrix: np.ndarray,\n",
    "#         t: float = 5.0) -> np.ndarray:\n",
    "#     num_nodes = adj_matrix.shape[0]\n",
    "#     A_tilde = adj_matrix + np.eye(num_nodes)\n",
    "#     D_tilde = np.diag(1/np.sqrt(A_tilde.sum(axis=1)))\n",
    "#     H = D_tilde @ A_tilde @ D_tilde\n",
    "#     return expm(-t * (np.eye(num_nodes) - H))\n",
    "\n",
    "\n",
    "# def get_top_k_matrix(A: np.ndarray, k: int = 128) -> np.ndarray:\n",
    "#     num_nodes = A.shape[0]\n",
    "#     row_idx = np.arange(num_nodes)\n",
    "#     A[A.argsort(axis=0)[:num_nodes - k], row_idx] = 0.\n",
    "#     norm = A.sum(axis=0)\n",
    "#     norm[norm <= 0] = 1 # avoid dividing by zero\n",
    "#     return A/norm\n",
    "\n",
    "\n",
    "# def get_clipped_matrix(A: np.ndarray, eps: float = 0.01) -> np.ndarray:\n",
    "#     num_nodes = A.shape[0]\n",
    "#     A[A < eps] = 0.\n",
    "#     norm = A.sum(axis=0)\n",
    "#     norm[norm <= 0] = 1 # avoid dividing by zero\n",
    "#     return A/norm\n",
    "\n",
    "\n",
    "# def set_train_val_test_split(\n",
    "#         seed: int,\n",
    "#         data: Data,\n",
    "#         num_development: int = 1500,\n",
    "#         num_per_class: int = 20) -> Data:\n",
    "#     rnd_state = np.random.RandomState(development_seed)\n",
    "#     num_nodes = data.y.shape[0]\n",
    "#     development_idx = rnd_state.choice(num_nodes, num_development, replace=False)\n",
    "#     test_idx = [i for i in np.arange(num_nodes) if i not in development_idx]\n",
    "\n",
    "#     train_idx = []\n",
    "#     rnd_state = np.random.RandomState(seed)\n",
    "#     for c in range(data.y.max() + 1):\n",
    "#         class_idx = development_idx[np.where(data.y[development_idx].cpu() == c)[0]]\n",
    "#         train_idx.extend(rnd_state.choice(class_idx, num_per_class, replace=False))\n",
    "\n",
    "#     val_idx = [i for i in development_idx if i not in train_idx]\n",
    "\n",
    "#     def get_mask(idx):\n",
    "#         mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "#         mask[idx] = 1\n",
    "#         return mask\n",
    "\n",
    "#     data.train_mask = get_mask(train_idx)\n",
    "#     data.val_mask = get_mask(val_idx)\n",
    "#     data.test_mask = get_mask(test_idx)\n",
    "\n",
    "#     return data\n",
    "\n",
    "# class PPRDataset(InMemoryDataset):\n",
    "#     \"\"\"\n",
    "#     Dataset preprocessed with GDC using PPR diffusion.\n",
    "#     Note that this implementations is not scalable\n",
    "#     since we directly invert the adjacency matrix.\n",
    "#     \"\"\"\n",
    "#     def __init__(self,noisy_data,\n",
    "#                  name: str = 'Cora',\n",
    "#                  use_lcc: bool = True,\n",
    "#                  alpha: float = 0.1,\n",
    "#                  k: int = 16,\n",
    "#                  eps: float = None):\n",
    "#         self.name = name\n",
    "#         self.use_lcc = use_lcc\n",
    "#         self.alpha = alpha\n",
    "#         self.k = k\n",
    "#         self.eps = eps\n",
    "#         self.noisy_data = noisy_data\n",
    "\n",
    "#         super(PPRDataset, self).__init__(DATA_PATH)\n",
    "#         self.data, self.slices = torch.load(self.processed_paths[0])\n",
    "\n",
    "#     @property\n",
    "#     def raw_file_names(self) -> list:\n",
    "#         return []\n",
    "\n",
    "#     @property\n",
    "#     def processed_file_names(self) -> list:\n",
    "#         return [str(self) + '.pt']\n",
    "\n",
    "#     def download(self):\n",
    "#         pass\n",
    "\n",
    "#     def process(self):\n",
    "#         # base = get_dataset(name=self.name, use_lcc=self.use_lcc)\n",
    "#         # generate adjacency matrix from sparse representation\n",
    "#         adj_matrix = get_adj_matrix(self.noisy_data)\n",
    "#         # obtain exact PPR matrix\n",
    "#         ppr_matrix = get_ppr_matrix(adj_matrix,\n",
    "#                                         alpha=self.alpha)\n",
    "\n",
    "#         if self.k:\n",
    "#             print(f'Selecting top {self.k} edges per node.')\n",
    "#             ppr_matrix = get_top_k_matrix(ppr_matrix, k=self.k)\n",
    "#         elif self.eps:\n",
    "#             print(f'Selecting edges with weight greater than {self.eps}.')\n",
    "#             ppr_matrix = get_clipped_matrix(ppr_matrix, eps=self.eps)\n",
    "#         else:\n",
    "#             raise ValueError\n",
    "\n",
    "#         # create PyG Data object\n",
    "#         edges_i = []\n",
    "#         edges_j = []\n",
    "#         edge_attr = []\n",
    "#         for i, row in enumerate(ppr_matrix):\n",
    "#             for j in np.where(row > 0)[0]:\n",
    "#                 edges_i.append(i)\n",
    "#                 edges_j.append(j)\n",
    "#                 edge_attr.append(ppr_matrix[i, j])\n",
    "#         edge_index = [edges_i, edges_j]\n",
    "\n",
    "#         data = Data(\n",
    "#             x=self.noisy_data.x,\n",
    "#             edge_index=torch.LongTensor(edge_index),\n",
    "#             edge_attr=torch.FloatTensor(edge_attr),\n",
    "#             y=self.noisy_data.y,\n",
    "#             train_mask=torch.zeros(self.noisy_data.train_mask.size()[0], dtype=torch.bool),\n",
    "#             test_mask=torch.zeros(self.noisy_data.test_mask.size()[0], dtype=torch.bool),\n",
    "#             val_mask=torch.zeros(self.noisy_data.val_mask.size()[0], dtype=torch.bool)\n",
    "#         )\n",
    "\n",
    "#         data, slices = self.collate([data])\n",
    "#         torch.save((data, slices), self.processed_paths[0])\n",
    "\n",
    "#     def __str__(self) -> str:\n",
    "#         return f'{self.name}_ppr_alpha={self.alpha}_k={self.k}_eps={self.eps}_lcc={self.use_lcc}'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unify Contrastive GNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backdoor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GNN+CL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Attach Nodes:10\n",
      "precent of left attach nodes: 1.000\n",
      "=== training gcn model ===\n",
      "Epoch 0, training loss: 26.898313522338867 = 1.953494906425476 + 3 * 7.664082050323486\n",
      "Epoch 0, val loss: 1.939407467842102\n",
      "Epoch 0, val acc: 0.2\n",
      "Epoch 10, training loss: 26.39559555053711 = 1.9167629480361938 + 3 * 7.52122163772583\n",
      "Epoch 10, val loss: 1.9424872398376465\n",
      "Epoch 10, val acc: 0.124\n",
      "Epoch 20, training loss: 25.13083267211914 = 1.8945739269256592 + 3 * 7.112472057342529\n",
      "Epoch 20, val loss: 1.9418469667434692\n",
      "Epoch 20, val acc: 0.124\n",
      "Epoch 30, training loss: 24.502307891845703 = 1.8459302186965942 + 3 * 6.938636302947998\n",
      "Epoch 30, val loss: 1.9187893867492676\n",
      "Epoch 30, val acc: 0.11800000000000001\n",
      "Epoch 40, training loss: 23.716419219970703 = 1.7346704006195068 + 3 * 6.748831272125244\n",
      "Epoch 40, val loss: 1.859037160873413\n",
      "Epoch 40, val acc: 0.152\n",
      "Epoch 50, training loss: 22.844816207885742 = 1.4259687662124634 + 3 * 6.649346351623535\n",
      "Epoch 50, val loss: 1.7625421285629272\n",
      "Epoch 50, val acc: 0.384\n",
      "Epoch 60, training loss: 21.779884338378906 = 1.014369010925293 + 3 * 6.583900451660156\n",
      "Epoch 60, val loss: 1.5839699506759644\n",
      "Epoch 60, val acc: 0.5700000000000001\n",
      "Epoch 70, training loss: 20.65555763244629 = 0.5468775629997253 + 3 * 6.510253429412842\n",
      "Epoch 70, val loss: 1.380153775215149\n",
      "Epoch 70, val acc: 0.648\n",
      "Epoch 80, training loss: 20.216506958007812 = 0.3524886965751648 + 3 * 6.5060648918151855\n",
      "Epoch 80, val loss: 1.2754416465759277\n",
      "Epoch 80, val acc: 0.668\n",
      "Epoch 90, training loss: 19.839595794677734 = 0.30238932371139526 + 3 * 6.414808750152588\n",
      "Epoch 90, val loss: 1.239016056060791\n",
      "Epoch 90, val acc: 0.686\n",
      "Epoch 100, training loss: 19.535343170166016 = 0.2715222239494324 + 3 * 6.350389003753662\n",
      "Epoch 100, val loss: 1.2160959243774414\n",
      "Epoch 100, val acc: 0.6960000000000001\n",
      "Epoch 110, training loss: 19.20449447631836 = 0.19698987901210785 + 3 * 6.264011383056641\n",
      "Epoch 110, val loss: 1.2088865041732788\n",
      "Epoch 110, val acc: 0.7000000000000001\n",
      "Epoch 120, training loss: 19.27934455871582 = 0.1992393583059311 + 3 * 6.279714107513428\n",
      "Epoch 120, val loss: 1.2072298526763916\n",
      "Epoch 120, val acc: 0.72\n",
      "Epoch 130, training loss: 19.02773094177246 = 0.21720543503761292 + 3 * 6.195868492126465\n",
      "Epoch 130, val loss: 1.2233350276947021\n",
      "Epoch 130, val acc: 0.6940000000000001\n",
      "Epoch 140, training loss: 18.793031692504883 = 0.23994126915931702 + 3 * 6.112765312194824\n",
      "Epoch 140, val loss: 1.2222365140914917\n",
      "Epoch 140, val acc: 0.6980000000000001\n",
      "Epoch 150, training loss: 18.708770751953125 = 0.19608932733535767 + 3 * 6.09805154800415\n",
      "Epoch 150, val loss: 1.2369863986968994\n",
      "Epoch 150, val acc: 0.7000000000000001\n",
      "Epoch 160, training loss: 18.84722328186035 = 0.24142266809940338 + 3 * 6.128023147583008\n",
      "Epoch 160, val loss: 1.244312047958374\n",
      "Epoch 160, val acc: 0.6980000000000001\n",
      "Epoch 170, training loss: 18.719192504882812 = 0.19413653016090393 + 3 * 6.10747766494751\n",
      "Epoch 170, val loss: 1.252731204032898\n",
      "Epoch 170, val acc: 0.6920000000000001\n",
      "Epoch 180, training loss: 18.643449783325195 = 0.20484556257724762 + 3 * 6.083962440490723\n",
      "Epoch 180, val loss: 1.2968473434448242\n",
      "Epoch 180, val acc: 0.6900000000000001\n",
      "Epoch 190, training loss: 18.666015625 = 0.21202360093593597 + 3 * 6.090382099151611\n",
      "Epoch 190, val loss: 1.2701256275177002\n",
      "Epoch 190, val acc: 0.6880000000000001\n",
      "Epoch 200, training loss: 18.492523193359375 = 0.18964913487434387 + 3 * 6.040775299072266\n",
      "Epoch 200, val loss: 1.2964918613433838\n",
      "Epoch 200, val acc: 0.686\n",
      "Epoch 210, training loss: 18.322608947753906 = 0.1694050282239914 + 3 * 5.990177154541016\n",
      "Epoch 210, val loss: 1.2741212844848633\n",
      "Epoch 210, val acc: 0.684\n",
      "Epoch 220, training loss: 18.428327560424805 = 0.18920384347438812 + 3 * 6.02020788192749\n",
      "Epoch 220, val loss: 1.311055064201355\n",
      "Epoch 220, val acc: 0.676\n",
      "Epoch 230, training loss: 18.428241729736328 = 0.15199308097362518 + 3 * 6.030563831329346\n",
      "Epoch 230, val loss: 1.32579505443573\n",
      "Epoch 230, val acc: 0.674\n",
      "Epoch 240, training loss: 18.316600799560547 = 0.1711755096912384 + 3 * 5.99318790435791\n",
      "Epoch 240, val loss: 1.3544327020645142\n",
      "Epoch 240, val acc: 0.672\n",
      "Epoch 250, training loss: 18.28782081604004 = 0.16067922115325928 + 3 * 5.981463432312012\n",
      "Epoch 250, val loss: 1.3477904796600342\n",
      "Epoch 250, val acc: 0.672\n",
      "Epoch 260, training loss: 18.07375717163086 = 0.16104130446910858 + 3 * 5.913799285888672\n",
      "Epoch 260, val loss: 1.3832687139511108\n",
      "Epoch 260, val acc: 0.686\n",
      "Epoch 270, training loss: 17.946699142456055 = 0.1565677374601364 + 3 * 5.871572017669678\n",
      "Epoch 270, val loss: 1.3578416109085083\n",
      "Epoch 270, val acc: 0.67\n",
      "Epoch 280, training loss: 18.19556999206543 = 0.15810604393482208 + 3 * 5.955830097198486\n",
      "Epoch 280, val loss: 1.392030954360962\n",
      "Epoch 280, val acc: 0.674\n",
      "Epoch 290, training loss: 17.934751510620117 = 0.16785159707069397 + 3 * 5.866537570953369\n",
      "Epoch 290, val loss: 1.3887195587158203\n",
      "Epoch 290, val acc: 0.678\n",
      "Epoch 300, training loss: 18.008407592773438 = 0.1563882827758789 + 3 * 5.89748477935791\n",
      "Epoch 300, val loss: 1.431199073791504\n",
      "Epoch 300, val acc: 0.672\n",
      "Epoch 310, training loss: 18.080175399780273 = 0.14909839630126953 + 3 * 5.930917739868164\n",
      "Epoch 310, val loss: 1.4088506698608398\n",
      "Epoch 310, val acc: 0.672\n",
      "Epoch 320, training loss: 17.998855590820312 = 0.14546076953411102 + 3 * 5.896108627319336\n",
      "Epoch 320, val loss: 1.4620503187179565\n",
      "Epoch 320, val acc: 0.676\n",
      "Epoch 330, training loss: 17.897417068481445 = 0.16445189714431763 + 3 * 5.853540420532227\n",
      "Epoch 330, val loss: 1.4487597942352295\n",
      "Epoch 330, val acc: 0.67\n",
      "Epoch 340, training loss: 17.948986053466797 = 0.16244091093540192 + 3 * 5.8747100830078125\n",
      "Epoch 340, val loss: 1.4584903717041016\n",
      "Epoch 340, val acc: 0.666\n",
      "Epoch 350, training loss: 17.781679153442383 = 0.16993902623653412 + 3 * 5.812421798706055\n",
      "Epoch 350, val loss: 1.4814027547836304\n",
      "Epoch 350, val acc: 0.662\n",
      "Epoch 360, training loss: 17.763259887695312 = 0.16799594461917877 + 3 * 5.819740295410156\n",
      "Epoch 360, val loss: 1.4797979593276978\n",
      "Epoch 360, val acc: 0.668\n",
      "Epoch 370, training loss: 17.939111709594727 = 0.1397635042667389 + 3 * 5.879538536071777\n",
      "Epoch 370, val loss: 1.5019744634628296\n",
      "Epoch 370, val acc: 0.648\n",
      "Epoch 380, training loss: 17.867538452148438 = 0.1376546025276184 + 3 * 5.856426239013672\n",
      "Epoch 380, val loss: 1.5251268148422241\n",
      "Epoch 380, val acc: 0.662\n",
      "Epoch 390, training loss: 17.555130004882812 = 0.11726640909910202 + 3 * 5.7746758460998535\n",
      "Epoch 390, val loss: 1.5289490222930908\n",
      "Epoch 390, val acc: 0.656\n",
      "Epoch 400, training loss: 17.68802261352539 = 0.15745943784713745 + 3 * 5.803109645843506\n",
      "Epoch 400, val loss: 1.5603179931640625\n",
      "Epoch 400, val acc: 0.652\n",
      "Epoch 410, training loss: 17.68016242980957 = 0.15548823773860931 + 3 * 5.800217151641846\n",
      "Epoch 410, val loss: 1.5844168663024902\n",
      "Epoch 410, val acc: 0.65\n",
      "Epoch 420, training loss: 17.443403244018555 = 0.12646740674972534 + 3 * 5.732296943664551\n",
      "Epoch 420, val loss: 1.5620496273040771\n",
      "Epoch 420, val acc: 0.648\n",
      "Epoch 430, training loss: 17.684314727783203 = 0.13215351104736328 + 3 * 5.803107261657715\n",
      "Epoch 430, val loss: 1.623293161392212\n",
      "Epoch 430, val acc: 0.636\n",
      "Epoch 440, training loss: 17.466066360473633 = 0.13458389043807983 + 3 * 5.733924865722656\n",
      "Epoch 440, val loss: 1.6316797733306885\n",
      "Epoch 440, val acc: 0.65\n",
      "Epoch 450, training loss: 17.660892486572266 = 0.13760556280612946 + 3 * 5.786537170410156\n",
      "Epoch 450, val loss: 1.6919260025024414\n",
      "Epoch 450, val acc: 0.638\n",
      "Epoch 460, training loss: 17.58489227294922 = 0.12831386923789978 + 3 * 5.784012317657471\n",
      "Epoch 460, val loss: 1.7314677238464355\n",
      "Epoch 460, val acc: 0.644\n",
      "Epoch 470, training loss: 17.578479766845703 = 0.11577728390693665 + 3 * 5.77665901184082\n",
      "Epoch 470, val loss: 1.688966155052185\n",
      "Epoch 470, val acc: 0.64\n",
      "Epoch 480, training loss: 17.600290298461914 = 0.12995250523090363 + 3 * 5.776883125305176\n",
      "Epoch 480, val loss: 1.7389540672302246\n",
      "Epoch 480, val acc: 0.646\n",
      "Epoch 490, training loss: 17.364328384399414 = 0.12512782216072083 + 3 * 5.706213474273682\n",
      "Epoch 490, val loss: 1.7251722812652588\n",
      "Epoch 490, val acc: 0.64\n",
      "Epoch 500, training loss: 17.46484375 = 0.15825515985488892 + 3 * 5.721306800842285\n",
      "Epoch 500, val loss: 1.6768125295639038\n",
      "Epoch 500, val acc: 0.636\n",
      "Epoch 510, training loss: 17.4090576171875 = 0.11822307854890823 + 3 * 5.719557285308838\n",
      "Epoch 510, val loss: 1.7451599836349487\n",
      "Epoch 510, val acc: 0.632\n",
      "Epoch 520, training loss: 17.425418853759766 = 0.13841430842876434 + 3 * 5.713850975036621\n",
      "Epoch 520, val loss: 1.750269889831543\n",
      "Epoch 520, val acc: 0.618\n",
      "Epoch 530, training loss: 17.387399673461914 = 0.1483437567949295 + 3 * 5.697617530822754\n",
      "Epoch 530, val loss: 1.7761846780776978\n",
      "Epoch 530, val acc: 0.632\n",
      "Epoch 540, training loss: 17.533702850341797 = 0.11429049074649811 + 3 * 5.762495040893555\n",
      "Epoch 540, val loss: 1.8103755712509155\n",
      "Epoch 540, val acc: 0.628\n",
      "Epoch 550, training loss: 17.55450439453125 = 0.12929774820804596 + 3 * 5.764339447021484\n",
      "Epoch 550, val loss: 1.8150261640548706\n",
      "Epoch 550, val acc: 0.638\n",
      "Epoch 560, training loss: 17.383508682250977 = 0.15133334696292877 + 3 * 5.712310791015625\n",
      "Epoch 560, val loss: 1.8014963865280151\n",
      "Epoch 560, val acc: 0.62\n",
      "Epoch 570, training loss: 17.405820846557617 = 0.16133491694927216 + 3 * 5.707735538482666\n",
      "Epoch 570, val loss: 1.774816632270813\n",
      "Epoch 570, val acc: 0.628\n",
      "Epoch 580, training loss: 17.40240478515625 = 0.11230792105197906 + 3 * 5.727566242218018\n",
      "Epoch 580, val loss: 1.7835044860839844\n",
      "Epoch 580, val acc: 0.626\n",
      "Epoch 590, training loss: 17.346837997436523 = 0.14333216845989227 + 3 * 5.696384906768799\n",
      "Epoch 590, val loss: 1.82884681224823\n",
      "Epoch 590, val acc: 0.628\n",
      "Epoch 600, training loss: 17.252960205078125 = 0.11150427162647247 + 3 * 5.670459270477295\n",
      "Epoch 600, val loss: 1.8203749656677246\n",
      "Epoch 600, val acc: 0.632\n",
      "Epoch 610, training loss: 17.244373321533203 = 0.1143219918012619 + 3 * 5.665194988250732\n",
      "Epoch 610, val loss: 1.828742504119873\n",
      "Epoch 610, val acc: 0.618\n",
      "Epoch 620, training loss: 17.15078353881836 = 0.10846422612667084 + 3 * 5.637716770172119\n",
      "Epoch 620, val loss: 1.8184117078781128\n",
      "Epoch 620, val acc: 0.628\n",
      "Epoch 630, training loss: 17.176572799682617 = 0.1248362585902214 + 3 * 5.649392127990723\n",
      "Epoch 630, val loss: 1.8229925632476807\n",
      "Epoch 630, val acc: 0.618\n",
      "Epoch 640, training loss: 17.26688575744629 = 0.0993727445602417 + 3 * 5.677374839782715\n",
      "Epoch 640, val loss: 1.8532812595367432\n",
      "Epoch 640, val acc: 0.626\n",
      "Epoch 650, training loss: 17.06595802307129 = 0.13193820416927338 + 3 * 5.6090087890625\n",
      "Epoch 650, val loss: 1.806359052658081\n",
      "Epoch 650, val acc: 0.624\n",
      "Epoch 660, training loss: 17.43436622619629 = 0.10377349704504013 + 3 * 5.7336859703063965\n",
      "Epoch 660, val loss: 1.7998294830322266\n",
      "Epoch 660, val acc: 0.624\n",
      "Epoch 670, training loss: 17.20482063293457 = 0.10301607102155685 + 3 * 5.665907859802246\n",
      "Epoch 670, val loss: 1.8293671607971191\n",
      "Epoch 670, val acc: 0.618\n",
      "Epoch 680, training loss: 17.281129837036133 = 0.12916432321071625 + 3 * 5.675160884857178\n",
      "Epoch 680, val loss: 1.8137284517288208\n",
      "Epoch 680, val acc: 0.626\n",
      "Epoch 690, training loss: 17.382123947143555 = 0.13262751698493958 + 3 * 5.718333721160889\n",
      "Epoch 690, val loss: 1.8715672492980957\n",
      "Epoch 690, val acc: 0.626\n",
      "=== picking the best model according to the performance on validation ===\n",
      "target class rate on Vs: 0.8000\n",
      "accuracy on clean test nodes: 0.7280\n",
      "Overall ASR: 0.6420\n",
      "Flip ASR: 0.6125/449 nodes\n",
      "The final ASR:0.64200, 0.00000, Accuracy:0.72800, 0.00000\n",
      "#Attach Nodes:10\n",
      "precent of left attach nodes: 1.000\n",
      "=== training gcn model ===\n",
      "Epoch 0, training loss: 26.707290649414062 = 1.942223310470581 + 3 * 7.607340335845947\n",
      "Epoch 0, val loss: 1.9531044960021973\n",
      "Epoch 0, val acc: 0.128\n",
      "Epoch 10, training loss: 26.598894119262695 = 1.9156502485275269 + 3 * 7.587462425231934\n",
      "Epoch 10, val loss: 1.9429587125778198\n",
      "Epoch 10, val acc: 0.128\n",
      "Epoch 20, training loss: 25.483360290527344 = 1.8988478183746338 + 3 * 7.229138374328613\n",
      "Epoch 20, val loss: 1.9502860307693481\n",
      "Epoch 20, val acc: 0.128\n",
      "Epoch 30, training loss: 24.757585525512695 = 1.8592169284820557 + 3 * 7.014804840087891\n",
      "Epoch 30, val loss: 1.9291620254516602\n",
      "Epoch 30, val acc: 0.122\n",
      "Epoch 40, training loss: 23.888017654418945 = 1.732917070388794 + 3 * 6.8012166023254395\n",
      "Epoch 40, val loss: 1.8892261981964111\n",
      "Epoch 40, val acc: 0.154\n",
      "Epoch 50, training loss: 23.1302490234375 = 1.569084644317627 + 3 * 6.656797409057617\n",
      "Epoch 50, val loss: 1.838515043258667\n",
      "Epoch 50, val acc: 0.278\n",
      "Epoch 60, training loss: 22.117549896240234 = 1.2058985233306885 + 3 * 6.5744476318359375\n",
      "Epoch 60, val loss: 1.7206432819366455\n",
      "Epoch 60, val acc: 0.42\n",
      "Epoch 70, training loss: 21.147550582885742 = 0.7129162549972534 + 3 * 6.566920757293701\n",
      "Epoch 70, val loss: 1.5250495672225952\n",
      "Epoch 70, val acc: 0.562\n",
      "Epoch 80, training loss: 20.253568649291992 = 0.4236392676830292 + 3 * 6.470208644866943\n",
      "Epoch 80, val loss: 1.4274392127990723\n",
      "Epoch 80, val acc: 0.61\n",
      "Epoch 90, training loss: 19.747461318969727 = 0.3329418897628784 + 3 * 6.367409706115723\n",
      "Epoch 90, val loss: 1.4387602806091309\n",
      "Epoch 90, val acc: 0.628\n",
      "Epoch 100, training loss: 19.641481399536133 = 0.265681654214859 + 3 * 6.3672380447387695\n",
      "Epoch 100, val loss: 1.5136374235153198\n",
      "Epoch 100, val acc: 0.622\n",
      "Epoch 110, training loss: 19.414627075195312 = 0.2551179826259613 + 3 * 6.305932521820068\n",
      "Epoch 110, val loss: 1.5638004541397095\n",
      "Epoch 110, val acc: 0.634\n",
      "Epoch 120, training loss: 19.36180305480957 = 0.22555914521217346 + 3 * 6.2938079833984375\n",
      "Epoch 120, val loss: 1.635288953781128\n",
      "Epoch 120, val acc: 0.64\n",
      "Epoch 130, training loss: 19.01707649230957 = 0.214528888463974 + 3 * 6.186541557312012\n",
      "Epoch 130, val loss: 1.6036746501922607\n",
      "Epoch 130, val acc: 0.646\n",
      "Epoch 140, training loss: 18.864887237548828 = 0.2131962925195694 + 3 * 6.14434814453125\n",
      "Epoch 140, val loss: 1.6596992015838623\n",
      "Epoch 140, val acc: 0.644\n",
      "Epoch 150, training loss: 18.896486282348633 = 0.19439852237701416 + 3 * 6.144931793212891\n",
      "Epoch 150, val loss: 1.6948109865188599\n",
      "Epoch 150, val acc: 0.636\n",
      "Epoch 160, training loss: 18.539749145507812 = 0.20904435217380524 + 3 * 6.049503803253174\n",
      "Epoch 160, val loss: 1.7078371047973633\n",
      "Epoch 160, val acc: 0.636\n",
      "Epoch 170, training loss: 18.582185745239258 = 0.18334393203258514 + 3 * 6.077827453613281\n",
      "Epoch 170, val loss: 1.7727946043014526\n",
      "Epoch 170, val acc: 0.626\n",
      "Epoch 180, training loss: 18.433429718017578 = 0.17978987097740173 + 3 * 6.02866792678833\n",
      "Epoch 180, val loss: 1.8224302530288696\n",
      "Epoch 180, val acc: 0.626\n",
      "Epoch 190, training loss: 18.504390716552734 = 0.17708377540111542 + 3 * 6.03782844543457\n",
      "Epoch 190, val loss: 1.862073540687561\n",
      "Epoch 190, val acc: 0.618\n",
      "Epoch 200, training loss: 18.2363224029541 = 0.1404738873243332 + 3 * 5.974063873291016\n",
      "Epoch 200, val loss: 1.8613851070404053\n",
      "Epoch 200, val acc: 0.626\n",
      "Epoch 210, training loss: 18.385433197021484 = 0.17887847125530243 + 3 * 6.009927749633789\n",
      "Epoch 210, val loss: 1.9243614673614502\n",
      "Epoch 210, val acc: 0.626\n",
      "Epoch 220, training loss: 18.465234756469727 = 0.17686180770397186 + 3 * 6.043553829193115\n",
      "Epoch 220, val loss: 1.8967108726501465\n",
      "Epoch 220, val acc: 0.634\n",
      "Epoch 230, training loss: 18.14212417602539 = 0.1597207933664322 + 3 * 5.9433088302612305\n",
      "Epoch 230, val loss: 2.0307164192199707\n",
      "Epoch 230, val acc: 0.624\n",
      "Epoch 240, training loss: 18.292095184326172 = 0.16007757186889648 + 3 * 5.991856575012207\n",
      "Epoch 240, val loss: 2.081042766571045\n",
      "Epoch 240, val acc: 0.632\n",
      "Epoch 250, training loss: 17.91958999633789 = 0.1387971043586731 + 3 * 5.878471851348877\n",
      "Epoch 250, val loss: 1.9999871253967285\n",
      "Epoch 250, val acc: 0.622\n",
      "Epoch 260, training loss: 18.153072357177734 = 0.17857500910758972 + 3 * 5.9349260330200195\n",
      "Epoch 260, val loss: 2.0480198860168457\n",
      "Epoch 260, val acc: 0.622\n",
      "Epoch 270, training loss: 17.942548751831055 = 0.1574239879846573 + 3 * 5.875429630279541\n",
      "Epoch 270, val loss: 2.1112277507781982\n",
      "Epoch 270, val acc: 0.628\n",
      "Epoch 280, training loss: 18.01333999633789 = 0.16100488603115082 + 3 * 5.891602516174316\n",
      "Epoch 280, val loss: 2.0979974269866943\n",
      "Epoch 280, val acc: 0.636\n",
      "Epoch 290, training loss: 18.026416778564453 = 0.1616397351026535 + 3 * 5.907330513000488\n",
      "Epoch 290, val loss: 2.075878143310547\n",
      "Epoch 290, val acc: 0.622\n",
      "Epoch 300, training loss: 17.842334747314453 = 0.1457066833972931 + 3 * 5.844374179840088\n",
      "Epoch 300, val loss: 2.1771762371063232\n",
      "Epoch 300, val acc: 0.622\n",
      "Epoch 310, training loss: 17.846881866455078 = 0.13824991881847382 + 3 * 5.852212905883789\n",
      "Epoch 310, val loss: 2.232325792312622\n",
      "Epoch 310, val acc: 0.624\n",
      "Epoch 320, training loss: 18.042137145996094 = 0.18000029027462006 + 3 * 5.899389743804932\n",
      "Epoch 320, val loss: 2.240832805633545\n",
      "Epoch 320, val acc: 0.628\n",
      "Epoch 330, training loss: 17.832849502563477 = 0.15878738462924957 + 3 * 5.8377509117126465\n",
      "Epoch 330, val loss: 2.2838447093963623\n",
      "Epoch 330, val acc: 0.628\n",
      "Epoch 340, training loss: 17.817468643188477 = 0.14713342487812042 + 3 * 5.829240798950195\n",
      "Epoch 340, val loss: 2.3200793266296387\n",
      "Epoch 340, val acc: 0.616\n",
      "Epoch 350, training loss: 18.013328552246094 = 0.17668253183364868 + 3 * 5.884632587432861\n",
      "Epoch 350, val loss: 2.3918497562408447\n",
      "Epoch 350, val acc: 0.612\n",
      "Epoch 360, training loss: 17.93450927734375 = 0.14002105593681335 + 3 * 5.877471446990967\n",
      "Epoch 360, val loss: 2.3746492862701416\n",
      "Epoch 360, val acc: 0.618\n",
      "Epoch 370, training loss: 17.670555114746094 = 0.15328843891620636 + 3 * 5.781024932861328\n",
      "Epoch 370, val loss: 2.473545789718628\n",
      "Epoch 370, val acc: 0.61\n",
      "Epoch 380, training loss: 17.685129165649414 = 0.13285711407661438 + 3 * 5.808762073516846\n",
      "Epoch 380, val loss: 2.4681148529052734\n",
      "Epoch 380, val acc: 0.618\n",
      "Epoch 390, training loss: 17.63896942138672 = 0.12529566884040833 + 3 * 5.7901530265808105\n",
      "Epoch 390, val loss: 2.5330519676208496\n",
      "Epoch 390, val acc: 0.608\n",
      "Epoch 400, training loss: 17.55637550354004 = 0.1691959947347641 + 3 * 5.757997512817383\n",
      "Epoch 400, val loss: 2.5214149951934814\n",
      "Epoch 400, val acc: 0.622\n",
      "Epoch 410, training loss: 17.687475204467773 = 0.14867070317268372 + 3 * 5.79871129989624\n",
      "Epoch 410, val loss: 2.5390875339508057\n",
      "Epoch 410, val acc: 0.624\n",
      "Epoch 420, training loss: 17.793127059936523 = 0.1661347895860672 + 3 * 5.8225626945495605\n",
      "Epoch 420, val loss: 2.49743390083313\n",
      "Epoch 420, val acc: 0.618\n",
      "Epoch 430, training loss: 17.563261032104492 = 0.15116724371910095 + 3 * 5.7579827308654785\n",
      "Epoch 430, val loss: 2.4824841022491455\n",
      "Epoch 430, val acc: 0.612\n",
      "Epoch 440, training loss: 17.347618103027344 = 0.14272990822792053 + 3 * 5.6827712059021\n",
      "Epoch 440, val loss: 2.530723810195923\n",
      "Epoch 440, val acc: 0.602\n",
      "Epoch 450, training loss: 17.779314041137695 = 0.13597871363162994 + 3 * 5.8245110511779785\n",
      "Epoch 450, val loss: 2.570725679397583\n",
      "Epoch 450, val acc: 0.612\n",
      "Epoch 460, training loss: 17.727664947509766 = 0.16019965708255768 + 3 * 5.8060302734375\n",
      "Epoch 460, val loss: 2.5466010570526123\n",
      "Epoch 460, val acc: 0.618\n",
      "Epoch 470, training loss: 17.72393798828125 = 0.18710291385650635 + 3 * 5.804866790771484\n",
      "Epoch 470, val loss: 2.5516409873962402\n",
      "Epoch 470, val acc: 0.606\n",
      "Epoch 480, training loss: 17.45671272277832 = 0.13040201365947723 + 3 * 5.721713066101074\n",
      "Epoch 480, val loss: 2.5602104663848877\n",
      "Epoch 480, val acc: 0.614\n",
      "Epoch 490, training loss: 17.32737159729004 = 0.1563122421503067 + 3 * 5.683201789855957\n",
      "Epoch 490, val loss: 2.648348093032837\n",
      "Epoch 490, val acc: 0.608\n",
      "Epoch 500, training loss: 17.549440383911133 = 0.15379254519939423 + 3 * 5.756625652313232\n",
      "Epoch 500, val loss: 2.6382381916046143\n",
      "Epoch 500, val acc: 0.606\n",
      "Epoch 510, training loss: 17.402179718017578 = 0.15081602334976196 + 3 * 5.7006916999816895\n",
      "Epoch 510, val loss: 2.6171841621398926\n",
      "Epoch 510, val acc: 0.604\n",
      "Epoch 520, training loss: 17.617395401000977 = 0.17401598393917084 + 3 * 5.769816875457764\n",
      "Epoch 520, val loss: 2.6992413997650146\n",
      "Epoch 520, val acc: 0.602\n",
      "Epoch 530, training loss: 17.3018798828125 = 0.11547357589006424 + 3 * 5.679224014282227\n",
      "Epoch 530, val loss: 2.6747703552246094\n",
      "Epoch 530, val acc: 0.612\n",
      "Epoch 540, training loss: 17.53795623779297 = 0.13996708393096924 + 3 * 5.755605697631836\n",
      "Epoch 540, val loss: 2.6723525524139404\n",
      "Epoch 540, val acc: 0.598\n",
      "Epoch 550, training loss: 17.386777877807617 = 0.1324828714132309 + 3 * 5.707861423492432\n",
      "Epoch 550, val loss: 2.6980578899383545\n",
      "Epoch 550, val acc: 0.62\n",
      "Epoch 560, training loss: 17.32177734375 = 0.1408015638589859 + 3 * 5.680924892425537\n",
      "Epoch 560, val loss: 2.6736371517181396\n",
      "Epoch 560, val acc: 0.602\n",
      "Epoch 570, training loss: 17.344711303710938 = 0.13767904043197632 + 3 * 5.683787822723389\n",
      "Epoch 570, val loss: 2.638496160507202\n",
      "Epoch 570, val acc: 0.598\n",
      "Epoch 580, training loss: 17.522686004638672 = 0.1683415025472641 + 3 * 5.7401862144470215\n",
      "Epoch 580, val loss: 2.6437859535217285\n",
      "Epoch 580, val acc: 0.6\n",
      "Epoch 590, training loss: 17.382081985473633 = 0.13242179155349731 + 3 * 5.698720932006836\n",
      "Epoch 590, val loss: 2.7028496265411377\n",
      "Epoch 590, val acc: 0.61\n",
      "Epoch 600, training loss: 17.285873413085938 = 0.1356072872877121 + 3 * 5.670536994934082\n",
      "Epoch 600, val loss: 2.7690353393554688\n",
      "Epoch 600, val acc: 0.598\n",
      "Epoch 610, training loss: 17.2181453704834 = 0.11963874101638794 + 3 * 5.655719757080078\n",
      "Epoch 610, val loss: 2.77402925491333\n",
      "Epoch 610, val acc: 0.596\n",
      "Epoch 620, training loss: 17.36011505126953 = 0.1260269284248352 + 3 * 5.698755264282227\n",
      "Epoch 620, val loss: 2.723661422729492\n",
      "Epoch 620, val acc: 0.602\n",
      "Epoch 630, training loss: 17.23741340637207 = 0.12049971520900726 + 3 * 5.647332191467285\n",
      "Epoch 630, val loss: 2.744260787963867\n",
      "Epoch 630, val acc: 0.594\n",
      "Epoch 640, training loss: 17.217491149902344 = 0.11376508325338364 + 3 * 5.6635870933532715\n",
      "Epoch 640, val loss: 2.7033324241638184\n",
      "Epoch 640, val acc: 0.596\n",
      "Epoch 650, training loss: 17.115358352661133 = 0.12010892480611801 + 3 * 5.62342643737793\n",
      "Epoch 650, val loss: 2.721083641052246\n",
      "Epoch 650, val acc: 0.588\n",
      "Epoch 660, training loss: 17.18315315246582 = 0.11446508765220642 + 3 * 5.6457037925720215\n",
      "Epoch 660, val loss: 2.6859238147735596\n",
      "Epoch 660, val acc: 0.59\n",
      "Epoch 670, training loss: 17.097673416137695 = 0.1275569051504135 + 3 * 5.623109340667725\n",
      "Epoch 670, val loss: 2.7629032135009766\n",
      "Epoch 670, val acc: 0.596\n",
      "Epoch 680, training loss: 17.336933135986328 = 0.12587641179561615 + 3 * 5.691473484039307\n",
      "Epoch 680, val loss: 2.810305118560791\n",
      "Epoch 680, val acc: 0.592\n",
      "Epoch 690, training loss: 17.219831466674805 = 0.14886866509914398 + 3 * 5.6405229568481445\n",
      "Epoch 690, val loss: 2.8225951194763184\n",
      "Epoch 690, val acc: 0.588\n",
      "=== picking the best model according to the performance on validation ===\n",
      "target class rate on Vs: 0.0000\n",
      "accuracy on clean test nodes: 0.7420\n",
      "Overall ASR: 0.0000\n",
      "Flip ASR: 0.0000/449 nodes\n",
      "The final ASR:0.00000, 0.00000, Accuracy:0.74200, 0.00000\n",
      "#Attach Nodes:10\n",
      "precent of left attach nodes: 1.000\n",
      "=== training gcn model ===\n",
      "Epoch 0, training loss: 26.96900177001953 = 1.9515069723129272 + 3 * 7.688740253448486\n",
      "Epoch 0, val loss: 1.9603643417358398\n",
      "Epoch 0, val acc: 0.07200000000000001\n",
      "Epoch 10, training loss: 26.13043785095215 = 1.9212546348571777 + 3 * 7.4290337562561035\n",
      "Epoch 10, val loss: 1.9492270946502686\n",
      "Epoch 10, val acc: 0.128\n",
      "Epoch 20, training loss: 25.24448013305664 = 1.89812171459198 + 3 * 7.147496700286865\n",
      "Epoch 20, val loss: 1.9461244344711304\n",
      "Epoch 20, val acc: 0.128\n",
      "Epoch 30, training loss: 24.511648178100586 = 1.8394675254821777 + 3 * 6.9426045417785645\n",
      "Epoch 30, val loss: 1.9259847402572632\n",
      "Epoch 30, val acc: 0.128\n",
      "Epoch 40, training loss: 23.77404022216797 = 1.7761415243148804 + 3 * 6.75035285949707\n",
      "Epoch 40, val loss: 1.8781927824020386\n",
      "Epoch 40, val acc: 0.138\n",
      "Epoch 50, training loss: 22.8518009185791 = 1.5237170457839966 + 3 * 6.6033711433410645\n",
      "Epoch 50, val loss: 1.7844734191894531\n",
      "Epoch 50, val acc: 0.28200000000000003\n",
      "Epoch 60, training loss: 22.080535888671875 = 1.1764857769012451 + 3 * 6.578658580780029\n",
      "Epoch 60, val loss: 1.620151162147522\n",
      "Epoch 60, val acc: 0.47600000000000003\n",
      "Epoch 70, training loss: 20.814863204956055 = 0.6499840021133423 + 3 * 6.496457099914551\n",
      "Epoch 70, val loss: 1.376746416091919\n",
      "Epoch 70, val acc: 0.616\n",
      "Epoch 80, training loss: 20.115068435668945 = 0.4184706211090088 + 3 * 6.4297261238098145\n",
      "Epoch 80, val loss: 1.2816598415374756\n",
      "Epoch 80, val acc: 0.648\n",
      "Epoch 90, training loss: 19.85274887084961 = 0.3262931704521179 + 3 * 6.390568256378174\n",
      "Epoch 90, val loss: 1.255326747894287\n",
      "Epoch 90, val acc: 0.644\n",
      "Epoch 100, training loss: 19.50803565979004 = 0.27547791600227356 + 3 * 6.324977397918701\n",
      "Epoch 100, val loss: 1.2951607704162598\n",
      "Epoch 100, val acc: 0.626\n",
      "Epoch 110, training loss: 19.33051109313965 = 0.26818498969078064 + 3 * 6.273867130279541\n",
      "Epoch 110, val loss: 1.3099603652954102\n",
      "Epoch 110, val acc: 0.644\n",
      "Epoch 120, training loss: 19.214702606201172 = 0.24176789820194244 + 3 * 6.243977069854736\n",
      "Epoch 120, val loss: 1.3101128339767456\n",
      "Epoch 120, val acc: 0.636\n",
      "Epoch 130, training loss: 19.23685646057129 = 0.19209998846054077 + 3 * 6.265836238861084\n",
      "Epoch 130, val loss: 1.3117185831069946\n",
      "Epoch 130, val acc: 0.65\n",
      "Epoch 140, training loss: 18.995859146118164 = 0.2364359200000763 + 3 * 6.184730052947998\n",
      "Epoch 140, val loss: 1.2864667177200317\n",
      "Epoch 140, val acc: 0.654\n",
      "Epoch 150, training loss: 18.77701759338379 = 0.20643775165081024 + 3 * 6.1304731369018555\n",
      "Epoch 150, val loss: 1.2904330492019653\n",
      "Epoch 150, val acc: 0.668\n",
      "Epoch 160, training loss: 18.806812286376953 = 0.21128062903881073 + 3 * 6.139839172363281\n",
      "Epoch 160, val loss: 1.2953615188598633\n",
      "Epoch 160, val acc: 0.67\n",
      "Epoch 170, training loss: 18.73684310913086 = 0.16847608983516693 + 3 * 6.1320037841796875\n",
      "Epoch 170, val loss: 1.2845829725265503\n",
      "Epoch 170, val acc: 0.686\n",
      "Epoch 180, training loss: 18.673593521118164 = 0.21882997453212738 + 3 * 6.09122371673584\n",
      "Epoch 180, val loss: 1.3074849843978882\n",
      "Epoch 180, val acc: 0.684\n",
      "Epoch 190, training loss: 18.441661834716797 = 0.17694854736328125 + 3 * 6.022419452667236\n",
      "Epoch 190, val loss: 1.3433665037155151\n",
      "Epoch 190, val acc: 0.674\n",
      "Epoch 200, training loss: 18.415090560913086 = 0.17826750874519348 + 3 * 6.022326469421387\n",
      "Epoch 200, val loss: 1.3475024700164795\n",
      "Epoch 200, val acc: 0.682\n",
      "Epoch 210, training loss: 18.43962287902832 = 0.170378640294075 + 3 * 6.025679588317871\n",
      "Epoch 210, val loss: 1.3564575910568237\n",
      "Epoch 210, val acc: 0.664\n",
      "Epoch 220, training loss: 18.361417770385742 = 0.15209344029426575 + 3 * 6.014683246612549\n",
      "Epoch 220, val loss: 1.3775376081466675\n",
      "Epoch 220, val acc: 0.666\n",
      "Epoch 230, training loss: 18.45488739013672 = 0.17616915702819824 + 3 * 6.014180660247803\n",
      "Epoch 230, val loss: 1.3873193264007568\n",
      "Epoch 230, val acc: 0.682\n",
      "Epoch 240, training loss: 18.27587127685547 = 0.14634767174720764 + 3 * 5.99330997467041\n",
      "Epoch 240, val loss: 1.4313057661056519\n",
      "Epoch 240, val acc: 0.662\n",
      "Epoch 250, training loss: 18.00542449951172 = 0.2084573656320572 + 3 * 5.881928443908691\n",
      "Epoch 250, val loss: 1.4188777208328247\n",
      "Epoch 250, val acc: 0.662\n",
      "Epoch 260, training loss: 18.11043930053711 = 0.17910236120224 + 3 * 5.918753623962402\n",
      "Epoch 260, val loss: 1.4249768257141113\n",
      "Epoch 260, val acc: 0.672\n",
      "Epoch 270, training loss: 18.152555465698242 = 0.1616743952035904 + 3 * 5.934257507324219\n",
      "Epoch 270, val loss: 1.4233959913253784\n",
      "Epoch 270, val acc: 0.666\n",
      "Epoch 280, training loss: 17.841230392456055 = 0.1506129950284958 + 3 * 5.845034122467041\n",
      "Epoch 280, val loss: 1.446536898612976\n",
      "Epoch 280, val acc: 0.668\n",
      "Epoch 290, training loss: 17.842304229736328 = 0.18122142553329468 + 3 * 5.837064743041992\n",
      "Epoch 290, val loss: 1.463370442390442\n",
      "Epoch 290, val acc: 0.67\n",
      "Epoch 300, training loss: 17.884689331054688 = 0.16577325761318207 + 3 * 5.856503486633301\n",
      "Epoch 300, val loss: 1.5017890930175781\n",
      "Epoch 300, val acc: 0.664\n",
      "Epoch 310, training loss: 18.039020538330078 = 0.193192258477211 + 3 * 5.894056797027588\n",
      "Epoch 310, val loss: 1.517769694328308\n",
      "Epoch 310, val acc: 0.658\n",
      "Epoch 320, training loss: 17.98711395263672 = 0.15942129492759705 + 3 * 5.891895771026611\n",
      "Epoch 320, val loss: 1.5403952598571777\n",
      "Epoch 320, val acc: 0.664\n",
      "Epoch 330, training loss: 17.94919776916504 = 0.15475092828273773 + 3 * 5.863116264343262\n",
      "Epoch 330, val loss: 1.519047498703003\n",
      "Epoch 330, val acc: 0.668\n",
      "Epoch 340, training loss: 17.840831756591797 = 0.17605432868003845 + 3 * 5.841702461242676\n",
      "Epoch 340, val loss: 1.5464973449707031\n",
      "Epoch 340, val acc: 0.67\n",
      "Epoch 350, training loss: 17.83205795288086 = 0.16702020168304443 + 3 * 5.841019630432129\n",
      "Epoch 350, val loss: 1.5796942710876465\n",
      "Epoch 350, val acc: 0.656\n",
      "Epoch 360, training loss: 17.598731994628906 = 0.14576680958271027 + 3 * 5.761772632598877\n",
      "Epoch 360, val loss: 1.5356825590133667\n",
      "Epoch 360, val acc: 0.666\n",
      "Epoch 370, training loss: 17.725788116455078 = 0.13851898908615112 + 3 * 5.819250106811523\n",
      "Epoch 370, val loss: 1.545472502708435\n",
      "Epoch 370, val acc: 0.656\n",
      "Epoch 380, training loss: 17.664710998535156 = 0.16275916993618011 + 3 * 5.78878116607666\n",
      "Epoch 380, val loss: 1.57859468460083\n",
      "Epoch 380, val acc: 0.664\n",
      "Epoch 390, training loss: 17.64651870727539 = 0.1685413271188736 + 3 * 5.781097888946533\n",
      "Epoch 390, val loss: 1.5653326511383057\n",
      "Epoch 390, val acc: 0.668\n",
      "Epoch 400, training loss: 17.731599807739258 = 0.13765130937099457 + 3 * 5.824315547943115\n",
      "Epoch 400, val loss: 1.581691861152649\n",
      "Epoch 400, val acc: 0.66\n",
      "Epoch 410, training loss: 17.642871856689453 = 0.14948497712612152 + 3 * 5.782015323638916\n",
      "Epoch 410, val loss: 1.6093618869781494\n",
      "Epoch 410, val acc: 0.664\n",
      "Epoch 420, training loss: 17.52623176574707 = 0.1412804126739502 + 3 * 5.756763458251953\n",
      "Epoch 420, val loss: 1.6161702871322632\n",
      "Epoch 420, val acc: 0.66\n",
      "Epoch 430, training loss: 17.647281646728516 = 0.14094118773937225 + 3 * 5.798402309417725\n",
      "Epoch 430, val loss: 1.6361368894577026\n",
      "Epoch 430, val acc: 0.666\n",
      "Epoch 440, training loss: 17.450069427490234 = 0.12475623935461044 + 3 * 5.726663112640381\n",
      "Epoch 440, val loss: 1.6681017875671387\n",
      "Epoch 440, val acc: 0.656\n",
      "Epoch 450, training loss: 17.547706604003906 = 0.18501673638820648 + 3 * 5.745206356048584\n",
      "Epoch 450, val loss: 1.6473567485809326\n",
      "Epoch 450, val acc: 0.642\n",
      "Epoch 460, training loss: 17.34309196472168 = 0.14319966733455658 + 3 * 5.689600944519043\n",
      "Epoch 460, val loss: 1.7135170698165894\n",
      "Epoch 460, val acc: 0.65\n",
      "Epoch 470, training loss: 17.439464569091797 = 0.127830371260643 + 3 * 5.7224931716918945\n",
      "Epoch 470, val loss: 1.6391792297363281\n",
      "Epoch 470, val acc: 0.652\n",
      "Epoch 480, training loss: 17.506088256835938 = 0.12629574537277222 + 3 * 5.7503437995910645\n",
      "Epoch 480, val loss: 1.6951440572738647\n",
      "Epoch 480, val acc: 0.642\n",
      "Epoch 490, training loss: 17.445114135742188 = 0.1360715627670288 + 3 * 5.732341766357422\n",
      "Epoch 490, val loss: 1.6767269372940063\n",
      "Epoch 490, val acc: 0.654\n",
      "Epoch 500, training loss: 17.44776725769043 = 0.13993272185325623 + 3 * 5.726271629333496\n",
      "Epoch 500, val loss: 1.7060887813568115\n",
      "Epoch 500, val acc: 0.648\n",
      "Epoch 510, training loss: 17.361005783081055 = 0.1160941869020462 + 3 * 5.712522029876709\n",
      "Epoch 510, val loss: 1.697159767150879\n",
      "Epoch 510, val acc: 0.65\n",
      "Epoch 520, training loss: 17.18206214904785 = 0.12431293725967407 + 3 * 5.650417804718018\n",
      "Epoch 520, val loss: 1.7160700559616089\n",
      "Epoch 520, val acc: 0.652\n",
      "Epoch 530, training loss: 17.31049346923828 = 0.1159500703215599 + 3 * 5.697909355163574\n",
      "Epoch 530, val loss: 1.6892338991165161\n",
      "Epoch 530, val acc: 0.636\n",
      "Epoch 540, training loss: 17.375808715820312 = 0.10150714963674545 + 3 * 5.706094264984131\n",
      "Epoch 540, val loss: 1.728315830230713\n",
      "Epoch 540, val acc: 0.654\n",
      "Epoch 550, training loss: 17.331560134887695 = 0.1300286203622818 + 3 * 5.695571422576904\n",
      "Epoch 550, val loss: 1.7201656103134155\n",
      "Epoch 550, val acc: 0.646\n",
      "Epoch 560, training loss: 17.151792526245117 = 0.11871755868196487 + 3 * 5.633437633514404\n",
      "Epoch 560, val loss: 1.7701925039291382\n",
      "Epoch 560, val acc: 0.644\n",
      "Epoch 570, training loss: 17.396255493164062 = 0.12409981340169907 + 3 * 5.708883285522461\n",
      "Epoch 570, val loss: 1.7382755279541016\n",
      "Epoch 570, val acc: 0.646\n",
      "Epoch 580, training loss: 17.090007781982422 = 0.10597612708806992 + 3 * 5.615200042724609\n",
      "Epoch 580, val loss: 1.7774450778961182\n",
      "Epoch 580, val acc: 0.64\n",
      "Epoch 590, training loss: 17.38732147216797 = 0.11498655378818512 + 3 * 5.718333721160889\n",
      "Epoch 590, val loss: 1.7377179861068726\n",
      "Epoch 590, val acc: 0.638\n",
      "Epoch 600, training loss: 17.239307403564453 = 0.11713328212499619 + 3 * 5.675448894500732\n",
      "Epoch 600, val loss: 1.784920334815979\n",
      "Epoch 600, val acc: 0.654\n",
      "Epoch 610, training loss: 17.036827087402344 = 0.1382543444633484 + 3 * 5.595761299133301\n",
      "Epoch 610, val loss: 1.7920398712158203\n",
      "Epoch 610, val acc: 0.652\n",
      "Epoch 620, training loss: 17.31635284423828 = 0.16148945689201355 + 3 * 5.680507659912109\n",
      "Epoch 620, val loss: 1.798086404800415\n",
      "Epoch 620, val acc: 0.642\n",
      "Epoch 630, training loss: 17.18815803527832 = 0.12502625584602356 + 3 * 5.639613151550293\n",
      "Epoch 630, val loss: 1.7829238176345825\n",
      "Epoch 630, val acc: 0.646\n",
      "Epoch 640, training loss: 17.2085018157959 = 0.10238571465015411 + 3 * 5.669286251068115\n",
      "Epoch 640, val loss: 1.7800281047821045\n",
      "Epoch 640, val acc: 0.644\n",
      "Epoch 650, training loss: 17.072750091552734 = 0.11872684210538864 + 3 * 5.61452054977417\n",
      "Epoch 650, val loss: 1.7727479934692383\n",
      "Epoch 650, val acc: 0.638\n",
      "Epoch 660, training loss: 16.989042282104492 = 0.11187580972909927 + 3 * 5.573636531829834\n",
      "Epoch 660, val loss: 1.7641043663024902\n",
      "Epoch 660, val acc: 0.65\n",
      "Epoch 670, training loss: 17.081764221191406 = 0.12443047761917114 + 3 * 5.609785079956055\n",
      "Epoch 670, val loss: 1.7945832014083862\n",
      "Epoch 670, val acc: 0.644\n",
      "Epoch 680, training loss: 16.965065002441406 = 0.1177765280008316 + 3 * 5.579104900360107\n",
      "Epoch 680, val loss: 1.834351658821106\n",
      "Epoch 680, val acc: 0.638\n",
      "Epoch 690, training loss: 17.27796173095703 = 0.13913382589817047 + 3 * 5.662810802459717\n",
      "Epoch 690, val loss: 1.794871211051941\n",
      "Epoch 690, val acc: 0.65\n",
      "=== picking the best model according to the performance on validation ===\n",
      "target class rate on Vs: 1.0000\n",
      "accuracy on clean test nodes: 0.7140\n",
      "Overall ASR: 0.9480\n",
      "Flip ASR: 0.9421/449 nodes\n",
      "The final ASR:0.94800, 0.00000, Accuracy:0.71400, 0.00000\n",
      "#Attach Nodes:10\n",
      "precent of left attach nodes: 1.000\n",
      "=== training gcn model ===\n",
      "Epoch 0, training loss: 26.63266944885254 = 1.9466770887374878 + 3 * 7.579596519470215\n",
      "Epoch 0, val loss: 1.9419647455215454\n",
      "Epoch 0, val acc: 0.17400000000000002\n",
      "Epoch 10, training loss: 26.471351623535156 = 1.910518765449524 + 3 * 7.550074100494385\n",
      "Epoch 10, val loss: 1.9463402032852173\n",
      "Epoch 10, val acc: 0.128\n",
      "Epoch 20, training loss: 24.986146926879883 = 1.8980334997177124 + 3 * 7.064387798309326\n",
      "Epoch 20, val loss: 1.9478566646575928\n",
      "Epoch 20, val acc: 0.128\n",
      "Epoch 30, training loss: 24.528345108032227 = 1.8402364253997803 + 3 * 6.947874069213867\n",
      "Epoch 30, val loss: 1.9278239011764526\n",
      "Epoch 30, val acc: 0.128\n",
      "Epoch 40, training loss: 23.847084045410156 = 1.7382982969284058 + 3 * 6.792266368865967\n",
      "Epoch 40, val loss: 1.8691304922103882\n",
      "Epoch 40, val acc: 0.156\n",
      "Epoch 50, training loss: 22.884620666503906 = 1.4614380598068237 + 3 * 6.642083644866943\n",
      "Epoch 50, val loss: 1.7565051317214966\n",
      "Epoch 50, val acc: 0.378\n",
      "Epoch 60, training loss: 21.97627830505371 = 1.0276148319244385 + 3 * 6.6567769050598145\n",
      "Epoch 60, val loss: 1.561905026435852\n",
      "Epoch 60, val acc: 0.5720000000000001\n",
      "Epoch 70, training loss: 20.708742141723633 = 0.570118248462677 + 3 * 6.522703170776367\n",
      "Epoch 70, val loss: 1.3784745931625366\n",
      "Epoch 70, val acc: 0.652\n",
      "Epoch 80, training loss: 20.16634178161621 = 0.3629496395587921 + 3 * 6.4811625480651855\n",
      "Epoch 80, val loss: 1.3516435623168945\n",
      "Epoch 80, val acc: 0.658\n",
      "Epoch 90, training loss: 19.584238052368164 = 0.27696090936660767 + 3 * 6.34510612487793\n",
      "Epoch 90, val loss: 1.3975707292556763\n",
      "Epoch 90, val acc: 0.65\n",
      "Epoch 100, training loss: 19.37769889831543 = 0.23020794987678528 + 3 * 6.301239490509033\n",
      "Epoch 100, val loss: 1.4744070768356323\n",
      "Epoch 100, val acc: 0.65\n",
      "Epoch 110, training loss: 19.32810401916504 = 0.2420983463525772 + 3 * 6.281390190124512\n",
      "Epoch 110, val loss: 1.4599801301956177\n",
      "Epoch 110, val acc: 0.66\n",
      "Epoch 120, training loss: 19.067834854125977 = 0.21365872025489807 + 3 * 6.208103179931641\n",
      "Epoch 120, val loss: 1.5170667171478271\n",
      "Epoch 120, val acc: 0.642\n",
      "Epoch 130, training loss: 18.816577911376953 = 0.21027447283267975 + 3 * 6.137490272521973\n",
      "Epoch 130, val loss: 1.5349093675613403\n",
      "Epoch 130, val acc: 0.636\n",
      "Epoch 140, training loss: 18.76983070373535 = 0.18673673272132874 + 3 * 6.128244400024414\n",
      "Epoch 140, val loss: 1.5681231021881104\n",
      "Epoch 140, val acc: 0.62\n",
      "Epoch 150, training loss: 18.81263542175293 = 0.19831566512584686 + 3 * 6.145686149597168\n",
      "Epoch 150, val loss: 1.6386198997497559\n",
      "Epoch 150, val acc: 0.602\n",
      "Epoch 160, training loss: 18.7673397064209 = 0.1713847815990448 + 3 * 6.1251115798950195\n",
      "Epoch 160, val loss: 1.6520432233810425\n",
      "Epoch 160, val acc: 0.592\n",
      "Epoch 170, training loss: 18.86518096923828 = 0.17350853979587555 + 3 * 6.16817569732666\n",
      "Epoch 170, val loss: 1.7148829698562622\n",
      "Epoch 170, val acc: 0.5740000000000001\n",
      "Epoch 180, training loss: 18.59784698486328 = 0.15950116515159607 + 3 * 6.085862159729004\n",
      "Epoch 180, val loss: 1.7308319807052612\n",
      "Epoch 180, val acc: 0.5720000000000001\n",
      "Epoch 190, training loss: 18.432369232177734 = 0.16498591005802155 + 3 * 6.0324177742004395\n",
      "Epoch 190, val loss: 1.7332476377487183\n",
      "Epoch 190, val acc: 0.5720000000000001\n",
      "Epoch 200, training loss: 18.53644371032715 = 0.18117555975914001 + 3 * 6.064320087432861\n",
      "Epoch 200, val loss: 1.8212331533432007\n",
      "Epoch 200, val acc: 0.562\n",
      "Epoch 210, training loss: 18.43511390686035 = 0.20341956615447998 + 3 * 6.027781963348389\n",
      "Epoch 210, val loss: 1.8171836137771606\n",
      "Epoch 210, val acc: 0.556\n",
      "Epoch 220, training loss: 18.267465591430664 = 0.19870054721832275 + 3 * 5.974124431610107\n",
      "Epoch 220, val loss: 1.8215181827545166\n",
      "Epoch 220, val acc: 0.5660000000000001\n",
      "Epoch 230, training loss: 18.31064796447754 = 0.1740991622209549 + 3 * 5.984334945678711\n",
      "Epoch 230, val loss: 1.8816590309143066\n",
      "Epoch 230, val acc: 0.56\n",
      "Epoch 240, training loss: 18.12362289428711 = 0.15416745841503143 + 3 * 5.940873146057129\n",
      "Epoch 240, val loss: 1.9025822877883911\n",
      "Epoch 240, val acc: 0.56\n",
      "Epoch 250, training loss: 18.241432189941406 = 0.1922893077135086 + 3 * 5.966619491577148\n",
      "Epoch 250, val loss: 1.9562740325927734\n",
      "Epoch 250, val acc: 0.546\n",
      "Epoch 260, training loss: 18.11966896057129 = 0.1509592980146408 + 3 * 5.930715084075928\n",
      "Epoch 260, val loss: 1.9732922315597534\n",
      "Epoch 260, val acc: 0.55\n",
      "Epoch 270, training loss: 17.916276931762695 = 0.16745634377002716 + 3 * 5.860788345336914\n",
      "Epoch 270, val loss: 1.9536182880401611\n",
      "Epoch 270, val acc: 0.556\n",
      "Epoch 280, training loss: 18.152238845825195 = 0.13962899148464203 + 3 * 5.953829288482666\n",
      "Epoch 280, val loss: 2.048034191131592\n",
      "Epoch 280, val acc: 0.542\n",
      "Epoch 290, training loss: 17.837406158447266 = 0.18999283015727997 + 3 * 5.835203170776367\n",
      "Epoch 290, val loss: 2.025008201599121\n",
      "Epoch 290, val acc: 0.548\n",
      "Epoch 300, training loss: 17.94854164123535 = 0.16194576025009155 + 3 * 5.870940685272217\n",
      "Epoch 300, val loss: 2.0909080505371094\n",
      "Epoch 300, val acc: 0.54\n",
      "Epoch 310, training loss: 17.805591583251953 = 0.14765678346157074 + 3 * 5.836019039154053\n",
      "Epoch 310, val loss: 2.082559585571289\n",
      "Epoch 310, val acc: 0.544\n",
      "Epoch 320, training loss: 17.897436141967773 = 0.15736114978790283 + 3 * 5.864634990692139\n",
      "Epoch 320, val loss: 2.1251633167266846\n",
      "Epoch 320, val acc: 0.542\n",
      "Epoch 330, training loss: 17.87970733642578 = 0.15156014263629913 + 3 * 5.865644454956055\n",
      "Epoch 330, val loss: 2.171072483062744\n",
      "Epoch 330, val acc: 0.532\n",
      "Epoch 340, training loss: 17.8009033203125 = 0.1854746788740158 + 3 * 5.831410884857178\n",
      "Epoch 340, val loss: 2.148704767227173\n",
      "Epoch 340, val acc: 0.538\n",
      "Epoch 350, training loss: 17.62006950378418 = 0.14846383035182953 + 3 * 5.782256603240967\n",
      "Epoch 350, val loss: 2.2796120643615723\n",
      "Epoch 350, val acc: 0.528\n",
      "Epoch 360, training loss: 17.697223663330078 = 0.13872386515140533 + 3 * 5.807565689086914\n",
      "Epoch 360, val loss: 2.23166823387146\n",
      "Epoch 360, val acc: 0.534\n",
      "Epoch 370, training loss: 17.791141510009766 = 0.12869983911514282 + 3 * 5.839833736419678\n",
      "Epoch 370, val loss: 2.3051528930664062\n",
      "Epoch 370, val acc: 0.536\n",
      "Epoch 380, training loss: 17.502805709838867 = 0.11248064786195755 + 3 * 5.747365474700928\n",
      "Epoch 380, val loss: 2.3093173503875732\n",
      "Epoch 380, val acc: 0.532\n",
      "Epoch 390, training loss: 17.67364501953125 = 0.15291889011859894 + 3 * 5.789468765258789\n",
      "Epoch 390, val loss: 2.332665205001831\n",
      "Epoch 390, val acc: 0.538\n",
      "Epoch 400, training loss: 17.44706153869629 = 0.19023577868938446 + 3 * 5.710378646850586\n",
      "Epoch 400, val loss: 2.3208632469177246\n",
      "Epoch 400, val acc: 0.534\n",
      "Epoch 410, training loss: 17.809734344482422 = 0.13186652958393097 + 3 * 5.836568355560303\n",
      "Epoch 410, val loss: 2.395047426223755\n",
      "Epoch 410, val acc: 0.536\n",
      "Epoch 420, training loss: 17.49016571044922 = 0.13260763883590698 + 3 * 5.74584436416626\n",
      "Epoch 420, val loss: 2.361722469329834\n",
      "Epoch 420, val acc: 0.534\n",
      "Epoch 430, training loss: 17.804733276367188 = 0.12538683414459229 + 3 * 5.843787670135498\n",
      "Epoch 430, val loss: 2.410305976867676\n",
      "Epoch 430, val acc: 0.528\n",
      "Epoch 440, training loss: 17.516517639160156 = 0.13137564063072205 + 3 * 5.750868320465088\n",
      "Epoch 440, val loss: 2.3955540657043457\n",
      "Epoch 440, val acc: 0.53\n",
      "Epoch 450, training loss: 17.553979873657227 = 0.15853722393512726 + 3 * 5.762914657592773\n",
      "Epoch 450, val loss: 2.416992664337158\n",
      "Epoch 450, val acc: 0.528\n",
      "Epoch 460, training loss: 17.27104377746582 = 0.14702804386615753 + 3 * 5.657400608062744\n",
      "Epoch 460, val loss: 2.412883758544922\n",
      "Epoch 460, val acc: 0.534\n",
      "Epoch 470, training loss: 17.45242691040039 = 0.16361087560653687 + 3 * 5.724704265594482\n",
      "Epoch 470, val loss: 2.4286811351776123\n",
      "Epoch 470, val acc: 0.526\n",
      "Epoch 480, training loss: 17.345821380615234 = 0.13319700956344604 + 3 * 5.694577217102051\n",
      "Epoch 480, val loss: 2.4536752700805664\n",
      "Epoch 480, val acc: 0.53\n",
      "Epoch 490, training loss: 17.307331085205078 = 0.13247846066951752 + 3 * 5.683065891265869\n",
      "Epoch 490, val loss: 2.4701809883117676\n",
      "Epoch 490, val acc: 0.526\n",
      "Epoch 500, training loss: 17.478866577148438 = 0.1550312340259552 + 3 * 5.726498126983643\n",
      "Epoch 500, val loss: 2.4655771255493164\n",
      "Epoch 500, val acc: 0.528\n",
      "Epoch 510, training loss: 17.17686653137207 = 0.11737772822380066 + 3 * 5.655779838562012\n",
      "Epoch 510, val loss: 2.5121397972106934\n",
      "Epoch 510, val acc: 0.518\n",
      "Epoch 520, training loss: 17.46786880493164 = 0.18575534224510193 + 3 * 5.726564884185791\n",
      "Epoch 520, val loss: 2.4778995513916016\n",
      "Epoch 520, val acc: 0.522\n",
      "Epoch 530, training loss: 17.292728424072266 = 0.11996474117040634 + 3 * 5.687302589416504\n",
      "Epoch 530, val loss: 2.5774970054626465\n",
      "Epoch 530, val acc: 0.53\n",
      "Epoch 540, training loss: 17.443174362182617 = 0.15086255967617035 + 3 * 5.724420547485352\n",
      "Epoch 540, val loss: 2.504493474960327\n",
      "Epoch 540, val acc: 0.53\n",
      "Epoch 550, training loss: 17.2065372467041 = 0.1251107007265091 + 3 * 5.647228717803955\n",
      "Epoch 550, val loss: 2.5493812561035156\n",
      "Epoch 550, val acc: 0.536\n",
      "Epoch 560, training loss: 17.628440856933594 = 0.1460111141204834 + 3 * 5.7814717292785645\n",
      "Epoch 560, val loss: 2.533745765686035\n",
      "Epoch 560, val acc: 0.536\n",
      "Epoch 570, training loss: 17.374000549316406 = 0.13389132916927338 + 3 * 5.705048084259033\n",
      "Epoch 570, val loss: 2.5757551193237305\n",
      "Epoch 570, val acc: 0.522\n",
      "Epoch 580, training loss: 17.251502990722656 = 0.13130712509155273 + 3 * 5.649799346923828\n",
      "Epoch 580, val loss: 2.5478570461273193\n",
      "Epoch 580, val acc: 0.532\n",
      "Epoch 590, training loss: 17.291685104370117 = 0.12280726432800293 + 3 * 5.680089950561523\n",
      "Epoch 590, val loss: 2.6223576068878174\n",
      "Epoch 590, val acc: 0.524\n",
      "Epoch 600, training loss: 17.4490909576416 = 0.14378100633621216 + 3 * 5.733440399169922\n",
      "Epoch 600, val loss: 2.5966782569885254\n",
      "Epoch 600, val acc: 0.526\n",
      "Epoch 610, training loss: 17.328413009643555 = 0.15255658328533173 + 3 * 5.684146881103516\n",
      "Epoch 610, val loss: 2.6150476932525635\n",
      "Epoch 610, val acc: 0.524\n",
      "Epoch 620, training loss: 17.19666862487793 = 0.11432502418756485 + 3 * 5.652494430541992\n",
      "Epoch 620, val loss: 2.6588876247406006\n",
      "Epoch 620, val acc: 0.522\n",
      "Epoch 630, training loss: 17.127283096313477 = 0.14310620725154877 + 3 * 5.620049476623535\n",
      "Epoch 630, val loss: 2.6939191818237305\n",
      "Epoch 630, val acc: 0.514\n",
      "Epoch 640, training loss: 17.340675354003906 = 0.12468039244413376 + 3 * 5.70119047164917\n",
      "Epoch 640, val loss: 2.661769151687622\n",
      "Epoch 640, val acc: 0.522\n",
      "Epoch 650, training loss: 16.936748504638672 = 0.10558310151100159 + 3 * 5.574716091156006\n",
      "Epoch 650, val loss: 2.719511032104492\n",
      "Epoch 650, val acc: 0.52\n",
      "Epoch 660, training loss: 17.508296966552734 = 0.10036773979663849 + 3 * 5.762086868286133\n",
      "Epoch 660, val loss: 2.6822757720947266\n",
      "Epoch 660, val acc: 0.51\n",
      "Epoch 670, training loss: 17.253215789794922 = 0.13458524644374847 + 3 * 5.655125617980957\n",
      "Epoch 670, val loss: 2.6996004581451416\n",
      "Epoch 670, val acc: 0.516\n",
      "Epoch 680, training loss: 17.252721786499023 = 0.1512576937675476 + 3 * 5.659663200378418\n",
      "Epoch 680, val loss: 2.6579489707946777\n",
      "Epoch 680, val acc: 0.516\n",
      "Epoch 690, training loss: 17.099123001098633 = 0.11680661141872406 + 3 * 5.623370170593262\n",
      "Epoch 690, val loss: 2.6308600902557373\n",
      "Epoch 690, val acc: 0.518\n",
      "=== picking the best model according to the performance on validation ===\n",
      "target class rate on Vs: 1.0000\n",
      "accuracy on clean test nodes: 0.7120\n",
      "Overall ASR: 0.9860\n",
      "Flip ASR: 0.9866/449 nodes\n",
      "The final ASR:0.98600, 0.00000, Accuracy:0.71200, 0.00000\n",
      "#Attach Nodes:10\n",
      "precent of left attach nodes: 1.000\n",
      "=== training gcn model ===\n",
      "Epoch 0, training loss: 26.76061248779297 = 1.9506866931915283 + 3 * 7.619539260864258\n",
      "Epoch 0, val loss: 1.9367698431015015\n",
      "Epoch 0, val acc: 0.158\n",
      "Epoch 10, training loss: 26.18442726135254 = 1.9178497791290283 + 3 * 7.448228359222412\n",
      "Epoch 10, val loss: 1.9390456676483154\n",
      "Epoch 10, val acc: 0.128\n",
      "Epoch 20, training loss: 25.139726638793945 = 1.9036320447921753 + 3 * 7.111234664916992\n",
      "Epoch 20, val loss: 1.9442628622055054\n",
      "Epoch 20, val acc: 0.128\n",
      "Epoch 30, training loss: 24.49510955810547 = 1.853776454925537 + 3 * 6.930639743804932\n",
      "Epoch 30, val loss: 1.934840440750122\n",
      "Epoch 30, val acc: 0.128\n",
      "Epoch 40, training loss: 23.884178161621094 = 1.7640740871429443 + 3 * 6.7865986824035645\n",
      "Epoch 40, val loss: 1.8958041667938232\n",
      "Epoch 40, val acc: 0.128\n",
      "Epoch 50, training loss: 22.964569091796875 = 1.567956566810608 + 3 * 6.613537311553955\n",
      "Epoch 50, val loss: 1.8194600343704224\n",
      "Epoch 50, val acc: 0.29\n",
      "Epoch 60, training loss: 22.001333236694336 = 1.16958487033844 + 3 * 6.547893524169922\n",
      "Epoch 60, val loss: 1.67695152759552\n",
      "Epoch 60, val acc: 0.486\n",
      "Epoch 70, training loss: 20.98293113708496 = 0.7446697354316711 + 3 * 6.503402233123779\n",
      "Epoch 70, val loss: 1.4986138343811035\n",
      "Epoch 70, val acc: 0.592\n",
      "Epoch 80, training loss: 20.153356552124023 = 0.43822750449180603 + 3 * 6.421273708343506\n",
      "Epoch 80, val loss: 1.3975104093551636\n",
      "Epoch 80, val acc: 0.612\n",
      "Epoch 90, training loss: 19.77192497253418 = 0.33506858348846436 + 3 * 6.36386775970459\n",
      "Epoch 90, val loss: 1.426925539970398\n",
      "Epoch 90, val acc: 0.618\n",
      "Epoch 100, training loss: 19.376747131347656 = 0.27790629863739014 + 3 * 6.279976844787598\n",
      "Epoch 100, val loss: 1.4788655042648315\n",
      "Epoch 100, val acc: 0.606\n",
      "Epoch 110, training loss: 19.33516502380371 = 0.2653948664665222 + 3 * 6.273512363433838\n",
      "Epoch 110, val loss: 1.5217576026916504\n",
      "Epoch 110, val acc: 0.59\n",
      "Epoch 120, training loss: 19.144763946533203 = 0.21205610036849976 + 3 * 6.231949329376221\n",
      "Epoch 120, val loss: 1.579389214515686\n",
      "Epoch 120, val acc: 0.5660000000000001\n",
      "Epoch 130, training loss: 18.879592895507812 = 0.21333995461463928 + 3 * 6.14743709564209\n",
      "Epoch 130, val loss: 1.6117647886276245\n",
      "Epoch 130, val acc: 0.5700000000000001\n",
      "Epoch 140, training loss: 18.781360626220703 = 0.24649742245674133 + 3 * 6.099709987640381\n",
      "Epoch 140, val loss: 1.604319453239441\n",
      "Epoch 140, val acc: 0.596\n",
      "Epoch 150, training loss: 18.86031723022461 = 0.19562554359436035 + 3 * 6.156833171844482\n",
      "Epoch 150, val loss: 1.689102292060852\n",
      "Epoch 150, val acc: 0.5700000000000001\n",
      "Epoch 160, training loss: 18.724367141723633 = 0.21725605428218842 + 3 * 6.091082572937012\n",
      "Epoch 160, val loss: 1.64847993850708\n",
      "Epoch 160, val acc: 0.592\n",
      "Epoch 170, training loss: 18.442916870117188 = 0.1862965077161789 + 3 * 6.007465839385986\n",
      "Epoch 170, val loss: 1.7532180547714233\n",
      "Epoch 170, val acc: 0.544\n",
      "Epoch 180, training loss: 18.449853897094727 = 0.18953774869441986 + 3 * 6.021082401275635\n",
      "Epoch 180, val loss: 1.7794852256774902\n",
      "Epoch 180, val acc: 0.552\n",
      "Epoch 190, training loss: 18.345138549804688 = 0.16778449714183807 + 3 * 6.0016984939575195\n",
      "Epoch 190, val loss: 1.827030062675476\n",
      "Epoch 190, val acc: 0.542\n",
      "Epoch 200, training loss: 18.427732467651367 = 0.21023353934288025 + 3 * 6.006588459014893\n",
      "Epoch 200, val loss: 1.8415815830230713\n",
      "Epoch 200, val acc: 0.538\n",
      "Epoch 210, training loss: 18.309410095214844 = 0.17664137482643127 + 3 * 5.995549201965332\n",
      "Epoch 210, val loss: 1.89006769657135\n",
      "Epoch 210, val acc: 0.524\n",
      "Epoch 220, training loss: 18.15163803100586 = 0.1645827293395996 + 3 * 5.932733535766602\n",
      "Epoch 220, val loss: 1.9449913501739502\n",
      "Epoch 220, val acc: 0.516\n",
      "Epoch 230, training loss: 18.24234390258789 = 0.19066864252090454 + 3 * 5.949680328369141\n",
      "Epoch 230, val loss: 1.9467445611953735\n",
      "Epoch 230, val acc: 0.52\n",
      "Epoch 240, training loss: 18.21428108215332 = 0.16483500599861145 + 3 * 5.952480792999268\n",
      "Epoch 240, val loss: 2.0007383823394775\n",
      "Epoch 240, val acc: 0.504\n",
      "Epoch 250, training loss: 18.096620559692383 = 0.18889553844928741 + 3 * 5.9148640632629395\n",
      "Epoch 250, val loss: 1.9969907999038696\n",
      "Epoch 250, val acc: 0.504\n",
      "Epoch 260, training loss: 17.760517120361328 = 0.19477076828479767 + 3 * 5.797695636749268\n",
      "Epoch 260, val loss: 2.01584792137146\n",
      "Epoch 260, val acc: 0.506\n",
      "Epoch 270, training loss: 17.839397430419922 = 0.14189574122428894 + 3 * 5.846542835235596\n",
      "Epoch 270, val loss: 2.1390390396118164\n",
      "Epoch 270, val acc: 0.47600000000000003\n",
      "Epoch 280, training loss: 17.69001007080078 = 0.1719495952129364 + 3 * 5.7906646728515625\n",
      "Epoch 280, val loss: 2.0883474349975586\n",
      "Epoch 280, val acc: 0.492\n",
      "Epoch 290, training loss: 17.813583374023438 = 0.15489402413368225 + 3 * 5.831701755523682\n",
      "Epoch 290, val loss: 2.1907079219818115\n",
      "Epoch 290, val acc: 0.462\n",
      "Epoch 300, training loss: 17.818387985229492 = 0.16613858938217163 + 3 * 5.818939685821533\n",
      "Epoch 300, val loss: 2.216420888900757\n",
      "Epoch 300, val acc: 0.462\n",
      "Epoch 310, training loss: 17.709897994995117 = 0.1477239578962326 + 3 * 5.805789470672607\n",
      "Epoch 310, val loss: 2.1990010738372803\n",
      "Epoch 310, val acc: 0.458\n",
      "Epoch 320, training loss: 17.716493606567383 = 0.18587805330753326 + 3 * 5.794656276702881\n",
      "Epoch 320, val loss: 2.2629201412200928\n",
      "Epoch 320, val acc: 0.444\n",
      "Epoch 330, training loss: 17.800661087036133 = 0.17905671894550323 + 3 * 5.820228576660156\n",
      "Epoch 330, val loss: 2.2380874156951904\n",
      "Epoch 330, val acc: 0.466\n",
      "Epoch 340, training loss: 17.689672470092773 = 0.15828561782836914 + 3 * 5.790428638458252\n",
      "Epoch 340, val loss: 2.2947041988372803\n",
      "Epoch 340, val acc: 0.444\n",
      "Epoch 350, training loss: 17.78319549560547 = 0.15643943846225739 + 3 * 5.8204569816589355\n",
      "Epoch 350, val loss: 2.2878952026367188\n",
      "Epoch 350, val acc: 0.448\n",
      "Epoch 360, training loss: 17.52562713623047 = 0.16224606335163116 + 3 * 5.734046459197998\n",
      "Epoch 360, val loss: 2.329806327819824\n",
      "Epoch 360, val acc: 0.44\n",
      "Epoch 370, training loss: 17.726289749145508 = 0.18349207937717438 + 3 * 5.793826103210449\n",
      "Epoch 370, val loss: 2.3285467624664307\n",
      "Epoch 370, val acc: 0.43\n",
      "Epoch 380, training loss: 17.585342407226562 = 0.14720271527767181 + 3 * 5.753594875335693\n",
      "Epoch 380, val loss: 2.359706163406372\n",
      "Epoch 380, val acc: 0.436\n",
      "Epoch 390, training loss: 17.669713973999023 = 0.17515669763088226 + 3 * 5.792198181152344\n",
      "Epoch 390, val loss: 2.4345831871032715\n",
      "Epoch 390, val acc: 0.432\n",
      "Epoch 400, training loss: 17.640186309814453 = 0.12520967423915863 + 3 * 5.791667938232422\n",
      "Epoch 400, val loss: 2.4477131366729736\n",
      "Epoch 400, val acc: 0.42\n",
      "Epoch 410, training loss: 17.419719696044922 = 0.13804928958415985 + 3 * 5.710088729858398\n",
      "Epoch 410, val loss: 2.488365650177002\n",
      "Epoch 410, val acc: 0.426\n",
      "Epoch 420, training loss: 17.574745178222656 = 0.17105284333229065 + 3 * 5.755766868591309\n",
      "Epoch 420, val loss: 2.4714457988739014\n",
      "Epoch 420, val acc: 0.418\n",
      "Epoch 430, training loss: 17.406044006347656 = 0.1664070338010788 + 3 * 5.703409671783447\n",
      "Epoch 430, val loss: 2.517745018005371\n",
      "Epoch 430, val acc: 0.42\n",
      "Epoch 440, training loss: 17.549341201782227 = 0.1472948044538498 + 3 * 5.757079601287842\n",
      "Epoch 440, val loss: 2.5758464336395264\n",
      "Epoch 440, val acc: 0.41000000000000003\n",
      "Epoch 450, training loss: 17.427204132080078 = 0.1580071747303009 + 3 * 5.702417850494385\n",
      "Epoch 450, val loss: 2.5079779624938965\n",
      "Epoch 450, val acc: 0.422\n",
      "Epoch 460, training loss: 17.574167251586914 = 0.12181048840284348 + 3 * 5.771399021148682\n",
      "Epoch 460, val loss: 2.6251604557037354\n",
      "Epoch 460, val acc: 0.396\n",
      "Epoch 470, training loss: 17.246009826660156 = 0.17450691759586334 + 3 * 5.639236927032471\n",
      "Epoch 470, val loss: 2.5779364109039307\n",
      "Epoch 470, val acc: 0.41000000000000003\n",
      "Epoch 480, training loss: 17.285661697387695 = 0.13560375571250916 + 3 * 5.672333717346191\n",
      "Epoch 480, val loss: 2.707369804382324\n",
      "Epoch 480, val acc: 0.392\n",
      "Epoch 490, training loss: 17.313879013061523 = 0.14552241563796997 + 3 * 5.68076229095459\n",
      "Epoch 490, val loss: 2.644489288330078\n",
      "Epoch 490, val acc: 0.396\n",
      "Epoch 500, training loss: 17.523754119873047 = 0.17802362143993378 + 3 * 5.737205982208252\n",
      "Epoch 500, val loss: 2.7652409076690674\n",
      "Epoch 500, val acc: 0.388\n",
      "Epoch 510, training loss: 17.26038360595703 = 0.1360963135957718 + 3 * 5.65839958190918\n",
      "Epoch 510, val loss: 2.6933212280273438\n",
      "Epoch 510, val acc: 0.398\n",
      "Epoch 520, training loss: 17.590730667114258 = 0.15714330971240997 + 3 * 5.761533260345459\n",
      "Epoch 520, val loss: 2.8171589374542236\n",
      "Epoch 520, val acc: 0.382\n",
      "Epoch 530, training loss: 17.45347023010254 = 0.1571096032857895 + 3 * 5.725675582885742\n",
      "Epoch 530, val loss: 2.7981677055358887\n",
      "Epoch 530, val acc: 0.382\n",
      "Epoch 540, training loss: 17.443042755126953 = 0.17724792659282684 + 3 * 5.712551116943359\n",
      "Epoch 540, val loss: 2.775311231613159\n",
      "Epoch 540, val acc: 0.39\n",
      "Epoch 550, training loss: 17.54463768005371 = 0.13681405782699585 + 3 * 5.758749961853027\n",
      "Epoch 550, val loss: 2.7922518253326416\n",
      "Epoch 550, val acc: 0.386\n",
      "Epoch 560, training loss: 17.284286499023438 = 0.11960067600011826 + 3 * 5.6786112785339355\n",
      "Epoch 560, val loss: 2.8751981258392334\n",
      "Epoch 560, val acc: 0.386\n",
      "Epoch 570, training loss: 17.136857986450195 = 0.13751725852489471 + 3 * 5.627983093261719\n",
      "Epoch 570, val loss: 2.8809759616851807\n",
      "Epoch 570, val acc: 0.39\n",
      "Epoch 580, training loss: 17.327510833740234 = 0.1362476497888565 + 3 * 5.680376052856445\n",
      "Epoch 580, val loss: 2.8896965980529785\n",
      "Epoch 580, val acc: 0.394\n",
      "Epoch 590, training loss: 17.254274368286133 = 0.1145704910159111 + 3 * 5.6674299240112305\n",
      "Epoch 590, val loss: 2.9711520671844482\n",
      "Epoch 590, val acc: 0.388\n",
      "Epoch 600, training loss: 17.18869400024414 = 0.12555700540542603 + 3 * 5.637127876281738\n",
      "Epoch 600, val loss: 2.921926975250244\n",
      "Epoch 600, val acc: 0.392\n",
      "Epoch 610, training loss: 17.202571868896484 = 0.14302513003349304 + 3 * 5.644992351531982\n",
      "Epoch 610, val loss: 2.9150283336639404\n",
      "Epoch 610, val acc: 0.39\n",
      "Epoch 620, training loss: 17.157209396362305 = 0.11842000484466553 + 3 * 5.645520210266113\n",
      "Epoch 620, val loss: 2.932206630706787\n",
      "Epoch 620, val acc: 0.39\n",
      "Epoch 630, training loss: 16.9290828704834 = 0.14456897974014282 + 3 * 5.556090354919434\n",
      "Epoch 630, val loss: 2.9591174125671387\n",
      "Epoch 630, val acc: 0.392\n",
      "Epoch 640, training loss: 17.154579162597656 = 0.11185792833566666 + 3 * 5.634267807006836\n",
      "Epoch 640, val loss: 3.010550022125244\n",
      "Epoch 640, val acc: 0.39\n",
      "Epoch 650, training loss: 17.123571395874023 = 0.10924452543258667 + 3 * 5.635481834411621\n",
      "Epoch 650, val loss: 2.9627180099487305\n",
      "Epoch 650, val acc: 0.388\n",
      "Epoch 660, training loss: 17.235742568969727 = 0.1314673274755478 + 3 * 5.6631646156311035\n",
      "Epoch 660, val loss: 3.0067739486694336\n",
      "Epoch 660, val acc: 0.392\n",
      "Epoch 670, training loss: 17.083852767944336 = 0.11563381552696228 + 3 * 5.612856864929199\n",
      "Epoch 670, val loss: 2.983944892883301\n",
      "Epoch 670, val acc: 0.398\n",
      "Epoch 680, training loss: 17.14193344116211 = 0.107148177921772 + 3 * 5.63244104385376\n",
      "Epoch 680, val loss: 2.997485876083374\n",
      "Epoch 680, val acc: 0.388\n",
      "Epoch 690, training loss: 17.06288719177246 = 0.11668326705694199 + 3 * 5.617101669311523\n",
      "Epoch 690, val loss: 3.0199406147003174\n",
      "Epoch 690, val acc: 0.386\n",
      "=== picking the best model according to the performance on validation ===\n",
      "target class rate on Vs: 1.0000\n",
      "accuracy on clean test nodes: 0.7100\n",
      "Overall ASR: 1.0000\n",
      "Flip ASR: 1.0000/449 nodes\n",
      "The final ASR:1.00000, 0.00000, Accuracy:0.71000, 0.00000\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    ''' Contrastive learning to backdoor in Contrastive learning'''\n",
    "    from models.GTA import Backdoor\n",
    "    import heuristic_selection as hs\n",
    "    from models.GCN import GCN\n",
    "    from models.construct import model_construct\n",
    "    import copy \n",
    "    from model import UnifyModel, Encoder\n",
    "    from models.construct import model_construct\n",
    "    from models.GCN_CL import GCN_Encoder\n",
    "    data = data.to(device)\n",
    "    # learning_rate = 0.0002\n",
    "    # weight_decay = config['weight_decay']\n",
    "    num_class = int(data.y.max()+1)\n",
    "    # args.cl_lr = 0.002\n",
    "    # args.weight_decay = 0.0005\n",
    "\n",
    "    size = args.vs_number #int((len(data.test_mask)-data.test_mask.sum())*args.vs_ratio)\n",
    "    print(\"#Attach Nodes:{}\".format(size))\n",
    "    from models.construct import model_construct\n",
    "    result_asr = []\n",
    "    result_acc = []\n",
    "    data = data.to(device)\n",
    "\n",
    "    seen_node_idx = (torch.bitwise_not(data.test_mask)).nonzero().flatten()\n",
    "    rs = np.random.RandomState(args.seed)\n",
    "    seeds = rs.randint(1000,size=1)\n",
    "    # seeds = [args.seed]\n",
    "    for seed in seeds:\n",
    "        np.random.seed(seed)\n",
    "        # torch.manual_seed(seed)\n",
    "        # torch.cuda.manual_seed(seed)\n",
    "        args.seed = seed\n",
    "        if(args.selection_method == 'none'):\n",
    "            idx_attach = hs.obtain_attach_nodes(args,unlabeled_idx,size)\n",
    "        elif(args.selection_method == 'cluster'):\n",
    "            idx_attach = hs.cluster_distance_selection(args,data,idx_train,idx_val,idx_clean_test,unlabeled_idx,train_edge_index,size,device)\n",
    "            idx_attach = torch.LongTensor(idx_attach).to(device)\n",
    "        elif(args.selection_method == 'cluster_degree'):\n",
    "            idx_attach = hs.cluster_degree_selection(args,data,idx_train,idx_val,idx_clean_test,unlabeled_idx,train_edge_index,size,device)\n",
    "            idx_attach = torch.LongTensor(idx_attach).to(device)\n",
    "        # train trigger generator \n",
    "        model = Backdoor(args,device)\n",
    "        model.fit(data.x, train_edge_index, None, data.y, idx_train,idx_attach, unlabeled_idx)\n",
    "        poison_x, poison_edge_index, poison_edge_weights, poison_labels = model.get_poisoned()\n",
    "\n",
    "        from help_funcs import prune_unrelated_edge,prune_unrelated_edge_isolated\n",
    "        if(args.defense_mode == 'prune'):\n",
    "            poison_edge_index,poison_edge_weights = prune_unrelated_edge(args,poison_edge_index,poison_edge_weights,poison_x,device,large_graph=False)\n",
    "            bkd_tn_nodes = torch.cat([idx_train,idx_attach]).to(device)\n",
    "        elif(args.defense_mode == 'isolate'):\n",
    "            poison_edge_index,poison_edge_weights,rel_nodes = prune_unrelated_edge_isolated(args,poison_edge_index,poison_edge_weights,poison_x,device,large_graph=False)\n",
    "            bkd_tn_nodes = torch.cat([idx_train,idx_attach]).tolist()\n",
    "            bkd_tn_nodes = torch.LongTensor(list(set(bkd_tn_nodes) - set(rel_nodes))).to(device)\n",
    "        else:\n",
    "            bkd_tn_nodes = torch.cat([idx_train,idx_attach]).to(device)\n",
    "\n",
    "        # bkd_tn_nodes = torch.cat([idx_train,idx_attach]).to(device)\n",
    "        print(\"precent of left attach nodes: {:.3f}\"\\\n",
    "            .format(len(set(bkd_tn_nodes.tolist()) & set(idx_attach.tolist()))/len(idx_attach)))\n",
    "        #%%\n",
    "\n",
    "        # model = GCN(nfeat=data.x.shape[1],\\\n",
    "        #             nhid=args.hidden,\\\n",
    "        #             nclass= int(data.y.max()+1),\\\n",
    "        #             dropout=args.dropout,\\\n",
    "        #             lr=args.train_lr,\\\n",
    "        #             weight_decay=args.weight_decay,\\\n",
    "        #             device=device)\n",
    "        test_model = GCN_Encoder(args, data.x.shape[1], args.num_hidden, num_class, unlabeled_idx,dropout=0.5, lr=0.01, weight_decay=args.cl_weight_decay, tau=args.tau, layer=2,device=device,use_ln=False,layer_norm_first=False)\n",
    "        test_model.fit(poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx,verbose=True)\n",
    "        # # test_model = model_construct(args,args.test_model,data,device).to(device) \n",
    "        # encoder = Encoder(dataset.num_features, args.num_hidden, args.cl_activation,\n",
    "        #                         base_model=args.cl_base_model, k=args.cl_num_layers).to(device)\n",
    "        # # test_model = UnifyModel(args, encoder, args.cl_num_hidden, args.cl_num_proj_hidden, num_class, args.tau, lr=args.cl_lr, weight_decay=args.cl_weight_decay, device=None).to(device)\n",
    "        # test_model = UnifyModel(args, encoder, args.num_hidden, args.num_proj_hidden, args.cl_num_proj_hidden, num_class, args.tau, lr=args.cl_lr, weight_decay=args.cl_weight_decay, device=device).to(device)\n",
    "        # # test_model = UnifyModel(args, encoder, num_hidden, num_proj_hidden, num_class, tau, lr=learning_rate, weight_decay=weight_decay, device=device,data1=noisy_data,data2=diff_noisy_data).to(device)\n",
    "        # test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)\n",
    "        test_embds = test_model(poison_x,poison_edge_index,poison_edge_weights)\n",
    "        output = test_model.clf_head(test_embds)\n",
    "        train_attach_rate = (output.argmax(dim=1)[idx_attach]==args.target_class).float().mean()\n",
    "        print(\"target class rate on Vs: {:.4f}\".format(train_attach_rate))\n",
    "        torch.cuda.empty_cache()\n",
    "        # test_model = GCN(nfeat=cont_poison_x.shape[1],\\\n",
    "        #             nhid=args.hidden,\\\n",
    "        #             nclass= int(data.y.max()+1),\\\n",
    "        #             dropout=args.dropout,\\\n",
    "        #             lr=args.train_lr,\\\n",
    "        #             weight_decay=args.weight_decay,\\\n",
    "        #             device=device).to(device) \n",
    "        # test_model.fit(cont_poison_x, poison_edge_index, poison_edge_weights, poison_labels, bkd_tn_nodes, idx_val,train_iters=args.epochs,verbose=False)\n",
    "\n",
    "        # output = test_model(cont_poison_x,poison_edge_index,poison_edge_weights)\n",
    "        # train_attach_rate = (output.argmax(dim=1)[idx_attach]==args.target_class).float().mean()\n",
    "        # print(\"target class rate on Vs: {:.4f}\".format(train_attach_rate))\n",
    "        # torch.cuda.empty_cache()\n",
    "        #%%\n",
    "        induct_edge_index = torch.cat([poison_edge_index,mask_edge_index],dim=1)\n",
    "        induct_edge_weights = torch.cat([poison_edge_weights,torch.ones([mask_edge_index.shape[1]],dtype=torch.float,device=device)])\n",
    "        clean_acc = test_model.test(poison_x,induct_edge_index,induct_edge_weights,data.y,idx_clean_test)\n",
    "        # test_model = test_model.cpu()\n",
    "\n",
    "        print(\"accuracy on clean test nodes: {:.4f}\".format(clean_acc))\n",
    "\n",
    "\n",
    "        if(args.evaluate_mode == '1by1'):\n",
    "            from torch_geometric.utils  import k_hop_subgraph\n",
    "            overall_induct_edge_index, overall_induct_edge_weights = induct_edge_index.clone(),induct_edge_weights.clone()\n",
    "            asr = 0\n",
    "            flip_asr = 0\n",
    "            flip_idx_atk = idx_atk[(data.y[idx_atk] != args.target_class).nonzero().flatten()]\n",
    "            for i, idx in enumerate(idx_atk):\n",
    "                idx=int(idx)\n",
    "                sub_induct_nodeset, sub_induct_edge_index, sub_mapping, sub_edge_mask  = k_hop_subgraph(node_idx = [idx], num_hops = 2, edge_index = overall_induct_edge_index, relabel_nodes=True) # sub_mapping means the index of [idx] in sub)nodeset\n",
    "                ori_node_idx = sub_induct_nodeset[sub_mapping]\n",
    "                relabeled_node_idx = sub_mapping\n",
    "                sub_induct_edge_weights = torch.ones([sub_induct_edge_index.shape[1]]).to(device)\n",
    "                # inject trigger on attack test nodes (idx_atk)'''\n",
    "                with torch.no_grad():\n",
    "                    induct_x, induct_edge_index,induct_edge_weights = model.inject_trigger(relabeled_node_idx,poison_x[sub_induct_nodeset],sub_induct_edge_index,sub_induct_edge_weights,device)\n",
    "                    induct_x, induct_edge_index,induct_edge_weights = induct_x.clone().detach(), induct_edge_index.clone().detach(),induct_edge_weights.clone().detach()\n",
    "                    if(args.defense_mode == 'prune' or args.defense_mode == 'isolate'):\n",
    "                        induct_edge_index,induct_edge_weights = prune_unrelated_edge(args,induct_edge_index,induct_edge_weights,induct_x,device,False)\n",
    "        \n",
    "                    test_embeds = test_model(induct_x, induct_edge_index,induct_edge_weights).to(device)\n",
    "                    output = test_model.clf_head(test_embeds)\n",
    "                    # cont_induct_x = contrastive_model(induct_x, induct_edge_index,induct_edge_weights).detach().to(device)\n",
    "                    # # do pruning in test datas'''\n",
    "                    # if(args.defense_mode == 'prune' or args.defense_mode == 'isolate'):\n",
    "                    #     induct_edge_index,induct_edge_weights = prune_unrelated_edge(args,induct_edge_index,induct_edge_weights,induct_x,device,large_graph=False)\n",
    "                    # attack evaluation\n",
    "\n",
    "                    # output = test_model(cont_induct_x,induct_edge_index,induct_edge_weights)\n",
    "                    train_attach_rate = (output.argmax(dim=1)[relabeled_node_idx]==args.target_class).float().mean()\n",
    "                    # print(\"Node {}: {}, Origin Label: {}\".format(i, idx, data.y[idx]))\n",
    "                    # print(\"ASR: {:.4f}\".format(train_attach_rate))\n",
    "                    asr += train_attach_rate\n",
    "                    if(data.y[idx] != args.target_class):\n",
    "                        flip_asr += train_attach_rate\n",
    "            asr = asr/(idx_atk.shape[0])\n",
    "            flip_asr = flip_asr/(flip_idx_atk.shape[0])\n",
    "            print(\"Overall ASR: {:.4f}\".format(asr))\n",
    "            print(\"Flip ASR: {:.4f}/{} nodes\".format(flip_asr,flip_idx_atk.shape[0]))\n",
    "        elif(args.evaluate_mode == 'overall'):\n",
    "            # %% inject trigger on attack test nodes (idx_atk)'''\n",
    "            induct_x, induct_edge_index,induct_edge_weights = model.inject_trigger(idx_atk,poison_x,induct_edge_index,induct_edge_weights,device)\n",
    "            induct_x, induct_edge_index,induct_edge_weights = induct_x.clone().detach(), induct_edge_index.clone().detach(),induct_edge_weights.clone().detach()\n",
    "            # do pruning in test datas'''\n",
    "            if(args.defense_mode == 'prune' or args.defense_mode == 'isolate'):\n",
    "                induct_edge_index,induct_edge_weights = prune_unrelated_edge(args,induct_edge_index,induct_edge_weights,induct_x,device)\n",
    "            # attack evaluation\n",
    "\n",
    "            test_embeds = test_model(induct_x, induct_edge_index,induct_edge_weights).to(device)\n",
    "            output = test_model.clf_head(test_embeds)\n",
    "            # test_model = test_model.to(device)\n",
    "            # output = test_model(induct_x,induct_edge_index,induct_edge_weights)\n",
    "            train_attach_rate = (output.argmax(dim=1)[idx_atk]==args.target_class).float().mean()\n",
    "            print(\"ASR: {:.4f}\".format(train_attach_rate))\n",
    "            flip_idx_atk = idx_atk[(data.y[idx_atk] != args.target_class).nonzero().flatten()]\n",
    "            flip_asr = (output.argmax(dim=1)[flip_idx_atk]==args.target_class).float().mean()\n",
    "            print(\"Flip ASR: {:.4f}/{} nodes\".format(flip_asr,flip_idx_atk.shape[0]))\n",
    "            ca = test_model.test(induct_x,induct_edge_index,induct_edge_weights,data.y,idx_clean_test)\n",
    "            print(\"CA: {:.4f}\".format(ca))\n",
    "\n",
    "        result_asr.append(float(asr))\n",
    "        result_acc.append(float(clean_acc))\n",
    "\n",
    "    print('The final ASR:{:.5f}, {:.5f}, Accuracy:{:.5f}, {:.5f}'\\\n",
    "                .format(np.average(result_asr),np.std(result_asr),np.average(result_acc),np.std(result_acc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_40375/2119401665.py:3: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pred_prob = F.softmax(output[unlabeled_idx])\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [17,0,0], thread: [96,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [17,0,0], thread: [97,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [17,0,0], thread: [98,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [17,0,0], thread: [99,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [17,0,0], thread: [100,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [17,0,0], thread: [101,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [17,0,0], thread: [102,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [17,0,0], thread: [103,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [17,0,0], thread: [104,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [17,0,0], thread: [105,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [17,0,0], thread: [106,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [17,0,0], thread: [107,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [17,0,0], thread: [108,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [17,0,0], thread: [109,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [17,0,0], thread: [110,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [17,0,0], thread: [111,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [17,0,0], thread: [112,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [17,0,0], thread: [113,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [17,0,0], thread: [114,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [17,0,0], thread: [115,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [17,0,0], thread: [116,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [17,0,0], thread: [117,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [17,0,0], thread: [118,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [17,0,0], thread: [119,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [17,0,0], thread: [120,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [17,0,0], thread: [121,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [17,0,0], thread: [122,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [17,0,0], thread: [123,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [17,0,0], thread: [124,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [17,0,0], thread: [125,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [17,0,0], thread: [126,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [17,0,0], thread: [127,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [18,0,0], thread: [0,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [18,0,0], thread: [1,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [18,0,0], thread: [2,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [18,0,0], thread: [3,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [18,0,0], thread: [4,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [18,0,0], thread: [5,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [18,0,0], thread: [6,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [18,0,0], thread: [7,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [18,0,0], thread: [8,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [18,0,0], thread: [9,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [18,0,0], thread: [10,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [18,0,0], thread: [11,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [18,0,0], thread: [12,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [18,0,0], thread: [13,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [18,0,0], thread: [14,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [18,0,0], thread: [15,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [18,0,0], thread: [16,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [18,0,0], thread: [17,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [18,0,0], thread: [18,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [18,0,0], thread: [19,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [18,0,0], thread: [20,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [18,0,0], thread: [21,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [18,0,0], thread: [22,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [18,0,0], thread: [23,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [18,0,0], thread: [24,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [18,0,0], thread: [25,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [18,0,0], thread: [26,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [18,0,0], thread: [27,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [18,0,0], thread: [28,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [18,0,0], thread: [29,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [18,0,0], thread: [30,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [18,0,0], thread: [31,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [10,0,0], thread: [32,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [10,0,0], thread: [33,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [10,0,0], thread: [34,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [10,0,0], thread: [35,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [10,0,0], thread: [36,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [10,0,0], thread: [37,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [10,0,0], thread: [38,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [10,0,0], thread: [39,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [10,0,0], thread: [40,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [10,0,0], thread: [41,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [10,0,0], thread: [42,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [10,0,0], thread: [43,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [10,0,0], thread: [44,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [10,0,0], thread: [45,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [10,0,0], thread: [46,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [10,0,0], thread: [47,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [10,0,0], thread: [48,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [10,0,0], thread: [49,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [10,0,0], thread: [50,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [10,0,0], thread: [51,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [10,0,0], thread: [52,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [10,0,0], thread: [53,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [10,0,0], thread: [54,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [10,0,0], thread: [55,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [10,0,0], thread: [56,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [10,0,0], thread: [57,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [10,0,0], thread: [58,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [10,0,0], thread: [59,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [10,0,0], thread: [60,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [10,0,0], thread: [61,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [10,0,0], thread: [62,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [10,0,0], thread: [63,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [10,0,0], thread: [0,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [10,0,0], thread: [1,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [10,0,0], thread: [2,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [10,0,0], thread: [3,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [10,0,0], thread: [4,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [10,0,0], thread: [5,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [10,0,0], thread: [6,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [10,0,0], thread: [7,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [10,0,0], thread: [8,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [10,0,0], thread: [9,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [10,0,0], thread: [10,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [10,0,0], thread: [11,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [10,0,0], thread: [12,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [10,0,0], thread: [13,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [10,0,0], thread: [14,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [10,0,0], thread: [15,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [10,0,0], thread: [16,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [10,0,0], thread: [17,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [10,0,0], thread: [18,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [10,0,0], thread: [19,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [10,0,0], thread: [20,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [10,0,0], thread: [21,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [10,0,0], thread: [22,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [10,0,0], thread: [23,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [10,0,0], thread: [24,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [10,0,0], thread: [25,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [10,0,0], thread: [26,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [10,0,0], thread: [27,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [10,0,0], thread: [28,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [10,0,0], thread: [29,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [10,0,0], thread: [30,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [10,0,0], thread: [31,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [15,0,0], thread: [32,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [15,0,0], thread: [33,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [15,0,0], thread: [34,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [15,0,0], thread: [35,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [15,0,0], thread: [36,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [15,0,0], thread: [37,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [15,0,0], thread: [38,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [15,0,0], thread: [39,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [15,0,0], thread: [40,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [15,0,0], thread: [41,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [15,0,0], thread: [42,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [15,0,0], thread: [43,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [15,0,0], thread: [44,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [15,0,0], thread: [45,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [15,0,0], thread: [46,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [15,0,0], thread: [47,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [15,0,0], thread: [48,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [15,0,0], thread: [49,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [15,0,0], thread: [50,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [15,0,0], thread: [51,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [15,0,0], thread: [52,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [15,0,0], thread: [53,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [15,0,0], thread: [54,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [15,0,0], thread: [55,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [15,0,0], thread: [56,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [15,0,0], thread: [57,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [15,0,0], thread: [58,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [15,0,0], thread: [59,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [15,0,0], thread: [60,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [15,0,0], thread: [61,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [15,0,0], thread: [62,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [15,0,0], thread: [63,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [19,0,0], thread: [32,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [19,0,0], thread: [33,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [19,0,0], thread: [34,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [19,0,0], thread: [35,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [19,0,0], thread: [36,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [19,0,0], thread: [37,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [19,0,0], thread: [38,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [19,0,0], thread: [39,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [19,0,0], thread: [40,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [19,0,0], thread: [41,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [19,0,0], thread: [42,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [19,0,0], thread: [43,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [19,0,0], thread: [44,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [19,0,0], thread: [45,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [19,0,0], thread: [46,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [19,0,0], thread: [47,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [19,0,0], thread: [48,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [19,0,0], thread: [49,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [19,0,0], thread: [50,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [19,0,0], thread: [51,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [19,0,0], thread: [52,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [19,0,0], thread: [53,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [19,0,0], thread: [54,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [19,0,0], thread: [55,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [19,0,0], thread: [56,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [19,0,0], thread: [57,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [19,0,0], thread: [58,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [19,0,0], thread: [59,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [19,0,0], thread: [60,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [19,0,0], thread: [61,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [19,0,0], thread: [62,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [19,0,0], thread: [63,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [19,0,0], thread: [96,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [19,0,0], thread: [97,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [19,0,0], thread: [98,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [19,0,0], thread: [99,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [19,0,0], thread: [100,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [19,0,0], thread: [101,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [19,0,0], thread: [102,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [19,0,0], thread: [103,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [19,0,0], thread: [104,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [19,0,0], thread: [105,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [19,0,0], thread: [106,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [19,0,0], thread: [107,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [19,0,0], thread: [108,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [19,0,0], thread: [109,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [19,0,0], thread: [110,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [19,0,0], thread: [111,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [19,0,0], thread: [112,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [19,0,0], thread: [113,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [19,0,0], thread: [114,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [19,0,0], thread: [115,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [19,0,0], thread: [116,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [19,0,0], thread: [117,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [19,0,0], thread: [118,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [19,0,0], thread: [119,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [19,0,0], thread: [120,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [19,0,0], thread: [121,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [19,0,0], thread: [122,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [19,0,0], thread: [123,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [19,0,0], thread: [124,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [19,0,0], thread: [125,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [19,0,0], thread: [126,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [19,0,0], thread: [127,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [20,0,0], thread: [64,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [20,0,0], thread: [65,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [20,0,0], thread: [66,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [20,0,0], thread: [67,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [20,0,0], thread: [68,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [20,0,0], thread: [69,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [20,0,0], thread: [70,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [20,0,0], thread: [71,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [20,0,0], thread: [72,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [20,0,0], thread: [73,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [20,0,0], thread: [74,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [20,0,0], thread: [75,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [20,0,0], thread: [76,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [20,0,0], thread: [77,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [20,0,0], thread: [78,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [20,0,0], thread: [79,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [20,0,0], thread: [80,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [20,0,0], thread: [81,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [20,0,0], thread: [82,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [20,0,0], thread: [83,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [20,0,0], thread: [84,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [20,0,0], thread: [85,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [20,0,0], thread: [86,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [20,0,0], thread: [87,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [20,0,0], thread: [88,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [20,0,0], thread: [89,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [20,0,0], thread: [90,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [20,0,0], thread: [91,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [20,0,0], thread: [92,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [20,0,0], thread: [93,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [20,0,0], thread: [94,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [20,0,0], thread: [95,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [14,0,0], thread: [0,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [14,0,0], thread: [1,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [14,0,0], thread: [2,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [14,0,0], thread: [3,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [14,0,0], thread: [4,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [14,0,0], thread: [5,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [14,0,0], thread: [6,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [14,0,0], thread: [7,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [14,0,0], thread: [8,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [14,0,0], thread: [9,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [14,0,0], thread: [10,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [14,0,0], thread: [11,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [14,0,0], thread: [12,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [14,0,0], thread: [13,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [14,0,0], thread: [14,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [14,0,0], thread: [15,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [14,0,0], thread: [16,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [14,0,0], thread: [17,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [14,0,0], thread: [18,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [14,0,0], thread: [19,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [14,0,0], thread: [20,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [14,0,0], thread: [21,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [14,0,0], thread: [22,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [14,0,0], thread: [23,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [14,0,0], thread: [24,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [14,0,0], thread: [25,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [14,0,0], thread: [26,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [14,0,0], thread: [27,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [14,0,0], thread: [28,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [14,0,0], thread: [29,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [14,0,0], thread: [30,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [14,0,0], thread: [31,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [8,0,0], thread: [96,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [8,0,0], thread: [97,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [8,0,0], thread: [98,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [8,0,0], thread: [99,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [8,0,0], thread: [100,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [8,0,0], thread: [101,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [8,0,0], thread: [102,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [8,0,0], thread: [103,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [8,0,0], thread: [104,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [8,0,0], thread: [105,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [8,0,0], thread: [106,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [8,0,0], thread: [107,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [8,0,0], thread: [108,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [8,0,0], thread: [109,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [8,0,0], thread: [110,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [8,0,0], thread: [111,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [8,0,0], thread: [112,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [8,0,0], thread: [113,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [8,0,0], thread: [114,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [8,0,0], thread: [115,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [8,0,0], thread: [116,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [8,0,0], thread: [117,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [8,0,0], thread: [118,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [8,0,0], thread: [119,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [8,0,0], thread: [120,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [8,0,0], thread: [121,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [8,0,0], thread: [122,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [8,0,0], thread: [123,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [8,0,0], thread: [124,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [8,0,0], thread: [125,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [8,0,0], thread: [126,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [8,0,0], thread: [127,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [8,0,0], thread: [0,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [8,0,0], thread: [1,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [8,0,0], thread: [2,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [8,0,0], thread: [3,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [8,0,0], thread: [4,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [8,0,0], thread: [5,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [8,0,0], thread: [6,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [8,0,0], thread: [7,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [8,0,0], thread: [8,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [8,0,0], thread: [9,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [8,0,0], thread: [10,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [8,0,0], thread: [11,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [8,0,0], thread: [12,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [8,0,0], thread: [13,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [8,0,0], thread: [14,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [8,0,0], thread: [15,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [8,0,0], thread: [16,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [8,0,0], thread: [17,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [8,0,0], thread: [18,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [8,0,0], thread: [19,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [8,0,0], thread: [20,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [8,0,0], thread: [21,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [8,0,0], thread: [22,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [8,0,0], thread: [23,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [8,0,0], thread: [24,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [8,0,0], thread: [25,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [8,0,0], thread: [26,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [8,0,0], thread: [27,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [8,0,0], thread: [28,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [8,0,0], thread: [29,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [8,0,0], thread: [30,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [8,0,0], thread: [31,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [4,0,0], thread: [32,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [4,0,0], thread: [33,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [4,0,0], thread: [34,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [4,0,0], thread: [35,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [4,0,0], thread: [36,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [4,0,0], thread: [37,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [4,0,0], thread: [38,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [4,0,0], thread: [39,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [4,0,0], thread: [40,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [4,0,0], thread: [41,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [4,0,0], thread: [42,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [4,0,0], thread: [43,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [4,0,0], thread: [44,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [4,0,0], thread: [45,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [4,0,0], thread: [46,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [4,0,0], thread: [47,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [4,0,0], thread: [48,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [4,0,0], thread: [49,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [4,0,0], thread: [50,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [4,0,0], thread: [51,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [4,0,0], thread: [52,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [4,0,0], thread: [53,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [4,0,0], thread: [54,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [4,0,0], thread: [55,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [4,0,0], thread: [56,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [4,0,0], thread: [57,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [4,0,0], thread: [58,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [4,0,0], thread: [59,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [4,0,0], thread: [60,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [4,0,0], thread: [61,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [4,0,0], thread: [62,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [4,0,0], thread: [63,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [1,0,0], thread: [64,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [1,0,0], thread: [65,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [1,0,0], thread: [66,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [1,0,0], thread: [67,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [1,0,0], thread: [68,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [1,0,0], thread: [69,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [1,0,0], thread: [70,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [1,0,0], thread: [71,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [1,0,0], thread: [72,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [1,0,0], thread: [73,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [1,0,0], thread: [74,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [1,0,0], thread: [75,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [1,0,0], thread: [76,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [1,0,0], thread: [77,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [1,0,0], thread: [78,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [1,0,0], thread: [79,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [1,0,0], thread: [80,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [1,0,0], thread: [81,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [1,0,0], thread: [82,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [1,0,0], thread: [83,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [1,0,0], thread: [84,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [1,0,0], thread: [85,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [1,0,0], thread: [86,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [1,0,0], thread: [87,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [1,0,0], thread: [88,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [1,0,0], thread: [89,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [1,0,0], thread: [90,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [1,0,0], thread: [91,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [1,0,0], thread: [92,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [1,0,0], thread: [93,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [1,0,0], thread: [94,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [1,0,0], thread: [95,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/project-robust-contrastive/Robust-Contrastive-Learning/run_pre_bkd.ipynb Cell 10\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bszw494-root/home/project-robust-contrastive/Robust-Contrastive-Learning/run_pre_bkd.ipynb#X44sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m T \u001b[39m=\u001b[39m \u001b[39m3\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bszw494-root/home/project-robust-contrastive/Robust-Contrastive-Learning/run_pre_bkd.ipynb#X44sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m pred_prob \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39msoftmax(output[unlabeled_idx])\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bszw494-root/home/project-robust-contrastive/Robust-Contrastive-Learning/run_pre_bkd.ipynb#X44sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m pred_prob\u001b[39m.\u001b[39mmax(dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mindices[(pred_prob\u001b[39m.\u001b[39;49mmax(dim\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\u001b[39m.\u001b[39;49mvalues\u001b[39m>\u001b[39;49m\u001b[39m0\u001b[39;49m)\u001b[39m.\u001b[39;49mnonzero()\u001b[39m.\u001b[39mflatten()]\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "T = 3\n",
    "pred_prob = F.softmax(output[unlabeled_idx])\n",
    "pred_prob.max(dim=1).indices[(pred_prob.max(dim=1).values>0).nonzero().flatten()]\n",
    "# mean_pred_prob = torch.mean(F.softmax(output),dim=0)\n",
    "# target_pred_prob = torch.pow(mean_pred_prob,1./T)\n",
    "# target_pred_prob /= torch.mean(target_pred_prob)\n",
    "# target_pred_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated explanations in ['edge_mask', 'node_mask']\n",
      "Feature importance plot has been saved to './figures/cln_feature_importance_1467.png'\n",
      "Subgraph visualization plot has been saved to './figures/cln_subgraph_1467.pdf'\n",
      "Generated explanations in ['edge_mask', 'node_mask']\n",
      "Feature importance plot has been saved to './figures/cln_feature_importance_446.png'\n",
      "Subgraph visualization plot has been saved to './figures/cln_subgraph_446.pdf'\n",
      "Generated explanations in ['edge_mask', 'node_mask']\n",
      "Feature importance plot has been saved to './figures/cln_feature_importance_1332.png'\n",
      "Subgraph visualization plot has been saved to './figures/cln_subgraph_1332.pdf'\n",
      "Generated explanations in ['edge_mask', 'node_mask']\n",
      "Feature importance plot has been saved to './figures/cln_feature_importance_606.png'\n",
      "Subgraph visualization plot has been saved to './figures/cln_subgraph_606.pdf'\n",
      "Generated explanations in ['edge_mask', 'node_mask']\n",
      "Feature importance plot has been saved to './figures/cln_feature_importance_330.png'\n",
      "Subgraph visualization plot has been saved to './figures/cln_subgraph_330.pdf'\n",
      "Generated explanations in ['edge_mask', 'node_mask']\n",
      "Feature importance plot has been saved to './figures/cln_feature_importance_753.png'\n",
      "Subgraph visualization plot has been saved to './figures/cln_subgraph_753.pdf'\n",
      "Generated explanations in ['edge_mask', 'node_mask']\n",
      "Feature importance plot has been saved to './figures/cln_feature_importance_1476.png'\n",
      "Subgraph visualization plot has been saved to './figures/cln_subgraph_1476.pdf'\n",
      "Generated explanations in ['edge_mask', 'node_mask']\n",
      "Feature importance plot has been saved to './figures/cln_feature_importance_759.png'\n",
      "Subgraph visualization plot has been saved to './figures/cln_subgraph_759.pdf'\n",
      "Generated explanations in ['edge_mask', 'node_mask']\n",
      "Feature importance plot has been saved to './figures/cln_feature_importance_1481.png'\n",
      "Subgraph visualization plot has been saved to './figures/cln_subgraph_1481.pdf'\n",
      "Generated explanations in ['edge_mask', 'node_mask']\n",
      "Feature importance plot has been saved to './figures/cln_feature_importance_1564.png'\n",
      "Subgraph visualization plot has been saved to './figures/cln_subgraph_1564.pdf'\n",
      "Generated explanations in ['edge_mask', 'node_mask']\n",
      "Feature importance plot has been saved to './figures/bkd_feature_importance_1467.png'\n",
      "Subgraph visualization plot has been saved to './figures/bkd_subgraph_1467.pdf'\n",
      "Generated explanations in ['edge_mask', 'node_mask']\n",
      "Feature importance plot has been saved to './figures/bkd_feature_importance_446.png'\n",
      "Subgraph visualization plot has been saved to './figures/bkd_subgraph_446.pdf'\n",
      "Generated explanations in ['edge_mask', 'node_mask']\n",
      "Feature importance plot has been saved to './figures/bkd_feature_importance_1332.png'\n",
      "Subgraph visualization plot has been saved to './figures/bkd_subgraph_1332.pdf'\n",
      "Generated explanations in ['edge_mask', 'node_mask']\n",
      "Feature importance plot has been saved to './figures/bkd_feature_importance_606.png'\n",
      "Subgraph visualization plot has been saved to './figures/bkd_subgraph_606.pdf'\n",
      "Generated explanations in ['edge_mask', 'node_mask']\n",
      "Feature importance plot has been saved to './figures/bkd_feature_importance_330.png'\n",
      "Subgraph visualization plot has been saved to './figures/bkd_subgraph_330.pdf'\n",
      "Generated explanations in ['edge_mask', 'node_mask']\n",
      "Feature importance plot has been saved to './figures/bkd_feature_importance_753.png'\n",
      "Subgraph visualization plot has been saved to './figures/bkd_subgraph_753.pdf'\n",
      "Generated explanations in ['edge_mask', 'node_mask']\n",
      "Feature importance plot has been saved to './figures/bkd_feature_importance_1476.png'\n",
      "Subgraph visualization plot has been saved to './figures/bkd_subgraph_1476.pdf'\n",
      "Generated explanations in ['edge_mask', 'node_mask']\n",
      "Feature importance plot has been saved to './figures/bkd_feature_importance_759.png'\n",
      "Subgraph visualization plot has been saved to './figures/bkd_subgraph_759.pdf'\n",
      "Generated explanations in ['edge_mask', 'node_mask']\n",
      "Feature importance plot has been saved to './figures/bkd_feature_importance_1481.png'\n",
      "Subgraph visualization plot has been saved to './figures/bkd_subgraph_1481.pdf'\n",
      "Generated explanations in ['edge_mask', 'node_mask']\n",
      "Feature importance plot has been saved to './figures/bkd_feature_importance_1564.png'\n",
      "Subgraph visualization plot has been saved to './figures/bkd_subgraph_1564.pdf'\n"
     ]
    }
   ],
   "source": [
    "import os.path as osp\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.explain import Explainer, GNNExplainer\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "explainer = Explainer(\n",
    "    model=test_model,\n",
    "    algorithm=GNNExplainer(epochs=200),\n",
    "    explanation_type='model',\n",
    "    node_mask_type='attributes',\n",
    "    edge_mask_type='object',\n",
    "    model_config=dict(\n",
    "        mode='multiclass_classification',\n",
    "        task_level='node',\n",
    "        return_type='log_probs',\n",
    "    ),\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "for i in range(len(idx_attach)):\n",
    "    node_index = idx_attach[i].item()\n",
    "    explanation_cln = explainer(data.x, train_edge_index, index=node_index)\n",
    "    print(f'Generated explanations in {explanation_cln.available_explanations}')\n",
    "\n",
    "    path = './figures/cln_feature_importance_{}.png'.format(node_index)\n",
    "    explanation_cln.visualize_feature_importance(path, top_k=10)\n",
    "    print(f\"Feature importance plot has been saved to '{path}'\")\n",
    "\n",
    "    path = './figures/cln_subgraph_{}.pdf'.format(node_index)\n",
    "    explanation_cln.visualize_graph(path)\n",
    "    print(f\"Subgraph visualization plot has been saved to '{path}'\")\n",
    "for i in range(len(idx_attach)):\n",
    "    node_index = idx_attach[i].item()\n",
    "    explanation_bkd = explainer(poison_x, poison_edge_index, index=node_index)\n",
    "    print(f'Generated explanations in {explanation_bkd.available_explanations}')\n",
    "\n",
    "    path = './figures/bkd_feature_importance_{}.png'.format(node_index)\n",
    "    explanation_bkd.visualize_feature_importance(path, top_k=10)\n",
    "    print(f\"Feature importance plot has been saved to '{path}'\")\n",
    "\n",
    "    path = './figures/bkd_subgraph_{}.pdf'.format(node_index)\n",
    "    explanation_bkd.visualize_graph(path)\n",
    "    print(f\"Subgraph visualization plot has been saved to '{path}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4447],\n",
      "        [4457]], device='cuda:3')\n",
      "tensor([1564, 2735], device='cuda:3') tensor([2735, 1564], device='cuda:3')\n"
     ]
    }
   ],
   "source": [
    "print(explanation_bkd.edge_mask.nonzero())\n",
    "print(explanation_bkd.edge_index[:,4447],explanation_bkd.edge_index[:,4457])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = Explainer(\n",
    "    model=test_model,\n",
    "    algorithm=GNNExplainer(epochs=200),\n",
    "    explanation_type='model',\n",
    "    node_mask_type='attributes',\n",
    "    edge_mask_type='object',\n",
    "    model_config=dict(\n",
    "        mode='multiclass_classification',\n",
    "        task_level='node',\n",
    "        return_type='log_probs',\n",
    "    ),\n",
    ")\n",
    "\n",
    "# node_index = idx_attach[0].item()\n",
    "explanation_cln = explainer(data.x, train_edge_index, index=1332)\n",
    "explanation_bkd = explainer(poison_x, poison_edge_index, index=1332)\n",
    "# print(f'Generated explanations in {explanation.available_explanations}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 653, 1106, 1128, 1135, 1136, 1137, 1138, 1139, 1140, 1141, 1142, 1143,\n",
      "        1144, 1145, 1146, 1272, 1273, 1309, 1632, 1633, 1735, 2107, 2108, 2258,\n",
      "        2259, 2273, 2276, 2277, 2609, 2611, 2868, 3368, 3369, 3842],\n",
      "       device='cuda:3')\n",
      "tensor([0.8975, 0.3870, 0.2195, 0.1435, 0.1484, 0.1446, 0.1448, 0.1502, 0.1387,\n",
      "        0.1463, 0.1435, 0.1470, 0.1430, 0.8873, 0.1480, 0.6948, 0.1476, 0.2151,\n",
      "        0.1445, 0.8789, 0.3935, 0.6912, 0.1440, 0.3895, 0.1461, 0.5392, 0.1432,\n",
      "        0.1433, 0.2793, 0.1439, 0.2056, 0.9169, 0.8846, 0.5285],\n",
      "       device='cuda:3')\n",
      "tensor([[ 249,  424,  433,  436,  436,  436,  436,  436,  436,  436,  436,  436,\n",
      "          436,  436,  436,  483,  483,  498,  621,  621,  668,  816,  816,  878,\n",
      "          878,  885,  885,  885, 1039, 1039, 1131, 1332, 1332, 1494],\n",
      "        [ 621,  436,  436,  424,  433,  483,  498,  668,  816,  878,  885, 1039,\n",
      "         1131, 1332, 1494,  436,  816,  436,  249, 1332,  436,  436,  483,  436,\n",
      "          885,  436,  878, 1039,  436,  885,  436,  436,  621,  436]],\n",
      "       device='cuda:3')\n"
     ]
    }
   ],
   "source": [
    "edge_mask_index = explanation.edge_mask.nonzero().flatten()\n",
    "print(edge_mask_index)\n",
    "print(explanation.edge_mask[edge_mask_index])\n",
    "print(explanation.edge_index[:,edge_mask_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 483  424 1131  621  878 1039  816  433  498  436  885 1332 1494  249\n",
      "  668]\n",
      "tensor([5, 5, 3, 5, 5, 5, 5, 3, 4, 3, 5, 0, 5, 5, 5], device='cuda:3')\n"
     ]
    }
   ],
   "source": [
    "a = np.array(list(set(explanation.edge_index[0,edge_mask_index].cpu().tolist())))\n",
    "print(a)\n",
    "print(poison_labels[a])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 653, 1106, 1128, 1135, 1136, 1137, 1138, 1139, 1140, 1141, 1142, 1143,\n",
      "        1144, 1145, 1146, 1272, 1273, 1309, 1632, 1633, 1735, 2107, 2108, 2258,\n",
      "        2259, 2273, 2276, 2277, 2609, 2611, 2868, 3368, 3369, 3842, 4440, 4450],\n",
      "       device='cuda:3')\n",
      "tensor([0.3871, 0.1555, 0.2587, 0.1460, 0.1461, 0.1468, 0.1463, 0.1437, 0.1456,\n",
      "        0.1464, 0.1495, 0.1438, 0.1454, 0.9041, 0.1492, 0.1828, 0.1456, 0.4570,\n",
      "        0.1451, 0.8974, 0.1843, 0.2488, 0.1445, 0.5494, 0.1452, 0.1949, 0.1413,\n",
      "        0.1451, 0.5756, 0.1435, 0.1741, 0.8342, 0.9003, 0.1696, 0.9068, 0.8778],\n",
      "       device='cuda:3')\n",
      "tensor([[ 249,  424,  433,  436,  436,  436,  436,  436,  436,  436,  436,  436,\n",
      "          436,  436,  436,  483,  483,  498,  621,  621,  668,  816,  816,  878,\n",
      "          878,  885,  885,  885, 1039, 1039, 1131, 1332, 1332, 1494, 1332, 2714],\n",
      "        [ 621,  436,  436,  424,  433,  483,  498,  668,  816,  878,  885, 1039,\n",
      "         1131, 1332, 1494,  436,  816,  436,  249, 1332,  436,  436,  483,  436,\n",
      "          885,  436,  878, 1039,  436,  885,  436,  436,  621,  436, 2714, 1332]],\n",
      "       device='cuda:3')\n"
     ]
    }
   ],
   "source": [
    "edge_mask_index = explanation_bkd.edge_mask.nonzero().flatten()\n",
    "print(edge_mask_index)\n",
    "print(explanation_bkd.edge_mask[edge_mask_index])\n",
    "print(explanation_bkd.edge_index[:,edge_mask_index])\n",
    "# explanation.edge_mask[edge_mask_index[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_36324/938229985.py:4: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  output = F.softmax(test_model.clf_head(test_embds))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.5963, device='cuda:2')\n",
      "[0.37481055 0.37481055 1.         0.37481055 0.37481055 1.\n",
      " 0.37481055 0.37481055 1.         0.37481055 0.37481055 0.9925921\n",
      " 0.37481055 0.37481055 1.        ]\n",
      "tensor([[  61],\n",
      "        [  76],\n",
      "        [  90],\n",
      "        [ 140],\n",
      "        [ 155],\n",
      "        [ 156],\n",
      "        [ 161],\n",
      "        [ 162],\n",
      "        [ 164],\n",
      "        [ 170],\n",
      "        [ 184],\n",
      "        [ 191],\n",
      "        [ 209],\n",
      "        [ 211],\n",
      "        [ 221],\n",
      "        [ 244],\n",
      "        [ 266],\n",
      "        [ 268],\n",
      "        [ 272],\n",
      "        [ 273],\n",
      "        [ 274],\n",
      "        [ 277],\n",
      "        [ 285],\n",
      "        [ 289],\n",
      "        [ 300],\n",
      "        [ 304],\n",
      "        [ 305],\n",
      "        [ 314],\n",
      "        [ 323],\n",
      "        [ 324],\n",
      "        [ 331],\n",
      "        [ 340],\n",
      "        [ 342],\n",
      "        [ 355],\n",
      "        [ 356],\n",
      "        [ 359],\n",
      "        [ 363],\n",
      "        [ 367],\n",
      "        [ 374],\n",
      "        [ 376],\n",
      "        [ 382],\n",
      "        [ 389],\n",
      "        [ 407],\n",
      "        [ 413],\n",
      "        [ 414],\n",
      "        [ 424],\n",
      "        [ 428],\n",
      "        [ 432],\n",
      "        [ 437],\n",
      "        [ 446],\n",
      "        [ 461],\n",
      "        [ 462],\n",
      "        [ 463],\n",
      "        [ 464],\n",
      "        [ 471],\n",
      "        [ 489],\n",
      "        [ 490],\n",
      "        [ 496],\n",
      "        [ 498],\n",
      "        [ 502],\n",
      "        [ 511],\n",
      "        [ 515],\n",
      "        [ 517],\n",
      "        [ 520],\n",
      "        [ 523],\n",
      "        [ 524],\n",
      "        [ 526],\n",
      "        [ 538],\n",
      "        [ 548],\n",
      "        [ 552],\n",
      "        [ 556],\n",
      "        [ 558],\n",
      "        [ 562],\n",
      "        [ 564],\n",
      "        [ 566],\n",
      "        [ 572],\n",
      "        [ 578],\n",
      "        [ 581],\n",
      "        [ 584],\n",
      "        [ 603],\n",
      "        [ 616],\n",
      "        [ 626],\n",
      "        [ 630],\n",
      "        [ 650],\n",
      "        [ 653],\n",
      "        [ 658],\n",
      "        [ 666],\n",
      "        [ 672],\n",
      "        [ 682],\n",
      "        [ 684],\n",
      "        [ 685],\n",
      "        [ 696],\n",
      "        [ 704],\n",
      "        [ 709],\n",
      "        [ 728],\n",
      "        [ 741],\n",
      "        [ 742],\n",
      "        [ 748],\n",
      "        [ 749],\n",
      "        [ 750],\n",
      "        [ 754],\n",
      "        [ 755],\n",
      "        [ 771],\n",
      "        [ 778],\n",
      "        [ 779],\n",
      "        [ 790],\n",
      "        [ 809],\n",
      "        [ 817],\n",
      "        [ 827],\n",
      "        [ 828],\n",
      "        [ 831],\n",
      "        [ 837],\n",
      "        [ 846],\n",
      "        [ 859],\n",
      "        [ 860],\n",
      "        [ 865],\n",
      "        [ 867],\n",
      "        [ 868],\n",
      "        [ 870],\n",
      "        [ 873],\n",
      "        [ 880],\n",
      "        [ 882],\n",
      "        [ 896],\n",
      "        [ 897],\n",
      "        [ 903],\n",
      "        [ 905],\n",
      "        [ 915],\n",
      "        [ 925],\n",
      "        [ 929],\n",
      "        [ 936],\n",
      "        [ 939],\n",
      "        [ 943],\n",
      "        [ 945],\n",
      "        [ 948],\n",
      "        [ 949],\n",
      "        [ 952],\n",
      "        [ 969],\n",
      "        [ 982],\n",
      "        [ 992],\n",
      "        [ 995],\n",
      "        [ 996],\n",
      "        [1003],\n",
      "        [1007],\n",
      "        [1017],\n",
      "        [1029],\n",
      "        [1030],\n",
      "        [1035],\n",
      "        [1037],\n",
      "        [1048],\n",
      "        [1060],\n",
      "        [1066],\n",
      "        [1070],\n",
      "        [1071],\n",
      "        [1077],\n",
      "        [1078],\n",
      "        [1079],\n",
      "        [1084],\n",
      "        [1085],\n",
      "        [1088],\n",
      "        [1091],\n",
      "        [1101],\n",
      "        [1102],\n",
      "        [1105],\n",
      "        [1110],\n",
      "        [1111],\n",
      "        [1115],\n",
      "        [1120],\n",
      "        [1125],\n",
      "        [1135],\n",
      "        [1136],\n",
      "        [1144],\n",
      "        [1145],\n",
      "        [1157],\n",
      "        [1172],\n",
      "        [1196],\n",
      "        [1203],\n",
      "        [1206],\n",
      "        [1207],\n",
      "        [1213],\n",
      "        [1216],\n",
      "        [1218],\n",
      "        [1222],\n",
      "        [1224],\n",
      "        [1231],\n",
      "        [1249],\n",
      "        [1259],\n",
      "        [1273],\n",
      "        [1275],\n",
      "        [1277],\n",
      "        [1282],\n",
      "        [1285],\n",
      "        [1286],\n",
      "        [1306],\n",
      "        [1329],\n",
      "        [1333],\n",
      "        [1338],\n",
      "        [1341],\n",
      "        [1357],\n",
      "        [1365],\n",
      "        [1367],\n",
      "        [1374],\n",
      "        [1384],\n",
      "        [1389],\n",
      "        [1394],\n",
      "        [1399],\n",
      "        [1411],\n",
      "        [1412],\n",
      "        [1420],\n",
      "        [1421],\n",
      "        [1422],\n",
      "        [1427],\n",
      "        [1434],\n",
      "        [1444],\n",
      "        [1451],\n",
      "        [1453],\n",
      "        [1458],\n",
      "        [1463],\n",
      "        [1468],\n",
      "        [1469],\n",
      "        [1483],\n",
      "        [1486],\n",
      "        [1492],\n",
      "        [1494],\n",
      "        [1498],\n",
      "        [1512],\n",
      "        [1523],\n",
      "        [1525],\n",
      "        [1526],\n",
      "        [1527],\n",
      "        [1548],\n",
      "        [1550],\n",
      "        [1564],\n",
      "        [1565],\n",
      "        [1568],\n",
      "        [1571],\n",
      "        [1574],\n",
      "        [1583],\n",
      "        [1584],\n",
      "        [1587],\n",
      "        [1592],\n",
      "        [1606],\n",
      "        [1610],\n",
      "        [1617],\n",
      "        [1620],\n",
      "        [1625],\n",
      "        [1638],\n",
      "        [1644],\n",
      "        [1652],\n",
      "        [1655],\n",
      "        [1656],\n",
      "        [1661],\n",
      "        [1668],\n",
      "        [1677],\n",
      "        [1682],\n",
      "        [1689],\n",
      "        [1697],\n",
      "        [1700],\n",
      "        [1710],\n",
      "        [1714],\n",
      "        [1724],\n",
      "        [1729],\n",
      "        [1731],\n",
      "        [1741],\n",
      "        [1751],\n",
      "        [1758],\n",
      "        [1767],\n",
      "        [1776],\n",
      "        [1791],\n",
      "        [1792],\n",
      "        [1800],\n",
      "        [1803],\n",
      "        [1804],\n",
      "        [1816],\n",
      "        [1830],\n",
      "        [1834],\n",
      "        [1840],\n",
      "        [1843],\n",
      "        [1848],\n",
      "        [1851],\n",
      "        [1857],\n",
      "        [1870],\n",
      "        [1879],\n",
      "        [1886],\n",
      "        [1888],\n",
      "        [1895],\n",
      "        [1897],\n",
      "        [1908],\n",
      "        [1910],\n",
      "        [1911],\n",
      "        [1915],\n",
      "        [1916],\n",
      "        [1921],\n",
      "        [1923],\n",
      "        [1926],\n",
      "        [1929],\n",
      "        [1938],\n",
      "        [1961],\n",
      "        [1972],\n",
      "        [1975],\n",
      "        [1979],\n",
      "        [1983],\n",
      "        [1998],\n",
      "        [2010],\n",
      "        [2016],\n",
      "        [2019],\n",
      "        [2022],\n",
      "        [2028],\n",
      "        [2030],\n",
      "        [2042],\n",
      "        [2047],\n",
      "        [2076],\n",
      "        [2092],\n",
      "        [2102],\n",
      "        [2103],\n",
      "        [2113],\n",
      "        [2116],\n",
      "        [2130],\n",
      "        [2137],\n",
      "        [2170],\n",
      "        [2174],\n",
      "        [2181],\n",
      "        [2190],\n",
      "        [2194],\n",
      "        [2195],\n",
      "        [2201],\n",
      "        [2221],\n",
      "        [2224],\n",
      "        [2232],\n",
      "        [2235],\n",
      "        [2240],\n",
      "        [2253],\n",
      "        [2258],\n",
      "        [2261],\n",
      "        [2265],\n",
      "        [2266],\n",
      "        [2275],\n",
      "        [2276],\n",
      "        [2277],\n",
      "        [2288],\n",
      "        [2290],\n",
      "        [2291],\n",
      "        [2299],\n",
      "        [2326],\n",
      "        [2332],\n",
      "        [2334],\n",
      "        [2342],\n",
      "        [2360],\n",
      "        [2390],\n",
      "        [2393],\n",
      "        [2396],\n",
      "        [2397],\n",
      "        [2399],\n",
      "        [2405],\n",
      "        [2413],\n",
      "        [2419],\n",
      "        [2421],\n",
      "        [2431],\n",
      "        [2432],\n",
      "        [2438],\n",
      "        [2464],\n",
      "        [2467],\n",
      "        [2468],\n",
      "        [2470],\n",
      "        [2472],\n",
      "        [2478],\n",
      "        [2482],\n",
      "        [2486],\n",
      "        [2489],\n",
      "        [2498],\n",
      "        [2525],\n",
      "        [2546],\n",
      "        [2556],\n",
      "        [2564],\n",
      "        [2569],\n",
      "        [2579],\n",
      "        [2581],\n",
      "        [2589],\n",
      "        [2596],\n",
      "        [2611],\n",
      "        [2617],\n",
      "        [2630],\n",
      "        [2647],\n",
      "        [2649],\n",
      "        [2652],\n",
      "        [2679],\n",
      "        [2690],\n",
      "        [2702],\n",
      "        [2705]], device='cuda:2')\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "Could not run 'aten::values' with arguments from the 'CUDA' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::values' is only available for these backends: [FuncTorchGradWrapper, Functionalize, MPS, IPU, UNKNOWN_TENSOR_TYPE_ID, QuantizedXPU, UNKNOWN_TENSOR_TYPE_ID, UNKNOWN_TENSOR_TYPE_ID, UNKNOWN_TENSOR_TYPE_ID, UNKNOWN_TENSOR_TYPE_ID, UNKNOWN_TENSOR_TYPE_ID, SparseCPU, SparseCUDA, SparseHIP, UNKNOWN_TENSOR_TYPE_ID, UNKNOWN_TENSOR_TYPE_ID, UNKNOWN_TENSOR_TYPE_ID, SparseXPU, UNKNOWN_TENSOR_TYPE_ID, SparseVE, UNKNOWN_TENSOR_TYPE_ID, UNKNOWN_TENSOR_TYPE_ID, UNKNOWN_TENSOR_TYPE_ID, UNKNOWN_TENSOR_TYPE_ID, UNKNOWN_TENSOR_TYPE_ID, NestedTensorCUDA, UNKNOWN_TENSOR_TYPE_ID, UNKNOWN_TENSOR_TYPE_ID, UNKNOWN_TENSOR_TYPE_ID, UNKNOWN_TENSOR_TYPE_ID, UNKNOWN_TENSOR_TYPE_ID, UNKNOWN_TENSOR_TYPE_ID].\n\nSparseCPU: registered at /opt/conda/conda-bld/pytorch_1659484810403/work/build/aten/src/ATen/RegisterSparseCPU.cpp:1858 [kernel]\nSparseCUDA: registered at /opt/conda/conda-bld/pytorch_1659484810403/work/build/aten/src/ATen/RegisterSparseCUDA.cpp:2018 [kernel]\nSparseCsrCPU: registered at /opt/conda/conda-bld/pytorch_1659484810403/work/build/aten/src/ATen/RegisterSparseCsrCPU.cpp:1507 [kernel]\nSparseCsrCUDA: registered at /opt/conda/conda-bld/pytorch_1659484810403/work/build/aten/src/ATen/RegisterSparseCsrCUDA.cpp:1657 [kernel]\nBackendSelect: fallthrough registered at /opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/core/PythonFallbackKernel.cpp:133 [backend fallback]\nNamed: registered at /opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /opt/conda/conda-bld/pytorch_1659484810403/work/torch/csrc/autograd/generated/ADInplaceOrViewType_0.cpp:3016 [kernel]\nAutogradOther: registered at /opt/conda/conda-bld/pytorch_1659484810403/work/torch/csrc/autograd/generated/VariableType_0.cpp:11935 [autograd kernel]\nAutogradCPU: registered at /opt/conda/conda-bld/pytorch_1659484810403/work/torch/csrc/autograd/generated/VariableType_0.cpp:11935 [autograd kernel]\nAutogradCUDA: registered at /opt/conda/conda-bld/pytorch_1659484810403/work/torch/csrc/autograd/generated/VariableType_0.cpp:11935 [autograd kernel]\nUNKNOWN_TENSOR_TYPE_ID: registered at /opt/conda/conda-bld/pytorch_1659484810403/work/torch/csrc/autograd/generated/VariableType_0.cpp:11935 [autograd kernel]\nAutogradXLA: registered at /opt/conda/conda-bld/pytorch_1659484810403/work/torch/csrc/autograd/generated/VariableType_0.cpp:11935 [autograd kernel]\nAutogradMPS: registered at /opt/conda/conda-bld/pytorch_1659484810403/work/torch/csrc/autograd/generated/VariableType_0.cpp:11935 [autograd kernel]\nAutogradIPU: registered at /opt/conda/conda-bld/pytorch_1659484810403/work/torch/csrc/autograd/generated/VariableType_0.cpp:11935 [autograd kernel]\nAutogradXPU: registered at /opt/conda/conda-bld/pytorch_1659484810403/work/torch/csrc/autograd/generated/VariableType_0.cpp:11935 [autograd kernel]\nAutogradHPU: registered at /opt/conda/conda-bld/pytorch_1659484810403/work/torch/csrc/autograd/generated/VariableType_0.cpp:11935 [autograd kernel]\nUNKNOWN_TENSOR_TYPE_ID: registered at /opt/conda/conda-bld/pytorch_1659484810403/work/torch/csrc/autograd/generated/VariableType_0.cpp:11935 [autograd kernel]\nAutogradLazy: registered at /opt/conda/conda-bld/pytorch_1659484810403/work/torch/csrc/autograd/generated/VariableType_0.cpp:11935 [autograd kernel]\nAutogradPrivateUse1: registered at /opt/conda/conda-bld/pytorch_1659484810403/work/torch/csrc/autograd/generated/VariableType_0.cpp:11935 [autograd kernel]\nAutogradPrivateUse2: registered at /opt/conda/conda-bld/pytorch_1659484810403/work/torch/csrc/autograd/generated/VariableType_0.cpp:11935 [autograd kernel]\nAutogradPrivateUse3: registered at /opt/conda/conda-bld/pytorch_1659484810403/work/torch/csrc/autograd/generated/VariableType_0.cpp:11935 [autograd kernel]\nTracer: registered at /opt/conda/conda-bld/pytorch_1659484810403/work/torch/csrc/autograd/generated/TraceType_0.cpp:13506 [kernel]\nAutocastCPU: fallthrough registered at /opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/autocast_mode.cpp:481 [backend fallback]\nAutocast: fallthrough registered at /opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nBatched: registered at /opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFunctionalize: registered at /opt/conda/conda-bld/pytorch_1659484810403/work/build/aten/src/ATen/RegisterFunctionalization_0.cpp:10912 [kernel]\nPythonTLSSnapshot: registered at /opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/core/PythonFallbackKernel.cpp:137 [backend fallback]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/home/project-robust-contrastive/Robust-Contrastive-Learning/run_pre_bkd.ipynb Cell 16\u001b[0m in \u001b[0;36m<cell line: 14>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bszw494-root/home/project-robust-contrastive/Robust-Contrastive-Learning/run_pre_bkd.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39mprint\u001b[39m((output\u001b[39m.\u001b[39mmax(dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mvalues\u001b[39m<\u001b[39m\u001b[39m0.3\u001b[39m)\u001b[39m.\u001b[39mnonzero())\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bszw494-root/home/project-robust-contrastive/Robust-Contrastive-Learning/run_pre_bkd.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39m# print(output.argmax(dim=1).flip(dims=[0]))\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bszw494-root/home/project-robust-contrastive/Robust-Contrastive-Learning/run_pre_bkd.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39mprint\u001b[39m(output\u001b[39m.\u001b[39;49mvalues())\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bszw494-root/home/project-robust-contrastive/Robust-Contrastive-Learning/run_pre_bkd.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m idx_attach:\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bszw494-root/home/project-robust-contrastive/Robust-Contrastive-Learning/run_pre_bkd.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m+=====\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: Could not run 'aten::values' with arguments from the 'CUDA' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::values' is only available for these backends: [FuncTorchGradWrapper, Functionalize, MPS, IPU, UNKNOWN_TENSOR_TYPE_ID, QuantizedXPU, UNKNOWN_TENSOR_TYPE_ID, UNKNOWN_TENSOR_TYPE_ID, UNKNOWN_TENSOR_TYPE_ID, UNKNOWN_TENSOR_TYPE_ID, UNKNOWN_TENSOR_TYPE_ID, SparseCPU, SparseCUDA, SparseHIP, UNKNOWN_TENSOR_TYPE_ID, UNKNOWN_TENSOR_TYPE_ID, UNKNOWN_TENSOR_TYPE_ID, SparseXPU, UNKNOWN_TENSOR_TYPE_ID, SparseVE, UNKNOWN_TENSOR_TYPE_ID, UNKNOWN_TENSOR_TYPE_ID, UNKNOWN_TENSOR_TYPE_ID, UNKNOWN_TENSOR_TYPE_ID, UNKNOWN_TENSOR_TYPE_ID, NestedTensorCUDA, UNKNOWN_TENSOR_TYPE_ID, UNKNOWN_TENSOR_TYPE_ID, UNKNOWN_TENSOR_TYPE_ID, UNKNOWN_TENSOR_TYPE_ID, UNKNOWN_TENSOR_TYPE_ID, UNKNOWN_TENSOR_TYPE_ID].\n\nSparseCPU: registered at /opt/conda/conda-bld/pytorch_1659484810403/work/build/aten/src/ATen/RegisterSparseCPU.cpp:1858 [kernel]\nSparseCUDA: registered at /opt/conda/conda-bld/pytorch_1659484810403/work/build/aten/src/ATen/RegisterSparseCUDA.cpp:2018 [kernel]\nSparseCsrCPU: registered at /opt/conda/conda-bld/pytorch_1659484810403/work/build/aten/src/ATen/RegisterSparseCsrCPU.cpp:1507 [kernel]\nSparseCsrCUDA: registered at /opt/conda/conda-bld/pytorch_1659484810403/work/build/aten/src/ATen/RegisterSparseCsrCUDA.cpp:1657 [kernel]\nBackendSelect: fallthrough registered at /opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/core/PythonFallbackKernel.cpp:133 [backend fallback]\nNamed: registered at /opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /opt/conda/conda-bld/pytorch_1659484810403/work/torch/csrc/autograd/generated/ADInplaceOrViewType_0.cpp:3016 [kernel]\nAutogradOther: registered at /opt/conda/conda-bld/pytorch_1659484810403/work/torch/csrc/autograd/generated/VariableType_0.cpp:11935 [autograd kernel]\nAutogradCPU: registered at /opt/conda/conda-bld/pytorch_1659484810403/work/torch/csrc/autograd/generated/VariableType_0.cpp:11935 [autograd kernel]\nAutogradCUDA: registered at /opt/conda/conda-bld/pytorch_1659484810403/work/torch/csrc/autograd/generated/VariableType_0.cpp:11935 [autograd kernel]\nUNKNOWN_TENSOR_TYPE_ID: registered at /opt/conda/conda-bld/pytorch_1659484810403/work/torch/csrc/autograd/generated/VariableType_0.cpp:11935 [autograd kernel]\nAutogradXLA: registered at /opt/conda/conda-bld/pytorch_1659484810403/work/torch/csrc/autograd/generated/VariableType_0.cpp:11935 [autograd kernel]\nAutogradMPS: registered at /opt/conda/conda-bld/pytorch_1659484810403/work/torch/csrc/autograd/generated/VariableType_0.cpp:11935 [autograd kernel]\nAutogradIPU: registered at /opt/conda/conda-bld/pytorch_1659484810403/work/torch/csrc/autograd/generated/VariableType_0.cpp:11935 [autograd kernel]\nAutogradXPU: registered at /opt/conda/conda-bld/pytorch_1659484810403/work/torch/csrc/autograd/generated/VariableType_0.cpp:11935 [autograd kernel]\nAutogradHPU: registered at /opt/conda/conda-bld/pytorch_1659484810403/work/torch/csrc/autograd/generated/VariableType_0.cpp:11935 [autograd kernel]\nUNKNOWN_TENSOR_TYPE_ID: registered at /opt/conda/conda-bld/pytorch_1659484810403/work/torch/csrc/autograd/generated/VariableType_0.cpp:11935 [autograd kernel]\nAutogradLazy: registered at /opt/conda/conda-bld/pytorch_1659484810403/work/torch/csrc/autograd/generated/VariableType_0.cpp:11935 [autograd kernel]\nAutogradPrivateUse1: registered at /opt/conda/conda-bld/pytorch_1659484810403/work/torch/csrc/autograd/generated/VariableType_0.cpp:11935 [autograd kernel]\nAutogradPrivateUse2: registered at /opt/conda/conda-bld/pytorch_1659484810403/work/torch/csrc/autograd/generated/VariableType_0.cpp:11935 [autograd kernel]\nAutogradPrivateUse3: registered at /opt/conda/conda-bld/pytorch_1659484810403/work/torch/csrc/autograd/generated/VariableType_0.cpp:11935 [autograd kernel]\nTracer: registered at /opt/conda/conda-bld/pytorch_1659484810403/work/torch/csrc/autograd/generated/TraceType_0.cpp:13506 [kernel]\nAutocastCPU: fallthrough registered at /opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/autocast_mode.cpp:481 [backend fallback]\nAutocast: fallthrough registered at /opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nBatched: registered at /opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFunctionalize: registered at /opt/conda/conda-bld/pytorch_1659484810403/work/build/aten/src/ATen/RegisterFunctionalization_0.cpp:10912 [kernel]\nPythonTLSSnapshot: registered at /opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/core/PythonFallbackKernel.cpp:137 [backend fallback]\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch_geometric.utils  import k_hop_subgraph\n",
    "test_embds = test_model(poison_x,poison_edge_index,poison_edge_weights)\n",
    "output = F.softmax(test_model.clf_head(test_embds))\n",
    "# print(output[idx_attach])\n",
    "# print(poison_labels[idx_attach])\n",
    "# print(data.y[idx_attach])\n",
    "unlabeled_idx\n",
    "up = (output.argmax(dim=1)[unlabeled_idx]==data.y[unlabeled_idx]).float().mean()\n",
    "print(up)\n",
    "print((output.max(dim=1).values).cpu().detach().numpy()[::-1][:15])\n",
    "print((output.max(dim=1).values<0.3).nonzero())\n",
    "# print(output.argmax(dim=1).flip(dims=[0]))\n",
    "print(output.values())\n",
    "\n",
    "for idx in idx_attach:\n",
    "    print(\"+=====\")\n",
    "    sub_induct_nodeset, sub_induct_edge_index, sub_mapping, sub_edge_mask = k_hop_subgraph(node_idx = [idx], num_hops = 2, edge_index = poison_edge_index, relabel_nodes=False) # sub_mapping means the index of [idx] in sub)nodeset\n",
    "    # print(sub_induct_nodeset, sub_induct_edge_index, sub_mapping, sub_edge_mask)\n",
    "    # print(sub_induct_nodeset[:-1:])\n",
    "    up = (output.argmax(dim=1)[sub_induct_nodeset[:-1:]]==data.y[sub_induct_nodeset[:-1:]]).float().mean()\n",
    "    print(up)\n",
    "    up = output.max(dim=1).values[sub_induct_nodeset[:-1:]]\n",
    "    print(up)\n",
    "    print(data.y[sub_induct_nodeset[:-1:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "test_embds = test_model(poison_x,poison_edge_index,poison_edge_weights)\n",
    "output = test_model.clf_head(test_embds)\n",
    "# torch.var(output,unbiased = False)\n",
    "sorted_pred =torch.argsort(output,dim=1,descending=False)\n",
    "# print(output[0])\n",
    "fst_prob = output[:,sorted_pred[:,0]][:,0].unsqueeze(dim=1)\n",
    "\n",
    "# print(fst_prob)\n",
    "# print(torch.concat([fst_prob,fst_prob],dim=1))\n",
    "\n",
    "pred1 = torch.argmax(output,dim=1)\n",
    "print(pred1)\n",
    "print(F.cross_entropy(output,pred1))\n",
    "# sec_prob = output[:,sorted_pred[:,1]][:,0]\n",
    "# loss = (fst_prob - sec_prob)[[1,2,3]].mean()\n",
    "# print(output.shape)\n",
    "# print(sorted_pred[:,0].shape)\n",
    "# print(fst_prob.shape)\n",
    "# print(output)\n",
    "# print(output[0,1])\n",
    "# print(fst_prob)\n",
    "# print(fst_prob[:,0])\n",
    "# loss.data = torch.tensor(0).to(device)\n",
    "# output\n",
    "# output = self.clf_head(z)\n",
    "# fst_prob = torch.argsort(output,dim=0)[0]\n",
    "# sec_prob = torch.argsort(output,dim=0)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Backdoor attack to GNN classifier'''\n",
    "from models.GTA import Backdoor\n",
    "import heuristic_selection as hs\n",
    "from models.GCN import GCN\n",
    "from models.construct import model_construct\n",
    "from construct_graph import *\n",
    "# args.homo_loss_weight=config['homo_loss_weight']\n",
    "# args.vs_number=config['vs_number']\n",
    "# args.trojan_epochs = config['trojan_epochs']\n",
    "\n",
    "data = data.to(device)\n",
    "num_class = int(data.y.max()+1)\n",
    "# args.seed = config['seed']\n",
    "\n",
    "size = args.vs_number #int((len(data.test_mask)-data.test_mask.sum())*args.vs_ratio)\n",
    "print(\"#Attach Nodes:{}\".format(size))\n",
    "from models.construct import model_construct\n",
    "result_asr = []\n",
    "result_acc = []\n",
    "data = data.to(device)\n",
    "\n",
    "noisy_data = construct_noisy_graph(data,perturb_ratio=0.1,mode='random_noise')\n",
    "data = noisy_data.to(device)\n",
    "rs = np.random.RandomState(args.seed)\n",
    "seeds = rs.randint(1000,size=3)\n",
    "# seeds = [args.seed]\n",
    "for seed in seeds:\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    args.seed = seed\n",
    "    if(args.selection_method == 'none'):\n",
    "        idx_attach = hs.obtain_attach_nodes(args,unlabeled_idx,size)\n",
    "    elif(args.selection_method == 'cluster'):\n",
    "        idx_attach = hs.cluster_distance_selection(args,data,idx_train,idx_val,idx_clean_test,unlabeled_idx,train_edge_index,size,device)\n",
    "        idx_attach = torch.LongTensor(idx_attach).to(device)\n",
    "    elif(args.selection_method == 'cluster_degree'):\n",
    "        idx_attach = hs.cluster_degree_selection(args,data,idx_train,idx_val,idx_clean_test,unlabeled_idx,train_edge_index,size,device)\n",
    "        idx_attach = torch.LongTensor(idx_attach).to(device)\n",
    "    # train trigger generator \n",
    "    model = Backdoor(args,device)\n",
    "    model.fit(data.x, train_edge_index, None, data.y, idx_train,idx_attach, unlabeled_idx)\n",
    "    poison_x, poison_edge_index, poison_edge_weights, poison_labels = model.get_poisoned()\n",
    "    bkd_tn_nodes = torch.cat([idx_train,idx_attach]).to(device)\n",
    "    # # learn contrastive node representation\n",
    "    # encoder = Encoder(dataset.num_features, num_hidden, activation,\n",
    "    #                     base_model=base_model, k=num_layers).to(device)\n",
    "    # contrastive_model = Model(encoder, num_hidden, num_proj_hidden, tau).to(device)\n",
    "    # optimizer = torch.optim.Adam(\n",
    "    #     contrastive_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    \n",
    "    # start = t()\n",
    "    # prev = start\n",
    "    # for epoch in range(1, num_epochs + 1):\n",
    "    #     # loss = train_(model, data.x, data.edge_index)\n",
    "    #     loss = train_1(contrastive_model, poison_x, poison_edge_index, poison_edge_weights)\n",
    "\n",
    "    #     now = t()\n",
    "    #     if(epoch%10 == 0):\n",
    "    #         print(f'(T) | Epoch={epoch:03d}, loss={loss:.4f}, '\n",
    "    #                 f'this epoch {now - prev:.4f}, total {now - start:.4f}')\n",
    "    #     prev = now\n",
    "\n",
    "    # contrastive_model.eval()\n",
    "    # cont_poison_x = contrastive_model(poison_x, poison_edge_index, poison_edge_weights).detach().to(device)\n",
    "\n",
    "    print(\"precent of left attach nodes: {:.3f}\"\\\n",
    "        .format(len(set(bkd_tn_nodes.tolist()) & set(idx_attach.tolist()))/len(idx_attach)))\n",
    "    #%%\n",
    "\n",
    "    # model = GCN(nfeat=data.x.shape[1],\\\n",
    "    #             nhid=args.hidden,\\\n",
    "    #             nclass= int(data.y.max()+1),\\\n",
    "    #             dropout=args.dropout,\\\n",
    "    #             lr=args.train_lr,\\\n",
    "    #             weight_decay=args.weight_decay,\\\n",
    "    #             device=device)\n",
    "    test_model = model_construct(args,args.test_model,data,device).to(device) \n",
    "    # test_model = GCN(nfeat=cont_poison_x.shape[1],\\\n",
    "    #             nhid=args.hidden,\\\n",
    "    #             nclass= int(data.y.max()+1),\\\n",
    "    #             dropout=args.dropout,\\\n",
    "    #             lr=args.train_lr,\\\n",
    "    #             weight_decay=args.weight_decay,\\\n",
    "    #             device=device).to(device) \n",
    "    test_model.fit(poison_x, poison_edge_index, poison_edge_weights, poison_labels, bkd_tn_nodes, idx_val,train_iters=args.epochs,verbose=False)\n",
    "    test_model.eval()\n",
    "    output = test_model(poison_x,poison_edge_index,poison_edge_weights)\n",
    "    train_attach_rate = (output.argmax(dim=1)[idx_attach]==args.target_class).float().mean()\n",
    "    print(\"target class rate on Vs: {:.4f}\".format(train_attach_rate))\n",
    "    torch.cuda.empty_cache()\n",
    "    #%%\n",
    "    induct_edge_index = torch.cat([poison_edge_index,mask_edge_index],dim=1)\n",
    "    induct_edge_weights = torch.cat([poison_edge_weights,torch.ones([mask_edge_index.shape[1]],dtype=torch.float,device=device)])\n",
    "    clean_acc = test_model.test(poison_x,induct_edge_index,induct_edge_weights,data.y,idx_clean_test)\n",
    "    # test_model = test_model.cpu()\n",
    "\n",
    "    print(\"accuracy on clean test nodes: {:.4f}\".format(clean_acc))\n",
    "\n",
    "\n",
    "    if(args.evaluate_mode == '1by1'):\n",
    "        from torch_geometric.utils  import k_hop_subgraph\n",
    "        overall_induct_edge_index, overall_induct_edge_weights = induct_edge_index.clone(),induct_edge_weights.clone()\n",
    "        asr = 0\n",
    "        flip_asr = 0\n",
    "        flip_idx_atk = idx_atk[(data.y[idx_atk] != args.target_class).nonzero().flatten()]\n",
    "        for i, idx in enumerate(idx_atk):\n",
    "            idx=int(idx)\n",
    "            sub_induct_nodeset, sub_induct_edge_index, sub_mapping, sub_edge_mask  = k_hop_subgraph(node_idx = [idx], num_hops = 2, edge_index = overall_induct_edge_index, relabel_nodes=True) # sub_mapping means the index of [idx] in sub)nodeset\n",
    "            ori_node_idx = sub_induct_nodeset[sub_mapping]\n",
    "            relabeled_node_idx = sub_mapping\n",
    "            sub_induct_edge_weights = torch.ones([sub_induct_edge_index.shape[1]]).to(device)\n",
    "            # inject trigger on attack test nodes (idx_atk)'''\n",
    "            with torch.no_grad():\n",
    "                induct_x, induct_edge_index,induct_edge_weights = model.inject_trigger(relabeled_node_idx,poison_x[sub_induct_nodeset],sub_induct_edge_index,sub_induct_edge_weights,device)\n",
    "                induct_x, induct_edge_index,induct_edge_weights = induct_x.clone().detach(), induct_edge_index.clone().detach(),induct_edge_weights.clone().detach()\n",
    "                # cont_induct_x = contrastive_model(induct_x, induct_edge_index,induct_edge_weights).detach().to(device)\n",
    "                # # do pruning in test datas'''\n",
    "                # if(args.defense_mode == 'prune' or args.defense_mode == 'isolate'):\n",
    "                #     induct_edge_index,induct_edge_weights = prune_unrelated_edge(args,induct_edge_index,induct_edge_weights,induct_x,device,large_graph=False)\n",
    "                # attack evaluation\n",
    "\n",
    "                output = test_model(induct_x,induct_edge_index,induct_edge_weights)\n",
    "                train_attach_rate = (output.argmax(dim=1)[relabeled_node_idx]==args.target_class).float().mean()\n",
    "                # print(\"Node {}: {}, Origin Label: {}\".format(i, idx, data.y[idx]))\n",
    "                # print(\"ASR: {:.4f}\".format(train_attach_rate))\n",
    "                asr += train_attach_rate\n",
    "                if(data.y[idx] != args.target_class):\n",
    "                    flip_asr += train_attach_rate\n",
    "        asr = asr/(idx_atk.shape[0])\n",
    "        flip_asr = flip_asr/(flip_idx_atk.shape[0])\n",
    "        print(\"Overall ASR: {:.4f}\".format(asr))\n",
    "        print(\"Flip ASR: {:.4f}/{} nodes\".format(flip_asr,flip_idx_atk.shape[0]))\n",
    "    result_asr.append(float(asr))\n",
    "    result_acc.append(float(clean_acc))\n",
    "print('The final ASR:{:.5f}, {:.5f}, Accuracy:{:.5f}, {:.5f}'\\\n",
    "            .format(np.average(result_asr),np.std(result_asr),np.average(result_acc),np.std(result_acc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model = model_construct(args,args.test_model,data,device).to(device) \n",
    "    # test_model = GCN(nfeat=cont_poison_x.shape[1],\\\n",
    "    #             nhid=args.hidden,\\\n",
    "    #             nclass= int(data.y.max()+1),\\\n",
    "    #             dropout=args.dropout,\\\n",
    "    #             lr=args.train_lr,\\\n",
    "    #             weight_decay=args.weight_decay,\\\n",
    "    #             device=device).to(device) \n",
    "test_model.fit(poison_x, poison_edge_index, poison_edge_weights, poison_labels, bkd_tn_nodes, idx_val,train_iters=args.epochs,verbose=False)\n",
    "\n",
    "output = test_model(poison_x,poison_edge_index,poison_edge_weights)\n",
    "train_attach_rate = (output.argmax(dim=1)[idx_attach]==args.target_class).float().mean()\n",
    "print(\"target class rate on Vs: {:.4f}\".format(train_attach_rate))\n",
    "torch.cuda.empty_cache()\n",
    "#%%\n",
    "induct_edge_index = torch.cat([poison_edge_index,mask_edge_index],dim=1)\n",
    "induct_edge_weights = torch.cat([poison_edge_weights,torch.ones([mask_edge_index.shape[1]],dtype=torch.float,device=device)])\n",
    "clean_acc = test_model.test(poison_x,induct_edge_index,induct_edge_weights,data.y,idx_clean_test)\n",
    "# test_model = test_model.cpu()\n",
    "\n",
    "print(\"accuracy on clean test nodes: {:.4f}\".format(clean_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "test_embds = test_model(poison_x,poison_edge_index,poison_edge_weights)\n",
    "# output = test_model.fc(test_embds)\n",
    "# output = test_embds\n",
    "output = test_model.clf_head(test_embds)\n",
    "# output = F.log_softmax(test_model.clf_head(test_embds),dim=1)\n",
    "train_attach_rate = (output.argmax(dim=1)[idx_attach]==args.target_class).float().mean()\n",
    "print(train_attach_rate)\n",
    "clean_acc = test_model.test(poison_x,induct_edge_index,induct_edge_weights,data.y,idx_clean_test)\n",
    "print(\"accuracy on clean test nodes: {:.4f}\".format(clean_acc))\n",
    "test_embds = test_model(poison_x,induct_edge_index,induct_edge_weights)\n",
    "poisoned_embds = test_embds[idx_attach]\n",
    "\n",
    "# # cos = F.CosineSimilarity(dim=1, eps=1e-6)\n",
    "# for idx in idx_attach:\n",
    "#     target_class_idx = (data.y!=args.target_class).nonzero().flatten()\n",
    "#     cos_sim = F.cosine_similarity(poisoned_embds[idx],test_embds[target_class_idx[:5]])\n",
    "#     print(cos_sim)\n",
    "\n",
    "for idx in idx_attach:\n",
    "    # get neighbor nodes\n",
    "    nbs_0 = (poison_edge_index[0]==idx).nonzero().flatten()\n",
    "    idx_nbs = poison_edge_index[1,nbs_0]\n",
    "    idx_labeled_nbs = idx_nbs[(idx_nbs<poison_labels.shape[0]).nonzero().flatten()]\n",
    "    print(output[idx])\n",
    "    # print(\"idx_nbs\",idx_nbs,output[idx_nbs])\n",
    "    # print(\"poison_labels\",poison_labels[idx_labeled_nbs])\n",
    "    # nbs_1 = (poison_edge_index[1]==idx).nonzero().flatten()\n",
    "    # nbs = torch.cat([nbs_0,nbs_1],dim=0)\n",
    "    # print(test_embds.shape)\n",
    "    # print(idx,poison_edge_index[1,nbs_0])\n",
    "    # print(idx,poison_edge_index[0,nbs_1])\n",
    "    # print(poison_x[idx].repeat(4,1).shape)\n",
    "    # print(nbs)\n",
    "    # print((poison_edge_index[0]==idx).nonzero().flatten())\n",
    "    # print((poison_edge_index[1]==idx).nonzero().flatten())\n",
    "    # target_class_idx = (data.y!=args.target_class).nonzero().flatten()\n",
    "    cos_sim = F.cosine_similarity(F.softmax(output[idx].repeat(len(idx_labeled_nbs),1)),F.softmax(output[idx_labeled_nbs]))\n",
    "    print(\"Sim\",cos_sim,poison_labels[idx_labeled_nbs])\n",
    "    # print(idx_nbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([[-2.1612e+00, -7.9176e+00, -2.4735e+01, -1.2284e-01, -1.3994e+01,\n",
    "         -1.0212e+01, -2.0797e+01]])\n",
    "b = torch.tensor([[   0.0000,  -43.7667, -108.6508,  -23.4506,  -69.0643,  -43.1416,\n",
    "        -103.0275]])\n",
    "\n",
    "c = torch.tensor([[  -1.1746,   -2.5948,   -6.4277,   -0.6488,   -3.3728,   -2.9037,\n",
    "           -5.8016]])\n",
    "d = torch.tensor([[  -1.2395,   -2.4253,   -5.1802,   -0.7167,   -2.7565,   -2.8887,\n",
    "           -4.7231]])\n",
    "\n",
    "\n",
    "# print(F.cosine_similarity(F.softmax(c),F.softmax(d)))\n",
    "print(output.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('py38_torch110')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13 (default, Mar 28 2022, 11:38:47) \n[GCC 7.5.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c4260eba67904b42d68a3963bc583366103d86fb6c89846e20de6072b78e7707"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
