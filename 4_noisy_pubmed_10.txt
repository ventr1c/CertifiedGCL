Begin epxeriment: noisy_level: 0.2 cont_weight: 10 epoch: der1:0.3 der2:0 dfr1:0.4 dfr2:0
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=2500, cl_num_layers=2, cl_num_proj_hidden=64, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=10.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.0, drop_feat_rate_1=0.4, drop_feat_rate_2=0.0, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=32, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, noisy_level=0.2, num_hidden=64, num_proj_hidden=64, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=40, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 88648])
add edge: torch.Size([2, 106200])
remove edge: torch.Size([2, 70924])
updated graph: torch.Size([2, 88476])
=== Noisy graph ===
/home/project-robust-contrastive/Robust-Contrastive-Learning/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 106.93480682373047 = 1.1119369268417358 + 10.0 * 10.582286834716797
Epoch 0, val loss: 1.1101051568984985
Epoch 10, training loss: 106.92721557617188 = 1.106742024421692 + 10.0 * 10.582047462463379
Epoch 10, val loss: 1.1049023866653442
Epoch 20, training loss: 106.91239166259766 = 1.1011946201324463 + 10.0 * 10.581119537353516
Epoch 20, val loss: 1.0993609428405762
Epoch 30, training loss: 106.86471557617188 = 1.0951982736587524 + 10.0 * 10.57695198059082
Epoch 30, val loss: 1.093353509902954
Epoch 40, training loss: 106.67829895019531 = 1.0886644124984741 + 10.0 * 10.558963775634766
Epoch 40, val loss: 1.0867993831634521
Epoch 50, training loss: 106.06039428710938 = 1.0816446542739868 + 10.0 * 10.497875213623047
Epoch 50, val loss: 1.0798014402389526
Epoch 60, training loss: 104.40662384033203 = 1.0746437311172485 + 10.0 * 10.333197593688965
Epoch 60, val loss: 1.0729104280471802
Epoch 70, training loss: 100.88514709472656 = 1.0677114725112915 + 10.0 * 9.981743812561035
Epoch 70, val loss: 1.066021203994751
Epoch 80, training loss: 97.65802001953125 = 1.0617402791976929 + 10.0 * 9.659627914428711
Epoch 80, val loss: 1.0603704452514648
Epoch 90, training loss: 95.17725372314453 = 1.0579228401184082 + 10.0 * 9.411932945251465
Epoch 90, val loss: 1.0568243265151978
Epoch 100, training loss: 93.37980651855469 = 1.0554373264312744 + 10.0 * 9.232437133789062
Epoch 100, val loss: 1.054521918296814
Epoch 110, training loss: 92.08543395996094 = 1.0532152652740479 + 10.0 * 9.103221893310547
Epoch 110, val loss: 1.0523474216461182
Epoch 120, training loss: 91.32775115966797 = 1.0512462854385376 + 10.0 * 9.027650833129883
Epoch 120, val loss: 1.0504366159439087
Epoch 130, training loss: 90.8039779663086 = 1.049337387084961 + 10.0 * 8.9754638671875
Epoch 130, val loss: 1.0485789775848389
Epoch 140, training loss: 90.35637664794922 = 1.0478240251541138 + 10.0 * 8.930855751037598
Epoch 140, val loss: 1.047222375869751
Epoch 150, training loss: 89.86701965332031 = 1.0466926097869873 + 10.0 * 8.88203239440918
Epoch 150, val loss: 1.0462167263031006
Epoch 160, training loss: 89.3241958618164 = 1.0457684993743896 + 10.0 * 8.827842712402344
Epoch 160, val loss: 1.0454127788543701
Epoch 170, training loss: 88.9198226928711 = 1.0449467897415161 + 10.0 * 8.787487983703613
Epoch 170, val loss: 1.0446375608444214
Epoch 180, training loss: 88.58453369140625 = 1.043763518333435 + 10.0 * 8.754076957702637
Epoch 180, val loss: 1.0435459613800049
Epoch 190, training loss: 88.29430389404297 = 1.0425974130630493 + 10.0 * 8.725171089172363
Epoch 190, val loss: 1.0425043106079102
Epoch 200, training loss: 88.06023406982422 = 1.0415295362472534 + 10.0 * 8.701870918273926
Epoch 200, val loss: 1.0415551662445068
Epoch 210, training loss: 87.83242797851562 = 1.0406824350357056 + 10.0 * 8.679174423217773
Epoch 210, val loss: 1.0408324003219604
Epoch 220, training loss: 87.60297393798828 = 1.0399670600891113 + 10.0 * 8.65630054473877
Epoch 220, val loss: 1.0402041673660278
Epoch 230, training loss: 87.42198944091797 = 1.0392800569534302 + 10.0 * 8.638270378112793
Epoch 230, val loss: 1.0395641326904297
Epoch 240, training loss: 87.24565124511719 = 1.0385277271270752 + 10.0 * 8.620712280273438
Epoch 240, val loss: 1.0388977527618408
Epoch 250, training loss: 87.10225677490234 = 1.0377446413040161 + 10.0 * 8.606451034545898
Epoch 250, val loss: 1.038185477256775
Epoch 260, training loss: 86.9890365600586 = 1.0368967056274414 + 10.0 * 8.595213890075684
Epoch 260, val loss: 1.037402868270874
Epoch 270, training loss: 86.89250183105469 = 1.035988450050354 + 10.0 * 8.585651397705078
Epoch 270, val loss: 1.036564826965332
Epoch 280, training loss: 86.7961654663086 = 1.0350717306137085 + 10.0 * 8.576108932495117
Epoch 280, val loss: 1.035714864730835
Epoch 290, training loss: 86.71165466308594 = 1.0341192483901978 + 10.0 * 8.567753791809082
Epoch 290, val loss: 1.0348303318023682
Epoch 300, training loss: 86.64128112792969 = 1.0331138372421265 + 10.0 * 8.560816764831543
Epoch 300, val loss: 1.0338983535766602
Epoch 310, training loss: 86.5628890991211 = 1.0320791006088257 + 10.0 * 8.553080558776855
Epoch 310, val loss: 1.0329352617263794
Epoch 320, training loss: 86.48493194580078 = 1.0310555696487427 + 10.0 * 8.545387268066406
Epoch 320, val loss: 1.032008171081543
Epoch 330, training loss: 86.43034362792969 = 1.0300136804580688 + 10.0 * 8.540033340454102
Epoch 330, val loss: 1.0310767889022827
Epoch 340, training loss: 86.34078979492188 = 1.0289596319198608 + 10.0 * 8.531183242797852
Epoch 340, val loss: 1.0300809144973755
Epoch 350, training loss: 86.27559661865234 = 1.0278981924057007 + 10.0 * 8.52476978302002
Epoch 350, val loss: 1.0291051864624023
Epoch 360, training loss: 86.22354125976562 = 1.0268182754516602 + 10.0 * 8.519672393798828
Epoch 360, val loss: 1.0281153917312622
Epoch 370, training loss: 86.1734619140625 = 1.0256870985031128 + 10.0 * 8.514777183532715
Epoch 370, val loss: 1.027040719985962
Epoch 380, training loss: 86.10749053955078 = 1.0245203971862793 + 10.0 * 8.508296966552734
Epoch 380, val loss: 1.0259696245193481
Epoch 390, training loss: 86.05681610107422 = 1.0233609676361084 + 10.0 * 8.503345489501953
Epoch 390, val loss: 1.0248695611953735
Epoch 400, training loss: 86.0221939086914 = 1.022150993347168 + 10.0 * 8.500004768371582
Epoch 400, val loss: 1.0237375497817993
Epoch 410, training loss: 85.96943664550781 = 1.020899772644043 + 10.0 * 8.494853973388672
Epoch 410, val loss: 1.0225378274917603
Epoch 420, training loss: 85.92620849609375 = 1.019594669342041 + 10.0 * 8.49066162109375
Epoch 420, val loss: 1.0213019847869873
Epoch 430, training loss: 85.90078735351562 = 1.0182167291641235 + 10.0 * 8.488256454467773
Epoch 430, val loss: 1.0199706554412842
Epoch 440, training loss: 85.84693145751953 = 1.016746163368225 + 10.0 * 8.48301887512207
Epoch 440, val loss: 1.018588662147522
Epoch 450, training loss: 85.8074951171875 = 1.0152872800827026 + 10.0 * 8.479220390319824
Epoch 450, val loss: 1.0172145366668701
Epoch 460, training loss: 85.77130126953125 = 1.0138052701950073 + 10.0 * 8.475749969482422
Epoch 460, val loss: 1.0158065557479858
Epoch 470, training loss: 85.75271606445312 = 1.0122581720352173 + 10.0 * 8.474045753479004
Epoch 470, val loss: 1.0143389701843262
Epoch 480, training loss: 85.71195983886719 = 1.010591745376587 + 10.0 * 8.470136642456055
Epoch 480, val loss: 1.0127389430999756
Epoch 490, training loss: 85.68034362792969 = 1.008878231048584 + 10.0 * 8.467145919799805
Epoch 490, val loss: 1.0111266374588013
Epoch 500, training loss: 85.64299774169922 = 1.0071651935577393 + 10.0 * 8.463582992553711
Epoch 500, val loss: 1.0094939470291138
Epoch 510, training loss: 85.61277770996094 = 1.0053911209106445 + 10.0 * 8.460738182067871
Epoch 510, val loss: 1.0078001022338867
Epoch 520, training loss: 85.6211929321289 = 1.0035103559494019 + 10.0 * 8.46176815032959
Epoch 520, val loss: 1.0059846639633179
Epoch 530, training loss: 85.57491302490234 = 1.0014809370040894 + 10.0 * 8.457343101501465
Epoch 530, val loss: 1.004075288772583
Epoch 540, training loss: 85.53881072998047 = 0.9994246959686279 + 10.0 * 8.453938484191895
Epoch 540, val loss: 1.0021367073059082
Epoch 550, training loss: 85.51138305664062 = 0.997306227684021 + 10.0 * 8.451407432556152
Epoch 550, val loss: 1.0001366138458252
Epoch 560, training loss: 85.49152374267578 = 0.9951130151748657 + 10.0 * 8.449641227722168
Epoch 560, val loss: 0.9980530738830566
Epoch 570, training loss: 85.46952819824219 = 0.9927668571472168 + 10.0 * 8.447675704956055
Epoch 570, val loss: 0.9958339333534241
Epoch 580, training loss: 85.4502944946289 = 0.990339994430542 + 10.0 * 8.445995330810547
Epoch 580, val loss: 0.9935280084609985
Epoch 590, training loss: 85.43157958984375 = 0.9878530502319336 + 10.0 * 8.444372177124023
Epoch 590, val loss: 0.9911754727363586
Epoch 600, training loss: 85.4101333618164 = 0.9852694869041443 + 10.0 * 8.442486763000488
Epoch 600, val loss: 0.9887163639068604
Epoch 610, training loss: 85.39237213134766 = 0.98256516456604 + 10.0 * 8.440980911254883
Epoch 610, val loss: 0.9861534237861633
Epoch 620, training loss: 85.38301849365234 = 0.9797478318214417 + 10.0 * 8.440326690673828
Epoch 620, val loss: 0.9834787249565125
Epoch 630, training loss: 85.38257598876953 = 0.9767386317253113 + 10.0 * 8.440584182739258
Epoch 630, val loss: 0.98064124584198
Epoch 640, training loss: 85.34687042236328 = 0.9736930727958679 + 10.0 * 8.437317848205566
Epoch 640, val loss: 0.9777530431747437
Epoch 650, training loss: 85.32164764404297 = 0.9706187844276428 + 10.0 * 8.435102462768555
Epoch 650, val loss: 0.9748327136039734
Epoch 660, training loss: 85.30534362792969 = 0.9674542546272278 + 10.0 * 8.433789253234863
Epoch 660, val loss: 0.9718243479728699
Epoch 670, training loss: 85.30229949951172 = 0.9641126394271851 + 10.0 * 8.433818817138672
Epoch 670, val loss: 0.9686380624771118
Epoch 680, training loss: 85.2800521850586 = 0.9606274366378784 + 10.0 * 8.4319429397583
Epoch 680, val loss: 0.9653418660163879
Epoch 690, training loss: 85.24813079833984 = 0.9571163058280945 + 10.0 * 8.429101943969727
Epoch 690, val loss: 0.9620233774185181
Epoch 700, training loss: 85.23072052001953 = 0.9535142779350281 + 10.0 * 8.42772102355957
Epoch 700, val loss: 0.958613395690918
Epoch 710, training loss: 85.21050262451172 = 0.9497881531715393 + 10.0 * 8.426071166992188
Epoch 710, val loss: 0.9550732374191284
Epoch 720, training loss: 85.2789535522461 = 0.9459225535392761 + 10.0 * 8.433302879333496
Epoch 720, val loss: 0.9513881206512451
Epoch 730, training loss: 85.1894760131836 = 0.9417741894721985 + 10.0 * 8.42477035522461
Epoch 730, val loss: 0.9474431276321411
Epoch 740, training loss: 85.17046356201172 = 0.937671422958374 + 10.0 * 8.423279762268066
Epoch 740, val loss: 0.9435499906539917
Epoch 750, training loss: 85.14534759521484 = 0.9335082769393921 + 10.0 * 8.421183586120605
Epoch 750, val loss: 0.9395938515663147
Epoch 760, training loss: 85.12708282470703 = 0.9292036294937134 + 10.0 * 8.419787406921387
Epoch 760, val loss: 0.9355059266090393
Epoch 770, training loss: 85.11202239990234 = 0.9247773289680481 + 10.0 * 8.41872501373291
Epoch 770, val loss: 0.9312958717346191
Epoch 780, training loss: 85.15245819091797 = 0.9201772212982178 + 10.0 * 8.42322826385498
Epoch 780, val loss: 0.9269260764122009
Epoch 790, training loss: 85.08451843261719 = 0.9153687953948975 + 10.0 * 8.416914939880371
Epoch 790, val loss: 0.9223158359527588
Epoch 800, training loss: 85.07196044921875 = 0.9105225801467896 + 10.0 * 8.416143417358398
Epoch 800, val loss: 0.9177154302597046
Epoch 810, training loss: 85.05432891845703 = 0.9056157469749451 + 10.0 * 8.414871215820312
Epoch 810, val loss: 0.9130504131317139
Epoch 820, training loss: 85.03840637207031 = 0.9005916118621826 + 10.0 * 8.413782119750977
Epoch 820, val loss: 0.9082539081573486
Epoch 830, training loss: 85.02604675292969 = 0.8953726887702942 + 10.0 * 8.413067817687988
Epoch 830, val loss: 0.9032717943191528
Epoch 840, training loss: 85.05779266357422 = 0.889990508556366 + 10.0 * 8.416780471801758
Epoch 840, val loss: 0.8981112837791443
Epoch 850, training loss: 85.00076293945312 = 0.8844833374023438 + 10.0 * 8.411627769470215
Epoch 850, val loss: 0.8928868770599365
Epoch 860, training loss: 84.98454284667969 = 0.8789639472961426 + 10.0 * 8.410557746887207
Epoch 860, val loss: 0.887643575668335
Epoch 870, training loss: 84.96916198730469 = 0.8734089732170105 + 10.0 * 8.409575462341309
Epoch 870, val loss: 0.8823569416999817
Epoch 880, training loss: 84.95397186279297 = 0.8677281737327576 + 10.0 * 8.408624649047852
Epoch 880, val loss: 0.8769301772117615
Epoch 890, training loss: 84.93946075439453 = 0.8619426488876343 + 10.0 * 8.407751083374023
Epoch 890, val loss: 0.8714239597320557
Epoch 900, training loss: 84.92597961425781 = 0.8560433983802795 + 10.0 * 8.406993865966797
Epoch 900, val loss: 0.8658039569854736
Epoch 910, training loss: 85.00479125976562 = 0.8500131368637085 + 10.0 * 8.415477752685547
Epoch 910, val loss: 0.8600223064422607
Epoch 920, training loss: 84.90487670898438 = 0.8437948822975159 + 10.0 * 8.406107902526855
Epoch 920, val loss: 0.8541494011878967
Epoch 930, training loss: 84.88774108886719 = 0.8376915454864502 + 10.0 * 8.405004501342773
Epoch 930, val loss: 0.848374605178833
Epoch 940, training loss: 84.87309265136719 = 0.8315854072570801 + 10.0 * 8.404150009155273
Epoch 940, val loss: 0.8425883054733276
Epoch 950, training loss: 84.8592758178711 = 0.8253970742225647 + 10.0 * 8.403388023376465
Epoch 950, val loss: 0.8367350697517395
Epoch 960, training loss: 84.8464584350586 = 0.81913161277771 + 10.0 * 8.402732849121094
Epoch 960, val loss: 0.8308066725730896
Epoch 970, training loss: 84.83802795410156 = 0.8127970099449158 + 10.0 * 8.402523040771484
Epoch 970, val loss: 0.8248189687728882
Epoch 980, training loss: 84.8355712890625 = 0.8062922358512878 + 10.0 * 8.402928352355957
Epoch 980, val loss: 0.8186524510383606
Epoch 990, training loss: 84.82587432861328 = 0.7997720241546631 + 10.0 * 8.402609825134277
Epoch 990, val loss: 0.8125463724136353
Epoch 1000, training loss: 84.79986572265625 = 0.7933067083358765 + 10.0 * 8.400655746459961
Epoch 1000, val loss: 0.8064919710159302
Epoch 1010, training loss: 84.78511810302734 = 0.7868590354919434 + 10.0 * 8.399826049804688
Epoch 1010, val loss: 0.800433874130249
Epoch 1020, training loss: 84.78263092041016 = 0.7803342342376709 + 10.0 * 8.400229454040527
Epoch 1020, val loss: 0.7943365573883057
Epoch 1030, training loss: 84.76519012451172 = 0.7737255692481995 + 10.0 * 8.39914608001709
Epoch 1030, val loss: 0.7881326675415039
Epoch 1040, training loss: 84.74921417236328 = 0.7671231627464294 + 10.0 * 8.398209571838379
Epoch 1040, val loss: 0.7819913029670715
Epoch 1050, training loss: 84.73823547363281 = 0.7605627179145813 + 10.0 * 8.397768020629883
Epoch 1050, val loss: 0.7758782505989075
Epoch 1060, training loss: 84.72607421875 = 0.7539743781089783 + 10.0 * 8.397210121154785
Epoch 1060, val loss: 0.7697648406028748
Epoch 1070, training loss: 84.72789001464844 = 0.7473427057266235 + 10.0 * 8.398054122924805
Epoch 1070, val loss: 0.7636284232139587
Epoch 1080, training loss: 84.70286560058594 = 0.7406467199325562 + 10.0 * 8.396222114562988
Epoch 1080, val loss: 0.7574101686477661
Epoch 1090, training loss: 84.69295501708984 = 0.7339924573898315 + 10.0 * 8.395895957946777
Epoch 1090, val loss: 0.7513088583946228
Epoch 1100, training loss: 84.6805419921875 = 0.7273783683776855 + 10.0 * 8.395316123962402
Epoch 1100, val loss: 0.7452027201652527
Epoch 1110, training loss: 84.66777038574219 = 0.7207455039024353 + 10.0 * 8.394701957702637
Epoch 1110, val loss: 0.73912513256073
Epoch 1120, training loss: 84.65668487548828 = 0.714103639125824 + 10.0 * 8.394258499145508
Epoch 1120, val loss: 0.7330309748649597
Epoch 1130, training loss: 84.69042205810547 = 0.7074359059333801 + 10.0 * 8.398298263549805
Epoch 1130, val loss: 0.7269113063812256
Epoch 1140, training loss: 84.65484619140625 = 0.7006753087043762 + 10.0 * 8.395417213439941
Epoch 1140, val loss: 0.7207515835762024
Epoch 1150, training loss: 84.62991333007812 = 0.694032609462738 + 10.0 * 8.393588066101074
Epoch 1150, val loss: 0.714770495891571
Epoch 1160, training loss: 84.61314392089844 = 0.6875080466270447 + 10.0 * 8.392563819885254
Epoch 1160, val loss: 0.7088337540626526
Epoch 1170, training loss: 84.59886169433594 = 0.681010901927948 + 10.0 * 8.391785621643066
Epoch 1170, val loss: 0.7029493451118469
Epoch 1180, training loss: 84.5886459350586 = 0.6745056509971619 + 10.0 * 8.391413688659668
Epoch 1180, val loss: 0.6970663666725159
Epoch 1190, training loss: 84.60616302490234 = 0.6679812073707581 + 10.0 * 8.393817901611328
Epoch 1190, val loss: 0.6911737322807312
Epoch 1200, training loss: 84.57149505615234 = 0.6613904237747192 + 10.0 * 8.391010284423828
Epoch 1200, val loss: 0.6852724552154541
Epoch 1210, training loss: 84.55967712402344 = 0.6549410223960876 + 10.0 * 8.390474319458008
Epoch 1210, val loss: 0.6794799566268921
Epoch 1220, training loss: 84.54667663574219 = 0.6485512256622314 + 10.0 * 8.389812469482422
Epoch 1220, val loss: 0.6737839579582214
Epoch 1230, training loss: 84.5751953125 = 0.6421993970870972 + 10.0 * 8.39330005645752
Epoch 1230, val loss: 0.6680796146392822
Epoch 1240, training loss: 84.54022979736328 = 0.6358121633529663 + 10.0 * 8.39044189453125
Epoch 1240, val loss: 0.6624110341072083
Epoch 1250, training loss: 84.51228332519531 = 0.6295802593231201 + 10.0 * 8.388270378112793
Epoch 1250, val loss: 0.6568781137466431
Epoch 1260, training loss: 84.50228881835938 = 0.6234220266342163 + 10.0 * 8.387887001037598
Epoch 1260, val loss: 0.6514158844947815
Epoch 1270, training loss: 84.51161193847656 = 0.6173122525215149 + 10.0 * 8.389430046081543
Epoch 1270, val loss: 0.6459994912147522
Epoch 1280, training loss: 84.4793701171875 = 0.6112550497055054 + 10.0 * 8.386812210083008
Epoch 1280, val loss: 0.6407033205032349
Epoch 1290, training loss: 84.46709442138672 = 0.6053052544593811 + 10.0 * 8.386178970336914
Epoch 1290, val loss: 0.6354720592498779
Epoch 1300, training loss: 84.4600830078125 = 0.5994293689727783 + 10.0 * 8.386065483093262
Epoch 1300, val loss: 0.6302909255027771
Epoch 1310, training loss: 84.468994140625 = 0.5936130285263062 + 10.0 * 8.387537956237793
Epoch 1310, val loss: 0.6252313852310181
Epoch 1320, training loss: 84.45086669921875 = 0.5878481268882751 + 10.0 * 8.38630199432373
Epoch 1320, val loss: 0.6202012896537781
Epoch 1330, training loss: 84.43931579589844 = 0.5822131037712097 + 10.0 * 8.385709762573242
Epoch 1330, val loss: 0.6152880787849426
Epoch 1340, training loss: 84.41558074951172 = 0.5767324566841125 + 10.0 * 8.383885383605957
Epoch 1340, val loss: 0.6105558276176453
Epoch 1350, training loss: 84.40830993652344 = 0.5713727474212646 + 10.0 * 8.38369369506836
Epoch 1350, val loss: 0.6059422492980957
Epoch 1360, training loss: 84.404296875 = 0.5661027431488037 + 10.0 * 8.383819580078125
Epoch 1360, val loss: 0.6013954877853394
Epoch 1370, training loss: 84.39513397216797 = 0.5609158873558044 + 10.0 * 8.383421897888184
Epoch 1370, val loss: 0.5969520211219788
Epoch 1380, training loss: 84.38606262207031 = 0.5558458566665649 + 10.0 * 8.383021354675293
Epoch 1380, val loss: 0.5926114320755005
Epoch 1390, training loss: 84.37718200683594 = 0.5508857369422913 + 10.0 * 8.38262939453125
Epoch 1390, val loss: 0.5883908271789551
Epoch 1400, training loss: 84.36982727050781 = 0.5460476875305176 + 10.0 * 8.382377624511719
Epoch 1400, val loss: 0.5842932462692261
Epoch 1410, training loss: 84.36436462402344 = 0.5413467884063721 + 10.0 * 8.382302284240723
Epoch 1410, val loss: 0.5802868008613586
Epoch 1420, training loss: 84.3404769897461 = 0.5367668867111206 + 10.0 * 8.38037109375
Epoch 1420, val loss: 0.5764465928077698
Epoch 1430, training loss: 84.33324432373047 = 0.5323193073272705 + 10.0 * 8.38009262084961
Epoch 1430, val loss: 0.5727197527885437
Epoch 1440, training loss: 84.33426666259766 = 0.5279921889305115 + 10.0 * 8.380627632141113
Epoch 1440, val loss: 0.5690775513648987
Epoch 1450, training loss: 84.33637237548828 = 0.523740291595459 + 10.0 * 8.38126277923584
Epoch 1450, val loss: 0.5655496120452881
Epoch 1460, training loss: 84.31970977783203 = 0.5196211338043213 + 10.0 * 8.380008697509766
Epoch 1460, val loss: 0.5621605515480042
Epoch 1470, training loss: 84.30350494384766 = 0.5156611204147339 + 10.0 * 8.3787841796875
Epoch 1470, val loss: 0.5589412450790405
Epoch 1480, training loss: 84.29010009765625 = 0.5118547081947327 + 10.0 * 8.377824783325195
Epoch 1480, val loss: 0.5558223128318787
Epoch 1490, training loss: 84.28007507324219 = 0.5081422328948975 + 10.0 * 8.377193450927734
Epoch 1490, val loss: 0.5528175234794617
Epoch 1500, training loss: 84.29627990722656 = 0.5045203566551208 + 10.0 * 8.379176139831543
Epoch 1500, val loss: 0.549888014793396
Epoch 1510, training loss: 84.28319549560547 = 0.5009613633155823 + 10.0 * 8.378223419189453
Epoch 1510, val loss: 0.5470480918884277
Epoch 1520, training loss: 84.26786804199219 = 0.4975613057613373 + 10.0 * 8.377031326293945
Epoch 1520, val loss: 0.5443323254585266
Epoch 1530, training loss: 84.25187683105469 = 0.4943056106567383 + 10.0 * 8.375757217407227
Epoch 1530, val loss: 0.5417450666427612
Epoch 1540, training loss: 84.24501037597656 = 0.4911603331565857 + 10.0 * 8.375385284423828
Epoch 1540, val loss: 0.5392475724220276
Epoch 1550, training loss: 84.26570129394531 = 0.4880917966365814 + 10.0 * 8.377760887145996
Epoch 1550, val loss: 0.5368289351463318
Epoch 1560, training loss: 84.23353576660156 = 0.48509538173675537 + 10.0 * 8.374844551086426
Epoch 1560, val loss: 0.5345794558525085
Epoch 1570, training loss: 84.25450897216797 = 0.4822181761264801 + 10.0 * 8.377229690551758
Epoch 1570, val loss: 0.5323261618614197
Epoch 1580, training loss: 84.22184753417969 = 0.47940877079963684 + 10.0 * 8.37424373626709
Epoch 1580, val loss: 0.5302339196205139
Epoch 1590, training loss: 84.2137222290039 = 0.47673094272613525 + 10.0 * 8.373699188232422
Epoch 1590, val loss: 0.5281757712364197
Epoch 1600, training loss: 84.2046127319336 = 0.4741503596305847 + 10.0 * 8.373045921325684
Epoch 1600, val loss: 0.5262835025787354
Epoch 1610, training loss: 84.19820404052734 = 0.4716392755508423 + 10.0 * 8.372655868530273
Epoch 1610, val loss: 0.5244036912918091
Epoch 1620, training loss: 84.21736145019531 = 0.4691946804523468 + 10.0 * 8.37481689453125
Epoch 1620, val loss: 0.5226141214370728
Epoch 1630, training loss: 84.19306182861328 = 0.46678999066352844 + 10.0 * 8.372627258300781
Epoch 1630, val loss: 0.5208706259727478
Epoch 1640, training loss: 84.1845703125 = 0.46449556946754456 + 10.0 * 8.372007369995117
Epoch 1640, val loss: 0.5192431211471558
Epoch 1650, training loss: 84.17436981201172 = 0.46227434277534485 + 10.0 * 8.371210098266602
Epoch 1650, val loss: 0.5176451206207275
Epoch 1660, training loss: 84.17112731933594 = 0.46011799573898315 + 10.0 * 8.371100425720215
Epoch 1660, val loss: 0.5161447525024414
Epoch 1670, training loss: 84.21871948242188 = 0.4580044746398926 + 10.0 * 8.376070976257324
Epoch 1670, val loss: 0.5146661996841431
Epoch 1680, training loss: 84.1723861694336 = 0.4559396803379059 + 10.0 * 8.371644973754883
Epoch 1680, val loss: 0.5132883787155151
Epoch 1690, training loss: 84.15309143066406 = 0.45397141575813293 + 10.0 * 8.369912147521973
Epoch 1690, val loss: 0.5119372010231018
Epoch 1700, training loss: 84.14578247070312 = 0.45205798745155334 + 10.0 * 8.369372367858887
Epoch 1700, val loss: 0.5106711387634277
Epoch 1710, training loss: 84.14132690429688 = 0.4501938223838806 + 10.0 * 8.369112968444824
Epoch 1710, val loss: 0.5094547867774963
Epoch 1720, training loss: 84.18912506103516 = 0.44836416840553284 + 10.0 * 8.374075889587402
Epoch 1720, val loss: 0.5082979798316956
Epoch 1730, training loss: 84.1776123046875 = 0.4465601444244385 + 10.0 * 8.3731050491333
Epoch 1730, val loss: 0.5071103572845459
Epoch 1740, training loss: 84.13822937011719 = 0.44485288858413696 + 10.0 * 8.36933708190918
Epoch 1740, val loss: 0.5060468316078186
Epoch 1750, training loss: 84.12456512451172 = 0.4432221055030823 + 10.0 * 8.368134498596191
Epoch 1750, val loss: 0.5050197243690491
Epoch 1760, training loss: 84.11750030517578 = 0.44163042306900024 + 10.0 * 8.367587089538574
Epoch 1760, val loss: 0.5040575861930847
Epoch 1770, training loss: 84.11344146728516 = 0.4400683343410492 + 10.0 * 8.367337226867676
Epoch 1770, val loss: 0.5031059980392456
Epoch 1780, training loss: 84.10922241210938 = 0.43853214383125305 + 10.0 * 8.367069244384766
Epoch 1780, val loss: 0.5021886825561523
Epoch 1790, training loss: 84.13445281982422 = 0.4370224177837372 + 10.0 * 8.369743347167969
Epoch 1790, val loss: 0.5012744665145874
Epoch 1800, training loss: 84.10804748535156 = 0.4355345666408539 + 10.0 * 8.3672513961792
Epoch 1800, val loss: 0.5004568099975586
Epoch 1810, training loss: 84.1031723022461 = 0.43410128355026245 + 10.0 * 8.366907119750977
Epoch 1810, val loss: 0.49963539838790894
Epoch 1820, training loss: 84.09495544433594 = 0.4327092170715332 + 10.0 * 8.36622428894043
Epoch 1820, val loss: 0.49887898564338684
Epoch 1830, training loss: 84.0928726196289 = 0.4313427805900574 + 10.0 * 8.3661527633667
Epoch 1830, val loss: 0.4981500804424286
Epoch 1840, training loss: 84.13184356689453 = 0.42999714612960815 + 10.0 * 8.370183944702148
Epoch 1840, val loss: 0.4974457919597626
Epoch 1850, training loss: 84.12483215332031 = 0.4286862313747406 + 10.0 * 8.369614601135254
Epoch 1850, val loss: 0.4966675937175751
Epoch 1860, training loss: 84.08011627197266 = 0.4274066984653473 + 10.0 * 8.365270614624023
Epoch 1860, val loss: 0.4960460662841797
Epoch 1870, training loss: 84.07380676269531 = 0.426176518201828 + 10.0 * 8.364763259887695
Epoch 1870, val loss: 0.49543604254722595
Epoch 1880, training loss: 84.07072448730469 = 0.4249687194824219 + 10.0 * 8.364575386047363
Epoch 1880, val loss: 0.4948134422302246
Epoch 1890, training loss: 84.06573486328125 = 0.42377710342407227 + 10.0 * 8.364195823669434
Epoch 1890, val loss: 0.4942484498023987
Epoch 1900, training loss: 84.06470489501953 = 0.4226014018058777 + 10.0 * 8.36421012878418
Epoch 1900, val loss: 0.4936746656894684
Epoch 1910, training loss: 84.10213470458984 = 0.42144012451171875 + 10.0 * 8.368069648742676
Epoch 1910, val loss: 0.4931238293647766
Epoch 1920, training loss: 84.06350708007812 = 0.42028865218162537 + 10.0 * 8.3643217086792
Epoch 1920, val loss: 0.49261727929115295
Epoch 1930, training loss: 84.05350494384766 = 0.4191778302192688 + 10.0 * 8.363432884216309
Epoch 1930, val loss: 0.49210840463638306
Epoch 1940, training loss: 84.0603256225586 = 0.418080598115921 + 10.0 * 8.364224433898926
Epoch 1940, val loss: 0.4916401207447052
Epoch 1950, training loss: 84.05332946777344 = 0.4170023798942566 + 10.0 * 8.363633155822754
Epoch 1950, val loss: 0.4911690652370453
Epoch 1960, training loss: 84.04434204101562 = 0.41594940423965454 + 10.0 * 8.362839698791504
Epoch 1960, val loss: 0.49070438742637634
Epoch 1970, training loss: 84.03858947753906 = 0.41491320729255676 + 10.0 * 8.362367630004883
Epoch 1970, val loss: 0.4902952015399933
Epoch 1980, training loss: 84.03679656982422 = 0.413894921541214 + 10.0 * 8.362290382385254
Epoch 1980, val loss: 0.4898742139339447
Epoch 1990, training loss: 84.08345794677734 = 0.4128844439983368 + 10.0 * 8.367056846618652
Epoch 1990, val loss: 0.4894392490386963
Epoch 2000, training loss: 84.04856872558594 = 0.4118770360946655 + 10.0 * 8.363668441772461
Epoch 2000, val loss: 0.4890975058078766
Epoch 2010, training loss: 84.02838134765625 = 0.4109093248844147 + 10.0 * 8.361746788024902
Epoch 2010, val loss: 0.4887542724609375
Epoch 2020, training loss: 84.02207946777344 = 0.40995046496391296 + 10.0 * 8.361212730407715
Epoch 2020, val loss: 0.48838499188423157
Epoch 2030, training loss: 84.01946258544922 = 0.4090054929256439 + 10.0 * 8.361045837402344
Epoch 2030, val loss: 0.4880690276622772
Epoch 2040, training loss: 84.03648376464844 = 0.40806564688682556 + 10.0 * 8.362841606140137
Epoch 2040, val loss: 0.48774585127830505
Epoch 2050, training loss: 84.01590728759766 = 0.4071308374404907 + 10.0 * 8.360877990722656
Epoch 2050, val loss: 0.4874066710472107
Epoch 2060, training loss: 84.01596069335938 = 0.4062223434448242 + 10.0 * 8.360974311828613
Epoch 2060, val loss: 0.4871176779270172
Epoch 2070, training loss: 84.0218505859375 = 0.4053339958190918 + 10.0 * 8.361651420593262
Epoch 2070, val loss: 0.4868009686470032
Epoch 2080, training loss: 84.0103530883789 = 0.4044544994831085 + 10.0 * 8.360589981079102
Epoch 2080, val loss: 0.4865630567073822
Epoch 2090, training loss: 84.00187683105469 = 0.40358811616897583 + 10.0 * 8.35982894897461
Epoch 2090, val loss: 0.48632103204727173
Epoch 2100, training loss: 84.00125122070312 = 0.40272724628448486 + 10.0 * 8.35985279083252
Epoch 2100, val loss: 0.4860854744911194
Epoch 2110, training loss: 84.033203125 = 0.40187180042266846 + 10.0 * 8.363133430480957
Epoch 2110, val loss: 0.4858914911746979
Epoch 2120, training loss: 83.99788665771484 = 0.4010252058506012 + 10.0 * 8.359685897827148
Epoch 2120, val loss: 0.48557767271995544
Epoch 2130, training loss: 83.99356079101562 = 0.4001968502998352 + 10.0 * 8.359335899353027
Epoch 2130, val loss: 0.4853915572166443
Epoch 2140, training loss: 83.9969482421875 = 0.39937731623649597 + 10.0 * 8.359757423400879
Epoch 2140, val loss: 0.48517176508903503
Epoch 2150, training loss: 83.9920425415039 = 0.39856407046318054 + 10.0 * 8.359347343444824
Epoch 2150, val loss: 0.4849777817726135
Epoch 2160, training loss: 83.98725128173828 = 0.39776191115379333 + 10.0 * 8.358949661254883
Epoch 2160, val loss: 0.4848179221153259
Epoch 2170, training loss: 84.00079345703125 = 0.396962970495224 + 10.0 * 8.360383033752441
Epoch 2170, val loss: 0.48461195826530457
Epoch 2180, training loss: 83.9847183227539 = 0.3961719572544098 + 10.0 * 8.358854293823242
Epoch 2180, val loss: 0.48447149991989136
Epoch 2190, training loss: 83.9792709350586 = 0.39539292454719543 + 10.0 * 8.35838794708252
Epoch 2190, val loss: 0.4842887818813324
Epoch 2200, training loss: 83.983642578125 = 0.39462530612945557 + 10.0 * 8.358901977539062
Epoch 2200, val loss: 0.48415347933769226
Epoch 2210, training loss: 83.98393249511719 = 0.39386051893234253 + 10.0 * 8.359006881713867
Epoch 2210, val loss: 0.48400041460990906
Epoch 2220, training loss: 83.9738998413086 = 0.3931063413619995 + 10.0 * 8.35807991027832
Epoch 2220, val loss: 0.48385995626449585
Epoch 2230, training loss: 83.97608184814453 = 0.39235788583755493 + 10.0 * 8.358372688293457
Epoch 2230, val loss: 0.48374077677726746
Epoch 2240, training loss: 83.98091888427734 = 0.39161252975463867 + 10.0 * 8.358930587768555
Epoch 2240, val loss: 0.4836227595806122
Epoch 2250, training loss: 83.97525024414062 = 0.39087769389152527 + 10.0 * 8.358437538146973
Epoch 2250, val loss: 0.48348939418792725
Epoch 2260, training loss: 83.9713134765625 = 0.39014679193496704 + 10.0 * 8.35811710357666
Epoch 2260, val loss: 0.48339560627937317
Epoch 2270, training loss: 83.96000671386719 = 0.38942331075668335 + 10.0 * 8.35705852508545
Epoch 2270, val loss: 0.4832904040813446
Epoch 2280, training loss: 83.97207641601562 = 0.38870540261268616 + 10.0 * 8.35833740234375
Epoch 2280, val loss: 0.48321372270584106
Epoch 2290, training loss: 83.9662094116211 = 0.38799017667770386 + 10.0 * 8.357821464538574
Epoch 2290, val loss: 0.4831097424030304
Epoch 2300, training loss: 83.95716857910156 = 0.38728344440460205 + 10.0 * 8.356988906860352
Epoch 2300, val loss: 0.48298150300979614
Epoch 2310, training loss: 83.95264434814453 = 0.3865867555141449 + 10.0 * 8.356605529785156
Epoch 2310, val loss: 0.48293331265449524
Epoch 2320, training loss: 83.97562408447266 = 0.3858908414840698 + 10.0 * 8.358973503112793
Epoch 2320, val loss: 0.48282837867736816
Epoch 2330, training loss: 83.9485092163086 = 0.38519778847694397 + 10.0 * 8.356330871582031
Epoch 2330, val loss: 0.48280125856399536
Epoch 2340, training loss: 83.94831848144531 = 0.38451603055000305 + 10.0 * 8.356380462646484
Epoch 2340, val loss: 0.48274022340774536
Epoch 2350, training loss: 83.94756317138672 = 0.3838375210762024 + 10.0 * 8.356372833251953
Epoch 2350, val loss: 0.4826793968677521
Epoch 2360, training loss: 83.94448852539062 = 0.38316333293914795 + 10.0 * 8.356132507324219
Epoch 2360, val loss: 0.48264363408088684
Epoch 2370, training loss: 83.94622802734375 = 0.38249191641807556 + 10.0 * 8.35637378692627
Epoch 2370, val loss: 0.48257267475128174
Epoch 2380, training loss: 83.95350646972656 = 0.3818240165710449 + 10.0 * 8.357168197631836
Epoch 2380, val loss: 0.48253417015075684
Epoch 2390, training loss: 83.94949340820312 = 0.3811621367931366 + 10.0 * 8.356832504272461
Epoch 2390, val loss: 0.4825490117073059
Epoch 2400, training loss: 83.9328384399414 = 0.3805072009563446 + 10.0 * 8.355233192443848
Epoch 2400, val loss: 0.482503205537796
Epoch 2410, training loss: 83.93089294433594 = 0.37985628843307495 + 10.0 * 8.355103492736816
Epoch 2410, val loss: 0.4824783504009247
Epoch 2420, training loss: 83.93110656738281 = 0.3792065680027008 + 10.0 * 8.35519027709961
Epoch 2420, val loss: 0.4824678897857666
Epoch 2430, training loss: 83.96879577636719 = 0.3785592317581177 + 10.0 * 8.359023094177246
Epoch 2430, val loss: 0.4824773967266083
Epoch 2440, training loss: 83.93386840820312 = 0.3779207468032837 + 10.0 * 8.355594635009766
Epoch 2440, val loss: 0.48245492577552795
Epoch 2450, training loss: 83.92776489257812 = 0.3772895634174347 + 10.0 * 8.355047225952148
Epoch 2450, val loss: 0.48244497179985046
Epoch 2460, training loss: 83.92261505126953 = 0.37666401267051697 + 10.0 * 8.354595184326172
Epoch 2460, val loss: 0.48245128989219666
Epoch 2470, training loss: 83.92230224609375 = 0.37603798508644104 + 10.0 * 8.354626655578613
Epoch 2470, val loss: 0.4824463129043579
Epoch 2480, training loss: 83.96114349365234 = 0.37541648745536804 + 10.0 * 8.358572006225586
Epoch 2480, val loss: 0.48242080211639404
Epoch 2490, training loss: 83.92809295654297 = 0.37479496002197266 + 10.0 * 8.355329513549805
Epoch 2490, val loss: 0.48253509402275085
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7995941146626078
0.8161993769470406
=== training gcn model ===
Epoch 0, training loss: 106.93921661376953 = 1.1169109344482422 + 10.0 * 10.582230567932129
Epoch 0, val loss: 1.1151468753814697
Epoch 10, training loss: 106.9294662475586 = 1.111403465270996 + 10.0 * 10.581806182861328
Epoch 10, val loss: 1.109657883644104
Epoch 20, training loss: 106.90725708007812 = 1.1054461002349854 + 10.0 * 10.580181121826172
Epoch 20, val loss: 1.103698968887329
Epoch 30, training loss: 106.8307876586914 = 1.098881721496582 + 10.0 * 10.573190689086914
Epoch 30, val loss: 1.0971322059631348
Epoch 40, training loss: 106.53334045410156 = 1.0915850400924683 + 10.0 * 10.54417610168457
Epoch 40, val loss: 1.0898337364196777
Epoch 50, training loss: 105.49691772460938 = 1.0835435390472412 + 10.0 * 10.441337585449219
Epoch 50, val loss: 1.0817999839782715
Epoch 60, training loss: 102.2944107055664 = 1.0750120878219604 + 10.0 * 10.121939659118652
Epoch 60, val loss: 1.0732755661010742
Epoch 70, training loss: 96.3140869140625 = 1.0656899213790894 + 10.0 * 9.524839401245117
Epoch 70, val loss: 1.0641056299209595
Epoch 80, training loss: 95.09290313720703 = 1.0589489936828613 + 10.0 * 9.40339469909668
Epoch 80, val loss: 1.0578608512878418
Epoch 90, training loss: 93.65753936767578 = 1.055190920829773 + 10.0 * 9.260234832763672
Epoch 90, val loss: 1.0544350147247314
Epoch 100, training loss: 93.01478576660156 = 1.0521258115768433 + 10.0 * 9.196266174316406
Epoch 100, val loss: 1.0515142679214478
Epoch 110, training loss: 92.68812561035156 = 1.0486282110214233 + 10.0 * 9.163949966430664
Epoch 110, val loss: 1.048147439956665
Epoch 120, training loss: 92.191162109375 = 1.0455197095870972 + 10.0 * 9.114564895629883
Epoch 120, val loss: 1.0452778339385986
Epoch 130, training loss: 91.50318908691406 = 1.043992042541504 + 10.0 * 9.045919418334961
Epoch 130, val loss: 1.0439727306365967
Epoch 140, training loss: 90.59217071533203 = 1.0434054136276245 + 10.0 * 8.954876899719238
Epoch 140, val loss: 1.0435066223144531
Epoch 150, training loss: 89.73855590820312 = 1.043192982673645 + 10.0 * 8.869536399841309
Epoch 150, val loss: 1.0433554649353027
Epoch 160, training loss: 89.13835144042969 = 1.0426665544509888 + 10.0 * 8.809568405151367
Epoch 160, val loss: 1.0428292751312256
Epoch 170, training loss: 88.66912841796875 = 1.041776418685913 + 10.0 * 8.762735366821289
Epoch 170, val loss: 1.0420292615890503
Epoch 180, training loss: 88.27337646484375 = 1.040972113609314 + 10.0 * 8.723240852355957
Epoch 180, val loss: 1.0413411855697632
Epoch 190, training loss: 87.9587173461914 = 1.040382742881775 + 10.0 * 8.69183349609375
Epoch 190, val loss: 1.0408676862716675
Epoch 200, training loss: 87.70551300048828 = 1.0398565530776978 + 10.0 * 8.666565895080566
Epoch 200, val loss: 1.040399193763733
Epoch 210, training loss: 87.51739501953125 = 1.0391288995742798 + 10.0 * 8.647826194763184
Epoch 210, val loss: 1.0397319793701172
Epoch 220, training loss: 87.39187622070312 = 1.038233757019043 + 10.0 * 8.635364532470703
Epoch 220, val loss: 1.038896918296814
Epoch 230, training loss: 87.30461120605469 = 1.0372058153152466 + 10.0 * 8.626740455627441
Epoch 230, val loss: 1.037958025932312
Epoch 240, training loss: 87.23509216308594 = 1.036139726638794 + 10.0 * 8.619894981384277
Epoch 240, val loss: 1.036993384361267
Epoch 250, training loss: 87.16046142578125 = 1.0351382493972778 + 10.0 * 8.612531661987305
Epoch 250, val loss: 1.0360584259033203
Epoch 260, training loss: 87.06063842773438 = 1.0341124534606934 + 10.0 * 8.602652549743652
Epoch 260, val loss: 1.0351206064224243
Epoch 270, training loss: 86.96044921875 = 1.0331569910049438 + 10.0 * 8.592729568481445
Epoch 270, val loss: 1.0342493057250977
Epoch 280, training loss: 86.8504867553711 = 1.0322819948196411 + 10.0 * 8.581820487976074
Epoch 280, val loss: 1.033449649810791
Epoch 290, training loss: 86.7347183227539 = 1.0314316749572754 + 10.0 * 8.570328712463379
Epoch 290, val loss: 1.0326766967773438
Epoch 300, training loss: 86.61058807373047 = 1.0306001901626587 + 10.0 * 8.557998657226562
Epoch 300, val loss: 1.0319207906723022
Epoch 310, training loss: 86.49609375 = 1.029772400856018 + 10.0 * 8.546632766723633
Epoch 310, val loss: 1.031153678894043
Epoch 320, training loss: 86.42961120605469 = 1.0289044380187988 + 10.0 * 8.540070533752441
Epoch 320, val loss: 1.0303435325622559
Epoch 330, training loss: 86.34087371826172 = 1.0279170274734497 + 10.0 * 8.531295776367188
Epoch 330, val loss: 1.0294162034988403
Epoch 340, training loss: 86.2676010131836 = 1.0268950462341309 + 10.0 * 8.524070739746094
Epoch 340, val loss: 1.0284574031829834
Epoch 350, training loss: 86.20659637451172 = 1.0258510112762451 + 10.0 * 8.518074989318848
Epoch 350, val loss: 1.0274826288223267
Epoch 360, training loss: 86.15097045898438 = 1.024743676185608 + 10.0 * 8.512622833251953
Epoch 360, val loss: 1.0264315605163574
Epoch 370, training loss: 86.10091400146484 = 1.0235817432403564 + 10.0 * 8.507733345031738
Epoch 370, val loss: 1.0253359079360962
Epoch 380, training loss: 86.05035400390625 = 1.0223857164382935 + 10.0 * 8.50279712677002
Epoch 380, val loss: 1.0242115259170532
Epoch 390, training loss: 86.00337219238281 = 1.0211375951766968 + 10.0 * 8.498223304748535
Epoch 390, val loss: 1.0230307579040527
Epoch 400, training loss: 85.96464538574219 = 1.0198101997375488 + 10.0 * 8.49448299407959
Epoch 400, val loss: 1.021775722503662
Epoch 410, training loss: 85.93064880371094 = 1.018403172492981 + 10.0 * 8.49122428894043
Epoch 410, val loss: 1.0204490423202515
Epoch 420, training loss: 85.88642883300781 = 1.0169565677642822 + 10.0 * 8.486947059631348
Epoch 420, val loss: 1.0190843343734741
Epoch 430, training loss: 85.89612579345703 = 1.0154471397399902 + 10.0 * 8.488067626953125
Epoch 430, val loss: 1.0176565647125244
Epoch 440, training loss: 85.82340240478516 = 1.0138299465179443 + 10.0 * 8.48095703125
Epoch 440, val loss: 1.016141414642334
Epoch 450, training loss: 85.78016662597656 = 1.0121949911117554 + 10.0 * 8.476797103881836
Epoch 450, val loss: 1.0146055221557617
Epoch 460, training loss: 85.74220275878906 = 1.010526418685913 + 10.0 * 8.473167419433594
Epoch 460, val loss: 1.013035535812378
Epoch 470, training loss: 85.70491027832031 = 1.0088058710098267 + 10.0 * 8.469610214233398
Epoch 470, val loss: 1.0114175081253052
Epoch 480, training loss: 85.68609619140625 = 1.007028579711914 + 10.0 * 8.467906951904297
Epoch 480, val loss: 1.0097581148147583
Epoch 490, training loss: 85.63733673095703 = 1.0051151514053345 + 10.0 * 8.46322250366211
Epoch 490, val loss: 1.0079522132873535
Epoch 500, training loss: 85.60064697265625 = 1.0031746625900269 + 10.0 * 8.459747314453125
Epoch 500, val loss: 1.0061312913894653
Epoch 510, training loss: 85.5635986328125 = 1.001230001449585 + 10.0 * 8.456236839294434
Epoch 510, val loss: 1.0043003559112549
Epoch 520, training loss: 85.52547454833984 = 0.999213457107544 + 10.0 * 8.45262622833252
Epoch 520, val loss: 1.0024099349975586
Epoch 530, training loss: 85.49574279785156 = 0.9971144795417786 + 10.0 * 8.449862480163574
Epoch 530, val loss: 1.0004363059997559
Epoch 540, training loss: 85.48722076416016 = 0.9948763847351074 + 10.0 * 8.449234962463379
Epoch 540, val loss: 0.9983267188072205
Epoch 550, training loss: 85.4367904663086 = 0.9925220608711243 + 10.0 * 8.444426536560059
Epoch 550, val loss: 0.9961196780204773
Epoch 560, training loss: 85.40481567382812 = 0.990125298500061 + 10.0 * 8.441469192504883
Epoch 560, val loss: 0.9938628673553467
Epoch 570, training loss: 85.37567901611328 = 0.9876277446746826 + 10.0 * 8.43880558013916
Epoch 570, val loss: 0.9915122985839844
Epoch 580, training loss: 85.37062072753906 = 0.9850032925605774 + 10.0 * 8.438562393188477
Epoch 580, val loss: 0.9890339374542236
Epoch 590, training loss: 85.32990264892578 = 0.9822105169296265 + 10.0 * 8.434769630432129
Epoch 590, val loss: 0.9863976836204529
Epoch 600, training loss: 85.30174255371094 = 0.9793108701705933 + 10.0 * 8.432243347167969
Epoch 600, val loss: 0.9836612939834595
Epoch 610, training loss: 85.276123046875 = 0.9762880802154541 + 10.0 * 8.429983139038086
Epoch 610, val loss: 0.9808103442192078
Epoch 620, training loss: 85.29676055908203 = 0.9731075763702393 + 10.0 * 8.432365417480469
Epoch 620, val loss: 0.9778067469596863
Epoch 630, training loss: 85.24418640136719 = 0.9697039127349854 + 10.0 * 8.427448272705078
Epoch 630, val loss: 0.9745731353759766
Epoch 640, training loss: 85.22576141357422 = 0.9662052989006042 + 10.0 * 8.425955772399902
Epoch 640, val loss: 0.9712751507759094
Epoch 650, training loss: 85.19981384277344 = 0.9625821113586426 + 10.0 * 8.423723220825195
Epoch 650, val loss: 0.9678545594215393
Epoch 660, training loss: 85.18168640136719 = 0.9588077068328857 + 10.0 * 8.422287940979004
Epoch 660, val loss: 0.9642878770828247
Epoch 670, training loss: 85.2345962524414 = 0.9548529386520386 + 10.0 * 8.427974700927734
Epoch 670, val loss: 0.960534393787384
Epoch 680, training loss: 85.15283203125 = 0.9506009817123413 + 10.0 * 8.420223236083984
Epoch 680, val loss: 0.9565190672874451
Epoch 690, training loss: 85.13849639892578 = 0.9462999105453491 + 10.0 * 8.419219970703125
Epoch 690, val loss: 0.9524490237236023
Epoch 700, training loss: 85.12161254882812 = 0.9418662786483765 + 10.0 * 8.417974472045898
Epoch 700, val loss: 0.9482504725456238
Epoch 710, training loss: 85.10594177246094 = 0.9372580647468567 + 10.0 * 8.416868209838867
Epoch 710, val loss: 0.9438906908035278
Epoch 720, training loss: 85.09048461914062 = 0.9324597716331482 + 10.0 * 8.415802955627441
Epoch 720, val loss: 0.9393468499183655
Epoch 730, training loss: 85.09004974365234 = 0.9274502396583557 + 10.0 * 8.416259765625
Epoch 730, val loss: 0.9346127510070801
Epoch 740, training loss: 85.07981872558594 = 0.9222092032432556 + 10.0 * 8.41576099395752
Epoch 740, val loss: 0.929620087146759
Epoch 750, training loss: 85.05131530761719 = 0.9168115854263306 + 10.0 * 8.413450241088867
Epoch 750, val loss: 0.9245237708091736
Epoch 760, training loss: 85.03345489501953 = 0.9112781882286072 + 10.0 * 8.41221809387207
Epoch 760, val loss: 0.9192879796028137
Epoch 770, training loss: 85.01924133300781 = 0.9055668115615845 + 10.0 * 8.411367416381836
Epoch 770, val loss: 0.9138858914375305
Epoch 780, training loss: 85.01128387451172 = 0.8996630907058716 + 10.0 * 8.411161422729492
Epoch 780, val loss: 0.9082900881767273
Epoch 790, training loss: 84.99627685546875 = 0.8934839963912964 + 10.0 * 8.410279273986816
Epoch 790, val loss: 0.902449369430542
Epoch 800, training loss: 84.98228454589844 = 0.8871707320213318 + 10.0 * 8.40951156616211
Epoch 800, val loss: 0.896476149559021
Epoch 810, training loss: 84.95889282226562 = 0.8807664513587952 + 10.0 * 8.407812118530273
Epoch 810, val loss: 0.8904195427894592
Epoch 820, training loss: 84.9438705444336 = 0.8741945028305054 + 10.0 * 8.406968116760254
Epoch 820, val loss: 0.8842185735702515
Epoch 830, training loss: 84.92964172363281 = 0.8674543499946594 + 10.0 * 8.406218528747559
Epoch 830, val loss: 0.8778398633003235
Epoch 840, training loss: 84.91724395751953 = 0.8604751825332642 + 10.0 * 8.40567684173584
Epoch 840, val loss: 0.8712345361709595
Epoch 850, training loss: 84.9117660522461 = 0.8533377647399902 + 10.0 * 8.405842781066895
Epoch 850, val loss: 0.8644991517066956
Epoch 860, training loss: 84.89378356933594 = 0.8460742235183716 + 10.0 * 8.404770851135254
Epoch 860, val loss: 0.8576454520225525
Epoch 870, training loss: 84.86759185791016 = 0.8387444615364075 + 10.0 * 8.402884483337402
Epoch 870, val loss: 0.8507097363471985
Epoch 880, training loss: 84.84866333007812 = 0.8312867879867554 + 10.0 * 8.401738166809082
Epoch 880, val loss: 0.8436732292175293
Epoch 890, training loss: 84.83468627929688 = 0.8237118124961853 + 10.0 * 8.401097297668457
Epoch 890, val loss: 0.8365262746810913
Epoch 900, training loss: 84.84033203125 = 0.8159798383712769 + 10.0 * 8.402435302734375
Epoch 900, val loss: 0.8292364478111267
Epoch 910, training loss: 84.80817413330078 = 0.808086097240448 + 10.0 * 8.400009155273438
Epoch 910, val loss: 0.8217835426330566
Epoch 920, training loss: 84.79254150390625 = 0.8001484274864197 + 10.0 * 8.399239540100098
Epoch 920, val loss: 0.8143138289451599
Epoch 930, training loss: 84.77481842041016 = 0.7921856641769409 + 10.0 * 8.398263931274414
Epoch 930, val loss: 0.8068156838417053
Epoch 940, training loss: 84.75731658935547 = 0.78413987159729 + 10.0 * 8.397317886352539
Epoch 940, val loss: 0.7992483973503113
Epoch 950, training loss: 84.74290466308594 = 0.7760111093521118 + 10.0 * 8.396689414978027
Epoch 950, val loss: 0.7916097640991211
Epoch 960, training loss: 84.75962829589844 = 0.7677446603775024 + 10.0 * 8.399188041687012
Epoch 960, val loss: 0.7838451266288757
Epoch 970, training loss: 84.721923828125 = 0.7594833374023438 + 10.0 * 8.396244049072266
Epoch 970, val loss: 0.7761011719703674
Epoch 980, training loss: 84.70154571533203 = 0.7512904405593872 + 10.0 * 8.395025253295898
Epoch 980, val loss: 0.7684071063995361
Epoch 990, training loss: 84.69025421142578 = 0.743096113204956 + 10.0 * 8.394716262817383
Epoch 990, val loss: 0.760738730430603
Epoch 1000, training loss: 84.67552947998047 = 0.7349182367324829 + 10.0 * 8.394061088562012
Epoch 1000, val loss: 0.7531006336212158
Epoch 1010, training loss: 84.66016387939453 = 0.7267444729804993 + 10.0 * 8.393342018127441
Epoch 1010, val loss: 0.7454624176025391
Epoch 1020, training loss: 84.66474151611328 = 0.7185863852500916 + 10.0 * 8.394615173339844
Epoch 1020, val loss: 0.7378327250480652
Epoch 1030, training loss: 84.63902282714844 = 0.7104227542877197 + 10.0 * 8.39285945892334
Epoch 1030, val loss: 0.7302711009979248
Epoch 1040, training loss: 84.61962890625 = 0.7023867964744568 + 10.0 * 8.391724586486816
Epoch 1040, val loss: 0.7227712273597717
Epoch 1050, training loss: 84.60813903808594 = 0.6944071054458618 + 10.0 * 8.391373634338379
Epoch 1050, val loss: 0.7153809666633606
Epoch 1060, training loss: 84.60350799560547 = 0.6864995360374451 + 10.0 * 8.391700744628906
Epoch 1060, val loss: 0.7080327272415161
Epoch 1070, training loss: 84.5774917602539 = 0.6786811351776123 + 10.0 * 8.389881134033203
Epoch 1070, val loss: 0.7008122205734253
Epoch 1080, training loss: 84.5730972290039 = 0.6709581613540649 + 10.0 * 8.390213966369629
Epoch 1080, val loss: 0.6936596632003784
Epoch 1090, training loss: 84.55567169189453 = 0.6632711291313171 + 10.0 * 8.389240264892578
Epoch 1090, val loss: 0.6865825653076172
Epoch 1100, training loss: 84.54014587402344 = 0.6557227373123169 + 10.0 * 8.388442039489746
Epoch 1100, val loss: 0.6796284914016724
Epoch 1110, training loss: 84.52327728271484 = 0.6483145952224731 + 10.0 * 8.387495994567871
Epoch 1110, val loss: 0.6728266477584839
Epoch 1120, training loss: 84.5172119140625 = 0.6410046219825745 + 10.0 * 8.38762092590332
Epoch 1120, val loss: 0.6661179661750793
Epoch 1130, training loss: 84.54629516601562 = 0.6337764263153076 + 10.0 * 8.391252517700195
Epoch 1130, val loss: 0.6594686508178711
Epoch 1140, training loss: 84.49227905273438 = 0.6266948580741882 + 10.0 * 8.386558532714844
Epoch 1140, val loss: 0.6530088782310486
Epoch 1150, training loss: 84.474609375 = 0.6198544502258301 + 10.0 * 8.385475158691406
Epoch 1150, val loss: 0.6467544436454773
Epoch 1160, training loss: 84.46221923828125 = 0.6131600737571716 + 10.0 * 8.384905815124512
Epoch 1160, val loss: 0.6406520009040833
Epoch 1170, training loss: 84.44853973388672 = 0.6065956354141235 + 10.0 * 8.384194374084473
Epoch 1170, val loss: 0.6346930265426636
Epoch 1180, training loss: 84.45346069335938 = 0.6001314520835876 + 10.0 * 8.385333061218262
Epoch 1180, val loss: 0.6288313865661621
Epoch 1190, training loss: 84.43800354003906 = 0.5937340259552002 + 10.0 * 8.384427070617676
Epoch 1190, val loss: 0.6230430006980896
Epoch 1200, training loss: 84.422607421875 = 0.5875871777534485 + 10.0 * 8.383502006530762
Epoch 1200, val loss: 0.6174856424331665
Epoch 1210, training loss: 84.40987396240234 = 0.5816305875778198 + 10.0 * 8.382824897766113
Epoch 1210, val loss: 0.6121310591697693
Epoch 1220, training loss: 84.39546203613281 = 0.5758113265037537 + 10.0 * 8.381964683532715
Epoch 1220, val loss: 0.6069086790084839
Epoch 1230, training loss: 84.38710021972656 = 0.5701267123222351 + 10.0 * 8.381696701049805
Epoch 1230, val loss: 0.6018160581588745
Epoch 1240, training loss: 84.43649291992188 = 0.5645308494567871 + 10.0 * 8.38719654083252
Epoch 1240, val loss: 0.5968074202537537
Epoch 1250, training loss: 84.3838119506836 = 0.5591036081314087 + 10.0 * 8.382471084594727
Epoch 1250, val loss: 0.5919986963272095
Epoch 1260, training loss: 84.35968017578125 = 0.5538615584373474 + 10.0 * 8.380581855773926
Epoch 1260, val loss: 0.5873510241508484
Epoch 1270, training loss: 84.3494873046875 = 0.5487862825393677 + 10.0 * 8.380069732666016
Epoch 1270, val loss: 0.5828647613525391
Epoch 1280, training loss: 84.33940124511719 = 0.5438541173934937 + 10.0 * 8.379554748535156
Epoch 1280, val loss: 0.5785257816314697
Epoch 1290, training loss: 84.33082580566406 = 0.5390439629554749 + 10.0 * 8.379178047180176
Epoch 1290, val loss: 0.5743072628974915
Epoch 1300, training loss: 84.34004974365234 = 0.5343486666679382 + 10.0 * 8.380570411682129
Epoch 1300, val loss: 0.5701773166656494
Epoch 1310, training loss: 84.33092498779297 = 0.5297765135765076 + 10.0 * 8.380114555358887
Epoch 1310, val loss: 0.5662692785263062
Epoch 1320, training loss: 84.31498718261719 = 0.5253921151161194 + 10.0 * 8.378959655761719
Epoch 1320, val loss: 0.5624337196350098
Epoch 1330, training loss: 84.30167388916016 = 0.5211696624755859 + 10.0 * 8.378049850463867
Epoch 1330, val loss: 0.5587976574897766
Epoch 1340, training loss: 84.29281616210938 = 0.51708984375 + 10.0 * 8.377573013305664
Epoch 1340, val loss: 0.55532306432724
Epoch 1350, training loss: 84.30740356445312 = 0.5131231546401978 + 10.0 * 8.379427909851074
Epoch 1350, val loss: 0.5519711971282959
Epoch 1360, training loss: 84.27706909179688 = 0.509235143661499 + 10.0 * 8.37678337097168
Epoch 1360, val loss: 0.5486481189727783
Epoch 1370, training loss: 84.27111053466797 = 0.5055044293403625 + 10.0 * 8.376561164855957
Epoch 1370, val loss: 0.5455166101455688
Epoch 1380, training loss: 84.26665496826172 = 0.5019016861915588 + 10.0 * 8.37647533416748
Epoch 1380, val loss: 0.5425188541412354
Epoch 1390, training loss: 84.28266143798828 = 0.4984022378921509 + 10.0 * 8.378425598144531
Epoch 1390, val loss: 0.5396209359169006
Epoch 1400, training loss: 84.25570678710938 = 0.4950072765350342 + 10.0 * 8.376070022583008
Epoch 1400, val loss: 0.5368036031723022
Epoch 1410, training loss: 84.24593353271484 = 0.491736501455307 + 10.0 * 8.375419616699219
Epoch 1410, val loss: 0.5341310501098633
Epoch 1420, training loss: 84.23797607421875 = 0.48857298493385315 + 10.0 * 8.374940872192383
Epoch 1420, val loss: 0.5315693020820618
Epoch 1430, training loss: 84.26424407958984 = 0.48549067974090576 + 10.0 * 8.377875328063965
Epoch 1430, val loss: 0.5290846228599548
Epoch 1440, training loss: 84.24537658691406 = 0.4824933707714081 + 10.0 * 8.376288414001465
Epoch 1440, val loss: 0.5266754627227783
Epoch 1450, training loss: 84.2236099243164 = 0.4796217978000641 + 10.0 * 8.374399185180664
Epoch 1450, val loss: 0.5244337320327759
Epoch 1460, training loss: 84.2122573852539 = 0.47684749960899353 + 10.0 * 8.373540878295898
Epoch 1460, val loss: 0.5222573280334473
Epoch 1470, training loss: 84.20767974853516 = 0.47416484355926514 + 10.0 * 8.373351097106934
Epoch 1470, val loss: 0.5201698541641235
Epoch 1480, training loss: 84.23551940917969 = 0.471556156873703 + 10.0 * 8.376396179199219
Epoch 1480, val loss: 0.5181910991668701
Epoch 1490, training loss: 84.21794128417969 = 0.46899765729904175 + 10.0 * 8.374895095825195
Epoch 1490, val loss: 0.5162233114242554
Epoch 1500, training loss: 84.19610595703125 = 0.46655330061912537 + 10.0 * 8.372955322265625
Epoch 1500, val loss: 0.5143845677375793
Epoch 1510, training loss: 84.18543243408203 = 0.4641878604888916 + 10.0 * 8.372124671936035
Epoch 1510, val loss: 0.5126135945320129
Epoch 1520, training loss: 84.18580627441406 = 0.4618960916996002 + 10.0 * 8.372390747070312
Epoch 1520, val loss: 0.5109195709228516
Epoch 1530, training loss: 84.20429992675781 = 0.45965638756752014 + 10.0 * 8.37446403503418
Epoch 1530, val loss: 0.5092765092849731
Epoch 1540, training loss: 84.17530822753906 = 0.45749929547309875 + 10.0 * 8.371781349182129
Epoch 1540, val loss: 0.5077850222587585
Epoch 1550, training loss: 84.16427612304688 = 0.45538991689682007 + 10.0 * 8.370888710021973
Epoch 1550, val loss: 0.5062653422355652
Epoch 1560, training loss: 84.16207122802734 = 0.45334839820861816 + 10.0 * 8.370872497558594
Epoch 1560, val loss: 0.504837155342102
Epoch 1570, training loss: 84.15576171875 = 0.4513561427593231 + 10.0 * 8.370440483093262
Epoch 1570, val loss: 0.5034630298614502
Epoch 1580, training loss: 84.15786743164062 = 0.44941091537475586 + 10.0 * 8.370845794677734
Epoch 1580, val loss: 0.5021267533302307
Epoch 1590, training loss: 84.16769409179688 = 0.4475095272064209 + 10.0 * 8.372018814086914
Epoch 1590, val loss: 0.5008519291877747
Epoch 1600, training loss: 84.1534423828125 = 0.44567951560020447 + 10.0 * 8.370776176452637
Epoch 1600, val loss: 0.49964192509651184
Epoch 1610, training loss: 84.14476776123047 = 0.4439058303833008 + 10.0 * 8.370085716247559
Epoch 1610, val loss: 0.49848300218582153
Epoch 1620, training loss: 84.1339340209961 = 0.44218987226486206 + 10.0 * 8.369174003601074
Epoch 1620, val loss: 0.4973577857017517
Epoch 1630, training loss: 84.13082122802734 = 0.4405187964439392 + 10.0 * 8.369029998779297
Epoch 1630, val loss: 0.49629461765289307
Epoch 1640, training loss: 84.13615417480469 = 0.4388786852359772 + 10.0 * 8.36972713470459
Epoch 1640, val loss: 0.49525511264801025
Epoch 1650, training loss: 84.14054870605469 = 0.437270849943161 + 10.0 * 8.370327949523926
Epoch 1650, val loss: 0.49427786469459534
Epoch 1660, training loss: 84.1331787109375 = 0.4357142448425293 + 10.0 * 8.369746208190918
Epoch 1660, val loss: 0.4933590292930603
Epoch 1670, training loss: 84.11470031738281 = 0.43419548869132996 + 10.0 * 8.368050575256348
Epoch 1670, val loss: 0.4924520254135132
Epoch 1680, training loss: 84.10836791992188 = 0.4327273666858673 + 10.0 * 8.36756420135498
Epoch 1680, val loss: 0.4915897250175476
Epoch 1690, training loss: 84.10484313964844 = 0.4312893748283386 + 10.0 * 8.367355346679688
Epoch 1690, val loss: 0.49076923727989197
Epoch 1700, training loss: 84.1050033569336 = 0.4298771619796753 + 10.0 * 8.367512702941895
Epoch 1700, val loss: 0.4899598956108093
Epoch 1710, training loss: 84.12242126464844 = 0.4284856915473938 + 10.0 * 8.369394302368164
Epoch 1710, val loss: 0.4891774654388428
Epoch 1720, training loss: 84.10892486572266 = 0.4271276295185089 + 10.0 * 8.368180274963379
Epoch 1720, val loss: 0.48844999074935913
Epoch 1730, training loss: 84.11296844482422 = 0.42579829692840576 + 10.0 * 8.368717193603516
Epoch 1730, val loss: 0.48771870136260986
Epoch 1740, training loss: 84.08724212646484 = 0.42450347542762756 + 10.0 * 8.366273880004883
Epoch 1740, val loss: 0.4870688319206238
Epoch 1750, training loss: 84.08633422851562 = 0.42324334383010864 + 10.0 * 8.36630916595459
Epoch 1750, val loss: 0.48643654584884644
Epoch 1760, training loss: 84.08056640625 = 0.4219996929168701 + 10.0 * 8.365857124328613
Epoch 1760, val loss: 0.4857911169528961
Epoch 1770, training loss: 84.07694244384766 = 0.4207807779312134 + 10.0 * 8.365615844726562
Epoch 1770, val loss: 0.4852025806903839
Epoch 1780, training loss: 84.08679962158203 = 0.41958171129226685 + 10.0 * 8.366722106933594
Epoch 1780, val loss: 0.48465412855148315
Epoch 1790, training loss: 84.10012817382812 = 0.41839879751205444 + 10.0 * 8.368173599243164
Epoch 1790, val loss: 0.48409682512283325
Epoch 1800, training loss: 84.07591247558594 = 0.41723620891571045 + 10.0 * 8.365867614746094
Epoch 1800, val loss: 0.48351728916168213
Epoch 1810, training loss: 84.06845092773438 = 0.41611501574516296 + 10.0 * 8.365233421325684
Epoch 1810, val loss: 0.48302388191223145
Epoch 1820, training loss: 84.05939483642578 = 0.4150135815143585 + 10.0 * 8.3644380569458
Epoch 1820, val loss: 0.4825308322906494
Epoch 1830, training loss: 84.05664825439453 = 0.4139285683631897 + 10.0 * 8.364272117614746
Epoch 1830, val loss: 0.48207372426986694
Epoch 1840, training loss: 84.0614242553711 = 0.412855863571167 + 10.0 * 8.364856719970703
Epoch 1840, val loss: 0.48162955045700073
Epoch 1850, training loss: 84.08092498779297 = 0.41179582476615906 + 10.0 * 8.366912841796875
Epoch 1850, val loss: 0.4812287390232086
Epoch 1860, training loss: 84.05793762207031 = 0.4107479453086853 + 10.0 * 8.364718437194824
Epoch 1860, val loss: 0.4807688295841217
Epoch 1870, training loss: 84.04766082763672 = 0.40972408652305603 + 10.0 * 8.363794326782227
Epoch 1870, val loss: 0.48035192489624023
Epoch 1880, training loss: 84.04217529296875 = 0.4087209105491638 + 10.0 * 8.3633451461792
Epoch 1880, val loss: 0.4799831807613373
Epoch 1890, training loss: 84.04473876953125 = 0.40772706270217896 + 10.0 * 8.363700866699219
Epoch 1890, val loss: 0.4795979857444763
Epoch 1900, training loss: 84.05451965332031 = 0.40674319863319397 + 10.0 * 8.364777565002441
Epoch 1900, val loss: 0.47922444343566895
Epoch 1910, training loss: 84.04582214355469 = 0.40577882528305054 + 10.0 * 8.364004135131836
Epoch 1910, val loss: 0.47890517115592957
Epoch 1920, training loss: 84.03345489501953 = 0.4048294126987457 + 10.0 * 8.362862586975098
Epoch 1920, val loss: 0.4786152243614197
Epoch 1930, training loss: 84.03584289550781 = 0.40389740467071533 + 10.0 * 8.363194465637207
Epoch 1930, val loss: 0.4783143997192383
Epoch 1940, training loss: 84.0215835571289 = 0.40296971797943115 + 10.0 * 8.361861228942871
Epoch 1940, val loss: 0.4779750406742096
Epoch 1950, training loss: 84.02056884765625 = 0.4020572900772095 + 10.0 * 8.361851692199707
Epoch 1950, val loss: 0.4776797890663147
Epoch 1960, training loss: 84.03121185302734 = 0.4011533856391907 + 10.0 * 8.363005638122559
Epoch 1960, val loss: 0.47738131880760193
Epoch 1970, training loss: 84.02757263183594 = 0.4002592861652374 + 10.0 * 8.362730979919434
Epoch 1970, val loss: 0.4771532118320465
Epoch 1980, training loss: 84.01768493652344 = 0.39938533306121826 + 10.0 * 8.36182975769043
Epoch 1980, val loss: 0.47692376375198364
Epoch 1990, training loss: 84.01451110839844 = 0.3985142111778259 + 10.0 * 8.361599922180176
Epoch 1990, val loss: 0.47663018107414246
Epoch 2000, training loss: 84.02134704589844 = 0.39766016602516174 + 10.0 * 8.3623685836792
Epoch 2000, val loss: 0.4764277935028076
Epoch 2010, training loss: 84.00240325927734 = 0.39681464433670044 + 10.0 * 8.360559463500977
Epoch 2010, val loss: 0.47621819376945496
Epoch 2020, training loss: 84.00120544433594 = 0.39597901701927185 + 10.0 * 8.360522270202637
Epoch 2020, val loss: 0.4760284721851349
Epoch 2030, training loss: 84.0018081665039 = 0.39515063166618347 + 10.0 * 8.360666275024414
Epoch 2030, val loss: 0.4758295714855194
Epoch 2040, training loss: 84.0216064453125 = 0.39432668685913086 + 10.0 * 8.362728118896484
Epoch 2040, val loss: 0.4756189286708832
Epoch 2050, training loss: 83.99994659423828 = 0.39351481199264526 + 10.0 * 8.36064338684082
Epoch 2050, val loss: 0.47540155053138733
Epoch 2060, training loss: 83.9892578125 = 0.392720490694046 + 10.0 * 8.35965347290039
Epoch 2060, val loss: 0.47524383664131165
Epoch 2070, training loss: 83.98693084716797 = 0.39192861318588257 + 10.0 * 8.35949993133545
Epoch 2070, val loss: 0.4750533699989319
Epoch 2080, training loss: 84.00755310058594 = 0.3911433517932892 + 10.0 * 8.361640930175781
Epoch 2080, val loss: 0.4748677611351013
Epoch 2090, training loss: 83.98473358154297 = 0.39037036895751953 + 10.0 * 8.35943603515625
Epoch 2090, val loss: 0.47479039430618286
Epoch 2100, training loss: 83.97970581054688 = 0.38960233330726624 + 10.0 * 8.359010696411133
Epoch 2100, val loss: 0.47461533546447754
Epoch 2110, training loss: 83.9791488647461 = 0.388842910528183 + 10.0 * 8.359030723571777
Epoch 2110, val loss: 0.4744713306427002
Epoch 2120, training loss: 83.98130798339844 = 0.3880918323993683 + 10.0 * 8.359321594238281
Epoch 2120, val loss: 0.4743552505970001
Epoch 2130, training loss: 83.97148895263672 = 0.38734742999076843 + 10.0 * 8.358414649963379
Epoch 2130, val loss: 0.4742549955844879
Epoch 2140, training loss: 83.9771957397461 = 0.3866075277328491 + 10.0 * 8.35905933380127
Epoch 2140, val loss: 0.4741489887237549
Epoch 2150, training loss: 83.98907470703125 = 0.3858720064163208 + 10.0 * 8.360320091247559
Epoch 2150, val loss: 0.4740316867828369
Epoch 2160, training loss: 83.96837615966797 = 0.3851461410522461 + 10.0 * 8.358323097229004
Epoch 2160, val loss: 0.4739098846912384
Epoch 2170, training loss: 83.9598159790039 = 0.38442593812942505 + 10.0 * 8.357539176940918
Epoch 2170, val loss: 0.47381505370140076
Epoch 2180, training loss: 83.95724487304688 = 0.38371095061302185 + 10.0 * 8.357353210449219
Epoch 2180, val loss: 0.47372668981552124
Epoch 2190, training loss: 83.9787368774414 = 0.38299718499183655 + 10.0 * 8.359574317932129
Epoch 2190, val loss: 0.4736103117465973
Epoch 2200, training loss: 83.95149230957031 = 0.3822944462299347 + 10.0 * 8.35692024230957
Epoch 2200, val loss: 0.4736076295375824
Epoch 2210, training loss: 83.9494857788086 = 0.38159307837486267 + 10.0 * 8.356789588928223
Epoch 2210, val loss: 0.4735122323036194
Epoch 2220, training loss: 83.96395874023438 = 0.38090354204177856 + 10.0 * 8.358304977416992
Epoch 2220, val loss: 0.47353604435920715
Epoch 2230, training loss: 83.94790649414062 = 0.38020652532577515 + 10.0 * 8.356770515441895
Epoch 2230, val loss: 0.4733746349811554
Epoch 2240, training loss: 83.942138671875 = 0.3795320391654968 + 10.0 * 8.356260299682617
Epoch 2240, val loss: 0.4733749330043793
Epoch 2250, training loss: 83.94105529785156 = 0.378856360912323 + 10.0 * 8.356220245361328
Epoch 2250, val loss: 0.47326961159706116
Epoch 2260, training loss: 83.93699645996094 = 0.3781897723674774 + 10.0 * 8.355880737304688
Epoch 2260, val loss: 0.47326478362083435
Epoch 2270, training loss: 83.93717193603516 = 0.3775208294391632 + 10.0 * 8.355965614318848
Epoch 2270, val loss: 0.47319933772087097
Epoch 2280, training loss: 83.98934173583984 = 0.3768562376499176 + 10.0 * 8.361248970031738
Epoch 2280, val loss: 0.4731692373752594
Epoch 2290, training loss: 83.93714904785156 = 0.37619879841804504 + 10.0 * 8.356095314025879
Epoch 2290, val loss: 0.47314774990081787
Epoch 2300, training loss: 83.93018341064453 = 0.3755555748939514 + 10.0 * 8.355463027954102
Epoch 2300, val loss: 0.47314223647117615
Epoch 2310, training loss: 83.92581176757812 = 0.37491196393966675 + 10.0 * 8.355090141296387
Epoch 2310, val loss: 0.4731215834617615
Epoch 2320, training loss: 83.92391204833984 = 0.3742712140083313 + 10.0 * 8.354964256286621
Epoch 2320, val loss: 0.473113477230072
Epoch 2330, training loss: 83.97472381591797 = 0.37363630533218384 + 10.0 * 8.360109329223633
Epoch 2330, val loss: 0.47315657138824463
Epoch 2340, training loss: 83.93988037109375 = 0.37300267815589905 + 10.0 * 8.356687545776367
Epoch 2340, val loss: 0.47309213876724243
Epoch 2350, training loss: 83.91996002197266 = 0.3723808825016022 + 10.0 * 8.354757308959961
Epoch 2350, val loss: 0.47310560941696167
Epoch 2360, training loss: 83.91284942626953 = 0.3717612624168396 + 10.0 * 8.354108810424805
Epoch 2360, val loss: 0.47311094403266907
Epoch 2370, training loss: 83.9097671508789 = 0.37114590406417847 + 10.0 * 8.353861808776855
Epoch 2370, val loss: 0.4731309711933136
Epoch 2380, training loss: 83.90721130371094 = 0.3705296516418457 + 10.0 * 8.353668212890625
Epoch 2380, val loss: 0.4731460511684418
Epoch 2390, training loss: 83.90896606445312 = 0.36991581320762634 + 10.0 * 8.353904724121094
Epoch 2390, val loss: 0.4731820523738861
Epoch 2400, training loss: 83.99227905273438 = 0.36931049823760986 + 10.0 * 8.362297058105469
Epoch 2400, val loss: 0.47327733039855957
Epoch 2410, training loss: 83.91020202636719 = 0.36869505047798157 + 10.0 * 8.354150772094727
Epoch 2410, val loss: 0.4732162356376648
Epoch 2420, training loss: 83.90602111816406 = 0.3680994510650635 + 10.0 * 8.353792190551758
Epoch 2420, val loss: 0.4732668697834015
Epoch 2430, training loss: 83.89718627929688 = 0.36750495433807373 + 10.0 * 8.352968215942383
Epoch 2430, val loss: 0.47330302000045776
Epoch 2440, training loss: 83.89421081542969 = 0.3669114410877228 + 10.0 * 8.352729797363281
Epoch 2440, val loss: 0.47334450483322144
Epoch 2450, training loss: 83.89190673828125 = 0.366317480802536 + 10.0 * 8.352559089660645
Epoch 2450, val loss: 0.47338926792144775
Epoch 2460, training loss: 83.88967895507812 = 0.3657250702381134 + 10.0 * 8.352396011352539
Epoch 2460, val loss: 0.4734463095664978
Epoch 2470, training loss: 83.90416717529297 = 0.36513176560401917 + 10.0 * 8.353902816772461
Epoch 2470, val loss: 0.4734742045402527
Epoch 2480, training loss: 83.89740753173828 = 0.3645413815975189 + 10.0 * 8.353286743164062
Epoch 2480, val loss: 0.47353339195251465
Epoch 2490, training loss: 83.89644622802734 = 0.3639664649963379 + 10.0 * 8.35324764251709
Epoch 2490, val loss: 0.4736374020576477
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8006088280060882
0.8117800478156924
=== training gcn model ===
Epoch 0, training loss: 106.91797637939453 = 1.095576524734497 + 10.0 * 10.582240104675293
Epoch 0, val loss: 1.0965272188186646
Epoch 10, training loss: 106.90947723388672 = 1.0913110971450806 + 10.0 * 10.581816673278809
Epoch 10, val loss: 1.092239260673523
Epoch 20, training loss: 106.88633728027344 = 1.0868592262268066 + 10.0 * 10.579947471618652
Epoch 20, val loss: 1.0877289772033691
Epoch 30, training loss: 106.79776000976562 = 1.0821105241775513 + 10.0 * 10.571565628051758
Epoch 30, val loss: 1.0829278230667114
Epoch 40, training loss: 106.45085144042969 = 1.0771245956420898 + 10.0 * 10.537372589111328
Epoch 40, val loss: 1.0778727531433105
Epoch 50, training loss: 105.3135986328125 = 1.071951985359192 + 10.0 * 10.424164772033691
Epoch 50, val loss: 1.072658896446228
Epoch 60, training loss: 102.08839416503906 = 1.0670886039733887 + 10.0 * 10.102130889892578
Epoch 60, val loss: 1.0677108764648438
Epoch 70, training loss: 97.114990234375 = 1.0620417594909668 + 10.0 * 9.605295181274414
Epoch 70, val loss: 1.0626450777053833
Epoch 80, training loss: 94.49651336669922 = 1.057032823562622 + 10.0 * 9.343948364257812
Epoch 80, val loss: 1.0577092170715332
Epoch 90, training loss: 93.267578125 = 1.0520203113555908 + 10.0 * 9.221555709838867
Epoch 90, val loss: 1.0528990030288696
Epoch 100, training loss: 92.90655517578125 = 1.0481724739074707 + 10.0 * 9.18583869934082
Epoch 100, val loss: 1.0492970943450928
Epoch 110, training loss: 92.43295288085938 = 1.0457466840744019 + 10.0 * 9.138720512390137
Epoch 110, val loss: 1.0470253229141235
Epoch 120, training loss: 91.75083923339844 = 1.044515609741211 + 10.0 * 9.070631980895996
Epoch 120, val loss: 1.0459028482437134
Epoch 130, training loss: 90.88899993896484 = 1.0440436601638794 + 10.0 * 8.984495162963867
Epoch 130, val loss: 1.045515775680542
Epoch 140, training loss: 90.14488983154297 = 1.043872594833374 + 10.0 * 8.910101890563965
Epoch 140, val loss: 1.045356035232544
Epoch 150, training loss: 89.56768035888672 = 1.0433521270751953 + 10.0 * 8.852433204650879
Epoch 150, val loss: 1.0448226928710938
Epoch 160, training loss: 88.964599609375 = 1.0429831743240356 + 10.0 * 8.79216194152832
Epoch 160, val loss: 1.0444380044937134
Epoch 170, training loss: 88.46986389160156 = 1.042880892753601 + 10.0 * 8.742698669433594
Epoch 170, val loss: 1.0442447662353516
Epoch 180, training loss: 88.02411651611328 = 1.0427228212356567 + 10.0 * 8.698139190673828
Epoch 180, val loss: 1.0440360307693481
Epoch 190, training loss: 87.69305419921875 = 1.042502522468567 + 10.0 * 8.665055274963379
Epoch 190, val loss: 1.043736219406128
Epoch 200, training loss: 87.46078491210938 = 1.042101263999939 + 10.0 * 8.641868591308594
Epoch 200, val loss: 1.0432167053222656
Epoch 210, training loss: 87.27995300292969 = 1.0415103435516357 + 10.0 * 8.623844146728516
Epoch 210, val loss: 1.0425963401794434
Epoch 220, training loss: 87.11380767822266 = 1.0409560203552246 + 10.0 * 8.607285499572754
Epoch 220, val loss: 1.0419832468032837
Epoch 230, training loss: 86.95337677001953 = 1.0404406785964966 + 10.0 * 8.591293334960938
Epoch 230, val loss: 1.0414880514144897
Epoch 240, training loss: 86.81183624267578 = 1.0399621725082397 + 10.0 * 8.577187538146973
Epoch 240, val loss: 1.0410329103469849
Epoch 250, training loss: 86.69587707519531 = 1.0394610166549683 + 10.0 * 8.565641403198242
Epoch 250, val loss: 1.04054594039917
Epoch 260, training loss: 86.55973815917969 = 1.038906455039978 + 10.0 * 8.552083015441895
Epoch 260, val loss: 1.0400254726409912
Epoch 270, training loss: 86.45263671875 = 1.0383260250091553 + 10.0 * 8.541431427001953
Epoch 270, val loss: 1.0394940376281738
Epoch 280, training loss: 86.35799407958984 = 1.0377070903778076 + 10.0 * 8.532029151916504
Epoch 280, val loss: 1.0388641357421875
Epoch 290, training loss: 86.2601089477539 = 1.0370185375213623 + 10.0 * 8.522309303283691
Epoch 290, val loss: 1.0382163524627686
Epoch 300, training loss: 86.17418670654297 = 1.0362882614135742 + 10.0 * 8.513790130615234
Epoch 300, val loss: 1.0375263690948486
Epoch 310, training loss: 86.14364624023438 = 1.0355035066604614 + 10.0 * 8.51081371307373
Epoch 310, val loss: 1.0367698669433594
Epoch 320, training loss: 86.05413818359375 = 1.034604549407959 + 10.0 * 8.501953125
Epoch 320, val loss: 1.0359448194503784
Epoch 330, training loss: 85.9905014038086 = 1.0336679220199585 + 10.0 * 8.495683670043945
Epoch 330, val loss: 1.035044550895691
Epoch 340, training loss: 85.94404602050781 = 1.032670497894287 + 10.0 * 8.491137504577637
Epoch 340, val loss: 1.0340993404388428
Epoch 350, training loss: 85.91399383544922 = 1.0315747261047363 + 10.0 * 8.488241195678711
Epoch 350, val loss: 1.033071756362915
Epoch 360, training loss: 85.8497085571289 = 1.0304213762283325 + 10.0 * 8.481928825378418
Epoch 360, val loss: 1.0319888591766357
Epoch 370, training loss: 85.81343841552734 = 1.0292317867279053 + 10.0 * 8.478421211242676
Epoch 370, val loss: 1.0308808088302612
Epoch 380, training loss: 85.76714324951172 = 1.0279916524887085 + 10.0 * 8.473915100097656
Epoch 380, val loss: 1.0296729803085327
Epoch 390, training loss: 85.7262954711914 = 1.0266802310943604 + 10.0 * 8.469961166381836
Epoch 390, val loss: 1.0284674167633057
Epoch 400, training loss: 85.68385314941406 = 1.0253547430038452 + 10.0 * 8.465849876403809
Epoch 400, val loss: 1.0272048711776733
Epoch 410, training loss: 85.65001678466797 = 1.0239675045013428 + 10.0 * 8.462605476379395
Epoch 410, val loss: 1.0259054899215698
Epoch 420, training loss: 85.64230346679688 = 1.0224888324737549 + 10.0 * 8.461980819702148
Epoch 420, val loss: 1.0244998931884766
Epoch 430, training loss: 85.59431457519531 = 1.0209128856658936 + 10.0 * 8.457340240478516
Epoch 430, val loss: 1.0230140686035156
Epoch 440, training loss: 85.55693817138672 = 1.0193248987197876 + 10.0 * 8.453761100769043
Epoch 440, val loss: 1.0215153694152832
Epoch 450, training loss: 85.52857208251953 = 1.0176820755004883 + 10.0 * 8.451088905334473
Epoch 450, val loss: 1.0199705362319946
Epoch 460, training loss: 85.52286529541016 = 1.0159540176391602 + 10.0 * 8.450691223144531
Epoch 460, val loss: 1.0183391571044922
Epoch 470, training loss: 85.49505615234375 = 1.0140973329544067 + 10.0 * 8.448095321655273
Epoch 470, val loss: 1.0165754556655884
Epoch 480, training loss: 85.46381378173828 = 1.0121709108352661 + 10.0 * 8.445164680480957
Epoch 480, val loss: 1.0147621631622314
Epoch 490, training loss: 85.43547058105469 = 1.010183334350586 + 10.0 * 8.44252872467041
Epoch 490, val loss: 1.0128957033157349
Epoch 500, training loss: 85.43186950683594 = 1.0081250667572021 + 10.0 * 8.442374229431152
Epoch 500, val loss: 1.0109295845031738
Epoch 510, training loss: 85.39730834960938 = 1.0059000253677368 + 10.0 * 8.439141273498535
Epoch 510, val loss: 1.0088825225830078
Epoch 520, training loss: 85.37580871582031 = 1.0036287307739258 + 10.0 * 8.437217712402344
Epoch 520, val loss: 1.0067222118377686
Epoch 530, training loss: 85.35307312011719 = 1.001251459121704 + 10.0 * 8.435182571411133
Epoch 530, val loss: 1.0044993162155151
Epoch 540, training loss: 85.33839416503906 = 0.9987663626670837 + 10.0 * 8.43396282196045
Epoch 540, val loss: 1.002152919769287
Epoch 550, training loss: 85.37312316894531 = 0.9961366057395935 + 10.0 * 8.437698364257812
Epoch 550, val loss: 0.9996259212493896
Epoch 560, training loss: 85.31658935546875 = 0.9932833313941956 + 10.0 * 8.432330131530762
Epoch 560, val loss: 0.996990442276001
Epoch 570, training loss: 85.28844451904297 = 0.9904237985610962 + 10.0 * 8.429801940917969
Epoch 570, val loss: 0.9943214654922485
Epoch 580, training loss: 85.26603698730469 = 0.9874782562255859 + 10.0 * 8.427855491638184
Epoch 580, val loss: 0.9915467500686646
Epoch 590, training loss: 85.24691009521484 = 0.9843830466270447 + 10.0 * 8.426252365112305
Epoch 590, val loss: 0.9886428713798523
Epoch 600, training loss: 85.23070526123047 = 0.9811356663703918 + 10.0 * 8.424957275390625
Epoch 600, val loss: 0.985601007938385
Epoch 610, training loss: 85.26835632324219 = 0.9776989221572876 + 10.0 * 8.429065704345703
Epoch 610, val loss: 0.9823954105377197
Epoch 620, training loss: 85.21463012695312 = 0.9740551710128784 + 10.0 * 8.424057960510254
Epoch 620, val loss: 0.9789435267448425
Epoch 630, training loss: 85.18683624267578 = 0.9703428745269775 + 10.0 * 8.421648979187012
Epoch 630, val loss: 0.9754667282104492
Epoch 640, training loss: 85.16687774658203 = 0.9665302038192749 + 10.0 * 8.420034408569336
Epoch 640, val loss: 0.9718986749649048
Epoch 650, training loss: 85.14842224121094 = 0.9625546932220459 + 10.0 * 8.418586730957031
Epoch 650, val loss: 0.9681646823883057
Epoch 660, training loss: 85.13249206542969 = 0.958379328250885 + 10.0 * 8.417410850524902
Epoch 660, val loss: 0.9642652273178101
Epoch 670, training loss: 85.2120590209961 = 0.9539388418197632 + 10.0 * 8.425811767578125
Epoch 670, val loss: 0.9601559638977051
Epoch 680, training loss: 85.13500213623047 = 0.9492722749710083 + 10.0 * 8.418573379516602
Epoch 680, val loss: 0.9557385444641113
Epoch 690, training loss: 85.0952377319336 = 0.9445706605911255 + 10.0 * 8.415066719055176
Epoch 690, val loss: 0.9513447880744934
Epoch 700, training loss: 85.07098388671875 = 0.9397868514060974 + 10.0 * 8.413119316101074
Epoch 700, val loss: 0.9468581080436707
Epoch 710, training loss: 85.05257415771484 = 0.9348085522651672 + 10.0 * 8.411776542663574
Epoch 710, val loss: 0.9421934485435486
Epoch 720, training loss: 85.0357894897461 = 0.9296110272407532 + 10.0 * 8.41061782836914
Epoch 720, val loss: 0.9373258948326111
Epoch 730, training loss: 85.01981353759766 = 0.9241994023323059 + 10.0 * 8.409561157226562
Epoch 730, val loss: 0.9322562217712402
Epoch 740, training loss: 85.05728912353516 = 0.9185827374458313 + 10.0 * 8.413870811462402
Epoch 740, val loss: 0.9269500970840454
Epoch 750, training loss: 85.00419616699219 = 0.912641704082489 + 10.0 * 8.409154891967773
Epoch 750, val loss: 0.9214062690734863
Epoch 760, training loss: 84.97777557373047 = 0.9065775275230408 + 10.0 * 8.407119750976562
Epoch 760, val loss: 0.9157930016517639
Epoch 770, training loss: 84.9603042602539 = 0.9004778265953064 + 10.0 * 8.405982971191406
Epoch 770, val loss: 0.9100697040557861
Epoch 780, training loss: 84.94381713867188 = 0.8941818475723267 + 10.0 * 8.404963493347168
Epoch 780, val loss: 0.9042101502418518
Epoch 790, training loss: 84.93022918701172 = 0.8876813054084778 + 10.0 * 8.404254913330078
Epoch 790, val loss: 0.8981486558914185
Epoch 800, training loss: 84.94608306884766 = 0.8809413313865662 + 10.0 * 8.406514167785645
Epoch 800, val loss: 0.8918911218643188
Epoch 810, training loss: 84.90363311767578 = 0.8740177750587463 + 10.0 * 8.402961730957031
Epoch 810, val loss: 0.8853544592857361
Epoch 820, training loss: 84.88417053222656 = 0.866949737071991 + 10.0 * 8.401721954345703
Epoch 820, val loss: 0.8788106441497803
Epoch 830, training loss: 84.86761474609375 = 0.8597770929336548 + 10.0 * 8.40078353881836
Epoch 830, val loss: 0.8721123933792114
Epoch 840, training loss: 84.8845443725586 = 0.8524208664894104 + 10.0 * 8.40321159362793
Epoch 840, val loss: 0.8652755618095398
Epoch 850, training loss: 84.86353302001953 = 0.8448401689529419 + 10.0 * 8.40186882019043
Epoch 850, val loss: 0.8582009673118591
Epoch 860, training loss: 84.82786560058594 = 0.8372076153755188 + 10.0 * 8.399065971374512
Epoch 860, val loss: 0.8510680794715881
Epoch 870, training loss: 84.812744140625 = 0.8295071721076965 + 10.0 * 8.398324012756348
Epoch 870, val loss: 0.8439120054244995
Epoch 880, training loss: 84.80028533935547 = 0.8216938972473145 + 10.0 * 8.397859573364258
Epoch 880, val loss: 0.8366523385047913
Epoch 890, training loss: 84.8230209350586 = 0.81377112865448 + 10.0 * 8.400924682617188
Epoch 890, val loss: 0.8292310237884521
Epoch 900, training loss: 84.78050231933594 = 0.8056464195251465 + 10.0 * 8.397485733032227
Epoch 900, val loss: 0.8218250870704651
Epoch 910, training loss: 84.7574462890625 = 0.7976329326629639 + 10.0 * 8.395981788635254
Epoch 910, val loss: 0.8143442869186401
Epoch 920, training loss: 84.73924255371094 = 0.7895416617393494 + 10.0 * 8.394969940185547
Epoch 920, val loss: 0.8068987131118774
Epoch 930, training loss: 84.72946166992188 = 0.7814173102378845 + 10.0 * 8.394804000854492
Epoch 930, val loss: 0.7994080185890198
Epoch 940, training loss: 84.74435424804688 = 0.7732394337654114 + 10.0 * 8.397111892700195
Epoch 940, val loss: 0.7918373346328735
Epoch 950, training loss: 84.70352935791016 = 0.7650073766708374 + 10.0 * 8.393852233886719
Epoch 950, val loss: 0.7842432856559753
Epoch 960, training loss: 84.69253540039062 = 0.7568473219871521 + 10.0 * 8.393568992614746
Epoch 960, val loss: 0.776719868183136
Epoch 970, training loss: 84.68171691894531 = 0.7487319707870483 + 10.0 * 8.393298149108887
Epoch 970, val loss: 0.7692769765853882
Epoch 980, training loss: 84.66670989990234 = 0.7406262755393982 + 10.0 * 8.392608642578125
Epoch 980, val loss: 0.7618290781974792
Epoch 990, training loss: 84.6434097290039 = 0.7325770258903503 + 10.0 * 8.391083717346191
Epoch 990, val loss: 0.7543991804122925
Epoch 1000, training loss: 84.63739013671875 = 0.7245855331420898 + 10.0 * 8.391280174255371
Epoch 1000, val loss: 0.7470544576644897
Epoch 1010, training loss: 84.65851593017578 = 0.7166033983230591 + 10.0 * 8.394190788269043
Epoch 1010, val loss: 0.7396979928016663
Epoch 1020, training loss: 84.62417602539062 = 0.7086029648780823 + 10.0 * 8.391557693481445
Epoch 1020, val loss: 0.7324721813201904
Epoch 1030, training loss: 84.60310363769531 = 0.7008179426193237 + 10.0 * 8.390228271484375
Epoch 1030, val loss: 0.7253305315971375
Epoch 1040, training loss: 84.58159637451172 = 0.6931142210960388 + 10.0 * 8.388848304748535
Epoch 1040, val loss: 0.7183152437210083
Epoch 1050, training loss: 84.5684814453125 = 0.6855151057243347 + 10.0 * 8.388296127319336
Epoch 1050, val loss: 0.7113857269287109
Epoch 1060, training loss: 84.56758117675781 = 0.677965521812439 + 10.0 * 8.388961791992188
Epoch 1060, val loss: 0.7045280337333679
Epoch 1070, training loss: 84.54490661621094 = 0.6704844236373901 + 10.0 * 8.387441635131836
Epoch 1070, val loss: 0.6977205276489258
Epoch 1080, training loss: 84.53507995605469 = 0.6631271839141846 + 10.0 * 8.387195587158203
Epoch 1080, val loss: 0.691038191318512
Epoch 1090, training loss: 84.53546905517578 = 0.6559132933616638 + 10.0 * 8.387955665588379
Epoch 1090, val loss: 0.6844868063926697
Epoch 1100, training loss: 84.51809692382812 = 0.6487756967544556 + 10.0 * 8.386932373046875
Epoch 1100, val loss: 0.6780514121055603
Epoch 1110, training loss: 84.50533294677734 = 0.6417616605758667 + 10.0 * 8.386357307434082
Epoch 1110, val loss: 0.6717087030410767
Epoch 1120, training loss: 84.486328125 = 0.6348705887794495 + 10.0 * 8.38514518737793
Epoch 1120, val loss: 0.6655398011207581
Epoch 1130, training loss: 84.47776794433594 = 0.6281242370605469 + 10.0 * 8.384963989257812
Epoch 1130, val loss: 0.6594811677932739
Epoch 1140, training loss: 84.49845886230469 = 0.6214587688446045 + 10.0 * 8.387700080871582
Epoch 1140, val loss: 0.6535086631774902
Epoch 1150, training loss: 84.45931243896484 = 0.6149178743362427 + 10.0 * 8.384439468383789
Epoch 1150, val loss: 0.6475508213043213
Epoch 1160, training loss: 84.44157409667969 = 0.6085071563720703 + 10.0 * 8.383306503295898
Epoch 1160, val loss: 0.6418580412864685
Epoch 1170, training loss: 84.43006896972656 = 0.6022793054580688 + 10.0 * 8.382779121398926
Epoch 1170, val loss: 0.6362942457199097
Epoch 1180, training loss: 84.42144775390625 = 0.5961745977401733 + 10.0 * 8.382527351379395
Epoch 1180, val loss: 0.6308476328849792
Epoch 1190, training loss: 84.43622589111328 = 0.5901625752449036 + 10.0 * 8.38460636138916
Epoch 1190, val loss: 0.6255283355712891
Epoch 1200, training loss: 84.41736602783203 = 0.584289014339447 + 10.0 * 8.383307456970215
Epoch 1200, val loss: 0.6202054023742676
Epoch 1210, training loss: 84.39815521240234 = 0.5785418748855591 + 10.0 * 8.38196086883545
Epoch 1210, val loss: 0.6151946187019348
Epoch 1220, training loss: 84.39195251464844 = 0.5729756355285645 + 10.0 * 8.381897926330566
Epoch 1220, val loss: 0.6102563738822937
Epoch 1230, training loss: 84.391845703125 = 0.5674866437911987 + 10.0 * 8.38243579864502
Epoch 1230, val loss: 0.6054015755653381
Epoch 1240, training loss: 84.36621856689453 = 0.5621747374534607 + 10.0 * 8.380404472351074
Epoch 1240, val loss: 0.6007190346717834
Epoch 1250, training loss: 84.35521697998047 = 0.5570343136787415 + 10.0 * 8.379817962646484
Epoch 1250, val loss: 0.5961881279945374
Epoch 1260, training loss: 84.34490966796875 = 0.5520324110984802 + 10.0 * 8.379287719726562
Epoch 1260, val loss: 0.5917972922325134
Epoch 1270, training loss: 84.33536529541016 = 0.5471442937850952 + 10.0 * 8.378822326660156
Epoch 1270, val loss: 0.5875262022018433
Epoch 1280, training loss: 84.35755157470703 = 0.5423480868339539 + 10.0 * 8.38152027130127
Epoch 1280, val loss: 0.5833178162574768
Epoch 1290, training loss: 84.3388900756836 = 0.5376536250114441 + 10.0 * 8.38012409210205
Epoch 1290, val loss: 0.5792961716651917
Epoch 1300, training loss: 84.32071685791016 = 0.5331055521965027 + 10.0 * 8.378761291503906
Epoch 1300, val loss: 0.575304388999939
Epoch 1310, training loss: 84.3024673461914 = 0.5287484526634216 + 10.0 * 8.377371788024902
Epoch 1310, val loss: 0.5715418457984924
Epoch 1320, training loss: 84.29518127441406 = 0.5245233178138733 + 10.0 * 8.377065658569336
Epoch 1320, val loss: 0.5679301619529724
Epoch 1330, training loss: 84.29209899902344 = 0.5204024314880371 + 10.0 * 8.377169609069824
Epoch 1330, val loss: 0.5643861889839172
Epoch 1340, training loss: 84.29270935058594 = 0.5163772106170654 + 10.0 * 8.377633094787598
Epoch 1340, val loss: 0.5609428882598877
Epoch 1350, training loss: 84.28340148925781 = 0.5124537944793701 + 10.0 * 8.377095222473145
Epoch 1350, val loss: 0.5576395988464355
Epoch 1360, training loss: 84.26847076416016 = 0.5086771249771118 + 10.0 * 8.37597942352295
Epoch 1360, val loss: 0.5544413328170776
Epoch 1370, training loss: 84.31463623046875 = 0.5049898624420166 + 10.0 * 8.380964279174805
Epoch 1370, val loss: 0.5513993501663208
Epoch 1380, training loss: 84.26333618164062 = 0.5014310479164124 + 10.0 * 8.376190185546875
Epoch 1380, val loss: 0.5483230948448181
Epoch 1390, training loss: 84.24970245361328 = 0.49798932671546936 + 10.0 * 8.375171661376953
Epoch 1390, val loss: 0.5455044507980347
Epoch 1400, training loss: 84.23884582519531 = 0.49469026923179626 + 10.0 * 8.374415397644043
Epoch 1400, val loss: 0.5427510738372803
Epoch 1410, training loss: 84.22949981689453 = 0.4914765954017639 + 10.0 * 8.373802185058594
Epoch 1410, val loss: 0.5401228666305542
Epoch 1420, training loss: 84.22421264648438 = 0.4883449673652649 + 10.0 * 8.373586654663086
Epoch 1420, val loss: 0.537563145160675
Epoch 1430, training loss: 84.27227020263672 = 0.4852823317050934 + 10.0 * 8.378698348999023
Epoch 1430, val loss: 0.5350606441497803
Epoch 1440, training loss: 84.23160552978516 = 0.48231005668640137 + 10.0 * 8.374929428100586
Epoch 1440, val loss: 0.5327064990997314
Epoch 1450, training loss: 84.20955657958984 = 0.4794624447822571 + 10.0 * 8.37300968170166
Epoch 1450, val loss: 0.5303933024406433
Epoch 1460, training loss: 84.19963836669922 = 0.4767085611820221 + 10.0 * 8.372293472290039
Epoch 1460, val loss: 0.5282137393951416
Epoch 1470, training loss: 84.21267700195312 = 0.47403261065483093 + 10.0 * 8.373865127563477
Epoch 1470, val loss: 0.5261240005493164
Epoch 1480, training loss: 84.1950454711914 = 0.47141560912132263 + 10.0 * 8.372363090515137
Epoch 1480, val loss: 0.5240840315818787
Epoch 1490, training loss: 84.21480560302734 = 0.46888110041618347 + 10.0 * 8.374592781066895
Epoch 1490, val loss: 0.5221397876739502
Epoch 1500, training loss: 84.1810531616211 = 0.46642398834228516 + 10.0 * 8.37146282196045
Epoch 1500, val loss: 0.5202010869979858
Epoch 1510, training loss: 84.17605590820312 = 0.4640612304210663 + 10.0 * 8.371199607849121
Epoch 1510, val loss: 0.5184348225593567
Epoch 1520, training loss: 84.16826629638672 = 0.46176421642303467 + 10.0 * 8.370650291442871
Epoch 1520, val loss: 0.5167096257209778
Epoch 1530, training loss: 84.16191864013672 = 0.4595297873020172 + 10.0 * 8.3702392578125
Epoch 1530, val loss: 0.5150355696678162
Epoch 1540, training loss: 84.17647552490234 = 0.4573410153388977 + 10.0 * 8.371912956237793
Epoch 1540, val loss: 0.5134128928184509
Epoch 1550, training loss: 84.17345428466797 = 0.4552094638347626 + 10.0 * 8.371824264526367
Epoch 1550, val loss: 0.5119370818138123
Epoch 1560, training loss: 84.15953826904297 = 0.4531448781490326 + 10.0 * 8.370638847351074
Epoch 1560, val loss: 0.5103737711906433
Epoch 1570, training loss: 84.14833068847656 = 0.45116543769836426 + 10.0 * 8.36971664428711
Epoch 1570, val loss: 0.5089925527572632
Epoch 1580, training loss: 84.13899230957031 = 0.449236661195755 + 10.0 * 8.368975639343262
Epoch 1580, val loss: 0.5076242089271545
Epoch 1590, training loss: 84.13616943359375 = 0.4473631978034973 + 10.0 * 8.368880271911621
Epoch 1590, val loss: 0.506304919719696
Epoch 1600, training loss: 84.15898895263672 = 0.44552820920944214 + 10.0 * 8.371346473693848
Epoch 1600, val loss: 0.50502610206604
Epoch 1610, training loss: 84.13446044921875 = 0.4437275528907776 + 10.0 * 8.369073867797852
Epoch 1610, val loss: 0.5039079189300537
Epoch 1620, training loss: 84.13792419433594 = 0.4419880211353302 + 10.0 * 8.369593620300293
Epoch 1620, val loss: 0.5026997327804565
Epoch 1630, training loss: 84.12190246582031 = 0.44029417634010315 + 10.0 * 8.36816120147705
Epoch 1630, val loss: 0.5016167163848877
Epoch 1640, training loss: 84.1143798828125 = 0.43865182995796204 + 10.0 * 8.367572784423828
Epoch 1640, val loss: 0.5005021095275879
Epoch 1650, training loss: 84.11225891113281 = 0.437043696641922 + 10.0 * 8.367521286010742
Epoch 1650, val loss: 0.4994862377643585
Epoch 1660, training loss: 84.14193725585938 = 0.43546950817108154 + 10.0 * 8.370646476745605
Epoch 1660, val loss: 0.49846842885017395
Epoch 1670, training loss: 84.10987091064453 = 0.4339081943035126 + 10.0 * 8.367596626281738
Epoch 1670, val loss: 0.4975796937942505
Epoch 1680, training loss: 84.10063171386719 = 0.4324009120464325 + 10.0 * 8.366823196411133
Epoch 1680, val loss: 0.4966372549533844
Epoch 1690, training loss: 84.10450744628906 = 0.43093985319137573 + 10.0 * 8.36735725402832
Epoch 1690, val loss: 0.4958011209964752
Epoch 1700, training loss: 84.09902954101562 = 0.42951035499572754 + 10.0 * 8.366951942443848
Epoch 1700, val loss: 0.49491721391677856
Epoch 1710, training loss: 84.08800506591797 = 0.4281071722507477 + 10.0 * 8.365989685058594
Epoch 1710, val loss: 0.49410223960876465
Epoch 1720, training loss: 84.08717346191406 = 0.42673373222351074 + 10.0 * 8.366044044494629
Epoch 1720, val loss: 0.4933226704597473
Epoch 1730, training loss: 84.0952377319336 = 0.4253847002983093 + 10.0 * 8.366985321044922
Epoch 1730, val loss: 0.4925510287284851
Epoch 1740, training loss: 84.09830474853516 = 0.4240562319755554 + 10.0 * 8.367424964904785
Epoch 1740, val loss: 0.49184972047805786
Epoch 1750, training loss: 84.08682250976562 = 0.4227654039859772 + 10.0 * 8.366405487060547
Epoch 1750, val loss: 0.4911362826824188
Epoch 1760, training loss: 84.08114624023438 = 0.4214969575405121 + 10.0 * 8.365964889526367
Epoch 1760, val loss: 0.49044033885002136
Epoch 1770, training loss: 84.0859603881836 = 0.42025378346443176 + 10.0 * 8.366570472717285
Epoch 1770, val loss: 0.4898459017276764
Epoch 1780, training loss: 84.06986236572266 = 0.4190428853034973 + 10.0 * 8.365081787109375
Epoch 1780, val loss: 0.4892079830169678
Epoch 1790, training loss: 84.0633316040039 = 0.4178483486175537 + 10.0 * 8.364548683166504
Epoch 1790, val loss: 0.4885966181755066
Epoch 1800, training loss: 84.07505798339844 = 0.4166746735572815 + 10.0 * 8.365839004516602
Epoch 1800, val loss: 0.48796546459198
Epoch 1810, training loss: 84.08644104003906 = 0.41550931334495544 + 10.0 * 8.367093086242676
Epoch 1810, val loss: 0.48752835392951965
Epoch 1820, training loss: 84.0586166381836 = 0.41437336802482605 + 10.0 * 8.364423751831055
Epoch 1820, val loss: 0.48688215017318726
Epoch 1830, training loss: 84.055419921875 = 0.4132670760154724 + 10.0 * 8.364214897155762
Epoch 1830, val loss: 0.4864431321620941
Epoch 1840, training loss: 84.04891967773438 = 0.4121784567832947 + 10.0 * 8.36367416381836
Epoch 1840, val loss: 0.48591580986976624
Epoch 1850, training loss: 84.04505920410156 = 0.4111030399799347 + 10.0 * 8.363395690917969
Epoch 1850, val loss: 0.4854448139667511
Epoch 1860, training loss: 84.05510711669922 = 0.4100402891635895 + 10.0 * 8.364506721496582
Epoch 1860, val loss: 0.4849567413330078
Epoch 1870, training loss: 84.05752563476562 = 0.408988893032074 + 10.0 * 8.364853858947754
Epoch 1870, val loss: 0.48454201221466064
Epoch 1880, training loss: 84.04376983642578 = 0.4079637825489044 + 10.0 * 8.363580703735352
Epoch 1880, val loss: 0.4841421842575073
Epoch 1890, training loss: 84.07532501220703 = 0.40696191787719727 + 10.0 * 8.366836547851562
Epoch 1890, val loss: 0.4837077856063843
Epoch 1900, training loss: 84.03382873535156 = 0.4059672951698303 + 10.0 * 8.362786293029785
Epoch 1900, val loss: 0.48333945870399475
Epoch 1910, training loss: 84.02755737304688 = 0.4050028920173645 + 10.0 * 8.362255096435547
Epoch 1910, val loss: 0.4829607307910919
Epoch 1920, training loss: 84.02693176269531 = 0.4040517508983612 + 10.0 * 8.362287521362305
Epoch 1920, val loss: 0.48262524604797363
Epoch 1930, training loss: 84.02212524414062 = 0.40310704708099365 + 10.0 * 8.361902236938477
Epoch 1930, val loss: 0.48228684067726135
Epoch 1940, training loss: 84.02488708496094 = 0.40217241644859314 + 10.0 * 8.362271308898926
Epoch 1940, val loss: 0.481987327337265
Epoch 1950, training loss: 84.04363250732422 = 0.4012468755245209 + 10.0 * 8.364238739013672
Epoch 1950, val loss: 0.4816882312297821
Epoch 1960, training loss: 84.0440902709961 = 0.4003267288208008 + 10.0 * 8.364376068115234
Epoch 1960, val loss: 0.481320321559906
Epoch 1970, training loss: 84.01689147949219 = 0.39942267537117004 + 10.0 * 8.361746788024902
Epoch 1970, val loss: 0.48106521368026733
Epoch 1980, training loss: 84.01207733154297 = 0.3985339105129242 + 10.0 * 8.361353874206543
Epoch 1980, val loss: 0.4807710349559784
Epoch 1990, training loss: 84.00956726074219 = 0.39765578508377075 + 10.0 * 8.361190795898438
Epoch 1990, val loss: 0.4805411696434021
Epoch 2000, training loss: 84.01998138427734 = 0.3967821002006531 + 10.0 * 8.362319946289062
Epoch 2000, val loss: 0.48029977083206177
Epoch 2010, training loss: 84.01061248779297 = 0.3959164619445801 + 10.0 * 8.361469268798828
Epoch 2010, val loss: 0.4800266921520233
Epoch 2020, training loss: 84.03290557861328 = 0.3950623869895935 + 10.0 * 8.363783836364746
Epoch 2020, val loss: 0.479754775762558
Epoch 2030, training loss: 84.00151062011719 = 0.39422765374183655 + 10.0 * 8.36072826385498
Epoch 2030, val loss: 0.4795793294906616
Epoch 2040, training loss: 83.9942398071289 = 0.39339807629585266 + 10.0 * 8.360084533691406
Epoch 2040, val loss: 0.47934967279434204
Epoch 2050, training loss: 83.99331665039062 = 0.3925783932209015 + 10.0 * 8.360074043273926
Epoch 2050, val loss: 0.47915035486221313
Epoch 2060, training loss: 84.0345458984375 = 0.3917675018310547 + 10.0 * 8.364277839660645
Epoch 2060, val loss: 0.4789343774318695
Epoch 2070, training loss: 84.00311279296875 = 0.39095747470855713 + 10.0 * 8.361215591430664
Epoch 2070, val loss: 0.47880902886390686
Epoch 2080, training loss: 83.98793029785156 = 0.39017075300216675 + 10.0 * 8.359776496887207
Epoch 2080, val loss: 0.4785967469215393
Epoch 2090, training loss: 84.00557708740234 = 0.38938629627227783 + 10.0 * 8.361618995666504
Epoch 2090, val loss: 0.4784362018108368
Epoch 2100, training loss: 83.98460388183594 = 0.3886105716228485 + 10.0 * 8.359599113464355
Epoch 2100, val loss: 0.47833019495010376
Epoch 2110, training loss: 83.97840118408203 = 0.3878462314605713 + 10.0 * 8.359055519104004
Epoch 2110, val loss: 0.4781360924243927
Epoch 2120, training loss: 83.97449493408203 = 0.3870879113674164 + 10.0 * 8.35874080657959
Epoch 2120, val loss: 0.4780026972293854
Epoch 2130, training loss: 83.97313690185547 = 0.3863351345062256 + 10.0 * 8.35867977142334
Epoch 2130, val loss: 0.47791534662246704
Epoch 2140, training loss: 84.05216217041016 = 0.3855854868888855 + 10.0 * 8.366658210754395
Epoch 2140, val loss: 0.47784140706062317
Epoch 2150, training loss: 83.98027038574219 = 0.3848482668399811 + 10.0 * 8.359541893005371
Epoch 2150, val loss: 0.4776349663734436
Epoch 2160, training loss: 83.9649658203125 = 0.3841250538825989 + 10.0 * 8.358083724975586
Epoch 2160, val loss: 0.47751033306121826
Epoch 2170, training loss: 83.96295166015625 = 0.3834054470062256 + 10.0 * 8.357954025268555
Epoch 2170, val loss: 0.47747060656547546
Epoch 2180, training loss: 83.95744323730469 = 0.3826897144317627 + 10.0 * 8.357475280761719
Epoch 2180, val loss: 0.4773470461368561
Epoch 2190, training loss: 83.95604705810547 = 0.38197487592697144 + 10.0 * 8.357407569885254
Epoch 2190, val loss: 0.477255642414093
Epoch 2200, training loss: 84.01751708984375 = 0.38126206398010254 + 10.0 * 8.363625526428223
Epoch 2200, val loss: 0.4771674573421478
Epoch 2210, training loss: 83.975830078125 = 0.3805624544620514 + 10.0 * 8.359526634216309
Epoch 2210, val loss: 0.47715944051742554
Epoch 2220, training loss: 83.96356201171875 = 0.3798709511756897 + 10.0 * 8.358369827270508
Epoch 2220, val loss: 0.47705134749412537
Epoch 2230, training loss: 83.94844818115234 = 0.3791864514350891 + 10.0 * 8.356925964355469
Epoch 2230, val loss: 0.47698745131492615
Epoch 2240, training loss: 83.94432067871094 = 0.3785039186477661 + 10.0 * 8.356581687927246
Epoch 2240, val loss: 0.47692981362342834
Epoch 2250, training loss: 83.9539794921875 = 0.3778221309185028 + 10.0 * 8.35761547088623
Epoch 2250, val loss: 0.47684812545776367
Epoch 2260, training loss: 83.95165252685547 = 0.377145379781723 + 10.0 * 8.357450485229492
Epoch 2260, val loss: 0.4768369197845459
Epoch 2270, training loss: 83.94464111328125 = 0.3764769434928894 + 10.0 * 8.356816291809082
Epoch 2270, val loss: 0.4768296182155609
Epoch 2280, training loss: 83.94170379638672 = 0.37581053376197815 + 10.0 * 8.356589317321777
Epoch 2280, val loss: 0.47676777839660645
Epoch 2290, training loss: 83.96973419189453 = 0.3751513957977295 + 10.0 * 8.359457969665527
Epoch 2290, val loss: 0.47676119208335876
Epoch 2300, training loss: 83.93329620361328 = 0.3744927942752838 + 10.0 * 8.355879783630371
Epoch 2300, val loss: 0.4767115116119385
Epoch 2310, training loss: 83.93260192871094 = 0.37384355068206787 + 10.0 * 8.355875968933105
Epoch 2310, val loss: 0.4767009913921356
Epoch 2320, training loss: 83.92704010009766 = 0.3731963634490967 + 10.0 * 8.35538387298584
Epoch 2320, val loss: 0.47668391466140747
Epoch 2330, training loss: 83.92675018310547 = 0.37255069613456726 + 10.0 * 8.355420112609863
Epoch 2330, val loss: 0.4766685664653778
Epoch 2340, training loss: 83.95199584960938 = 0.37190893292427063 + 10.0 * 8.35800838470459
Epoch 2340, val loss: 0.47667068243026733
Epoch 2350, training loss: 83.95265197753906 = 0.37127283215522766 + 10.0 * 8.358138084411621
Epoch 2350, val loss: 0.47668492794036865
Epoch 2360, training loss: 83.93272399902344 = 0.3706457316875458 + 10.0 * 8.356207847595215
Epoch 2360, val loss: 0.4766737222671509
Epoch 2370, training loss: 83.92274475097656 = 0.3700270354747772 + 10.0 * 8.35527229309082
Epoch 2370, val loss: 0.4766990840435028
Epoch 2380, training loss: 83.9151611328125 = 0.3694075644016266 + 10.0 * 8.354575157165527
Epoch 2380, val loss: 0.4767157733440399
Epoch 2390, training loss: 83.91351318359375 = 0.368791788816452 + 10.0 * 8.354472160339355
Epoch 2390, val loss: 0.47674810886383057
Epoch 2400, training loss: 83.91362762451172 = 0.368177592754364 + 10.0 * 8.354544639587402
Epoch 2400, val loss: 0.47679001092910767
Epoch 2410, training loss: 83.9465103149414 = 0.36756637692451477 + 10.0 * 8.357893943786621
Epoch 2410, val loss: 0.4768555164337158
Epoch 2420, training loss: 83.9571762084961 = 0.36696600914001465 + 10.0 * 8.359021186828613
Epoch 2420, val loss: 0.47673970460891724
Epoch 2430, training loss: 83.92559051513672 = 0.3663633167743683 + 10.0 * 8.35592269897461
Epoch 2430, val loss: 0.4769519567489624
Epoch 2440, training loss: 83.90975189208984 = 0.36577484011650085 + 10.0 * 8.354397773742676
Epoch 2440, val loss: 0.47693881392478943
Epoch 2450, training loss: 83.91431427001953 = 0.36518800258636475 + 10.0 * 8.354912757873535
Epoch 2450, val loss: 0.4769747257232666
Epoch 2460, training loss: 83.9116439819336 = 0.3646034598350525 + 10.0 * 8.354703903198242
Epoch 2460, val loss: 0.47705838084220886
Epoch 2470, training loss: 83.89579010009766 = 0.36402013897895813 + 10.0 * 8.353177070617676
Epoch 2470, val loss: 0.47709035873413086
Epoch 2480, training loss: 83.8968276977539 = 0.36343950033187866 + 10.0 * 8.353338241577148
Epoch 2480, val loss: 0.4771571457386017
Epoch 2490, training loss: 83.91719818115234 = 0.3628605604171753 + 10.0 * 8.355433464050293
Epoch 2490, val loss: 0.4772157669067383
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7985794013191273
0.8134463522422662
The final CL Acc:0.79959, 0.00083, The final GNN Acc:0.81381, 0.00182
raw graph: torch.Size([2, 88648])
add edge: torch.Size([2, 110794])
remove edge: torch.Size([2, 66638])
updated graph: torch.Size([2, 88784])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 106.91221618652344 = 1.0897384881973267 + 10.0 * 10.582247734069824
Epoch 0, val loss: 1.0903693437576294
Epoch 10, training loss: 106.90341186523438 = 1.0855700969696045 + 10.0 * 10.58178424835205
Epoch 10, val loss: 1.086205005645752
Epoch 20, training loss: 106.87871551513672 = 1.0813244581222534 + 10.0 * 10.579739570617676
Epoch 20, val loss: 1.0819519758224487
Epoch 30, training loss: 106.78659057617188 = 1.0768885612487793 + 10.0 * 10.57097053527832
Epoch 30, val loss: 1.0774946212768555
Epoch 40, training loss: 106.4501724243164 = 1.0722407102584839 + 10.0 * 10.537793159484863
Epoch 40, val loss: 1.0728222131729126
Epoch 50, training loss: 105.43699645996094 = 1.0672780275344849 + 10.0 * 10.436971664428711
Epoch 50, val loss: 1.0678225755691528
Epoch 60, training loss: 102.68614196777344 = 1.0621070861816406 + 10.0 * 10.162403106689453
Epoch 60, val loss: 1.0626351833343506
Epoch 70, training loss: 97.22925567626953 = 1.05630624294281 + 10.0 * 9.617295265197754
Epoch 70, val loss: 1.056788682937622
Epoch 80, training loss: 95.58751678466797 = 1.0511276721954346 + 10.0 * 9.453639030456543
Epoch 80, val loss: 1.0518765449523926
Epoch 90, training loss: 94.74601745605469 = 1.0468965768814087 + 10.0 * 9.369912147521973
Epoch 90, val loss: 1.0478322505950928
Epoch 100, training loss: 93.81768035888672 = 1.0430876016616821 + 10.0 * 9.277459144592285
Epoch 100, val loss: 1.0441633462905884
Epoch 110, training loss: 92.86913299560547 = 1.0399903059005737 + 10.0 * 9.182913780212402
Epoch 110, val loss: 1.0411648750305176
Epoch 120, training loss: 92.13383483886719 = 1.0374720096588135 + 10.0 * 9.109636306762695
Epoch 120, val loss: 1.038658618927002
Epoch 130, training loss: 91.3719253540039 = 1.035405158996582 + 10.0 * 9.033651351928711
Epoch 130, val loss: 1.0366053581237793
Epoch 140, training loss: 90.54041290283203 = 1.0340646505355835 + 10.0 * 8.950634956359863
Epoch 140, val loss: 1.0353518724441528
Epoch 150, training loss: 90.01343536376953 = 1.0330095291137695 + 10.0 * 8.898042678833008
Epoch 150, val loss: 1.0343282222747803
Epoch 160, training loss: 89.59339141845703 = 1.0317001342773438 + 10.0 * 8.856168746948242
Epoch 160, val loss: 1.033039927482605
Epoch 170, training loss: 89.05870056152344 = 1.0306025743484497 + 10.0 * 8.802809715270996
Epoch 170, val loss: 1.0320273637771606
Epoch 180, training loss: 88.65035247802734 = 1.0301722288131714 + 10.0 * 8.762018203735352
Epoch 180, val loss: 1.031556248664856
Epoch 190, training loss: 88.26150512695312 = 1.0294009447097778 + 10.0 * 8.723210334777832
Epoch 190, val loss: 1.0305922031402588
Epoch 200, training loss: 87.81245422363281 = 1.0286074876785278 + 10.0 * 8.678384780883789
Epoch 200, val loss: 1.0296720266342163
Epoch 210, training loss: 87.50243377685547 = 1.0279639959335327 + 10.0 * 8.64744758605957
Epoch 210, val loss: 1.0289721488952637
Epoch 220, training loss: 87.30970001220703 = 1.0266770124435425 + 10.0 * 8.628301620483398
Epoch 220, val loss: 1.027649164199829
Epoch 230, training loss: 87.1543960571289 = 1.0251573324203491 + 10.0 * 8.612924575805664
Epoch 230, val loss: 1.026168704032898
Epoch 240, training loss: 87.00569152832031 = 1.023936152458191 + 10.0 * 8.598176002502441
Epoch 240, val loss: 1.0250691175460815
Epoch 250, training loss: 86.84640502929688 = 1.0228700637817383 + 10.0 * 8.582353591918945
Epoch 250, val loss: 1.0240339040756226
Epoch 260, training loss: 86.70348358154297 = 1.0217539072036743 + 10.0 * 8.5681734085083
Epoch 260, val loss: 1.0229482650756836
Epoch 270, training loss: 86.58887481689453 = 1.0204877853393555 + 10.0 * 8.556838989257812
Epoch 270, val loss: 1.0216529369354248
Epoch 280, training loss: 86.45794677734375 = 1.019069790840149 + 10.0 * 8.5438871383667
Epoch 280, val loss: 1.0203328132629395
Epoch 290, training loss: 86.35515594482422 = 1.0176409482955933 + 10.0 * 8.533751487731934
Epoch 290, val loss: 1.018977403640747
Epoch 300, training loss: 86.26254272460938 = 1.0162018537521362 + 10.0 * 8.524633407592773
Epoch 300, val loss: 1.0175116062164307
Epoch 310, training loss: 86.16295623779297 = 1.0146894454956055 + 10.0 * 8.514826774597168
Epoch 310, val loss: 1.0161012411117554
Epoch 320, training loss: 86.07221221923828 = 1.0131603479385376 + 10.0 * 8.505905151367188
Epoch 320, val loss: 1.0146338939666748
Epoch 330, training loss: 85.98637390136719 = 1.0115618705749512 + 10.0 * 8.497481346130371
Epoch 330, val loss: 1.013113021850586
Epoch 340, training loss: 85.92215728759766 = 1.0098567008972168 + 10.0 * 8.491230010986328
Epoch 340, val loss: 1.011447787284851
Epoch 350, training loss: 85.85751342773438 = 1.007980227470398 + 10.0 * 8.484952926635742
Epoch 350, val loss: 1.0096760988235474
Epoch 360, training loss: 85.78775787353516 = 1.0060962438583374 + 10.0 * 8.478166580200195
Epoch 360, val loss: 1.007920503616333
Epoch 370, training loss: 85.73619842529297 = 1.0041371583938599 + 10.0 * 8.473206520080566
Epoch 370, val loss: 1.006067156791687
Epoch 380, training loss: 85.6624755859375 = 1.0021436214447021 + 10.0 * 8.466032981872559
Epoch 380, val loss: 1.0041745901107788
Epoch 390, training loss: 85.6061019897461 = 1.0000956058502197 + 10.0 * 8.460599899291992
Epoch 390, val loss: 1.0022008419036865
Epoch 400, training loss: 85.55387115478516 = 0.9979208111763 + 10.0 * 8.455595016479492
Epoch 400, val loss: 1.000129222869873
Epoch 410, training loss: 85.51067352294922 = 0.9956392049789429 + 10.0 * 8.45150375366211
Epoch 410, val loss: 0.9979350566864014
Epoch 420, training loss: 85.45706939697266 = 0.9931161403656006 + 10.0 * 8.446394920349121
Epoch 420, val loss: 0.9956212043762207
Epoch 430, training loss: 85.41259765625 = 0.9905700087547302 + 10.0 * 8.4422025680542
Epoch 430, val loss: 0.9931406378746033
Epoch 440, training loss: 85.37017822265625 = 0.9878362417221069 + 10.0 * 8.438234329223633
Epoch 440, val loss: 0.9905893802642822
Epoch 450, training loss: 85.32914733886719 = 0.9849072098731995 + 10.0 * 8.434423446655273
Epoch 450, val loss: 0.9877837896347046
Epoch 460, training loss: 85.29750061035156 = 0.9818123579025269 + 10.0 * 8.43156909942627
Epoch 460, val loss: 0.9848278760910034
Epoch 470, training loss: 85.27513122558594 = 0.9785972237586975 + 10.0 * 8.42965316772461
Epoch 470, val loss: 0.981827437877655
Epoch 480, training loss: 85.23775482177734 = 0.975261390209198 + 10.0 * 8.426249504089355
Epoch 480, val loss: 0.9785466194152832
Epoch 490, training loss: 85.20632934570312 = 0.9717181324958801 + 10.0 * 8.423460960388184
Epoch 490, val loss: 0.9752280712127686
Epoch 500, training loss: 85.17919921875 = 0.9680313467979431 + 10.0 * 8.421116828918457
Epoch 500, val loss: 0.9717215895652771
Epoch 510, training loss: 85.17448425292969 = 0.9641327857971191 + 10.0 * 8.421034812927246
Epoch 510, val loss: 0.9680234789848328
Epoch 520, training loss: 85.12776947021484 = 0.9600437879562378 + 10.0 * 8.416772842407227
Epoch 520, val loss: 0.9640771746635437
Epoch 530, training loss: 85.10093688964844 = 0.9558047652244568 + 10.0 * 8.41451358795166
Epoch 530, val loss: 0.9600440263748169
Epoch 540, training loss: 85.08078002929688 = 0.9514167308807373 + 10.0 * 8.412936210632324
Epoch 540, val loss: 0.9557943940162659
Epoch 550, training loss: 85.04835510253906 = 0.9466526508331299 + 10.0 * 8.41016960144043
Epoch 550, val loss: 0.9513492584228516
Epoch 560, training loss: 85.0262451171875 = 0.9417867660522461 + 10.0 * 8.408445358276367
Epoch 560, val loss: 0.9466568231582642
Epoch 570, training loss: 84.99734497070312 = 0.9367352724075317 + 10.0 * 8.406061172485352
Epoch 570, val loss: 0.9418619275093079
Epoch 580, training loss: 84.97138214111328 = 0.9314761757850647 + 10.0 * 8.403990745544434
Epoch 580, val loss: 0.9368392825126648
Epoch 590, training loss: 84.9882583618164 = 0.925977349281311 + 10.0 * 8.406228065490723
Epoch 590, val loss: 0.9314857125282288
Epoch 600, training loss: 84.92274475097656 = 0.9200584292411804 + 10.0 * 8.4002685546875
Epoch 600, val loss: 0.925939679145813
Epoch 610, training loss: 84.89844512939453 = 0.9140523076057434 + 10.0 * 8.398439407348633
Epoch 610, val loss: 0.9202175736427307
Epoch 620, training loss: 84.8732681274414 = 0.9077993631362915 + 10.0 * 8.396547317504883
Epoch 620, val loss: 0.9142537713050842
Epoch 630, training loss: 84.8607177734375 = 0.9012840390205383 + 10.0 * 8.395943641662598
Epoch 630, val loss: 0.9079785943031311
Epoch 640, training loss: 84.83307647705078 = 0.8943557143211365 + 10.0 * 8.393872261047363
Epoch 640, val loss: 0.9013941287994385
Epoch 650, training loss: 84.80741882324219 = 0.8872116208076477 + 10.0 * 8.392020225524902
Epoch 650, val loss: 0.8945736885070801
Epoch 660, training loss: 84.77996063232422 = 0.879854142665863 + 10.0 * 8.390010833740234
Epoch 660, val loss: 0.8875457644462585
Epoch 670, training loss: 84.76181030273438 = 0.8722113370895386 + 10.0 * 8.388959884643555
Epoch 670, val loss: 0.8802137970924377
Epoch 680, training loss: 84.7446060180664 = 0.8641565442085266 + 10.0 * 8.388044357299805
Epoch 680, val loss: 0.8725773096084595
Epoch 690, training loss: 84.71659851074219 = 0.8558382391929626 + 10.0 * 8.386075973510742
Epoch 690, val loss: 0.8646255135536194
Epoch 700, training loss: 84.6885757446289 = 0.8473329544067383 + 10.0 * 8.384123802185059
Epoch 700, val loss: 0.856495201587677
Epoch 710, training loss: 84.67217254638672 = 0.8385095596313477 + 10.0 * 8.383366584777832
Epoch 710, val loss: 0.8481313586235046
Epoch 720, training loss: 84.64906311035156 = 0.829325795173645 + 10.0 * 8.381974220275879
Epoch 720, val loss: 0.8393176198005676
Epoch 730, training loss: 84.62447357177734 = 0.819884717464447 + 10.0 * 8.38045883178711
Epoch 730, val loss: 0.8302900195121765
Epoch 740, training loss: 84.60244750976562 = 0.8102249503135681 + 10.0 * 8.37922191619873
Epoch 740, val loss: 0.8209990859031677
Epoch 750, training loss: 84.59930419921875 = 0.8002659678459167 + 10.0 * 8.379903793334961
Epoch 750, val loss: 0.8114356398582458
Epoch 760, training loss: 84.56417846679688 = 0.7899020314216614 + 10.0 * 8.37742805480957
Epoch 760, val loss: 0.8016365766525269
Epoch 770, training loss: 84.53971862792969 = 0.7794363498687744 + 10.0 * 8.376028060913086
Epoch 770, val loss: 0.7915880084037781
Epoch 780, training loss: 84.51289367675781 = 0.768727719783783 + 10.0 * 8.37441635131836
Epoch 780, val loss: 0.7813904285430908
Epoch 790, training loss: 84.49214935302734 = 0.7577928304672241 + 10.0 * 8.373435974121094
Epoch 790, val loss: 0.7709550857543945
Epoch 800, training loss: 84.55528259277344 = 0.7465241551399231 + 10.0 * 8.380876541137695
Epoch 800, val loss: 0.7602917551994324
Epoch 810, training loss: 84.45006561279297 = 0.7351505756378174 + 10.0 * 8.371491432189941
Epoch 810, val loss: 0.7493631839752197
Epoch 820, training loss: 84.42681121826172 = 0.7238368391990662 + 10.0 * 8.3702974319458
Epoch 820, val loss: 0.7385686635971069
Epoch 830, training loss: 84.40599060058594 = 0.712497353553772 + 10.0 * 8.369349479675293
Epoch 830, val loss: 0.7277413606643677
Epoch 840, training loss: 84.38334655761719 = 0.7010480165481567 + 10.0 * 8.368229866027832
Epoch 840, val loss: 0.71686190366745
Epoch 850, training loss: 84.36023712158203 = 0.6895025968551636 + 10.0 * 8.367074012756348
Epoch 850, val loss: 0.7058930397033691
Epoch 860, training loss: 84.38114929199219 = 0.6778548955917358 + 10.0 * 8.370328903198242
Epoch 860, val loss: 0.69490647315979
Epoch 870, training loss: 84.32099914550781 = 0.6662790179252625 + 10.0 * 8.365471839904785
Epoch 870, val loss: 0.6838110685348511
Epoch 880, training loss: 84.29813385009766 = 0.654858410358429 + 10.0 * 8.364327430725098
Epoch 880, val loss: 0.6730603575706482
Epoch 890, training loss: 84.27474975585938 = 0.643652617931366 + 10.0 * 8.363109588623047
Epoch 890, val loss: 0.6624107956886292
Epoch 900, training loss: 84.25308990478516 = 0.6325482130050659 + 10.0 * 8.362054824829102
Epoch 900, val loss: 0.6519249677658081
Epoch 910, training loss: 84.23727416992188 = 0.6215459108352661 + 10.0 * 8.361573219299316
Epoch 910, val loss: 0.6415420174598694
Epoch 920, training loss: 84.21382141113281 = 0.6106707453727722 + 10.0 * 8.360315322875977
Epoch 920, val loss: 0.6312301158905029
Epoch 930, training loss: 84.20263671875 = 0.6000280976295471 + 10.0 * 8.360260963439941
Epoch 930, val loss: 0.6212525367736816
Epoch 940, training loss: 84.175537109375 = 0.589694082736969 + 10.0 * 8.3585844039917
Epoch 940, val loss: 0.6114838719367981
Epoch 950, training loss: 84.1805191040039 = 0.5796003937721252 + 10.0 * 8.360092163085938
Epoch 950, val loss: 0.6019119620323181
Epoch 960, training loss: 84.14212036132812 = 0.5696576237678528 + 10.0 * 8.357246398925781
Epoch 960, val loss: 0.5927284359931946
Epoch 970, training loss: 84.11658477783203 = 0.5601246356964111 + 10.0 * 8.355646133422852
Epoch 970, val loss: 0.5837289094924927
Epoch 980, training loss: 84.10427856445312 = 0.5508463382720947 + 10.0 * 8.355342864990234
Epoch 980, val loss: 0.5750537514686584
Epoch 990, training loss: 84.11402893066406 = 0.5417956709861755 + 10.0 * 8.357223510742188
Epoch 990, val loss: 0.5666192770004272
Epoch 1000, training loss: 84.07259368896484 = 0.5330746173858643 + 10.0 * 8.353952407836914
Epoch 1000, val loss: 0.5584563612937927
Epoch 1010, training loss: 84.05168914794922 = 0.5247078537940979 + 10.0 * 8.352697372436523
Epoch 1010, val loss: 0.5506314039230347
Epoch 1020, training loss: 84.03522491455078 = 0.516638994216919 + 10.0 * 8.351858139038086
Epoch 1020, val loss: 0.5431181788444519
Epoch 1030, training loss: 84.0357437133789 = 0.5088566541671753 + 10.0 * 8.352688789367676
Epoch 1030, val loss: 0.53584223985672
Epoch 1040, training loss: 84.00821685791016 = 0.5012792944908142 + 10.0 * 8.350693702697754
Epoch 1040, val loss: 0.5288978219032288
Epoch 1050, training loss: 83.99275970458984 = 0.4940623939037323 + 10.0 * 8.349869728088379
Epoch 1050, val loss: 0.5222247838973999
Epoch 1060, training loss: 83.9792709350586 = 0.48717257380485535 + 10.0 * 8.349209785461426
Epoch 1060, val loss: 0.5158578753471375
Epoch 1070, training loss: 83.96859741210938 = 0.4805280864238739 + 10.0 * 8.348806381225586
Epoch 1070, val loss: 0.5097624063491821
Epoch 1080, training loss: 83.97527313232422 = 0.47414374351501465 + 10.0 * 8.350112915039062
Epoch 1080, val loss: 0.5038738250732422
Epoch 1090, training loss: 83.94324493408203 = 0.4680482745170593 + 10.0 * 8.347519874572754
Epoch 1090, val loss: 0.49826735258102417
Epoch 1100, training loss: 83.93219757080078 = 0.4622364044189453 + 10.0 * 8.346996307373047
Epoch 1100, val loss: 0.49297764897346497
Epoch 1110, training loss: 83.93209838867188 = 0.45667368173599243 + 10.0 * 8.347542762756348
Epoch 1110, val loss: 0.48788943886756897
Epoch 1120, training loss: 83.91201782226562 = 0.451317697763443 + 10.0 * 8.346070289611816
Epoch 1120, val loss: 0.48302000761032104
Epoch 1130, training loss: 83.89742279052734 = 0.44620823860168457 + 10.0 * 8.345121383666992
Epoch 1130, val loss: 0.4784354567527771
Epoch 1140, training loss: 83.88517761230469 = 0.44135481119155884 + 10.0 * 8.344382286071777
Epoch 1140, val loss: 0.47405239939689636
Epoch 1150, training loss: 83.88130187988281 = 0.43671533465385437 + 10.0 * 8.34445858001709
Epoch 1150, val loss: 0.4698640704154968
Epoch 1160, training loss: 83.87530517578125 = 0.4322332441806793 + 10.0 * 8.344306945800781
Epoch 1160, val loss: 0.4658711552619934
Epoch 1170, training loss: 83.85467529296875 = 0.42794427275657654 + 10.0 * 8.342672348022461
Epoch 1170, val loss: 0.46209827065467834
Epoch 1180, training loss: 83.84871673583984 = 0.42387935519218445 + 10.0 * 8.342483520507812
Epoch 1180, val loss: 0.4584850072860718
Epoch 1190, training loss: 83.85470581054688 = 0.41996240615844727 + 10.0 * 8.343474388122559
Epoch 1190, val loss: 0.4550303816795349
Epoch 1200, training loss: 83.83018493652344 = 0.4162476360797882 + 10.0 * 8.341394424438477
Epoch 1200, val loss: 0.45172399282455444
Epoch 1210, training loss: 83.81724548339844 = 0.4126640856266022 + 10.0 * 8.340457916259766
Epoch 1210, val loss: 0.4485912322998047
Epoch 1220, training loss: 83.8109130859375 = 0.40924420952796936 + 10.0 * 8.340167045593262
Epoch 1220, val loss: 0.44559741020202637
Epoch 1230, training loss: 83.81623840332031 = 0.40592896938323975 + 10.0 * 8.341031074523926
Epoch 1230, val loss: 0.4427264928817749
Epoch 1240, training loss: 83.79888153076172 = 0.4027711749076843 + 10.0 * 8.339611053466797
Epoch 1240, val loss: 0.43992897868156433
Epoch 1250, training loss: 83.7951889038086 = 0.3997182250022888 + 10.0 * 8.339547157287598
Epoch 1250, val loss: 0.43730428814888
Epoch 1260, training loss: 83.78453826904297 = 0.39678964018821716 + 10.0 * 8.338774681091309
Epoch 1260, val loss: 0.4347892999649048
Epoch 1270, training loss: 83.77570343017578 = 0.3939594328403473 + 10.0 * 8.338174819946289
Epoch 1270, val loss: 0.432399183511734
Epoch 1280, training loss: 83.76255798339844 = 0.3912390470504761 + 10.0 * 8.337132453918457
Epoch 1280, val loss: 0.43013477325439453
Epoch 1290, training loss: 83.7518539428711 = 0.3886372745037079 + 10.0 * 8.336321830749512
Epoch 1290, val loss: 0.42790642380714417
Epoch 1300, training loss: 83.74357604980469 = 0.3861158788204193 + 10.0 * 8.335745811462402
Epoch 1300, val loss: 0.4258008599281311
Epoch 1310, training loss: 83.76602172851562 = 0.38366690278053284 + 10.0 * 8.338235855102539
Epoch 1310, val loss: 0.4237777292728424
Epoch 1320, training loss: 83.7770767211914 = 0.38126900792121887 + 10.0 * 8.339580535888672
Epoch 1320, val loss: 0.421806663274765
Epoch 1330, training loss: 83.72883605957031 = 0.3790132403373718 + 10.0 * 8.334981918334961
Epoch 1330, val loss: 0.419938862323761
Epoch 1340, training loss: 83.72059631347656 = 0.3768593370914459 + 10.0 * 8.334373474121094
Epoch 1340, val loss: 0.4181772470474243
Epoch 1350, training loss: 83.71403503417969 = 0.37478742003440857 + 10.0 * 8.333925247192383
Epoch 1350, val loss: 0.4164859652519226
Epoch 1360, training loss: 83.70555877685547 = 0.3727702796459198 + 10.0 * 8.33327865600586
Epoch 1360, val loss: 0.414861261844635
Epoch 1370, training loss: 83.6998291015625 = 0.37080368399620056 + 10.0 * 8.332902908325195
Epoch 1370, val loss: 0.41330045461654663
Epoch 1380, training loss: 83.696533203125 = 0.3688872158527374 + 10.0 * 8.332764625549316
Epoch 1380, val loss: 0.4117980897426605
Epoch 1390, training loss: 83.73634338378906 = 0.3670048415660858 + 10.0 * 8.336934089660645
Epoch 1390, val loss: 0.41034749150276184
Epoch 1400, training loss: 83.68553924560547 = 0.36519286036491394 + 10.0 * 8.33203411102295
Epoch 1400, val loss: 0.4089381992816925
Epoch 1410, training loss: 83.6827621459961 = 0.36346039175987244 + 10.0 * 8.331930160522461
Epoch 1410, val loss: 0.4075275659561157
Epoch 1420, training loss: 83.67576599121094 = 0.36177459359169006 + 10.0 * 8.331398963928223
Epoch 1420, val loss: 0.40623345971107483
Epoch 1430, training loss: 83.6686019897461 = 0.3601287603378296 + 10.0 * 8.330846786499023
Epoch 1430, val loss: 0.4049922227859497
Epoch 1440, training loss: 83.66600799560547 = 0.3585195243358612 + 10.0 * 8.330748558044434
Epoch 1440, val loss: 0.40379050374031067
Epoch 1450, training loss: 83.69103240966797 = 0.35694560408592224 + 10.0 * 8.33340835571289
Epoch 1450, val loss: 0.4026094079017639
Epoch 1460, training loss: 83.67544555664062 = 0.35540249943733215 + 10.0 * 8.33200454711914
Epoch 1460, val loss: 0.40149685740470886
Epoch 1470, training loss: 83.666748046875 = 0.3539280295372009 + 10.0 * 8.331281661987305
Epoch 1470, val loss: 0.4004063010215759
Epoch 1480, training loss: 83.64935302734375 = 0.3525067865848541 + 10.0 * 8.329684257507324
Epoch 1480, val loss: 0.39936089515686035
Epoch 1490, training loss: 83.64098358154297 = 0.35110825300216675 + 10.0 * 8.328988075256348
Epoch 1490, val loss: 0.3983606696128845
Epoch 1500, training loss: 83.63748931884766 = 0.34974005818367004 + 10.0 * 8.328775405883789
Epoch 1500, val loss: 0.3974032700061798
Epoch 1510, training loss: 83.64090728759766 = 0.3483926057815552 + 10.0 * 8.329251289367676
Epoch 1510, val loss: 0.3964564800262451
Epoch 1520, training loss: 83.63383483886719 = 0.34707707166671753 + 10.0 * 8.328676223754883
Epoch 1520, val loss: 0.3955361843109131
Epoch 1530, training loss: 83.64253234863281 = 0.34579935669898987 + 10.0 * 8.329672813415527
Epoch 1530, val loss: 0.39462584257125854
Epoch 1540, training loss: 83.6407470703125 = 0.3445269465446472 + 10.0 * 8.329622268676758
Epoch 1540, val loss: 0.3938530683517456
Epoch 1550, training loss: 83.62435150146484 = 0.3433177173137665 + 10.0 * 8.328104019165039
Epoch 1550, val loss: 0.3929705321788788
Epoch 1560, training loss: 83.61428833007812 = 0.342131644487381 + 10.0 * 8.327215194702148
Epoch 1560, val loss: 0.39219799637794495
Epoch 1570, training loss: 83.60810089111328 = 0.3409760892391205 + 10.0 * 8.326712608337402
Epoch 1570, val loss: 0.39145320653915405
Epoch 1580, training loss: 83.60334777832031 = 0.33982956409454346 + 10.0 * 8.3263521194458
Epoch 1580, val loss: 0.3907126188278198
Epoch 1590, training loss: 83.60423278808594 = 0.3387017846107483 + 10.0 * 8.326553344726562
Epoch 1590, val loss: 0.3900298476219177
Epoch 1600, training loss: 83.61719512939453 = 0.3375859260559082 + 10.0 * 8.327960968017578
Epoch 1600, val loss: 0.38931548595428467
Epoch 1610, training loss: 83.59677124023438 = 0.33649691939353943 + 10.0 * 8.326027870178223
Epoch 1610, val loss: 0.38863468170166016
Epoch 1620, training loss: 83.58985900878906 = 0.33542725443840027 + 10.0 * 8.325443267822266
Epoch 1620, val loss: 0.3879393935203552
Epoch 1630, training loss: 83.5928955078125 = 0.33438101410865784 + 10.0 * 8.325851440429688
Epoch 1630, val loss: 0.3873361647129059
Epoch 1640, training loss: 83.61097717285156 = 0.33334043622016907 + 10.0 * 8.327763557434082
Epoch 1640, val loss: 0.3866879642009735
Epoch 1650, training loss: 83.58770751953125 = 0.332307904958725 + 10.0 * 8.325540542602539
Epoch 1650, val loss: 0.38613876700401306
Epoch 1660, training loss: 83.57664489746094 = 0.3313218355178833 + 10.0 * 8.324532508850098
Epoch 1660, val loss: 0.38551411032676697
Epoch 1670, training loss: 83.56970977783203 = 0.3303380608558655 + 10.0 * 8.32393741607666
Epoch 1670, val loss: 0.38498544692993164
Epoch 1680, training loss: 83.56716918945312 = 0.3293713927268982 + 10.0 * 8.323780059814453
Epoch 1680, val loss: 0.3844251036643982
Epoch 1690, training loss: 83.60210418701172 = 0.32841917872428894 + 10.0 * 8.327367782592773
Epoch 1690, val loss: 0.3838815689086914
Epoch 1700, training loss: 83.57553100585938 = 0.32746127247810364 + 10.0 * 8.324807167053223
Epoch 1700, val loss: 0.3833812177181244
Epoch 1710, training loss: 83.56057739257812 = 0.32654353976249695 + 10.0 * 8.323403358459473
Epoch 1710, val loss: 0.38286787271499634
Epoch 1720, training loss: 83.552490234375 = 0.32563015818595886 + 10.0 * 8.322686195373535
Epoch 1720, val loss: 0.3823935091495514
Epoch 1730, training loss: 83.55238342285156 = 0.32472851872444153 + 10.0 * 8.322765350341797
Epoch 1730, val loss: 0.3819222152233124
Epoch 1740, training loss: 83.57524108886719 = 0.32383519411087036 + 10.0 * 8.325139999389648
Epoch 1740, val loss: 0.3814529478549957
Epoch 1750, training loss: 83.57492065429688 = 0.32295188307762146 + 10.0 * 8.325197219848633
Epoch 1750, val loss: 0.3809773623943329
Epoch 1760, training loss: 83.54991912841797 = 0.32207486033439636 + 10.0 * 8.322784423828125
Epoch 1760, val loss: 0.3806086778640747
Epoch 1770, training loss: 83.54019165039062 = 0.32123491168022156 + 10.0 * 8.321895599365234
Epoch 1770, val loss: 0.3801776170730591
Epoch 1780, training loss: 83.53501892089844 = 0.32040080428123474 + 10.0 * 8.32146167755127
Epoch 1780, val loss: 0.379747211933136
Epoch 1790, training loss: 83.52973175048828 = 0.3195731043815613 + 10.0 * 8.321016311645508
Epoch 1790, val loss: 0.37938007712364197
Epoch 1800, training loss: 83.52762603759766 = 0.31875085830688477 + 10.0 * 8.320887565612793
Epoch 1800, val loss: 0.37899675965309143
Epoch 1810, training loss: 83.53128051757812 = 0.31793123483657837 + 10.0 * 8.321334838867188
Epoch 1810, val loss: 0.37862858176231384
Epoch 1820, training loss: 83.55680084228516 = 0.3171141445636749 + 10.0 * 8.323968887329102
Epoch 1820, val loss: 0.37827926874160767
Epoch 1830, training loss: 83.53234100341797 = 0.31632325053215027 + 10.0 * 8.321601867675781
Epoch 1830, val loss: 0.3778999149799347
Epoch 1840, training loss: 83.51863861083984 = 0.3155299425125122 + 10.0 * 8.320310592651367
Epoch 1840, val loss: 0.3775559067726135
Epoch 1850, training loss: 83.51338958740234 = 0.31475546956062317 + 10.0 * 8.319863319396973
Epoch 1850, val loss: 0.37720170617103577
Epoch 1860, training loss: 83.51107025146484 = 0.31397905945777893 + 10.0 * 8.319708824157715
Epoch 1860, val loss: 0.3769053518772125
Epoch 1870, training loss: 83.52730560302734 = 0.3132108449935913 + 10.0 * 8.321409225463867
Epoch 1870, val loss: 0.3766147494316101
Epoch 1880, training loss: 83.52086639404297 = 0.3124377727508545 + 10.0 * 8.320842742919922
Epoch 1880, val loss: 0.37620308995246887
Epoch 1890, training loss: 83.51038360595703 = 0.31169047951698303 + 10.0 * 8.319869041442871
Epoch 1890, val loss: 0.37599608302116394
Epoch 1900, training loss: 83.50091552734375 = 0.310947984457016 + 10.0 * 8.31899642944336
Epoch 1900, val loss: 0.3756408095359802
Epoch 1910, training loss: 83.49739837646484 = 0.31021320819854736 + 10.0 * 8.318718910217285
Epoch 1910, val loss: 0.3753776252269745
Epoch 1920, training loss: 83.50819396972656 = 0.3094818592071533 + 10.0 * 8.31987190246582
Epoch 1920, val loss: 0.37511324882507324
Epoch 1930, training loss: 83.5010986328125 = 0.3087582588195801 + 10.0 * 8.319233894348145
Epoch 1930, val loss: 0.3748096227645874
Epoch 1940, training loss: 83.48949432373047 = 0.30803972482681274 + 10.0 * 8.318145751953125
Epoch 1940, val loss: 0.37458279728889465
Epoch 1950, training loss: 83.48786163330078 = 0.30732759833335876 + 10.0 * 8.318053245544434
Epoch 1950, val loss: 0.3743281960487366
Epoch 1960, training loss: 83.48478698730469 = 0.3066251575946808 + 10.0 * 8.317815780639648
Epoch 1960, val loss: 0.3740703761577606
Epoch 1970, training loss: 83.49295806884766 = 0.3059205412864685 + 10.0 * 8.318703651428223
Epoch 1970, val loss: 0.37382423877716064
Epoch 1980, training loss: 83.49044799804688 = 0.30522486567497253 + 10.0 * 8.318522453308105
Epoch 1980, val loss: 0.373603880405426
Epoch 1990, training loss: 83.49217987060547 = 0.30453890562057495 + 10.0 * 8.318763732910156
Epoch 1990, val loss: 0.37333279848098755
Epoch 2000, training loss: 83.48030853271484 = 0.30385860800743103 + 10.0 * 8.317645072937012
Epoch 2000, val loss: 0.3731442093849182
Epoch 2010, training loss: 83.4742660522461 = 0.3031836152076721 + 10.0 * 8.317108154296875
Epoch 2010, val loss: 0.37292295694351196
Epoch 2020, training loss: 83.46830749511719 = 0.3025197386741638 + 10.0 * 8.31657886505127
Epoch 2020, val loss: 0.37272605299949646
Epoch 2030, training loss: 83.46743774414062 = 0.3018541932106018 + 10.0 * 8.316557884216309
Epoch 2030, val loss: 0.37253329157829285
Epoch 2040, training loss: 83.48236083984375 = 0.3011915683746338 + 10.0 * 8.318117141723633
Epoch 2040, val loss: 0.372378945350647
Epoch 2050, training loss: 83.47947692871094 = 0.3005334734916687 + 10.0 * 8.317893981933594
Epoch 2050, val loss: 0.3721034526824951
Epoch 2060, training loss: 83.46243286132812 = 0.29988592863082886 + 10.0 * 8.316254615783691
Epoch 2060, val loss: 0.37195777893066406
Epoch 2070, training loss: 83.45612335205078 = 0.2992432415485382 + 10.0 * 8.315688133239746
Epoch 2070, val loss: 0.3717542886734009
Epoch 2080, training loss: 83.45404815673828 = 0.2986029386520386 + 10.0 * 8.315545082092285
Epoch 2080, val loss: 0.37161001563072205
Epoch 2090, training loss: 83.45576477050781 = 0.2979642450809479 + 10.0 * 8.315779685974121
Epoch 2090, val loss: 0.37144652009010315
Epoch 2100, training loss: 83.46638488769531 = 0.2973286211490631 + 10.0 * 8.316905975341797
Epoch 2100, val loss: 0.3712838590145111
Epoch 2110, training loss: 83.4688491821289 = 0.29670432209968567 + 10.0 * 8.317214012145996
Epoch 2110, val loss: 0.3710692822933197
Epoch 2120, training loss: 83.44963836669922 = 0.29607295989990234 + 10.0 * 8.315356254577637
Epoch 2120, val loss: 0.37095707654953003
Epoch 2130, training loss: 83.46044158935547 = 0.29546016454696655 + 10.0 * 8.316497802734375
Epoch 2130, val loss: 0.3707933723926544
Epoch 2140, training loss: 83.44926452636719 = 0.29484447836875916 + 10.0 * 8.315442085266113
Epoch 2140, val loss: 0.37067580223083496
Epoch 2150, training loss: 83.4415512084961 = 0.29424136877059937 + 10.0 * 8.314730644226074
Epoch 2150, val loss: 0.37053540349006653
Epoch 2160, training loss: 83.43562316894531 = 0.2936391830444336 + 10.0 * 8.31419849395752
Epoch 2160, val loss: 0.37040722370147705
Epoch 2170, training loss: 83.4327163696289 = 0.2930397391319275 + 10.0 * 8.31396770477295
Epoch 2170, val loss: 0.3702808916568756
Epoch 2180, training loss: 83.43448638916016 = 0.2924395799636841 + 10.0 * 8.314204216003418
Epoch 2180, val loss: 0.370160847902298
Epoch 2190, training loss: 83.46464538574219 = 0.29184168577194214 + 10.0 * 8.317280769348145
Epoch 2190, val loss: 0.37004250288009644
Epoch 2200, training loss: 83.43266296386719 = 0.29124870896339417 + 10.0 * 8.314141273498535
Epoch 2200, val loss: 0.3699720799922943
Epoch 2210, training loss: 83.42684936523438 = 0.29066187143325806 + 10.0 * 8.313618659973145
Epoch 2210, val loss: 0.3698153495788574
Epoch 2220, training loss: 83.4285659790039 = 0.29007869958877563 + 10.0 * 8.313848495483398
Epoch 2220, val loss: 0.36971911787986755
Epoch 2230, training loss: 83.44256591796875 = 0.28950035572052 + 10.0 * 8.315306663513184
Epoch 2230, val loss: 0.36964237689971924
Epoch 2240, training loss: 83.42585754394531 = 0.28891852498054504 + 10.0 * 8.31369400024414
Epoch 2240, val loss: 0.3695502281188965
Epoch 2250, training loss: 83.41810607910156 = 0.2883472740650177 + 10.0 * 8.312975883483887
Epoch 2250, val loss: 0.369448184967041
Epoch 2260, training loss: 83.41609191894531 = 0.2877781391143799 + 10.0 * 8.312830924987793
Epoch 2260, val loss: 0.3693288266658783
Epoch 2270, training loss: 83.4118423461914 = 0.2872093617916107 + 10.0 * 8.312463760375977
Epoch 2270, val loss: 0.36925557255744934
Epoch 2280, training loss: 83.4124984741211 = 0.28664249181747437 + 10.0 * 8.312585830688477
Epoch 2280, val loss: 0.3691483736038208
Epoch 2290, training loss: 83.45030212402344 = 0.28607797622680664 + 10.0 * 8.316422462463379
Epoch 2290, val loss: 0.36898869276046753
Epoch 2300, training loss: 83.42741394042969 = 0.2855139672756195 + 10.0 * 8.314189910888672
Epoch 2300, val loss: 0.36914074420928955
Epoch 2310, training loss: 83.4097671508789 = 0.2849605977535248 + 10.0 * 8.312480926513672
Epoch 2310, val loss: 0.3689432740211487
Epoch 2320, training loss: 83.40203857421875 = 0.28441187739372253 + 10.0 * 8.311762809753418
Epoch 2320, val loss: 0.3688932955265045
Epoch 2330, training loss: 83.40064239501953 = 0.2838638722896576 + 10.0 * 8.311677932739258
Epoch 2330, val loss: 0.3688659369945526
Epoch 2340, training loss: 83.39924621582031 = 0.28331583738327026 + 10.0 * 8.311593055725098
Epoch 2340, val loss: 0.3687705099582672
Epoch 2350, training loss: 83.42500305175781 = 0.28276708722114563 + 10.0 * 8.314223289489746
Epoch 2350, val loss: 0.3687382936477661
Epoch 2360, training loss: 83.41558837890625 = 0.28223171830177307 + 10.0 * 8.313335418701172
Epoch 2360, val loss: 0.36868801712989807
Epoch 2370, training loss: 83.40177917480469 = 0.28169095516204834 + 10.0 * 8.31200885772705
Epoch 2370, val loss: 0.3686444163322449
Epoch 2380, training loss: 83.39071655273438 = 0.2811649739742279 + 10.0 * 8.310955047607422
Epoch 2380, val loss: 0.3686179518699646
Epoch 2390, training loss: 83.39010620117188 = 0.2806392014026642 + 10.0 * 8.310946464538574
Epoch 2390, val loss: 0.3685747981071472
Epoch 2400, training loss: 83.39498138427734 = 0.2801099419593811 + 10.0 * 8.311487197875977
Epoch 2400, val loss: 0.36853423714637756
Epoch 2410, training loss: 83.4352798461914 = 0.2795812487602234 + 10.0 * 8.315569877624512
Epoch 2410, val loss: 0.36852651834487915
Epoch 2420, training loss: 83.39147186279297 = 0.27906420826911926 + 10.0 * 8.311240196228027
Epoch 2420, val loss: 0.3685007691383362
Epoch 2430, training loss: 83.38195037841797 = 0.27854835987091064 + 10.0 * 8.31033992767334
Epoch 2430, val loss: 0.36849090456962585
Epoch 2440, training loss: 83.38091278076172 = 0.27803677320480347 + 10.0 * 8.310287475585938
Epoch 2440, val loss: 0.3684515357017517
Epoch 2450, training loss: 83.37879180908203 = 0.2775266468524933 + 10.0 * 8.310126304626465
Epoch 2450, val loss: 0.36844590306282043
Epoch 2460, training loss: 83.37713623046875 = 0.277014821767807 + 10.0 * 8.310011863708496
Epoch 2460, val loss: 0.368414044380188
Epoch 2470, training loss: 83.39356994628906 = 0.2765052616596222 + 10.0 * 8.31170654296875
Epoch 2470, val loss: 0.36841222643852234
Epoch 2480, training loss: 83.3755874633789 = 0.2759924829006195 + 10.0 * 8.309959411621094
Epoch 2480, val loss: 0.36842426657676697
Epoch 2490, training loss: 83.37281036376953 = 0.2754911482334137 + 10.0 * 8.309732437133789
Epoch 2490, val loss: 0.36841902136802673
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8548959918822931
0.8638701731507644
=== training gcn model ===
Epoch 0, training loss: 106.91104125976562 = 1.0884108543395996 + 10.0 * 10.582262992858887
Epoch 0, val loss: 1.088044285774231
Epoch 10, training loss: 106.90228271484375 = 1.0841872692108154 + 10.0 * 10.581809043884277
Epoch 10, val loss: 1.0838851928710938
Epoch 20, training loss: 106.876953125 = 1.0800656080245972 + 10.0 * 10.579689025878906
Epoch 20, val loss: 1.0798207521438599
Epoch 30, training loss: 106.77925109863281 = 1.075954556465149 + 10.0 * 10.570329666137695
Epoch 30, val loss: 1.0757349729537964
Epoch 40, training loss: 106.42892456054688 = 1.0714237689971924 + 10.0 * 10.535749435424805
Epoch 40, val loss: 1.0711827278137207
Epoch 50, training loss: 105.46885681152344 = 1.0661629438400269 + 10.0 * 10.440269470214844
Epoch 50, val loss: 1.0658923387527466
Epoch 60, training loss: 103.30213928222656 = 1.0604352951049805 + 10.0 * 10.224170684814453
Epoch 60, val loss: 1.0601341724395752
Epoch 70, training loss: 99.15605926513672 = 1.0540367364883423 + 10.0 * 9.810201644897461
Epoch 70, val loss: 1.0536521673202515
Epoch 80, training loss: 94.51959228515625 = 1.047548770904541 + 10.0 * 9.347204208374023
Epoch 80, val loss: 1.0472816228866577
Epoch 90, training loss: 93.55853271484375 = 1.0414791107177734 + 10.0 * 9.251705169677734
Epoch 90, val loss: 1.0413838624954224
Epoch 100, training loss: 93.02307891845703 = 1.0363253355026245 + 10.0 * 9.198675155639648
Epoch 100, val loss: 1.036462426185608
Epoch 110, training loss: 92.81189727783203 = 1.0324152708053589 + 10.0 * 9.177947998046875
Epoch 110, val loss: 1.0327504873275757
Epoch 120, training loss: 92.58695220947266 = 1.0293750762939453 + 10.0 * 9.155757904052734
Epoch 120, val loss: 1.0298372507095337
Epoch 130, training loss: 92.30802917480469 = 1.0269442796707153 + 10.0 * 9.128108024597168
Epoch 130, val loss: 1.0275071859359741
Epoch 140, training loss: 91.9926986694336 = 1.0250691175460815 + 10.0 * 9.096762657165527
Epoch 140, val loss: 1.0257059335708618
Epoch 150, training loss: 91.62762451171875 = 1.023409366607666 + 10.0 * 9.06042194366455
Epoch 150, val loss: 1.0240991115570068
Epoch 160, training loss: 91.20258331298828 = 1.022066593170166 + 10.0 * 9.018052101135254
Epoch 160, val loss: 1.022850751876831
Epoch 170, training loss: 90.78385162353516 = 1.0213149785995483 + 10.0 * 8.976253509521484
Epoch 170, val loss: 1.0221624374389648
Epoch 180, training loss: 90.48213958740234 = 1.02103853225708 + 10.0 * 8.946109771728516
Epoch 180, val loss: 1.021888017654419
Epoch 190, training loss: 90.2817611694336 = 1.0206739902496338 + 10.0 * 8.926108360290527
Epoch 190, val loss: 1.0214751958847046
Epoch 200, training loss: 90.01343536376953 = 1.0197412967681885 + 10.0 * 8.899369239807129
Epoch 200, val loss: 1.0205397605895996
Epoch 210, training loss: 89.61222839355469 = 1.0186923742294312 + 10.0 * 8.859354019165039
Epoch 210, val loss: 1.0195573568344116
Epoch 220, training loss: 89.163818359375 = 1.0177308320999146 + 10.0 * 8.814608573913574
Epoch 220, val loss: 1.0185896158218384
Epoch 230, training loss: 88.85115051269531 = 1.0166304111480713 + 10.0 * 8.783452033996582
Epoch 230, val loss: 1.0174442529678345
Epoch 240, training loss: 88.61292266845703 = 1.015374779701233 + 10.0 * 8.75975513458252
Epoch 240, val loss: 1.0162276029586792
Epoch 250, training loss: 88.34964752197266 = 1.014662504196167 + 10.0 * 8.733498573303223
Epoch 250, val loss: 1.0156227350234985
Epoch 260, training loss: 88.11119079589844 = 1.0145493745803833 + 10.0 * 8.709664344787598
Epoch 260, val loss: 1.0154945850372314
Epoch 270, training loss: 87.94637298583984 = 1.0138546228408813 + 10.0 * 8.693251609802246
Epoch 270, val loss: 1.0147117376327515
Epoch 280, training loss: 87.81396484375 = 1.0127073526382446 + 10.0 * 8.68012523651123
Epoch 280, val loss: 1.0135164260864258
Epoch 290, training loss: 87.6014175415039 = 1.0118952989578247 + 10.0 * 8.658952713012695
Epoch 290, val loss: 1.0128657817840576
Epoch 300, training loss: 87.38783264160156 = 1.0117806196212769 + 10.0 * 8.637605667114258
Epoch 300, val loss: 1.0128304958343506
Epoch 310, training loss: 87.1657943725586 = 1.011762022972107 + 10.0 * 8.615403175354004
Epoch 310, val loss: 1.0128722190856934
Epoch 320, training loss: 86.972900390625 = 1.0114550590515137 + 10.0 * 8.596144676208496
Epoch 320, val loss: 1.0124902725219727
Epoch 330, training loss: 86.777099609375 = 1.0107611417770386 + 10.0 * 8.576634407043457
Epoch 330, val loss: 1.0118813514709473
Epoch 340, training loss: 86.60774993896484 = 1.0099819898605347 + 10.0 * 8.55977725982666
Epoch 340, val loss: 1.0110714435577393
Epoch 350, training loss: 86.47504425048828 = 1.0088797807693481 + 10.0 * 8.546616554260254
Epoch 350, val loss: 1.00999116897583
Epoch 360, training loss: 86.37847137451172 = 1.0074349641799927 + 10.0 * 8.537103652954102
Epoch 360, val loss: 1.008570671081543
Epoch 370, training loss: 86.3395767211914 = 1.005753755569458 + 10.0 * 8.533382415771484
Epoch 370, val loss: 1.0069794654846191
Epoch 380, training loss: 86.2545394897461 = 1.0038681030273438 + 10.0 * 8.525067329406738
Epoch 380, val loss: 1.0051225423812866
Epoch 390, training loss: 86.20550537109375 = 1.0019042491912842 + 10.0 * 8.520359992980957
Epoch 390, val loss: 1.003200888633728
Epoch 400, training loss: 86.16592407226562 = 0.9997665286064148 + 10.0 * 8.516615867614746
Epoch 400, val loss: 1.0011404752731323
Epoch 410, training loss: 86.13632202148438 = 0.9974333643913269 + 10.0 * 8.513888359069824
Epoch 410, val loss: 0.998865008354187
Epoch 420, training loss: 86.10740661621094 = 0.9949181079864502 + 10.0 * 8.511248588562012
Epoch 420, val loss: 0.9964274168014526
Epoch 430, training loss: 86.08154296875 = 0.9923103451728821 + 10.0 * 8.508923530578613
Epoch 430, val loss: 0.9939047694206238
Epoch 440, training loss: 86.05116271972656 = 0.9896219968795776 + 10.0 * 8.50615406036377
Epoch 440, val loss: 0.9913128614425659
Epoch 450, training loss: 86.02154541015625 = 0.986858606338501 + 10.0 * 8.50346851348877
Epoch 450, val loss: 0.9886547923088074
Epoch 460, training loss: 86.01879119873047 = 0.9839895963668823 + 10.0 * 8.503480911254883
Epoch 460, val loss: 0.9858396053314209
Epoch 470, training loss: 85.96536254882812 = 0.9809572100639343 + 10.0 * 8.498440742492676
Epoch 470, val loss: 0.9829463958740234
Epoch 480, training loss: 85.92337799072266 = 0.9780003428459167 + 10.0 * 8.494538307189941
Epoch 480, val loss: 0.9801228642463684
Epoch 490, training loss: 85.88140106201172 = 0.9749864935874939 + 10.0 * 8.490641593933105
Epoch 490, val loss: 0.9772218465805054
Epoch 500, training loss: 85.837646484375 = 0.9718842506408691 + 10.0 * 8.486576080322266
Epoch 500, val loss: 0.9742411971092224
Epoch 510, training loss: 85.79486083984375 = 0.9686854481697083 + 10.0 * 8.482617378234863
Epoch 510, val loss: 0.9710900187492371
Epoch 520, training loss: 85.73829650878906 = 0.9653904438018799 + 10.0 * 8.477290153503418
Epoch 520, val loss: 0.9679784774780273
Epoch 530, training loss: 85.68399047851562 = 0.9621120691299438 + 10.0 * 8.472187995910645
Epoch 530, val loss: 0.9648134112358093
Epoch 540, training loss: 85.6712646484375 = 0.9586004018783569 + 10.0 * 8.47126579284668
Epoch 540, val loss: 0.9614635109901428
Epoch 550, training loss: 85.57838439941406 = 0.9548174738883972 + 10.0 * 8.462356567382812
Epoch 550, val loss: 0.957729697227478
Epoch 560, training loss: 85.53083038330078 = 0.9509941935539246 + 10.0 * 8.457983016967773
Epoch 560, val loss: 0.9539979100227356
Epoch 570, training loss: 85.48107147216797 = 0.9468854665756226 + 10.0 * 8.453418731689453
Epoch 570, val loss: 0.9499940872192383
Epoch 580, training loss: 85.43722534179688 = 0.94244384765625 + 10.0 * 8.449478149414062
Epoch 580, val loss: 0.9456630945205688
Epoch 590, training loss: 85.44349670410156 = 0.9376682043075562 + 10.0 * 8.450582504272461
Epoch 590, val loss: 0.940940260887146
Epoch 600, training loss: 85.37300109863281 = 0.93238765001297 + 10.0 * 8.444061279296875
Epoch 600, val loss: 0.9359230399131775
Epoch 610, training loss: 85.32112121582031 = 0.9271063804626465 + 10.0 * 8.439401626586914
Epoch 610, val loss: 0.9308566451072693
Epoch 620, training loss: 85.27935028076172 = 0.9216094613075256 + 10.0 * 8.435773849487305
Epoch 620, val loss: 0.9255561232566833
Epoch 630, training loss: 85.24227142333984 = 0.9157879948616028 + 10.0 * 8.432648658752441
Epoch 630, val loss: 0.9199590682983398
Epoch 640, training loss: 85.21226501464844 = 0.9095335602760315 + 10.0 * 8.430273056030273
Epoch 640, val loss: 0.9138453602790833
Epoch 650, training loss: 85.17283630371094 = 0.9031057357788086 + 10.0 * 8.426973342895508
Epoch 650, val loss: 0.907734751701355
Epoch 660, training loss: 85.13732147216797 = 0.896679699420929 + 10.0 * 8.424063682556152
Epoch 660, val loss: 0.9015642404556274
Epoch 670, training loss: 85.09886169433594 = 0.8898805379867554 + 10.0 * 8.4208984375
Epoch 670, val loss: 0.8950347304344177
Epoch 680, training loss: 85.06590270996094 = 0.8827522397041321 + 10.0 * 8.418314933776855
Epoch 680, val loss: 0.8881798386573792
Epoch 690, training loss: 85.03480529785156 = 0.8752744793891907 + 10.0 * 8.415952682495117
Epoch 690, val loss: 0.8809782266616821
Epoch 700, training loss: 85.02461242675781 = 0.8673971891403198 + 10.0 * 8.415721893310547
Epoch 700, val loss: 0.8734893798828125
Epoch 710, training loss: 84.98756408691406 = 0.8591834902763367 + 10.0 * 8.412837982177734
Epoch 710, val loss: 0.8654988408088684
Epoch 720, training loss: 84.96072387695312 = 0.8508508801460266 + 10.0 * 8.41098690032959
Epoch 720, val loss: 0.8574833869934082
Epoch 730, training loss: 84.9249267578125 = 0.8424068093299866 + 10.0 * 8.408251762390137
Epoch 730, val loss: 0.849243700504303
Epoch 740, training loss: 84.89867401123047 = 0.8335652947425842 + 10.0 * 8.406511306762695
Epoch 740, val loss: 0.8407007455825806
Epoch 750, training loss: 84.87199401855469 = 0.8243528008460999 + 10.0 * 8.404764175415039
Epoch 750, val loss: 0.8318460583686829
Epoch 760, training loss: 84.84601593017578 = 0.8148865103721619 + 10.0 * 8.403112411499023
Epoch 760, val loss: 0.8227502107620239
Epoch 770, training loss: 84.83007049560547 = 0.8051126599311829 + 10.0 * 8.402495384216309
Epoch 770, val loss: 0.8133814334869385
Epoch 780, training loss: 84.8126220703125 = 0.7950482368469238 + 10.0 * 8.40175724029541
Epoch 780, val loss: 0.8036919236183167
Epoch 790, training loss: 84.7765121459961 = 0.7849625945091248 + 10.0 * 8.399154663085938
Epoch 790, val loss: 0.7940204739570618
Epoch 800, training loss: 84.74485778808594 = 0.7747570276260376 + 10.0 * 8.39700984954834
Epoch 800, val loss: 0.7842729687690735
Epoch 810, training loss: 84.7223129272461 = 0.7643821835517883 + 10.0 * 8.395792961120605
Epoch 810, val loss: 0.7743793725967407
Epoch 820, training loss: 84.70753479003906 = 0.7537394165992737 + 10.0 * 8.395380020141602
Epoch 820, val loss: 0.7641876935958862
Epoch 830, training loss: 84.6652603149414 = 0.7431058883666992 + 10.0 * 8.392215728759766
Epoch 830, val loss: 0.7540296912193298
Epoch 840, training loss: 84.63936614990234 = 0.7325842976570129 + 10.0 * 8.390678405761719
Epoch 840, val loss: 0.7439740896224976
Epoch 850, training loss: 84.6178970336914 = 0.7219290733337402 + 10.0 * 8.389596939086914
Epoch 850, val loss: 0.7339021563529968
Epoch 860, training loss: 84.5903549194336 = 0.711226224899292 + 10.0 * 8.38791275024414
Epoch 860, val loss: 0.7235903143882751
Epoch 870, training loss: 84.56095123291016 = 0.7005437016487122 + 10.0 * 8.386040687561035
Epoch 870, val loss: 0.7135159969329834
Epoch 880, training loss: 84.53038787841797 = 0.6899968981742859 + 10.0 * 8.384038925170898
Epoch 880, val loss: 0.703477144241333
Epoch 890, training loss: 84.50459289550781 = 0.6794614195823669 + 10.0 * 8.382513046264648
Epoch 890, val loss: 0.6934775710105896
Epoch 900, training loss: 84.49835205078125 = 0.6689300537109375 + 10.0 * 8.382942199707031
Epoch 900, val loss: 0.6834625601768494
Epoch 910, training loss: 84.50741577148438 = 0.6581441760063171 + 10.0 * 8.384927749633789
Epoch 910, val loss: 0.6733225584030151
Epoch 920, training loss: 84.45093536376953 = 0.6479026675224304 + 10.0 * 8.380303382873535
Epoch 920, val loss: 0.6636123657226562
Epoch 930, training loss: 84.41651153564453 = 0.6379663944244385 + 10.0 * 8.377854347229004
Epoch 930, val loss: 0.6542174816131592
Epoch 940, training loss: 84.3911361694336 = 0.6280730962753296 + 10.0 * 8.376306533813477
Epoch 940, val loss: 0.6448559761047363
Epoch 950, training loss: 84.36900329589844 = 0.6182249188423157 + 10.0 * 8.375078201293945
Epoch 950, val loss: 0.6355549097061157
Epoch 960, training loss: 84.3475570678711 = 0.6084955334663391 + 10.0 * 8.373906135559082
Epoch 960, val loss: 0.6263803839683533
Epoch 970, training loss: 84.35326385498047 = 0.598842442035675 + 10.0 * 8.375442504882812
Epoch 970, val loss: 0.6174222230911255
Epoch 980, training loss: 84.32238006591797 = 0.5894424915313721 + 10.0 * 8.37329387664795
Epoch 980, val loss: 0.6084057688713074
Epoch 990, training loss: 84.2897720336914 = 0.5803229808807373 + 10.0 * 8.37094497680664
Epoch 990, val loss: 0.599835216999054
Epoch 1000, training loss: 84.26861572265625 = 0.5714991688728333 + 10.0 * 8.369710922241211
Epoch 1000, val loss: 0.5915765166282654
Epoch 1010, training loss: 84.24964141845703 = 0.56288081407547 + 10.0 * 8.36867618560791
Epoch 1010, val loss: 0.5834912657737732
Epoch 1020, training loss: 84.23310852050781 = 0.5544267892837524 + 10.0 * 8.367868423461914
Epoch 1020, val loss: 0.5755800604820251
Epoch 1030, training loss: 84.2236557006836 = 0.5460956692695618 + 10.0 * 8.367755889892578
Epoch 1030, val loss: 0.5677748322486877
Epoch 1040, training loss: 84.20541381835938 = 0.5380253791809082 + 10.0 * 8.366739273071289
Epoch 1040, val loss: 0.5602766275405884
Epoch 1050, training loss: 84.18133544921875 = 0.5303266048431396 + 10.0 * 8.365100860595703
Epoch 1050, val loss: 0.5530804991722107
Epoch 1060, training loss: 84.1637191772461 = 0.5228621363639832 + 10.0 * 8.36408519744873
Epoch 1060, val loss: 0.5460999608039856
Epoch 1070, training loss: 84.16019439697266 = 0.5155102014541626 + 10.0 * 8.364468574523926
Epoch 1070, val loss: 0.5393046736717224
Epoch 1080, training loss: 84.13626861572266 = 0.5083075165748596 + 10.0 * 8.36279582977295
Epoch 1080, val loss: 0.5326169729232788
Epoch 1090, training loss: 84.12164306640625 = 0.5014544129371643 + 10.0 * 8.362018585205078
Epoch 1090, val loss: 0.5263186693191528
Epoch 1100, training loss: 84.10405731201172 = 0.4948660433292389 + 10.0 * 8.360918998718262
Epoch 1100, val loss: 0.5201971530914307
Epoch 1110, training loss: 84.08799743652344 = 0.4884508550167084 + 10.0 * 8.359954833984375
Epoch 1110, val loss: 0.5143458843231201
Epoch 1120, training loss: 84.11422729492188 = 0.4821474254131317 + 10.0 * 8.363207817077637
Epoch 1120, val loss: 0.50860196352005
Epoch 1130, training loss: 84.06339263916016 = 0.47617509961128235 + 10.0 * 8.358721733093262
Epoch 1130, val loss: 0.5030319094657898
Epoch 1140, training loss: 84.05122375488281 = 0.4704834818840027 + 10.0 * 8.358074188232422
Epoch 1140, val loss: 0.4978772699832916
Epoch 1150, training loss: 84.03752136230469 = 0.46497470140457153 + 10.0 * 8.357254981994629
Epoch 1150, val loss: 0.49291056394577026
Epoch 1160, training loss: 84.05337524414062 = 0.4596099555492401 + 10.0 * 8.359376907348633
Epoch 1160, val loss: 0.4880528151988983
Epoch 1170, training loss: 84.01826477050781 = 0.45441126823425293 + 10.0 * 8.356385231018066
Epoch 1170, val loss: 0.4833627939224243
Epoch 1180, training loss: 84.00138092041016 = 0.4495212435722351 + 10.0 * 8.355185508728027
Epoch 1180, val loss: 0.47896715998649597
Epoch 1190, training loss: 83.98805236816406 = 0.44478219747543335 + 10.0 * 8.354327201843262
Epoch 1190, val loss: 0.47473037242889404
Epoch 1200, training loss: 83.9970703125 = 0.4402565360069275 + 10.0 * 8.355681419372559
Epoch 1200, val loss: 0.4706358313560486
Epoch 1210, training loss: 83.97860717773438 = 0.43586575984954834 + 10.0 * 8.354273796081543
Epoch 1210, val loss: 0.46693164110183716
Epoch 1220, training loss: 83.96115112304688 = 0.4317103624343872 + 10.0 * 8.352944374084473
Epoch 1220, val loss: 0.4631820619106293
Epoch 1230, training loss: 83.94635009765625 = 0.42772176861763 + 10.0 * 8.351862907409668
Epoch 1230, val loss: 0.4597083032131195
Epoch 1240, training loss: 83.94856262207031 = 0.4238906502723694 + 10.0 * 8.35246753692627
Epoch 1240, val loss: 0.45641839504241943
Epoch 1250, training loss: 83.93199920654297 = 0.4201768636703491 + 10.0 * 8.35118293762207
Epoch 1250, val loss: 0.45322296023368835
Epoch 1260, training loss: 83.91703796386719 = 0.4166562855243683 + 10.0 * 8.350038528442383
Epoch 1260, val loss: 0.45016494393348694
Epoch 1270, training loss: 83.90776062011719 = 0.41330835223197937 + 10.0 * 8.349445343017578
Epoch 1270, val loss: 0.44732892513275146
Epoch 1280, training loss: 83.89997100830078 = 0.41006866097450256 + 10.0 * 8.348989486694336
Epoch 1280, val loss: 0.4445771872997284
Epoch 1290, training loss: 83.9280014038086 = 0.4069303274154663 + 10.0 * 8.352107048034668
Epoch 1290, val loss: 0.441919207572937
Epoch 1300, training loss: 83.88668060302734 = 0.4038727879524231 + 10.0 * 8.348280906677246
Epoch 1300, val loss: 0.43941977620124817
Epoch 1310, training loss: 83.87530517578125 = 0.4010225236415863 + 10.0 * 8.347428321838379
Epoch 1310, val loss: 0.437057226896286
Epoch 1320, training loss: 83.86643981933594 = 0.39826756715774536 + 10.0 * 8.346817016601562
Epoch 1320, val loss: 0.43477702140808105
Epoch 1330, training loss: 83.86144256591797 = 0.39559605717658997 + 10.0 * 8.34658432006836
Epoch 1330, val loss: 0.43259578943252563
Epoch 1340, training loss: 83.86512756347656 = 0.39299774169921875 + 10.0 * 8.347212791442871
Epoch 1340, val loss: 0.4305262565612793
Epoch 1350, training loss: 83.85249328613281 = 0.3905000686645508 + 10.0 * 8.346199035644531
Epoch 1350, val loss: 0.4285106062889099
Epoch 1360, training loss: 83.837646484375 = 0.38813117146492004 + 10.0 * 8.344951629638672
Epoch 1360, val loss: 0.42666342854499817
Epoch 1370, training loss: 83.82817840576172 = 0.38586947321891785 + 10.0 * 8.344230651855469
Epoch 1370, val loss: 0.42486444115638733
Epoch 1380, training loss: 83.82107543945312 = 0.3836691081523895 + 10.0 * 8.343740463256836
Epoch 1380, val loss: 0.4231538772583008
Epoch 1390, training loss: 83.81729888916016 = 0.38153496384620667 + 10.0 * 8.343576431274414
Epoch 1390, val loss: 0.42151159048080444
Epoch 1400, training loss: 83.83456420898438 = 0.37944865226745605 + 10.0 * 8.345511436462402
Epoch 1400, val loss: 0.4199598431587219
Epoch 1410, training loss: 83.80498504638672 = 0.3774167597293854 + 10.0 * 8.342756271362305
Epoch 1410, val loss: 0.4184333086013794
Epoch 1420, training loss: 83.7972640991211 = 0.3754825294017792 + 10.0 * 8.342178344726562
Epoch 1420, val loss: 0.4169659912586212
Epoch 1430, training loss: 83.79029846191406 = 0.37362897396087646 + 10.0 * 8.341667175292969
Epoch 1430, val loss: 0.4155697822570801
Epoch 1440, training loss: 83.79297637939453 = 0.3718050718307495 + 10.0 * 8.342117309570312
Epoch 1440, val loss: 0.4142385721206665
Epoch 1450, training loss: 83.7829360961914 = 0.3700195848941803 + 10.0 * 8.341291427612305
Epoch 1450, val loss: 0.4129681885242462
Epoch 1460, training loss: 83.77664947509766 = 0.36830684542655945 + 10.0 * 8.34083366394043
Epoch 1460, val loss: 0.4117446541786194
Epoch 1470, training loss: 83.76622772216797 = 0.36664021015167236 + 10.0 * 8.339959144592285
Epoch 1470, val loss: 0.4105469882488251
Epoch 1480, training loss: 83.76636505126953 = 0.36501550674438477 + 10.0 * 8.34013557434082
Epoch 1480, val loss: 0.4094374477863312
Epoch 1490, training loss: 83.763427734375 = 0.3634229004383087 + 10.0 * 8.34000015258789
Epoch 1490, val loss: 0.40833184123039246
Epoch 1500, training loss: 83.75138854980469 = 0.361894428730011 + 10.0 * 8.338949203491211
Epoch 1500, val loss: 0.40723544359207153
Epoch 1510, training loss: 83.74462890625 = 0.3603990972042084 + 10.0 * 8.338422775268555
Epoch 1510, val loss: 0.4062132239341736
Epoch 1520, training loss: 83.74579620361328 = 0.3589383363723755 + 10.0 * 8.338685989379883
Epoch 1520, val loss: 0.4052196145057678
Epoch 1530, training loss: 83.74113464355469 = 0.35751304030418396 + 10.0 * 8.338361740112305
Epoch 1530, val loss: 0.4042949974536896
Epoch 1540, training loss: 83.75080108642578 = 0.35611090064048767 + 10.0 * 8.339468955993652
Epoch 1540, val loss: 0.40344539284706116
Epoch 1550, training loss: 83.72905731201172 = 0.3547447621822357 + 10.0 * 8.337430953979492
Epoch 1550, val loss: 0.402483195066452
Epoch 1560, training loss: 83.72000885009766 = 0.3534300625324249 + 10.0 * 8.336657524108887
Epoch 1560, val loss: 0.4016745984554291
Epoch 1570, training loss: 83.71524047851562 = 0.3521333336830139 + 10.0 * 8.336310386657715
Epoch 1570, val loss: 0.4008353352546692
Epoch 1580, training loss: 83.71363067626953 = 0.35086461901664734 + 10.0 * 8.336276054382324
Epoch 1580, val loss: 0.4000372886657715
Epoch 1590, training loss: 83.72188568115234 = 0.34961721301078796 + 10.0 * 8.337226867675781
Epoch 1590, val loss: 0.3992687463760376
Epoch 1600, training loss: 83.70562744140625 = 0.348387211561203 + 10.0 * 8.335723876953125
Epoch 1600, val loss: 0.3985755443572998
Epoch 1610, training loss: 83.73373413085938 = 0.3471902310848236 + 10.0 * 8.338654518127441
Epoch 1610, val loss: 0.3977885842323303
Epoch 1620, training loss: 83.702880859375 = 0.3460098206996918 + 10.0 * 8.335687637329102
Epoch 1620, val loss: 0.397185742855072
Epoch 1630, training loss: 83.69148254394531 = 0.3448813557624817 + 10.0 * 8.334660530090332
Epoch 1630, val loss: 0.3964797258377075
Epoch 1640, training loss: 83.68360137939453 = 0.343761146068573 + 10.0 * 8.333984375
Epoch 1640, val loss: 0.3958361744880676
Epoch 1650, training loss: 83.67799377441406 = 0.34266138076782227 + 10.0 * 8.33353328704834
Epoch 1650, val loss: 0.3952091336250305
Epoch 1660, training loss: 83.67501831054688 = 0.3415709435939789 + 10.0 * 8.333345413208008
Epoch 1660, val loss: 0.39461299777030945
Epoch 1670, training loss: 83.70169830322266 = 0.3404940068721771 + 10.0 * 8.33612060546875
Epoch 1670, val loss: 0.39401206374168396
Epoch 1680, training loss: 83.67332458496094 = 0.3394179046154022 + 10.0 * 8.333391189575195
Epoch 1680, val loss: 0.393419474363327
Epoch 1690, training loss: 83.68412017822266 = 0.3383902311325073 + 10.0 * 8.334573745727539
Epoch 1690, val loss: 0.3929000794887543
Epoch 1700, training loss: 83.65863800048828 = 0.33736076951026917 + 10.0 * 8.332127571105957
Epoch 1700, val loss: 0.39233312010765076
Epoch 1710, training loss: 83.65511322021484 = 0.3363695740699768 + 10.0 * 8.331873893737793
Epoch 1710, val loss: 0.39180636405944824
Epoch 1720, training loss: 83.64950561523438 = 0.33538565039634705 + 10.0 * 8.331411361694336
Epoch 1720, val loss: 0.39130640029907227
Epoch 1730, training loss: 83.6455078125 = 0.33441591262817383 + 10.0 * 8.331109046936035
Epoch 1730, val loss: 0.3908003270626068
Epoch 1740, training loss: 83.6427993774414 = 0.3334549069404602 + 10.0 * 8.330934524536133
Epoch 1740, val loss: 0.39031562209129333
Epoch 1750, training loss: 83.69634246826172 = 0.33249935507774353 + 10.0 * 8.336384773254395
Epoch 1750, val loss: 0.38983532786369324
Epoch 1760, training loss: 83.65801239013672 = 0.33153703808784485 + 10.0 * 8.332647323608398
Epoch 1760, val loss: 0.38941213488578796
Epoch 1770, training loss: 83.63353729248047 = 0.3306237757205963 + 10.0 * 8.330290794372559
Epoch 1770, val loss: 0.3889467716217041
Epoch 1780, training loss: 83.62848663330078 = 0.3297283351421356 + 10.0 * 8.329875946044922
Epoch 1780, val loss: 0.38851919770240784
Epoch 1790, training loss: 83.62451934814453 = 0.32883650064468384 + 10.0 * 8.329568862915039
Epoch 1790, val loss: 0.3880882263183594
Epoch 1800, training loss: 83.62029266357422 = 0.3279474079608917 + 10.0 * 8.32923412322998
Epoch 1800, val loss: 0.3876797556877136
Epoch 1810, training loss: 83.61907196044922 = 0.3270663917064667 + 10.0 * 8.329200744628906
Epoch 1810, val loss: 0.38728007674217224
Epoch 1820, training loss: 83.6658935546875 = 0.32618603110313416 + 10.0 * 8.33397102355957
Epoch 1820, val loss: 0.3869723379611969
Epoch 1830, training loss: 83.61307525634766 = 0.3253253102302551 + 10.0 * 8.328775405883789
Epoch 1830, val loss: 0.38648203015327454
Epoch 1840, training loss: 83.61270141601562 = 0.3244900405406952 + 10.0 * 8.328821182250977
Epoch 1840, val loss: 0.3861508369445801
Epoch 1850, training loss: 83.60381317138672 = 0.3236607015132904 + 10.0 * 8.328015327453613
Epoch 1850, val loss: 0.3857819437980652
Epoch 1860, training loss: 83.60201263427734 = 0.32283851504325867 + 10.0 * 8.327917098999023
Epoch 1860, val loss: 0.385459303855896
Epoch 1870, training loss: 83.63431549072266 = 0.3220207989215851 + 10.0 * 8.331229209899902
Epoch 1870, val loss: 0.3851362466812134
Epoch 1880, training loss: 83.60611724853516 = 0.32119205594062805 + 10.0 * 8.328493118286133
Epoch 1880, val loss: 0.38476189970970154
Epoch 1890, training loss: 83.595947265625 = 0.32040777802467346 + 10.0 * 8.327554702758789
Epoch 1890, val loss: 0.3844832479953766
Epoch 1900, training loss: 83.58829498291016 = 0.31961578130722046 + 10.0 * 8.326868057250977
Epoch 1900, val loss: 0.3841482996940613
Epoch 1910, training loss: 83.58399200439453 = 0.31883394718170166 + 10.0 * 8.326516151428223
Epoch 1910, val loss: 0.3838422894477844
Epoch 1920, training loss: 83.58073425292969 = 0.3180539309978485 + 10.0 * 8.326268196105957
Epoch 1920, val loss: 0.38355061411857605
Epoch 1930, training loss: 83.5792465209961 = 0.31728020310401917 + 10.0 * 8.326196670532227
Epoch 1930, val loss: 0.38325586915016174
Epoch 1940, training loss: 83.62259674072266 = 0.31651055812835693 + 10.0 * 8.330608367919922
Epoch 1940, val loss: 0.3829960227012634
Epoch 1950, training loss: 83.58577728271484 = 0.31572940945625305 + 10.0 * 8.327005386352539
Epoch 1950, val loss: 0.38265925645828247
Epoch 1960, training loss: 83.57137298583984 = 0.3149893879890442 + 10.0 * 8.325638771057129
Epoch 1960, val loss: 0.3824429512023926
Epoch 1970, training loss: 83.566650390625 = 0.3142443299293518 + 10.0 * 8.325240135192871
Epoch 1970, val loss: 0.3821331560611725
Epoch 1980, training loss: 83.56462860107422 = 0.3135075271129608 + 10.0 * 8.325112342834473
Epoch 1980, val loss: 0.38189056515693665
Epoch 1990, training loss: 83.59573364257812 = 0.31276923418045044 + 10.0 * 8.328296661376953
Epoch 1990, val loss: 0.3815639615058899
Epoch 2000, training loss: 83.5666275024414 = 0.3120437562465668 + 10.0 * 8.325458526611328
Epoch 2000, val loss: 0.3814534544944763
Epoch 2010, training loss: 83.55430603027344 = 0.311324805021286 + 10.0 * 8.324297904968262
Epoch 2010, val loss: 0.38117650151252747
Epoch 2020, training loss: 83.5510482788086 = 0.3106129467487335 + 10.0 * 8.324043273925781
Epoch 2020, val loss: 0.38092875480651855
Epoch 2030, training loss: 83.5538101196289 = 0.30990520119667053 + 10.0 * 8.324390411376953
Epoch 2030, val loss: 0.38074085116386414
Epoch 2040, training loss: 83.5775375366211 = 0.30919745564460754 + 10.0 * 8.326833724975586
Epoch 2040, val loss: 0.3805294632911682
Epoch 2050, training loss: 83.55199432373047 = 0.30850011110305786 + 10.0 * 8.324349403381348
Epoch 2050, val loss: 0.38025224208831787
Epoch 2060, training loss: 83.54248046875 = 0.3078167140483856 + 10.0 * 8.323466300964355
Epoch 2060, val loss: 0.3800813555717468
Epoch 2070, training loss: 83.54234313964844 = 0.3071326017379761 + 10.0 * 8.323521614074707
Epoch 2070, val loss: 0.37985050678253174
Epoch 2080, training loss: 83.55449676513672 = 0.30644741654396057 + 10.0 * 8.324804306030273
Epoch 2080, val loss: 0.37968942523002625
Epoch 2090, training loss: 83.53278350830078 = 0.30577751994132996 + 10.0 * 8.322700500488281
Epoch 2090, val loss: 0.379466712474823
Epoch 2100, training loss: 83.53142547607422 = 0.3051110506057739 + 10.0 * 8.322630882263184
Epoch 2100, val loss: 0.379283607006073
Epoch 2110, training loss: 83.52596282958984 = 0.3044455349445343 + 10.0 * 8.322152137756348
Epoch 2110, val loss: 0.3790905773639679
Epoch 2120, training loss: 83.530029296875 = 0.3037835657596588 + 10.0 * 8.322624206542969
Epoch 2120, val loss: 0.37893185019493103
Epoch 2130, training loss: 83.55043029785156 = 0.30312180519104004 + 10.0 * 8.32473087310791
Epoch 2130, val loss: 0.3787918984889984
Epoch 2140, training loss: 83.52055358886719 = 0.30247044563293457 + 10.0 * 8.321808815002441
Epoch 2140, val loss: 0.37854450941085815
Epoch 2150, training loss: 83.51669311523438 = 0.3018306791782379 + 10.0 * 8.32148551940918
Epoch 2150, val loss: 0.37840986251831055
Epoch 2160, training loss: 83.51332092285156 = 0.30119025707244873 + 10.0 * 8.321212768554688
Epoch 2160, val loss: 0.3782380223274231
Epoch 2170, training loss: 83.50977325439453 = 0.3005521893501282 + 10.0 * 8.320921897888184
Epoch 2170, val loss: 0.3780897855758667
Epoch 2180, training loss: 83.50753784179688 = 0.2999154031276703 + 10.0 * 8.320761680603027
Epoch 2180, val loss: 0.37794366478919983
Epoch 2190, training loss: 83.52277374267578 = 0.29927754402160645 + 10.0 * 8.322349548339844
Epoch 2190, val loss: 0.3778160810470581
Epoch 2200, training loss: 83.5031509399414 = 0.2986363470554352 + 10.0 * 8.320451736450195
Epoch 2200, val loss: 0.37764865159988403
Epoch 2210, training loss: 83.50077056884766 = 0.29801300168037415 + 10.0 * 8.320276260375977
Epoch 2210, val loss: 0.3775435984134674
Epoch 2220, training loss: 83.5002212524414 = 0.29739251732826233 + 10.0 * 8.320282936096191
Epoch 2220, val loss: 0.3773854672908783
Epoch 2230, training loss: 83.56253814697266 = 0.296766996383667 + 10.0 * 8.326577186584473
Epoch 2230, val loss: 0.3773305118083954
Epoch 2240, training loss: 83.50289916992188 = 0.29615548253059387 + 10.0 * 8.320673942565918
Epoch 2240, val loss: 0.37714043259620667
Epoch 2250, training loss: 83.49425506591797 = 0.2955578565597534 + 10.0 * 8.319869995117188
Epoch 2250, val loss: 0.3770533800125122
Epoch 2260, training loss: 83.48652648925781 = 0.2949593663215637 + 10.0 * 8.319156646728516
Epoch 2260, val loss: 0.37690818309783936
Epoch 2270, training loss: 83.48442077636719 = 0.29435962438583374 + 10.0 * 8.319005966186523
Epoch 2270, val loss: 0.37682607769966125
Epoch 2280, training loss: 83.48152160644531 = 0.2937621772289276 + 10.0 * 8.31877613067627
Epoch 2280, val loss: 0.37669771909713745
Epoch 2290, training loss: 83.4795150756836 = 0.2931627929210663 + 10.0 * 8.318635940551758
Epoch 2290, val loss: 0.3765829801559448
Epoch 2300, training loss: 83.5130386352539 = 0.29255858063697815 + 10.0 * 8.32204818725586
Epoch 2300, val loss: 0.3764301836490631
Epoch 2310, training loss: 83.49869537353516 = 0.2919803261756897 + 10.0 * 8.320672035217285
Epoch 2310, val loss: 0.3764178156852722
Epoch 2320, training loss: 83.473876953125 = 0.2913895845413208 + 10.0 * 8.318248748779297
Epoch 2320, val loss: 0.37633875012397766
Epoch 2330, training loss: 83.47220611572266 = 0.2908081114292145 + 10.0 * 8.318140029907227
Epoch 2330, val loss: 0.37620624899864197
Epoch 2340, training loss: 83.46979522705078 = 0.290231853723526 + 10.0 * 8.317956924438477
Epoch 2340, val loss: 0.3761565387248993
Epoch 2350, training loss: 83.47665405273438 = 0.2896554470062256 + 10.0 * 8.318699836730957
Epoch 2350, val loss: 0.376089870929718
Epoch 2360, training loss: 83.47305297851562 = 0.28908219933509827 + 10.0 * 8.31839656829834
Epoch 2360, val loss: 0.3759908378124237
Epoch 2370, training loss: 83.46427917480469 = 0.2885148227214813 + 10.0 * 8.31757640838623
Epoch 2370, val loss: 0.3758823275566101
Epoch 2380, training loss: 83.45891571044922 = 0.287946492433548 + 10.0 * 8.317096710205078
Epoch 2380, val loss: 0.3758203387260437
Epoch 2390, training loss: 83.45903015136719 = 0.28738266229629517 + 10.0 * 8.317164421081543
Epoch 2390, val loss: 0.3757607340812683
Epoch 2400, training loss: 83.47052764892578 = 0.28682002425193787 + 10.0 * 8.318370819091797
Epoch 2400, val loss: 0.37569472193717957
Epoch 2410, training loss: 83.47608184814453 = 0.2862589657306671 + 10.0 * 8.318982124328613
Epoch 2410, val loss: 0.37559789419174194
Epoch 2420, training loss: 83.45662689208984 = 0.2857065796852112 + 10.0 * 8.317091941833496
Epoch 2420, val loss: 0.37558040022850037
Epoch 2430, training loss: 83.44928741455078 = 0.28515860438346863 + 10.0 * 8.316412925720215
Epoch 2430, val loss: 0.375476211309433
Epoch 2440, training loss: 83.44576263427734 = 0.28460970520973206 + 10.0 * 8.316115379333496
Epoch 2440, val loss: 0.3754577934741974
Epoch 2450, training loss: 83.44683074951172 = 0.28406286239624023 + 10.0 * 8.316276550292969
Epoch 2450, val loss: 0.37538570165634155
Epoch 2460, training loss: 83.4843978881836 = 0.28351932764053345 + 10.0 * 8.320088386535645
Epoch 2460, val loss: 0.3753892481327057
Epoch 2470, training loss: 83.44880676269531 = 0.28297850489616394 + 10.0 * 8.316582679748535
Epoch 2470, val loss: 0.3752492070198059
Epoch 2480, training loss: 83.43824005126953 = 0.28244513273239136 + 10.0 * 8.315579414367676
Epoch 2480, val loss: 0.37527844309806824
Epoch 2490, training loss: 83.43445587158203 = 0.2819107472896576 + 10.0 * 8.315254211425781
Epoch 2490, val loss: 0.37520352005958557
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8488077118214103
0.8631456929652975
=== training gcn model ===
Epoch 0, training loss: 106.92222595214844 = 1.1000908613204956 + 10.0 * 10.582213401794434
Epoch 0, val loss: 1.0990732908248901
Epoch 10, training loss: 106.9111328125 = 1.0952033996582031 + 10.0 * 10.581592559814453
Epoch 10, val loss: 1.0942659378051758
Epoch 20, training loss: 106.87659454345703 = 1.0901532173156738 + 10.0 * 10.578643798828125
Epoch 20, val loss: 1.0892752408981323
Epoch 30, training loss: 106.73374938964844 = 1.084808349609375 + 10.0 * 10.56489372253418
Epoch 30, val loss: 1.0839987993240356
Epoch 40, training loss: 106.19100952148438 = 1.0790519714355469 + 10.0 * 10.511195182800293
Epoch 40, val loss: 1.0782991647720337
Epoch 50, training loss: 104.50984954833984 = 1.0729562044143677 + 10.0 * 10.34368896484375
Epoch 50, val loss: 1.072309136390686
Epoch 60, training loss: 100.25013732910156 = 1.0666463375091553 + 10.0 * 9.918349266052246
Epoch 60, val loss: 1.066053867340088
Epoch 70, training loss: 95.10098266601562 = 1.0594987869262695 + 10.0 * 9.40414810180664
Epoch 70, val loss: 1.0590306520462036
Epoch 80, training loss: 93.75712585449219 = 1.0532031059265137 + 10.0 * 9.270392417907715
Epoch 80, val loss: 1.0529106855392456
Epoch 90, training loss: 93.02123260498047 = 1.0482566356658936 + 10.0 * 9.197298049926758
Epoch 90, val loss: 1.048078179359436
Epoch 100, training loss: 92.82797241210938 = 1.04433012008667 + 10.0 * 9.178364753723145
Epoch 100, val loss: 1.0442794561386108
Epoch 110, training loss: 92.58492279052734 = 1.0414276123046875 + 10.0 * 9.154349327087402
Epoch 110, val loss: 1.041542410850525
Epoch 120, training loss: 92.2772445678711 = 1.0395679473876953 + 10.0 * 9.123767852783203
Epoch 120, val loss: 1.0398465394973755
Epoch 130, training loss: 91.81040954589844 = 1.038552165031433 + 10.0 * 9.07718563079834
Epoch 130, val loss: 1.0389987230300903
Epoch 140, training loss: 91.0289077758789 = 1.0382511615753174 + 10.0 * 8.999065399169922
Epoch 140, val loss: 1.0388422012329102
Epoch 150, training loss: 89.97440338134766 = 1.0383816957473755 + 10.0 * 8.89360237121582
Epoch 150, val loss: 1.0390849113464355
Epoch 160, training loss: 89.16230773925781 = 1.0385141372680664 + 10.0 * 8.812379837036133
Epoch 160, val loss: 1.039245843887329
Epoch 170, training loss: 88.69773864746094 = 1.038024663925171 + 10.0 * 8.765971183776855
Epoch 170, val loss: 1.038773536682129
Epoch 180, training loss: 88.32897186279297 = 1.0371901988983154 + 10.0 * 8.729177474975586
Epoch 180, val loss: 1.0380109548568726
Epoch 190, training loss: 88.07311248779297 = 1.0366286039352417 + 10.0 * 8.703648567199707
Epoch 190, val loss: 1.037473440170288
Epoch 200, training loss: 87.91143798828125 = 1.0360208749771118 + 10.0 * 8.687541961669922
Epoch 200, val loss: 1.0368345975875854
Epoch 210, training loss: 87.72119140625 = 1.035286545753479 + 10.0 * 8.668590545654297
Epoch 210, val loss: 1.0361192226409912
Epoch 220, training loss: 87.49877166748047 = 1.0347583293914795 + 10.0 * 8.646401405334473
Epoch 220, val loss: 1.0356431007385254
Epoch 230, training loss: 87.27490997314453 = 1.0344363451004028 + 10.0 * 8.62404727935791
Epoch 230, val loss: 1.0353758335113525
Epoch 240, training loss: 87.05892944335938 = 1.0341956615447998 + 10.0 * 8.602473258972168
Epoch 240, val loss: 1.0351940393447876
Epoch 250, training loss: 86.87184143066406 = 1.033923864364624 + 10.0 * 8.583791732788086
Epoch 250, val loss: 1.0349757671356201
Epoch 260, training loss: 86.73048400878906 = 1.0334793329238892 + 10.0 * 8.569700241088867
Epoch 260, val loss: 1.0345683097839355
Epoch 270, training loss: 86.62323760986328 = 1.032810091972351 + 10.0 * 8.559042930603027
Epoch 270, val loss: 1.0339484214782715
Epoch 280, training loss: 86.50409698486328 = 1.0320782661437988 + 10.0 * 8.547201156616211
Epoch 280, val loss: 1.0332937240600586
Epoch 290, training loss: 86.3851089477539 = 1.0314158201217651 + 10.0 * 8.535368919372559
Epoch 290, val loss: 1.0326788425445557
Epoch 300, training loss: 86.27544403076172 = 1.0307965278625488 + 10.0 * 8.52446460723877
Epoch 300, val loss: 1.0320900678634644
Epoch 310, training loss: 86.17088317871094 = 1.0300629138946533 + 10.0 * 8.514081954956055
Epoch 310, val loss: 1.03139328956604
Epoch 320, training loss: 86.08761596679688 = 1.029314637184143 + 10.0 * 8.505830764770508
Epoch 320, val loss: 1.0306634902954102
Epoch 330, training loss: 86.0016860961914 = 1.0285123586654663 + 10.0 * 8.49731731414795
Epoch 330, val loss: 1.0298970937728882
Epoch 340, training loss: 85.93219757080078 = 1.0276429653167725 + 10.0 * 8.490455627441406
Epoch 340, val loss: 1.0290641784667969
Epoch 350, training loss: 85.8829345703125 = 1.0267012119293213 + 10.0 * 8.485623359680176
Epoch 350, val loss: 1.0281250476837158
Epoch 360, training loss: 85.80433654785156 = 1.0256754159927368 + 10.0 * 8.477866172790527
Epoch 360, val loss: 1.027156949043274
Epoch 370, training loss: 85.7437515258789 = 1.0246485471725464 + 10.0 * 8.47191047668457
Epoch 370, val loss: 1.026167392730713
Epoch 380, training loss: 85.7214584350586 = 1.0235669612884521 + 10.0 * 8.469789505004883
Epoch 380, val loss: 1.0251152515411377
Epoch 390, training loss: 85.63182067871094 = 1.0223582983016968 + 10.0 * 8.460946083068848
Epoch 390, val loss: 1.0239299535751343
Epoch 400, training loss: 85.56390380859375 = 1.0211296081542969 + 10.0 * 8.454277038574219
Epoch 400, val loss: 1.0227570533752441
Epoch 410, training loss: 85.50938415527344 = 1.01985764503479 + 10.0 * 8.448952674865723
Epoch 410, val loss: 1.0215226411819458
Epoch 420, training loss: 85.46529388427734 = 1.0184273719787598 + 10.0 * 8.444686889648438
Epoch 420, val loss: 1.020125150680542
Epoch 430, training loss: 85.41658782958984 = 1.0168728828430176 + 10.0 * 8.439970970153809
Epoch 430, val loss: 1.0186253786087036
Epoch 440, training loss: 85.36591339111328 = 1.0153111219406128 + 10.0 * 8.435060501098633
Epoch 440, val loss: 1.0171196460723877
Epoch 450, training loss: 85.31893920898438 = 1.0136581659317017 + 10.0 * 8.43052864074707
Epoch 450, val loss: 1.0155093669891357
Epoch 460, training loss: 85.27823638916016 = 1.0118863582611084 + 10.0 * 8.426634788513184
Epoch 460, val loss: 1.0137810707092285
Epoch 470, training loss: 85.24053955078125 = 1.0099924802780151 + 10.0 * 8.423054695129395
Epoch 470, val loss: 1.0119398832321167
Epoch 480, training loss: 85.212158203125 = 1.007968544960022 + 10.0 * 8.420419692993164
Epoch 480, val loss: 1.0099830627441406
Epoch 490, training loss: 85.18946075439453 = 1.005772352218628 + 10.0 * 8.418368339538574
Epoch 490, val loss: 1.0077929496765137
Epoch 500, training loss: 85.15528106689453 = 1.003469467163086 + 10.0 * 8.415181159973145
Epoch 500, val loss: 1.0055750608444214
Epoch 510, training loss: 85.12362670898438 = 1.0011283159255981 + 10.0 * 8.412249565124512
Epoch 510, val loss: 1.003310203552246
Epoch 520, training loss: 85.09796905517578 = 0.9986830353736877 + 10.0 * 8.409929275512695
Epoch 520, val loss: 1.0009400844573975
Epoch 530, training loss: 85.07550811767578 = 0.9960887432098389 + 10.0 * 8.407941818237305
Epoch 530, val loss: 0.9984189867973328
Epoch 540, training loss: 85.05352020263672 = 0.9933496713638306 + 10.0 * 8.406017303466797
Epoch 540, val loss: 0.995769739151001
Epoch 550, training loss: 85.03189849853516 = 0.9905045032501221 + 10.0 * 8.404139518737793
Epoch 550, val loss: 0.9930034875869751
Epoch 560, training loss: 85.04198455810547 = 0.9874796271324158 + 10.0 * 8.405450820922852
Epoch 560, val loss: 0.9900513887405396
Epoch 570, training loss: 84.99259185791016 = 0.9842682480812073 + 10.0 * 8.400832176208496
Epoch 570, val loss: 0.9869629144668579
Epoch 580, training loss: 84.97403717041016 = 0.9809791445732117 + 10.0 * 8.39930534362793
Epoch 580, val loss: 0.9837719202041626
Epoch 590, training loss: 84.9537124633789 = 0.9775295257568359 + 10.0 * 8.397618293762207
Epoch 590, val loss: 0.9804259538650513
Epoch 600, training loss: 84.93492889404297 = 0.9738902449607849 + 10.0 * 8.396103858947754
Epoch 600, val loss: 0.9768938422203064
Epoch 610, training loss: 84.91801452636719 = 0.9700485467910767 + 10.0 * 8.394796371459961
Epoch 610, val loss: 0.9731724858283997
Epoch 620, training loss: 84.98766326904297 = 0.9659527540206909 + 10.0 * 8.40217113494873
Epoch 620, val loss: 0.9692168235778809
Epoch 630, training loss: 84.9132080078125 = 0.9615232348442078 + 10.0 * 8.39516830444336
Epoch 630, val loss: 0.964907705783844
Epoch 640, training loss: 84.87496948242188 = 0.9570654034614563 + 10.0 * 8.391790390014648
Epoch 640, val loss: 0.9605793952941895
Epoch 650, training loss: 84.85621643066406 = 0.9524738192558289 + 10.0 * 8.390374183654785
Epoch 650, val loss: 0.9561247229576111
Epoch 660, training loss: 84.83982849121094 = 0.9476531147956848 + 10.0 * 8.389217376708984
Epoch 660, val loss: 0.9514567852020264
Epoch 670, training loss: 84.8230972290039 = 0.942567765712738 + 10.0 * 8.388052940368652
Epoch 670, val loss: 0.9465246796607971
Epoch 680, training loss: 84.80912017822266 = 0.9372089505195618 + 10.0 * 8.387190818786621
Epoch 680, val loss: 0.941325843334198
Epoch 690, training loss: 84.82749938964844 = 0.9315377473831177 + 10.0 * 8.389595985412598
Epoch 690, val loss: 0.9358034729957581
Epoch 700, training loss: 84.78118896484375 = 0.9255660176277161 + 10.0 * 8.3855619430542
Epoch 700, val loss: 0.9300494194030762
Epoch 710, training loss: 84.75981903076172 = 0.9194477200508118 + 10.0 * 8.384037017822266
Epoch 710, val loss: 0.9241320490837097
Epoch 720, training loss: 84.74210357666016 = 0.9130807518959045 + 10.0 * 8.382902145385742
Epoch 720, val loss: 0.9179731607437134
Epoch 730, training loss: 84.72492218017578 = 0.9064228534698486 + 10.0 * 8.38184928894043
Epoch 730, val loss: 0.9115192890167236
Epoch 740, training loss: 84.74213409423828 = 0.8994240760803223 + 10.0 * 8.384271621704102
Epoch 740, val loss: 0.904728353023529
Epoch 750, training loss: 84.69831848144531 = 0.8920302987098694 + 10.0 * 8.38062858581543
Epoch 750, val loss: 0.8975851535797119
Epoch 760, training loss: 84.67091369628906 = 0.884435772895813 + 10.0 * 8.378647804260254
Epoch 760, val loss: 0.8902671337127686
Epoch 770, training loss: 84.65823364257812 = 0.8766157627105713 + 10.0 * 8.378161430358887
Epoch 770, val loss: 0.8827265501022339
Epoch 780, training loss: 84.6688232421875 = 0.868457555770874 + 10.0 * 8.380037307739258
Epoch 780, val loss: 0.8748452663421631
Epoch 790, training loss: 84.62806701660156 = 0.8599831461906433 + 10.0 * 8.376808166503906
Epoch 790, val loss: 0.8666494488716125
Epoch 800, training loss: 84.60246276855469 = 0.8512722253799438 + 10.0 * 8.37511920928955
Epoch 800, val loss: 0.85826176404953
Epoch 810, training loss: 84.57880401611328 = 0.8422755002975464 + 10.0 * 8.373652458190918
Epoch 810, val loss: 0.8495913743972778
Epoch 820, training loss: 84.56673431396484 = 0.8329455256462097 + 10.0 * 8.37337875366211
Epoch 820, val loss: 0.8405882120132446
Epoch 830, training loss: 84.55731964111328 = 0.8232214450836182 + 10.0 * 8.37341022491455
Epoch 830, val loss: 0.8312197923660278
Epoch 840, training loss: 84.5343246459961 = 0.8133013248443604 + 10.0 * 8.372102737426758
Epoch 840, val loss: 0.8216978311538696
Epoch 850, training loss: 84.50428771972656 = 0.8032722473144531 + 10.0 * 8.370100975036621
Epoch 850, val loss: 0.8121197819709778
Epoch 860, training loss: 84.4858627319336 = 0.7930644750595093 + 10.0 * 8.369279861450195
Epoch 860, val loss: 0.8023506999015808
Epoch 870, training loss: 84.49247741699219 = 0.7825984954833984 + 10.0 * 8.370987892150879
Epoch 870, val loss: 0.7923522591590881
Epoch 880, training loss: 84.47149658203125 = 0.7718095183372498 + 10.0 * 8.36996841430664
Epoch 880, val loss: 0.7820158004760742
Epoch 890, training loss: 84.43251037597656 = 0.7610302567481995 + 10.0 * 8.367147445678711
Epoch 890, val loss: 0.7717477083206177
Epoch 900, training loss: 84.4140625 = 0.7502834796905518 + 10.0 * 8.366377830505371
Epoch 900, val loss: 0.7615280151367188
Epoch 910, training loss: 84.39160919189453 = 0.7394286394119263 + 10.0 * 8.365218162536621
Epoch 910, val loss: 0.7512096166610718
Epoch 920, training loss: 84.37293243408203 = 0.7284172177314758 + 10.0 * 8.36445140838623
Epoch 920, val loss: 0.740748405456543
Epoch 930, training loss: 84.35421752929688 = 0.7172715067863464 + 10.0 * 8.36369514465332
Epoch 930, val loss: 0.7301790714263916
Epoch 940, training loss: 84.39813232421875 = 0.7060080766677856 + 10.0 * 8.36921215057373
Epoch 940, val loss: 0.7195243835449219
Epoch 950, training loss: 84.34013366699219 = 0.6946740746498108 + 10.0 * 8.364545822143555
Epoch 950, val loss: 0.7088135480880737
Epoch 960, training loss: 84.30952453613281 = 0.6835892796516418 + 10.0 * 8.362593650817871
Epoch 960, val loss: 0.6983692049980164
Epoch 970, training loss: 84.28518676757812 = 0.6726511120796204 + 10.0 * 8.36125373840332
Epoch 970, val loss: 0.6880921721458435
Epoch 980, training loss: 84.26571655273438 = 0.6617789268493652 + 10.0 * 8.360393524169922
Epoch 980, val loss: 0.6778844594955444
Epoch 990, training loss: 84.2475814819336 = 0.6509419679641724 + 10.0 * 8.359663963317871
Epoch 990, val loss: 0.6677350401878357
Epoch 1000, training loss: 84.27973937988281 = 0.6401816606521606 + 10.0 * 8.3639554977417
Epoch 1000, val loss: 0.657697319984436
Epoch 1010, training loss: 84.21981048583984 = 0.6295362114906311 + 10.0 * 8.359026908874512
Epoch 1010, val loss: 0.6477128863334656
Epoch 1020, training loss: 84.20418548583984 = 0.6192206740379333 + 10.0 * 8.35849666595459
Epoch 1020, val loss: 0.6380810141563416
Epoch 1030, training loss: 84.18235778808594 = 0.60911625623703 + 10.0 * 8.357324600219727
Epoch 1030, val loss: 0.6286697387695312
Epoch 1040, training loss: 84.18617248535156 = 0.599183976650238 + 10.0 * 8.358698844909668
Epoch 1040, val loss: 0.6194208264350891
Epoch 1050, training loss: 84.16309356689453 = 0.589340090751648 + 10.0 * 8.357375144958496
Epoch 1050, val loss: 0.6103360652923584
Epoch 1060, training loss: 84.14364624023438 = 0.5798506140708923 + 10.0 * 8.356379508972168
Epoch 1060, val loss: 0.6014958024024963
Epoch 1070, training loss: 84.12269592285156 = 0.57061767578125 + 10.0 * 8.355207443237305
Epoch 1070, val loss: 0.5929974317550659
Epoch 1080, training loss: 84.10546112060547 = 0.561606228351593 + 10.0 * 8.354385375976562
Epoch 1080, val loss: 0.5846522450447083
Epoch 1090, training loss: 84.09190368652344 = 0.5527787804603577 + 10.0 * 8.353912353515625
Epoch 1090, val loss: 0.5765291452407837
Epoch 1100, training loss: 84.13851165771484 = 0.5441213846206665 + 10.0 * 8.3594388961792
Epoch 1100, val loss: 0.5685462951660156
Epoch 1110, training loss: 84.0914306640625 = 0.53566974401474 + 10.0 * 8.355576515197754
Epoch 1110, val loss: 0.5608280897140503
Epoch 1120, training loss: 84.05372619628906 = 0.5276162624359131 + 10.0 * 8.35261058807373
Epoch 1120, val loss: 0.5534427762031555
Epoch 1130, training loss: 84.03717041015625 = 0.5198635458946228 + 10.0 * 8.351730346679688
Epoch 1130, val loss: 0.5463825464248657
Epoch 1140, training loss: 84.02435302734375 = 0.5123300552368164 + 10.0 * 8.351202011108398
Epoch 1140, val loss: 0.5394986271858215
Epoch 1150, training loss: 84.01215362548828 = 0.5050156116485596 + 10.0 * 8.350713729858398
Epoch 1150, val loss: 0.5328754782676697
Epoch 1160, training loss: 84.02958679199219 = 0.4979107081890106 + 10.0 * 8.353167533874512
Epoch 1160, val loss: 0.5264883637428284
Epoch 1170, training loss: 84.01056671142578 = 0.4910856783390045 + 10.0 * 8.351947784423828
Epoch 1170, val loss: 0.5202285647392273
Epoch 1180, training loss: 83.97785186767578 = 0.48454776406288147 + 10.0 * 8.349329948425293
Epoch 1180, val loss: 0.5143780708312988
Epoch 1190, training loss: 83.9669189453125 = 0.4783143103122711 + 10.0 * 8.348859786987305
Epoch 1190, val loss: 0.5087871551513672
Epoch 1200, training loss: 83.97148895263672 = 0.4723040759563446 + 10.0 * 8.349918365478516
Epoch 1200, val loss: 0.5033581852912903
Epoch 1210, training loss: 83.95550537109375 = 0.4664343595504761 + 10.0 * 8.348907470703125
Epoch 1210, val loss: 0.4981597363948822
Epoch 1220, training loss: 83.93543243408203 = 0.46088966727256775 + 10.0 * 8.347454071044922
Epoch 1220, val loss: 0.49323570728302
Epoch 1230, training loss: 83.9237289428711 = 0.45562440156936646 + 10.0 * 8.346810340881348
Epoch 1230, val loss: 0.4886164367198944
Epoch 1240, training loss: 83.91293334960938 = 0.450553297996521 + 10.0 * 8.346238136291504
Epoch 1240, val loss: 0.484112024307251
Epoch 1250, training loss: 83.9101791381836 = 0.44567662477493286 + 10.0 * 8.346449851989746
Epoch 1250, val loss: 0.47985026240348816
Epoch 1260, training loss: 83.90245056152344 = 0.4409855604171753 + 10.0 * 8.346146583557129
Epoch 1260, val loss: 0.4757894277572632
Epoch 1270, training loss: 83.89291381835938 = 0.4364958703517914 + 10.0 * 8.34564208984375
Epoch 1270, val loss: 0.47185125946998596
Epoch 1280, training loss: 83.912353515625 = 0.4322051405906677 + 10.0 * 8.348014831542969
Epoch 1280, val loss: 0.4681806266307831
Epoch 1290, training loss: 83.87459564208984 = 0.4281407296657562 + 10.0 * 8.344645500183105
Epoch 1290, val loss: 0.4646371901035309
Epoch 1300, training loss: 83.85835266113281 = 0.42427775263786316 + 10.0 * 8.34340763092041
Epoch 1300, val loss: 0.46137428283691406
Epoch 1310, training loss: 83.85181427001953 = 0.4205673933029175 + 10.0 * 8.343124389648438
Epoch 1310, val loss: 0.4582099914550781
Epoch 1320, training loss: 83.8433837890625 = 0.4169895350933075 + 10.0 * 8.342638969421387
Epoch 1320, val loss: 0.4551927447319031
Epoch 1330, training loss: 83.86154174804688 = 0.4135434627532959 + 10.0 * 8.344799995422363
Epoch 1330, val loss: 0.45232340693473816
Epoch 1340, training loss: 83.8303451538086 = 0.41020676493644714 + 10.0 * 8.342013359069824
Epoch 1340, val loss: 0.4495187997817993
Epoch 1350, training loss: 83.8182373046875 = 0.40704816579818726 + 10.0 * 8.341118812561035
Epoch 1350, val loss: 0.4469110071659088
Epoch 1360, training loss: 83.81098937988281 = 0.4040246307849884 + 10.0 * 8.340696334838867
Epoch 1360, val loss: 0.444430410861969
Epoch 1370, training loss: 83.81004333496094 = 0.40111401677131653 + 10.0 * 8.340892791748047
Epoch 1370, val loss: 0.4420345723628998
Epoch 1380, training loss: 83.83026885986328 = 0.39827960729599 + 10.0 * 8.343198776245117
Epoch 1380, val loss: 0.4397917091846466
Epoch 1390, training loss: 83.79998016357422 = 0.39559003710746765 + 10.0 * 8.340438842773438
Epoch 1390, val loss: 0.43760019540786743
Epoch 1400, training loss: 83.7850341796875 = 0.3930201530456543 + 10.0 * 8.339200973510742
Epoch 1400, val loss: 0.43556296825408936
Epoch 1410, training loss: 83.7780532836914 = 0.3905535042285919 + 10.0 * 8.338749885559082
Epoch 1410, val loss: 0.4336085915565491
Epoch 1420, training loss: 83.78345489501953 = 0.38816016912460327 + 10.0 * 8.339529037475586
Epoch 1420, val loss: 0.4317270517349243
Epoch 1430, training loss: 83.78678131103516 = 0.3858201503753662 + 10.0 * 8.340096473693848
Epoch 1430, val loss: 0.42994099855422974
Epoch 1440, training loss: 83.7673568725586 = 0.3835998773574829 + 10.0 * 8.33837604522705
Epoch 1440, val loss: 0.42822709679603577
Epoch 1450, training loss: 83.7555923461914 = 0.3814622461795807 + 10.0 * 8.33741283416748
Epoch 1450, val loss: 0.4265918433666229
Epoch 1460, training loss: 83.74742889404297 = 0.3793964982032776 + 10.0 * 8.336803436279297
Epoch 1460, val loss: 0.4250205457210541
Epoch 1470, training loss: 83.74604797363281 = 0.3773926794528961 + 10.0 * 8.336865425109863
Epoch 1470, val loss: 0.42350757122039795
Epoch 1480, training loss: 83.78085327148438 = 0.37543800473213196 + 10.0 * 8.34054183959961
Epoch 1480, val loss: 0.4220432639122009
Epoch 1490, training loss: 83.73316192626953 = 0.373555451631546 + 10.0 * 8.335960388183594
Epoch 1490, val loss: 0.42071184515953064
Epoch 1500, training loss: 83.7315902709961 = 0.3717608153820038 + 10.0 * 8.335983276367188
Epoch 1500, val loss: 0.419455885887146
Epoch 1510, training loss: 83.72177124023438 = 0.3700357675552368 + 10.0 * 8.335173606872559
Epoch 1510, val loss: 0.41819554567337036
Epoch 1520, training loss: 83.71611785888672 = 0.368354469537735 + 10.0 * 8.334775924682617
Epoch 1520, val loss: 0.41698628664016724
Epoch 1530, training loss: 83.71826171875 = 0.3667057454586029 + 10.0 * 8.335155487060547
Epoch 1530, val loss: 0.4158364534378052
Epoch 1540, training loss: 83.74481964111328 = 0.3650742471218109 + 10.0 * 8.337974548339844
Epoch 1540, val loss: 0.41471153497695923
Epoch 1550, training loss: 83.71061706542969 = 0.36352506279945374 + 10.0 * 8.334709167480469
Epoch 1550, val loss: 0.4136582612991333
Epoch 1560, training loss: 83.69705963134766 = 0.3620203137397766 + 10.0 * 8.333503723144531
Epoch 1560, val loss: 0.4126403033733368
Epoch 1570, training loss: 83.69352722167969 = 0.36056140065193176 + 10.0 * 8.333295822143555
Epoch 1570, val loss: 0.411657452583313
Epoch 1580, training loss: 83.73458862304688 = 0.35912662744522095 + 10.0 * 8.337546348571777
Epoch 1580, val loss: 0.4107140302658081
Epoch 1590, training loss: 83.69633483886719 = 0.3577236831188202 + 10.0 * 8.333860397338867
Epoch 1590, val loss: 0.40977543592453003
Epoch 1600, training loss: 83.68355560302734 = 0.3563845753669739 + 10.0 * 8.332716941833496
Epoch 1600, val loss: 0.40893426537513733
Epoch 1610, training loss: 83.67566680908203 = 0.3550633490085602 + 10.0 * 8.332059860229492
Epoch 1610, val loss: 0.4080657362937927
Epoch 1620, training loss: 83.67393493652344 = 0.35376837849617004 + 10.0 * 8.332016944885254
Epoch 1620, val loss: 0.4072609543800354
Epoch 1630, training loss: 83.7003402709961 = 0.35248470306396484 + 10.0 * 8.334785461425781
Epoch 1630, val loss: 0.4064748287200928
Epoch 1640, training loss: 83.66922760009766 = 0.3512350022792816 + 10.0 * 8.331799507141113
Epoch 1640, val loss: 0.40568608045578003
Epoch 1650, training loss: 83.65685272216797 = 0.3500274121761322 + 10.0 * 8.330682754516602
Epoch 1650, val loss: 0.4049229323863983
Epoch 1660, training loss: 83.6559066772461 = 0.3488423824310303 + 10.0 * 8.330706596374512
Epoch 1660, val loss: 0.4042223393917084
Epoch 1670, training loss: 83.68354034423828 = 0.3476708233356476 + 10.0 * 8.333586692810059
Epoch 1670, val loss: 0.4034992754459381
Epoch 1680, training loss: 83.643798828125 = 0.3465195596218109 + 10.0 * 8.329728126525879
Epoch 1680, val loss: 0.40284696221351624
Epoch 1690, training loss: 83.642578125 = 0.3454011082649231 + 10.0 * 8.329717636108398
Epoch 1690, val loss: 0.4021969139575958
Epoch 1700, training loss: 83.63992309570312 = 0.3443029820919037 + 10.0 * 8.329562187194824
Epoch 1700, val loss: 0.40155017375946045
Epoch 1710, training loss: 83.63224029541016 = 0.3432222902774811 + 10.0 * 8.328901290893555
Epoch 1710, val loss: 0.4009469449520111
Epoch 1720, training loss: 83.63325500488281 = 0.3421555757522583 + 10.0 * 8.329110145568848
Epoch 1720, val loss: 0.40037646889686584
Epoch 1730, training loss: 83.64888763427734 = 0.3410930037498474 + 10.0 * 8.330779075622559
Epoch 1730, val loss: 0.3997955024242401
Epoch 1740, training loss: 83.64891052246094 = 0.340058833360672 + 10.0 * 8.33088493347168
Epoch 1740, val loss: 0.3991570472717285
Epoch 1750, training loss: 83.6234130859375 = 0.33904826641082764 + 10.0 * 8.328435897827148
Epoch 1750, val loss: 0.3986491560935974
Epoch 1760, training loss: 83.62079620361328 = 0.3380582332611084 + 10.0 * 8.32827377319336
Epoch 1760, val loss: 0.39815282821655273
Epoch 1770, training loss: 83.620849609375 = 0.3370826542377472 + 10.0 * 8.328376770019531
Epoch 1770, val loss: 0.39761966466903687
Epoch 1780, training loss: 83.62915802001953 = 0.3361159563064575 + 10.0 * 8.329304695129395
Epoch 1780, val loss: 0.39710181951522827
Epoch 1790, training loss: 83.60417175292969 = 0.33517542481422424 + 10.0 * 8.326899528503418
Epoch 1790, val loss: 0.3966428339481354
Epoch 1800, training loss: 83.60372924804688 = 0.33423832058906555 + 10.0 * 8.326949119567871
Epoch 1800, val loss: 0.3961679935455322
Epoch 1810, training loss: 83.61164855957031 = 0.33331477642059326 + 10.0 * 8.32783317565918
Epoch 1810, val loss: 0.3957323431968689
Epoch 1820, training loss: 83.60099792480469 = 0.33240270614624023 + 10.0 * 8.326859474182129
Epoch 1820, val loss: 0.3952915072441101
Epoch 1830, training loss: 83.59178924560547 = 0.3315059542655945 + 10.0 * 8.326028823852539
Epoch 1830, val loss: 0.39485180377960205
Epoch 1840, training loss: 83.61670684814453 = 0.33061373233795166 + 10.0 * 8.328609466552734
Epoch 1840, val loss: 0.3944700360298157
Epoch 1850, training loss: 83.60833740234375 = 0.3297407329082489 + 10.0 * 8.327859878540039
Epoch 1850, val loss: 0.39401671290397644
Epoch 1860, training loss: 83.5963134765625 = 0.32888224720954895 + 10.0 * 8.326743125915527
Epoch 1860, val loss: 0.393624871969223
Epoch 1870, training loss: 83.57709503173828 = 0.3280399441719055 + 10.0 * 8.324905395507812
Epoch 1870, val loss: 0.3932552933692932
Epoch 1880, training loss: 83.57463836669922 = 0.3272053599357605 + 10.0 * 8.324743270874023
Epoch 1880, val loss: 0.39288726449012756
Epoch 1890, training loss: 83.5802001953125 = 0.3263744115829468 + 10.0 * 8.325382232666016
Epoch 1890, val loss: 0.39249172806739807
Epoch 1900, training loss: 83.59203338623047 = 0.32554230093955994 + 10.0 * 8.32664966583252
Epoch 1900, val loss: 0.392141193151474
Epoch 1910, training loss: 83.57109832763672 = 0.32472923398017883 + 10.0 * 8.324636459350586
Epoch 1910, val loss: 0.39181941747665405
Epoch 1920, training loss: 83.55860900878906 = 0.323926717042923 + 10.0 * 8.323468208312988
Epoch 1920, val loss: 0.3914742171764374
Epoch 1930, training loss: 83.5591049194336 = 0.3231298625469208 + 10.0 * 8.32359790802002
Epoch 1930, val loss: 0.3911232054233551
Epoch 1940, training loss: 83.5679931640625 = 0.3223356306552887 + 10.0 * 8.324565887451172
Epoch 1940, val loss: 0.3907848596572876
Epoch 1950, training loss: 83.57173919677734 = 0.3215393126010895 + 10.0 * 8.325019836425781
Epoch 1950, val loss: 0.3905114531517029
Epoch 1960, training loss: 83.5594253540039 = 0.3207660913467407 + 10.0 * 8.32386589050293
Epoch 1960, val loss: 0.39018645882606506
Epoch 1970, training loss: 83.54615783691406 = 0.3200024664402008 + 10.0 * 8.322615623474121
Epoch 1970, val loss: 0.38991791009902954
Epoch 1980, training loss: 83.54244232177734 = 0.3192433714866638 + 10.0 * 8.322319984436035
Epoch 1980, val loss: 0.3895868957042694
Epoch 1990, training loss: 83.54084777832031 = 0.31849291920661926 + 10.0 * 8.322235107421875
Epoch 1990, val loss: 0.38930585980415344
Epoch 2000, training loss: 83.57819366455078 = 0.31774193048477173 + 10.0 * 8.326045036315918
Epoch 2000, val loss: 0.38899391889572144
Epoch 2010, training loss: 83.54833984375 = 0.31699255108833313 + 10.0 * 8.323134422302246
Epoch 2010, val loss: 0.3888312578201294
Epoch 2020, training loss: 83.53913879394531 = 0.31626805663108826 + 10.0 * 8.322286605834961
Epoch 2020, val loss: 0.38848891854286194
Epoch 2030, training loss: 83.52869415283203 = 0.31554755568504333 + 10.0 * 8.321314811706543
Epoch 2030, val loss: 0.38828304409980774
Epoch 2040, training loss: 83.52361297607422 = 0.3148347735404968 + 10.0 * 8.320878028869629
Epoch 2040, val loss: 0.38801467418670654
Epoch 2050, training loss: 83.52247619628906 = 0.31412187218666077 + 10.0 * 8.32083511352539
Epoch 2050, val loss: 0.38778388500213623
Epoch 2060, training loss: 83.57781219482422 = 0.3134102523326874 + 10.0 * 8.326440811157227
Epoch 2060, val loss: 0.3875330984592438
Epoch 2070, training loss: 83.53692626953125 = 0.3127056956291199 + 10.0 * 8.32242202758789
Epoch 2070, val loss: 0.3872981667518616
Epoch 2080, training loss: 83.52154541015625 = 0.31201568245887756 + 10.0 * 8.320952415466309
Epoch 2080, val loss: 0.3871072232723236
Epoch 2090, training loss: 83.51103973388672 = 0.31132981181144714 + 10.0 * 8.319971084594727
Epoch 2090, val loss: 0.386858731508255
Epoch 2100, training loss: 83.50763702392578 = 0.3106495440006256 + 10.0 * 8.31969928741455
Epoch 2100, val loss: 0.38664931058883667
Epoch 2110, training loss: 83.55174255371094 = 0.3099675476551056 + 10.0 * 8.324177742004395
Epoch 2110, val loss: 0.38644981384277344
Epoch 2120, training loss: 83.51618957519531 = 0.30928248167037964 + 10.0 * 8.320691108703613
Epoch 2120, val loss: 0.3862250745296478
Epoch 2130, training loss: 83.50900268554688 = 0.3086231052875519 + 10.0 * 8.320037841796875
Epoch 2130, val loss: 0.38605234026908875
Epoch 2140, training loss: 83.498046875 = 0.30796998739242554 + 10.0 * 8.319007873535156
Epoch 2140, val loss: 0.38584181666374207
Epoch 2150, training loss: 83.49436950683594 = 0.3073126971721649 + 10.0 * 8.318705558776855
Epoch 2150, val loss: 0.3856571912765503
Epoch 2160, training loss: 83.49108123779297 = 0.3066582977771759 + 10.0 * 8.318442344665527
Epoch 2160, val loss: 0.38546237349510193
Epoch 2170, training loss: 83.48839569091797 = 0.3060047924518585 + 10.0 * 8.318239212036133
Epoch 2170, val loss: 0.3852757215499878
Epoch 2180, training loss: 83.4977035522461 = 0.30535101890563965 + 10.0 * 8.319234848022461
Epoch 2180, val loss: 0.3851034939289093
Epoch 2190, training loss: 83.51023864746094 = 0.30468621850013733 + 10.0 * 8.320554733276367
Epoch 2190, val loss: 0.384907990694046
Epoch 2200, training loss: 83.49787139892578 = 0.30405646562576294 + 10.0 * 8.319381713867188
Epoch 2200, val loss: 0.3847774267196655
Epoch 2210, training loss: 83.48199462890625 = 0.30342918634414673 + 10.0 * 8.317856788635254
Epoch 2210, val loss: 0.38459140062332153
Epoch 2220, training loss: 83.47806549072266 = 0.3028012216091156 + 10.0 * 8.317525863647461
Epoch 2220, val loss: 0.38441306352615356
Epoch 2230, training loss: 83.47395324707031 = 0.3021731376647949 + 10.0 * 8.317178726196289
Epoch 2230, val loss: 0.3842635452747345
Epoch 2240, training loss: 83.47117614746094 = 0.30154848098754883 + 10.0 * 8.316962242126465
Epoch 2240, val loss: 0.3840947449207306
Epoch 2250, training loss: 83.47087860107422 = 0.30092379450798035 + 10.0 * 8.316995620727539
Epoch 2250, val loss: 0.3839135468006134
Epoch 2260, training loss: 83.49761199951172 = 0.3002984821796417 + 10.0 * 8.319730758666992
Epoch 2260, val loss: 0.38370445370674133
Epoch 2270, training loss: 83.50617218017578 = 0.2996792197227478 + 10.0 * 8.320649147033691
Epoch 2270, val loss: 0.38369497656822205
Epoch 2280, training loss: 83.4679183959961 = 0.2990663945674896 + 10.0 * 8.316884994506836
Epoch 2280, val loss: 0.3835003077983856
Epoch 2290, training loss: 83.4639892578125 = 0.29845985770225525 + 10.0 * 8.316553115844727
Epoch 2290, val loss: 0.3833732008934021
Epoch 2300, training loss: 83.46125030517578 = 0.2978631556034088 + 10.0 * 8.316338539123535
Epoch 2300, val loss: 0.3832578957080841
Epoch 2310, training loss: 83.45523071289062 = 0.29726442694664 + 10.0 * 8.315796852111816
Epoch 2310, val loss: 0.38310548663139343
Epoch 2320, training loss: 83.45598602294922 = 0.2966634929180145 + 10.0 * 8.315932273864746
Epoch 2320, val loss: 0.38299238681793213
Epoch 2330, training loss: 83.48331451416016 = 0.29606354236602783 + 10.0 * 8.318724632263184
Epoch 2330, val loss: 0.3828620910644531
Epoch 2340, training loss: 83.47905731201172 = 0.2954730987548828 + 10.0 * 8.318358421325684
Epoch 2340, val loss: 0.38273030519485474
Epoch 2350, training loss: 83.45072174072266 = 0.29488563537597656 + 10.0 * 8.315584182739258
Epoch 2350, val loss: 0.38264310359954834
Epoch 2360, training loss: 83.44979095458984 = 0.2943030297756195 + 10.0 * 8.31554889678955
Epoch 2360, val loss: 0.3825199007987976
Epoch 2370, training loss: 83.44405364990234 = 0.29372283816337585 + 10.0 * 8.315032958984375
Epoch 2370, val loss: 0.38241246342658997
Epoch 2380, training loss: 83.44325256347656 = 0.2931439280509949 + 10.0 * 8.315011024475098
Epoch 2380, val loss: 0.382302463054657
Epoch 2390, training loss: 83.47240447998047 = 0.29256245493888855 + 10.0 * 8.317983627319336
Epoch 2390, val loss: 0.3822081387042999
Epoch 2400, training loss: 83.43792724609375 = 0.29198649525642395 + 10.0 * 8.314594268798828
Epoch 2400, val loss: 0.3820957541465759
Epoch 2410, training loss: 83.43733215332031 = 0.29141518473625183 + 10.0 * 8.314592361450195
Epoch 2410, val loss: 0.3820066452026367
Epoch 2420, training loss: 83.43462371826172 = 0.29084697365760803 + 10.0 * 8.314377784729004
Epoch 2420, val loss: 0.38189196586608887
Epoch 2430, training loss: 83.42900848388672 = 0.29027849435806274 + 10.0 * 8.313873291015625
Epoch 2430, val loss: 0.3817967474460602
Epoch 2440, training loss: 83.43972778320312 = 0.2897104322910309 + 10.0 * 8.315001487731934
Epoch 2440, val loss: 0.38169077038764954
Epoch 2450, training loss: 83.44912719726562 = 0.2891411781311035 + 10.0 * 8.315999031066895
Epoch 2450, val loss: 0.3816261291503906
Epoch 2460, training loss: 83.43486785888672 = 0.2885783612728119 + 10.0 * 8.314628601074219
Epoch 2460, val loss: 0.38154834508895874
Epoch 2470, training loss: 83.42374420166016 = 0.2880273163318634 + 10.0 * 8.31357192993164
Epoch 2470, val loss: 0.3814481496810913
Epoch 2480, training loss: 83.4201889038086 = 0.28747260570526123 + 10.0 * 8.313271522521973
Epoch 2480, val loss: 0.38139599561691284
Epoch 2490, training loss: 83.42340850830078 = 0.28692054748535156 + 10.0 * 8.31364917755127
Epoch 2490, val loss: 0.3813209533691406
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8427194317605275
0.8655364775773383
The final CL Acc:0.84881, 0.00497, The final GNN Acc:0.86418, 0.00100
Begin epxeriment: noisy_level: 0.2 cont_weight: 10 epoch: der1:0.3 der2:0 dfr1:0.4 dfr2:0.4
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=2500, cl_num_layers=2, cl_num_proj_hidden=64, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=10.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.0, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=32, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, noisy_level=0.2, num_hidden=64, num_proj_hidden=64, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=40, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 88648])
add edge: torch.Size([2, 106326])
remove edge: torch.Size([2, 70666])
updated graph: torch.Size([2, 88344])
=== Noisy graph ===
/home/project-robust-contrastive/Robust-Contrastive-Learning/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 106.90886688232422 = 1.085740089416504 + 10.0 * 10.58231258392334
Epoch 0, val loss: 1.0862020254135132
Epoch 10, training loss: 106.90279388427734 = 1.0816997289657593 + 10.0 * 10.582109451293945
Epoch 10, val loss: 1.0822097063064575
Epoch 20, training loss: 106.88758850097656 = 1.0773855447769165 + 10.0 * 10.58102035522461
Epoch 20, val loss: 1.0779356956481934
Epoch 30, training loss: 106.82955169677734 = 1.0727967023849487 + 10.0 * 10.575675010681152
Epoch 30, val loss: 1.0733966827392578
Epoch 40, training loss: 106.61066436767578 = 1.0679516792297363 + 10.0 * 10.55427074432373
Epoch 40, val loss: 1.0685887336730957
Epoch 50, training loss: 105.991943359375 = 1.0628352165222168 + 10.0 * 10.492910385131836
Epoch 50, val loss: 1.0635102987289429
Epoch 60, training loss: 104.66181945800781 = 1.0579837560653687 + 10.0 * 10.360383987426758
Epoch 60, val loss: 1.0586689710617065
Epoch 70, training loss: 102.60481262207031 = 1.053181529045105 + 10.0 * 10.155162811279297
Epoch 70, val loss: 1.0536370277404785
Epoch 80, training loss: 101.34468078613281 = 1.0481842756271362 + 10.0 * 10.02964973449707
Epoch 80, val loss: 1.0484369993209839
Epoch 90, training loss: 99.45012664794922 = 1.0429961681365967 + 10.0 * 9.840712547302246
Epoch 90, val loss: 1.0431294441223145
Epoch 100, training loss: 97.39889526367188 = 1.0378209352493286 + 10.0 * 9.636107444763184
Epoch 100, val loss: 1.038108229637146
Epoch 110, training loss: 96.30072021484375 = 1.0332311391830444 + 10.0 * 9.526748657226562
Epoch 110, val loss: 1.0337388515472412
Epoch 120, training loss: 95.28355407714844 = 1.0283973217010498 + 10.0 * 9.425516128540039
Epoch 120, val loss: 1.0289392471313477
Epoch 130, training loss: 93.7999496459961 = 1.0226383209228516 + 10.0 * 9.277730941772461
Epoch 130, val loss: 1.0232232809066772
Epoch 140, training loss: 92.78101348876953 = 1.0177651643753052 + 10.0 * 9.176324844360352
Epoch 140, val loss: 1.018373966217041
Epoch 150, training loss: 92.02996063232422 = 1.0137382745742798 + 10.0 * 9.101621627807617
Epoch 150, val loss: 1.0143318176269531
Epoch 160, training loss: 91.400390625 = 1.0102595090866089 + 10.0 * 9.039012908935547
Epoch 160, val loss: 1.010767936706543
Epoch 170, training loss: 91.13176727294922 = 1.0058947801589966 + 10.0 * 9.01258659362793
Epoch 170, val loss: 1.0062530040740967
Epoch 180, training loss: 90.74752044677734 = 1.0006203651428223 + 10.0 * 8.974690437316895
Epoch 180, val loss: 1.000969409942627
Epoch 190, training loss: 90.32122039794922 = 0.9954624772071838 + 10.0 * 8.932576179504395
Epoch 190, val loss: 0.9959108233451843
Epoch 200, training loss: 89.90826416015625 = 0.9906094074249268 + 10.0 * 8.891765594482422
Epoch 200, val loss: 0.9911667108535767
Epoch 210, training loss: 89.63533020019531 = 0.9856606125831604 + 10.0 * 8.86496639251709
Epoch 210, val loss: 0.9862717986106873
Epoch 220, training loss: 89.4208984375 = 0.9801317453384399 + 10.0 * 8.844076156616211
Epoch 220, val loss: 0.9807649254798889
Epoch 230, training loss: 89.18071746826172 = 0.9741204380989075 + 10.0 * 8.820659637451172
Epoch 230, val loss: 0.9748123288154602
Epoch 240, training loss: 88.9288558959961 = 0.9681009650230408 + 10.0 * 8.796075820922852
Epoch 240, val loss: 0.9688622951507568
Epoch 250, training loss: 88.72162628173828 = 0.9620288610458374 + 10.0 * 8.775959968566895
Epoch 250, val loss: 0.9628350138664246
Epoch 260, training loss: 88.5711441040039 = 0.955397367477417 + 10.0 * 8.761574745178223
Epoch 260, val loss: 0.9562328457832336
Epoch 270, training loss: 88.41197967529297 = 0.9481449127197266 + 10.0 * 8.746383666992188
Epoch 270, val loss: 0.9490231275558472
Epoch 280, training loss: 88.25609588623047 = 0.9404542446136475 + 10.0 * 8.73156452178955
Epoch 280, val loss: 0.9414088726043701
Epoch 290, training loss: 88.11576080322266 = 0.9324958920478821 + 10.0 * 8.718326568603516
Epoch 290, val loss: 0.9335408806800842
Epoch 300, training loss: 88.0005874633789 = 0.9242757558822632 + 10.0 * 8.70763111114502
Epoch 300, val loss: 0.9254337549209595
Epoch 310, training loss: 87.9169921875 = 0.9156639575958252 + 10.0 * 8.700132369995117
Epoch 310, val loss: 0.9169328808784485
Epoch 320, training loss: 87.82372283935547 = 0.9066857099533081 + 10.0 * 8.691703796386719
Epoch 320, val loss: 0.9080828428268433
Epoch 330, training loss: 87.7338638305664 = 0.8975058197975159 + 10.0 * 8.683635711669922
Epoch 330, val loss: 0.8990679383277893
Epoch 340, training loss: 87.63711547851562 = 0.8882302641868591 + 10.0 * 8.674888610839844
Epoch 340, val loss: 0.8900066614151001
Epoch 350, training loss: 87.56353759765625 = 0.8789509534835815 + 10.0 * 8.668458938598633
Epoch 350, val loss: 0.8809354901313782
Epoch 360, training loss: 87.42760467529297 = 0.8696351051330566 + 10.0 * 8.655797004699707
Epoch 360, val loss: 0.8718838691711426
Epoch 370, training loss: 87.31612396240234 = 0.860325813293457 + 10.0 * 8.64557933807373
Epoch 370, val loss: 0.8628575801849365
Epoch 380, training loss: 87.20443725585938 = 0.8510637283325195 + 10.0 * 8.635336875915527
Epoch 380, val loss: 0.8538565039634705
Epoch 390, training loss: 87.09980010986328 = 0.8416228890419006 + 10.0 * 8.625818252563477
Epoch 390, val loss: 0.8447254300117493
Epoch 400, training loss: 87.05384063720703 = 0.8320285081863403 + 10.0 * 8.622180938720703
Epoch 400, val loss: 0.8355264663696289
Epoch 410, training loss: 86.93586730957031 = 0.822307825088501 + 10.0 * 8.611355781555176
Epoch 410, val loss: 0.8261708617210388
Epoch 420, training loss: 86.85511779785156 = 0.8126459121704102 + 10.0 * 8.604247093200684
Epoch 420, val loss: 0.8169082403182983
Epoch 430, training loss: 86.78687286376953 = 0.8030231595039368 + 10.0 * 8.598384857177734
Epoch 430, val loss: 0.8077757358551025
Epoch 440, training loss: 86.72098541259766 = 0.7934519052505493 + 10.0 * 8.592753410339355
Epoch 440, val loss: 0.798743486404419
Epoch 450, training loss: 86.65774536132812 = 0.7839153409004211 + 10.0 * 8.587383270263672
Epoch 450, val loss: 0.7897998690605164
Epoch 460, training loss: 86.60528564453125 = 0.7745259404182434 + 10.0 * 8.583075523376465
Epoch 460, val loss: 0.7809857130050659
Epoch 470, training loss: 86.535400390625 = 0.7653173804283142 + 10.0 * 8.577008247375488
Epoch 470, val loss: 0.7724364995956421
Epoch 480, training loss: 86.51249694824219 = 0.7563111782073975 + 10.0 * 8.575618743896484
Epoch 480, val loss: 0.7642107605934143
Epoch 490, training loss: 86.43380737304688 = 0.7474400401115417 + 10.0 * 8.568636894226074
Epoch 490, val loss: 0.7559486031532288
Epoch 500, training loss: 86.35822296142578 = 0.7388691306114197 + 10.0 * 8.561935424804688
Epoch 500, val loss: 0.7480790019035339
Epoch 510, training loss: 86.3072509765625 = 0.7305833697319031 + 10.0 * 8.557666778564453
Epoch 510, val loss: 0.7405933141708374
Epoch 520, training loss: 86.2522964477539 = 0.722482442855835 + 10.0 * 8.55298137664795
Epoch 520, val loss: 0.7333099246025085
Epoch 530, training loss: 86.20271301269531 = 0.7145559787750244 + 10.0 * 8.548815727233887
Epoch 530, val loss: 0.7262191772460938
Epoch 540, training loss: 86.15441131591797 = 0.7068277597427368 + 10.0 * 8.544758796691895
Epoch 540, val loss: 0.7193775773048401
Epoch 550, training loss: 86.10823822021484 = 0.6993190050125122 + 10.0 * 8.540891647338867
Epoch 550, val loss: 0.7127872705459595
Epoch 560, training loss: 86.07769012451172 = 0.6919801831245422 + 10.0 * 8.53857135772705
Epoch 560, val loss: 0.7064524292945862
Epoch 570, training loss: 86.04354095458984 = 0.6846942901611328 + 10.0 * 8.535884857177734
Epoch 570, val loss: 0.7002098560333252
Epoch 580, training loss: 85.98906707763672 = 0.6775232553482056 + 10.0 * 8.53115463256836
Epoch 580, val loss: 0.6940538287162781
Epoch 590, training loss: 85.9511947631836 = 0.6707231402397156 + 10.0 * 8.528047561645508
Epoch 590, val loss: 0.6883870363235474
Epoch 600, training loss: 85.91191101074219 = 0.664177417755127 + 10.0 * 8.524773597717285
Epoch 600, val loss: 0.682919442653656
Epoch 610, training loss: 85.90870666503906 = 0.6578565239906311 + 10.0 * 8.525084495544434
Epoch 610, val loss: 0.6777994632720947
Epoch 620, training loss: 85.8570556640625 = 0.6518851518630981 + 10.0 * 8.520517349243164
Epoch 620, val loss: 0.6727203130722046
Epoch 630, training loss: 85.81027221679688 = 0.646170973777771 + 10.0 * 8.516409873962402
Epoch 630, val loss: 0.6681009531021118
Epoch 640, training loss: 85.77568817138672 = 0.6407262086868286 + 10.0 * 8.513496398925781
Epoch 640, val loss: 0.6637815237045288
Epoch 650, training loss: 85.74441528320312 = 0.6355522871017456 + 10.0 * 8.510886192321777
Epoch 650, val loss: 0.659658670425415
Epoch 660, training loss: 85.7507095336914 = 0.6306334733963013 + 10.0 * 8.512007713317871
Epoch 660, val loss: 0.6557042002677917
Epoch 670, training loss: 85.70401000976562 = 0.6258370280265808 + 10.0 * 8.507817268371582
Epoch 670, val loss: 0.652073085308075
Epoch 680, training loss: 85.66978454589844 = 0.6213196516036987 + 10.0 * 8.504846572875977
Epoch 680, val loss: 0.648502767086029
Epoch 690, training loss: 85.64137268066406 = 0.6170399785041809 + 10.0 * 8.502432823181152
Epoch 690, val loss: 0.6452170610427856
Epoch 700, training loss: 85.61415100097656 = 0.612938642501831 + 10.0 * 8.500121116638184
Epoch 700, val loss: 0.6421322822570801
Epoch 710, training loss: 85.59693145751953 = 0.6090281009674072 + 10.0 * 8.49878978729248
Epoch 710, val loss: 0.6392050385475159
Epoch 720, training loss: 85.57482147216797 = 0.6052184104919434 + 10.0 * 8.496960639953613
Epoch 720, val loss: 0.6362900733947754
Epoch 730, training loss: 85.55431365966797 = 0.6015850305557251 + 10.0 * 8.495272636413574
Epoch 730, val loss: 0.633625864982605
Epoch 740, training loss: 85.52865600585938 = 0.5981529951095581 + 10.0 * 8.493050575256348
Epoch 740, val loss: 0.631061315536499
Epoch 750, training loss: 85.50741577148438 = 0.5948617458343506 + 10.0 * 8.491254806518555
Epoch 750, val loss: 0.6286700963973999
Epoch 760, training loss: 85.49541473388672 = 0.5916940569877625 + 10.0 * 8.490371704101562
Epoch 760, val loss: 0.62638258934021
Epoch 770, training loss: 85.47877502441406 = 0.5886310935020447 + 10.0 * 8.489014625549316
Epoch 770, val loss: 0.6242051720619202
Epoch 780, training loss: 85.45319366455078 = 0.5856826305389404 + 10.0 * 8.486750602722168
Epoch 780, val loss: 0.6220848560333252
Epoch 790, training loss: 85.43716430664062 = 0.5828463435173035 + 10.0 * 8.485431671142578
Epoch 790, val loss: 0.6200746893882751
Epoch 800, training loss: 85.481689453125 = 0.5801083445549011 + 10.0 * 8.490158081054688
Epoch 800, val loss: 0.6180342435836792
Epoch 810, training loss: 85.41445922851562 = 0.5774040222167969 + 10.0 * 8.483705520629883
Epoch 810, val loss: 0.6163521409034729
Epoch 820, training loss: 85.39076232910156 = 0.574847936630249 + 10.0 * 8.481592178344727
Epoch 820, val loss: 0.6145938038825989
Epoch 830, training loss: 85.37242126464844 = 0.5723885297775269 + 10.0 * 8.480003356933594
Epoch 830, val loss: 0.6129568219184875
Epoch 840, training loss: 85.35618591308594 = 0.5699924230575562 + 10.0 * 8.478619575500488
Epoch 840, val loss: 0.6113422513008118
Epoch 850, training loss: 85.34137725830078 = 0.5676632523536682 + 10.0 * 8.477371215820312
Epoch 850, val loss: 0.6098355054855347
Epoch 860, training loss: 85.33130645751953 = 0.565390408039093 + 10.0 * 8.476591110229492
Epoch 860, val loss: 0.6084060668945312
Epoch 870, training loss: 85.36309051513672 = 0.563138484954834 + 10.0 * 8.479994773864746
Epoch 870, val loss: 0.606929361820221
Epoch 880, training loss: 85.31887817382812 = 0.5609163641929626 + 10.0 * 8.475796699523926
Epoch 880, val loss: 0.6054543256759644
Epoch 890, training loss: 85.29605102539062 = 0.5587792992591858 + 10.0 * 8.473727226257324
Epoch 890, val loss: 0.6041010022163391
Epoch 900, training loss: 85.28012084960938 = 0.5567183494567871 + 10.0 * 8.47234058380127
Epoch 900, val loss: 0.6028465628623962
Epoch 910, training loss: 85.26686096191406 = 0.5546997785568237 + 10.0 * 8.471216201782227
Epoch 910, val loss: 0.6016054153442383
Epoch 920, training loss: 85.25393676757812 = 0.5527170896530151 + 10.0 * 8.470121383666992
Epoch 920, val loss: 0.6003958582878113
Epoch 930, training loss: 85.24352264404297 = 0.5507616996765137 + 10.0 * 8.469276428222656
Epoch 930, val loss: 0.5992069244384766
Epoch 940, training loss: 85.29145050048828 = 0.5488256812095642 + 10.0 * 8.474262237548828
Epoch 940, val loss: 0.5979407429695129
Epoch 950, training loss: 85.24568939208984 = 0.5468524098396301 + 10.0 * 8.469883918762207
Epoch 950, val loss: 0.5969142913818359
Epoch 960, training loss: 85.22531127929688 = 0.5449486374855042 + 10.0 * 8.468035697937012
Epoch 960, val loss: 0.5956469774246216
Epoch 970, training loss: 85.2000961303711 = 0.5431036949157715 + 10.0 * 8.465699195861816
Epoch 970, val loss: 0.5946549773216248
Epoch 980, training loss: 85.18706512451172 = 0.5413063168525696 + 10.0 * 8.46457576751709
Epoch 980, val loss: 0.5936078429222107
Epoch 990, training loss: 85.17595672607422 = 0.5395254492759705 + 10.0 * 8.463643074035645
Epoch 990, val loss: 0.5925918221473694
Epoch 1000, training loss: 85.16393280029297 = 0.5377575755119324 + 10.0 * 8.462617874145508
Epoch 1000, val loss: 0.5915616750717163
Epoch 1010, training loss: 85.15421295166016 = 0.5359921455383301 + 10.0 * 8.461821556091309
Epoch 1010, val loss: 0.5905287861824036
Epoch 1020, training loss: 85.24787902832031 = 0.534223735332489 + 10.0 * 8.471364974975586
Epoch 1020, val loss: 0.5893457531929016
Epoch 1030, training loss: 85.1346664428711 = 0.5323889851570129 + 10.0 * 8.460227966308594
Epoch 1030, val loss: 0.5884587168693542
Epoch 1040, training loss: 85.12226104736328 = 0.5306318998336792 + 10.0 * 8.459162712097168
Epoch 1040, val loss: 0.5874971151351929
Epoch 1050, training loss: 85.11128234863281 = 0.528910219669342 + 10.0 * 8.458237648010254
Epoch 1050, val loss: 0.5865345001220703
Epoch 1060, training loss: 85.09522247314453 = 0.5271905660629272 + 10.0 * 8.456803321838379
Epoch 1060, val loss: 0.5855691432952881
Epoch 1070, training loss: 85.08409118652344 = 0.5254614949226379 + 10.0 * 8.455862998962402
Epoch 1070, val loss: 0.5846033692359924
Epoch 1080, training loss: 85.07209777832031 = 0.52372145652771 + 10.0 * 8.454837799072266
Epoch 1080, val loss: 0.5836185216903687
Epoch 1090, training loss: 85.0760498046875 = 0.5219735503196716 + 10.0 * 8.455408096313477
Epoch 1090, val loss: 0.5825215578079224
Epoch 1100, training loss: 85.08924865722656 = 0.5201692581176758 + 10.0 * 8.456907272338867
Epoch 1100, val loss: 0.5816872119903564
Epoch 1110, training loss: 85.05471801757812 = 0.5183653235435486 + 10.0 * 8.453635215759277
Epoch 1110, val loss: 0.5805611610412598
Epoch 1120, training loss: 85.03174591064453 = 0.5165935754776001 + 10.0 * 8.451515197753906
Epoch 1120, val loss: 0.5795762538909912
Epoch 1130, training loss: 85.01689910888672 = 0.5148261189460754 + 10.0 * 8.450207710266113
Epoch 1130, val loss: 0.5785257816314697
Epoch 1140, training loss: 85.02046203613281 = 0.5130466818809509 + 10.0 * 8.4507417678833
Epoch 1140, val loss: 0.5774640440940857
Epoch 1150, training loss: 85.0030746459961 = 0.511225163936615 + 10.0 * 8.449185371398926
Epoch 1150, val loss: 0.5765528082847595
Epoch 1160, training loss: 85.01130676269531 = 0.5093994736671448 + 10.0 * 8.450190544128418
Epoch 1160, val loss: 0.5755108594894409
Epoch 1170, training loss: 84.97430419921875 = 0.5075543522834778 + 10.0 * 8.446675300598145
Epoch 1170, val loss: 0.5743204355239868
Epoch 1180, training loss: 84.9668960571289 = 0.5057132840156555 + 10.0 * 8.446118354797363
Epoch 1180, val loss: 0.5732313990592957
Epoch 1190, training loss: 84.94935607910156 = 0.5038654208183289 + 10.0 * 8.444548606872559
Epoch 1190, val loss: 0.5722249746322632
Epoch 1200, training loss: 84.94925689697266 = 0.5020069479942322 + 10.0 * 8.444725036621094
Epoch 1200, val loss: 0.5711907744407654
Epoch 1210, training loss: 84.95067596435547 = 0.500108540058136 + 10.0 * 8.445056915283203
Epoch 1210, val loss: 0.5700323581695557
Epoch 1220, training loss: 84.92948913574219 = 0.49820247292518616 + 10.0 * 8.44312858581543
Epoch 1220, val loss: 0.5687450766563416
Epoch 1230, training loss: 84.91167449951172 = 0.49628329277038574 + 10.0 * 8.44153881072998
Epoch 1230, val loss: 0.5676711797714233
Epoch 1240, training loss: 84.90213012695312 = 0.49436578154563904 + 10.0 * 8.440775871276855
Epoch 1240, val loss: 0.5664607286453247
Epoch 1250, training loss: 84.89107513427734 = 0.49242231249809265 + 10.0 * 8.439865112304688
Epoch 1250, val loss: 0.5652565360069275
Epoch 1260, training loss: 84.8770523071289 = 0.49044734239578247 + 10.0 * 8.438660621643066
Epoch 1260, val loss: 0.5640318989753723
Epoch 1270, training loss: 84.8668212890625 = 0.4884622395038605 + 10.0 * 8.437835693359375
Epoch 1270, val loss: 0.5628504753112793
Epoch 1280, training loss: 84.86254119873047 = 0.48646706342697144 + 10.0 * 8.437607765197754
Epoch 1280, val loss: 0.5615690350532532
Epoch 1290, training loss: 84.91734313964844 = 0.48443591594696045 + 10.0 * 8.443290710449219
Epoch 1290, val loss: 0.5601910352706909
Epoch 1300, training loss: 84.86719512939453 = 0.48235419392585754 + 10.0 * 8.438484191894531
Epoch 1300, val loss: 0.5591592192649841
Epoch 1310, training loss: 84.83816528320312 = 0.4803127944469452 + 10.0 * 8.435785293579102
Epoch 1310, val loss: 0.5578440427780151
Epoch 1320, training loss: 84.8250503540039 = 0.47828397154808044 + 10.0 * 8.434676170349121
Epoch 1320, val loss: 0.5565779209136963
Epoch 1330, training loss: 84.8148422241211 = 0.4762377142906189 + 10.0 * 8.433860778808594
Epoch 1330, val loss: 0.5553222894668579
Epoch 1340, training loss: 84.80640411376953 = 0.47416818141937256 + 10.0 * 8.433223724365234
Epoch 1340, val loss: 0.5540878772735596
Epoch 1350, training loss: 84.79854583740234 = 0.47208768129348755 + 10.0 * 8.432645797729492
Epoch 1350, val loss: 0.552811861038208
Epoch 1360, training loss: 84.79106140136719 = 0.4699856638908386 + 10.0 * 8.432107925415039
Epoch 1360, val loss: 0.5515294075012207
Epoch 1370, training loss: 84.78982543945312 = 0.467867910861969 + 10.0 * 8.432195663452148
Epoch 1370, val loss: 0.5502397418022156
Epoch 1380, training loss: 84.82861328125 = 0.46573153138160706 + 10.0 * 8.436288833618164
Epoch 1380, val loss: 0.5488212704658508
Epoch 1390, training loss: 84.79986572265625 = 0.4635556638240814 + 10.0 * 8.43363094329834
Epoch 1390, val loss: 0.547735333442688
Epoch 1400, training loss: 84.77516174316406 = 0.46141913533210754 + 10.0 * 8.431374549865723
Epoch 1400, val loss: 0.5462695360183716
Epoch 1410, training loss: 84.76092529296875 = 0.4592866897583008 + 10.0 * 8.430163383483887
Epoch 1410, val loss: 0.5450823903083801
Epoch 1420, training loss: 84.7519302368164 = 0.4571576416492462 + 10.0 * 8.429476737976074
Epoch 1420, val loss: 0.5438101887702942
Epoch 1430, training loss: 84.743896484375 = 0.4550209641456604 + 10.0 * 8.428887367248535
Epoch 1430, val loss: 0.5425783395767212
Epoch 1440, training loss: 84.73760986328125 = 0.45288020372390747 + 10.0 * 8.428472518920898
Epoch 1440, val loss: 0.541318953037262
Epoch 1450, training loss: 84.73749542236328 = 0.4507312774658203 + 10.0 * 8.42867660522461
Epoch 1450, val loss: 0.5401276350021362
Epoch 1460, training loss: 84.78216552734375 = 0.44856634736061096 + 10.0 * 8.43336009979248
Epoch 1460, val loss: 0.5389254093170166
Epoch 1470, training loss: 84.7354736328125 = 0.4464108347892761 + 10.0 * 8.428906440734863
Epoch 1470, val loss: 0.5374236106872559
Epoch 1480, training loss: 84.71663665771484 = 0.4442634880542755 + 10.0 * 8.427236557006836
Epoch 1480, val loss: 0.53627610206604
Epoch 1490, training loss: 84.70820617675781 = 0.44215211272239685 + 10.0 * 8.426605224609375
Epoch 1490, val loss: 0.5350394248962402
Epoch 1500, training loss: 84.70188903808594 = 0.44004926085472107 + 10.0 * 8.426183700561523
Epoch 1500, val loss: 0.5338983535766602
Epoch 1510, training loss: 84.69586181640625 = 0.43796101212501526 + 10.0 * 8.425790786743164
Epoch 1510, val loss: 0.5326709151268005
Epoch 1520, training loss: 84.7261734008789 = 0.435867577791214 + 10.0 * 8.429030418395996
Epoch 1520, val loss: 0.5315468311309814
Epoch 1530, training loss: 84.7020492553711 = 0.43376681208610535 + 10.0 * 8.426828384399414
Epoch 1530, val loss: 0.5304531455039978
Epoch 1540, training loss: 84.68093872070312 = 0.43168652057647705 + 10.0 * 8.424924850463867
Epoch 1540, val loss: 0.5292651057243347
Epoch 1550, training loss: 84.67562103271484 = 0.4296201765537262 + 10.0 * 8.424600601196289
Epoch 1550, val loss: 0.5282280445098877
Epoch 1560, training loss: 84.68952178955078 = 0.4275611639022827 + 10.0 * 8.426196098327637
Epoch 1560, val loss: 0.5272173881530762
Epoch 1570, training loss: 84.66497039794922 = 0.425495982170105 + 10.0 * 8.42394733428955
Epoch 1570, val loss: 0.5260571837425232
Epoch 1580, training loss: 84.6574478149414 = 0.4234562814235687 + 10.0 * 8.423398971557617
Epoch 1580, val loss: 0.524943470954895
Epoch 1590, training loss: 84.65145111083984 = 0.42143726348876953 + 10.0 * 8.423001289367676
Epoch 1590, val loss: 0.5240020155906677
Epoch 1600, training loss: 84.64554595947266 = 0.41943997144699097 + 10.0 * 8.42261028289795
Epoch 1600, val loss: 0.5229345560073853
Epoch 1610, training loss: 84.63971710205078 = 0.4174629747867584 + 10.0 * 8.422224998474121
Epoch 1610, val loss: 0.521982729434967
Epoch 1620, training loss: 84.64202880859375 = 0.41549116373062134 + 10.0 * 8.422654151916504
Epoch 1620, val loss: 0.520987331867218
Epoch 1630, training loss: 84.64332580566406 = 0.41351282596588135 + 10.0 * 8.422981262207031
Epoch 1630, val loss: 0.5201073884963989
Epoch 1640, training loss: 84.6259994506836 = 0.41157054901123047 + 10.0 * 8.421442985534668
Epoch 1640, val loss: 0.5192288756370544
Epoch 1650, training loss: 84.62520599365234 = 0.409644216299057 + 10.0 * 8.42155647277832
Epoch 1650, val loss: 0.5184201002120972
Epoch 1660, training loss: 84.70252227783203 = 0.40774771571159363 + 10.0 * 8.42947769165039
Epoch 1660, val loss: 0.5178385972976685
Epoch 1670, training loss: 84.6191635131836 = 0.40587183833122253 + 10.0 * 8.421329498291016
Epoch 1670, val loss: 0.51674884557724
Epoch 1680, training loss: 84.6091537475586 = 0.4040222465991974 + 10.0 * 8.420513153076172
Epoch 1680, val loss: 0.5159497857093811
Epoch 1690, training loss: 84.59706115722656 = 0.4021899402141571 + 10.0 * 8.419486999511719
Epoch 1690, val loss: 0.5153694152832031
Epoch 1700, training loss: 84.59044647216797 = 0.40037837624549866 + 10.0 * 8.419007301330566
Epoch 1700, val loss: 0.5147358775138855
Epoch 1710, training loss: 84.58517456054688 = 0.3985765874385834 + 10.0 * 8.418660163879395
Epoch 1710, val loss: 0.5140300393104553
Epoch 1720, training loss: 84.58112335205078 = 0.39677610993385315 + 10.0 * 8.418435096740723
Epoch 1720, val loss: 0.5134575366973877
Epoch 1730, training loss: 84.62450408935547 = 0.39498528838157654 + 10.0 * 8.422951698303223
Epoch 1730, val loss: 0.5129414796829224
Epoch 1740, training loss: 84.57730102539062 = 0.3931935429573059 + 10.0 * 8.418410301208496
Epoch 1740, val loss: 0.5122162103652954
Epoch 1750, training loss: 84.5651626586914 = 0.39143088459968567 + 10.0 * 8.417372703552246
Epoch 1750, val loss: 0.5117589831352234
Epoch 1760, training loss: 84.55735778808594 = 0.38969680666923523 + 10.0 * 8.416766166687012
Epoch 1760, val loss: 0.5111885070800781
Epoch 1770, training loss: 84.5518798828125 = 0.38798195123672485 + 10.0 * 8.416389465332031
Epoch 1770, val loss: 0.5107231140136719
Epoch 1780, training loss: 84.54741668701172 = 0.3862821161746979 + 10.0 * 8.416112899780273
Epoch 1780, val loss: 0.5103466510772705
Epoch 1790, training loss: 84.58320617675781 = 0.38460153341293335 + 10.0 * 8.41986083984375
Epoch 1790, val loss: 0.5098714232444763
Epoch 1800, training loss: 84.56442260742188 = 0.38292232155799866 + 10.0 * 8.418149948120117
Epoch 1800, val loss: 0.5096763372421265
Epoch 1810, training loss: 84.53345489501953 = 0.3812643587589264 + 10.0 * 8.4152193069458
Epoch 1810, val loss: 0.5092585682868958
Epoch 1820, training loss: 84.52442932128906 = 0.3796234428882599 + 10.0 * 8.414480209350586
Epoch 1820, val loss: 0.5089361071586609
Epoch 1830, training loss: 84.51808166503906 = 0.37800145149230957 + 10.0 * 8.414008140563965
Epoch 1830, val loss: 0.508701503276825
Epoch 1840, training loss: 84.51262664794922 = 0.3763900399208069 + 10.0 * 8.413623809814453
Epoch 1840, val loss: 0.5084746479988098
Epoch 1850, training loss: 84.6171875 = 0.3747870922088623 + 10.0 * 8.424240112304688
Epoch 1850, val loss: 0.5085087418556213
Epoch 1860, training loss: 84.52342224121094 = 0.37318822741508484 + 10.0 * 8.415022850036621
Epoch 1860, val loss: 0.5078915357589722
Epoch 1870, training loss: 84.51166534423828 = 0.37160277366638184 + 10.0 * 8.414006233215332
Epoch 1870, val loss: 0.507783830165863
Epoch 1880, training loss: 84.49578857421875 = 0.37004926800727844 + 10.0 * 8.41257381439209
Epoch 1880, val loss: 0.5076252818107605
Epoch 1890, training loss: 84.48643493652344 = 0.36851274967193604 + 10.0 * 8.411791801452637
Epoch 1890, val loss: 0.5074964761734009
Epoch 1900, training loss: 84.47905731201172 = 0.3669973313808441 + 10.0 * 8.411206245422363
Epoch 1900, val loss: 0.5073872804641724
Epoch 1910, training loss: 84.47254180908203 = 0.36549413204193115 + 10.0 * 8.410704612731934
Epoch 1910, val loss: 0.5073309540748596
Epoch 1920, training loss: 84.46705627441406 = 0.36399808526039124 + 10.0 * 8.410305976867676
Epoch 1920, val loss: 0.5072672367095947
Epoch 1930, training loss: 84.4625244140625 = 0.3625025153160095 + 10.0 * 8.410001754760742
Epoch 1930, val loss: 0.5072356462478638
Epoch 1940, training loss: 84.51617431640625 = 0.36100903153419495 + 10.0 * 8.41551685333252
Epoch 1940, val loss: 0.5073251724243164
Epoch 1950, training loss: 84.47640991210938 = 0.35951027274131775 + 10.0 * 8.411689758300781
Epoch 1950, val loss: 0.5071695446968079
Epoch 1960, training loss: 84.45956420898438 = 0.3580402731895447 + 10.0 * 8.410152435302734
Epoch 1960, val loss: 0.507142186164856
Epoch 1970, training loss: 84.44135284423828 = 0.3565778136253357 + 10.0 * 8.408477783203125
Epoch 1970, val loss: 0.507138729095459
Epoch 1980, training loss: 84.43727111816406 = 0.35513195395469666 + 10.0 * 8.40821361541748
Epoch 1980, val loss: 0.5071938037872314
Epoch 1990, training loss: 84.43164825439453 = 0.3537020981311798 + 10.0 * 8.407794952392578
Epoch 1990, val loss: 0.5072498321533203
Epoch 2000, training loss: 84.42772674560547 = 0.3522876799106598 + 10.0 * 8.407544136047363
Epoch 2000, val loss: 0.5074287056922913
Epoch 2010, training loss: 84.47769165039062 = 0.3508855998516083 + 10.0 * 8.412680625915527
Epoch 2010, val loss: 0.5077930688858032
Epoch 2020, training loss: 84.43331909179688 = 0.3494948446750641 + 10.0 * 8.408382415771484
Epoch 2020, val loss: 0.5075450539588928
Epoch 2030, training loss: 84.41268920898438 = 0.34809017181396484 + 10.0 * 8.40645980834961
Epoch 2030, val loss: 0.5077769160270691
Epoch 2040, training loss: 84.41134643554688 = 0.3466976583003998 + 10.0 * 8.406465530395508
Epoch 2040, val loss: 0.5080568194389343
Epoch 2050, training loss: 84.45111083984375 = 0.34531280398368835 + 10.0 * 8.410579681396484
Epoch 2050, val loss: 0.508367121219635
Epoch 2060, training loss: 84.40660095214844 = 0.34393855929374695 + 10.0 * 8.406266212463379
Epoch 2060, val loss: 0.5082402229309082
Epoch 2070, training loss: 84.39893341064453 = 0.34256717562675476 + 10.0 * 8.40563678741455
Epoch 2070, val loss: 0.5086009502410889
Epoch 2080, training loss: 84.39302062988281 = 0.3412145972251892 + 10.0 * 8.405179977416992
Epoch 2080, val loss: 0.5088409185409546
Epoch 2090, training loss: 84.38773345947266 = 0.33986520767211914 + 10.0 * 8.404787063598633
Epoch 2090, val loss: 0.5090867877006531
Epoch 2100, training loss: 84.38233184814453 = 0.3385152816772461 + 10.0 * 8.40438175201416
Epoch 2100, val loss: 0.5093662142753601
Epoch 2110, training loss: 84.38824462890625 = 0.33717018365859985 + 10.0 * 8.405107498168945
Epoch 2110, val loss: 0.5097417235374451
Epoch 2120, training loss: 84.38824462890625 = 0.3358277380466461 + 10.0 * 8.405241012573242
Epoch 2120, val loss: 0.5099657773971558
Epoch 2130, training loss: 84.39006042480469 = 0.33449074625968933 + 10.0 * 8.405557632446289
Epoch 2130, val loss: 0.5104180574417114
Epoch 2140, training loss: 84.40009307861328 = 0.3331601619720459 + 10.0 * 8.406693458557129
Epoch 2140, val loss: 0.5107995867729187
Epoch 2150, training loss: 84.37054443359375 = 0.3318483829498291 + 10.0 * 8.40386962890625
Epoch 2150, val loss: 0.5109557509422302
Epoch 2160, training loss: 84.35879516601562 = 0.3305380642414093 + 10.0 * 8.402826309204102
Epoch 2160, val loss: 0.5115101933479309
Epoch 2170, training loss: 84.35759735107422 = 0.32923635840415955 + 10.0 * 8.402835845947266
Epoch 2170, val loss: 0.5119307041168213
Epoch 2180, training loss: 84.35496520996094 = 0.3279415965080261 + 10.0 * 8.402702331542969
Epoch 2180, val loss: 0.5123594999313354
Epoch 2190, training loss: 84.36460876464844 = 0.3266463577747345 + 10.0 * 8.403796195983887
Epoch 2190, val loss: 0.5129026174545288
Epoch 2200, training loss: 84.35999298095703 = 0.32534822821617126 + 10.0 * 8.403464317321777
Epoch 2200, val loss: 0.513420045375824
Epoch 2210, training loss: 84.34932708740234 = 0.3240548372268677 + 10.0 * 8.40252685546875
Epoch 2210, val loss: 0.5139005184173584
Epoch 2220, training loss: 84.34101104736328 = 0.3227691352367401 + 10.0 * 8.401823997497559
Epoch 2220, val loss: 0.5144570469856262
Epoch 2230, training loss: 84.35794067382812 = 0.32149556279182434 + 10.0 * 8.403644561767578
Epoch 2230, val loss: 0.5150637626647949
Epoch 2240, training loss: 84.33061218261719 = 0.32022207975387573 + 10.0 * 8.401039123535156
Epoch 2240, val loss: 0.5155084133148193
Epoch 2250, training loss: 84.33113861083984 = 0.31894615292549133 + 10.0 * 8.401219367980957
Epoch 2250, val loss: 0.5161004662513733
Epoch 2260, training loss: 84.35137176513672 = 0.3176748752593994 + 10.0 * 8.403369903564453
Epoch 2260, val loss: 0.5167014002799988
Epoch 2270, training loss: 84.32714080810547 = 0.31641390919685364 + 10.0 * 8.40107250213623
Epoch 2270, val loss: 0.517109215259552
Epoch 2280, training loss: 84.31703186035156 = 0.3151565194129944 + 10.0 * 8.400187492370605
Epoch 2280, val loss: 0.5177602171897888
Epoch 2290, training loss: 84.31376647949219 = 0.31390464305877686 + 10.0 * 8.399986267089844
Epoch 2290, val loss: 0.5183541774749756
Epoch 2300, training loss: 84.31804656982422 = 0.31265953183174133 + 10.0 * 8.400538444519043
Epoch 2300, val loss: 0.5190119743347168
Epoch 2310, training loss: 84.3569107055664 = 0.3114166259765625 + 10.0 * 8.404549598693848
Epoch 2310, val loss: 0.5197398066520691
Epoch 2320, training loss: 84.31539916992188 = 0.31018340587615967 + 10.0 * 8.400522232055664
Epoch 2320, val loss: 0.5199958682060242
Epoch 2330, training loss: 84.30284118652344 = 0.30892515182495117 + 10.0 * 8.399392127990723
Epoch 2330, val loss: 0.5208237171173096
Epoch 2340, training loss: 84.29678344726562 = 0.30768299102783203 + 10.0 * 8.398909568786621
Epoch 2340, val loss: 0.5214135646820068
Epoch 2350, training loss: 84.29371643066406 = 0.30643710494041443 + 10.0 * 8.398728370666504
Epoch 2350, val loss: 0.5221021771430969
Epoch 2360, training loss: 84.2993392944336 = 0.305196613073349 + 10.0 * 8.3994140625
Epoch 2360, val loss: 0.5227530002593994
Epoch 2370, training loss: 84.33399200439453 = 0.3039502501487732 + 10.0 * 8.40300464630127
Epoch 2370, val loss: 0.5235644578933716
Epoch 2380, training loss: 84.29743957519531 = 0.30269816517829895 + 10.0 * 8.399474143981934
Epoch 2380, val loss: 0.5241503715515137
Epoch 2390, training loss: 84.28596496582031 = 0.3014472723007202 + 10.0 * 8.398451805114746
Epoch 2390, val loss: 0.5246925950050354
Epoch 2400, training loss: 84.28059387207031 = 0.300193190574646 + 10.0 * 8.398039817810059
Epoch 2400, val loss: 0.5253862738609314
Epoch 2410, training loss: 84.29485321044922 = 0.29894644021987915 + 10.0 * 8.399590492248535
Epoch 2410, val loss: 0.5260627865791321
Epoch 2420, training loss: 84.27476501464844 = 0.2977031171321869 + 10.0 * 8.397706031799316
Epoch 2420, val loss: 0.5266290307044983
Epoch 2430, training loss: 84.26898956298828 = 0.29647013545036316 + 10.0 * 8.397252082824707
Epoch 2430, val loss: 0.5272456407546997
Epoch 2440, training loss: 84.26766204833984 = 0.29523783922195435 + 10.0 * 8.397242546081543
Epoch 2440, val loss: 0.5279754400253296
Epoch 2450, training loss: 84.27750396728516 = 0.2940145432949066 + 10.0 * 8.398348808288574
Epoch 2450, val loss: 0.5285215973854065
Epoch 2460, training loss: 84.29944610595703 = 0.29278749227523804 + 10.0 * 8.400666236877441
Epoch 2460, val loss: 0.5293043851852417
Epoch 2470, training loss: 84.2789077758789 = 0.29154831171035767 + 10.0 * 8.398736000061035
Epoch 2470, val loss: 0.5301991105079651
Epoch 2480, training loss: 84.26016235351562 = 0.29031914472579956 + 10.0 * 8.396984100341797
Epoch 2480, val loss: 0.5308435559272766
Epoch 2490, training loss: 84.25110626220703 = 0.28908416628837585 + 10.0 * 8.396202087402344
Epoch 2490, val loss: 0.5317288637161255
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8021308980213089
0.8145330725204666
=== training gcn model ===
Epoch 0, training loss: 106.9330825805664 = 1.1101073026657104 + 10.0 * 10.582297325134277
Epoch 0, val loss: 1.108897089958191
Epoch 10, training loss: 106.92496490478516 = 1.104460597038269 + 10.0 * 10.582050323486328
Epoch 10, val loss: 1.1032930612564087
Epoch 20, training loss: 106.9053955078125 = 1.098604440689087 + 10.0 * 10.580678939819336
Epoch 20, val loss: 1.0974628925323486
Epoch 30, training loss: 106.8341064453125 = 1.092416763305664 + 10.0 * 10.574169158935547
Epoch 30, val loss: 1.0913150310516357
Epoch 40, training loss: 106.58135986328125 = 1.085811972618103 + 10.0 * 10.549554824829102
Epoch 40, val loss: 1.0847535133361816
Epoch 50, training loss: 105.8838119506836 = 1.0783007144927979 + 10.0 * 10.480550765991211
Epoch 50, val loss: 1.0773266553878784
Epoch 60, training loss: 104.29736328125 = 1.070698857307434 + 10.0 * 10.32266616821289
Epoch 60, val loss: 1.0699509382247925
Epoch 70, training loss: 102.00226593017578 = 1.062967300415039 + 10.0 * 10.0939302444458
Epoch 70, val loss: 1.0622831583023071
Epoch 80, training loss: 100.92540740966797 = 1.0556987524032593 + 10.0 * 9.986970901489258
Epoch 80, val loss: 1.0553420782089233
Epoch 90, training loss: 99.52405548095703 = 1.0498682260513306 + 10.0 * 9.847418785095215
Epoch 90, val loss: 1.0498204231262207
Epoch 100, training loss: 97.86945343017578 = 1.045633316040039 + 10.0 * 9.682382583618164
Epoch 100, val loss: 1.046040654182434
Epoch 110, training loss: 96.68910217285156 = 1.0436233282089233 + 10.0 * 9.564547538757324
Epoch 110, val loss: 1.0444753170013428
Epoch 120, training loss: 95.92406463623047 = 1.0422435998916626 + 10.0 * 9.488182067871094
Epoch 120, val loss: 1.0432133674621582
Epoch 130, training loss: 94.4710693359375 = 1.0398240089416504 + 10.0 * 9.343124389648438
Epoch 130, val loss: 1.0407248735427856
Epoch 140, training loss: 92.48287200927734 = 1.0379096269607544 + 10.0 * 9.144495964050293
Epoch 140, val loss: 1.0389974117279053
Epoch 150, training loss: 91.21343994140625 = 1.0370380878448486 + 10.0 * 9.017640113830566
Epoch 150, val loss: 1.038087010383606
Epoch 160, training loss: 90.34947967529297 = 1.0353343486785889 + 10.0 * 8.931414604187012
Epoch 160, val loss: 1.0363974571228027
Epoch 170, training loss: 89.78089141845703 = 1.032902717590332 + 10.0 * 8.874798774719238
Epoch 170, val loss: 1.034077763557434
Epoch 180, training loss: 89.41963958740234 = 1.0299098491668701 + 10.0 * 8.838973045349121
Epoch 180, val loss: 1.0312464237213135
Epoch 190, training loss: 89.13299560546875 = 1.026750087738037 + 10.0 * 8.810625076293945
Epoch 190, val loss: 1.0282397270202637
Epoch 200, training loss: 88.88712310791016 = 1.0236139297485352 + 10.0 * 8.786351203918457
Epoch 200, val loss: 1.0252296924591064
Epoch 210, training loss: 88.71538543701172 = 1.0202640295028687 + 10.0 * 8.769512176513672
Epoch 210, val loss: 1.0219911336898804
Epoch 220, training loss: 88.55248260498047 = 1.0166324377059937 + 10.0 * 8.753584861755371
Epoch 220, val loss: 1.0185024738311768
Epoch 230, training loss: 88.37681579589844 = 1.0128957033157349 + 10.0 * 8.7363920211792
Epoch 230, val loss: 1.0149341821670532
Epoch 240, training loss: 88.21343231201172 = 1.0090885162353516 + 10.0 * 8.720434188842773
Epoch 240, val loss: 1.0113471746444702
Epoch 250, training loss: 88.02055358886719 = 1.0050909519195557 + 10.0 * 8.701546669006348
Epoch 250, val loss: 1.0074808597564697
Epoch 260, training loss: 87.85745239257812 = 1.0006999969482422 + 10.0 * 8.685674667358398
Epoch 260, val loss: 1.0032404661178589
Epoch 270, training loss: 87.72350311279297 = 0.9957854151725769 + 10.0 * 8.672771453857422
Epoch 270, val loss: 0.99851393699646
Epoch 280, training loss: 87.56179809570312 = 0.9903876185417175 + 10.0 * 8.657140731811523
Epoch 280, val loss: 0.993299126625061
Epoch 290, training loss: 87.4205322265625 = 0.9846436381340027 + 10.0 * 8.64358901977539
Epoch 290, val loss: 0.9878283739089966
Epoch 300, training loss: 87.27262115478516 = 0.9785935282707214 + 10.0 * 8.629403114318848
Epoch 300, val loss: 0.9819860458374023
Epoch 310, training loss: 87.12873840332031 = 0.9722001552581787 + 10.0 * 8.615653991699219
Epoch 310, val loss: 0.975795567035675
Epoch 320, training loss: 87.01423645019531 = 0.9653325080871582 + 10.0 * 8.604890823364258
Epoch 320, val loss: 0.9691652655601501
Epoch 330, training loss: 86.92086029052734 = 0.957964301109314 + 10.0 * 8.59628963470459
Epoch 330, val loss: 0.9620132446289062
Epoch 340, training loss: 86.87490844726562 = 0.9499925971031189 + 10.0 * 8.59249210357666
Epoch 340, val loss: 0.9542104005813599
Epoch 350, training loss: 86.7750244140625 = 0.9414733648300171 + 10.0 * 8.583354949951172
Epoch 350, val loss: 0.9459912180900574
Epoch 360, training loss: 86.69943237304688 = 0.9326082468032837 + 10.0 * 8.576682090759277
Epoch 360, val loss: 0.9374306201934814
Epoch 370, training loss: 86.63182067871094 = 0.9233975410461426 + 10.0 * 8.570841789245605
Epoch 370, val loss: 0.9284882545471191
Epoch 380, training loss: 86.57456970214844 = 0.9138180613517761 + 10.0 * 8.566075325012207
Epoch 380, val loss: 0.9192014336585999
Epoch 390, training loss: 86.5224609375 = 0.9039467573165894 + 10.0 * 8.561851501464844
Epoch 390, val loss: 0.9095995426177979
Epoch 400, training loss: 86.47513580322266 = 0.8938881158828735 + 10.0 * 8.558124542236328
Epoch 400, val loss: 0.8998722434043884
Epoch 410, training loss: 86.42544555664062 = 0.8837152719497681 + 10.0 * 8.554173469543457
Epoch 410, val loss: 0.8900304436683655
Epoch 420, training loss: 86.40635681152344 = 0.8734473586082458 + 10.0 * 8.553291320800781
Epoch 420, val loss: 0.8800377249717712
Epoch 430, training loss: 86.33352661132812 = 0.8630167841911316 + 10.0 * 8.547051429748535
Epoch 430, val loss: 0.8700194358825684
Epoch 440, training loss: 86.28144836425781 = 0.8526319265365601 + 10.0 * 8.542881965637207
Epoch 440, val loss: 0.8599675893783569
Epoch 450, training loss: 86.235595703125 = 0.842258632183075 + 10.0 * 8.53933334350586
Epoch 450, val loss: 0.8498899936676025
Epoch 460, training loss: 86.22596740722656 = 0.8319004774093628 + 10.0 * 8.539406776428223
Epoch 460, val loss: 0.8398504853248596
Epoch 470, training loss: 86.15756225585938 = 0.8215725421905518 + 10.0 * 8.533598899841309
Epoch 470, val loss: 0.8298441767692566
Epoch 480, training loss: 86.11187744140625 = 0.8113443851470947 + 10.0 * 8.53005313873291
Epoch 480, val loss: 0.8199809789657593
Epoch 490, training loss: 86.06883239746094 = 0.8012074828147888 + 10.0 * 8.526762008666992
Epoch 490, val loss: 0.81020587682724
Epoch 500, training loss: 86.10888671875 = 0.7911798357963562 + 10.0 * 8.531770706176758
Epoch 500, val loss: 0.8004429340362549
Epoch 510, training loss: 86.00638580322266 = 0.781198263168335 + 10.0 * 8.5225191116333
Epoch 510, val loss: 0.7908795475959778
Epoch 520, training loss: 85.95637512207031 = 0.7714296579360962 + 10.0 * 8.518494606018066
Epoch 520, val loss: 0.781539261341095
Epoch 530, training loss: 85.91403198242188 = 0.7617490887641907 + 10.0 * 8.515228271484375
Epoch 530, val loss: 0.7722741961479187
Epoch 540, training loss: 85.87545776367188 = 0.7521783709526062 + 10.0 * 8.512327194213867
Epoch 540, val loss: 0.7631404995918274
Epoch 550, training loss: 85.83370208740234 = 0.7428181171417236 + 10.0 * 8.509088516235352
Epoch 550, val loss: 0.7541807889938354
Epoch 560, training loss: 85.84789276123047 = 0.7336108684539795 + 10.0 * 8.511427879333496
Epoch 560, val loss: 0.7454193234443665
Epoch 570, training loss: 85.78152465820312 = 0.7245295643806458 + 10.0 * 8.505699157714844
Epoch 570, val loss: 0.7367616295814514
Epoch 580, training loss: 85.71525573730469 = 0.7157415747642517 + 10.0 * 8.499951362609863
Epoch 580, val loss: 0.7285358309745789
Epoch 590, training loss: 85.68257904052734 = 0.7071572542190552 + 10.0 * 8.497541427612305
Epoch 590, val loss: 0.7204158902168274
Epoch 600, training loss: 85.68696594238281 = 0.6988217830657959 + 10.0 * 8.498814582824707
Epoch 600, val loss: 0.7125198245048523
Epoch 610, training loss: 85.60750579833984 = 0.690683901309967 + 10.0 * 8.491682052612305
Epoch 610, val loss: 0.7049988508224487
Epoch 620, training loss: 85.58078002929688 = 0.682873547077179 + 10.0 * 8.489789962768555
Epoch 620, val loss: 0.6978129744529724
Epoch 630, training loss: 85.54878997802734 = 0.6753333210945129 + 10.0 * 8.487345695495605
Epoch 630, val loss: 0.6907857656478882
Epoch 640, training loss: 85.51966857910156 = 0.6680447459220886 + 10.0 * 8.485162734985352
Epoch 640, val loss: 0.6841417551040649
Epoch 650, training loss: 85.53855895996094 = 0.6610128879547119 + 10.0 * 8.487754821777344
Epoch 650, val loss: 0.6777833104133606
Epoch 660, training loss: 85.46983337402344 = 0.6542797088623047 + 10.0 * 8.481554985046387
Epoch 660, val loss: 0.6716349124908447
Epoch 670, training loss: 85.44601440429688 = 0.64789879322052 + 10.0 * 8.479811668395996
Epoch 670, val loss: 0.665876567363739
Epoch 680, training loss: 85.45297241210938 = 0.6417999863624573 + 10.0 * 8.481117248535156
Epoch 680, val loss: 0.660487174987793
Epoch 690, training loss: 85.44530487060547 = 0.635979413986206 + 10.0 * 8.480932235717773
Epoch 690, val loss: 0.6551704406738281
Epoch 700, training loss: 85.39087677001953 = 0.6304764151573181 + 10.0 * 8.47603988647461
Epoch 700, val loss: 0.6504553556442261
Epoch 710, training loss: 85.36798095703125 = 0.6252850294113159 + 10.0 * 8.47426986694336
Epoch 710, val loss: 0.645946204662323
Epoch 720, training loss: 85.34683227539062 = 0.6203506588935852 + 10.0 * 8.472647666931152
Epoch 720, val loss: 0.6416787505149841
Epoch 730, training loss: 85.32876586914062 = 0.6156735420227051 + 10.0 * 8.471308708190918
Epoch 730, val loss: 0.6376939415931702
Epoch 740, training loss: 85.31216430664062 = 0.6112282276153564 + 10.0 * 8.470093727111816
Epoch 740, val loss: 0.6339338421821594
Epoch 750, training loss: 85.29581451416016 = 0.6069998741149902 + 10.0 * 8.468881607055664
Epoch 750, val loss: 0.6304075717926025
Epoch 760, training loss: 85.2809829711914 = 0.602988600730896 + 10.0 * 8.467799186706543
Epoch 760, val loss: 0.627082884311676
Epoch 770, training loss: 85.31548309326172 = 0.5991546511650085 + 10.0 * 8.471632957458496
Epoch 770, val loss: 0.6238880753517151
Epoch 780, training loss: 85.31806945800781 = 0.5955037474632263 + 10.0 * 8.472256660461426
Epoch 780, val loss: 0.6209531426429749
Epoch 790, training loss: 85.24847412109375 = 0.5920682549476624 + 10.0 * 8.4656400680542
Epoch 790, val loss: 0.6183176636695862
Epoch 800, training loss: 85.22325897216797 = 0.5888563990592957 + 10.0 * 8.46343994140625
Epoch 800, val loss: 0.6157271265983582
Epoch 810, training loss: 85.20895385742188 = 0.5857845544815063 + 10.0 * 8.462316513061523
Epoch 810, val loss: 0.6133255958557129
Epoch 820, training loss: 85.19519805908203 = 0.5828577280044556 + 10.0 * 8.461234092712402
Epoch 820, val loss: 0.6110878586769104
Epoch 830, training loss: 85.18095397949219 = 0.5800653696060181 + 10.0 * 8.460088729858398
Epoch 830, val loss: 0.6089815497398376
Epoch 840, training loss: 85.22737121582031 = 0.5773980021476746 + 10.0 * 8.464997291564941
Epoch 840, val loss: 0.6068428754806519
Epoch 850, training loss: 85.17906188964844 = 0.5747867226600647 + 10.0 * 8.460428237915039
Epoch 850, val loss: 0.6051242351531982
Epoch 860, training loss: 85.13731384277344 = 0.5723522305488586 + 10.0 * 8.456496238708496
Epoch 860, val loss: 0.6033065915107727
Epoch 870, training loss: 85.12442779541016 = 0.5700182318687439 + 10.0 * 8.45544147491455
Epoch 870, val loss: 0.6015185117721558
Epoch 880, training loss: 85.10987091064453 = 0.5677565336227417 + 10.0 * 8.454211235046387
Epoch 880, val loss: 0.5999805331230164
Epoch 890, training loss: 85.13018798828125 = 0.5655865669250488 + 10.0 * 8.456459999084473
Epoch 890, val loss: 0.5984196662902832
Epoch 900, training loss: 85.10054016113281 = 0.5634815096855164 + 10.0 * 8.453705787658691
Epoch 900, val loss: 0.5969154238700867
Epoch 910, training loss: 85.10747528076172 = 0.5614590644836426 + 10.0 * 8.454601287841797
Epoch 910, val loss: 0.5954972505569458
Epoch 920, training loss: 85.06426239013672 = 0.5594736337661743 + 10.0 * 8.450479507446289
Epoch 920, val loss: 0.5941669344902039
Epoch 930, training loss: 85.04325103759766 = 0.5575847029685974 + 10.0 * 8.448566436767578
Epoch 930, val loss: 0.5928378105163574
Epoch 940, training loss: 85.03236389160156 = 0.5557317733764648 + 10.0 * 8.447663307189941
Epoch 940, val loss: 0.5916619896888733
Epoch 950, training loss: 85.05950164794922 = 0.5539352893829346 + 10.0 * 8.450556755065918
Epoch 950, val loss: 0.5904919505119324
Epoch 960, training loss: 85.01569366455078 = 0.5521519184112549 + 10.0 * 8.446353912353516
Epoch 960, val loss: 0.5891358256340027
Epoch 970, training loss: 84.99726867675781 = 0.550437867641449 + 10.0 * 8.444683074951172
Epoch 970, val loss: 0.5881155729293823
Epoch 980, training loss: 84.98036193847656 = 0.5487626791000366 + 10.0 * 8.443160057067871
Epoch 980, val loss: 0.5869283676147461
Epoch 990, training loss: 84.97045135498047 = 0.5471262335777283 + 10.0 * 8.44233226776123
Epoch 990, val loss: 0.5858959555625916
Epoch 1000, training loss: 84.96025085449219 = 0.5455184578895569 + 10.0 * 8.441473007202148
Epoch 1000, val loss: 0.5848791003227234
Epoch 1010, training loss: 85.00664520263672 = 0.5439148545265198 + 10.0 * 8.446272850036621
Epoch 1010, val loss: 0.5838814377784729
Epoch 1020, training loss: 84.97091674804688 = 0.5423287749290466 + 10.0 * 8.442858695983887
Epoch 1020, val loss: 0.5828313231468201
Epoch 1030, training loss: 84.93651580810547 = 0.5408051609992981 + 10.0 * 8.439571380615234
Epoch 1030, val loss: 0.5818853974342346
Epoch 1040, training loss: 84.9200668334961 = 0.5393278002738953 + 10.0 * 8.438074111938477
Epoch 1040, val loss: 0.5809433460235596
Epoch 1050, training loss: 84.92842864990234 = 0.5378727316856384 + 10.0 * 8.439055442810059
Epoch 1050, val loss: 0.5801103115081787
Epoch 1060, training loss: 84.89840698242188 = 0.5363953709602356 + 10.0 * 8.436201095581055
Epoch 1060, val loss: 0.5789750814437866
Epoch 1070, training loss: 84.8973388671875 = 0.5349622368812561 + 10.0 * 8.436237335205078
Epoch 1070, val loss: 0.5782569050788879
Epoch 1080, training loss: 84.88263702392578 = 0.5335655808448792 + 10.0 * 8.434906959533691
Epoch 1080, val loss: 0.5772738456726074
Epoch 1090, training loss: 84.87166595458984 = 0.532181978225708 + 10.0 * 8.433948516845703
Epoch 1090, val loss: 0.576481282711029
Epoch 1100, training loss: 84.86203002929688 = 0.5308189988136292 + 10.0 * 8.433120727539062
Epoch 1100, val loss: 0.5756477117538452
Epoch 1110, training loss: 84.854248046875 = 0.5294589400291443 + 10.0 * 8.432478904724121
Epoch 1110, val loss: 0.5748428106307983
Epoch 1120, training loss: 84.86540222167969 = 0.5281089544296265 + 10.0 * 8.43372917175293
Epoch 1120, val loss: 0.5740442276000977
Epoch 1130, training loss: 84.8604736328125 = 0.5267466902732849 + 10.0 * 8.433372497558594
Epoch 1130, val loss: 0.5730805993080139
Epoch 1140, training loss: 84.84598541259766 = 0.5253818035125732 + 10.0 * 8.432060241699219
Epoch 1140, val loss: 0.5725135207176208
Epoch 1150, training loss: 84.8233871459961 = 0.5240726470947266 + 10.0 * 8.429931640625
Epoch 1150, val loss: 0.5716404318809509
Epoch 1160, training loss: 84.8180160522461 = 0.5227759480476379 + 10.0 * 8.429524421691895
Epoch 1160, val loss: 0.5708481073379517
Epoch 1170, training loss: 84.81297302246094 = 0.5214824080467224 + 10.0 * 8.42914867401123
Epoch 1170, val loss: 0.570072591304779
Epoch 1180, training loss: 84.8233413696289 = 0.5201933979988098 + 10.0 * 8.430315017700195
Epoch 1180, val loss: 0.5693156719207764
Epoch 1190, training loss: 84.81552124023438 = 0.5188899636268616 + 10.0 * 8.429662704467773
Epoch 1190, val loss: 0.5686295032501221
Epoch 1200, training loss: 84.80699157714844 = 0.5175923705101013 + 10.0 * 8.428939819335938
Epoch 1200, val loss: 0.567833662033081
Epoch 1210, training loss: 84.79444122314453 = 0.5163207054138184 + 10.0 * 8.427812576293945
Epoch 1210, val loss: 0.5671194195747375
Epoch 1220, training loss: 84.77791595458984 = 0.5150759220123291 + 10.0 * 8.426283836364746
Epoch 1220, val loss: 0.5664457678794861
Epoch 1230, training loss: 84.76920318603516 = 0.5138412117958069 + 10.0 * 8.425536155700684
Epoch 1230, val loss: 0.5657994747161865
Epoch 1240, training loss: 84.76155090332031 = 0.5126170516014099 + 10.0 * 8.424893379211426
Epoch 1240, val loss: 0.5650647878646851
Epoch 1250, training loss: 84.75447082519531 = 0.5113867521286011 + 10.0 * 8.424308776855469
Epoch 1250, val loss: 0.5644122362136841
Epoch 1260, training loss: 84.75257873535156 = 0.5101635456085205 + 10.0 * 8.42424201965332
Epoch 1260, val loss: 0.5637155771255493
Epoch 1270, training loss: 84.79814910888672 = 0.5089185833930969 + 10.0 * 8.428922653198242
Epoch 1270, val loss: 0.5629805326461792
Epoch 1280, training loss: 84.74076080322266 = 0.5076830983161926 + 10.0 * 8.423307418823242
Epoch 1280, val loss: 0.562379002571106
Epoch 1290, training loss: 84.73174285888672 = 0.5064662098884583 + 10.0 * 8.422527313232422
Epoch 1290, val loss: 0.5616395473480225
Epoch 1300, training loss: 84.72258758544922 = 0.5052684545516968 + 10.0 * 8.421731948852539
Epoch 1300, val loss: 0.5610325932502747
Epoch 1310, training loss: 84.71601867675781 = 0.5040769577026367 + 10.0 * 8.421194076538086
Epoch 1310, val loss: 0.5603732466697693
Epoch 1320, training loss: 84.76134490966797 = 0.5028751492500305 + 10.0 * 8.425847053527832
Epoch 1320, val loss: 0.5597002506256104
Epoch 1330, training loss: 84.72782135009766 = 0.5016782879829407 + 10.0 * 8.422614097595215
Epoch 1330, val loss: 0.5590200424194336
Epoch 1340, training loss: 84.69676208496094 = 0.5004749894142151 + 10.0 * 8.419629096984863
Epoch 1340, val loss: 0.5584655404090881
Epoch 1350, training loss: 84.69113159179688 = 0.49930354952812195 + 10.0 * 8.419182777404785
Epoch 1350, val loss: 0.5578522086143494
Epoch 1360, training loss: 84.68196105957031 = 0.4981502294540405 + 10.0 * 8.418380737304688
Epoch 1360, val loss: 0.5572046041488647
Epoch 1370, training loss: 84.67488861083984 = 0.49698662757873535 + 10.0 * 8.417790412902832
Epoch 1370, val loss: 0.5565994381904602
Epoch 1380, training loss: 84.67720794677734 = 0.4958300292491913 + 10.0 * 8.41813850402832
Epoch 1380, val loss: 0.555907130241394
Epoch 1390, training loss: 84.75175476074219 = 0.49463337659835815 + 10.0 * 8.425711631774902
Epoch 1390, val loss: 0.5552886724472046
Epoch 1400, training loss: 84.6972427368164 = 0.49341288208961487 + 10.0 * 8.420382499694824
Epoch 1400, val loss: 0.5547863841056824
Epoch 1410, training loss: 84.65460205078125 = 0.49223387241363525 + 10.0 * 8.416236877441406
Epoch 1410, val loss: 0.5540898442268372
Epoch 1420, training loss: 84.64468383789062 = 0.4910765290260315 + 10.0 * 8.415361404418945
Epoch 1420, val loss: 0.5534787774085999
Epoch 1430, training loss: 84.63797760009766 = 0.48992055654525757 + 10.0 * 8.41480541229248
Epoch 1430, val loss: 0.552940845489502
Epoch 1440, training loss: 84.63113403320312 = 0.48876824975013733 + 10.0 * 8.414236068725586
Epoch 1440, val loss: 0.5523450970649719
Epoch 1450, training loss: 84.62464904785156 = 0.48761630058288574 + 10.0 * 8.413702964782715
Epoch 1450, val loss: 0.5517644882202148
Epoch 1460, training loss: 84.6187744140625 = 0.4864616096019745 + 10.0 * 8.41323184967041
Epoch 1460, val loss: 0.5511723756790161
Epoch 1470, training loss: 84.61859130859375 = 0.485306054353714 + 10.0 * 8.413328170776367
Epoch 1470, val loss: 0.5505906343460083
Epoch 1480, training loss: 84.63752746582031 = 0.48412978649139404 + 10.0 * 8.415339469909668
Epoch 1480, val loss: 0.5499489903450012
Epoch 1490, training loss: 84.61194610595703 = 0.4829480051994324 + 10.0 * 8.4128999710083
Epoch 1490, val loss: 0.5493780374526978
Epoch 1500, training loss: 84.59807586669922 = 0.48179081082344055 + 10.0 * 8.411628723144531
Epoch 1500, val loss: 0.5487640500068665
Epoch 1510, training loss: 84.59862518310547 = 0.48064279556274414 + 10.0 * 8.411798477172852
Epoch 1510, val loss: 0.5482076406478882
Epoch 1520, training loss: 84.64226531982422 = 0.47948652505874634 + 10.0 * 8.416277885437012
Epoch 1520, val loss: 0.5475647449493408
Epoch 1530, training loss: 84.59886169433594 = 0.47831636667251587 + 10.0 * 8.412054061889648
Epoch 1530, val loss: 0.5471542477607727
Epoch 1540, training loss: 84.58051300048828 = 0.47716036438941956 + 10.0 * 8.410335540771484
Epoch 1540, val loss: 0.5464606881141663
Epoch 1550, training loss: 84.56966400146484 = 0.4760062098503113 + 10.0 * 8.4093656539917
Epoch 1550, val loss: 0.5459989309310913
Epoch 1560, training loss: 84.56407165527344 = 0.47485238313674927 + 10.0 * 8.40892219543457
Epoch 1560, val loss: 0.545437753200531
Epoch 1570, training loss: 84.57243347167969 = 0.47369059920310974 + 10.0 * 8.409873962402344
Epoch 1570, val loss: 0.5448800325393677
Epoch 1580, training loss: 84.56555938720703 = 0.47251415252685547 + 10.0 * 8.40930461883545
Epoch 1580, val loss: 0.5442397594451904
Epoch 1590, training loss: 84.55481719970703 = 0.471341609954834 + 10.0 * 8.408347129821777
Epoch 1590, val loss: 0.5436824560165405
Epoch 1600, training loss: 84.54750061035156 = 0.47016310691833496 + 10.0 * 8.407733917236328
Epoch 1600, val loss: 0.5431490540504456
Epoch 1610, training loss: 84.56218719482422 = 0.46900099515914917 + 10.0 * 8.409318923950195
Epoch 1610, val loss: 0.5425345301628113
Epoch 1620, training loss: 84.54390716552734 = 0.46781226992607117 + 10.0 * 8.407609939575195
Epoch 1620, val loss: 0.5420535802841187
Epoch 1630, training loss: 84.58064270019531 = 0.46664080023765564 + 10.0 * 8.411399841308594
Epoch 1630, val loss: 0.5413601398468018
Epoch 1640, training loss: 84.53263092041016 = 0.46543994545936584 + 10.0 * 8.406719207763672
Epoch 1640, val loss: 0.5410635471343994
Epoch 1650, training loss: 84.5187759399414 = 0.4642794132232666 + 10.0 * 8.405449867248535
Epoch 1650, val loss: 0.5403862595558167
Epoch 1660, training loss: 84.51497650146484 = 0.4631138741970062 + 10.0 * 8.405186653137207
Epoch 1660, val loss: 0.5399060845375061
Epoch 1670, training loss: 84.50727844238281 = 0.4619481563568115 + 10.0 * 8.404533386230469
Epoch 1670, val loss: 0.5394030213356018
Epoch 1680, training loss: 84.50444793701172 = 0.4607839584350586 + 10.0 * 8.404366493225098
Epoch 1680, val loss: 0.5388768911361694
Epoch 1690, training loss: 84.52871704101562 = 0.45961135625839233 + 10.0 * 8.40691089630127
Epoch 1690, val loss: 0.5384809374809265
Epoch 1700, training loss: 84.50782775878906 = 0.4584360420703888 + 10.0 * 8.404939651489258
Epoch 1700, val loss: 0.5377593636512756
Epoch 1710, training loss: 84.53194427490234 = 0.457248330116272 + 10.0 * 8.407469749450684
Epoch 1710, val loss: 0.5372889637947083
Epoch 1720, training loss: 84.49870300292969 = 0.45605745911598206 + 10.0 * 8.404264450073242
Epoch 1720, val loss: 0.5366442203521729
Epoch 1730, training loss: 84.47946166992188 = 0.4548906981945038 + 10.0 * 8.402457237243652
Epoch 1730, val loss: 0.5361517667770386
Epoch 1740, training loss: 84.47484588623047 = 0.45372602343559265 + 10.0 * 8.402112007141113
Epoch 1740, val loss: 0.5356724858283997
Epoch 1750, training loss: 84.472900390625 = 0.45256346464157104 + 10.0 * 8.402033805847168
Epoch 1750, val loss: 0.5351268649101257
Epoch 1760, training loss: 84.49053192138672 = 0.45139411091804504 + 10.0 * 8.403913497924805
Epoch 1760, val loss: 0.5346269607543945
Epoch 1770, training loss: 84.4777603149414 = 0.4502159357070923 + 10.0 * 8.402753829956055
Epoch 1770, val loss: 0.5340325832366943
Epoch 1780, training loss: 84.45858001708984 = 0.44903844594955444 + 10.0 * 8.400954246520996
Epoch 1780, val loss: 0.5334286093711853
Epoch 1790, training loss: 84.45217895507812 = 0.44787487387657166 + 10.0 * 8.400430679321289
Epoch 1790, val loss: 0.5329212546348572
Epoch 1800, training loss: 84.4487533569336 = 0.4467102885246277 + 10.0 * 8.4002046585083
Epoch 1800, val loss: 0.5324108004570007
Epoch 1810, training loss: 84.44906616210938 = 0.44554492831230164 + 10.0 * 8.400352478027344
Epoch 1810, val loss: 0.5319563746452332
Epoch 1820, training loss: 84.46137237548828 = 0.4443718492984772 + 10.0 * 8.401700019836426
Epoch 1820, val loss: 0.5314740538597107
Epoch 1830, training loss: 84.45199584960938 = 0.44321033358573914 + 10.0 * 8.40087890625
Epoch 1830, val loss: 0.530828058719635
Epoch 1840, training loss: 84.43556213378906 = 0.44204169511795044 + 10.0 * 8.399352073669434
Epoch 1840, val loss: 0.530371904373169
Epoch 1850, training loss: 84.5091781616211 = 0.4408904016017914 + 10.0 * 8.406828880310059
Epoch 1850, val loss: 0.5296375155448914
Epoch 1860, training loss: 84.44709777832031 = 0.4396843910217285 + 10.0 * 8.400741577148438
Epoch 1860, val loss: 0.5294560194015503
Epoch 1870, training loss: 84.42940521240234 = 0.4385274648666382 + 10.0 * 8.399087905883789
Epoch 1870, val loss: 0.5288066864013672
Epoch 1880, training loss: 84.4145736694336 = 0.43737316131591797 + 10.0 * 8.397720336914062
Epoch 1880, val loss: 0.528367280960083
Epoch 1890, training loss: 84.40740203857422 = 0.4362177848815918 + 10.0 * 8.39711856842041
Epoch 1890, val loss: 0.5278825759887695
Epoch 1900, training loss: 84.40254974365234 = 0.43506544828414917 + 10.0 * 8.396748542785645
Epoch 1900, val loss: 0.5274176597595215
Epoch 1910, training loss: 84.39794158935547 = 0.4339042603969574 + 10.0 * 8.396403312683105
Epoch 1910, val loss: 0.5269326567649841
Epoch 1920, training loss: 84.39500427246094 = 0.43273985385894775 + 10.0 * 8.39622688293457
Epoch 1920, val loss: 0.5264790058135986
Epoch 1930, training loss: 84.45630645751953 = 0.43156248331069946 + 10.0 * 8.402474403381348
Epoch 1930, val loss: 0.5261182188987732
Epoch 1940, training loss: 84.41255187988281 = 0.4303896129131317 + 10.0 * 8.398216247558594
Epoch 1940, val loss: 0.5254068970680237
Epoch 1950, training loss: 84.3868637084961 = 0.42920780181884766 + 10.0 * 8.39576530456543
Epoch 1950, val loss: 0.5250880718231201
Epoch 1960, training loss: 84.3814926147461 = 0.42805302143096924 + 10.0 * 8.395343780517578
Epoch 1960, val loss: 0.5245782136917114
Epoch 1970, training loss: 84.37972259521484 = 0.4268912374973297 + 10.0 * 8.395283699035645
Epoch 1970, val loss: 0.5241461992263794
Epoch 1980, training loss: 84.44015502929688 = 0.42571985721588135 + 10.0 * 8.401443481445312
Epoch 1980, val loss: 0.5237783193588257
Epoch 1990, training loss: 84.38719940185547 = 0.4245355427265167 + 10.0 * 8.396265983581543
Epoch 1990, val loss: 0.5230618119239807
Epoch 2000, training loss: 84.37062072753906 = 0.4233650863170624 + 10.0 * 8.394725799560547
Epoch 2000, val loss: 0.5227755904197693
Epoch 2010, training loss: 84.3623275756836 = 0.4222087860107422 + 10.0 * 8.394011497497559
Epoch 2010, val loss: 0.5222306251525879
Epoch 2020, training loss: 84.35774230957031 = 0.4210491180419922 + 10.0 * 8.393669128417969
Epoch 2020, val loss: 0.5218412280082703
Epoch 2030, training loss: 84.40003204345703 = 0.41988682746887207 + 10.0 * 8.398015022277832
Epoch 2030, val loss: 0.5213030576705933
Epoch 2040, training loss: 84.36799621582031 = 0.418683797121048 + 10.0 * 8.394930839538574
Epoch 2040, val loss: 0.5209956765174866
Epoch 2050, training loss: 84.35286712646484 = 0.4175078272819519 + 10.0 * 8.393535614013672
Epoch 2050, val loss: 0.5205379128456116
Epoch 2060, training loss: 84.343017578125 = 0.416326105594635 + 10.0 * 8.392668724060059
Epoch 2060, val loss: 0.5201036334037781
Epoch 2070, training loss: 84.33976745605469 = 0.4151441752910614 + 10.0 * 8.392461776733398
Epoch 2070, val loss: 0.5197261571884155
Epoch 2080, training loss: 84.36792755126953 = 0.41395220160484314 + 10.0 * 8.395397186279297
Epoch 2080, val loss: 0.5194080471992493
Epoch 2090, training loss: 84.3401870727539 = 0.4127645790576935 + 10.0 * 8.392742156982422
Epoch 2090, val loss: 0.5187498331069946
Epoch 2100, training loss: 84.35572052001953 = 0.4115672707557678 + 10.0 * 8.394414901733398
Epoch 2100, val loss: 0.5184879899024963
Epoch 2110, training loss: 84.3394775390625 = 0.4103674590587616 + 10.0 * 8.392910957336426
Epoch 2110, val loss: 0.5179031491279602
Epoch 2120, training loss: 84.32988739013672 = 0.40917474031448364 + 10.0 * 8.392071723937988
Epoch 2120, val loss: 0.5174547433853149
Epoch 2130, training loss: 84.31908416748047 = 0.4079911708831787 + 10.0 * 8.391109466552734
Epoch 2130, val loss: 0.5170692205429077
Epoch 2140, training loss: 84.31813049316406 = 0.4068027138710022 + 10.0 * 8.391133308410645
Epoch 2140, val loss: 0.5166921615600586
Epoch 2150, training loss: 84.32389831542969 = 0.4056091904640198 + 10.0 * 8.391828536987305
Epoch 2150, val loss: 0.5163077712059021
Epoch 2160, training loss: 84.32290649414062 = 0.4044100046157837 + 10.0 * 8.391849517822266
Epoch 2160, val loss: 0.5158482193946838
Epoch 2170, training loss: 84.35404968261719 = 0.4032144546508789 + 10.0 * 8.3950834274292
Epoch 2170, val loss: 0.5152725577354431
Epoch 2180, training loss: 84.31417846679688 = 0.401989609003067 + 10.0 * 8.391218185424805
Epoch 2180, val loss: 0.5151063203811646
Epoch 2190, training loss: 84.29922485351562 = 0.4007948040962219 + 10.0 * 8.389842987060547
Epoch 2190, val loss: 0.5146043300628662
Epoch 2200, training loss: 84.29475402832031 = 0.3995993137359619 + 10.0 * 8.38951587677002
Epoch 2200, val loss: 0.5142898559570312
Epoch 2210, training loss: 84.29164123535156 = 0.3984033763408661 + 10.0 * 8.389323234558105
Epoch 2210, val loss: 0.5139335989952087
Epoch 2220, training loss: 84.28787994384766 = 0.3972070813179016 + 10.0 * 8.389066696166992
Epoch 2220, val loss: 0.5135250687599182
Epoch 2230, training loss: 84.29545593261719 = 0.39600670337677 + 10.0 * 8.389945030212402
Epoch 2230, val loss: 0.5131417512893677
Epoch 2240, training loss: 84.32787322998047 = 0.39478620886802673 + 10.0 * 8.393308639526367
Epoch 2240, val loss: 0.5128053426742554
Epoch 2250, training loss: 84.27980041503906 = 0.39355364441871643 + 10.0 * 8.38862419128418
Epoch 2250, val loss: 0.5123366117477417
Epoch 2260, training loss: 84.27937316894531 = 0.39234381914138794 + 10.0 * 8.388703346252441
Epoch 2260, val loss: 0.5120894312858582
Epoch 2270, training loss: 84.27312469482422 = 0.39114242792129517 + 10.0 * 8.388197898864746
Epoch 2270, val loss: 0.5116162300109863
Epoch 2280, training loss: 84.2692642211914 = 0.38993531465530396 + 10.0 * 8.387932777404785
Epoch 2280, val loss: 0.5112456679344177
Epoch 2290, training loss: 84.31015014648438 = 0.3887273073196411 + 10.0 * 8.392142295837402
Epoch 2290, val loss: 0.5107359290122986
Epoch 2300, training loss: 84.27584075927734 = 0.3874832093715668 + 10.0 * 8.388835906982422
Epoch 2300, val loss: 0.510657548904419
Epoch 2310, training loss: 84.26322937011719 = 0.3862614035606384 + 10.0 * 8.387697219848633
Epoch 2310, val loss: 0.5102311372756958
Epoch 2320, training loss: 84.25657653808594 = 0.38503578305244446 + 10.0 * 8.387154579162598
Epoch 2320, val loss: 0.5100274682044983
Epoch 2330, training loss: 84.25405883789062 = 0.3838088810443878 + 10.0 * 8.387024879455566
Epoch 2330, val loss: 0.5096560120582581
Epoch 2340, training loss: 84.28699493408203 = 0.3825782239437103 + 10.0 * 8.39044189453125
Epoch 2340, val loss: 0.5093622803688049
Epoch 2350, training loss: 84.25289916992188 = 0.3813190162181854 + 10.0 * 8.387158393859863
Epoch 2350, val loss: 0.5091731548309326
Epoch 2360, training loss: 84.2521743774414 = 0.3800749182701111 + 10.0 * 8.38720989227295
Epoch 2360, val loss: 0.5088995695114136
Epoch 2370, training loss: 84.24986267089844 = 0.37883251905441284 + 10.0 * 8.387103080749512
Epoch 2370, val loss: 0.5085975527763367
Epoch 2380, training loss: 84.2358169555664 = 0.37758868932724 + 10.0 * 8.385823249816895
Epoch 2380, val loss: 0.5081962943077087
Epoch 2390, training loss: 84.23126220703125 = 0.3763411343097687 + 10.0 * 8.385492324829102
Epoch 2390, val loss: 0.5079126954078674
Epoch 2400, training loss: 84.23124694824219 = 0.3750915229320526 + 10.0 * 8.385615348815918
Epoch 2400, val loss: 0.5076960325241089
Epoch 2410, training loss: 84.28070831298828 = 0.37383031845092773 + 10.0 * 8.390687942504883
Epoch 2410, val loss: 0.507590115070343
Epoch 2420, training loss: 84.2316665649414 = 0.3725578486919403 + 10.0 * 8.385910987854004
Epoch 2420, val loss: 0.5071269273757935
Epoch 2430, training loss: 84.21935272216797 = 0.3713032901287079 + 10.0 * 8.384805679321289
Epoch 2430, val loss: 0.5068343877792358
Epoch 2440, training loss: 84.21231842041016 = 0.3700430989265442 + 10.0 * 8.384227752685547
Epoch 2440, val loss: 0.5066210031509399
Epoch 2450, training loss: 84.20891571044922 = 0.36878401041030884 + 10.0 * 8.384013175964355
Epoch 2450, val loss: 0.5063778162002563
Epoch 2460, training loss: 84.21159362792969 = 0.3675146996974945 + 10.0 * 8.384407997131348
Epoch 2460, val loss: 0.5062272548675537
Epoch 2470, training loss: 84.24386596679688 = 0.3662341833114624 + 10.0 * 8.387763023376465
Epoch 2470, val loss: 0.5060358047485352
Epoch 2480, training loss: 84.219970703125 = 0.36497390270233154 + 10.0 * 8.385499954223633
Epoch 2480, val loss: 0.5053830742835999
Epoch 2490, training loss: 84.22691345214844 = 0.36368805170059204 + 10.0 * 8.386322975158691
Epoch 2490, val loss: 0.5052585601806641
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7955352612886859
0.8126494240382526
=== training gcn model ===
Epoch 0, training loss: 106.92509460449219 = 1.1021833419799805 + 10.0 * 10.582291603088379
Epoch 0, val loss: 1.1014972925186157
Epoch 10, training loss: 106.9173355102539 = 1.0968104600906372 + 10.0 * 10.582052230834961
Epoch 10, val loss: 1.096186876296997
Epoch 20, training loss: 106.90145874023438 = 1.091299295425415 + 10.0 * 10.581015586853027
Epoch 20, val loss: 1.0906881093978882
Epoch 30, training loss: 106.85140991210938 = 1.0853557586669922 + 10.0 * 10.576604843139648
Epoch 30, val loss: 1.084750771522522
Epoch 40, training loss: 106.66905212402344 = 1.0790127515792847 + 10.0 * 10.559003829956055
Epoch 40, val loss: 1.0784541368484497
Epoch 50, training loss: 106.06703186035156 = 1.0722169876098633 + 10.0 * 10.499481201171875
Epoch 50, val loss: 1.0717355012893677
Epoch 60, training loss: 104.32723999023438 = 1.0657448768615723 + 10.0 * 10.326149940490723
Epoch 60, val loss: 1.0655268430709839
Epoch 70, training loss: 100.15742492675781 = 1.0600117444992065 + 10.0 * 9.909741401672363
Epoch 70, val loss: 1.0600337982177734
Epoch 80, training loss: 96.37649536132812 = 1.0544487237930298 + 10.0 * 9.532204627990723
Epoch 80, val loss: 1.0547934770584106
Epoch 90, training loss: 95.28699493408203 = 1.0495805740356445 + 10.0 * 9.423741340637207
Epoch 90, val loss: 1.050125002861023
Epoch 100, training loss: 94.12118530273438 = 1.0447460412979126 + 10.0 * 9.30764389038086
Epoch 100, val loss: 1.045417070388794
Epoch 110, training loss: 93.39940643310547 = 1.0399560928344727 + 10.0 * 9.235944747924805
Epoch 110, val loss: 1.040701150894165
Epoch 120, training loss: 93.1219253540039 = 1.0352789163589478 + 10.0 * 9.208664894104004
Epoch 120, val loss: 1.0360885858535767
Epoch 130, training loss: 92.78607940673828 = 1.0308663845062256 + 10.0 * 9.175520896911621
Epoch 130, val loss: 1.031775951385498
Epoch 140, training loss: 92.3006591796875 = 1.027055263519287 + 10.0 * 9.127360343933105
Epoch 140, val loss: 1.0281041860580444
Epoch 150, training loss: 91.6337890625 = 1.024096965789795 + 10.0 * 9.060969352722168
Epoch 150, val loss: 1.0252896547317505
Epoch 160, training loss: 91.09941864013672 = 1.0216453075408936 + 10.0 * 9.007777214050293
Epoch 160, val loss: 1.0228315591812134
Epoch 170, training loss: 90.93711853027344 = 1.0184645652770996 + 10.0 * 8.991865158081055
Epoch 170, val loss: 1.0194885730743408
Epoch 180, training loss: 90.74821472167969 = 1.0135815143585205 + 10.0 * 8.97346305847168
Epoch 180, val loss: 1.014699101448059
Epoch 190, training loss: 90.48782348632812 = 1.0088980197906494 + 10.0 * 8.947893142700195
Epoch 190, val loss: 1.0103559494018555
Epoch 200, training loss: 90.11631774902344 = 1.0049084424972534 + 10.0 * 8.911141395568848
Epoch 200, val loss: 1.0065962076187134
Epoch 210, training loss: 89.59075927734375 = 1.0010895729064941 + 10.0 * 8.858966827392578
Epoch 210, val loss: 1.002922773361206
Epoch 220, training loss: 89.16355895996094 = 0.997195303440094 + 10.0 * 8.81663703918457
Epoch 220, val loss: 0.9991123080253601
Epoch 230, training loss: 88.85281372070312 = 0.9926314353942871 + 10.0 * 8.786018371582031
Epoch 230, val loss: 0.9944231510162354
Epoch 240, training loss: 88.4913101196289 = 0.9874972105026245 + 10.0 * 8.750381469726562
Epoch 240, val loss: 0.9893726706504822
Epoch 250, training loss: 88.17765045166016 = 0.9825485348701477 + 10.0 * 8.719510078430176
Epoch 250, val loss: 0.984611451625824
Epoch 260, training loss: 87.94436645507812 = 0.9770976901054382 + 10.0 * 8.69672679901123
Epoch 260, val loss: 0.9792442321777344
Epoch 270, training loss: 87.7515869140625 = 0.9708279371261597 + 10.0 * 8.678075790405273
Epoch 270, val loss: 0.9730380177497864
Epoch 280, training loss: 87.58355712890625 = 0.9637527465820312 + 10.0 * 8.661980628967285
Epoch 280, val loss: 0.9661216139793396
Epoch 290, training loss: 87.41187286376953 = 0.9562916159629822 + 10.0 * 8.64555835723877
Epoch 290, val loss: 0.9588398337364197
Epoch 300, training loss: 87.22552490234375 = 0.9487565755844116 + 10.0 * 8.627676963806152
Epoch 300, val loss: 0.9515430331230164
Epoch 310, training loss: 87.1551742553711 = 0.9410989880561829 + 10.0 * 8.621407508850098
Epoch 310, val loss: 0.9441829919815063
Epoch 320, training loss: 86.94139099121094 = 0.9330894947052002 + 10.0 * 8.600830078125
Epoch 320, val loss: 0.9363148212432861
Epoch 330, training loss: 86.8301773071289 = 0.924748420715332 + 10.0 * 8.590542793273926
Epoch 330, val loss: 0.9282307624816895
Epoch 340, training loss: 86.73616027832031 = 0.9161624312400818 + 10.0 * 8.581999778747559
Epoch 340, val loss: 0.9199144840240479
Epoch 350, training loss: 86.65541076660156 = 0.9073972105979919 + 10.0 * 8.574801445007324
Epoch 350, val loss: 0.9114301800727844
Epoch 360, training loss: 86.5718994140625 = 0.8987130522727966 + 10.0 * 8.5673189163208
Epoch 360, val loss: 0.9030366539955139
Epoch 370, training loss: 86.507080078125 = 0.8901029825210571 + 10.0 * 8.561697006225586
Epoch 370, val loss: 0.894666850566864
Epoch 380, training loss: 86.4148178100586 = 0.8813817501068115 + 10.0 * 8.553343772888184
Epoch 380, val loss: 0.8862438797950745
Epoch 390, training loss: 86.35640716552734 = 0.8726962208747864 + 10.0 * 8.548371315002441
Epoch 390, val loss: 0.8778810501098633
Epoch 400, training loss: 86.28269958496094 = 0.8639840483665466 + 10.0 * 8.541872024536133
Epoch 400, val loss: 0.8695068955421448
Epoch 410, training loss: 86.2249755859375 = 0.8552713394165039 + 10.0 * 8.536970138549805
Epoch 410, val loss: 0.8611071109771729
Epoch 420, training loss: 86.18253326416016 = 0.8465366363525391 + 10.0 * 8.533599853515625
Epoch 420, val loss: 0.8527269959449768
Epoch 430, training loss: 86.12129211425781 = 0.8378168344497681 + 10.0 * 8.528347969055176
Epoch 430, val loss: 0.8443140983581543
Epoch 440, training loss: 86.07147979736328 = 0.8291298151016235 + 10.0 * 8.524234771728516
Epoch 440, val loss: 0.8359724879264832
Epoch 450, training loss: 86.02318572998047 = 0.8204777836799622 + 10.0 * 8.520270347595215
Epoch 450, val loss: 0.827663004398346
Epoch 460, training loss: 85.97513580322266 = 0.8118518590927124 + 10.0 * 8.516328811645508
Epoch 460, val loss: 0.8193838000297546
Epoch 470, training loss: 85.92973327636719 = 0.8032703399658203 + 10.0 * 8.512646675109863
Epoch 470, val loss: 0.8111377358436584
Epoch 480, training loss: 85.88919830322266 = 0.7946829795837402 + 10.0 * 8.509450912475586
Epoch 480, val loss: 0.8029151558876038
Epoch 490, training loss: 85.8639144897461 = 0.7861435413360596 + 10.0 * 8.507777214050293
Epoch 490, val loss: 0.7946941256523132
Epoch 500, training loss: 85.80672454833984 = 0.7777311205863953 + 10.0 * 8.502899169921875
Epoch 500, val loss: 0.7866101861000061
Epoch 510, training loss: 85.76058197021484 = 0.7693510055541992 + 10.0 * 8.499123573303223
Epoch 510, val loss: 0.778608500957489
Epoch 520, training loss: 85.71510314941406 = 0.7610598802566528 + 10.0 * 8.495404243469238
Epoch 520, val loss: 0.7706580758094788
Epoch 530, training loss: 85.67382049560547 = 0.7528279423713684 + 10.0 * 8.492098808288574
Epoch 530, val loss: 0.7628005743026733
Epoch 540, training loss: 85.63323974609375 = 0.7446728944778442 + 10.0 * 8.488856315612793
Epoch 540, val loss: 0.7550266981124878
Epoch 550, training loss: 85.59776306152344 = 0.7366062998771667 + 10.0 * 8.486116409301758
Epoch 550, val loss: 0.7473509311676025
Epoch 560, training loss: 85.61754608154297 = 0.7286162972450256 + 10.0 * 8.488893508911133
Epoch 560, val loss: 0.7397095561027527
Epoch 570, training loss: 85.53108215332031 = 0.720707356929779 + 10.0 * 8.481037139892578
Epoch 570, val loss: 0.7322883009910583
Epoch 580, training loss: 85.49775695800781 = 0.7130410075187683 + 10.0 * 8.478471755981445
Epoch 580, val loss: 0.7250970602035522
Epoch 590, training loss: 85.46968078613281 = 0.7055515646934509 + 10.0 * 8.476412773132324
Epoch 590, val loss: 0.7180719971656799
Epoch 600, training loss: 85.5168685913086 = 0.6982263922691345 + 10.0 * 8.481863975524902
Epoch 600, val loss: 0.7112176418304443
Epoch 610, training loss: 85.4133529663086 = 0.6910247802734375 + 10.0 * 8.472232818603516
Epoch 610, val loss: 0.7045946717262268
Epoch 620, training loss: 85.38645935058594 = 0.6841097474098206 + 10.0 * 8.470234870910645
Epoch 620, val loss: 0.6982669234275818
Epoch 630, training loss: 85.36222076416016 = 0.6774386167526245 + 10.0 * 8.468478202819824
Epoch 630, val loss: 0.6922000050544739
Epoch 640, training loss: 85.33302307128906 = 0.670991063117981 + 10.0 * 8.466203689575195
Epoch 640, val loss: 0.6863669753074646
Epoch 650, training loss: 85.30762481689453 = 0.6647548079490662 + 10.0 * 8.464286804199219
Epoch 650, val loss: 0.6807637214660645
Epoch 660, training loss: 85.28378295898438 = 0.6587404608726501 + 10.0 * 8.462504386901855
Epoch 660, val loss: 0.6754171848297119
Epoch 670, training loss: 85.32440948486328 = 0.652948260307312 + 10.0 * 8.467145919799805
Epoch 670, val loss: 0.6703518629074097
Epoch 680, training loss: 85.24915313720703 = 0.647330105304718 + 10.0 * 8.460182189941406
Epoch 680, val loss: 0.6654606461524963
Epoch 690, training loss: 85.21630096435547 = 0.6419808864593506 + 10.0 * 8.45743179321289
Epoch 690, val loss: 0.6608569622039795
Epoch 700, training loss: 85.1938247680664 = 0.6368688941001892 + 10.0 * 8.455695152282715
Epoch 700, val loss: 0.656532347202301
Epoch 710, training loss: 85.18541717529297 = 0.6319555640220642 + 10.0 * 8.45534610748291
Epoch 710, val loss: 0.6524037718772888
Epoch 720, training loss: 85.14090728759766 = 0.6272521018981934 + 10.0 * 8.45136547088623
Epoch 720, val loss: 0.6485423445701599
Epoch 730, training loss: 85.11825561523438 = 0.6227890253067017 + 10.0 * 8.449546813964844
Epoch 730, val loss: 0.6449004411697388
Epoch 740, training loss: 85.09486389160156 = 0.6184993982315063 + 10.0 * 8.447636604309082
Epoch 740, val loss: 0.6414847373962402
Epoch 750, training loss: 85.07576751708984 = 0.6143875122070312 + 10.0 * 8.446138381958008
Epoch 750, val loss: 0.6382583975791931
Epoch 760, training loss: 85.05846405029297 = 0.6104371547698975 + 10.0 * 8.444803237915039
Epoch 760, val loss: 0.6351974606513977
Epoch 770, training loss: 85.0921401977539 = 0.6066182851791382 + 10.0 * 8.448552131652832
Epoch 770, val loss: 0.6323299407958984
Epoch 780, training loss: 85.02662658691406 = 0.6029240489006042 + 10.0 * 8.442370414733887
Epoch 780, val loss: 0.6295441389083862
Epoch 790, training loss: 84.99089050292969 = 0.5994437336921692 + 10.0 * 8.4391450881958
Epoch 790, val loss: 0.6269624829292297
Epoch 800, training loss: 84.97371673583984 = 0.5961127877235413 + 10.0 * 8.437760353088379
Epoch 800, val loss: 0.6245691776275635
Epoch 810, training loss: 84.9524154663086 = 0.5929014086723328 + 10.0 * 8.435951232910156
Epoch 810, val loss: 0.6223191618919373
Epoch 820, training loss: 84.9352798461914 = 0.5897939801216125 + 10.0 * 8.434549331665039
Epoch 820, val loss: 0.6201557517051697
Epoch 830, training loss: 84.91851043701172 = 0.5867877006530762 + 10.0 * 8.433172225952148
Epoch 830, val loss: 0.6181232929229736
Epoch 840, training loss: 84.95824432373047 = 0.5838631987571716 + 10.0 * 8.437438011169434
Epoch 840, val loss: 0.6162416338920593
Epoch 850, training loss: 84.89136505126953 = 0.5809952020645142 + 10.0 * 8.431036949157715
Epoch 850, val loss: 0.6143153309822083
Epoch 860, training loss: 84.87776947021484 = 0.5782399773597717 + 10.0 * 8.429952621459961
Epoch 860, val loss: 0.6125281453132629
Epoch 870, training loss: 84.86410522460938 = 0.5755911469459534 + 10.0 * 8.428851127624512
Epoch 870, val loss: 0.610843300819397
Epoch 880, training loss: 84.86970520019531 = 0.573010265827179 + 10.0 * 8.429669380187988
Epoch 880, val loss: 0.6092324256896973
Epoch 890, training loss: 84.83904266357422 = 0.5704822540283203 + 10.0 * 8.42685604095459
Epoch 890, val loss: 0.6077216863632202
Epoch 900, training loss: 84.82967376708984 = 0.5680471062660217 + 10.0 * 8.426162719726562
Epoch 900, val loss: 0.6062445640563965
Epoch 910, training loss: 84.81732177734375 = 0.5657053589820862 + 10.0 * 8.425161361694336
Epoch 910, val loss: 0.6048672795295715
Epoch 920, training loss: 84.80485534667969 = 0.5634154677391052 + 10.0 * 8.42414379119873
Epoch 920, val loss: 0.6035643219947815
Epoch 930, training loss: 84.79300689697266 = 0.5611810088157654 + 10.0 * 8.423182487487793
Epoch 930, val loss: 0.6022737622261047
Epoch 940, training loss: 84.80467224121094 = 0.5590017437934875 + 10.0 * 8.424567222595215
Epoch 940, val loss: 0.601058840751648
Epoch 950, training loss: 84.81786346435547 = 0.556831955909729 + 10.0 * 8.426103591918945
Epoch 950, val loss: 0.5997312068939209
Epoch 960, training loss: 84.76895141601562 = 0.554741621017456 + 10.0 * 8.42142105102539
Epoch 960, val loss: 0.5986078381538391
Epoch 970, training loss: 84.75823211669922 = 0.5527235865592957 + 10.0 * 8.420550346374512
Epoch 970, val loss: 0.5974898338317871
Epoch 980, training loss: 84.75054168701172 = 0.5507527589797974 + 10.0 * 8.419979095458984
Epoch 980, val loss: 0.5964516997337341
Epoch 990, training loss: 84.74056243896484 = 0.5488196611404419 + 10.0 * 8.419174194335938
Epoch 990, val loss: 0.5953893661499023
Epoch 1000, training loss: 84.7325210571289 = 0.5469171404838562 + 10.0 * 8.418560028076172
Epoch 1000, val loss: 0.5943334102630615
Epoch 1010, training loss: 84.74617767333984 = 0.5450456738471985 + 10.0 * 8.420113563537598
Epoch 1010, val loss: 0.5932703018188477
Epoch 1020, training loss: 84.7312240600586 = 0.5431727766990662 + 10.0 * 8.418805122375488
Epoch 1020, val loss: 0.5923590660095215
Epoch 1030, training loss: 84.7207260131836 = 0.5413479208946228 + 10.0 * 8.417937278747559
Epoch 1030, val loss: 0.5912637114524841
Epoch 1040, training loss: 84.70600128173828 = 0.5395824909210205 + 10.0 * 8.416642189025879
Epoch 1040, val loss: 0.590257465839386
Epoch 1050, training loss: 84.69622039794922 = 0.5378513336181641 + 10.0 * 8.415837287902832
Epoch 1050, val loss: 0.5893325805664062
Epoch 1060, training loss: 84.68883514404297 = 0.5361418724060059 + 10.0 * 8.41526985168457
Epoch 1060, val loss: 0.5884000658988953
Epoch 1070, training loss: 84.6811294555664 = 0.5344500541687012 + 10.0 * 8.414668083190918
Epoch 1070, val loss: 0.5874242186546326
Epoch 1080, training loss: 84.67448425292969 = 0.5327764749526978 + 10.0 * 8.41417121887207
Epoch 1080, val loss: 0.5864934921264648
Epoch 1090, training loss: 84.71221160888672 = 0.5311194658279419 + 10.0 * 8.418108940124512
Epoch 1090, val loss: 0.5855719447135925
Epoch 1100, training loss: 84.68699645996094 = 0.5294387936592102 + 10.0 * 8.415755271911621
Epoch 1100, val loss: 0.5846171379089355
Epoch 1110, training loss: 84.66381072998047 = 0.5278028845787048 + 10.0 * 8.41360092163086
Epoch 1110, val loss: 0.5835813283920288
Epoch 1120, training loss: 84.6482162475586 = 0.5262065529823303 + 10.0 * 8.412200927734375
Epoch 1120, val loss: 0.5826901197433472
Epoch 1130, training loss: 84.64152526855469 = 0.5246363282203674 + 10.0 * 8.411688804626465
Epoch 1130, val loss: 0.5818153619766235
Epoch 1140, training loss: 84.63523864746094 = 0.5230766534805298 + 10.0 * 8.411215782165527
Epoch 1140, val loss: 0.5809009671211243
Epoch 1150, training loss: 84.6292724609375 = 0.5215247869491577 + 10.0 * 8.410775184631348
Epoch 1150, val loss: 0.5799825191497803
Epoch 1160, training loss: 84.6562728881836 = 0.5199827551841736 + 10.0 * 8.413629531860352
Epoch 1160, val loss: 0.5790584087371826
Epoch 1170, training loss: 84.6267318725586 = 0.5184171199798584 + 10.0 * 8.410831451416016
Epoch 1170, val loss: 0.5781100392341614
Epoch 1180, training loss: 84.61956787109375 = 0.5168981552124023 + 10.0 * 8.410266876220703
Epoch 1180, val loss: 0.5771987438201904
Epoch 1190, training loss: 84.60620880126953 = 0.5153896808624268 + 10.0 * 8.409082412719727
Epoch 1190, val loss: 0.5762532353401184
Epoch 1200, training loss: 84.59890747070312 = 0.5138992071151733 + 10.0 * 8.408500671386719
Epoch 1200, val loss: 0.5753199458122253
Epoch 1210, training loss: 84.59918212890625 = 0.512410581111908 + 10.0 * 8.408677101135254
Epoch 1210, val loss: 0.5743621587753296
Epoch 1220, training loss: 84.62406158447266 = 0.510918378829956 + 10.0 * 8.411314010620117
Epoch 1220, val loss: 0.5734403729438782
Epoch 1230, training loss: 84.58887481689453 = 0.5094223618507385 + 10.0 * 8.40794563293457
Epoch 1230, val loss: 0.572530210018158
Epoch 1240, training loss: 84.5753402709961 = 0.5079540014266968 + 10.0 * 8.40673828125
Epoch 1240, val loss: 0.5715547800064087
Epoch 1250, training loss: 84.56975555419922 = 0.5064924955368042 + 10.0 * 8.406326293945312
Epoch 1250, val loss: 0.5706217885017395
Epoch 1260, training loss: 84.5630874633789 = 0.505039632320404 + 10.0 * 8.405804634094238
Epoch 1260, val loss: 0.5696684718132019
Epoch 1270, training loss: 84.55754852294922 = 0.503588855266571 + 10.0 * 8.405396461486816
Epoch 1270, val loss: 0.5687316060066223
Epoch 1280, training loss: 84.57019805908203 = 0.502138078212738 + 10.0 * 8.406805992126465
Epoch 1280, val loss: 0.5677469372749329
Epoch 1290, training loss: 84.57869720458984 = 0.5006645321846008 + 10.0 * 8.407803535461426
Epoch 1290, val loss: 0.5668096542358398
Epoch 1300, training loss: 84.55298614501953 = 0.4991873502731323 + 10.0 * 8.405380249023438
Epoch 1300, val loss: 0.5657414793968201
Epoch 1310, training loss: 84.54308319091797 = 0.497758686542511 + 10.0 * 8.404532432556152
Epoch 1310, val loss: 0.5648078322410583
Epoch 1320, training loss: 84.53206634521484 = 0.4963465631008148 + 10.0 * 8.403572082519531
Epoch 1320, val loss: 0.563871443271637
Epoch 1330, training loss: 84.52462005615234 = 0.4949376881122589 + 10.0 * 8.402968406677246
Epoch 1330, val loss: 0.562879204750061
Epoch 1340, training loss: 84.51905822753906 = 0.4935309886932373 + 10.0 * 8.402552604675293
Epoch 1340, val loss: 0.5619111061096191
Epoch 1350, training loss: 84.51390838623047 = 0.4921208322048187 + 10.0 * 8.402178764343262
Epoch 1350, val loss: 0.560940682888031
Epoch 1360, training loss: 84.56842803955078 = 0.49070480465888977 + 10.0 * 8.407772064208984
Epoch 1360, val loss: 0.5599436163902283
Epoch 1370, training loss: 84.54356384277344 = 0.48926106095314026 + 10.0 * 8.405430793762207
Epoch 1370, val loss: 0.5589631795883179
Epoch 1380, training loss: 84.50006866455078 = 0.48784565925598145 + 10.0 * 8.401222229003906
Epoch 1380, val loss: 0.5579782128334045
Epoch 1390, training loss: 84.49748992919922 = 0.48645275831222534 + 10.0 * 8.401103973388672
Epoch 1390, val loss: 0.5569290518760681
Epoch 1400, training loss: 84.48800659179688 = 0.4850637912750244 + 10.0 * 8.400294303894043
Epoch 1400, val loss: 0.5559908151626587
Epoch 1410, training loss: 84.48263549804688 = 0.48367297649383545 + 10.0 * 8.399896621704102
Epoch 1410, val loss: 0.5550075769424438
Epoch 1420, training loss: 84.47821044921875 = 0.482276976108551 + 10.0 * 8.399593353271484
Epoch 1420, val loss: 0.5540172457695007
Epoch 1430, training loss: 84.52769470214844 = 0.48087313771247864 + 10.0 * 8.404682159423828
Epoch 1430, val loss: 0.5530734658241272
Epoch 1440, training loss: 84.47391510009766 = 0.4794401526451111 + 10.0 * 8.399447441101074
Epoch 1440, val loss: 0.5520055294036865
Epoch 1450, training loss: 84.46905517578125 = 0.47803354263305664 + 10.0 * 8.399102210998535
Epoch 1450, val loss: 0.5510175228118896
Epoch 1460, training loss: 84.45779418945312 = 0.4766395092010498 + 10.0 * 8.398115158081055
Epoch 1460, val loss: 0.5500531196594238
Epoch 1470, training loss: 84.4538803100586 = 0.47524604201316833 + 10.0 * 8.397863388061523
Epoch 1470, val loss: 0.549072802066803
Epoch 1480, training loss: 84.47721862792969 = 0.4738471210002899 + 10.0 * 8.400337219238281
Epoch 1480, val loss: 0.5481352806091309
Epoch 1490, training loss: 84.44447326660156 = 0.4724290072917938 + 10.0 * 8.397204399108887
Epoch 1490, val loss: 0.5469976663589478
Epoch 1500, training loss: 84.43958282470703 = 0.4710306227207184 + 10.0 * 8.396855354309082
Epoch 1500, val loss: 0.5460714101791382
Epoch 1510, training loss: 84.43367767333984 = 0.46964508295059204 + 10.0 * 8.396403312683105
Epoch 1510, val loss: 0.5450296401977539
Epoch 1520, training loss: 84.42875671386719 = 0.4682631194591522 + 10.0 * 8.396049499511719
Epoch 1520, val loss: 0.5440753102302551
Epoch 1530, training loss: 84.42452239990234 = 0.46687835454940796 + 10.0 * 8.395764350891113
Epoch 1530, val loss: 0.543084979057312
Epoch 1540, training loss: 84.42571258544922 = 0.46548935770988464 + 10.0 * 8.396021842956543
Epoch 1540, val loss: 0.5421242117881775
Epoch 1550, training loss: 84.4445571899414 = 0.4640892446041107 + 10.0 * 8.398046493530273
Epoch 1550, val loss: 0.5411227941513062
Epoch 1560, training loss: 84.430419921875 = 0.46267327666282654 + 10.0 * 8.396774291992188
Epoch 1560, val loss: 0.5400897860527039
Epoch 1570, training loss: 84.40827178955078 = 0.461282879114151 + 10.0 * 8.394699096679688
Epoch 1570, val loss: 0.5391356348991394
Epoch 1580, training loss: 84.40314483642578 = 0.4599035680294037 + 10.0 * 8.39432430267334
Epoch 1580, val loss: 0.5381777286529541
Epoch 1590, training loss: 84.3972396850586 = 0.45852795243263245 + 10.0 * 8.393871307373047
Epoch 1590, val loss: 0.5371789336204529
Epoch 1600, training loss: 84.40632629394531 = 0.45715248584747314 + 10.0 * 8.394917488098145
Epoch 1600, val loss: 0.5362515449523926
Epoch 1610, training loss: 84.4107894897461 = 0.45575812458992004 + 10.0 * 8.395503044128418
Epoch 1610, val loss: 0.5352952480316162
Epoch 1620, training loss: 84.39450073242188 = 0.45437443256378174 + 10.0 * 8.394012451171875
Epoch 1620, val loss: 0.5342574715614319
Epoch 1630, training loss: 84.38360595703125 = 0.4530067443847656 + 10.0 * 8.393060684204102
Epoch 1630, val loss: 0.533364474773407
Epoch 1640, training loss: 84.37489318847656 = 0.4516477584838867 + 10.0 * 8.392324447631836
Epoch 1640, val loss: 0.5323858261108398
Epoch 1650, training loss: 84.37531280517578 = 0.4502900242805481 + 10.0 * 8.392502784729004
Epoch 1650, val loss: 0.5315109491348267
Epoch 1660, training loss: 84.3796157836914 = 0.4489247798919678 + 10.0 * 8.39306926727295
Epoch 1660, val loss: 0.5306025147438049
Epoch 1670, training loss: 84.3658447265625 = 0.44755762815475464 + 10.0 * 8.391828536987305
Epoch 1670, val loss: 0.5296428203582764
Epoch 1680, training loss: 84.36054229736328 = 0.44621041417121887 + 10.0 * 8.391432762145996
Epoch 1680, val loss: 0.5286759734153748
Epoch 1690, training loss: 84.37960815429688 = 0.44486555457115173 + 10.0 * 8.393474578857422
Epoch 1690, val loss: 0.5278252959251404
Epoch 1700, training loss: 84.37139129638672 = 0.44350481033325195 + 10.0 * 8.392788887023926
Epoch 1700, val loss: 0.5268152356147766
Epoch 1710, training loss: 84.3519287109375 = 0.4421602487564087 + 10.0 * 8.390976905822754
Epoch 1710, val loss: 0.5260231494903564
Epoch 1720, training loss: 84.3441162109375 = 0.4408228397369385 + 10.0 * 8.390329360961914
Epoch 1720, val loss: 0.5250668525695801
Epoch 1730, training loss: 84.33895111083984 = 0.43949368596076965 + 10.0 * 8.389945983886719
Epoch 1730, val loss: 0.5242050290107727
Epoch 1740, training loss: 84.33808135986328 = 0.43816208839416504 + 10.0 * 8.389991760253906
Epoch 1740, val loss: 0.5232837796211243
Epoch 1750, training loss: 84.34890747070312 = 0.43683090806007385 + 10.0 * 8.391207695007324
Epoch 1750, val loss: 0.5224560499191284
Epoch 1760, training loss: 84.3833236694336 = 0.43549206852912903 + 10.0 * 8.394783020019531
Epoch 1760, val loss: 0.5216201543807983
Epoch 1770, training loss: 84.33968353271484 = 0.43415367603302 + 10.0 * 8.39055347442627
Epoch 1770, val loss: 0.5205944776535034
Epoch 1780, training loss: 84.31926727294922 = 0.4328412115573883 + 10.0 * 8.388643264770508
Epoch 1780, val loss: 0.5198308825492859
Epoch 1790, training loss: 84.31100463867188 = 0.4315353333950043 + 10.0 * 8.387947082519531
Epoch 1790, val loss: 0.5189568400382996
Epoch 1800, training loss: 84.30707550048828 = 0.43023255467414856 + 10.0 * 8.38768482208252
Epoch 1800, val loss: 0.5181651711463928
Epoch 1810, training loss: 84.30604553222656 = 0.42892956733703613 + 10.0 * 8.387711524963379
Epoch 1810, val loss: 0.5173559784889221
Epoch 1820, training loss: 84.352294921875 = 0.4276221990585327 + 10.0 * 8.392467498779297
Epoch 1820, val loss: 0.516566276550293
Epoch 1830, training loss: 84.30713653564453 = 0.4262876510620117 + 10.0 * 8.38808536529541
Epoch 1830, val loss: 0.5157879590988159
Epoch 1840, training loss: 84.29794311523438 = 0.42498841881752014 + 10.0 * 8.387295722961426
Epoch 1840, val loss: 0.5150381922721863
Epoch 1850, training loss: 84.28881072998047 = 0.4236913323402405 + 10.0 * 8.38651180267334
Epoch 1850, val loss: 0.5142614841461182
Epoch 1860, training loss: 84.28414916992188 = 0.4223972260951996 + 10.0 * 8.386175155639648
Epoch 1860, val loss: 0.5135285258293152
Epoch 1870, training loss: 84.29621124267578 = 0.42110130190849304 + 10.0 * 8.387510299682617
Epoch 1870, val loss: 0.512823760509491
Epoch 1880, training loss: 84.2904052734375 = 0.41979286074638367 + 10.0 * 8.38706111907959
Epoch 1880, val loss: 0.5119688510894775
Epoch 1890, training loss: 84.27902221679688 = 0.41848817467689514 + 10.0 * 8.386053085327148
Epoch 1890, val loss: 0.511371910572052
Epoch 1900, training loss: 84.27554321289062 = 0.4171985685825348 + 10.0 * 8.385834693908691
Epoch 1900, val loss: 0.5106028318405151
Epoch 1910, training loss: 84.26529693603516 = 0.415921688079834 + 10.0 * 8.384937286376953
Epoch 1910, val loss: 0.5099698901176453
Epoch 1920, training loss: 84.25880432128906 = 0.4146442115306854 + 10.0 * 8.384416580200195
Epoch 1920, val loss: 0.5092687606811523
Epoch 1930, training loss: 84.2551498413086 = 0.41336962580680847 + 10.0 * 8.384178161621094
Epoch 1930, val loss: 0.5086085796356201
Epoch 1940, training loss: 84.29285430908203 = 0.41209420561790466 + 10.0 * 8.388075828552246
Epoch 1940, val loss: 0.5080153346061707
Epoch 1950, training loss: 84.26231384277344 = 0.4107952415943146 + 10.0 * 8.385151863098145
Epoch 1950, val loss: 0.5073538422584534
Epoch 1960, training loss: 84.26173400878906 = 0.4095189571380615 + 10.0 * 8.385221481323242
Epoch 1960, val loss: 0.5066653490066528
Epoch 1970, training loss: 84.2421646118164 = 0.4082695543766022 + 10.0 * 8.383389472961426
Epoch 1970, val loss: 0.5061021447181702
Epoch 1980, training loss: 84.23654174804688 = 0.40702441334724426 + 10.0 * 8.382951736450195
Epoch 1980, val loss: 0.5054736137390137
Epoch 1990, training loss: 84.23249816894531 = 0.40577805042266846 + 10.0 * 8.382672309875488
Epoch 1990, val loss: 0.5049259662628174
Epoch 2000, training loss: 84.23274230957031 = 0.40452829003334045 + 10.0 * 8.382822036743164
Epoch 2000, val loss: 0.5042688250541687
Epoch 2010, training loss: 84.2904052734375 = 0.40327081084251404 + 10.0 * 8.388712882995605
Epoch 2010, val loss: 0.5037075877189636
Epoch 2020, training loss: 84.22538757324219 = 0.40200942754745483 + 10.0 * 8.38233757019043
Epoch 2020, val loss: 0.5031124949455261
Epoch 2030, training loss: 84.22608947753906 = 0.40077465772628784 + 10.0 * 8.382532119750977
Epoch 2030, val loss: 0.5026752948760986
Epoch 2040, training loss: 84.21393585205078 = 0.3995407223701477 + 10.0 * 8.381439208984375
Epoch 2040, val loss: 0.5020976662635803
Epoch 2050, training loss: 84.20980072021484 = 0.39831164479255676 + 10.0 * 8.381148338317871
Epoch 2050, val loss: 0.5015509724617004
Epoch 2060, training loss: 84.20689392089844 = 0.39708495140075684 + 10.0 * 8.380980491638184
Epoch 2060, val loss: 0.5010620355606079
Epoch 2070, training loss: 84.24089050292969 = 0.39585912227630615 + 10.0 * 8.384503364562988
Epoch 2070, val loss: 0.5006105303764343
Epoch 2080, training loss: 84.21668243408203 = 0.3946332037448883 + 10.0 * 8.38220500946045
Epoch 2080, val loss: 0.4999554753303528
Epoch 2090, training loss: 84.19735717773438 = 0.3934062123298645 + 10.0 * 8.38039493560791
Epoch 2090, val loss: 0.49953585863113403
Epoch 2100, training loss: 84.199951171875 = 0.3921983242034912 + 10.0 * 8.380775451660156
Epoch 2100, val loss: 0.4990890324115753
Epoch 2110, training loss: 84.26182556152344 = 0.39098256826400757 + 10.0 * 8.387084007263184
Epoch 2110, val loss: 0.4986831247806549
Epoch 2120, training loss: 84.206298828125 = 0.38976600766181946 + 10.0 * 8.381653785705566
Epoch 2120, val loss: 0.49807968735694885
Epoch 2130, training loss: 84.18655395507812 = 0.3885626196861267 + 10.0 * 8.379798889160156
Epoch 2130, val loss: 0.49776601791381836
Epoch 2140, training loss: 84.17924499511719 = 0.387369304895401 + 10.0 * 8.37918758392334
Epoch 2140, val loss: 0.4973795413970947
Epoch 2150, training loss: 84.17558288574219 = 0.3861768841743469 + 10.0 * 8.37894058227539
Epoch 2150, val loss: 0.4969650208950043
Epoch 2160, training loss: 84.1717758178711 = 0.38498592376708984 + 10.0 * 8.378679275512695
Epoch 2160, val loss: 0.49654829502105713
Epoch 2170, training loss: 84.1689224243164 = 0.38379523158073425 + 10.0 * 8.378512382507324
Epoch 2170, val loss: 0.4961375892162323
Epoch 2180, training loss: 84.20204162597656 = 0.3826046288013458 + 10.0 * 8.381943702697754
Epoch 2180, val loss: 0.4956825375556946
Epoch 2190, training loss: 84.18180847167969 = 0.381409227848053 + 10.0 * 8.380040168762207
Epoch 2190, val loss: 0.495540589094162
Epoch 2200, training loss: 84.16680908203125 = 0.3802173137664795 + 10.0 * 8.37865924835205
Epoch 2200, val loss: 0.49501922726631165
Epoch 2210, training loss: 84.15692901611328 = 0.3790443539619446 + 10.0 * 8.377788543701172
Epoch 2210, val loss: 0.494777113199234
Epoch 2220, training loss: 84.15202331542969 = 0.37786900997161865 + 10.0 * 8.377415657043457
Epoch 2220, val loss: 0.49446821212768555
Epoch 2230, training loss: 84.152099609375 = 0.376695841550827 + 10.0 * 8.377540588378906
Epoch 2230, val loss: 0.49418625235557556
Epoch 2240, training loss: 84.2376937866211 = 0.37552410364151 + 10.0 * 8.38621711730957
Epoch 2240, val loss: 0.49402958154678345
Epoch 2250, training loss: 84.19841003417969 = 0.3743285536766052 + 10.0 * 8.382408142089844
Epoch 2250, val loss: 0.4935969114303589
Epoch 2260, training loss: 84.14750671386719 = 0.37315016984939575 + 10.0 * 8.377435684204102
Epoch 2260, val loss: 0.4933004379272461
Epoch 2270, training loss: 84.1384048461914 = 0.37199679017066956 + 10.0 * 8.376641273498535
Epoch 2270, val loss: 0.49311116337776184
Epoch 2280, training loss: 84.1360855102539 = 0.3708440363407135 + 10.0 * 8.376523971557617
Epoch 2280, val loss: 0.49286168813705444
Epoch 2290, training loss: 84.13127136230469 = 0.36968445777893066 + 10.0 * 8.376158714294434
Epoch 2290, val loss: 0.4926890432834625
Epoch 2300, training loss: 84.12779998779297 = 0.36852437257766724 + 10.0 * 8.375927925109863
Epoch 2300, val loss: 0.4924966096878052
Epoch 2310, training loss: 84.12525939941406 = 0.36735525727272034 + 10.0 * 8.3757905960083
Epoch 2310, val loss: 0.4922982156276703
Epoch 2320, training loss: 84.14139556884766 = 0.3661862313747406 + 10.0 * 8.377520561218262
Epoch 2320, val loss: 0.492169588804245
Epoch 2330, training loss: 84.12342071533203 = 0.36500024795532227 + 10.0 * 8.375842094421387
Epoch 2330, val loss: 0.49187180399894714
Epoch 2340, training loss: 84.11661529541016 = 0.36382269859313965 + 10.0 * 8.375279426574707
Epoch 2340, val loss: 0.49166756868362427
Epoch 2350, training loss: 84.1134262084961 = 0.36264967918395996 + 10.0 * 8.375078201293945
Epoch 2350, val loss: 0.4914289116859436
Epoch 2360, training loss: 84.1099624633789 = 0.3614760637283325 + 10.0 * 8.374849319458008
Epoch 2360, val loss: 0.49129459261894226
Epoch 2370, training loss: 84.1072006225586 = 0.36030149459838867 + 10.0 * 8.374690055847168
Epoch 2370, val loss: 0.4911328852176666
Epoch 2380, training loss: 84.11280822753906 = 0.3591293394565582 + 10.0 * 8.375368118286133
Epoch 2380, val loss: 0.4910014569759369
Epoch 2390, training loss: 84.11576080322266 = 0.357948899269104 + 10.0 * 8.375781059265137
Epoch 2390, val loss: 0.49083274602890015
Epoch 2400, training loss: 84.15827941894531 = 0.35677120089530945 + 10.0 * 8.38015079498291
Epoch 2400, val loss: 0.49058377742767334
Epoch 2410, training loss: 84.10971069335938 = 0.35557904839515686 + 10.0 * 8.37541389465332
Epoch 2410, val loss: 0.49049612879753113
Epoch 2420, training loss: 84.09683990478516 = 0.35440772771835327 + 10.0 * 8.374242782592773
Epoch 2420, val loss: 0.4903615713119507
Epoch 2430, training loss: 84.09202575683594 = 0.3532467484474182 + 10.0 * 8.37387752532959
Epoch 2430, val loss: 0.49038809537887573
Epoch 2440, training loss: 84.08834075927734 = 0.35208576917648315 + 10.0 * 8.373624801635742
Epoch 2440, val loss: 0.4902702867984772
Epoch 2450, training loss: 84.0843734741211 = 0.3509224057197571 + 10.0 * 8.373345375061035
Epoch 2450, val loss: 0.49021294713020325
Epoch 2460, training loss: 84.08212280273438 = 0.349759966135025 + 10.0 * 8.373235702514648
Epoch 2460, val loss: 0.49019286036491394
Epoch 2470, training loss: 84.08272552490234 = 0.34860002994537354 + 10.0 * 8.373412132263184
Epoch 2470, val loss: 0.490172415971756
Epoch 2480, training loss: 84.12895965576172 = 0.34744393825531006 + 10.0 * 8.378151893615723
Epoch 2480, val loss: 0.4901558756828308
Epoch 2490, training loss: 84.093017578125 = 0.3462655544281006 + 10.0 * 8.374674797058105
Epoch 2490, val loss: 0.4903075397014618
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7945205479452054
0.8152575527059336
The final CL Acc:0.79740, 0.00337, The final GNN Acc:0.81415, 0.00110
raw graph: torch.Size([2, 88648])
add edge: torch.Size([2, 110826])
remove edge: torch.Size([2, 66874])
updated graph: torch.Size([2, 89052])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 106.90225219726562 = 1.0794264078140259 + 10.0 * 10.582282066345215
Epoch 0, val loss: 1.07963228225708
Epoch 10, training loss: 106.89429473876953 = 1.0756494998931885 + 10.0 * 10.581864356994629
Epoch 10, val loss: 1.0758029222488403
Epoch 20, training loss: 106.86875915527344 = 1.0715718269348145 + 10.0 * 10.579718589782715
Epoch 20, val loss: 1.0716859102249146
Epoch 30, training loss: 106.75904083251953 = 1.0672893524169922 + 10.0 * 10.569174766540527
Epoch 30, val loss: 1.0673707723617554
Epoch 40, training loss: 106.33114624023438 = 1.0628812313079834 + 10.0 * 10.526826858520508
Epoch 40, val loss: 1.0629327297210693
Epoch 50, training loss: 105.15328216552734 = 1.0581750869750977 + 10.0 * 10.409510612487793
Epoch 50, val loss: 1.0582119226455688
Epoch 60, training loss: 103.15422821044922 = 1.053625464439392 + 10.0 * 10.210060119628906
Epoch 60, val loss: 1.0536420345306396
Epoch 70, training loss: 101.26205444335938 = 1.0489500761032104 + 10.0 * 10.021310806274414
Epoch 70, val loss: 1.0488797426223755
Epoch 80, training loss: 98.24784088134766 = 1.0433869361877441 + 10.0 * 9.72044563293457
Epoch 80, val loss: 1.0432761907577515
Epoch 90, training loss: 95.82260131835938 = 1.0371983051300049 + 10.0 * 9.478540420532227
Epoch 90, val loss: 1.0371973514556885
Epoch 100, training loss: 93.83406066894531 = 1.0322881937026978 + 10.0 * 9.280177116394043
Epoch 100, val loss: 1.0325987339019775
Epoch 110, training loss: 92.48078918457031 = 1.0284634828567505 + 10.0 * 9.145232200622559
Epoch 110, val loss: 1.0289827585220337
Epoch 120, training loss: 91.73094177246094 = 1.024430513381958 + 10.0 * 9.070651054382324
Epoch 120, val loss: 1.0251579284667969
Epoch 130, training loss: 90.8922348022461 = 1.0198386907577515 + 10.0 * 8.987239837646484
Epoch 130, val loss: 1.0208131074905396
Epoch 140, training loss: 90.27888488769531 = 1.015423059463501 + 10.0 * 8.926345825195312
Epoch 140, val loss: 1.0166282653808594
Epoch 150, training loss: 89.81190490722656 = 1.0113149881362915 + 10.0 * 8.880059242248535
Epoch 150, val loss: 1.012690782546997
Epoch 160, training loss: 89.28527069091797 = 1.0073391199111938 + 10.0 * 8.82779312133789
Epoch 160, val loss: 1.0088547468185425
Epoch 170, training loss: 88.86397552490234 = 1.0032521486282349 + 10.0 * 8.786072731018066
Epoch 170, val loss: 1.0048826932907104
Epoch 180, training loss: 88.52706146240234 = 0.9986822009086609 + 10.0 * 8.752838134765625
Epoch 180, val loss: 1.0003639459609985
Epoch 190, training loss: 88.25569915771484 = 0.9935950636863708 + 10.0 * 8.726210594177246
Epoch 190, val loss: 0.995418906211853
Epoch 200, training loss: 88.03900909423828 = 0.9879875779151917 + 10.0 * 8.70510196685791
Epoch 200, val loss: 0.9899803400039673
Epoch 210, training loss: 87.8482666015625 = 0.981844961643219 + 10.0 * 8.68664264678955
Epoch 210, val loss: 0.9840334057807922
Epoch 220, training loss: 87.69995880126953 = 0.9750906825065613 + 10.0 * 8.672487258911133
Epoch 220, val loss: 0.9774973392486572
Epoch 230, training loss: 87.6119384765625 = 0.9676660895347595 + 10.0 * 8.664426803588867
Epoch 230, val loss: 0.9703298807144165
Epoch 240, training loss: 87.47799682617188 = 0.9597282409667969 + 10.0 * 8.651826858520508
Epoch 240, val loss: 0.9626767039299011
Epoch 250, training loss: 87.3709487915039 = 0.9514891505241394 + 10.0 * 8.641945838928223
Epoch 250, val loss: 0.9547549486160278
Epoch 260, training loss: 87.2555923461914 = 0.9429689645767212 + 10.0 * 8.631261825561523
Epoch 260, val loss: 0.9465765953063965
Epoch 270, training loss: 87.13566589355469 = 0.9341146349906921 + 10.0 * 8.620155334472656
Epoch 270, val loss: 0.9380785226821899
Epoch 280, training loss: 87.08042907714844 = 0.9248672723770142 + 10.0 * 8.615556716918945
Epoch 280, val loss: 0.9292184114456177
Epoch 290, training loss: 86.92082977294922 = 0.9151657223701477 + 10.0 * 8.600565910339355
Epoch 290, val loss: 0.9198615550994873
Epoch 300, training loss: 86.82559204101562 = 0.9050275087356567 + 10.0 * 8.592056274414062
Epoch 300, val loss: 0.9101250767707825
Epoch 310, training loss: 86.7366943359375 = 0.8944792747497559 + 10.0 * 8.584221839904785
Epoch 310, val loss: 0.8999953866004944
Epoch 320, training loss: 86.65755462646484 = 0.8835657835006714 + 10.0 * 8.577398300170898
Epoch 320, val loss: 0.8895572423934937
Epoch 330, training loss: 86.55250549316406 = 0.8723531365394592 + 10.0 * 8.568015098571777
Epoch 330, val loss: 0.8788058757781982
Epoch 340, training loss: 86.45169830322266 = 0.8609473705291748 + 10.0 * 8.559075355529785
Epoch 340, val loss: 0.8678785562515259
Epoch 350, training loss: 86.35240173339844 = 0.8493545055389404 + 10.0 * 8.550304412841797
Epoch 350, val loss: 0.856789767742157
Epoch 360, training loss: 86.26022338867188 = 0.8375087380409241 + 10.0 * 8.542271614074707
Epoch 360, val loss: 0.845451295375824
Epoch 370, training loss: 86.17096710205078 = 0.8254404067993164 + 10.0 * 8.534552574157715
Epoch 370, val loss: 0.833909809589386
Epoch 380, training loss: 86.08325958251953 = 0.813266396522522 + 10.0 * 8.526999473571777
Epoch 380, val loss: 0.8222631812095642
Epoch 390, training loss: 86.02791595458984 = 0.800927460193634 + 10.0 * 8.522699356079102
Epoch 390, val loss: 0.8104836344718933
Epoch 400, training loss: 85.91974639892578 = 0.7885950207710266 + 10.0 * 8.513114929199219
Epoch 400, val loss: 0.7986679673194885
Epoch 410, training loss: 85.84626770019531 = 0.7763047218322754 + 10.0 * 8.506996154785156
Epoch 410, val loss: 0.7869275808334351
Epoch 420, training loss: 85.77420043945312 = 0.7640261650085449 + 10.0 * 8.501017570495605
Epoch 420, val loss: 0.7752233147621155
Epoch 430, training loss: 85.75010681152344 = 0.7517708539962769 + 10.0 * 8.499834060668945
Epoch 430, val loss: 0.7635294795036316
Epoch 440, training loss: 85.6574935913086 = 0.7394798994064331 + 10.0 * 8.491801261901855
Epoch 440, val loss: 0.7517985105514526
Epoch 450, training loss: 85.60063934326172 = 0.7273051142692566 + 10.0 * 8.487333297729492
Epoch 450, val loss: 0.7402194738388062
Epoch 460, training loss: 85.54874420166016 = 0.7152900099754333 + 10.0 * 8.483345985412598
Epoch 460, val loss: 0.7287929654121399
Epoch 470, training loss: 85.54656219482422 = 0.7034181356430054 + 10.0 * 8.484314918518066
Epoch 470, val loss: 0.7175238132476807
Epoch 480, training loss: 85.46753692626953 = 0.691730797290802 + 10.0 * 8.477580070495605
Epoch 480, val loss: 0.7064393162727356
Epoch 490, training loss: 85.42115783691406 = 0.6803615689277649 + 10.0 * 8.474080085754395
Epoch 490, val loss: 0.6956875324249268
Epoch 500, training loss: 85.38539123535156 = 0.6692997217178345 + 10.0 * 8.471609115600586
Epoch 500, val loss: 0.685245931148529
Epoch 510, training loss: 85.37074279785156 = 0.6585620641708374 + 10.0 * 8.47121810913086
Epoch 510, val loss: 0.6751662492752075
Epoch 520, training loss: 85.32275390625 = 0.6481378078460693 + 10.0 * 8.467461585998535
Epoch 520, val loss: 0.6653302311897278
Epoch 530, training loss: 85.28968811035156 = 0.6381110548973083 + 10.0 * 8.465158462524414
Epoch 530, val loss: 0.6559500098228455
Epoch 540, training loss: 85.26895904541016 = 0.6284891366958618 + 10.0 * 8.4640474319458
Epoch 540, val loss: 0.6469559073448181
Epoch 550, training loss: 85.2530517578125 = 0.6192003488540649 + 10.0 * 8.463384628295898
Epoch 550, val loss: 0.6383184194564819
Epoch 560, training loss: 85.2061538696289 = 0.6103694438934326 + 10.0 * 8.459578514099121
Epoch 560, val loss: 0.6301186680793762
Epoch 570, training loss: 85.17845916748047 = 0.6019672155380249 + 10.0 * 8.457649230957031
Epoch 570, val loss: 0.6223464608192444
Epoch 580, training loss: 85.15303039550781 = 0.5939483642578125 + 10.0 * 8.455907821655273
Epoch 580, val loss: 0.6149671673774719
Epoch 590, training loss: 85.12825775146484 = 0.5862898826599121 + 10.0 * 8.45419692993164
Epoch 590, val loss: 0.6079670190811157
Epoch 600, training loss: 85.1053466796875 = 0.5789831280708313 + 10.0 * 8.45263671875
Epoch 600, val loss: 0.6013106107711792
Epoch 610, training loss: 85.12364959716797 = 0.5719919204711914 + 10.0 * 8.45516586303711
Epoch 610, val loss: 0.5949867367744446
Epoch 620, training loss: 85.06858825683594 = 0.5653218626976013 + 10.0 * 8.450326919555664
Epoch 620, val loss: 0.5889989733695984
Epoch 630, training loss: 85.04740905761719 = 0.5591108202934265 + 10.0 * 8.448829650878906
Epoch 630, val loss: 0.5834429264068604
Epoch 640, training loss: 85.02125549316406 = 0.5532636046409607 + 10.0 * 8.446799278259277
Epoch 640, val loss: 0.5782458186149597
Epoch 650, training loss: 84.99998474121094 = 0.5477179288864136 + 10.0 * 8.445226669311523
Epoch 650, val loss: 0.5733651518821716
Epoch 660, training loss: 84.978515625 = 0.5424453020095825 + 10.0 * 8.443607330322266
Epoch 660, val loss: 0.5687635540962219
Epoch 670, training loss: 84.9661865234375 = 0.5374350547790527 + 10.0 * 8.442874908447266
Epoch 670, val loss: 0.5644417405128479
Epoch 680, training loss: 84.9646987915039 = 0.5326334834098816 + 10.0 * 8.443206787109375
Epoch 680, val loss: 0.5602438449859619
Epoch 690, training loss: 84.92754364013672 = 0.528087854385376 + 10.0 * 8.439945220947266
Epoch 690, val loss: 0.5563849210739136
Epoch 700, training loss: 84.90137481689453 = 0.5238456130027771 + 10.0 * 8.437753677368164
Epoch 700, val loss: 0.5528383851051331
Epoch 710, training loss: 84.87672424316406 = 0.5198392271995544 + 10.0 * 8.435688972473145
Epoch 710, val loss: 0.5494999289512634
Epoch 720, training loss: 84.85411071777344 = 0.5160220861434937 + 10.0 * 8.433809280395508
Epoch 720, val loss: 0.5463408827781677
Epoch 730, training loss: 84.8437728881836 = 0.5123855471611023 + 10.0 * 8.433138847351074
Epoch 730, val loss: 0.5433676838874817
Epoch 740, training loss: 84.83274841308594 = 0.508858323097229 + 10.0 * 8.432389259338379
Epoch 740, val loss: 0.5405307412147522
Epoch 750, training loss: 84.79772186279297 = 0.5055338144302368 + 10.0 * 8.429219245910645
Epoch 750, val loss: 0.5378586053848267
Epoch 760, training loss: 84.7777099609375 = 0.5023918747901917 + 10.0 * 8.427531242370605
Epoch 760, val loss: 0.5353575348854065
Epoch 770, training loss: 84.75643920898438 = 0.49940237402915955 + 10.0 * 8.425703048706055
Epoch 770, val loss: 0.5329864025115967
Epoch 780, training loss: 84.75498962402344 = 0.496533066034317 + 10.0 * 8.4258451461792
Epoch 780, val loss: 0.5307353138923645
Epoch 790, training loss: 84.76354217529297 = 0.4937064051628113 + 10.0 * 8.426983833312988
Epoch 790, val loss: 0.5285155773162842
Epoch 800, training loss: 84.71895599365234 = 0.4910023510456085 + 10.0 * 8.422795295715332
Epoch 800, val loss: 0.5264568328857422
Epoch 810, training loss: 84.693603515625 = 0.4884790778160095 + 10.0 * 8.420512199401855
Epoch 810, val loss: 0.5245391726493835
Epoch 820, training loss: 84.67643737792969 = 0.4860702455043793 + 10.0 * 8.419036865234375
Epoch 820, val loss: 0.5227320194244385
Epoch 830, training loss: 84.6603775024414 = 0.4837486743927002 + 10.0 * 8.417662620544434
Epoch 830, val loss: 0.5210062861442566
Epoch 840, training loss: 84.64456176757812 = 0.48150092363357544 + 10.0 * 8.416306495666504
Epoch 840, val loss: 0.519342303276062
Epoch 850, training loss: 84.62963104248047 = 0.4793231189250946 + 10.0 * 8.415030479431152
Epoch 850, val loss: 0.5177517533302307
Epoch 860, training loss: 84.66832733154297 = 0.4771958887577057 + 10.0 * 8.419113159179688
Epoch 860, val loss: 0.5161840319633484
Epoch 870, training loss: 84.64472961425781 = 0.4750552475452423 + 10.0 * 8.416967391967773
Epoch 870, val loss: 0.5146561861038208
Epoch 880, training loss: 84.60034942626953 = 0.47303280234336853 + 10.0 * 8.412732124328613
Epoch 880, val loss: 0.5132034420967102
Epoch 890, training loss: 84.57473754882812 = 0.4711109697818756 + 10.0 * 8.41036319732666
Epoch 890, val loss: 0.5118231177330017
Epoch 900, training loss: 84.56212615966797 = 0.46925315260887146 + 10.0 * 8.409287452697754
Epoch 900, val loss: 0.5105234980583191
Epoch 910, training loss: 84.54876708984375 = 0.4674362242221832 + 10.0 * 8.408132553100586
Epoch 910, val loss: 0.509243905544281
Epoch 920, training loss: 84.61702728271484 = 0.4656478464603424 + 10.0 * 8.415138244628906
Epoch 920, val loss: 0.507945716381073
Epoch 930, training loss: 84.5404281616211 = 0.46382832527160645 + 10.0 * 8.407659530639648
Epoch 930, val loss: 0.5067017674446106
Epoch 940, training loss: 84.51638793945312 = 0.4620976746082306 + 10.0 * 8.405428886413574
Epoch 940, val loss: 0.5054931044578552
Epoch 950, training loss: 84.5030517578125 = 0.46041780710220337 + 10.0 * 8.404263496398926
Epoch 950, val loss: 0.5043172836303711
Epoch 960, training loss: 84.50139617919922 = 0.4587733745574951 + 10.0 * 8.40426254272461
Epoch 960, val loss: 0.5032245516777039
Epoch 970, training loss: 84.4828872680664 = 0.45711761713027954 + 10.0 * 8.40257740020752
Epoch 970, val loss: 0.5020687580108643
Epoch 980, training loss: 84.48030853271484 = 0.4554949104785919 + 10.0 * 8.402481079101562
Epoch 980, val loss: 0.5009530782699585
Epoch 990, training loss: 84.46180725097656 = 0.45392417907714844 + 10.0 * 8.400788307189941
Epoch 990, val loss: 0.49987107515335083
Epoch 1000, training loss: 84.45050048828125 = 0.4523877501487732 + 10.0 * 8.399811744689941
Epoch 1000, val loss: 0.4988305866718292
Epoch 1010, training loss: 84.44622039794922 = 0.4508693516254425 + 10.0 * 8.399535179138184
Epoch 1010, val loss: 0.4978005886077881
Epoch 1020, training loss: 84.46147155761719 = 0.4493309557437897 + 10.0 * 8.401213645935059
Epoch 1020, val loss: 0.49679818749427795
Epoch 1030, training loss: 84.43864440917969 = 0.44779494404792786 + 10.0 * 8.39908504486084
Epoch 1030, val loss: 0.49573656916618347
Epoch 1040, training loss: 84.41981506347656 = 0.44631922245025635 + 10.0 * 8.39734935760498
Epoch 1040, val loss: 0.4947216510772705
Epoch 1050, training loss: 84.40428161621094 = 0.4448718726634979 + 10.0 * 8.395940780639648
Epoch 1050, val loss: 0.4937942624092102
Epoch 1060, training loss: 84.39419555664062 = 0.4434472918510437 + 10.0 * 8.395074844360352
Epoch 1060, val loss: 0.492861270904541
Epoch 1070, training loss: 84.38513946533203 = 0.4420338273048401 + 10.0 * 8.39431095123291
Epoch 1070, val loss: 0.4919239580631256
Epoch 1080, training loss: 84.37602233886719 = 0.44062888622283936 + 10.0 * 8.393539428710938
Epoch 1080, val loss: 0.4910050332546234
Epoch 1090, training loss: 84.39422607421875 = 0.43922901153564453 + 10.0 * 8.395499229431152
Epoch 1090, val loss: 0.4900659918785095
Epoch 1100, training loss: 84.40605926513672 = 0.4377670884132385 + 10.0 * 8.396829605102539
Epoch 1100, val loss: 0.4891742169857025
Epoch 1110, training loss: 84.35316467285156 = 0.43635648488998413 + 10.0 * 8.391680717468262
Epoch 1110, val loss: 0.4882538318634033
Epoch 1120, training loss: 84.34333801269531 = 0.43499955534935 + 10.0 * 8.390833854675293
Epoch 1120, val loss: 0.48731666803359985
Epoch 1130, training loss: 84.33731842041016 = 0.43367013335227966 + 10.0 * 8.390364646911621
Epoch 1130, val loss: 0.48651376366615295
Epoch 1140, training loss: 84.32805633544922 = 0.4323504865169525 + 10.0 * 8.389570236206055
Epoch 1140, val loss: 0.48564261198043823
Epoch 1150, training loss: 84.322265625 = 0.4310356378555298 + 10.0 * 8.38912296295166
Epoch 1150, val loss: 0.4848378002643585
Epoch 1160, training loss: 84.3756103515625 = 0.4297040104866028 + 10.0 * 8.394590377807617
Epoch 1160, val loss: 0.48399102687835693
Epoch 1170, training loss: 84.31280517578125 = 0.4283575415611267 + 10.0 * 8.388444900512695
Epoch 1170, val loss: 0.4831669330596924
Epoch 1180, training loss: 84.30216217041016 = 0.4270550608634949 + 10.0 * 8.387510299682617
Epoch 1180, val loss: 0.482288658618927
Epoch 1190, training loss: 84.29646301269531 = 0.4257788062095642 + 10.0 * 8.387067794799805
Epoch 1190, val loss: 0.4815157651901245
Epoch 1200, training loss: 84.29385375976562 = 0.42451393604278564 + 10.0 * 8.386934280395508
Epoch 1200, val loss: 0.4806925356388092
Epoch 1210, training loss: 84.2889175415039 = 0.4232388138771057 + 10.0 * 8.386568069458008
Epoch 1210, val loss: 0.47993189096450806
Epoch 1220, training loss: 84.27957153320312 = 0.4219702482223511 + 10.0 * 8.385760307312012
Epoch 1220, val loss: 0.47917434573173523
Epoch 1230, training loss: 84.27352142333984 = 0.420722633600235 + 10.0 * 8.385279655456543
Epoch 1230, val loss: 0.4783773422241211
Epoch 1240, training loss: 84.29806518554688 = 0.41947075724601746 + 10.0 * 8.387859344482422
Epoch 1240, val loss: 0.47765159606933594
Epoch 1250, training loss: 84.26412963867188 = 0.4182097017765045 + 10.0 * 8.384592056274414
Epoch 1250, val loss: 0.4768437147140503
Epoch 1260, training loss: 84.25425720214844 = 0.4169822335243225 + 10.0 * 8.383727073669434
Epoch 1260, val loss: 0.4761132597923279
Epoch 1270, training loss: 84.2480239868164 = 0.4157678186893463 + 10.0 * 8.383225440979004
Epoch 1270, val loss: 0.4753704369068146
Epoch 1280, training loss: 84.25750732421875 = 0.41456231474876404 + 10.0 * 8.384294509887695
Epoch 1280, val loss: 0.47463080286979675
Epoch 1290, training loss: 84.25245666503906 = 0.41333502531051636 + 10.0 * 8.383912086486816
Epoch 1290, val loss: 0.4738920331001282
Epoch 1300, training loss: 84.23788452148438 = 0.41211840510368347 + 10.0 * 8.382576942443848
Epoch 1300, val loss: 0.4731426239013672
Epoch 1310, training loss: 84.22761535644531 = 0.4109230637550354 + 10.0 * 8.381669044494629
Epoch 1310, val loss: 0.47238004207611084
Epoch 1320, training loss: 84.22032928466797 = 0.40974393486976624 + 10.0 * 8.381058692932129
Epoch 1320, val loss: 0.4716721177101135
Epoch 1330, training loss: 84.22228240966797 = 0.40856677293777466 + 10.0 * 8.38137149810791
Epoch 1330, val loss: 0.47093623876571655
Epoch 1340, training loss: 84.21791076660156 = 0.4073779881000519 + 10.0 * 8.38105297088623
Epoch 1340, val loss: 0.4702116549015045
Epoch 1350, training loss: 84.20826721191406 = 0.406198650598526 + 10.0 * 8.380207061767578
Epoch 1350, val loss: 0.4695281982421875
Epoch 1360, training loss: 84.23799133300781 = 0.40503206849098206 + 10.0 * 8.383296012878418
Epoch 1360, val loss: 0.46878781914711
Epoch 1370, training loss: 84.21513366699219 = 0.4038311243057251 + 10.0 * 8.38113021850586
Epoch 1370, val loss: 0.468153715133667
Epoch 1380, training loss: 84.18486785888672 = 0.4026678502559662 + 10.0 * 8.378220558166504
Epoch 1380, val loss: 0.4674079716205597
Epoch 1390, training loss: 84.18070983886719 = 0.40152621269226074 + 10.0 * 8.377918243408203
Epoch 1390, val loss: 0.4667413532733917
Epoch 1400, training loss: 84.17237854003906 = 0.4003962576389313 + 10.0 * 8.377198219299316
Epoch 1400, val loss: 0.46605268120765686
Epoch 1410, training loss: 84.1672592163086 = 0.39926865696907043 + 10.0 * 8.376798629760742
Epoch 1410, val loss: 0.4654406011104584
Epoch 1420, training loss: 84.1645736694336 = 0.39814049005508423 + 10.0 * 8.376643180847168
Epoch 1420, val loss: 0.4647715985774994
Epoch 1430, training loss: 84.19086456298828 = 0.39699986577033997 + 10.0 * 8.379385948181152
Epoch 1430, val loss: 0.46410858631134033
Epoch 1440, training loss: 84.18142700195312 = 0.39584437012672424 + 10.0 * 8.378558158874512
Epoch 1440, val loss: 0.46343299746513367
Epoch 1450, training loss: 84.14671325683594 = 0.3947146236896515 + 10.0 * 8.375200271606445
Epoch 1450, val loss: 0.4627723693847656
Epoch 1460, training loss: 84.16211700439453 = 0.39360588788986206 + 10.0 * 8.376851081848145
Epoch 1460, val loss: 0.462129145860672
Epoch 1470, training loss: 84.13383483886719 = 0.39247146248817444 + 10.0 * 8.374135971069336
Epoch 1470, val loss: 0.46151837706565857
Epoch 1480, training loss: 84.13072204589844 = 0.39135628938674927 + 10.0 * 8.373936653137207
Epoch 1480, val loss: 0.4608199894428253
Epoch 1490, training loss: 84.12386322021484 = 0.3902589678764343 + 10.0 * 8.373360633850098
Epoch 1490, val loss: 0.4602630138397217
Epoch 1500, training loss: 84.11509704589844 = 0.38916775584220886 + 10.0 * 8.37259292602539
Epoch 1500, val loss: 0.4596036672592163
Epoch 1510, training loss: 84.10871124267578 = 0.38807716965675354 + 10.0 * 8.372063636779785
Epoch 1510, val loss: 0.45901232957839966
Epoch 1520, training loss: 84.10578155517578 = 0.3869887590408325 + 10.0 * 8.371879577636719
Epoch 1520, val loss: 0.45836716890335083
Epoch 1530, training loss: 84.15947723388672 = 0.38588428497314453 + 10.0 * 8.377359390258789
Epoch 1530, val loss: 0.457750529050827
Epoch 1540, training loss: 84.094970703125 = 0.3847658932209015 + 10.0 * 8.371020317077637
Epoch 1540, val loss: 0.4570848345756531
Epoch 1550, training loss: 84.09324645996094 = 0.38367587327957153 + 10.0 * 8.370957374572754
Epoch 1550, val loss: 0.4564887285232544
Epoch 1560, training loss: 84.08072662353516 = 0.3825986683368683 + 10.0 * 8.369812965393066
Epoch 1560, val loss: 0.4558511972427368
Epoch 1570, training loss: 84.09669494628906 = 0.38152408599853516 + 10.0 * 8.371517181396484
Epoch 1570, val loss: 0.45523086190223694
Epoch 1580, training loss: 84.07455444335938 = 0.3804173767566681 + 10.0 * 8.369413375854492
Epoch 1580, val loss: 0.45457547903060913
Epoch 1590, training loss: 84.06657409667969 = 0.37932640314102173 + 10.0 * 8.368724822998047
Epoch 1590, val loss: 0.4539564549922943
Epoch 1600, training loss: 84.06217956542969 = 0.3782503306865692 + 10.0 * 8.368392944335938
Epoch 1600, val loss: 0.4533189833164215
Epoch 1610, training loss: 84.05545043945312 = 0.3771796226501465 + 10.0 * 8.367826461791992
Epoch 1610, val loss: 0.4527150094509125
Epoch 1620, training loss: 84.06057739257812 = 0.37610650062561035 + 10.0 * 8.368447303771973
Epoch 1620, val loss: 0.45209836959838867
Epoch 1630, training loss: 84.04435729980469 = 0.3750153183937073 + 10.0 * 8.366933822631836
Epoch 1630, val loss: 0.45147040486335754
Epoch 1640, training loss: 84.04029846191406 = 0.37392503023147583 + 10.0 * 8.366637229919434
Epoch 1640, val loss: 0.45084699988365173
Epoch 1650, training loss: 84.06324005126953 = 0.37283727526664734 + 10.0 * 8.369039535522461
Epoch 1650, val loss: 0.45026519894599915
Epoch 1660, training loss: 84.04210662841797 = 0.3717261254787445 + 10.0 * 8.367037773132324
Epoch 1660, val loss: 0.4495251476764679
Epoch 1670, training loss: 84.03028869628906 = 0.3706318438053131 + 10.0 * 8.365965843200684
Epoch 1670, val loss: 0.44892892241477966
Epoch 1680, training loss: 84.02311706542969 = 0.369548499584198 + 10.0 * 8.365357398986816
Epoch 1680, val loss: 0.4482564330101013
Epoch 1690, training loss: 84.0255355834961 = 0.36846765875816345 + 10.0 * 8.365706443786621
Epoch 1690, val loss: 0.4476366937160492
Epoch 1700, training loss: 84.0410385131836 = 0.36736953258514404 + 10.0 * 8.367366790771484
Epoch 1700, val loss: 0.4469606280326843
Epoch 1710, training loss: 84.02214050292969 = 0.3662583529949188 + 10.0 * 8.365588188171387
Epoch 1710, val loss: 0.4463448226451874
Epoch 1720, training loss: 84.00895690917969 = 0.3651602268218994 + 10.0 * 8.3643798828125
Epoch 1720, val loss: 0.4457005262374878
Epoch 1730, training loss: 84.00294494628906 = 0.3640662133693695 + 10.0 * 8.363887786865234
Epoch 1730, val loss: 0.4450759291648865
Epoch 1740, training loss: 84.00348663330078 = 0.36297157406806946 + 10.0 * 8.364051818847656
Epoch 1740, val loss: 0.44445887207984924
Epoch 1750, training loss: 84.01485443115234 = 0.36186543107032776 + 10.0 * 8.365299224853516
Epoch 1750, val loss: 0.44381919503211975
Epoch 1760, training loss: 83.99095916748047 = 0.36074909567832947 + 10.0 * 8.363020896911621
Epoch 1760, val loss: 0.44314059615135193
Epoch 1770, training loss: 83.99834442138672 = 0.35964396595954895 + 10.0 * 8.363870620727539
Epoch 1770, val loss: 0.44249606132507324
Epoch 1780, training loss: 83.98328399658203 = 0.35851573944091797 + 10.0 * 8.36247730255127
Epoch 1780, val loss: 0.441867470741272
Epoch 1790, training loss: 83.98487091064453 = 0.35739871859550476 + 10.0 * 8.362747192382812
Epoch 1790, val loss: 0.4412526488304138
Epoch 1800, training loss: 83.97360229492188 = 0.3562883734703064 + 10.0 * 8.36173152923584
Epoch 1800, val loss: 0.44057413935661316
Epoch 1810, training loss: 83.96721649169922 = 0.35518741607666016 + 10.0 * 8.36120319366455
Epoch 1810, val loss: 0.43997761607170105
Epoch 1820, training loss: 83.96311950683594 = 0.3540826737880707 + 10.0 * 8.3609037399292
Epoch 1820, val loss: 0.43935179710388184
Epoch 1830, training loss: 83.98633575439453 = 0.35297077894210815 + 10.0 * 8.363336563110352
Epoch 1830, val loss: 0.4387526512145996
Epoch 1840, training loss: 83.97700500488281 = 0.35181596875190735 + 10.0 * 8.362519264221191
Epoch 1840, val loss: 0.43803897500038147
Epoch 1850, training loss: 83.96855926513672 = 0.35066840052604675 + 10.0 * 8.361788749694824
Epoch 1850, val loss: 0.43740373849868774
Epoch 1860, training loss: 83.95252227783203 = 0.3495427072048187 + 10.0 * 8.360298156738281
Epoch 1860, val loss: 0.4367930293083191
Epoch 1870, training loss: 83.94690704345703 = 0.34842702746391296 + 10.0 * 8.359848022460938
Epoch 1870, val loss: 0.436149001121521
Epoch 1880, training loss: 83.95175170898438 = 0.34731101989746094 + 10.0 * 8.360444068908691
Epoch 1880, val loss: 0.4355194568634033
Epoch 1890, training loss: 83.9593505859375 = 0.34617891907691956 + 10.0 * 8.36131763458252
Epoch 1890, val loss: 0.4348750114440918
Epoch 1900, training loss: 83.9407958984375 = 0.34504392743110657 + 10.0 * 8.359575271606445
Epoch 1900, val loss: 0.4342510402202606
Epoch 1910, training loss: 83.92845916748047 = 0.34391120076179504 + 10.0 * 8.358454704284668
Epoch 1910, val loss: 0.4336300492286682
Epoch 1920, training loss: 83.92809295654297 = 0.34278032183647156 + 10.0 * 8.35853099822998
Epoch 1920, val loss: 0.4330017566680908
Epoch 1930, training loss: 83.93294525146484 = 0.3416472375392914 + 10.0 * 8.359129905700684
Epoch 1930, val loss: 0.43241190910339355
Epoch 1940, training loss: 83.95742797851562 = 0.3404908776283264 + 10.0 * 8.361693382263184
Epoch 1940, val loss: 0.43178412318229675
Epoch 1950, training loss: 83.91941833496094 = 0.3393239974975586 + 10.0 * 8.358009338378906
Epoch 1950, val loss: 0.431123822927475
Epoch 1960, training loss: 83.91090393066406 = 0.33817583322525024 + 10.0 * 8.35727310180664
Epoch 1960, val loss: 0.4304947555065155
Epoch 1970, training loss: 83.90640258789062 = 0.3370360732078552 + 10.0 * 8.35693645477295
Epoch 1970, val loss: 0.4298689663410187
Epoch 1980, training loss: 83.90338897705078 = 0.33589303493499756 + 10.0 * 8.356749534606934
Epoch 1980, val loss: 0.42924070358276367
Epoch 1990, training loss: 83.96697998046875 = 0.33473774790763855 + 10.0 * 8.363224029541016
Epoch 1990, val loss: 0.4286259114742279
Epoch 2000, training loss: 83.9201431274414 = 0.3335510194301605 + 10.0 * 8.358659744262695
Epoch 2000, val loss: 0.42800799012184143
Epoch 2010, training loss: 83.8931655883789 = 0.33238402009010315 + 10.0 * 8.356078147888184
Epoch 2010, val loss: 0.4273473918437958
Epoch 2020, training loss: 83.8879623413086 = 0.33122411370277405 + 10.0 * 8.355673789978027
Epoch 2020, val loss: 0.4267914295196533
Epoch 2030, training loss: 83.88902282714844 = 0.3300676941871643 + 10.0 * 8.355895042419434
Epoch 2030, val loss: 0.4261825382709503
Epoch 2040, training loss: 83.94200897216797 = 0.32889556884765625 + 10.0 * 8.361310958862305
Epoch 2040, val loss: 0.4255979657173157
Epoch 2050, training loss: 83.89059448242188 = 0.32770055532455444 + 10.0 * 8.356289863586426
Epoch 2050, val loss: 0.42497408390045166
Epoch 2060, training loss: 83.87228393554688 = 0.3265262246131897 + 10.0 * 8.354576110839844
Epoch 2060, val loss: 0.42438721656799316
Epoch 2070, training loss: 83.86589050292969 = 0.32535380125045776 + 10.0 * 8.354053497314453
Epoch 2070, val loss: 0.42376235127449036
Epoch 2080, training loss: 83.86190032958984 = 0.32418012619018555 + 10.0 * 8.353772163391113
Epoch 2080, val loss: 0.4231819212436676
Epoch 2090, training loss: 83.85838317871094 = 0.3229980170726776 + 10.0 * 8.353538513183594
Epoch 2090, val loss: 0.4225611984729767
Epoch 2100, training loss: 83.88512420654297 = 0.3218131363391876 + 10.0 * 8.356330871582031
Epoch 2100, val loss: 0.4219389855861664
Epoch 2110, training loss: 83.87104797363281 = 0.320604532957077 + 10.0 * 8.3550443649292
Epoch 2110, val loss: 0.4214440882205963
Epoch 2120, training loss: 83.85869598388672 = 0.3194049894809723 + 10.0 * 8.35392951965332
Epoch 2120, val loss: 0.4207935631275177
Epoch 2130, training loss: 83.84512329101562 = 0.3182171881198883 + 10.0 * 8.352690696716309
Epoch 2130, val loss: 0.42021143436431885
Epoch 2140, training loss: 83.8384780883789 = 0.31703871488571167 + 10.0 * 8.352144241333008
Epoch 2140, val loss: 0.41968175768852234
Epoch 2150, training loss: 83.8331527709961 = 0.3158596456050873 + 10.0 * 8.351729393005371
Epoch 2150, val loss: 0.419109046459198
Epoch 2160, training loss: 83.84284973144531 = 0.3146740794181824 + 10.0 * 8.35281753540039
Epoch 2160, val loss: 0.41855162382125854
Epoch 2170, training loss: 83.83332061767578 = 0.31346583366394043 + 10.0 * 8.351984977722168
Epoch 2170, val loss: 0.4179651439189911
Epoch 2180, training loss: 83.85267639160156 = 0.31225448846817017 + 10.0 * 8.354042053222656
Epoch 2180, val loss: 0.4174002707004547
Epoch 2190, training loss: 83.82376098632812 = 0.3110470473766327 + 10.0 * 8.35127067565918
Epoch 2190, val loss: 0.4168068468570709
Epoch 2200, training loss: 83.81573486328125 = 0.30984801054000854 + 10.0 * 8.35058879852295
Epoch 2200, val loss: 0.41629162430763245
Epoch 2210, training loss: 83.80917358398438 = 0.30864959955215454 + 10.0 * 8.350052833557129
Epoch 2210, val loss: 0.41577014327049255
Epoch 2220, training loss: 83.8165283203125 = 0.3074515163898468 + 10.0 * 8.350908279418945
Epoch 2220, val loss: 0.41516509652137756
Epoch 2230, training loss: 83.8315658569336 = 0.306233286857605 + 10.0 * 8.352533340454102
Epoch 2230, val loss: 0.4146590232849121
Epoch 2240, training loss: 83.79881286621094 = 0.30501413345336914 + 10.0 * 8.349379539489746
Epoch 2240, val loss: 0.41412124037742615
Epoch 2250, training loss: 83.7933349609375 = 0.30380234122276306 + 10.0 * 8.348953247070312
Epoch 2250, val loss: 0.4136168360710144
Epoch 2260, training loss: 83.790283203125 = 0.30259430408477783 + 10.0 * 8.34876823425293
Epoch 2260, val loss: 0.4130421280860901
Epoch 2270, training loss: 83.7960205078125 = 0.30138474702835083 + 10.0 * 8.34946346282959
Epoch 2270, val loss: 0.412476122379303
Epoch 2280, training loss: 83.81315612792969 = 0.3001589775085449 + 10.0 * 8.351300239562988
Epoch 2280, val loss: 0.4119276702404022
Epoch 2290, training loss: 83.79122924804688 = 0.29894348978996277 + 10.0 * 8.349228858947754
Epoch 2290, val loss: 0.4114701449871063
Epoch 2300, training loss: 83.79078674316406 = 0.2977135479450226 + 10.0 * 8.3493070602417
Epoch 2300, val loss: 0.4109097123146057
Epoch 2310, training loss: 83.77593994140625 = 0.29648539423942566 + 10.0 * 8.347945213317871
Epoch 2310, val loss: 0.4104035794734955
Epoch 2320, training loss: 83.76903533935547 = 0.29526376724243164 + 10.0 * 8.347376823425293
Epoch 2320, val loss: 0.4098878800868988
Epoch 2330, training loss: 83.76324462890625 = 0.2940441370010376 + 10.0 * 8.346920013427734
Epoch 2330, val loss: 0.40935713052749634
Epoch 2340, training loss: 83.7571792602539 = 0.2928250730037689 + 10.0 * 8.346435546875
Epoch 2340, val loss: 0.4088371992111206
Epoch 2350, training loss: 83.75223541259766 = 0.2916022539138794 + 10.0 * 8.346063613891602
Epoch 2350, val loss: 0.40835821628570557
Epoch 2360, training loss: 83.75849914550781 = 0.290371835231781 + 10.0 * 8.34681224822998
Epoch 2360, val loss: 0.40786024928092957
Epoch 2370, training loss: 83.80329132080078 = 0.28912144899368286 + 10.0 * 8.35141658782959
Epoch 2370, val loss: 0.40730786323547363
Epoch 2380, training loss: 83.74738311767578 = 0.2878705561161041 + 10.0 * 8.345951080322266
Epoch 2380, val loss: 0.4069192111492157
Epoch 2390, training loss: 83.74208068847656 = 0.28662213683128357 + 10.0 * 8.345545768737793
Epoch 2390, val loss: 0.406334787607193
Epoch 2400, training loss: 83.73677825927734 = 0.2853851914405823 + 10.0 * 8.345139503479004
Epoch 2400, val loss: 0.40590596199035645
Epoch 2410, training loss: 83.73019409179688 = 0.2841486632823944 + 10.0 * 8.3446044921875
Epoch 2410, val loss: 0.40542155504226685
Epoch 2420, training loss: 83.74140167236328 = 0.28291556239128113 + 10.0 * 8.34584903717041
Epoch 2420, val loss: 0.40494999289512634
Epoch 2430, training loss: 83.7262191772461 = 0.28166383504867554 + 10.0 * 8.34445571899414
Epoch 2430, val loss: 0.4045206606388092
Epoch 2440, training loss: 83.72752380371094 = 0.28042417764663696 + 10.0 * 8.344709396362305
Epoch 2440, val loss: 0.40409988164901733
Epoch 2450, training loss: 83.72003936767578 = 0.27918341755867004 + 10.0 * 8.344085693359375
Epoch 2450, val loss: 0.4036943018436432
Epoch 2460, training loss: 83.7121353149414 = 0.27794504165649414 + 10.0 * 8.343419075012207
Epoch 2460, val loss: 0.4032251238822937
Epoch 2470, training loss: 83.73211669921875 = 0.27670377492904663 + 10.0 * 8.345541000366211
Epoch 2470, val loss: 0.4028327167034149
Epoch 2480, training loss: 83.7532958984375 = 0.2754417657852173 + 10.0 * 8.347784996032715
Epoch 2480, val loss: 0.4024393558502197
Epoch 2490, training loss: 83.72673034667969 = 0.27419909834861755 + 10.0 * 8.345252990722656
Epoch 2490, val loss: 0.40212482213974
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8381532217148655
0.864667101354778
=== training gcn model ===
Epoch 0, training loss: 106.91986083984375 = 1.097474455833435 + 10.0 * 10.582239151000977
Epoch 0, val loss: 1.097415566444397
Epoch 10, training loss: 106.90890502929688 = 1.0927151441574097 + 10.0 * 10.581619262695312
Epoch 10, val loss: 1.0926769971847534
Epoch 20, training loss: 106.87106323242188 = 1.0877840518951416 + 10.0 * 10.578328132629395
Epoch 20, val loss: 1.0877357721328735
Epoch 30, training loss: 106.71197509765625 = 1.082592487335205 + 10.0 * 10.56293773651123
Epoch 30, val loss: 1.0825283527374268
Epoch 40, training loss: 106.15061950683594 = 1.077130675315857 + 10.0 * 10.507349014282227
Epoch 40, val loss: 1.0770190954208374
Epoch 50, training loss: 104.7315444946289 = 1.0712209939956665 + 10.0 * 10.366032600402832
Epoch 50, val loss: 1.0710924863815308
Epoch 60, training loss: 102.1072006225586 = 1.065473198890686 + 10.0 * 10.104172706604004
Epoch 60, val loss: 1.0653135776519775
Epoch 70, training loss: 99.16976165771484 = 1.0596846342086792 + 10.0 * 9.811007499694824
Epoch 70, val loss: 1.0595673322677612
Epoch 80, training loss: 97.72605895996094 = 1.0543142557144165 + 10.0 * 9.667174339294434
Epoch 80, val loss: 1.0544334650039673
Epoch 90, training loss: 96.68232727050781 = 1.0497004985809326 + 10.0 * 9.563262939453125
Epoch 90, val loss: 1.0500396490097046
Epoch 100, training loss: 95.88601684570312 = 1.0458424091339111 + 10.0 * 9.484017372131348
Epoch 100, val loss: 1.0463134050369263
Epoch 110, training loss: 94.50231170654297 = 1.042647361755371 + 10.0 * 9.345966339111328
Epoch 110, val loss: 1.0432307720184326
Epoch 120, training loss: 92.39466857910156 = 1.03988778591156 + 10.0 * 9.135478019714355
Epoch 120, val loss: 1.0405688285827637
Epoch 130, training loss: 91.14198303222656 = 1.0372021198272705 + 10.0 * 9.010478019714355
Epoch 130, val loss: 1.0379925966262817
Epoch 140, training loss: 90.22589874267578 = 1.0347824096679688 + 10.0 * 8.919111251831055
Epoch 140, val loss: 1.0357004404067993
Epoch 150, training loss: 89.66040802001953 = 1.0325909852981567 + 10.0 * 8.862781524658203
Epoch 150, val loss: 1.0334596633911133
Epoch 160, training loss: 89.40510559082031 = 1.0296964645385742 + 10.0 * 8.837541580200195
Epoch 160, val loss: 1.030500888824463
Epoch 170, training loss: 89.1573257446289 = 1.026142954826355 + 10.0 * 8.813117980957031
Epoch 170, val loss: 1.0270211696624756
Epoch 180, training loss: 88.88383483886719 = 1.0227278470993042 + 10.0 * 8.786110877990723
Epoch 180, val loss: 1.0237869024276733
Epoch 190, training loss: 88.55087280273438 = 1.0197877883911133 + 10.0 * 8.753108024597168
Epoch 190, val loss: 1.0210951566696167
Epoch 200, training loss: 88.25301361083984 = 1.0169875621795654 + 10.0 * 8.723602294921875
Epoch 200, val loss: 1.0184385776519775
Epoch 210, training loss: 88.03995513916016 = 1.0137087106704712 + 10.0 * 8.702624320983887
Epoch 210, val loss: 1.0152517557144165
Epoch 220, training loss: 87.8203125 = 1.00999915599823 + 10.0 * 8.681031227111816
Epoch 220, val loss: 1.011757493019104
Epoch 230, training loss: 87.6064224243164 = 1.0062626600265503 + 10.0 * 8.660016059875488
Epoch 230, val loss: 1.0082498788833618
Epoch 240, training loss: 87.42523956298828 = 1.0022432804107666 + 10.0 * 8.64229965209961
Epoch 240, val loss: 1.0044993162155151
Epoch 250, training loss: 87.25366973876953 = 0.997876763343811 + 10.0 * 8.625578880310059
Epoch 250, val loss: 1.000383734703064
Epoch 260, training loss: 87.09226989746094 = 0.9931356310844421 + 10.0 * 8.60991382598877
Epoch 260, val loss: 0.995920717716217
Epoch 270, training loss: 86.9548110961914 = 0.9880520105361938 + 10.0 * 8.596675872802734
Epoch 270, val loss: 0.9911465048789978
Epoch 280, training loss: 86.82003021240234 = 0.9825528264045715 + 10.0 * 8.583747863769531
Epoch 280, val loss: 0.9860259890556335
Epoch 290, training loss: 86.69066619873047 = 0.976754903793335 + 10.0 * 8.571391105651855
Epoch 290, val loss: 0.9806005954742432
Epoch 300, training loss: 86.57066345214844 = 0.9707192182540894 + 10.0 * 8.5599946975708
Epoch 300, val loss: 0.9750313758850098
Epoch 310, training loss: 86.44377136230469 = 0.9645060300827026 + 10.0 * 8.54792594909668
Epoch 310, val loss: 0.9692686796188354
Epoch 320, training loss: 86.32075500488281 = 0.9581403732299805 + 10.0 * 8.536261558532715
Epoch 320, val loss: 0.9633945226669312
Epoch 330, training loss: 86.22184753417969 = 0.9515219330787659 + 10.0 * 8.527032852172852
Epoch 330, val loss: 0.9572781324386597
Epoch 340, training loss: 86.11106872558594 = 0.9445211291313171 + 10.0 * 8.516654968261719
Epoch 340, val loss: 0.9507720470428467
Epoch 350, training loss: 86.03491973876953 = 0.9371009469032288 + 10.0 * 8.509781837463379
Epoch 350, val loss: 0.9438976645469666
Epoch 360, training loss: 85.95970916748047 = 0.9292604327201843 + 10.0 * 8.503045082092285
Epoch 360, val loss: 0.9366636872291565
Epoch 370, training loss: 85.89212036132812 = 0.9211541414260864 + 10.0 * 8.497096061706543
Epoch 370, val loss: 0.9291654825210571
Epoch 380, training loss: 85.84481811523438 = 0.9128071069717407 + 10.0 * 8.49320125579834
Epoch 380, val loss: 0.921454668045044
Epoch 390, training loss: 85.77982330322266 = 0.9041696786880493 + 10.0 * 8.487565994262695
Epoch 390, val loss: 0.9135523438453674
Epoch 400, training loss: 85.72819519042969 = 0.8953396677970886 + 10.0 * 8.483285903930664
Epoch 400, val loss: 0.9053891897201538
Epoch 410, training loss: 85.68621826171875 = 0.886226236820221 + 10.0 * 8.479999542236328
Epoch 410, val loss: 0.8970306515693665
Epoch 420, training loss: 85.62339782714844 = 0.8769656419754028 + 10.0 * 8.474642753601074
Epoch 420, val loss: 0.888469934463501
Epoch 430, training loss: 85.57782745361328 = 0.8675111532211304 + 10.0 * 8.47103214263916
Epoch 430, val loss: 0.8797827959060669
Epoch 440, training loss: 85.5359115600586 = 0.857787013053894 + 10.0 * 8.467812538146973
Epoch 440, val loss: 0.8708055019378662
Epoch 450, training loss: 85.47917938232422 = 0.8479079604148865 + 10.0 * 8.463127136230469
Epoch 450, val loss: 0.8617140650749207
Epoch 460, training loss: 85.42561340332031 = 0.8378878831863403 + 10.0 * 8.458772659301758
Epoch 460, val loss: 0.8524812459945679
Epoch 470, training loss: 85.39418029785156 = 0.8276695609092712 + 10.0 * 8.45665168762207
Epoch 470, val loss: 0.843052089214325
Epoch 480, training loss: 85.3615493774414 = 0.8171373009681702 + 10.0 * 8.45444107055664
Epoch 480, val loss: 0.8333600759506226
Epoch 490, training loss: 85.2999267578125 = 0.8065780997276306 + 10.0 * 8.449335098266602
Epoch 490, val loss: 0.8236242532730103
Epoch 500, training loss: 85.25455474853516 = 0.7959230542182922 + 10.0 * 8.445863723754883
Epoch 500, val loss: 0.8138099312782288
Epoch 510, training loss: 85.21582794189453 = 0.7851220369338989 + 10.0 * 8.443070411682129
Epoch 510, val loss: 0.803856611251831
Epoch 520, training loss: 85.17741394042969 = 0.7742159366607666 + 10.0 * 8.440320014953613
Epoch 520, val loss: 0.7938212156295776
Epoch 530, training loss: 85.14205932617188 = 0.7632409334182739 + 10.0 * 8.437881469726562
Epoch 530, val loss: 0.7837358117103577
Epoch 540, training loss: 85.1394271850586 = 0.7521607279777527 + 10.0 * 8.438726425170898
Epoch 540, val loss: 0.7735776901245117
Epoch 550, training loss: 85.08251953125 = 0.7410544753074646 + 10.0 * 8.434146881103516
Epoch 550, val loss: 0.7633806467056274
Epoch 560, training loss: 85.04798126220703 = 0.7301915287971497 + 10.0 * 8.431778907775879
Epoch 560, val loss: 0.7534322738647461
Epoch 570, training loss: 85.01768493652344 = 0.7193912267684937 + 10.0 * 8.429829597473145
Epoch 570, val loss: 0.7435727715492249
Epoch 580, training loss: 84.98777770996094 = 0.7086675763130188 + 10.0 * 8.427911758422852
Epoch 580, val loss: 0.7338185906410217
Epoch 590, training loss: 84.99250030517578 = 0.6980808973312378 + 10.0 * 8.429441452026367
Epoch 590, val loss: 0.7241887450218201
Epoch 600, training loss: 84.94549560546875 = 0.6875478029251099 + 10.0 * 8.42579460144043
Epoch 600, val loss: 0.7146896719932556
Epoch 610, training loss: 84.90991973876953 = 0.6773967146873474 + 10.0 * 8.42325210571289
Epoch 610, val loss: 0.7055376172065735
Epoch 620, training loss: 84.88338470458984 = 0.6675480604171753 + 10.0 * 8.42158317565918
Epoch 620, val loss: 0.6967140436172485
Epoch 630, training loss: 84.8558578491211 = 0.657934844493866 + 10.0 * 8.419792175292969
Epoch 630, val loss: 0.6881215572357178
Epoch 640, training loss: 84.83733367919922 = 0.6485826969146729 + 10.0 * 8.418874740600586
Epoch 640, val loss: 0.6797996163368225
Epoch 650, training loss: 84.80585479736328 = 0.6393951177597046 + 10.0 * 8.416646003723145
Epoch 650, val loss: 0.6716637015342712
Epoch 660, training loss: 84.79420471191406 = 0.6305803656578064 + 10.0 * 8.416362762451172
Epoch 660, val loss: 0.663881778717041
Epoch 670, training loss: 84.7567367553711 = 0.6221200823783875 + 10.0 * 8.413461685180664
Epoch 670, val loss: 0.6564786434173584
Epoch 680, training loss: 84.73380279541016 = 0.6139326691627502 + 10.0 * 8.4119873046875
Epoch 680, val loss: 0.6493375897407532
Epoch 690, training loss: 84.70996856689453 = 0.6060514450073242 + 10.0 * 8.410391807556152
Epoch 690, val loss: 0.6424627900123596
Epoch 700, training loss: 84.74883270263672 = 0.5984048247337341 + 10.0 * 8.415042877197266
Epoch 700, val loss: 0.6358581185340881
Epoch 710, training loss: 84.66770935058594 = 0.5909741520881653 + 10.0 * 8.407673835754395
Epoch 710, val loss: 0.6294285655021667
Epoch 720, training loss: 84.6450424194336 = 0.5839623808860779 + 10.0 * 8.406107902526855
Epoch 720, val loss: 0.6234092712402344
Epoch 730, training loss: 84.62347412109375 = 0.5772185325622559 + 10.0 * 8.40462589263916
Epoch 730, val loss: 0.6176477074623108
Epoch 740, training loss: 84.61005401611328 = 0.5707177519798279 + 10.0 * 8.40393352508545
Epoch 740, val loss: 0.6121153831481934
Epoch 750, training loss: 84.59138488769531 = 0.5643752813339233 + 10.0 * 8.402700424194336
Epoch 750, val loss: 0.6067032814025879
Epoch 760, training loss: 84.5704116821289 = 0.558338463306427 + 10.0 * 8.401206970214844
Epoch 760, val loss: 0.6016276478767395
Epoch 770, training loss: 84.55368041992188 = 0.5525603890419006 + 10.0 * 8.40011215209961
Epoch 770, val loss: 0.5967546701431274
Epoch 780, training loss: 84.533203125 = 0.546981155872345 + 10.0 * 8.398622512817383
Epoch 780, val loss: 0.5921042561531067
Epoch 790, training loss: 84.51994323730469 = 0.5416039228439331 + 10.0 * 8.397833824157715
Epoch 790, val loss: 0.5876368284225464
Epoch 800, training loss: 84.507080078125 = 0.5363768339157104 + 10.0 * 8.397069931030273
Epoch 800, val loss: 0.5833005309104919
Epoch 810, training loss: 84.48576354980469 = 0.5314000248908997 + 10.0 * 8.39543628692627
Epoch 810, val loss: 0.5792043209075928
Epoch 820, training loss: 84.47067260742188 = 0.5266523957252502 + 10.0 * 8.394402503967285
Epoch 820, val loss: 0.5753394365310669
Epoch 830, training loss: 84.4561767578125 = 0.5220860242843628 + 10.0 * 8.39340877532959
Epoch 830, val loss: 0.5716250538825989
Epoch 840, training loss: 84.45153045654297 = 0.5176749229431152 + 10.0 * 8.39338493347168
Epoch 840, val loss: 0.5680375099182129
Epoch 850, training loss: 84.42990112304688 = 0.5133344531059265 + 10.0 * 8.391656875610352
Epoch 850, val loss: 0.564558207988739
Epoch 860, training loss: 84.43402099609375 = 0.5092307925224304 + 10.0 * 8.392478942871094
Epoch 860, val loss: 0.5612379312515259
Epoch 870, training loss: 84.40320587158203 = 0.5053067803382874 + 10.0 * 8.389789581298828
Epoch 870, val loss: 0.558143675327301
Epoch 880, training loss: 84.39275360107422 = 0.5015192031860352 + 10.0 * 8.389123916625977
Epoch 880, val loss: 0.5551490187644958
Epoch 890, training loss: 84.37809753417969 = 0.4978625178337097 + 10.0 * 8.388023376464844
Epoch 890, val loss: 0.5522676110267639
Epoch 900, training loss: 84.37283325195312 = 0.49430719017982483 + 10.0 * 8.387852668762207
Epoch 900, val loss: 0.5495032668113708
Epoch 910, training loss: 84.36326599121094 = 0.490802526473999 + 10.0 * 8.387247085571289
Epoch 910, val loss: 0.5467141270637512
Epoch 920, training loss: 84.34198760986328 = 0.48746490478515625 + 10.0 * 8.385452270507812
Epoch 920, val loss: 0.5441567301750183
Epoch 930, training loss: 84.3315200805664 = 0.4842642545700073 + 10.0 * 8.384725570678711
Epoch 930, val loss: 0.541718065738678
Epoch 940, training loss: 84.31983947753906 = 0.4811495244503021 + 10.0 * 8.383869171142578
Epoch 940, val loss: 0.5393366813659668
Epoch 950, training loss: 84.3188247680664 = 0.4781293570995331 + 10.0 * 8.384069442749023
Epoch 950, val loss: 0.5370274186134338
Epoch 960, training loss: 84.32276916503906 = 0.4751020073890686 + 10.0 * 8.384766578674316
Epoch 960, val loss: 0.5346982479095459
Epoch 970, training loss: 84.28923034667969 = 0.47224661707878113 + 10.0 * 8.381698608398438
Epoch 970, val loss: 0.5325448513031006
Epoch 980, training loss: 84.277587890625 = 0.4694967567920685 + 10.0 * 8.38080883026123
Epoch 980, val loss: 0.5305128693580627
Epoch 990, training loss: 84.26774597167969 = 0.4667987525463104 + 10.0 * 8.380094528198242
Epoch 990, val loss: 0.5284737348556519
Epoch 1000, training loss: 84.28641510009766 = 0.4641609787940979 + 10.0 * 8.382225036621094
Epoch 1000, val loss: 0.5265475511550903
Epoch 1010, training loss: 84.25384521484375 = 0.46153783798217773 + 10.0 * 8.379230499267578
Epoch 1010, val loss: 0.5245299935340881
Epoch 1020, training loss: 84.24386596679688 = 0.459042489528656 + 10.0 * 8.3784818649292
Epoch 1020, val loss: 0.522724449634552
Epoch 1030, training loss: 84.23123168945312 = 0.45659494400024414 + 10.0 * 8.377463340759277
Epoch 1030, val loss: 0.5208986401557922
Epoch 1040, training loss: 84.22109985351562 = 0.4541921615600586 + 10.0 * 8.376690864562988
Epoch 1040, val loss: 0.5191534161567688
Epoch 1050, training loss: 84.21099853515625 = 0.4518358111381531 + 10.0 * 8.375916481018066
Epoch 1050, val loss: 0.5174490809440613
Epoch 1060, training loss: 84.20909118652344 = 0.449518620967865 + 10.0 * 8.375957489013672
Epoch 1060, val loss: 0.5157985687255859
Epoch 1070, training loss: 84.24250793457031 = 0.44720837473869324 + 10.0 * 8.37952995300293
Epoch 1070, val loss: 0.5141229033470154
Epoch 1080, training loss: 84.20370483398438 = 0.44493696093559265 + 10.0 * 8.375876426696777
Epoch 1080, val loss: 0.5124428868293762
Epoch 1090, training loss: 84.1804428100586 = 0.44277265667915344 + 10.0 * 8.373766899108887
Epoch 1090, val loss: 0.5109406113624573
Epoch 1100, training loss: 84.17082214355469 = 0.440650075674057 + 10.0 * 8.373017311096191
Epoch 1100, val loss: 0.5093669891357422
Epoch 1110, training loss: 84.16111755371094 = 0.4385668933391571 + 10.0 * 8.372255325317383
Epoch 1110, val loss: 0.5078899264335632
Epoch 1120, training loss: 84.15434265136719 = 0.4365074336528778 + 10.0 * 8.371783256530762
Epoch 1120, val loss: 0.506412923336029
Epoch 1130, training loss: 84.19335174560547 = 0.43446335196495056 + 10.0 * 8.37588882446289
Epoch 1130, val loss: 0.5049583911895752
Epoch 1140, training loss: 84.14193725585938 = 0.43244072794914246 + 10.0 * 8.370949745178223
Epoch 1140, val loss: 0.5034812092781067
Epoch 1150, training loss: 84.1362533569336 = 0.43049606680870056 + 10.0 * 8.370575904846191
Epoch 1150, val loss: 0.5021265745162964
Epoch 1160, training loss: 84.12608337402344 = 0.4285876154899597 + 10.0 * 8.369749069213867
Epoch 1160, val loss: 0.5007553100585938
Epoch 1170, training loss: 84.15143585205078 = 0.426697701215744 + 10.0 * 8.37247371673584
Epoch 1170, val loss: 0.49946045875549316
Epoch 1180, training loss: 84.11286163330078 = 0.42481592297554016 + 10.0 * 8.368804931640625
Epoch 1180, val loss: 0.4980573356151581
Epoch 1190, training loss: 84.09732055664062 = 0.4229961037635803 + 10.0 * 8.367432594299316
Epoch 1190, val loss: 0.4968253970146179
Epoch 1200, training loss: 84.0901870727539 = 0.42120230197906494 + 10.0 * 8.366898536682129
Epoch 1200, val loss: 0.4955395460128784
Epoch 1210, training loss: 84.08929443359375 = 0.4194301962852478 + 10.0 * 8.366986274719238
Epoch 1210, val loss: 0.49429258704185486
Epoch 1220, training loss: 84.0812759399414 = 0.417648583650589 + 10.0 * 8.366362571716309
Epoch 1220, val loss: 0.4930700659751892
Epoch 1230, training loss: 84.07889556884766 = 0.41591429710388184 + 10.0 * 8.366297721862793
Epoch 1230, val loss: 0.491813063621521
Epoch 1240, training loss: 84.05847930908203 = 0.414225697517395 + 10.0 * 8.364425659179688
Epoch 1240, val loss: 0.49066102504730225
Epoch 1250, training loss: 84.05168914794922 = 0.412548303604126 + 10.0 * 8.363913536071777
Epoch 1250, val loss: 0.48948898911476135
Epoch 1260, training loss: 84.04827880859375 = 0.41088569164276123 + 10.0 * 8.363739013671875
Epoch 1260, val loss: 0.48832499980926514
Epoch 1270, training loss: 84.11228942871094 = 0.4092028737068176 + 10.0 * 8.370308876037598
Epoch 1270, val loss: 0.4871275722980499
Epoch 1280, training loss: 84.04814910888672 = 0.4075358510017395 + 10.0 * 8.36406135559082
Epoch 1280, val loss: 0.4859188199043274
Epoch 1290, training loss: 84.03084564208984 = 0.40597203373908997 + 10.0 * 8.362486839294434
Epoch 1290, val loss: 0.48495620489120483
Epoch 1300, training loss: 84.01532745361328 = 0.4044158160686493 + 10.0 * 8.361090660095215
Epoch 1300, val loss: 0.4838413596153259
Epoch 1310, training loss: 84.00738525390625 = 0.4028712809085846 + 10.0 * 8.360451698303223
Epoch 1310, val loss: 0.4827682077884674
Epoch 1320, training loss: 83.99954223632812 = 0.4013386368751526 + 10.0 * 8.359820365905762
Epoch 1320, val loss: 0.4817112982273102
Epoch 1330, training loss: 83.99261474609375 = 0.3998152017593384 + 10.0 * 8.35927963256836
Epoch 1330, val loss: 0.480644553899765
Epoch 1340, training loss: 83.98564147949219 = 0.39829543232917786 + 10.0 * 8.358735084533691
Epoch 1340, val loss: 0.4795909821987152
Epoch 1350, training loss: 83.97888946533203 = 0.3967835605144501 + 10.0 * 8.358210563659668
Epoch 1350, val loss: 0.4785311222076416
Epoch 1360, training loss: 83.98106384277344 = 0.3952799141407013 + 10.0 * 8.3585786819458
Epoch 1360, val loss: 0.4774587154388428
Epoch 1370, training loss: 83.98030853271484 = 0.3937642574310303 + 10.0 * 8.358654975891113
Epoch 1370, val loss: 0.4764765501022339
Epoch 1380, training loss: 83.97628021240234 = 0.39226672053337097 + 10.0 * 8.35840129852295
Epoch 1380, val loss: 0.47537651658058167
Epoch 1390, training loss: 83.96212005615234 = 0.390817254781723 + 10.0 * 8.35713005065918
Epoch 1390, val loss: 0.47436001896858215
Epoch 1400, training loss: 83.94984436035156 = 0.3893740773200989 + 10.0 * 8.356046676635742
Epoch 1400, val loss: 0.47336113452911377
Epoch 1410, training loss: 83.94355773925781 = 0.38794276118278503 + 10.0 * 8.355562210083008
Epoch 1410, val loss: 0.4723947048187256
Epoch 1420, training loss: 83.96481323242188 = 0.3865146338939667 + 10.0 * 8.357830047607422
Epoch 1420, val loss: 0.47138309478759766
Epoch 1430, training loss: 83.92926788330078 = 0.38506922125816345 + 10.0 * 8.354419708251953
Epoch 1430, val loss: 0.4704270660877228
Epoch 1440, training loss: 83.92362976074219 = 0.38366422057151794 + 10.0 * 8.353996276855469
Epoch 1440, val loss: 0.4694511890411377
Epoch 1450, training loss: 83.91893768310547 = 0.38226595520973206 + 10.0 * 8.353667259216309
Epoch 1450, val loss: 0.46848511695861816
Epoch 1460, training loss: 83.91502380371094 = 0.38087525963783264 + 10.0 * 8.353414535522461
Epoch 1460, val loss: 0.46755272150039673
Epoch 1470, training loss: 83.9371566772461 = 0.3794815242290497 + 10.0 * 8.355768203735352
Epoch 1470, val loss: 0.46658846735954285
Epoch 1480, training loss: 83.91422271728516 = 0.3780851364135742 + 10.0 * 8.35361385345459
Epoch 1480, val loss: 0.46561717987060547
Epoch 1490, training loss: 83.90238189697266 = 0.3767131268978119 + 10.0 * 8.352566719055176
Epoch 1490, val loss: 0.4647081196308136
Epoch 1500, training loss: 83.8926773071289 = 0.3753562867641449 + 10.0 * 8.35173225402832
Epoch 1500, val loss: 0.46377211809158325
Epoch 1510, training loss: 83.92987060546875 = 0.37400180101394653 + 10.0 * 8.355587005615234
Epoch 1510, val loss: 0.46290427446365356
Epoch 1520, training loss: 83.90740203857422 = 0.3726207911968231 + 10.0 * 8.35347843170166
Epoch 1520, val loss: 0.4618654251098633
Epoch 1530, training loss: 83.87995910644531 = 0.3712935447692871 + 10.0 * 8.350866317749023
Epoch 1530, val loss: 0.46100863814353943
Epoch 1540, training loss: 83.87286376953125 = 0.369973361492157 + 10.0 * 8.350289344787598
Epoch 1540, val loss: 0.460117369890213
Epoch 1550, training loss: 83.86685180664062 = 0.36865857243537903 + 10.0 * 8.34981918334961
Epoch 1550, val loss: 0.4592423141002655
Epoch 1560, training loss: 83.87557983398438 = 0.3673483431339264 + 10.0 * 8.350823402404785
Epoch 1560, val loss: 0.45832929015159607
Epoch 1570, training loss: 83.86680603027344 = 0.36601805686950684 + 10.0 * 8.350078582763672
Epoch 1570, val loss: 0.45749807357788086
Epoch 1580, training loss: 83.85482025146484 = 0.3647100329399109 + 10.0 * 8.349011421203613
Epoch 1580, val loss: 0.4566532373428345
Epoch 1590, training loss: 83.8484115600586 = 0.36341536045074463 + 10.0 * 8.348499298095703
Epoch 1590, val loss: 0.45579516887664795
Epoch 1600, training loss: 83.8429946899414 = 0.36212578415870667 + 10.0 * 8.3480863571167
Epoch 1600, val loss: 0.4549679458141327
Epoch 1610, training loss: 83.83712768554688 = 0.36084190011024475 + 10.0 * 8.347628593444824
Epoch 1610, val loss: 0.45413559675216675
Epoch 1620, training loss: 83.83428192138672 = 0.3595634996891022 + 10.0 * 8.347471237182617
Epoch 1620, val loss: 0.4533318281173706
Epoch 1630, training loss: 83.85848999023438 = 0.3582841455936432 + 10.0 * 8.350020408630371
Epoch 1630, val loss: 0.4525514841079712
Epoch 1640, training loss: 83.84038543701172 = 0.3569989502429962 + 10.0 * 8.34833812713623
Epoch 1640, val loss: 0.4516538679599762
Epoch 1650, training loss: 83.8369140625 = 0.3557361960411072 + 10.0 * 8.34811782836914
Epoch 1650, val loss: 0.45096555352211
Epoch 1660, training loss: 83.82866668701172 = 0.3544737696647644 + 10.0 * 8.347418785095215
Epoch 1660, val loss: 0.4500996768474579
Epoch 1670, training loss: 83.81200408935547 = 0.35323137044906616 + 10.0 * 8.345876693725586
Epoch 1670, val loss: 0.4492673873901367
Epoch 1680, training loss: 83.8127212524414 = 0.35199955105781555 + 10.0 * 8.34607219696045
Epoch 1680, val loss: 0.4485369026660919
Epoch 1690, training loss: 83.81439208984375 = 0.3507641851902008 + 10.0 * 8.346363067626953
Epoch 1690, val loss: 0.4477267563343048
Epoch 1700, training loss: 83.81038665771484 = 0.3495287001132965 + 10.0 * 8.346086502075195
Epoch 1700, val loss: 0.4469679296016693
Epoch 1710, training loss: 83.79639434814453 = 0.3483071029186249 + 10.0 * 8.344808578491211
Epoch 1710, val loss: 0.4462623596191406
Epoch 1720, training loss: 83.7994384765625 = 0.34708818793296814 + 10.0 * 8.345234870910645
Epoch 1720, val loss: 0.4455397427082062
Epoch 1730, training loss: 83.80753326416016 = 0.34586501121520996 + 10.0 * 8.346166610717773
Epoch 1730, val loss: 0.4447755515575409
Epoch 1740, training loss: 83.78809356689453 = 0.3446510434150696 + 10.0 * 8.344344139099121
Epoch 1740, val loss: 0.4439789950847626
Epoch 1750, training loss: 83.77965545654297 = 0.3434489667415619 + 10.0 * 8.343620300292969
Epoch 1750, val loss: 0.4433126449584961
Epoch 1760, training loss: 83.79316711425781 = 0.3422493636608124 + 10.0 * 8.345091819763184
Epoch 1760, val loss: 0.4425632059574127
Epoch 1770, training loss: 83.76969146728516 = 0.3410487174987793 + 10.0 * 8.342864036560059
Epoch 1770, val loss: 0.44182342290878296
Epoch 1780, training loss: 83.7693862915039 = 0.3398682475090027 + 10.0 * 8.342951774597168
Epoch 1780, val loss: 0.441144734621048
Epoch 1790, training loss: 83.790283203125 = 0.33868786692619324 + 10.0 * 8.345159530639648
Epoch 1790, val loss: 0.44040292501449585
Epoch 1800, training loss: 83.75540161132812 = 0.337507963180542 + 10.0 * 8.341789245605469
Epoch 1800, val loss: 0.4397490918636322
Epoch 1810, training loss: 83.75348663330078 = 0.3363478183746338 + 10.0 * 8.341713905334473
Epoch 1810, val loss: 0.43911126255989075
Epoch 1820, training loss: 83.74980163574219 = 0.33518627285957336 + 10.0 * 8.341461181640625
Epoch 1820, val loss: 0.4384002387523651
Epoch 1830, training loss: 83.74641418457031 = 0.33403363823890686 + 10.0 * 8.341238021850586
Epoch 1830, val loss: 0.4377824664115906
Epoch 1840, training loss: 83.77166748046875 = 0.3328814208507538 + 10.0 * 8.343878746032715
Epoch 1840, val loss: 0.43715837597846985
Epoch 1850, training loss: 83.75322723388672 = 0.33172187209129333 + 10.0 * 8.342150688171387
Epoch 1850, val loss: 0.4364810585975647
Epoch 1860, training loss: 83.73589324951172 = 0.33058229088783264 + 10.0 * 8.340531349182129
Epoch 1860, val loss: 0.4358541965484619
Epoch 1870, training loss: 83.73123931884766 = 0.3294478952884674 + 10.0 * 8.340179443359375
Epoch 1870, val loss: 0.43525931239128113
Epoch 1880, training loss: 83.7468490600586 = 0.3283177614212036 + 10.0 * 8.341853141784668
Epoch 1880, val loss: 0.4346494674682617
Epoch 1890, training loss: 83.74018859863281 = 0.32718223333358765 + 10.0 * 8.341300964355469
Epoch 1890, val loss: 0.43407490849494934
Epoch 1900, training loss: 83.71831512451172 = 0.3260553181171417 + 10.0 * 8.339225769042969
Epoch 1900, val loss: 0.4334057867527008
Epoch 1910, training loss: 83.71475219726562 = 0.3249463737010956 + 10.0 * 8.338980674743652
Epoch 1910, val loss: 0.43282073736190796
Epoch 1920, training loss: 83.71163940429688 = 0.3238404393196106 + 10.0 * 8.338780403137207
Epoch 1920, val loss: 0.4322124123573303
Epoch 1930, training loss: 83.71562194824219 = 0.3227364122867584 + 10.0 * 8.339288711547852
Epoch 1930, val loss: 0.43161600828170776
Epoch 1940, training loss: 83.727783203125 = 0.3216293752193451 + 10.0 * 8.340615272521973
Epoch 1940, val loss: 0.4310359060764313
Epoch 1950, training loss: 83.7134780883789 = 0.320526123046875 + 10.0 * 8.339295387268066
Epoch 1950, val loss: 0.4305275082588196
Epoch 1960, training loss: 83.7022705078125 = 0.31944140791893005 + 10.0 * 8.338282585144043
Epoch 1960, val loss: 0.4299852252006531
Epoch 1970, training loss: 83.69328308105469 = 0.3183620274066925 + 10.0 * 8.337491989135742
Epoch 1970, val loss: 0.42945799231529236
Epoch 1980, training loss: 83.68928527832031 = 0.31728917360305786 + 10.0 * 8.337199211120605
Epoch 1980, val loss: 0.4289478361606598
Epoch 1990, training loss: 83.6891860961914 = 0.31622061133384705 + 10.0 * 8.337296485900879
Epoch 1990, val loss: 0.428458034992218
Epoch 2000, training loss: 83.75044250488281 = 0.3151487112045288 + 10.0 * 8.34352970123291
Epoch 2000, val loss: 0.4280257523059845
Epoch 2010, training loss: 83.70303344726562 = 0.31406500935554504 + 10.0 * 8.338896751403809
Epoch 2010, val loss: 0.4273771643638611
Epoch 2020, training loss: 83.68216705322266 = 0.3130151629447937 + 10.0 * 8.336915016174316
Epoch 2020, val loss: 0.42696619033813477
Epoch 2030, training loss: 83.67276000976562 = 0.31196701526641846 + 10.0 * 8.336079597473145
Epoch 2030, val loss: 0.4264795482158661
Epoch 2040, training loss: 83.6685791015625 = 0.3109251856803894 + 10.0 * 8.33576488494873
Epoch 2040, val loss: 0.4260319471359253
Epoch 2050, training loss: 83.66525268554688 = 0.3098852038383484 + 10.0 * 8.33553695678711
Epoch 2050, val loss: 0.42557355761528015
Epoch 2060, training loss: 83.66287994384766 = 0.3088468015193939 + 10.0 * 8.335403442382812
Epoch 2060, val loss: 0.4251360297203064
Epoch 2070, training loss: 83.7182388305664 = 0.30780908465385437 + 10.0 * 8.341043472290039
Epoch 2070, val loss: 0.42475736141204834
Epoch 2080, training loss: 83.69351196289062 = 0.30675992369651794 + 10.0 * 8.338674545288086
Epoch 2080, val loss: 0.42419904470443726
Epoch 2090, training loss: 83.65914154052734 = 0.30573633313179016 + 10.0 * 8.33534049987793
Epoch 2090, val loss: 0.4238211214542389
Epoch 2100, training loss: 83.65464782714844 = 0.3047199845314026 + 10.0 * 8.334993362426758
Epoch 2100, val loss: 0.4234265685081482
Epoch 2110, training loss: 83.649169921875 = 0.3037091791629791 + 10.0 * 8.334546089172363
Epoch 2110, val loss: 0.42304348945617676
Epoch 2120, training loss: 83.65379333496094 = 0.30270111560821533 + 10.0 * 8.335108757019043
Epoch 2120, val loss: 0.4226369559764862
Epoch 2130, training loss: 83.64359283447266 = 0.3016917109489441 + 10.0 * 8.334190368652344
Epoch 2130, val loss: 0.4222235083580017
Epoch 2140, training loss: 83.64427185058594 = 0.30068978667259216 + 10.0 * 8.334358215332031
Epoch 2140, val loss: 0.4217934012413025
Epoch 2150, training loss: 83.73096466064453 = 0.29968520998954773 + 10.0 * 8.343128204345703
Epoch 2150, val loss: 0.4213749170303345
Epoch 2160, training loss: 83.6439208984375 = 0.29867827892303467 + 10.0 * 8.334524154663086
Epoch 2160, val loss: 0.4210694134235382
Epoch 2170, training loss: 83.63113403320312 = 0.29769161343574524 + 10.0 * 8.333344459533691
Epoch 2170, val loss: 0.42070335149765015
Epoch 2180, training loss: 83.6299057006836 = 0.29670804738998413 + 10.0 * 8.333319664001465
Epoch 2180, val loss: 0.4203270673751831
Epoch 2190, training loss: 83.6238021850586 = 0.2957266867160797 + 10.0 * 8.332807540893555
Epoch 2190, val loss: 0.4200051724910736
Epoch 2200, training loss: 83.6200942993164 = 0.29474392533302307 + 10.0 * 8.332534790039062
Epoch 2200, val loss: 0.419653058052063
Epoch 2210, training loss: 83.6171646118164 = 0.29376280307769775 + 10.0 * 8.332340240478516
Epoch 2210, val loss: 0.4193165600299835
Epoch 2220, training loss: 83.61888885498047 = 0.29278281331062317 + 10.0 * 8.332610130310059
Epoch 2220, val loss: 0.41895055770874023
Epoch 2230, training loss: 83.66009521484375 = 0.29179972410202026 + 10.0 * 8.33682918548584
Epoch 2230, val loss: 0.41862696409225464
Epoch 2240, training loss: 83.61437225341797 = 0.29082179069519043 + 10.0 * 8.332354545593262
Epoch 2240, val loss: 0.418369859457016
Epoch 2250, training loss: 83.60813903808594 = 0.28985267877578735 + 10.0 * 8.331828117370605
Epoch 2250, val loss: 0.4180172383785248
Epoch 2260, training loss: 83.60482025146484 = 0.28888460993766785 + 10.0 * 8.33159351348877
Epoch 2260, val loss: 0.4177088439464569
Epoch 2270, training loss: 83.60074615478516 = 0.2879159450531006 + 10.0 * 8.331282615661621
Epoch 2270, val loss: 0.417406290769577
Epoch 2280, training loss: 83.59847259521484 = 0.2869488596916199 + 10.0 * 8.331151962280273
Epoch 2280, val loss: 0.41709738969802856
Epoch 2290, training loss: 83.63420867919922 = 0.28598085045814514 + 10.0 * 8.334822654724121
Epoch 2290, val loss: 0.416813462972641
Epoch 2300, training loss: 83.60932159423828 = 0.28500738739967346 + 10.0 * 8.33243179321289
Epoch 2300, val loss: 0.4165360927581787
Epoch 2310, training loss: 83.59932708740234 = 0.28404611349105835 + 10.0 * 8.331528663635254
Epoch 2310, val loss: 0.4162253141403198
Epoch 2320, training loss: 83.60773468017578 = 0.28308966755867004 + 10.0 * 8.332464218139648
Epoch 2320, val loss: 0.4159289598464966
Epoch 2330, training loss: 83.59686279296875 = 0.28213098645210266 + 10.0 * 8.331473350524902
Epoch 2330, val loss: 0.41574209928512573
Epoch 2340, training loss: 83.58866882324219 = 0.28118467330932617 + 10.0 * 8.330748558044434
Epoch 2340, val loss: 0.41550371050834656
Epoch 2350, training loss: 83.57892608642578 = 0.28023815155029297 + 10.0 * 8.329869270324707
Epoch 2350, val loss: 0.415241539478302
Epoch 2360, training loss: 83.57825469970703 = 0.27929314970970154 + 10.0 * 8.329895973205566
Epoch 2360, val loss: 0.4149962365627289
Epoch 2370, training loss: 83.5797348022461 = 0.2783479690551758 + 10.0 * 8.330138206481934
Epoch 2370, val loss: 0.4147312641143799
Epoch 2380, training loss: 83.59870147705078 = 0.27740243077278137 + 10.0 * 8.33212947845459
Epoch 2380, val loss: 0.4144548177719116
Epoch 2390, training loss: 83.56964874267578 = 0.27645736932754517 + 10.0 * 8.32931900024414
Epoch 2390, val loss: 0.414332777261734
Epoch 2400, training loss: 83.5710220336914 = 0.2755209803581238 + 10.0 * 8.329549789428711
Epoch 2400, val loss: 0.4141298830509186
Epoch 2410, training loss: 83.5652084350586 = 0.2745826840400696 + 10.0 * 8.329062461853027
Epoch 2410, val loss: 0.41390034556388855
Epoch 2420, training loss: 83.5712661743164 = 0.27364951372146606 + 10.0 * 8.329761505126953
Epoch 2420, val loss: 0.41365471482276917
Epoch 2430, training loss: 83.57566833496094 = 0.27271535992622375 + 10.0 * 8.33029556274414
Epoch 2430, val loss: 0.41354167461395264
Epoch 2440, training loss: 83.57160186767578 = 0.27177760004997253 + 10.0 * 8.32998275756836
Epoch 2440, val loss: 0.4133561849594116
Epoch 2450, training loss: 83.58018493652344 = 0.2708417773246765 + 10.0 * 8.330934524536133
Epoch 2450, val loss: 0.4130913019180298
Epoch 2460, training loss: 83.55450439453125 = 0.26991209387779236 + 10.0 * 8.328458786010742
Epoch 2460, val loss: 0.41292107105255127
Epoch 2470, training loss: 83.55113983154297 = 0.26898670196533203 + 10.0 * 8.328214645385742
Epoch 2470, val loss: 0.41275453567504883
Epoch 2480, training loss: 83.552978515625 = 0.2680627703666687 + 10.0 * 8.3284912109375
Epoch 2480, val loss: 0.4126054644584656
Epoch 2490, training loss: 83.58600616455078 = 0.26713553071022034 + 10.0 * 8.331887245178223
Epoch 2490, val loss: 0.4124077558517456
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8457635717909691
0.8633630370209375
=== training gcn model ===
Epoch 0, training loss: 106.90919494628906 = 1.086146354675293 + 10.0 * 10.582304954528809
Epoch 0, val loss: 1.0858445167541504
Epoch 10, training loss: 106.90280151367188 = 1.0820751190185547 + 10.0 * 10.582072257995605
Epoch 10, val loss: 1.0818543434143066
Epoch 20, training loss: 106.88685607910156 = 1.0778721570968628 + 10.0 * 10.58089828491211
Epoch 20, val loss: 1.0777173042297363
Epoch 30, training loss: 106.82691955566406 = 1.0734820365905762 + 10.0 * 10.57534408569336
Epoch 30, val loss: 1.0733858346939087
Epoch 40, training loss: 106.60057067871094 = 1.0688624382019043 + 10.0 * 10.553171157836914
Epoch 40, val loss: 1.0688107013702393
Epoch 50, training loss: 105.97464752197266 = 1.0640192031860352 + 10.0 * 10.491063117980957
Epoch 50, val loss: 1.0640041828155518
Epoch 60, training loss: 104.81397247314453 = 1.059325933456421 + 10.0 * 10.37546443939209
Epoch 60, val loss: 1.059412956237793
Epoch 70, training loss: 103.24516296386719 = 1.055392861366272 + 10.0 * 10.218976974487305
Epoch 70, val loss: 1.0555020570755005
Epoch 80, training loss: 102.50038146972656 = 1.0517030954360962 + 10.0 * 10.144867897033691
Epoch 80, val loss: 1.0518321990966797
Epoch 90, training loss: 101.37279510498047 = 1.0474075078964233 + 10.0 * 10.032538414001465
Epoch 90, val loss: 1.0475422143936157
Epoch 100, training loss: 98.00408935546875 = 1.041442632675171 + 10.0 * 9.696264266967773
Epoch 100, val loss: 1.0415918827056885
Epoch 110, training loss: 96.66739654541016 = 1.0343213081359863 + 10.0 * 9.56330680847168
Epoch 110, val loss: 1.0348567962646484
Epoch 120, training loss: 95.7562026977539 = 1.0271824598312378 + 10.0 * 9.472902297973633
Epoch 120, val loss: 1.0281509160995483
Epoch 130, training loss: 94.5438461303711 = 1.0202062129974365 + 10.0 * 9.352364540100098
Epoch 130, val loss: 1.0215007066726685
Epoch 140, training loss: 92.92481994628906 = 1.0129668712615967 + 10.0 * 9.191184997558594
Epoch 140, val loss: 1.0145113468170166
Epoch 150, training loss: 91.89469146728516 = 1.0055547952651978 + 10.0 * 9.088913917541504
Epoch 150, val loss: 1.0072684288024902
Epoch 160, training loss: 91.3685302734375 = 0.9987326860427856 + 10.0 * 9.036979675292969
Epoch 160, val loss: 1.0007011890411377
Epoch 170, training loss: 90.68285369873047 = 0.9930304288864136 + 10.0 * 8.968982696533203
Epoch 170, val loss: 0.9952952861785889
Epoch 180, training loss: 90.0721206665039 = 0.9882567524909973 + 10.0 * 8.90838623046875
Epoch 180, val loss: 0.9907273650169373
Epoch 190, training loss: 89.59098815917969 = 0.9835010766983032 + 10.0 * 8.860749244689941
Epoch 190, val loss: 0.9859988689422607
Epoch 200, training loss: 89.2881088256836 = 0.9777732491493225 + 10.0 * 8.831033706665039
Epoch 200, val loss: 0.9801732301712036
Epoch 210, training loss: 89.08808135986328 = 0.9701947569847107 + 10.0 * 8.811788558959961
Epoch 210, val loss: 0.9725995063781738
Epoch 220, training loss: 88.84081268310547 = 0.9614095091819763 + 10.0 * 8.78794002532959
Epoch 220, val loss: 0.9641135334968567
Epoch 230, training loss: 88.5586166381836 = 0.9524500370025635 + 10.0 * 8.760616302490234
Epoch 230, val loss: 0.9555938839912415
Epoch 240, training loss: 88.35580444335938 = 0.9435234069824219 + 10.0 * 8.741228103637695
Epoch 240, val loss: 0.9470980167388916
Epoch 250, training loss: 88.2164535522461 = 0.9342901706695557 + 10.0 * 8.728216171264648
Epoch 250, val loss: 0.938108503818512
Epoch 260, training loss: 88.11054229736328 = 0.9244072437286377 + 10.0 * 8.718613624572754
Epoch 260, val loss: 0.9286225438117981
Epoch 270, training loss: 87.99890899658203 = 0.9140951633453369 + 10.0 * 8.708481788635254
Epoch 270, val loss: 0.9187444448471069
Epoch 280, training loss: 87.87467193603516 = 0.9035595059394836 + 10.0 * 8.697111129760742
Epoch 280, val loss: 0.9087547063827515
Epoch 290, training loss: 87.72675323486328 = 0.8930566310882568 + 10.0 * 8.683369636535645
Epoch 290, val loss: 0.8988144397735596
Epoch 300, training loss: 87.55526733398438 = 0.8826414942741394 + 10.0 * 8.667262077331543
Epoch 300, val loss: 0.8889651894569397
Epoch 310, training loss: 87.38592529296875 = 0.8721454739570618 + 10.0 * 8.65137767791748
Epoch 310, val loss: 0.8790627717971802
Epoch 320, training loss: 87.23724365234375 = 0.8612778186798096 + 10.0 * 8.63759708404541
Epoch 320, val loss: 0.8687592148780823
Epoch 330, training loss: 87.09941864013672 = 0.8499559164047241 + 10.0 * 8.624946594238281
Epoch 330, val loss: 0.8580402731895447
Epoch 340, training loss: 86.967041015625 = 0.8383848071098328 + 10.0 * 8.612865447998047
Epoch 340, val loss: 0.8471051454544067
Epoch 350, training loss: 86.83170318603516 = 0.8267251253128052 + 10.0 * 8.600497245788574
Epoch 350, val loss: 0.836069643497467
Epoch 360, training loss: 86.71888732910156 = 0.8149610161781311 + 10.0 * 8.590392112731934
Epoch 360, val loss: 0.8249061703681946
Epoch 370, training loss: 86.61823272705078 = 0.802872359752655 + 10.0 * 8.581536293029785
Epoch 370, val loss: 0.8134651780128479
Epoch 380, training loss: 86.52401733398438 = 0.7904976010322571 + 10.0 * 8.573351860046387
Epoch 380, val loss: 0.8017402291297913
Epoch 390, training loss: 86.43561553955078 = 0.7779299020767212 + 10.0 * 8.565768241882324
Epoch 390, val loss: 0.7898367047309875
Epoch 400, training loss: 86.3533706665039 = 0.7652849555015564 + 10.0 * 8.558809280395508
Epoch 400, val loss: 0.7778701782226562
Epoch 410, training loss: 86.3123779296875 = 0.7526142597198486 + 10.0 * 8.555975914001465
Epoch 410, val loss: 0.7658599615097046
Epoch 420, training loss: 86.22137451171875 = 0.7398531436920166 + 10.0 * 8.548151969909668
Epoch 420, val loss: 0.753828227519989
Epoch 430, training loss: 86.15739440917969 = 0.7271426320075989 + 10.0 * 8.543025016784668
Epoch 430, val loss: 0.7418388724327087
Epoch 440, training loss: 86.09700775146484 = 0.7145581841468811 + 10.0 * 8.538244247436523
Epoch 440, val loss: 0.7299792170524597
Epoch 450, training loss: 86.0391845703125 = 0.7021854519844055 + 10.0 * 8.533699989318848
Epoch 450, val loss: 0.7183314561843872
Epoch 460, training loss: 85.97898864746094 = 0.6901288032531738 + 10.0 * 8.528885841369629
Epoch 460, val loss: 0.7069528698921204
Epoch 470, training loss: 85.90542602539062 = 0.6784325838088989 + 10.0 * 8.522699356079102
Epoch 470, val loss: 0.6959888935089111
Epoch 480, training loss: 85.83548736572266 = 0.667183518409729 + 10.0 * 8.516830444335938
Epoch 480, val loss: 0.6854538917541504
Epoch 490, training loss: 85.76016235351562 = 0.6563330888748169 + 10.0 * 8.510382652282715
Epoch 490, val loss: 0.6753086447715759
Epoch 500, training loss: 85.69255065917969 = 0.6458912491798401 + 10.0 * 8.504666328430176
Epoch 500, val loss: 0.6655474901199341
Epoch 510, training loss: 85.61958312988281 = 0.6358656287193298 + 10.0 * 8.498372077941895
Epoch 510, val loss: 0.6561892628669739
Epoch 520, training loss: 85.55485534667969 = 0.6261740326881409 + 10.0 * 8.492868423461914
Epoch 520, val loss: 0.6471607685089111
Epoch 530, training loss: 85.49592590332031 = 0.6167606115341187 + 10.0 * 8.487916946411133
Epoch 530, val loss: 0.6384093165397644
Epoch 540, training loss: 85.44298553466797 = 0.6076111793518066 + 10.0 * 8.483537673950195
Epoch 540, val loss: 0.6299251914024353
Epoch 550, training loss: 85.40158081054688 = 0.5987709164619446 + 10.0 * 8.480280876159668
Epoch 550, val loss: 0.6217356324195862
Epoch 560, training loss: 85.34423065185547 = 0.5903237462043762 + 10.0 * 8.475390434265137
Epoch 560, val loss: 0.6139368414878845
Epoch 570, training loss: 85.33676147460938 = 0.5822645425796509 + 10.0 * 8.475449562072754
Epoch 570, val loss: 0.6065160036087036
Epoch 580, training loss: 85.26519775390625 = 0.5745254158973694 + 10.0 * 8.469067573547363
Epoch 580, val loss: 0.5994294881820679
Epoch 590, training loss: 85.22213745117188 = 0.5671787261962891 + 10.0 * 8.465496063232422
Epoch 590, val loss: 0.5926966071128845
Epoch 600, training loss: 85.18626403808594 = 0.5601858496665955 + 10.0 * 8.462607383728027
Epoch 600, val loss: 0.5863054990768433
Epoch 610, training loss: 85.14961242675781 = 0.5535043478012085 + 10.0 * 8.459610939025879
Epoch 610, val loss: 0.5802334547042847
Epoch 620, training loss: 85.14494323730469 = 0.5471200942993164 + 10.0 * 8.459782600402832
Epoch 620, val loss: 0.5744266510009766
Epoch 630, training loss: 85.08724212646484 = 0.5410040020942688 + 10.0 * 8.45462417602539
Epoch 630, val loss: 0.5689105987548828
Epoch 640, training loss: 85.06653594970703 = 0.5352075099945068 + 10.0 * 8.453132629394531
Epoch 640, val loss: 0.5636950731277466
Epoch 650, training loss: 85.02075958251953 = 0.52970290184021 + 10.0 * 8.449106216430664
Epoch 650, val loss: 0.5587769746780396
Epoch 660, training loss: 84.98799896240234 = 0.5245029330253601 + 10.0 * 8.446349143981934
Epoch 660, val loss: 0.5541598796844482
Epoch 670, training loss: 84.95161437988281 = 0.5195774435997009 + 10.0 * 8.443203926086426
Epoch 670, val loss: 0.5498036742210388
Epoch 680, training loss: 84.97225189208984 = 0.5148994326591492 + 10.0 * 8.445734977722168
Epoch 680, val loss: 0.5456929802894592
Epoch 690, training loss: 84.89413452148438 = 0.5103772282600403 + 10.0 * 8.438375473022461
Epoch 690, val loss: 0.54175865650177
Epoch 700, training loss: 84.85926818847656 = 0.5061283707618713 + 10.0 * 8.435314178466797
Epoch 700, val loss: 0.5380544066429138
Epoch 710, training loss: 84.82513427734375 = 0.5021226406097412 + 10.0 * 8.43230152130127
Epoch 710, val loss: 0.5345961451530457
Epoch 720, training loss: 84.79378509521484 = 0.4982992112636566 + 10.0 * 8.429548263549805
Epoch 720, val loss: 0.5313457250595093
Epoch 730, training loss: 84.76558685302734 = 0.4946203827857971 + 10.0 * 8.427096366882324
Epoch 730, val loss: 0.5282348990440369
Epoch 740, training loss: 84.73905944824219 = 0.49106135964393616 + 10.0 * 8.424799919128418
Epoch 740, val loss: 0.5252434611320496
Epoch 750, training loss: 84.71768951416016 = 0.48761358857154846 + 10.0 * 8.42300796508789
Epoch 750, val loss: 0.5223498940467834
Epoch 760, training loss: 84.74478149414062 = 0.4842256009578705 + 10.0 * 8.426055908203125
Epoch 760, val loss: 0.5196067094802856
Epoch 770, training loss: 84.67623901367188 = 0.4809257388114929 + 10.0 * 8.419530868530273
Epoch 770, val loss: 0.5168184041976929
Epoch 780, training loss: 84.66072845458984 = 0.4777890145778656 + 10.0 * 8.418293952941895
Epoch 780, val loss: 0.5142621994018555
Epoch 790, training loss: 84.63963317871094 = 0.4747816026210785 + 10.0 * 8.416484832763672
Epoch 790, val loss: 0.5118533968925476
Epoch 800, training loss: 84.62310791015625 = 0.47186899185180664 + 10.0 * 8.41512393951416
Epoch 800, val loss: 0.5095254778862
Epoch 810, training loss: 84.61233520507812 = 0.4690294861793518 + 10.0 * 8.41433048248291
Epoch 810, val loss: 0.5072977542877197
Epoch 820, training loss: 84.60610961914062 = 0.46623867750167847 + 10.0 * 8.413987159729004
Epoch 820, val loss: 0.5051180124282837
Epoch 830, training loss: 84.5825424194336 = 0.46352434158325195 + 10.0 * 8.411901473999023
Epoch 830, val loss: 0.5030205249786377
Epoch 840, training loss: 84.57038879394531 = 0.4609190821647644 + 10.0 * 8.4109468460083
Epoch 840, val loss: 0.5010333061218262
Epoch 850, training loss: 84.55389404296875 = 0.4584026336669922 + 10.0 * 8.40954875946045
Epoch 850, val loss: 0.49914902448654175
Epoch 860, training loss: 84.54052734375 = 0.4559600055217743 + 10.0 * 8.408456802368164
Epoch 860, val loss: 0.49734896421432495
Epoch 870, training loss: 84.5291976928711 = 0.4535776376724243 + 10.0 * 8.407562255859375
Epoch 870, val loss: 0.49562883377075195
Epoch 880, training loss: 84.5234375 = 0.4512424170970917 + 10.0 * 8.407219886779785
Epoch 880, val loss: 0.49392834305763245
Epoch 890, training loss: 84.515625 = 0.4489594101905823 + 10.0 * 8.40666675567627
Epoch 890, val loss: 0.4923356771469116
Epoch 900, training loss: 84.48783111572266 = 0.44674187898635864 + 10.0 * 8.404109001159668
Epoch 900, val loss: 0.4907606542110443
Epoch 910, training loss: 84.48336791992188 = 0.44460180401802063 + 10.0 * 8.403876304626465
Epoch 910, val loss: 0.4892793893814087
Epoch 920, training loss: 84.47395324707031 = 0.44250959157943726 + 10.0 * 8.403143882751465
Epoch 920, val loss: 0.48787567019462585
Epoch 930, training loss: 84.45440673828125 = 0.44046667218208313 + 10.0 * 8.40139389038086
Epoch 930, val loss: 0.4865184426307678
Epoch 940, training loss: 84.43583679199219 = 0.43847721815109253 + 10.0 * 8.399736404418945
Epoch 940, val loss: 0.4852296710014343
Epoch 950, training loss: 84.43457794189453 = 0.4365353286266327 + 10.0 * 8.39980411529541
Epoch 950, val loss: 0.4839942157268524
Epoch 960, training loss: 84.41139221191406 = 0.4346179962158203 + 10.0 * 8.397677421569824
Epoch 960, val loss: 0.48274484276771545
Epoch 970, training loss: 84.39991760253906 = 0.43275341391563416 + 10.0 * 8.396716117858887
Epoch 970, val loss: 0.48160654306411743
Epoch 980, training loss: 84.40209197998047 = 0.43088874220848083 + 10.0 * 8.397120475769043
Epoch 980, val loss: 0.48042604327201843
Epoch 990, training loss: 84.38057708740234 = 0.4290565252304077 + 10.0 * 8.39515209197998
Epoch 990, val loss: 0.4793020784854889
Epoch 1000, training loss: 84.35906982421875 = 0.4273085296154022 + 10.0 * 8.393176078796387
Epoch 1000, val loss: 0.4782501459121704
Epoch 1010, training loss: 84.34567260742188 = 0.42561519145965576 + 10.0 * 8.392005920410156
Epoch 1010, val loss: 0.4772603213787079
Epoch 1020, training loss: 84.33328247070312 = 0.42395296692848206 + 10.0 * 8.3909330368042
Epoch 1020, val loss: 0.4762875437736511
Epoch 1030, training loss: 84.32176971435547 = 0.42231079936027527 + 10.0 * 8.389945983886719
Epoch 1030, val loss: 0.47533488273620605
Epoch 1040, training loss: 84.30995178222656 = 0.42068567872047424 + 10.0 * 8.38892650604248
Epoch 1040, val loss: 0.474402517080307
Epoch 1050, training loss: 84.29877471923828 = 0.41907423734664917 + 10.0 * 8.387969970703125
Epoch 1050, val loss: 0.4734807312488556
Epoch 1060, training loss: 84.28840637207031 = 0.4174789488315582 + 10.0 * 8.387092590332031
Epoch 1060, val loss: 0.47257381677627563
Epoch 1070, training loss: 84.35193634033203 = 0.41589105129241943 + 10.0 * 8.393604278564453
Epoch 1070, val loss: 0.47165703773498535
Epoch 1080, training loss: 84.2905044555664 = 0.41427817940711975 + 10.0 * 8.387622833251953
Epoch 1080, val loss: 0.4707167148590088
Epoch 1090, training loss: 84.2624740600586 = 0.4127304255962372 + 10.0 * 8.384974479675293
Epoch 1090, val loss: 0.4698290228843689
Epoch 1100, training loss: 84.2510986328125 = 0.4112400412559509 + 10.0 * 8.38398551940918
Epoch 1100, val loss: 0.46901679039001465
Epoch 1110, training loss: 84.2391128540039 = 0.409778892993927 + 10.0 * 8.382932662963867
Epoch 1110, val loss: 0.46820276975631714
Epoch 1120, training loss: 84.22935485839844 = 0.40833133459091187 + 10.0 * 8.382102012634277
Epoch 1120, val loss: 0.467411071062088
Epoch 1130, training loss: 84.21974182128906 = 0.40689119696617126 + 10.0 * 8.381284713745117
Epoch 1130, val loss: 0.46663209795951843
Epoch 1140, training loss: 84.21619415283203 = 0.40545663237571716 + 10.0 * 8.381073951721191
Epoch 1140, val loss: 0.46582579612731934
Epoch 1150, training loss: 84.23541259765625 = 0.40401941537857056 + 10.0 * 8.383138656616211
Epoch 1150, val loss: 0.4651205539703369
Epoch 1160, training loss: 84.20357513427734 = 0.40257248282432556 + 10.0 * 8.38010025024414
Epoch 1160, val loss: 0.4643119275569916
Epoch 1170, training loss: 84.19020080566406 = 0.40117737650871277 + 10.0 * 8.378902435302734
Epoch 1170, val loss: 0.4635393023490906
Epoch 1180, training loss: 84.17877197265625 = 0.3998192548751831 + 10.0 * 8.37789535522461
Epoch 1180, val loss: 0.4628430902957916
Epoch 1190, training loss: 84.16801452636719 = 0.3984777629375458 + 10.0 * 8.376954078674316
Epoch 1190, val loss: 0.46213752031326294
Epoch 1200, training loss: 84.16040802001953 = 0.39714547991752625 + 10.0 * 8.376325607299805
Epoch 1200, val loss: 0.46143612265586853
Epoch 1210, training loss: 84.1600570678711 = 0.39581844210624695 + 10.0 * 8.376423835754395
Epoch 1210, val loss: 0.46074190735816956
Epoch 1220, training loss: 84.16181182861328 = 0.3944825530052185 + 10.0 * 8.37673282623291
Epoch 1220, val loss: 0.46005555987358093
Epoch 1230, training loss: 84.1405258178711 = 0.39315953850746155 + 10.0 * 8.374736785888672
Epoch 1230, val loss: 0.4593583941459656
Epoch 1240, training loss: 84.12985229492188 = 0.3918679356575012 + 10.0 * 8.373798370361328
Epoch 1240, val loss: 0.45870235562324524
Epoch 1250, training loss: 84.12189483642578 = 0.3905918002128601 + 10.0 * 8.373129844665527
Epoch 1250, val loss: 0.4580437242984772
Epoch 1260, training loss: 84.11396026611328 = 0.3893256187438965 + 10.0 * 8.37246322631836
Epoch 1260, val loss: 0.4573873281478882
Epoch 1270, training loss: 84.11017608642578 = 0.3880663514137268 + 10.0 * 8.372210502624512
Epoch 1270, val loss: 0.4567424952983856
Epoch 1280, training loss: 84.11151123046875 = 0.3868066668510437 + 10.0 * 8.372469902038574
Epoch 1280, val loss: 0.4561060965061188
Epoch 1290, training loss: 84.09920501708984 = 0.3855551779270172 + 10.0 * 8.371365547180176
Epoch 1290, val loss: 0.4554854929447174
Epoch 1300, training loss: 84.08747100830078 = 0.38431516289711 + 10.0 * 8.370315551757812
Epoch 1300, val loss: 0.45482873916625977
Epoch 1310, training loss: 84.10198211669922 = 0.3830922245979309 + 10.0 * 8.371889114379883
Epoch 1310, val loss: 0.4542061984539032
Epoch 1320, training loss: 84.09295654296875 = 0.3818497955799103 + 10.0 * 8.371110916137695
Epoch 1320, val loss: 0.4535887539386749
Epoch 1330, training loss: 84.0730972290039 = 0.38062769174575806 + 10.0 * 8.369246482849121
Epoch 1330, val loss: 0.45295995473861694
Epoch 1340, training loss: 84.06626892089844 = 0.3794495165348053 + 10.0 * 8.368681907653809
Epoch 1340, val loss: 0.4523642063140869
Epoch 1350, training loss: 84.0571060180664 = 0.3782951235771179 + 10.0 * 8.367880821228027
Epoch 1350, val loss: 0.45178505778312683
Epoch 1360, training loss: 84.05059814453125 = 0.3771495223045349 + 10.0 * 8.367344856262207
Epoch 1360, val loss: 0.4512440860271454
Epoch 1370, training loss: 84.04434204101562 = 0.3760071396827698 + 10.0 * 8.366833686828613
Epoch 1370, val loss: 0.4506954550743103
Epoch 1380, training loss: 84.0387191772461 = 0.37486732006073 + 10.0 * 8.366384506225586
Epoch 1380, val loss: 0.45014920830726624
Epoch 1390, training loss: 84.03312683105469 = 0.37372997403144836 + 10.0 * 8.365939140319824
Epoch 1390, val loss: 0.449607253074646
Epoch 1400, training loss: 84.02764129638672 = 0.37259531021118164 + 10.0 * 8.365504264831543
Epoch 1400, val loss: 0.4490627646446228
Epoch 1410, training loss: 84.02301788330078 = 0.37146687507629395 + 10.0 * 8.365155220031738
Epoch 1410, val loss: 0.4485131502151489
Epoch 1420, training loss: 84.08233642578125 = 0.37033942341804504 + 10.0 * 8.371199607849121
Epoch 1420, val loss: 0.44790536165237427
Epoch 1430, training loss: 84.01631164550781 = 0.3692052960395813 + 10.0 * 8.364710807800293
Epoch 1430, val loss: 0.4474318325519562
Epoch 1440, training loss: 84.0137939453125 = 0.36810046434402466 + 10.0 * 8.364568710327148
Epoch 1440, val loss: 0.44692736864089966
Epoch 1450, training loss: 84.00518798828125 = 0.36701804399490356 + 10.0 * 8.36381721496582
Epoch 1450, val loss: 0.4464093744754791
Epoch 1460, training loss: 83.997314453125 = 0.3659457862377167 + 10.0 * 8.363137245178223
Epoch 1460, val loss: 0.4459066689014435
Epoch 1470, training loss: 83.99284362792969 = 0.3648804724216461 + 10.0 * 8.36279582977295
Epoch 1470, val loss: 0.44539082050323486
Epoch 1480, training loss: 84.00117492675781 = 0.3638198673725128 + 10.0 * 8.36373519897461
Epoch 1480, val loss: 0.44488978385925293
Epoch 1490, training loss: 83.99830627441406 = 0.36273881793022156 + 10.0 * 8.363556861877441
Epoch 1490, val loss: 0.444355309009552
Epoch 1500, training loss: 83.99510192871094 = 0.3616727888584137 + 10.0 * 8.363343238830566
Epoch 1500, val loss: 0.4438689947128296
Epoch 1510, training loss: 83.97553253173828 = 0.36063212156295776 + 10.0 * 8.361490249633789
Epoch 1510, val loss: 0.44336560368537903
Epoch 1520, training loss: 83.97163391113281 = 0.35961219668388367 + 10.0 * 8.361202239990234
Epoch 1520, val loss: 0.44288843870162964
Epoch 1530, training loss: 83.96649169921875 = 0.35859987139701843 + 10.0 * 8.36078929901123
Epoch 1530, val loss: 0.44241562485694885
Epoch 1540, training loss: 83.96212768554688 = 0.35759237408638 + 10.0 * 8.360453605651855
Epoch 1540, val loss: 0.44195276498794556
Epoch 1550, training loss: 83.95784759521484 = 0.3565877079963684 + 10.0 * 8.360125541687012
Epoch 1550, val loss: 0.4414839446544647
Epoch 1560, training loss: 83.95597839355469 = 0.35558533668518066 + 10.0 * 8.360039710998535
Epoch 1560, val loss: 0.4410046935081482
Epoch 1570, training loss: 83.97394561767578 = 0.3545788824558258 + 10.0 * 8.361936569213867
Epoch 1570, val loss: 0.4405502378940582
Epoch 1580, training loss: 83.98943328857422 = 0.3535725176334381 + 10.0 * 8.36358642578125
Epoch 1580, val loss: 0.44004717469215393
Epoch 1590, training loss: 83.95316314697266 = 0.3525756895542145 + 10.0 * 8.360058784484863
Epoch 1590, val loss: 0.4395434260368347
Epoch 1600, training loss: 83.94196319580078 = 0.3516034781932831 + 10.0 * 8.359036445617676
Epoch 1600, val loss: 0.43913620710372925
Epoch 1610, training loss: 83.93556213378906 = 0.35064569115638733 + 10.0 * 8.358491897583008
Epoch 1610, val loss: 0.4386867880821228
Epoch 1620, training loss: 83.93191528320312 = 0.3496968448162079 + 10.0 * 8.358222007751465
Epoch 1620, val loss: 0.4382583200931549
Epoch 1630, training loss: 83.92756652832031 = 0.3487512469291687 + 10.0 * 8.357881546020508
Epoch 1630, val loss: 0.43783098459243774
Epoch 1640, training loss: 83.92603302001953 = 0.3478095233440399 + 10.0 * 8.35782241821289
Epoch 1640, val loss: 0.43741458654403687
Epoch 1650, training loss: 83.9628677368164 = 0.3468686044216156 + 10.0 * 8.361599922180176
Epoch 1650, val loss: 0.43702080845832825
Epoch 1660, training loss: 83.93431091308594 = 0.34591859579086304 + 10.0 * 8.35883903503418
Epoch 1660, val loss: 0.43651801347732544
Epoch 1670, training loss: 83.91813659667969 = 0.34499067068099976 + 10.0 * 8.357314109802246
Epoch 1670, val loss: 0.43611010909080505
Epoch 1680, training loss: 83.91075134277344 = 0.3440803587436676 + 10.0 * 8.356667518615723
Epoch 1680, val loss: 0.4357043206691742
Epoch 1690, training loss: 83.906494140625 = 0.34317734837532043 + 10.0 * 8.356331825256348
Epoch 1690, val loss: 0.43532034754753113
Epoch 1700, training loss: 83.90479278564453 = 0.34227749705314636 + 10.0 * 8.35625171661377
Epoch 1700, val loss: 0.4349212348461151
Epoch 1710, training loss: 83.93608856201172 = 0.3413783013820648 + 10.0 * 8.359471321105957
Epoch 1710, val loss: 0.43454909324645996
Epoch 1720, training loss: 83.89857482910156 = 0.3404727876186371 + 10.0 * 8.355810165405273
Epoch 1720, val loss: 0.4341354966163635
Epoch 1730, training loss: 83.89132690429688 = 0.339582622051239 + 10.0 * 8.35517406463623
Epoch 1730, val loss: 0.43376442790031433
Epoch 1740, training loss: 83.88661193847656 = 0.338705837726593 + 10.0 * 8.354790687561035
Epoch 1740, val loss: 0.43339550495147705
Epoch 1750, training loss: 83.88420867919922 = 0.3378368318080902 + 10.0 * 8.354637145996094
Epoch 1750, val loss: 0.43303966522216797
Epoch 1760, training loss: 83.88056945800781 = 0.3369729816913605 + 10.0 * 8.35435962677002
Epoch 1760, val loss: 0.43267765641212463
Epoch 1770, training loss: 83.92530059814453 = 0.3361092507839203 + 10.0 * 8.358919143676758
Epoch 1770, val loss: 0.43228456377983093
Epoch 1780, training loss: 83.87812042236328 = 0.33524465560913086 + 10.0 * 8.354288101196289
Epoch 1780, val loss: 0.4319806694984436
Epoch 1790, training loss: 83.87065887451172 = 0.33439570665359497 + 10.0 * 8.353626251220703
Epoch 1790, val loss: 0.43161913752555847
Epoch 1800, training loss: 83.86640167236328 = 0.33356374502182007 + 10.0 * 8.353283882141113
Epoch 1800, val loss: 0.43129828572273254
Epoch 1810, training loss: 83.8609619140625 = 0.33273857831954956 + 10.0 * 8.352822303771973
Epoch 1810, val loss: 0.4309689700603485
Epoch 1820, training loss: 83.86577606201172 = 0.33191603422164917 + 10.0 * 8.353385925292969
Epoch 1820, val loss: 0.43066108226776123
Epoch 1830, training loss: 83.87993621826172 = 0.3310873508453369 + 10.0 * 8.35488510131836
Epoch 1830, val loss: 0.43033355474472046
Epoch 1840, training loss: 83.85347747802734 = 0.33026444911956787 + 10.0 * 8.35232162475586
Epoch 1840, val loss: 0.4299790561199188
Epoch 1850, training loss: 83.84424591064453 = 0.3294585645198822 + 10.0 * 8.351478576660156
Epoch 1850, val loss: 0.4296596646308899
Epoch 1860, training loss: 83.84001922607422 = 0.3286615312099457 + 10.0 * 8.351136207580566
Epoch 1860, val loss: 0.4293498992919922
Epoch 1870, training loss: 83.8362045288086 = 0.327867716550827 + 10.0 * 8.350833892822266
Epoch 1870, val loss: 0.4290476441383362
Epoch 1880, training loss: 83.83226013183594 = 0.3270752429962158 + 10.0 * 8.350519180297852
Epoch 1880, val loss: 0.4287329614162445
Epoch 1890, training loss: 83.83123779296875 = 0.3262843191623688 + 10.0 * 8.350495338439941
Epoch 1890, val loss: 0.42841172218322754
Epoch 1900, training loss: 83.86994934082031 = 0.32549014687538147 + 10.0 * 8.354445457458496
Epoch 1900, val loss: 0.42811551690101624
Epoch 1910, training loss: 83.83291625976562 = 0.3246935307979584 + 10.0 * 8.350822448730469
Epoch 1910, val loss: 0.42777031660079956
Epoch 1920, training loss: 83.81824493408203 = 0.323914498090744 + 10.0 * 8.349432945251465
Epoch 1920, val loss: 0.42747950553894043
Epoch 1930, training loss: 83.81494903564453 = 0.3231453001499176 + 10.0 * 8.349180221557617
Epoch 1930, val loss: 0.42719900608062744
Epoch 1940, training loss: 83.8097152709961 = 0.32237908244132996 + 10.0 * 8.348733901977539
Epoch 1940, val loss: 0.4269081652164459
Epoch 1950, training loss: 83.8136978149414 = 0.3216122090816498 + 10.0 * 8.34920883178711
Epoch 1950, val loss: 0.42658349871635437
Epoch 1960, training loss: 83.83981323242188 = 0.32083985209465027 + 10.0 * 8.351897239685059
Epoch 1960, val loss: 0.42628616094589233
Epoch 1970, training loss: 83.79962158203125 = 0.3200703263282776 + 10.0 * 8.347955703735352
Epoch 1970, val loss: 0.42601269483566284
Epoch 1980, training loss: 83.79563903808594 = 0.319318026304245 + 10.0 * 8.347631454467773
Epoch 1980, val loss: 0.425762414932251
Epoch 1990, training loss: 83.79257202148438 = 0.3185713291168213 + 10.0 * 8.347399711608887
Epoch 1990, val loss: 0.42548587918281555
Epoch 2000, training loss: 83.78789520263672 = 0.31782418489456177 + 10.0 * 8.347006797790527
Epoch 2000, val loss: 0.42524322867393494
Epoch 2010, training loss: 83.7839584350586 = 0.3170792758464813 + 10.0 * 8.346688270568848
Epoch 2010, val loss: 0.4249746799468994
Epoch 2020, training loss: 83.83324432373047 = 0.31634023785591125 + 10.0 * 8.351690292358398
Epoch 2020, val loss: 0.4248158931732178
Epoch 2030, training loss: 83.78681182861328 = 0.31557682156562805 + 10.0 * 8.347124099731445
Epoch 2030, val loss: 0.42440274357795715
Epoch 2040, training loss: 83.77998352050781 = 0.3148348033428192 + 10.0 * 8.346514701843262
Epoch 2040, val loss: 0.42415329813957214
Epoch 2050, training loss: 83.77153015136719 = 0.31410208344459534 + 10.0 * 8.345743179321289
Epoch 2050, val loss: 0.4239175617694855
Epoch 2060, training loss: 83.76718139648438 = 0.31337258219718933 + 10.0 * 8.345380783081055
Epoch 2060, val loss: 0.42368224263191223
Epoch 2070, training loss: 83.76747131347656 = 0.3126446306705475 + 10.0 * 8.34548282623291
Epoch 2070, val loss: 0.42345285415649414
Epoch 2080, training loss: 83.81444549560547 = 0.3119141459465027 + 10.0 * 8.350253105163574
Epoch 2080, val loss: 0.4231874942779541
Epoch 2090, training loss: 83.77393341064453 = 0.3111730217933655 + 10.0 * 8.34627628326416
Epoch 2090, val loss: 0.42298993468284607
Epoch 2100, training loss: 83.75657653808594 = 0.31044647097587585 + 10.0 * 8.344613075256348
Epoch 2100, val loss: 0.42274221777915955
Epoch 2110, training loss: 83.7515869140625 = 0.30972743034362793 + 10.0 * 8.344185829162598
Epoch 2110, val loss: 0.4224907457828522
Epoch 2120, training loss: 83.74850463867188 = 0.30901196599006653 + 10.0 * 8.343949317932129
Epoch 2120, val loss: 0.42228108644485474
Epoch 2130, training loss: 83.74465942382812 = 0.30829519033432007 + 10.0 * 8.343636512756348
Epoch 2130, val loss: 0.42201632261276245
Epoch 2140, training loss: 83.7431411743164 = 0.30757832527160645 + 10.0 * 8.34355640411377
Epoch 2140, val loss: 0.42179208993911743
Epoch 2150, training loss: 83.7828140258789 = 0.3068608045578003 + 10.0 * 8.34759521484375
Epoch 2150, val loss: 0.4215752184391022
Epoch 2160, training loss: 83.7673568725586 = 0.30612999200820923 + 10.0 * 8.346122741699219
Epoch 2160, val loss: 0.4213399887084961
Epoch 2170, training loss: 83.73538970947266 = 0.3054121136665344 + 10.0 * 8.342997550964355
Epoch 2170, val loss: 0.4210551381111145
Epoch 2180, training loss: 83.73126220703125 = 0.304700642824173 + 10.0 * 8.342656135559082
Epoch 2180, val loss: 0.4208237826824188
Epoch 2190, training loss: 83.72721862792969 = 0.30399370193481445 + 10.0 * 8.34232234954834
Epoch 2190, val loss: 0.4206138849258423
Epoch 2200, training loss: 83.7245101928711 = 0.30328837037086487 + 10.0 * 8.342122077941895
Epoch 2200, val loss: 0.42039236426353455
Epoch 2210, training loss: 83.74451446533203 = 0.3025815486907959 + 10.0 * 8.344193458557129
Epoch 2210, val loss: 0.4201945960521698
Epoch 2220, training loss: 83.740478515625 = 0.30186718702316284 + 10.0 * 8.34386157989502
Epoch 2220, val loss: 0.4198894798755646
Epoch 2230, training loss: 83.72142791748047 = 0.3011557161808014 + 10.0 * 8.34202766418457
Epoch 2230, val loss: 0.4197234809398651
Epoch 2240, training loss: 83.7178726196289 = 0.30045247077941895 + 10.0 * 8.341741561889648
Epoch 2240, val loss: 0.4194323420524597
Epoch 2250, training loss: 83.71021270751953 = 0.29975375533103943 + 10.0 * 8.341046333312988
Epoch 2250, val loss: 0.41924983263015747
Epoch 2260, training loss: 83.70558166503906 = 0.2990554869174957 + 10.0 * 8.340652465820312
Epoch 2260, val loss: 0.419010192155838
Epoch 2270, training loss: 83.70221710205078 = 0.29835760593414307 + 10.0 * 8.340386390686035
Epoch 2270, val loss: 0.41879966855049133
Epoch 2280, training loss: 83.69904327392578 = 0.2976590394973755 + 10.0 * 8.34013843536377
Epoch 2280, val loss: 0.41856786608695984
Epoch 2290, training loss: 83.69719696044922 = 0.29695799946784973 + 10.0 * 8.3400239944458
Epoch 2290, val loss: 0.41834017634391785
Epoch 2300, training loss: 83.7959213256836 = 0.2962587773799896 + 10.0 * 8.349966049194336
Epoch 2300, val loss: 0.418092280626297
Epoch 2310, training loss: 83.71209716796875 = 0.2955387830734253 + 10.0 * 8.341655731201172
Epoch 2310, val loss: 0.4179210960865021
Epoch 2320, training loss: 83.7018051147461 = 0.2948410212993622 + 10.0 * 8.340696334838867
Epoch 2320, val loss: 0.41767582297325134
Epoch 2330, training loss: 83.68706512451172 = 0.29415082931518555 + 10.0 * 8.3392915725708
Epoch 2330, val loss: 0.417479008436203
Epoch 2340, training loss: 83.68202209472656 = 0.2934626340866089 + 10.0 * 8.338855743408203
Epoch 2340, val loss: 0.4172655940055847
Epoch 2350, training loss: 83.67845916748047 = 0.2927728295326233 + 10.0 * 8.338568687438965
Epoch 2350, val loss: 0.4170515835285187
Epoch 2360, training loss: 83.67549133300781 = 0.2920827269554138 + 10.0 * 8.338340759277344
Epoch 2360, val loss: 0.41684287786483765
Epoch 2370, training loss: 83.67223358154297 = 0.29139184951782227 + 10.0 * 8.33808422088623
Epoch 2370, val loss: 0.41663411259651184
Epoch 2380, training loss: 83.66961669921875 = 0.2907005250453949 + 10.0 * 8.337891578674316
Epoch 2380, val loss: 0.41643574833869934
Epoch 2390, training loss: 83.70870208740234 = 0.2900126576423645 + 10.0 * 8.34186840057373
Epoch 2390, val loss: 0.41633278131484985
Epoch 2400, training loss: 83.69928741455078 = 0.28930211067199707 + 10.0 * 8.340998649597168
Epoch 2400, val loss: 0.41596755385398865
Epoch 2410, training loss: 83.67090606689453 = 0.2886069118976593 + 10.0 * 8.33823013305664
Epoch 2410, val loss: 0.41574734449386597
Epoch 2420, training loss: 83.66012573242188 = 0.2879181504249573 + 10.0 * 8.337221145629883
Epoch 2420, val loss: 0.4156194031238556
Epoch 2430, training loss: 83.65560150146484 = 0.28723204135894775 + 10.0 * 8.336836814880371
Epoch 2430, val loss: 0.4154234230518341
Epoch 2440, training loss: 83.66246032714844 = 0.286548376083374 + 10.0 * 8.337591171264648
Epoch 2440, val loss: 0.415253221988678
Epoch 2450, training loss: 83.67263793945312 = 0.28585895895957947 + 10.0 * 8.338678359985352
Epoch 2450, val loss: 0.4150499403476715
Epoch 2460, training loss: 83.64815521240234 = 0.2851654887199402 + 10.0 * 8.336298942565918
Epoch 2460, val loss: 0.4148867130279541
Epoch 2470, training loss: 83.64466857910156 = 0.2844817638397217 + 10.0 * 8.336018562316895
Epoch 2470, val loss: 0.41468486189842224
Epoch 2480, training loss: 83.6417236328125 = 0.28380125761032104 + 10.0 * 8.335792541503906
Epoch 2480, val loss: 0.4145190417766571
Epoch 2490, training loss: 83.6401596069336 = 0.28312066197395325 + 10.0 * 8.33570384979248
Epoch 2490, val loss: 0.41431066393852234
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8417047184170472
0.863507933058031
The final CL Acc:0.84187, 0.00311, The final GNN Acc:0.86385, 0.00058
Begin epxeriment: noisy_level: 0.2 cont_weight: 10 epoch: der1:0.3 der2:0 dfr1:0.4 dfr2:0.3
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=2500, cl_num_layers=2, cl_num_proj_hidden=64, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=10.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.0, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=32, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, noisy_level=0.2, num_hidden=64, num_proj_hidden=64, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=40, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 88648])
add edge: torch.Size([2, 106282])
remove edge: torch.Size([2, 71012])
updated graph: torch.Size([2, 88646])
=== Noisy graph ===
/home/project-robust-contrastive/Robust-Contrastive-Learning/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 106.92173767089844 = 1.0986911058425903 + 10.0 * 10.582304000854492
Epoch 0, val loss: 1.0996578931808472
Epoch 10, training loss: 106.91492462158203 = 1.0940028429031372 + 10.0 * 10.58209228515625
Epoch 10, val loss: 1.0949957370758057
Epoch 20, training loss: 106.90021514892578 = 1.0890262126922607 + 10.0 * 10.5811185836792
Epoch 20, val loss: 1.090086579322815
Epoch 30, training loss: 106.84776306152344 = 1.083869457244873 + 10.0 * 10.57638931274414
Epoch 30, val loss: 1.0849930047988892
Epoch 40, training loss: 106.64537048339844 = 1.0783218145370483 + 10.0 * 10.5567045211792
Epoch 40, val loss: 1.0794838666915894
Epoch 50, training loss: 106.01878356933594 = 1.0720281600952148 + 10.0 * 10.494675636291504
Epoch 50, val loss: 1.073246717453003
Epoch 60, training loss: 104.49665832519531 = 1.0651512145996094 + 10.0 * 10.343151092529297
Epoch 60, val loss: 1.0665428638458252
Epoch 70, training loss: 102.00772094726562 = 1.058203101158142 + 10.0 * 10.094951629638672
Epoch 70, val loss: 1.0596768856048584
Epoch 80, training loss: 100.14910125732422 = 1.0511960983276367 + 10.0 * 9.909790992736816
Epoch 80, val loss: 1.0528537034988403
Epoch 90, training loss: 97.58673095703125 = 1.045708417892456 + 10.0 * 9.654102325439453
Epoch 90, val loss: 1.0475341081619263
Epoch 100, training loss: 94.67214965820312 = 1.041511058807373 + 10.0 * 9.36306381225586
Epoch 100, val loss: 1.0434201955795288
Epoch 110, training loss: 93.1163558959961 = 1.038644552230835 + 10.0 * 9.207771301269531
Epoch 110, val loss: 1.040550947189331
Epoch 120, training loss: 92.3858871459961 = 1.0361851453781128 + 10.0 * 9.134969711303711
Epoch 120, val loss: 1.0380184650421143
Epoch 130, training loss: 91.59441375732422 = 1.0338882207870483 + 10.0 * 9.056052207946777
Epoch 130, val loss: 1.0356805324554443
Epoch 140, training loss: 91.13360595703125 = 1.0319160223007202 + 10.0 * 9.01016902923584
Epoch 140, val loss: 1.033571720123291
Epoch 150, training loss: 90.92311096191406 = 1.0291658639907837 + 10.0 * 8.989394187927246
Epoch 150, val loss: 1.030665397644043
Epoch 160, training loss: 90.61421203613281 = 1.0259690284729004 + 10.0 * 8.958824157714844
Epoch 160, val loss: 1.027488112449646
Epoch 170, training loss: 90.19247436523438 = 1.0235307216644287 + 10.0 * 8.916894912719727
Epoch 170, val loss: 1.0251775979995728
Epoch 180, training loss: 89.64702606201172 = 1.021877646446228 + 10.0 * 8.86251449584961
Epoch 180, val loss: 1.0235705375671387
Epoch 190, training loss: 89.109619140625 = 1.020153522491455 + 10.0 * 8.80894660949707
Epoch 190, val loss: 1.0217955112457275
Epoch 200, training loss: 88.72612762451172 = 1.0178613662719727 + 10.0 * 8.77082633972168
Epoch 200, val loss: 1.0193623304367065
Epoch 210, training loss: 88.38015747070312 = 1.0146269798278809 + 10.0 * 8.736553192138672
Epoch 210, val loss: 1.01619291305542
Epoch 220, training loss: 88.16421508789062 = 1.011055588722229 + 10.0 * 8.715315818786621
Epoch 220, val loss: 1.0126140117645264
Epoch 230, training loss: 88.0337142944336 = 1.0071970224380493 + 10.0 * 8.702651977539062
Epoch 230, val loss: 1.0088086128234863
Epoch 240, training loss: 87.93054962158203 = 1.0031486749649048 + 10.0 * 8.692739486694336
Epoch 240, val loss: 1.0047723054885864
Epoch 250, training loss: 87.79855346679688 = 0.9989030361175537 + 10.0 * 8.679965019226074
Epoch 250, val loss: 1.000596523284912
Epoch 260, training loss: 87.66790771484375 = 0.9945651888847351 + 10.0 * 8.667333602905273
Epoch 260, val loss: 0.9963289499282837
Epoch 270, training loss: 87.5249252319336 = 0.9901590347290039 + 10.0 * 8.65347671508789
Epoch 270, val loss: 0.992000162601471
Epoch 280, training loss: 87.38945007324219 = 0.9854865074157715 + 10.0 * 8.640396118164062
Epoch 280, val loss: 0.9873994588851929
Epoch 290, training loss: 87.28974151611328 = 0.9803972244262695 + 10.0 * 8.63093376159668
Epoch 290, val loss: 0.9823186993598938
Epoch 300, training loss: 87.18285369873047 = 0.9748232364654541 + 10.0 * 8.620802879333496
Epoch 300, val loss: 0.9768803119659424
Epoch 310, training loss: 87.08308410644531 = 0.9688993096351624 + 10.0 * 8.611417770385742
Epoch 310, val loss: 0.9710489511489868
Epoch 320, training loss: 86.98388671875 = 0.962620198726654 + 10.0 * 8.602126121520996
Epoch 320, val loss: 0.9649193286895752
Epoch 330, training loss: 86.93296813964844 = 0.9560081958770752 + 10.0 * 8.597696304321289
Epoch 330, val loss: 0.9584771990776062
Epoch 340, training loss: 86.8084945678711 = 0.9490410685539246 + 10.0 * 8.585945129394531
Epoch 340, val loss: 0.9516463875770569
Epoch 350, training loss: 86.72045135498047 = 0.9417527914047241 + 10.0 * 8.57787036895752
Epoch 350, val loss: 0.9445080757141113
Epoch 360, training loss: 86.64454650878906 = 0.9341362714767456 + 10.0 * 8.571041107177734
Epoch 360, val loss: 0.9370591044425964
Epoch 370, training loss: 86.61711883544922 = 0.9261858463287354 + 10.0 * 8.569093704223633
Epoch 370, val loss: 0.9292357563972473
Epoch 380, training loss: 86.51863098144531 = 0.9178013205528259 + 10.0 * 8.560083389282227
Epoch 380, val loss: 0.9210470914840698
Epoch 390, training loss: 86.45116424560547 = 0.9091662764549255 + 10.0 * 8.554200172424316
Epoch 390, val loss: 0.9126215577125549
Epoch 400, training loss: 86.39458465576172 = 0.9002645015716553 + 10.0 * 8.549432754516602
Epoch 400, val loss: 0.9039389491081238
Epoch 410, training loss: 86.3404541015625 = 0.8911105394363403 + 10.0 * 8.544934272766113
Epoch 410, val loss: 0.8949893712997437
Epoch 420, training loss: 86.33221435546875 = 0.8817254900932312 + 10.0 * 8.545048713684082
Epoch 420, val loss: 0.8858557343482971
Epoch 430, training loss: 86.25224304199219 = 0.87202388048172 + 10.0 * 8.5380220413208
Epoch 430, val loss: 0.8763507604598999
Epoch 440, training loss: 86.19703674316406 = 0.8622462749481201 + 10.0 * 8.533479690551758
Epoch 440, val loss: 0.8668262958526611
Epoch 450, training loss: 86.14661407470703 = 0.8523163199424744 + 10.0 * 8.52942943572998
Epoch 450, val loss: 0.8571654558181763
Epoch 460, training loss: 86.1045150756836 = 0.842221200466156 + 10.0 * 8.526228904724121
Epoch 460, val loss: 0.8473007678985596
Epoch 470, training loss: 86.06299591064453 = 0.8321484923362732 + 10.0 * 8.52308464050293
Epoch 470, val loss: 0.8375533819198608
Epoch 480, training loss: 86.00988006591797 = 0.8220803141593933 + 10.0 * 8.518779754638672
Epoch 480, val loss: 0.8277740478515625
Epoch 490, training loss: 85.96700286865234 = 0.8119860887527466 + 10.0 * 8.515501022338867
Epoch 490, val loss: 0.8179879188537598
Epoch 500, training loss: 85.93054962158203 = 0.8018600344657898 + 10.0 * 8.512868881225586
Epoch 500, val loss: 0.8082348704338074
Epoch 510, training loss: 85.90836334228516 = 0.7917287349700928 + 10.0 * 8.511663436889648
Epoch 510, val loss: 0.7984046936035156
Epoch 520, training loss: 85.84466552734375 = 0.7817208170890808 + 10.0 * 8.506294250488281
Epoch 520, val loss: 0.7887919545173645
Epoch 530, training loss: 85.80770111083984 = 0.7718555331230164 + 10.0 * 8.503583908081055
Epoch 530, val loss: 0.7792937755584717
Epoch 540, training loss: 85.76122283935547 = 0.7620506882667542 + 10.0 * 8.499917030334473
Epoch 540, val loss: 0.7699234485626221
Epoch 550, training loss: 85.72671508789062 = 0.7524977922439575 + 10.0 * 8.497422218322754
Epoch 550, val loss: 0.7607809901237488
Epoch 560, training loss: 85.68482208251953 = 0.7431495785713196 + 10.0 * 8.49416732788086
Epoch 560, val loss: 0.7519069314002991
Epoch 570, training loss: 85.64493560791016 = 0.734006941318512 + 10.0 * 8.491092681884766
Epoch 570, val loss: 0.7432043552398682
Epoch 580, training loss: 85.68574523925781 = 0.725068211555481 + 10.0 * 8.496068000793457
Epoch 580, val loss: 0.7346734404563904
Epoch 590, training loss: 85.58160400390625 = 0.7163114547729492 + 10.0 * 8.486529350280762
Epoch 590, val loss: 0.7264959216117859
Epoch 600, training loss: 85.53704071044922 = 0.7079331278800964 + 10.0 * 8.482911109924316
Epoch 600, val loss: 0.7186775803565979
Epoch 610, training loss: 85.4968032836914 = 0.6997610926628113 + 10.0 * 8.479703903198242
Epoch 610, val loss: 0.7110362648963928
Epoch 620, training loss: 85.460693359375 = 0.6919242739677429 + 10.0 * 8.476877212524414
Epoch 620, val loss: 0.7037745118141174
Epoch 630, training loss: 85.5020980834961 = 0.684321403503418 + 10.0 * 8.481778144836426
Epoch 630, val loss: 0.6966816782951355
Epoch 640, training loss: 85.41177368164062 = 0.6768951416015625 + 10.0 * 8.473487854003906
Epoch 640, val loss: 0.689897358417511
Epoch 650, training loss: 85.36981201171875 = 0.6699079871177673 + 10.0 * 8.469990730285645
Epoch 650, val loss: 0.6835347414016724
Epoch 660, training loss: 85.33303833007812 = 0.6631644368171692 + 10.0 * 8.466987609863281
Epoch 660, val loss: 0.6773645281791687
Epoch 670, training loss: 85.30340576171875 = 0.6567062735557556 + 10.0 * 8.464670181274414
Epoch 670, val loss: 0.6715330481529236
Epoch 680, training loss: 85.32335662841797 = 0.6504234075546265 + 10.0 * 8.467293739318848
Epoch 680, val loss: 0.6658657193183899
Epoch 690, training loss: 85.2537612915039 = 0.6444504857063293 + 10.0 * 8.460931777954102
Epoch 690, val loss: 0.6605051755905151
Epoch 700, training loss: 85.22015380859375 = 0.6387357115745544 + 10.0 * 8.458142280578613
Epoch 700, val loss: 0.655447244644165
Epoch 710, training loss: 85.19233703613281 = 0.6332731246948242 + 10.0 * 8.455906867980957
Epoch 710, val loss: 0.6505873203277588
Epoch 720, training loss: 85.16791534423828 = 0.628017008304596 + 10.0 * 8.45398998260498
Epoch 720, val loss: 0.64601731300354
Epoch 730, training loss: 85.21760559082031 = 0.6228875517845154 + 10.0 * 8.459471702575684
Epoch 730, val loss: 0.6415390372276306
Epoch 740, training loss: 85.13382720947266 = 0.6179867386817932 + 10.0 * 8.451583862304688
Epoch 740, val loss: 0.6372958421707153
Epoch 750, training loss: 85.10224151611328 = 0.6133768558502197 + 10.0 * 8.448885917663574
Epoch 750, val loss: 0.6333638429641724
Epoch 760, training loss: 85.0823745727539 = 0.6089373826980591 + 10.0 * 8.447343826293945
Epoch 760, val loss: 0.6295800805091858
Epoch 770, training loss: 85.07340240478516 = 0.6046535968780518 + 10.0 * 8.446874618530273
Epoch 770, val loss: 0.6259917616844177
Epoch 780, training loss: 85.05498504638672 = 0.6004937291145325 + 10.0 * 8.445448875427246
Epoch 780, val loss: 0.6224117279052734
Epoch 790, training loss: 85.04265594482422 = 0.5965169072151184 + 10.0 * 8.444613456726074
Epoch 790, val loss: 0.6192409992218018
Epoch 800, training loss: 85.0166015625 = 0.5927274227142334 + 10.0 * 8.442387580871582
Epoch 800, val loss: 0.6160690188407898
Epoch 810, training loss: 85.00191497802734 = 0.5890646576881409 + 10.0 * 8.441285133361816
Epoch 810, val loss: 0.613068163394928
Epoch 820, training loss: 84.98950958251953 = 0.5855355858802795 + 10.0 * 8.440397262573242
Epoch 820, val loss: 0.6102453470230103
Epoch 830, training loss: 85.03958892822266 = 0.5821013450622559 + 10.0 * 8.445749282836914
Epoch 830, val loss: 0.6074479818344116
Epoch 840, training loss: 84.97816467285156 = 0.5787217020988464 + 10.0 * 8.43994426727295
Epoch 840, val loss: 0.6048504114151001
Epoch 850, training loss: 84.95323181152344 = 0.5755277276039124 + 10.0 * 8.437769889831543
Epoch 850, val loss: 0.6023263335227966
Epoch 860, training loss: 84.94200897216797 = 0.5724186301231384 + 10.0 * 8.436959266662598
Epoch 860, val loss: 0.5999206900596619
Epoch 870, training loss: 84.93000030517578 = 0.569410502910614 + 10.0 * 8.43605899810791
Epoch 870, val loss: 0.5976220965385437
Epoch 880, training loss: 84.93941497802734 = 0.5664743185043335 + 10.0 * 8.437294006347656
Epoch 880, val loss: 0.5953670144081116
Epoch 890, training loss: 84.92967224121094 = 0.5635625123977661 + 10.0 * 8.43661117553711
Epoch 890, val loss: 0.5931918025016785
Epoch 900, training loss: 84.90397644042969 = 0.5607738494873047 + 10.0 * 8.434320449829102
Epoch 900, val loss: 0.591137707233429
Epoch 910, training loss: 84.89257049560547 = 0.5580681562423706 + 10.0 * 8.433450698852539
Epoch 910, val loss: 0.5891464948654175
Epoch 920, training loss: 84.88424682617188 = 0.555435299873352 + 10.0 * 8.432881355285645
Epoch 920, val loss: 0.5872408151626587
Epoch 930, training loss: 84.8858642578125 = 0.5528661012649536 + 10.0 * 8.433300018310547
Epoch 930, val loss: 0.5853680372238159
Epoch 940, training loss: 84.87631225585938 = 0.5503201484680176 + 10.0 * 8.432599067687988
Epoch 940, val loss: 0.5835719108581543
Epoch 950, training loss: 84.8589096069336 = 0.547856867313385 + 10.0 * 8.43110466003418
Epoch 950, val loss: 0.5818567872047424
Epoch 960, training loss: 84.85135650634766 = 0.5454607605934143 + 10.0 * 8.43058967590332
Epoch 960, val loss: 0.5801780223846436
Epoch 970, training loss: 84.84317779541016 = 0.5431339740753174 + 10.0 * 8.430004119873047
Epoch 970, val loss: 0.5785523653030396
Epoch 980, training loss: 84.83853149414062 = 0.5408511757850647 + 10.0 * 8.429768562316895
Epoch 980, val loss: 0.5769798159599304
Epoch 990, training loss: 84.84063720703125 = 0.5385910868644714 + 10.0 * 8.430204391479492
Epoch 990, val loss: 0.57540363073349
Epoch 1000, training loss: 84.82289123535156 = 0.5363741517066956 + 10.0 * 8.428651809692383
Epoch 1000, val loss: 0.5739423632621765
Epoch 1010, training loss: 84.81551361083984 = 0.5342103242874146 + 10.0 * 8.428130149841309
Epoch 1010, val loss: 0.5724745392799377
Epoch 1020, training loss: 84.82672119140625 = 0.5320957899093628 + 10.0 * 8.429462432861328
Epoch 1020, val loss: 0.5710561275482178
Epoch 1030, training loss: 84.80408477783203 = 0.529982328414917 + 10.0 * 8.427410125732422
Epoch 1030, val loss: 0.5696647763252258
Epoch 1040, training loss: 84.79698181152344 = 0.5279437899589539 + 10.0 * 8.42690372467041
Epoch 1040, val loss: 0.5682915449142456
Epoch 1050, training loss: 84.78572845458984 = 0.525945246219635 + 10.0 * 8.42597770690918
Epoch 1050, val loss: 0.5669660568237305
Epoch 1060, training loss: 84.77753448486328 = 0.5239937901496887 + 10.0 * 8.42535400390625
Epoch 1060, val loss: 0.5656858086585999
Epoch 1070, training loss: 84.81804656982422 = 0.52205491065979 + 10.0 * 8.429598808288574
Epoch 1070, val loss: 0.5643497109413147
Epoch 1080, training loss: 84.77708435058594 = 0.5201110243797302 + 10.0 * 8.425697326660156
Epoch 1080, val loss: 0.5632418394088745
Epoch 1090, training loss: 84.75658416748047 = 0.518233060836792 + 10.0 * 8.423834800720215
Epoch 1090, val loss: 0.5619593262672424
Epoch 1100, training loss: 84.7461929321289 = 0.5163887143135071 + 10.0 * 8.422980308532715
Epoch 1100, val loss: 0.5607816576957703
Epoch 1110, training loss: 84.73851013183594 = 0.5145713686943054 + 10.0 * 8.422393798828125
Epoch 1110, val loss: 0.55964595079422
Epoch 1120, training loss: 84.73479461669922 = 0.5127794742584229 + 10.0 * 8.422201156616211
Epoch 1120, val loss: 0.558504045009613
Epoch 1130, training loss: 84.7644271850586 = 0.5109917521476746 + 10.0 * 8.42534351348877
Epoch 1130, val loss: 0.5573165416717529
Epoch 1140, training loss: 84.7203598022461 = 0.5091665387153625 + 10.0 * 8.421119689941406
Epoch 1140, val loss: 0.5562164187431335
Epoch 1150, training loss: 84.71387481689453 = 0.5074018836021423 + 10.0 * 8.420647621154785
Epoch 1150, val loss: 0.5550898313522339
Epoch 1160, training loss: 84.70523834228516 = 0.5056822299957275 + 10.0 * 8.419955253601074
Epoch 1160, val loss: 0.5540059208869934
Epoch 1170, training loss: 84.69537353515625 = 0.5039966702461243 + 10.0 * 8.419137954711914
Epoch 1170, val loss: 0.552984893321991
Epoch 1180, training loss: 84.68824768066406 = 0.5023308992385864 + 10.0 * 8.418591499328613
Epoch 1180, val loss: 0.5519132614135742
Epoch 1190, training loss: 84.68475341796875 = 0.5006759166717529 + 10.0 * 8.418407440185547
Epoch 1190, val loss: 0.5508816838264465
Epoch 1200, training loss: 84.73184204101562 = 0.4990149438381195 + 10.0 * 8.423282623291016
Epoch 1200, val loss: 0.5498014092445374
Epoch 1210, training loss: 84.68730163574219 = 0.49732083082199097 + 10.0 * 8.418997764587402
Epoch 1210, val loss: 0.548790454864502
Epoch 1220, training loss: 84.6697998046875 = 0.4956834316253662 + 10.0 * 8.417411804199219
Epoch 1220, val loss: 0.5478185415267944
Epoch 1230, training loss: 84.6573486328125 = 0.49410104751586914 + 10.0 * 8.416324615478516
Epoch 1230, val loss: 0.546815037727356
Epoch 1240, training loss: 84.64757537841797 = 0.49253085255622864 + 10.0 * 8.415504455566406
Epoch 1240, val loss: 0.5458605885505676
Epoch 1250, training loss: 84.6399917602539 = 0.4909662902355194 + 10.0 * 8.414902687072754
Epoch 1250, val loss: 0.5448971390724182
Epoch 1260, training loss: 84.66893005371094 = 0.489411860704422 + 10.0 * 8.417951583862305
Epoch 1260, val loss: 0.5438751578330994
Epoch 1270, training loss: 84.6764907836914 = 0.48780956864356995 + 10.0 * 8.418868064880371
Epoch 1270, val loss: 0.5429951548576355
Epoch 1280, training loss: 84.62460327148438 = 0.4862560033798218 + 10.0 * 8.413834571838379
Epoch 1280, val loss: 0.5420868992805481
Epoch 1290, training loss: 84.61493682861328 = 0.48473596572875977 + 10.0 * 8.413020133972168
Epoch 1290, val loss: 0.5410895943641663
Epoch 1300, training loss: 84.60899353027344 = 0.4832395613193512 + 10.0 * 8.412575721740723
Epoch 1300, val loss: 0.5401918888092041
Epoch 1310, training loss: 84.61176300048828 = 0.48175162076950073 + 10.0 * 8.41300106048584
Epoch 1310, val loss: 0.5392747521400452
Epoch 1320, training loss: 84.61834716796875 = 0.480253130197525 + 10.0 * 8.413808822631836
Epoch 1320, val loss: 0.538360059261322
Epoch 1330, training loss: 84.5987548828125 = 0.4787672758102417 + 10.0 * 8.411998748779297
Epoch 1330, val loss: 0.5375102758407593
Epoch 1340, training loss: 84.58013153076172 = 0.4773011803627014 + 10.0 * 8.410283088684082
Epoch 1340, val loss: 0.536577582359314
Epoch 1350, training loss: 84.57779693603516 = 0.47584667801856995 + 10.0 * 8.410195350646973
Epoch 1350, val loss: 0.5356876850128174
Epoch 1360, training loss: 84.58322143554688 = 0.4743979573249817 + 10.0 * 8.410882949829102
Epoch 1360, val loss: 0.5348290801048279
Epoch 1370, training loss: 84.56751251220703 = 0.4729284346103668 + 10.0 * 8.40945816040039
Epoch 1370, val loss: 0.5339477062225342
Epoch 1380, training loss: 84.56707763671875 = 0.471483051776886 + 10.0 * 8.40955924987793
Epoch 1380, val loss: 0.5330923199653625
Epoch 1390, training loss: 84.54771423339844 = 0.4700632095336914 + 10.0 * 8.40776538848877
Epoch 1390, val loss: 0.5321985483169556
Epoch 1400, training loss: 84.54602813720703 = 0.4686577618122101 + 10.0 * 8.407736778259277
Epoch 1400, val loss: 0.531366765499115
Epoch 1410, training loss: 84.59883880615234 = 0.46725189685821533 + 10.0 * 8.413158416748047
Epoch 1410, val loss: 0.5304855704307556
Epoch 1420, training loss: 84.5504150390625 = 0.4658195376396179 + 10.0 * 8.408459663391113
Epoch 1420, val loss: 0.5297511219978333
Epoch 1430, training loss: 84.53102111816406 = 0.4644307494163513 + 10.0 * 8.406659126281738
Epoch 1430, val loss: 0.5288441777229309
Epoch 1440, training loss: 84.53108978271484 = 0.46304845809936523 + 10.0 * 8.406804084777832
Epoch 1440, val loss: 0.5280464291572571
Epoch 1450, training loss: 84.51651000976562 = 0.4616732597351074 + 10.0 * 8.405484199523926
Epoch 1450, val loss: 0.5272261500358582
Epoch 1460, training loss: 84.50526428222656 = 0.46030154824256897 + 10.0 * 8.404496192932129
Epoch 1460, val loss: 0.5264533758163452
Epoch 1470, training loss: 84.50183868408203 = 0.45893844962120056 + 10.0 * 8.404290199279785
Epoch 1470, val loss: 0.5256398320198059
Epoch 1480, training loss: 84.5107192993164 = 0.4575752019882202 + 10.0 * 8.405314445495605
Epoch 1480, val loss: 0.5248671770095825
Epoch 1490, training loss: 84.49886322021484 = 0.4562062919139862 + 10.0 * 8.404265403747559
Epoch 1490, val loss: 0.5240331888198853
Epoch 1500, training loss: 84.4935531616211 = 0.4548426866531372 + 10.0 * 8.403871536254883
Epoch 1500, val loss: 0.5232317447662354
Epoch 1510, training loss: 84.47742462158203 = 0.4534967243671417 + 10.0 * 8.402392387390137
Epoch 1510, val loss: 0.5224452018737793
Epoch 1520, training loss: 84.47332000732422 = 0.45216187834739685 + 10.0 * 8.402115821838379
Epoch 1520, val loss: 0.5216418504714966
Epoch 1530, training loss: 84.4713363647461 = 0.45083850622177124 + 10.0 * 8.402050018310547
Epoch 1530, val loss: 0.5208752155303955
Epoch 1540, training loss: 84.49552917480469 = 0.44951149821281433 + 10.0 * 8.40460205078125
Epoch 1540, val loss: 0.520085871219635
Epoch 1550, training loss: 84.45794677734375 = 0.44817981123924255 + 10.0 * 8.400976181030273
Epoch 1550, val loss: 0.5194047689437866
Epoch 1560, training loss: 84.44873809814453 = 0.44687655568122864 + 10.0 * 8.400186538696289
Epoch 1560, val loss: 0.5186359286308289
Epoch 1570, training loss: 84.47132110595703 = 0.445576548576355 + 10.0 * 8.40257453918457
Epoch 1570, val loss: 0.5179121494293213
Epoch 1580, training loss: 84.44798278808594 = 0.4442676901817322 + 10.0 * 8.400371551513672
Epoch 1580, val loss: 0.517177164554596
Epoch 1590, training loss: 84.4359130859375 = 0.4429774880409241 + 10.0 * 8.399293899536133
Epoch 1590, val loss: 0.516442596912384
Epoch 1600, training loss: 84.428466796875 = 0.44169777631759644 + 10.0 * 8.398676872253418
Epoch 1600, val loss: 0.5157142281532288
Epoch 1610, training loss: 84.44576263427734 = 0.4404207170009613 + 10.0 * 8.400533676147461
Epoch 1610, val loss: 0.5149915218353271
Epoch 1620, training loss: 84.43726348876953 = 0.4391257166862488 + 10.0 * 8.399813652038574
Epoch 1620, val loss: 0.5143524408340454
Epoch 1630, training loss: 84.44273376464844 = 0.4378422796726227 + 10.0 * 8.40048885345459
Epoch 1630, val loss: 0.5136097073554993
Epoch 1640, training loss: 84.41246795654297 = 0.43656378984451294 + 10.0 * 8.397590637207031
Epoch 1640, val loss: 0.5130024552345276
Epoch 1650, training loss: 84.40278625488281 = 0.43531176447868347 + 10.0 * 8.396747589111328
Epoch 1650, val loss: 0.512337863445282
Epoch 1660, training loss: 84.39631652832031 = 0.4340611696243286 + 10.0 * 8.396225929260254
Epoch 1660, val loss: 0.5116856098175049
Epoch 1670, training loss: 84.39037322998047 = 0.4328133463859558 + 10.0 * 8.395755767822266
Epoch 1670, val loss: 0.5110511779785156
Epoch 1680, training loss: 84.39299011230469 = 0.4315662682056427 + 10.0 * 8.396142959594727
Epoch 1680, val loss: 0.5104213953018188
Epoch 1690, training loss: 84.44638061523438 = 0.43030697107315063 + 10.0 * 8.401607513427734
Epoch 1690, val loss: 0.509751558303833
Epoch 1700, training loss: 84.39488983154297 = 0.4290231168270111 + 10.0 * 8.396586418151855
Epoch 1700, val loss: 0.509212851524353
Epoch 1710, training loss: 84.37554931640625 = 0.42778080701828003 + 10.0 * 8.394777297973633
Epoch 1710, val loss: 0.5085434317588806
Epoch 1720, training loss: 84.36819458007812 = 0.42653730511665344 + 10.0 * 8.394165992736816
Epoch 1720, val loss: 0.5079746246337891
Epoch 1730, training loss: 84.3648452758789 = 0.4253040552139282 + 10.0 * 8.393954277038574
Epoch 1730, val loss: 0.5073733329772949
Epoch 1740, training loss: 84.40179443359375 = 0.42406633496284485 + 10.0 * 8.397772789001465
Epoch 1740, val loss: 0.506794810295105
Epoch 1750, training loss: 84.36734771728516 = 0.4228053092956543 + 10.0 * 8.394454002380371
Epoch 1750, val loss: 0.5062465071678162
Epoch 1760, training loss: 84.36055755615234 = 0.42156359553337097 + 10.0 * 8.393899917602539
Epoch 1760, val loss: 0.5056520700454712
Epoch 1770, training loss: 84.34647369384766 = 0.4203294515609741 + 10.0 * 8.392614364624023
Epoch 1770, val loss: 0.5051456093788147
Epoch 1780, training loss: 84.3451156616211 = 0.4190976321697235 + 10.0 * 8.39260196685791
Epoch 1780, val loss: 0.5046069025993347
Epoch 1790, training loss: 84.34700012207031 = 0.4178600013256073 + 10.0 * 8.392913818359375
Epoch 1790, val loss: 0.5040920376777649
Epoch 1800, training loss: 84.38245391845703 = 0.4166145920753479 + 10.0 * 8.396583557128906
Epoch 1800, val loss: 0.5035104155540466
Epoch 1810, training loss: 84.33848571777344 = 0.4153457581996918 + 10.0 * 8.392313957214355
Epoch 1810, val loss: 0.5031026601791382
Epoch 1820, training loss: 84.328125 = 0.41411012411117554 + 10.0 * 8.391401290893555
Epoch 1820, val loss: 0.5025334358215332
Epoch 1830, training loss: 84.32176208496094 = 0.41287747025489807 + 10.0 * 8.390888214111328
Epoch 1830, val loss: 0.5021189451217651
Epoch 1840, training loss: 84.31707000732422 = 0.41165050864219666 + 10.0 * 8.390542030334473
Epoch 1840, val loss: 0.501637876033783
Epoch 1850, training loss: 84.31653594970703 = 0.4104219675064087 + 10.0 * 8.39061164855957
Epoch 1850, val loss: 0.5011700987815857
Epoch 1860, training loss: 84.37533569335938 = 0.4091837406158447 + 10.0 * 8.396615028381348
Epoch 1860, val loss: 0.5007078051567078
Epoch 1870, training loss: 84.32587432861328 = 0.40792322158813477 + 10.0 * 8.39179515838623
Epoch 1870, val loss: 0.500290036201477
Epoch 1880, training loss: 84.30961608886719 = 0.40669137239456177 + 10.0 * 8.390292167663574
Epoch 1880, val loss: 0.4998471736907959
Epoch 1890, training loss: 84.30069732666016 = 0.405462384223938 + 10.0 * 8.38952350616455
Epoch 1890, val loss: 0.4994184374809265
Epoch 1900, training loss: 84.29656219482422 = 0.4042419493198395 + 10.0 * 8.38923168182373
Epoch 1900, val loss: 0.4990491271018982
Epoch 1910, training loss: 84.3006591796875 = 0.40301570296287537 + 10.0 * 8.389764785766602
Epoch 1910, val loss: 0.49866533279418945
Epoch 1920, training loss: 84.3320541381836 = 0.40178054571151733 + 10.0 * 8.393027305603027
Epoch 1920, val loss: 0.4983106553554535
Epoch 1930, training loss: 84.29893493652344 = 0.4005359709262848 + 10.0 * 8.389840126037598
Epoch 1930, val loss: 0.49802228808403015
Epoch 1940, training loss: 84.28588104248047 = 0.3993103802204132 + 10.0 * 8.388657569885254
Epoch 1940, val loss: 0.49771955609321594
Epoch 1950, training loss: 84.28015899658203 = 0.3980940580368042 + 10.0 * 8.388206481933594
Epoch 1950, val loss: 0.49740836024284363
Epoch 1960, training loss: 84.2767333984375 = 0.3968825340270996 + 10.0 * 8.387985229492188
Epoch 1960, val loss: 0.49713510274887085
Epoch 1970, training loss: 84.2881851196289 = 0.3956708610057831 + 10.0 * 8.389251708984375
Epoch 1970, val loss: 0.49690476059913635
Epoch 1980, training loss: 84.29119873046875 = 0.3944433331489563 + 10.0 * 8.389676094055176
Epoch 1980, val loss: 0.4966535270214081
Epoch 1990, training loss: 84.27056884765625 = 0.39323490858078003 + 10.0 * 8.387733459472656
Epoch 1990, val loss: 0.49633103609085083
Epoch 2000, training loss: 84.26314544677734 = 0.39204010367393494 + 10.0 * 8.387110710144043
Epoch 2000, val loss: 0.4961268901824951
Epoch 2010, training loss: 84.26132202148438 = 0.390852153301239 + 10.0 * 8.387046813964844
Epoch 2010, val loss: 0.4959217309951782
Epoch 2020, training loss: 84.28219604492188 = 0.38966068625450134 + 10.0 * 8.389253616333008
Epoch 2020, val loss: 0.49573054909706116
Epoch 2030, training loss: 84.26705169677734 = 0.3884522318840027 + 10.0 * 8.387860298156738
Epoch 2030, val loss: 0.4955044388771057
Epoch 2040, training loss: 84.25406646728516 = 0.387252539396286 + 10.0 * 8.38668155670166
Epoch 2040, val loss: 0.4953482151031494
Epoch 2050, training loss: 84.24795532226562 = 0.3860645294189453 + 10.0 * 8.386189460754395
Epoch 2050, val loss: 0.49518659710884094
Epoch 2060, training loss: 84.24614715576172 = 0.38488107919692993 + 10.0 * 8.386126518249512
Epoch 2060, val loss: 0.49505993723869324
Epoch 2070, training loss: 84.25931549072266 = 0.3836956322193146 + 10.0 * 8.387561798095703
Epoch 2070, val loss: 0.49495863914489746
Epoch 2080, training loss: 84.2563247680664 = 0.3825046718120575 + 10.0 * 8.387381553649902
Epoch 2080, val loss: 0.49480628967285156
Epoch 2090, training loss: 84.2374496459961 = 0.3813243806362152 + 10.0 * 8.385612487792969
Epoch 2090, val loss: 0.4946741759777069
Epoch 2100, training loss: 84.23340606689453 = 0.3801444470882416 + 10.0 * 8.385326385498047
Epoch 2100, val loss: 0.4945948123931885
Epoch 2110, training loss: 84.23078155517578 = 0.3789658546447754 + 10.0 * 8.385181427001953
Epoch 2110, val loss: 0.49449363350868225
Epoch 2120, training loss: 84.23371887207031 = 0.3777914047241211 + 10.0 * 8.385592460632324
Epoch 2120, val loss: 0.49444907903671265
Epoch 2130, training loss: 84.27210998535156 = 0.37660926580429077 + 10.0 * 8.38955020904541
Epoch 2130, val loss: 0.49439024925231934
Epoch 2140, training loss: 84.24357604980469 = 0.37540507316589355 + 10.0 * 8.38681697845459
Epoch 2140, val loss: 0.4944416582584381
Epoch 2150, training loss: 84.22174835205078 = 0.37422996759414673 + 10.0 * 8.38475227355957
Epoch 2150, val loss: 0.49436232447624207
Epoch 2160, training loss: 84.21333312988281 = 0.3730598986148834 + 10.0 * 8.384027481079102
Epoch 2160, val loss: 0.49439531564712524
Epoch 2170, training loss: 84.20960998535156 = 0.37190136313438416 + 10.0 * 8.383770942687988
Epoch 2170, val loss: 0.4944356381893158
Epoch 2180, training loss: 84.20829772949219 = 0.3707445561885834 + 10.0 * 8.383755683898926
Epoch 2180, val loss: 0.49449053406715393
Epoch 2190, training loss: 84.24688720703125 = 0.3695841431617737 + 10.0 * 8.387730598449707
Epoch 2190, val loss: 0.4946117699146271
Epoch 2200, training loss: 84.21411895751953 = 0.36840999126434326 + 10.0 * 8.384571075439453
Epoch 2200, val loss: 0.49466031789779663
Epoch 2210, training loss: 84.2035140991211 = 0.36726000905036926 + 10.0 * 8.383625030517578
Epoch 2210, val loss: 0.49474310874938965
Epoch 2220, training loss: 84.21826934814453 = 0.36611902713775635 + 10.0 * 8.385214805603027
Epoch 2220, val loss: 0.49481576681137085
Epoch 2230, training loss: 84.20232391357422 = 0.36497268080711365 + 10.0 * 8.383734703063965
Epoch 2230, val loss: 0.49498844146728516
Epoch 2240, training loss: 84.1971435546875 = 0.3638383448123932 + 10.0 * 8.383330345153809
Epoch 2240, val loss: 0.4951137602329254
Epoch 2250, training loss: 84.18560028076172 = 0.36272308230400085 + 10.0 * 8.382287979125977
Epoch 2250, val loss: 0.49526771903038025
Epoch 2260, training loss: 84.18183135986328 = 0.3616122305393219 + 10.0 * 8.3820219039917
Epoch 2260, val loss: 0.49541616439819336
Epoch 2270, training loss: 84.19525146484375 = 0.36050358414649963 + 10.0 * 8.383474349975586
Epoch 2270, val loss: 0.4955916702747345
Epoch 2280, training loss: 84.18067932128906 = 0.3593863248825073 + 10.0 * 8.382129669189453
Epoch 2280, val loss: 0.49575600028038025
Epoch 2290, training loss: 84.17405700683594 = 0.35828205943107605 + 10.0 * 8.381577491760254
Epoch 2290, val loss: 0.4959307909011841
Epoch 2300, training loss: 84.17120361328125 = 0.3571804463863373 + 10.0 * 8.381402969360352
Epoch 2300, val loss: 0.4961395263671875
Epoch 2310, training loss: 84.18103790283203 = 0.3560762107372284 + 10.0 * 8.382495880126953
Epoch 2310, val loss: 0.49639320373535156
Epoch 2320, training loss: 84.17945098876953 = 0.35497426986694336 + 10.0 * 8.382448196411133
Epoch 2320, val loss: 0.4966431260108948
Epoch 2330, training loss: 84.18510437011719 = 0.35387033224105835 + 10.0 * 8.383123397827148
Epoch 2330, val loss: 0.4968724250793457
Epoch 2340, training loss: 84.15864562988281 = 0.352783203125 + 10.0 * 8.380586624145508
Epoch 2340, val loss: 0.4971093535423279
Epoch 2350, training loss: 84.15560150146484 = 0.3516985774040222 + 10.0 * 8.380390167236328
Epoch 2350, val loss: 0.4973767101764679
Epoch 2360, training loss: 84.14981842041016 = 0.3506166338920593 + 10.0 * 8.37992000579834
Epoch 2360, val loss: 0.49764034152030945
Epoch 2370, training loss: 84.15362548828125 = 0.34954363107681274 + 10.0 * 8.38040828704834
Epoch 2370, val loss: 0.49788373708724976
Epoch 2380, training loss: 84.20061492919922 = 0.34847041964530945 + 10.0 * 8.385213851928711
Epoch 2380, val loss: 0.49815210700035095
Epoch 2390, training loss: 84.1551284790039 = 0.34737417101860046 + 10.0 * 8.380775451660156
Epoch 2390, val loss: 0.4985474944114685
Epoch 2400, training loss: 84.13935089111328 = 0.34631162881851196 + 10.0 * 8.379303932189941
Epoch 2400, val loss: 0.4988139867782593
Epoch 2410, training loss: 84.13255310058594 = 0.3452453017234802 + 10.0 * 8.378730773925781
Epoch 2410, val loss: 0.4991701543331146
Epoch 2420, training loss: 84.1488265991211 = 0.3441818952560425 + 10.0 * 8.380464553833008
Epoch 2420, val loss: 0.4995591640472412
Epoch 2430, training loss: 84.13867950439453 = 0.34311825037002563 + 10.0 * 8.379556655883789
Epoch 2430, val loss: 0.4998481571674347
Epoch 2440, training loss: 84.1325454711914 = 0.34204769134521484 + 10.0 * 8.379049301147461
Epoch 2440, val loss: 0.5002005100250244
Epoch 2450, training loss: 84.12631225585938 = 0.34099987149238586 + 10.0 * 8.378530502319336
Epoch 2450, val loss: 0.5005167722702026
Epoch 2460, training loss: 84.11833190917969 = 0.3399445414543152 + 10.0 * 8.377839088439941
Epoch 2460, val loss: 0.500900387763977
Epoch 2470, training loss: 84.1246337890625 = 0.3388860821723938 + 10.0 * 8.378575325012207
Epoch 2470, val loss: 0.5012917518615723
Epoch 2480, training loss: 84.13182830810547 = 0.3378157913684845 + 10.0 * 8.379401206970215
Epoch 2480, val loss: 0.5016831755638123
Epoch 2490, training loss: 84.11003875732422 = 0.33676087856292725 + 10.0 * 8.377327919006348
Epoch 2490, val loss: 0.5020349621772766
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.800101471334348
0.8146055205390134
=== training gcn model ===
Epoch 0, training loss: 106.92964935302734 = 1.1066794395446777 + 10.0 * 10.582296371459961
Epoch 0, val loss: 1.1045432090759277
Epoch 10, training loss: 106.92231750488281 = 1.1014705896377563 + 10.0 * 10.582084655761719
Epoch 10, val loss: 1.0994573831558228
Epoch 20, training loss: 106.90656280517578 = 1.0961370468139648 + 10.0 * 10.581042289733887
Epoch 20, val loss: 1.0942212343215942
Epoch 30, training loss: 106.84923553466797 = 1.0902619361877441 + 10.0 * 10.575897216796875
Epoch 30, val loss: 1.0884485244750977
Epoch 40, training loss: 106.63015747070312 = 1.0837433338165283 + 10.0 * 10.554641723632812
Epoch 40, val loss: 1.0821083784103394
Epoch 50, training loss: 105.99616241455078 = 1.0768440961837769 + 10.0 * 10.491931915283203
Epoch 50, val loss: 1.0754746198654175
Epoch 60, training loss: 104.59236145019531 = 1.0703895092010498 + 10.0 * 10.352197647094727
Epoch 60, val loss: 1.0693964958190918
Epoch 70, training loss: 102.25880432128906 = 1.063948154449463 + 10.0 * 10.119485855102539
Epoch 70, val loss: 1.0632282495498657
Epoch 80, training loss: 100.71179962158203 = 1.0584959983825684 + 10.0 * 9.965330123901367
Epoch 80, val loss: 1.0581843852996826
Epoch 90, training loss: 98.21685791015625 = 1.0541719198226929 + 10.0 * 9.716268539428711
Epoch 90, val loss: 1.0539840459823608
Epoch 100, training loss: 95.85021209716797 = 1.0502644777297974 + 10.0 * 9.479994773864746
Epoch 100, val loss: 1.0500527620315552
Epoch 110, training loss: 94.00247192382812 = 1.0467593669891357 + 10.0 * 9.295571327209473
Epoch 110, val loss: 1.0465195178985596
Epoch 120, training loss: 92.96265411376953 = 1.043436050415039 + 10.0 * 9.191922187805176
Epoch 120, val loss: 1.0431174039840698
Epoch 130, training loss: 92.39663696289062 = 1.0399091243743896 + 10.0 * 9.135672569274902
Epoch 130, val loss: 1.039609670639038
Epoch 140, training loss: 91.62394714355469 = 1.0368757247924805 + 10.0 * 9.058707237243652
Epoch 140, val loss: 1.0367348194122314
Epoch 150, training loss: 90.97602081298828 = 1.0346862077713013 + 10.0 * 8.994133949279785
Epoch 150, val loss: 1.0345652103424072
Epoch 160, training loss: 90.61608123779297 = 1.0321122407913208 + 10.0 * 8.958396911621094
Epoch 160, val loss: 1.0318806171417236
Epoch 170, training loss: 90.16445922851562 = 1.0289533138275146 + 10.0 * 8.91355037689209
Epoch 170, val loss: 1.0286908149719238
Epoch 180, training loss: 89.7846450805664 = 1.0259100198745728 + 10.0 * 8.875873565673828
Epoch 180, val loss: 1.0255573987960815
Epoch 190, training loss: 89.5852279663086 = 1.022929310798645 + 10.0 * 8.856229782104492
Epoch 190, val loss: 1.022544503211975
Epoch 200, training loss: 89.40750122070312 = 1.0198049545288086 + 10.0 * 8.838769912719727
Epoch 200, val loss: 1.0193047523498535
Epoch 210, training loss: 89.19393920898438 = 1.0165376663208008 + 10.0 * 8.817739486694336
Epoch 210, val loss: 1.0159651041030884
Epoch 220, training loss: 88.94013977050781 = 1.013439655303955 + 10.0 * 8.792669296264648
Epoch 220, val loss: 1.0128679275512695
Epoch 230, training loss: 88.61367797851562 = 1.01064133644104 + 10.0 * 8.760303497314453
Epoch 230, val loss: 1.0100092887878418
Epoch 240, training loss: 88.26253509521484 = 1.0079634189605713 + 10.0 * 8.725457191467285
Epoch 240, val loss: 1.0073387622833252
Epoch 250, training loss: 87.89803314208984 = 1.0052478313446045 + 10.0 * 8.689278602600098
Epoch 250, val loss: 1.0044848918914795
Epoch 260, training loss: 87.62239837646484 = 1.0021870136260986 + 10.0 * 8.662020683288574
Epoch 260, val loss: 1.0012754201889038
Epoch 270, training loss: 87.3973617553711 = 0.9986650943756104 + 10.0 * 8.639869689941406
Epoch 270, val loss: 0.9977272152900696
Epoch 280, training loss: 87.22783660888672 = 0.9948182106018066 + 10.0 * 8.62330150604248
Epoch 280, val loss: 0.9937641620635986
Epoch 290, training loss: 87.08714294433594 = 0.990598738193512 + 10.0 * 8.609654426574707
Epoch 290, val loss: 0.9894767999649048
Epoch 300, training loss: 86.98160552978516 = 0.986003041267395 + 10.0 * 8.599560737609863
Epoch 300, val loss: 0.9848612546920776
Epoch 310, training loss: 86.87338256835938 = 0.9811317920684814 + 10.0 * 8.589224815368652
Epoch 310, val loss: 0.9798782467842102
Epoch 320, training loss: 86.77397918701172 = 0.9759127497673035 + 10.0 * 8.579806327819824
Epoch 320, val loss: 0.9746035933494568
Epoch 330, training loss: 86.68944549560547 = 0.9704212546348572 + 10.0 * 8.57190227508545
Epoch 330, val loss: 0.9690269231796265
Epoch 340, training loss: 86.62018585205078 = 0.9646055102348328 + 10.0 * 8.565557479858398
Epoch 340, val loss: 0.9631211757659912
Epoch 350, training loss: 86.57437133789062 = 0.958405077457428 + 10.0 * 8.561596870422363
Epoch 350, val loss: 0.9568885564804077
Epoch 360, training loss: 86.5003890991211 = 0.9518429636955261 + 10.0 * 8.554854393005371
Epoch 360, val loss: 0.9502930641174316
Epoch 370, training loss: 86.44124603271484 = 0.9449674487113953 + 10.0 * 8.549627304077148
Epoch 370, val loss: 0.9433807134628296
Epoch 380, training loss: 86.39195251464844 = 0.937789797782898 + 10.0 * 8.545415878295898
Epoch 380, val loss: 0.9361803531646729
Epoch 390, training loss: 86.34425354003906 = 0.9302951693534851 + 10.0 * 8.54139518737793
Epoch 390, val loss: 0.9287013411521912
Epoch 400, training loss: 86.30046844482422 = 0.9224751591682434 + 10.0 * 8.537798881530762
Epoch 400, val loss: 0.9209113121032715
Epoch 410, training loss: 86.25529479980469 = 0.9144198298454285 + 10.0 * 8.534087181091309
Epoch 410, val loss: 0.9129490256309509
Epoch 420, training loss: 86.21086120605469 = 0.9061399102210999 + 10.0 * 8.530471801757812
Epoch 420, val loss: 0.9047751426696777
Epoch 430, training loss: 86.17781829833984 = 0.897628128528595 + 10.0 * 8.528018951416016
Epoch 430, val loss: 0.8963924646377563
Epoch 440, training loss: 86.1588363647461 = 0.8888276219367981 + 10.0 * 8.52700138092041
Epoch 440, val loss: 0.8878100514411926
Epoch 450, training loss: 86.0947036743164 = 0.8799232840538025 + 10.0 * 8.521478652954102
Epoch 450, val loss: 0.8791143298149109
Epoch 460, training loss: 86.06114196777344 = 0.8709404468536377 + 10.0 * 8.519020080566406
Epoch 460, val loss: 0.8703692555427551
Epoch 470, training loss: 86.02006530761719 = 0.8618142008781433 + 10.0 * 8.515825271606445
Epoch 470, val loss: 0.8615198731422424
Epoch 480, training loss: 85.98323059082031 = 0.8525830507278442 + 10.0 * 8.51306438446045
Epoch 480, val loss: 0.8526195883750916
Epoch 490, training loss: 85.9474868774414 = 0.8432603478431702 + 10.0 * 8.510422706604004
Epoch 490, val loss: 0.8436647653579712
Epoch 500, training loss: 85.91370391845703 = 0.8338696956634521 + 10.0 * 8.507983207702637
Epoch 500, val loss: 0.8346783518791199
Epoch 510, training loss: 85.88721466064453 = 0.8244140148162842 + 10.0 * 8.506279945373535
Epoch 510, val loss: 0.8256552815437317
Epoch 520, training loss: 85.87249755859375 = 0.8149758577346802 + 10.0 * 8.505751609802246
Epoch 520, val loss: 0.8167282342910767
Epoch 530, training loss: 85.82860565185547 = 0.8056888580322266 + 10.0 * 8.502291679382324
Epoch 530, val loss: 0.8080288171768188
Epoch 540, training loss: 85.79544067382812 = 0.7965076565742493 + 10.0 * 8.499893188476562
Epoch 540, val loss: 0.7994168400764465
Epoch 550, training loss: 85.76835632324219 = 0.7874379754066467 + 10.0 * 8.498091697692871
Epoch 550, val loss: 0.7909559011459351
Epoch 560, training loss: 85.8285903930664 = 0.7785300612449646 + 10.0 * 8.505006790161133
Epoch 560, val loss: 0.7825674414634705
Epoch 570, training loss: 85.73883819580078 = 0.7696644067764282 + 10.0 * 8.496917724609375
Epoch 570, val loss: 0.7744618654251099
Epoch 580, training loss: 85.69217681884766 = 0.7611441016197205 + 10.0 * 8.49310302734375
Epoch 580, val loss: 0.7667226791381836
Epoch 590, training loss: 85.65679168701172 = 0.7528589963912964 + 10.0 * 8.490392684936523
Epoch 590, val loss: 0.7592074871063232
Epoch 600, training loss: 85.62532806396484 = 0.7447924017906189 + 10.0 * 8.488054275512695
Epoch 600, val loss: 0.751915693283081
Epoch 610, training loss: 85.59262084960938 = 0.7369593381881714 + 10.0 * 8.485566139221191
Epoch 610, val loss: 0.7449051141738892
Epoch 620, training loss: 85.56702423095703 = 0.7293789982795715 + 10.0 * 8.4837646484375
Epoch 620, val loss: 0.7381672263145447
Epoch 630, training loss: 85.56187438964844 = 0.7219722867012024 + 10.0 * 8.483990669250488
Epoch 630, val loss: 0.7316730618476868
Epoch 640, training loss: 85.50029754638672 = 0.7148833870887756 + 10.0 * 8.478541374206543
Epoch 640, val loss: 0.7254665493965149
Epoch 650, training loss: 85.46504211425781 = 0.7080812454223633 + 10.0 * 8.475695610046387
Epoch 650, val loss: 0.7195764183998108
Epoch 660, training loss: 85.44268798828125 = 0.7015058398246765 + 10.0 * 8.47411823272705
Epoch 660, val loss: 0.7138994932174683
Epoch 670, training loss: 85.42676544189453 = 0.6951059103012085 + 10.0 * 8.473165512084961
Epoch 670, val loss: 0.7084881663322449
Epoch 680, training loss: 85.38375091552734 = 0.6889957189559937 + 10.0 * 8.469475746154785
Epoch 680, val loss: 0.7033126354217529
Epoch 690, training loss: 85.34681701660156 = 0.6831188201904297 + 10.0 * 8.46636962890625
Epoch 690, val loss: 0.6984477639198303
Epoch 700, training loss: 85.31611633300781 = 0.6774742007255554 + 10.0 * 8.46386432647705
Epoch 700, val loss: 0.6937605142593384
Epoch 710, training loss: 85.28921508789062 = 0.6720417141914368 + 10.0 * 8.46171760559082
Epoch 710, val loss: 0.6893171668052673
Epoch 720, training loss: 85.30767822265625 = 0.6667962670326233 + 10.0 * 8.464088439941406
Epoch 720, val loss: 0.6849940419197083
Epoch 730, training loss: 85.23591613769531 = 0.6617109179496765 + 10.0 * 8.457420349121094
Epoch 730, val loss: 0.6810091137886047
Epoch 740, training loss: 85.21006774902344 = 0.6569089889526367 + 10.0 * 8.455316543579102
Epoch 740, val loss: 0.6772112846374512
Epoch 750, training loss: 85.18742370605469 = 0.6522777080535889 + 10.0 * 8.45351505279541
Epoch 750, val loss: 0.6735525727272034
Epoch 760, training loss: 85.19483184814453 = 0.6478369235992432 + 10.0 * 8.454699516296387
Epoch 760, val loss: 0.6700659990310669
Epoch 770, training loss: 85.15090942382812 = 0.6435008645057678 + 10.0 * 8.450740814208984
Epoch 770, val loss: 0.6668659448623657
Epoch 780, training loss: 85.12150573730469 = 0.639407217502594 + 10.0 * 8.448209762573242
Epoch 780, val loss: 0.6637052297592163
Epoch 790, training loss: 85.09709930419922 = 0.6354438662528992 + 10.0 * 8.446165084838867
Epoch 790, val loss: 0.6607611179351807
Epoch 800, training loss: 85.09526824951172 = 0.6316201686859131 + 10.0 * 8.446364402770996
Epoch 800, val loss: 0.6579365134239197
Epoch 810, training loss: 85.08605194091797 = 0.6278817653656006 + 10.0 * 8.445816993713379
Epoch 810, val loss: 0.6551616191864014
Epoch 820, training loss: 85.04669189453125 = 0.6243064999580383 + 10.0 * 8.442238807678223
Epoch 820, val loss: 0.6526085734367371
Epoch 830, training loss: 85.0263900756836 = 0.6208859086036682 + 10.0 * 8.440549850463867
Epoch 830, val loss: 0.6501681804656982
Epoch 840, training loss: 85.0672836303711 = 0.6175917387008667 + 10.0 * 8.444969177246094
Epoch 840, val loss: 0.6477481126785278
Epoch 850, training loss: 85.02037048339844 = 0.6143100261688232 + 10.0 * 8.440606117248535
Epoch 850, val loss: 0.6455640196800232
Epoch 860, training loss: 84.98210906982422 = 0.6112135648727417 + 10.0 * 8.437089920043945
Epoch 860, val loss: 0.6434026956558228
Epoch 870, training loss: 84.97017669677734 = 0.6082303524017334 + 10.0 * 8.43619441986084
Epoch 870, val loss: 0.6413237452507019
Epoch 880, training loss: 84.95450592041016 = 0.605320930480957 + 10.0 * 8.434918403625488
Epoch 880, val loss: 0.6393793225288391
Epoch 890, training loss: 84.94109344482422 = 0.6024970412254333 + 10.0 * 8.433859825134277
Epoch 890, val loss: 0.6374626755714417
Epoch 900, training loss: 84.97488403320312 = 0.5997381210327148 + 10.0 * 8.437514305114746
Epoch 900, val loss: 0.6356567144393921
Epoch 910, training loss: 84.93428039550781 = 0.597006618976593 + 10.0 * 8.433727264404297
Epoch 910, val loss: 0.6338024735450745
Epoch 920, training loss: 84.91315460205078 = 0.5943846702575684 + 10.0 * 8.431877136230469
Epoch 920, val loss: 0.6321531534194946
Epoch 930, training loss: 84.89314270019531 = 0.5918790102005005 + 10.0 * 8.430126190185547
Epoch 930, val loss: 0.6304930448532104
Epoch 940, training loss: 84.8763427734375 = 0.5894454121589661 + 10.0 * 8.428689956665039
Epoch 940, val loss: 0.6289294362068176
Epoch 950, training loss: 84.86380767822266 = 0.5870763063430786 + 10.0 * 8.42767333984375
Epoch 950, val loss: 0.6274227499961853
Epoch 960, training loss: 84.9359359741211 = 0.5847365856170654 + 10.0 * 8.43511962890625
Epoch 960, val loss: 0.6260151267051697
Epoch 970, training loss: 84.85526275634766 = 0.5823854207992554 + 10.0 * 8.427288055419922
Epoch 970, val loss: 0.6244041919708252
Epoch 980, training loss: 84.84078979492188 = 0.5801540017127991 + 10.0 * 8.426063537597656
Epoch 980, val loss: 0.623028039932251
Epoch 990, training loss: 84.82078552246094 = 0.5779864192008972 + 10.0 * 8.424280166625977
Epoch 990, val loss: 0.6216286420822144
Epoch 1000, training loss: 84.80492401123047 = 0.5758600831031799 + 10.0 * 8.422906875610352
Epoch 1000, val loss: 0.6203085780143738
Epoch 1010, training loss: 84.79476928710938 = 0.5737746953964233 + 10.0 * 8.422099113464355
Epoch 1010, val loss: 0.6190289855003357
Epoch 1020, training loss: 84.82623291015625 = 0.5717191696166992 + 10.0 * 8.425451278686523
Epoch 1020, val loss: 0.6176677942276001
Epoch 1030, training loss: 84.79146575927734 = 0.5696210861206055 + 10.0 * 8.422184944152832
Epoch 1030, val loss: 0.616456151008606
Epoch 1040, training loss: 84.76907348632812 = 0.5676020979881287 + 10.0 * 8.420146942138672
Epoch 1040, val loss: 0.6152302622795105
Epoch 1050, training loss: 84.75096130371094 = 0.565628170967102 + 10.0 * 8.418533325195312
Epoch 1050, val loss: 0.6139768958091736
Epoch 1060, training loss: 84.74219512939453 = 0.5636695623397827 + 10.0 * 8.417852401733398
Epoch 1060, val loss: 0.6127455234527588
Epoch 1070, training loss: 84.75948333740234 = 0.5617283582687378 + 10.0 * 8.419775009155273
Epoch 1070, val loss: 0.611537754535675
Epoch 1080, training loss: 84.74402618408203 = 0.5597723126411438 + 10.0 * 8.418425559997559
Epoch 1080, val loss: 0.6103307008743286
Epoch 1090, training loss: 84.71531677246094 = 0.557847261428833 + 10.0 * 8.415746688842773
Epoch 1090, val loss: 0.6091623306274414
Epoch 1100, training loss: 84.7057876586914 = 0.55596923828125 + 10.0 * 8.414981842041016
Epoch 1100, val loss: 0.6079683899879456
Epoch 1110, training loss: 84.69664764404297 = 0.5541076064109802 + 10.0 * 8.414254188537598
Epoch 1110, val loss: 0.606830894947052
Epoch 1120, training loss: 84.72258758544922 = 0.5522444248199463 + 10.0 * 8.417034149169922
Epoch 1120, val loss: 0.6056988835334778
Epoch 1130, training loss: 84.68343353271484 = 0.550361692905426 + 10.0 * 8.413307189941406
Epoch 1130, val loss: 0.6044301390647888
Epoch 1140, training loss: 84.6695327758789 = 0.5485174655914307 + 10.0 * 8.412101745605469
Epoch 1140, val loss: 0.6033403277397156
Epoch 1150, training loss: 84.66343688964844 = 0.5466897487640381 + 10.0 * 8.411674499511719
Epoch 1150, val loss: 0.6021541357040405
Epoch 1160, training loss: 84.67461395263672 = 0.5448645353317261 + 10.0 * 8.412975311279297
Epoch 1160, val loss: 0.6009707450866699
Epoch 1170, training loss: 84.64927673339844 = 0.5430287718772888 + 10.0 * 8.410624504089355
Epoch 1170, val loss: 0.5998800992965698
Epoch 1180, training loss: 84.637451171875 = 0.5412276387214661 + 10.0 * 8.409622192382812
Epoch 1180, val loss: 0.5987063646316528
Epoch 1190, training loss: 84.63259887695312 = 0.5394319891929626 + 10.0 * 8.409317016601562
Epoch 1190, val loss: 0.5975686311721802
Epoch 1200, training loss: 84.67740631103516 = 0.5376242399215698 + 10.0 * 8.413978576660156
Epoch 1200, val loss: 0.5964379906654358
Epoch 1210, training loss: 84.6331787109375 = 0.5357943773269653 + 10.0 * 8.409738540649414
Epoch 1210, val loss: 0.5952390432357788
Epoch 1220, training loss: 84.61089324951172 = 0.5340074896812439 + 10.0 * 8.407689094543457
Epoch 1220, val loss: 0.5940508842468262
Epoch 1230, training loss: 84.60443878173828 = 0.5322334170341492 + 10.0 * 8.407220840454102
Epoch 1230, val loss: 0.5929086208343506
Epoch 1240, training loss: 84.5964126586914 = 0.5304745435714722 + 10.0 * 8.406594276428223
Epoch 1240, val loss: 0.5917676687240601
Epoch 1250, training loss: 84.59163665771484 = 0.5287123322486877 + 10.0 * 8.406292915344238
Epoch 1250, val loss: 0.5906490087509155
Epoch 1260, training loss: 84.6292953491211 = 0.5269395709037781 + 10.0 * 8.410235404968262
Epoch 1260, val loss: 0.5895397663116455
Epoch 1270, training loss: 84.58645629882812 = 0.5251438021659851 + 10.0 * 8.40613079071045
Epoch 1270, val loss: 0.5882452726364136
Epoch 1280, training loss: 84.57144927978516 = 0.5233744382858276 + 10.0 * 8.404807090759277
Epoch 1280, val loss: 0.5870610475540161
Epoch 1290, training loss: 84.56665802001953 = 0.5216206908226013 + 10.0 * 8.40450382232666
Epoch 1290, val loss: 0.5859543085098267
Epoch 1300, training loss: 84.56027221679688 = 0.5198785066604614 + 10.0 * 8.40403938293457
Epoch 1300, val loss: 0.5847666263580322
Epoch 1310, training loss: 84.56145477294922 = 0.5181395411491394 + 10.0 * 8.40433120727539
Epoch 1310, val loss: 0.5836681723594666
Epoch 1320, training loss: 84.58058166503906 = 0.5163844227790833 + 10.0 * 8.40641975402832
Epoch 1320, val loss: 0.5824600458145142
Epoch 1330, training loss: 84.5448989868164 = 0.5146252512931824 + 10.0 * 8.403027534484863
Epoch 1330, val loss: 0.5812785029411316
Epoch 1340, training loss: 84.54177856445312 = 0.5128961801528931 + 10.0 * 8.402888298034668
Epoch 1340, val loss: 0.5800624489784241
Epoch 1350, training loss: 84.53279876708984 = 0.5111790895462036 + 10.0 * 8.402162551879883
Epoch 1350, val loss: 0.5789342522621155
Epoch 1360, training loss: 84.53013610839844 = 0.5094721913337708 + 10.0 * 8.402066230773926
Epoch 1360, val loss: 0.5777639150619507
Epoch 1370, training loss: 84.55719757080078 = 0.507753849029541 + 10.0 * 8.40494441986084
Epoch 1370, val loss: 0.5765635967254639
Epoch 1380, training loss: 84.52840423583984 = 0.506031334400177 + 10.0 * 8.402236938476562
Epoch 1380, val loss: 0.5754268765449524
Epoch 1390, training loss: 84.51226806640625 = 0.5043216347694397 + 10.0 * 8.400794982910156
Epoch 1390, val loss: 0.5742279291152954
Epoch 1400, training loss: 84.50821685791016 = 0.5026286244392395 + 10.0 * 8.400558471679688
Epoch 1400, val loss: 0.5730631947517395
Epoch 1410, training loss: 84.51332092285156 = 0.5009422898292542 + 10.0 * 8.401237487792969
Epoch 1410, val loss: 0.5718724131584167
Epoch 1420, training loss: 84.51168823242188 = 0.49924036860466003 + 10.0 * 8.4012451171875
Epoch 1420, val loss: 0.5707038044929504
Epoch 1430, training loss: 84.4986343383789 = 0.4975558817386627 + 10.0 * 8.400107383728027
Epoch 1430, val loss: 0.569541871547699
Epoch 1440, training loss: 84.4900894165039 = 0.49587908387184143 + 10.0 * 8.399420738220215
Epoch 1440, val loss: 0.5683563351631165
Epoch 1450, training loss: 84.48755645751953 = 0.49421361088752747 + 10.0 * 8.399333953857422
Epoch 1450, val loss: 0.5672050714492798
Epoch 1460, training loss: 84.5116958618164 = 0.49253517389297485 + 10.0 * 8.401915550231934
Epoch 1460, val loss: 0.5660223960876465
Epoch 1470, training loss: 84.475341796875 = 0.49086031317710876 + 10.0 * 8.39844799041748
Epoch 1470, val loss: 0.5647961497306824
Epoch 1480, training loss: 84.46731567382812 = 0.4892106056213379 + 10.0 * 8.397809982299805
Epoch 1480, val loss: 0.5636062622070312
Epoch 1490, training loss: 84.4638442993164 = 0.4875723421573639 + 10.0 * 8.397626876831055
Epoch 1490, val loss: 0.5624496936798096
Epoch 1500, training loss: 84.46048736572266 = 0.4859393537044525 + 10.0 * 8.397455215454102
Epoch 1500, val loss: 0.5612741112709045
Epoch 1510, training loss: 84.49081420898438 = 0.4843043088912964 + 10.0 * 8.400650978088379
Epoch 1510, val loss: 0.5601274371147156
Epoch 1520, training loss: 84.45252227783203 = 0.48265933990478516 + 10.0 * 8.39698600769043
Epoch 1520, val loss: 0.5588809251785278
Epoch 1530, training loss: 84.44808197021484 = 0.4810410141944885 + 10.0 * 8.396703720092773
Epoch 1530, val loss: 0.557747483253479
Epoch 1540, training loss: 84.45518493652344 = 0.4794318675994873 + 10.0 * 8.397575378417969
Epoch 1540, val loss: 0.5565372705459595
Epoch 1550, training loss: 84.43997192382812 = 0.4778288006782532 + 10.0 * 8.396214485168457
Epoch 1550, val loss: 0.5553823113441467
Epoch 1560, training loss: 84.43144226074219 = 0.4762455224990845 + 10.0 * 8.395520210266113
Epoch 1560, val loss: 0.5542510151863098
Epoch 1570, training loss: 84.43988037109375 = 0.4746681749820709 + 10.0 * 8.396520614624023
Epoch 1570, val loss: 0.553124189376831
Epoch 1580, training loss: 84.42443084716797 = 0.4730839431285858 + 10.0 * 8.395134925842285
Epoch 1580, val loss: 0.5519664287567139
Epoch 1590, training loss: 84.41698455810547 = 0.4715167284011841 + 10.0 * 8.394546508789062
Epoch 1590, val loss: 0.5508033037185669
Epoch 1600, training loss: 84.41283416748047 = 0.469966858625412 + 10.0 * 8.394287109375
Epoch 1600, val loss: 0.549673318862915
Epoch 1610, training loss: 84.41542053222656 = 0.46842747926712036 + 10.0 * 8.394699096679688
Epoch 1610, val loss: 0.548515796661377
Epoch 1620, training loss: 84.42149353027344 = 0.46688365936279297 + 10.0 * 8.395461082458496
Epoch 1620, val loss: 0.5474077463150024
Epoch 1630, training loss: 84.40880584716797 = 0.46535155177116394 + 10.0 * 8.3943452835083
Epoch 1630, val loss: 0.546347439289093
Epoch 1640, training loss: 84.43461608886719 = 0.4638323187828064 + 10.0 * 8.397078514099121
Epoch 1640, val loss: 0.5451862812042236
Epoch 1650, training loss: 84.39859771728516 = 0.4623214900493622 + 10.0 * 8.393628120422363
Epoch 1650, val loss: 0.544094979763031
Epoch 1660, training loss: 84.38676452636719 = 0.4608396887779236 + 10.0 * 8.392592430114746
Epoch 1660, val loss: 0.5429657101631165
Epoch 1670, training loss: 84.38275909423828 = 0.45937976241111755 + 10.0 * 8.392337799072266
Epoch 1670, val loss: 0.5419373512268066
Epoch 1680, training loss: 84.37776184082031 = 0.45793408155441284 + 10.0 * 8.391983032226562
Epoch 1680, val loss: 0.5408722758293152
Epoch 1690, training loss: 84.37480926513672 = 0.45649611949920654 + 10.0 * 8.391831398010254
Epoch 1690, val loss: 0.5398204922676086
Epoch 1700, training loss: 84.38886260986328 = 0.45506036281585693 + 10.0 * 8.393380165100098
Epoch 1700, val loss: 0.5387936234474182
Epoch 1710, training loss: 84.37095642089844 = 0.4536129832267761 + 10.0 * 8.39173412322998
Epoch 1710, val loss: 0.5377209782600403
Epoch 1720, training loss: 84.3713150024414 = 0.4521840512752533 + 10.0 * 8.391912460327148
Epoch 1720, val loss: 0.5366476774215698
Epoch 1730, training loss: 84.35820007324219 = 0.45077013969421387 + 10.0 * 8.390743255615234
Epoch 1730, val loss: 0.5356982946395874
Epoch 1740, training loss: 84.36153411865234 = 0.4493752717971802 + 10.0 * 8.391215324401855
Epoch 1740, val loss: 0.5347157120704651
Epoch 1750, training loss: 84.36121368408203 = 0.44799038767814636 + 10.0 * 8.391322135925293
Epoch 1750, val loss: 0.5336979627609253
Epoch 1760, training loss: 84.35799407958984 = 0.4466201364994049 + 10.0 * 8.391138076782227
Epoch 1760, val loss: 0.5326981544494629
Epoch 1770, training loss: 84.34142303466797 = 0.44525986909866333 + 10.0 * 8.389616012573242
Epoch 1770, val loss: 0.5317991971969604
Epoch 1780, training loss: 84.3357162475586 = 0.44391703605651855 + 10.0 * 8.389180183410645
Epoch 1780, val loss: 0.5308508276939392
Epoch 1790, training loss: 84.33304595947266 = 0.4425855576992035 + 10.0 * 8.389045715332031
Epoch 1790, val loss: 0.5298970937728882
Epoch 1800, training loss: 84.34896087646484 = 0.44125956296920776 + 10.0 * 8.390769958496094
Epoch 1800, val loss: 0.5289291739463806
Epoch 1810, training loss: 84.361083984375 = 0.4399280548095703 + 10.0 * 8.392115592956543
Epoch 1810, val loss: 0.5281204581260681
Epoch 1820, training loss: 84.35092163085938 = 0.4385976791381836 + 10.0 * 8.39123249053955
Epoch 1820, val loss: 0.5271296501159668
Epoch 1830, training loss: 84.32598114013672 = 0.43729618191719055 + 10.0 * 8.38886833190918
Epoch 1830, val loss: 0.5263314843177795
Epoch 1840, training loss: 84.31172180175781 = 0.4360212981700897 + 10.0 * 8.38757038116455
Epoch 1840, val loss: 0.5254478454589844
Epoch 1850, training loss: 84.30718231201172 = 0.434760719537735 + 10.0 * 8.387242317199707
Epoch 1850, val loss: 0.5246108174324036
Epoch 1860, training loss: 84.3022689819336 = 0.43350961804389954 + 10.0 * 8.386876106262207
Epoch 1860, val loss: 0.5237874388694763
Epoch 1870, training loss: 84.30453491210938 = 0.4322674572467804 + 10.0 * 8.387227058410645
Epoch 1870, val loss: 0.5229560732841492
Epoch 1880, training loss: 84.32964324951172 = 0.4310259521007538 + 10.0 * 8.389862060546875
Epoch 1880, val loss: 0.5221407413482666
Epoch 1890, training loss: 84.3038101196289 = 0.42979297041893005 + 10.0 * 8.387401580810547
Epoch 1890, val loss: 0.5213937163352966
Epoch 1900, training loss: 84.31554412841797 = 0.42857956886291504 + 10.0 * 8.388696670532227
Epoch 1900, val loss: 0.5206526517868042
Epoch 1910, training loss: 84.28265380859375 = 0.42737382650375366 + 10.0 * 8.385527610778809
Epoch 1910, val loss: 0.5198426246643066
Epoch 1920, training loss: 84.28430938720703 = 0.4261883795261383 + 10.0 * 8.385812759399414
Epoch 1920, val loss: 0.5191166996955872
Epoch 1930, training loss: 84.275390625 = 0.4250212609767914 + 10.0 * 8.385037422180176
Epoch 1930, val loss: 0.5184081196784973
Epoch 1940, training loss: 84.27275085449219 = 0.42386505007743835 + 10.0 * 8.384888648986816
Epoch 1940, val loss: 0.5177502036094666
Epoch 1950, training loss: 84.2806625366211 = 0.4227128326892853 + 10.0 * 8.385794639587402
Epoch 1950, val loss: 0.517051637172699
Epoch 1960, training loss: 84.27863311767578 = 0.42156079411506653 + 10.0 * 8.385706901550293
Epoch 1960, val loss: 0.5164110064506531
Epoch 1970, training loss: 84.27649688720703 = 0.4204190969467163 + 10.0 * 8.385607719421387
Epoch 1970, val loss: 0.5157618522644043
Epoch 1980, training loss: 84.26030731201172 = 0.4192928671836853 + 10.0 * 8.384100914001465
Epoch 1980, val loss: 0.5151044726371765
Epoch 1990, training loss: 84.25114440917969 = 0.418176531791687 + 10.0 * 8.383296966552734
Epoch 1990, val loss: 0.5144880414009094
Epoch 2000, training loss: 84.26644134521484 = 0.41707104444503784 + 10.0 * 8.384937286376953
Epoch 2000, val loss: 0.5138897895812988
Epoch 2010, training loss: 84.25748443603516 = 0.4159674644470215 + 10.0 * 8.384151458740234
Epoch 2010, val loss: 0.51336270570755
Epoch 2020, training loss: 84.24939727783203 = 0.4148779511451721 + 10.0 * 8.383451461791992
Epoch 2020, val loss: 0.512712299823761
Epoch 2030, training loss: 84.23995208740234 = 0.4138069748878479 + 10.0 * 8.382614135742188
Epoch 2030, val loss: 0.5121701955795288
Epoch 2040, training loss: 84.23527526855469 = 0.412748247385025 + 10.0 * 8.38225269317627
Epoch 2040, val loss: 0.5116502642631531
Epoch 2050, training loss: 84.25755310058594 = 0.4116947650909424 + 10.0 * 8.3845853805542
Epoch 2050, val loss: 0.5110965967178345
Epoch 2060, training loss: 84.2256851196289 = 0.41063976287841797 + 10.0 * 8.381505012512207
Epoch 2060, val loss: 0.5106590390205383
Epoch 2070, training loss: 84.22571563720703 = 0.409597247838974 + 10.0 * 8.381611824035645
Epoch 2070, val loss: 0.5101528763771057
Epoch 2080, training loss: 84.2220687866211 = 0.4085632264614105 + 10.0 * 8.38135051727295
Epoch 2080, val loss: 0.5096553564071655
Epoch 2090, training loss: 84.24490356445312 = 0.4075378179550171 + 10.0 * 8.383736610412598
Epoch 2090, val loss: 0.509204626083374
Epoch 2100, training loss: 84.22835540771484 = 0.40650278329849243 + 10.0 * 8.382184982299805
Epoch 2100, val loss: 0.5088539123535156
Epoch 2110, training loss: 84.21804809570312 = 0.4054836332798004 + 10.0 * 8.381256103515625
Epoch 2110, val loss: 0.5083194971084595
Epoch 2120, training loss: 84.20645904541016 = 0.40447551012039185 + 10.0 * 8.38019847869873
Epoch 2120, val loss: 0.5078985691070557
Epoch 2130, training loss: 84.20572662353516 = 0.4034779667854309 + 10.0 * 8.380224227905273
Epoch 2130, val loss: 0.5075125098228455
Epoch 2140, training loss: 84.23644256591797 = 0.4024862051010132 + 10.0 * 8.383395195007324
Epoch 2140, val loss: 0.5071588158607483
Epoch 2150, training loss: 84.20133972167969 = 0.40148720145225525 + 10.0 * 8.379984855651855
Epoch 2150, val loss: 0.5067749619483948
Epoch 2160, training loss: 84.19281768798828 = 0.40050262212753296 + 10.0 * 8.379231452941895
Epoch 2160, val loss: 0.5063613057136536
Epoch 2170, training loss: 84.1896743774414 = 0.39953088760375977 + 10.0 * 8.37901496887207
Epoch 2170, val loss: 0.5059781670570374
Epoch 2180, training loss: 84.18814849853516 = 0.398566335439682 + 10.0 * 8.378957748413086
Epoch 2180, val loss: 0.505626916885376
Epoch 2190, training loss: 84.19902038574219 = 0.39760446548461914 + 10.0 * 8.380141258239746
Epoch 2190, val loss: 0.5052438378334045
Epoch 2200, training loss: 84.18680572509766 = 0.3966403007507324 + 10.0 * 8.379016876220703
Epoch 2200, val loss: 0.5049752593040466
Epoch 2210, training loss: 84.18726348876953 = 0.395683616399765 + 10.0 * 8.379158020019531
Epoch 2210, val loss: 0.5047032237052917
Epoch 2220, training loss: 84.1974105834961 = 0.3947298526763916 + 10.0 * 8.380268096923828
Epoch 2220, val loss: 0.5043493509292603
Epoch 2230, training loss: 84.17540740966797 = 0.3937824070453644 + 10.0 * 8.378162384033203
Epoch 2230, val loss: 0.5039814114570618
Epoch 2240, training loss: 84.16901397705078 = 0.3928459882736206 + 10.0 * 8.377616882324219
Epoch 2240, val loss: 0.5037333965301514
Epoch 2250, training loss: 84.16710662841797 = 0.39191538095474243 + 10.0 * 8.377519607543945
Epoch 2250, val loss: 0.5034367442131042
Epoch 2260, training loss: 84.17520904541016 = 0.3909877836704254 + 10.0 * 8.378421783447266
Epoch 2260, val loss: 0.5031921863555908
Epoch 2270, training loss: 84.18807983398438 = 0.3900569677352905 + 10.0 * 8.379801750183105
Epoch 2270, val loss: 0.5029169917106628
Epoch 2280, training loss: 84.16876983642578 = 0.3891296088695526 + 10.0 * 8.37796401977539
Epoch 2280, val loss: 0.5026695728302002
Epoch 2290, training loss: 84.15535736083984 = 0.3882084786891937 + 10.0 * 8.376714706420898
Epoch 2290, val loss: 0.5024482607841492
Epoch 2300, training loss: 84.15177917480469 = 0.38729268312454224 + 10.0 * 8.376448631286621
Epoch 2300, val loss: 0.5022240877151489
Epoch 2310, training loss: 84.15009307861328 = 0.3863786458969116 + 10.0 * 8.376371383666992
Epoch 2310, val loss: 0.501992404460907
Epoch 2320, training loss: 84.1849136352539 = 0.3854660987854004 + 10.0 * 8.379944801330566
Epoch 2320, val loss: 0.5017603039741516
Epoch 2330, training loss: 84.16700744628906 = 0.3845451772212982 + 10.0 * 8.378246307373047
Epoch 2330, val loss: 0.5015817284584045
Epoch 2340, training loss: 84.15668487548828 = 0.383634090423584 + 10.0 * 8.377305030822754
Epoch 2340, val loss: 0.5013462901115417
Epoch 2350, training loss: 84.14031982421875 = 0.38273388147354126 + 10.0 * 8.375758171081543
Epoch 2350, val loss: 0.5012241005897522
Epoch 2360, training loss: 84.13818359375 = 0.3818437159061432 + 10.0 * 8.37563419342041
Epoch 2360, val loss: 0.5010485649108887
Epoch 2370, training loss: 84.1456069946289 = 0.38095468282699585 + 10.0 * 8.376465797424316
Epoch 2370, val loss: 0.5008792281150818
Epoch 2380, training loss: 84.16644287109375 = 0.3800635039806366 + 10.0 * 8.378637313842773
Epoch 2380, val loss: 0.5006784200668335
Epoch 2390, training loss: 84.14450073242188 = 0.3791728913784027 + 10.0 * 8.376532554626465
Epoch 2390, val loss: 0.5005660653114319
Epoch 2400, training loss: 84.13340759277344 = 0.3782911002635956 + 10.0 * 8.37551212310791
Epoch 2400, val loss: 0.5004504919052124
Epoch 2410, training loss: 84.12725830078125 = 0.37741395831108093 + 10.0 * 8.374984741210938
Epoch 2410, val loss: 0.5002881288528442
Epoch 2420, training loss: 84.12409210205078 = 0.37653928995132446 + 10.0 * 8.374754905700684
Epoch 2420, val loss: 0.5001365542411804
Epoch 2430, training loss: 84.13839721679688 = 0.37566661834716797 + 10.0 * 8.376273155212402
Epoch 2430, val loss: 0.49992552399635315
Epoch 2440, training loss: 84.13883209228516 = 0.37478992342948914 + 10.0 * 8.376404762268066
Epoch 2440, val loss: 0.4999569356441498
Epoch 2450, training loss: 84.1280746459961 = 0.37391799688339233 + 10.0 * 8.375415802001953
Epoch 2450, val loss: 0.49975165724754333
Epoch 2460, training loss: 84.1169204711914 = 0.3730577826499939 + 10.0 * 8.37438678741455
Epoch 2460, val loss: 0.49967965483665466
Epoch 2470, training loss: 84.11385345458984 = 0.37220320105552673 + 10.0 * 8.374165534973145
Epoch 2470, val loss: 0.4996160566806793
Epoch 2480, training loss: 84.12273406982422 = 0.371351420879364 + 10.0 * 8.375138282775879
Epoch 2480, val loss: 0.4994710385799408
Epoch 2490, training loss: 84.14552307128906 = 0.3704971671104431 + 10.0 * 8.37750244140625
Epoch 2490, val loss: 0.49937984347343445
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7940131912734651
0.8163442729841339
=== training gcn model ===
Epoch 0, training loss: 106.94930267333984 = 1.126779556274414 + 10.0 * 10.582252502441406
Epoch 0, val loss: 1.1270743608474731
Epoch 10, training loss: 106.93798065185547 = 1.120091199874878 + 10.0 * 10.581789016723633
Epoch 10, val loss: 1.1204040050506592
Epoch 20, training loss: 106.90753173828125 = 1.1127504110336304 + 10.0 * 10.57947826385498
Epoch 20, val loss: 1.1130777597427368
Epoch 30, training loss: 106.7874526977539 = 1.1046197414398193 + 10.0 * 10.568283081054688
Epoch 30, val loss: 1.104999303817749
Epoch 40, training loss: 106.34610748291016 = 1.095910906791687 + 10.0 * 10.525019645690918
Epoch 40, val loss: 1.0964152812957764
Epoch 50, training loss: 105.05793762207031 = 1.0869933366775513 + 10.0 * 10.3970947265625
Epoch 50, val loss: 1.0877172946929932
Epoch 60, training loss: 102.51799011230469 = 1.0787848234176636 + 10.0 * 10.1439208984375
Epoch 60, val loss: 1.0797662734985352
Epoch 70, training loss: 99.90144348144531 = 1.0720380544662476 + 10.0 * 9.882940292358398
Epoch 70, val loss: 1.0734747648239136
Epoch 80, training loss: 97.12084197998047 = 1.0673807859420776 + 10.0 * 9.605345726013184
Epoch 80, val loss: 1.0688735246658325
Epoch 90, training loss: 94.43807220458984 = 1.06294846534729 + 10.0 * 9.337512016296387
Epoch 90, val loss: 1.0644049644470215
Epoch 100, training loss: 93.06468963623047 = 1.0586938858032227 + 10.0 * 9.200599670410156
Epoch 100, val loss: 1.060036063194275
Epoch 110, training loss: 92.34234619140625 = 1.0548545122146606 + 10.0 * 9.128748893737793
Epoch 110, val loss: 1.0562701225280762
Epoch 120, training loss: 91.48152923583984 = 1.0520343780517578 + 10.0 * 9.042949676513672
Epoch 120, val loss: 1.053545594215393
Epoch 130, training loss: 90.73125457763672 = 1.049688696861267 + 10.0 * 8.968156814575195
Epoch 130, val loss: 1.0512484312057495
Epoch 140, training loss: 90.21968841552734 = 1.0476207733154297 + 10.0 * 8.917206764221191
Epoch 140, val loss: 1.049189805984497
Epoch 150, training loss: 89.71023559570312 = 1.045487403869629 + 10.0 * 8.866475105285645
Epoch 150, val loss: 1.0470467805862427
Epoch 160, training loss: 89.3038101196289 = 1.0433543920516968 + 10.0 * 8.826045036315918
Epoch 160, val loss: 1.0449074506759644
Epoch 170, training loss: 88.94957733154297 = 1.0413262844085693 + 10.0 * 8.790824890136719
Epoch 170, val loss: 1.0428800582885742
Epoch 180, training loss: 88.59276580810547 = 1.0393532514572144 + 10.0 * 8.755341529846191
Epoch 180, val loss: 1.0409085750579834
Epoch 190, training loss: 88.28958892822266 = 1.0373953580856323 + 10.0 * 8.7252197265625
Epoch 190, val loss: 1.0389593839645386
Epoch 200, training loss: 88.052490234375 = 1.035426139831543 + 10.0 * 8.701706886291504
Epoch 200, val loss: 1.037009358406067
Epoch 210, training loss: 87.84917449951172 = 1.0334135293960571 + 10.0 * 8.681575775146484
Epoch 210, val loss: 1.0350152254104614
Epoch 220, training loss: 87.70511627197266 = 1.0313338041305542 + 10.0 * 8.667378425598145
Epoch 220, val loss: 1.0329612493515015
Epoch 230, training loss: 87.52640533447266 = 1.0291783809661865 + 10.0 * 8.649723052978516
Epoch 230, val loss: 1.0308421850204468
Epoch 240, training loss: 87.3755111694336 = 1.026972770690918 + 10.0 * 8.634854316711426
Epoch 240, val loss: 1.0286777019500732
Epoch 250, training loss: 87.24784088134766 = 1.0246914625167847 + 10.0 * 8.622315406799316
Epoch 250, val loss: 1.0264400243759155
Epoch 260, training loss: 87.14996337890625 = 1.0222700834274292 + 10.0 * 8.61276912689209
Epoch 260, val loss: 1.0240803956985474
Epoch 270, training loss: 87.02629089355469 = 1.0196805000305176 + 10.0 * 8.60066032409668
Epoch 270, val loss: 1.0215513706207275
Epoch 280, training loss: 86.9150390625 = 1.0169377326965332 + 10.0 * 8.589810371398926
Epoch 280, val loss: 1.0188850164413452
Epoch 290, training loss: 86.82367706298828 = 1.0140424966812134 + 10.0 * 8.580963134765625
Epoch 290, val loss: 1.01608145236969
Epoch 300, training loss: 86.72148895263672 = 1.01095449924469 + 10.0 * 8.571053504943848
Epoch 300, val loss: 1.0131042003631592
Epoch 310, training loss: 86.62520599365234 = 1.0076826810836792 + 10.0 * 8.561752319335938
Epoch 310, val loss: 1.0099306106567383
Epoch 320, training loss: 86.54803466796875 = 1.0042150020599365 + 10.0 * 8.55438232421875
Epoch 320, val loss: 1.0065759420394897
Epoch 330, training loss: 86.47331237792969 = 1.0005099773406982 + 10.0 * 8.547280311584473
Epoch 330, val loss: 1.0030086040496826
Epoch 340, training loss: 86.41084289550781 = 0.996539294719696 + 10.0 * 8.541430473327637
Epoch 340, val loss: 0.9991671442985535
Epoch 350, training loss: 86.35382843017578 = 0.9922689199447632 + 10.0 * 8.536155700683594
Epoch 350, val loss: 0.9950474500656128
Epoch 360, training loss: 86.3020248413086 = 0.9877089858055115 + 10.0 * 8.531431198120117
Epoch 360, val loss: 0.990671694278717
Epoch 370, training loss: 86.24930572509766 = 0.9828454256057739 + 10.0 * 8.52664566040039
Epoch 370, val loss: 0.9860330820083618
Epoch 380, training loss: 86.20020294189453 = 0.9776897430419922 + 10.0 * 8.52225112915039
Epoch 380, val loss: 0.9810935258865356
Epoch 390, training loss: 86.15906524658203 = 0.9722122550010681 + 10.0 * 8.518685340881348
Epoch 390, val loss: 0.9758604764938354
Epoch 400, training loss: 86.17115783691406 = 0.9663702249526978 + 10.0 * 8.520479202270508
Epoch 400, val loss: 0.970306396484375
Epoch 410, training loss: 86.10188293457031 = 0.9601876735687256 + 10.0 * 8.514169692993164
Epoch 410, val loss: 0.964409589767456
Epoch 420, training loss: 86.06002807617188 = 0.9536916017532349 + 10.0 * 8.51063346862793
Epoch 420, val loss: 0.9582350254058838
Epoch 430, training loss: 86.02847290039062 = 0.9468680620193481 + 10.0 * 8.508160591125488
Epoch 430, val loss: 0.9517812728881836
Epoch 440, training loss: 85.9976806640625 = 0.9397146105766296 + 10.0 * 8.505796432495117
Epoch 440, val loss: 0.945038378238678
Epoch 450, training loss: 85.9664535522461 = 0.9322390556335449 + 10.0 * 8.503421783447266
Epoch 450, val loss: 0.9380003809928894
Epoch 460, training loss: 85.93553924560547 = 0.9244431853294373 + 10.0 * 8.50110912322998
Epoch 460, val loss: 0.9306857585906982
Epoch 470, training loss: 85.93697357177734 = 0.9163326025009155 + 10.0 * 8.502063751220703
Epoch 470, val loss: 0.9231085181236267
Epoch 480, training loss: 85.88667297363281 = 0.9078682661056519 + 10.0 * 8.497880935668945
Epoch 480, val loss: 0.9152426719665527
Epoch 490, training loss: 85.84622192382812 = 0.8992193937301636 + 10.0 * 8.49470043182373
Epoch 490, val loss: 0.907164454460144
Epoch 500, training loss: 85.81614685058594 = 0.8903853893280029 + 10.0 * 8.492575645446777
Epoch 500, val loss: 0.8989692330360413
Epoch 510, training loss: 85.78036499023438 = 0.8813375234603882 + 10.0 * 8.48990249633789
Epoch 510, val loss: 0.8906480669975281
Epoch 520, training loss: 85.75323486328125 = 0.8721364140510559 + 10.0 * 8.488109588623047
Epoch 520, val loss: 0.8821776509284973
Epoch 530, training loss: 85.72566223144531 = 0.8627459406852722 + 10.0 * 8.486291885375977
Epoch 530, val loss: 0.8735471367835999
Epoch 540, training loss: 85.69108581542969 = 0.8532008528709412 + 10.0 * 8.48378849029541
Epoch 540, val loss: 0.8648662567138672
Epoch 550, training loss: 85.65868377685547 = 0.8436384201049805 + 10.0 * 8.481504440307617
Epoch 550, val loss: 0.8561756610870361
Epoch 560, training loss: 85.62743377685547 = 0.8340389132499695 + 10.0 * 8.479339599609375
Epoch 560, val loss: 0.8475044965744019
Epoch 570, training loss: 85.59990692138672 = 0.8244314789772034 + 10.0 * 8.477547645568848
Epoch 570, val loss: 0.8388684988021851
Epoch 580, training loss: 85.60203552246094 = 0.8147796988487244 + 10.0 * 8.47872543334961
Epoch 580, val loss: 0.8302299976348877
Epoch 590, training loss: 85.55220794677734 = 0.8051974773406982 + 10.0 * 8.474700927734375
Epoch 590, val loss: 0.8216651678085327
Epoch 600, training loss: 85.51746368408203 = 0.7957444190979004 + 10.0 * 8.472171783447266
Epoch 600, val loss: 0.8132671117782593
Epoch 610, training loss: 85.48902893066406 = 0.7864372134208679 + 10.0 * 8.470258712768555
Epoch 610, val loss: 0.8050655126571655
Epoch 620, training loss: 85.46215057373047 = 0.7772757411003113 + 10.0 * 8.468487739562988
Epoch 620, val loss: 0.7970397472381592
Epoch 630, training loss: 85.45978546142578 = 0.7682550549507141 + 10.0 * 8.469152450561523
Epoch 630, val loss: 0.7891831994056702
Epoch 640, training loss: 85.42761993408203 = 0.7593954801559448 + 10.0 * 8.466822624206543
Epoch 640, val loss: 0.7814942002296448
Epoch 650, training loss: 85.38947296142578 = 0.7507175207138062 + 10.0 * 8.463875770568848
Epoch 650, val loss: 0.7740446925163269
Epoch 660, training loss: 85.36383819580078 = 0.7423136830329895 + 10.0 * 8.462152481079102
Epoch 660, val loss: 0.7668989896774292
Epoch 670, training loss: 85.34197235107422 = 0.7341704964637756 + 10.0 * 8.460780143737793
Epoch 670, val loss: 0.7600252032279968
Epoch 680, training loss: 85.36006164550781 = 0.7262240648269653 + 10.0 * 8.463383674621582
Epoch 680, val loss: 0.7533803582191467
Epoch 690, training loss: 85.29607391357422 = 0.7185223698616028 + 10.0 * 8.457755088806152
Epoch 690, val loss: 0.7469556927680969
Epoch 700, training loss: 85.27777099609375 = 0.711101770401001 + 10.0 * 8.456666946411133
Epoch 700, val loss: 0.740848183631897
Epoch 710, training loss: 85.2555160522461 = 0.7039569616317749 + 10.0 * 8.455156326293945
Epoch 710, val loss: 0.7350488901138306
Epoch 720, training loss: 85.27788543701172 = 0.6970488429069519 + 10.0 * 8.458083152770996
Epoch 720, val loss: 0.7294688820838928
Epoch 730, training loss: 85.24396514892578 = 0.6903573274612427 + 10.0 * 8.455360412597656
Epoch 730, val loss: 0.7241138815879822
Epoch 740, training loss: 85.20134735107422 = 0.6839233040809631 + 10.0 * 8.451742172241211
Epoch 740, val loss: 0.7190277576446533
Epoch 750, training loss: 85.18455505371094 = 0.6777956485748291 + 10.0 * 8.450675964355469
Epoch 750, val loss: 0.7142470479011536
Epoch 760, training loss: 85.16462707519531 = 0.6719092726707458 + 10.0 * 8.449272155761719
Epoch 760, val loss: 0.7096931338310242
Epoch 770, training loss: 85.15080261230469 = 0.6662581562995911 + 10.0 * 8.448453903198242
Epoch 770, val loss: 0.7053654789924622
Epoch 780, training loss: 85.16891479492188 = 0.660793662071228 + 10.0 * 8.450811386108398
Epoch 780, val loss: 0.701208233833313
Epoch 790, training loss: 85.1324691772461 = 0.655473530292511 + 10.0 * 8.447699546813965
Epoch 790, val loss: 0.6971866488456726
Epoch 800, training loss: 85.10552978515625 = 0.6504299640655518 + 10.0 * 8.445509910583496
Epoch 800, val loss: 0.6934648752212524
Epoch 810, training loss: 85.08881378173828 = 0.6456161737442017 + 10.0 * 8.444319725036621
Epoch 810, val loss: 0.6899341344833374
Epoch 820, training loss: 85.11557006835938 = 0.6409719586372375 + 10.0 * 8.447460174560547
Epoch 820, val loss: 0.6865138411521912
Epoch 830, training loss: 85.06087493896484 = 0.6365188360214233 + 10.0 * 8.442435264587402
Epoch 830, val loss: 0.683335542678833
Epoch 840, training loss: 85.04304504394531 = 0.6322285532951355 + 10.0 * 8.441082000732422
Epoch 840, val loss: 0.6802588701248169
Epoch 850, training loss: 85.02733612060547 = 0.6281071305274963 + 10.0 * 8.439923286437988
Epoch 850, val loss: 0.6773242354393005
Epoch 860, training loss: 85.0811996459961 = 0.6241232752799988 + 10.0 * 8.445707321166992
Epoch 860, val loss: 0.6744990944862366
Epoch 870, training loss: 85.01296997070312 = 0.6201872825622559 + 10.0 * 8.439278602600098
Epoch 870, val loss: 0.671655535697937
Epoch 880, training loss: 84.98136138916016 = 0.6165098547935486 + 10.0 * 8.436485290527344
Epoch 880, val loss: 0.6691147089004517
Epoch 890, training loss: 84.96766662597656 = 0.6129730343818665 + 10.0 * 8.435468673706055
Epoch 890, val loss: 0.6666794419288635
Epoch 900, training loss: 84.95377349853516 = 0.6095619797706604 + 10.0 * 8.434420585632324
Epoch 900, val loss: 0.664327085018158
Epoch 910, training loss: 84.94313049316406 = 0.6062444448471069 + 10.0 * 8.433688163757324
Epoch 910, val loss: 0.6620511412620544
Epoch 920, training loss: 84.97486877441406 = 0.602999210357666 + 10.0 * 8.437187194824219
Epoch 920, val loss: 0.6598008871078491
Epoch 930, training loss: 84.92866516113281 = 0.5997562408447266 + 10.0 * 8.432890892028809
Epoch 930, val loss: 0.6574943661689758
Epoch 940, training loss: 84.90556335449219 = 0.5966787338256836 + 10.0 * 8.430888175964355
Epoch 940, val loss: 0.6553981900215149
Epoch 950, training loss: 84.8899154663086 = 0.593720018863678 + 10.0 * 8.429619789123535
Epoch 950, val loss: 0.6533656716346741
Epoch 960, training loss: 84.89537048339844 = 0.5908365249633789 + 10.0 * 8.430453300476074
Epoch 960, val loss: 0.6513991355895996
Epoch 970, training loss: 84.864990234375 = 0.5879901051521301 + 10.0 * 8.42770004272461
Epoch 970, val loss: 0.6493696570396423
Epoch 980, training loss: 84.85120391845703 = 0.5852354168891907 + 10.0 * 8.426596641540527
Epoch 980, val loss: 0.6474653482437134
Epoch 990, training loss: 84.84471130371094 = 0.5825613141059875 + 10.0 * 8.426215171813965
Epoch 990, val loss: 0.6456410884857178
Epoch 1000, training loss: 84.87757110595703 = 0.5799351334571838 + 10.0 * 8.429763793945312
Epoch 1000, val loss: 0.6437702178955078
Epoch 1010, training loss: 84.8292236328125 = 0.5773035287857056 + 10.0 * 8.425191879272461
Epoch 1010, val loss: 0.6418768763542175
Epoch 1020, training loss: 84.8070297241211 = 0.5747874975204468 + 10.0 * 8.423223495483398
Epoch 1020, val loss: 0.6401132345199585
Epoch 1030, training loss: 84.79742431640625 = 0.5723444223403931 + 10.0 * 8.422508239746094
Epoch 1030, val loss: 0.6383788585662842
Epoch 1040, training loss: 84.78838348388672 = 0.5699470043182373 + 10.0 * 8.421843528747559
Epoch 1040, val loss: 0.6366788148880005
Epoch 1050, training loss: 84.81970977783203 = 0.5675914287567139 + 10.0 * 8.425211906433105
Epoch 1050, val loss: 0.6350012421607971
Epoch 1060, training loss: 84.79096221923828 = 0.5651445984840393 + 10.0 * 8.422581672668457
Epoch 1060, val loss: 0.6330795288085938
Epoch 1070, training loss: 84.7778549194336 = 0.5628573298454285 + 10.0 * 8.421499252319336
Epoch 1070, val loss: 0.6314703822135925
Epoch 1080, training loss: 84.75109100341797 = 0.5605817437171936 + 10.0 * 8.419050216674805
Epoch 1080, val loss: 0.6298118829727173
Epoch 1090, training loss: 84.7435531616211 = 0.5583761930465698 + 10.0 * 8.41851806640625
Epoch 1090, val loss: 0.6281838417053223
Epoch 1100, training loss: 84.73481750488281 = 0.5562076568603516 + 10.0 * 8.417860984802246
Epoch 1100, val loss: 0.626575231552124
Epoch 1110, training loss: 84.75643157958984 = 0.5540482401847839 + 10.0 * 8.420238494873047
Epoch 1110, val loss: 0.6249603033065796
Epoch 1120, training loss: 84.74217224121094 = 0.5518496632575989 + 10.0 * 8.419032096862793
Epoch 1120, val loss: 0.623185396194458
Epoch 1130, training loss: 84.71318817138672 = 0.5497228503227234 + 10.0 * 8.416346549987793
Epoch 1130, val loss: 0.6215947866439819
Epoch 1140, training loss: 84.70079803466797 = 0.5476612448692322 + 10.0 * 8.415313720703125
Epoch 1140, val loss: 0.6200773119926453
Epoch 1150, training loss: 84.69226837158203 = 0.5456201434135437 + 10.0 * 8.414664268493652
Epoch 1150, val loss: 0.6184591054916382
Epoch 1160, training loss: 84.68455505371094 = 0.5436112880706787 + 10.0 * 8.414094924926758
Epoch 1160, val loss: 0.6169240474700928
Epoch 1170, training loss: 84.73120880126953 = 0.5416305661201477 + 10.0 * 8.418957710266113
Epoch 1170, val loss: 0.6153954863548279
Epoch 1180, training loss: 84.71463012695312 = 0.539523184299469 + 10.0 * 8.417510986328125
Epoch 1180, val loss: 0.6136645674705505
Epoch 1190, training loss: 84.6756362915039 = 0.5375353693962097 + 10.0 * 8.413809776306152
Epoch 1190, val loss: 0.6121296882629395
Epoch 1200, training loss: 84.65765380859375 = 0.5356062054634094 + 10.0 * 8.41220474243164
Epoch 1200, val loss: 0.6105968952178955
Epoch 1210, training loss: 84.65116119384766 = 0.5337239503860474 + 10.0 * 8.411744117736816
Epoch 1210, val loss: 0.6091158390045166
Epoch 1220, training loss: 84.6443099975586 = 0.5318551063537598 + 10.0 * 8.411245346069336
Epoch 1220, val loss: 0.6076087951660156
Epoch 1230, training loss: 84.63726043701172 = 0.5300038456916809 + 10.0 * 8.410725593566895
Epoch 1230, val loss: 0.6061643362045288
Epoch 1240, training loss: 84.63091278076172 = 0.5281632542610168 + 10.0 * 8.41027545928955
Epoch 1240, val loss: 0.6046879291534424
Epoch 1250, training loss: 84.63673400878906 = 0.5263357758522034 + 10.0 * 8.411039352416992
Epoch 1250, val loss: 0.6031919121742249
Epoch 1260, training loss: 84.6773910522461 = 0.5244725942611694 + 10.0 * 8.415291786193848
Epoch 1260, val loss: 0.6017351746559143
Epoch 1270, training loss: 84.61972045898438 = 0.5226209759712219 + 10.0 * 8.409709930419922
Epoch 1270, val loss: 0.6002250909805298
Epoch 1280, training loss: 84.61087799072266 = 0.520829439163208 + 10.0 * 8.409005165100098
Epoch 1280, val loss: 0.5988135933876038
Epoch 1290, training loss: 84.60417938232422 = 0.5190934538841248 + 10.0 * 8.40850830078125
Epoch 1290, val loss: 0.5974041223526001
Epoch 1300, training loss: 84.61145782470703 = 0.517373263835907 + 10.0 * 8.409408569335938
Epoch 1300, val loss: 0.5960301756858826
Epoch 1310, training loss: 84.60099029541016 = 0.5156336426734924 + 10.0 * 8.408535957336426
Epoch 1310, val loss: 0.5946509838104248
Epoch 1320, training loss: 84.5888442993164 = 0.5139363408088684 + 10.0 * 8.407490730285645
Epoch 1320, val loss: 0.5933230519294739
Epoch 1330, training loss: 84.58102416992188 = 0.5122494697570801 + 10.0 * 8.406877517700195
Epoch 1330, val loss: 0.5919659733772278
Epoch 1340, training loss: 84.57357025146484 = 0.5106156468391418 + 10.0 * 8.406295776367188
Epoch 1340, val loss: 0.5907009243965149
Epoch 1350, training loss: 84.56600952148438 = 0.5089879035949707 + 10.0 * 8.405702590942383
Epoch 1350, val loss: 0.5894257426261902
Epoch 1360, training loss: 84.56189727783203 = 0.5073782205581665 + 10.0 * 8.405451774597168
Epoch 1360, val loss: 0.5882038474082947
Epoch 1370, training loss: 84.61237335205078 = 0.5057918429374695 + 10.0 * 8.41065788269043
Epoch 1370, val loss: 0.5870937705039978
Epoch 1380, training loss: 84.57003784179688 = 0.5041232705116272 + 10.0 * 8.406591415405273
Epoch 1380, val loss: 0.5855510830879211
Epoch 1390, training loss: 84.57170104980469 = 0.5025519132614136 + 10.0 * 8.406915664672852
Epoch 1390, val loss: 0.5843674540519714
Epoch 1400, training loss: 84.54530334472656 = 0.5009799003601074 + 10.0 * 8.40443229675293
Epoch 1400, val loss: 0.5831884145736694
Epoch 1410, training loss: 84.54013061523438 = 0.49944859743118286 + 10.0 * 8.404067993164062
Epoch 1410, val loss: 0.5820253491401672
Epoch 1420, training loss: 84.53025817871094 = 0.4979665279388428 + 10.0 * 8.403229713439941
Epoch 1420, val loss: 0.5808343291282654
Epoch 1430, training loss: 84.52833557128906 = 0.49650710821151733 + 10.0 * 8.403182983398438
Epoch 1430, val loss: 0.57972252368927
Epoch 1440, training loss: 84.5445556640625 = 0.4950554668903351 + 10.0 * 8.404950141906738
Epoch 1440, val loss: 0.578635036945343
Epoch 1450, training loss: 84.51203918457031 = 0.49357303977012634 + 10.0 * 8.401845932006836
Epoch 1450, val loss: 0.5774685144424438
Epoch 1460, training loss: 84.51362609863281 = 0.4921420216560364 + 10.0 * 8.402148246765137
Epoch 1460, val loss: 0.5764095783233643
Epoch 1470, training loss: 84.55252838134766 = 0.49072393774986267 + 10.0 * 8.406180381774902
Epoch 1470, val loss: 0.5753150582313538
Epoch 1480, training loss: 84.51242065429688 = 0.48930349946022034 + 10.0 * 8.402311325073242
Epoch 1480, val loss: 0.5742087364196777
Epoch 1490, training loss: 84.49314880371094 = 0.48793163895606995 + 10.0 * 8.400522232055664
Epoch 1490, val loss: 0.5732245445251465
Epoch 1500, training loss: 84.48936462402344 = 0.4865858554840088 + 10.0 * 8.400278091430664
Epoch 1500, val loss: 0.5722168684005737
Epoch 1510, training loss: 84.4830322265625 = 0.48526811599731445 + 10.0 * 8.399776458740234
Epoch 1510, val loss: 0.5712441802024841
Epoch 1520, training loss: 84.48287963867188 = 0.4839622974395752 + 10.0 * 8.39989185333252
Epoch 1520, val loss: 0.5702822208404541
Epoch 1530, training loss: 84.52652740478516 = 0.482648104429245 + 10.0 * 8.404387474060059
Epoch 1530, val loss: 0.5693202018737793
Epoch 1540, training loss: 84.48546600341797 = 0.4813249707221985 + 10.0 * 8.40041446685791
Epoch 1540, val loss: 0.5683028697967529
Epoch 1550, training loss: 84.476318359375 = 0.4800471365451813 + 10.0 * 8.399626731872559
Epoch 1550, val loss: 0.5674172639846802
Epoch 1560, training loss: 84.45854949951172 = 0.4788130521774292 + 10.0 * 8.397974014282227
Epoch 1560, val loss: 0.5665618777275085
Epoch 1570, training loss: 84.45355224609375 = 0.4776100218296051 + 10.0 * 8.397594451904297
Epoch 1570, val loss: 0.5656901597976685
Epoch 1580, training loss: 84.44758605957031 = 0.47642773389816284 + 10.0 * 8.397115707397461
Epoch 1580, val loss: 0.5648770332336426
Epoch 1590, training loss: 84.44219207763672 = 0.4752501845359802 + 10.0 * 8.39669418334961
Epoch 1590, val loss: 0.5640617609024048
Epoch 1600, training loss: 84.441650390625 = 0.4740773141384125 + 10.0 * 8.396757125854492
Epoch 1600, val loss: 0.5632180571556091
Epoch 1610, training loss: 84.51239776611328 = 0.4728723168373108 + 10.0 * 8.403952598571777
Epoch 1610, val loss: 0.5624088048934937
Epoch 1620, training loss: 84.45635223388672 = 0.4716677963733673 + 10.0 * 8.398468971252441
Epoch 1620, val loss: 0.5615660548210144
Epoch 1630, training loss: 84.43265533447266 = 0.4705222547054291 + 10.0 * 8.39621353149414
Epoch 1630, val loss: 0.560804545879364
Epoch 1640, training loss: 84.42120361328125 = 0.4694198668003082 + 10.0 * 8.395177841186523
Epoch 1640, val loss: 0.5601012706756592
Epoch 1650, training loss: 84.41534423828125 = 0.4683399796485901 + 10.0 * 8.39470100402832
Epoch 1650, val loss: 0.5593970417976379
Epoch 1660, training loss: 84.4110107421875 = 0.46727046370506287 + 10.0 * 8.394373893737793
Epoch 1660, val loss: 0.5587121248245239
Epoch 1670, training loss: 84.41776275634766 = 0.46619582176208496 + 10.0 * 8.395156860351562
Epoch 1670, val loss: 0.5580093264579773
Epoch 1680, training loss: 84.4298324584961 = 0.4650934040546417 + 10.0 * 8.39647388458252
Epoch 1680, val loss: 0.5573117733001709
Epoch 1690, training loss: 84.4050521850586 = 0.4639962911605835 + 10.0 * 8.394105911254883
Epoch 1690, val loss: 0.5565741658210754
Epoch 1700, training loss: 84.40120697021484 = 0.46296221017837524 + 10.0 * 8.393824577331543
Epoch 1700, val loss: 0.5559626221656799
Epoch 1710, training loss: 84.38873291015625 = 0.4619317054748535 + 10.0 * 8.392680168151855
Epoch 1710, val loss: 0.5553037524223328
Epoch 1720, training loss: 84.38404083251953 = 0.46092796325683594 + 10.0 * 8.392311096191406
Epoch 1720, val loss: 0.5546966195106506
Epoch 1730, training loss: 84.38001251220703 = 0.45993441343307495 + 10.0 * 8.392007827758789
Epoch 1730, val loss: 0.5540981888771057
Epoch 1740, training loss: 84.40785217285156 = 0.45895183086395264 + 10.0 * 8.394889831542969
Epoch 1740, val loss: 0.5536598563194275
Epoch 1750, training loss: 84.39325714111328 = 0.45790228247642517 + 10.0 * 8.393535614013672
Epoch 1750, val loss: 0.5526701211929321
Epoch 1760, training loss: 84.37332153320312 = 0.4569222033023834 + 10.0 * 8.391639709472656
Epoch 1760, val loss: 0.5522841215133667
Epoch 1770, training loss: 84.36383056640625 = 0.45594143867492676 + 10.0 * 8.390789031982422
Epoch 1770, val loss: 0.5516953468322754
Epoch 1780, training loss: 84.35862731933594 = 0.4549804627895355 + 10.0 * 8.390364646911621
Epoch 1780, val loss: 0.5510491728782654
Epoch 1790, training loss: 84.35377502441406 = 0.45402899384498596 + 10.0 * 8.389974594116211
Epoch 1790, val loss: 0.5504903793334961
Epoch 1800, training loss: 84.45909118652344 = 0.4530597925186157 + 10.0 * 8.400603294372559
Epoch 1800, val loss: 0.5498475432395935
Epoch 1810, training loss: 84.39181518554688 = 0.45206522941589355 + 10.0 * 8.393975257873535
Epoch 1810, val loss: 0.5493329167366028
Epoch 1820, training loss: 84.35858154296875 = 0.45106884837150574 + 10.0 * 8.390751838684082
Epoch 1820, val loss: 0.5487579107284546
Epoch 1830, training loss: 84.3385238647461 = 0.4501449763774872 + 10.0 * 8.388837814331055
Epoch 1830, val loss: 0.5481736063957214
Epoch 1840, training loss: 84.33277893066406 = 0.449245810508728 + 10.0 * 8.38835334777832
Epoch 1840, val loss: 0.5476862788200378
Epoch 1850, training loss: 84.32743072509766 = 0.4483468234539032 + 10.0 * 8.387907981872559
Epoch 1850, val loss: 0.5471670627593994
Epoch 1860, training loss: 84.3225326538086 = 0.44745081663131714 + 10.0 * 8.387508392333984
Epoch 1860, val loss: 0.5466704964637756
Epoch 1870, training loss: 84.32090759277344 = 0.4465509057044983 + 10.0 * 8.387435913085938
Epoch 1870, val loss: 0.5461411476135254
Epoch 1880, training loss: 84.3960189819336 = 0.4456358253955841 + 10.0 * 8.395038604736328
Epoch 1880, val loss: 0.5456717610359192
Epoch 1890, training loss: 84.31491088867188 = 0.4447099566459656 + 10.0 * 8.387020111083984
Epoch 1890, val loss: 0.5450951457023621
Epoch 1900, training loss: 84.31172180175781 = 0.4438122510910034 + 10.0 * 8.386791229248047
Epoch 1900, val loss: 0.5446357727050781
Epoch 1910, training loss: 84.30366516113281 = 0.4429234266281128 + 10.0 * 8.38607406616211
Epoch 1910, val loss: 0.5441545248031616
Epoch 1920, training loss: 84.3008804321289 = 0.44205141067504883 + 10.0 * 8.385882377624512
Epoch 1920, val loss: 0.5436737537384033
Epoch 1930, training loss: 84.36963653564453 = 0.44117921590805054 + 10.0 * 8.39284610748291
Epoch 1930, val loss: 0.5432292222976685
Epoch 1940, training loss: 84.31415557861328 = 0.4402608275413513 + 10.0 * 8.387389183044434
Epoch 1940, val loss: 0.5427426099777222
Epoch 1950, training loss: 84.29531860351562 = 0.43938112258911133 + 10.0 * 8.38559341430664
Epoch 1950, val loss: 0.5422026515007019
Epoch 1960, training loss: 84.28466796875 = 0.43851980566978455 + 10.0 * 8.384614944458008
Epoch 1960, val loss: 0.5418166518211365
Epoch 1970, training loss: 84.2798080444336 = 0.4376627504825592 + 10.0 * 8.384214401245117
Epoch 1970, val loss: 0.5413273572921753
Epoch 1980, training loss: 84.31330871582031 = 0.436797171831131 + 10.0 * 8.387651443481445
Epoch 1980, val loss: 0.5408035516738892
Epoch 1990, training loss: 84.27195739746094 = 0.4359283149242401 + 10.0 * 8.3836030960083
Epoch 1990, val loss: 0.5404937267303467
Epoch 2000, training loss: 84.26799011230469 = 0.43505555391311646 + 10.0 * 8.383293151855469
Epoch 2000, val loss: 0.5399484634399414
Epoch 2010, training loss: 84.26080322265625 = 0.43420174717903137 + 10.0 * 8.382659912109375
Epoch 2010, val loss: 0.5395920276641846
Epoch 2020, training loss: 84.2581787109375 = 0.4333510398864746 + 10.0 * 8.382482528686523
Epoch 2020, val loss: 0.539140522480011
Epoch 2030, training loss: 84.26276397705078 = 0.43249741196632385 + 10.0 * 8.383027076721191
Epoch 2030, val loss: 0.5386669039726257
Epoch 2040, training loss: 84.31550598144531 = 0.43162885308265686 + 10.0 * 8.388387680053711
Epoch 2040, val loss: 0.538218080997467
Epoch 2050, training loss: 84.26533508300781 = 0.43076691031455994 + 10.0 * 8.38345718383789
Epoch 2050, val loss: 0.5378178358078003
Epoch 2060, training loss: 84.24514770507812 = 0.42990192770957947 + 10.0 * 8.381525039672852
Epoch 2060, val loss: 0.5373163819313049
Epoch 2070, training loss: 84.24009704589844 = 0.4290592670440674 + 10.0 * 8.381103515625
Epoch 2070, val loss: 0.5369067788124084
Epoch 2080, training loss: 84.23702239990234 = 0.42821788787841797 + 10.0 * 8.380880355834961
Epoch 2080, val loss: 0.5364648103713989
Epoch 2090, training loss: 84.28144073486328 = 0.4273700714111328 + 10.0 * 8.385407447814941
Epoch 2090, val loss: 0.5359009504318237
Epoch 2100, training loss: 84.24868774414062 = 0.42651259899139404 + 10.0 * 8.382217407226562
Epoch 2100, val loss: 0.5357558727264404
Epoch 2110, training loss: 84.24323272705078 = 0.4256429970264435 + 10.0 * 8.381758689880371
Epoch 2110, val loss: 0.5350922346115112
Epoch 2120, training loss: 84.23983764648438 = 0.4247930943965912 + 10.0 * 8.381505012512207
Epoch 2120, val loss: 0.5347946286201477
Epoch 2130, training loss: 84.22144317626953 = 0.42394155263900757 + 10.0 * 8.37975025177002
Epoch 2130, val loss: 0.5343664288520813
Epoch 2140, training loss: 84.23039245605469 = 0.42309945821762085 + 10.0 * 8.380729675292969
Epoch 2140, val loss: 0.5339792370796204
Epoch 2150, training loss: 84.23107147216797 = 0.4222472906112671 + 10.0 * 8.380882263183594
Epoch 2150, val loss: 0.5335126519203186
Epoch 2160, training loss: 84.22238159179688 = 0.42138415575027466 + 10.0 * 8.380099296569824
Epoch 2160, val loss: 0.5330357551574707
Epoch 2170, training loss: 84.20985412597656 = 0.420533686876297 + 10.0 * 8.378931999206543
Epoch 2170, val loss: 0.5326811075210571
Epoch 2180, training loss: 84.20791625976562 = 0.4196852445602417 + 10.0 * 8.378823280334473
Epoch 2180, val loss: 0.5321910977363586
Epoch 2190, training loss: 84.23174285888672 = 0.41883495450019836 + 10.0 * 8.381290435791016
Epoch 2190, val loss: 0.5317939519882202
Epoch 2200, training loss: 84.199462890625 = 0.417982816696167 + 10.0 * 8.378148078918457
Epoch 2200, val loss: 0.5314127802848816
Epoch 2210, training loss: 84.21356964111328 = 0.41713401675224304 + 10.0 * 8.379643440246582
Epoch 2210, val loss: 0.5309918522834778
Epoch 2220, training loss: 84.22372436523438 = 0.41626960039138794 + 10.0 * 8.380745887756348
Epoch 2220, val loss: 0.5305668115615845
Epoch 2230, training loss: 84.1998291015625 = 0.41542312502861023 + 10.0 * 8.378440856933594
Epoch 2230, val loss: 0.5301928520202637
Epoch 2240, training loss: 84.18721008300781 = 0.4145675003528595 + 10.0 * 8.377264022827148
Epoch 2240, val loss: 0.5297666788101196
Epoch 2250, training loss: 84.18219757080078 = 0.41372501850128174 + 10.0 * 8.376847267150879
Epoch 2250, val loss: 0.5294017195701599
Epoch 2260, training loss: 84.18865966796875 = 0.4128842055797577 + 10.0 * 8.37757682800293
Epoch 2260, val loss: 0.5290994644165039
Epoch 2270, training loss: 84.1993408203125 = 0.412024587392807 + 10.0 * 8.378731727600098
Epoch 2270, val loss: 0.5286625027656555
Epoch 2280, training loss: 84.18913269042969 = 0.411144495010376 + 10.0 * 8.377798080444336
Epoch 2280, val loss: 0.5281845331192017
Epoch 2290, training loss: 84.17943572998047 = 0.41029298305511475 + 10.0 * 8.376914024353027
Epoch 2290, val loss: 0.527816653251648
Epoch 2300, training loss: 84.18194580078125 = 0.4094368815422058 + 10.0 * 8.377250671386719
Epoch 2300, val loss: 0.5274543762207031
Epoch 2310, training loss: 84.17021179199219 = 0.4085763990879059 + 10.0 * 8.376163482666016
Epoch 2310, val loss: 0.5269978642463684
Epoch 2320, training loss: 84.17576599121094 = 0.40772759914398193 + 10.0 * 8.376803398132324
Epoch 2320, val loss: 0.5266414880752563
Epoch 2330, training loss: 84.17561340332031 = 0.40687233209609985 + 10.0 * 8.376873970031738
Epoch 2330, val loss: 0.5262351036071777
Epoch 2340, training loss: 84.17989349365234 = 0.4060104489326477 + 10.0 * 8.377388000488281
Epoch 2340, val loss: 0.5258368253707886
Epoch 2350, training loss: 84.1701889038086 = 0.4051402509212494 + 10.0 * 8.376504898071289
Epoch 2350, val loss: 0.5253726840019226
Epoch 2360, training loss: 84.15360260009766 = 0.40428251028060913 + 10.0 * 8.374932289123535
Epoch 2360, val loss: 0.5250030755996704
Epoch 2370, training loss: 84.16600799560547 = 0.403426855802536 + 10.0 * 8.37625789642334
Epoch 2370, val loss: 0.5245924592018127
Epoch 2380, training loss: 84.15957641601562 = 0.4025639593601227 + 10.0 * 8.375700950622559
Epoch 2380, val loss: 0.5242065191268921
Epoch 2390, training loss: 84.15869140625 = 0.4017098844051361 + 10.0 * 8.37569808959961
Epoch 2390, val loss: 0.5239553451538086
Epoch 2400, training loss: 84.16511535644531 = 0.400848925113678 + 10.0 * 8.376426696777344
Epoch 2400, val loss: 0.5234972238540649
Epoch 2410, training loss: 84.14028930664062 = 0.39998742938041687 + 10.0 * 8.374030113220215
Epoch 2410, val loss: 0.5231338739395142
Epoch 2420, training loss: 84.13871765136719 = 0.39913758635520935 + 10.0 * 8.373957633972168
Epoch 2420, val loss: 0.5228309035301208
Epoch 2430, training loss: 84.13359069824219 = 0.3982850909233093 + 10.0 * 8.373530387878418
Epoch 2430, val loss: 0.5224423408508301
Epoch 2440, training loss: 84.14492797851562 = 0.3974328339099884 + 10.0 * 8.374750137329102
Epoch 2440, val loss: 0.5220827460289001
Epoch 2450, training loss: 84.19457244873047 = 0.39657062292099 + 10.0 * 8.379800796508789
Epoch 2450, val loss: 0.5218648910522461
Epoch 2460, training loss: 84.1436996459961 = 0.3956775665283203 + 10.0 * 8.374802589416504
Epoch 2460, val loss: 0.5213459134101868
Epoch 2470, training loss: 84.12727355957031 = 0.39482054114341736 + 10.0 * 8.373245239257812
Epoch 2470, val loss: 0.5211062431335449
Epoch 2480, training loss: 84.12006378173828 = 0.3939612805843353 + 10.0 * 8.372610092163086
Epoch 2480, val loss: 0.5206932425498962
Epoch 2490, training loss: 84.11804962158203 = 0.3931043744087219 + 10.0 * 8.3724946975708
Epoch 2490, val loss: 0.5203647017478943
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7990867579908676
0.8140259363906398
The final CL Acc:0.79773, 0.00266, The final GNN Acc:0.81499, 0.00099
raw graph: torch.Size([2, 88648])
add edge: torch.Size([2, 111056])
remove edge: torch.Size([2, 66614])
updated graph: torch.Size([2, 89022])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 106.9215087890625 = 1.099022626876831 + 10.0 * 10.58224868774414
Epoch 0, val loss: 1.0975160598754883
Epoch 10, training loss: 106.91219329833984 = 1.0944771766662598 + 10.0 * 10.581771850585938
Epoch 10, val loss: 1.0930014848709106
Epoch 20, training loss: 106.884765625 = 1.08966863155365 + 10.0 * 10.579509735107422
Epoch 20, val loss: 1.0882550477981567
Epoch 30, training loss: 106.77580261230469 = 1.0845675468444824 + 10.0 * 10.569124221801758
Epoch 30, val loss: 1.0832350254058838
Epoch 40, training loss: 106.3713150024414 = 1.0789529085159302 + 10.0 * 10.52923583984375
Epoch 40, val loss: 1.0776578187942505
Epoch 50, training loss: 105.15863037109375 = 1.0725584030151367 + 10.0 * 10.408607482910156
Epoch 50, val loss: 1.071305751800537
Epoch 60, training loss: 102.23199462890625 = 1.0658038854599 + 10.0 * 10.116619110107422
Epoch 60, val loss: 1.0645898580551147
Epoch 70, training loss: 98.50081634521484 = 1.0579555034637451 + 10.0 * 9.74428653717041
Epoch 70, val loss: 1.0567561388015747
Epoch 80, training loss: 97.36651611328125 = 1.0496528148651123 + 10.0 * 9.631686210632324
Epoch 80, val loss: 1.0485578775405884
Epoch 90, training loss: 96.60639953613281 = 1.0421950817108154 + 10.0 * 9.55642032623291
Epoch 90, val loss: 1.0413044691085815
Epoch 100, training loss: 95.56875610351562 = 1.03634512424469 + 10.0 * 9.453241348266602
Epoch 100, val loss: 1.0356547832489014
Epoch 110, training loss: 94.0846176147461 = 1.0323355197906494 + 10.0 * 9.305228233337402
Epoch 110, val loss: 1.0318515300750732
Epoch 120, training loss: 92.96508026123047 = 1.0295705795288086 + 10.0 * 9.193551063537598
Epoch 120, val loss: 1.0292414426803589
Epoch 130, training loss: 92.35798645019531 = 1.0269138813018799 + 10.0 * 9.13310718536377
Epoch 130, val loss: 1.0266318321228027
Epoch 140, training loss: 91.62841033935547 = 1.024003505706787 + 10.0 * 9.060441017150879
Epoch 140, val loss: 1.023708462715149
Epoch 150, training loss: 90.66541290283203 = 1.0207996368408203 + 10.0 * 8.964461326599121
Epoch 150, val loss: 1.0205495357513428
Epoch 160, training loss: 90.0025634765625 = 1.0178076028823853 + 10.0 * 8.898475646972656
Epoch 160, val loss: 1.017657995223999
Epoch 170, training loss: 89.44117736816406 = 1.014754056930542 + 10.0 * 8.842641830444336
Epoch 170, val loss: 1.0146515369415283
Epoch 180, training loss: 88.79876708984375 = 1.0117517709732056 + 10.0 * 8.778701782226562
Epoch 180, val loss: 1.0116791725158691
Epoch 190, training loss: 88.45655059814453 = 1.0085705518722534 + 10.0 * 8.74479866027832
Epoch 190, val loss: 1.008476972579956
Epoch 200, training loss: 88.2636947631836 = 1.0044853687286377 + 10.0 * 8.725920677185059
Epoch 200, val loss: 1.0044560432434082
Epoch 210, training loss: 88.12055206298828 = 0.9997383952140808 + 10.0 * 8.712080955505371
Epoch 210, val loss: 0.9998285174369812
Epoch 220, training loss: 87.98107147216797 = 0.99472975730896 + 10.0 * 8.698634147644043
Epoch 220, val loss: 0.9949222207069397
Epoch 230, training loss: 87.8240737915039 = 0.989891767501831 + 10.0 * 8.683418273925781
Epoch 230, val loss: 0.9901596307754517
Epoch 240, training loss: 87.64078521728516 = 0.9854678511619568 + 10.0 * 8.665532112121582
Epoch 240, val loss: 0.9858236908912659
Epoch 250, training loss: 87.45157623291016 = 0.981267511844635 + 10.0 * 8.6470308303833
Epoch 250, val loss: 0.9816980957984924
Epoch 260, training loss: 87.2187271118164 = 0.9771354794502258 + 10.0 * 8.62415885925293
Epoch 260, val loss: 0.9776374697685242
Epoch 270, training loss: 87.01702880859375 = 0.9728599190711975 + 10.0 * 8.604416847229004
Epoch 270, val loss: 0.973385214805603
Epoch 280, training loss: 86.84944152832031 = 0.9680519700050354 + 10.0 * 8.588138580322266
Epoch 280, val loss: 0.9686315059661865
Epoch 290, training loss: 86.72171783447266 = 0.9625751376152039 + 10.0 * 8.57591438293457
Epoch 290, val loss: 0.9631743431091309
Epoch 300, training loss: 86.59419250488281 = 0.9565925598144531 + 10.0 * 8.563759803771973
Epoch 300, val loss: 0.9572553038597107
Epoch 310, training loss: 86.48094940185547 = 0.9502149224281311 + 10.0 * 8.553072929382324
Epoch 310, val loss: 0.9509921669960022
Epoch 320, training loss: 86.38851928710938 = 0.9434551000595093 + 10.0 * 8.544506072998047
Epoch 320, val loss: 0.9443352818489075
Epoch 330, training loss: 86.28506469726562 = 0.9362631440162659 + 10.0 * 8.534879684448242
Epoch 330, val loss: 0.9372894763946533
Epoch 340, training loss: 86.19267272949219 = 0.9287574291229248 + 10.0 * 8.526391983032227
Epoch 340, val loss: 0.9299021363258362
Epoch 350, training loss: 86.11397552490234 = 0.9208777546882629 + 10.0 * 8.519309997558594
Epoch 350, val loss: 0.9221440553665161
Epoch 360, training loss: 86.04503631591797 = 0.9125983715057373 + 10.0 * 8.513243675231934
Epoch 360, val loss: 0.913998007774353
Epoch 370, training loss: 85.9942398071289 = 0.9039132595062256 + 10.0 * 8.509032249450684
Epoch 370, val loss: 0.9054757356643677
Epoch 380, training loss: 85.90991973876953 = 0.8948891758918762 + 10.0 * 8.501502990722656
Epoch 380, val loss: 0.8966528177261353
Epoch 390, training loss: 85.85099029541016 = 0.8856930732727051 + 10.0 * 8.496529579162598
Epoch 390, val loss: 0.8876651525497437
Epoch 400, training loss: 85.80241394042969 = 0.8763089776039124 + 10.0 * 8.492609977722168
Epoch 400, val loss: 0.8784939050674438
Epoch 410, training loss: 85.75566101074219 = 0.8666701316833496 + 10.0 * 8.488899230957031
Epoch 410, val loss: 0.8691542148590088
Epoch 420, training loss: 85.70503997802734 = 0.8568950295448303 + 10.0 * 8.484814643859863
Epoch 420, val loss: 0.859641432762146
Epoch 430, training loss: 85.66008758544922 = 0.8470516204833984 + 10.0 * 8.481303215026855
Epoch 430, val loss: 0.8501251935958862
Epoch 440, training loss: 85.6175308227539 = 0.8371453881263733 + 10.0 * 8.478038787841797
Epoch 440, val loss: 0.8405606150627136
Epoch 450, training loss: 85.5673599243164 = 0.8272560834884644 + 10.0 * 8.474010467529297
Epoch 450, val loss: 0.8309986591339111
Epoch 460, training loss: 85.52635192871094 = 0.8173951506614685 + 10.0 * 8.470895767211914
Epoch 460, val loss: 0.8214985728263855
Epoch 470, training loss: 85.47175598144531 = 0.8075587749481201 + 10.0 * 8.46642017364502
Epoch 470, val loss: 0.8120570778846741
Epoch 480, training loss: 85.43431091308594 = 0.7977848052978516 + 10.0 * 8.463652610778809
Epoch 480, val loss: 0.8026903867721558
Epoch 490, training loss: 85.39126586914062 = 0.7880997657775879 + 10.0 * 8.46031665802002
Epoch 490, val loss: 0.7933099269866943
Epoch 500, training loss: 85.33687591552734 = 0.7783921957015991 + 10.0 * 8.455848693847656
Epoch 500, val loss: 0.7840322256088257
Epoch 510, training loss: 85.30130767822266 = 0.7687286138534546 + 10.0 * 8.45325756072998
Epoch 510, val loss: 0.774762749671936
Epoch 520, training loss: 85.2707290649414 = 0.7590833306312561 + 10.0 * 8.451164245605469
Epoch 520, val loss: 0.7654774188995361
Epoch 530, training loss: 85.23755645751953 = 0.7494862079620361 + 10.0 * 8.448806762695312
Epoch 530, val loss: 0.7562586069107056
Epoch 540, training loss: 85.210205078125 = 0.739909827709198 + 10.0 * 8.447030067443848
Epoch 540, val loss: 0.7470694184303284
Epoch 550, training loss: 85.16732025146484 = 0.7305039167404175 + 10.0 * 8.443681716918945
Epoch 550, val loss: 0.738031268119812
Epoch 560, training loss: 85.13436126708984 = 0.7212300300598145 + 10.0 * 8.441312789916992
Epoch 560, val loss: 0.72911137342453
Epoch 570, training loss: 85.10518646240234 = 0.712066650390625 + 10.0 * 8.439311981201172
Epoch 570, val loss: 0.7202900648117065
Epoch 580, training loss: 85.08934783935547 = 0.7029973864555359 + 10.0 * 8.438634872436523
Epoch 580, val loss: 0.7115507125854492
Epoch 590, training loss: 85.06269836425781 = 0.6940375566482544 + 10.0 * 8.43686580657959
Epoch 590, val loss: 0.7029494643211365
Epoch 600, training loss: 85.03911590576172 = 0.6852363348007202 + 10.0 * 8.435388565063477
Epoch 600, val loss: 0.6944819688796997
Epoch 610, training loss: 84.99414825439453 = 0.6766849160194397 + 10.0 * 8.431746482849121
Epoch 610, val loss: 0.6862775087356567
Epoch 620, training loss: 84.96466064453125 = 0.6683642864227295 + 10.0 * 8.4296293258667
Epoch 620, val loss: 0.6782607436180115
Epoch 630, training loss: 84.935791015625 = 0.6601922512054443 + 10.0 * 8.427559852600098
Epoch 630, val loss: 0.6703994274139404
Epoch 640, training loss: 84.90642547607422 = 0.6521932482719421 + 10.0 * 8.425423622131348
Epoch 640, val loss: 0.6627123951911926
Epoch 650, training loss: 84.87776184082031 = 0.6443275213241577 + 10.0 * 8.423343658447266
Epoch 650, val loss: 0.6551434993743896
Epoch 660, training loss: 84.90645599365234 = 0.6366328597068787 + 10.0 * 8.426981925964355
Epoch 660, val loss: 0.6477024555206299
Epoch 670, training loss: 84.83692932128906 = 0.628930389881134 + 10.0 * 8.42080020904541
Epoch 670, val loss: 0.6403455138206482
Epoch 680, training loss: 84.80253601074219 = 0.6215499043464661 + 10.0 * 8.418098449707031
Epoch 680, val loss: 0.6333064436912537
Epoch 690, training loss: 84.76991271972656 = 0.6144420504570007 + 10.0 * 8.415547370910645
Epoch 690, val loss: 0.6265201568603516
Epoch 700, training loss: 84.74382019042969 = 0.6075065732002258 + 10.0 * 8.413631439208984
Epoch 700, val loss: 0.6198937296867371
Epoch 710, training loss: 84.75155639648438 = 0.6006913781166077 + 10.0 * 8.41508674621582
Epoch 710, val loss: 0.6133933663368225
Epoch 720, training loss: 84.6998291015625 = 0.5939456820487976 + 10.0 * 8.410588264465332
Epoch 720, val loss: 0.6069858074188232
Epoch 730, training loss: 84.66804504394531 = 0.5874665975570679 + 10.0 * 8.408058166503906
Epoch 730, val loss: 0.6008703112602234
Epoch 740, training loss: 84.6478271484375 = 0.5811936259269714 + 10.0 * 8.40666389465332
Epoch 740, val loss: 0.5949687957763672
Epoch 750, training loss: 84.64066314697266 = 0.5750338435173035 + 10.0 * 8.406562805175781
Epoch 750, val loss: 0.5891807675361633
Epoch 760, training loss: 84.60421752929688 = 0.5690656900405884 + 10.0 * 8.403514862060547
Epoch 760, val loss: 0.5836067199707031
Epoch 770, training loss: 84.58041381835938 = 0.5632773637771606 + 10.0 * 8.401713371276855
Epoch 770, val loss: 0.5782307386398315
Epoch 780, training loss: 84.56460571289062 = 0.5576315522193909 + 10.0 * 8.400697708129883
Epoch 780, val loss: 0.5729966163635254
Epoch 790, training loss: 84.55470275878906 = 0.5520882606506348 + 10.0 * 8.400261878967285
Epoch 790, val loss: 0.5678899884223938
Epoch 800, training loss: 84.52851867675781 = 0.5466313362121582 + 10.0 * 8.398188591003418
Epoch 800, val loss: 0.5628783702850342
Epoch 810, training loss: 84.5057601928711 = 0.5414853096008301 + 10.0 * 8.396427154541016
Epoch 810, val loss: 0.5581841468811035
Epoch 820, training loss: 84.48825073242188 = 0.5365110635757446 + 10.0 * 8.395174026489258
Epoch 820, val loss: 0.5536788702011108
Epoch 830, training loss: 84.47016906738281 = 0.5316427946090698 + 10.0 * 8.393853187561035
Epoch 830, val loss: 0.5492883324623108
Epoch 840, training loss: 84.46259307861328 = 0.5269271731376648 + 10.0 * 8.393567085266113
Epoch 840, val loss: 0.5450407266616821
Epoch 850, training loss: 84.44159698486328 = 0.522260308265686 + 10.0 * 8.39193344116211
Epoch 850, val loss: 0.5409207344055176
Epoch 860, training loss: 84.43234252929688 = 0.517765998840332 + 10.0 * 8.391457557678223
Epoch 860, val loss: 0.5369186997413635
Epoch 870, training loss: 84.4103012084961 = 0.5134727358818054 + 10.0 * 8.38968276977539
Epoch 870, val loss: 0.533158004283905
Epoch 880, training loss: 84.39495849609375 = 0.5093075037002563 + 10.0 * 8.388565063476562
Epoch 880, val loss: 0.5294970273971558
Epoch 890, training loss: 84.39273071289062 = 0.5052599310874939 + 10.0 * 8.388747215270996
Epoch 890, val loss: 0.5259571671485901
Epoch 900, training loss: 84.36796569824219 = 0.5013170838356018 + 10.0 * 8.386664390563965
Epoch 900, val loss: 0.522558331489563
Epoch 910, training loss: 84.35458374023438 = 0.49750950932502747 + 10.0 * 8.385706901550293
Epoch 910, val loss: 0.5192610621452332
Epoch 920, training loss: 84.3542251586914 = 0.4937889873981476 + 10.0 * 8.386043548583984
Epoch 920, val loss: 0.5160876512527466
Epoch 930, training loss: 84.33231353759766 = 0.49015167355537415 + 10.0 * 8.38421630859375
Epoch 930, val loss: 0.5130090117454529
Epoch 940, training loss: 84.32341003417969 = 0.48665502667427063 + 10.0 * 8.383675575256348
Epoch 940, val loss: 0.5100561380386353
Epoch 950, training loss: 84.30855560302734 = 0.4832181930541992 + 10.0 * 8.38253402709961
Epoch 950, val loss: 0.5071737170219421
Epoch 960, training loss: 84.2976303100586 = 0.4799136221408844 + 10.0 * 8.3817720413208
Epoch 960, val loss: 0.5043761730194092
Epoch 970, training loss: 84.29163360595703 = 0.4766218066215515 + 10.0 * 8.381501197814941
Epoch 970, val loss: 0.5016888380050659
Epoch 980, training loss: 84.27982330322266 = 0.47345778346061707 + 10.0 * 8.380636215209961
Epoch 980, val loss: 0.4990343451499939
Epoch 990, training loss: 84.26248931884766 = 0.47044727206230164 + 10.0 * 8.379204750061035
Epoch 990, val loss: 0.4965738356113434
Epoch 1000, training loss: 84.24559783935547 = 0.4675109088420868 + 10.0 * 8.377808570861816
Epoch 1000, val loss: 0.4941691756248474
Epoch 1010, training loss: 84.23287200927734 = 0.46465131640434265 + 10.0 * 8.376821517944336
Epoch 1010, val loss: 0.4918372929096222
Epoch 1020, training loss: 84.22233581542969 = 0.46185067296028137 + 10.0 * 8.37604808807373
Epoch 1020, val loss: 0.4895801544189453
Epoch 1030, training loss: 84.21151733398438 = 0.459094375371933 + 10.0 * 8.375242233276367
Epoch 1030, val loss: 0.48734089732170105
Epoch 1040, training loss: 84.25005340576172 = 0.4563804268836975 + 10.0 * 8.379366874694824
Epoch 1040, val loss: 0.48516231775283813
Epoch 1050, training loss: 84.20318603515625 = 0.4536612927913666 + 10.0 * 8.37495231628418
Epoch 1050, val loss: 0.48294681310653687
Epoch 1060, training loss: 84.18505096435547 = 0.4510875344276428 + 10.0 * 8.373395919799805
Epoch 1060, val loss: 0.4808901250362396
Epoch 1070, training loss: 84.17327880859375 = 0.44857820868492126 + 10.0 * 8.372469902038574
Epoch 1070, val loss: 0.4788947105407715
Epoch 1080, training loss: 84.16210174560547 = 0.44611531496047974 + 10.0 * 8.371599197387695
Epoch 1080, val loss: 0.4769032597541809
Epoch 1090, training loss: 84.18408966064453 = 0.4436933398246765 + 10.0 * 8.374039649963379
Epoch 1090, val loss: 0.4749862253665924
Epoch 1100, training loss: 84.15808868408203 = 0.4412722587585449 + 10.0 * 8.371682167053223
Epoch 1100, val loss: 0.47310858964920044
Epoch 1110, training loss: 84.13372039794922 = 0.43895599246025085 + 10.0 * 8.369476318359375
Epoch 1110, val loss: 0.4712704122066498
Epoch 1120, training loss: 84.1270980834961 = 0.43667492270469666 + 10.0 * 8.36904239654541
Epoch 1120, val loss: 0.46948036551475525
Epoch 1130, training loss: 84.14983367919922 = 0.43441125750541687 + 10.0 * 8.371541976928711
Epoch 1130, val loss: 0.4676873981952667
Epoch 1140, training loss: 84.1164779663086 = 0.4321782886981964 + 10.0 * 8.368430137634277
Epoch 1140, val loss: 0.46600037813186646
Epoch 1150, training loss: 84.10127258300781 = 0.43000948429107666 + 10.0 * 8.36712646484375
Epoch 1150, val loss: 0.4643060266971588
Epoch 1160, training loss: 84.09071350097656 = 0.42787793278694153 + 10.0 * 8.366283416748047
Epoch 1160, val loss: 0.46267274022102356
Epoch 1170, training loss: 84.09855651855469 = 0.42578136920928955 + 10.0 * 8.367277145385742
Epoch 1170, val loss: 0.46108534932136536
Epoch 1180, training loss: 84.08013916015625 = 0.4236821234226227 + 10.0 * 8.365645408630371
Epoch 1180, val loss: 0.4594338536262512
Epoch 1190, training loss: 84.07270050048828 = 0.4216445982456207 + 10.0 * 8.365105628967285
Epoch 1190, val loss: 0.4578983783721924
Epoch 1200, training loss: 84.06171417236328 = 0.41963547468185425 + 10.0 * 8.364208221435547
Epoch 1200, val loss: 0.4563583731651306
Epoch 1210, training loss: 84.0682144165039 = 0.41765666007995605 + 10.0 * 8.365056037902832
Epoch 1210, val loss: 0.45483046770095825
Epoch 1220, training loss: 84.0494384765625 = 0.41566789150238037 + 10.0 * 8.363377571105957
Epoch 1220, val loss: 0.4533703029155731
Epoch 1230, training loss: 84.03968811035156 = 0.41374143958091736 + 10.0 * 8.362594604492188
Epoch 1230, val loss: 0.4518943130970001
Epoch 1240, training loss: 84.03369140625 = 0.4118562936782837 + 10.0 * 8.362183570861816
Epoch 1240, val loss: 0.4505370557308197
Epoch 1250, training loss: 84.02693939208984 = 0.4099956154823303 + 10.0 * 8.3616943359375
Epoch 1250, val loss: 0.4491071105003357
Epoch 1260, training loss: 84.043701171875 = 0.40815040469169617 + 10.0 * 8.363554954528809
Epoch 1260, val loss: 0.44778165221214294
Epoch 1270, training loss: 84.0218734741211 = 0.4063219726085663 + 10.0 * 8.361555099487305
Epoch 1270, val loss: 0.44638267159461975
Epoch 1280, training loss: 84.01380920410156 = 0.40453511476516724 + 10.0 * 8.36092758178711
Epoch 1280, val loss: 0.44509318470954895
Epoch 1290, training loss: 84.01233673095703 = 0.4027753174304962 + 10.0 * 8.360956192016602
Epoch 1290, val loss: 0.44379451870918274
Epoch 1300, training loss: 83.99424743652344 = 0.4010249376296997 + 10.0 * 8.359322547912598
Epoch 1300, val loss: 0.44250792264938354
Epoch 1310, training loss: 83.98892974853516 = 0.39931046962738037 + 10.0 * 8.358962059020996
Epoch 1310, val loss: 0.4412746727466583
Epoch 1320, training loss: 83.98377227783203 = 0.39761945605278015 + 10.0 * 8.358614921569824
Epoch 1320, val loss: 0.44003576040267944
Epoch 1330, training loss: 83.99697875976562 = 0.39593490958213806 + 10.0 * 8.36010456085205
Epoch 1330, val loss: 0.43884775042533875
Epoch 1340, training loss: 83.97880554199219 = 0.3942593038082123 + 10.0 * 8.358454704284668
Epoch 1340, val loss: 0.4376159906387329
Epoch 1350, training loss: 83.99140167236328 = 0.39261072874069214 + 10.0 * 8.359879493713379
Epoch 1350, val loss: 0.4364829957485199
Epoch 1360, training loss: 83.962890625 = 0.3909749984741211 + 10.0 * 8.35719108581543
Epoch 1360, val loss: 0.4352249801158905
Epoch 1370, training loss: 83.9518051147461 = 0.3893822431564331 + 10.0 * 8.356242179870605
Epoch 1370, val loss: 0.4341377913951874
Epoch 1380, training loss: 83.94474029541016 = 0.3878101408481598 + 10.0 * 8.355692863464355
Epoch 1380, val loss: 0.43300461769104004
Epoch 1390, training loss: 83.93946075439453 = 0.386250376701355 + 10.0 * 8.355320930480957
Epoch 1390, val loss: 0.431915819644928
Epoch 1400, training loss: 83.94142150878906 = 0.38470256328582764 + 10.0 * 8.355671882629395
Epoch 1400, val loss: 0.43082916736602783
Epoch 1410, training loss: 83.9406967163086 = 0.3831571340560913 + 10.0 * 8.355753898620605
Epoch 1410, val loss: 0.42976027727127075
Epoch 1420, training loss: 83.9300308227539 = 0.3816169500350952 + 10.0 * 8.354841232299805
Epoch 1420, val loss: 0.4286613464355469
Epoch 1430, training loss: 83.91899871826172 = 0.3801133632659912 + 10.0 * 8.353888511657715
Epoch 1430, val loss: 0.4275915324687958
Epoch 1440, training loss: 83.9144058227539 = 0.3786378502845764 + 10.0 * 8.35357666015625
Epoch 1440, val loss: 0.4266115128993988
Epoch 1450, training loss: 83.92107391357422 = 0.3771710991859436 + 10.0 * 8.354390144348145
Epoch 1450, val loss: 0.42559927701950073
Epoch 1460, training loss: 83.90512084960938 = 0.3757166862487793 + 10.0 * 8.352940559387207
Epoch 1460, val loss: 0.4245871305465698
Epoch 1470, training loss: 83.89554595947266 = 0.3742872476577759 + 10.0 * 8.35212516784668
Epoch 1470, val loss: 0.42359453439712524
Epoch 1480, training loss: 83.89033508300781 = 0.37287843227386475 + 10.0 * 8.35174560546875
Epoch 1480, val loss: 0.4226411283016205
Epoch 1490, training loss: 83.89321899414062 = 0.37148258090019226 + 10.0 * 8.352173805236816
Epoch 1490, val loss: 0.4216793179512024
Epoch 1500, training loss: 83.90919494628906 = 0.3700747787952423 + 10.0 * 8.353912353515625
Epoch 1500, val loss: 0.42072784900665283
Epoch 1510, training loss: 83.87889099121094 = 0.3686915338039398 + 10.0 * 8.351019859313965
Epoch 1510, val loss: 0.41984233260154724
Epoch 1520, training loss: 83.87059020996094 = 0.36733922362327576 + 10.0 * 8.350324630737305
Epoch 1520, val loss: 0.41898083686828613
Epoch 1530, training loss: 83.86585235595703 = 0.36600300669670105 + 10.0 * 8.349985122680664
Epoch 1530, val loss: 0.41805756092071533
Epoch 1540, training loss: 83.86113739013672 = 0.3646795153617859 + 10.0 * 8.349645614624023
Epoch 1540, val loss: 0.4172281324863434
Epoch 1550, training loss: 83.86290740966797 = 0.3633649945259094 + 10.0 * 8.349954605102539
Epoch 1550, val loss: 0.4163496792316437
Epoch 1560, training loss: 83.87350463867188 = 0.3620487153530121 + 10.0 * 8.35114574432373
Epoch 1560, val loss: 0.4155055284500122
Epoch 1570, training loss: 83.85917663574219 = 0.36074528098106384 + 10.0 * 8.34984302520752
Epoch 1570, val loss: 0.41471707820892334
Epoch 1580, training loss: 83.84899139404297 = 0.3594571650028229 + 10.0 * 8.348953247070312
Epoch 1580, val loss: 0.41393420100212097
Epoch 1590, training loss: 83.8490982055664 = 0.35819101333618164 + 10.0 * 8.349090576171875
Epoch 1590, val loss: 0.4131332337856293
Epoch 1600, training loss: 83.84463500976562 = 0.35692930221557617 + 10.0 * 8.348771095275879
Epoch 1600, val loss: 0.41237035393714905
Epoch 1610, training loss: 83.83551025390625 = 0.3556816875934601 + 10.0 * 8.347982406616211
Epoch 1610, val loss: 0.4116002321243286
Epoch 1620, training loss: 83.82747650146484 = 0.3544531762599945 + 10.0 * 8.347302436828613
Epoch 1620, val loss: 0.41086840629577637
Epoch 1630, training loss: 83.83818817138672 = 0.3532335162162781 + 10.0 * 8.348495483398438
Epoch 1630, val loss: 0.41012054681777954
Epoch 1640, training loss: 83.83411407470703 = 0.3519965708255768 + 10.0 * 8.348211288452148
Epoch 1640, val loss: 0.409415602684021
Epoch 1650, training loss: 83.8199462890625 = 0.3507899045944214 + 10.0 * 8.346915245056152
Epoch 1650, val loss: 0.40870970487594604
Epoch 1660, training loss: 83.81427764892578 = 0.3496010899543762 + 10.0 * 8.346467971801758
Epoch 1660, val loss: 0.4080449044704437
Epoch 1670, training loss: 83.80845642089844 = 0.3484329283237457 + 10.0 * 8.346002578735352
Epoch 1670, val loss: 0.4073491096496582
Epoch 1680, training loss: 83.80908966064453 = 0.34726980328559875 + 10.0 * 8.346181869506836
Epoch 1680, val loss: 0.4067124128341675
Epoch 1690, training loss: 83.81389617919922 = 0.34610670804977417 + 10.0 * 8.346778869628906
Epoch 1690, val loss: 0.4060615599155426
Epoch 1700, training loss: 83.80085754394531 = 0.34495294094085693 + 10.0 * 8.345590591430664
Epoch 1700, val loss: 0.40542325377464294
Epoch 1710, training loss: 83.79376220703125 = 0.3438110649585724 + 10.0 * 8.344995498657227
Epoch 1710, val loss: 0.40479519963264465
Epoch 1720, training loss: 83.79071807861328 = 0.3426811695098877 + 10.0 * 8.344803810119629
Epoch 1720, val loss: 0.4042071998119354
Epoch 1730, training loss: 83.85356140136719 = 0.3415498733520508 + 10.0 * 8.351201057434082
Epoch 1730, val loss: 0.4036750793457031
Epoch 1740, training loss: 83.799072265625 = 0.34039798378944397 + 10.0 * 8.345867156982422
Epoch 1740, val loss: 0.4029541015625
Epoch 1750, training loss: 83.77840423583984 = 0.3392905592918396 + 10.0 * 8.343912124633789
Epoch 1750, val loss: 0.40246468782424927
Epoch 1760, training loss: 83.77595520019531 = 0.33819785714149475 + 10.0 * 8.343775749206543
Epoch 1760, val loss: 0.40185531973838806
Epoch 1770, training loss: 83.76953887939453 = 0.3371124565601349 + 10.0 * 8.343242645263672
Epoch 1770, val loss: 0.401340126991272
Epoch 1780, training loss: 83.76536560058594 = 0.3360305726528168 + 10.0 * 8.342933654785156
Epoch 1780, val loss: 0.40078505873680115
Epoch 1790, training loss: 83.77173614501953 = 0.3349531590938568 + 10.0 * 8.34367847442627
Epoch 1790, val loss: 0.4002704620361328
Epoch 1800, training loss: 83.77594757080078 = 0.33386483788490295 + 10.0 * 8.344208717346191
Epoch 1800, val loss: 0.39977186918258667
Epoch 1810, training loss: 83.76776885986328 = 0.3327905535697937 + 10.0 * 8.343497276306152
Epoch 1810, val loss: 0.3991990089416504
Epoch 1820, training loss: 83.75250244140625 = 0.3317451477050781 + 10.0 * 8.342076301574707
Epoch 1820, val loss: 0.3987447917461395
Epoch 1830, training loss: 83.7485122680664 = 0.33070987462997437 + 10.0 * 8.341779708862305
Epoch 1830, val loss: 0.39823511242866516
Epoch 1840, training loss: 83.74409484863281 = 0.3296809196472168 + 10.0 * 8.34144115447998
Epoch 1840, val loss: 0.397788941860199
Epoch 1850, training loss: 83.74401092529297 = 0.32865455746650696 + 10.0 * 8.341535568237305
Epoch 1850, val loss: 0.39731737971305847
Epoch 1860, training loss: 83.79446411132812 = 0.32762381434440613 + 10.0 * 8.346684455871582
Epoch 1860, val loss: 0.3969101905822754
Epoch 1870, training loss: 83.74359893798828 = 0.32659071683883667 + 10.0 * 8.341700553894043
Epoch 1870, val loss: 0.3963448107242584
Epoch 1880, training loss: 83.73306274414062 = 0.32558223605155945 + 10.0 * 8.340747833251953
Epoch 1880, val loss: 0.39597293734550476
Epoch 1890, training loss: 83.7282485961914 = 0.3245833218097687 + 10.0 * 8.34036636352539
Epoch 1890, val loss: 0.39546337723731995
Epoch 1900, training loss: 83.72377014160156 = 0.3235868811607361 + 10.0 * 8.340018272399902
Epoch 1900, val loss: 0.3950778543949127
Epoch 1910, training loss: 83.72180938720703 = 0.3225908577442169 + 10.0 * 8.339921951293945
Epoch 1910, val loss: 0.3946680724620819
Epoch 1920, training loss: 83.75138092041016 = 0.321597158908844 + 10.0 * 8.342978477478027
Epoch 1920, val loss: 0.3942742943763733
Epoch 1930, training loss: 83.72345733642578 = 0.32060155272483826 + 10.0 * 8.340285301208496
Epoch 1930, val loss: 0.39380550384521484
Epoch 1940, training loss: 83.71498107910156 = 0.31962451338768005 + 10.0 * 8.3395357131958
Epoch 1940, val loss: 0.3934597969055176
Epoch 1950, training loss: 83.70870208740234 = 0.3186577260494232 + 10.0 * 8.339004516601562
Epoch 1950, val loss: 0.393039345741272
Epoch 1960, training loss: 83.70785522460938 = 0.317695289850235 + 10.0 * 8.33901596069336
Epoch 1960, val loss: 0.39270198345184326
Epoch 1970, training loss: 83.77384185791016 = 0.3167296051979065 + 10.0 * 8.345711708068848
Epoch 1970, val loss: 0.39237305521965027
Epoch 1980, training loss: 83.71015167236328 = 0.3157555162906647 + 10.0 * 8.339439392089844
Epoch 1980, val loss: 0.3919040560722351
Epoch 1990, training loss: 83.6968765258789 = 0.31481045484542847 + 10.0 * 8.33820629119873
Epoch 1990, val loss: 0.39154425263404846
Epoch 2000, training loss: 83.69284057617188 = 0.3138689398765564 + 10.0 * 8.337897300720215
Epoch 2000, val loss: 0.3911885917186737
Epoch 2010, training loss: 83.68788146972656 = 0.3129284977912903 + 10.0 * 8.337495803833008
Epoch 2010, val loss: 0.390861451625824
Epoch 2020, training loss: 83.68437957763672 = 0.31198716163635254 + 10.0 * 8.337239265441895
Epoch 2020, val loss: 0.3905206024646759
Epoch 2030, training loss: 83.68614959716797 = 0.31104519963264465 + 10.0 * 8.33751106262207
Epoch 2030, val loss: 0.3901767134666443
Epoch 2040, training loss: 83.70478820800781 = 0.3100937604904175 + 10.0 * 8.339468955993652
Epoch 2040, val loss: 0.389863520860672
Epoch 2050, training loss: 83.6847915649414 = 0.3091481924057007 + 10.0 * 8.337564468383789
Epoch 2050, val loss: 0.38951700925827026
Epoch 2060, training loss: 83.6736068725586 = 0.30822262167930603 + 10.0 * 8.336538314819336
Epoch 2060, val loss: 0.3892306685447693
Epoch 2070, training loss: 83.67037200927734 = 0.30730029940605164 + 10.0 * 8.336307525634766
Epoch 2070, val loss: 0.38891011476516724
Epoch 2080, training loss: 83.66569519042969 = 0.3063816428184509 + 10.0 * 8.335931777954102
Epoch 2080, val loss: 0.38863861560821533
Epoch 2090, training loss: 83.6695785522461 = 0.30546629428863525 + 10.0 * 8.336411476135254
Epoch 2090, val loss: 0.38837525248527527
Epoch 2100, training loss: 83.69503784179688 = 0.30453962087631226 + 10.0 * 8.339049339294434
Epoch 2100, val loss: 0.388115257024765
Epoch 2110, training loss: 83.65846252441406 = 0.3036162257194519 + 10.0 * 8.335484504699707
Epoch 2110, val loss: 0.38780251145362854
Epoch 2120, training loss: 83.65750122070312 = 0.30271077156066895 + 10.0 * 8.335478782653809
Epoch 2120, val loss: 0.38752391934394836
Epoch 2130, training loss: 83.6511459350586 = 0.30180859565734863 + 10.0 * 8.334933280944824
Epoch 2130, val loss: 0.3872280716896057
Epoch 2140, training loss: 83.64686584472656 = 0.300906240940094 + 10.0 * 8.334596633911133
Epoch 2140, val loss: 0.38698989152908325
Epoch 2150, training loss: 83.65396881103516 = 0.30000510811805725 + 10.0 * 8.335396766662598
Epoch 2150, val loss: 0.3867291510105133
Epoch 2160, training loss: 83.65702819824219 = 0.2990948259830475 + 10.0 * 8.335793495178223
Epoch 2160, val loss: 0.3864990770816803
Epoch 2170, training loss: 83.6405029296875 = 0.2981862425804138 + 10.0 * 8.33423137664795
Epoch 2170, val loss: 0.386231005191803
Epoch 2180, training loss: 83.63652038574219 = 0.29728689789772034 + 10.0 * 8.33392333984375
Epoch 2180, val loss: 0.3859506845474243
Epoch 2190, training loss: 83.63190460205078 = 0.29639366269111633 + 10.0 * 8.333551406860352
Epoch 2190, val loss: 0.3856911063194275
Epoch 2200, training loss: 83.62895202636719 = 0.29550275206565857 + 10.0 * 8.333345413208008
Epoch 2200, val loss: 0.385453462600708
Epoch 2210, training loss: 83.65050506591797 = 0.2946113049983978 + 10.0 * 8.335589408874512
Epoch 2210, val loss: 0.38518115878105164
Epoch 2220, training loss: 83.62088012695312 = 0.293716162443161 + 10.0 * 8.33271598815918
Epoch 2220, val loss: 0.38503146171569824
Epoch 2230, training loss: 83.6172866821289 = 0.2928239703178406 + 10.0 * 8.332446098327637
Epoch 2230, val loss: 0.38481947779655457
Epoch 2240, training loss: 83.61487579345703 = 0.29193517565727234 + 10.0 * 8.332293510437012
Epoch 2240, val loss: 0.3845392167568207
Epoch 2250, training loss: 83.61775207519531 = 0.29104742407798767 + 10.0 * 8.332670211791992
Epoch 2250, val loss: 0.384365439414978
Epoch 2260, training loss: 83.63237762451172 = 0.2901498079299927 + 10.0 * 8.334222793579102
Epoch 2260, val loss: 0.38414955139160156
Epoch 2270, training loss: 83.60489654541016 = 0.28925400972366333 + 10.0 * 8.331563949584961
Epoch 2270, val loss: 0.38394221663475037
Epoch 2280, training loss: 83.6035385131836 = 0.2883680462837219 + 10.0 * 8.331517219543457
Epoch 2280, val loss: 0.38372093439102173
Epoch 2290, training loss: 83.59776306152344 = 0.2874772250652313 + 10.0 * 8.331028938293457
Epoch 2290, val loss: 0.38353368639945984
Epoch 2300, training loss: 83.59514617919922 = 0.28658899664878845 + 10.0 * 8.330855369567871
Epoch 2300, val loss: 0.38333776593208313
Epoch 2310, training loss: 83.63473510742188 = 0.2856961488723755 + 10.0 * 8.334903717041016
Epoch 2310, val loss: 0.3831479549407959
Epoch 2320, training loss: 83.61044311523438 = 0.2848058342933655 + 10.0 * 8.332563400268555
Epoch 2320, val loss: 0.38301557302474976
Epoch 2330, training loss: 83.59300231933594 = 0.2839203476905823 + 10.0 * 8.330907821655273
Epoch 2330, val loss: 0.38285917043685913
Epoch 2340, training loss: 83.58787536621094 = 0.28304362297058105 + 10.0 * 8.330483436584473
Epoch 2340, val loss: 0.38271820545196533
Epoch 2350, training loss: 83.58317565917969 = 0.28216660022735596 + 10.0 * 8.330101013183594
Epoch 2350, val loss: 0.3826139569282532
Epoch 2360, training loss: 83.58709716796875 = 0.281289666891098 + 10.0 * 8.330580711364746
Epoch 2360, val loss: 0.38250184059143066
Epoch 2370, training loss: 83.57936096191406 = 0.2804115116596222 + 10.0 * 8.32989501953125
Epoch 2370, val loss: 0.38237595558166504
Epoch 2380, training loss: 83.56868743896484 = 0.2795374095439911 + 10.0 * 8.328914642333984
Epoch 2380, val loss: 0.3822517693042755
Epoch 2390, training loss: 83.5714111328125 = 0.27867189049720764 + 10.0 * 8.32927417755127
Epoch 2390, val loss: 0.3821167051792145
Epoch 2400, training loss: 83.57506561279297 = 0.2778058350086212 + 10.0 * 8.32972526550293
Epoch 2400, val loss: 0.3820238709449768
Epoch 2410, training loss: 83.58297729492188 = 0.2769402265548706 + 10.0 * 8.33060359954834
Epoch 2410, val loss: 0.3820011615753174
Epoch 2420, training loss: 83.566162109375 = 0.2760660648345947 + 10.0 * 8.329009056091309
Epoch 2420, val loss: 0.3818797469139099
Epoch 2430, training loss: 83.56084442138672 = 0.27520108222961426 + 10.0 * 8.328564643859863
Epoch 2430, val loss: 0.3818560540676117
Epoch 2440, training loss: 83.55321502685547 = 0.27434447407722473 + 10.0 * 8.327886581420898
Epoch 2440, val loss: 0.38174188137054443
Epoch 2450, training loss: 83.54756927490234 = 0.2734888792037964 + 10.0 * 8.327407836914062
Epoch 2450, val loss: 0.38169586658477783
Epoch 2460, training loss: 83.5479965209961 = 0.2726303040981293 + 10.0 * 8.327536582946777
Epoch 2460, val loss: 0.38162049651145935
Epoch 2470, training loss: 83.57669067382812 = 0.27177104353904724 + 10.0 * 8.33049201965332
Epoch 2470, val loss: 0.38156482577323914
Epoch 2480, training loss: 83.54712677001953 = 0.2709154188632965 + 10.0 * 8.327621459960938
Epoch 2480, val loss: 0.3815675675868988
Epoch 2490, training loss: 83.53772735595703 = 0.27006644010543823 + 10.0 * 8.326766014099121
Epoch 2490, val loss: 0.38149556517601013
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8437341451040081
0.8645222053176846
=== training gcn model ===
Epoch 0, training loss: 106.91705322265625 = 1.094231367111206 + 10.0 * 10.582282066345215
Epoch 0, val loss: 1.093311071395874
Epoch 10, training loss: 106.90872955322266 = 1.089526891708374 + 10.0 * 10.581920623779297
Epoch 10, val loss: 1.0886567831039429
Epoch 20, training loss: 106.88720703125 = 1.0846025943756104 + 10.0 * 10.580260276794434
Epoch 20, val loss: 1.0837719440460205
Epoch 30, training loss: 106.80517578125 = 1.079453945159912 + 10.0 * 10.572572708129883
Epoch 30, val loss: 1.078688383102417
Epoch 40, training loss: 106.49028015136719 = 1.0740947723388672 + 10.0 * 10.541618347167969
Epoch 40, val loss: 1.0733857154846191
Epoch 50, training loss: 105.51134490966797 = 1.0682833194732666 + 10.0 * 10.444306373596191
Epoch 50, val loss: 1.067676305770874
Epoch 60, training loss: 103.10173797607422 = 1.0625847578048706 + 10.0 * 10.2039155960083
Epoch 60, val loss: 1.0621650218963623
Epoch 70, training loss: 98.7600326538086 = 1.0574805736541748 + 10.0 * 9.770255088806152
Epoch 70, val loss: 1.057153582572937
Epoch 80, training loss: 96.27741241455078 = 1.0528347492218018 + 10.0 * 9.52245807647705
Epoch 80, val loss: 1.0526586771011353
Epoch 90, training loss: 94.86273193359375 = 1.0480773448944092 + 10.0 * 9.381464958190918
Epoch 90, val loss: 1.0479763746261597
Epoch 100, training loss: 93.42433166503906 = 1.0425148010253906 + 10.0 * 9.238181114196777
Epoch 100, val loss: 1.0424764156341553
Epoch 110, training loss: 92.3662109375 = 1.0364964008331299 + 10.0 * 9.132970809936523
Epoch 110, val loss: 1.0365921258926392
Epoch 120, training loss: 91.71109771728516 = 1.030444860458374 + 10.0 * 9.068065643310547
Epoch 120, val loss: 1.030741572380066
Epoch 130, training loss: 91.04529571533203 = 1.0250823497772217 + 10.0 * 9.002020835876465
Epoch 130, val loss: 1.0256820917129517
Epoch 140, training loss: 90.42869567871094 = 1.020593523979187 + 10.0 * 8.940810203552246
Epoch 140, val loss: 1.0214427709579468
Epoch 150, training loss: 90.02865600585938 = 1.0166147947311401 + 10.0 * 8.901204109191895
Epoch 150, val loss: 1.0175637006759644
Epoch 160, training loss: 89.64254760742188 = 1.0127047300338745 + 10.0 * 8.862984657287598
Epoch 160, val loss: 1.0136290788650513
Epoch 170, training loss: 89.2999038696289 = 1.0086742639541626 + 10.0 * 8.829122543334961
Epoch 170, val loss: 1.0095938444137573
Epoch 180, training loss: 89.09395599365234 = 1.0045043230056763 + 10.0 * 8.808945655822754
Epoch 180, val loss: 1.005495548248291
Epoch 190, training loss: 88.905517578125 = 0.9998282194137573 + 10.0 * 8.790569305419922
Epoch 190, val loss: 1.0009725093841553
Epoch 200, training loss: 88.66458129882812 = 0.9948249459266663 + 10.0 * 8.766975402832031
Epoch 200, val loss: 0.9962115287780762
Epoch 210, training loss: 88.36967468261719 = 0.9901376366615295 + 10.0 * 8.737954139709473
Epoch 210, val loss: 0.9917348027229309
Epoch 220, training loss: 88.06888580322266 = 0.9855221509933472 + 10.0 * 8.70833683013916
Epoch 220, val loss: 0.9873051047325134
Epoch 230, training loss: 87.83576965332031 = 0.9802161455154419 + 10.0 * 8.685555458068848
Epoch 230, val loss: 0.9821427464485168
Epoch 240, training loss: 87.62078094482422 = 0.9739776253700256 + 10.0 * 8.664680480957031
Epoch 240, val loss: 0.9761084914207458
Epoch 250, training loss: 87.4053955078125 = 0.9674860835075378 + 10.0 * 8.643791198730469
Epoch 250, val loss: 0.9699299931526184
Epoch 260, training loss: 87.23274993896484 = 0.9608938097953796 + 10.0 * 8.627185821533203
Epoch 260, val loss: 0.9636003971099854
Epoch 270, training loss: 87.06847381591797 = 0.9539408087730408 + 10.0 * 8.61145305633545
Epoch 270, val loss: 0.9569236040115356
Epoch 280, training loss: 86.94510650634766 = 0.9465356469154358 + 10.0 * 8.599857330322266
Epoch 280, val loss: 0.9497732520103455
Epoch 290, training loss: 86.84236907958984 = 0.9385430812835693 + 10.0 * 8.59038257598877
Epoch 290, val loss: 0.9420442581176758
Epoch 300, training loss: 86.78132629394531 = 0.9300360679626465 + 10.0 * 8.585128784179688
Epoch 300, val loss: 0.9338662028312683
Epoch 310, training loss: 86.67787170410156 = 0.921301007270813 + 10.0 * 8.57565689086914
Epoch 310, val loss: 0.9254292845726013
Epoch 320, training loss: 86.59471130371094 = 0.9123660326004028 + 10.0 * 8.56823444366455
Epoch 320, val loss: 0.9168532490730286
Epoch 330, training loss: 86.51131439208984 = 0.9032642245292664 + 10.0 * 8.56080436706543
Epoch 330, val loss: 0.9081059098243713
Epoch 340, training loss: 86.42906188964844 = 0.8940457105636597 + 10.0 * 8.553502082824707
Epoch 340, val loss: 0.8992648720741272
Epoch 350, training loss: 86.35417175292969 = 0.884692370891571 + 10.0 * 8.546948432922363
Epoch 350, val loss: 0.8902952075004578
Epoch 360, training loss: 86.30157470703125 = 0.8751521706581116 + 10.0 * 8.542642593383789
Epoch 360, val loss: 0.8811495900154114
Epoch 370, training loss: 86.22923278808594 = 0.8654558658599854 + 10.0 * 8.536377906799316
Epoch 370, val loss: 0.8718836307525635
Epoch 380, training loss: 86.16680145263672 = 0.855749785900116 + 10.0 * 8.531105041503906
Epoch 380, val loss: 0.8626192808151245
Epoch 390, training loss: 86.10518646240234 = 0.8460541367530823 + 10.0 * 8.52591323852539
Epoch 390, val loss: 0.853377103805542
Epoch 400, training loss: 86.04854583740234 = 0.8363832235336304 + 10.0 * 8.52121639251709
Epoch 400, val loss: 0.8441659212112427
Epoch 410, training loss: 85.98412322998047 = 0.8267883062362671 + 10.0 * 8.51573371887207
Epoch 410, val loss: 0.8350498080253601
Epoch 420, training loss: 85.91010284423828 = 0.8173999786376953 + 10.0 * 8.509270668029785
Epoch 420, val loss: 0.8261422514915466
Epoch 430, training loss: 85.83729553222656 = 0.8080726265907288 + 10.0 * 8.502922058105469
Epoch 430, val loss: 0.8173071146011353
Epoch 440, training loss: 85.76284790039062 = 0.798904299736023 + 10.0 * 8.496394157409668
Epoch 440, val loss: 0.8086223602294922
Epoch 450, training loss: 85.69522094726562 = 0.7898128628730774 + 10.0 * 8.490541458129883
Epoch 450, val loss: 0.8000009059906006
Epoch 460, training loss: 85.65068054199219 = 0.7807257175445557 + 10.0 * 8.486995697021484
Epoch 460, val loss: 0.7914076447486877
Epoch 470, training loss: 85.58453369140625 = 0.7716370224952698 + 10.0 * 8.481289863586426
Epoch 470, val loss: 0.782794177532196
Epoch 480, training loss: 85.53020477294922 = 0.7625170350074768 + 10.0 * 8.476768493652344
Epoch 480, val loss: 0.7741620540618896
Epoch 490, training loss: 85.4566650390625 = 0.7534826397895813 + 10.0 * 8.470318794250488
Epoch 490, val loss: 0.7656218409538269
Epoch 500, training loss: 85.40794372558594 = 0.7444860339164734 + 10.0 * 8.46634578704834
Epoch 500, val loss: 0.7571036219596863
Epoch 510, training loss: 85.36227416992188 = 0.7354552745819092 + 10.0 * 8.462681770324707
Epoch 510, val loss: 0.7485588788986206
Epoch 520, training loss: 85.3507080078125 = 0.7264170050621033 + 10.0 * 8.46242904663086
Epoch 520, val loss: 0.7400046586990356
Epoch 530, training loss: 85.28618621826172 = 0.7172554731369019 + 10.0 * 8.456892967224121
Epoch 530, val loss: 0.7313135266304016
Epoch 540, training loss: 85.25154113769531 = 0.7082794904708862 + 10.0 * 8.454325675964355
Epoch 540, val loss: 0.7228407859802246
Epoch 550, training loss: 85.21881103515625 = 0.6993891000747681 + 10.0 * 8.451942443847656
Epoch 550, val loss: 0.7144291400909424
Epoch 560, training loss: 85.21257781982422 = 0.6905533075332642 + 10.0 * 8.452202796936035
Epoch 560, val loss: 0.7060707211494446
Epoch 570, training loss: 85.15242767333984 = 0.6817936897277832 + 10.0 * 8.447063446044922
Epoch 570, val loss: 0.6978136897087097
Epoch 580, training loss: 85.11518096923828 = 0.673206090927124 + 10.0 * 8.444197654724121
Epoch 580, val loss: 0.6897461414337158
Epoch 590, training loss: 85.07823181152344 = 0.6647456288337708 + 10.0 * 8.4413480758667
Epoch 590, val loss: 0.681778609752655
Epoch 600, training loss: 85.06333923339844 = 0.6564309000968933 + 10.0 * 8.440690994262695
Epoch 600, val loss: 0.6739705801010132
Epoch 610, training loss: 85.0355453491211 = 0.648167073726654 + 10.0 * 8.438737869262695
Epoch 610, val loss: 0.6663098931312561
Epoch 620, training loss: 84.97734069824219 = 0.6402007937431335 + 10.0 * 8.433713912963867
Epoch 620, val loss: 0.6588773131370544
Epoch 630, training loss: 84.94271087646484 = 0.6324241757392883 + 10.0 * 8.431028366088867
Epoch 630, val loss: 0.6516136527061462
Epoch 640, training loss: 84.90884399414062 = 0.6248024702072144 + 10.0 * 8.428403854370117
Epoch 640, val loss: 0.6445423364639282
Epoch 650, training loss: 84.87738800048828 = 0.6173706650733948 + 10.0 * 8.42600154876709
Epoch 650, val loss: 0.6376885771751404
Epoch 660, training loss: 84.8884048461914 = 0.610051691532135 + 10.0 * 8.427835464477539
Epoch 660, val loss: 0.6309426426887512
Epoch 670, training loss: 84.8437271118164 = 0.6028577089309692 + 10.0 * 8.424086570739746
Epoch 670, val loss: 0.6243933439254761
Epoch 680, training loss: 84.79715728759766 = 0.5959989428520203 + 10.0 * 8.42011547088623
Epoch 680, val loss: 0.6181362867355347
Epoch 690, training loss: 84.76856231689453 = 0.589409351348877 + 10.0 * 8.417915344238281
Epoch 690, val loss: 0.6121618747711182
Epoch 700, training loss: 84.74201202392578 = 0.5830132961273193 + 10.0 * 8.415899276733398
Epoch 700, val loss: 0.6063953042030334
Epoch 710, training loss: 84.71956634521484 = 0.5768218636512756 + 10.0 * 8.414274215698242
Epoch 710, val loss: 0.6008500456809998
Epoch 720, training loss: 84.7584228515625 = 0.5707922577857971 + 10.0 * 8.418763160705566
Epoch 720, val loss: 0.5954199433326721
Epoch 730, training loss: 84.679443359375 = 0.5649512410163879 + 10.0 * 8.411449432373047
Epoch 730, val loss: 0.5903427600860596
Epoch 740, training loss: 84.66055297851562 = 0.5594393014907837 + 10.0 * 8.410111427307129
Epoch 740, val loss: 0.5855024456977844
Epoch 750, training loss: 84.63526916503906 = 0.5541235208511353 + 10.0 * 8.408114433288574
Epoch 750, val loss: 0.5808416604995728
Epoch 760, training loss: 84.61743927001953 = 0.5489528179168701 + 10.0 * 8.406848907470703
Epoch 760, val loss: 0.5763694047927856
Epoch 770, training loss: 84.62632751464844 = 0.5439594388008118 + 10.0 * 8.408236503601074
Epoch 770, val loss: 0.5720133781433105
Epoch 780, training loss: 84.59219360351562 = 0.5391212701797485 + 10.0 * 8.405306816101074
Epoch 780, val loss: 0.567919135093689
Epoch 790, training loss: 84.56688690185547 = 0.5345084071159363 + 10.0 * 8.403238296508789
Epoch 790, val loss: 0.5639788508415222
Epoch 800, training loss: 84.54615020751953 = 0.5301069021224976 + 10.0 * 8.401604652404785
Epoch 800, val loss: 0.5602554678916931
Epoch 810, training loss: 84.53003692626953 = 0.5258570313453674 + 10.0 * 8.400418281555176
Epoch 810, val loss: 0.556693971157074
Epoch 820, training loss: 84.55609130859375 = 0.5217248797416687 + 10.0 * 8.403436660766602
Epoch 820, val loss: 0.553218424320221
Epoch 830, training loss: 84.50254821777344 = 0.5177589058876038 + 10.0 * 8.398478507995605
Epoch 830, val loss: 0.550004243850708
Epoch 840, training loss: 84.4787368774414 = 0.5140088796615601 + 10.0 * 8.396472930908203
Epoch 840, val loss: 0.5468944907188416
Epoch 850, training loss: 84.46344757080078 = 0.5103827118873596 + 10.0 * 8.395306587219238
Epoch 850, val loss: 0.5439545512199402
Epoch 860, training loss: 84.44470977783203 = 0.5068896412849426 + 10.0 * 8.393781661987305
Epoch 860, val loss: 0.5411560535430908
Epoch 870, training loss: 84.43077087402344 = 0.5035068392753601 + 10.0 * 8.392725944519043
Epoch 870, val loss: 0.5384469628334045
Epoch 880, training loss: 84.47723388671875 = 0.500167191028595 + 10.0 * 8.397706985473633
Epoch 880, val loss: 0.5357478260993958
Epoch 890, training loss: 84.41766357421875 = 0.49689948558807373 + 10.0 * 8.39207649230957
Epoch 890, val loss: 0.5332049131393433
Epoch 900, training loss: 84.38899230957031 = 0.4938786029815674 + 10.0 * 8.389511108398438
Epoch 900, val loss: 0.5307995080947876
Epoch 910, training loss: 84.3713150024414 = 0.4909326434135437 + 10.0 * 8.38803768157959
Epoch 910, val loss: 0.5284742712974548
Epoch 920, training loss: 84.35688018798828 = 0.48805034160614014 + 10.0 * 8.386882781982422
Epoch 920, val loss: 0.5263018012046814
Epoch 930, training loss: 84.3426513671875 = 0.48526477813720703 + 10.0 * 8.385738372802734
Epoch 930, val loss: 0.5241563320159912
Epoch 940, training loss: 84.33697509765625 = 0.48253822326660156 + 10.0 * 8.385443687438965
Epoch 940, val loss: 0.522042453289032
Epoch 950, training loss: 84.32926940917969 = 0.47984716296195984 + 10.0 * 8.384942054748535
Epoch 950, val loss: 0.5200477838516235
Epoch 960, training loss: 84.30886840820312 = 0.4772284924983978 + 10.0 * 8.383164405822754
Epoch 960, val loss: 0.5180860161781311
Epoch 970, training loss: 84.29345703125 = 0.47470253705978394 + 10.0 * 8.381875991821289
Epoch 970, val loss: 0.5161658525466919
Epoch 980, training loss: 84.28263092041016 = 0.4722430408000946 + 10.0 * 8.381038665771484
Epoch 980, val loss: 0.5143259167671204
Epoch 990, training loss: 84.37195587158203 = 0.46983057260513306 + 10.0 * 8.390212059020996
Epoch 990, val loss: 0.5124167203903198
Epoch 1000, training loss: 84.27861022949219 = 0.4673628807067871 + 10.0 * 8.381124496459961
Epoch 1000, val loss: 0.5107036828994751
Epoch 1010, training loss: 84.26058197021484 = 0.4650666117668152 + 10.0 * 8.379551887512207
Epoch 1010, val loss: 0.5090406537055969
Epoch 1020, training loss: 84.23994445800781 = 0.4628196656703949 + 10.0 * 8.37771224975586
Epoch 1020, val loss: 0.5073296427726746
Epoch 1030, training loss: 84.227783203125 = 0.46061697602272034 + 10.0 * 8.376716613769531
Epoch 1030, val loss: 0.5057154893875122
Epoch 1040, training loss: 84.21570587158203 = 0.45845621824264526 + 10.0 * 8.375724792480469
Epoch 1040, val loss: 0.5041468143463135
Epoch 1050, training loss: 84.20500183105469 = 0.4563260078430176 + 10.0 * 8.37486743927002
Epoch 1050, val loss: 0.5025726556777954
Epoch 1060, training loss: 84.20787048339844 = 0.45422446727752686 + 10.0 * 8.375364303588867
Epoch 1060, val loss: 0.5010277628898621
Epoch 1070, training loss: 84.20104217529297 = 0.4521164298057556 + 10.0 * 8.374892234802246
Epoch 1070, val loss: 0.49948248267173767
Epoch 1080, training loss: 84.17964172363281 = 0.45007145404815674 + 10.0 * 8.372957229614258
Epoch 1080, val loss: 0.497997909784317
Epoch 1090, training loss: 84.23133087158203 = 0.4480528235435486 + 10.0 * 8.378328323364258
Epoch 1090, val loss: 0.4964706003665924
Epoch 1100, training loss: 84.16534423828125 = 0.4460446834564209 + 10.0 * 8.371930122375488
Epoch 1100, val loss: 0.4951057434082031
Epoch 1110, training loss: 84.14923095703125 = 0.44412127137184143 + 10.0 * 8.370511054992676
Epoch 1110, val loss: 0.49367135763168335
Epoch 1120, training loss: 84.1354751586914 = 0.442221075296402 + 10.0 * 8.369325637817383
Epoch 1120, val loss: 0.4923022985458374
Epoch 1130, training loss: 84.12435150146484 = 0.44034677743911743 + 10.0 * 8.368400573730469
Epoch 1130, val loss: 0.49096375703811646
Epoch 1140, training loss: 84.12330627441406 = 0.4384925961494446 + 10.0 * 8.368481636047363
Epoch 1140, val loss: 0.48964905738830566
Epoch 1150, training loss: 84.1373291015625 = 0.4366134703159332 + 10.0 * 8.370071411132812
Epoch 1150, val loss: 0.48828211426734924
Epoch 1160, training loss: 84.0985336303711 = 0.43476274609565735 + 10.0 * 8.366376876831055
Epoch 1160, val loss: 0.48692548274993896
Epoch 1170, training loss: 84.08553314208984 = 0.4329702854156494 + 10.0 * 8.365256309509277
Epoch 1170, val loss: 0.4856317341327667
Epoch 1180, training loss: 84.07615661621094 = 0.43120431900024414 + 10.0 * 8.364495277404785
Epoch 1180, val loss: 0.4843917489051819
Epoch 1190, training loss: 84.06895446777344 = 0.4294631779193878 + 10.0 * 8.363948822021484
Epoch 1190, val loss: 0.48317596316337585
Epoch 1200, training loss: 84.0967788696289 = 0.4277171194553375 + 10.0 * 8.36690616607666
Epoch 1200, val loss: 0.481929749250412
Epoch 1210, training loss: 84.05306243896484 = 0.4259715974330902 + 10.0 * 8.362709045410156
Epoch 1210, val loss: 0.4807069003582001
Epoch 1220, training loss: 84.04352569580078 = 0.424283504486084 + 10.0 * 8.361924171447754
Epoch 1220, val loss: 0.4794904291629791
Epoch 1230, training loss: 84.03790283203125 = 0.42262324690818787 + 10.0 * 8.361528396606445
Epoch 1230, val loss: 0.4782870411872864
Epoch 1240, training loss: 84.08905792236328 = 0.42095187306404114 + 10.0 * 8.36681079864502
Epoch 1240, val loss: 0.47709572315216064
Epoch 1250, training loss: 84.0235595703125 = 0.4192845821380615 + 10.0 * 8.360427856445312
Epoch 1250, val loss: 0.4760299026966095
Epoch 1260, training loss: 84.01238250732422 = 0.4176839590072632 + 10.0 * 8.359469413757324
Epoch 1260, val loss: 0.4749048352241516
Epoch 1270, training loss: 84.00364685058594 = 0.41611552238464355 + 10.0 * 8.358753204345703
Epoch 1270, val loss: 0.47380414605140686
Epoch 1280, training loss: 83.9946517944336 = 0.41455960273742676 + 10.0 * 8.358009338378906
Epoch 1280, val loss: 0.4727712869644165
Epoch 1290, training loss: 83.99089813232422 = 0.4130229651927948 + 10.0 * 8.357787132263184
Epoch 1290, val loss: 0.47174614667892456
Epoch 1300, training loss: 84.03124237060547 = 0.41147807240486145 + 10.0 * 8.361976623535156
Epoch 1300, val loss: 0.4707385301589966
Epoch 1310, training loss: 83.9801254272461 = 0.40993157029151917 + 10.0 * 8.357019424438477
Epoch 1310, val loss: 0.46962860226631165
Epoch 1320, training loss: 83.970947265625 = 0.4084326922893524 + 10.0 * 8.35625171661377
Epoch 1320, val loss: 0.4685900807380676
Epoch 1330, training loss: 83.96405029296875 = 0.4069654941558838 + 10.0 * 8.355708122253418
Epoch 1330, val loss: 0.4676124155521393
Epoch 1340, training loss: 83.97601318359375 = 0.40550583600997925 + 10.0 * 8.357050895690918
Epoch 1340, val loss: 0.4665979743003845
Epoch 1350, training loss: 83.97318267822266 = 0.4040440022945404 + 10.0 * 8.356913566589355
Epoch 1350, val loss: 0.46564239263534546
Epoch 1360, training loss: 83.95364379882812 = 0.40260517597198486 + 10.0 * 8.355104446411133
Epoch 1360, val loss: 0.4646960496902466
Epoch 1370, training loss: 83.93821716308594 = 0.4012114405632019 + 10.0 * 8.353700637817383
Epoch 1370, val loss: 0.4637547731399536
Epoch 1380, training loss: 83.93244171142578 = 0.3998302221298218 + 10.0 * 8.35326099395752
Epoch 1380, val loss: 0.4627993404865265
Epoch 1390, training loss: 83.93423461914062 = 0.3984645903110504 + 10.0 * 8.35357666015625
Epoch 1390, val loss: 0.46186360716819763
Epoch 1400, training loss: 83.9546890258789 = 0.3970918655395508 + 10.0 * 8.355759620666504
Epoch 1400, val loss: 0.46091943979263306
Epoch 1410, training loss: 83.9242935180664 = 0.39573267102241516 + 10.0 * 8.352856636047363
Epoch 1410, val loss: 0.4600996673107147
Epoch 1420, training loss: 83.90811157226562 = 0.3944077491760254 + 10.0 * 8.351369857788086
Epoch 1420, val loss: 0.45917290449142456
Epoch 1430, training loss: 83.9033203125 = 0.39309611916542053 + 10.0 * 8.351022720336914
Epoch 1430, val loss: 0.45828232169151306
Epoch 1440, training loss: 83.91593170166016 = 0.39180177450180054 + 10.0 * 8.352413177490234
Epoch 1440, val loss: 0.45743826031684875
Epoch 1450, training loss: 83.91026306152344 = 0.3904750645160675 + 10.0 * 8.35197925567627
Epoch 1450, val loss: 0.4565224349498749
Epoch 1460, training loss: 83.88679504394531 = 0.3891933560371399 + 10.0 * 8.349760055541992
Epoch 1460, val loss: 0.4556712508201599
Epoch 1470, training loss: 83.88370513916016 = 0.38793477416038513 + 10.0 * 8.349576950073242
Epoch 1470, val loss: 0.45484471321105957
Epoch 1480, training loss: 83.87371826171875 = 0.38668757677078247 + 10.0 * 8.348703384399414
Epoch 1480, val loss: 0.454017698764801
Epoch 1490, training loss: 83.90033721923828 = 0.38544636964797974 + 10.0 * 8.351489067077637
Epoch 1490, val loss: 0.4531385898590088
Epoch 1500, training loss: 83.87763214111328 = 0.3841724991798401 + 10.0 * 8.349346160888672
Epoch 1500, val loss: 0.452424556016922
Epoch 1510, training loss: 83.87417602539062 = 0.3829262852668762 + 10.0 * 8.349124908447266
Epoch 1510, val loss: 0.45150768756866455
Epoch 1520, training loss: 83.85538482666016 = 0.3817288279533386 + 10.0 * 8.347365379333496
Epoch 1520, val loss: 0.45074689388275146
Epoch 1530, training loss: 83.84896850585938 = 0.3805525600910187 + 10.0 * 8.346841812133789
Epoch 1530, val loss: 0.4500356614589691
Epoch 1540, training loss: 83.84236907958984 = 0.37938112020492554 + 10.0 * 8.346299171447754
Epoch 1540, val loss: 0.4492645859718323
Epoch 1550, training loss: 83.837158203125 = 0.3782143294811249 + 10.0 * 8.345894813537598
Epoch 1550, val loss: 0.4485429525375366
Epoch 1560, training loss: 83.83356475830078 = 0.3770529627799988 + 10.0 * 8.345651626586914
Epoch 1560, val loss: 0.44779863953590393
Epoch 1570, training loss: 83.88524627685547 = 0.3758850693702698 + 10.0 * 8.350935935974121
Epoch 1570, val loss: 0.4470635652542114
Epoch 1580, training loss: 83.83052825927734 = 0.3747096657752991 + 10.0 * 8.345582008361816
Epoch 1580, val loss: 0.4463460147380829
Epoch 1590, training loss: 83.82140350341797 = 0.37356680631637573 + 10.0 * 8.344783782958984
Epoch 1590, val loss: 0.4455946087837219
Epoch 1600, training loss: 83.81343841552734 = 0.37243473529815674 + 10.0 * 8.344099998474121
Epoch 1600, val loss: 0.44488391280174255
Epoch 1610, training loss: 83.81489562988281 = 0.3713088631629944 + 10.0 * 8.344358444213867
Epoch 1610, val loss: 0.44418057799339294
Epoch 1620, training loss: 83.85448455810547 = 0.3701757788658142 + 10.0 * 8.348430633544922
Epoch 1620, val loss: 0.4434153735637665
Epoch 1630, training loss: 83.8113021850586 = 0.3690356910228729 + 10.0 * 8.344226837158203
Epoch 1630, val loss: 0.44274741411209106
Epoch 1640, training loss: 83.80027770996094 = 0.3679268956184387 + 10.0 * 8.34323501586914
Epoch 1640, val loss: 0.44206029176712036
Epoch 1650, training loss: 83.79377746582031 = 0.3668413758277893 + 10.0 * 8.342693328857422
Epoch 1650, val loss: 0.4413904547691345
Epoch 1660, training loss: 83.7854995727539 = 0.36576804518699646 + 10.0 * 8.341973304748535
Epoch 1660, val loss: 0.44073113799095154
Epoch 1670, training loss: 83.78095245361328 = 0.36469969153404236 + 10.0 * 8.341625213623047
Epoch 1670, val loss: 0.4400922358036041
Epoch 1680, training loss: 83.77729797363281 = 0.3636344075202942 + 10.0 * 8.3413667678833
Epoch 1680, val loss: 0.43945181369781494
Epoch 1690, training loss: 83.8328628540039 = 0.3625732958316803 + 10.0 * 8.347028732299805
Epoch 1690, val loss: 0.43889081478118896
Epoch 1700, training loss: 83.79735565185547 = 0.3614596128463745 + 10.0 * 8.343589782714844
Epoch 1700, val loss: 0.4380589425563812
Epoch 1710, training loss: 83.78084564208984 = 0.36040085554122925 + 10.0 * 8.342044830322266
Epoch 1710, val loss: 0.43747958540916443
Epoch 1720, training loss: 83.76171112060547 = 0.35936400294303894 + 10.0 * 8.340234756469727
Epoch 1720, val loss: 0.4368419051170349
Epoch 1730, training loss: 83.75784301757812 = 0.3583430051803589 + 10.0 * 8.339949607849121
Epoch 1730, val loss: 0.4362027645111084
Epoch 1740, training loss: 83.75323486328125 = 0.357329785823822 + 10.0 * 8.339590072631836
Epoch 1740, val loss: 0.43561917543411255
Epoch 1750, training loss: 83.74881744384766 = 0.3563169538974762 + 10.0 * 8.339250564575195
Epoch 1750, val loss: 0.43499428033828735
Epoch 1760, training loss: 83.7446517944336 = 0.35530591011047363 + 10.0 * 8.338933944702148
Epoch 1760, val loss: 0.43439987301826477
Epoch 1770, training loss: 83.75889587402344 = 0.35429543256759644 + 10.0 * 8.340459823608398
Epoch 1770, val loss: 0.4337829649448395
Epoch 1780, training loss: 83.7581787109375 = 0.35327279567718506 + 10.0 * 8.340490341186523
Epoch 1780, val loss: 0.43322235345840454
Epoch 1790, training loss: 83.73714447021484 = 0.3522590100765228 + 10.0 * 8.338488578796387
Epoch 1790, val loss: 0.4325806200504303
Epoch 1800, training loss: 83.73479461669922 = 0.35127219557762146 + 10.0 * 8.33835220336914
Epoch 1800, val loss: 0.4319774806499481
Epoch 1810, training loss: 83.72706604003906 = 0.35029521584510803 + 10.0 * 8.337677001953125
Epoch 1810, val loss: 0.43137672543525696
Epoch 1820, training loss: 83.7302017211914 = 0.3493254482746124 + 10.0 * 8.33808708190918
Epoch 1820, val loss: 0.4308048486709595
Epoch 1830, training loss: 83.7867202758789 = 0.3483413755893707 + 10.0 * 8.34383773803711
Epoch 1830, val loss: 0.43018200993537903
Epoch 1840, training loss: 83.72927856445312 = 0.34735479950904846 + 10.0 * 8.3381929397583
Epoch 1840, val loss: 0.4296822249889374
Epoch 1850, training loss: 83.71300506591797 = 0.3463892936706543 + 10.0 * 8.336661338806152
Epoch 1850, val loss: 0.42906251549720764
Epoch 1860, training loss: 83.70933532714844 = 0.3454413115978241 + 10.0 * 8.336389541625977
Epoch 1860, val loss: 0.4285181164741516
Epoch 1870, training loss: 83.70630645751953 = 0.34449440240859985 + 10.0 * 8.336180686950684
Epoch 1870, val loss: 0.427981972694397
Epoch 1880, training loss: 83.71380615234375 = 0.34354865550994873 + 10.0 * 8.33702564239502
Epoch 1880, val loss: 0.427436888217926
Epoch 1890, training loss: 83.7034912109375 = 0.34259116649627686 + 10.0 * 8.336090087890625
Epoch 1890, val loss: 0.4268968999385834
Epoch 1900, training loss: 83.6961898803711 = 0.3416392505168915 + 10.0 * 8.335454940795898
Epoch 1900, val loss: 0.42633622884750366
Epoch 1910, training loss: 83.69465637207031 = 0.34069961309432983 + 10.0 * 8.335395812988281
Epoch 1910, val loss: 0.425834983587265
Epoch 1920, training loss: 83.7037582397461 = 0.3397608995437622 + 10.0 * 8.336400032043457
Epoch 1920, val loss: 0.42526671290397644
Epoch 1930, training loss: 83.70796966552734 = 0.3388204574584961 + 10.0 * 8.336915016174316
Epoch 1930, val loss: 0.4247700273990631
Epoch 1940, training loss: 83.68620300292969 = 0.33789128065109253 + 10.0 * 8.334831237792969
Epoch 1940, val loss: 0.42430242896080017
Epoch 1950, training loss: 83.68299102783203 = 0.33696964383125305 + 10.0 * 8.334602355957031
Epoch 1950, val loss: 0.4238070547580719
Epoch 1960, training loss: 83.67891693115234 = 0.33606231212615967 + 10.0 * 8.334285736083984
Epoch 1960, val loss: 0.42333388328552246
Epoch 1970, training loss: 83.68185424804688 = 0.33515411615371704 + 10.0 * 8.334670066833496
Epoch 1970, val loss: 0.4228717088699341
Epoch 1980, training loss: 83.7147216796875 = 0.334248811006546 + 10.0 * 8.33804702758789
Epoch 1980, val loss: 0.42243051528930664
Epoch 1990, training loss: 83.67434692382812 = 0.33332958817481995 + 10.0 * 8.334101676940918
Epoch 1990, val loss: 0.4218684732913971
Epoch 2000, training loss: 83.66429901123047 = 0.33243316411972046 + 10.0 * 8.333186149597168
Epoch 2000, val loss: 0.4214405119419098
Epoch 2010, training loss: 83.66405487060547 = 0.33154720067977905 + 10.0 * 8.333250045776367
Epoch 2010, val loss: 0.4210115969181061
Epoch 2020, training loss: 83.65999603271484 = 0.330665647983551 + 10.0 * 8.33293342590332
Epoch 2020, val loss: 0.4205540716648102
Epoch 2030, training loss: 83.69636535644531 = 0.32978326082229614 + 10.0 * 8.336658477783203
Epoch 2030, val loss: 0.4201136827468872
Epoch 2040, training loss: 83.67398834228516 = 0.3288904130458832 + 10.0 * 8.33450984954834
Epoch 2040, val loss: 0.4196341335773468
Epoch 2050, training loss: 83.65936279296875 = 0.3280150294303894 + 10.0 * 8.333134651184082
Epoch 2050, val loss: 0.419236421585083
Epoch 2060, training loss: 83.64784240722656 = 0.3271479904651642 + 10.0 * 8.332069396972656
Epoch 2060, val loss: 0.4188149571418762
Epoch 2070, training loss: 83.6438980102539 = 0.32628923654556274 + 10.0 * 8.331761360168457
Epoch 2070, val loss: 0.418404221534729
Epoch 2080, training loss: 83.65414428710938 = 0.3254343271255493 + 10.0 * 8.332871437072754
Epoch 2080, val loss: 0.4180413484573364
Epoch 2090, training loss: 83.67517852783203 = 0.324562132358551 + 10.0 * 8.335062026977539
Epoch 2090, val loss: 0.41759851574897766
Epoch 2100, training loss: 83.6400146484375 = 0.3236880600452423 + 10.0 * 8.331632614135742
Epoch 2100, val loss: 0.4171431064605713
Epoch 2110, training loss: 83.6374740600586 = 0.3228411376476288 + 10.0 * 8.331463813781738
Epoch 2110, val loss: 0.41671061515808105
Epoch 2120, training loss: 83.63127136230469 = 0.32200887799263 + 10.0 * 8.330926895141602
Epoch 2120, val loss: 0.4163239598274231
Epoch 2130, training loss: 83.62664794921875 = 0.3211771249771118 + 10.0 * 8.330547332763672
Epoch 2130, val loss: 0.4159308671951294
Epoch 2140, training loss: 83.62358856201172 = 0.32034772634506226 + 10.0 * 8.330324172973633
Epoch 2140, val loss: 0.41554364562034607
Epoch 2150, training loss: 83.625244140625 = 0.3195168375968933 + 10.0 * 8.330572128295898
Epoch 2150, val loss: 0.4151526987552643
Epoch 2160, training loss: 83.65496826171875 = 0.3186807930469513 + 10.0 * 8.33362865447998
Epoch 2160, val loss: 0.4147412180900574
Epoch 2170, training loss: 83.63641357421875 = 0.3178454041481018 + 10.0 * 8.331856727600098
Epoch 2170, val loss: 0.41442734003067017
Epoch 2180, training loss: 83.61666870117188 = 0.31701868772506714 + 10.0 * 8.329965591430664
Epoch 2180, val loss: 0.41407662630081177
Epoch 2190, training loss: 83.61180114746094 = 0.3162010610103607 + 10.0 * 8.329560279846191
Epoch 2190, val loss: 0.41369903087615967
Epoch 2200, training loss: 83.60797119140625 = 0.3153911232948303 + 10.0 * 8.32925796508789
Epoch 2200, val loss: 0.41336727142333984
Epoch 2210, training loss: 83.61021423339844 = 0.3145809769630432 + 10.0 * 8.32956314086914
Epoch 2210, val loss: 0.4130152463912964
Epoch 2220, training loss: 83.63731384277344 = 0.3137682378292084 + 10.0 * 8.332354545593262
Epoch 2220, val loss: 0.41268348693847656
Epoch 2230, training loss: 83.60919189453125 = 0.3129587769508362 + 10.0 * 8.329623222351074
Epoch 2230, val loss: 0.41237759590148926
Epoch 2240, training loss: 83.60608673095703 = 0.312154084444046 + 10.0 * 8.32939338684082
Epoch 2240, val loss: 0.412054181098938
Epoch 2250, training loss: 83.65945434570312 = 0.31135398149490356 + 10.0 * 8.334810256958008
Epoch 2250, val loss: 0.41175758838653564
Epoch 2260, training loss: 83.60848236083984 = 0.31053829193115234 + 10.0 * 8.329793930053711
Epoch 2260, val loss: 0.4113367795944214
Epoch 2270, training loss: 83.59490203857422 = 0.3097432553768158 + 10.0 * 8.328516006469727
Epoch 2270, val loss: 0.4110483229160309
Epoch 2280, training loss: 83.58867645263672 = 0.3089595139026642 + 10.0 * 8.327971458435059
Epoch 2280, val loss: 0.41070088744163513
Epoch 2290, training loss: 83.58353424072266 = 0.3081757426261902 + 10.0 * 8.327535629272461
Epoch 2290, val loss: 0.41042935848236084
Epoch 2300, training loss: 83.58006286621094 = 0.3073943853378296 + 10.0 * 8.327266693115234
Epoch 2300, val loss: 0.410095751285553
Epoch 2310, training loss: 83.58213806152344 = 0.30661529302597046 + 10.0 * 8.32755184173584
Epoch 2310, val loss: 0.4097956418991089
Epoch 2320, training loss: 83.63117980957031 = 0.3058340847492218 + 10.0 * 8.332534790039062
Epoch 2320, val loss: 0.4095294177532196
Epoch 2330, training loss: 83.5910873413086 = 0.30503928661346436 + 10.0 * 8.328604698181152
Epoch 2330, val loss: 0.4091852009296417
Epoch 2340, training loss: 83.5740966796875 = 0.3042523264884949 + 10.0 * 8.326984405517578
Epoch 2340, val loss: 0.40886056423187256
Epoch 2350, training loss: 83.56977844238281 = 0.30348414182662964 + 10.0 * 8.326629638671875
Epoch 2350, val loss: 0.4086107611656189
Epoch 2360, training loss: 83.56707000732422 = 0.3027198314666748 + 10.0 * 8.326435089111328
Epoch 2360, val loss: 0.4083271026611328
Epoch 2370, training loss: 83.56439971923828 = 0.3019547462463379 + 10.0 * 8.326244354248047
Epoch 2370, val loss: 0.40806853771209717
Epoch 2380, training loss: 83.57098388671875 = 0.30119264125823975 + 10.0 * 8.32697868347168
Epoch 2380, val loss: 0.40782099962234497
Epoch 2390, training loss: 83.58817291259766 = 0.30042406916618347 + 10.0 * 8.328775405883789
Epoch 2390, val loss: 0.4075702130794525
Epoch 2400, training loss: 83.58049011230469 = 0.2996490001678467 + 10.0 * 8.328083992004395
Epoch 2400, val loss: 0.4072399139404297
Epoch 2410, training loss: 83.57374572753906 = 0.2988901436328888 + 10.0 * 8.327486038208008
Epoch 2410, val loss: 0.40704911947250366
Epoch 2420, training loss: 83.5548095703125 = 0.29813310503959656 + 10.0 * 8.325667381286621
Epoch 2420, val loss: 0.40670037269592285
Epoch 2430, training loss: 83.5503158569336 = 0.29738351702690125 + 10.0 * 8.325292587280273
Epoch 2430, val loss: 0.40644386410713196
Epoch 2440, training loss: 83.54966735839844 = 0.29664090275764465 + 10.0 * 8.325303077697754
Epoch 2440, val loss: 0.4061688482761383
Epoch 2450, training loss: 83.56922912597656 = 0.2958952486515045 + 10.0 * 8.327333450317383
Epoch 2450, val loss: 0.4058723747730255
Epoch 2460, training loss: 83.55233001708984 = 0.29514527320861816 + 10.0 * 8.325718879699707
Epoch 2460, val loss: 0.4056655764579773
Epoch 2470, training loss: 83.54754638671875 = 0.2944026589393616 + 10.0 * 8.32531452178955
Epoch 2470, val loss: 0.40544676780700684
Epoch 2480, training loss: 83.5412368774414 = 0.29366374015808105 + 10.0 * 8.32475757598877
Epoch 2480, val loss: 0.4051913022994995
Epoch 2490, training loss: 83.53777313232422 = 0.2929289638996124 + 10.0 * 8.324483871459961
Epoch 2490, val loss: 0.4049954116344452
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8513444951801116
0.8626385568354706
=== training gcn model ===
Epoch 0, training loss: 106.91045379638672 = 1.0875345468521118 + 10.0 * 10.582292556762695
Epoch 0, val loss: 1.0854427814483643
Epoch 10, training loss: 106.9036636352539 = 1.083893060684204 + 10.0 * 10.581976890563965
Epoch 10, val loss: 1.0819615125656128
Epoch 20, training loss: 106.88524627685547 = 1.0798883438110352 + 10.0 * 10.580535888671875
Epoch 20, val loss: 1.0781196355819702
Epoch 30, training loss: 106.81246948242188 = 1.0754010677337646 + 10.0 * 10.57370662689209
Epoch 30, val loss: 1.0738078355789185
Epoch 40, training loss: 106.50540924072266 = 1.0703210830688477 + 10.0 * 10.543508529663086
Epoch 40, val loss: 1.0688916444778442
Epoch 50, training loss: 105.41664123535156 = 1.0645349025726318 + 10.0 * 10.435210227966309
Epoch 50, val loss: 1.0633059740066528
Epoch 60, training loss: 102.2022933959961 = 1.0579726696014404 + 10.0 * 10.114431381225586
Epoch 60, val loss: 1.0569231510162354
Epoch 70, training loss: 97.32142639160156 = 1.050025463104248 + 10.0 * 9.627140045166016
Epoch 70, val loss: 1.0491855144500732
Epoch 80, training loss: 96.2294921875 = 1.0424854755401611 + 10.0 * 9.51870059967041
Epoch 80, val loss: 1.0420986413955688
Epoch 90, training loss: 95.08135986328125 = 1.036216378211975 + 10.0 * 9.40451431274414
Epoch 90, val loss: 1.0361268520355225
Epoch 100, training loss: 93.6721420288086 = 1.0311524868011475 + 10.0 * 9.26409912109375
Epoch 100, val loss: 1.031351089477539
Epoch 110, training loss: 92.9250717163086 = 1.0266574621200562 + 10.0 * 9.189841270446777
Epoch 110, val loss: 1.0270143747329712
Epoch 120, training loss: 92.66928100585938 = 1.0221024751663208 + 10.0 * 9.164717674255371
Epoch 120, val loss: 1.022568941116333
Epoch 130, training loss: 92.32875061035156 = 1.0176374912261963 + 10.0 * 9.131111145019531
Epoch 130, val loss: 1.0183265209197998
Epoch 140, training loss: 91.77876281738281 = 1.014032006263733 + 10.0 * 9.076473236083984
Epoch 140, val loss: 1.015008568763733
Epoch 150, training loss: 90.93328094482422 = 1.011926531791687 + 10.0 * 8.992136001586914
Epoch 150, val loss: 1.013200044631958
Epoch 160, training loss: 90.08089447021484 = 1.0111783742904663 + 10.0 * 8.90697193145752
Epoch 160, val loss: 1.0126025676727295
Epoch 170, training loss: 89.49352264404297 = 1.0100029706954956 + 10.0 * 8.848352432250977
Epoch 170, val loss: 1.0113496780395508
Epoch 180, training loss: 88.8454360961914 = 1.0077868700027466 + 10.0 * 8.783764839172363
Epoch 180, val loss: 1.0092214345932007
Epoch 190, training loss: 88.48495483398438 = 1.005011796951294 + 10.0 * 8.747994422912598
Epoch 190, val loss: 1.0063787698745728
Epoch 200, training loss: 88.23739624023438 = 1.0011017322540283 + 10.0 * 8.72362995147705
Epoch 200, val loss: 1.0025300979614258
Epoch 210, training loss: 88.05792999267578 = 0.9963229298591614 + 10.0 * 8.706160545349121
Epoch 210, val loss: 0.997821569442749
Epoch 220, training loss: 87.86796569824219 = 0.9912864565849304 + 10.0 * 8.687667846679688
Epoch 220, val loss: 0.992883563041687
Epoch 230, training loss: 87.6690444946289 = 0.986373782157898 + 10.0 * 8.668267250061035
Epoch 230, val loss: 0.9881201982498169
Epoch 240, training loss: 87.49712371826172 = 0.9814287424087524 + 10.0 * 8.651569366455078
Epoch 240, val loss: 0.9833016991615295
Epoch 250, training loss: 87.35687255859375 = 0.976000189781189 + 10.0 * 8.638087272644043
Epoch 250, val loss: 0.9780007004737854
Epoch 260, training loss: 87.2225341796875 = 0.9699981808662415 + 10.0 * 8.625253677368164
Epoch 260, val loss: 0.9721068143844604
Epoch 270, training loss: 87.08097076416016 = 0.963616669178009 + 10.0 * 8.611735343933105
Epoch 270, val loss: 0.9659221172332764
Epoch 280, training loss: 86.9298095703125 = 0.9570807814598083 + 10.0 * 8.597272872924805
Epoch 280, val loss: 0.9595791697502136
Epoch 290, training loss: 86.78558349609375 = 0.950404703617096 + 10.0 * 8.583518028259277
Epoch 290, val loss: 0.9530903100967407
Epoch 300, training loss: 86.67137908935547 = 0.9433991312980652 + 10.0 * 8.572797775268555
Epoch 300, val loss: 0.9463029503822327
Epoch 310, training loss: 86.55155181884766 = 0.9360392689704895 + 10.0 * 8.561551094055176
Epoch 310, val loss: 0.9390654563903809
Epoch 320, training loss: 86.43344116210938 = 0.9282940030097961 + 10.0 * 8.550515174865723
Epoch 320, val loss: 0.9315717220306396
Epoch 330, training loss: 86.34404754638672 = 0.9203033447265625 + 10.0 * 8.542374610900879
Epoch 330, val loss: 0.9238417744636536
Epoch 340, training loss: 86.245361328125 = 0.9120514392852783 + 10.0 * 8.533330917358398
Epoch 340, val loss: 0.9157460331916809
Epoch 350, training loss: 86.158203125 = 0.9035282731056213 + 10.0 * 8.525467872619629
Epoch 350, val loss: 0.9074572920799255
Epoch 360, training loss: 86.08012390136719 = 0.8947245478630066 + 10.0 * 8.518540382385254
Epoch 360, val loss: 0.8988844752311707
Epoch 370, training loss: 86.01069641113281 = 0.8855603337287903 + 10.0 * 8.512514114379883
Epoch 370, val loss: 0.889944851398468
Epoch 380, training loss: 85.98226165771484 = 0.8761010766029358 + 10.0 * 8.510616302490234
Epoch 380, val loss: 0.8808011412620544
Epoch 390, training loss: 85.90055847167969 = 0.8663689494132996 + 10.0 * 8.503418922424316
Epoch 390, val loss: 0.8712328672409058
Epoch 400, training loss: 85.83230590820312 = 0.8564906716346741 + 10.0 * 8.497581481933594
Epoch 400, val loss: 0.8616791367530823
Epoch 410, training loss: 85.77391815185547 = 0.8465008735656738 + 10.0 * 8.492741584777832
Epoch 410, val loss: 0.85200434923172
Epoch 420, training loss: 85.71952056884766 = 0.836398184299469 + 10.0 * 8.488312721252441
Epoch 420, val loss: 0.8421875834465027
Epoch 430, training loss: 85.69415283203125 = 0.8262282013893127 + 10.0 * 8.48679256439209
Epoch 430, val loss: 0.8322866559028625
Epoch 440, training loss: 85.6374740600586 = 0.8158842325210571 + 10.0 * 8.482158660888672
Epoch 440, val loss: 0.822394073009491
Epoch 450, training loss: 85.58904266357422 = 0.8057069778442383 + 10.0 * 8.478333473205566
Epoch 450, val loss: 0.8125765323638916
Epoch 460, training loss: 85.54153442382812 = 0.7955917119979858 + 10.0 * 8.474594116210938
Epoch 460, val loss: 0.8028262853622437
Epoch 470, training loss: 85.49626922607422 = 0.7855636477470398 + 10.0 * 8.471071243286133
Epoch 470, val loss: 0.7931563854217529
Epoch 480, training loss: 85.4768295288086 = 0.7755663394927979 + 10.0 * 8.470126152038574
Epoch 480, val loss: 0.7835663557052612
Epoch 490, training loss: 85.41647338867188 = 0.7657039165496826 + 10.0 * 8.46507740020752
Epoch 490, val loss: 0.774093747138977
Epoch 500, training loss: 85.36508178710938 = 0.7560749650001526 + 10.0 * 8.460901260375977
Epoch 500, val loss: 0.7648884654045105
Epoch 510, training loss: 85.32511138916016 = 0.7465899586677551 + 10.0 * 8.457852363586426
Epoch 510, val loss: 0.7558189034461975
Epoch 520, training loss: 85.3087387084961 = 0.7370679974555969 + 10.0 * 8.45716667175293
Epoch 520, val loss: 0.7467407584190369
Epoch 530, training loss: 85.2313003540039 = 0.7277418971061707 + 10.0 * 8.450355529785156
Epoch 530, val loss: 0.737912118434906
Epoch 540, training loss: 85.1865463256836 = 0.7187086939811707 + 10.0 * 8.446783065795898
Epoch 540, val loss: 0.7293537855148315
Epoch 550, training loss: 85.14570617675781 = 0.7097364068031311 + 10.0 * 8.443596839904785
Epoch 550, val loss: 0.7208877205848694
Epoch 560, training loss: 85.14607238769531 = 0.700847327709198 + 10.0 * 8.444522857666016
Epoch 560, val loss: 0.7125311493873596
Epoch 570, training loss: 85.0685043334961 = 0.692119300365448 + 10.0 * 8.437639236450195
Epoch 570, val loss: 0.7042781710624695
Epoch 580, training loss: 85.01644134521484 = 0.6835981011390686 + 10.0 * 8.433283805847168
Epoch 580, val loss: 0.6963038444519043
Epoch 590, training loss: 84.97509765625 = 0.675210177898407 + 10.0 * 8.429988861083984
Epoch 590, val loss: 0.6884490847587585
Epoch 600, training loss: 84.955078125 = 0.6669276356697083 + 10.0 * 8.428814888000488
Epoch 600, val loss: 0.6806783676147461
Epoch 610, training loss: 84.94094848632812 = 0.6587169170379639 + 10.0 * 8.428223609924316
Epoch 610, val loss: 0.6730509400367737
Epoch 620, training loss: 84.87149810791016 = 0.6506161093711853 + 10.0 * 8.422087669372559
Epoch 620, val loss: 0.6655606627464294
Epoch 630, training loss: 84.82322692871094 = 0.6427438855171204 + 10.0 * 8.418047904968262
Epoch 630, val loss: 0.658277690410614
Epoch 640, training loss: 84.78614044189453 = 0.6349555253982544 + 10.0 * 8.415118217468262
Epoch 640, val loss: 0.6510637998580933
Epoch 650, training loss: 84.79961395263672 = 0.6272385120391846 + 10.0 * 8.417238235473633
Epoch 650, val loss: 0.6439059972763062
Epoch 660, training loss: 84.73627471923828 = 0.6194137930870056 + 10.0 * 8.411685943603516
Epoch 660, val loss: 0.6367566585540771
Epoch 670, training loss: 84.69512176513672 = 0.6119133830070496 + 10.0 * 8.408320426940918
Epoch 670, val loss: 0.6298927664756775
Epoch 680, training loss: 84.670166015625 = 0.604585587978363 + 10.0 * 8.4065580368042
Epoch 680, val loss: 0.6232094168663025
Epoch 690, training loss: 84.64103698730469 = 0.5973141193389893 + 10.0 * 8.404372215270996
Epoch 690, val loss: 0.6166023015975952
Epoch 700, training loss: 84.61443328857422 = 0.5901489853858948 + 10.0 * 8.40242862701416
Epoch 700, val loss: 0.6100987792015076
Epoch 710, training loss: 84.5931396484375 = 0.5830806493759155 + 10.0 * 8.401005744934082
Epoch 710, val loss: 0.603750467300415
Epoch 720, training loss: 84.57471466064453 = 0.5760592818260193 + 10.0 * 8.399866104125977
Epoch 720, val loss: 0.5973789691925049
Epoch 730, training loss: 84.54532623291016 = 0.569200336933136 + 10.0 * 8.397612571716309
Epoch 730, val loss: 0.5912306904792786
Epoch 740, training loss: 84.52410888671875 = 0.5625565052032471 + 10.0 * 8.39615535736084
Epoch 740, val loss: 0.585322380065918
Epoch 750, training loss: 84.5027084350586 = 0.5560580492019653 + 10.0 * 8.394664764404297
Epoch 750, val loss: 0.5795755386352539
Epoch 760, training loss: 84.4825439453125 = 0.5496974587440491 + 10.0 * 8.393284797668457
Epoch 760, val loss: 0.5739549398422241
Epoch 770, training loss: 84.57357788085938 = 0.5434912443161011 + 10.0 * 8.403009414672852
Epoch 770, val loss: 0.5684401988983154
Epoch 780, training loss: 84.45929718017578 = 0.5373231172561646 + 10.0 * 8.392197608947754
Epoch 780, val loss: 0.5630879998207092
Epoch 790, training loss: 84.4410400390625 = 0.5315448045730591 + 10.0 * 8.390949249267578
Epoch 790, val loss: 0.5580775141716003
Epoch 800, training loss: 84.4126205444336 = 0.5259877443313599 + 10.0 * 8.388663291931152
Epoch 800, val loss: 0.5533076524734497
Epoch 810, training loss: 84.3956069946289 = 0.5205532312393188 + 10.0 * 8.387505531311035
Epoch 810, val loss: 0.5486313700675964
Epoch 820, training loss: 84.37991333007812 = 0.5152731537818909 + 10.0 * 8.38646411895752
Epoch 820, val loss: 0.5441334843635559
Epoch 830, training loss: 84.36463165283203 = 0.5101418495178223 + 10.0 * 8.385449409484863
Epoch 830, val loss: 0.5397856831550598
Epoch 840, training loss: 84.34976196289062 = 0.5051562786102295 + 10.0 * 8.38446044921875
Epoch 840, val loss: 0.5355849862098694
Epoch 850, training loss: 84.33544158935547 = 0.5003162026405334 + 10.0 * 8.383512496948242
Epoch 850, val loss: 0.5315397381782532
Epoch 860, training loss: 84.32303619384766 = 0.495612770318985 + 10.0 * 8.382741928100586
Epoch 860, val loss: 0.527643084526062
Epoch 870, training loss: 84.36203002929688 = 0.49101701378822327 + 10.0 * 8.387101173400879
Epoch 870, val loss: 0.5238456726074219
Epoch 880, training loss: 84.31317138671875 = 0.48656976222991943 + 10.0 * 8.382659912109375
Epoch 880, val loss: 0.5201989412307739
Epoch 890, training loss: 84.28923034667969 = 0.4823770225048065 + 10.0 * 8.380685806274414
Epoch 890, val loss: 0.5168213844299316
Epoch 900, training loss: 84.2715835571289 = 0.4783148765563965 + 10.0 * 8.379326820373535
Epoch 900, val loss: 0.5135483741760254
Epoch 910, training loss: 84.25708770751953 = 0.4743877947330475 + 10.0 * 8.378270149230957
Epoch 910, val loss: 0.5103984475135803
Epoch 920, training loss: 84.28785705566406 = 0.47060224413871765 + 10.0 * 8.381725311279297
Epoch 920, val loss: 0.5073774456977844
Epoch 930, training loss: 84.24845886230469 = 0.4668791890144348 + 10.0 * 8.378157615661621
Epoch 930, val loss: 0.5045363306999207
Epoch 940, training loss: 84.2276840209961 = 0.4633760154247284 + 10.0 * 8.37643051147461
Epoch 940, val loss: 0.5018060803413391
Epoch 950, training loss: 84.21027374267578 = 0.4600156545639038 + 10.0 * 8.375025749206543
Epoch 950, val loss: 0.49921369552612305
Epoch 960, training loss: 84.19673156738281 = 0.4567367732524872 + 10.0 * 8.37399959564209
Epoch 960, val loss: 0.4967043995857239
Epoch 970, training loss: 84.1886978149414 = 0.4535760283470154 + 10.0 * 8.373512268066406
Epoch 970, val loss: 0.4943484663963318
Epoch 980, training loss: 84.22246551513672 = 0.45048588514328003 + 10.0 * 8.377198219299316
Epoch 980, val loss: 0.4920189678668976
Epoch 990, training loss: 84.16597747802734 = 0.4475002884864807 + 10.0 * 8.371847152709961
Epoch 990, val loss: 0.48977914452552795
Epoch 1000, training loss: 84.14994049072266 = 0.44466593861579895 + 10.0 * 8.370527267456055
Epoch 1000, val loss: 0.48765596747398376
Epoch 1010, training loss: 84.13862609863281 = 0.44189390540122986 + 10.0 * 8.369672775268555
Epoch 1010, val loss: 0.48564213514328003
Epoch 1020, training loss: 84.12642669677734 = 0.43920809030532837 + 10.0 * 8.368721961975098
Epoch 1020, val loss: 0.483654260635376
Epoch 1030, training loss: 84.11976623535156 = 0.4365963637828827 + 10.0 * 8.368316650390625
Epoch 1030, val loss: 0.48178860545158386
Epoch 1040, training loss: 84.12417602539062 = 0.43400660157203674 + 10.0 * 8.369016647338867
Epoch 1040, val loss: 0.4798811674118042
Epoch 1050, training loss: 84.11084747314453 = 0.43150365352630615 + 10.0 * 8.367934226989746
Epoch 1050, val loss: 0.4781765639781952
Epoch 1060, training loss: 84.08672332763672 = 0.42912155389785767 + 10.0 * 8.36575984954834
Epoch 1060, val loss: 0.4764690697193146
Epoch 1070, training loss: 84.0743179321289 = 0.42679503560066223 + 10.0 * 8.364751815795898
Epoch 1070, val loss: 0.474749892950058
Epoch 1080, training loss: 84.06564331054688 = 0.42451125383377075 + 10.0 * 8.364112854003906
Epoch 1080, val loss: 0.4731059670448303
Epoch 1090, training loss: 84.19280242919922 = 0.42228493094444275 + 10.0 * 8.37705135345459
Epoch 1090, val loss: 0.471391886472702
Epoch 1100, training loss: 84.05015563964844 = 0.4199979603290558 + 10.0 * 8.363016128540039
Epoch 1100, val loss: 0.4699257016181946
Epoch 1110, training loss: 84.04248809814453 = 0.41790875792503357 + 10.0 * 8.362458229064941
Epoch 1110, val loss: 0.4684999883174896
Epoch 1120, training loss: 84.03117370605469 = 0.41587233543395996 + 10.0 * 8.361530303955078
Epoch 1120, val loss: 0.46707144379615784
Epoch 1130, training loss: 84.02119445800781 = 0.413864403963089 + 10.0 * 8.360733032226562
Epoch 1130, val loss: 0.46571752429008484
Epoch 1140, training loss: 84.01261901855469 = 0.41189026832580566 + 10.0 * 8.36007308959961
Epoch 1140, val loss: 0.46439218521118164
Epoch 1150, training loss: 84.00376892089844 = 0.4099408686161041 + 10.0 * 8.359382629394531
Epoch 1150, val loss: 0.46303805708885193
Epoch 1160, training loss: 83.99675750732422 = 0.40802517533302307 + 10.0 * 8.35887336730957
Epoch 1160, val loss: 0.46172916889190674
Epoch 1170, training loss: 84.04019927978516 = 0.40612107515335083 + 10.0 * 8.363408088684082
Epoch 1170, val loss: 0.4604674279689789
Epoch 1180, training loss: 83.98808288574219 = 0.4042462408542633 + 10.0 * 8.358384132385254
Epoch 1180, val loss: 0.45916953682899475
Epoch 1190, training loss: 83.97672271728516 = 0.40244314074516296 + 10.0 * 8.357427597045898
Epoch 1190, val loss: 0.4579140245914459
Epoch 1200, training loss: 83.96710205078125 = 0.40066611766815186 + 10.0 * 8.356643676757812
Epoch 1200, val loss: 0.45667576789855957
Epoch 1210, training loss: 83.95834350585938 = 0.3989180624485016 + 10.0 * 8.355942726135254
Epoch 1210, val loss: 0.4554949402809143
Epoch 1220, training loss: 83.9513168334961 = 0.3971936106681824 + 10.0 * 8.355412483215332
Epoch 1220, val loss: 0.45431190729141235
Epoch 1230, training loss: 84.0235824584961 = 0.39549520611763 + 10.0 * 8.362809181213379
Epoch 1230, val loss: 0.45307815074920654
Epoch 1240, training loss: 83.95854949951172 = 0.39374786615371704 + 10.0 * 8.356480598449707
Epoch 1240, val loss: 0.45197561383247375
Epoch 1250, training loss: 83.94103240966797 = 0.3921118974685669 + 10.0 * 8.354891777038574
Epoch 1250, val loss: 0.45090344548225403
Epoch 1260, training loss: 83.92740631103516 = 0.3905061185359955 + 10.0 * 8.353690147399902
Epoch 1260, val loss: 0.4498066306114197
Epoch 1270, training loss: 83.9184799194336 = 0.38892093300819397 + 10.0 * 8.35295581817627
Epoch 1270, val loss: 0.44867226481437683
Epoch 1280, training loss: 83.91120910644531 = 0.3873552083969116 + 10.0 * 8.352385520935059
Epoch 1280, val loss: 0.44762134552001953
Epoch 1290, training loss: 83.90489959716797 = 0.38580048084259033 + 10.0 * 8.351909637451172
Epoch 1290, val loss: 0.4465588331222534
Epoch 1300, training loss: 83.90967559814453 = 0.3842507302761078 + 10.0 * 8.35254192352295
Epoch 1300, val loss: 0.44552579522132874
Epoch 1310, training loss: 83.91281127929688 = 0.38267481327056885 + 10.0 * 8.35301399230957
Epoch 1310, val loss: 0.4444085955619812
Epoch 1320, training loss: 83.89814758300781 = 0.3811551630496979 + 10.0 * 8.351698875427246
Epoch 1320, val loss: 0.4434681832790375
Epoch 1330, training loss: 83.88838958740234 = 0.37970906496047974 + 10.0 * 8.350868225097656
Epoch 1330, val loss: 0.4424007833003998
Epoch 1340, training loss: 83.87875366210938 = 0.37826627492904663 + 10.0 * 8.350049018859863
Epoch 1340, val loss: 0.4413810670375824
Epoch 1350, training loss: 83.871337890625 = 0.3768390417098999 + 10.0 * 8.34945011138916
Epoch 1350, val loss: 0.44041839241981506
Epoch 1360, training loss: 83.86518859863281 = 0.37542152404785156 + 10.0 * 8.348977088928223
Epoch 1360, val loss: 0.4394398331642151
Epoch 1370, training loss: 83.85997009277344 = 0.37401384115219116 + 10.0 * 8.34859561920166
Epoch 1370, val loss: 0.43846502900123596
Epoch 1380, training loss: 83.8890609741211 = 0.3726137578487396 + 10.0 * 8.351644515991211
Epoch 1380, val loss: 0.4374712407588959
Epoch 1390, training loss: 83.86883544921875 = 0.3711768090724945 + 10.0 * 8.34976577758789
Epoch 1390, val loss: 0.43661031126976013
Epoch 1400, training loss: 83.84834289550781 = 0.36982351541519165 + 10.0 * 8.347851753234863
Epoch 1400, val loss: 0.43564918637275696
Epoch 1410, training loss: 83.84178161621094 = 0.3684823513031006 + 10.0 * 8.347330093383789
Epoch 1410, val loss: 0.43466657400131226
Epoch 1420, training loss: 83.833984375 = 0.36714011430740356 + 10.0 * 8.346684455871582
Epoch 1420, val loss: 0.4337933361530304
Epoch 1430, training loss: 83.8282470703125 = 0.36581453680992126 + 10.0 * 8.346242904663086
Epoch 1430, val loss: 0.4329327940940857
Epoch 1440, training loss: 83.82363891601562 = 0.3644995093345642 + 10.0 * 8.345913887023926
Epoch 1440, val loss: 0.4320315420627594
Epoch 1450, training loss: 83.83999633789062 = 0.36317700147628784 + 10.0 * 8.347681999206543
Epoch 1450, val loss: 0.4312262535095215
Epoch 1460, training loss: 83.81550598144531 = 0.3618467450141907 + 10.0 * 8.345365524291992
Epoch 1460, val loss: 0.43014493584632874
Epoch 1470, training loss: 83.81758880615234 = 0.3605572283267975 + 10.0 * 8.345703125
Epoch 1470, val loss: 0.4294262230396271
Epoch 1480, training loss: 83.80854034423828 = 0.3593086898326874 + 10.0 * 8.34492301940918
Epoch 1480, val loss: 0.4284682869911194
Epoch 1490, training loss: 83.7990951538086 = 0.3580530881881714 + 10.0 * 8.344103813171387
Epoch 1490, val loss: 0.4276747405529022
Epoch 1500, training loss: 83.79331970214844 = 0.3568129241466522 + 10.0 * 8.343650817871094
Epoch 1500, val loss: 0.4268306791782379
Epoch 1510, training loss: 83.78943634033203 = 0.3555711507797241 + 10.0 * 8.34338665008545
Epoch 1510, val loss: 0.4260164797306061
Epoch 1520, training loss: 83.88951873779297 = 0.35433000326156616 + 10.0 * 8.35351848602295
Epoch 1520, val loss: 0.4251834750175476
Epoch 1530, training loss: 83.78768157958984 = 0.3530617952346802 + 10.0 * 8.343461990356445
Epoch 1530, val loss: 0.4243807792663574
Epoch 1540, training loss: 83.78453826904297 = 0.35185620188713074 + 10.0 * 8.343268394470215
Epoch 1540, val loss: 0.42354264855384827
Epoch 1550, training loss: 83.7708511352539 = 0.3506554961204529 + 10.0 * 8.342020034790039
Epoch 1550, val loss: 0.42275452613830566
Epoch 1560, training loss: 83.76514434814453 = 0.3494551181793213 + 10.0 * 8.341568946838379
Epoch 1560, val loss: 0.4219481647014618
Epoch 1570, training loss: 83.7810287475586 = 0.34825369715690613 + 10.0 * 8.343277931213379
Epoch 1570, val loss: 0.4211440980434418
Epoch 1580, training loss: 83.75690460205078 = 0.34703922271728516 + 10.0 * 8.340986251831055
Epoch 1580, val loss: 0.42039772868156433
Epoch 1590, training loss: 83.7525405883789 = 0.34585657715797424 + 10.0 * 8.340668678283691
Epoch 1590, val loss: 0.41964560747146606
Epoch 1600, training loss: 83.74586486816406 = 0.34468263387680054 + 10.0 * 8.340118408203125
Epoch 1600, val loss: 0.41889455914497375
Epoch 1610, training loss: 83.73987579345703 = 0.3435079753398895 + 10.0 * 8.33963680267334
Epoch 1610, val loss: 0.4181569516658783
Epoch 1620, training loss: 83.73619079589844 = 0.3423440754413605 + 10.0 * 8.339384078979492
Epoch 1620, val loss: 0.41743144392967224
Epoch 1630, training loss: 83.81525421142578 = 0.34116655588150024 + 10.0 * 8.34740924835205
Epoch 1630, val loss: 0.4168155789375305
Epoch 1640, training loss: 83.7597885131836 = 0.34000590443611145 + 10.0 * 8.341978073120117
Epoch 1640, val loss: 0.41589152812957764
Epoch 1650, training loss: 83.72547149658203 = 0.33885830640792847 + 10.0 * 8.338661193847656
Epoch 1650, val loss: 0.4152354896068573
Epoch 1660, training loss: 83.71975708007812 = 0.3377312123775482 + 10.0 * 8.338202476501465
Epoch 1660, val loss: 0.41453981399536133
Epoch 1670, training loss: 83.71183776855469 = 0.33660557866096497 + 10.0 * 8.337522506713867
Epoch 1670, val loss: 0.4138652980327606
Epoch 1680, training loss: 83.70709991455078 = 0.3354814648628235 + 10.0 * 8.337162017822266
Epoch 1680, val loss: 0.413161963224411
Epoch 1690, training loss: 83.70198822021484 = 0.33435553312301636 + 10.0 * 8.336763381958008
Epoch 1690, val loss: 0.4124909043312073
Epoch 1700, training loss: 83.70694732666016 = 0.3332327604293823 + 10.0 * 8.337371826171875
Epoch 1700, val loss: 0.4117777645587921
Epoch 1710, training loss: 83.71249389648438 = 0.3320806324481964 + 10.0 * 8.338041305541992
Epoch 1710, val loss: 0.41126900911331177
Epoch 1720, training loss: 83.70626831054688 = 0.33098191022872925 + 10.0 * 8.337529182434082
Epoch 1720, val loss: 0.410413533449173
Epoch 1730, training loss: 83.6837387084961 = 0.329875111579895 + 10.0 * 8.335386276245117
Epoch 1730, val loss: 0.4098125994205475
Epoch 1740, training loss: 83.68209075927734 = 0.3287833034992218 + 10.0 * 8.335330963134766
Epoch 1740, val loss: 0.4092789888381958
Epoch 1750, training loss: 83.67571258544922 = 0.32769954204559326 + 10.0 * 8.33480167388916
Epoch 1750, val loss: 0.40867024660110474
Epoch 1760, training loss: 83.68356323242188 = 0.3266139626502991 + 10.0 * 8.335695266723633
Epoch 1760, val loss: 0.40804222226142883
Epoch 1770, training loss: 83.67809295654297 = 0.32551389932632446 + 10.0 * 8.335257530212402
Epoch 1770, val loss: 0.40754008293151855
Epoch 1780, training loss: 83.71633911132812 = 0.3244238495826721 + 10.0 * 8.339191436767578
Epoch 1780, val loss: 0.40694764256477356
Epoch 1790, training loss: 83.67809295654297 = 0.3233534097671509 + 10.0 * 8.335474014282227
Epoch 1790, val loss: 0.40629300475120544
Epoch 1800, training loss: 83.65532684326172 = 0.3222878575325012 + 10.0 * 8.333303451538086
Epoch 1800, val loss: 0.40579190850257874
Epoch 1810, training loss: 83.65039825439453 = 0.3212413191795349 + 10.0 * 8.332915306091309
Epoch 1810, val loss: 0.40523847937583923
Epoch 1820, training loss: 83.64535522460938 = 0.320196270942688 + 10.0 * 8.332515716552734
Epoch 1820, val loss: 0.4047202467918396
Epoch 1830, training loss: 83.64256286621094 = 0.3191540241241455 + 10.0 * 8.332341194152832
Epoch 1830, val loss: 0.4041975438594818
Epoch 1840, training loss: 83.66747283935547 = 0.3181113302707672 + 10.0 * 8.334936141967773
Epoch 1840, val loss: 0.40361878275871277
Epoch 1850, training loss: 83.63800811767578 = 0.31703707575798035 + 10.0 * 8.332097053527832
Epoch 1850, val loss: 0.4032594859600067
Epoch 1860, training loss: 83.62933349609375 = 0.31599804759025574 + 10.0 * 8.331334114074707
Epoch 1860, val loss: 0.40268242359161377
Epoch 1870, training loss: 83.62589263916016 = 0.31496602296829224 + 10.0 * 8.331092834472656
Epoch 1870, val loss: 0.4022708535194397
Epoch 1880, training loss: 83.62451171875 = 0.3139343857765198 + 10.0 * 8.33105754852295
Epoch 1880, val loss: 0.4018116295337677
Epoch 1890, training loss: 83.64551544189453 = 0.3129006326198578 + 10.0 * 8.333261489868164
Epoch 1890, val loss: 0.40133169293403625
Epoch 1900, training loss: 83.6248779296875 = 0.31185588240623474 + 10.0 * 8.33130168914795
Epoch 1900, val loss: 0.4009183645248413
Epoch 1910, training loss: 83.61634063720703 = 0.31082895398139954 + 10.0 * 8.330551147460938
Epoch 1910, val loss: 0.40042901039123535
Epoch 1920, training loss: 83.60631561279297 = 0.30981093645095825 + 10.0 * 8.329649925231934
Epoch 1920, val loss: 0.3999829888343811
Epoch 1930, training loss: 83.60394287109375 = 0.30879345536231995 + 10.0 * 8.32951545715332
Epoch 1930, val loss: 0.39959126710891724
Epoch 1940, training loss: 83.61669158935547 = 0.3077794909477234 + 10.0 * 8.330891609191895
Epoch 1940, val loss: 0.3991597294807434
Epoch 1950, training loss: 83.63097381591797 = 0.3067642152309418 + 10.0 * 8.33242130279541
Epoch 1950, val loss: 0.3986436426639557
Epoch 1960, training loss: 83.59224700927734 = 0.30572500824928284 + 10.0 * 8.328652381896973
Epoch 1960, val loss: 0.39839091897010803
Epoch 1970, training loss: 83.589111328125 = 0.30471640825271606 + 10.0 * 8.328439712524414
Epoch 1970, val loss: 0.39795568585395813
Epoch 1980, training loss: 83.58566284179688 = 0.30370861291885376 + 10.0 * 8.328195571899414
Epoch 1980, val loss: 0.39756473898887634
Epoch 1990, training loss: 83.58216094970703 = 0.30269357562065125 + 10.0 * 8.327946662902832
Epoch 1990, val loss: 0.3972548544406891
Epoch 2000, training loss: 83.5848617553711 = 0.30168619751930237 + 10.0 * 8.328317642211914
Epoch 2000, val loss: 0.39689046144485474
Epoch 2010, training loss: 83.6073989868164 = 0.30067741870880127 + 10.0 * 8.330672264099121
Epoch 2010, val loss: 0.3965071141719818
Epoch 2020, training loss: 83.58906555175781 = 0.2996634542942047 + 10.0 * 8.328940391540527
Epoch 2020, val loss: 0.39616328477859497
Epoch 2030, training loss: 83.57157135009766 = 0.29865217208862305 + 10.0 * 8.327291488647461
Epoch 2030, val loss: 0.3959469497203827
Epoch 2040, training loss: 83.56608581542969 = 0.29766225814819336 + 10.0 * 8.326842308044434
Epoch 2040, val loss: 0.39561012387275696
Epoch 2050, training loss: 83.5649185180664 = 0.296669602394104 + 10.0 * 8.326825141906738
Epoch 2050, val loss: 0.39532557129859924
Epoch 2060, training loss: 83.58489227294922 = 0.2956736981868744 + 10.0 * 8.3289213180542
Epoch 2060, val loss: 0.39508137106895447
Epoch 2070, training loss: 83.57282257080078 = 0.29466569423675537 + 10.0 * 8.327816009521484
Epoch 2070, val loss: 0.3949873745441437
Epoch 2080, training loss: 83.5682601928711 = 0.2936778664588928 + 10.0 * 8.327458381652832
Epoch 2080, val loss: 0.3945876955986023
Epoch 2090, training loss: 83.55062866210938 = 0.2926959991455078 + 10.0 * 8.325793266296387
Epoch 2090, val loss: 0.3943839967250824
Epoch 2100, training loss: 83.54711151123047 = 0.2917269468307495 + 10.0 * 8.325538635253906
Epoch 2100, val loss: 0.3940655291080475
Epoch 2110, training loss: 83.55252075195312 = 0.29075491428375244 + 10.0 * 8.326176643371582
Epoch 2110, val loss: 0.3938441872596741
Epoch 2120, training loss: 83.58118438720703 = 0.28978273272514343 + 10.0 * 8.329140663146973
Epoch 2120, val loss: 0.3936181664466858
Epoch 2130, training loss: 83.5572280883789 = 0.2887945771217346 + 10.0 * 8.32684326171875
Epoch 2130, val loss: 0.3935512900352478
Epoch 2140, training loss: 83.53955841064453 = 0.28784048557281494 + 10.0 * 8.32517147064209
Epoch 2140, val loss: 0.39323869347572327
Epoch 2150, training loss: 83.5310287475586 = 0.28687596321105957 + 10.0 * 8.32441520690918
Epoch 2150, val loss: 0.39313429594039917
Epoch 2160, training loss: 83.52899169921875 = 0.2859177589416504 + 10.0 * 8.324307441711426
Epoch 2160, val loss: 0.3929283320903778
Epoch 2170, training loss: 83.5425796508789 = 0.2849540412425995 + 10.0 * 8.325762748718262
Epoch 2170, val loss: 0.392732709646225
Epoch 2180, training loss: 83.54497528076172 = 0.28398653864860535 + 10.0 * 8.326098442077637
Epoch 2180, val loss: 0.3925420939922333
Epoch 2190, training loss: 83.52930450439453 = 0.2830159366130829 + 10.0 * 8.324628829956055
Epoch 2190, val loss: 0.39242520928382874
Epoch 2200, training loss: 83.52967834472656 = 0.282066285610199 + 10.0 * 8.324761390686035
Epoch 2200, val loss: 0.3922007083892822
Epoch 2210, training loss: 83.51535034179688 = 0.28111621737480164 + 10.0 * 8.323423385620117
Epoch 2210, val loss: 0.3921261131763458
Epoch 2220, training loss: 83.50910186767578 = 0.28017404675483704 + 10.0 * 8.322893142700195
Epoch 2220, val loss: 0.3920287787914276
Epoch 2230, training loss: 83.50654602050781 = 0.2792356312274933 + 10.0 * 8.322731018066406
Epoch 2230, val loss: 0.391924649477005
Epoch 2240, training loss: 83.5271987915039 = 0.27829235792160034 + 10.0 * 8.324891090393066
Epoch 2240, val loss: 0.39188140630722046
Epoch 2250, training loss: 83.51025390625 = 0.2773372232913971 + 10.0 * 8.323291778564453
Epoch 2250, val loss: 0.39184895157814026
Epoch 2260, training loss: 83.52529907226562 = 0.27640241384506226 + 10.0 * 8.324889183044434
Epoch 2260, val loss: 0.3917276859283447
Epoch 2270, training loss: 83.4980697631836 = 0.275478720664978 + 10.0 * 8.322258949279785
Epoch 2270, val loss: 0.3915998637676239
Epoch 2280, training loss: 83.49646759033203 = 0.27456578612327576 + 10.0 * 8.322190284729004
Epoch 2280, val loss: 0.3915424048900604
Epoch 2290, training loss: 83.49622344970703 = 0.27364909648895264 + 10.0 * 8.322257041931152
Epoch 2290, val loss: 0.39148855209350586
Epoch 2300, training loss: 83.50431823730469 = 0.27273017168045044 + 10.0 * 8.323159217834473
Epoch 2300, val loss: 0.3914225697517395
Epoch 2310, training loss: 83.49337768554688 = 0.27180176973342896 + 10.0 * 8.32215690612793
Epoch 2310, val loss: 0.3914176821708679
Epoch 2320, training loss: 83.48241424560547 = 0.2708790898323059 + 10.0 * 8.32115364074707
Epoch 2320, val loss: 0.3914587199687958
Epoch 2330, training loss: 83.48851013183594 = 0.2699623703956604 + 10.0 * 8.321854591369629
Epoch 2330, val loss: 0.39147597551345825
Epoch 2340, training loss: 83.49664306640625 = 0.26904505491256714 + 10.0 * 8.322759628295898
Epoch 2340, val loss: 0.3915051221847534
Epoch 2350, training loss: 83.50196075439453 = 0.26813584566116333 + 10.0 * 8.323382377624512
Epoch 2350, val loss: 0.3914119005203247
Epoch 2360, training loss: 83.47367095947266 = 0.2672197222709656 + 10.0 * 8.320645332336426
Epoch 2360, val loss: 0.3914322257041931
Epoch 2370, training loss: 83.47193145751953 = 0.2663041651248932 + 10.0 * 8.320562362670898
Epoch 2370, val loss: 0.3915713131427765
Epoch 2380, training loss: 83.468017578125 = 0.26539894938468933 + 10.0 * 8.32026195526123
Epoch 2380, val loss: 0.3916136622428894
Epoch 2390, training loss: 83.46269226074219 = 0.2644960284233093 + 10.0 * 8.319819450378418
Epoch 2390, val loss: 0.39164865016937256
Epoch 2400, training loss: 83.46405792236328 = 0.26359444856643677 + 10.0 * 8.320046424865723
Epoch 2400, val loss: 0.3917091190814972
Epoch 2410, training loss: 83.48977661132812 = 0.26268723607063293 + 10.0 * 8.322709083557129
Epoch 2410, val loss: 0.3917745351791382
Epoch 2420, training loss: 83.48426818847656 = 0.2617790997028351 + 10.0 * 8.322248458862305
Epoch 2420, val loss: 0.39190155267715454
Epoch 2430, training loss: 83.47224426269531 = 0.26087725162506104 + 10.0 * 8.321136474609375
Epoch 2430, val loss: 0.3919175863265991
Epoch 2440, training loss: 83.45506286621094 = 0.25996264815330505 + 10.0 * 8.319509506225586
Epoch 2440, val loss: 0.392196923494339
Epoch 2450, training loss: 83.44963836669922 = 0.2590745985507965 + 10.0 * 8.319056510925293
Epoch 2450, val loss: 0.39224445819854736
Epoch 2460, training loss: 83.45709228515625 = 0.2581802010536194 + 10.0 * 8.319890975952148
Epoch 2460, val loss: 0.39231470227241516
Epoch 2470, training loss: 83.46346282958984 = 0.2572775185108185 + 10.0 * 8.320618629455566
Epoch 2470, val loss: 0.39247214794158936
Epoch 2480, training loss: 83.46432495117188 = 0.2563777565956116 + 10.0 * 8.320795059204102
Epoch 2480, val loss: 0.392503559589386
Epoch 2490, training loss: 83.44522094726562 = 0.2554783225059509 + 10.0 * 8.318974494934082
Epoch 2490, val loss: 0.3926766514778137
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8427194317605275
0.8656813736144318
The final CL Acc:0.84593, 0.00385, The final GNN Acc:0.86428, 0.00125
Begin epxeriment: noisy_level: 0.2 cont_weight: 10 epoch: der1:0.3 der2:0 dfr1:0.4 dfr2:0.5
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=2500, cl_num_layers=2, cl_num_proj_hidden=64, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=10.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.0, drop_feat_rate_1=0.4, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=32, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, noisy_level=0.2, num_hidden=64, num_proj_hidden=64, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=40, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 88648])
add edge: torch.Size([2, 106344])
remove edge: torch.Size([2, 70868])
updated graph: torch.Size([2, 88564])
=== Noisy graph ===
/home/project-robust-contrastive/Robust-Contrastive-Learning/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 106.93486785888672 = 1.1117992401123047 + 10.0 * 10.582306861877441
Epoch 0, val loss: 1.1112514734268188
Epoch 10, training loss: 106.92680358886719 = 1.106123685836792 + 10.0 * 10.582067489624023
Epoch 10, val loss: 1.105590581893921
Epoch 20, training loss: 106.90919494628906 = 1.1001594066619873 + 10.0 * 10.580904006958008
Epoch 20, val loss: 1.0995965003967285
Epoch 30, training loss: 106.8428726196289 = 1.093576431274414 + 10.0 * 10.574930191040039
Epoch 30, val loss: 1.092993974685669
Epoch 40, training loss: 106.5545425415039 = 1.0864362716674805 + 10.0 * 10.5468111038208
Epoch 40, val loss: 1.0858632326126099
Epoch 50, training loss: 105.48851776123047 = 1.0788483619689941 + 10.0 * 10.440966606140137
Epoch 50, val loss: 1.078307867050171
Epoch 60, training loss: 102.64092254638672 = 1.0710524320602417 + 10.0 * 10.156987190246582
Epoch 60, val loss: 1.0705429315567017
Epoch 70, training loss: 99.44528198242188 = 1.0624263286590576 + 10.0 * 9.838285446166992
Epoch 70, val loss: 1.0621120929718018
Epoch 80, training loss: 96.80596923828125 = 1.055065393447876 + 10.0 * 9.575090408325195
Epoch 80, val loss: 1.055031180381775
Epoch 90, training loss: 94.3774185180664 = 1.049547553062439 + 10.0 * 9.33278751373291
Epoch 90, val loss: 1.04975163936615
Epoch 100, training loss: 93.29981231689453 = 1.0452942848205566 + 10.0 * 9.225451469421387
Epoch 100, val loss: 1.0456383228302002
Epoch 110, training loss: 92.78609466552734 = 1.0413367748260498 + 10.0 * 9.17447566986084
Epoch 110, val loss: 1.0418214797973633
Epoch 120, training loss: 92.05147552490234 = 1.0375313758850098 + 10.0 * 9.101394653320312
Epoch 120, val loss: 1.0381749868392944
Epoch 130, training loss: 91.285400390625 = 1.0337809324264526 + 10.0 * 9.025161743164062
Epoch 130, val loss: 1.0345923900604248
Epoch 140, training loss: 90.76119995117188 = 1.0300638675689697 + 10.0 * 8.973113059997559
Epoch 140, val loss: 1.0310112237930298
Epoch 150, training loss: 90.32694244384766 = 1.0262566804885864 + 10.0 * 8.930068016052246
Epoch 150, val loss: 1.0273035764694214
Epoch 160, training loss: 89.85890197753906 = 1.0223395824432373 + 10.0 * 8.88365650177002
Epoch 160, val loss: 1.0235259532928467
Epoch 170, training loss: 89.45903015136719 = 1.0183945894241333 + 10.0 * 8.844063758850098
Epoch 170, val loss: 1.0197174549102783
Epoch 180, training loss: 89.10069274902344 = 1.014258623123169 + 10.0 * 8.808643341064453
Epoch 180, val loss: 1.015676736831665
Epoch 190, training loss: 88.8147201538086 = 1.009791374206543 + 10.0 * 8.780492782592773
Epoch 190, val loss: 1.0112934112548828
Epoch 200, training loss: 88.57887268066406 = 1.0049018859863281 + 10.0 * 8.757396697998047
Epoch 200, val loss: 1.0065540075302124
Epoch 210, training loss: 88.37785339355469 = 0.9996535778045654 + 10.0 * 8.73781967163086
Epoch 210, val loss: 1.00149667263031
Epoch 220, training loss: 88.18566131591797 = 0.9941486120223999 + 10.0 * 8.719151496887207
Epoch 220, val loss: 0.9961872696876526
Epoch 230, training loss: 88.02546691894531 = 0.9883339405059814 + 10.0 * 8.703713417053223
Epoch 230, val loss: 0.9905929565429688
Epoch 240, training loss: 87.845458984375 = 0.9821507930755615 + 10.0 * 8.686330795288086
Epoch 240, val loss: 0.9846442937850952
Epoch 250, training loss: 87.69678497314453 = 0.9755861163139343 + 10.0 * 8.672120094299316
Epoch 250, val loss: 0.9783019423484802
Epoch 260, training loss: 87.5561752319336 = 0.968586266040802 + 10.0 * 8.658758163452148
Epoch 260, val loss: 0.9715368151664734
Epoch 270, training loss: 87.45347595214844 = 0.9610832929611206 + 10.0 * 8.649239540100098
Epoch 270, val loss: 0.9643306136131287
Epoch 280, training loss: 87.31864166259766 = 0.9532086849212646 + 10.0 * 8.636543273925781
Epoch 280, val loss: 0.9567103385925293
Epoch 290, training loss: 87.20156860351562 = 0.9449298977851868 + 10.0 * 8.625663757324219
Epoch 290, val loss: 0.9487354159355164
Epoch 300, training loss: 87.09056854248047 = 0.9362372159957886 + 10.0 * 8.615433692932129
Epoch 300, val loss: 0.940373957157135
Epoch 310, training loss: 86.9919204711914 = 0.9271280765533447 + 10.0 * 8.606478691101074
Epoch 310, val loss: 0.9316110014915466
Epoch 320, training loss: 86.91979217529297 = 0.9175062775611877 + 10.0 * 8.600229263305664
Epoch 320, val loss: 0.9223829507827759
Epoch 330, training loss: 86.81997680664062 = 0.9074992537498474 + 10.0 * 8.59124755859375
Epoch 330, val loss: 0.9127881526947021
Epoch 340, training loss: 86.73004913330078 = 0.8971482515335083 + 10.0 * 8.583290100097656
Epoch 340, val loss: 0.9028787612915039
Epoch 350, training loss: 86.64605712890625 = 0.8864328861236572 + 10.0 * 8.57596206665039
Epoch 350, val loss: 0.8926489949226379
Epoch 360, training loss: 86.63380432128906 = 0.8754339814186096 + 10.0 * 8.575837135314941
Epoch 360, val loss: 0.8821073174476624
Epoch 370, training loss: 86.51268768310547 = 0.8640661835670471 + 10.0 * 8.564862251281738
Epoch 370, val loss: 0.871327817440033
Epoch 380, training loss: 86.43028259277344 = 0.852573573589325 + 10.0 * 8.557770729064941
Epoch 380, val loss: 0.8604297637939453
Epoch 390, training loss: 86.36519622802734 = 0.8409211039543152 + 10.0 * 8.552427291870117
Epoch 390, val loss: 0.849405825138092
Epoch 400, training loss: 86.30575561523438 = 0.8291618227958679 + 10.0 * 8.547658920288086
Epoch 400, val loss: 0.8382989168167114
Epoch 410, training loss: 86.28813934326172 = 0.8172921538352966 + 10.0 * 8.54708480834961
Epoch 410, val loss: 0.8270998001098633
Epoch 420, training loss: 86.20218658447266 = 0.8054443597793579 + 10.0 * 8.539674758911133
Epoch 420, val loss: 0.8159699440002441
Epoch 430, training loss: 86.15445709228516 = 0.7937752604484558 + 10.0 * 8.536067962646484
Epoch 430, val loss: 0.8050256371498108
Epoch 440, training loss: 86.09996032714844 = 0.7822301387786865 + 10.0 * 8.531773567199707
Epoch 440, val loss: 0.794244110584259
Epoch 450, training loss: 86.05487060546875 = 0.7708961367607117 + 10.0 * 8.528397560119629
Epoch 450, val loss: 0.7836927771568298
Epoch 460, training loss: 86.03608703613281 = 0.7597508430480957 + 10.0 * 8.527633666992188
Epoch 460, val loss: 0.7733412981033325
Epoch 470, training loss: 85.98065948486328 = 0.7489467859268188 + 10.0 * 8.523171424865723
Epoch 470, val loss: 0.7633166313171387
Epoch 480, training loss: 85.92372131347656 = 0.738538384437561 + 10.0 * 8.518518447875977
Epoch 480, val loss: 0.753707230091095
Epoch 490, training loss: 85.87564086914062 = 0.7285016179084778 + 10.0 * 8.514714241027832
Epoch 490, val loss: 0.7444614171981812
Epoch 500, training loss: 85.84203338623047 = 0.7187888026237488 + 10.0 * 8.512324333190918
Epoch 500, val loss: 0.7355338335037231
Epoch 510, training loss: 85.79972076416016 = 0.709416925907135 + 10.0 * 8.50903034210205
Epoch 510, val loss: 0.7269779443740845
Epoch 520, training loss: 85.75428771972656 = 0.7004269361495972 + 10.0 * 8.505386352539062
Epoch 520, val loss: 0.7187395095825195
Epoch 530, training loss: 85.74185943603516 = 0.6918094754219055 + 10.0 * 8.5050048828125
Epoch 530, val loss: 0.7108959555625916
Epoch 540, training loss: 85.72210693359375 = 0.6835172176361084 + 10.0 * 8.50385856628418
Epoch 540, val loss: 0.7033780813217163
Epoch 550, training loss: 85.66555786132812 = 0.6757087707519531 + 10.0 * 8.498985290527344
Epoch 550, val loss: 0.6962999105453491
Epoch 560, training loss: 85.6275405883789 = 0.6682746410369873 + 10.0 * 8.495926856994629
Epoch 560, val loss: 0.6896154880523682
Epoch 570, training loss: 85.59864044189453 = 0.6612359881401062 + 10.0 * 8.49374008178711
Epoch 570, val loss: 0.6832899451255798
Epoch 580, training loss: 85.60797882080078 = 0.6545520424842834 + 10.0 * 8.495343208312988
Epoch 580, val loss: 0.6773533225059509
Epoch 590, training loss: 85.5763168334961 = 0.648174524307251 + 10.0 * 8.492814064025879
Epoch 590, val loss: 0.6715667843818665
Epoch 600, training loss: 85.53298950195312 = 0.6421989798545837 + 10.0 * 8.489079475402832
Epoch 600, val loss: 0.6662331819534302
Epoch 610, training loss: 85.50123596191406 = 0.6365506649017334 + 10.0 * 8.486468315124512
Epoch 610, val loss: 0.6612353324890137
Epoch 620, training loss: 85.47835540771484 = 0.6312413811683655 + 10.0 * 8.484711647033691
Epoch 620, val loss: 0.6565601825714111
Epoch 630, training loss: 85.50466918945312 = 0.6261977553367615 + 10.0 * 8.487847328186035
Epoch 630, val loss: 0.6521409153938293
Epoch 640, training loss: 85.44011688232422 = 0.621453046798706 + 10.0 * 8.481866836547852
Epoch 640, val loss: 0.6479684710502625
Epoch 650, training loss: 85.4151611328125 = 0.6169977188110352 + 10.0 * 8.479816436767578
Epoch 650, val loss: 0.6440518498420715
Epoch 660, training loss: 85.38927459716797 = 0.6127796769142151 + 10.0 * 8.477649688720703
Epoch 660, val loss: 0.640410840511322
Epoch 670, training loss: 85.36732482910156 = 0.6088078618049622 + 10.0 * 8.475851058959961
Epoch 670, val loss: 0.6369536519050598
Epoch 680, training loss: 85.4410629272461 = 0.6050328612327576 + 10.0 * 8.483602523803711
Epoch 680, val loss: 0.6336884498596191
Epoch 690, training loss: 85.34351348876953 = 0.6014174222946167 + 10.0 * 8.474209785461426
Epoch 690, val loss: 0.6305825710296631
Epoch 700, training loss: 85.31258392333984 = 0.5980300903320312 + 10.0 * 8.471455574035645
Epoch 700, val loss: 0.627717912197113
Epoch 710, training loss: 85.28779602050781 = 0.5948401093482971 + 10.0 * 8.469295501708984
Epoch 710, val loss: 0.6250332593917847
Epoch 720, training loss: 85.27153778076172 = 0.5918160676956177 + 10.0 * 8.467971801757812
Epoch 720, val loss: 0.6225231289863586
Epoch 730, training loss: 85.25713348388672 = 0.5888991951942444 + 10.0 * 8.46682357788086
Epoch 730, val loss: 0.6200500726699829
Epoch 740, training loss: 85.26752471923828 = 0.5861124992370605 + 10.0 * 8.468141555786133
Epoch 740, val loss: 0.6177773475646973
Epoch 750, training loss: 85.21235656738281 = 0.5834659337997437 + 10.0 * 8.462888717651367
Epoch 750, val loss: 0.6155768632888794
Epoch 760, training loss: 85.19319915771484 = 0.5809531807899475 + 10.0 * 8.461224555969238
Epoch 760, val loss: 0.6135215759277344
Epoch 770, training loss: 85.17947387695312 = 0.578554630279541 + 10.0 * 8.460092544555664
Epoch 770, val loss: 0.6115741729736328
Epoch 780, training loss: 85.15974426269531 = 0.5762486457824707 + 10.0 * 8.458349227905273
Epoch 780, val loss: 0.6097472310066223
Epoch 790, training loss: 85.14899444580078 = 0.5740232467651367 + 10.0 * 8.457497596740723
Epoch 790, val loss: 0.6079803705215454
Epoch 800, training loss: 85.163330078125 = 0.5718409419059753 + 10.0 * 8.459149360656738
Epoch 800, val loss: 0.6062813997268677
Epoch 810, training loss: 85.13836669921875 = 0.5697194933891296 + 10.0 * 8.456865310668945
Epoch 810, val loss: 0.604574978351593
Epoch 820, training loss: 85.111572265625 = 0.5677196979522705 + 10.0 * 8.454385757446289
Epoch 820, val loss: 0.6030521988868713
Epoch 830, training loss: 85.09332275390625 = 0.5657981038093567 + 10.0 * 8.452753067016602
Epoch 830, val loss: 0.6015669107437134
Epoch 840, training loss: 85.07688903808594 = 0.5639358162879944 + 10.0 * 8.451295852661133
Epoch 840, val loss: 0.6001572012901306
Epoch 850, training loss: 85.06649017333984 = 0.562125027179718 + 10.0 * 8.45043659210205
Epoch 850, val loss: 0.5988061428070068
Epoch 860, training loss: 85.12513732910156 = 0.5603328943252563 + 10.0 * 8.456480026245117
Epoch 860, val loss: 0.5974647998809814
Epoch 870, training loss: 85.06773376464844 = 0.5585688352584839 + 10.0 * 8.450916290283203
Epoch 870, val loss: 0.5961556434631348
Epoch 880, training loss: 85.03379821777344 = 0.5568835139274597 + 10.0 * 8.447690963745117
Epoch 880, val loss: 0.5949046611785889
Epoch 890, training loss: 85.01838684082031 = 0.5552553534507751 + 10.0 * 8.446313858032227
Epoch 890, val loss: 0.5936951637268066
Epoch 900, training loss: 85.00816345214844 = 0.5536715984344482 + 10.0 * 8.445448875427246
Epoch 900, val loss: 0.5925477743148804
Epoch 910, training loss: 84.99874877929688 = 0.5521163940429688 + 10.0 * 8.444663047790527
Epoch 910, val loss: 0.5914561748504639
Epoch 920, training loss: 85.06253814697266 = 0.5505731701850891 + 10.0 * 8.451196670532227
Epoch 920, val loss: 0.5903627276420593
Epoch 930, training loss: 85.00553131103516 = 0.5490176677703857 + 10.0 * 8.445651054382324
Epoch 930, val loss: 0.5892847776412964
Epoch 940, training loss: 84.97453308105469 = 0.5475313663482666 + 10.0 * 8.442700386047363
Epoch 940, val loss: 0.5882464647293091
Epoch 950, training loss: 84.95897674560547 = 0.5460910201072693 + 10.0 * 8.441288948059082
Epoch 950, val loss: 0.5872401595115662
Epoch 960, training loss: 84.94783020019531 = 0.5446784496307373 + 10.0 * 8.440315246582031
Epoch 960, val loss: 0.5862850546836853
Epoch 970, training loss: 84.93953704833984 = 0.5432795882225037 + 10.0 * 8.43962574005127
Epoch 970, val loss: 0.5853250026702881
Epoch 980, training loss: 84.96278381347656 = 0.5418961048126221 + 10.0 * 8.442089080810547
Epoch 980, val loss: 0.5843773484230042
Epoch 990, training loss: 84.93084716796875 = 0.5404807329177856 + 10.0 * 8.43903636932373
Epoch 990, val loss: 0.5834450125694275
Epoch 1000, training loss: 84.93394470214844 = 0.5391141176223755 + 10.0 * 8.439482688903809
Epoch 1000, val loss: 0.5825086832046509
Epoch 1010, training loss: 84.91036224365234 = 0.5377673506736755 + 10.0 * 8.437259674072266
Epoch 1010, val loss: 0.5816332101821899
Epoch 1020, training loss: 84.89788818359375 = 0.5364528298377991 + 10.0 * 8.43614387512207
Epoch 1020, val loss: 0.5807666778564453
Epoch 1030, training loss: 84.89418029785156 = 0.5351532697677612 + 10.0 * 8.43590259552002
Epoch 1030, val loss: 0.5799323916435242
Epoch 1040, training loss: 84.88970947265625 = 0.533860445022583 + 10.0 * 8.435585021972656
Epoch 1040, val loss: 0.579105019569397
Epoch 1050, training loss: 84.87738800048828 = 0.5325716137886047 + 10.0 * 8.434481620788574
Epoch 1050, val loss: 0.5782838463783264
Epoch 1060, training loss: 84.91085815429688 = 0.5312895178794861 + 10.0 * 8.437956809997559
Epoch 1060, val loss: 0.5774969458580017
Epoch 1070, training loss: 84.87216186523438 = 0.5299952030181885 + 10.0 * 8.434216499328613
Epoch 1070, val loss: 0.57657790184021
Epoch 1080, training loss: 84.85567474365234 = 0.528735876083374 + 10.0 * 8.432694435119629
Epoch 1080, val loss: 0.575843334197998
Epoch 1090, training loss: 84.8514633178711 = 0.5274873375892639 + 10.0 * 8.432397842407227
Epoch 1090, val loss: 0.5750546455383301
Epoch 1100, training loss: 84.86924743652344 = 0.5262331366539001 + 10.0 * 8.434301376342773
Epoch 1100, val loss: 0.5742817521095276
Epoch 1110, training loss: 84.84052276611328 = 0.5249854922294617 + 10.0 * 8.431553840637207
Epoch 1110, val loss: 0.5735528469085693
Epoch 1120, training loss: 84.82341003417969 = 0.5237451791763306 + 10.0 * 8.429966926574707
Epoch 1120, val loss: 0.5727865099906921
Epoch 1130, training loss: 84.82881927490234 = 0.5225194692611694 + 10.0 * 8.43062973022461
Epoch 1130, val loss: 0.5720775723457336
Epoch 1140, training loss: 84.83224487304688 = 0.5212727785110474 + 10.0 * 8.431097030639648
Epoch 1140, val loss: 0.5712838172912598
Epoch 1150, training loss: 84.80613708496094 = 0.5200262665748596 + 10.0 * 8.428610801696777
Epoch 1150, val loss: 0.5705211162567139
Epoch 1160, training loss: 84.79339599609375 = 0.5188024044036865 + 10.0 * 8.427459716796875
Epoch 1160, val loss: 0.5698213577270508
Epoch 1170, training loss: 84.79207611083984 = 0.5175864100456238 + 10.0 * 8.427449226379395
Epoch 1170, val loss: 0.5690803527832031
Epoch 1180, training loss: 84.81892395019531 = 0.5163602828979492 + 10.0 * 8.430256843566895
Epoch 1180, val loss: 0.5683804750442505
Epoch 1190, training loss: 84.78295135498047 = 0.5151081085205078 + 10.0 * 8.42678451538086
Epoch 1190, val loss: 0.5675832033157349
Epoch 1200, training loss: 84.76679992675781 = 0.5138835310935974 + 10.0 * 8.425291061401367
Epoch 1200, val loss: 0.566859245300293
Epoch 1210, training loss: 84.75713348388672 = 0.5126643180847168 + 10.0 * 8.424447059631348
Epoch 1210, val loss: 0.5661245584487915
Epoch 1220, training loss: 84.75948333740234 = 0.5114462971687317 + 10.0 * 8.424803733825684
Epoch 1220, val loss: 0.5654138922691345
Epoch 1230, training loss: 84.77100372314453 = 0.5101984739303589 + 10.0 * 8.426080703735352
Epoch 1230, val loss: 0.5646510720252991
Epoch 1240, training loss: 84.74787902832031 = 0.5089425444602966 + 10.0 * 8.423893928527832
Epoch 1240, val loss: 0.5638987421989441
Epoch 1250, training loss: 84.73688507080078 = 0.5076961517333984 + 10.0 * 8.422918319702148
Epoch 1250, val loss: 0.5631346702575684
Epoch 1260, training loss: 84.73063659667969 = 0.5064630508422852 + 10.0 * 8.422417640686035
Epoch 1260, val loss: 0.5624247789382935
Epoch 1270, training loss: 84.75191497802734 = 0.505210816860199 + 10.0 * 8.424670219421387
Epoch 1270, val loss: 0.5616967082023621
Epoch 1280, training loss: 84.72279357910156 = 0.503922700881958 + 10.0 * 8.421887397766113
Epoch 1280, val loss: 0.5608276128768921
Epoch 1290, training loss: 84.71004486083984 = 0.5026513934135437 + 10.0 * 8.42073917388916
Epoch 1290, val loss: 0.5601229667663574
Epoch 1300, training loss: 84.75035858154297 = 0.5013583898544312 + 10.0 * 8.42490005493164
Epoch 1300, val loss: 0.5592964291572571
Epoch 1310, training loss: 84.70370483398438 = 0.5000522136688232 + 10.0 * 8.420365333557129
Epoch 1310, val loss: 0.5585386157035828
Epoch 1320, training loss: 84.68880462646484 = 0.49873512983322144 + 10.0 * 8.419007301330566
Epoch 1320, val loss: 0.557738721370697
Epoch 1330, training loss: 84.68065643310547 = 0.4974249005317688 + 10.0 * 8.418323516845703
Epoch 1330, val loss: 0.5569570064544678
Epoch 1340, training loss: 84.67529296875 = 0.496106892824173 + 10.0 * 8.41791820526123
Epoch 1340, val loss: 0.5561776757240295
Epoch 1350, training loss: 84.6910171508789 = 0.49476921558380127 + 10.0 * 8.419625282287598
Epoch 1350, val loss: 0.5553416013717651
Epoch 1360, training loss: 84.67552185058594 = 0.4933786392211914 + 10.0 * 8.418214797973633
Epoch 1360, val loss: 0.5545533299446106
Epoch 1370, training loss: 84.6771011352539 = 0.4919811189174652 + 10.0 * 8.418512344360352
Epoch 1370, val loss: 0.5536842942237854
Epoch 1380, training loss: 84.65250396728516 = 0.49059414863586426 + 10.0 * 8.416191101074219
Epoch 1380, val loss: 0.5528892874717712
Epoch 1390, training loss: 84.64654541015625 = 0.48920536041259766 + 10.0 * 8.41573429107666
Epoch 1390, val loss: 0.5520992875099182
Epoch 1400, training loss: 84.65116882324219 = 0.48779958486557007 + 10.0 * 8.416337013244629
Epoch 1400, val loss: 0.5512499809265137
Epoch 1410, training loss: 84.66824340820312 = 0.4863491952419281 + 10.0 * 8.41818904876709
Epoch 1410, val loss: 0.5504184365272522
Epoch 1420, training loss: 84.62960052490234 = 0.48488569259643555 + 10.0 * 8.414471626281738
Epoch 1420, val loss: 0.5495652556419373
Epoch 1430, training loss: 84.6266098022461 = 0.4834297299385071 + 10.0 * 8.414318084716797
Epoch 1430, val loss: 0.5487498044967651
Epoch 1440, training loss: 84.61972045898438 = 0.48196762800216675 + 10.0 * 8.413775444030762
Epoch 1440, val loss: 0.5479089617729187
Epoch 1450, training loss: 84.61841583251953 = 0.4804953932762146 + 10.0 * 8.413792610168457
Epoch 1450, val loss: 0.5470702052116394
Epoch 1460, training loss: 84.65033721923828 = 0.47897860407829285 + 10.0 * 8.417135238647461
Epoch 1460, val loss: 0.5461683869361877
Epoch 1470, training loss: 84.61164855957031 = 0.47743380069732666 + 10.0 * 8.413421630859375
Epoch 1470, val loss: 0.5453105568885803
Epoch 1480, training loss: 84.60215759277344 = 0.475892037153244 + 10.0 * 8.412626266479492
Epoch 1480, val loss: 0.5444545745849609
Epoch 1490, training loss: 84.59476470947266 = 0.47435101866722107 + 10.0 * 8.412041664123535
Epoch 1490, val loss: 0.5435806512832642
Epoch 1500, training loss: 84.61871337890625 = 0.47278910875320435 + 10.0 * 8.414592742919922
Epoch 1500, val loss: 0.5426791310310364
Epoch 1510, training loss: 84.58319854736328 = 0.4712007939815521 + 10.0 * 8.411199569702148
Epoch 1510, val loss: 0.5418413281440735
Epoch 1520, training loss: 84.5765380859375 = 0.46960577368736267 + 10.0 * 8.410693168640137
Epoch 1520, val loss: 0.540961742401123
Epoch 1530, training loss: 84.57167053222656 = 0.4680107533931732 + 10.0 * 8.41036605834961
Epoch 1530, val loss: 0.5400934815406799
Epoch 1540, training loss: 84.5675048828125 = 0.466407835483551 + 10.0 * 8.410109519958496
Epoch 1540, val loss: 0.5392232537269592
Epoch 1550, training loss: 84.58343505859375 = 0.4647887051105499 + 10.0 * 8.411864280700684
Epoch 1550, val loss: 0.53837651014328
Epoch 1560, training loss: 84.56909942626953 = 0.4631008207798004 + 10.0 * 8.410599708557129
Epoch 1560, val loss: 0.5373270511627197
Epoch 1570, training loss: 84.5753402709961 = 0.461429625749588 + 10.0 * 8.41139030456543
Epoch 1570, val loss: 0.5365305542945862
Epoch 1580, training loss: 84.55033874511719 = 0.4597644805908203 + 10.0 * 8.4090576171875
Epoch 1580, val loss: 0.5356131792068481
Epoch 1590, training loss: 84.54510498046875 = 0.4581095576286316 + 10.0 * 8.408699989318848
Epoch 1590, val loss: 0.5347349047660828
Epoch 1600, training loss: 84.5494613647461 = 0.45645296573638916 + 10.0 * 8.409300804138184
Epoch 1600, val loss: 0.533943235874176
Epoch 1610, training loss: 84.5604476928711 = 0.4547569751739502 + 10.0 * 8.410569190979004
Epoch 1610, val loss: 0.5330407023429871
Epoch 1620, training loss: 84.53678131103516 = 0.4530418813228607 + 10.0 * 8.408373832702637
Epoch 1620, val loss: 0.5320907831192017
Epoch 1630, training loss: 84.52217102050781 = 0.4513474404811859 + 10.0 * 8.407082557678223
Epoch 1630, val loss: 0.531275749206543
Epoch 1640, training loss: 84.51993560791016 = 0.4496428966522217 + 10.0 * 8.407029151916504
Epoch 1640, val loss: 0.5304334163665771
Epoch 1650, training loss: 84.53620910644531 = 0.44791918992996216 + 10.0 * 8.408828735351562
Epoch 1650, val loss: 0.5295664668083191
Epoch 1660, training loss: 84.53076934814453 = 0.44617795944213867 + 10.0 * 8.408459663391113
Epoch 1660, val loss: 0.5287027359008789
Epoch 1670, training loss: 84.51335144042969 = 0.4444381594657898 + 10.0 * 8.406891822814941
Epoch 1670, val loss: 0.5279257297515869
Epoch 1680, training loss: 84.502685546875 = 0.4427160620689392 + 10.0 * 8.405996322631836
Epoch 1680, val loss: 0.5270968079566956
Epoch 1690, training loss: 84.49370574951172 = 0.44100335240364075 + 10.0 * 8.40527057647705
Epoch 1690, val loss: 0.526320219039917
Epoch 1700, training loss: 84.48921966552734 = 0.4392976462841034 + 10.0 * 8.40499210357666
Epoch 1700, val loss: 0.5255974531173706
Epoch 1710, training loss: 84.49859619140625 = 0.43758833408355713 + 10.0 * 8.406100273132324
Epoch 1710, val loss: 0.5248749852180481
Epoch 1720, training loss: 84.5059814453125 = 0.4358539283275604 + 10.0 * 8.407012939453125
Epoch 1720, val loss: 0.5241271257400513
Epoch 1730, training loss: 84.49065399169922 = 0.4341094493865967 + 10.0 * 8.405653953552246
Epoch 1730, val loss: 0.5234042406082153
Epoch 1740, training loss: 84.48080444335938 = 0.43237176537513733 + 10.0 * 8.4048433303833
Epoch 1740, val loss: 0.5226479768753052
Epoch 1750, training loss: 84.46760559082031 = 0.4306754469871521 + 10.0 * 8.403693199157715
Epoch 1750, val loss: 0.5219939351081848
Epoch 1760, training loss: 84.46044921875 = 0.4289867877960205 + 10.0 * 8.403146743774414
Epoch 1760, val loss: 0.5213775634765625
Epoch 1770, training loss: 84.46533966064453 = 0.4272877871990204 + 10.0 * 8.40380573272705
Epoch 1770, val loss: 0.5206959843635559
Epoch 1780, training loss: 84.50448608398438 = 0.4255719482898712 + 10.0 * 8.407891273498535
Epoch 1780, val loss: 0.5200521349906921
Epoch 1790, training loss: 84.46390533447266 = 0.4238932728767395 + 10.0 * 8.404001235961914
Epoch 1790, val loss: 0.5195828080177307
Epoch 1800, training loss: 84.44792175292969 = 0.4221947193145752 + 10.0 * 8.402572631835938
Epoch 1800, val loss: 0.5189040899276733
Epoch 1810, training loss: 84.4408187866211 = 0.42055273056030273 + 10.0 * 8.402026176452637
Epoch 1810, val loss: 0.5184338688850403
Epoch 1820, training loss: 84.44622039794922 = 0.41889867186546326 + 10.0 * 8.402731895446777
Epoch 1820, val loss: 0.5178682804107666
Epoch 1830, training loss: 84.47602844238281 = 0.41725608706474304 + 10.0 * 8.405877113342285
Epoch 1830, val loss: 0.5173965096473694
Epoch 1840, training loss: 84.4433822631836 = 0.41560161113739014 + 10.0 * 8.402777671813965
Epoch 1840, val loss: 0.5169094800949097
Epoch 1850, training loss: 84.42463684082031 = 0.41396892070770264 + 10.0 * 8.401066780090332
Epoch 1850, val loss: 0.5164468288421631
Epoch 1860, training loss: 84.41468048095703 = 0.4123483896255493 + 10.0 * 8.400233268737793
Epoch 1860, val loss: 0.5160576105117798
Epoch 1870, training loss: 84.41246795654297 = 0.4107339084148407 + 10.0 * 8.40017318725586
Epoch 1870, val loss: 0.5156752467155457
Epoch 1880, training loss: 84.42388916015625 = 0.40913504362106323 + 10.0 * 8.40147590637207
Epoch 1880, val loss: 0.5153234601020813
Epoch 1890, training loss: 84.42034912109375 = 0.40751513838768005 + 10.0 * 8.401283264160156
Epoch 1890, val loss: 0.5149288177490234
Epoch 1900, training loss: 84.4073486328125 = 0.4059121608734131 + 10.0 * 8.40014362335205
Epoch 1900, val loss: 0.5146116614341736
Epoch 1910, training loss: 84.4053955078125 = 0.4043424427509308 + 10.0 * 8.400105476379395
Epoch 1910, val loss: 0.5143112540245056
Epoch 1920, training loss: 84.4263916015625 = 0.4028070867061615 + 10.0 * 8.402358055114746
Epoch 1920, val loss: 0.5140965580940247
Epoch 1930, training loss: 84.39635467529297 = 0.4012349545955658 + 10.0 * 8.399511337280273
Epoch 1930, val loss: 0.5137254595756531
Epoch 1940, training loss: 84.3814468383789 = 0.39971062541007996 + 10.0 * 8.398173332214355
Epoch 1940, val loss: 0.5135563611984253
Epoch 1950, training loss: 84.37907409667969 = 0.3982086479663849 + 10.0 * 8.398086547851562
Epoch 1950, val loss: 0.5133823156356812
Epoch 1960, training loss: 84.37297058105469 = 0.3967170715332031 + 10.0 * 8.397624969482422
Epoch 1960, val loss: 0.5132094025611877
Epoch 1970, training loss: 84.37300872802734 = 0.39523300528526306 + 10.0 * 8.397777557373047
Epoch 1970, val loss: 0.5130650997161865
Epoch 1980, training loss: 84.4461669921875 = 0.39376506209373474 + 10.0 * 8.405240058898926
Epoch 1980, val loss: 0.512989342212677
Epoch 1990, training loss: 84.4008560180664 = 0.39225757122039795 + 10.0 * 8.400859832763672
Epoch 1990, val loss: 0.512802243232727
Epoch 2000, training loss: 84.37120819091797 = 0.39079561829566956 + 10.0 * 8.398041725158691
Epoch 2000, val loss: 0.5127914547920227
Epoch 2010, training loss: 84.35921478271484 = 0.3893490731716156 + 10.0 * 8.39698600769043
Epoch 2010, val loss: 0.512696385383606
Epoch 2020, training loss: 84.3517074584961 = 0.38792315125465393 + 10.0 * 8.396378517150879
Epoch 2020, val loss: 0.5126946568489075
Epoch 2030, training loss: 84.34739685058594 = 0.3865031898021698 + 10.0 * 8.396089553833008
Epoch 2030, val loss: 0.5127217769622803
Epoch 2040, training loss: 84.35305786132812 = 0.3850855827331543 + 10.0 * 8.396797180175781
Epoch 2040, val loss: 0.5126979351043701
Epoch 2050, training loss: 84.38582611083984 = 0.3836633861064911 + 10.0 * 8.400216102600098
Epoch 2050, val loss: 0.5127276182174683
Epoch 2060, training loss: 84.35323333740234 = 0.3822939395904541 + 10.0 * 8.397093772888184
Epoch 2060, val loss: 0.512943685054779
Epoch 2070, training loss: 84.33700561523438 = 0.38089317083358765 + 10.0 * 8.395611763000488
Epoch 2070, val loss: 0.5129359364509583
Epoch 2080, training loss: 84.32805633544922 = 0.3795413076877594 + 10.0 * 8.394851684570312
Epoch 2080, val loss: 0.51310795545578
Epoch 2090, training loss: 84.3257064819336 = 0.3781760632991791 + 10.0 * 8.394753456115723
Epoch 2090, val loss: 0.5132485628128052
Epoch 2100, training loss: 84.3263931274414 = 0.37681153416633606 + 10.0 * 8.39495849609375
Epoch 2100, val loss: 0.5133644938468933
Epoch 2110, training loss: 84.40657806396484 = 0.3754366338253021 + 10.0 * 8.403114318847656
Epoch 2110, val loss: 0.5134316086769104
Epoch 2120, training loss: 84.33358001708984 = 0.3740854859352112 + 10.0 * 8.395949363708496
Epoch 2120, val loss: 0.5137282609939575
Epoch 2130, training loss: 84.31845092773438 = 0.37272554636001587 + 10.0 * 8.394572257995605
Epoch 2130, val loss: 0.5138936638832092
Epoch 2140, training loss: 84.31140899658203 = 0.3714068830013275 + 10.0 * 8.394000053405762
Epoch 2140, val loss: 0.5141695141792297
Epoch 2150, training loss: 84.30403900146484 = 0.37010228633880615 + 10.0 * 8.393393516540527
Epoch 2150, val loss: 0.5144720673561096
Epoch 2160, training loss: 84.30089569091797 = 0.3687935769557953 + 10.0 * 8.393209457397461
Epoch 2160, val loss: 0.5147494077682495
Epoch 2170, training loss: 84.3025894165039 = 0.36750519275665283 + 10.0 * 8.393507957458496
Epoch 2170, val loss: 0.5150898694992065
Epoch 2180, training loss: 84.40335083007812 = 0.3662271499633789 + 10.0 * 8.403712272644043
Epoch 2180, val loss: 0.5154628753662109
Epoch 2190, training loss: 84.31430053710938 = 0.36488521099090576 + 10.0 * 8.394941329956055
Epoch 2190, val loss: 0.5156413316726685
Epoch 2200, training loss: 84.29109191894531 = 0.363620400428772 + 10.0 * 8.39274787902832
Epoch 2200, val loss: 0.5160261988639832
Epoch 2210, training loss: 84.28726959228516 = 0.36236584186553955 + 10.0 * 8.39249038696289
Epoch 2210, val loss: 0.5164941549301147
Epoch 2220, training loss: 84.28521728515625 = 0.36110711097717285 + 10.0 * 8.392411231994629
Epoch 2220, val loss: 0.5169450044631958
Epoch 2230, training loss: 84.32938385009766 = 0.3598857820034027 + 10.0 * 8.396949768066406
Epoch 2230, val loss: 0.5175033807754517
Epoch 2240, training loss: 84.2817153930664 = 0.3585870862007141 + 10.0 * 8.392313003540039
Epoch 2240, val loss: 0.5177271962165833
Epoch 2250, training loss: 84.27291107177734 = 0.3573713004589081 + 10.0 * 8.39155387878418
Epoch 2250, val loss: 0.5182803273200989
Epoch 2260, training loss: 84.26929473876953 = 0.3561606705188751 + 10.0 * 8.391313552856445
Epoch 2260, val loss: 0.5187451839447021
Epoch 2270, training loss: 84.26856994628906 = 0.3549584448337555 + 10.0 * 8.391361236572266
Epoch 2270, val loss: 0.5192489624023438
Epoch 2280, training loss: 84.3195571899414 = 0.3537653982639313 + 10.0 * 8.396578788757324
Epoch 2280, val loss: 0.51976478099823
Epoch 2290, training loss: 84.2752685546875 = 0.3525663912296295 + 10.0 * 8.3922700881958
Epoch 2290, val loss: 0.5202667713165283
Epoch 2300, training loss: 84.26570892333984 = 0.35138165950775146 + 10.0 * 8.391432762145996
Epoch 2300, val loss: 0.5208195447921753
Epoch 2310, training loss: 84.25699615478516 = 0.35021036863327026 + 10.0 * 8.390678405761719
Epoch 2310, val loss: 0.5213185548782349
Epoch 2320, training loss: 84.25279235839844 = 0.34904876351356506 + 10.0 * 8.390374183654785
Epoch 2320, val loss: 0.5219181776046753
Epoch 2330, training loss: 84.2667236328125 = 0.3478972315788269 + 10.0 * 8.391881942749023
Epoch 2330, val loss: 0.522453248500824
Epoch 2340, training loss: 84.27742767333984 = 0.34675201773643494 + 10.0 * 8.393068313598633
Epoch 2340, val loss: 0.5231242775917053
Epoch 2350, training loss: 84.26557159423828 = 0.34562191367149353 + 10.0 * 8.391995429992676
Epoch 2350, val loss: 0.5238103270530701
Epoch 2360, training loss: 84.24393463134766 = 0.3444614112377167 + 10.0 * 8.389947891235352
Epoch 2360, val loss: 0.5243270993232727
Epoch 2370, training loss: 84.2398681640625 = 0.3433380722999573 + 10.0 * 8.389653205871582
Epoch 2370, val loss: 0.5250223875045776
Epoch 2380, training loss: 84.2469711303711 = 0.3422022759914398 + 10.0 * 8.390477180480957
Epoch 2380, val loss: 0.5255700945854187
Epoch 2390, training loss: 84.26056671142578 = 0.34107255935668945 + 10.0 * 8.391949653625488
Epoch 2390, val loss: 0.5262041687965393
Epoch 2400, training loss: 84.24069213867188 = 0.33996307849884033 + 10.0 * 8.3900728225708
Epoch 2400, val loss: 0.5269062519073486
Epoch 2410, training loss: 84.24004364013672 = 0.3388383388519287 + 10.0 * 8.390120506286621
Epoch 2410, val loss: 0.5274785757064819
Epoch 2420, training loss: 84.22663116455078 = 0.33774080872535706 + 10.0 * 8.38888931274414
Epoch 2420, val loss: 0.5282019376754761
Epoch 2430, training loss: 84.22203063964844 = 0.3366539478302002 + 10.0 * 8.388537406921387
Epoch 2430, val loss: 0.5288755893707275
Epoch 2440, training loss: 84.22834777832031 = 0.33556801080703735 + 10.0 * 8.389277458190918
Epoch 2440, val loss: 0.5295689105987549
Epoch 2450, training loss: 84.24324035644531 = 0.33447152376174927 + 10.0 * 8.390876770019531
Epoch 2450, val loss: 0.5301814079284668
Epoch 2460, training loss: 84.23319244384766 = 0.3333548605442047 + 10.0 * 8.389984130859375
Epoch 2460, val loss: 0.5307076573371887
Epoch 2470, training loss: 84.22789001464844 = 0.3322841227054596 + 10.0 * 8.38956069946289
Epoch 2470, val loss: 0.531509518623352
Epoch 2480, training loss: 84.22123718261719 = 0.3311924934387207 + 10.0 * 8.389004707336426
Epoch 2480, val loss: 0.5321000218391418
Epoch 2490, training loss: 84.20753479003906 = 0.3301294147968292 + 10.0 * 8.387740135192871
Epoch 2490, val loss: 0.5328920483589172
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7955352612886859
0.8151126566688401
=== training gcn model ===
Epoch 0, training loss: 106.91664123535156 = 1.0935758352279663 + 10.0 * 10.582306861877441
Epoch 0, val loss: 1.094260573387146
Epoch 10, training loss: 106.90984344482422 = 1.0892376899719238 + 10.0 * 10.582059860229492
Epoch 10, val loss: 1.0899145603179932
Epoch 20, training loss: 106.8944091796875 = 1.0845973491668701 + 10.0 * 10.580981254577637
Epoch 20, val loss: 1.0852471590042114
Epoch 30, training loss: 106.84140014648438 = 1.0796418190002441 + 10.0 * 10.576175689697266
Epoch 30, val loss: 1.0802690982818604
Epoch 40, training loss: 106.62567138671875 = 1.0744569301605225 + 10.0 * 10.555121421813965
Epoch 40, val loss: 1.0750600099563599
Epoch 50, training loss: 105.75089263916016 = 1.0691126585006714 + 10.0 * 10.468177795410156
Epoch 50, val loss: 1.0697158575057983
Epoch 60, training loss: 102.61844635009766 = 1.0638537406921387 + 10.0 * 10.1554594039917
Epoch 60, val loss: 1.0644583702087402
Epoch 70, training loss: 97.51115417480469 = 1.058633804321289 + 10.0 * 9.645252227783203
Epoch 70, val loss: 1.0592758655548096
Epoch 80, training loss: 97.04083251953125 = 1.0545903444290161 + 10.0 * 9.598624229431152
Epoch 80, val loss: 1.0552761554718018
Epoch 90, training loss: 96.527099609375 = 1.0514194965362549 + 10.0 * 9.547567367553711
Epoch 90, val loss: 1.0521196126937866
Epoch 100, training loss: 95.76624298095703 = 1.0482933521270752 + 10.0 * 9.471795082092285
Epoch 100, val loss: 1.0489383935928345
Epoch 110, training loss: 94.49202728271484 = 1.0447187423706055 + 10.0 * 9.344731330871582
Epoch 110, val loss: 1.0453988313674927
Epoch 120, training loss: 93.11746978759766 = 1.0412096977233887 + 10.0 * 9.207626342773438
Epoch 120, val loss: 1.0419927835464478
Epoch 130, training loss: 92.2924575805664 = 1.0380377769470215 + 10.0 * 9.125441551208496
Epoch 130, val loss: 1.038871169090271
Epoch 140, training loss: 91.53028869628906 = 1.0353024005889893 + 10.0 * 9.049498558044434
Epoch 140, val loss: 1.0361770391464233
Epoch 150, training loss: 91.03627014160156 = 1.0328456163406372 + 10.0 * 9.00034236907959
Epoch 150, val loss: 1.0336936712265015
Epoch 160, training loss: 90.73143768310547 = 1.0298144817352295 + 10.0 * 8.970162391662598
Epoch 160, val loss: 1.0306081771850586
Epoch 170, training loss: 90.3170394897461 = 1.0263123512268066 + 10.0 * 8.929072380065918
Epoch 170, val loss: 1.0271737575531006
Epoch 180, training loss: 89.79798126220703 = 1.0231032371520996 + 10.0 * 8.877488136291504
Epoch 180, val loss: 1.0240367650985718
Epoch 190, training loss: 89.31623840332031 = 1.0200061798095703 + 10.0 * 8.829623222351074
Epoch 190, val loss: 1.0209569931030273
Epoch 200, training loss: 88.98493194580078 = 1.016377568244934 + 10.0 * 8.796854972839355
Epoch 200, val loss: 1.0172744989395142
Epoch 210, training loss: 88.6916732788086 = 1.01218843460083 + 10.0 * 8.767948150634766
Epoch 210, val loss: 1.0130696296691895
Epoch 220, training loss: 88.44497680664062 = 1.007537841796875 + 10.0 * 8.743743896484375
Epoch 220, val loss: 1.0084786415100098
Epoch 230, training loss: 88.21040344238281 = 1.002489686012268 + 10.0 * 8.720791816711426
Epoch 230, val loss: 1.0034767389297485
Epoch 240, training loss: 88.0422592163086 = 0.9969646334648132 + 10.0 * 8.704529762268066
Epoch 240, val loss: 0.9980402588844299
Epoch 250, training loss: 87.92257690429688 = 0.9908791780471802 + 10.0 * 8.693169593811035
Epoch 250, val loss: 0.9920336604118347
Epoch 260, training loss: 87.81582641601562 = 0.9843299388885498 + 10.0 * 8.683149337768555
Epoch 260, val loss: 0.9856298565864563
Epoch 270, training loss: 87.70342254638672 = 0.977659285068512 + 10.0 * 8.672575950622559
Epoch 270, val loss: 0.9790871739387512
Epoch 280, training loss: 87.5898666381836 = 0.9706646800041199 + 10.0 * 8.661920547485352
Epoch 280, val loss: 0.972251296043396
Epoch 290, training loss: 87.49282836914062 = 0.9633232951164246 + 10.0 * 8.652950286865234
Epoch 290, val loss: 0.9650940895080566
Epoch 300, training loss: 87.36190795898438 = 0.9555979371070862 + 10.0 * 8.640630722045898
Epoch 300, val loss: 0.9575312733650208
Epoch 310, training loss: 87.24465942382812 = 0.9474990367889404 + 10.0 * 8.629715919494629
Epoch 310, val loss: 0.94963139295578
Epoch 320, training loss: 87.17086791992188 = 0.9390228390693665 + 10.0 * 8.623184204101562
Epoch 320, val loss: 0.9413692951202393
Epoch 330, training loss: 87.02979278564453 = 0.9301258325576782 + 10.0 * 8.609967231750488
Epoch 330, val loss: 0.9327208995819092
Epoch 340, training loss: 86.92704010009766 = 0.9209332466125488 + 10.0 * 8.600610733032227
Epoch 340, val loss: 0.9238073229789734
Epoch 350, training loss: 86.82723236083984 = 0.9114485383033752 + 10.0 * 8.591578483581543
Epoch 350, val loss: 0.9145967364311218
Epoch 360, training loss: 86.71692657470703 = 0.9017150402069092 + 10.0 * 8.581521034240723
Epoch 360, val loss: 0.9052041172981262
Epoch 370, training loss: 86.61831665039062 = 0.8918006420135498 + 10.0 * 8.572651863098145
Epoch 370, val loss: 0.8956502079963684
Epoch 380, training loss: 86.54535675048828 = 0.8817033171653748 + 10.0 * 8.566365242004395
Epoch 380, val loss: 0.8859053254127502
Epoch 390, training loss: 86.45255279541016 = 0.8713873028755188 + 10.0 * 8.558116912841797
Epoch 390, val loss: 0.8760358095169067
Epoch 400, training loss: 86.37722778320312 = 0.8609382510185242 + 10.0 * 8.551629066467285
Epoch 400, val loss: 0.8660145401954651
Epoch 410, training loss: 86.3017807006836 = 0.8503493666648865 + 10.0 * 8.545143127441406
Epoch 410, val loss: 0.8558913469314575
Epoch 420, training loss: 86.2510986328125 = 0.8397178649902344 + 10.0 * 8.541138648986816
Epoch 420, val loss: 0.8457784652709961
Epoch 430, training loss: 86.20209503173828 = 0.8290008902549744 + 10.0 * 8.537309646606445
Epoch 430, val loss: 0.8355733156204224
Epoch 440, training loss: 86.14297485351562 = 0.8182721734046936 + 10.0 * 8.532469749450684
Epoch 440, val loss: 0.825421929359436
Epoch 450, training loss: 86.10448455810547 = 0.8076145052909851 + 10.0 * 8.52968692779541
Epoch 450, val loss: 0.8153574466705322
Epoch 460, training loss: 86.07464599609375 = 0.7969958782196045 + 10.0 * 8.527765274047852
Epoch 460, val loss: 0.8053779602050781
Epoch 470, training loss: 86.02365112304688 = 0.7865024209022522 + 10.0 * 8.523715019226074
Epoch 470, val loss: 0.7955154776573181
Epoch 480, training loss: 85.98267364501953 = 0.776150107383728 + 10.0 * 8.520651817321777
Epoch 480, val loss: 0.7858557105064392
Epoch 490, training loss: 85.94660949707031 = 0.7659460306167603 + 10.0 * 8.51806640625
Epoch 490, val loss: 0.7763657569885254
Epoch 500, training loss: 85.93518829345703 = 0.7559088468551636 + 10.0 * 8.517928123474121
Epoch 500, val loss: 0.7670493125915527
Epoch 510, training loss: 85.88423919677734 = 0.7460535168647766 + 10.0 * 8.513818740844727
Epoch 510, val loss: 0.7579319477081299
Epoch 520, training loss: 85.8468246459961 = 0.7364347577095032 + 10.0 * 8.511038780212402
Epoch 520, val loss: 0.7490817904472351
Epoch 530, training loss: 85.816162109375 = 0.7269704341888428 + 10.0 * 8.508919715881348
Epoch 530, val loss: 0.740408718585968
Epoch 540, training loss: 85.78538513183594 = 0.7178038954734802 + 10.0 * 8.506757736206055
Epoch 540, val loss: 0.7320535778999329
Epoch 550, training loss: 85.76750183105469 = 0.7089139223098755 + 10.0 * 8.505858421325684
Epoch 550, val loss: 0.7240031957626343
Epoch 560, training loss: 85.73249053955078 = 0.7003086805343628 + 10.0 * 8.503217697143555
Epoch 560, val loss: 0.716266393661499
Epoch 570, training loss: 85.68827056884766 = 0.6920066475868225 + 10.0 * 8.499626159667969
Epoch 570, val loss: 0.708884596824646
Epoch 580, training loss: 85.65492248535156 = 0.6840569972991943 + 10.0 * 8.497086524963379
Epoch 580, val loss: 0.7018297910690308
Epoch 590, training loss: 85.63697814941406 = 0.6764621734619141 + 10.0 * 8.496051788330078
Epoch 590, val loss: 0.6951613426208496
Epoch 600, training loss: 85.60779571533203 = 0.6691405773162842 + 10.0 * 8.493865013122559
Epoch 600, val loss: 0.6887102723121643
Epoch 610, training loss: 85.56326293945312 = 0.6622329354286194 + 10.0 * 8.490102767944336
Epoch 610, val loss: 0.6827215552330017
Epoch 620, training loss: 85.53204345703125 = 0.6556823253631592 + 10.0 * 8.487635612487793
Epoch 620, val loss: 0.6770923733711243
Epoch 630, training loss: 85.51615905761719 = 0.64947909116745 + 10.0 * 8.48666763305664
Epoch 630, val loss: 0.6717766523361206
Epoch 640, training loss: 85.48870849609375 = 0.6435731053352356 + 10.0 * 8.484514236450195
Epoch 640, val loss: 0.6668088436126709
Epoch 650, training loss: 85.44159698486328 = 0.6379916071891785 + 10.0 * 8.48036003112793
Epoch 650, val loss: 0.6621494889259338
Epoch 660, training loss: 85.40681457519531 = 0.6327347755432129 + 10.0 * 8.477407455444336
Epoch 660, val loss: 0.6578068137168884
Epoch 670, training loss: 85.4343032836914 = 0.6277422904968262 + 10.0 * 8.480656623840332
Epoch 670, val loss: 0.6537002325057983
Epoch 680, training loss: 85.3580322265625 = 0.6229895949363708 + 10.0 * 8.473505020141602
Epoch 680, val loss: 0.6498892903327942
Epoch 690, training loss: 85.32603454589844 = 0.618515133857727 + 10.0 * 8.470751762390137
Epoch 690, val loss: 0.6463814377784729
Epoch 700, training loss: 85.3010025024414 = 0.614296019077301 + 10.0 * 8.468670845031738
Epoch 700, val loss: 0.6430853009223938
Epoch 710, training loss: 85.2783432006836 = 0.610289454460144 + 10.0 * 8.466805458068848
Epoch 710, val loss: 0.6400048136711121
Epoch 720, training loss: 85.26988983154297 = 0.6064531803131104 + 10.0 * 8.466343879699707
Epoch 720, val loss: 0.6370303630828857
Epoch 730, training loss: 85.24825286865234 = 0.6027855277061462 + 10.0 * 8.464547157287598
Epoch 730, val loss: 0.6343072652816772
Epoch 740, training loss: 85.21871185302734 = 0.599349856376648 + 10.0 * 8.461935997009277
Epoch 740, val loss: 0.6318356990814209
Epoch 750, training loss: 85.19818878173828 = 0.5960974097251892 + 10.0 * 8.460208892822266
Epoch 750, val loss: 0.6295052766799927
Epoch 760, training loss: 85.18199920654297 = 0.5929856300354004 + 10.0 * 8.458901405334473
Epoch 760, val loss: 0.6273151636123657
Epoch 770, training loss: 85.22327423095703 = 0.5899803638458252 + 10.0 * 8.463329315185547
Epoch 770, val loss: 0.6252365112304688
Epoch 780, training loss: 85.16250610351562 = 0.5870646834373474 + 10.0 * 8.457544326782227
Epoch 780, val loss: 0.6232638359069824
Epoch 790, training loss: 85.14228057861328 = 0.5843105912208557 + 10.0 * 8.45579719543457
Epoch 790, val loss: 0.6214086413383484
Epoch 800, training loss: 85.1218490600586 = 0.5817030668258667 + 10.0 * 8.454014778137207
Epoch 800, val loss: 0.619734525680542
Epoch 810, training loss: 85.10823822021484 = 0.5792019963264465 + 10.0 * 8.452903747558594
Epoch 810, val loss: 0.6181364059448242
Epoch 820, training loss: 85.1422119140625 = 0.576796293258667 + 10.0 * 8.456541061401367
Epoch 820, val loss: 0.6165159344673157
Epoch 830, training loss: 85.08990478515625 = 0.5744000673294067 + 10.0 * 8.451550483703613
Epoch 830, val loss: 0.6150578260421753
Epoch 840, training loss: 85.0801010131836 = 0.5721262693405151 + 10.0 * 8.450797080993652
Epoch 840, val loss: 0.6136953234672546
Epoch 850, training loss: 85.05863952636719 = 0.5699664354324341 + 10.0 * 8.448866844177246
Epoch 850, val loss: 0.6124224662780762
Epoch 860, training loss: 85.04646301269531 = 0.5678883194923401 + 10.0 * 8.447857856750488
Epoch 860, val loss: 0.6112222671508789
Epoch 870, training loss: 85.03365325927734 = 0.5658687353134155 + 10.0 * 8.446778297424316
Epoch 870, val loss: 0.6100637316703796
Epoch 880, training loss: 85.02185821533203 = 0.5638971924781799 + 10.0 * 8.445796012878418
Epoch 880, val loss: 0.6089438796043396
Epoch 890, training loss: 85.04045867919922 = 0.5619627237319946 + 10.0 * 8.44784927368164
Epoch 890, val loss: 0.6078022718429565
Epoch 900, training loss: 85.06761932373047 = 0.5600154995918274 + 10.0 * 8.450760841369629
Epoch 900, val loss: 0.6067925691604614
Epoch 910, training loss: 85.00958251953125 = 0.5581342577934265 + 10.0 * 8.445144653320312
Epoch 910, val loss: 0.6057105660438538
Epoch 920, training loss: 84.98393249511719 = 0.5563492178916931 + 10.0 * 8.442758560180664
Epoch 920, val loss: 0.604730486869812
Epoch 930, training loss: 84.97062683105469 = 0.5546144843101501 + 10.0 * 8.441601753234863
Epoch 930, val loss: 0.6037920713424683
Epoch 940, training loss: 84.96011352539062 = 0.5529069900512695 + 10.0 * 8.440720558166504
Epoch 940, val loss: 0.6028809547424316
Epoch 950, training loss: 84.9501724243164 = 0.5512214303016663 + 10.0 * 8.439894676208496
Epoch 950, val loss: 0.6019867062568665
Epoch 960, training loss: 84.94022369384766 = 0.5495561957359314 + 10.0 * 8.439066886901855
Epoch 960, val loss: 0.6010725498199463
Epoch 970, training loss: 84.93675231933594 = 0.5479077100753784 + 10.0 * 8.438884735107422
Epoch 970, val loss: 0.6001607775688171
Epoch 980, training loss: 84.92704010009766 = 0.5462487936019897 + 10.0 * 8.438078880310059
Epoch 980, val loss: 0.5992913842201233
Epoch 990, training loss: 84.91474914550781 = 0.5446207523345947 + 10.0 * 8.437012672424316
Epoch 990, val loss: 0.598383367061615
Epoch 1000, training loss: 84.90550231933594 = 0.5430466532707214 + 10.0 * 8.436245918273926
Epoch 1000, val loss: 0.5975274443626404
Epoch 1010, training loss: 84.8965072631836 = 0.5414945483207703 + 10.0 * 8.435501098632812
Epoch 1010, val loss: 0.5966879725456238
Epoch 1020, training loss: 84.8877944946289 = 0.5399536490440369 + 10.0 * 8.434783935546875
Epoch 1020, val loss: 0.5958578586578369
Epoch 1030, training loss: 84.8863754272461 = 0.5384219288825989 + 10.0 * 8.434795379638672
Epoch 1030, val loss: 0.5950349569320679
Epoch 1040, training loss: 84.88299560546875 = 0.5368652939796448 + 10.0 * 8.434613227844238
Epoch 1040, val loss: 0.5941614508628845
Epoch 1050, training loss: 84.86846923828125 = 0.5353197455406189 + 10.0 * 8.43331527709961
Epoch 1050, val loss: 0.5932906866073608
Epoch 1060, training loss: 84.85734558105469 = 0.5338087677955627 + 10.0 * 8.432353973388672
Epoch 1060, val loss: 0.592458963394165
Epoch 1070, training loss: 84.84768676757812 = 0.5323200225830078 + 10.0 * 8.431536674499512
Epoch 1070, val loss: 0.5916460752487183
Epoch 1080, training loss: 84.83971405029297 = 0.5308296084403992 + 10.0 * 8.430888175964355
Epoch 1080, val loss: 0.5908151268959045
Epoch 1090, training loss: 84.84806823730469 = 0.5293366312980652 + 10.0 * 8.431873321533203
Epoch 1090, val loss: 0.5900011658668518
Epoch 1100, training loss: 84.84627532958984 = 0.5277997255325317 + 10.0 * 8.43184757232666
Epoch 1100, val loss: 0.5891123414039612
Epoch 1110, training loss: 84.82228088378906 = 0.5263025164604187 + 10.0 * 8.429597854614258
Epoch 1110, val loss: 0.5883148908615112
Epoch 1120, training loss: 84.81300354003906 = 0.5248154997825623 + 10.0 * 8.428818702697754
Epoch 1120, val loss: 0.5874626636505127
Epoch 1130, training loss: 84.81121063232422 = 0.5233387351036072 + 10.0 * 8.428787231445312
Epoch 1130, val loss: 0.5866379737854004
Epoch 1140, training loss: 84.80669403076172 = 0.5218546986579895 + 10.0 * 8.428483963012695
Epoch 1140, val loss: 0.5857966542243958
Epoch 1150, training loss: 84.82000732421875 = 0.5203756093978882 + 10.0 * 8.429963111877441
Epoch 1150, val loss: 0.58492511510849
Epoch 1160, training loss: 84.79459381103516 = 0.5188549757003784 + 10.0 * 8.427574157714844
Epoch 1160, val loss: 0.5840237140655518
Epoch 1170, training loss: 84.78321838378906 = 0.517367422580719 + 10.0 * 8.42658519744873
Epoch 1170, val loss: 0.583128035068512
Epoch 1180, training loss: 84.7712173461914 = 0.5158923864364624 + 10.0 * 8.425532341003418
Epoch 1180, val loss: 0.5822716951370239
Epoch 1190, training loss: 84.76397705078125 = 0.5144217014312744 + 10.0 * 8.424955368041992
Epoch 1190, val loss: 0.5813860297203064
Epoch 1200, training loss: 84.75653839111328 = 0.5129438042640686 + 10.0 * 8.424359321594238
Epoch 1200, val loss: 0.5805264711380005
Epoch 1210, training loss: 84.75059509277344 = 0.511459231376648 + 10.0 * 8.423913955688477
Epoch 1210, val loss: 0.5796353816986084
Epoch 1220, training loss: 84.80819702148438 = 0.5099654197692871 + 10.0 * 8.42982292175293
Epoch 1220, val loss: 0.5787186622619629
Epoch 1230, training loss: 84.75738525390625 = 0.5084272027015686 + 10.0 * 8.424895286560059
Epoch 1230, val loss: 0.5778387784957886
Epoch 1240, training loss: 84.74030303955078 = 0.5069226026535034 + 10.0 * 8.423337936401367
Epoch 1240, val loss: 0.5769562125205994
Epoch 1250, training loss: 84.74237060546875 = 0.5054309368133545 + 10.0 * 8.423693656921387
Epoch 1250, val loss: 0.5760475993156433
Epoch 1260, training loss: 84.71916198730469 = 0.5039278864860535 + 10.0 * 8.421523094177246
Epoch 1260, val loss: 0.5750873684883118
Epoch 1270, training loss: 84.71876525878906 = 0.5024275779724121 + 10.0 * 8.42163372039795
Epoch 1270, val loss: 0.5741700530052185
Epoch 1280, training loss: 84.71826934814453 = 0.5009100437164307 + 10.0 * 8.421735763549805
Epoch 1280, val loss: 0.5732406377792358
Epoch 1290, training loss: 84.7081069946289 = 0.49939075112342834 + 10.0 * 8.42087173461914
Epoch 1290, val loss: 0.5723514556884766
Epoch 1300, training loss: 84.73271179199219 = 0.4978548288345337 + 10.0 * 8.42348575592041
Epoch 1300, val loss: 0.5714412927627563
Epoch 1310, training loss: 84.69164276123047 = 0.4963105022907257 + 10.0 * 8.419533729553223
Epoch 1310, val loss: 0.5704686045646667
Epoch 1320, training loss: 84.68431854248047 = 0.494785875082016 + 10.0 * 8.418952941894531
Epoch 1320, val loss: 0.5695054531097412
Epoch 1330, training loss: 84.67935943603516 = 0.49326077103614807 + 10.0 * 8.418609619140625
Epoch 1330, val loss: 0.5686085224151611
Epoch 1340, training loss: 84.68342590332031 = 0.49172767996788025 + 10.0 * 8.419169425964355
Epoch 1340, val loss: 0.5676193237304688
Epoch 1350, training loss: 84.68607330322266 = 0.49015212059020996 + 10.0 * 8.419591903686523
Epoch 1350, val loss: 0.5666669607162476
Epoch 1360, training loss: 84.66605377197266 = 0.48857221007347107 + 10.0 * 8.41774845123291
Epoch 1360, val loss: 0.5657232403755188
Epoch 1370, training loss: 84.6576919555664 = 0.4870104491710663 + 10.0 * 8.417068481445312
Epoch 1370, val loss: 0.5648324489593506
Epoch 1380, training loss: 84.6518783569336 = 0.4854626953601837 + 10.0 * 8.416641235351562
Epoch 1380, val loss: 0.5638957619667053
Epoch 1390, training loss: 84.6497802734375 = 0.4839057922363281 + 10.0 * 8.416587829589844
Epoch 1390, val loss: 0.5629792809486389
Epoch 1400, training loss: 84.6636962890625 = 0.4823322892189026 + 10.0 * 8.418136596679688
Epoch 1400, val loss: 0.5620578527450562
Epoch 1410, training loss: 84.63577270507812 = 0.48074111342430115 + 10.0 * 8.415502548217773
Epoch 1410, val loss: 0.5610315799713135
Epoch 1420, training loss: 84.6298828125 = 0.4791595935821533 + 10.0 * 8.415072441101074
Epoch 1420, val loss: 0.560126543045044
Epoch 1430, training loss: 84.63941955566406 = 0.47757580876350403 + 10.0 * 8.416184425354004
Epoch 1430, val loss: 0.5592027306556702
Epoch 1440, training loss: 84.61541748046875 = 0.47598010301589966 + 10.0 * 8.41394329071045
Epoch 1440, val loss: 0.5582205653190613
Epoch 1450, training loss: 84.61305236816406 = 0.47437795996665955 + 10.0 * 8.413866996765137
Epoch 1450, val loss: 0.5572804808616638
Epoch 1460, training loss: 84.65557861328125 = 0.4727599024772644 + 10.0 * 8.418281555175781
Epoch 1460, val loss: 0.5563549995422363
Epoch 1470, training loss: 84.6147689819336 = 0.4711342751979828 + 10.0 * 8.414362907409668
Epoch 1470, val loss: 0.555357813835144
Epoch 1480, training loss: 84.59578704833984 = 0.46951794624328613 + 10.0 * 8.412626266479492
Epoch 1480, val loss: 0.554492175579071
Epoch 1490, training loss: 84.60025024414062 = 0.4679107666015625 + 10.0 * 8.413233757019043
Epoch 1490, val loss: 0.5535987615585327
Epoch 1500, training loss: 84.58946228027344 = 0.466266393661499 + 10.0 * 8.412320137023926
Epoch 1500, val loss: 0.5526347756385803
Epoch 1510, training loss: 84.57716369628906 = 0.4646332263946533 + 10.0 * 8.411252975463867
Epoch 1510, val loss: 0.5517244935035706
Epoch 1520, training loss: 84.5723876953125 = 0.4630148708820343 + 10.0 * 8.410937309265137
Epoch 1520, val loss: 0.5508090853691101
Epoch 1530, training loss: 84.59162139892578 = 0.4613778293132782 + 10.0 * 8.413023948669434
Epoch 1530, val loss: 0.5499395132064819
Epoch 1540, training loss: 84.56214904785156 = 0.4597373306751251 + 10.0 * 8.41024112701416
Epoch 1540, val loss: 0.5489823818206787
Epoch 1550, training loss: 84.55500030517578 = 0.45809727907180786 + 10.0 * 8.409689903259277
Epoch 1550, val loss: 0.5481215715408325
Epoch 1560, training loss: 84.56149291992188 = 0.4564572274684906 + 10.0 * 8.410503387451172
Epoch 1560, val loss: 0.5472694039344788
Epoch 1570, training loss: 84.56074523925781 = 0.4547933042049408 + 10.0 * 8.410594940185547
Epoch 1570, val loss: 0.5463224053382874
Epoch 1580, training loss: 84.53512573242188 = 0.45312729477882385 + 10.0 * 8.40820026397705
Epoch 1580, val loss: 0.5454424023628235
Epoch 1590, training loss: 84.52790069580078 = 0.45148640871047974 + 10.0 * 8.407641410827637
Epoch 1590, val loss: 0.5445705056190491
Epoch 1600, training loss: 84.52204132080078 = 0.44984468817710876 + 10.0 * 8.407219886779785
Epoch 1600, val loss: 0.5437548756599426
Epoch 1610, training loss: 84.52230834960938 = 0.4481964409351349 + 10.0 * 8.407411575317383
Epoch 1610, val loss: 0.5429499745368958
Epoch 1620, training loss: 84.55268096923828 = 0.4465316832065582 + 10.0 * 8.410614967346191
Epoch 1620, val loss: 0.5421324372291565
Epoch 1630, training loss: 84.5250473022461 = 0.4448544979095459 + 10.0 * 8.408019065856934
Epoch 1630, val loss: 0.5413187146186829
Epoch 1640, training loss: 84.50643920898438 = 0.44319188594818115 + 10.0 * 8.40632438659668
Epoch 1640, val loss: 0.5404250025749207
Epoch 1650, training loss: 84.49895477294922 = 0.4415377378463745 + 10.0 * 8.405741691589355
Epoch 1650, val loss: 0.5396224856376648
Epoch 1660, training loss: 84.49273681640625 = 0.4398846924304962 + 10.0 * 8.405284881591797
Epoch 1660, val loss: 0.5388354063034058
Epoch 1670, training loss: 84.51754760742188 = 0.4382321536540985 + 10.0 * 8.407931327819824
Epoch 1670, val loss: 0.5379838943481445
Epoch 1680, training loss: 84.52909851074219 = 0.43654462695121765 + 10.0 * 8.409255027770996
Epoch 1680, val loss: 0.5374013185501099
Epoch 1690, training loss: 84.4893569946289 = 0.4348805546760559 + 10.0 * 8.405447006225586
Epoch 1690, val loss: 0.5363787412643433
Epoch 1700, training loss: 84.46947479248047 = 0.4332200586795807 + 10.0 * 8.40362548828125
Epoch 1700, val loss: 0.5357066988945007
Epoch 1710, training loss: 84.46387481689453 = 0.4315831661224365 + 10.0 * 8.403229713439941
Epoch 1710, val loss: 0.5349850058555603
Epoch 1720, training loss: 84.45702362060547 = 0.42993566393852234 + 10.0 * 8.402708053588867
Epoch 1720, val loss: 0.534209668636322
Epoch 1730, training loss: 84.46326446533203 = 0.4282792806625366 + 10.0 * 8.403498649597168
Epoch 1730, val loss: 0.533475935459137
Epoch 1740, training loss: 84.45989227294922 = 0.42661356925964355 + 10.0 * 8.403327941894531
Epoch 1740, val loss: 0.53267502784729
Epoch 1750, training loss: 84.4699935913086 = 0.4249463975429535 + 10.0 * 8.404504776000977
Epoch 1750, val loss: 0.5320398211479187
Epoch 1760, training loss: 84.44813537597656 = 0.4232952296733856 + 10.0 * 8.402483940124512
Epoch 1760, val loss: 0.5313241481781006
Epoch 1770, training loss: 84.4395523071289 = 0.421655535697937 + 10.0 * 8.401789665222168
Epoch 1770, val loss: 0.5306726098060608
Epoch 1780, training loss: 84.42621612548828 = 0.42002636194229126 + 10.0 * 8.400618553161621
Epoch 1780, val loss: 0.530002772808075
Epoch 1790, training loss: 84.42250061035156 = 0.4184003174304962 + 10.0 * 8.400409698486328
Epoch 1790, val loss: 0.529374897480011
Epoch 1800, training loss: 84.43500518798828 = 0.41677090525627136 + 10.0 * 8.401823043823242
Epoch 1800, val loss: 0.5287502408027649
Epoch 1810, training loss: 84.4127426147461 = 0.4151324927806854 + 10.0 * 8.399761199951172
Epoch 1810, val loss: 0.5281556844711304
Epoch 1820, training loss: 84.42744445800781 = 0.4135117828845978 + 10.0 * 8.401392936706543
Epoch 1820, val loss: 0.5275148153305054
Epoch 1830, training loss: 84.42522430419922 = 0.4118706285953522 + 10.0 * 8.401334762573242
Epoch 1830, val loss: 0.5269172191619873
Epoch 1840, training loss: 84.40140533447266 = 0.4102329611778259 + 10.0 * 8.399117469787598
Epoch 1840, val loss: 0.526420533657074
Epoch 1850, training loss: 84.3942642211914 = 0.4086254835128784 + 10.0 * 8.398564338684082
Epoch 1850, val loss: 0.525923490524292
Epoch 1860, training loss: 84.38701629638672 = 0.4070383906364441 + 10.0 * 8.397997856140137
Epoch 1860, val loss: 0.5253744721412659
Epoch 1870, training loss: 84.3805923461914 = 0.40544402599334717 + 10.0 * 8.397515296936035
Epoch 1870, val loss: 0.5249217748641968
Epoch 1880, training loss: 84.37661743164062 = 0.4038579761981964 + 10.0 * 8.397275924682617
Epoch 1880, val loss: 0.5244508981704712
Epoch 1890, training loss: 84.42166900634766 = 0.40227124094963074 + 10.0 * 8.40194034576416
Epoch 1890, val loss: 0.523908257484436
Epoch 1900, training loss: 84.38652801513672 = 0.4006423056125641 + 10.0 * 8.398588180541992
Epoch 1900, val loss: 0.5236048698425293
Epoch 1910, training loss: 84.37369537353516 = 0.3990464508533478 + 10.0 * 8.397464752197266
Epoch 1910, val loss: 0.5231401324272156
Epoch 1920, training loss: 84.35787200927734 = 0.39745327830314636 + 10.0 * 8.396041870117188
Epoch 1920, val loss: 0.5227705240249634
Epoch 1930, training loss: 84.35243225097656 = 0.39586979150772095 + 10.0 * 8.39565658569336
Epoch 1930, val loss: 0.522402822971344
Epoch 1940, training loss: 84.37557220458984 = 0.39428475499153137 + 10.0 * 8.398128509521484
Epoch 1940, val loss: 0.5220435857772827
Epoch 1950, training loss: 84.37307739257812 = 0.3926810622215271 + 10.0 * 8.398039817810059
Epoch 1950, val loss: 0.5218256115913391
Epoch 1960, training loss: 84.35382080078125 = 0.3911009728908539 + 10.0 * 8.396272659301758
Epoch 1960, val loss: 0.5213898420333862
Epoch 1970, training loss: 84.33496856689453 = 0.38952136039733887 + 10.0 * 8.39454460144043
Epoch 1970, val loss: 0.5211518406867981
Epoch 1980, training loss: 84.33235168457031 = 0.38794761896133423 + 10.0 * 8.394440650939941
Epoch 1980, val loss: 0.5208486318588257
Epoch 1990, training loss: 84.3609848022461 = 0.386370450258255 + 10.0 * 8.397461891174316
Epoch 1990, val loss: 0.5204909443855286
Epoch 2000, training loss: 84.32279205322266 = 0.3847605884075165 + 10.0 * 8.393803596496582
Epoch 2000, val loss: 0.520281970500946
Epoch 2010, training loss: 84.31956481933594 = 0.3831775486469269 + 10.0 * 8.393638610839844
Epoch 2010, val loss: 0.5199986100196838
Epoch 2020, training loss: 84.31471252441406 = 0.38161104917526245 + 10.0 * 8.393309593200684
Epoch 2020, val loss: 0.519763708114624
Epoch 2030, training loss: 84.30945587158203 = 0.38005387783050537 + 10.0 * 8.392940521240234
Epoch 2030, val loss: 0.5195720791816711
Epoch 2040, training loss: 84.30603790283203 = 0.3784986138343811 + 10.0 * 8.392753601074219
Epoch 2040, val loss: 0.519363522529602
Epoch 2050, training loss: 84.34961700439453 = 0.3769531846046448 + 10.0 * 8.397266387939453
Epoch 2050, val loss: 0.5191499590873718
Epoch 2060, training loss: 84.32190704345703 = 0.37536388635635376 + 10.0 * 8.394654273986816
Epoch 2060, val loss: 0.5191644430160522
Epoch 2070, training loss: 84.3009033203125 = 0.37380820512771606 + 10.0 * 8.392709732055664
Epoch 2070, val loss: 0.5190049409866333
Epoch 2080, training loss: 84.29068756103516 = 0.37225955724716187 + 10.0 * 8.39184284210205
Epoch 2080, val loss: 0.518908679485321
Epoch 2090, training loss: 84.28752899169922 = 0.370718389749527 + 10.0 * 8.391680717468262
Epoch 2090, val loss: 0.5188683867454529
Epoch 2100, training loss: 84.30573272705078 = 0.36918848752975464 + 10.0 * 8.393654823303223
Epoch 2100, val loss: 0.5187505483627319
Epoch 2110, training loss: 84.28447723388672 = 0.3676331341266632 + 10.0 * 8.391684532165527
Epoch 2110, val loss: 0.5187958478927612
Epoch 2120, training loss: 84.27847290039062 = 0.36608588695526123 + 10.0 * 8.39123821258545
Epoch 2120, val loss: 0.5188038945198059
Epoch 2130, training loss: 84.27393341064453 = 0.36455413699150085 + 10.0 * 8.390937805175781
Epoch 2130, val loss: 0.5188537240028381
Epoch 2140, training loss: 84.26911926269531 = 0.36303046345710754 + 10.0 * 8.390608787536621
Epoch 2140, val loss: 0.5188778638839722
Epoch 2150, training loss: 84.27073669433594 = 0.361507385969162 + 10.0 * 8.390923500061035
Epoch 2150, val loss: 0.5189802050590515
Epoch 2160, training loss: 84.31884002685547 = 0.35998234152793884 + 10.0 * 8.395885467529297
Epoch 2160, val loss: 0.5190180540084839
Epoch 2170, training loss: 84.27681732177734 = 0.3584432005882263 + 10.0 * 8.391837120056152
Epoch 2170, val loss: 0.5190973281860352
Epoch 2180, training loss: 84.25814056396484 = 0.35692399740219116 + 10.0 * 8.390121459960938
Epoch 2180, val loss: 0.5191426277160645
Epoch 2190, training loss: 84.25257110595703 = 0.3554061949253082 + 10.0 * 8.389716148376465
Epoch 2190, val loss: 0.5191939473152161
Epoch 2200, training loss: 84.25957489013672 = 0.3539009690284729 + 10.0 * 8.3905668258667
Epoch 2200, val loss: 0.5192404389381409
Epoch 2210, training loss: 84.26753997802734 = 0.35239338874816895 + 10.0 * 8.391514778137207
Epoch 2210, val loss: 0.5193511843681335
Epoch 2220, training loss: 84.24729919433594 = 0.35088104009628296 + 10.0 * 8.389641761779785
Epoch 2220, val loss: 0.5195434093475342
Epoch 2230, training loss: 84.2401351928711 = 0.3493887484073639 + 10.0 * 8.389074325561523
Epoch 2230, val loss: 0.5196752548217773
Epoch 2240, training loss: 84.2365951538086 = 0.347900927066803 + 10.0 * 8.388869285583496
Epoch 2240, val loss: 0.5198225975036621
Epoch 2250, training loss: 84.23759460449219 = 0.34641650319099426 + 10.0 * 8.389117240905762
Epoch 2250, val loss: 0.5200589895248413
Epoch 2260, training loss: 84.26529693603516 = 0.3449329435825348 + 10.0 * 8.392036437988281
Epoch 2260, val loss: 0.520298957824707
Epoch 2270, training loss: 84.23558807373047 = 0.3434557318687439 + 10.0 * 8.389213562011719
Epoch 2270, val loss: 0.5204483270645142
Epoch 2280, training loss: 84.2323989868164 = 0.3419899344444275 + 10.0 * 8.38904094696045
Epoch 2280, val loss: 0.520638644695282
Epoch 2290, training loss: 84.26048278808594 = 0.34052881598472595 + 10.0 * 8.391995429992676
Epoch 2290, val loss: 0.5208814740180969
Epoch 2300, training loss: 84.23075103759766 = 0.3390548527240753 + 10.0 * 8.389169692993164
Epoch 2300, val loss: 0.521267294883728
Epoch 2310, training loss: 84.21570587158203 = 0.3376086354255676 + 10.0 * 8.387809753417969
Epoch 2310, val loss: 0.5214918255805969
Epoch 2320, training loss: 84.20951843261719 = 0.3361593186855316 + 10.0 * 8.387335777282715
Epoch 2320, val loss: 0.5218017101287842
Epoch 2330, training loss: 84.20797729492188 = 0.3347148597240448 + 10.0 * 8.38732624053955
Epoch 2330, val loss: 0.5221496224403381
Epoch 2340, training loss: 84.23564910888672 = 0.3332764208316803 + 10.0 * 8.390237808227539
Epoch 2340, val loss: 0.5225244164466858
Epoch 2350, training loss: 84.20841217041016 = 0.33183103799819946 + 10.0 * 8.38765811920166
Epoch 2350, val loss: 0.5228207111358643
Epoch 2360, training loss: 84.208251953125 = 0.3303871154785156 + 10.0 * 8.387785911560059
Epoch 2360, val loss: 0.5232806205749512
Epoch 2370, training loss: 84.19734191894531 = 0.3289622366428375 + 10.0 * 8.38683795928955
Epoch 2370, val loss: 0.5236230492591858
Epoch 2380, training loss: 84.19121551513672 = 0.32752808928489685 + 10.0 * 8.386368751525879
Epoch 2380, val loss: 0.5240251421928406
Epoch 2390, training loss: 84.19168853759766 = 0.3260959982872009 + 10.0 * 8.38655948638916
Epoch 2390, val loss: 0.5244985818862915
Epoch 2400, training loss: 84.26264190673828 = 0.32467028498649597 + 10.0 * 8.393796920776367
Epoch 2400, val loss: 0.5250033140182495
Epoch 2410, training loss: 84.21253967285156 = 0.3232418894767761 + 10.0 * 8.38892936706543
Epoch 2410, val loss: 0.5252993702888489
Epoch 2420, training loss: 84.1925048828125 = 0.3218231201171875 + 10.0 * 8.387067794799805
Epoch 2420, val loss: 0.525784432888031
Epoch 2430, training loss: 84.20203399658203 = 0.3204226493835449 + 10.0 * 8.388161659240723
Epoch 2430, val loss: 0.5262048840522766
Epoch 2440, training loss: 84.1772689819336 = 0.31900539994239807 + 10.0 * 8.385826110839844
Epoch 2440, val loss: 0.5267463326454163
Epoch 2450, training loss: 84.17135620117188 = 0.31760528683662415 + 10.0 * 8.385375022888184
Epoch 2450, val loss: 0.5271990299224854
Epoch 2460, training loss: 84.17047882080078 = 0.3162127137184143 + 10.0 * 8.38542652130127
Epoch 2460, val loss: 0.5276610255241394
Epoch 2470, training loss: 84.17518615722656 = 0.3148210346698761 + 10.0 * 8.38603687286377
Epoch 2470, val loss: 0.5282148122787476
Epoch 2480, training loss: 84.19110107421875 = 0.3134302496910095 + 10.0 * 8.38776683807373
Epoch 2480, val loss: 0.5287700891494751
Epoch 2490, training loss: 84.1634292602539 = 0.31203940510749817 + 10.0 * 8.385138511657715
Epoch 2490, val loss: 0.5293570160865784
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7929984779299848
0.8125045280011592
=== training gcn model ===
Epoch 0, training loss: 106.91769409179688 = 1.0948461294174194 + 10.0 * 10.582284927368164
Epoch 0, val loss: 1.0950390100479126
Epoch 10, training loss: 106.91011047363281 = 1.09041428565979 + 10.0 * 10.581969261169434
Epoch 10, val loss: 1.0906587839126587
Epoch 20, training loss: 106.88907623291016 = 1.0858604907989502 + 10.0 * 10.580321311950684
Epoch 20, val loss: 1.0861231088638306
Epoch 30, training loss: 106.79953002929688 = 1.0809226036071777 + 10.0 * 10.571860313415527
Epoch 30, val loss: 1.0811831951141357
Epoch 40, training loss: 106.4460678100586 = 1.0755956172943115 + 10.0 * 10.537047386169434
Epoch 40, val loss: 1.075845718383789
Epoch 50, training loss: 105.43640899658203 = 1.0697952508926392 + 10.0 * 10.436661720275879
Epoch 50, val loss: 1.070038914680481
Epoch 60, training loss: 103.20661926269531 = 1.064161777496338 + 10.0 * 10.214245796203613
Epoch 60, val loss: 1.064387321472168
Epoch 70, training loss: 100.54808807373047 = 1.0583487749099731 + 10.0 * 9.948973655700684
Epoch 70, val loss: 1.0585529804229736
Epoch 80, training loss: 98.35491180419922 = 1.0535104274749756 + 10.0 * 9.73013973236084
Epoch 80, val loss: 1.0538601875305176
Epoch 90, training loss: 96.1559829711914 = 1.0496938228607178 + 10.0 * 9.510629653930664
Epoch 90, val loss: 1.0502134561538696
Epoch 100, training loss: 94.48685455322266 = 1.0464965105056763 + 10.0 * 9.344036102294922
Epoch 100, val loss: 1.0471845865249634
Epoch 110, training loss: 93.52981567382812 = 1.0433467626571655 + 10.0 * 9.24864673614502
Epoch 110, val loss: 1.0440735816955566
Epoch 120, training loss: 93.1223373413086 = 1.0392892360687256 + 10.0 * 9.208304405212402
Epoch 120, val loss: 1.040037989616394
Epoch 130, training loss: 92.58507537841797 = 1.0355101823806763 + 10.0 * 9.154956817626953
Epoch 130, val loss: 1.036397099494934
Epoch 140, training loss: 91.77326965332031 = 1.0328681468963623 + 10.0 * 9.074040412902832
Epoch 140, val loss: 1.033939242362976
Epoch 150, training loss: 90.82942199707031 = 1.0306042432785034 + 10.0 * 8.97988224029541
Epoch 150, val loss: 1.0317800045013428
Epoch 160, training loss: 90.15438842773438 = 1.0281057357788086 + 10.0 * 8.912628173828125
Epoch 160, val loss: 1.0292789936065674
Epoch 170, training loss: 89.63802337646484 = 1.0251221656799316 + 10.0 * 8.861289978027344
Epoch 170, val loss: 1.0263279676437378
Epoch 180, training loss: 89.30126953125 = 1.0216374397277832 + 10.0 * 8.827962875366211
Epoch 180, val loss: 1.022839903831482
Epoch 190, training loss: 89.0523681640625 = 1.017438530921936 + 10.0 * 8.803492546081543
Epoch 190, val loss: 1.018841028213501
Epoch 200, training loss: 88.89543914794922 = 1.0128123760223389 + 10.0 * 8.788263320922852
Epoch 200, val loss: 1.0144758224487305
Epoch 210, training loss: 88.77558135986328 = 1.0080255270004272 + 10.0 * 8.776755332946777
Epoch 210, val loss: 1.009945273399353
Epoch 220, training loss: 88.66858673095703 = 1.0030580759048462 + 10.0 * 8.766552925109863
Epoch 220, val loss: 1.0052602291107178
Epoch 230, training loss: 88.56431579589844 = 0.9978598356246948 + 10.0 * 8.756646156311035
Epoch 230, val loss: 1.0002747774124146
Epoch 240, training loss: 88.44924926757812 = 0.9923692941665649 + 10.0 * 8.745687484741211
Epoch 240, val loss: 0.9950658082962036
Epoch 250, training loss: 88.31665802001953 = 0.98665851354599 + 10.0 * 8.732999801635742
Epoch 250, val loss: 0.9896307587623596
Epoch 260, training loss: 88.22286224365234 = 0.9807518720626831 + 10.0 * 8.724210739135742
Epoch 260, val loss: 0.9838802218437195
Epoch 270, training loss: 88.03500366210938 = 0.9743828177452087 + 10.0 * 8.706062316894531
Epoch 270, val loss: 0.9779569506645203
Epoch 280, training loss: 87.869140625 = 0.9678778052330017 + 10.0 * 8.690126419067383
Epoch 280, val loss: 0.9716755747795105
Epoch 290, training loss: 87.72189331054688 = 0.9608160853385925 + 10.0 * 8.676107406616211
Epoch 290, val loss: 0.9649581909179688
Epoch 300, training loss: 87.61007690429688 = 0.9533362984657288 + 10.0 * 8.665674209594727
Epoch 300, val loss: 0.9577891230583191
Epoch 310, training loss: 87.48267364501953 = 0.9453516602516174 + 10.0 * 8.653732299804688
Epoch 310, val loss: 0.9501211047172546
Epoch 320, training loss: 87.3567886352539 = 0.9369425177574158 + 10.0 * 8.641984939575195
Epoch 320, val loss: 0.9421105980873108
Epoch 330, training loss: 87.2306900024414 = 0.9283133745193481 + 10.0 * 8.630237579345703
Epoch 330, val loss: 0.9338473677635193
Epoch 340, training loss: 87.12310791015625 = 0.9193909764289856 + 10.0 * 8.62037181854248
Epoch 340, val loss: 0.9253878593444824
Epoch 350, training loss: 86.99647521972656 = 0.9102612733840942 + 10.0 * 8.608621597290039
Epoch 350, val loss: 0.916670024394989
Epoch 360, training loss: 86.92660522460938 = 0.9009440541267395 + 10.0 * 8.60256576538086
Epoch 360, val loss: 0.9077439904212952
Epoch 370, training loss: 86.7995834350586 = 0.8913165330886841 + 10.0 * 8.590826034545898
Epoch 370, val loss: 0.8985589146614075
Epoch 380, training loss: 86.69943237304688 = 0.8815508484840393 + 10.0 * 8.581788063049316
Epoch 380, val loss: 0.8892809748649597
Epoch 390, training loss: 86.6175537109375 = 0.8716315031051636 + 10.0 * 8.574592590332031
Epoch 390, val loss: 0.879869282245636
Epoch 400, training loss: 86.5809097290039 = 0.8616015911102295 + 10.0 * 8.571930885314941
Epoch 400, val loss: 0.8702815771102905
Epoch 410, training loss: 86.48253631591797 = 0.8514298796653748 + 10.0 * 8.5631103515625
Epoch 410, val loss: 0.8605912327766418
Epoch 420, training loss: 86.40536499023438 = 0.8411902785301208 + 10.0 * 8.556417465209961
Epoch 420, val loss: 0.8509042859077454
Epoch 430, training loss: 86.39698791503906 = 0.8308969736099243 + 10.0 * 8.556609153747559
Epoch 430, val loss: 0.8412403464317322
Epoch 440, training loss: 86.29087829589844 = 0.8206603527069092 + 10.0 * 8.547021865844727
Epoch 440, val loss: 0.8314552307128906
Epoch 450, training loss: 86.23869323730469 = 0.8104169964790344 + 10.0 * 8.542827606201172
Epoch 450, val loss: 0.8218017220497131
Epoch 460, training loss: 86.1898422241211 = 0.8002414703369141 + 10.0 * 8.538960456848145
Epoch 460, val loss: 0.8122232556343079
Epoch 470, training loss: 86.14678955078125 = 0.7900984883308411 + 10.0 * 8.535669326782227
Epoch 470, val loss: 0.8026937246322632
Epoch 480, training loss: 86.10873413085938 = 0.7799822688102722 + 10.0 * 8.532875061035156
Epoch 480, val loss: 0.7932551503181458
Epoch 490, training loss: 86.0713882446289 = 0.7699063420295715 + 10.0 * 8.53014850616455
Epoch 490, val loss: 0.7838374972343445
Epoch 500, training loss: 86.03520202636719 = 0.7599170207977295 + 10.0 * 8.527528762817383
Epoch 500, val loss: 0.7746039628982544
Epoch 510, training loss: 85.99571990966797 = 0.7500802874565125 + 10.0 * 8.524563789367676
Epoch 510, val loss: 0.7655153274536133
Epoch 520, training loss: 85.95862579345703 = 0.7403604984283447 + 10.0 * 8.521825790405273
Epoch 520, val loss: 0.756637692451477
Epoch 530, training loss: 85.9191665649414 = 0.7308005690574646 + 10.0 * 8.518836975097656
Epoch 530, val loss: 0.7479166388511658
Epoch 540, training loss: 85.9085464477539 = 0.7214089632034302 + 10.0 * 8.518712997436523
Epoch 540, val loss: 0.7393488883972168
Epoch 550, training loss: 85.88066101074219 = 0.7121099829673767 + 10.0 * 8.516855239868164
Epoch 550, val loss: 0.7311157584190369
Epoch 560, training loss: 85.81661987304688 = 0.7031329274177551 + 10.0 * 8.511348724365234
Epoch 560, val loss: 0.7231211066246033
Epoch 570, training loss: 85.76783752441406 = 0.6944268941879272 + 10.0 * 8.507341384887695
Epoch 570, val loss: 0.7154353857040405
Epoch 580, training loss: 85.72391510009766 = 0.6859245896339417 + 10.0 * 8.503798484802246
Epoch 580, val loss: 0.7079783082008362
Epoch 590, training loss: 85.68616485595703 = 0.6776350736618042 + 10.0 * 8.500852584838867
Epoch 590, val loss: 0.7007333636283875
Epoch 600, training loss: 85.66533660888672 = 0.669577419757843 + 10.0 * 8.4995756149292
Epoch 600, val loss: 0.6937639117240906
Epoch 610, training loss: 85.62109375 = 0.6618066430091858 + 10.0 * 8.495928764343262
Epoch 610, val loss: 0.6871793270111084
Epoch 620, training loss: 85.575927734375 = 0.6544191837310791 + 10.0 * 8.492151260375977
Epoch 620, val loss: 0.6809990406036377
Epoch 630, training loss: 85.53632354736328 = 0.6473473310470581 + 10.0 * 8.488897323608398
Epoch 630, val loss: 0.6750747561454773
Epoch 640, training loss: 85.5646743774414 = 0.6405794620513916 + 10.0 * 8.492409706115723
Epoch 640, val loss: 0.6694043874740601
Epoch 650, training loss: 85.48690032958984 = 0.6339940428733826 + 10.0 * 8.48529052734375
Epoch 650, val loss: 0.6641605496406555
Epoch 660, training loss: 85.45053100585938 = 0.6277943849563599 + 10.0 * 8.482274055480957
Epoch 660, val loss: 0.6592066884040833
Epoch 670, training loss: 85.4155502319336 = 0.6219316124916077 + 10.0 * 8.479361534118652
Epoch 670, val loss: 0.6545159220695496
Epoch 680, training loss: 85.38534545898438 = 0.6163359880447388 + 10.0 * 8.476901054382324
Epoch 680, val loss: 0.650150716304779
Epoch 690, training loss: 85.38334655761719 = 0.6110140681266785 + 10.0 * 8.477232933044434
Epoch 690, val loss: 0.6460625529289246
Epoch 700, training loss: 85.3669204711914 = 0.6059146523475647 + 10.0 * 8.47610092163086
Epoch 700, val loss: 0.6420847773551941
Epoch 710, training loss: 85.31912231445312 = 0.6010943651199341 + 10.0 * 8.471802711486816
Epoch 710, val loss: 0.6384650468826294
Epoch 720, training loss: 85.2895736694336 = 0.5965450406074524 + 10.0 * 8.469303131103516
Epoch 720, val loss: 0.6350762844085693
Epoch 730, training loss: 85.2830810546875 = 0.5922319293022156 + 10.0 * 8.469084739685059
Epoch 730, val loss: 0.631862223148346
Epoch 740, training loss: 85.25497436523438 = 0.5880560874938965 + 10.0 * 8.466691970825195
Epoch 740, val loss: 0.6289239525794983
Epoch 750, training loss: 85.23606872558594 = 0.5841330289840698 + 10.0 * 8.465193748474121
Epoch 750, val loss: 0.6260691285133362
Epoch 760, training loss: 85.20807647705078 = 0.5804194808006287 + 10.0 * 8.46276569366455
Epoch 760, val loss: 0.6234184503555298
Epoch 770, training loss: 85.18907928466797 = 0.5768912434577942 + 10.0 * 8.46121883392334
Epoch 770, val loss: 0.6209257245063782
Epoch 780, training loss: 85.16901397705078 = 0.5735105276107788 + 10.0 * 8.459550857543945
Epoch 780, val loss: 0.6186280846595764
Epoch 790, training loss: 85.15554809570312 = 0.5702731609344482 + 10.0 * 8.458527565002441
Epoch 790, val loss: 0.6164306402206421
Epoch 800, training loss: 85.15418243408203 = 0.5671485066413879 + 10.0 * 8.458703994750977
Epoch 800, val loss: 0.6143130660057068
Epoch 810, training loss: 85.13262939453125 = 0.5641428232192993 + 10.0 * 8.456849098205566
Epoch 810, val loss: 0.6123812198638916
Epoch 820, training loss: 85.12117004394531 = 0.5612518191337585 + 10.0 * 8.455991744995117
Epoch 820, val loss: 0.6104372143745422
Epoch 830, training loss: 85.09273529052734 = 0.558497428894043 + 10.0 * 8.453424453735352
Epoch 830, val loss: 0.6086421012878418
Epoch 840, training loss: 85.07133483886719 = 0.5558618903160095 + 10.0 * 8.451547622680664
Epoch 840, val loss: 0.6069757342338562
Epoch 850, training loss: 85.05952453613281 = 0.5533257126808167 + 10.0 * 8.4506196975708
Epoch 850, val loss: 0.6054131984710693
Epoch 860, training loss: 85.0715103149414 = 0.5508604049682617 + 10.0 * 8.452065467834473
Epoch 860, val loss: 0.6039463877677917
Epoch 870, training loss: 85.04166412353516 = 0.5484464168548584 + 10.0 * 8.449321746826172
Epoch 870, val loss: 0.6023626923561096
Epoch 880, training loss: 85.02350616455078 = 0.5461071133613586 + 10.0 * 8.44774055480957
Epoch 880, val loss: 0.6010489463806152
Epoch 890, training loss: 85.01107788085938 = 0.5438685417175293 + 10.0 * 8.446721076965332
Epoch 890, val loss: 0.5996725559234619
Epoch 900, training loss: 85.0387954711914 = 0.5416847467422485 + 10.0 * 8.449710845947266
Epoch 900, val loss: 0.5984285473823547
Epoch 910, training loss: 84.99008178710938 = 0.5395459532737732 + 10.0 * 8.445054054260254
Epoch 910, val loss: 0.5972011685371399
Epoch 920, training loss: 84.96772766113281 = 0.5374866127967834 + 10.0 * 8.443024635314941
Epoch 920, val loss: 0.5960255861282349
Epoch 930, training loss: 84.96018981933594 = 0.5354908108711243 + 10.0 * 8.442469596862793
Epoch 930, val loss: 0.5949190855026245
Epoch 940, training loss: 84.9620132446289 = 0.533535361289978 + 10.0 * 8.44284725189209
Epoch 940, val loss: 0.5938286781311035
Epoch 950, training loss: 84.94855499267578 = 0.5316004753112793 + 10.0 * 8.441695213317871
Epoch 950, val loss: 0.5927839875221252
Epoch 960, training loss: 84.93494415283203 = 0.5297042727470398 + 10.0 * 8.440524101257324
Epoch 960, val loss: 0.5918204188346863
Epoch 970, training loss: 84.92034149169922 = 0.5278549790382385 + 10.0 * 8.439249038696289
Epoch 970, val loss: 0.5908361673355103
Epoch 980, training loss: 84.95063018798828 = 0.5260006785392761 + 10.0 * 8.442462921142578
Epoch 980, val loss: 0.5899040699005127
Epoch 990, training loss: 84.90045928955078 = 0.5241537094116211 + 10.0 * 8.437630653381348
Epoch 990, val loss: 0.5890626907348633
Epoch 1000, training loss: 84.8936538696289 = 0.5224069356918335 + 10.0 * 8.437124252319336
Epoch 1000, val loss: 0.5881617665290833
Epoch 1010, training loss: 84.8795166015625 = 0.5207093358039856 + 10.0 * 8.435880661010742
Epoch 1010, val loss: 0.5873486995697021
Epoch 1020, training loss: 84.92278289794922 = 0.5190282464027405 + 10.0 * 8.440375328063965
Epoch 1020, val loss: 0.586577296257019
Epoch 1030, training loss: 84.89712524414062 = 0.5173162817955017 + 10.0 * 8.437980651855469
Epoch 1030, val loss: 0.5856397747993469
Epoch 1040, training loss: 84.86280059814453 = 0.5156770348548889 + 10.0 * 8.434712409973145
Epoch 1040, val loss: 0.5849159955978394
Epoch 1050, training loss: 84.84620666503906 = 0.5140917301177979 + 10.0 * 8.433211326599121
Epoch 1050, val loss: 0.5841804146766663
Epoch 1060, training loss: 84.83978271484375 = 0.5125371217727661 + 10.0 * 8.432724952697754
Epoch 1060, val loss: 0.5834538340568542
Epoch 1070, training loss: 84.84913635253906 = 0.5110029578208923 + 10.0 * 8.433813095092773
Epoch 1070, val loss: 0.5828295946121216
Epoch 1080, training loss: 84.83502960205078 = 0.5094580054283142 + 10.0 * 8.432557106018066
Epoch 1080, val loss: 0.5819942951202393
Epoch 1090, training loss: 84.82244873046875 = 0.5079449415206909 + 10.0 * 8.431450843811035
Epoch 1090, val loss: 0.5813767313957214
Epoch 1100, training loss: 84.81034088134766 = 0.5064778327941895 + 10.0 * 8.430386543273926
Epoch 1100, val loss: 0.580696165561676
Epoch 1110, training loss: 84.80118560791016 = 0.5050355792045593 + 10.0 * 8.429615020751953
Epoch 1110, val loss: 0.5800380110740662
Epoch 1120, training loss: 84.7938461303711 = 0.5036048293113708 + 10.0 * 8.429024696350098
Epoch 1120, val loss: 0.5793874263763428
Epoch 1130, training loss: 84.78724670410156 = 0.5021848082542419 + 10.0 * 8.428506851196289
Epoch 1130, val loss: 0.5787484645843506
Epoch 1140, training loss: 84.86450958251953 = 0.5007725954055786 + 10.0 * 8.436373710632324
Epoch 1140, val loss: 0.57813960313797
Epoch 1150, training loss: 84.81876373291016 = 0.49931520223617554 + 10.0 * 8.431944847106934
Epoch 1150, val loss: 0.5774298906326294
Epoch 1160, training loss: 84.77488708496094 = 0.4979151785373688 + 10.0 * 8.42769718170166
Epoch 1160, val loss: 0.5768018364906311
Epoch 1170, training loss: 84.75997924804688 = 0.4965652823448181 + 10.0 * 8.42634105682373
Epoch 1170, val loss: 0.5761922597885132
Epoch 1180, training loss: 84.75479125976562 = 0.4952312707901001 + 10.0 * 8.425955772399902
Epoch 1180, val loss: 0.5756116509437561
Epoch 1190, training loss: 84.7462158203125 = 0.4939052164554596 + 10.0 * 8.425230979919434
Epoch 1190, val loss: 0.5750223398208618
Epoch 1200, training loss: 84.7400131225586 = 0.49258196353912354 + 10.0 * 8.424742698669434
Epoch 1200, val loss: 0.5744360685348511
Epoch 1210, training loss: 84.80659484863281 = 0.49125581979751587 + 10.0 * 8.431533813476562
Epoch 1210, val loss: 0.573807418346405
Epoch 1220, training loss: 84.77997589111328 = 0.4898897707462311 + 10.0 * 8.429008483886719
Epoch 1220, val loss: 0.5732761025428772
Epoch 1230, training loss: 84.7359619140625 = 0.4885607957839966 + 10.0 * 8.424739837646484
Epoch 1230, val loss: 0.5726485252380371
Epoch 1240, training loss: 84.71976470947266 = 0.48727864027023315 + 10.0 * 8.423248291015625
Epoch 1240, val loss: 0.5720766186714172
Epoch 1250, training loss: 84.70974731445312 = 0.48600906133651733 + 10.0 * 8.42237377166748
Epoch 1250, val loss: 0.5715686678886414
Epoch 1260, training loss: 84.70147705078125 = 0.48474374413490295 + 10.0 * 8.421673774719238
Epoch 1260, val loss: 0.571029007434845
Epoch 1270, training loss: 84.69733428955078 = 0.4834803640842438 + 10.0 * 8.421384811401367
Epoch 1270, val loss: 0.5705122947692871
Epoch 1280, training loss: 84.7958984375 = 0.4822019040584564 + 10.0 * 8.43136978149414
Epoch 1280, val loss: 0.5699741840362549
Epoch 1290, training loss: 84.70134735107422 = 0.480891615152359 + 10.0 * 8.422045707702637
Epoch 1290, val loss: 0.5694836974143982
Epoch 1300, training loss: 84.68153381347656 = 0.47963064908981323 + 10.0 * 8.420190811157227
Epoch 1300, val loss: 0.568839967250824
Epoch 1310, training loss: 84.67239379882812 = 0.4783921241760254 + 10.0 * 8.419400215148926
Epoch 1310, val loss: 0.5683305859565735
Epoch 1320, training loss: 84.66636657714844 = 0.47715553641319275 + 10.0 * 8.418920516967773
Epoch 1320, val loss: 0.5678191184997559
Epoch 1330, training loss: 84.6591567993164 = 0.47591644525527954 + 10.0 * 8.41832447052002
Epoch 1330, val loss: 0.5673114657402039
Epoch 1340, training loss: 84.65376281738281 = 0.4746762216091156 + 10.0 * 8.417908668518066
Epoch 1340, val loss: 0.5667954087257385
Epoch 1350, training loss: 84.71746826171875 = 0.47342824935913086 + 10.0 * 8.42440414428711
Epoch 1350, val loss: 0.5661939978599548
Epoch 1360, training loss: 84.67583465576172 = 0.4721270799636841 + 10.0 * 8.420370101928711
Epoch 1360, val loss: 0.5657647848129272
Epoch 1370, training loss: 84.64614868164062 = 0.47085317969322205 + 10.0 * 8.417529106140137
Epoch 1370, val loss: 0.5651933550834656
Epoch 1380, training loss: 84.63311004638672 = 0.4696120321750641 + 10.0 * 8.416349411010742
Epoch 1380, val loss: 0.5646862387657166
Epoch 1390, training loss: 84.62500762939453 = 0.46837329864501953 + 10.0 * 8.41566276550293
Epoch 1390, val loss: 0.5641719698905945
Epoch 1400, training loss: 84.61924743652344 = 0.4671293795108795 + 10.0 * 8.41521167755127
Epoch 1400, val loss: 0.5636450052261353
Epoch 1410, training loss: 84.61820983886719 = 0.4658755362033844 + 10.0 * 8.415233612060547
Epoch 1410, val loss: 0.5631332993507385
Epoch 1420, training loss: 84.69119262695312 = 0.4645976722240448 + 10.0 * 8.422658920288086
Epoch 1420, val loss: 0.5625887513160706
Epoch 1430, training loss: 84.6234130859375 = 0.4632979929447174 + 10.0 * 8.416011810302734
Epoch 1430, val loss: 0.562041163444519
Epoch 1440, training loss: 84.59854125976562 = 0.4620375335216522 + 10.0 * 8.413650512695312
Epoch 1440, val loss: 0.5614734292030334
Epoch 1450, training loss: 84.59051513671875 = 0.46078163385391235 + 10.0 * 8.412973403930664
Epoch 1450, val loss: 0.5609607696533203
Epoch 1460, training loss: 84.58534240722656 = 0.4595222771167755 + 10.0 * 8.412581443786621
Epoch 1460, val loss: 0.560434103012085
Epoch 1470, training loss: 84.5991439819336 = 0.4582582414150238 + 10.0 * 8.414088249206543
Epoch 1470, val loss: 0.5599340796470642
Epoch 1480, training loss: 84.58576202392578 = 0.45695430040359497 + 10.0 * 8.412880897521973
Epoch 1480, val loss: 0.559332013130188
Epoch 1490, training loss: 84.57354736328125 = 0.4556604325771332 + 10.0 * 8.411788940429688
Epoch 1490, val loss: 0.5587917566299438
Epoch 1500, training loss: 84.56318664550781 = 0.4543761909008026 + 10.0 * 8.410881042480469
Epoch 1500, val loss: 0.5582082867622375
Epoch 1510, training loss: 84.5581283569336 = 0.4530886113643646 + 10.0 * 8.410504341125488
Epoch 1510, val loss: 0.5577192306518555
Epoch 1520, training loss: 84.57075500488281 = 0.4517955780029297 + 10.0 * 8.411895751953125
Epoch 1520, val loss: 0.557137131690979
Epoch 1530, training loss: 84.56336212158203 = 0.4504687786102295 + 10.0 * 8.41128921508789
Epoch 1530, val loss: 0.5565805435180664
Epoch 1540, training loss: 84.5528335571289 = 0.4491415321826935 + 10.0 * 8.410368919372559
Epoch 1540, val loss: 0.5559779405593872
Epoch 1550, training loss: 84.54314422607422 = 0.4478244483470917 + 10.0 * 8.40953254699707
Epoch 1550, val loss: 0.5554496049880981
Epoch 1560, training loss: 84.55065155029297 = 0.4464928209781647 + 10.0 * 8.410415649414062
Epoch 1560, val loss: 0.5548527836799622
Epoch 1570, training loss: 84.5273666381836 = 0.4451519250869751 + 10.0 * 8.408221244812012
Epoch 1570, val loss: 0.5543765425682068
Epoch 1580, training loss: 84.5196762084961 = 0.4438219666481018 + 10.0 * 8.407585144042969
Epoch 1580, val loss: 0.5537784099578857
Epoch 1590, training loss: 84.51226043701172 = 0.4424784779548645 + 10.0 * 8.406977653503418
Epoch 1590, val loss: 0.5532522201538086
Epoch 1600, training loss: 84.56190490722656 = 0.44113513827323914 + 10.0 * 8.412076950073242
Epoch 1600, val loss: 0.5527021884918213
Epoch 1610, training loss: 84.5119857788086 = 0.4397376477718353 + 10.0 * 8.407224655151367
Epoch 1610, val loss: 0.5520715117454529
Epoch 1620, training loss: 84.50546264648438 = 0.4383712410926819 + 10.0 * 8.406709671020508
Epoch 1620, val loss: 0.5515056848526001
Epoch 1630, training loss: 84.49339294433594 = 0.4370053708553314 + 10.0 * 8.405638694763184
Epoch 1630, val loss: 0.5509440302848816
Epoch 1640, training loss: 84.5331039428711 = 0.4356428384780884 + 10.0 * 8.409746170043945
Epoch 1640, val loss: 0.5504195690155029
Epoch 1650, training loss: 84.49575805664062 = 0.43424129486083984 + 10.0 * 8.40615177154541
Epoch 1650, val loss: 0.5497625470161438
Epoch 1660, training loss: 84.47797393798828 = 0.43285897374153137 + 10.0 * 8.404511451721191
Epoch 1660, val loss: 0.5492276549339294
Epoch 1670, training loss: 84.46624755859375 = 0.43147996068000793 + 10.0 * 8.40347671508789
Epoch 1670, val loss: 0.5486491322517395
Epoch 1680, training loss: 84.4609375 = 0.43009528517723083 + 10.0 * 8.403084754943848
Epoch 1680, val loss: 0.5481355786323547
Epoch 1690, training loss: 84.54103088378906 = 0.4287089705467224 + 10.0 * 8.411231994628906
Epoch 1690, val loss: 0.5476263761520386
Epoch 1700, training loss: 84.47107696533203 = 0.42726361751556396 + 10.0 * 8.40438175201416
Epoch 1700, val loss: 0.5469858050346375
Epoch 1710, training loss: 84.45073699951172 = 0.42584115266799927 + 10.0 * 8.40248966217041
Epoch 1710, val loss: 0.5464679598808289
Epoch 1720, training loss: 84.44092559814453 = 0.4244265854358673 + 10.0 * 8.401650428771973
Epoch 1720, val loss: 0.5459211468696594
Epoch 1730, training loss: 84.43359375 = 0.42300891876220703 + 10.0 * 8.401058197021484
Epoch 1730, val loss: 0.5454283356666565
Epoch 1740, training loss: 84.42928314208984 = 0.42157915234565735 + 10.0 * 8.40077018737793
Epoch 1740, val loss: 0.5449146032333374
Epoch 1750, training loss: 84.47470092773438 = 0.4201483130455017 + 10.0 * 8.405454635620117
Epoch 1750, val loss: 0.5443394780158997
Epoch 1760, training loss: 84.45162200927734 = 0.41867825388908386 + 10.0 * 8.403294563293457
Epoch 1760, val loss: 0.5439639091491699
Epoch 1770, training loss: 84.42190551757812 = 0.41722917556762695 + 10.0 * 8.400467872619629
Epoch 1770, val loss: 0.5433521866798401
Epoch 1780, training loss: 84.4096450805664 = 0.4157802164554596 + 10.0 * 8.399386405944824
Epoch 1780, val loss: 0.5429069995880127
Epoch 1790, training loss: 84.41570281982422 = 0.41433241963386536 + 10.0 * 8.400136947631836
Epoch 1790, val loss: 0.5424407720565796
Epoch 1800, training loss: 84.44320678710938 = 0.4128689169883728 + 10.0 * 8.403033256530762
Epoch 1800, val loss: 0.5419539213180542
Epoch 1810, training loss: 84.40898895263672 = 0.41140371561050415 + 10.0 * 8.399758338928223
Epoch 1810, val loss: 0.5415449738502502
Epoch 1820, training loss: 84.38822937011719 = 0.4099459946155548 + 10.0 * 8.397829055786133
Epoch 1820, val loss: 0.541101336479187
Epoch 1830, training loss: 84.38674926757812 = 0.40848812460899353 + 10.0 * 8.397826194763184
Epoch 1830, val loss: 0.5406835675239563
Epoch 1840, training loss: 84.37937927246094 = 0.4070293605327606 + 10.0 * 8.397234916687012
Epoch 1840, val loss: 0.5402494668960571
Epoch 1850, training loss: 84.5135269165039 = 0.40556278824806213 + 10.0 * 8.410796165466309
Epoch 1850, val loss: 0.5397474765777588
Epoch 1860, training loss: 84.3848876953125 = 0.4040670096874237 + 10.0 * 8.39808177947998
Epoch 1860, val loss: 0.5394662618637085
Epoch 1870, training loss: 84.37947845458984 = 0.4025958776473999 + 10.0 * 8.397687911987305
Epoch 1870, val loss: 0.5390835404396057
Epoch 1880, training loss: 84.361083984375 = 0.4011341333389282 + 10.0 * 8.395995140075684
Epoch 1880, val loss: 0.5387284755706787
Epoch 1890, training loss: 84.35604095458984 = 0.39968106150627136 + 10.0 * 8.395635604858398
Epoch 1890, val loss: 0.5383377075195312
Epoch 1900, training loss: 84.36085510253906 = 0.39822477102279663 + 10.0 * 8.396263122558594
Epoch 1900, val loss: 0.537993848323822
Epoch 1910, training loss: 84.42220306396484 = 0.396751344203949 + 10.0 * 8.402544975280762
Epoch 1910, val loss: 0.5376206040382385
Epoch 1920, training loss: 84.3647689819336 = 0.3952730596065521 + 10.0 * 8.396949768066406
Epoch 1920, val loss: 0.5372600555419922
Epoch 1930, training loss: 84.3437271118164 = 0.3938034772872925 + 10.0 * 8.394991874694824
Epoch 1930, val loss: 0.5369126200675964
Epoch 1940, training loss: 84.3359146118164 = 0.3923510015010834 + 10.0 * 8.394356727600098
Epoch 1940, val loss: 0.5366141200065613
Epoch 1950, training loss: 84.33040618896484 = 0.3908957540988922 + 10.0 * 8.393951416015625
Epoch 1950, val loss: 0.5363123416900635
Epoch 1960, training loss: 84.32427215576172 = 0.38944461941719055 + 10.0 * 8.39348316192627
Epoch 1960, val loss: 0.5360232591629028
Epoch 1970, training loss: 84.3208999633789 = 0.38799360394477844 + 10.0 * 8.393290519714355
Epoch 1970, val loss: 0.535778284072876
Epoch 1980, training loss: 84.32791137695312 = 0.38655057549476624 + 10.0 * 8.394136428833008
Epoch 1980, val loss: 0.5355434417724609
Epoch 1990, training loss: 84.37777709960938 = 0.3850926458835602 + 10.0 * 8.39926815032959
Epoch 1990, val loss: 0.535269558429718
Epoch 2000, training loss: 84.34676361083984 = 0.3836109936237335 + 10.0 * 8.39631462097168
Epoch 2000, val loss: 0.5350397825241089
Epoch 2010, training loss: 84.31221008300781 = 0.38215696811676025 + 10.0 * 8.39300537109375
Epoch 2010, val loss: 0.5347335934638977
Epoch 2020, training loss: 84.30538177490234 = 0.3807043731212616 + 10.0 * 8.392467498779297
Epoch 2020, val loss: 0.5346647500991821
Epoch 2030, training loss: 84.29792022705078 = 0.37926265597343445 + 10.0 * 8.391865730285645
Epoch 2030, val loss: 0.5344839692115784
Epoch 2040, training loss: 84.29206848144531 = 0.377816766500473 + 10.0 * 8.391425132751465
Epoch 2040, val loss: 0.5344192981719971
Epoch 2050, training loss: 84.28816223144531 = 0.37637266516685486 + 10.0 * 8.391179084777832
Epoch 2050, val loss: 0.5343039631843567
Epoch 2060, training loss: 84.29866027832031 = 0.37492701411247253 + 10.0 * 8.392374038696289
Epoch 2060, val loss: 0.5342380404472351
Epoch 2070, training loss: 84.2885971069336 = 0.37346649169921875 + 10.0 * 8.391512870788574
Epoch 2070, val loss: 0.534101665019989
Epoch 2080, training loss: 84.30327606201172 = 0.37201201915740967 + 10.0 * 8.393126487731934
Epoch 2080, val loss: 0.5339754223823547
Epoch 2090, training loss: 84.28948974609375 = 0.3705628514289856 + 10.0 * 8.39189338684082
Epoch 2090, val loss: 0.5338501334190369
Epoch 2100, training loss: 84.2730941772461 = 0.3691142201423645 + 10.0 * 8.390398025512695
Epoch 2100, val loss: 0.533820390701294
Epoch 2110, training loss: 84.272216796875 = 0.36767706274986267 + 10.0 * 8.390454292297363
Epoch 2110, val loss: 0.5337523818016052
Epoch 2120, training loss: 84.27433776855469 = 0.3662417232990265 + 10.0 * 8.390810012817383
Epoch 2120, val loss: 0.5336976647377014
Epoch 2130, training loss: 84.29485321044922 = 0.3648081123828888 + 10.0 * 8.393004417419434
Epoch 2130, val loss: 0.53360515832901
Epoch 2140, training loss: 84.28881072998047 = 0.36337393522262573 + 10.0 * 8.39254379272461
Epoch 2140, val loss: 0.5335695743560791
Epoch 2150, training loss: 84.26138305664062 = 0.3619384169578552 + 10.0 * 8.389944076538086
Epoch 2150, val loss: 0.5336296558380127
Epoch 2160, training loss: 84.25044250488281 = 0.36051008105278015 + 10.0 * 8.388993263244629
Epoch 2160, val loss: 0.5336043834686279
Epoch 2170, training loss: 84.24494934082031 = 0.35908710956573486 + 10.0 * 8.388586044311523
Epoch 2170, val loss: 0.5336546301841736
Epoch 2180, training loss: 84.24664306640625 = 0.35766729712486267 + 10.0 * 8.388897895812988
Epoch 2180, val loss: 0.5337250828742981
Epoch 2190, training loss: 84.29582214355469 = 0.3562363386154175 + 10.0 * 8.39395809173584
Epoch 2190, val loss: 0.5337430834770203
Epoch 2200, training loss: 84.24510955810547 = 0.3548089563846588 + 10.0 * 8.389029502868652
Epoch 2200, val loss: 0.5337576866149902
Epoch 2210, training loss: 84.23829650878906 = 0.3533950448036194 + 10.0 * 8.388490676879883
Epoch 2210, val loss: 0.5338263511657715
Epoch 2220, training loss: 84.22881317138672 = 0.35198524594306946 + 10.0 * 8.387682914733887
Epoch 2220, val loss: 0.5339003801345825
Epoch 2230, training loss: 84.22765350341797 = 0.3505760133266449 + 10.0 * 8.387707710266113
Epoch 2230, val loss: 0.5339948534965515
Epoch 2240, training loss: 84.24593353271484 = 0.3491749167442322 + 10.0 * 8.389676094055176
Epoch 2240, val loss: 0.5341348052024841
Epoch 2250, training loss: 84.21916198730469 = 0.3477664887905121 + 10.0 * 8.387140274047852
Epoch 2250, val loss: 0.5342052578926086
Epoch 2260, training loss: 84.22293853759766 = 0.34636828303337097 + 10.0 * 8.387657165527344
Epoch 2260, val loss: 0.5342922806739807
Epoch 2270, training loss: 84.31722259521484 = 0.3449816107749939 + 10.0 * 8.397224426269531
Epoch 2270, val loss: 0.5345090627670288
Epoch 2280, training loss: 84.24534606933594 = 0.3435538709163666 + 10.0 * 8.390179634094238
Epoch 2280, val loss: 0.5344770550727844
Epoch 2290, training loss: 84.21163177490234 = 0.342155784368515 + 10.0 * 8.386947631835938
Epoch 2290, val loss: 0.5347256064414978
Epoch 2300, training loss: 84.20169067382812 = 0.34076017141342163 + 10.0 * 8.386093139648438
Epoch 2300, val loss: 0.5347962975502014
Epoch 2310, training loss: 84.19857788085938 = 0.3393602669239044 + 10.0 * 8.385921478271484
Epoch 2310, val loss: 0.5349684357643127
Epoch 2320, training loss: 84.19477081298828 = 0.33796173334121704 + 10.0 * 8.38568115234375
Epoch 2320, val loss: 0.5351271033287048
Epoch 2330, training loss: 84.19254302978516 = 0.3365631699562073 + 10.0 * 8.385598182678223
Epoch 2330, val loss: 0.535353422164917
Epoch 2340, training loss: 84.22982025146484 = 0.3351669907569885 + 10.0 * 8.38946533203125
Epoch 2340, val loss: 0.5356172919273376
Epoch 2350, training loss: 84.1952133178711 = 0.3337637186050415 + 10.0 * 8.386144638061523
Epoch 2350, val loss: 0.5356934666633606
Epoch 2360, training loss: 84.18697357177734 = 0.33235618472099304 + 10.0 * 8.385461807250977
Epoch 2360, val loss: 0.5359973311424255
Epoch 2370, training loss: 84.18099212646484 = 0.3309686779975891 + 10.0 * 8.385002136230469
Epoch 2370, val loss: 0.5361440777778625
Epoch 2380, training loss: 84.17701721191406 = 0.3295779228210449 + 10.0 * 8.384744644165039
Epoch 2380, val loss: 0.5364276766777039
Epoch 2390, training loss: 84.1873779296875 = 0.3281886577606201 + 10.0 * 8.385919570922852
Epoch 2390, val loss: 0.5367364287376404
Epoch 2400, training loss: 84.19290924072266 = 0.3267876207828522 + 10.0 * 8.386611938476562
Epoch 2400, val loss: 0.5370339751243591
Epoch 2410, training loss: 84.17964935302734 = 0.325373113155365 + 10.0 * 8.385427474975586
Epoch 2410, val loss: 0.5372592806816101
Epoch 2420, training loss: 84.16482543945312 = 0.3239613473415375 + 10.0 * 8.384086608886719
Epoch 2420, val loss: 0.5375707149505615
Epoch 2430, training loss: 84.16191101074219 = 0.32255658507347107 + 10.0 * 8.383935928344727
Epoch 2430, val loss: 0.5378353595733643
Epoch 2440, training loss: 84.17796325683594 = 0.32114917039871216 + 10.0 * 8.38568115234375
Epoch 2440, val loss: 0.5381633043289185
Epoch 2450, training loss: 84.1762466430664 = 0.3197314441204071 + 10.0 * 8.385651588439941
Epoch 2450, val loss: 0.5384473204612732
Epoch 2460, training loss: 84.15959167480469 = 0.31831520795822144 + 10.0 * 8.384127616882324
Epoch 2460, val loss: 0.5388285517692566
Epoch 2470, training loss: 84.14957427978516 = 0.31689924001693726 + 10.0 * 8.383267402648926
Epoch 2470, val loss: 0.539176881313324
Epoch 2480, training loss: 84.14735412597656 = 0.3154848515987396 + 10.0 * 8.383187294006348
Epoch 2480, val loss: 0.5395839810371399
Epoch 2490, training loss: 84.17317199707031 = 0.3140775263309479 + 10.0 * 8.385909080505371
Epoch 2490, val loss: 0.5399926900863647
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7945205479452054
0.8120698398898791
The final CL Acc:0.79435, 0.00104, The final GNN Acc:0.81323, 0.00134
raw graph: torch.Size([2, 88648])
add edge: torch.Size([2, 110788])
remove edge: torch.Size([2, 66582])
updated graph: torch.Size([2, 88722])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 106.92667388916016 = 1.1037228107452393 + 10.0 * 10.582295417785645
Epoch 0, val loss: 1.1030468940734863
Epoch 10, training loss: 106.91878509521484 = 1.098838448524475 + 10.0 * 10.581995010375977
Epoch 10, val loss: 1.0981649160385132
Epoch 20, training loss: 106.89784240722656 = 1.0936354398727417 + 10.0 * 10.58042049407959
Epoch 20, val loss: 1.0929450988769531
Epoch 30, training loss: 106.81398010253906 = 1.0878461599349976 + 10.0 * 10.572613716125488
Epoch 30, val loss: 1.0871251821517944
Epoch 40, training loss: 106.49090576171875 = 1.081426978111267 + 10.0 * 10.540947914123535
Epoch 40, val loss: 1.0806760787963867
Epoch 50, training loss: 105.58477783203125 = 1.0744004249572754 + 10.0 * 10.451037406921387
Epoch 50, val loss: 1.0736342668533325
Epoch 60, training loss: 103.84617614746094 = 1.067229151725769 + 10.0 * 10.277894973754883
Epoch 60, val loss: 1.066467523574829
Epoch 70, training loss: 101.81201934814453 = 1.0598286390304565 + 10.0 * 10.07521915435791
Epoch 70, val loss: 1.0590600967407227
Epoch 80, training loss: 98.4169921875 = 1.0522319078445435 + 10.0 * 9.736475944519043
Epoch 80, val loss: 1.0514450073242188
Epoch 90, training loss: 97.07373809814453 = 1.0448505878448486 + 10.0 * 9.602888107299805
Epoch 90, val loss: 1.044180989265442
Epoch 100, training loss: 96.2166748046875 = 1.03786039352417 + 10.0 * 9.517881393432617
Epoch 100, val loss: 1.037348985671997
Epoch 110, training loss: 95.27838897705078 = 1.0311135053634644 + 10.0 * 9.424727439880371
Epoch 110, val loss: 1.0307682752609253
Epoch 120, training loss: 94.11804962158203 = 1.0245643854141235 + 10.0 * 9.309348106384277
Epoch 120, val loss: 1.0243006944656372
Epoch 130, training loss: 93.11285400390625 = 1.0179824829101562 + 10.0 * 9.209486961364746
Epoch 130, val loss: 1.0177924633026123
Epoch 140, training loss: 92.49745178222656 = 1.0115976333618164 + 10.0 * 9.148585319519043
Epoch 140, val loss: 1.0115073919296265
Epoch 150, training loss: 92.06096649169922 = 1.0056257247924805 + 10.0 * 9.105534553527832
Epoch 150, val loss: 1.005630373954773
Epoch 160, training loss: 91.5091781616211 = 1.0001004934310913 + 10.0 * 9.050908088684082
Epoch 160, val loss: 1.0002813339233398
Epoch 170, training loss: 90.86670684814453 = 0.99534010887146 + 10.0 * 8.987136840820312
Epoch 170, val loss: 0.9957187175750732
Epoch 180, training loss: 90.39521789550781 = 0.990700364112854 + 10.0 * 8.940451622009277
Epoch 180, val loss: 0.9911535382270813
Epoch 190, training loss: 89.97147369384766 = 0.984822154045105 + 10.0 * 8.898664474487305
Epoch 190, val loss: 0.9852566123008728
Epoch 200, training loss: 89.42262268066406 = 0.9781942367553711 + 10.0 * 8.844442367553711
Epoch 200, val loss: 0.9786960482597351
Epoch 210, training loss: 88.90946197509766 = 0.9715684652328491 + 10.0 * 8.793789863586426
Epoch 210, val loss: 0.9721689820289612
Epoch 220, training loss: 88.56977081298828 = 0.9644411206245422 + 10.0 * 8.760533332824707
Epoch 220, val loss: 0.9651066064834595
Epoch 230, training loss: 88.39462280273438 = 0.9561194777488708 + 10.0 * 8.743850708007812
Epoch 230, val loss: 0.9567965269088745
Epoch 240, training loss: 88.27117919921875 = 0.9467248916625977 + 10.0 * 8.73244571685791
Epoch 240, val loss: 0.9475235939025879
Epoch 250, training loss: 88.15470123291016 = 0.9366833567619324 + 10.0 * 8.7218017578125
Epoch 250, val loss: 0.9376416802406311
Epoch 260, training loss: 88.0339584350586 = 0.9262349605560303 + 10.0 * 8.710772514343262
Epoch 260, val loss: 0.92739337682724
Epoch 270, training loss: 87.89527130126953 = 0.9155243039131165 + 10.0 * 8.69797420501709
Epoch 270, val loss: 0.9169622659683228
Epoch 280, training loss: 87.73157501220703 = 0.9046469330787659 + 10.0 * 8.682692527770996
Epoch 280, val loss: 0.9063403010368347
Epoch 290, training loss: 87.55320739746094 = 0.8936318755149841 + 10.0 * 8.6659574508667
Epoch 290, val loss: 0.8956407904624939
Epoch 300, training loss: 87.3673324584961 = 0.8821924328804016 + 10.0 * 8.648513793945312
Epoch 300, val loss: 0.8845316171646118
Epoch 310, training loss: 87.21797180175781 = 0.8700426816940308 + 10.0 * 8.634793281555176
Epoch 310, val loss: 0.8726752996444702
Epoch 320, training loss: 87.07046508789062 = 0.8580843210220337 + 10.0 * 8.621237754821777
Epoch 320, val loss: 0.8611553311347961
Epoch 330, training loss: 86.94707489013672 = 0.8457077741622925 + 10.0 * 8.610136032104492
Epoch 330, val loss: 0.8491368293762207
Epoch 340, training loss: 86.84213256835938 = 0.8331397771835327 + 10.0 * 8.600899696350098
Epoch 340, val loss: 0.8370932340621948
Epoch 350, training loss: 86.75382232666016 = 0.820283055305481 + 10.0 * 8.593354225158691
Epoch 350, val loss: 0.8247483968734741
Epoch 360, training loss: 86.67366027832031 = 0.8071773648262024 + 10.0 * 8.586648941040039
Epoch 360, val loss: 0.8122722506523132
Epoch 370, training loss: 86.59571075439453 = 0.7940802574157715 + 10.0 * 8.58016300201416
Epoch 370, val loss: 0.7998566627502441
Epoch 380, training loss: 86.5093765258789 = 0.7811377048492432 + 10.0 * 8.572824478149414
Epoch 380, val loss: 0.7876662015914917
Epoch 390, training loss: 86.42443084716797 = 0.7683325409889221 + 10.0 * 8.5656099319458
Epoch 390, val loss: 0.7756329774856567
Epoch 400, training loss: 86.3482894897461 = 0.7556347846984863 + 10.0 * 8.55926513671875
Epoch 400, val loss: 0.7637433409690857
Epoch 410, training loss: 86.26476287841797 = 0.7430621981620789 + 10.0 * 8.552169799804688
Epoch 410, val loss: 0.7520183324813843
Epoch 420, training loss: 86.17777252197266 = 0.7306495904922485 + 10.0 * 8.54471206665039
Epoch 420, val loss: 0.7403990030288696
Epoch 430, training loss: 86.10917663574219 = 0.7184381484985352 + 10.0 * 8.539073944091797
Epoch 430, val loss: 0.7290219068527222
Epoch 440, training loss: 86.0236587524414 = 0.706394612789154 + 10.0 * 8.531725883483887
Epoch 440, val loss: 0.7178672552108765
Epoch 450, training loss: 85.97066497802734 = 0.6946520209312439 + 10.0 * 8.52760124206543
Epoch 450, val loss: 0.7070650458335876
Epoch 460, training loss: 85.88833618164062 = 0.6832249760627747 + 10.0 * 8.52051067352295
Epoch 460, val loss: 0.6964259743690491
Epoch 470, training loss: 85.80470275878906 = 0.6722409725189209 + 10.0 * 8.513246536254883
Epoch 470, val loss: 0.6863141655921936
Epoch 480, training loss: 85.74044799804688 = 0.6616441011428833 + 10.0 * 8.507880210876465
Epoch 480, val loss: 0.6765804886817932
Epoch 490, training loss: 85.72374725341797 = 0.6513441801071167 + 10.0 * 8.507240295410156
Epoch 490, val loss: 0.6671906113624573
Epoch 500, training loss: 85.62042236328125 = 0.6413097381591797 + 10.0 * 8.49791145324707
Epoch 500, val loss: 0.6579769849777222
Epoch 510, training loss: 85.5693130493164 = 0.6317560076713562 + 10.0 * 8.493755340576172
Epoch 510, val loss: 0.6492952108383179
Epoch 520, training loss: 85.51244354248047 = 0.6225627660751343 + 10.0 * 8.488987922668457
Epoch 520, val loss: 0.6409181952476501
Epoch 530, training loss: 85.46216583251953 = 0.613694965839386 + 10.0 * 8.484847068786621
Epoch 530, val loss: 0.6328944563865662
Epoch 540, training loss: 85.43769836425781 = 0.6051475405693054 + 10.0 * 8.483255386352539
Epoch 540, val loss: 0.6252318024635315
Epoch 550, training loss: 85.38735961914062 = 0.5968978404998779 + 10.0 * 8.479045867919922
Epoch 550, val loss: 0.6177946329116821
Epoch 560, training loss: 85.3377456665039 = 0.5890474915504456 + 10.0 * 8.474869728088379
Epoch 560, val loss: 0.6108478307723999
Epoch 570, training loss: 85.29972076416016 = 0.581536054611206 + 10.0 * 8.471818923950195
Epoch 570, val loss: 0.6041655540466309
Epoch 580, training loss: 85.26863861083984 = 0.5743150115013123 + 10.0 * 8.46943187713623
Epoch 580, val loss: 0.5977843999862671
Epoch 590, training loss: 85.23312377929688 = 0.5674137473106384 + 10.0 * 8.466570854187012
Epoch 590, val loss: 0.5917758345603943
Epoch 600, training loss: 85.198486328125 = 0.5608146786689758 + 10.0 * 8.463767051696777
Epoch 600, val loss: 0.5860465168952942
Epoch 610, training loss: 85.23818969726562 = 0.5544611811637878 + 10.0 * 8.46837329864502
Epoch 610, val loss: 0.5805417895317078
Epoch 620, training loss: 85.14911651611328 = 0.5483603477478027 + 10.0 * 8.460075378417969
Epoch 620, val loss: 0.5753103494644165
Epoch 630, training loss: 85.1091079711914 = 0.5426222681999207 + 10.0 * 8.456647872924805
Epoch 630, val loss: 0.5704237222671509
Epoch 640, training loss: 85.07755279541016 = 0.537115216255188 + 10.0 * 8.4540433883667
Epoch 640, val loss: 0.5657105445861816
Epoch 650, training loss: 85.04669952392578 = 0.531840980052948 + 10.0 * 8.451486587524414
Epoch 650, val loss: 0.5612640976905823
Epoch 660, training loss: 85.01868438720703 = 0.5267866849899292 + 10.0 * 8.449190139770508
Epoch 660, val loss: 0.5570457577705383
Epoch 670, training loss: 85.0483627319336 = 0.5219221711158752 + 10.0 * 8.452644348144531
Epoch 670, val loss: 0.5529962778091431
Epoch 680, training loss: 84.96947479248047 = 0.5171785354614258 + 10.0 * 8.445229530334473
Epoch 680, val loss: 0.5491172671318054
Epoch 690, training loss: 84.94734191894531 = 0.5127153992652893 + 10.0 * 8.443462371826172
Epoch 690, val loss: 0.5455151200294495
Epoch 700, training loss: 84.91799926757812 = 0.5084342956542969 + 10.0 * 8.440957069396973
Epoch 700, val loss: 0.5420348048210144
Epoch 710, training loss: 84.89111328125 = 0.5043209791183472 + 10.0 * 8.438679695129395
Epoch 710, val loss: 0.5387856960296631
Epoch 720, training loss: 84.90553283691406 = 0.5003556609153748 + 10.0 * 8.44051742553711
Epoch 720, val loss: 0.5356959104537964
Epoch 730, training loss: 84.87619018554688 = 0.4964742660522461 + 10.0 * 8.437971115112305
Epoch 730, val loss: 0.5325837731361389
Epoch 740, training loss: 84.83277130126953 = 0.4928256869316101 + 10.0 * 8.43399429321289
Epoch 740, val loss: 0.5298612117767334
Epoch 750, training loss: 84.80573272705078 = 0.4893395006656647 + 10.0 * 8.431638717651367
Epoch 750, val loss: 0.5271790027618408
Epoch 760, training loss: 84.7838134765625 = 0.48598095774650574 + 10.0 * 8.429783821105957
Epoch 760, val loss: 0.5246750116348267
Epoch 770, training loss: 84.76298522949219 = 0.4827466905117035 + 10.0 * 8.428023338317871
Epoch 770, val loss: 0.5222970247268677
Epoch 780, training loss: 84.74479675292969 = 0.47962504625320435 + 10.0 * 8.426517486572266
Epoch 780, val loss: 0.520001232624054
Epoch 790, training loss: 84.783935546875 = 0.47659724950790405 + 10.0 * 8.430733680725098
Epoch 790, val loss: 0.5178071856498718
Epoch 800, training loss: 84.72779846191406 = 0.4736561179161072 + 10.0 * 8.425414085388184
Epoch 800, val loss: 0.5156946182250977
Epoch 810, training loss: 84.69905853271484 = 0.4708653688430786 + 10.0 * 8.422819137573242
Epoch 810, val loss: 0.5137396454811096
Epoch 820, training loss: 84.67808532714844 = 0.4681739807128906 + 10.0 * 8.420991897583008
Epoch 820, val loss: 0.5118432641029358
Epoch 830, training loss: 84.6619873046875 = 0.46557775139808655 + 10.0 * 8.419641494750977
Epoch 830, val loss: 0.5100683569908142
Epoch 840, training loss: 84.68619537353516 = 0.463063508272171 + 10.0 * 8.42231273651123
Epoch 840, val loss: 0.5083420276641846
Epoch 850, training loss: 84.65007019042969 = 0.46059855818748474 + 10.0 * 8.418947219848633
Epoch 850, val loss: 0.5067504644393921
Epoch 860, training loss: 84.62013244628906 = 0.45826131105422974 + 10.0 * 8.416187286376953
Epoch 860, val loss: 0.5051838159561157
Epoch 870, training loss: 84.60558319091797 = 0.45599839091300964 + 10.0 * 8.414958000183105
Epoch 870, val loss: 0.5036474466323853
Epoch 880, training loss: 84.63085174560547 = 0.45379745960235596 + 10.0 * 8.417705535888672
Epoch 880, val loss: 0.5022348761558533
Epoch 890, training loss: 84.5924301147461 = 0.45164987444877625 + 10.0 * 8.414077758789062
Epoch 890, val loss: 0.5008371472358704
Epoch 900, training loss: 84.5677719116211 = 0.44959399104118347 + 10.0 * 8.41181755065918
Epoch 900, val loss: 0.4995480477809906
Epoch 910, training loss: 84.55159759521484 = 0.4476032555103302 + 10.0 * 8.410399436950684
Epoch 910, val loss: 0.4982373118400574
Epoch 920, training loss: 84.54078674316406 = 0.44567281007766724 + 10.0 * 8.40951156616211
Epoch 920, val loss: 0.4970790147781372
Epoch 930, training loss: 84.58848571777344 = 0.44377729296684265 + 10.0 * 8.414470672607422
Epoch 930, val loss: 0.49585282802581787
Epoch 940, training loss: 84.51726531982422 = 0.44191673398017883 + 10.0 * 8.4075345993042
Epoch 940, val loss: 0.4947255849838257
Epoch 950, training loss: 84.50680541992188 = 0.44014325737953186 + 10.0 * 8.40666675567627
Epoch 950, val loss: 0.4935990273952484
Epoch 960, training loss: 84.49077606201172 = 0.438422292470932 + 10.0 * 8.405235290527344
Epoch 960, val loss: 0.49254152178764343
Epoch 970, training loss: 84.47789764404297 = 0.4367484450340271 + 10.0 * 8.404115676879883
Epoch 970, val loss: 0.49153947830200195
Epoch 980, training loss: 84.47541809082031 = 0.43510961532592773 + 10.0 * 8.404030799865723
Epoch 980, val loss: 0.4905204176902771
Epoch 990, training loss: 84.4629898071289 = 0.4334813356399536 + 10.0 * 8.40295124053955
Epoch 990, val loss: 0.489533007144928
Epoch 1000, training loss: 84.45316314697266 = 0.4318944811820984 + 10.0 * 8.402127265930176
Epoch 1000, val loss: 0.48860523104667664
Epoch 1010, training loss: 84.43120574951172 = 0.4303566813468933 + 10.0 * 8.400084495544434
Epoch 1010, val loss: 0.4876616597175598
Epoch 1020, training loss: 84.42206573486328 = 0.4288594126701355 + 10.0 * 8.399320602416992
Epoch 1020, val loss: 0.48678433895111084
Epoch 1030, training loss: 84.41956329345703 = 0.4273945689201355 + 10.0 * 8.39921760559082
Epoch 1030, val loss: 0.48589417338371277
Epoch 1040, training loss: 84.40956115722656 = 0.42593181133270264 + 10.0 * 8.39836311340332
Epoch 1040, val loss: 0.4850369691848755
Epoch 1050, training loss: 84.40144348144531 = 0.4245087504386902 + 10.0 * 8.397693634033203
Epoch 1050, val loss: 0.4842148423194885
Epoch 1060, training loss: 84.38634490966797 = 0.42311498522758484 + 10.0 * 8.396322250366211
Epoch 1060, val loss: 0.48337534070014954
Epoch 1070, training loss: 84.37946319580078 = 0.42174872756004333 + 10.0 * 8.395771980285645
Epoch 1070, val loss: 0.4825867712497711
Epoch 1080, training loss: 84.36209869384766 = 0.4204072654247284 + 10.0 * 8.394168853759766
Epoch 1080, val loss: 0.48180142045021057
Epoch 1090, training loss: 84.36922454833984 = 0.419089674949646 + 10.0 * 8.395013809204102
Epoch 1090, val loss: 0.4810296893119812
Epoch 1100, training loss: 84.35176849365234 = 0.4177561402320862 + 10.0 * 8.393401145935059
Epoch 1100, val loss: 0.4802236258983612
Epoch 1110, training loss: 84.35003662109375 = 0.4164610803127289 + 10.0 * 8.39335823059082
Epoch 1110, val loss: 0.47953546047210693
Epoch 1120, training loss: 84.32942962646484 = 0.4152001738548279 + 10.0 * 8.391423225402832
Epoch 1120, val loss: 0.4787214398384094
Epoch 1130, training loss: 84.3280258178711 = 0.41396304965019226 + 10.0 * 8.391406059265137
Epoch 1130, val loss: 0.47803547978401184
Epoch 1140, training loss: 84.33618927001953 = 0.41273462772369385 + 10.0 * 8.392345428466797
Epoch 1140, val loss: 0.47728803753852844
Epoch 1150, training loss: 84.3141860961914 = 0.41152241826057434 + 10.0 * 8.390266418457031
Epoch 1150, val loss: 0.476652055978775
Epoch 1160, training loss: 84.30154418945312 = 0.4103294014930725 + 10.0 * 8.389121055603027
Epoch 1160, val loss: 0.4759258031845093
Epoch 1170, training loss: 84.29022216796875 = 0.40915876626968384 + 10.0 * 8.388106346130371
Epoch 1170, val loss: 0.4752839505672455
Epoch 1180, training loss: 84.30448913574219 = 0.40799999237060547 + 10.0 * 8.389649391174316
Epoch 1180, val loss: 0.47461745142936707
Epoch 1190, training loss: 84.28831481933594 = 0.4068416953086853 + 10.0 * 8.388147354125977
Epoch 1190, val loss: 0.47394031286239624
Epoch 1200, training loss: 84.27798461914062 = 0.4057020843029022 + 10.0 * 8.387228012084961
Epoch 1200, val loss: 0.47331148386001587
Epoch 1210, training loss: 84.26087188720703 = 0.40458935499191284 + 10.0 * 8.385628700256348
Epoch 1210, val loss: 0.472695529460907
Epoch 1220, training loss: 84.25261688232422 = 0.4034951627254486 + 10.0 * 8.384912490844727
Epoch 1220, val loss: 0.4720723628997803
Epoch 1230, training loss: 84.30347442626953 = 0.402412474155426 + 10.0 * 8.390106201171875
Epoch 1230, val loss: 0.47149229049682617
Epoch 1240, training loss: 84.27079010009766 = 0.40130287408828735 + 10.0 * 8.386948585510254
Epoch 1240, val loss: 0.4708230495452881
Epoch 1250, training loss: 84.23292541503906 = 0.4002349078655243 + 10.0 * 8.383268356323242
Epoch 1250, val loss: 0.4702422618865967
Epoch 1260, training loss: 84.22341918945312 = 0.3991924524307251 + 10.0 * 8.38242244720459
Epoch 1260, val loss: 0.46966034173965454
Epoch 1270, training loss: 84.21587371826172 = 0.3981684148311615 + 10.0 * 8.381770133972168
Epoch 1270, val loss: 0.4690708518028259
Epoch 1280, training loss: 84.207763671875 = 0.3971538245677948 + 10.0 * 8.381060600280762
Epoch 1280, val loss: 0.46854108572006226
Epoch 1290, training loss: 84.27922821044922 = 0.3961437940597534 + 10.0 * 8.38830852508545
Epoch 1290, val loss: 0.46794313192367554
Epoch 1300, training loss: 84.20354461669922 = 0.39511892199516296 + 10.0 * 8.380842208862305
Epoch 1300, val loss: 0.46742284297943115
Epoch 1310, training loss: 84.19432067871094 = 0.39412596821784973 + 10.0 * 8.380019187927246
Epoch 1310, val loss: 0.46687236428260803
Epoch 1320, training loss: 84.18262481689453 = 0.3931502401828766 + 10.0 * 8.378947257995605
Epoch 1320, val loss: 0.46631747484207153
Epoch 1330, training loss: 84.17256927490234 = 0.3921895921230316 + 10.0 * 8.37803840637207
Epoch 1330, val loss: 0.46581020951271057
Epoch 1340, training loss: 84.19520568847656 = 0.39123332500457764 + 10.0 * 8.380396842956543
Epoch 1340, val loss: 0.46527302265167236
Epoch 1350, training loss: 84.19556427001953 = 0.3902635872364044 + 10.0 * 8.380529403686523
Epoch 1350, val loss: 0.4647635221481323
Epoch 1360, training loss: 84.15762329101562 = 0.3893049359321594 + 10.0 * 8.376832008361816
Epoch 1360, val loss: 0.46424680948257446
Epoch 1370, training loss: 84.14813232421875 = 0.38837552070617676 + 10.0 * 8.375975608825684
Epoch 1370, val loss: 0.4637148380279541
Epoch 1380, training loss: 84.13817596435547 = 0.38745561242103577 + 10.0 * 8.37507152557373
Epoch 1380, val loss: 0.46326178312301636
Epoch 1390, training loss: 84.13130950927734 = 0.3865451514720917 + 10.0 * 8.374476432800293
Epoch 1390, val loss: 0.4627560079097748
Epoch 1400, training loss: 84.12948608398438 = 0.3856375515460968 + 10.0 * 8.374384880065918
Epoch 1400, val loss: 0.46227410435676575
Epoch 1410, training loss: 84.2013931274414 = 0.38471898436546326 + 10.0 * 8.381667137145996
Epoch 1410, val loss: 0.46172380447387695
Epoch 1420, training loss: 84.11813354492188 = 0.3837926685810089 + 10.0 * 8.373434066772461
Epoch 1420, val loss: 0.4613298773765564
Epoch 1430, training loss: 84.11415100097656 = 0.38288822770118713 + 10.0 * 8.373126029968262
Epoch 1430, val loss: 0.46076881885528564
Epoch 1440, training loss: 84.10681915283203 = 0.38199958205223083 + 10.0 * 8.372482299804688
Epoch 1440, val loss: 0.46034783124923706
Epoch 1450, training loss: 84.0983657836914 = 0.38112369179725647 + 10.0 * 8.371724128723145
Epoch 1450, val loss: 0.45984238386154175
Epoch 1460, training loss: 84.09330749511719 = 0.38025006651878357 + 10.0 * 8.371305465698242
Epoch 1460, val loss: 0.45941510796546936
Epoch 1470, training loss: 84.13943481445312 = 0.3793756365776062 + 10.0 * 8.376005172729492
Epoch 1470, val loss: 0.4588978886604309
Epoch 1480, training loss: 84.11434936523438 = 0.3784790635108948 + 10.0 * 8.373586654663086
Epoch 1480, val loss: 0.4584718644618988
Epoch 1490, training loss: 84.0782699584961 = 0.3776019513607025 + 10.0 * 8.37006664276123
Epoch 1490, val loss: 0.4579533636569977
Epoch 1500, training loss: 84.07710266113281 = 0.3767406940460205 + 10.0 * 8.370036125183105
Epoch 1500, val loss: 0.4575194716453552
Epoch 1510, training loss: 84.07008361816406 = 0.37588605284690857 + 10.0 * 8.369420051574707
Epoch 1510, val loss: 0.4570285677909851
Epoch 1520, training loss: 84.09918212890625 = 0.37502971291542053 + 10.0 * 8.372415542602539
Epoch 1520, val loss: 0.45658448338508606
Epoch 1530, training loss: 84.06752014160156 = 0.3741651773452759 + 10.0 * 8.369335174560547
Epoch 1530, val loss: 0.4561072587966919
Epoch 1540, training loss: 84.05882263183594 = 0.37330782413482666 + 10.0 * 8.368551254272461
Epoch 1540, val loss: 0.4556339383125305
Epoch 1550, training loss: 84.0509033203125 = 0.37246182560920715 + 10.0 * 8.367844581604004
Epoch 1550, val loss: 0.4551912844181061
Epoch 1560, training loss: 84.04615020751953 = 0.37161943316459656 + 10.0 * 8.367452621459961
Epoch 1560, val loss: 0.4547170102596283
Epoch 1570, training loss: 84.09083557128906 = 0.37077921628952026 + 10.0 * 8.372005462646484
Epoch 1570, val loss: 0.4543125033378601
Epoch 1580, training loss: 84.07780456542969 = 0.3699048161506653 + 10.0 * 8.370790481567383
Epoch 1580, val loss: 0.4537721872329712
Epoch 1590, training loss: 84.03605651855469 = 0.36904585361480713 + 10.0 * 8.366701126098633
Epoch 1590, val loss: 0.4533356726169586
Epoch 1600, training loss: 84.03004455566406 = 0.3682088255882263 + 10.0 * 8.366183280944824
Epoch 1600, val loss: 0.45285266637802124
Epoch 1610, training loss: 84.02529907226562 = 0.3673821687698364 + 10.0 * 8.365791320800781
Epoch 1610, val loss: 0.4524521827697754
Epoch 1620, training loss: 84.0190200805664 = 0.36655712127685547 + 10.0 * 8.365246772766113
Epoch 1620, val loss: 0.45199015736579895
Epoch 1630, training loss: 84.0144271850586 = 0.3657298684120178 + 10.0 * 8.364870071411133
Epoch 1630, val loss: 0.4515761137008667
Epoch 1640, training loss: 84.00990295410156 = 0.3649018704891205 + 10.0 * 8.364500045776367
Epoch 1640, val loss: 0.45112401247024536
Epoch 1650, training loss: 84.00601196289062 = 0.36407291889190674 + 10.0 * 8.3641939163208
Epoch 1650, val loss: 0.4507003128528595
Epoch 1660, training loss: 84.09332275390625 = 0.3632381856441498 + 10.0 * 8.373008728027344
Epoch 1660, val loss: 0.45024538040161133
Epoch 1670, training loss: 84.03034210205078 = 0.3623771071434021 + 10.0 * 8.366796493530273
Epoch 1670, val loss: 0.4498327374458313
Epoch 1680, training loss: 84.01119995117188 = 0.3615332543849945 + 10.0 * 8.36496639251709
Epoch 1680, val loss: 0.4493260681629181
Epoch 1690, training loss: 83.99409484863281 = 0.36070603132247925 + 10.0 * 8.3633394241333
Epoch 1690, val loss: 0.44891616702079773
Epoch 1700, training loss: 83.98497772216797 = 0.35988983511924744 + 10.0 * 8.362508773803711
Epoch 1700, val loss: 0.44850990176200867
Epoch 1710, training loss: 83.98070526123047 = 0.35907644033432007 + 10.0 * 8.362162590026855
Epoch 1710, val loss: 0.4481205940246582
Epoch 1720, training loss: 83.9765396118164 = 0.3582618534564972 + 10.0 * 8.361827850341797
Epoch 1720, val loss: 0.44769158959388733
Epoch 1730, training loss: 83.99090576171875 = 0.3574455976486206 + 10.0 * 8.363346099853516
Epoch 1730, val loss: 0.44729986786842346
Epoch 1740, training loss: 83.97064208984375 = 0.3565988540649414 + 10.0 * 8.361404418945312
Epoch 1740, val loss: 0.4467686712741852
Epoch 1750, training loss: 83.98817443847656 = 0.35575777292251587 + 10.0 * 8.363241195678711
Epoch 1750, val loss: 0.4463961720466614
Epoch 1760, training loss: 83.96309661865234 = 0.35493209958076477 + 10.0 * 8.36081600189209
Epoch 1760, val loss: 0.44593554735183716
Epoch 1770, training loss: 83.9583969116211 = 0.35411712527275085 + 10.0 * 8.360427856445312
Epoch 1770, val loss: 0.4455222189426422
Epoch 1780, training loss: 83.95237731933594 = 0.35330480337142944 + 10.0 * 8.359907150268555
Epoch 1780, val loss: 0.4451426863670349
Epoch 1790, training loss: 83.94821166992188 = 0.3524906039237976 + 10.0 * 8.35957145690918
Epoch 1790, val loss: 0.44471994042396545
Epoch 1800, training loss: 83.94415283203125 = 0.35167157649993896 + 10.0 * 8.359248161315918
Epoch 1800, val loss: 0.4443241357803345
Epoch 1810, training loss: 83.98143005371094 = 0.35084712505340576 + 10.0 * 8.363058090209961
Epoch 1810, val loss: 0.443882018327713
Epoch 1820, training loss: 83.97775268554688 = 0.35000452399253845 + 10.0 * 8.362774848937988
Epoch 1820, val loss: 0.4435045123100281
Epoch 1830, training loss: 83.9395980834961 = 0.34916505217552185 + 10.0 * 8.35904312133789
Epoch 1830, val loss: 0.4430885314941406
Epoch 1840, training loss: 83.93045043945312 = 0.3483421504497528 + 10.0 * 8.358210563659668
Epoch 1840, val loss: 0.44270041584968567
Epoch 1850, training loss: 83.92536163330078 = 0.34752658009529114 + 10.0 * 8.357783317565918
Epoch 1850, val loss: 0.4422556161880493
Epoch 1860, training loss: 83.9203872680664 = 0.3467142581939697 + 10.0 * 8.357366561889648
Epoch 1860, val loss: 0.44186580181121826
Epoch 1870, training loss: 83.91615295410156 = 0.34590020775794983 + 10.0 * 8.357025146484375
Epoch 1870, val loss: 0.4414430260658264
Epoch 1880, training loss: 83.92522430419922 = 0.3450815975666046 + 10.0 * 8.358014106750488
Epoch 1880, val loss: 0.4410631060600281
Epoch 1890, training loss: 83.91786193847656 = 0.3442513048648834 + 10.0 * 8.35736083984375
Epoch 1890, val loss: 0.4406101405620575
Epoch 1900, training loss: 83.9322738647461 = 0.34341883659362793 + 10.0 * 8.358884811401367
Epoch 1900, val loss: 0.4402535557746887
Epoch 1910, training loss: 83.90930938720703 = 0.3425852060317993 + 10.0 * 8.356672286987305
Epoch 1910, val loss: 0.4398496747016907
Epoch 1920, training loss: 83.90267181396484 = 0.3417564332485199 + 10.0 * 8.356091499328613
Epoch 1920, val loss: 0.4394329786300659
Epoch 1930, training loss: 83.89412689208984 = 0.3409327566623688 + 10.0 * 8.355319023132324
Epoch 1930, val loss: 0.43906813859939575
Epoch 1940, training loss: 83.8882064819336 = 0.3401099443435669 + 10.0 * 8.354809761047363
Epoch 1940, val loss: 0.438698410987854
Epoch 1950, training loss: 83.89148712158203 = 0.33928602933883667 + 10.0 * 8.355219841003418
Epoch 1950, val loss: 0.4382878243923187
Epoch 1960, training loss: 83.93511199951172 = 0.33845141530036926 + 10.0 * 8.359665870666504
Epoch 1960, val loss: 0.4378824234008789
Epoch 1970, training loss: 83.89649963378906 = 0.3376097083091736 + 10.0 * 8.355889320373535
Epoch 1970, val loss: 0.4375583827495575
Epoch 1980, training loss: 83.87555694580078 = 0.3367740213871002 + 10.0 * 8.353878021240234
Epoch 1980, val loss: 0.4371356666088104
Epoch 1990, training loss: 83.86901092529297 = 0.3359484076499939 + 10.0 * 8.353306770324707
Epoch 1990, val loss: 0.4367826282978058
Epoch 2000, training loss: 83.8756103515625 = 0.3351229429244995 + 10.0 * 8.354048728942871
Epoch 2000, val loss: 0.43640902638435364
Epoch 2010, training loss: 83.89033508300781 = 0.3342883586883545 + 10.0 * 8.35560417175293
Epoch 2010, val loss: 0.4360356330871582
Epoch 2020, training loss: 83.86555480957031 = 0.33345019817352295 + 10.0 * 8.35321044921875
Epoch 2020, val loss: 0.43565094470977783
Epoch 2030, training loss: 83.85443878173828 = 0.33261895179748535 + 10.0 * 8.352182388305664
Epoch 2030, val loss: 0.435275673866272
Epoch 2040, training loss: 83.84849548339844 = 0.33179333806037903 + 10.0 * 8.351670265197754
Epoch 2040, val loss: 0.43491554260253906
Epoch 2050, training loss: 83.84329986572266 = 0.33096829056739807 + 10.0 * 8.351232528686523
Epoch 2050, val loss: 0.4345580041408539
Epoch 2060, training loss: 83.83943939208984 = 0.33014115691185 + 10.0 * 8.350930213928223
Epoch 2060, val loss: 0.43419405817985535
Epoch 2070, training loss: 83.86420440673828 = 0.3293110132217407 + 10.0 * 8.353489875793457
Epoch 2070, val loss: 0.4338127672672272
Epoch 2080, training loss: 83.843017578125 = 0.3284631073474884 + 10.0 * 8.351455688476562
Epoch 2080, val loss: 0.4335038959980011
Epoch 2090, training loss: 83.84249877929688 = 0.32761991024017334 + 10.0 * 8.35148811340332
Epoch 2090, val loss: 0.43311187624931335
Epoch 2100, training loss: 83.82738494873047 = 0.32678624987602234 + 10.0 * 8.350059509277344
Epoch 2100, val loss: 0.4327411949634552
Epoch 2110, training loss: 83.81876373291016 = 0.32595571875572205 + 10.0 * 8.34928035736084
Epoch 2110, val loss: 0.4324313998222351
Epoch 2120, training loss: 83.81917572021484 = 0.3251250982284546 + 10.0 * 8.349405288696289
Epoch 2120, val loss: 0.43206897377967834
Epoch 2130, training loss: 83.88689422607422 = 0.324285626411438 + 10.0 * 8.356260299682617
Epoch 2130, val loss: 0.43170809745788574
Epoch 2140, training loss: 83.81392669677734 = 0.32343319058418274 + 10.0 * 8.34904956817627
Epoch 2140, val loss: 0.4313838481903076
Epoch 2150, training loss: 83.81208038330078 = 0.3225930631160736 + 10.0 * 8.34894847869873
Epoch 2150, val loss: 0.4310166835784912
Epoch 2160, training loss: 83.80111694335938 = 0.32175853848457336 + 10.0 * 8.347935676574707
Epoch 2160, val loss: 0.43069905042648315
Epoch 2170, training loss: 83.79744720458984 = 0.32092636823654175 + 10.0 * 8.347652435302734
Epoch 2170, val loss: 0.4303678870201111
Epoch 2180, training loss: 83.8319320678711 = 0.3200903832912445 + 10.0 * 8.351183891296387
Epoch 2180, val loss: 0.43000510334968567
Epoch 2190, training loss: 83.79349517822266 = 0.31923922896385193 + 10.0 * 8.34742546081543
Epoch 2190, val loss: 0.429701030254364
Epoch 2200, training loss: 83.79055786132812 = 0.31839147210121155 + 10.0 * 8.347216606140137
Epoch 2200, val loss: 0.4293288588523865
Epoch 2210, training loss: 83.78382110595703 = 0.3175535202026367 + 10.0 * 8.346627235412598
Epoch 2210, val loss: 0.42901086807250977
Epoch 2220, training loss: 83.77806854248047 = 0.31671667098999023 + 10.0 * 8.346135139465332
Epoch 2220, val loss: 0.4287246763706207
Epoch 2230, training loss: 83.77490234375 = 0.31588003039360046 + 10.0 * 8.345902442932129
Epoch 2230, val loss: 0.42839106917381287
Epoch 2240, training loss: 83.79446411132812 = 0.31504034996032715 + 10.0 * 8.347942352294922
Epoch 2240, val loss: 0.428120881319046
Epoch 2250, training loss: 83.76912689208984 = 0.3141832947731018 + 10.0 * 8.345494270324707
Epoch 2250, val loss: 0.4277747571468353
Epoch 2260, training loss: 83.77586364746094 = 0.3133251368999481 + 10.0 * 8.346254348754883
Epoch 2260, val loss: 0.42747294902801514
Epoch 2270, training loss: 83.76387786865234 = 0.31247392296791077 + 10.0 * 8.34514045715332
Epoch 2270, val loss: 0.42713412642478943
Epoch 2280, training loss: 83.76010131835938 = 0.3116256296634674 + 10.0 * 8.344847679138184
Epoch 2280, val loss: 0.42684951424598694
Epoch 2290, training loss: 83.80611419677734 = 0.3107757866382599 + 10.0 * 8.349534034729004
Epoch 2290, val loss: 0.42653971910476685
Epoch 2300, training loss: 83.75911712646484 = 0.3099110722541809 + 10.0 * 8.34492015838623
Epoch 2300, val loss: 0.4262557625770569
Epoch 2310, training loss: 83.74978637695312 = 0.30904874205589294 + 10.0 * 8.344073295593262
Epoch 2310, val loss: 0.42591047286987305
Epoch 2320, training loss: 83.74510955810547 = 0.30818822979927063 + 10.0 * 8.3436918258667
Epoch 2320, val loss: 0.4255925118923187
Epoch 2330, training loss: 83.74100494384766 = 0.30732935667037964 + 10.0 * 8.343367576599121
Epoch 2330, val loss: 0.42530274391174316
Epoch 2340, training loss: 83.73771667480469 = 0.30646848678588867 + 10.0 * 8.343125343322754
Epoch 2340, val loss: 0.42499592900276184
Epoch 2350, training loss: 83.73762512207031 = 0.3056052029132843 + 10.0 * 8.343202590942383
Epoch 2350, val loss: 0.4246925711631775
Epoch 2360, training loss: 83.8094482421875 = 0.3047351837158203 + 10.0 * 8.350471496582031
Epoch 2360, val loss: 0.4243631958961487
Epoch 2370, training loss: 83.7634506225586 = 0.30385422706604004 + 10.0 * 8.345959663391113
Epoch 2370, val loss: 0.42405208945274353
Epoch 2380, training loss: 83.7291030883789 = 0.30297204852104187 + 10.0 * 8.342613220214844
Epoch 2380, val loss: 0.42373278737068176
Epoch 2390, training loss: 83.72128295898438 = 0.3021005094051361 + 10.0 * 8.341917991638184
Epoch 2390, val loss: 0.42340147495269775
Epoch 2400, training loss: 83.71859741210938 = 0.30123043060302734 + 10.0 * 8.341736793518066
Epoch 2400, val loss: 0.4231241047382355
Epoch 2410, training loss: 83.72694396972656 = 0.30036160349845886 + 10.0 * 8.342658042907715
Epoch 2410, val loss: 0.4227949380874634
Epoch 2420, training loss: 83.75837707519531 = 0.299481600522995 + 10.0 * 8.3458890914917
Epoch 2420, val loss: 0.4224650263786316
Epoch 2430, training loss: 83.71315002441406 = 0.29858624935150146 + 10.0 * 8.341456413269043
Epoch 2430, val loss: 0.4221435785293579
Epoch 2440, training loss: 83.71086883544922 = 0.2977015972137451 + 10.0 * 8.341317176818848
Epoch 2440, val loss: 0.42183536291122437
Epoch 2450, training loss: 83.70635223388672 = 0.2968217134475708 + 10.0 * 8.34095287322998
Epoch 2450, val loss: 0.4215278923511505
Epoch 2460, training loss: 83.70086669921875 = 0.295940101146698 + 10.0 * 8.340493202209473
Epoch 2460, val loss: 0.42121297121047974
Epoch 2470, training loss: 83.69751739501953 = 0.29505497217178345 + 10.0 * 8.340246200561523
Epoch 2470, val loss: 0.4209166169166565
Epoch 2480, training loss: 83.70317840576172 = 0.29416513442993164 + 10.0 * 8.340901374816895
Epoch 2480, val loss: 0.42058002948760986
Epoch 2490, training loss: 83.71305084228516 = 0.29326534271240234 + 10.0 * 8.341978073120117
Epoch 2490, val loss: 0.4202885329723358
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8386605783866057
0.8631456929652975
=== training gcn model ===
Epoch 0, training loss: 106.91928100585938 = 1.096183180809021 + 10.0 * 10.58230972290039
Epoch 0, val loss: 1.0944510698318481
Epoch 10, training loss: 106.91249084472656 = 1.091965675354004 + 10.0 * 10.582052230834961
Epoch 10, val loss: 1.0903270244598389
Epoch 20, training loss: 106.89630126953125 = 1.08759343624115 + 10.0 * 10.580870628356934
Epoch 20, val loss: 1.0860450267791748
Epoch 30, training loss: 106.839599609375 = 1.0828311443328857 + 10.0 * 10.575676918029785
Epoch 30, val loss: 1.08139169216156
Epoch 40, training loss: 106.61746215820312 = 1.0775694847106934 + 10.0 * 10.55398941040039
Epoch 40, val loss: 1.076253890991211
Epoch 50, training loss: 105.82096862792969 = 1.071434736251831 + 10.0 * 10.474953651428223
Epoch 50, val loss: 1.070212721824646
Epoch 60, training loss: 103.48242950439453 = 1.0638684034347534 + 10.0 * 10.241856575012207
Epoch 60, val loss: 1.062719464302063
Epoch 70, training loss: 99.26815795898438 = 1.054571270942688 + 10.0 * 9.821358680725098
Epoch 70, val loss: 1.0535955429077148
Epoch 80, training loss: 97.48614501953125 = 1.045396327972412 + 10.0 * 9.644075393676758
Epoch 80, val loss: 1.0448342561721802
Epoch 90, training loss: 96.97660827636719 = 1.0382962226867676 + 10.0 * 9.593831062316895
Epoch 90, val loss: 1.0380077362060547
Epoch 100, training loss: 96.56022644042969 = 1.0328738689422607 + 10.0 * 9.552735328674316
Epoch 100, val loss: 1.032785177230835
Epoch 110, training loss: 96.06398010253906 = 1.0274195671081543 + 10.0 * 9.503656387329102
Epoch 110, val loss: 1.0273792743682861
Epoch 120, training loss: 95.18826293945312 = 1.021244764328003 + 10.0 * 9.416701316833496
Epoch 120, val loss: 1.0214096307754517
Epoch 130, training loss: 93.80413055419922 = 1.0156694650650024 + 10.0 * 9.27884578704834
Epoch 130, val loss: 1.016087293624878
Epoch 140, training loss: 92.61441802978516 = 1.0103834867477417 + 10.0 * 9.16040325164795
Epoch 140, val loss: 1.0110087394714355
Epoch 150, training loss: 91.96327209472656 = 1.0047240257263184 + 10.0 * 9.095854759216309
Epoch 150, val loss: 1.005458950996399
Epoch 160, training loss: 91.07057189941406 = 0.998671293258667 + 10.0 * 9.007189750671387
Epoch 160, val loss: 0.9996083974838257
Epoch 170, training loss: 90.54549407958984 = 0.9937286972999573 + 10.0 * 8.95517635345459
Epoch 170, val loss: 0.9946901798248291
Epoch 180, training loss: 90.27490997314453 = 0.9873136281967163 + 10.0 * 8.928759574890137
Epoch 180, val loss: 0.9878538250923157
Epoch 190, training loss: 89.88922882080078 = 0.9782770276069641 + 10.0 * 8.891095161437988
Epoch 190, val loss: 0.9787943363189697
Epoch 200, training loss: 89.4637451171875 = 0.9703694581985474 + 10.0 * 8.849337577819824
Epoch 200, val loss: 0.9710807800292969
Epoch 210, training loss: 89.13260650634766 = 0.962729275226593 + 10.0 * 8.816987991333008
Epoch 210, val loss: 0.9633840918540955
Epoch 220, training loss: 88.90690612792969 = 0.9539438486099243 + 10.0 * 8.795296669006348
Epoch 220, val loss: 0.9545835852622986
Epoch 230, training loss: 88.6482162475586 = 0.9449540972709656 + 10.0 * 8.770326614379883
Epoch 230, val loss: 0.9456292986869812
Epoch 240, training loss: 88.35230255126953 = 0.9357753396034241 + 10.0 * 8.741652488708496
Epoch 240, val loss: 0.9363816976547241
Epoch 250, training loss: 88.08089447021484 = 0.9258289337158203 + 10.0 * 8.715506553649902
Epoch 250, val loss: 0.9262569546699524
Epoch 260, training loss: 87.87252044677734 = 0.9147697687149048 + 10.0 * 8.695775032043457
Epoch 260, val loss: 0.9152721762657166
Epoch 270, training loss: 87.67229461669922 = 0.903186023235321 + 10.0 * 8.676911354064941
Epoch 270, val loss: 0.903631329536438
Epoch 280, training loss: 87.48369598388672 = 0.891352653503418 + 10.0 * 8.659235000610352
Epoch 280, val loss: 0.8919137716293335
Epoch 290, training loss: 87.23814392089844 = 0.8796221017837524 + 10.0 * 8.635851860046387
Epoch 290, val loss: 0.8802312016487122
Epoch 300, training loss: 87.02407836914062 = 0.8678883910179138 + 10.0 * 8.615618705749512
Epoch 300, val loss: 0.8685104250907898
Epoch 310, training loss: 86.84554290771484 = 0.8557235598564148 + 10.0 * 8.598981857299805
Epoch 310, val loss: 0.8564819097518921
Epoch 320, training loss: 86.69430541992188 = 0.8433166742324829 + 10.0 * 8.585099220275879
Epoch 320, val loss: 0.8441553115844727
Epoch 330, training loss: 86.6079330444336 = 0.8306511640548706 + 10.0 * 8.577728271484375
Epoch 330, val loss: 0.8316988945007324
Epoch 340, training loss: 86.45818328857422 = 0.8178622722625732 + 10.0 * 8.564031600952148
Epoch 340, val loss: 0.819000780582428
Epoch 350, training loss: 86.34329986572266 = 0.805149495601654 + 10.0 * 8.553814888000488
Epoch 350, val loss: 0.8064156174659729
Epoch 360, training loss: 86.28085327148438 = 0.7923890948295593 + 10.0 * 8.548846244812012
Epoch 360, val loss: 0.7938781976699829
Epoch 370, training loss: 86.15613555908203 = 0.7795526385307312 + 10.0 * 8.537657737731934
Epoch 370, val loss: 0.7813565731048584
Epoch 380, training loss: 86.06753540039062 = 0.7669090628623962 + 10.0 * 8.530062675476074
Epoch 380, val loss: 0.7689943313598633
Epoch 390, training loss: 85.98474884033203 = 0.7544885277748108 + 10.0 * 8.523026466369629
Epoch 390, val loss: 0.7568683624267578
Epoch 400, training loss: 85.90837097167969 = 0.7421540021896362 + 10.0 * 8.516621589660645
Epoch 400, val loss: 0.7448727488517761
Epoch 410, training loss: 85.86724853515625 = 0.7298321723937988 + 10.0 * 8.513741493225098
Epoch 410, val loss: 0.7329208850860596
Epoch 420, training loss: 85.77643585205078 = 0.7176586985588074 + 10.0 * 8.505877494812012
Epoch 420, val loss: 0.7211331129074097
Epoch 430, training loss: 85.70150756835938 = 0.7056239247322083 + 10.0 * 8.499588012695312
Epoch 430, val loss: 0.7095069885253906
Epoch 440, training loss: 85.6702880859375 = 0.6937044858932495 + 10.0 * 8.497658729553223
Epoch 440, val loss: 0.698012113571167
Epoch 450, training loss: 85.58467102050781 = 0.6817446351051331 + 10.0 * 8.4902925491333
Epoch 450, val loss: 0.6865891814231873
Epoch 460, training loss: 85.52545166015625 = 0.669887125492096 + 10.0 * 8.485556602478027
Epoch 460, val loss: 0.675223171710968
Epoch 470, training loss: 85.4681396484375 = 0.6580978035926819 + 10.0 * 8.48100471496582
Epoch 470, val loss: 0.6639683246612549
Epoch 480, training loss: 85.42024993896484 = 0.6463671326637268 + 10.0 * 8.477388381958008
Epoch 480, val loss: 0.6528569459915161
Epoch 490, training loss: 85.36878967285156 = 0.6347159147262573 + 10.0 * 8.473407745361328
Epoch 490, val loss: 0.6417884230613708
Epoch 500, training loss: 85.31892395019531 = 0.6231988668441772 + 10.0 * 8.469572067260742
Epoch 500, val loss: 0.6309952139854431
Epoch 510, training loss: 85.27557373046875 = 0.6118626594543457 + 10.0 * 8.466371536254883
Epoch 510, val loss: 0.6203848719596863
Epoch 520, training loss: 85.26506042480469 = 0.6006948351860046 + 10.0 * 8.466436386108398
Epoch 520, val loss: 0.6099514961242676
Epoch 530, training loss: 85.20343780517578 = 0.5897462964057922 + 10.0 * 8.461369514465332
Epoch 530, val loss: 0.5997844338417053
Epoch 540, training loss: 85.14965057373047 = 0.5790749788284302 + 10.0 * 8.457056999206543
Epoch 540, val loss: 0.5900514721870422
Epoch 550, training loss: 85.10856628417969 = 0.5687445402145386 + 10.0 * 8.45398235321045
Epoch 550, val loss: 0.5805770754814148
Epoch 560, training loss: 85.0694351196289 = 0.5587852597236633 + 10.0 * 8.451065063476562
Epoch 560, val loss: 0.5714884400367737
Epoch 570, training loss: 85.05775451660156 = 0.5492586493492126 + 10.0 * 8.450849533081055
Epoch 570, val loss: 0.5627998113632202
Epoch 580, training loss: 85.049072265625 = 0.5400229692459106 + 10.0 * 8.450904846191406
Epoch 580, val loss: 0.5545074343681335
Epoch 590, training loss: 84.96957397460938 = 0.5311021208763123 + 10.0 * 8.443846702575684
Epoch 590, val loss: 0.5466472506523132
Epoch 600, training loss: 84.9320068359375 = 0.522669792175293 + 10.0 * 8.440934181213379
Epoch 600, val loss: 0.5392125844955444
Epoch 610, training loss: 84.90025329589844 = 0.5146462917327881 + 10.0 * 8.438560485839844
Epoch 610, val loss: 0.5321752429008484
Epoch 620, training loss: 84.870849609375 = 0.5069844722747803 + 10.0 * 8.436387062072754
Epoch 620, val loss: 0.5255294442176819
Epoch 630, training loss: 84.85897827148438 = 0.4996948838233948 + 10.0 * 8.435928344726562
Epoch 630, val loss: 0.5192428827285767
Epoch 640, training loss: 84.83961486816406 = 0.49273163080215454 + 10.0 * 8.434688568115234
Epoch 640, val loss: 0.5132924318313599
Epoch 650, training loss: 84.79556274414062 = 0.4861440658569336 + 10.0 * 8.430941581726074
Epoch 650, val loss: 0.5077763795852661
Epoch 660, training loss: 84.77556610107422 = 0.47994592785835266 + 10.0 * 8.42956256866455
Epoch 660, val loss: 0.5026482343673706
Epoch 670, training loss: 84.75178527832031 = 0.47407808899879456 + 10.0 * 8.427770614624023
Epoch 670, val loss: 0.4978390634059906
Epoch 680, training loss: 84.73308563232422 = 0.46851029992103577 + 10.0 * 8.426457405090332
Epoch 680, val loss: 0.4933522641658783
Epoch 690, training loss: 84.76403045654297 = 0.46323105692863464 + 10.0 * 8.430079460144043
Epoch 690, val loss: 0.4892129600048065
Epoch 700, training loss: 84.69605255126953 = 0.45823946595191956 + 10.0 * 8.423781394958496
Epoch 700, val loss: 0.4852759838104248
Epoch 710, training loss: 84.67904663085938 = 0.4535354673862457 + 10.0 * 8.422551155090332
Epoch 710, val loss: 0.48159343004226685
Epoch 720, training loss: 84.6602783203125 = 0.44907695055007935 + 10.0 * 8.421120643615723
Epoch 720, val loss: 0.47820383310317993
Epoch 730, training loss: 84.64503479003906 = 0.4448315501213074 + 10.0 * 8.42002010345459
Epoch 730, val loss: 0.47510826587677
Epoch 740, training loss: 84.64630889892578 = 0.44078782200813293 + 10.0 * 8.420552253723145
Epoch 740, val loss: 0.47211867570877075
Epoch 750, training loss: 84.62576293945312 = 0.4369546175003052 + 10.0 * 8.418880462646484
Epoch 750, val loss: 0.46938082575798035
Epoch 760, training loss: 84.60246276855469 = 0.43334323167800903 + 10.0 * 8.416912078857422
Epoch 760, val loss: 0.46688312292099
Epoch 770, training loss: 84.58478546142578 = 0.42988747358322144 + 10.0 * 8.41549015045166
Epoch 770, val loss: 0.4645419418811798
Epoch 780, training loss: 84.57453155517578 = 0.42658451199531555 + 10.0 * 8.414794921875
Epoch 780, val loss: 0.4623431861400604
Epoch 790, training loss: 84.57492065429688 = 0.42341580986976624 + 10.0 * 8.41515064239502
Epoch 790, val loss: 0.4602542817592621
Epoch 800, training loss: 84.55789947509766 = 0.42040398716926575 + 10.0 * 8.413749694824219
Epoch 800, val loss: 0.45840904116630554
Epoch 810, training loss: 84.5289535522461 = 0.41754433512687683 + 10.0 * 8.411141395568848
Epoch 810, val loss: 0.4565633237361908
Epoch 820, training loss: 84.51824951171875 = 0.4147985577583313 + 10.0 * 8.410345077514648
Epoch 820, val loss: 0.45483461022377014
Epoch 830, training loss: 84.50350189208984 = 0.4121454060077667 + 10.0 * 8.409135818481445
Epoch 830, val loss: 0.45329099893569946
Epoch 840, training loss: 84.55318450927734 = 0.40958625078201294 + 10.0 * 8.414360046386719
Epoch 840, val loss: 0.4518328905105591
Epoch 850, training loss: 84.49039459228516 = 0.4071088135242462 + 10.0 * 8.40832805633545
Epoch 850, val loss: 0.4503408968448639
Epoch 860, training loss: 84.46926879882812 = 0.4047391414642334 + 10.0 * 8.406453132629395
Epoch 860, val loss: 0.4490564167499542
Epoch 870, training loss: 84.45709991455078 = 0.40245118737220764 + 10.0 * 8.405465126037598
Epoch 870, val loss: 0.44784852862358093
Epoch 880, training loss: 84.45133972167969 = 0.40024322271347046 + 10.0 * 8.405109405517578
Epoch 880, val loss: 0.4465906023979187
Epoch 890, training loss: 84.43265533447266 = 0.39809513092041016 + 10.0 * 8.40345573425293
Epoch 890, val loss: 0.44551846385002136
Epoch 900, training loss: 84.42829895019531 = 0.3960227370262146 + 10.0 * 8.403227806091309
Epoch 900, val loss: 0.4443555474281311
Epoch 910, training loss: 84.41352844238281 = 0.3940136134624481 + 10.0 * 8.401951789855957
Epoch 910, val loss: 0.4433889091014862
Epoch 920, training loss: 84.42913818359375 = 0.39205601811408997 + 10.0 * 8.403707504272461
Epoch 920, val loss: 0.4423118531703949
Epoch 930, training loss: 84.40782165527344 = 0.3901202380657196 + 10.0 * 8.401769638061523
Epoch 930, val loss: 0.44158560037612915
Epoch 940, training loss: 84.3860092163086 = 0.3882637619972229 + 10.0 * 8.399774551391602
Epoch 940, val loss: 0.4405672550201416
Epoch 950, training loss: 84.37100219726562 = 0.38645678758621216 + 10.0 * 8.398454666137695
Epoch 950, val loss: 0.43972304463386536
Epoch 960, training loss: 84.36358642578125 = 0.38469377160072327 + 10.0 * 8.397889137268066
Epoch 960, val loss: 0.43889132142066956
Epoch 970, training loss: 84.39724731445312 = 0.3829725384712219 + 10.0 * 8.401427268981934
Epoch 970, val loss: 0.43798163533210754
Epoch 980, training loss: 84.3565444946289 = 0.3812769949436188 + 10.0 * 8.397526741027832
Epoch 980, val loss: 0.4373885989189148
Epoch 990, training loss: 84.33802795410156 = 0.3796389400959015 + 10.0 * 8.395838737487793
Epoch 990, val loss: 0.4365484416484833
Epoch 1000, training loss: 84.32481384277344 = 0.378034770488739 + 10.0 * 8.394678115844727
Epoch 1000, val loss: 0.43590593338012695
Epoch 1010, training loss: 84.31571960449219 = 0.3764650523662567 + 10.0 * 8.393925666809082
Epoch 1010, val loss: 0.43523725867271423
Epoch 1020, training loss: 84.3444595336914 = 0.3749276399612427 + 10.0 * 8.396952629089355
Epoch 1020, val loss: 0.4345610439777374
Epoch 1030, training loss: 84.31517791748047 = 0.37340307235717773 + 10.0 * 8.394177436828613
Epoch 1030, val loss: 0.4338507056236267
Epoch 1040, training loss: 84.31346893310547 = 0.3719129264354706 + 10.0 * 8.394155502319336
Epoch 1040, val loss: 0.43339648842811584
Epoch 1050, training loss: 84.28507995605469 = 0.3704552948474884 + 10.0 * 8.391462326049805
Epoch 1050, val loss: 0.4325849711894989
Epoch 1060, training loss: 84.27394104003906 = 0.369028776884079 + 10.0 * 8.390491485595703
Epoch 1060, val loss: 0.4320864677429199
Epoch 1070, training loss: 84.26522064208984 = 0.36763009428977966 + 10.0 * 8.389759063720703
Epoch 1070, val loss: 0.431475430727005
Epoch 1080, training loss: 84.2977523803711 = 0.3662537932395935 + 10.0 * 8.393149375915527
Epoch 1080, val loss: 0.4307795763015747
Epoch 1090, training loss: 84.26567840576172 = 0.36487793922424316 + 10.0 * 8.390080451965332
Epoch 1090, val loss: 0.4304734468460083
Epoch 1100, training loss: 84.24754333496094 = 0.3635396361351013 + 10.0 * 8.388400077819824
Epoch 1100, val loss: 0.42980360984802246
Epoch 1110, training loss: 84.23237609863281 = 0.3622169494628906 + 10.0 * 8.387015342712402
Epoch 1110, val loss: 0.42936742305755615
Epoch 1120, training loss: 84.22465515136719 = 0.36091771721839905 + 10.0 * 8.386373519897461
Epoch 1120, val loss: 0.42881208658218384
Epoch 1130, training loss: 84.24429321289062 = 0.3596342206001282 + 10.0 * 8.388465881347656
Epoch 1130, val loss: 0.4283420145511627
Epoch 1140, training loss: 84.23746490478516 = 0.3583592474460602 + 10.0 * 8.387910842895508
Epoch 1140, val loss: 0.42775699496269226
Epoch 1150, training loss: 84.20269775390625 = 0.35709497332572937 + 10.0 * 8.384560585021973
Epoch 1150, val loss: 0.427310049533844
Epoch 1160, training loss: 84.19679260253906 = 0.35585808753967285 + 10.0 * 8.384093284606934
Epoch 1160, val loss: 0.42701518535614014
Epoch 1170, training loss: 84.1844482421875 = 0.354636549949646 + 10.0 * 8.382981300354004
Epoch 1170, val loss: 0.42648187279701233
Epoch 1180, training loss: 84.20738983154297 = 0.35342851281166077 + 10.0 * 8.385396003723145
Epoch 1180, val loss: 0.4260821044445038
Epoch 1190, training loss: 84.1853256225586 = 0.3522225022315979 + 10.0 * 8.383310317993164
Epoch 1190, val loss: 0.42554670572280884
Epoch 1200, training loss: 84.162841796875 = 0.35103681683540344 + 10.0 * 8.381180763244629
Epoch 1200, val loss: 0.425184965133667
Epoch 1210, training loss: 84.15428924560547 = 0.3498630225658417 + 10.0 * 8.38044261932373
Epoch 1210, val loss: 0.42474740743637085
Epoch 1220, training loss: 84.14771270751953 = 0.34870079159736633 + 10.0 * 8.379900932312012
Epoch 1220, val loss: 0.42437663674354553
Epoch 1230, training loss: 84.24707794189453 = 0.34754693508148193 + 10.0 * 8.389952659606934
Epoch 1230, val loss: 0.4242016673088074
Epoch 1240, training loss: 84.13407897949219 = 0.3463912010192871 + 10.0 * 8.378768920898438
Epoch 1240, val loss: 0.42351916432380676
Epoch 1250, training loss: 84.13048553466797 = 0.3452705144882202 + 10.0 * 8.378521919250488
Epoch 1250, val loss: 0.42314577102661133
Epoch 1260, training loss: 84.11506652832031 = 0.3441620469093323 + 10.0 * 8.377090454101562
Epoch 1260, val loss: 0.4227202534675598
Epoch 1270, training loss: 84.10828399658203 = 0.34306448698043823 + 10.0 * 8.376522064208984
Epoch 1270, val loss: 0.42240768671035767
Epoch 1280, training loss: 84.13959503173828 = 0.34197887778282166 + 10.0 * 8.379761695861816
Epoch 1280, val loss: 0.4219653606414795
Epoch 1290, training loss: 84.11720275878906 = 0.34088659286499023 + 10.0 * 8.377631187438965
Epoch 1290, val loss: 0.4216999411582947
Epoch 1300, training loss: 84.08821868896484 = 0.33981630206108093 + 10.0 * 8.37484073638916
Epoch 1300, val loss: 0.4212453067302704
Epoch 1310, training loss: 84.07992553710938 = 0.3387610912322998 + 10.0 * 8.374116897583008
Epoch 1310, val loss: 0.42095747590065
Epoch 1320, training loss: 84.0699234008789 = 0.3377171456813812 + 10.0 * 8.373220443725586
Epoch 1320, val loss: 0.42057687044143677
Epoch 1330, training loss: 84.0689697265625 = 0.33668211102485657 + 10.0 * 8.373228073120117
Epoch 1330, val loss: 0.4202650189399719
Epoch 1340, training loss: 84.07867431640625 = 0.33564841747283936 + 10.0 * 8.374302864074707
Epoch 1340, val loss: 0.41984981298446655
Epoch 1350, training loss: 84.06121826171875 = 0.3346216678619385 + 10.0 * 8.372659683227539
Epoch 1350, val loss: 0.4195961654186249
Epoch 1360, training loss: 84.0442886352539 = 0.3336135745048523 + 10.0 * 8.371068000793457
Epoch 1360, val loss: 0.4192407429218292
Epoch 1370, training loss: 84.03951263427734 = 0.3326171338558197 + 10.0 * 8.370689392089844
Epoch 1370, val loss: 0.4188721179962158
Epoch 1380, training loss: 84.07228088378906 = 0.3316247761249542 + 10.0 * 8.374065399169922
Epoch 1380, val loss: 0.4186825752258301
Epoch 1390, training loss: 84.0377197265625 = 0.33062952756881714 + 10.0 * 8.370709419250488
Epoch 1390, val loss: 0.4182742238044739
Epoch 1400, training loss: 84.02574920654297 = 0.32964974641799927 + 10.0 * 8.369609832763672
Epoch 1400, val loss: 0.41788819432258606
Epoch 1410, training loss: 84.01319122314453 = 0.3286782503128052 + 10.0 * 8.368451118469238
Epoch 1410, val loss: 0.41760873794555664
Epoch 1420, training loss: 84.0082778930664 = 0.3277110457420349 + 10.0 * 8.368056297302246
Epoch 1420, val loss: 0.4172978401184082
Epoch 1430, training loss: 84.04470825195312 = 0.3267455995082855 + 10.0 * 8.371796607971191
Epoch 1430, val loss: 0.41692763566970825
Epoch 1440, training loss: 84.01354217529297 = 0.32578420639038086 + 10.0 * 8.368776321411133
Epoch 1440, val loss: 0.4166535437107086
Epoch 1450, training loss: 84.03055572509766 = 0.3248352110385895 + 10.0 * 8.370572090148926
Epoch 1450, val loss: 0.4162958860397339
Epoch 1460, training loss: 83.99488830566406 = 0.32388436794281006 + 10.0 * 8.367100715637207
Epoch 1460, val loss: 0.4161463677883148
Epoch 1470, training loss: 83.98191833496094 = 0.3229524493217468 + 10.0 * 8.365896224975586
Epoch 1470, val loss: 0.41585981845855713
Epoch 1480, training loss: 83.97552490234375 = 0.3220246434211731 + 10.0 * 8.365350723266602
Epoch 1480, val loss: 0.4155668020248413
Epoch 1490, training loss: 83.97164916992188 = 0.3211013376712799 + 10.0 * 8.365055084228516
Epoch 1490, val loss: 0.4153132438659668
Epoch 1500, training loss: 83.99130249023438 = 0.32018181681632996 + 10.0 * 8.367112159729004
Epoch 1500, val loss: 0.4150576591491699
Epoch 1510, training loss: 83.9884262084961 = 0.31926029920578003 + 10.0 * 8.36691665649414
Epoch 1510, val loss: 0.41465330123901367
Epoch 1520, training loss: 83.9637680053711 = 0.318348228931427 + 10.0 * 8.364542007446289
Epoch 1520, val loss: 0.41451865434646606
Epoch 1530, training loss: 83.95925903320312 = 0.31744471192359924 + 10.0 * 8.364181518554688
Epoch 1530, val loss: 0.41434669494628906
Epoch 1540, training loss: 83.95118713378906 = 0.31655004620552063 + 10.0 * 8.363463401794434
Epoch 1540, val loss: 0.41402554512023926
Epoch 1550, training loss: 83.9481201171875 = 0.3156616687774658 + 10.0 * 8.363245964050293
Epoch 1550, val loss: 0.4138214588165283
Epoch 1560, training loss: 83.9789047241211 = 0.31477293372154236 + 10.0 * 8.366413116455078
Epoch 1560, val loss: 0.41366279125213623
Epoch 1570, training loss: 83.94066619873047 = 0.3138788938522339 + 10.0 * 8.362678527832031
Epoch 1570, val loss: 0.4132792055606842
Epoch 1580, training loss: 83.9342041015625 = 0.3129972517490387 + 10.0 * 8.362120628356934
Epoch 1580, val loss: 0.41314247250556946
Epoch 1590, training loss: 83.93450927734375 = 0.3121207058429718 + 10.0 * 8.362238883972168
Epoch 1590, val loss: 0.41292276978492737
Epoch 1600, training loss: 83.9407958984375 = 0.31124964356422424 + 10.0 * 8.362955093383789
Epoch 1600, val loss: 0.4126720130443573
Epoch 1610, training loss: 83.93601989746094 = 0.3103819191455841 + 10.0 * 8.362564086914062
Epoch 1610, val loss: 0.4123002290725708
Epoch 1620, training loss: 83.92540740966797 = 0.30951786041259766 + 10.0 * 8.361589431762695
Epoch 1620, val loss: 0.41219189763069153
Epoch 1630, training loss: 83.9170150756836 = 0.3086613118648529 + 10.0 * 8.360835075378418
Epoch 1630, val loss: 0.4120563864707947
Epoch 1640, training loss: 83.91150665283203 = 0.3078112006187439 + 10.0 * 8.360369682312012
Epoch 1640, val loss: 0.4118254482746124
Epoch 1650, training loss: 83.92341613769531 = 0.30696359276771545 + 10.0 * 8.361645698547363
Epoch 1650, val loss: 0.41174378991127014
Epoch 1660, training loss: 83.92417907714844 = 0.3061099946498871 + 10.0 * 8.361806869506836
Epoch 1660, val loss: 0.41146162152290344
Epoch 1670, training loss: 83.90776824951172 = 0.30526208877563477 + 10.0 * 8.360250473022461
Epoch 1670, val loss: 0.41117727756500244
Epoch 1680, training loss: 83.90319061279297 = 0.30442100763320923 + 10.0 * 8.35987663269043
Epoch 1680, val loss: 0.4111081659793854
Epoch 1690, training loss: 83.90019226074219 = 0.3035811483860016 + 10.0 * 8.359661102294922
Epoch 1690, val loss: 0.410863995552063
Epoch 1700, training loss: 83.88969421386719 = 0.3027462363243103 + 10.0 * 8.358694076538086
Epoch 1700, val loss: 0.4106217920780182
Epoch 1710, training loss: 83.88496398925781 = 0.301913321018219 + 10.0 * 8.358304977416992
Epoch 1710, val loss: 0.41046062111854553
Epoch 1720, training loss: 83.88844299316406 = 0.30108314752578735 + 10.0 * 8.358736038208008
Epoch 1720, val loss: 0.41035616397857666
Epoch 1730, training loss: 83.89997863769531 = 0.3002507984638214 + 10.0 * 8.359972953796387
Epoch 1730, val loss: 0.4101569354534149
Epoch 1740, training loss: 83.89453887939453 = 0.29941895604133606 + 10.0 * 8.359512329101562
Epoch 1740, val loss: 0.41001349687576294
Epoch 1750, training loss: 83.87921142578125 = 0.2985948622226715 + 10.0 * 8.358061790466309
Epoch 1750, val loss: 0.40971946716308594
Epoch 1760, training loss: 83.86905670166016 = 0.29778289794921875 + 10.0 * 8.35712718963623
Epoch 1760, val loss: 0.4096667170524597
Epoch 1770, training loss: 83.86214447021484 = 0.296975702047348 + 10.0 * 8.35651683807373
Epoch 1770, val loss: 0.40950632095336914
Epoch 1780, training loss: 83.86741638183594 = 0.29617539048194885 + 10.0 * 8.357124328613281
Epoch 1780, val loss: 0.4094475507736206
Epoch 1790, training loss: 83.88847351074219 = 0.29536765813827515 + 10.0 * 8.3593111038208
Epoch 1790, val loss: 0.4092996418476105
Epoch 1800, training loss: 83.85596466064453 = 0.29456189274787903 + 10.0 * 8.35614013671875
Epoch 1800, val loss: 0.40912747383117676
Epoch 1810, training loss: 83.8537368774414 = 0.29376929998397827 + 10.0 * 8.355997085571289
Epoch 1810, val loss: 0.40884724259376526
Epoch 1820, training loss: 83.84357452392578 = 0.2929795980453491 + 10.0 * 8.355059623718262
Epoch 1820, val loss: 0.40876561403274536
Epoch 1830, training loss: 83.83924865722656 = 0.29219403862953186 + 10.0 * 8.354705810546875
Epoch 1830, val loss: 0.4086318910121918
Epoch 1840, training loss: 83.83602905273438 = 0.2914097011089325 + 10.0 * 8.354461669921875
Epoch 1840, val loss: 0.4085015058517456
Epoch 1850, training loss: 83.86017608642578 = 0.29062679409980774 + 10.0 * 8.356954574584961
Epoch 1850, val loss: 0.4084084630012512
Epoch 1860, training loss: 83.86882019042969 = 0.2898341715335846 + 10.0 * 8.357898712158203
Epoch 1860, val loss: 0.4083879292011261
Epoch 1870, training loss: 83.83604431152344 = 0.2890467643737793 + 10.0 * 8.354700088500977
Epoch 1870, val loss: 0.40813443064689636
Epoch 1880, training loss: 83.82190704345703 = 0.28826624155044556 + 10.0 * 8.353363990783691
Epoch 1880, val loss: 0.4080706834793091
Epoch 1890, training loss: 83.82028198242188 = 0.28748705983161926 + 10.0 * 8.353279113769531
Epoch 1890, val loss: 0.40801480412483215
Epoch 1900, training loss: 83.81582641601562 = 0.28671136498451233 + 10.0 * 8.352910995483398
Epoch 1900, val loss: 0.40790972113609314
Epoch 1910, training loss: 83.82833862304688 = 0.28593993186950684 + 10.0 * 8.354239463806152
Epoch 1910, val loss: 0.4078677296638489
Epoch 1920, training loss: 83.82142639160156 = 0.28516241908073425 + 10.0 * 8.353626251220703
Epoch 1920, val loss: 0.407711923122406
Epoch 1930, training loss: 83.81590270996094 = 0.2843930423259735 + 10.0 * 8.353151321411133
Epoch 1930, val loss: 0.40746939182281494
Epoch 1940, training loss: 83.81646728515625 = 0.28362852334976196 + 10.0 * 8.353283882141113
Epoch 1940, val loss: 0.4074353575706482
Epoch 1950, training loss: 83.80403137207031 = 0.28286588191986084 + 10.0 * 8.352116584777832
Epoch 1950, val loss: 0.40736445784568787
Epoch 1960, training loss: 83.79869842529297 = 0.28210899233818054 + 10.0 * 8.351658821105957
Epoch 1960, val loss: 0.4074012041091919
Epoch 1970, training loss: 83.79793548583984 = 0.2813478708267212 + 10.0 * 8.351658821105957
Epoch 1970, val loss: 0.4073342978954315
Epoch 1980, training loss: 83.80262756347656 = 0.2805854380130768 + 10.0 * 8.352204322814941
Epoch 1980, val loss: 0.4072425365447998
Epoch 1990, training loss: 83.7857666015625 = 0.2798256278038025 + 10.0 * 8.350594520568848
Epoch 1990, val loss: 0.40709349513053894
Epoch 2000, training loss: 83.82567596435547 = 0.2790657877922058 + 10.0 * 8.354660987854004
Epoch 2000, val loss: 0.40706297755241394
Epoch 2010, training loss: 83.77696228027344 = 0.27830222249031067 + 10.0 * 8.349865913391113
Epoch 2010, val loss: 0.40702638030052185
Epoch 2020, training loss: 83.77452087402344 = 0.2775447368621826 + 10.0 * 8.349698066711426
Epoch 2020, val loss: 0.40693578124046326
Epoch 2030, training loss: 83.77134704589844 = 0.27679091691970825 + 10.0 * 8.349454879760742
Epoch 2030, val loss: 0.4068106412887573
Epoch 2040, training loss: 83.76830291748047 = 0.2760376036167145 + 10.0 * 8.349225997924805
Epoch 2040, val loss: 0.40674230456352234
Epoch 2050, training loss: 83.801513671875 = 0.27528658509254456 + 10.0 * 8.352622985839844
Epoch 2050, val loss: 0.4065834581851959
Epoch 2060, training loss: 83.76948547363281 = 0.2745298445224762 + 10.0 * 8.349495887756348
Epoch 2060, val loss: 0.4068111479282379
Epoch 2070, training loss: 83.7563247680664 = 0.2737787663936615 + 10.0 * 8.348254203796387
Epoch 2070, val loss: 0.40667420625686646
Epoch 2080, training loss: 83.7501220703125 = 0.2730277478694916 + 10.0 * 8.347709655761719
Epoch 2080, val loss: 0.4067308306694031
Epoch 2090, training loss: 83.74781799316406 = 0.27227941155433655 + 10.0 * 8.347554206848145
Epoch 2090, val loss: 0.4067152142524719
Epoch 2100, training loss: 83.76100158691406 = 0.27153244614601135 + 10.0 * 8.348947525024414
Epoch 2100, val loss: 0.4066735804080963
Epoch 2110, training loss: 83.76367950439453 = 0.2707854211330414 + 10.0 * 8.349289894104004
Epoch 2110, val loss: 0.4067782759666443
Epoch 2120, training loss: 83.74736785888672 = 0.270032674074173 + 10.0 * 8.347733497619629
Epoch 2120, val loss: 0.4066851735115051
Epoch 2130, training loss: 83.73905944824219 = 0.26928675174713135 + 10.0 * 8.346977233886719
Epoch 2130, val loss: 0.40672871470451355
Epoch 2140, training loss: 83.73468017578125 = 0.26854208111763 + 10.0 * 8.346613883972168
Epoch 2140, val loss: 0.40665847063064575
Epoch 2150, training loss: 83.72562408447266 = 0.26780200004577637 + 10.0 * 8.345782279968262
Epoch 2150, val loss: 0.40669915080070496
Epoch 2160, training loss: 83.72273254394531 = 0.26706236600875854 + 10.0 * 8.34556770324707
Epoch 2160, val loss: 0.40670302510261536
Epoch 2170, training loss: 83.7430419921875 = 0.26632586121559143 + 10.0 * 8.347671508789062
Epoch 2170, val loss: 0.4066537320613861
Epoch 2180, training loss: 83.73176574707031 = 0.2655777335166931 + 10.0 * 8.34661865234375
Epoch 2180, val loss: 0.4066736102104187
Epoch 2190, training loss: 83.71460723876953 = 0.2648376226425171 + 10.0 * 8.344976425170898
Epoch 2190, val loss: 0.4068196415901184
Epoch 2200, training loss: 83.71041870117188 = 0.26410138607025146 + 10.0 * 8.344632148742676
Epoch 2200, val loss: 0.40678152441978455
Epoch 2210, training loss: 83.70578002929688 = 0.26336443424224854 + 10.0 * 8.34424114227295
Epoch 2210, val loss: 0.4067743718624115
Epoch 2220, training loss: 83.70367431640625 = 0.2626265287399292 + 10.0 * 8.344104766845703
Epoch 2220, val loss: 0.4067881107330322
Epoch 2230, training loss: 83.75831604003906 = 0.26188716292381287 + 10.0 * 8.349642753601074
Epoch 2230, val loss: 0.40675118565559387
Epoch 2240, training loss: 83.71240997314453 = 0.26114022731781006 + 10.0 * 8.34512710571289
Epoch 2240, val loss: 0.4069599509239197
Epoch 2250, training loss: 83.69408416748047 = 0.26039719581604004 + 10.0 * 8.343368530273438
Epoch 2250, val loss: 0.4068666100502014
Epoch 2260, training loss: 83.68848419189453 = 0.25965335965156555 + 10.0 * 8.342883110046387
Epoch 2260, val loss: 0.40692833065986633
Epoch 2270, training loss: 83.6853256225586 = 0.2589071989059448 + 10.0 * 8.342641830444336
Epoch 2270, val loss: 0.4069298505783081
Epoch 2280, training loss: 83.70890808105469 = 0.2581629157066345 + 10.0 * 8.345074653625488
Epoch 2280, val loss: 0.40683335065841675
Epoch 2290, training loss: 83.69209289550781 = 0.2574121356010437 + 10.0 * 8.343467712402344
Epoch 2290, val loss: 0.40707752108573914
Epoch 2300, training loss: 83.68401336669922 = 0.25666725635528564 + 10.0 * 8.342734336853027
Epoch 2300, val loss: 0.40701422095298767
Epoch 2310, training loss: 83.67649841308594 = 0.2559274435043335 + 10.0 * 8.342057228088379
Epoch 2310, val loss: 0.40723124146461487
Epoch 2320, training loss: 83.67334747314453 = 0.25518783926963806 + 10.0 * 8.341815948486328
Epoch 2320, val loss: 0.4073024392127991
Epoch 2330, training loss: 83.69538116455078 = 0.25445330142974854 + 10.0 * 8.34409236907959
Epoch 2330, val loss: 0.40745431184768677
Epoch 2340, training loss: 83.67253112792969 = 0.25371235609054565 + 10.0 * 8.34188175201416
Epoch 2340, val loss: 0.407470166683197
Epoch 2350, training loss: 83.67057800292969 = 0.2529793381690979 + 10.0 * 8.34175968170166
Epoch 2350, val loss: 0.4075729548931122
Epoch 2360, training loss: 83.68794250488281 = 0.25224602222442627 + 10.0 * 8.3435697555542
Epoch 2360, val loss: 0.40767213702201843
Epoch 2370, training loss: 83.65829467773438 = 0.2515110373497009 + 10.0 * 8.340678215026855
Epoch 2370, val loss: 0.40770354866981506
Epoch 2380, training loss: 83.65254211425781 = 0.250779390335083 + 10.0 * 8.340176582336426
Epoch 2380, val loss: 0.4077807068824768
Epoch 2390, training loss: 83.65299987792969 = 0.25004714727401733 + 10.0 * 8.340295791625977
Epoch 2390, val loss: 0.40786105394363403
Epoch 2400, training loss: 83.6622085571289 = 0.24931420385837555 + 10.0 * 8.341289520263672
Epoch 2400, val loss: 0.40791991353034973
Epoch 2410, training loss: 83.66101837158203 = 0.24857419729232788 + 10.0 * 8.3412446975708
Epoch 2410, val loss: 0.40804871916770935
Epoch 2420, training loss: 83.6487045288086 = 0.24783632159233093 + 10.0 * 8.340086936950684
Epoch 2420, val loss: 0.40827932953834534
Epoch 2430, training loss: 83.64923858642578 = 0.24709799885749817 + 10.0 * 8.340213775634766
Epoch 2430, val loss: 0.4083387851715088
Epoch 2440, training loss: 83.63787841796875 = 0.24635596573352814 + 10.0 * 8.339152336120605
Epoch 2440, val loss: 0.40837562084198
Epoch 2450, training loss: 83.6380844116211 = 0.24561676383018494 + 10.0 * 8.33924674987793
Epoch 2450, val loss: 0.4084797501564026
Epoch 2460, training loss: 83.64945983886719 = 0.24487577378749847 + 10.0 * 8.340458869934082
Epoch 2460, val loss: 0.40861061215400696
Epoch 2470, training loss: 83.62962341308594 = 0.24412649869918823 + 10.0 * 8.338549613952637
Epoch 2470, val loss: 0.40874576568603516
Epoch 2480, training loss: 83.63114929199219 = 0.24337990581989288 + 10.0 * 8.338777542114258
Epoch 2480, val loss: 0.4088965654373169
Epoch 2490, training loss: 83.63558959960938 = 0.24263598024845123 + 10.0 * 8.339295387268066
Epoch 2490, val loss: 0.40910932421684265
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8427194317605275
0.8651017894660582
=== training gcn model ===
Epoch 0, training loss: 106.91228485107422 = 1.0891010761260986 + 10.0 * 10.582318305969238
Epoch 0, val loss: 1.086961030960083
Epoch 10, training loss: 106.90596008300781 = 1.0848546028137207 + 10.0 * 10.582110404968262
Epoch 10, val loss: 1.0827919244766235
Epoch 20, training loss: 106.89156341552734 = 1.0801829099655151 + 10.0 * 10.581137657165527
Epoch 20, val loss: 1.0782567262649536
Epoch 30, training loss: 106.84011840820312 = 1.0750696659088135 + 10.0 * 10.576504707336426
Epoch 30, val loss: 1.0733113288879395
Epoch 40, training loss: 106.63720703125 = 1.069648265838623 + 10.0 * 10.556756019592285
Epoch 40, val loss: 1.0680898427963257
Epoch 50, training loss: 105.99906921386719 = 1.0639182329177856 + 10.0 * 10.493515014648438
Epoch 50, val loss: 1.0625461339950562
Epoch 60, training loss: 104.605224609375 = 1.0578516721725464 + 10.0 * 10.354737281799316
Epoch 60, val loss: 1.0567201375961304
Epoch 70, training loss: 102.69567108154297 = 1.0518378019332886 + 10.0 * 10.164383888244629
Epoch 70, val loss: 1.0509111881256104
Epoch 80, training loss: 100.26573944091797 = 1.0452466011047363 + 10.0 * 9.922048568725586
Epoch 80, val loss: 1.0444504022598267
Epoch 90, training loss: 96.83007049560547 = 1.0376535654067993 + 10.0 * 9.579241752624512
Epoch 90, val loss: 1.036950945854187
Epoch 100, training loss: 95.36034393310547 = 1.0294495820999146 + 10.0 * 9.433089256286621
Epoch 100, val loss: 1.028943419456482
Epoch 110, training loss: 93.9237289428711 = 1.0217549800872803 + 10.0 * 9.290197372436523
Epoch 110, val loss: 1.0214958190917969
Epoch 120, training loss: 92.95709991455078 = 1.0148180723190308 + 10.0 * 9.194228172302246
Epoch 120, val loss: 1.0148279666900635
Epoch 130, training loss: 92.4725341796875 = 1.0079216957092285 + 10.0 * 9.146461486816406
Epoch 130, val loss: 1.008110761642456
Epoch 140, training loss: 92.07600402832031 = 1.0004746913909912 + 10.0 * 9.107553482055664
Epoch 140, val loss: 1.000809907913208
Epoch 150, training loss: 91.53182220458984 = 0.99282306432724 + 10.0 * 9.053899765014648
Epoch 150, val loss: 0.9933196902275085
Epoch 160, training loss: 90.82842254638672 = 0.9856597781181335 + 10.0 * 8.98427677154541
Epoch 160, val loss: 0.9863474369049072
Epoch 170, training loss: 90.17131042480469 = 0.9786882996559143 + 10.0 * 8.919261932373047
Epoch 170, val loss: 0.9794949293136597
Epoch 180, training loss: 89.75284576416016 = 0.9707397818565369 + 10.0 * 8.878210067749023
Epoch 180, val loss: 0.9715897440910339
Epoch 190, training loss: 89.47430419921875 = 0.9613805413246155 + 10.0 * 8.851292610168457
Epoch 190, val loss: 0.9623379111289978
Epoch 200, training loss: 89.31568145751953 = 0.951024055480957 + 10.0 * 8.836465835571289
Epoch 200, val loss: 0.9522233009338379
Epoch 210, training loss: 89.18536376953125 = 0.9400162696838379 + 10.0 * 8.82453441619873
Epoch 210, val loss: 0.9415860176086426
Epoch 220, training loss: 89.05060577392578 = 0.9290858507156372 + 10.0 * 8.812151908874512
Epoch 220, val loss: 0.9310281276702881
Epoch 230, training loss: 88.89891815185547 = 0.9185076355934143 + 10.0 * 8.798040390014648
Epoch 230, val loss: 0.9207172393798828
Epoch 240, training loss: 88.715087890625 = 0.9080110192298889 + 10.0 * 8.780707359313965
Epoch 240, val loss: 0.9104960560798645
Epoch 250, training loss: 88.49813079833984 = 0.8974076509475708 + 10.0 * 8.760072708129883
Epoch 250, val loss: 0.9002140760421753
Epoch 260, training loss: 88.28541564941406 = 0.8865681886672974 + 10.0 * 8.739885330200195
Epoch 260, val loss: 0.8896881341934204
Epoch 270, training loss: 88.13108825683594 = 0.87496018409729 + 10.0 * 8.72561264038086
Epoch 270, val loss: 0.8783822059631348
Epoch 280, training loss: 88.01338195800781 = 0.8623999357223511 + 10.0 * 8.71509838104248
Epoch 280, val loss: 0.8660803437232971
Epoch 290, training loss: 87.8935775756836 = 0.849506139755249 + 10.0 * 8.704407691955566
Epoch 290, val loss: 0.8536350131034851
Epoch 300, training loss: 87.76504516601562 = 0.836776077747345 + 10.0 * 8.692827224731445
Epoch 300, val loss: 0.8413509130477905
Epoch 310, training loss: 87.63160705566406 = 0.8241099119186401 + 10.0 * 8.680749893188477
Epoch 310, val loss: 0.8291246891021729
Epoch 320, training loss: 87.50953674316406 = 0.8113717436790466 + 10.0 * 8.669816970825195
Epoch 320, val loss: 0.8168324828147888
Epoch 330, training loss: 87.4131088256836 = 0.7984148263931274 + 10.0 * 8.661469459533691
Epoch 330, val loss: 0.8043159246444702
Epoch 340, training loss: 87.3393325805664 = 0.7850574254989624 + 10.0 * 8.655427932739258
Epoch 340, val loss: 0.7914271354675293
Epoch 350, training loss: 87.27534484863281 = 0.7714249491691589 + 10.0 * 8.650392532348633
Epoch 350, val loss: 0.7783204913139343
Epoch 360, training loss: 87.21475982666016 = 0.757831871509552 + 10.0 * 8.645692825317383
Epoch 360, val loss: 0.765274167060852
Epoch 370, training loss: 87.16095733642578 = 0.7444515228271484 + 10.0 * 8.641650199890137
Epoch 370, val loss: 0.7524623870849609
Epoch 380, training loss: 87.09742736816406 = 0.7312932014465332 + 10.0 * 8.636613845825195
Epoch 380, val loss: 0.7399170994758606
Epoch 390, training loss: 87.03203582763672 = 0.7182915806770325 + 10.0 * 8.63137435913086
Epoch 390, val loss: 0.7275402545928955
Epoch 400, training loss: 86.96144104003906 = 0.7055261135101318 + 10.0 * 8.625591278076172
Epoch 400, val loss: 0.7154111266136169
Epoch 410, training loss: 86.88204956054688 = 0.6930872797966003 + 10.0 * 8.618896484375
Epoch 410, val loss: 0.7036210298538208
Epoch 420, training loss: 86.845947265625 = 0.680957019329071 + 10.0 * 8.616498947143555
Epoch 420, val loss: 0.6920779347419739
Epoch 430, training loss: 86.71395874023438 = 0.6689717769622803 + 10.0 * 8.604498863220215
Epoch 430, val loss: 0.6808634400367737
Epoch 440, training loss: 86.61746978759766 = 0.6574503779411316 + 10.0 * 8.596002578735352
Epoch 440, val loss: 0.6699236631393433
Epoch 450, training loss: 86.52153015136719 = 0.646117627620697 + 10.0 * 8.587541580200195
Epoch 450, val loss: 0.6592535972595215
Epoch 460, training loss: 86.43154907226562 = 0.63493412733078 + 10.0 * 8.57966136932373
Epoch 460, val loss: 0.6486644148826599
Epoch 470, training loss: 86.34819793701172 = 0.6238737106323242 + 10.0 * 8.572432518005371
Epoch 470, val loss: 0.638218343257904
Epoch 480, training loss: 86.2817611694336 = 0.6129277348518372 + 10.0 * 8.566883087158203
Epoch 480, val loss: 0.6280137896537781
Epoch 490, training loss: 86.17852020263672 = 0.6023512482643127 + 10.0 * 8.5576171875
Epoch 490, val loss: 0.6180167198181152
Epoch 500, training loss: 86.0955810546875 = 0.59215247631073 + 10.0 * 8.550342559814453
Epoch 500, val loss: 0.6084966659545898
Epoch 510, training loss: 86.01448822021484 = 0.5822770595550537 + 10.0 * 8.543221473693848
Epoch 510, val loss: 0.5992817878723145
Epoch 520, training loss: 85.94502258300781 = 0.5726253986358643 + 10.0 * 8.537240028381348
Epoch 520, val loss: 0.5902572274208069
Epoch 530, training loss: 85.88782501220703 = 0.5631589889526367 + 10.0 * 8.532466888427734
Epoch 530, val loss: 0.5814715027809143
Epoch 540, training loss: 85.84381866455078 = 0.5539756417274475 + 10.0 * 8.528984069824219
Epoch 540, val loss: 0.5729642510414124
Epoch 550, training loss: 85.79499053955078 = 0.545315682888031 + 10.0 * 8.524967193603516
Epoch 550, val loss: 0.5649608969688416
Epoch 560, training loss: 85.75348663330078 = 0.5370504856109619 + 10.0 * 8.52164363861084
Epoch 560, val loss: 0.5573880076408386
Epoch 570, training loss: 85.71524810791016 = 0.5291600823402405 + 10.0 * 8.518609046936035
Epoch 570, val loss: 0.5501701831817627
Epoch 580, training loss: 85.67892456054688 = 0.521681547164917 + 10.0 * 8.515724182128906
Epoch 580, val loss: 0.5433986783027649
Epoch 590, training loss: 85.6435317993164 = 0.5145885944366455 + 10.0 * 8.512894630432129
Epoch 590, val loss: 0.5369886755943298
Epoch 600, training loss: 85.60832977294922 = 0.5078579187393188 + 10.0 * 8.51004695892334
Epoch 600, val loss: 0.5309317111968994
Epoch 610, training loss: 85.57341003417969 = 0.5014678835868835 + 10.0 * 8.507194519042969
Epoch 610, val loss: 0.5252294540405273
Epoch 620, training loss: 85.54117584228516 = 0.4953833222389221 + 10.0 * 8.504579544067383
Epoch 620, val loss: 0.5198371410369873
Epoch 630, training loss: 85.51519775390625 = 0.4895714223384857 + 10.0 * 8.502562522888184
Epoch 630, val loss: 0.5147086977958679
Epoch 640, training loss: 85.48683166503906 = 0.4840450584888458 + 10.0 * 8.50027847290039
Epoch 640, val loss: 0.5098473429679871
Epoch 650, training loss: 85.45310974121094 = 0.47881805896759033 + 10.0 * 8.497428894042969
Epoch 650, val loss: 0.5053290128707886
Epoch 660, training loss: 85.42632293701172 = 0.4738413989543915 + 10.0 * 8.495248794555664
Epoch 660, val loss: 0.5010625123977661
Epoch 670, training loss: 85.40238952636719 = 0.46907225251197815 + 10.0 * 8.493331909179688
Epoch 670, val loss: 0.49698254466056824
Epoch 680, training loss: 85.38048553466797 = 0.4645124077796936 + 10.0 * 8.491597175598145
Epoch 680, val loss: 0.49312570691108704
Epoch 690, training loss: 85.38997650146484 = 0.46016138792037964 + 10.0 * 8.492981910705566
Epoch 690, val loss: 0.4894498884677887
Epoch 700, training loss: 85.35079193115234 = 0.45596277713775635 + 10.0 * 8.489482879638672
Epoch 700, val loss: 0.4860399067401886
Epoch 710, training loss: 85.31705474853516 = 0.45201706886291504 + 10.0 * 8.486503601074219
Epoch 710, val loss: 0.482826292514801
Epoch 720, training loss: 85.29541778564453 = 0.4482450783252716 + 10.0 * 8.48471736907959
Epoch 720, val loss: 0.4797852635383606
Epoch 730, training loss: 85.27041625976562 = 0.444622278213501 + 10.0 * 8.482579231262207
Epoch 730, val loss: 0.47691646218299866
Epoch 740, training loss: 85.24748229980469 = 0.4411791265010834 + 10.0 * 8.480630874633789
Epoch 740, val loss: 0.47422555088996887
Epoch 750, training loss: 85.28095245361328 = 0.4378884434700012 + 10.0 * 8.484306335449219
Epoch 750, val loss: 0.4716969132423401
Epoch 760, training loss: 85.21763610839844 = 0.434702068567276 + 10.0 * 8.478293418884277
Epoch 760, val loss: 0.46931785345077515
Epoch 770, training loss: 85.18336486816406 = 0.4316917657852173 + 10.0 * 8.475167274475098
Epoch 770, val loss: 0.46706512570381165
Epoch 780, training loss: 85.1556625366211 = 0.42879819869995117 + 10.0 * 8.472686767578125
Epoch 780, val loss: 0.4650046229362488
Epoch 790, training loss: 85.13276672363281 = 0.4260096848011017 + 10.0 * 8.470675468444824
Epoch 790, val loss: 0.4630039930343628
Epoch 800, training loss: 85.12557220458984 = 0.42331719398498535 + 10.0 * 8.47022533416748
Epoch 800, val loss: 0.461137592792511
Epoch 810, training loss: 85.10062408447266 = 0.42070454359054565 + 10.0 * 8.467991828918457
Epoch 810, val loss: 0.45932310819625854
Epoch 820, training loss: 85.07891082763672 = 0.41819339990615845 + 10.0 * 8.466072082519531
Epoch 820, val loss: 0.4576151669025421
Epoch 830, training loss: 85.05577850341797 = 0.41577303409576416 + 10.0 * 8.464000701904297
Epoch 830, val loss: 0.4560414254665375
Epoch 840, training loss: 85.03781127929688 = 0.4134427309036255 + 10.0 * 8.46243667602539
Epoch 840, val loss: 0.45451217889785767
Epoch 850, training loss: 85.0390853881836 = 0.41118359565734863 + 10.0 * 8.462789535522461
Epoch 850, val loss: 0.45308056473731995
Epoch 860, training loss: 85.01274871826172 = 0.40899187326431274 + 10.0 * 8.460375785827637
Epoch 860, val loss: 0.4516776204109192
Epoch 870, training loss: 84.98388671875 = 0.4068714380264282 + 10.0 * 8.457701683044434
Epoch 870, val loss: 0.45033666491508484
Epoch 880, training loss: 84.96327209472656 = 0.4048156440258026 + 10.0 * 8.455845832824707
Epoch 880, val loss: 0.44910815358161926
Epoch 890, training loss: 84.94509887695312 = 0.4028233289718628 + 10.0 * 8.454227447509766
Epoch 890, val loss: 0.44790300726890564
Epoch 900, training loss: 84.93450927734375 = 0.4008900821208954 + 10.0 * 8.453362464904785
Epoch 900, val loss: 0.4467707574367523
Epoch 910, training loss: 84.94677734375 = 0.39896920323371887 + 10.0 * 8.454780578613281
Epoch 910, val loss: 0.4456740915775299
Epoch 920, training loss: 84.91529083251953 = 0.39710476994514465 + 10.0 * 8.451818466186523
Epoch 920, val loss: 0.4444962739944458
Epoch 930, training loss: 84.88359069824219 = 0.39530691504478455 + 10.0 * 8.448827743530273
Epoch 930, val loss: 0.44358348846435547
Epoch 940, training loss: 84.86766815185547 = 0.39355891942977905 + 10.0 * 8.447410583496094
Epoch 940, val loss: 0.4425686299800873
Epoch 950, training loss: 84.85421752929688 = 0.39185652136802673 + 10.0 * 8.446236610412598
Epoch 950, val loss: 0.44163480401039124
Epoch 960, training loss: 84.8407974243164 = 0.3901878595352173 + 10.0 * 8.445060729980469
Epoch 960, val loss: 0.4407472014427185
Epoch 970, training loss: 84.8279037475586 = 0.3885478377342224 + 10.0 * 8.44393539428711
Epoch 970, val loss: 0.4398607611656189
Epoch 980, training loss: 84.81519317626953 = 0.3869386315345764 + 10.0 * 8.442825317382812
Epoch 980, val loss: 0.4390207529067993
Epoch 990, training loss: 84.8109359741211 = 0.3853600323200226 + 10.0 * 8.442557334899902
Epoch 990, val loss: 0.4381800889968872
Epoch 1000, training loss: 84.82005310058594 = 0.38380083441734314 + 10.0 * 8.443624496459961
Epoch 1000, val loss: 0.43746861815452576
Epoch 1010, training loss: 84.78577423095703 = 0.3822765648365021 + 10.0 * 8.440349578857422
Epoch 1010, val loss: 0.4366367757320404
Epoch 1020, training loss: 84.7686538696289 = 0.38079744577407837 + 10.0 * 8.438785552978516
Epoch 1020, val loss: 0.43596121668815613
Epoch 1030, training loss: 84.76087188720703 = 0.37935304641723633 + 10.0 * 8.438151359558105
Epoch 1030, val loss: 0.43524792790412903
Epoch 1040, training loss: 84.74671936035156 = 0.37793728709220886 + 10.0 * 8.436878204345703
Epoch 1040, val loss: 0.43457818031311035
Epoch 1050, training loss: 84.73126220703125 = 0.37655600905418396 + 10.0 * 8.435470581054688
Epoch 1050, val loss: 0.43393832445144653
Epoch 1060, training loss: 84.71150970458984 = 0.37520161271095276 + 10.0 * 8.43363094329834
Epoch 1060, val loss: 0.43332159519195557
Epoch 1070, training loss: 84.7061538696289 = 0.37387555837631226 + 10.0 * 8.4332275390625
Epoch 1070, val loss: 0.4327074885368347
Epoch 1080, training loss: 84.72972869873047 = 0.3725719153881073 + 10.0 * 8.435715675354004
Epoch 1080, val loss: 0.432158887386322
Epoch 1090, training loss: 84.68241119384766 = 0.3712780177593231 + 10.0 * 8.431113243103027
Epoch 1090, val loss: 0.43153247237205505
Epoch 1100, training loss: 84.65929412841797 = 0.37001535296440125 + 10.0 * 8.428927421569824
Epoch 1100, val loss: 0.43102768063545227
Epoch 1110, training loss: 84.6417236328125 = 0.3687770962715149 + 10.0 * 8.427294731140137
Epoch 1110, val loss: 0.4304702877998352
Epoch 1120, training loss: 84.62687683105469 = 0.36755725741386414 + 10.0 * 8.425931930541992
Epoch 1120, val loss: 0.42996343970298767
Epoch 1130, training loss: 84.66633605957031 = 0.3663485050201416 + 10.0 * 8.429998397827148
Epoch 1130, val loss: 0.4294538199901581
Epoch 1140, training loss: 84.60161590576172 = 0.3651370406150818 + 10.0 * 8.4236478805542
Epoch 1140, val loss: 0.4288822114467621
Epoch 1150, training loss: 84.59341430664062 = 0.36395159363746643 + 10.0 * 8.422945976257324
Epoch 1150, val loss: 0.4284720718860626
Epoch 1160, training loss: 84.57678985595703 = 0.36278262734413147 + 10.0 * 8.42140007019043
Epoch 1160, val loss: 0.427935928106308
Epoch 1170, training loss: 84.64005279541016 = 0.361625999212265 + 10.0 * 8.42784309387207
Epoch 1170, val loss: 0.4274693429470062
Epoch 1180, training loss: 84.5702133178711 = 0.3604569435119629 + 10.0 * 8.420975685119629
Epoch 1180, val loss: 0.4269571900367737
Epoch 1190, training loss: 84.54483032226562 = 0.35931596159935 + 10.0 * 8.418551445007324
Epoch 1190, val loss: 0.4265282154083252
Epoch 1200, training loss: 84.53534698486328 = 0.35819026827812195 + 10.0 * 8.417715072631836
Epoch 1200, val loss: 0.4260811507701874
Epoch 1210, training loss: 84.52288055419922 = 0.3570750951766968 + 10.0 * 8.416580200195312
Epoch 1210, val loss: 0.4256395101547241
Epoch 1220, training loss: 84.51341247558594 = 0.35596877336502075 + 10.0 * 8.415743827819824
Epoch 1220, val loss: 0.42521384358406067
Epoch 1230, training loss: 84.50524139404297 = 0.3548678457736969 + 10.0 * 8.415037155151367
Epoch 1230, val loss: 0.42478829622268677
Epoch 1240, training loss: 84.53295135498047 = 0.35376855731010437 + 10.0 * 8.41791820526123
Epoch 1240, val loss: 0.4243711531162262
Epoch 1250, training loss: 84.4911117553711 = 0.352664589881897 + 10.0 * 8.41384506225586
Epoch 1250, val loss: 0.42390894889831543
Epoch 1260, training loss: 84.48406982421875 = 0.351579487323761 + 10.0 * 8.413249015808105
Epoch 1260, val loss: 0.4235537648200989
Epoch 1270, training loss: 84.52481842041016 = 0.35050615668296814 + 10.0 * 8.417430877685547
Epoch 1270, val loss: 0.4231314957141876
Epoch 1280, training loss: 84.48521423339844 = 0.34942877292633057 + 10.0 * 8.413578987121582
Epoch 1280, val loss: 0.4227584898471832
Epoch 1290, training loss: 84.46417236328125 = 0.3483734726905823 + 10.0 * 8.411580085754395
Epoch 1290, val loss: 0.4223960340023041
Epoch 1300, training loss: 84.45187377929688 = 0.34733423590660095 + 10.0 * 8.410453796386719
Epoch 1300, val loss: 0.4220261871814728
Epoch 1310, training loss: 84.44396209716797 = 0.34630873799324036 + 10.0 * 8.409765243530273
Epoch 1310, val loss: 0.4216807782649994
Epoch 1320, training loss: 84.43694305419922 = 0.34529441595077515 + 10.0 * 8.409165382385254
Epoch 1320, val loss: 0.42134833335876465
Epoch 1330, training loss: 84.43738555908203 = 0.3442898988723755 + 10.0 * 8.409309387207031
Epoch 1330, val loss: 0.42100998759269714
Epoch 1340, training loss: 84.43501281738281 = 0.3432852029800415 + 10.0 * 8.409173011779785
Epoch 1340, val loss: 0.4206666350364685
Epoch 1350, training loss: 84.4245376586914 = 0.342286080121994 + 10.0 * 8.408225059509277
Epoch 1350, val loss: 0.42033979296684265
Epoch 1360, training loss: 84.41486358642578 = 0.34130412340164185 + 10.0 * 8.407356262207031
Epoch 1360, val loss: 0.42004746198654175
Epoch 1370, training loss: 84.4006576538086 = 0.3403373956680298 + 10.0 * 8.406031608581543
Epoch 1370, val loss: 0.41975927352905273
Epoch 1380, training loss: 84.39334106445312 = 0.3393860459327698 + 10.0 * 8.4053955078125
Epoch 1380, val loss: 0.4194450378417969
Epoch 1390, training loss: 84.42172241210938 = 0.338442862033844 + 10.0 * 8.40832805633545
Epoch 1390, val loss: 0.4191642701625824
Epoch 1400, training loss: 84.40373992919922 = 0.3375014066696167 + 10.0 * 8.406623840332031
Epoch 1400, val loss: 0.41894662380218506
Epoch 1410, training loss: 84.39726257324219 = 0.33656495809555054 + 10.0 * 8.4060697555542
Epoch 1410, val loss: 0.41871464252471924
Epoch 1420, training loss: 84.37483978271484 = 0.335640549659729 + 10.0 * 8.40392017364502
Epoch 1420, val loss: 0.41836676001548767
Epoch 1430, training loss: 84.36065673828125 = 0.3347357213497162 + 10.0 * 8.402592658996582
Epoch 1430, val loss: 0.41816818714141846
Epoch 1440, training loss: 84.3480453491211 = 0.3338405191898346 + 10.0 * 8.401420593261719
Epoch 1440, val loss: 0.41791507601737976
Epoch 1450, training loss: 84.3442153930664 = 0.3329525589942932 + 10.0 * 8.40112590789795
Epoch 1450, val loss: 0.41769933700561523
Epoch 1460, training loss: 84.36202239990234 = 0.33206707239151 + 10.0 * 8.402995109558105
Epoch 1460, val loss: 0.41745612025260925
Epoch 1470, training loss: 84.32660675048828 = 0.33117547631263733 + 10.0 * 8.399542808532715
Epoch 1470, val loss: 0.41724517941474915
Epoch 1480, training loss: 84.32264709472656 = 0.3302975594997406 + 10.0 * 8.399234771728516
Epoch 1480, val loss: 0.4170691668987274
Epoch 1490, training loss: 84.31651306152344 = 0.32942745089530945 + 10.0 * 8.39870834350586
Epoch 1490, val loss: 0.4168223440647125
Epoch 1500, training loss: 84.31270599365234 = 0.32856225967407227 + 10.0 * 8.398414611816406
Epoch 1500, val loss: 0.4166439175605774
Epoch 1510, training loss: 84.30923461914062 = 0.32769840955734253 + 10.0 * 8.398153305053711
Epoch 1510, val loss: 0.4164639711380005
Epoch 1520, training loss: 84.32920837402344 = 0.32683756947517395 + 10.0 * 8.400237083435059
Epoch 1520, val loss: 0.4162943661212921
Epoch 1530, training loss: 84.29142761230469 = 0.3259643316268921 + 10.0 * 8.396546363830566
Epoch 1530, val loss: 0.41603711247444153
Epoch 1540, training loss: 84.28401184082031 = 0.32510727643966675 + 10.0 * 8.395891189575195
Epoch 1540, val loss: 0.4159209728240967
Epoch 1550, training loss: 84.27662658691406 = 0.324260413646698 + 10.0 * 8.39523696899414
Epoch 1550, val loss: 0.4156990051269531
Epoch 1560, training loss: 84.30764770507812 = 0.32341939210891724 + 10.0 * 8.398423194885254
Epoch 1560, val loss: 0.41557440161705017
Epoch 1570, training loss: 84.27791595458984 = 0.3225765526294708 + 10.0 * 8.395533561706543
Epoch 1570, val loss: 0.41537705063819885
Epoch 1580, training loss: 84.25878143310547 = 0.3217432498931885 + 10.0 * 8.39370346069336
Epoch 1580, val loss: 0.41525036096572876
Epoch 1590, training loss: 84.24950408935547 = 0.3209182620048523 + 10.0 * 8.392858505249023
Epoch 1590, val loss: 0.41509878635406494
Epoch 1600, training loss: 84.24358367919922 = 0.32009798288345337 + 10.0 * 8.392348289489746
Epoch 1600, val loss: 0.4149438142776489
Epoch 1610, training loss: 84.2411880493164 = 0.31928104162216187 + 10.0 * 8.392190933227539
Epoch 1610, val loss: 0.41480398178100586
Epoch 1620, training loss: 84.25834655761719 = 0.3184591233730316 + 10.0 * 8.393988609313965
Epoch 1620, val loss: 0.4147001802921295
Epoch 1630, training loss: 84.28339385986328 = 0.3176392912864685 + 10.0 * 8.396574974060059
Epoch 1630, val loss: 0.41454648971557617
Epoch 1640, training loss: 84.236083984375 = 0.3168186843395233 + 10.0 * 8.391926765441895
Epoch 1640, val loss: 0.41434764862060547
Epoch 1650, training loss: 84.2215347290039 = 0.3160158693790436 + 10.0 * 8.390551567077637
Epoch 1650, val loss: 0.4143015146255493
Epoch 1660, training loss: 84.21495056152344 = 0.31522366404533386 + 10.0 * 8.389972686767578
Epoch 1660, val loss: 0.4141634702682495
Epoch 1670, training loss: 84.20832061767578 = 0.3144374489784241 + 10.0 * 8.389388084411621
Epoch 1670, val loss: 0.41408559679985046
Epoch 1680, training loss: 84.20308685302734 = 0.3136562407016754 + 10.0 * 8.38894271850586
Epoch 1680, val loss: 0.41397881507873535
Epoch 1690, training loss: 84.20474243164062 = 0.3128787875175476 + 10.0 * 8.389185905456543
Epoch 1690, val loss: 0.4138815701007843
Epoch 1700, training loss: 84.2214126586914 = 0.3120986521244049 + 10.0 * 8.390932083129883
Epoch 1700, val loss: 0.41383907198905945
Epoch 1710, training loss: 84.20352935791016 = 0.3113173246383667 + 10.0 * 8.38922119140625
Epoch 1710, val loss: 0.4136945307254791
Epoch 1720, training loss: 84.19252014160156 = 0.3105417490005493 + 10.0 * 8.388197898864746
Epoch 1720, val loss: 0.4136165380477905
Epoch 1730, training loss: 84.18230438232422 = 0.3097800612449646 + 10.0 * 8.387252807617188
Epoch 1730, val loss: 0.4135340750217438
Epoch 1740, training loss: 84.17681121826172 = 0.30902281403541565 + 10.0 * 8.386778831481934
Epoch 1740, val loss: 0.41346555948257446
Epoch 1750, training loss: 84.1910171508789 = 0.3082721531391144 + 10.0 * 8.388274192810059
Epoch 1750, val loss: 0.41339558362960815
Epoch 1760, training loss: 84.1751937866211 = 0.30751457810401917 + 10.0 * 8.386767387390137
Epoch 1760, val loss: 0.4133549630641937
Epoch 1770, training loss: 84.16404724121094 = 0.3067616820335388 + 10.0 * 8.38572883605957
Epoch 1770, val loss: 0.41333967447280884
Epoch 1780, training loss: 84.15727233886719 = 0.3060207962989807 + 10.0 * 8.385125160217285
Epoch 1780, val loss: 0.4132803678512573
Epoch 1790, training loss: 84.1535873413086 = 0.305287629365921 + 10.0 * 8.3848295211792
Epoch 1790, val loss: 0.413253515958786
Epoch 1800, training loss: 84.15497589111328 = 0.30455976724624634 + 10.0 * 8.385042190551758
Epoch 1800, val loss: 0.41322001814842224
Epoch 1810, training loss: 84.18403625488281 = 0.3038310706615448 + 10.0 * 8.388020515441895
Epoch 1810, val loss: 0.4131999611854553
Epoch 1820, training loss: 84.15216827392578 = 0.3030971586704254 + 10.0 * 8.384906768798828
Epoch 1820, val loss: 0.4131917357444763
Epoch 1830, training loss: 84.13995361328125 = 0.3023742735385895 + 10.0 * 8.383757591247559
Epoch 1830, val loss: 0.4132036864757538
Epoch 1840, training loss: 84.13553619384766 = 0.30165526270866394 + 10.0 * 8.383387565612793
Epoch 1840, val loss: 0.4132196009159088
Epoch 1850, training loss: 84.17382049560547 = 0.30094102025032043 + 10.0 * 8.387288093566895
Epoch 1850, val loss: 0.4132799506187439
Epoch 1860, training loss: 84.13127899169922 = 0.3002197742462158 + 10.0 * 8.383106231689453
Epoch 1860, val loss: 0.41313520073890686
Epoch 1870, training loss: 84.11931610107422 = 0.299508661031723 + 10.0 * 8.381980895996094
Epoch 1870, val loss: 0.4132387638092041
Epoch 1880, training loss: 84.11224365234375 = 0.29880374670028687 + 10.0 * 8.381343841552734
Epoch 1880, val loss: 0.4132544994354248
Epoch 1890, training loss: 84.10879516601562 = 0.29810404777526855 + 10.0 * 8.38106918334961
Epoch 1890, val loss: 0.4132624864578247
Epoch 1900, training loss: 84.12728118896484 = 0.297405481338501 + 10.0 * 8.382987022399902
Epoch 1900, val loss: 0.4133381247520447
Epoch 1910, training loss: 84.1024398803711 = 0.2967011630535126 + 10.0 * 8.380574226379395
Epoch 1910, val loss: 0.4133787155151367
Epoch 1920, training loss: 84.09304809570312 = 0.29600754380226135 + 10.0 * 8.379704475402832
Epoch 1920, val loss: 0.4133557677268982
Epoch 1930, training loss: 84.08738708496094 = 0.295320987701416 + 10.0 * 8.379206657409668
Epoch 1930, val loss: 0.41344574093818665
Epoch 1940, training loss: 84.0854721069336 = 0.2946413457393646 + 10.0 * 8.379083633422852
Epoch 1940, val loss: 0.41346314549446106
Epoch 1950, training loss: 84.14717864990234 = 0.2939625680446625 + 10.0 * 8.385321617126465
Epoch 1950, val loss: 0.41351813077926636
Epoch 1960, training loss: 84.0738296508789 = 0.2932734787464142 + 10.0 * 8.378055572509766
Epoch 1960, val loss: 0.4135613441467285
Epoch 1970, training loss: 84.07318878173828 = 0.29259926080703735 + 10.0 * 8.378058433532715
Epoch 1970, val loss: 0.41360411047935486
Epoch 1980, training loss: 84.05963134765625 = 0.29193225502967834 + 10.0 * 8.37677001953125
Epoch 1980, val loss: 0.4136652946472168
Epoch 1990, training loss: 84.05553436279297 = 0.29126957058906555 + 10.0 * 8.376426696777344
Epoch 1990, val loss: 0.4137077331542969
Epoch 2000, training loss: 84.05826568603516 = 0.2906095087528229 + 10.0 * 8.376765251159668
Epoch 2000, val loss: 0.41377002000808716
Epoch 2010, training loss: 84.05768585205078 = 0.2899460792541504 + 10.0 * 8.376773834228516
Epoch 2010, val loss: 0.413849800825119
Epoch 2020, training loss: 84.04107666015625 = 0.2892824709415436 + 10.0 * 8.375179290771484
Epoch 2020, val loss: 0.4139042794704437
Epoch 2030, training loss: 84.05763244628906 = 0.28862595558166504 + 10.0 * 8.376900672912598
Epoch 2030, val loss: 0.4139248728752136
Epoch 2040, training loss: 84.0368881225586 = 0.2879612147808075 + 10.0 * 8.374892234802246
Epoch 2040, val loss: 0.4140758216381073
Epoch 2050, training loss: 84.03404998779297 = 0.2872985005378723 + 10.0 * 8.374674797058105
Epoch 2050, val loss: 0.41406992077827454
Epoch 2060, training loss: 84.02945709228516 = 0.2866467237472534 + 10.0 * 8.37428092956543
Epoch 2060, val loss: 0.41418805718421936
Epoch 2070, training loss: 84.02070617675781 = 0.2860005497932434 + 10.0 * 8.373470306396484
Epoch 2070, val loss: 0.41424739360809326
Epoch 2080, training loss: 84.01480102539062 = 0.28535670042037964 + 10.0 * 8.372944831848145
Epoch 2080, val loss: 0.4143620431423187
Epoch 2090, training loss: 84.00980377197266 = 0.28471478819847107 + 10.0 * 8.372509002685547
Epoch 2090, val loss: 0.4144267141819
Epoch 2100, training loss: 84.00524139404297 = 0.28407183289527893 + 10.0 * 8.372117042541504
Epoch 2100, val loss: 0.41453835368156433
Epoch 2110, training loss: 84.0008544921875 = 0.28342756628990173 + 10.0 * 8.371743202209473
Epoch 2110, val loss: 0.41465112566947937
Epoch 2120, training loss: 83.99861145019531 = 0.28278255462646484 + 10.0 * 8.371582984924316
Epoch 2120, val loss: 0.41473662853240967
Epoch 2130, training loss: 84.06353759765625 = 0.28214117884635925 + 10.0 * 8.37813949584961
Epoch 2130, val loss: 0.41476690769195557
Epoch 2140, training loss: 84.00735473632812 = 0.2814786434173584 + 10.0 * 8.372587203979492
Epoch 2140, val loss: 0.41492360830307007
Epoch 2150, training loss: 83.98939514160156 = 0.2808270752429962 + 10.0 * 8.370856285095215
Epoch 2150, val loss: 0.4150603413581848
Epoch 2160, training loss: 83.98170471191406 = 0.2801857590675354 + 10.0 * 8.37015151977539
Epoch 2160, val loss: 0.4151610732078552
Epoch 2170, training loss: 83.97885131835938 = 0.27954789996147156 + 10.0 * 8.369930267333984
Epoch 2170, val loss: 0.4152987599372864
Epoch 2180, training loss: 83.97473907470703 = 0.2789100706577301 + 10.0 * 8.369583129882812
Epoch 2180, val loss: 0.4154067039489746
Epoch 2190, training loss: 83.98616790771484 = 0.27827200293540955 + 10.0 * 8.370789527893066
Epoch 2190, val loss: 0.4154963493347168
Epoch 2200, training loss: 83.98654174804688 = 0.2776245176792145 + 10.0 * 8.370891571044922
Epoch 2200, val loss: 0.4156211018562317
Epoch 2210, training loss: 83.96920776367188 = 0.2769738733768463 + 10.0 * 8.369222640991211
Epoch 2210, val loss: 0.41578710079193115
Epoch 2220, training loss: 83.96160125732422 = 0.2763291895389557 + 10.0 * 8.36852741241455
Epoch 2220, val loss: 0.41587352752685547
Epoch 2230, training loss: 83.96045684814453 = 0.27568739652633667 + 10.0 * 8.368476867675781
Epoch 2230, val loss: 0.416025847196579
Epoch 2240, training loss: 84.00006103515625 = 0.27504780888557434 + 10.0 * 8.372501373291016
Epoch 2240, val loss: 0.4162033200263977
Epoch 2250, training loss: 83.96211242675781 = 0.2743980586528778 + 10.0 * 8.36877155303955
Epoch 2250, val loss: 0.41624024510383606
Epoch 2260, training loss: 83.94943237304688 = 0.27375516295433044 + 10.0 * 8.36756706237793
Epoch 2260, val loss: 0.4164201319217682
Epoch 2270, training loss: 83.942626953125 = 0.27311304211616516 + 10.0 * 8.366951942443848
Epoch 2270, val loss: 0.4165500998497009
Epoch 2280, training loss: 83.93938446044922 = 0.27247077226638794 + 10.0 * 8.366691589355469
Epoch 2280, val loss: 0.4167172312736511
Epoch 2290, training loss: 83.96550750732422 = 0.27182796597480774 + 10.0 * 8.369367599487305
Epoch 2290, val loss: 0.416912704706192
Epoch 2300, training loss: 83.93184661865234 = 0.27117082476615906 + 10.0 * 8.366067886352539
Epoch 2300, val loss: 0.4169129729270935
Epoch 2310, training loss: 83.92839050292969 = 0.27051714062690735 + 10.0 * 8.365787506103516
Epoch 2310, val loss: 0.41710472106933594
Epoch 2320, training loss: 83.92517852783203 = 0.2698676884174347 + 10.0 * 8.365530967712402
Epoch 2320, val loss: 0.4172436594963074
Epoch 2330, training loss: 83.92743682861328 = 0.26922163367271423 + 10.0 * 8.365821838378906
Epoch 2330, val loss: 0.4173562824726105
Epoch 2340, training loss: 83.92736053466797 = 0.2685732841491699 + 10.0 * 8.36587905883789
Epoch 2340, val loss: 0.41752010583877563
Epoch 2350, training loss: 83.9185791015625 = 0.26792246103286743 + 10.0 * 8.365065574645996
Epoch 2350, val loss: 0.41767674684524536
Epoch 2360, training loss: 83.91596221923828 = 0.26727068424224854 + 10.0 * 8.364869117736816
Epoch 2360, val loss: 0.41783973574638367
Epoch 2370, training loss: 83.92598724365234 = 0.26661694049835205 + 10.0 * 8.365937232971191
Epoch 2370, val loss: 0.4180002808570862
Epoch 2380, training loss: 83.90425872802734 = 0.26595446467399597 + 10.0 * 8.36383056640625
Epoch 2380, val loss: 0.4180905222892761
Epoch 2390, training loss: 83.903076171875 = 0.26529309153556824 + 10.0 * 8.363779067993164
Epoch 2390, val loss: 0.4182717502117157
Epoch 2400, training loss: 83.99227905273438 = 0.264639675617218 + 10.0 * 8.372763633728027
Epoch 2400, val loss: 0.41857272386550903
Epoch 2410, training loss: 83.94009399414062 = 0.2639639973640442 + 10.0 * 8.367612838745117
Epoch 2410, val loss: 0.41844090819358826
Epoch 2420, training loss: 83.89299011230469 = 0.263292521238327 + 10.0 * 8.362970352172852
Epoch 2420, val loss: 0.41870442032814026
Epoch 2430, training loss: 83.88885498046875 = 0.26263195276260376 + 10.0 * 8.362622261047363
Epoch 2430, val loss: 0.41890937089920044
Epoch 2440, training loss: 83.88329315185547 = 0.2619723081588745 + 10.0 * 8.36213207244873
Epoch 2440, val loss: 0.41898924112319946
Epoch 2450, training loss: 83.87785339355469 = 0.26130977272987366 + 10.0 * 8.361654281616211
Epoch 2450, val loss: 0.4191734790802002
Epoch 2460, training loss: 83.87394714355469 = 0.26064690947532654 + 10.0 * 8.361330032348633
Epoch 2460, val loss: 0.41931188106536865
Epoch 2470, training loss: 83.87157440185547 = 0.2599828839302063 + 10.0 * 8.361159324645996
Epoch 2470, val loss: 0.4194810688495636
Epoch 2480, training loss: 83.92279052734375 = 0.25931939482688904 + 10.0 * 8.36634635925293
Epoch 2480, val loss: 0.4196510314941406
Epoch 2490, training loss: 83.88169860839844 = 0.25864487886428833 + 10.0 * 8.362305641174316
Epoch 2490, val loss: 0.41979002952575684
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8396752917300863
0.8650293414475115
The final CL Acc:0.84035, 0.00172, The final GNN Acc:0.86443, 0.00091
Begin epxeriment: noisy_level: 0.2 cont_weight: 10 epoch: der1:0.3 der2:0.2 dfr1:0.4 dfr2:0
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=2500, cl_num_layers=2, cl_num_proj_hidden=64, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=10.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.2, drop_feat_rate_1=0.4, drop_feat_rate_2=0.0, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=32, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, noisy_level=0.2, num_hidden=64, num_proj_hidden=64, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=40, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 88648])
add edge: torch.Size([2, 106612])
remove edge: torch.Size([2, 71058])
updated graph: torch.Size([2, 89022])
=== Noisy graph ===
/home/project-robust-contrastive/Robust-Contrastive-Learning/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 106.92898559570312 = 1.1068559885025024 + 10.0 * 10.582212448120117
Epoch 0, val loss: 1.1070020198822021
Epoch 10, training loss: 106.91934967041016 = 1.101996898651123 + 10.0 * 10.581735610961914
Epoch 10, val loss: 1.102145791053772
Epoch 20, training loss: 106.89571380615234 = 1.096992015838623 + 10.0 * 10.579872131347656
Epoch 20, val loss: 1.0971019268035889
Epoch 30, training loss: 106.81163787841797 = 1.0916670560836792 + 10.0 * 10.571996688842773
Epoch 30, val loss: 1.0917434692382812
Epoch 40, training loss: 106.4959716796875 = 1.086011528968811 + 10.0 * 10.540995597839355
Epoch 40, val loss: 1.0860494375228882
Epoch 50, training loss: 105.43559265136719 = 1.0798931121826172 + 10.0 * 10.435569763183594
Epoch 50, val loss: 1.0798940658569336
Epoch 60, training loss: 102.13224792480469 = 1.0736730098724365 + 10.0 * 10.105857849121094
Epoch 60, val loss: 1.0736490488052368
Epoch 70, training loss: 97.88438415527344 = 1.0668280124664307 + 10.0 * 9.681756019592285
Epoch 70, val loss: 1.0668514966964722
Epoch 80, training loss: 97.40552520751953 = 1.0613133907318115 + 10.0 * 9.634421348571777
Epoch 80, val loss: 1.0614502429962158
Epoch 90, training loss: 96.6977310180664 = 1.0574314594268799 + 10.0 * 9.564029693603516
Epoch 90, val loss: 1.0576874017715454
Epoch 100, training loss: 95.38314819335938 = 1.0548230409622192 + 10.0 * 9.432832717895508
Epoch 100, val loss: 1.0552221536636353
Epoch 110, training loss: 93.38103485107422 = 1.0534000396728516 + 10.0 * 9.232763290405273
Epoch 110, val loss: 1.0539238452911377
Epoch 120, training loss: 91.75684356689453 = 1.0518797636032104 + 10.0 * 9.070496559143066
Epoch 120, val loss: 1.0524868965148926
Epoch 130, training loss: 90.93109893798828 = 1.0504164695739746 + 10.0 * 8.988068580627441
Epoch 130, val loss: 1.051070213317871
Epoch 140, training loss: 90.39471435546875 = 1.0487873554229736 + 10.0 * 8.934592247009277
Epoch 140, val loss: 1.0494633913040161
Epoch 150, training loss: 90.03043365478516 = 1.0470792055130005 + 10.0 * 8.898335456848145
Epoch 150, val loss: 1.0477888584136963
Epoch 160, training loss: 89.73394775390625 = 1.0455825328826904 + 10.0 * 8.868836402893066
Epoch 160, val loss: 1.0463612079620361
Epoch 170, training loss: 89.3621597290039 = 1.0445250272750854 + 10.0 * 8.83176326751709
Epoch 170, val loss: 1.0453848838806152
Epoch 180, training loss: 89.03290557861328 = 1.0438475608825684 + 10.0 * 8.798906326293945
Epoch 180, val loss: 1.0447279214859009
Epoch 190, training loss: 88.77391052246094 = 1.0431938171386719 + 10.0 * 8.773072242736816
Epoch 190, val loss: 1.0441105365753174
Epoch 200, training loss: 88.53801727294922 = 1.0425039529800415 + 10.0 * 8.749551773071289
Epoch 200, val loss: 1.0434489250183105
Epoch 210, training loss: 88.29253387451172 = 1.0418548583984375 + 10.0 * 8.725068092346191
Epoch 210, val loss: 1.0428420305252075
Epoch 220, training loss: 88.07231140136719 = 1.041258692741394 + 10.0 * 8.703104972839355
Epoch 220, val loss: 1.0422935485839844
Epoch 230, training loss: 87.89079284667969 = 1.0406745672225952 + 10.0 * 8.685011863708496
Epoch 230, val loss: 1.0417476892471313
Epoch 240, training loss: 87.7623291015625 = 1.0400508642196655 + 10.0 * 8.67222785949707
Epoch 240, val loss: 1.0411434173583984
Epoch 250, training loss: 87.64677429199219 = 1.039339542388916 + 10.0 * 8.660743713378906
Epoch 250, val loss: 1.0404729843139648
Epoch 260, training loss: 87.53651428222656 = 1.0385956764221191 + 10.0 * 8.649791717529297
Epoch 260, val loss: 1.0397725105285645
Epoch 270, training loss: 87.45527648925781 = 1.037855625152588 + 10.0 * 8.641741752624512
Epoch 270, val loss: 1.0390721559524536
Epoch 280, training loss: 87.36531066894531 = 1.0370770692825317 + 10.0 * 8.63282299041748
Epoch 280, val loss: 1.0383645296096802
Epoch 290, training loss: 87.2872314453125 = 1.0362812280654907 + 10.0 * 8.62509536743164
Epoch 290, val loss: 1.0376259088516235
Epoch 300, training loss: 87.2073974609375 = 1.0354533195495605 + 10.0 * 8.617194175720215
Epoch 300, val loss: 1.0368472337722778
Epoch 310, training loss: 87.12785339355469 = 1.0346015691757202 + 10.0 * 8.609325408935547
Epoch 310, val loss: 1.036067008972168
Epoch 320, training loss: 87.0550765991211 = 1.0337369441986084 + 10.0 * 8.602133750915527
Epoch 320, val loss: 1.0352689027786255
Epoch 330, training loss: 86.98322296142578 = 1.0328208208084106 + 10.0 * 8.595040321350098
Epoch 330, val loss: 1.034415364265442
Epoch 340, training loss: 86.90789031982422 = 1.031876564025879 + 10.0 * 8.587601661682129
Epoch 340, val loss: 1.033547282218933
Epoch 350, training loss: 86.84552001953125 = 1.0309314727783203 + 10.0 * 8.581459045410156
Epoch 350, val loss: 1.032675862312317
Epoch 360, training loss: 86.780517578125 = 1.0299568176269531 + 10.0 * 8.575056076049805
Epoch 360, val loss: 1.0317739248275757
Epoch 370, training loss: 86.7230224609375 = 1.028947353363037 + 10.0 * 8.56940746307373
Epoch 370, val loss: 1.030845046043396
Epoch 380, training loss: 86.70658874511719 = 1.0278842449188232 + 10.0 * 8.567870140075684
Epoch 380, val loss: 1.0298378467559814
Epoch 390, training loss: 86.62628173828125 = 1.0267547369003296 + 10.0 * 8.559952735900879
Epoch 390, val loss: 1.0288182497024536
Epoch 400, training loss: 86.573974609375 = 1.0256370306015015 + 10.0 * 8.554834365844727
Epoch 400, val loss: 1.0277880430221558
Epoch 410, training loss: 86.5287094116211 = 1.0244814157485962 + 10.0 * 8.550422668457031
Epoch 410, val loss: 1.0267254114151
Epoch 420, training loss: 86.48872375488281 = 1.023270606994629 + 10.0 * 8.546545028686523
Epoch 420, val loss: 1.025604486465454
Epoch 430, training loss: 86.50405883789062 = 1.0219635963439941 + 10.0 * 8.548209190368652
Epoch 430, val loss: 1.024398922920227
Epoch 440, training loss: 86.43032836914062 = 1.0205494165420532 + 10.0 * 8.54097843170166
Epoch 440, val loss: 1.0230730772018433
Epoch 450, training loss: 86.3941421508789 = 1.0191112756729126 + 10.0 * 8.537503242492676
Epoch 450, val loss: 1.021737813949585
Epoch 460, training loss: 86.36583709716797 = 1.017615556716919 + 10.0 * 8.534822463989258
Epoch 460, val loss: 1.020349144935608
Epoch 470, training loss: 86.3387680053711 = 1.0160328149795532 + 10.0 * 8.53227424621582
Epoch 470, val loss: 1.0188788175582886
Epoch 480, training loss: 86.31632232666016 = 1.014367699623108 + 10.0 * 8.530195236206055
Epoch 480, val loss: 1.0173364877700806
Epoch 490, training loss: 86.2945556640625 = 1.0126018524169922 + 10.0 * 8.52819538116455
Epoch 490, val loss: 1.0156835317611694
Epoch 500, training loss: 86.26458740234375 = 1.0107470750808716 + 10.0 * 8.525383949279785
Epoch 500, val loss: 1.013972282409668
Epoch 510, training loss: 86.24101257324219 = 1.0088344812393188 + 10.0 * 8.523218154907227
Epoch 510, val loss: 1.0122069120407104
Epoch 520, training loss: 86.21556091308594 = 1.0068483352661133 + 10.0 * 8.52087116241455
Epoch 520, val loss: 1.0103669166564941
Epoch 530, training loss: 86.19412231445312 = 1.0047705173492432 + 10.0 * 8.518935203552246
Epoch 530, val loss: 1.0084298849105835
Epoch 540, training loss: 86.20002746582031 = 1.0025075674057007 + 10.0 * 8.51975154876709
Epoch 540, val loss: 1.0063461065292358
Epoch 550, training loss: 86.14959716796875 = 1.0001682043075562 + 10.0 * 8.51494312286377
Epoch 550, val loss: 1.004179835319519
Epoch 560, training loss: 86.12759399414062 = 0.9978062510490417 + 10.0 * 8.512979507446289
Epoch 560, val loss: 1.001988410949707
Epoch 570, training loss: 86.10137176513672 = 0.9953380823135376 + 10.0 * 8.510602951049805
Epoch 570, val loss: 0.9996981024742126
Epoch 580, training loss: 86.08019256591797 = 0.9927420616149902 + 10.0 * 8.508745193481445
Epoch 580, val loss: 0.9972926378250122
Epoch 590, training loss: 86.0588607788086 = 0.9900096654891968 + 10.0 * 8.506884574890137
Epoch 590, val loss: 0.9947599768638611
Epoch 600, training loss: 86.09432983398438 = 0.9871394038200378 + 10.0 * 8.510719299316406
Epoch 600, val loss: 0.9920889139175415
Epoch 610, training loss: 86.03673553466797 = 0.98404860496521 + 10.0 * 8.505269050598145
Epoch 610, val loss: 0.9892084002494812
Epoch 620, training loss: 86.00372314453125 = 0.9809021949768066 + 10.0 * 8.50228214263916
Epoch 620, val loss: 0.9863052368164062
Epoch 630, training loss: 85.9814682006836 = 0.9776439070701599 + 10.0 * 8.500382423400879
Epoch 630, val loss: 0.9832829833030701
Epoch 640, training loss: 85.96204376220703 = 0.974236011505127 + 10.0 * 8.498781204223633
Epoch 640, val loss: 0.9801190495491028
Epoch 650, training loss: 85.9620132446289 = 0.9706463813781738 + 10.0 * 8.499135971069336
Epoch 650, val loss: 0.9767995476722717
Epoch 660, training loss: 85.9500503540039 = 0.9668174982070923 + 10.0 * 8.498323440551758
Epoch 660, val loss: 0.9731802344322205
Epoch 670, training loss: 85.90967559814453 = 0.9628844857215881 + 10.0 * 8.49467945098877
Epoch 670, val loss: 0.9695526957511902
Epoch 680, training loss: 85.88868713378906 = 0.9588357210159302 + 10.0 * 8.492984771728516
Epoch 680, val loss: 0.9657972455024719
Epoch 690, training loss: 85.8708724975586 = 0.95462566614151 + 10.0 * 8.49162483215332
Epoch 690, val loss: 0.9618806838989258
Epoch 700, training loss: 85.86524963378906 = 0.950213611125946 + 10.0 * 8.491503715515137
Epoch 700, val loss: 0.957759439945221
Epoch 710, training loss: 85.84430694580078 = 0.9455485343933105 + 10.0 * 8.489875793457031
Epoch 710, val loss: 0.9534239768981934
Epoch 720, training loss: 85.81560516357422 = 0.940741240978241 + 10.0 * 8.487485885620117
Epoch 720, val loss: 0.9489495158195496
Epoch 730, training loss: 85.79508972167969 = 0.9357690811157227 + 10.0 * 8.485932350158691
Epoch 730, val loss: 0.9443279504776001
Epoch 740, training loss: 85.77583312988281 = 0.9305945634841919 + 10.0 * 8.48452377319336
Epoch 740, val loss: 0.9394988417625427
Epoch 750, training loss: 85.7566146850586 = 0.9251877665519714 + 10.0 * 8.483142852783203
Epoch 750, val loss: 0.9344639778137207
Epoch 760, training loss: 85.81773376464844 = 0.9195507168769836 + 10.0 * 8.489818572998047
Epoch 760, val loss: 0.929140031337738
Epoch 770, training loss: 85.74803924560547 = 0.913529634475708 + 10.0 * 8.483450889587402
Epoch 770, val loss: 0.9235836267471313
Epoch 780, training loss: 85.7055435180664 = 0.9075626134872437 + 10.0 * 8.479798316955566
Epoch 780, val loss: 0.9180746078491211
Epoch 790, training loss: 85.68669128417969 = 0.9015176296234131 + 10.0 * 8.478517532348633
Epoch 790, val loss: 0.9124673008918762
Epoch 800, training loss: 85.66654968261719 = 0.8952674865722656 + 10.0 * 8.477128982543945
Epoch 800, val loss: 0.9066339731216431
Epoch 810, training loss: 85.64790344238281 = 0.8887794613838196 + 10.0 * 8.475912094116211
Epoch 810, val loss: 0.9005868434906006
Epoch 820, training loss: 85.63001251220703 = 0.8820593357086182 + 10.0 * 8.4747953414917
Epoch 820, val loss: 0.8943373560905457
Epoch 830, training loss: 85.61355590820312 = 0.8751296997070312 + 10.0 * 8.47384262084961
Epoch 830, val loss: 0.8878891468048096
Epoch 840, training loss: 85.62266540527344 = 0.8679429292678833 + 10.0 * 8.475472450256348
Epoch 840, val loss: 0.8811769485473633
Epoch 850, training loss: 85.59346771240234 = 0.8605648875236511 + 10.0 * 8.47329044342041
Epoch 850, val loss: 0.8743380308151245
Epoch 860, training loss: 85.56597137451172 = 0.8531396389007568 + 10.0 * 8.471282958984375
Epoch 860, val loss: 0.8674579858779907
Epoch 870, training loss: 85.54505157470703 = 0.8455665111541748 + 10.0 * 8.469948768615723
Epoch 870, val loss: 0.8604373931884766
Epoch 880, training loss: 85.5267333984375 = 0.837829053401947 + 10.0 * 8.468890190124512
Epoch 880, val loss: 0.8532671332359314
Epoch 890, training loss: 85.5100326538086 = 0.8299064040184021 + 10.0 * 8.468012809753418
Epoch 890, val loss: 0.845925509929657
Epoch 900, training loss: 85.49826049804688 = 0.8218263983726501 + 10.0 * 8.467643737792969
Epoch 900, val loss: 0.8384298086166382
Epoch 910, training loss: 85.49447631835938 = 0.8134694695472717 + 10.0 * 8.468100547790527
Epoch 910, val loss: 0.8306680917739868
Epoch 920, training loss: 85.46475219726562 = 0.8051167130470276 + 10.0 * 8.465963363647461
Epoch 920, val loss: 0.8229851722717285
Epoch 930, training loss: 85.45054626464844 = 0.796839714050293 + 10.0 * 8.465371131896973
Epoch 930, val loss: 0.8153279423713684
Epoch 940, training loss: 85.43121337890625 = 0.7884849905967712 + 10.0 * 8.464273452758789
Epoch 940, val loss: 0.8076183795928955
Epoch 950, training loss: 85.41355895996094 = 0.7800394296646118 + 10.0 * 8.46335220336914
Epoch 950, val loss: 0.7998150587081909
Epoch 960, training loss: 85.39799499511719 = 0.7714641690254211 + 10.0 * 8.462653160095215
Epoch 960, val loss: 0.7919027805328369
Epoch 970, training loss: 85.422607421875 = 0.7627623081207275 + 10.0 * 8.465984344482422
Epoch 970, val loss: 0.7839183211326599
Epoch 980, training loss: 85.36930847167969 = 0.7540408968925476 + 10.0 * 8.461526870727539
Epoch 980, val loss: 0.7758473753929138
Epoch 990, training loss: 85.35540008544922 = 0.7454049587249756 + 10.0 * 8.460999488830566
Epoch 990, val loss: 0.7679018378257751
Epoch 1000, training loss: 85.33585357666016 = 0.7367364168167114 + 10.0 * 8.459911346435547
Epoch 1000, val loss: 0.7599577307701111
Epoch 1010, training loss: 85.32056427001953 = 0.728052020072937 + 10.0 * 8.459251403808594
Epoch 1010, val loss: 0.7520086765289307
Epoch 1020, training loss: 85.31165313720703 = 0.7193687558174133 + 10.0 * 8.459228515625
Epoch 1020, val loss: 0.7440648078918457
Epoch 1030, training loss: 85.29817199707031 = 0.7106295824050903 + 10.0 * 8.45875358581543
Epoch 1030, val loss: 0.7360503673553467
Epoch 1040, training loss: 85.27415466308594 = 0.7019944190979004 + 10.0 * 8.457216262817383
Epoch 1040, val loss: 0.7281824946403503
Epoch 1050, training loss: 85.26065826416016 = 0.6934661269187927 + 10.0 * 8.456719398498535
Epoch 1050, val loss: 0.7203970551490784
Epoch 1060, training loss: 85.24462127685547 = 0.6849976778030396 + 10.0 * 8.455962181091309
Epoch 1060, val loss: 0.7127035856246948
Epoch 1070, training loss: 85.2288589477539 = 0.676581859588623 + 10.0 * 8.455227851867676
Epoch 1070, val loss: 0.7050530910491943
Epoch 1080, training loss: 85.21499633789062 = 0.66819828748703 + 10.0 * 8.454679489135742
Epoch 1080, val loss: 0.6974482536315918
Epoch 1090, training loss: 85.23495483398438 = 0.6598463654518127 + 10.0 * 8.457510948181152
Epoch 1090, val loss: 0.6898934245109558
Epoch 1100, training loss: 85.20515441894531 = 0.6515921950340271 + 10.0 * 8.45535659790039
Epoch 1100, val loss: 0.6824488639831543
Epoch 1110, training loss: 85.17538452148438 = 0.6435218453407288 + 10.0 * 8.45318603515625
Epoch 1110, val loss: 0.6752028465270996
Epoch 1120, training loss: 85.16309356689453 = 0.6356152296066284 + 10.0 * 8.45274829864502
Epoch 1120, val loss: 0.6680890917778015
Epoch 1130, training loss: 85.15226745605469 = 0.6278431415557861 + 10.0 * 8.452442169189453
Epoch 1130, val loss: 0.6611070036888123
Epoch 1140, training loss: 85.12853240966797 = 0.6202130317687988 + 10.0 * 8.450831413269043
Epoch 1140, val loss: 0.6542766094207764
Epoch 1150, training loss: 85.11732482910156 = 0.612712025642395 + 10.0 * 8.450461387634277
Epoch 1150, val loss: 0.6475824117660522
Epoch 1160, training loss: 85.13834381103516 = 0.605344831943512 + 10.0 * 8.453299522399902
Epoch 1160, val loss: 0.641003429889679
Epoch 1170, training loss: 85.1061019897461 = 0.5980859398841858 + 10.0 * 8.450801849365234
Epoch 1170, val loss: 0.6346073746681213
Epoch 1180, training loss: 85.07984924316406 = 0.5911307334899902 + 10.0 * 8.448871612548828
Epoch 1180, val loss: 0.628442645072937
Epoch 1190, training loss: 85.06336212158203 = 0.5843260288238525 + 10.0 * 8.447903633117676
Epoch 1190, val loss: 0.6224730014801025
Epoch 1200, training loss: 85.05388641357422 = 0.5777175426483154 + 10.0 * 8.447616577148438
Epoch 1200, val loss: 0.6166749596595764
Epoch 1210, training loss: 85.05354309082031 = 0.5712379813194275 + 10.0 * 8.448230743408203
Epoch 1210, val loss: 0.6110139489173889
Epoch 1220, training loss: 85.0496826171875 = 0.5649660229682922 + 10.0 * 8.448472023010254
Epoch 1220, val loss: 0.6055366396903992
Epoch 1230, training loss: 85.02079772949219 = 0.5589697360992432 + 10.0 * 8.446183204650879
Epoch 1230, val loss: 0.6003880500793457
Epoch 1240, training loss: 85.00627899169922 = 0.5531728267669678 + 10.0 * 8.445310592651367
Epoch 1240, val loss: 0.5953737497329712
Epoch 1250, training loss: 84.99375915527344 = 0.5475752353668213 + 10.0 * 8.444618225097656
Epoch 1250, val loss: 0.5905971527099609
Epoch 1260, training loss: 84.9833755493164 = 0.5421451330184937 + 10.0 * 8.444123268127441
Epoch 1260, val loss: 0.5859894752502441
Epoch 1270, training loss: 84.99391174316406 = 0.536890983581543 + 10.0 * 8.44570255279541
Epoch 1270, val loss: 0.5815607905387878
Epoch 1280, training loss: 84.98709869384766 = 0.5317314267158508 + 10.0 * 8.445536613464355
Epoch 1280, val loss: 0.5772035717964172
Epoch 1290, training loss: 84.95540618896484 = 0.5269197225570679 + 10.0 * 8.442849159240723
Epoch 1290, val loss: 0.573189377784729
Epoch 1300, training loss: 84.94528198242188 = 0.5223116874694824 + 10.0 * 8.442296981811523
Epoch 1300, val loss: 0.5693983435630798
Epoch 1310, training loss: 84.9355697631836 = 0.5179058909416199 + 10.0 * 8.441766738891602
Epoch 1310, val loss: 0.5657823085784912
Epoch 1320, training loss: 84.92509460449219 = 0.5136332511901855 + 10.0 * 8.441145896911621
Epoch 1320, val loss: 0.56230628490448
Epoch 1330, training loss: 84.91643524169922 = 0.5095032453536987 + 10.0 * 8.440692901611328
Epoch 1330, val loss: 0.5589768290519714
Epoch 1340, training loss: 84.9427719116211 = 0.5054985284805298 + 10.0 * 8.443727493286133
Epoch 1340, val loss: 0.5557303428649902
Epoch 1350, training loss: 84.92255401611328 = 0.5016207695007324 + 10.0 * 8.442093849182129
Epoch 1350, val loss: 0.552776038646698
Epoch 1360, training loss: 84.89238739013672 = 0.497951477766037 + 10.0 * 8.439443588256836
Epoch 1360, val loss: 0.5498618483543396
Epoch 1370, training loss: 84.88616943359375 = 0.49443137645721436 + 10.0 * 8.439173698425293
Epoch 1370, val loss: 0.5471351742744446
Epoch 1380, training loss: 84.87577056884766 = 0.4910506010055542 + 10.0 * 8.438471794128418
Epoch 1380, val loss: 0.544559121131897
Epoch 1390, training loss: 84.86918640136719 = 0.48777714371681213 + 10.0 * 8.438140869140625
Epoch 1390, val loss: 0.5420892834663391
Epoch 1400, training loss: 84.92463684082031 = 0.4845889210700989 + 10.0 * 8.444005012512207
Epoch 1400, val loss: 0.5396693348884583
Epoch 1410, training loss: 84.85853576660156 = 0.4815588891506195 + 10.0 * 8.437697410583496
Epoch 1410, val loss: 0.5375117659568787
Epoch 1420, training loss: 84.8532943725586 = 0.4786608815193176 + 10.0 * 8.437463760375977
Epoch 1420, val loss: 0.5353915095329285
Epoch 1430, training loss: 84.84041595458984 = 0.4758777320384979 + 10.0 * 8.436453819274902
Epoch 1430, val loss: 0.5334129333496094
Epoch 1440, training loss: 84.8340835571289 = 0.4731921851634979 + 10.0 * 8.436089515686035
Epoch 1440, val loss: 0.5315239429473877
Epoch 1450, training loss: 84.85100555419922 = 0.47059550881385803 + 10.0 * 8.438040733337402
Epoch 1450, val loss: 0.5297337174415588
Epoch 1460, training loss: 84.83029174804688 = 0.46804967522621155 + 10.0 * 8.436223983764648
Epoch 1460, val loss: 0.5280080437660217
Epoch 1470, training loss: 84.82022857666016 = 0.4656482934951782 + 10.0 * 8.435458183288574
Epoch 1470, val loss: 0.5263800024986267
Epoch 1480, training loss: 84.8111343383789 = 0.4633297324180603 + 10.0 * 8.43478012084961
Epoch 1480, val loss: 0.5248218774795532
Epoch 1490, training loss: 84.80323028564453 = 0.46111196279525757 + 10.0 * 8.434211730957031
Epoch 1490, val loss: 0.5234118700027466
Epoch 1500, training loss: 84.79671478271484 = 0.45894765853881836 + 10.0 * 8.43377685546875
Epoch 1500, val loss: 0.522028386592865
Epoch 1510, training loss: 84.80162048339844 = 0.4568420350551605 + 10.0 * 8.434477806091309
Epoch 1510, val loss: 0.5206859111785889
Epoch 1520, training loss: 84.79263305664062 = 0.45479246973991394 + 10.0 * 8.433783531188965
Epoch 1520, val loss: 0.5194879174232483
Epoch 1530, training loss: 84.78623962402344 = 0.452831506729126 + 10.0 * 8.433340072631836
Epoch 1530, val loss: 0.5182731747627258
Epoch 1540, training loss: 84.77792358398438 = 0.45094558596611023 + 10.0 * 8.432698249816895
Epoch 1540, val loss: 0.51717609167099
Epoch 1550, training loss: 84.76874542236328 = 0.4491250216960907 + 10.0 * 8.431962013244629
Epoch 1550, val loss: 0.5161162614822388
Epoch 1560, training loss: 84.7646484375 = 0.4473438262939453 + 10.0 * 8.431730270385742
Epoch 1560, val loss: 0.5151159763336182
Epoch 1570, training loss: 84.7924575805664 = 0.445603609085083 + 10.0 * 8.434685707092285
Epoch 1570, val loss: 0.5140849351882935
Epoch 1580, training loss: 84.76663970947266 = 0.44390058517456055 + 10.0 * 8.432273864746094
Epoch 1580, val loss: 0.5133090615272522
Epoch 1590, training loss: 84.75778198242188 = 0.4422929883003235 + 10.0 * 8.431549072265625
Epoch 1590, val loss: 0.5123574733734131
Epoch 1600, training loss: 84.74490356445312 = 0.44073599576950073 + 10.0 * 8.43041706085205
Epoch 1600, val loss: 0.5115869045257568
Epoch 1610, training loss: 84.73928833007812 = 0.43921977281570435 + 10.0 * 8.430006980895996
Epoch 1610, val loss: 0.5108327865600586
Epoch 1620, training loss: 84.73658752441406 = 0.43773242831230164 + 10.0 * 8.429885864257812
Epoch 1620, val loss: 0.5100966691970825
Epoch 1630, training loss: 84.78191375732422 = 0.4362735450267792 + 10.0 * 8.434564590454102
Epoch 1630, val loss: 0.5094605088233948
Epoch 1640, training loss: 84.73359680175781 = 0.4348459839820862 + 10.0 * 8.429875373840332
Epoch 1640, val loss: 0.508703887462616
Epoch 1650, training loss: 84.72337341308594 = 0.43348103761672974 + 10.0 * 8.42898941040039
Epoch 1650, val loss: 0.5080896615982056
Epoch 1660, training loss: 84.71728515625 = 0.4321497976779938 + 10.0 * 8.428513526916504
Epoch 1660, val loss: 0.5075318217277527
Epoch 1670, training loss: 84.7118911743164 = 0.4308471977710724 + 10.0 * 8.428104400634766
Epoch 1670, val loss: 0.5069553852081299
Epoch 1680, training loss: 84.71165466308594 = 0.4295676052570343 + 10.0 * 8.42820930480957
Epoch 1680, val loss: 0.5064341425895691
Epoch 1690, training loss: 84.72907257080078 = 0.42830586433410645 + 10.0 * 8.430076599121094
Epoch 1690, val loss: 0.505955696105957
Epoch 1700, training loss: 84.70388793945312 = 0.427066832780838 + 10.0 * 8.427681922912598
Epoch 1700, val loss: 0.5053990483283997
Epoch 1710, training loss: 84.70381927490234 = 0.42587369680404663 + 10.0 * 8.427794456481934
Epoch 1710, val loss: 0.5049694180488586
Epoch 1720, training loss: 84.7009048461914 = 0.4246997535228729 + 10.0 * 8.427620887756348
Epoch 1720, val loss: 0.5045436024665833
Epoch 1730, training loss: 84.68839263916016 = 0.4235489070415497 + 10.0 * 8.426485061645508
Epoch 1730, val loss: 0.5041005611419678
Epoch 1740, training loss: 84.68656921386719 = 0.4224175810813904 + 10.0 * 8.42641544342041
Epoch 1740, val loss: 0.5036898255348206
Epoch 1750, training loss: 84.69703674316406 = 0.42130720615386963 + 10.0 * 8.427572250366211
Epoch 1750, val loss: 0.5032743215560913
Epoch 1760, training loss: 84.67881774902344 = 0.42021358013153076 + 10.0 * 8.425860404968262
Epoch 1760, val loss: 0.5029603838920593
Epoch 1770, training loss: 84.68360900878906 = 0.4191516935825348 + 10.0 * 8.426445960998535
Epoch 1770, val loss: 0.5026342868804932
Epoch 1780, training loss: 84.67987060546875 = 0.4181017577648163 + 10.0 * 8.426177024841309
Epoch 1780, val loss: 0.5022834539413452
Epoch 1790, training loss: 84.6664810180664 = 0.41707369685173035 + 10.0 * 8.42494010925293
Epoch 1790, val loss: 0.5019443035125732
Epoch 1800, training loss: 84.66348266601562 = 0.41606470942497253 + 10.0 * 8.424741744995117
Epoch 1800, val loss: 0.5016494989395142
Epoch 1810, training loss: 84.67115783691406 = 0.41506731510162354 + 10.0 * 8.42560863494873
Epoch 1810, val loss: 0.5013559460639954
Epoch 1820, training loss: 84.65702819824219 = 0.41408756375312805 + 10.0 * 8.424294471740723
Epoch 1820, val loss: 0.5010718107223511
Epoch 1830, training loss: 84.66613006591797 = 0.41312262415885925 + 10.0 * 8.425300598144531
Epoch 1830, val loss: 0.5007579922676086
Epoch 1840, training loss: 84.64884948730469 = 0.41217708587646484 + 10.0 * 8.423666954040527
Epoch 1840, val loss: 0.5006211400032043
Epoch 1850, training loss: 84.65342712402344 = 0.4112539291381836 + 10.0 * 8.424217224121094
Epoch 1850, val loss: 0.5003896951675415
Epoch 1860, training loss: 84.665283203125 = 0.4103347957134247 + 10.0 * 8.425495147705078
Epoch 1860, val loss: 0.5001980066299438
Epoch 1870, training loss: 84.63932800292969 = 0.409434974193573 + 10.0 * 8.422989845275879
Epoch 1870, val loss: 0.49990183115005493
Epoch 1880, training loss: 84.63392639160156 = 0.40855497121810913 + 10.0 * 8.422536849975586
Epoch 1880, val loss: 0.49971216917037964
Epoch 1890, training loss: 84.63069915771484 = 0.40768861770629883 + 10.0 * 8.422300338745117
Epoch 1890, val loss: 0.4995381236076355
Epoch 1900, training loss: 84.62834167480469 = 0.4068261384963989 + 10.0 * 8.422151565551758
Epoch 1900, val loss: 0.49935558438301086
Epoch 1910, training loss: 84.64878845214844 = 0.40597105026245117 + 10.0 * 8.42428207397461
Epoch 1910, val loss: 0.4992060959339142
Epoch 1920, training loss: 84.62603759765625 = 0.40512019395828247 + 10.0 * 8.422091484069824
Epoch 1920, val loss: 0.4990308880805969
Epoch 1930, training loss: 84.62663269042969 = 0.4042908549308777 + 10.0 * 8.422234535217285
Epoch 1930, val loss: 0.49883872270584106
Epoch 1940, training loss: 84.61725616455078 = 0.4034755825996399 + 10.0 * 8.421378135681152
Epoch 1940, val loss: 0.4987272322177887
Epoch 1950, training loss: 84.61219024658203 = 0.40266817808151245 + 10.0 * 8.420951843261719
Epoch 1950, val loss: 0.4985809922218323
Epoch 1960, training loss: 84.61112213134766 = 0.4018676280975342 + 10.0 * 8.42092514038086
Epoch 1960, val loss: 0.4984443187713623
Epoch 1970, training loss: 84.61980438232422 = 0.40107202529907227 + 10.0 * 8.421873092651367
Epoch 1970, val loss: 0.49832069873809814
Epoch 1980, training loss: 84.62516784667969 = 0.40027904510498047 + 10.0 * 8.422489166259766
Epoch 1980, val loss: 0.49809730052948
Epoch 1990, training loss: 84.61799621582031 = 0.39950722455978394 + 10.0 * 8.421849250793457
Epoch 1990, val loss: 0.4981246292591095
Epoch 2000, training loss: 84.60263061523438 = 0.398743599653244 + 10.0 * 8.420389175415039
Epoch 2000, val loss: 0.4979362487792969
Epoch 2010, training loss: 84.59416198730469 = 0.3979968726634979 + 10.0 * 8.41961669921875
Epoch 2010, val loss: 0.4979216456413269
Epoch 2020, training loss: 84.59139251708984 = 0.39725106954574585 + 10.0 * 8.419414520263672
Epoch 2020, val loss: 0.4978320002555847
Epoch 2030, training loss: 84.60370635986328 = 0.3965141475200653 + 10.0 * 8.420719146728516
Epoch 2030, val loss: 0.4978064000606537
Epoch 2040, training loss: 84.59819030761719 = 0.3957828879356384 + 10.0 * 8.42024040222168
Epoch 2040, val loss: 0.49766263365745544
Epoch 2050, training loss: 84.58101654052734 = 0.39506056904792786 + 10.0 * 8.418596267700195
Epoch 2050, val loss: 0.4975813031196594
Epoch 2060, training loss: 84.57860565185547 = 0.39435020089149475 + 10.0 * 8.418425559997559
Epoch 2060, val loss: 0.497481107711792
Epoch 2070, training loss: 84.57577514648438 = 0.39364469051361084 + 10.0 * 8.418212890625
Epoch 2070, val loss: 0.49741068482398987
Epoch 2080, training loss: 84.57853698730469 = 0.3929424285888672 + 10.0 * 8.418559074401855
Epoch 2080, val loss: 0.4973577558994293
Epoch 2090, training loss: 84.5978775024414 = 0.39224544167518616 + 10.0 * 8.420563697814941
Epoch 2090, val loss: 0.4972614347934723
Epoch 2100, training loss: 84.5707015991211 = 0.3915626108646393 + 10.0 * 8.417913436889648
Epoch 2100, val loss: 0.4973280131816864
Epoch 2110, training loss: 84.56270599365234 = 0.39087966084480286 + 10.0 * 8.417182922363281
Epoch 2110, val loss: 0.4972699284553528
Epoch 2120, training loss: 84.58442687988281 = 0.39020320773124695 + 10.0 * 8.419422149658203
Epoch 2120, val loss: 0.49719446897506714
Epoch 2130, training loss: 84.57048797607422 = 0.38953879475593567 + 10.0 * 8.418094635009766
Epoch 2130, val loss: 0.4972301423549652
Epoch 2140, training loss: 84.5621337890625 = 0.38887354731559753 + 10.0 * 8.417325973510742
Epoch 2140, val loss: 0.4971367418766022
Epoch 2150, training loss: 84.55089569091797 = 0.38822251558303833 + 10.0 * 8.416267395019531
Epoch 2150, val loss: 0.4971448481082916
Epoch 2160, training loss: 84.54637145996094 = 0.38757047057151794 + 10.0 * 8.41588020324707
Epoch 2160, val loss: 0.4971277415752411
Epoch 2170, training loss: 84.5542221069336 = 0.3869222402572632 + 10.0 * 8.416729927062988
Epoch 2170, val loss: 0.4971317946910858
Epoch 2180, training loss: 84.55219268798828 = 0.38627752661705017 + 10.0 * 8.41659164428711
Epoch 2180, val loss: 0.49712634086608887
Epoch 2190, training loss: 84.54090118408203 = 0.38564103841781616 + 10.0 * 8.415525436401367
Epoch 2190, val loss: 0.4970666766166687
Epoch 2200, training loss: 84.5469970703125 = 0.3850110173225403 + 10.0 * 8.41619873046875
Epoch 2200, val loss: 0.4970515966415405
Epoch 2210, training loss: 84.542236328125 = 0.38438525795936584 + 10.0 * 8.41578483581543
Epoch 2210, val loss: 0.4970736801624298
Epoch 2220, training loss: 84.52951049804688 = 0.3837660253047943 + 10.0 * 8.41457462310791
Epoch 2220, val loss: 0.49710384011268616
Epoch 2230, training loss: 84.53086853027344 = 0.38314834237098694 + 10.0 * 8.414772033691406
Epoch 2230, val loss: 0.4971495270729065
Epoch 2240, training loss: 84.54328918457031 = 0.38253456354141235 + 10.0 * 8.416074752807617
Epoch 2240, val loss: 0.4971871078014374
Epoch 2250, training loss: 84.53419494628906 = 0.38192421197891235 + 10.0 * 8.415226936340332
Epoch 2250, val loss: 0.49716439843177795
Epoch 2260, training loss: 84.52373504638672 = 0.3813178539276123 + 10.0 * 8.414241790771484
Epoch 2260, val loss: 0.4971669316291809
Epoch 2270, training loss: 84.51924133300781 = 0.38072100281715393 + 10.0 * 8.413851737976074
Epoch 2270, val loss: 0.4972309470176697
Epoch 2280, training loss: 84.51811981201172 = 0.3801276385784149 + 10.0 * 8.413799285888672
Epoch 2280, val loss: 0.4972727298736572
Epoch 2290, training loss: 84.51787567138672 = 0.37953558564186096 + 10.0 * 8.413834571838379
Epoch 2290, val loss: 0.49730047583580017
Epoch 2300, training loss: 84.51184844970703 = 0.3789476454257965 + 10.0 * 8.413290023803711
Epoch 2300, val loss: 0.49728086590766907
Epoch 2310, training loss: 84.51324462890625 = 0.37835898995399475 + 10.0 * 8.413488388061523
Epoch 2310, val loss: 0.4972603917121887
Epoch 2320, training loss: 84.52848052978516 = 0.3777766227722168 + 10.0 * 8.415070533752441
Epoch 2320, val loss: 0.4972713589668274
Epoch 2330, training loss: 84.50841522216797 = 0.3772064745426178 + 10.0 * 8.413121223449707
Epoch 2330, val loss: 0.49746081233024597
Epoch 2340, training loss: 84.50566101074219 = 0.376629501581192 + 10.0 * 8.41290283203125
Epoch 2340, val loss: 0.49741506576538086
Epoch 2350, training loss: 84.50572967529297 = 0.37606287002563477 + 10.0 * 8.41296672821045
Epoch 2350, val loss: 0.4974803030490875
Epoch 2360, training loss: 84.49560546875 = 0.37549883127212524 + 10.0 * 8.41201114654541
Epoch 2360, val loss: 0.497565895318985
Epoch 2370, training loss: 84.50313568115234 = 0.37493592500686646 + 10.0 * 8.412819862365723
Epoch 2370, val loss: 0.4976416826248169
Epoch 2380, training loss: 84.49409484863281 = 0.37437793612480164 + 10.0 * 8.411972045898438
Epoch 2380, val loss: 0.49763578176498413
Epoch 2390, training loss: 84.49225616455078 = 0.3738214671611786 + 10.0 * 8.411843299865723
Epoch 2390, val loss: 0.497665673494339
Epoch 2400, training loss: 84.49243927001953 = 0.3732711970806122 + 10.0 * 8.411916732788086
Epoch 2400, val loss: 0.4977701008319855
Epoch 2410, training loss: 84.48731231689453 = 0.37272098660469055 + 10.0 * 8.411458969116211
Epoch 2410, val loss: 0.49780020117759705
Epoch 2420, training loss: 84.49539947509766 = 0.37217360734939575 + 10.0 * 8.412322044372559
Epoch 2420, val loss: 0.4978170692920685
Epoch 2430, training loss: 84.4813003540039 = 0.3716345727443695 + 10.0 * 8.410966873168945
Epoch 2430, val loss: 0.4980083405971527
Epoch 2440, training loss: 84.47806549072266 = 0.37109753489494324 + 10.0 * 8.410696983337402
Epoch 2440, val loss: 0.498080313205719
Epoch 2450, training loss: 84.48676300048828 = 0.3705623745918274 + 10.0 * 8.411620140075684
Epoch 2450, val loss: 0.49817773699760437
Epoch 2460, training loss: 84.47207641601562 = 0.3700288236141205 + 10.0 * 8.410204887390137
Epoch 2460, val loss: 0.498239129781723
Epoch 2470, training loss: 84.46868896484375 = 0.36949780583381653 + 10.0 * 8.409918785095215
Epoch 2470, val loss: 0.49829307198524475
Epoch 2480, training loss: 84.47168731689453 = 0.3689706027507782 + 10.0 * 8.410271644592285
Epoch 2480, val loss: 0.4983755946159363
Epoch 2490, training loss: 84.49181365966797 = 0.3684435188770294 + 10.0 * 8.412336349487305
Epoch 2490, val loss: 0.49838829040527344
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7889396245560629
0.8111280156487721
=== training gcn model ===
Epoch 0, training loss: 106.91541290283203 = 1.092673420906067 + 10.0 * 10.582273483276367
Epoch 0, val loss: 1.09372878074646
Epoch 10, training loss: 106.90840148925781 = 1.0886073112487793 + 10.0 * 10.581979751586914
Epoch 10, val loss: 1.0896375179290771
Epoch 20, training loss: 106.89256286621094 = 1.0843507051467896 + 10.0 * 10.58082103729248
Epoch 20, val loss: 1.0853272676467896
Epoch 30, training loss: 106.83536529541016 = 1.0797429084777832 + 10.0 * 10.575562477111816
Epoch 30, val loss: 1.0806422233581543
Epoch 40, training loss: 106.60115051269531 = 1.0747631788253784 + 10.0 * 10.55263900756836
Epoch 40, val loss: 1.0755407810211182
Epoch 50, training loss: 105.75929260253906 = 1.069350004196167 + 10.0 * 10.468994140625
Epoch 50, val loss: 1.06998872756958
Epoch 60, training loss: 103.44752502441406 = 1.0638301372528076 + 10.0 * 10.238369941711426
Epoch 60, val loss: 1.0643826723098755
Epoch 70, training loss: 100.17965698242188 = 1.0582499504089355 + 10.0 * 9.912140846252441
Epoch 70, val loss: 1.0588195323944092
Epoch 80, training loss: 98.22917175292969 = 1.0542289018630981 + 10.0 * 9.717494010925293
Epoch 80, val loss: 1.0550071001052856
Epoch 90, training loss: 96.81388854980469 = 1.0517560243606567 + 10.0 * 9.576212882995605
Epoch 90, val loss: 1.0526669025421143
Epoch 100, training loss: 95.2353286743164 = 1.0498006343841553 + 10.0 * 9.418553352355957
Epoch 100, val loss: 1.0506833791732788
Epoch 110, training loss: 93.04066467285156 = 1.0474278926849365 + 10.0 * 9.199323654174805
Epoch 110, val loss: 1.048357367515564
Epoch 120, training loss: 91.57222747802734 = 1.0450979471206665 + 10.0 * 9.052713394165039
Epoch 120, val loss: 1.0461026430130005
Epoch 130, training loss: 90.99584197998047 = 1.0431578159332275 + 10.0 * 8.995267868041992
Epoch 130, val loss: 1.0442554950714111
Epoch 140, training loss: 90.34196472167969 = 1.0421169996261597 + 10.0 * 8.929985046386719
Epoch 140, val loss: 1.0433645248413086
Epoch 150, training loss: 89.67997741699219 = 1.042080283164978 + 10.0 * 8.863789558410645
Epoch 150, val loss: 1.0434106588363647
Epoch 160, training loss: 89.26753997802734 = 1.0419566631317139 + 10.0 * 8.822558403015137
Epoch 160, val loss: 1.0432707071304321
Epoch 170, training loss: 89.02488708496094 = 1.0412564277648926 + 10.0 * 8.798362731933594
Epoch 170, val loss: 1.0426204204559326
Epoch 180, training loss: 88.81486511230469 = 1.040390133857727 + 10.0 * 8.777447700500488
Epoch 180, val loss: 1.0418369770050049
Epoch 190, training loss: 88.58316040039062 = 1.0397192239761353 + 10.0 * 8.75434398651123
Epoch 190, val loss: 1.0412427186965942
Epoch 200, training loss: 88.34681701660156 = 1.039379596710205 + 10.0 * 8.730743408203125
Epoch 200, val loss: 1.0409377813339233
Epoch 210, training loss: 88.0309829711914 = 1.0391085147857666 + 10.0 * 8.699187278747559
Epoch 210, val loss: 1.0407295227050781
Epoch 220, training loss: 87.78050231933594 = 1.0389084815979004 + 10.0 * 8.674159049987793
Epoch 220, val loss: 1.0405404567718506
Epoch 230, training loss: 87.58948516845703 = 1.038487195968628 + 10.0 * 8.655099868774414
Epoch 230, val loss: 1.0401098728179932
Epoch 240, training loss: 87.46561431884766 = 1.0377546548843384 + 10.0 * 8.642786026000977
Epoch 240, val loss: 1.0394132137298584
Epoch 250, training loss: 87.36820220947266 = 1.0368784666061401 + 10.0 * 8.633131980895996
Epoch 250, val loss: 1.0385832786560059
Epoch 260, training loss: 87.28300476074219 = 1.0359454154968262 + 10.0 * 8.624706268310547
Epoch 260, val loss: 1.0377185344696045
Epoch 270, training loss: 87.20954132080078 = 1.034967303276062 + 10.0 * 8.617457389831543
Epoch 270, val loss: 1.0368105173110962
Epoch 280, training loss: 87.16345977783203 = 1.033929705619812 + 10.0 * 8.612953186035156
Epoch 280, val loss: 1.0358788967132568
Epoch 290, training loss: 87.08440399169922 = 1.032886266708374 + 10.0 * 8.605152130126953
Epoch 290, val loss: 1.0349050760269165
Epoch 300, training loss: 87.02761840820312 = 1.0318371057510376 + 10.0 * 8.599577903747559
Epoch 300, val loss: 1.0339375734329224
Epoch 310, training loss: 86.96995544433594 = 1.0307655334472656 + 10.0 * 8.59391975402832
Epoch 310, val loss: 1.0329533815383911
Epoch 320, training loss: 86.93254089355469 = 1.0296710729599 + 10.0 * 8.590287208557129
Epoch 320, val loss: 1.0319370031356812
Epoch 330, training loss: 86.86044311523438 = 1.0285347700119019 + 10.0 * 8.58319091796875
Epoch 330, val loss: 1.0309072732925415
Epoch 340, training loss: 86.79659271240234 = 1.02741539478302 + 10.0 * 8.57691764831543
Epoch 340, val loss: 1.0298810005187988
Epoch 350, training loss: 86.74652862548828 = 1.026283621788025 + 10.0 * 8.57202434539795
Epoch 350, val loss: 1.0288375616073608
Epoch 360, training loss: 86.70220184326172 = 1.0250451564788818 + 10.0 * 8.567715644836426
Epoch 360, val loss: 1.0277020931243896
Epoch 370, training loss: 86.63455200195312 = 1.0237739086151123 + 10.0 * 8.561078071594238
Epoch 370, val loss: 1.0265283584594727
Epoch 380, training loss: 86.58949279785156 = 1.0224933624267578 + 10.0 * 8.556699752807617
Epoch 380, val loss: 1.0253477096557617
Epoch 390, training loss: 86.54393768310547 = 1.0211600065231323 + 10.0 * 8.552278518676758
Epoch 390, val loss: 1.024121880531311
Epoch 400, training loss: 86.4996337890625 = 1.01975417137146 + 10.0 * 8.547987937927246
Epoch 400, val loss: 1.0228211879730225
Epoch 410, training loss: 86.46855926513672 = 1.018286108970642 + 10.0 * 8.545026779174805
Epoch 410, val loss: 1.0214787721633911
Epoch 420, training loss: 86.4717788696289 = 1.016670823097229 + 10.0 * 8.545511245727539
Epoch 420, val loss: 1.0199509859085083
Epoch 430, training loss: 86.39633178710938 = 1.0150014162063599 + 10.0 * 8.53813362121582
Epoch 430, val loss: 1.018446445465088
Epoch 440, training loss: 86.3682632446289 = 1.013333797454834 + 10.0 * 8.535492897033691
Epoch 440, val loss: 1.0169007778167725
Epoch 450, training loss: 86.33892059326172 = 1.0115747451782227 + 10.0 * 8.532734870910645
Epoch 450, val loss: 1.0152641534805298
Epoch 460, training loss: 86.315185546875 = 1.0097182989120483 + 10.0 * 8.530546188354492
Epoch 460, val loss: 1.013550877571106
Epoch 470, training loss: 86.29222869873047 = 1.0077697038650513 + 10.0 * 8.528446197509766
Epoch 470, val loss: 1.011751413345337
Epoch 480, training loss: 86.27116394042969 = 1.0057103633880615 + 10.0 * 8.526545524597168
Epoch 480, val loss: 1.0098474025726318
Epoch 490, training loss: 86.2542953491211 = 1.003553867340088 + 10.0 * 8.525074005126953
Epoch 490, val loss: 1.0078504085540771
Epoch 500, training loss: 86.24787902832031 = 1.00125253200531 + 10.0 * 8.524662971496582
Epoch 500, val loss: 1.0057275295257568
Epoch 510, training loss: 86.2179946899414 = 0.9988633394241333 + 10.0 * 8.521913528442383
Epoch 510, val loss: 1.0035148859024048
Epoch 520, training loss: 86.19876861572266 = 0.9963939785957336 + 10.0 * 8.520237922668457
Epoch 520, val loss: 1.001229166984558
Epoch 530, training loss: 86.18217468261719 = 0.9938156604766846 + 10.0 * 8.51883602142334
Epoch 530, val loss: 0.9988455772399902
Epoch 540, training loss: 86.20085906982422 = 0.9910977482795715 + 10.0 * 8.520976066589355
Epoch 540, val loss: 0.9963440299034119
Epoch 550, training loss: 86.16238403320312 = 0.9881864786148071 + 10.0 * 8.517419815063477
Epoch 550, val loss: 0.9936202168464661
Epoch 560, training loss: 86.1351547241211 = 0.9852442145347595 + 10.0 * 8.51499080657959
Epoch 560, val loss: 0.9909199476242065
Epoch 570, training loss: 86.12078094482422 = 0.9822189211845398 + 10.0 * 8.513856887817383
Epoch 570, val loss: 0.9881256222724915
Epoch 580, training loss: 86.10258483886719 = 0.9790692329406738 + 10.0 * 8.512351036071777
Epoch 580, val loss: 0.9852099418640137
Epoch 590, training loss: 86.0849838256836 = 0.9757907390594482 + 10.0 * 8.510919570922852
Epoch 590, val loss: 0.982184648513794
Epoch 600, training loss: 86.06980895996094 = 0.9723542928695679 + 10.0 * 8.509745597839355
Epoch 600, val loss: 0.979009211063385
Epoch 610, training loss: 86.06488037109375 = 0.9687188267707825 + 10.0 * 8.509615898132324
Epoch 610, val loss: 0.9756550192832947
Epoch 620, training loss: 86.03360748291016 = 0.9649908542633057 + 10.0 * 8.506861686706543
Epoch 620, val loss: 0.9722038507461548
Epoch 630, training loss: 86.01504516601562 = 0.9611901640892029 + 10.0 * 8.505385398864746
Epoch 630, val loss: 0.9686948657035828
Epoch 640, training loss: 85.99378967285156 = 0.957237958908081 + 10.0 * 8.503655433654785
Epoch 640, val loss: 0.9650608897209167
Epoch 650, training loss: 85.9783706665039 = 0.9531546235084534 + 10.0 * 8.502521514892578
Epoch 650, val loss: 0.9613157510757446
Epoch 660, training loss: 85.95420837402344 = 0.9487828016281128 + 10.0 * 8.500542640686035
Epoch 660, val loss: 0.9572376012802124
Epoch 670, training loss: 85.93829345703125 = 0.9443049430847168 + 10.0 * 8.499399185180664
Epoch 670, val loss: 0.9531480669975281
Epoch 680, training loss: 85.91709899902344 = 0.9398325681686401 + 10.0 * 8.497726440429688
Epoch 680, val loss: 0.9490306973457336
Epoch 690, training loss: 85.89724731445312 = 0.9352028965950012 + 10.0 * 8.496204376220703
Epoch 690, val loss: 0.9447771310806274
Epoch 700, training loss: 85.87586212158203 = 0.9304248690605164 + 10.0 * 8.494543075561523
Epoch 700, val loss: 0.9403847455978394
Epoch 710, training loss: 85.85523986816406 = 0.9254655241966248 + 10.0 * 8.492977142333984
Epoch 710, val loss: 0.9358070492744446
Epoch 720, training loss: 85.83528137207031 = 0.9203119277954102 + 10.0 * 8.491497039794922
Epoch 720, val loss: 0.9310681819915771
Epoch 730, training loss: 85.82125854492188 = 0.9149785041809082 + 10.0 * 8.490628242492676
Epoch 730, val loss: 0.9261723756790161
Epoch 740, training loss: 85.81031036376953 = 0.90938800573349 + 10.0 * 8.490092277526855
Epoch 740, val loss: 0.9209935665130615
Epoch 750, training loss: 85.7811050415039 = 0.9036719799041748 + 10.0 * 8.487743377685547
Epoch 750, val loss: 0.9157582521438599
Epoch 760, training loss: 85.76126861572266 = 0.8978697657585144 + 10.0 * 8.486339569091797
Epoch 760, val loss: 0.9104394912719727
Epoch 770, training loss: 85.7602767944336 = 0.8918717503547668 + 10.0 * 8.486841201782227
Epoch 770, val loss: 0.904961109161377
Epoch 780, training loss: 85.73428344726562 = 0.8855816125869751 + 10.0 * 8.484869956970215
Epoch 780, val loss: 0.8991304636001587
Epoch 790, training loss: 85.71710205078125 = 0.8792273998260498 + 10.0 * 8.483787536621094
Epoch 790, val loss: 0.8933340311050415
Epoch 800, training loss: 85.69127655029297 = 0.8728265762329102 + 10.0 * 8.481844902038574
Epoch 800, val loss: 0.8874543905258179
Epoch 810, training loss: 85.67374420166016 = 0.8662485480308533 + 10.0 * 8.480749130249023
Epoch 810, val loss: 0.8814305067062378
Epoch 820, training loss: 85.65675354003906 = 0.8595102429389954 + 10.0 * 8.479723930358887
Epoch 820, val loss: 0.8752515316009521
Epoch 830, training loss: 85.6876220703125 = 0.8525276184082031 + 10.0 * 8.483509063720703
Epoch 830, val loss: 0.8688904643058777
Epoch 840, training loss: 85.62806701660156 = 0.8453868627548218 + 10.0 * 8.478267669677734
Epoch 840, val loss: 0.8622734546661377
Epoch 850, training loss: 85.61144256591797 = 0.8382455706596375 + 10.0 * 8.477319717407227
Epoch 850, val loss: 0.8557623028755188
Epoch 860, training loss: 85.58983612060547 = 0.8310392498970032 + 10.0 * 8.475879669189453
Epoch 860, val loss: 0.8491660952568054
Epoch 870, training loss: 85.5730209350586 = 0.8237046599388123 + 10.0 * 8.474931716918945
Epoch 870, val loss: 0.8424462080001831
Epoch 880, training loss: 85.56901550292969 = 0.8162462115287781 + 10.0 * 8.475276947021484
Epoch 880, val loss: 0.8355870246887207
Epoch 890, training loss: 85.54259490966797 = 0.808523952960968 + 10.0 * 8.473406791687012
Epoch 890, val loss: 0.8285447955131531
Epoch 900, training loss: 85.52361297607422 = 0.8008544445037842 + 10.0 * 8.472275733947754
Epoch 900, val loss: 0.8215351104736328
Epoch 910, training loss: 85.50745391845703 = 0.7931864857673645 + 10.0 * 8.471426010131836
Epoch 910, val loss: 0.8145622611045837
Epoch 920, training loss: 85.48970031738281 = 0.7854613661766052 + 10.0 * 8.470423698425293
Epoch 920, val loss: 0.8074787855148315
Epoch 930, training loss: 85.47868347167969 = 0.7776502966880798 + 10.0 * 8.47010326385498
Epoch 930, val loss: 0.80035799741745
Epoch 940, training loss: 85.4669418334961 = 0.7696928381919861 + 10.0 * 8.469724655151367
Epoch 940, val loss: 0.7930575609207153
Epoch 950, training loss: 85.45008087158203 = 0.7617031335830688 + 10.0 * 8.46883773803711
Epoch 950, val loss: 0.7858532667160034
Epoch 960, training loss: 85.42784118652344 = 0.7538501620292664 + 10.0 * 8.467398643493652
Epoch 960, val loss: 0.778685986995697
Epoch 970, training loss: 85.40975189208984 = 0.7459790110588074 + 10.0 * 8.466377258300781
Epoch 970, val loss: 0.7715157270431519
Epoch 980, training loss: 85.40449523925781 = 0.7380867600440979 + 10.0 * 8.46664047241211
Epoch 980, val loss: 0.7643201351165771
Epoch 990, training loss: 85.38774108886719 = 0.7300748229026794 + 10.0 * 8.465766906738281
Epoch 990, val loss: 0.7570597529411316
Epoch 1000, training loss: 85.36674499511719 = 0.7221226692199707 + 10.0 * 8.464462280273438
Epoch 1000, val loss: 0.7499141097068787
Epoch 1010, training loss: 85.34925079345703 = 0.7143123149871826 + 10.0 * 8.463494300842285
Epoch 1010, val loss: 0.7428045272827148
Epoch 1020, training loss: 85.33222961425781 = 0.70649254322052 + 10.0 * 8.462574005126953
Epoch 1020, val loss: 0.7357791066169739
Epoch 1030, training loss: 85.34722137451172 = 0.6986797451972961 + 10.0 * 8.46485424041748
Epoch 1030, val loss: 0.7287424802780151
Epoch 1040, training loss: 85.30963897705078 = 0.6909216046333313 + 10.0 * 8.461872100830078
Epoch 1040, val loss: 0.7217276692390442
Epoch 1050, training loss: 85.28868865966797 = 0.6832976937294006 + 10.0 * 8.460538864135742
Epoch 1050, val loss: 0.7148898243904114
Epoch 1060, training loss: 85.2721176147461 = 0.6757801175117493 + 10.0 * 8.459633827209473
Epoch 1060, val loss: 0.7081454396247864
Epoch 1070, training loss: 85.25711059570312 = 0.6683337092399597 + 10.0 * 8.458877563476562
Epoch 1070, val loss: 0.7014671564102173
Epoch 1080, training loss: 85.38323974609375 = 0.6609229445457458 + 10.0 * 8.4722318649292
Epoch 1080, val loss: 0.6947377324104309
Epoch 1090, training loss: 85.24201965332031 = 0.6533651947975159 + 10.0 * 8.45886516571045
Epoch 1090, val loss: 0.6880521774291992
Epoch 1100, training loss: 85.21576690673828 = 0.6462311744689941 + 10.0 * 8.456953048706055
Epoch 1100, val loss: 0.6817588806152344
Epoch 1110, training loss: 85.20167541503906 = 0.6393048167228699 + 10.0 * 8.456236839294434
Epoch 1110, val loss: 0.675636351108551
Epoch 1120, training loss: 85.18892669677734 = 0.6324962973594666 + 10.0 * 8.455642700195312
Epoch 1120, val loss: 0.669623076915741
Epoch 1130, training loss: 85.17457580566406 = 0.6257644295692444 + 10.0 * 8.45488166809082
Epoch 1130, val loss: 0.6636790037155151
Epoch 1140, training loss: 85.16085052490234 = 0.6190840601921082 + 10.0 * 8.45417594909668
Epoch 1140, val loss: 0.6577924489974976
Epoch 1150, training loss: 85.14703369140625 = 0.6124719381332397 + 10.0 * 8.453455924987793
Epoch 1150, val loss: 0.6519950032234192
Epoch 1160, training loss: 85.15843200683594 = 0.6059572696685791 + 10.0 * 8.45524787902832
Epoch 1160, val loss: 0.6462936997413635
Epoch 1170, training loss: 85.13081359863281 = 0.5995696187019348 + 10.0 * 8.453124046325684
Epoch 1170, val loss: 0.6407182812690735
Epoch 1180, training loss: 85.1070327758789 = 0.5933451652526855 + 10.0 * 8.45136833190918
Epoch 1180, val loss: 0.6353261470794678
Epoch 1190, training loss: 85.09246063232422 = 0.5872971415519714 + 10.0 * 8.450516700744629
Epoch 1190, val loss: 0.6301141977310181
Epoch 1200, training loss: 85.08177947998047 = 0.5813851356506348 + 10.0 * 8.450039863586426
Epoch 1200, val loss: 0.6250336766242981
Epoch 1210, training loss: 85.11487579345703 = 0.5755425095558167 + 10.0 * 8.453932762145996
Epoch 1210, val loss: 0.6200668811798096
Epoch 1220, training loss: 85.06056213378906 = 0.5699113011360168 + 10.0 * 8.449065208435059
Epoch 1220, val loss: 0.6152455806732178
Epoch 1230, training loss: 85.04641723632812 = 0.5644833445549011 + 10.0 * 8.448193550109863
Epoch 1230, val loss: 0.6106474995613098
Epoch 1240, training loss: 85.03173828125 = 0.5592389702796936 + 10.0 * 8.447249412536621
Epoch 1240, val loss: 0.6062210202217102
Epoch 1250, training loss: 85.04027557373047 = 0.5541148781776428 + 10.0 * 8.448616027832031
Epoch 1250, val loss: 0.6019625663757324
Epoch 1260, training loss: 85.00779724121094 = 0.5490758419036865 + 10.0 * 8.44587230682373
Epoch 1260, val loss: 0.5977357625961304
Epoch 1270, training loss: 84.99940490722656 = 0.5442601442337036 + 10.0 * 8.445514678955078
Epoch 1270, val loss: 0.5937700271606445
Epoch 1280, training loss: 84.98846435546875 = 0.5396392941474915 + 10.0 * 8.4448823928833
Epoch 1280, val loss: 0.589957594871521
Epoch 1290, training loss: 84.9758071899414 = 0.5351475477218628 + 10.0 * 8.444066047668457
Epoch 1290, val loss: 0.5863193869590759
Epoch 1300, training loss: 84.96446228027344 = 0.5307837128639221 + 10.0 * 8.443367958068848
Epoch 1300, val loss: 0.5827674269676208
Epoch 1310, training loss: 84.95679473876953 = 0.5265389680862427 + 10.0 * 8.443025588989258
Epoch 1310, val loss: 0.5793629884719849
Epoch 1320, training loss: 85.0013656616211 = 0.5223891139030457 + 10.0 * 8.447896957397461
Epoch 1320, val loss: 0.5760682225227356
Epoch 1330, training loss: 84.9423828125 = 0.5183526277542114 + 10.0 * 8.442402839660645
Epoch 1330, val loss: 0.5728446245193481
Epoch 1340, training loss: 84.927978515625 = 0.5145294070243835 + 10.0 * 8.44134521484375
Epoch 1340, val loss: 0.5698573589324951
Epoch 1350, training loss: 84.92236328125 = 0.5108543038368225 + 10.0 * 8.441150665283203
Epoch 1350, val loss: 0.5669899582862854
Epoch 1360, training loss: 84.96617126464844 = 0.5072636008262634 + 10.0 * 8.445890426635742
Epoch 1360, val loss: 0.5642600655555725
Epoch 1370, training loss: 84.92009735107422 = 0.5038155913352966 + 10.0 * 8.441628456115723
Epoch 1370, val loss: 0.5615736842155457
Epoch 1380, training loss: 84.9007339477539 = 0.5004801750183105 + 10.0 * 8.440025329589844
Epoch 1380, val loss: 0.559085488319397
Epoch 1390, training loss: 84.8878402709961 = 0.4973081052303314 + 10.0 * 8.439053535461426
Epoch 1390, val loss: 0.5566917061805725
Epoch 1400, training loss: 84.87724304199219 = 0.49421557784080505 + 10.0 * 8.438302993774414
Epoch 1400, val loss: 0.5544105172157288
Epoch 1410, training loss: 84.87529754638672 = 0.4912171959877014 + 10.0 * 8.438407897949219
Epoch 1410, val loss: 0.5522341132164001
Epoch 1420, training loss: 84.869873046875 = 0.48828327655792236 + 10.0 * 8.438158988952637
Epoch 1420, val loss: 0.5501143336296082
Epoch 1430, training loss: 84.86005401611328 = 0.4854739010334015 + 10.0 * 8.437458038330078
Epoch 1430, val loss: 0.5480958223342896
Epoch 1440, training loss: 84.85738372802734 = 0.48275452852249146 + 10.0 * 8.43746280670166
Epoch 1440, val loss: 0.5461771488189697
Epoch 1450, training loss: 84.88027954101562 = 0.48011714220046997 + 10.0 * 8.44001579284668
Epoch 1450, val loss: 0.5443639159202576
Epoch 1460, training loss: 84.84856414794922 = 0.47756829857826233 + 10.0 * 8.43709945678711
Epoch 1460, val loss: 0.542574942111969
Epoch 1470, training loss: 84.83323669433594 = 0.4751463532447815 + 10.0 * 8.435809135437012
Epoch 1470, val loss: 0.5409562587738037
Epoch 1480, training loss: 84.8226089477539 = 0.4727969467639923 + 10.0 * 8.434981346130371
Epoch 1480, val loss: 0.5393961071968079
Epoch 1490, training loss: 84.81527709960938 = 0.47051867842674255 + 10.0 * 8.434475898742676
Epoch 1490, val loss: 0.5378949642181396
Epoch 1500, training loss: 84.80924224853516 = 0.46830257773399353 + 10.0 * 8.434094429016113
Epoch 1500, val loss: 0.5364713668823242
Epoch 1510, training loss: 84.84730529785156 = 0.4661343991756439 + 10.0 * 8.438117027282715
Epoch 1510, val loss: 0.5351014733314514
Epoch 1520, training loss: 84.8321762084961 = 0.4640088677406311 + 10.0 * 8.436816215515137
Epoch 1520, val loss: 0.5337401628494263
Epoch 1530, training loss: 84.79328918457031 = 0.46198922395706177 + 10.0 * 8.433130264282227
Epoch 1530, val loss: 0.5325219631195068
Epoch 1540, training loss: 84.78984832763672 = 0.4600556790828705 + 10.0 * 8.432979583740234
Epoch 1540, val loss: 0.5313412547111511
Epoch 1550, training loss: 84.78877258300781 = 0.4581848084926605 + 10.0 * 8.433058738708496
Epoch 1550, val loss: 0.5302461981773376
Epoch 1560, training loss: 84.80248260498047 = 0.4563501477241516 + 10.0 * 8.434613227844238
Epoch 1560, val loss: 0.5291940569877625
Epoch 1570, training loss: 84.7737808227539 = 0.45456069707870483 + 10.0 * 8.43192195892334
Epoch 1570, val loss: 0.5281333327293396
Epoch 1580, training loss: 84.76521301269531 = 0.45283859968185425 + 10.0 * 8.431238174438477
Epoch 1580, val loss: 0.5271918773651123
Epoch 1590, training loss: 84.76022338867188 = 0.4511735141277313 + 10.0 * 8.43090534210205
Epoch 1590, val loss: 0.5262846350669861
Epoch 1600, training loss: 84.7685775756836 = 0.4495396912097931 + 10.0 * 8.431903839111328
Epoch 1600, val loss: 0.525393545627594
Epoch 1610, training loss: 84.75977325439453 = 0.4479270279407501 + 10.0 * 8.431184768676758
Epoch 1610, val loss: 0.5245746970176697
Epoch 1620, training loss: 84.7635498046875 = 0.4463740885257721 + 10.0 * 8.431717872619629
Epoch 1620, val loss: 0.5237900614738464
Epoch 1630, training loss: 84.75663757324219 = 0.4448559582233429 + 10.0 * 8.431178092956543
Epoch 1630, val loss: 0.5230134725570679
Epoch 1640, training loss: 84.7369384765625 = 0.4433831572532654 + 10.0 * 8.42935562133789
Epoch 1640, val loss: 0.5222746133804321
Epoch 1650, training loss: 84.73480224609375 = 0.44195273518562317 + 10.0 * 8.429285049438477
Epoch 1650, val loss: 0.5215746760368347
Epoch 1660, training loss: 84.75131225585938 = 0.4405497908592224 + 10.0 * 8.431076049804688
Epoch 1660, val loss: 0.520920991897583
Epoch 1670, training loss: 84.72201538085938 = 0.43917691707611084 + 10.0 * 8.42828369140625
Epoch 1670, val loss: 0.520287275314331
Epoch 1680, training loss: 84.7281494140625 = 0.4378432631492615 + 10.0 * 8.429030418395996
Epoch 1680, val loss: 0.5196881890296936
Epoch 1690, training loss: 84.73797607421875 = 0.4365403950214386 + 10.0 * 8.430143356323242
Epoch 1690, val loss: 0.5191100835800171
Epoch 1700, training loss: 84.70832824707031 = 0.43527474999427795 + 10.0 * 8.427305221557617
Epoch 1700, val loss: 0.5186055302619934
Epoch 1710, training loss: 84.7094955444336 = 0.43404266238212585 + 10.0 * 8.427545547485352
Epoch 1710, val loss: 0.5181244611740112
Epoch 1720, training loss: 84.70059967041016 = 0.43283024430274963 + 10.0 * 8.426776885986328
Epoch 1720, val loss: 0.5176081657409668
Epoch 1730, training loss: 84.70225524902344 = 0.4316380023956299 + 10.0 * 8.427061080932617
Epoch 1730, val loss: 0.5171343088150024
Epoch 1740, training loss: 84.72602844238281 = 0.4304611384868622 + 10.0 * 8.429556846618652
Epoch 1740, val loss: 0.5166984796524048
Epoch 1750, training loss: 84.70226287841797 = 0.42931264638900757 + 10.0 * 8.427294731140137
Epoch 1750, val loss: 0.5163432955741882
Epoch 1760, training loss: 84.69486999511719 = 0.4281923770904541 + 10.0 * 8.426668167114258
Epoch 1760, val loss: 0.5159027576446533
Epoch 1770, training loss: 84.68758392333984 = 0.4270938038825989 + 10.0 * 8.42604923248291
Epoch 1770, val loss: 0.5154967904090881
Epoch 1780, training loss: 84.70884704589844 = 0.42602425813674927 + 10.0 * 8.428281784057617
Epoch 1780, val loss: 0.5152201652526855
Epoch 1790, training loss: 84.68611145019531 = 0.4249722957611084 + 10.0 * 8.426114082336426
Epoch 1790, val loss: 0.5148051381111145
Epoch 1800, training loss: 84.6744155883789 = 0.4239403307437897 + 10.0 * 8.425046920776367
Epoch 1800, val loss: 0.5145130753517151
Epoch 1810, training loss: 84.67344665527344 = 0.42292699217796326 + 10.0 * 8.42505168914795
Epoch 1810, val loss: 0.5142244696617126
Epoch 1820, training loss: 84.7085952758789 = 0.42192530632019043 + 10.0 * 8.428667068481445
Epoch 1820, val loss: 0.5139519572257996
Epoch 1830, training loss: 84.6714859008789 = 0.4209345877170563 + 10.0 * 8.425054550170898
Epoch 1830, val loss: 0.5136458277702332
Epoch 1840, training loss: 84.6580810546875 = 0.4199710786342621 + 10.0 * 8.423810958862305
Epoch 1840, val loss: 0.5133882761001587
Epoch 1850, training loss: 84.65310668945312 = 0.4190300405025482 + 10.0 * 8.423407554626465
Epoch 1850, val loss: 0.5131411552429199
Epoch 1860, training loss: 84.65603637695312 = 0.4180966317653656 + 10.0 * 8.42379379272461
Epoch 1860, val loss: 0.5128970146179199
Epoch 1870, training loss: 84.69196319580078 = 0.4171648919582367 + 10.0 * 8.42747974395752
Epoch 1870, val loss: 0.5126817226409912
Epoch 1880, training loss: 84.64830780029297 = 0.4162648320198059 + 10.0 * 8.42320442199707
Epoch 1880, val loss: 0.5124819874763489
Epoch 1890, training loss: 84.64053344726562 = 0.4153761863708496 + 10.0 * 8.422515869140625
Epoch 1890, val loss: 0.5122998356819153
Epoch 1900, training loss: 84.6378402709961 = 0.4145100712776184 + 10.0 * 8.422332763671875
Epoch 1900, val loss: 0.512112021446228
Epoch 1910, training loss: 84.63591766357422 = 0.4136451184749603 + 10.0 * 8.42222785949707
Epoch 1910, val loss: 0.5119261145591736
Epoch 1920, training loss: 84.6686782836914 = 0.4127863049507141 + 10.0 * 8.425588607788086
Epoch 1920, val loss: 0.5117709636688232
Epoch 1930, training loss: 84.63214111328125 = 0.4119379222393036 + 10.0 * 8.422019958496094
Epoch 1930, val loss: 0.5115918517112732
Epoch 1940, training loss: 84.6256103515625 = 0.41111573576927185 + 10.0 * 8.421449661254883
Epoch 1940, val loss: 0.5115188956260681
Epoch 1950, training loss: 84.62133026123047 = 0.4103024899959564 + 10.0 * 8.421102523803711
Epoch 1950, val loss: 0.5113512277603149
Epoch 1960, training loss: 84.64183044433594 = 0.4094981849193573 + 10.0 * 8.423233032226562
Epoch 1960, val loss: 0.5112589001655579
Epoch 1970, training loss: 84.61637115478516 = 0.40869611501693726 + 10.0 * 8.420766830444336
Epoch 1970, val loss: 0.5111069083213806
Epoch 1980, training loss: 84.61061096191406 = 0.40791556239128113 + 10.0 * 8.420269966125488
Epoch 1980, val loss: 0.5110321640968323
Epoch 1990, training loss: 84.60758209228516 = 0.4071391224861145 + 10.0 * 8.4200439453125
Epoch 1990, val loss: 0.5109087824821472
Epoch 2000, training loss: 84.64425659179688 = 0.40636417269706726 + 10.0 * 8.423789024353027
Epoch 2000, val loss: 0.5107811689376831
Epoch 2010, training loss: 84.60846710205078 = 0.4055975377559662 + 10.0 * 8.420287132263184
Epoch 2010, val loss: 0.5108285546302795
Epoch 2020, training loss: 84.61141204833984 = 0.4048548936843872 + 10.0 * 8.420656204223633
Epoch 2020, val loss: 0.5106678605079651
Epoch 2030, training loss: 84.59883117675781 = 0.40412914752960205 + 10.0 * 8.419469833374023
Epoch 2030, val loss: 0.5106607675552368
Epoch 2040, training loss: 84.59159088134766 = 0.40340572595596313 + 10.0 * 8.418818473815918
Epoch 2040, val loss: 0.5105932354927063
Epoch 2050, training loss: 84.58799743652344 = 0.4026912450790405 + 10.0 * 8.418530464172363
Epoch 2050, val loss: 0.5105385780334473
Epoch 2060, training loss: 84.58485412597656 = 0.40197551250457764 + 10.0 * 8.41828727722168
Epoch 2060, val loss: 0.5104902386665344
Epoch 2070, training loss: 84.5826416015625 = 0.4012647271156311 + 10.0 * 8.418137550354004
Epoch 2070, val loss: 0.5104576945304871
Epoch 2080, training loss: 84.63396453857422 = 0.40054887533187866 + 10.0 * 8.423341751098633
Epoch 2080, val loss: 0.5104299783706665
Epoch 2090, training loss: 84.60308837890625 = 0.39984330534935 + 10.0 * 8.420324325561523
Epoch 2090, val loss: 0.5103645324707031
Epoch 2100, training loss: 84.57588195800781 = 0.39915961027145386 + 10.0 * 8.417672157287598
Epoch 2100, val loss: 0.5103703141212463
Epoch 2110, training loss: 84.57423400878906 = 0.39847978949546814 + 10.0 * 8.417574882507324
Epoch 2110, val loss: 0.5103346109390259
Epoch 2120, training loss: 84.57131958007812 = 0.3978114724159241 + 10.0 * 8.417350769042969
Epoch 2120, val loss: 0.5103186368942261
Epoch 2130, training loss: 84.61116027832031 = 0.3971382677555084 + 10.0 * 8.421401977539062
Epoch 2130, val loss: 0.5103144645690918
Epoch 2140, training loss: 84.5692367553711 = 0.39647895097732544 + 10.0 * 8.417276382446289
Epoch 2140, val loss: 0.5102869272232056
Epoch 2150, training loss: 84.5628890991211 = 0.3958301246166229 + 10.0 * 8.416706085205078
Epoch 2150, val loss: 0.5103487968444824
Epoch 2160, training loss: 84.55713653564453 = 0.39518019556999207 + 10.0 * 8.4161958694458
Epoch 2160, val loss: 0.510294497013092
Epoch 2170, training loss: 84.55397033691406 = 0.39453810453414917 + 10.0 * 8.415943145751953
Epoch 2170, val loss: 0.5103235244750977
Epoch 2180, training loss: 84.56729125976562 = 0.3938995599746704 + 10.0 * 8.417339324951172
Epoch 2180, val loss: 0.5103824138641357
Epoch 2190, training loss: 84.58057403564453 = 0.39326292276382446 + 10.0 * 8.418730735778809
Epoch 2190, val loss: 0.5104115605354309
Epoch 2200, training loss: 84.55271911621094 = 0.39262667298316956 + 10.0 * 8.416009902954102
Epoch 2200, val loss: 0.5103464722633362
Epoch 2210, training loss: 84.54017639160156 = 0.39200785756111145 + 10.0 * 8.414816856384277
Epoch 2210, val loss: 0.5104183554649353
Epoch 2220, training loss: 84.53876495361328 = 0.3913910388946533 + 10.0 * 8.414737701416016
Epoch 2220, val loss: 0.5104416012763977
Epoch 2230, training loss: 84.53545379638672 = 0.39077475666999817 + 10.0 * 8.414467811584473
Epoch 2230, val loss: 0.5104739665985107
Epoch 2240, training loss: 84.53974914550781 = 0.39016252756118774 + 10.0 * 8.414958953857422
Epoch 2240, val loss: 0.5105041861534119
Epoch 2250, training loss: 84.57954406738281 = 0.38954871892929077 + 10.0 * 8.418999671936035
Epoch 2250, val loss: 0.5105857253074646
Epoch 2260, training loss: 84.54107666015625 = 0.38894572854042053 + 10.0 * 8.415212631225586
Epoch 2260, val loss: 0.5105839967727661
Epoch 2270, training loss: 84.52806854248047 = 0.3883453607559204 + 10.0 * 8.413972854614258
Epoch 2270, val loss: 0.5106096863746643
Epoch 2280, training loss: 84.52491760253906 = 0.3877582550048828 + 10.0 * 8.413716316223145
Epoch 2280, val loss: 0.5107178688049316
Epoch 2290, training loss: 84.5671157836914 = 0.38716962933540344 + 10.0 * 8.417994499206543
Epoch 2290, val loss: 0.5108084082603455
Epoch 2300, training loss: 84.52701568603516 = 0.38657820224761963 + 10.0 * 8.414043426513672
Epoch 2300, val loss: 0.5107696652412415
Epoch 2310, training loss: 84.51708984375 = 0.3860045373439789 + 10.0 * 8.413108825683594
Epoch 2310, val loss: 0.5108305811882019
Epoch 2320, training loss: 84.51065063476562 = 0.38542914390563965 + 10.0 * 8.412522315979004
Epoch 2320, val loss: 0.5108893513679504
Epoch 2330, training loss: 84.51056671142578 = 0.38485848903656006 + 10.0 * 8.41257095336914
Epoch 2330, val loss: 0.5109326243400574
Epoch 2340, training loss: 84.55998992919922 = 0.38427963852882385 + 10.0 * 8.417571067810059
Epoch 2340, val loss: 0.5109508037567139
Epoch 2350, training loss: 84.51239776611328 = 0.3837202489376068 + 10.0 * 8.412867546081543
Epoch 2350, val loss: 0.5111486315727234
Epoch 2360, training loss: 84.502197265625 = 0.3831580579280853 + 10.0 * 8.411904335021973
Epoch 2360, val loss: 0.5111632943153381
Epoch 2370, training loss: 84.49919128417969 = 0.38260170817375183 + 10.0 * 8.411659240722656
Epoch 2370, val loss: 0.5112654566764832
Epoch 2380, training loss: 84.4935073852539 = 0.3820451498031616 + 10.0 * 8.41114616394043
Epoch 2380, val loss: 0.5113059878349304
Epoch 2390, training loss: 84.5125732421875 = 0.3814931809902191 + 10.0 * 8.413107872009277
Epoch 2390, val loss: 0.5114038586616516
Epoch 2400, training loss: 84.49494171142578 = 0.38093364238739014 + 10.0 * 8.41140079498291
Epoch 2400, val loss: 0.5115035772323608
Epoch 2410, training loss: 84.49201202392578 = 0.3803933560848236 + 10.0 * 8.411161422729492
Epoch 2410, val loss: 0.5115580558776855
Epoch 2420, training loss: 84.48503112792969 = 0.3798455595970154 + 10.0 * 8.410518646240234
Epoch 2420, val loss: 0.5116236805915833
Epoch 2430, training loss: 84.48442077636719 = 0.3793066442012787 + 10.0 * 8.41051197052002
Epoch 2430, val loss: 0.511688768863678
Epoch 2440, training loss: 84.54277801513672 = 0.37876275181770325 + 10.0 * 8.416401863098145
Epoch 2440, val loss: 0.5117455124855042
Epoch 2450, training loss: 84.494873046875 = 0.37823915481567383 + 10.0 * 8.411663055419922
Epoch 2450, val loss: 0.5119428038597107
Epoch 2460, training loss: 84.48075103759766 = 0.3777070641517639 + 10.0 * 8.410304069519043
Epoch 2460, val loss: 0.5119616985321045
Epoch 2470, training loss: 84.4726791381836 = 0.37718817591667175 + 10.0 * 8.40954875946045
Epoch 2470, val loss: 0.512100338935852
Epoch 2480, training loss: 84.46807861328125 = 0.37666454911231995 + 10.0 * 8.409141540527344
Epoch 2480, val loss: 0.5121699571609497
Epoch 2490, training loss: 84.46587371826172 = 0.3761427402496338 + 10.0 * 8.40897274017334
Epoch 2490, val loss: 0.5122448205947876
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7950279046169457
0.8133014562051729
=== training gcn model ===
Epoch 0, training loss: 106.90636444091797 = 1.0836818218231201 + 10.0 * 10.582268714904785
Epoch 0, val loss: 1.0845595598220825
Epoch 10, training loss: 106.8998794555664 = 1.0802053213119507 + 10.0 * 10.5819673538208
Epoch 10, val loss: 1.081113576889038
Epoch 20, training loss: 106.88481140136719 = 1.0767030715942383 + 10.0 * 10.580810546875
Epoch 20, val loss: 1.077626347541809
Epoch 30, training loss: 106.8324203491211 = 1.0729496479034424 + 10.0 * 10.575946807861328
Epoch 30, val loss: 1.073884129524231
Epoch 40, training loss: 106.61679077148438 = 1.0689276456832886 + 10.0 * 10.554786682128906
Epoch 40, val loss: 1.0698626041412354
Epoch 50, training loss: 105.75496673583984 = 1.0642948150634766 + 10.0 * 10.469067573547363
Epoch 50, val loss: 1.065195083618164
Epoch 60, training loss: 103.02494049072266 = 1.0589300394058228 + 10.0 * 10.196600914001465
Epoch 60, val loss: 1.0598009824752808
Epoch 70, training loss: 99.16664123535156 = 1.0524832010269165 + 10.0 * 9.811415672302246
Epoch 70, val loss: 1.0533027648925781
Epoch 80, training loss: 97.88987731933594 = 1.0467103719711304 + 10.0 * 9.684316635131836
Epoch 80, val loss: 1.0477581024169922
Epoch 90, training loss: 96.93305969238281 = 1.0438519716262817 + 10.0 * 9.588920593261719
Epoch 90, val loss: 1.0450279712677002
Epoch 100, training loss: 95.8703842163086 = 1.0423074960708618 + 10.0 * 9.482808113098145
Epoch 100, val loss: 1.0433543920516968
Epoch 110, training loss: 94.19608306884766 = 1.0410112142562866 + 10.0 * 9.315507888793945
Epoch 110, val loss: 1.0420310497283936
Epoch 120, training loss: 93.00550842285156 = 1.040284276008606 + 10.0 * 9.19652271270752
Epoch 120, val loss: 1.0413521528244019
Epoch 130, training loss: 92.2590103149414 = 1.039788007736206 + 10.0 * 9.121922492980957
Epoch 130, val loss: 1.0408672094345093
Epoch 140, training loss: 91.54859161376953 = 1.0393197536468506 + 10.0 * 9.05092716217041
Epoch 140, val loss: 1.0404784679412842
Epoch 150, training loss: 91.0667724609375 = 1.0388436317443848 + 10.0 * 9.002793312072754
Epoch 150, val loss: 1.0400668382644653
Epoch 160, training loss: 90.48845672607422 = 1.0382626056671143 + 10.0 * 8.945019721984863
Epoch 160, val loss: 1.0395796298980713
Epoch 170, training loss: 89.79310607910156 = 1.0383902788162231 + 10.0 * 8.875471115112305
Epoch 170, val loss: 1.0397803783416748
Epoch 180, training loss: 89.30154418945312 = 1.0386223793029785 + 10.0 * 8.826292037963867
Epoch 180, val loss: 1.0399818420410156
Epoch 190, training loss: 88.99050903320312 = 1.0382822751998901 + 10.0 * 8.795222282409668
Epoch 190, val loss: 1.0395931005477905
Epoch 200, training loss: 88.77265167236328 = 1.03744375705719 + 10.0 * 8.773520469665527
Epoch 200, val loss: 1.0387115478515625
Epoch 210, training loss: 88.5235595703125 = 1.0364972352981567 + 10.0 * 8.748705863952637
Epoch 210, val loss: 1.0377711057662964
Epoch 220, training loss: 88.25914764404297 = 1.03569495677948 + 10.0 * 8.722345352172852
Epoch 220, val loss: 1.037003755569458
Epoch 230, training loss: 88.03392791748047 = 1.0349422693252563 + 10.0 * 8.699898719787598
Epoch 230, val loss: 1.0362732410430908
Epoch 240, training loss: 87.87669372558594 = 1.0340718030929565 + 10.0 * 8.6842622756958
Epoch 240, val loss: 1.0354422330856323
Epoch 250, training loss: 87.75956726074219 = 1.0330798625946045 + 10.0 * 8.672648429870605
Epoch 250, val loss: 1.0345007181167603
Epoch 260, training loss: 87.6543960571289 = 1.032039999961853 + 10.0 * 8.662235260009766
Epoch 260, val loss: 1.0335088968276978
Epoch 270, training loss: 87.55193328857422 = 1.0310232639312744 + 10.0 * 8.652091026306152
Epoch 270, val loss: 1.032537579536438
Epoch 280, training loss: 87.4409408569336 = 1.030069351196289 + 10.0 * 8.641087532043457
Epoch 280, val loss: 1.031649112701416
Epoch 290, training loss: 87.33759307861328 = 1.0291599035263062 + 10.0 * 8.630843162536621
Epoch 290, val loss: 1.0307778120040894
Epoch 300, training loss: 87.25833129882812 = 1.028182864189148 + 10.0 * 8.623014450073242
Epoch 300, val loss: 1.0298486948013306
Epoch 310, training loss: 87.16239929199219 = 1.0271127223968506 + 10.0 * 8.61352825164795
Epoch 310, val loss: 1.0288193225860596
Epoch 320, training loss: 87.07527160644531 = 1.0259681940078735 + 10.0 * 8.60492992401123
Epoch 320, val loss: 1.0277280807495117
Epoch 330, training loss: 86.99418640136719 = 1.0247770547866821 + 10.0 * 8.596940994262695
Epoch 330, val loss: 1.0265804529190063
Epoch 340, training loss: 86.92144012451172 = 1.0235437154769897 + 10.0 * 8.589789390563965
Epoch 340, val loss: 1.025391936302185
Epoch 350, training loss: 86.86399841308594 = 1.022244930267334 + 10.0 * 8.584175109863281
Epoch 350, val loss: 1.0241376161575317
Epoch 360, training loss: 86.79084777832031 = 1.020856261253357 + 10.0 * 8.576998710632324
Epoch 360, val loss: 1.0227998495101929
Epoch 370, training loss: 86.73393249511719 = 1.0194520950317383 + 10.0 * 8.571447372436523
Epoch 370, val loss: 1.0214506387710571
Epoch 380, training loss: 86.72256469726562 = 1.0180232524871826 + 10.0 * 8.570454597473145
Epoch 380, val loss: 1.020062804222107
Epoch 390, training loss: 86.63093566894531 = 1.016494870185852 + 10.0 * 8.561444282531738
Epoch 390, val loss: 1.0185985565185547
Epoch 400, training loss: 86.5754165649414 = 1.0149651765823364 + 10.0 * 8.556044578552246
Epoch 400, val loss: 1.0171284675598145
Epoch 410, training loss: 86.52789306640625 = 1.013393759727478 + 10.0 * 8.5514497756958
Epoch 410, val loss: 1.0156184434890747
Epoch 420, training loss: 86.4805908203125 = 1.0117660760879517 + 10.0 * 8.546882629394531
Epoch 420, val loss: 1.0140492916107178
Epoch 430, training loss: 86.44782257080078 = 1.0100361108779907 + 10.0 * 8.543779373168945
Epoch 430, val loss: 1.0123786926269531
Epoch 440, training loss: 86.39278411865234 = 1.0081802606582642 + 10.0 * 8.538460731506348
Epoch 440, val loss: 1.0105878114700317
Epoch 450, training loss: 86.35711669921875 = 1.0062777996063232 + 10.0 * 8.535083770751953
Epoch 450, val loss: 1.008765459060669
Epoch 460, training loss: 86.316650390625 = 1.0043057203292847 + 10.0 * 8.531234741210938
Epoch 460, val loss: 1.0068738460540771
Epoch 470, training loss: 86.33334350585938 = 1.002232313156128 + 10.0 * 8.533110618591309
Epoch 470, val loss: 1.0048627853393555
Epoch 480, training loss: 86.25171661376953 = 0.9999517798423767 + 10.0 * 8.525176048278809
Epoch 480, val loss: 1.0026661157608032
Epoch 490, training loss: 86.2205581665039 = 0.9976180791854858 + 10.0 * 8.522294044494629
Epoch 490, val loss: 1.0004191398620605
Epoch 500, training loss: 86.19023132324219 = 0.9951885938644409 + 10.0 * 8.51950454711914
Epoch 500, val loss: 0.9980844259262085
Epoch 510, training loss: 86.16079711914062 = 0.9926642775535583 + 10.0 * 8.516813278198242
Epoch 510, val loss: 0.9956521391868591
Epoch 520, training loss: 86.129150390625 = 0.9900364279747009 + 10.0 * 8.513911247253418
Epoch 520, val loss: 0.993114173412323
Epoch 530, training loss: 86.1001205444336 = 0.9872728586196899 + 10.0 * 8.511284828186035
Epoch 530, val loss: 0.9904630184173584
Epoch 540, training loss: 86.09432220458984 = 0.984352171421051 + 10.0 * 8.51099681854248
Epoch 540, val loss: 0.9876104593276978
Epoch 550, training loss: 86.04139709472656 = 0.9812764525413513 + 10.0 * 8.506011962890625
Epoch 550, val loss: 0.9846842288970947
Epoch 560, training loss: 86.01026153564453 = 0.9781292676925659 + 10.0 * 8.503213882446289
Epoch 560, val loss: 0.9816678762435913
Epoch 570, training loss: 85.98815155029297 = 0.9748589992523193 + 10.0 * 8.50132942199707
Epoch 570, val loss: 0.9785086512565613
Epoch 580, training loss: 85.96607208251953 = 0.9713805317878723 + 10.0 * 8.499468803405762
Epoch 580, val loss: 0.9751449227333069
Epoch 590, training loss: 85.93597412109375 = 0.9676785469055176 + 10.0 * 8.49682903289795
Epoch 590, val loss: 0.9715822339057922
Epoch 600, training loss: 85.9124984741211 = 0.9638375639915466 + 10.0 * 8.494866371154785
Epoch 600, val loss: 0.967892587184906
Epoch 610, training loss: 85.8885726928711 = 0.9598275423049927 + 10.0 * 8.492874145507812
Epoch 610, val loss: 0.9640272855758667
Epoch 620, training loss: 85.86935424804688 = 0.9556276798248291 + 10.0 * 8.491373062133789
Epoch 620, val loss: 0.9599828124046326
Epoch 630, training loss: 85.84978485107422 = 0.951238751411438 + 10.0 * 8.48985481262207
Epoch 630, val loss: 0.9557614922523499
Epoch 640, training loss: 85.86573028564453 = 0.9466094374656677 + 10.0 * 8.491911888122559
Epoch 640, val loss: 0.951295793056488
Epoch 650, training loss: 85.82247924804688 = 0.9417424201965332 + 10.0 * 8.488073348999023
Epoch 650, val loss: 0.9466313123703003
Epoch 660, training loss: 85.79325103759766 = 0.9366836547851562 + 10.0 * 8.48565673828125
Epoch 660, val loss: 0.941777765750885
Epoch 670, training loss: 85.77652740478516 = 0.931405782699585 + 10.0 * 8.484512329101562
Epoch 670, val loss: 0.9367174506187439
Epoch 680, training loss: 85.75873565673828 = 0.9259272217750549 + 10.0 * 8.483281135559082
Epoch 680, val loss: 0.9314630031585693
Epoch 690, training loss: 85.74201965332031 = 0.9200760126113892 + 10.0 * 8.482194900512695
Epoch 690, val loss: 0.9258413314819336
Epoch 700, training loss: 85.7461929321289 = 0.9138525724411011 + 10.0 * 8.483234405517578
Epoch 700, val loss: 0.9199160933494568
Epoch 710, training loss: 85.71854400634766 = 0.9076792001724243 + 10.0 * 8.481086730957031
Epoch 710, val loss: 0.9140231609344482
Epoch 720, training loss: 85.69449615478516 = 0.9012500643730164 + 10.0 * 8.479324340820312
Epoch 720, val loss: 0.9078660607337952
Epoch 730, training loss: 85.68373107910156 = 0.8945610523223877 + 10.0 * 8.478917121887207
Epoch 730, val loss: 0.9014821648597717
Epoch 740, training loss: 85.65692901611328 = 0.887636125087738 + 10.0 * 8.476929664611816
Epoch 740, val loss: 0.8948794007301331
Epoch 750, training loss: 85.63672637939453 = 0.8804700374603271 + 10.0 * 8.475625991821289
Epoch 750, val loss: 0.8880557417869568
Epoch 760, training loss: 85.62042236328125 = 0.8730244040489197 + 10.0 * 8.474740028381348
Epoch 760, val loss: 0.880973756313324
Epoch 770, training loss: 85.6404800415039 = 0.8652392029762268 + 10.0 * 8.477523803710938
Epoch 770, val loss: 0.8735926151275635
Epoch 780, training loss: 85.59928894042969 = 0.8571724891662598 + 10.0 * 8.474211692810059
Epoch 780, val loss: 0.8659424185752869
Epoch 790, training loss: 85.56433868408203 = 0.8490099310874939 + 10.0 * 8.471532821655273
Epoch 790, val loss: 0.8582214713096619
Epoch 800, training loss: 85.54608154296875 = 0.8408092856407166 + 10.0 * 8.470526695251465
Epoch 800, val loss: 0.850509762763977
Epoch 810, training loss: 85.52590942382812 = 0.8325779438018799 + 10.0 * 8.469332695007324
Epoch 810, val loss: 0.842705249786377
Epoch 820, training loss: 85.52066802978516 = 0.8241320848464966 + 10.0 * 8.469653129577637
Epoch 820, val loss: 0.8347182273864746
Epoch 830, training loss: 85.50999450683594 = 0.8155199289321899 + 10.0 * 8.469447135925293
Epoch 830, val loss: 0.8266336917877197
Epoch 840, training loss: 85.4736328125 = 0.807075560092926 + 10.0 * 8.466655731201172
Epoch 840, val loss: 0.8186847567558289
Epoch 850, training loss: 85.45397186279297 = 0.7986986637115479 + 10.0 * 8.465527534484863
Epoch 850, val loss: 0.8108087182044983
Epoch 860, training loss: 85.43598937988281 = 0.7903125882148743 + 10.0 * 8.464567184448242
Epoch 860, val loss: 0.8029506206512451
Epoch 870, training loss: 85.41743469238281 = 0.7819375991821289 + 10.0 * 8.463549613952637
Epoch 870, val loss: 0.7951005101203918
Epoch 880, training loss: 85.43362426757812 = 0.7735496163368225 + 10.0 * 8.466007232666016
Epoch 880, val loss: 0.7872388958930969
Epoch 890, training loss: 85.39511108398438 = 0.7651524543762207 + 10.0 * 8.462995529174805
Epoch 890, val loss: 0.7794705033302307
Epoch 900, training loss: 85.38088989257812 = 0.7568763494491577 + 10.0 * 8.462401390075684
Epoch 900, val loss: 0.7717703580856323
Epoch 910, training loss: 85.35181427001953 = 0.7487264275550842 + 10.0 * 8.460309028625488
Epoch 910, val loss: 0.7642029523849487
Epoch 920, training loss: 85.33051300048828 = 0.7407074570655823 + 10.0 * 8.458980560302734
Epoch 920, val loss: 0.7567986249923706
Epoch 930, training loss: 85.31672668457031 = 0.7327677607536316 + 10.0 * 8.458395957946777
Epoch 930, val loss: 0.7494810223579407
Epoch 940, training loss: 85.3197250366211 = 0.724854052066803 + 10.0 * 8.459486961364746
Epoch 940, val loss: 0.7422064542770386
Epoch 950, training loss: 85.28482055664062 = 0.7169783711433411 + 10.0 * 8.45678424835205
Epoch 950, val loss: 0.734942615032196
Epoch 960, training loss: 85.26325988769531 = 0.7092626690864563 + 10.0 * 8.455400466918945
Epoch 960, val loss: 0.7278952598571777
Epoch 970, training loss: 85.25480651855469 = 0.701657235622406 + 10.0 * 8.455314636230469
Epoch 970, val loss: 0.720969021320343
Epoch 980, training loss: 85.24900817871094 = 0.6940897107124329 + 10.0 * 8.45549201965332
Epoch 980, val loss: 0.7140341401100159
Epoch 990, training loss: 85.22369384765625 = 0.6866549849510193 + 10.0 * 8.453703880310059
Epoch 990, val loss: 0.7073356509208679
Epoch 1000, training loss: 85.20218658447266 = 0.6794078350067139 + 10.0 * 8.452278137207031
Epoch 1000, val loss: 0.7007446885108948
Epoch 1010, training loss: 85.18851470947266 = 0.6722853779792786 + 10.0 * 8.45162296295166
Epoch 1010, val loss: 0.6942957639694214
Epoch 1020, training loss: 85.17842864990234 = 0.6652542352676392 + 10.0 * 8.45131778717041
Epoch 1020, val loss: 0.6879298686981201
Epoch 1030, training loss: 85.16706085205078 = 0.6582898497581482 + 10.0 * 8.45087718963623
Epoch 1030, val loss: 0.6816697716712952
Epoch 1040, training loss: 85.1517333984375 = 0.6514462232589722 + 10.0 * 8.450029373168945
Epoch 1040, val loss: 0.6755846738815308
Epoch 1050, training loss: 85.14405822753906 = 0.6447133421897888 + 10.0 * 8.449934005737305
Epoch 1050, val loss: 0.6695808172225952
Epoch 1060, training loss: 85.12178039550781 = 0.6381218433380127 + 10.0 * 8.448366165161133
Epoch 1060, val loss: 0.6636862754821777
Epoch 1070, training loss: 85.10672760009766 = 0.6317128539085388 + 10.0 * 8.447501182556152
Epoch 1070, val loss: 0.6579970717430115
Epoch 1080, training loss: 85.09058380126953 = 0.625437319278717 + 10.0 * 8.446515083312988
Epoch 1080, val loss: 0.6524494290351868
Epoch 1090, training loss: 85.07806396484375 = 0.6192527413368225 + 10.0 * 8.445880889892578
Epoch 1090, val loss: 0.6470102071762085
Epoch 1100, training loss: 85.06607055664062 = 0.6131650805473328 + 10.0 * 8.445290565490723
Epoch 1100, val loss: 0.6416445374488831
Epoch 1110, training loss: 85.09981536865234 = 0.6071624159812927 + 10.0 * 8.449265480041504
Epoch 1110, val loss: 0.6363001465797424
Epoch 1120, training loss: 85.04927825927734 = 0.6012076735496521 + 10.0 * 8.444807052612305
Epoch 1120, val loss: 0.6312116980552673
Epoch 1130, training loss: 85.0374984741211 = 0.5954689979553223 + 10.0 * 8.44420337677002
Epoch 1130, val loss: 0.6262451410293579
Epoch 1140, training loss: 85.02104187011719 = 0.5898933410644531 + 10.0 * 8.443114280700684
Epoch 1140, val loss: 0.6214261651039124
Epoch 1150, training loss: 85.01239013671875 = 0.5844366550445557 + 10.0 * 8.442795753479004
Epoch 1150, val loss: 0.616724967956543
Epoch 1160, training loss: 85.03996276855469 = 0.579073429107666 + 10.0 * 8.446088790893555
Epoch 1160, val loss: 0.6121559739112854
Epoch 1170, training loss: 85.00108337402344 = 0.5738129019737244 + 10.0 * 8.442727088928223
Epoch 1170, val loss: 0.6075906157493591
Epoch 1180, training loss: 84.981689453125 = 0.5687295794487 + 10.0 * 8.441295623779297
Epoch 1180, val loss: 0.6033231616020203
Epoch 1190, training loss: 84.969482421875 = 0.5637808442115784 + 10.0 * 8.440569877624512
Epoch 1190, val loss: 0.5991322994232178
Epoch 1200, training loss: 84.95992279052734 = 0.5589447021484375 + 10.0 * 8.44009780883789
Epoch 1200, val loss: 0.5950969457626343
Epoch 1210, training loss: 84.97476959228516 = 0.5542117953300476 + 10.0 * 8.442055702209473
Epoch 1210, val loss: 0.5911865830421448
Epoch 1220, training loss: 84.95990753173828 = 0.549555242061615 + 10.0 * 8.441035270690918
Epoch 1220, val loss: 0.5872403383255005
Epoch 1230, training loss: 84.93405151367188 = 0.5450885891914368 + 10.0 * 8.438896179199219
Epoch 1230, val loss: 0.5835912227630615
Epoch 1240, training loss: 84.92607116699219 = 0.5407756567001343 + 10.0 * 8.438529014587402
Epoch 1240, val loss: 0.5800727605819702
Epoch 1250, training loss: 84.91471099853516 = 0.5365807414054871 + 10.0 * 8.437812805175781
Epoch 1250, val loss: 0.5766550898551941
Epoch 1260, training loss: 84.9068832397461 = 0.5324886441230774 + 10.0 * 8.437439918518066
Epoch 1260, val loss: 0.5733637809753418
Epoch 1270, training loss: 84.91969299316406 = 0.5284889936447144 + 10.0 * 8.439120292663574
Epoch 1270, val loss: 0.5702454447746277
Epoch 1280, training loss: 84.91592407226562 = 0.5245943665504456 + 10.0 * 8.439132690429688
Epoch 1280, val loss: 0.5670149922370911
Epoch 1290, training loss: 84.88798522949219 = 0.5208123922348022 + 10.0 * 8.43671703338623
Epoch 1290, val loss: 0.5640843510627747
Epoch 1300, training loss: 84.87681579589844 = 0.517193078994751 + 10.0 * 8.435961723327637
Epoch 1300, val loss: 0.5612583160400391
Epoch 1310, training loss: 84.86792755126953 = 0.5136843323707581 + 10.0 * 8.435423851013184
Epoch 1310, val loss: 0.5585404634475708
Epoch 1320, training loss: 84.86397552490234 = 0.5102635622024536 + 10.0 * 8.435371398925781
Epoch 1320, val loss: 0.5559290051460266
Epoch 1330, training loss: 84.8841323852539 = 0.5069206953048706 + 10.0 * 8.437721252441406
Epoch 1330, val loss: 0.5534400939941406
Epoch 1340, training loss: 84.85386657714844 = 0.503661572933197 + 10.0 * 8.435020446777344
Epoch 1340, val loss: 0.5508847832679749
Epoch 1350, training loss: 84.83989715576172 = 0.5005252361297607 + 10.0 * 8.433937072753906
Epoch 1350, val loss: 0.5485982298851013
Epoch 1360, training loss: 84.8309326171875 = 0.49747949838638306 + 10.0 * 8.433344841003418
Epoch 1360, val loss: 0.5463602542877197
Epoch 1370, training loss: 84.82626342773438 = 0.49451595544815063 + 10.0 * 8.433175086975098
Epoch 1370, val loss: 0.5442028045654297
Epoch 1380, training loss: 84.84424591064453 = 0.4916277527809143 + 10.0 * 8.435261726379395
Epoch 1380, val loss: 0.5421947240829468
Epoch 1390, training loss: 84.83991241455078 = 0.48881030082702637 + 10.0 * 8.435110092163086
Epoch 1390, val loss: 0.5401636362075806
Epoch 1400, training loss: 84.8138198852539 = 0.48609471321105957 + 10.0 * 8.432772636413574
Epoch 1400, val loss: 0.5382030606269836
Epoch 1410, training loss: 84.7984390258789 = 0.4834812581539154 + 10.0 * 8.431495666503906
Epoch 1410, val loss: 0.5364131927490234
Epoch 1420, training loss: 84.79276275634766 = 0.48094648122787476 + 10.0 * 8.431180953979492
Epoch 1420, val loss: 0.5347108244895935
Epoch 1430, training loss: 84.79205322265625 = 0.4784751236438751 + 10.0 * 8.431357383728027
Epoch 1430, val loss: 0.5330811738967896
Epoch 1440, training loss: 84.80585479736328 = 0.4760585129261017 + 10.0 * 8.432979583740234
Epoch 1440, val loss: 0.5315123200416565
Epoch 1450, training loss: 84.78485870361328 = 0.4737045466899872 + 10.0 * 8.431116104125977
Epoch 1450, val loss: 0.5298586487770081
Epoch 1460, training loss: 84.78126525878906 = 0.4714324176311493 + 10.0 * 8.43098258972168
Epoch 1460, val loss: 0.5284949541091919
Epoch 1470, training loss: 84.7632827758789 = 0.469234824180603 + 10.0 * 8.429404258728027
Epoch 1470, val loss: 0.5269764065742493
Epoch 1480, training loss: 84.7568130493164 = 0.4670998156070709 + 10.0 * 8.428971290588379
Epoch 1480, val loss: 0.5256681442260742
Epoch 1490, training loss: 84.75196838378906 = 0.4650195240974426 + 10.0 * 8.428694725036621
Epoch 1490, val loss: 0.5244168639183044
Epoch 1500, training loss: 84.76630401611328 = 0.4629891812801361 + 10.0 * 8.430331230163574
Epoch 1500, val loss: 0.5231364369392395
Epoch 1510, training loss: 84.76176452636719 = 0.46101194620132446 + 10.0 * 8.430074691772461
Epoch 1510, val loss: 0.5219131708145142
Epoch 1520, training loss: 84.74122619628906 = 0.4590851962566376 + 10.0 * 8.428214073181152
Epoch 1520, val loss: 0.5208908915519714
Epoch 1530, training loss: 84.72895812988281 = 0.4572271406650543 + 10.0 * 8.427172660827637
Epoch 1530, val loss: 0.519812822341919
Epoch 1540, training loss: 84.73075866699219 = 0.45541641116142273 + 10.0 * 8.427534103393555
Epoch 1540, val loss: 0.5187882781028748
Epoch 1550, training loss: 84.7315673828125 = 0.453645259141922 + 10.0 * 8.4277925491333
Epoch 1550, val loss: 0.5177726745605469
Epoch 1560, training loss: 84.72944641113281 = 0.45192253589630127 + 10.0 * 8.427752494812012
Epoch 1560, val loss: 0.5167550444602966
Epoch 1570, training loss: 84.70834350585938 = 0.45024290680885315 + 10.0 * 8.425809860229492
Epoch 1570, val loss: 0.5159862637519836
Epoch 1580, training loss: 84.70333099365234 = 0.44861963391304016 + 10.0 * 8.425471305847168
Epoch 1580, val loss: 0.5151289105415344
Epoch 1590, training loss: 84.7003173828125 = 0.44703662395477295 + 10.0 * 8.425328254699707
Epoch 1590, val loss: 0.5143203139305115
Epoch 1600, training loss: 84.71961212158203 = 0.44548308849334717 + 10.0 * 8.427412986755371
Epoch 1600, val loss: 0.5136001110076904
Epoch 1610, training loss: 84.69530487060547 = 0.44396108388900757 + 10.0 * 8.425134658813477
Epoch 1610, val loss: 0.5127833485603333
Epoch 1620, training loss: 84.68362426757812 = 0.44247621297836304 + 10.0 * 8.424115180969238
Epoch 1620, val loss: 0.512044370174408
Epoch 1630, training loss: 84.67808532714844 = 0.44103139638900757 + 10.0 * 8.423705101013184
Epoch 1630, val loss: 0.5113400220870972
Epoch 1640, training loss: 84.68006134033203 = 0.43961232900619507 + 10.0 * 8.424044609069824
Epoch 1640, val loss: 0.5106302499771118
Epoch 1650, training loss: 84.69888305664062 = 0.43821465969085693 + 10.0 * 8.426066398620605
Epoch 1650, val loss: 0.509981095790863
Epoch 1660, training loss: 84.67678833007812 = 0.436845988035202 + 10.0 * 8.423994064331055
Epoch 1660, val loss: 0.5095125436782837
Epoch 1670, training loss: 84.6740951538086 = 0.43551912903785706 + 10.0 * 8.423857688903809
Epoch 1670, val loss: 0.5088194012641907
Epoch 1680, training loss: 84.65603637695312 = 0.43421685695648193 + 10.0 * 8.422182083129883
Epoch 1680, val loss: 0.5083682537078857
Epoch 1690, training loss: 84.65091705322266 = 0.4329449534416199 + 10.0 * 8.421796798706055
Epoch 1690, val loss: 0.5078155398368835
Epoch 1700, training loss: 84.64875030517578 = 0.4316938519477844 + 10.0 * 8.42170524597168
Epoch 1700, val loss: 0.5072636604309082
Epoch 1710, training loss: 84.66645050048828 = 0.4304623007774353 + 10.0 * 8.423598289489746
Epoch 1710, val loss: 0.5067720413208008
Epoch 1720, training loss: 84.65550994873047 = 0.4292490482330322 + 10.0 * 8.422625541687012
Epoch 1720, val loss: 0.5063660144805908
Epoch 1730, training loss: 84.63797760009766 = 0.4280645251274109 + 10.0 * 8.420991897583008
Epoch 1730, val loss: 0.5059378147125244
Epoch 1740, training loss: 84.63311767578125 = 0.42691147327423096 + 10.0 * 8.420620918273926
Epoch 1740, val loss: 0.505482017993927
Epoch 1750, training loss: 84.62716674804688 = 0.42577922344207764 + 10.0 * 8.420138359069824
Epoch 1750, val loss: 0.5050444006919861
Epoch 1760, training loss: 84.63585662841797 = 0.4246620535850525 + 10.0 * 8.421119689941406
Epoch 1760, val loss: 0.5045693516731262
Epoch 1770, training loss: 84.63583374023438 = 0.4235539436340332 + 10.0 * 8.421228408813477
Epoch 1770, val loss: 0.5042660236358643
Epoch 1780, training loss: 84.61837005615234 = 0.42247143387794495 + 10.0 * 8.41958999633789
Epoch 1780, val loss: 0.5039924383163452
Epoch 1790, training loss: 84.61229705810547 = 0.4214131236076355 + 10.0 * 8.419088363647461
Epoch 1790, val loss: 0.5035629868507385
Epoch 1800, training loss: 84.60498046875 = 0.42036741971969604 + 10.0 * 8.418461799621582
Epoch 1800, val loss: 0.503282904624939
Epoch 1810, training loss: 84.60279846191406 = 0.4193359315395355 + 10.0 * 8.418346405029297
Epoch 1810, val loss: 0.5029378533363342
Epoch 1820, training loss: 84.62584686279297 = 0.4183141589164734 + 10.0 * 8.420753479003906
Epoch 1820, val loss: 0.5025519728660583
Epoch 1830, training loss: 84.60585021972656 = 0.41730770468711853 + 10.0 * 8.418854713439941
Epoch 1830, val loss: 0.5022982358932495
Epoch 1840, training loss: 84.59408569335938 = 0.4163197875022888 + 10.0 * 8.417776107788086
Epoch 1840, val loss: 0.5020754933357239
Epoch 1850, training loss: 84.58785247802734 = 0.41535279154777527 + 10.0 * 8.41724967956543
Epoch 1850, val loss: 0.501800000667572
Epoch 1860, training loss: 84.58493041992188 = 0.4143955707550049 + 10.0 * 8.41705322265625
Epoch 1860, val loss: 0.5015466213226318
Epoch 1870, training loss: 84.61768341064453 = 0.4134482741355896 + 10.0 * 8.42042350769043
Epoch 1870, val loss: 0.5014019012451172
Epoch 1880, training loss: 84.58839416503906 = 0.41251662373542786 + 10.0 * 8.417588233947754
Epoch 1880, val loss: 0.5009706020355225
Epoch 1890, training loss: 84.57799530029297 = 0.4115969240665436 + 10.0 * 8.41663932800293
Epoch 1890, val loss: 0.5008547902107239
Epoch 1900, training loss: 84.60469055175781 = 0.41069361567497253 + 10.0 * 8.419400215148926
Epoch 1900, val loss: 0.5006462335586548
Epoch 1910, training loss: 84.56765747070312 = 0.4097966253757477 + 10.0 * 8.415785789489746
Epoch 1910, val loss: 0.500331461429596
Epoch 1920, training loss: 84.56415557861328 = 0.4089187979698181 + 10.0 * 8.415523529052734
Epoch 1920, val loss: 0.5001693367958069
Epoch 1930, training loss: 84.55909729003906 = 0.4080537259578705 + 10.0 * 8.415104866027832
Epoch 1930, val loss: 0.4999525547027588
Epoch 1940, training loss: 84.5548095703125 = 0.4071958065032959 + 10.0 * 8.414761543273926
Epoch 1940, val loss: 0.49978914856910706
Epoch 1950, training loss: 84.55516052246094 = 0.4063447117805481 + 10.0 * 8.414881706237793
Epoch 1950, val loss: 0.4996313154697418
Epoch 1960, training loss: 84.5887451171875 = 0.4055008292198181 + 10.0 * 8.41832447052002
Epoch 1960, val loss: 0.499535471200943
Epoch 1970, training loss: 84.55634307861328 = 0.40466246008872986 + 10.0 * 8.415167808532715
Epoch 1970, val loss: 0.49920889735221863
Epoch 1980, training loss: 84.542236328125 = 0.4038386940956116 + 10.0 * 8.413839340209961
Epoch 1980, val loss: 0.4990968406200409
Epoch 1990, training loss: 84.53997039794922 = 0.4030258059501648 + 10.0 * 8.413694381713867
Epoch 1990, val loss: 0.49894753098487854
Epoch 2000, training loss: 84.53929138183594 = 0.4022178649902344 + 10.0 * 8.41370677947998
Epoch 2000, val loss: 0.49876293540000916
Epoch 2010, training loss: 84.5569076538086 = 0.4014146029949188 + 10.0 * 8.415549278259277
Epoch 2010, val loss: 0.4986339211463928
Epoch 2020, training loss: 84.53529357910156 = 0.40062466263771057 + 10.0 * 8.413466453552246
Epoch 2020, val loss: 0.49853745102882385
Epoch 2030, training loss: 84.53455352783203 = 0.3998418152332306 + 10.0 * 8.413471221923828
Epoch 2030, val loss: 0.4983539879322052
Epoch 2040, training loss: 84.56127166748047 = 0.39907070994377136 + 10.0 * 8.416219711303711
Epoch 2040, val loss: 0.4981333911418915
Epoch 2050, training loss: 84.53199768066406 = 0.3983062207698822 + 10.0 * 8.413369178771973
Epoch 2050, val loss: 0.49825534224510193
Epoch 2060, training loss: 84.5210952758789 = 0.39755603671073914 + 10.0 * 8.412354469299316
Epoch 2060, val loss: 0.4980435073375702
Epoch 2070, training loss: 84.51648712158203 = 0.3968116044998169 + 10.0 * 8.411967277526855
Epoch 2070, val loss: 0.49798667430877686
Epoch 2080, training loss: 84.51235961914062 = 0.3960725665092468 + 10.0 * 8.411628723144531
Epoch 2080, val loss: 0.4978836476802826
Epoch 2090, training loss: 84.5195541381836 = 0.3953358232975006 + 10.0 * 8.412422180175781
Epoch 2090, val loss: 0.497797429561615
Epoch 2100, training loss: 84.5250473022461 = 0.3946031928062439 + 10.0 * 8.413044929504395
Epoch 2100, val loss: 0.49766668677330017
Epoch 2110, training loss: 84.51307678222656 = 0.3938831686973572 + 10.0 * 8.411919593811035
Epoch 2110, val loss: 0.4975666403770447
Epoch 2120, training loss: 84.50321197509766 = 0.39316660165786743 + 10.0 * 8.411005020141602
Epoch 2120, val loss: 0.49756601452827454
Epoch 2130, training loss: 84.49896240234375 = 0.39245712757110596 + 10.0 * 8.410650253295898
Epoch 2130, val loss: 0.4974675178527832
Epoch 2140, training loss: 84.49957275390625 = 0.3917498290538788 + 10.0 * 8.410782814025879
Epoch 2140, val loss: 0.49737128615379333
Epoch 2150, training loss: 84.5658950805664 = 0.3910489082336426 + 10.0 * 8.417484283447266
Epoch 2150, val loss: 0.49715760350227356
Epoch 2160, training loss: 84.50984954833984 = 0.39035341143608093 + 10.0 * 8.41195011138916
Epoch 2160, val loss: 0.49741265177726746
Epoch 2170, training loss: 84.4884033203125 = 0.3896690905094147 + 10.0 * 8.409873008728027
Epoch 2170, val loss: 0.49729758501052856
Epoch 2180, training loss: 84.48467254638672 = 0.3889942765235901 + 10.0 * 8.409567832946777
Epoch 2180, val loss: 0.4972006380558014
Epoch 2190, training loss: 84.4820556640625 = 0.3883211016654968 + 10.0 * 8.40937328338623
Epoch 2190, val loss: 0.49721065163612366
Epoch 2200, training loss: 84.48002624511719 = 0.3876486122608185 + 10.0 * 8.4092378616333
Epoch 2200, val loss: 0.49713119864463806
Epoch 2210, training loss: 84.50042724609375 = 0.3869783878326416 + 10.0 * 8.411344528198242
Epoch 2210, val loss: 0.4970790147781372
Epoch 2220, training loss: 84.48826599121094 = 0.3863152861595154 + 10.0 * 8.410195350646973
Epoch 2220, val loss: 0.49703171849250793
Epoch 2230, training loss: 84.4787826538086 = 0.38565903902053833 + 10.0 * 8.40931224822998
Epoch 2230, val loss: 0.4970705807209015
Epoch 2240, training loss: 84.472412109375 = 0.3850105404853821 + 10.0 * 8.408740043640137
Epoch 2240, val loss: 0.4970598816871643
Epoch 2250, training loss: 84.46745300292969 = 0.3843642771244049 + 10.0 * 8.408308982849121
Epoch 2250, val loss: 0.4970349371433258
Epoch 2260, training loss: 84.46617889404297 = 0.3837197721004486 + 10.0 * 8.408246040344238
Epoch 2260, val loss: 0.49701714515686035
Epoch 2270, training loss: 84.48297882080078 = 0.3830760717391968 + 10.0 * 8.409990310668945
Epoch 2270, val loss: 0.49704116582870483
Epoch 2280, training loss: 84.46508026123047 = 0.38244113326072693 + 10.0 * 8.40826416015625
Epoch 2280, val loss: 0.49702486395835876
Epoch 2290, training loss: 84.4590835571289 = 0.3818114399909973 + 10.0 * 8.407727241516113
Epoch 2290, val loss: 0.4970357418060303
Epoch 2300, training loss: 84.4980697631836 = 0.3811887502670288 + 10.0 * 8.411687850952148
Epoch 2300, val loss: 0.4971684515476227
Epoch 2310, training loss: 84.46035766601562 = 0.38056641817092896 + 10.0 * 8.407979011535645
Epoch 2310, val loss: 0.4969712495803833
Epoch 2320, training loss: 84.45580291748047 = 0.3799576163291931 + 10.0 * 8.407584190368652
Epoch 2320, val loss: 0.4970824718475342
Epoch 2330, training loss: 84.44901275634766 = 0.37935030460357666 + 10.0 * 8.406966209411621
Epoch 2330, val loss: 0.4970553517341614
Epoch 2340, training loss: 84.44451141357422 = 0.3787451684474945 + 10.0 * 8.406576156616211
Epoch 2340, val loss: 0.49708032608032227
Epoch 2350, training loss: 84.44171905517578 = 0.37814053893089294 + 10.0 * 8.406357765197754
Epoch 2350, val loss: 0.49707168340682983
Epoch 2360, training loss: 84.44071960449219 = 0.3775363564491272 + 10.0 * 8.406318664550781
Epoch 2360, val loss: 0.49705594778060913
Epoch 2370, training loss: 84.46517944335938 = 0.3769356608390808 + 10.0 * 8.40882396697998
Epoch 2370, val loss: 0.496946781873703
Epoch 2380, training loss: 84.44055938720703 = 0.376336008310318 + 10.0 * 8.40642261505127
Epoch 2380, val loss: 0.4972496032714844
Epoch 2390, training loss: 84.45411682128906 = 0.3757481873035431 + 10.0 * 8.4078369140625
Epoch 2390, val loss: 0.4971713423728943
Epoch 2400, training loss: 84.43490600585938 = 0.37516143918037415 + 10.0 * 8.405974388122559
Epoch 2400, val loss: 0.4972764849662781
Epoch 2410, training loss: 84.43138885498047 = 0.37458178400993347 + 10.0 * 8.405680656433105
Epoch 2410, val loss: 0.49725642800331116
Epoch 2420, training loss: 84.4283447265625 = 0.37400561571121216 + 10.0 * 8.405433654785156
Epoch 2420, val loss: 0.4972963035106659
Epoch 2430, training loss: 84.4241714477539 = 0.3734285235404968 + 10.0 * 8.405074119567871
Epoch 2430, val loss: 0.49733051657676697
Epoch 2440, training loss: 84.43315124511719 = 0.37285250425338745 + 10.0 * 8.40602970123291
Epoch 2440, val loss: 0.49728262424468994
Epoch 2450, training loss: 84.44950866699219 = 0.37227940559387207 + 10.0 * 8.407723426818848
Epoch 2450, val loss: 0.4973944425582886
Epoch 2460, training loss: 84.42144012451172 = 0.3717166781425476 + 10.0 * 8.404972076416016
Epoch 2460, val loss: 0.49745652079582214
Epoch 2470, training loss: 84.4224624633789 = 0.3711594343185425 + 10.0 * 8.405130386352539
Epoch 2470, val loss: 0.49760740995407104
Epoch 2480, training loss: 84.41461944580078 = 0.3705996572971344 + 10.0 * 8.404401779174805
Epoch 2480, val loss: 0.4975937008857727
Epoch 2490, training loss: 84.412353515625 = 0.3700406551361084 + 10.0 * 8.404231071472168
Epoch 2490, val loss: 0.49765193462371826
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7960426179604261
0.8102586394262118
The final CL Acc:0.79334, 0.00314, The final GNN Acc:0.81156, 0.00128
raw graph: torch.Size([2, 88648])
add edge: torch.Size([2, 110644])
remove edge: torch.Size([2, 66892])
updated graph: torch.Size([2, 88888])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 106.92871856689453 = 1.1064592599868774 + 10.0 * 10.582225799560547
Epoch 0, val loss: 1.1053014993667603
Epoch 10, training loss: 106.91771697998047 = 1.1013569831848145 + 10.0 * 10.581636428833008
Epoch 10, val loss: 1.10025954246521
Epoch 20, training loss: 106.88346862792969 = 1.0959913730621338 + 10.0 * 10.578747749328613
Epoch 20, val loss: 1.0949329137802124
Epoch 30, training loss: 106.74609375 = 1.0901625156402588 + 10.0 * 10.565592765808105
Epoch 30, val loss: 1.0891362428665161
Epoch 40, training loss: 106.24906921386719 = 1.0838268995285034 + 10.0 * 10.516524314880371
Epoch 40, val loss: 1.082811951637268
Epoch 50, training loss: 104.84695434570312 = 1.0766979455947876 + 10.0 * 10.377025604248047
Epoch 50, val loss: 1.0756422281265259
Epoch 60, training loss: 102.14730834960938 = 1.0684051513671875 + 10.0 * 10.107890129089355
Epoch 60, val loss: 1.0673044919967651
Epoch 70, training loss: 99.48456573486328 = 1.0600225925445557 + 10.0 * 9.84245491027832
Epoch 70, val loss: 1.059325933456421
Epoch 80, training loss: 97.97396087646484 = 1.0542374849319458 + 10.0 * 9.691972732543945
Epoch 80, val loss: 1.0540143251419067
Epoch 90, training loss: 96.41259002685547 = 1.0509722232818604 + 10.0 * 9.536161422729492
Epoch 90, val loss: 1.0511555671691895
Epoch 100, training loss: 94.61386108398438 = 1.0496994256973267 + 10.0 * 9.356416702270508
Epoch 100, val loss: 1.0500526428222656
Epoch 110, training loss: 93.14712524414062 = 1.0487316846847534 + 10.0 * 9.209839820861816
Epoch 110, val loss: 1.0490468740463257
Epoch 120, training loss: 92.21102142333984 = 1.047226071357727 + 10.0 * 9.116379737854004
Epoch 120, val loss: 1.0473644733428955
Epoch 130, training loss: 91.36096954345703 = 1.0456119775772095 + 10.0 * 9.031536102294922
Epoch 130, val loss: 1.0456550121307373
Epoch 140, training loss: 90.89305877685547 = 1.0441573858261108 + 10.0 * 8.98488998413086
Epoch 140, val loss: 1.044121503829956
Epoch 150, training loss: 90.56204986572266 = 1.0424861907958984 + 10.0 * 8.951955795288086
Epoch 150, val loss: 1.0424882173538208
Epoch 160, training loss: 90.08325958251953 = 1.0415489673614502 + 10.0 * 8.904170989990234
Epoch 160, val loss: 1.0417327880859375
Epoch 170, training loss: 89.4172134399414 = 1.0415772199630737 + 10.0 * 8.837563514709473
Epoch 170, val loss: 1.0418589115142822
Epoch 180, training loss: 88.79928588867188 = 1.0415676832199097 + 10.0 * 8.775772094726562
Epoch 180, val loss: 1.0418190956115723
Epoch 190, training loss: 88.38589477539062 = 1.0411484241485596 + 10.0 * 8.734475135803223
Epoch 190, val loss: 1.0414010286331177
Epoch 200, training loss: 88.05841064453125 = 1.0404338836669922 + 10.0 * 8.701797485351562
Epoch 200, val loss: 1.0407055616378784
Epoch 210, training loss: 87.76861572265625 = 1.0396779775619507 + 10.0 * 8.672893524169922
Epoch 210, val loss: 1.0399789810180664
Epoch 220, training loss: 87.47891235351562 = 1.0390996932983398 + 10.0 * 8.643980979919434
Epoch 220, val loss: 1.0394725799560547
Epoch 230, training loss: 87.21715545654297 = 1.038611888885498 + 10.0 * 8.617854118347168
Epoch 230, val loss: 1.0390335321426392
Epoch 240, training loss: 87.02075958251953 = 1.037984013557434 + 10.0 * 8.59827709197998
Epoch 240, val loss: 1.0384644269943237
Epoch 250, training loss: 86.84835052490234 = 1.037231206893921 + 10.0 * 8.581111907958984
Epoch 250, val loss: 1.037741780281067
Epoch 260, training loss: 86.69767761230469 = 1.0365047454833984 + 10.0 * 8.566117286682129
Epoch 260, val loss: 1.0370560884475708
Epoch 270, training loss: 86.5569076538086 = 1.03582763671875 + 10.0 * 8.552107810974121
Epoch 270, val loss: 1.0364316701889038
Epoch 280, training loss: 86.43304443359375 = 1.0350914001464844 + 10.0 * 8.539795875549316
Epoch 280, val loss: 1.0357400178909302
Epoch 290, training loss: 86.37393188476562 = 1.034213900566101 + 10.0 * 8.533971786499023
Epoch 290, val loss: 1.0349358320236206
Epoch 300, training loss: 86.25595092773438 = 1.0332330465316772 + 10.0 * 8.522272109985352
Epoch 300, val loss: 1.03396475315094
Epoch 310, training loss: 86.19163513183594 = 1.0322099924087524 + 10.0 * 8.515942573547363
Epoch 310, val loss: 1.0329785346984863
Epoch 320, training loss: 86.13142395019531 = 1.0311282873153687 + 10.0 * 8.510029792785645
Epoch 320, val loss: 1.031935453414917
Epoch 330, training loss: 86.07889556884766 = 1.0299842357635498 + 10.0 * 8.504891395568848
Epoch 330, val loss: 1.030832052230835
Epoch 340, training loss: 86.03070068359375 = 1.028779149055481 + 10.0 * 8.500192642211914
Epoch 340, val loss: 1.0296615362167358
Epoch 350, training loss: 86.01234436035156 = 1.027525782585144 + 10.0 * 8.498481750488281
Epoch 350, val loss: 1.0284007787704468
Epoch 360, training loss: 85.96672821044922 = 1.0261117219924927 + 10.0 * 8.494061470031738
Epoch 360, val loss: 1.0270715951919556
Epoch 370, training loss: 85.90975189208984 = 1.0247220993041992 + 10.0 * 8.488503456115723
Epoch 370, val loss: 1.0257577896118164
Epoch 380, training loss: 85.87203979492188 = 1.023297905921936 + 10.0 * 8.48487377166748
Epoch 380, val loss: 1.0243617296218872
Epoch 390, training loss: 85.8365478515625 = 1.0218009948730469 + 10.0 * 8.481473922729492
Epoch 390, val loss: 1.022913932800293
Epoch 400, training loss: 85.79998016357422 = 1.0202478170394897 + 10.0 * 8.477972984313965
Epoch 400, val loss: 1.0214248895645142
Epoch 410, training loss: 85.76415252685547 = 1.0186353921890259 + 10.0 * 8.4745512008667
Epoch 410, val loss: 1.0198743343353271
Epoch 420, training loss: 85.75988006591797 = 1.016951322555542 + 10.0 * 8.474292755126953
Epoch 420, val loss: 1.0182714462280273
Epoch 430, training loss: 85.72139739990234 = 1.0151240825653076 + 10.0 * 8.470627784729004
Epoch 430, val loss: 1.0164930820465088
Epoch 440, training loss: 85.67105865478516 = 1.0132944583892822 + 10.0 * 8.465776443481445
Epoch 440, val loss: 1.0147489309310913
Epoch 450, training loss: 85.62781524658203 = 1.0114290714263916 + 10.0 * 8.461638450622559
Epoch 450, val loss: 1.0129523277282715
Epoch 460, training loss: 85.59225463867188 = 1.0094661712646484 + 10.0 * 8.45827865600586
Epoch 460, val loss: 1.0110454559326172
Epoch 470, training loss: 85.55863952636719 = 1.0073868036270142 + 10.0 * 8.45512580871582
Epoch 470, val loss: 1.0090540647506714
Epoch 480, training loss: 85.52713775634766 = 1.0051785707473755 + 10.0 * 8.45219612121582
Epoch 480, val loss: 1.0069315433502197
Epoch 490, training loss: 85.54607391357422 = 1.0028151273727417 + 10.0 * 8.454325675964355
Epoch 490, val loss: 1.0047129392623901
Epoch 500, training loss: 85.4703140258789 = 1.0003260374069214 + 10.0 * 8.446998596191406
Epoch 500, val loss: 1.0022671222686768
Epoch 510, training loss: 85.44632720947266 = 0.997742772102356 + 10.0 * 8.44485855102539
Epoch 510, val loss: 0.9997598528862
Epoch 520, training loss: 85.42316436767578 = 0.9950045943260193 + 10.0 * 8.442815780639648
Epoch 520, val loss: 0.9971228837966919
Epoch 530, training loss: 85.3981704711914 = 0.9921203851699829 + 10.0 * 8.440605163574219
Epoch 530, val loss: 0.9943642616271973
Epoch 540, training loss: 85.42684936523438 = 0.9890121817588806 + 10.0 * 8.4437837600708
Epoch 540, val loss: 0.9913905262947083
Epoch 550, training loss: 85.36306762695312 = 0.985717236995697 + 10.0 * 8.437734603881836
Epoch 550, val loss: 0.9882142543792725
Epoch 560, training loss: 85.33411407470703 = 0.9823836088180542 + 10.0 * 8.435173034667969
Epoch 560, val loss: 0.9849944710731506
Epoch 570, training loss: 85.31417083740234 = 0.9788545966148376 + 10.0 * 8.433531761169434
Epoch 570, val loss: 0.9816028475761414
Epoch 580, training loss: 85.2921371459961 = 0.9751668572425842 + 10.0 * 8.431696891784668
Epoch 580, val loss: 0.9780630469322205
Epoch 590, training loss: 85.272705078125 = 0.97129225730896 + 10.0 * 8.43014144897461
Epoch 590, val loss: 0.9743314981460571
Epoch 600, training loss: 85.26304626464844 = 0.9671175479888916 + 10.0 * 8.429593086242676
Epoch 600, val loss: 0.9703390598297119
Epoch 610, training loss: 85.23556518554688 = 0.9628139138221741 + 10.0 * 8.427274703979492
Epoch 610, val loss: 0.966151237487793
Epoch 620, training loss: 85.21329498291016 = 0.9583241939544678 + 10.0 * 8.425497055053711
Epoch 620, val loss: 0.9618461728096008
Epoch 630, training loss: 85.19358825683594 = 0.9536129236221313 + 10.0 * 8.42399787902832
Epoch 630, val loss: 0.9572938680648804
Epoch 640, training loss: 85.17664337158203 = 0.9486786723136902 + 10.0 * 8.422796249389648
Epoch 640, val loss: 0.9525266289710999
Epoch 650, training loss: 85.2042465209961 = 0.9434764385223389 + 10.0 * 8.426076889038086
Epoch 650, val loss: 0.947425127029419
Epoch 660, training loss: 85.1423568725586 = 0.9379172325134277 + 10.0 * 8.420443534851074
Epoch 660, val loss: 0.9421887397766113
Epoch 670, training loss: 85.12747192382812 = 0.9322635531425476 + 10.0 * 8.419520378112793
Epoch 670, val loss: 0.9367664456367493
Epoch 680, training loss: 85.10624694824219 = 0.9263681769371033 + 10.0 * 8.417987823486328
Epoch 680, val loss: 0.9310935735702515
Epoch 690, training loss: 85.08918762207031 = 0.920215368270874 + 10.0 * 8.416897773742676
Epoch 690, val loss: 0.9251568913459778
Epoch 700, training loss: 85.07238006591797 = 0.9137676358222961 + 10.0 * 8.415861129760742
Epoch 700, val loss: 0.9189680218696594
Epoch 710, training loss: 85.09565734863281 = 0.9070265889167786 + 10.0 * 8.418863296508789
Epoch 710, val loss: 0.9124324917793274
Epoch 720, training loss: 85.0366439819336 = 0.8999825119972229 + 10.0 * 8.413665771484375
Epoch 720, val loss: 0.9057264924049377
Epoch 730, training loss: 85.02298736572266 = 0.8928225636482239 + 10.0 * 8.413016319274902
Epoch 730, val loss: 0.8988639116287231
Epoch 740, training loss: 85.0001449584961 = 0.8853901028633118 + 10.0 * 8.41147518157959
Epoch 740, val loss: 0.8917273879051208
Epoch 750, training loss: 84.98207092285156 = 0.8776932954788208 + 10.0 * 8.41043758392334
Epoch 750, val loss: 0.8843552470207214
Epoch 760, training loss: 85.02194213867188 = 0.8696832060813904 + 10.0 * 8.415225982666016
Epoch 760, val loss: 0.8767770528793335
Epoch 770, training loss: 84.9525375366211 = 0.861399233341217 + 10.0 * 8.409113883972168
Epoch 770, val loss: 0.8687094449996948
Epoch 780, training loss: 84.93143463134766 = 0.8530958890914917 + 10.0 * 8.40783405303955
Epoch 780, val loss: 0.8607688546180725
Epoch 790, training loss: 84.90970611572266 = 0.8446123003959656 + 10.0 * 8.406509399414062
Epoch 790, val loss: 0.8526391386985779
Epoch 800, training loss: 84.88787841796875 = 0.8358786106109619 + 10.0 * 8.405200004577637
Epoch 800, val loss: 0.8442844748497009
Epoch 810, training loss: 84.86711120605469 = 0.8269026279449463 + 10.0 * 8.404020309448242
Epoch 810, val loss: 0.8357154726982117
Epoch 820, training loss: 84.88758850097656 = 0.8176950812339783 + 10.0 * 8.406989097595215
Epoch 820, val loss: 0.8269028067588806
Epoch 830, training loss: 84.83423614501953 = 0.808204174041748 + 10.0 * 8.402603149414062
Epoch 830, val loss: 0.8178394436836243
Epoch 840, training loss: 84.81257629394531 = 0.7987098693847656 + 10.0 * 8.401386260986328
Epoch 840, val loss: 0.8087753653526306
Epoch 850, training loss: 84.7942123413086 = 0.7890856862068176 + 10.0 * 8.4005126953125
Epoch 850, val loss: 0.7995747327804565
Epoch 860, training loss: 84.79013061523438 = 0.7792537212371826 + 10.0 * 8.401087760925293
Epoch 860, val loss: 0.7901895642280579
Epoch 870, training loss: 84.75669860839844 = 0.7693562507629395 + 10.0 * 8.398734092712402
Epoch 870, val loss: 0.7807616591453552
Epoch 880, training loss: 84.7326431274414 = 0.7594173550605774 + 10.0 * 8.397322654724121
Epoch 880, val loss: 0.7712899446487427
Epoch 890, training loss: 84.71244049072266 = 0.7493602633476257 + 10.0 * 8.396307945251465
Epoch 890, val loss: 0.7616931796073914
Epoch 900, training loss: 84.69279479980469 = 0.7391735315322876 + 10.0 * 8.39536190032959
Epoch 900, val loss: 0.7520087361335754
Epoch 910, training loss: 84.67827606201172 = 0.7289062142372131 + 10.0 * 8.394937515258789
Epoch 910, val loss: 0.7422524690628052
Epoch 920, training loss: 84.67691040039062 = 0.7184724807739258 + 10.0 * 8.395843505859375
Epoch 920, val loss: 0.732335090637207
Epoch 930, training loss: 84.63992309570312 = 0.7081457376480103 + 10.0 * 8.39317798614502
Epoch 930, val loss: 0.7224834561347961
Epoch 940, training loss: 84.62003326416016 = 0.6978961825370789 + 10.0 * 8.392213821411133
Epoch 940, val loss: 0.71271151304245
Epoch 950, training loss: 84.59854888916016 = 0.6876282691955566 + 10.0 * 8.391092300415039
Epoch 950, val loss: 0.7029796838760376
Epoch 960, training loss: 84.58439636230469 = 0.6773476600646973 + 10.0 * 8.390705108642578
Epoch 960, val loss: 0.6931943893432617
Epoch 970, training loss: 84.56903839111328 = 0.6669795513153076 + 10.0 * 8.390206336975098
Epoch 970, val loss: 0.6833746433258057
Epoch 980, training loss: 84.55429077148438 = 0.6568174362182617 + 10.0 * 8.389747619628906
Epoch 980, val loss: 0.6737357378005981
Epoch 990, training loss: 84.53041076660156 = 0.6468245387077332 + 10.0 * 8.388358116149902
Epoch 990, val loss: 0.6642740368843079
Epoch 1000, training loss: 84.50973510742188 = 0.6368874311447144 + 10.0 * 8.387285232543945
Epoch 1000, val loss: 0.6548660397529602
Epoch 1010, training loss: 84.4930191040039 = 0.6270144581794739 + 10.0 * 8.386600494384766
Epoch 1010, val loss: 0.6455557346343994
Epoch 1020, training loss: 84.54557037353516 = 0.617165744304657 + 10.0 * 8.392840385437012
Epoch 1020, val loss: 0.6364068984985352
Epoch 1030, training loss: 84.46652221679688 = 0.6075416207313538 + 10.0 * 8.385897636413574
Epoch 1030, val loss: 0.6271421313285828
Epoch 1040, training loss: 84.44750213623047 = 0.5981850028038025 + 10.0 * 8.384931564331055
Epoch 1040, val loss: 0.6183967590332031
Epoch 1050, training loss: 84.42943572998047 = 0.5890244245529175 + 10.0 * 8.384040832519531
Epoch 1050, val loss: 0.6097897291183472
Epoch 1060, training loss: 84.4112548828125 = 0.5799845457077026 + 10.0 * 8.383127212524414
Epoch 1060, val loss: 0.6013028025627136
Epoch 1070, training loss: 84.39633178710938 = 0.5710722804069519 + 10.0 * 8.382525444030762
Epoch 1070, val loss: 0.5929616093635559
Epoch 1080, training loss: 84.43472290039062 = 0.5622891783714294 + 10.0 * 8.387243270874023
Epoch 1080, val loss: 0.584859311580658
Epoch 1090, training loss: 84.37918853759766 = 0.5536435842514038 + 10.0 * 8.38255500793457
Epoch 1090, val loss: 0.5766705870628357
Epoch 1100, training loss: 84.359619140625 = 0.5453793406486511 + 10.0 * 8.381423950195312
Epoch 1100, val loss: 0.5689827799797058
Epoch 1110, training loss: 84.34198760986328 = 0.5373709797859192 + 10.0 * 8.380461692810059
Epoch 1110, val loss: 0.5615382790565491
Epoch 1120, training loss: 84.32575988769531 = 0.5295686721801758 + 10.0 * 8.379618644714355
Epoch 1120, val loss: 0.5543251633644104
Epoch 1130, training loss: 84.31835174560547 = 0.5219537019729614 + 10.0 * 8.379639625549316
Epoch 1130, val loss: 0.5473215579986572
Epoch 1140, training loss: 84.3077621459961 = 0.5144748687744141 + 10.0 * 8.379328727722168
Epoch 1140, val loss: 0.5404706001281738
Epoch 1150, training loss: 84.30860900878906 = 0.5072940587997437 + 10.0 * 8.380131721496582
Epoch 1150, val loss: 0.533950686454773
Epoch 1160, training loss: 84.2763442993164 = 0.500446081161499 + 10.0 * 8.37759017944336
Epoch 1160, val loss: 0.5275908708572388
Epoch 1170, training loss: 84.2669906616211 = 0.4938318729400635 + 10.0 * 8.377315521240234
Epoch 1170, val loss: 0.5214970707893372
Epoch 1180, training loss: 84.27268981933594 = 0.48742371797561646 + 10.0 * 8.37852668762207
Epoch 1180, val loss: 0.515583336353302
Epoch 1190, training loss: 84.2403793334961 = 0.48116618394851685 + 10.0 * 8.375921249389648
Epoch 1190, val loss: 0.5100927352905273
Epoch 1200, training loss: 84.23136138916016 = 0.47518572211265564 + 10.0 * 8.375617027282715
Epoch 1200, val loss: 0.5047079920768738
Epoch 1210, training loss: 84.2213134765625 = 0.4694433808326721 + 10.0 * 8.375186920166016
Epoch 1210, val loss: 0.49945035576820374
Epoch 1220, training loss: 84.2126235961914 = 0.4638778567314148 + 10.0 * 8.37487506866455
Epoch 1220, val loss: 0.49446210265159607
Epoch 1230, training loss: 84.21550750732422 = 0.45850715041160583 + 10.0 * 8.375699996948242
Epoch 1230, val loss: 0.4896310567855835
Epoch 1240, training loss: 84.18831634521484 = 0.45330560207366943 + 10.0 * 8.37350082397461
Epoch 1240, val loss: 0.48516011238098145
Epoch 1250, training loss: 84.20445251464844 = 0.44833171367645264 + 10.0 * 8.375612258911133
Epoch 1250, val loss: 0.4808332026004791
Epoch 1260, training loss: 84.17652130126953 = 0.44360288977622986 + 10.0 * 8.373291969299316
Epoch 1260, val loss: 0.476497620344162
Epoch 1270, training loss: 84.1596450805664 = 0.43903204798698425 + 10.0 * 8.372060775756836
Epoch 1270, val loss: 0.47264599800109863
Epoch 1280, training loss: 84.15103149414062 = 0.43466222286224365 + 10.0 * 8.371637344360352
Epoch 1280, val loss: 0.4687955975532532
Epoch 1290, training loss: 84.16014099121094 = 0.43045154213905334 + 10.0 * 8.372968673706055
Epoch 1290, val loss: 0.46518653631210327
Epoch 1300, training loss: 84.13079071044922 = 0.4263859987258911 + 10.0 * 8.370440483093262
Epoch 1300, val loss: 0.4616762399673462
Epoch 1310, training loss: 84.1200942993164 = 0.4225105941295624 + 10.0 * 8.369758605957031
Epoch 1310, val loss: 0.4583738148212433
Epoch 1320, training loss: 84.13396453857422 = 0.4187719225883484 + 10.0 * 8.371519088745117
Epoch 1320, val loss: 0.4553223252296448
Epoch 1330, training loss: 84.1118392944336 = 0.415168821811676 + 10.0 * 8.369667053222656
Epoch 1330, val loss: 0.4521162509918213
Epoch 1340, training loss: 84.10321807861328 = 0.41174963116645813 + 10.0 * 8.369146347045898
Epoch 1340, val loss: 0.44935518503189087
Epoch 1350, training loss: 84.0848388671875 = 0.4084942042827606 + 10.0 * 8.367634773254395
Epoch 1350, val loss: 0.4465521574020386
Epoch 1360, training loss: 84.07637786865234 = 0.40534070134162903 + 10.0 * 8.367103576660156
Epoch 1360, val loss: 0.4439668357372284
Epoch 1370, training loss: 84.07958984375 = 0.4022985100746155 + 10.0 * 8.367729187011719
Epoch 1370, val loss: 0.44145122170448303
Epoch 1380, training loss: 84.0665054321289 = 0.39933106303215027 + 10.0 * 8.366717338562012
Epoch 1380, val loss: 0.4389928877353668
Epoch 1390, training loss: 84.05787658691406 = 0.3964945077896118 + 10.0 * 8.366138458251953
Epoch 1390, val loss: 0.4368244707584381
Epoch 1400, training loss: 84.04842376708984 = 0.39379146695137024 + 10.0 * 8.365463256835938
Epoch 1400, val loss: 0.4345815181732178
Epoch 1410, training loss: 84.0372085571289 = 0.391188383102417 + 10.0 * 8.364602088928223
Epoch 1410, val loss: 0.4325350821018219
Epoch 1420, training loss: 84.04000091552734 = 0.3886570930480957 + 10.0 * 8.365134239196777
Epoch 1420, val loss: 0.43059632182121277
Epoch 1430, training loss: 84.02421569824219 = 0.3861772119998932 + 10.0 * 8.36380386352539
Epoch 1430, val loss: 0.42861056327819824
Epoch 1440, training loss: 84.03150939941406 = 0.3838229477405548 + 10.0 * 8.364768981933594
Epoch 1440, val loss: 0.42685461044311523
Epoch 1450, training loss: 84.00907897949219 = 0.3815724551677704 + 10.0 * 8.362751007080078
Epoch 1450, val loss: 0.42508813738822937
Epoch 1460, training loss: 84.0044937133789 = 0.3793925940990448 + 10.0 * 8.362509727478027
Epoch 1460, val loss: 0.4233976900577545
Epoch 1470, training loss: 84.02916717529297 = 0.3772830665111542 + 10.0 * 8.365188598632812
Epoch 1470, val loss: 0.42166030406951904
Epoch 1480, training loss: 83.99716186523438 = 0.37519288063049316 + 10.0 * 8.362196922302246
Epoch 1480, val loss: 0.42047688364982605
Epoch 1490, training loss: 83.98844146728516 = 0.37321946024894714 + 10.0 * 8.36152172088623
Epoch 1490, val loss: 0.4188328683376312
Epoch 1500, training loss: 83.97919464111328 = 0.3712955415248871 + 10.0 * 8.360790252685547
Epoch 1500, val loss: 0.41753706336021423
Epoch 1510, training loss: 83.97118377685547 = 0.369430273771286 + 10.0 * 8.360175132751465
Epoch 1510, val loss: 0.4161517024040222
Epoch 1520, training loss: 83.96429443359375 = 0.36760473251342773 + 10.0 * 8.359668731689453
Epoch 1520, val loss: 0.4148862659931183
Epoch 1530, training loss: 83.96088409423828 = 0.36582693457603455 + 10.0 * 8.359505653381348
Epoch 1530, val loss: 0.41365259885787964
Epoch 1540, training loss: 84.0169677734375 = 0.3640965521335602 + 10.0 * 8.365286827087402
Epoch 1540, val loss: 0.41229403018951416
Epoch 1550, training loss: 83.96612548828125 = 0.3623943626880646 + 10.0 * 8.360372543334961
Epoch 1550, val loss: 0.4114321172237396
Epoch 1560, training loss: 83.9449234008789 = 0.36079132556915283 + 10.0 * 8.358412742614746
Epoch 1560, val loss: 0.4102264642715454
Epoch 1570, training loss: 83.93822479248047 = 0.35923731327056885 + 10.0 * 8.357898712158203
Epoch 1570, val loss: 0.4092109501361847
Epoch 1580, training loss: 83.93242645263672 = 0.35771897435188293 + 10.0 * 8.357470512390137
Epoch 1580, val loss: 0.4081967771053314
Epoch 1590, training loss: 83.9281234741211 = 0.35623103380203247 + 10.0 * 8.357189178466797
Epoch 1590, val loss: 0.4072762429714203
Epoch 1600, training loss: 83.960205078125 = 0.35477307438850403 + 10.0 * 8.360543251037598
Epoch 1600, val loss: 0.40633612871170044
Epoch 1610, training loss: 83.94239807128906 = 0.35333946347236633 + 10.0 * 8.358905792236328
Epoch 1610, val loss: 0.4053417146205902
Epoch 1620, training loss: 83.9198226928711 = 0.35195058584213257 + 10.0 * 8.356786727905273
Epoch 1620, val loss: 0.40456631779670715
Epoch 1630, training loss: 83.91258239746094 = 0.3505975604057312 + 10.0 * 8.35619831085205
Epoch 1630, val loss: 0.4038175344467163
Epoch 1640, training loss: 83.90202331542969 = 0.34928497672080994 + 10.0 * 8.355274200439453
Epoch 1640, val loss: 0.40296921133995056
Epoch 1650, training loss: 83.89845275878906 = 0.34799790382385254 + 10.0 * 8.355045318603516
Epoch 1650, val loss: 0.4022088348865509
Epoch 1660, training loss: 83.90721130371094 = 0.3467317819595337 + 10.0 * 8.356047630310059
Epoch 1660, val loss: 0.4014684855937958
Epoch 1670, training loss: 83.89644622802734 = 0.34547948837280273 + 10.0 * 8.355096817016602
Epoch 1670, val loss: 0.4008535146713257
Epoch 1680, training loss: 83.91690826416016 = 0.3442584276199341 + 10.0 * 8.357264518737793
Epoch 1680, val loss: 0.4002073109149933
Epoch 1690, training loss: 83.88563537597656 = 0.34307730197906494 + 10.0 * 8.354255676269531
Epoch 1690, val loss: 0.3993745744228363
Epoch 1700, training loss: 83.87625122070312 = 0.3419298231601715 + 10.0 * 8.353432655334473
Epoch 1700, val loss: 0.3987885117530823
Epoch 1710, training loss: 83.86956024169922 = 0.34080296754837036 + 10.0 * 8.352875709533691
Epoch 1710, val loss: 0.39817166328430176
Epoch 1720, training loss: 83.86475372314453 = 0.33969762921333313 + 10.0 * 8.352505683898926
Epoch 1720, val loss: 0.39757561683654785
Epoch 1730, training loss: 83.86012268066406 = 0.338605135679245 + 10.0 * 8.352151870727539
Epoch 1730, val loss: 0.3969954550266266
Epoch 1740, training loss: 83.8650894165039 = 0.3375280797481537 + 10.0 * 8.35275650024414
Epoch 1740, val loss: 0.3964010775089264
Epoch 1750, training loss: 83.87593841552734 = 0.33646461367607117 + 10.0 * 8.353947639465332
Epoch 1750, val loss: 0.39578571915626526
Epoch 1760, training loss: 83.84918212890625 = 0.3354181945323944 + 10.0 * 8.3513765335083
Epoch 1760, val loss: 0.39541518688201904
Epoch 1770, training loss: 83.84534454345703 = 0.33440491557121277 + 10.0 * 8.351094245910645
Epoch 1770, val loss: 0.39487922191619873
Epoch 1780, training loss: 83.84109497070312 = 0.3334095776081085 + 10.0 * 8.350768089294434
Epoch 1780, val loss: 0.39440351724624634
Epoch 1790, training loss: 83.84331512451172 = 0.33242568373680115 + 10.0 * 8.351088523864746
Epoch 1790, val loss: 0.393981009721756
Epoch 1800, training loss: 83.86127471923828 = 0.3314499855041504 + 10.0 * 8.352982521057129
Epoch 1800, val loss: 0.3935462534427643
Epoch 1810, training loss: 83.83476257324219 = 0.3304898142814636 + 10.0 * 8.350427627563477
Epoch 1810, val loss: 0.39298200607299805
Epoch 1820, training loss: 83.826904296875 = 0.32955124974250793 + 10.0 * 8.349735260009766
Epoch 1820, val loss: 0.3925149738788605
Epoch 1830, training loss: 83.82133483886719 = 0.32862749695777893 + 10.0 * 8.349270820617676
Epoch 1830, val loss: 0.39215800166130066
Epoch 1840, training loss: 83.82142639160156 = 0.3277140259742737 + 10.0 * 8.349370956420898
Epoch 1840, val loss: 0.3917217552661896
Epoch 1850, training loss: 83.83463287353516 = 0.3268086612224579 + 10.0 * 8.35078239440918
Epoch 1850, val loss: 0.39134806394577026
Epoch 1860, training loss: 83.815673828125 = 0.32592126727104187 + 10.0 * 8.34897518157959
Epoch 1860, val loss: 0.3910246193408966
Epoch 1870, training loss: 83.8284912109375 = 0.3250396251678467 + 10.0 * 8.35034465789795
Epoch 1870, val loss: 0.39070042967796326
Epoch 1880, training loss: 83.80438232421875 = 0.32417032122612 + 10.0 * 8.348020553588867
Epoch 1880, val loss: 0.39022377133369446
Epoch 1890, training loss: 83.80315399169922 = 0.323316752910614 + 10.0 * 8.347983360290527
Epoch 1890, val loss: 0.38980159163475037
Epoch 1900, training loss: 83.7965087890625 = 0.32247382402420044 + 10.0 * 8.347403526306152
Epoch 1900, val loss: 0.38955360651016235
Epoch 1910, training loss: 83.79310607910156 = 0.32163822650909424 + 10.0 * 8.347146987915039
Epoch 1910, val loss: 0.3892073929309845
Epoch 1920, training loss: 83.7975845336914 = 0.320809006690979 + 10.0 * 8.347677230834961
Epoch 1920, val loss: 0.3889237940311432
Epoch 1930, training loss: 83.8052749633789 = 0.3199838399887085 + 10.0 * 8.348528861999512
Epoch 1930, val loss: 0.38863489031791687
Epoch 1940, training loss: 83.79776000976562 = 0.31916487216949463 + 10.0 * 8.347859382629395
Epoch 1940, val loss: 0.38827306032180786
Epoch 1950, training loss: 83.78955078125 = 0.31836065649986267 + 10.0 * 8.347119331359863
Epoch 1950, val loss: 0.3880362808704376
Epoch 1960, training loss: 83.7777328491211 = 0.3175624907016754 + 10.0 * 8.346016883850098
Epoch 1960, val loss: 0.3876227140426636
Epoch 1970, training loss: 83.7820053100586 = 0.31677380204200745 + 10.0 * 8.34652328491211
Epoch 1970, val loss: 0.38731831312179565
Epoch 1980, training loss: 83.79476928710938 = 0.31599166989326477 + 10.0 * 8.347877502441406
Epoch 1980, val loss: 0.38710206747055054
Epoch 1990, training loss: 83.77450561523438 = 0.3152252435684204 + 10.0 * 8.345928192138672
Epoch 1990, val loss: 0.38686636090278625
Epoch 2000, training loss: 83.7669448852539 = 0.31446367502212524 + 10.0 * 8.345248222351074
Epoch 2000, val loss: 0.38655421137809753
Epoch 2010, training loss: 83.76342010498047 = 0.31371012330055237 + 10.0 * 8.344970703125
Epoch 2010, val loss: 0.38630029559135437
Epoch 2020, training loss: 83.76106262207031 = 0.3129614591598511 + 10.0 * 8.344810485839844
Epoch 2020, val loss: 0.3860168755054474
Epoch 2030, training loss: 83.80525970458984 = 0.31221750378608704 + 10.0 * 8.34930419921875
Epoch 2030, val loss: 0.3856111466884613
Epoch 2040, training loss: 83.77145385742188 = 0.3114786148071289 + 10.0 * 8.34599781036377
Epoch 2040, val loss: 0.3857593834400177
Epoch 2050, training loss: 83.76254272460938 = 0.310747355222702 + 10.0 * 8.345179557800293
Epoch 2050, val loss: 0.385320246219635
Epoch 2060, training loss: 83.75093841552734 = 0.31003326177597046 + 10.0 * 8.344090461730957
Epoch 2060, val loss: 0.38520365953445435
Epoch 2070, training loss: 83.74645233154297 = 0.309320330619812 + 10.0 * 8.343713760375977
Epoch 2070, val loss: 0.3849966526031494
Epoch 2080, training loss: 83.74530029296875 = 0.3086116313934326 + 10.0 * 8.343668937683105
Epoch 2080, val loss: 0.3848111033439636
Epoch 2090, training loss: 83.79564666748047 = 0.3079072833061218 + 10.0 * 8.348773956298828
Epoch 2090, val loss: 0.3847425878047943
Epoch 2100, training loss: 83.76100158691406 = 0.3072047531604767 + 10.0 * 8.345379829406738
Epoch 2100, val loss: 0.3843330144882202
Epoch 2110, training loss: 83.7427749633789 = 0.30651089549064636 + 10.0 * 8.343626022338867
Epoch 2110, val loss: 0.38428330421447754
Epoch 2120, training loss: 83.7341537475586 = 0.3058251738548279 + 10.0 * 8.342832565307617
Epoch 2120, val loss: 0.3840542435646057
Epoch 2130, training loss: 83.73151397705078 = 0.30514413118362427 + 10.0 * 8.342637062072754
Epoch 2130, val loss: 0.383902370929718
Epoch 2140, training loss: 83.7343521118164 = 0.30446338653564453 + 10.0 * 8.342988967895508
Epoch 2140, val loss: 0.38374730944633484
Epoch 2150, training loss: 83.76370239257812 = 0.3037857711315155 + 10.0 * 8.345991134643555
Epoch 2150, val loss: 0.3835866153240204
Epoch 2160, training loss: 83.7284927368164 = 0.3031194806098938 + 10.0 * 8.342537879943848
Epoch 2160, val loss: 0.3834591805934906
Epoch 2170, training loss: 83.72309875488281 = 0.3024580776691437 + 10.0 * 8.342063903808594
Epoch 2170, val loss: 0.3832951486110687
Epoch 2180, training loss: 83.72091674804688 = 0.30179980397224426 + 10.0 * 8.341911315917969
Epoch 2180, val loss: 0.38316309452056885
Epoch 2190, training loss: 83.71778869628906 = 0.30114585161209106 + 10.0 * 8.34166431427002
Epoch 2190, val loss: 0.38303321599960327
Epoch 2200, training loss: 83.7151870727539 = 0.30049172043800354 + 10.0 * 8.341469764709473
Epoch 2200, val loss: 0.3829118311405182
Epoch 2210, training loss: 83.72311401367188 = 0.29984235763549805 + 10.0 * 8.342327117919922
Epoch 2210, val loss: 0.3828667104244232
Epoch 2220, training loss: 83.74009704589844 = 0.2991928458213806 + 10.0 * 8.344090461730957
Epoch 2220, val loss: 0.3827360272407532
Epoch 2230, training loss: 83.72557067871094 = 0.2985507547855377 + 10.0 * 8.34270191192627
Epoch 2230, val loss: 0.38255441188812256
Epoch 2240, training loss: 83.70711517333984 = 0.29792022705078125 + 10.0 * 8.340919494628906
Epoch 2240, val loss: 0.38241955637931824
Epoch 2250, training loss: 83.70470428466797 = 0.297294557094574 + 10.0 * 8.340741157531738
Epoch 2250, val loss: 0.382343590259552
Epoch 2260, training loss: 83.70208740234375 = 0.2966691255569458 + 10.0 * 8.34054183959961
Epoch 2260, val loss: 0.3822052478790283
Epoch 2270, training loss: 83.69930267333984 = 0.29604488611221313 + 10.0 * 8.340326309204102
Epoch 2270, val loss: 0.38211381435394287
Epoch 2280, training loss: 83.69725036621094 = 0.29542163014411926 + 10.0 * 8.340182304382324
Epoch 2280, val loss: 0.38201767206192017
Epoch 2290, training loss: 83.7007827758789 = 0.2947990596294403 + 10.0 * 8.340598106384277
Epoch 2290, val loss: 0.3818713128566742
Epoch 2300, training loss: 83.74100494384766 = 0.29418042302131653 + 10.0 * 8.344682693481445
Epoch 2300, val loss: 0.3817092180252075
Epoch 2310, training loss: 83.70097351074219 = 0.2935680150985718 + 10.0 * 8.340740203857422
Epoch 2310, val loss: 0.3817915916442871
Epoch 2320, training loss: 83.69023895263672 = 0.29296019673347473 + 10.0 * 8.339727401733398
Epoch 2320, val loss: 0.3816678524017334
Epoch 2330, training loss: 83.68885803222656 = 0.29236042499542236 + 10.0 * 8.33965015411377
Epoch 2330, val loss: 0.381610244512558
Epoch 2340, training loss: 83.68932342529297 = 0.29176169633865356 + 10.0 * 8.33975601196289
Epoch 2340, val loss: 0.38156548142433167
Epoch 2350, training loss: 83.71961975097656 = 0.2911667227745056 + 10.0 * 8.34284496307373
Epoch 2350, val loss: 0.3815876841545105
Epoch 2360, training loss: 83.68679809570312 = 0.29057222604751587 + 10.0 * 8.339622497558594
Epoch 2360, val loss: 0.3813899755477905
Epoch 2370, training loss: 83.67876434326172 = 0.28998252749443054 + 10.0 * 8.33887767791748
Epoch 2370, val loss: 0.38134515285491943
Epoch 2380, training loss: 83.67654418945312 = 0.2893948256969452 + 10.0 * 8.338714599609375
Epoch 2380, val loss: 0.3812720775604248
Epoch 2390, training loss: 83.6778335571289 = 0.2888081967830658 + 10.0 * 8.338902473449707
Epoch 2390, val loss: 0.38115519285202026
Epoch 2400, training loss: 83.70362854003906 = 0.2882228493690491 + 10.0 * 8.341540336608887
Epoch 2400, val loss: 0.3810052275657654
Epoch 2410, training loss: 83.67828369140625 = 0.28764644265174866 + 10.0 * 8.33906364440918
Epoch 2410, val loss: 0.38123515248298645
Epoch 2420, training loss: 83.67161560058594 = 0.28706666827201843 + 10.0 * 8.338455200195312
Epoch 2420, val loss: 0.38105309009552
Epoch 2430, training loss: 83.66722106933594 = 0.28649377822875977 + 10.0 * 8.338072776794434
Epoch 2430, val loss: 0.3810877799987793
Epoch 2440, training loss: 83.66802215576172 = 0.28591811656951904 + 10.0 * 8.338210105895996
Epoch 2440, val loss: 0.380980908870697
Epoch 2450, training loss: 83.70454406738281 = 0.2853451371192932 + 10.0 * 8.341919898986816
Epoch 2450, val loss: 0.3809192478656769
Epoch 2460, training loss: 83.67496490478516 = 0.2847824990749359 + 10.0 * 8.339017868041992
Epoch 2460, val loss: 0.38100582361221313
Epoch 2470, training loss: 83.66358947753906 = 0.2842165529727936 + 10.0 * 8.337937355041504
Epoch 2470, val loss: 0.38091540336608887
Epoch 2480, training loss: 83.66338348388672 = 0.2836569547653198 + 10.0 * 8.337972640991211
Epoch 2480, val loss: 0.38088029623031616
Epoch 2490, training loss: 83.68426513671875 = 0.2830982804298401 + 10.0 * 8.340116500854492
Epoch 2490, val loss: 0.3807607591152191
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8498224251648909
0.8626385568354706
=== training gcn model ===
Epoch 0, training loss: 106.91276550292969 = 1.0902308225631714 + 10.0 * 10.582253456115723
Epoch 0, val loss: 1.0896791219711304
Epoch 10, training loss: 106.90492248535156 = 1.0854113101959229 + 10.0 * 10.581951141357422
Epoch 10, val loss: 1.0849465131759644
Epoch 20, training loss: 106.88802337646484 = 1.0803639888763428 + 10.0 * 10.580766677856445
Epoch 20, val loss: 1.0799765586853027
Epoch 30, training loss: 106.8346939086914 = 1.0749139785766602 + 10.0 * 10.57597827911377
Epoch 30, val loss: 1.0746055841445923
Epoch 40, training loss: 106.63894653320312 = 1.0690399408340454 + 10.0 * 10.556990623474121
Epoch 40, val loss: 1.0688556432724
Epoch 50, training loss: 105.98555755615234 = 1.0628849267959595 + 10.0 * 10.492267608642578
Epoch 50, val loss: 1.0628490447998047
Epoch 60, training loss: 104.1370620727539 = 1.056740641593933 + 10.0 * 10.308032035827637
Epoch 60, val loss: 1.0568755865097046
Epoch 70, training loss: 100.42414855957031 = 1.0506700277328491 + 10.0 * 9.937348365783691
Epoch 70, val loss: 1.0508910417556763
Epoch 80, training loss: 98.00045013427734 = 1.0437686443328857 + 10.0 * 9.69566822052002
Epoch 80, val loss: 1.0439956188201904
Epoch 90, training loss: 97.50137329101562 = 1.037687063217163 + 10.0 * 9.646368026733398
Epoch 90, val loss: 1.0379835367202759
Epoch 100, training loss: 96.9744873046875 = 1.0332460403442383 + 10.0 * 9.594123840332031
Epoch 100, val loss: 1.033599853515625
Epoch 110, training loss: 96.1687240600586 = 1.0297491550445557 + 10.0 * 9.513897895812988
Epoch 110, val loss: 1.0301289558410645
Epoch 120, training loss: 94.74356079101562 = 1.0257741212844849 + 10.0 * 9.37177848815918
Epoch 120, val loss: 1.0262917280197144
Epoch 130, training loss: 93.21087646484375 = 1.021834373474121 + 10.0 * 9.218904495239258
Epoch 130, val loss: 1.0227091312408447
Epoch 140, training loss: 92.34651947021484 = 1.0191553831100464 + 10.0 * 9.132736206054688
Epoch 140, val loss: 1.0204044580459595
Epoch 150, training loss: 91.35289001464844 = 1.0181069374084473 + 10.0 * 9.033478736877441
Epoch 150, val loss: 1.019629955291748
Epoch 160, training loss: 90.48318481445312 = 1.0174320936203003 + 10.0 * 8.946575164794922
Epoch 160, val loss: 1.01897132396698
Epoch 170, training loss: 89.97843170166016 = 1.016034722328186 + 10.0 * 8.896239280700684
Epoch 170, val loss: 1.0174102783203125
Epoch 180, training loss: 89.50057220458984 = 1.0138192176818848 + 10.0 * 8.848675727844238
Epoch 180, val loss: 1.0150716304779053
Epoch 190, training loss: 89.23200988769531 = 1.011948823928833 + 10.0 * 8.822006225585938
Epoch 190, val loss: 1.0131741762161255
Epoch 200, training loss: 89.03453063964844 = 1.0102028846740723 + 10.0 * 8.802433013916016
Epoch 200, val loss: 1.0114635229110718
Epoch 210, training loss: 88.7857666015625 = 1.0085922479629517 + 10.0 * 8.777717590332031
Epoch 210, val loss: 1.0099822282791138
Epoch 220, training loss: 88.46167755126953 = 1.0077780485153198 + 10.0 * 8.745389938354492
Epoch 220, val loss: 1.0093098878860474
Epoch 230, training loss: 88.12317657470703 = 1.0076839923858643 + 10.0 * 8.711549758911133
Epoch 230, val loss: 1.0092719793319702
Epoch 240, training loss: 87.86402893066406 = 1.0073014497756958 + 10.0 * 8.685672760009766
Epoch 240, val loss: 1.0088624954223633
Epoch 250, training loss: 87.64894104003906 = 1.0058767795562744 + 10.0 * 8.664306640625
Epoch 250, val loss: 1.0074459314346313
Epoch 260, training loss: 87.47911071777344 = 1.0041520595550537 + 10.0 * 8.647496223449707
Epoch 260, val loss: 1.0058000087738037
Epoch 270, training loss: 87.32525634765625 = 1.0026291608810425 + 10.0 * 8.632262229919434
Epoch 270, val loss: 1.0043431520462036
Epoch 280, training loss: 87.16130065917969 = 1.001417875289917 + 10.0 * 8.615987777709961
Epoch 280, val loss: 1.003162145614624
Epoch 290, training loss: 86.99240112304688 = 1.00027596950531 + 10.0 * 8.599212646484375
Epoch 290, val loss: 1.0021098852157593
Epoch 300, training loss: 86.85359191894531 = 0.9989884495735168 + 10.0 * 8.585460662841797
Epoch 300, val loss: 1.0008147954940796
Epoch 310, training loss: 86.75447082519531 = 0.9971286654472351 + 10.0 * 8.57573413848877
Epoch 310, val loss: 0.9989487528800964
Epoch 320, training loss: 86.6847915649414 = 0.9945446252822876 + 10.0 * 8.569025039672852
Epoch 320, val loss: 0.9964077472686768
Epoch 330, training loss: 86.63465881347656 = 0.9915400743484497 + 10.0 * 8.564311981201172
Epoch 330, val loss: 0.993518590927124
Epoch 340, training loss: 86.56055450439453 = 0.9885014295578003 + 10.0 * 8.557205200195312
Epoch 340, val loss: 0.9905598163604736
Epoch 350, training loss: 86.48880767822266 = 0.985529899597168 + 10.0 * 8.550328254699707
Epoch 350, val loss: 0.9877364039421082
Epoch 360, training loss: 86.41685485839844 = 0.9826194047927856 + 10.0 * 8.543423652648926
Epoch 360, val loss: 0.9849808216094971
Epoch 370, training loss: 86.33708953857422 = 0.9797329306602478 + 10.0 * 8.535735130310059
Epoch 370, val loss: 0.9821726679801941
Epoch 380, training loss: 86.24764251708984 = 0.976863443851471 + 10.0 * 8.527078628540039
Epoch 380, val loss: 0.9794325828552246
Epoch 390, training loss: 86.16410064697266 = 0.973961353302002 + 10.0 * 8.519014358520508
Epoch 390, val loss: 0.9766550064086914
Epoch 400, training loss: 86.0908203125 = 0.9708872437477112 + 10.0 * 8.511993408203125
Epoch 400, val loss: 0.9737164378166199
Epoch 410, training loss: 86.01513671875 = 0.9676136374473572 + 10.0 * 8.504752159118652
Epoch 410, val loss: 0.9705322980880737
Epoch 420, training loss: 85.94416809082031 = 0.9642342329025269 + 10.0 * 8.497993469238281
Epoch 420, val loss: 0.9672533869743347
Epoch 430, training loss: 85.8763656616211 = 0.9606293439865112 + 10.0 * 8.491573333740234
Epoch 430, val loss: 0.9638264179229736
Epoch 440, training loss: 85.80927276611328 = 0.9568615555763245 + 10.0 * 8.485240936279297
Epoch 440, val loss: 0.9601594805717468
Epoch 450, training loss: 85.75169372558594 = 0.9528219699859619 + 10.0 * 8.479887008666992
Epoch 450, val loss: 0.9562416672706604
Epoch 460, training loss: 85.71424865722656 = 0.9484249949455261 + 10.0 * 8.476582527160645
Epoch 460, val loss: 0.9519456028938293
Epoch 470, training loss: 85.65505981445312 = 0.9436015486717224 + 10.0 * 8.471145629882812
Epoch 470, val loss: 0.9473209977149963
Epoch 480, training loss: 85.6138687133789 = 0.9384872913360596 + 10.0 * 8.467538833618164
Epoch 480, val loss: 0.9424185156822205
Epoch 490, training loss: 85.59495544433594 = 0.9330763816833496 + 10.0 * 8.466188430786133
Epoch 490, val loss: 0.9371650218963623
Epoch 500, training loss: 85.53779602050781 = 0.9273256659507751 + 10.0 * 8.461047172546387
Epoch 500, val loss: 0.9316430687904358
Epoch 510, training loss: 85.49996185302734 = 0.9213914275169373 + 10.0 * 8.457857131958008
Epoch 510, val loss: 0.9259850382804871
Epoch 520, training loss: 85.46666717529297 = 0.9150639176368713 + 10.0 * 8.455160140991211
Epoch 520, val loss: 0.9199041128158569
Epoch 530, training loss: 85.4573974609375 = 0.9083881974220276 + 10.0 * 8.454900741577148
Epoch 530, val loss: 0.9134954214096069
Epoch 540, training loss: 85.41146850585938 = 0.9013523459434509 + 10.0 * 8.451011657714844
Epoch 540, val loss: 0.9067931175231934
Epoch 550, training loss: 85.37661743164062 = 0.8941250443458557 + 10.0 * 8.448248863220215
Epoch 550, val loss: 0.8998688459396362
Epoch 560, training loss: 85.3470458984375 = 0.8864170908927917 + 10.0 * 8.446063041687012
Epoch 560, val loss: 0.8925122022628784
Epoch 570, training loss: 85.3193359375 = 0.8784019351005554 + 10.0 * 8.444093704223633
Epoch 570, val loss: 0.884814441204071
Epoch 580, training loss: 85.31966400146484 = 0.8700323700904846 + 10.0 * 8.444963455200195
Epoch 580, val loss: 0.8767062425613403
Epoch 590, training loss: 85.27479553222656 = 0.8611541986465454 + 10.0 * 8.441364288330078
Epoch 590, val loss: 0.8682467937469482
Epoch 600, training loss: 85.24240112304688 = 0.8520383834838867 + 10.0 * 8.43903636932373
Epoch 600, val loss: 0.8595252633094788
Epoch 610, training loss: 85.21339416503906 = 0.8426262140274048 + 10.0 * 8.437076568603516
Epoch 610, val loss: 0.8505384922027588
Epoch 620, training loss: 85.20873260498047 = 0.8328924179077148 + 10.0 * 8.437583923339844
Epoch 620, val loss: 0.8411049842834473
Epoch 630, training loss: 85.1577377319336 = 0.8224464058876038 + 10.0 * 8.433528900146484
Epoch 630, val loss: 0.8313171863555908
Epoch 640, training loss: 85.12510681152344 = 0.8121523857116699 + 10.0 * 8.431295394897461
Epoch 640, val loss: 0.821471631526947
Epoch 650, training loss: 85.09232330322266 = 0.801639974117279 + 10.0 * 8.429067611694336
Epoch 650, val loss: 0.8114045858383179
Epoch 660, training loss: 85.06038665771484 = 0.790672242641449 + 10.0 * 8.426971435546875
Epoch 660, val loss: 0.8009981513023376
Epoch 670, training loss: 85.05020904541016 = 0.7792831659317017 + 10.0 * 8.427092552185059
Epoch 670, val loss: 0.7902088761329651
Epoch 680, training loss: 85.00047302246094 = 0.7678462862968445 + 10.0 * 8.423262596130371
Epoch 680, val loss: 0.7790488600730896
Epoch 690, training loss: 84.96387481689453 = 0.7561029195785522 + 10.0 * 8.420777320861816
Epoch 690, val loss: 0.7678385972976685
Epoch 700, training loss: 84.93695831298828 = 0.7441754341125488 + 10.0 * 8.419278144836426
Epoch 700, val loss: 0.7564230561256409
Epoch 710, training loss: 84.90516662597656 = 0.7320422530174255 + 10.0 * 8.417312622070312
Epoch 710, val loss: 0.7448937296867371
Epoch 720, training loss: 84.87541961669922 = 0.719870924949646 + 10.0 * 8.415555000305176
Epoch 720, val loss: 0.7331979870796204
Epoch 730, training loss: 84.84579467773438 = 0.7076189517974854 + 10.0 * 8.413817405700684
Epoch 730, val loss: 0.7214755415916443
Epoch 740, training loss: 84.81668853759766 = 0.6953293681144714 + 10.0 * 8.41213607788086
Epoch 740, val loss: 0.7097241282463074
Epoch 750, training loss: 84.82954406738281 = 0.6830253601074219 + 10.0 * 8.414651870727539
Epoch 750, val loss: 0.6978423595428467
Epoch 760, training loss: 84.76568603515625 = 0.6706560850143433 + 10.0 * 8.409502983093262
Epoch 760, val loss: 0.6861172914505005
Epoch 770, training loss: 84.73401641845703 = 0.6585924029350281 + 10.0 * 8.40754222869873
Epoch 770, val loss: 0.6746433973312378
Epoch 780, training loss: 84.70516967773438 = 0.6466755867004395 + 10.0 * 8.40584945678711
Epoch 780, val loss: 0.6633127331733704
Epoch 790, training loss: 84.67870330810547 = 0.6348292231559753 + 10.0 * 8.404387474060059
Epoch 790, val loss: 0.6520508527755737
Epoch 800, training loss: 84.67273712158203 = 0.6230557560920715 + 10.0 * 8.40496826171875
Epoch 800, val loss: 0.6408957839012146
Epoch 810, training loss: 84.63837432861328 = 0.6115756034851074 + 10.0 * 8.402680397033691
Epoch 810, val loss: 0.6298130750656128
Epoch 820, training loss: 84.59952545166016 = 0.6003791689872742 + 10.0 * 8.399914741516113
Epoch 820, val loss: 0.6192558407783508
Epoch 830, training loss: 84.5759048461914 = 0.5895599722862244 + 10.0 * 8.39863395690918
Epoch 830, val loss: 0.6089518666267395
Epoch 840, training loss: 84.58275604248047 = 0.5789957046508789 + 10.0 * 8.400376319885254
Epoch 840, val loss: 0.5988785028457642
Epoch 850, training loss: 84.5322036743164 = 0.5687209963798523 + 10.0 * 8.39634895324707
Epoch 850, val loss: 0.5891786217689514
Epoch 860, training loss: 84.50586700439453 = 0.55884850025177 + 10.0 * 8.394701957702637
Epoch 860, val loss: 0.5798783898353577
Epoch 870, training loss: 84.48421478271484 = 0.5493455529212952 + 10.0 * 8.393486976623535
Epoch 870, val loss: 0.570854663848877
Epoch 880, training loss: 84.46316528320312 = 0.5400978922843933 + 10.0 * 8.392306327819824
Epoch 880, val loss: 0.5621421337127686
Epoch 890, training loss: 84.45071411132812 = 0.5311013460159302 + 10.0 * 8.391961097717285
Epoch 890, val loss: 0.5537007451057434
Epoch 900, training loss: 84.42808532714844 = 0.5225132703781128 + 10.0 * 8.390557289123535
Epoch 900, val loss: 0.5455929636955261
Epoch 910, training loss: 84.40557861328125 = 0.514254093170166 + 10.0 * 8.389132499694824
Epoch 910, val loss: 0.5378488898277283
Epoch 920, training loss: 84.3864974975586 = 0.5062927603721619 + 10.0 * 8.388020515441895
Epoch 920, val loss: 0.5304076671600342
Epoch 930, training loss: 84.39203643798828 = 0.49863114953041077 + 10.0 * 8.3893404006958
Epoch 930, val loss: 0.5232477784156799
Epoch 940, training loss: 84.3626708984375 = 0.491219162940979 + 10.0 * 8.387145042419434
Epoch 940, val loss: 0.5164096355438232
Epoch 950, training loss: 84.33821868896484 = 0.48422563076019287 + 10.0 * 8.38539981842041
Epoch 950, val loss: 0.5099409818649292
Epoch 960, training loss: 84.32073974609375 = 0.4775095582008362 + 10.0 * 8.384323120117188
Epoch 960, val loss: 0.5037420392036438
Epoch 970, training loss: 84.306396484375 = 0.47106269001960754 + 10.0 * 8.383533477783203
Epoch 970, val loss: 0.49781370162963867
Epoch 980, training loss: 84.31246185302734 = 0.46487292647361755 + 10.0 * 8.384758949279785
Epoch 980, val loss: 0.49212661385536194
Epoch 990, training loss: 84.28756713867188 = 0.4588949978351593 + 10.0 * 8.382867813110352
Epoch 990, val loss: 0.48676103353500366
Epoch 1000, training loss: 84.2667007446289 = 0.4532628357410431 + 10.0 * 8.381343841552734
Epoch 1000, val loss: 0.48164689540863037
Epoch 1010, training loss: 84.26150512695312 = 0.44785889983177185 + 10.0 * 8.381364822387695
Epoch 1010, val loss: 0.4767933487892151
Epoch 1020, training loss: 84.2422103881836 = 0.4427133798599243 + 10.0 * 8.379949569702148
Epoch 1020, val loss: 0.47218236327171326
Epoch 1030, training loss: 84.23057556152344 = 0.4378074109554291 + 10.0 * 8.379277229309082
Epoch 1030, val loss: 0.46781110763549805
Epoch 1040, training loss: 84.24723815917969 = 0.4330940246582031 + 10.0 * 8.381414413452148
Epoch 1040, val loss: 0.4636293649673462
Epoch 1050, training loss: 84.20970916748047 = 0.4286189675331116 + 10.0 * 8.378108978271484
Epoch 1050, val loss: 0.4597325921058655
Epoch 1060, training loss: 84.19841766357422 = 0.424384742975235 + 10.0 * 8.377403259277344
Epoch 1060, val loss: 0.4560239613056183
Epoch 1070, training loss: 84.18620300292969 = 0.4203411638736725 + 10.0 * 8.376585960388184
Epoch 1070, val loss: 0.4525219202041626
Epoch 1080, training loss: 84.17559051513672 = 0.41645264625549316 + 10.0 * 8.375913619995117
Epoch 1080, val loss: 0.44917961955070496
Epoch 1090, training loss: 84.17073822021484 = 0.41271719336509705 + 10.0 * 8.375802040100098
Epoch 1090, val loss: 0.44599980115890503
Epoch 1100, training loss: 84.16183471679688 = 0.4091232419013977 + 10.0 * 8.37527084350586
Epoch 1100, val loss: 0.44293949007987976
Epoch 1110, training loss: 84.15654754638672 = 0.4056892693042755 + 10.0 * 8.375085830688477
Epoch 1110, val loss: 0.4400930106639862
Epoch 1120, training loss: 84.13831329345703 = 0.40244314074516296 + 10.0 * 8.373586654663086
Epoch 1120, val loss: 0.437386691570282
Epoch 1130, training loss: 84.13084411621094 = 0.3993246555328369 + 10.0 * 8.373151779174805
Epoch 1130, val loss: 0.43479257822036743
Epoch 1140, training loss: 84.13330078125 = 0.39631494879722595 + 10.0 * 8.373698234558105
Epoch 1140, val loss: 0.43230393528938293
Epoch 1150, training loss: 84.11314392089844 = 0.3933953046798706 + 10.0 * 8.37197494506836
Epoch 1150, val loss: 0.42997047305107117
Epoch 1160, training loss: 84.10442352294922 = 0.3906272351741791 + 10.0 * 8.371379852294922
Epoch 1160, val loss: 0.42771115899086
Epoch 1170, training loss: 84.14068603515625 = 0.3879407048225403 + 10.0 * 8.375274658203125
Epoch 1170, val loss: 0.425530344247818
Epoch 1180, training loss: 84.0916519165039 = 0.38535553216934204 + 10.0 * 8.370630264282227
Epoch 1180, val loss: 0.4235079884529114
Epoch 1190, training loss: 84.08033752441406 = 0.3828940689563751 + 10.0 * 8.369744300842285
Epoch 1190, val loss: 0.42160817980766296
Epoch 1200, training loss: 84.07080078125 = 0.38052064180374146 + 10.0 * 8.369028091430664
Epoch 1200, val loss: 0.4197491705417633
Epoch 1210, training loss: 84.06218719482422 = 0.37822631001472473 + 10.0 * 8.368395805358887
Epoch 1210, val loss: 0.41799405217170715
Epoch 1220, training loss: 84.07630157470703 = 0.3760005533695221 + 10.0 * 8.370030403137207
Epoch 1220, val loss: 0.4163140058517456
Epoch 1230, training loss: 84.056884765625 = 0.37383341789245605 + 10.0 * 8.368305206298828
Epoch 1230, val loss: 0.41464918851852417
Epoch 1240, training loss: 84.04142761230469 = 0.3717494606971741 + 10.0 * 8.366968154907227
Epoch 1240, val loss: 0.4130992293357849
Epoch 1250, training loss: 84.031982421875 = 0.3697557747364044 + 10.0 * 8.366222381591797
Epoch 1250, val loss: 0.4116400182247162
Epoch 1260, training loss: 84.02360534667969 = 0.3678111433982849 + 10.0 * 8.365579605102539
Epoch 1260, val loss: 0.4102197587490082
Epoch 1270, training loss: 84.02283477783203 = 0.36591705679893494 + 10.0 * 8.365692138671875
Epoch 1270, val loss: 0.40886878967285156
Epoch 1280, training loss: 84.01686096191406 = 0.36407706141471863 + 10.0 * 8.365278244018555
Epoch 1280, val loss: 0.40751293301582336
Epoch 1290, training loss: 84.00601959228516 = 0.3622966706752777 + 10.0 * 8.364372253417969
Epoch 1290, val loss: 0.4063120186328888
Epoch 1300, training loss: 83.99791717529297 = 0.3605864346027374 + 10.0 * 8.363733291625977
Epoch 1300, val loss: 0.40511777997016907
Epoch 1310, training loss: 83.98787689208984 = 0.35892564058303833 + 10.0 * 8.362895011901855
Epoch 1310, val loss: 0.4039769172668457
Epoch 1320, training loss: 83.98575592041016 = 0.3573019504547119 + 10.0 * 8.362845420837402
Epoch 1320, val loss: 0.40287432074546814
Epoch 1330, training loss: 83.99440002441406 = 0.35570037364959717 + 10.0 * 8.363870620727539
Epoch 1330, val loss: 0.4018028974533081
Epoch 1340, training loss: 83.973876953125 = 0.35415640473365784 + 10.0 * 8.361971855163574
Epoch 1340, val loss: 0.40076154470443726
Epoch 1350, training loss: 83.96185302734375 = 0.35265201330184937 + 10.0 * 8.360919952392578
Epoch 1350, val loss: 0.39981263875961304
Epoch 1360, training loss: 83.95362854003906 = 0.3511935770511627 + 10.0 * 8.36024284362793
Epoch 1360, val loss: 0.39885830879211426
Epoch 1370, training loss: 83.94739532470703 = 0.3497670590877533 + 10.0 * 8.359762191772461
Epoch 1370, val loss: 0.3979545533657074
Epoch 1380, training loss: 83.98211669921875 = 0.3483564853668213 + 10.0 * 8.363375663757324
Epoch 1380, val loss: 0.3970898687839508
Epoch 1390, training loss: 83.94866180419922 = 0.3469873368740082 + 10.0 * 8.360167503356934
Epoch 1390, val loss: 0.396205335855484
Epoch 1400, training loss: 83.92876434326172 = 0.3456560969352722 + 10.0 * 8.35831069946289
Epoch 1400, val loss: 0.39541488885879517
Epoch 1410, training loss: 83.9231185913086 = 0.34435442090034485 + 10.0 * 8.357876777648926
Epoch 1410, val loss: 0.3946247398853302
Epoch 1420, training loss: 83.91524505615234 = 0.34307751059532166 + 10.0 * 8.357216835021973
Epoch 1420, val loss: 0.39385783672332764
Epoch 1430, training loss: 83.91572570800781 = 0.34181901812553406 + 10.0 * 8.357390403747559
Epoch 1430, val loss: 0.3931078612804413
Epoch 1440, training loss: 83.91350555419922 = 0.3405689299106598 + 10.0 * 8.357294082641602
Epoch 1440, val loss: 0.39239269495010376
Epoch 1450, training loss: 83.89730834960938 = 0.33934879302978516 + 10.0 * 8.355795860290527
Epoch 1450, val loss: 0.39170464873313904
Epoch 1460, training loss: 83.89176177978516 = 0.33815598487854004 + 10.0 * 8.35536003112793
Epoch 1460, val loss: 0.39105942845344543
Epoch 1470, training loss: 83.88700866699219 = 0.3369811475276947 + 10.0 * 8.355002403259277
Epoch 1470, val loss: 0.39038988947868347
Epoch 1480, training loss: 83.90039825439453 = 0.3358243703842163 + 10.0 * 8.356457710266113
Epoch 1480, val loss: 0.38976821303367615
Epoch 1490, training loss: 83.88188171386719 = 0.33468055725097656 + 10.0 * 8.354720115661621
Epoch 1490, val loss: 0.3891780376434326
Epoch 1500, training loss: 83.87223815917969 = 0.33356261253356934 + 10.0 * 8.353867530822754
Epoch 1500, val loss: 0.3885928690433502
Epoch 1510, training loss: 83.8646469116211 = 0.3324650526046753 + 10.0 * 8.353218078613281
Epoch 1510, val loss: 0.3880521059036255
Epoch 1520, training loss: 83.85932159423828 = 0.33138275146484375 + 10.0 * 8.35279369354248
Epoch 1520, val loss: 0.38749685883522034
Epoch 1530, training loss: 83.88211822509766 = 0.33031558990478516 + 10.0 * 8.355180740356445
Epoch 1530, val loss: 0.38694387674331665
Epoch 1540, training loss: 83.86925506591797 = 0.32924604415893555 + 10.0 * 8.35400104522705
Epoch 1540, val loss: 0.38646236062049866
Epoch 1550, training loss: 83.84423828125 = 0.32822880148887634 + 10.0 * 8.351600646972656
Epoch 1550, val loss: 0.3859790861606598
Epoch 1560, training loss: 83.84194946289062 = 0.3272252678871155 + 10.0 * 8.351472854614258
Epoch 1560, val loss: 0.38549354672431946
Epoch 1570, training loss: 83.83576202392578 = 0.3262328803539276 + 10.0 * 8.350953102111816
Epoch 1570, val loss: 0.38502851128578186
Epoch 1580, training loss: 83.87005615234375 = 0.3252514600753784 + 10.0 * 8.354480743408203
Epoch 1580, val loss: 0.3845515251159668
Epoch 1590, training loss: 83.83967590332031 = 0.32427334785461426 + 10.0 * 8.351540565490723
Epoch 1590, val loss: 0.384181410074234
Epoch 1600, training loss: 83.82231140136719 = 0.32332491874694824 + 10.0 * 8.349898338317871
Epoch 1600, val loss: 0.38374990224838257
Epoch 1610, training loss: 83.81902313232422 = 0.3223862648010254 + 10.0 * 8.349663734436035
Epoch 1610, val loss: 0.38332125544548035
Epoch 1620, training loss: 83.81369018554688 = 0.3214569091796875 + 10.0 * 8.349223136901855
Epoch 1620, val loss: 0.3829435408115387
Epoch 1630, training loss: 83.81056213378906 = 0.3205348253250122 + 10.0 * 8.349002838134766
Epoch 1630, val loss: 0.38255977630615234
Epoch 1640, training loss: 83.86100006103516 = 0.3196164667606354 + 10.0 * 8.354138374328613
Epoch 1640, val loss: 0.3821828067302704
Epoch 1650, training loss: 83.8111572265625 = 0.3187119960784912 + 10.0 * 8.349245071411133
Epoch 1650, val loss: 0.381838858127594
Epoch 1660, training loss: 83.8011474609375 = 0.31782710552215576 + 10.0 * 8.348332405090332
Epoch 1660, val loss: 0.38146573305130005
Epoch 1670, training loss: 83.79515075683594 = 0.3169567584991455 + 10.0 * 8.347819328308105
Epoch 1670, val loss: 0.3811369836330414
Epoch 1680, training loss: 83.79047393798828 = 0.3160920739173889 + 10.0 * 8.347437858581543
Epoch 1680, val loss: 0.3808024525642395
Epoch 1690, training loss: 83.8068618774414 = 0.31523236632347107 + 10.0 * 8.349163055419922
Epoch 1690, val loss: 0.38044506311416626
Epoch 1700, training loss: 83.79144287109375 = 0.3143812417984009 + 10.0 * 8.347705841064453
Epoch 1700, val loss: 0.3802199959754944
Epoch 1710, training loss: 83.78323364257812 = 0.31354448199272156 + 10.0 * 8.346968650817871
Epoch 1710, val loss: 0.37986811995506287
Epoch 1720, training loss: 83.77653503417969 = 0.3127191960811615 + 10.0 * 8.346381187438965
Epoch 1720, val loss: 0.3795989453792572
Epoch 1730, training loss: 83.77267456054688 = 0.31190386414527893 + 10.0 * 8.346076965332031
Epoch 1730, val loss: 0.3793174922466278
Epoch 1740, training loss: 83.76961517333984 = 0.3110905885696411 + 10.0 * 8.345852851867676
Epoch 1740, val loss: 0.3790319561958313
Epoch 1750, training loss: 83.81610870361328 = 0.31028032302856445 + 10.0 * 8.35058307647705
Epoch 1750, val loss: 0.3787672817707062
Epoch 1760, training loss: 83.76702880859375 = 0.309476375579834 + 10.0 * 8.345754623413086
Epoch 1760, val loss: 0.37852078676223755
Epoch 1770, training loss: 83.7656021118164 = 0.3086937665939331 + 10.0 * 8.345690727233887
Epoch 1770, val loss: 0.3782317638397217
Epoch 1780, training loss: 83.75688934326172 = 0.3079150915145874 + 10.0 * 8.344897270202637
Epoch 1780, val loss: 0.3780064582824707
Epoch 1790, training loss: 83.75251770019531 = 0.3071433901786804 + 10.0 * 8.344537734985352
Epoch 1790, val loss: 0.3777647316455841
Epoch 1800, training loss: 83.75154113769531 = 0.3063763380050659 + 10.0 * 8.34451675415039
Epoch 1800, val loss: 0.3775462806224823
Epoch 1810, training loss: 83.76995086669922 = 0.30561181902885437 + 10.0 * 8.346433639526367
Epoch 1810, val loss: 0.3773373067378998
Epoch 1820, training loss: 83.74884033203125 = 0.3048524856567383 + 10.0 * 8.344398498535156
Epoch 1820, val loss: 0.37705883383750916
Epoch 1830, training loss: 83.74126434326172 = 0.30410245060920715 + 10.0 * 8.343716621398926
Epoch 1830, val loss: 0.3768848776817322
Epoch 1840, training loss: 83.73709106445312 = 0.303358256816864 + 10.0 * 8.34337329864502
Epoch 1840, val loss: 0.3766604959964752
Epoch 1850, training loss: 83.7440185546875 = 0.30261847376823425 + 10.0 * 8.34414005279541
Epoch 1850, val loss: 0.3764805793762207
Epoch 1860, training loss: 83.73396301269531 = 0.3018812835216522 + 10.0 * 8.343208312988281
Epoch 1860, val loss: 0.3762752115726471
Epoch 1870, training loss: 83.72972869873047 = 0.30115726590156555 + 10.0 * 8.342857360839844
Epoch 1870, val loss: 0.37609776854515076
Epoch 1880, training loss: 83.73023223876953 = 0.30043476819992065 + 10.0 * 8.342979431152344
Epoch 1880, val loss: 0.37593233585357666
Epoch 1890, training loss: 83.72464752197266 = 0.2997184991836548 + 10.0 * 8.342493057250977
Epoch 1890, val loss: 0.37573954463005066
Epoch 1900, training loss: 83.71846008300781 = 0.29900848865509033 + 10.0 * 8.341944694519043
Epoch 1900, val loss: 0.3755556344985962
Epoch 1910, training loss: 83.71446990966797 = 0.2983016073703766 + 10.0 * 8.3416166305542
Epoch 1910, val loss: 0.3754216730594635
Epoch 1920, training loss: 83.7104263305664 = 0.29759639501571655 + 10.0 * 8.341282844543457
Epoch 1920, val loss: 0.3752468228340149
Epoch 1930, training loss: 83.72476196289062 = 0.29689255356788635 + 10.0 * 8.34278678894043
Epoch 1930, val loss: 0.37506020069122314
Epoch 1940, training loss: 83.71049499511719 = 0.29618412256240845 + 10.0 * 8.341431617736816
Epoch 1940, val loss: 0.3749948740005493
Epoch 1950, training loss: 83.7124252319336 = 0.2954977750778198 + 10.0 * 8.341692924499512
Epoch 1950, val loss: 0.3747919797897339
Epoch 1960, training loss: 83.69786834716797 = 0.29481521248817444 + 10.0 * 8.34030532836914
Epoch 1960, val loss: 0.3746633231639862
Epoch 1970, training loss: 83.69615173339844 = 0.29413869976997375 + 10.0 * 8.340201377868652
Epoch 1970, val loss: 0.3745391368865967
Epoch 1980, training loss: 83.69193267822266 = 0.293463796377182 + 10.0 * 8.33984661102295
Epoch 1980, val loss: 0.3744080662727356
Epoch 1990, training loss: 83.69083404541016 = 0.29278993606567383 + 10.0 * 8.339803695678711
Epoch 1990, val loss: 0.37427598237991333
Epoch 2000, training loss: 83.72721862792969 = 0.29211604595184326 + 10.0 * 8.343510627746582
Epoch 2000, val loss: 0.37418994307518005
Epoch 2010, training loss: 83.69593048095703 = 0.2914551794528961 + 10.0 * 8.340447425842285
Epoch 2010, val loss: 0.37403982877731323
Epoch 2020, training loss: 83.68323516845703 = 0.2907978594303131 + 10.0 * 8.33924388885498
Epoch 2020, val loss: 0.3739149570465088
Epoch 2030, training loss: 83.67967224121094 = 0.29014652967453003 + 10.0 * 8.338953018188477
Epoch 2030, val loss: 0.3738265633583069
Epoch 2040, training loss: 83.67794036865234 = 0.28949886560440063 + 10.0 * 8.338844299316406
Epoch 2040, val loss: 0.37371736764907837
Epoch 2050, training loss: 83.67852783203125 = 0.2888522148132324 + 10.0 * 8.338968276977539
Epoch 2050, val loss: 0.3736175298690796
Epoch 2060, training loss: 83.66947937011719 = 0.28821060061454773 + 10.0 * 8.338127136230469
Epoch 2060, val loss: 0.37353232502937317
Epoch 2070, training loss: 83.6686019897461 = 0.28756964206695557 + 10.0 * 8.338103294372559
Epoch 2070, val loss: 0.37343138456344604
Epoch 2080, training loss: 83.69300079345703 = 0.28692880272865295 + 10.0 * 8.340607643127441
Epoch 2080, val loss: 0.3732777237892151
Epoch 2090, training loss: 83.67082977294922 = 0.2862965166568756 + 10.0 * 8.33845329284668
Epoch 2090, val loss: 0.37331339716911316
Epoch 2100, training loss: 83.66090393066406 = 0.28567051887512207 + 10.0 * 8.337523460388184
Epoch 2100, val loss: 0.37318822741508484
Epoch 2110, training loss: 83.6548843383789 = 0.2850467264652252 + 10.0 * 8.336983680725098
Epoch 2110, val loss: 0.373124897480011
Epoch 2120, training loss: 83.65339660644531 = 0.28442415595054626 + 10.0 * 8.336896896362305
Epoch 2120, val loss: 0.37306201457977295
Epoch 2130, training loss: 83.68624114990234 = 0.28380194306373596 + 10.0 * 8.34024429321289
Epoch 2130, val loss: 0.372993528842926
Epoch 2140, training loss: 83.65869903564453 = 0.2831842303276062 + 10.0 * 8.33755111694336
Epoch 2140, val loss: 0.372932106256485
Epoch 2150, training loss: 83.64608001708984 = 0.28257378935813904 + 10.0 * 8.336350440979004
Epoch 2150, val loss: 0.3728780150413513
Epoch 2160, training loss: 83.6417236328125 = 0.28196537494659424 + 10.0 * 8.335975646972656
Epoch 2160, val loss: 0.37281784415245056
Epoch 2170, training loss: 83.6797866821289 = 0.2813560664653778 + 10.0 * 8.339842796325684
Epoch 2170, val loss: 0.37273669242858887
Epoch 2180, training loss: 83.65254974365234 = 0.28075623512268066 + 10.0 * 8.337179183959961
Epoch 2180, val loss: 0.3727549910545349
Epoch 2190, training loss: 83.63801574707031 = 0.28016194701194763 + 10.0 * 8.335785865783691
Epoch 2190, val loss: 0.37270841002464294
Epoch 2200, training loss: 83.63180541992188 = 0.27956917881965637 + 10.0 * 8.335223197937012
Epoch 2200, val loss: 0.37264490127563477
Epoch 2210, training loss: 83.62789154052734 = 0.27897897362709045 + 10.0 * 8.334891319274902
Epoch 2210, val loss: 0.37262430787086487
Epoch 2220, training loss: 83.62487030029297 = 0.278389573097229 + 10.0 * 8.334648132324219
Epoch 2220, val loss: 0.3725961148738861
Epoch 2230, training loss: 83.6219482421875 = 0.2777988910675049 + 10.0 * 8.3344144821167
Epoch 2230, val loss: 0.3725607693195343
Epoch 2240, training loss: 83.6194839477539 = 0.2772097885608673 + 10.0 * 8.334227561950684
Epoch 2240, val loss: 0.3725445866584778
Epoch 2250, training loss: 83.6509780883789 = 0.276619017124176 + 10.0 * 8.337435722351074
Epoch 2250, val loss: 0.37255585193634033
Epoch 2260, training loss: 83.64000701904297 = 0.27603620290756226 + 10.0 * 8.336397171020508
Epoch 2260, val loss: 0.3724868893623352
Epoch 2270, training loss: 83.61383819580078 = 0.2754547595977783 + 10.0 * 8.33383846282959
Epoch 2270, val loss: 0.37248799204826355
Epoch 2280, training loss: 83.60979461669922 = 0.2748797535896301 + 10.0 * 8.333491325378418
Epoch 2280, val loss: 0.37248069047927856
Epoch 2290, training loss: 83.60739135742188 = 0.2743071913719177 + 10.0 * 8.333308219909668
Epoch 2290, val loss: 0.3724771738052368
Epoch 2300, training loss: 83.605712890625 = 0.2737347483634949 + 10.0 * 8.333197593688965
Epoch 2300, val loss: 0.37248390913009644
Epoch 2310, training loss: 83.63578796386719 = 0.2731611132621765 + 10.0 * 8.336262702941895
Epoch 2310, val loss: 0.3725191652774811
Epoch 2320, training loss: 83.6097183227539 = 0.27259254455566406 + 10.0 * 8.333712577819824
Epoch 2320, val loss: 0.37243199348449707
Epoch 2330, training loss: 83.60205841064453 = 0.2720257341861725 + 10.0 * 8.333003044128418
Epoch 2330, val loss: 0.3724810481071472
Epoch 2340, training loss: 83.63484954833984 = 0.27146220207214355 + 10.0 * 8.336338996887207
Epoch 2340, val loss: 0.37250858545303345
Epoch 2350, training loss: 83.60115051269531 = 0.2709061801433563 + 10.0 * 8.333024978637695
Epoch 2350, val loss: 0.3724803328514099
Epoch 2360, training loss: 83.58853149414062 = 0.2703527510166168 + 10.0 * 8.331817626953125
Epoch 2360, val loss: 0.3725128769874573
Epoch 2370, training loss: 83.58570098876953 = 0.2698007822036743 + 10.0 * 8.33159065246582
Epoch 2370, val loss: 0.3725334107875824
Epoch 2380, training loss: 83.58301544189453 = 0.26925089955329895 + 10.0 * 8.331377029418945
Epoch 2380, val loss: 0.37256041169166565
Epoch 2390, training loss: 83.5809326171875 = 0.26869991421699524 + 10.0 * 8.331223487854004
Epoch 2390, val loss: 0.3725859224796295
Epoch 2400, training loss: 83.5964584350586 = 0.2681483328342438 + 10.0 * 8.332830429077148
Epoch 2400, val loss: 0.3725971579551697
Epoch 2410, training loss: 83.57889556884766 = 0.2675975263118744 + 10.0 * 8.33112907409668
Epoch 2410, val loss: 0.37266311049461365
Epoch 2420, training loss: 83.58053588867188 = 0.26705458760261536 + 10.0 * 8.331348419189453
Epoch 2420, val loss: 0.3726857006549835
Epoch 2430, training loss: 83.57405090332031 = 0.26651424169540405 + 10.0 * 8.330753326416016
Epoch 2430, val loss: 0.37273508310317993
Epoch 2440, training loss: 83.56873321533203 = 0.26597464084625244 + 10.0 * 8.330275535583496
Epoch 2440, val loss: 0.3727734684944153
Epoch 2450, training loss: 83.56880187988281 = 0.26543498039245605 + 10.0 * 8.330336570739746
Epoch 2450, val loss: 0.37279844284057617
Epoch 2460, training loss: 83.59557342529297 = 0.2648952305316925 + 10.0 * 8.333067893981934
Epoch 2460, val loss: 0.37284669280052185
Epoch 2470, training loss: 83.571533203125 = 0.2643568515777588 + 10.0 * 8.330717086791992
Epoch 2470, val loss: 0.37294259667396545
Epoch 2480, training loss: 83.56205749511719 = 0.26382288336753845 + 10.0 * 8.32982349395752
Epoch 2480, val loss: 0.3729524314403534
Epoch 2490, training loss: 83.55735778808594 = 0.2632896900177002 + 10.0 * 8.32940673828125
Epoch 2490, val loss: 0.37301966547966003
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8503297818366311
0.864449757299138
=== training gcn model ===
Epoch 0, training loss: 106.92232513427734 = 1.1001708507537842 + 10.0 * 10.582215309143066
Epoch 0, val loss: 1.0996173620224
Epoch 10, training loss: 106.91288757324219 = 1.0958375930786133 + 10.0 * 10.581705093383789
Epoch 10, val loss: 1.0952903032302856
Epoch 20, training loss: 106.88554382324219 = 1.091180682182312 + 10.0 * 10.579436302185059
Epoch 20, val loss: 1.0906275510787964
Epoch 30, training loss: 106.77981567382812 = 1.085996389389038 + 10.0 * 10.569381713867188
Epoch 30, val loss: 1.0854307413101196
Epoch 40, training loss: 106.40611267089844 = 1.080252766609192 + 10.0 * 10.532586097717285
Epoch 40, val loss: 1.079673171043396
Epoch 50, training loss: 105.42652130126953 = 1.0739448070526123 + 10.0 * 10.435257911682129
Epoch 50, val loss: 1.0733492374420166
Epoch 60, training loss: 103.35887145996094 = 1.0673805475234985 + 10.0 * 10.229148864746094
Epoch 60, val loss: 1.0666735172271729
Epoch 70, training loss: 99.80878448486328 = 1.0596532821655273 + 10.0 * 9.874913215637207
Epoch 70, val loss: 1.058772087097168
Epoch 80, training loss: 95.79188537597656 = 1.0530362129211426 + 10.0 * 9.473884582519531
Epoch 80, val loss: 1.0526149272918701
Epoch 90, training loss: 94.08515167236328 = 1.0488773584365845 + 10.0 * 9.303627967834473
Epoch 90, val loss: 1.0486301183700562
Epoch 100, training loss: 93.27735900878906 = 1.0450669527053833 + 10.0 * 9.22322940826416
Epoch 100, val loss: 1.0449142456054688
Epoch 110, training loss: 92.84846496582031 = 1.041684627532959 + 10.0 * 9.18067741394043
Epoch 110, val loss: 1.0415995121002197
Epoch 120, training loss: 92.44078063964844 = 1.0389121770858765 + 10.0 * 9.14018726348877
Epoch 120, val loss: 1.0388866662979126
Epoch 130, training loss: 91.8907241821289 = 1.0367234945297241 + 10.0 * 9.085400581359863
Epoch 130, val loss: 1.0367262363433838
Epoch 140, training loss: 91.11788940429688 = 1.0351821184158325 + 10.0 * 9.008271217346191
Epoch 140, val loss: 1.0352849960327148
Epoch 150, training loss: 90.23704528808594 = 1.0346137285232544 + 10.0 * 8.920243263244629
Epoch 150, val loss: 1.0347737073898315
Epoch 160, training loss: 89.70279693603516 = 1.0344312191009521 + 10.0 * 8.866836547851562
Epoch 160, val loss: 1.034477710723877
Epoch 170, training loss: 89.4924087524414 = 1.0331668853759766 + 10.0 * 8.845924377441406
Epoch 170, val loss: 1.0330334901809692
Epoch 180, training loss: 89.26488494873047 = 1.030816912651062 + 10.0 * 8.823407173156738
Epoch 180, val loss: 1.0306826829910278
Epoch 190, training loss: 88.99885559082031 = 1.028773307800293 + 10.0 * 8.797008514404297
Epoch 190, val loss: 1.0287989377975464
Epoch 200, training loss: 88.65067291259766 = 1.0278104543685913 + 10.0 * 8.762286186218262
Epoch 200, val loss: 1.0279662609100342
Epoch 210, training loss: 88.26373291015625 = 1.0273375511169434 + 10.0 * 8.723639488220215
Epoch 210, val loss: 1.0275269746780396
Epoch 220, training loss: 87.965087890625 = 1.0266213417053223 + 10.0 * 8.693846702575684
Epoch 220, val loss: 1.0268146991729736
Epoch 230, training loss: 87.76010131835938 = 1.0254199504852295 + 10.0 * 8.673467636108398
Epoch 230, val loss: 1.0256381034851074
Epoch 240, training loss: 87.61779022216797 = 1.024051547050476 + 10.0 * 8.659374237060547
Epoch 240, val loss: 1.0243233442306519
Epoch 250, training loss: 87.4981460571289 = 1.0227651596069336 + 10.0 * 8.647538185119629
Epoch 250, val loss: 1.0230687856674194
Epoch 260, training loss: 87.36505889892578 = 1.0216760635375977 + 10.0 * 8.63433837890625
Epoch 260, val loss: 1.0220160484313965
Epoch 270, training loss: 87.20716094970703 = 1.0208536386489868 + 10.0 * 8.618631362915039
Epoch 270, val loss: 1.0212080478668213
Epoch 280, training loss: 87.03038024902344 = 1.020213007926941 + 10.0 * 8.601016998291016
Epoch 280, val loss: 1.0205767154693604
Epoch 290, training loss: 86.86414337158203 = 1.0195223093032837 + 10.0 * 8.58446216583252
Epoch 290, val loss: 1.0198630094528198
Epoch 300, training loss: 86.71732330322266 = 1.0186563730239868 + 10.0 * 8.569867134094238
Epoch 300, val loss: 1.0189626216888428
Epoch 310, training loss: 86.59380340576172 = 1.017646312713623 + 10.0 * 8.557615280151367
Epoch 310, val loss: 1.0179413557052612
Epoch 320, training loss: 86.49967193603516 = 1.0164940357208252 + 10.0 * 8.548317909240723
Epoch 320, val loss: 1.0167779922485352
Epoch 330, training loss: 86.40972900390625 = 1.0151420831680298 + 10.0 * 8.539458274841309
Epoch 330, val loss: 1.0154889822006226
Epoch 340, training loss: 86.32498931884766 = 1.0137734413146973 + 10.0 * 8.531122207641602
Epoch 340, val loss: 1.014181137084961
Epoch 350, training loss: 86.2455825805664 = 1.0124006271362305 + 10.0 * 8.52331829071045
Epoch 350, val loss: 1.0128566026687622
Epoch 360, training loss: 86.19719696044922 = 1.0108551979064941 + 10.0 * 8.518633842468262
Epoch 360, val loss: 1.0114009380340576
Epoch 370, training loss: 86.11324310302734 = 1.0092229843139648 + 10.0 * 8.510401725769043
Epoch 370, val loss: 1.0098124742507935
Epoch 380, training loss: 86.05061340332031 = 1.0075851678848267 + 10.0 * 8.504302978515625
Epoch 380, val loss: 1.0082480907440186
Epoch 390, training loss: 85.99002075195312 = 1.0058752298355103 + 10.0 * 8.498414993286133
Epoch 390, val loss: 1.006611943244934
Epoch 400, training loss: 85.93196105957031 = 1.0040984153747559 + 10.0 * 8.492786407470703
Epoch 400, val loss: 1.004906177520752
Epoch 410, training loss: 85.88432312011719 = 1.0022047758102417 + 10.0 * 8.488211631774902
Epoch 410, val loss: 1.0030616521835327
Epoch 420, training loss: 85.836669921875 = 1.0002132654190063 + 10.0 * 8.48364543914795
Epoch 420, val loss: 1.0011523962020874
Epoch 430, training loss: 85.78007507324219 = 0.9982112646102905 + 10.0 * 8.478185653686523
Epoch 430, val loss: 0.9992227554321289
Epoch 440, training loss: 85.73690795898438 = 0.9961225986480713 + 10.0 * 8.474078178405762
Epoch 440, val loss: 0.9972085356712341
Epoch 450, training loss: 85.69905853271484 = 0.9938702583312988 + 10.0 * 8.470518112182617
Epoch 450, val loss: 0.9950211644172668
Epoch 460, training loss: 85.65086364746094 = 0.9915350079536438 + 10.0 * 8.465932846069336
Epoch 460, val loss: 0.9927695393562317
Epoch 470, training loss: 85.61148071289062 = 0.9891178011894226 + 10.0 * 8.462236404418945
Epoch 470, val loss: 0.9904460906982422
Epoch 480, training loss: 85.5723648071289 = 0.9865497946739197 + 10.0 * 8.458581924438477
Epoch 480, val loss: 0.9879632592201233
Epoch 490, training loss: 85.53404235839844 = 0.9838758707046509 + 10.0 * 8.455016136169434
Epoch 490, val loss: 0.9853774905204773
Epoch 500, training loss: 85.52115631103516 = 0.9811170697212219 + 10.0 * 8.454004287719727
Epoch 500, val loss: 0.982681393623352
Epoch 510, training loss: 85.46607208251953 = 0.9780892729759216 + 10.0 * 8.448798179626465
Epoch 510, val loss: 0.9798269271850586
Epoch 520, training loss: 85.42942810058594 = 0.974993884563446 + 10.0 * 8.445444107055664
Epoch 520, val loss: 0.9768383502960205
Epoch 530, training loss: 85.3965835571289 = 0.9717342257499695 + 10.0 * 8.442484855651855
Epoch 530, val loss: 0.9736676216125488
Epoch 540, training loss: 85.36575317382812 = 0.9682292938232422 + 10.0 * 8.439752578735352
Epoch 540, val loss: 0.9702759385108948
Epoch 550, training loss: 85.34310150146484 = 0.9644806981086731 + 10.0 * 8.437862396240234
Epoch 550, val loss: 0.9666566848754883
Epoch 560, training loss: 85.33509826660156 = 0.9604815244674683 + 10.0 * 8.437461853027344
Epoch 560, val loss: 0.9627071619033813
Epoch 570, training loss: 85.29414367675781 = 0.9562522768974304 + 10.0 * 8.433789253234863
Epoch 570, val loss: 0.9586412906646729
Epoch 580, training loss: 85.26458740234375 = 0.9518863558769226 + 10.0 * 8.431269645690918
Epoch 580, val loss: 0.9544140100479126
Epoch 590, training loss: 85.24118041992188 = 0.9473143815994263 + 10.0 * 8.429387092590332
Epoch 590, val loss: 0.9499752521514893
Epoch 600, training loss: 85.24686431884766 = 0.9425044059753418 + 10.0 * 8.430436134338379
Epoch 600, val loss: 0.94524085521698
Epoch 610, training loss: 85.19760131835938 = 0.9373358488082886 + 10.0 * 8.426027297973633
Epoch 610, val loss: 0.9403043985366821
Epoch 620, training loss: 85.17603302001953 = 0.9320646524429321 + 10.0 * 8.424396514892578
Epoch 620, val loss: 0.93519127368927
Epoch 630, training loss: 85.15184783935547 = 0.9265534281730652 + 10.0 * 8.422529220581055
Epoch 630, val loss: 0.9298374652862549
Epoch 640, training loss: 85.13009643554688 = 0.9207664132118225 + 10.0 * 8.42093276977539
Epoch 640, val loss: 0.9241909980773926
Epoch 650, training loss: 85.1105728149414 = 0.9146732091903687 + 10.0 * 8.41958999633789
Epoch 650, val loss: 0.9182752370834351
Epoch 660, training loss: 85.0951919555664 = 0.9081951379776001 + 10.0 * 8.418699264526367
Epoch 660, val loss: 0.9119442701339722
Epoch 670, training loss: 85.0703125 = 0.9013840556144714 + 10.0 * 8.416893005371094
Epoch 670, val loss: 0.905381977558136
Epoch 680, training loss: 85.05308532714844 = 0.8944439888000488 + 10.0 * 8.415863990783691
Epoch 680, val loss: 0.8986380100250244
Epoch 690, training loss: 85.0331802368164 = 0.8871874809265137 + 10.0 * 8.414599418640137
Epoch 690, val loss: 0.8915852308273315
Epoch 700, training loss: 85.01358795166016 = 0.8795917630195618 + 10.0 * 8.413399696350098
Epoch 700, val loss: 0.884200394153595
Epoch 710, training loss: 84.99524688720703 = 0.8716446757316589 + 10.0 * 8.412360191345215
Epoch 710, val loss: 0.8764759302139282
Epoch 720, training loss: 84.99879455566406 = 0.8633608222007751 + 10.0 * 8.413543701171875
Epoch 720, val loss: 0.8683569431304932
Epoch 730, training loss: 84.96855926513672 = 0.8545987010002136 + 10.0 * 8.411396026611328
Epoch 730, val loss: 0.8599746227264404
Epoch 740, training loss: 84.94569396972656 = 0.8456977605819702 + 10.0 * 8.40999984741211
Epoch 740, val loss: 0.8513379693031311
Epoch 750, training loss: 84.92387390136719 = 0.8365164399147034 + 10.0 * 8.408735275268555
Epoch 750, val loss: 0.8424191474914551
Epoch 760, training loss: 84.90587615966797 = 0.827008068561554 + 10.0 * 8.407886505126953
Epoch 760, val loss: 0.8331866264343262
Epoch 770, training loss: 84.89354705810547 = 0.8171892762184143 + 10.0 * 8.407635688781738
Epoch 770, val loss: 0.8236494064331055
Epoch 780, training loss: 84.87010955810547 = 0.8070135116577148 + 10.0 * 8.406309127807617
Epoch 780, val loss: 0.813817024230957
Epoch 790, training loss: 84.85305786132812 = 0.7966708540916443 + 10.0 * 8.405638694763184
Epoch 790, val loss: 0.8037934303283691
Epoch 800, training loss: 84.83174896240234 = 0.7860972881317139 + 10.0 * 8.404565811157227
Epoch 800, val loss: 0.7936076521873474
Epoch 810, training loss: 84.81552124023438 = 0.7753308415412903 + 10.0 * 8.404019355773926
Epoch 810, val loss: 0.7832035422325134
Epoch 820, training loss: 84.7950439453125 = 0.764317512512207 + 10.0 * 8.403072357177734
Epoch 820, val loss: 0.772559404373169
Epoch 830, training loss: 84.77206420898438 = 0.7531886696815491 + 10.0 * 8.401887893676758
Epoch 830, val loss: 0.7618261575698853
Epoch 840, training loss: 84.752197265625 = 0.7419657111167908 + 10.0 * 8.401022911071777
Epoch 840, val loss: 0.7509675025939941
Epoch 850, training loss: 84.73056030273438 = 0.7306066155433655 + 10.0 * 8.399995803833008
Epoch 850, val loss: 0.7400614023208618
Epoch 860, training loss: 84.71865844726562 = 0.7191879153251648 + 10.0 * 8.399947166442871
Epoch 860, val loss: 0.7290216088294983
Epoch 870, training loss: 84.70702362060547 = 0.7075397968292236 + 10.0 * 8.399948120117188
Epoch 870, val loss: 0.7179699540138245
Epoch 880, training loss: 84.67091369628906 = 0.6961588263511658 + 10.0 * 8.397475242614746
Epoch 880, val loss: 0.70704185962677
Epoch 890, training loss: 84.6465835571289 = 0.6849299073219299 + 10.0 * 8.39616584777832
Epoch 890, val loss: 0.6962627172470093
Epoch 900, training loss: 84.62477111816406 = 0.6737383604049683 + 10.0 * 8.395103454589844
Epoch 900, val loss: 0.6855500936508179
Epoch 910, training loss: 84.60375213623047 = 0.66257643699646 + 10.0 * 8.39411735534668
Epoch 910, val loss: 0.6748850345611572
Epoch 920, training loss: 84.58301544189453 = 0.6514646410942078 + 10.0 * 8.393155097961426
Epoch 920, val loss: 0.6642845869064331
Epoch 930, training loss: 84.6290512084961 = 0.6404882073402405 + 10.0 * 8.398856163024902
Epoch 930, val loss: 0.653734564781189
Epoch 940, training loss: 84.54923248291016 = 0.6295465230941772 + 10.0 * 8.391968727111816
Epoch 940, val loss: 0.6434599161148071
Epoch 950, training loss: 84.52420806884766 = 0.6190263628959656 + 10.0 * 8.390518188476562
Epoch 950, val loss: 0.6335387229919434
Epoch 960, training loss: 84.50247955322266 = 0.6087587475776672 + 10.0 * 8.389371871948242
Epoch 960, val loss: 0.6238596439361572
Epoch 970, training loss: 84.4826889038086 = 0.598669707775116 + 10.0 * 8.388401985168457
Epoch 970, val loss: 0.6143701076507568
Epoch 980, training loss: 84.4830093383789 = 0.5887699723243713 + 10.0 * 8.389424324035645
Epoch 980, val loss: 0.6050880551338196
Epoch 990, training loss: 84.47525024414062 = 0.5790418386459351 + 10.0 * 8.389620780944824
Epoch 990, val loss: 0.5959416627883911
Epoch 1000, training loss: 84.43439483642578 = 0.5697267651557922 + 10.0 * 8.386466979980469
Epoch 1000, val loss: 0.5872713923454285
Epoch 1010, training loss: 84.41260528564453 = 0.560795783996582 + 10.0 * 8.385180473327637
Epoch 1010, val loss: 0.5789465308189392
Epoch 1020, training loss: 84.3947525024414 = 0.5521396994590759 + 10.0 * 8.384261131286621
Epoch 1020, val loss: 0.5709123015403748
Epoch 1030, training loss: 84.37794494628906 = 0.5437342524528503 + 10.0 * 8.383420944213867
Epoch 1030, val loss: 0.5631355047225952
Epoch 1040, training loss: 84.3616714477539 = 0.5355608463287354 + 10.0 * 8.382611274719238
Epoch 1040, val loss: 0.5555799007415771
Epoch 1050, training loss: 84.35065460205078 = 0.5276365280151367 + 10.0 * 8.382302284240723
Epoch 1050, val loss: 0.5482876300811768
Epoch 1060, training loss: 84.35401916503906 = 0.5199097990989685 + 10.0 * 8.383410453796387
Epoch 1060, val loss: 0.5411975979804993
Epoch 1070, training loss: 84.32634735107422 = 0.5125081539154053 + 10.0 * 8.381383895874023
Epoch 1070, val loss: 0.5344330668449402
Epoch 1080, training loss: 84.3041000366211 = 0.5054408311843872 + 10.0 * 8.379865646362305
Epoch 1080, val loss: 0.5280042290687561
Epoch 1090, training loss: 84.2871322631836 = 0.4986347258090973 + 10.0 * 8.378849983215332
Epoch 1090, val loss: 0.521828293800354
Epoch 1100, training loss: 84.27696990966797 = 0.4920596778392792 + 10.0 * 8.378491401672363
Epoch 1100, val loss: 0.5158591866493225
Epoch 1110, training loss: 84.28409576416016 = 0.4856775403022766 + 10.0 * 8.379841804504395
Epoch 1110, val loss: 0.5100910663604736
Epoch 1120, training loss: 84.24699401855469 = 0.4795655608177185 + 10.0 * 8.376742362976074
Epoch 1120, val loss: 0.5046587586402893
Epoch 1130, training loss: 84.23604583740234 = 0.4737486243247986 + 10.0 * 8.376230239868164
Epoch 1130, val loss: 0.4995039403438568
Epoch 1140, training loss: 84.22238159179688 = 0.46817106008529663 + 10.0 * 8.375421524047852
Epoch 1140, val loss: 0.4945412874221802
Epoch 1150, training loss: 84.21246337890625 = 0.46277523040771484 + 10.0 * 8.374968528747559
Epoch 1150, val loss: 0.4898090660572052
Epoch 1160, training loss: 84.21778869628906 = 0.45756322145462036 + 10.0 * 8.376022338867188
Epoch 1160, val loss: 0.48520776629447937
Epoch 1170, training loss: 84.19612884521484 = 0.45261481404304504 + 10.0 * 8.374351501464844
Epoch 1170, val loss: 0.48093360662460327
Epoch 1180, training loss: 84.17497253417969 = 0.4479505121707916 + 10.0 * 8.372701644897461
Epoch 1180, val loss: 0.4768480956554413
Epoch 1190, training loss: 84.16629028320312 = 0.4434509575366974 + 10.0 * 8.372283935546875
Epoch 1190, val loss: 0.47295957803726196
Epoch 1200, training loss: 84.15540313720703 = 0.43909725546836853 + 10.0 * 8.371630668640137
Epoch 1200, val loss: 0.469251424074173
Epoch 1210, training loss: 84.19644165039062 = 0.434919536113739 + 10.0 * 8.376152038574219
Epoch 1210, val loss: 0.4656631052494049
Epoch 1220, training loss: 84.14329528808594 = 0.4309066832065582 + 10.0 * 8.371238708496094
Epoch 1220, val loss: 0.46230146288871765
Epoch 1230, training loss: 84.13141632080078 = 0.42714834213256836 + 10.0 * 8.370427131652832
Epoch 1230, val loss: 0.4591183662414551
Epoch 1240, training loss: 84.11751556396484 = 0.4235263764858246 + 10.0 * 8.369399070739746
Epoch 1240, val loss: 0.4561156630516052
Epoch 1250, training loss: 84.10908508300781 = 0.4200407862663269 + 10.0 * 8.368904113769531
Epoch 1250, val loss: 0.45322927832603455
Epoch 1260, training loss: 84.10098266601562 = 0.41668617725372314 + 10.0 * 8.368429183959961
Epoch 1260, val loss: 0.4504721760749817
Epoch 1270, training loss: 84.11272430419922 = 0.4134247899055481 + 10.0 * 8.369930267333984
Epoch 1270, val loss: 0.447823703289032
Epoch 1280, training loss: 84.09789276123047 = 0.41033032536506653 + 10.0 * 8.368756294250488
Epoch 1280, val loss: 0.4452977180480957
Epoch 1290, training loss: 84.09251403808594 = 0.4073484241962433 + 10.0 * 8.36851692199707
Epoch 1290, val loss: 0.4428861439228058
Epoch 1300, training loss: 84.07142639160156 = 0.40449807047843933 + 10.0 * 8.366693496704102
Epoch 1300, val loss: 0.4406276047229767
Epoch 1310, training loss: 84.06475067138672 = 0.40176939964294434 + 10.0 * 8.366297721862793
Epoch 1310, val loss: 0.438471257686615
Epoch 1320, training loss: 84.05603790283203 = 0.3991297781467438 + 10.0 * 8.365690231323242
Epoch 1320, val loss: 0.43638700246810913
Epoch 1330, training loss: 84.05464935302734 = 0.39657795429229736 + 10.0 * 8.36580753326416
Epoch 1330, val loss: 0.43438979983329773
Epoch 1340, training loss: 84.05916595458984 = 0.39409056305885315 + 10.0 * 8.366507530212402
Epoch 1340, val loss: 0.4324868321418762
Epoch 1350, training loss: 84.04296112060547 = 0.3916953504085541 + 10.0 * 8.365126609802246
Epoch 1350, val loss: 0.43066900968551636
Epoch 1360, training loss: 84.02993774414062 = 0.3894162178039551 + 10.0 * 8.364051818847656
Epoch 1360, val loss: 0.428941547870636
Epoch 1370, training loss: 84.02249145507812 = 0.38720300793647766 + 10.0 * 8.363529205322266
Epoch 1370, val loss: 0.4272822439670563
Epoch 1380, training loss: 84.01564025878906 = 0.3850446939468384 + 10.0 * 8.363059043884277
Epoch 1380, val loss: 0.42570003867149353
Epoch 1390, training loss: 84.0169677734375 = 0.3829473555088043 + 10.0 * 8.363401412963867
Epoch 1390, val loss: 0.4241566061973572
Epoch 1400, training loss: 84.00703430175781 = 0.38089579343795776 + 10.0 * 8.362613677978516
Epoch 1400, val loss: 0.42266955971717834
Epoch 1410, training loss: 84.01864624023438 = 0.37891867756843567 + 10.0 * 8.363972663879395
Epoch 1410, val loss: 0.42126402258872986
Epoch 1420, training loss: 83.99555969238281 = 0.3770124614238739 + 10.0 * 8.361854553222656
Epoch 1420, val loss: 0.41988083720207214
Epoch 1430, training loss: 83.988037109375 = 0.37517639994621277 + 10.0 * 8.361286163330078
Epoch 1430, val loss: 0.4185977876186371
Epoch 1440, training loss: 83.98075103759766 = 0.3733939528465271 + 10.0 * 8.360735893249512
Epoch 1440, val loss: 0.4173588752746582
Epoch 1450, training loss: 83.97381591796875 = 0.3716464638710022 + 10.0 * 8.360217094421387
Epoch 1450, val loss: 0.4161564111709595
Epoch 1460, training loss: 83.96876525878906 = 0.3699369728565216 + 10.0 * 8.359883308410645
Epoch 1460, val loss: 0.41498231887817383
Epoch 1470, training loss: 83.96815490722656 = 0.36825910210609436 + 10.0 * 8.359990119934082
Epoch 1470, val loss: 0.4138495922088623
Epoch 1480, training loss: 83.98114776611328 = 0.36661040782928467 + 10.0 * 8.361454010009766
Epoch 1480, val loss: 0.4127289056777954
Epoch 1490, training loss: 83.9570083618164 = 0.36501196026802063 + 10.0 * 8.359199523925781
Epoch 1490, val loss: 0.41169312596321106
Epoch 1500, training loss: 83.95067596435547 = 0.3634592592716217 + 10.0 * 8.358721733093262
Epoch 1500, val loss: 0.41067031025886536
Epoch 1510, training loss: 83.94212341308594 = 0.36194881796836853 + 10.0 * 8.358017921447754
Epoch 1510, val loss: 0.40969011187553406
Epoch 1520, training loss: 83.9375228881836 = 0.3604709208011627 + 10.0 * 8.357705116271973
Epoch 1520, val loss: 0.40874388813972473
Epoch 1530, training loss: 83.95167541503906 = 0.3590103089809418 + 10.0 * 8.35926628112793
Epoch 1530, val loss: 0.4078321158885956
Epoch 1540, training loss: 83.95196533203125 = 0.3575948178768158 + 10.0 * 8.359436988830566
Epoch 1540, val loss: 0.4068778455257416
Epoch 1550, training loss: 83.92372131347656 = 0.35622483491897583 + 10.0 * 8.356749534606934
Epoch 1550, val loss: 0.40605518221855164
Epoch 1560, training loss: 83.91991424560547 = 0.3548853099346161 + 10.0 * 8.356502532958984
Epoch 1560, val loss: 0.40522927045822144
Epoch 1570, training loss: 83.91475677490234 = 0.353571355342865 + 10.0 * 8.356119155883789
Epoch 1570, val loss: 0.40445855259895325
Epoch 1580, training loss: 83.90846252441406 = 0.3522780239582062 + 10.0 * 8.355618476867676
Epoch 1580, val loss: 0.40365755558013916
Epoch 1590, training loss: 83.90447235107422 = 0.3509974479675293 + 10.0 * 8.355347633361816
Epoch 1590, val loss: 0.4029051661491394
Epoch 1600, training loss: 83.92691802978516 = 0.3497259318828583 + 10.0 * 8.357719421386719
Epoch 1600, val loss: 0.4021540582180023
Epoch 1610, training loss: 83.91586303710938 = 0.34849825501441956 + 10.0 * 8.35673713684082
Epoch 1610, val loss: 0.40143513679504395
Epoch 1620, training loss: 83.89718627929688 = 0.34728875756263733 + 10.0 * 8.354990005493164
Epoch 1620, val loss: 0.4007139801979065
Epoch 1630, training loss: 83.88812255859375 = 0.34610557556152344 + 10.0 * 8.354201316833496
Epoch 1630, val loss: 0.400056928396225
Epoch 1640, training loss: 83.88340759277344 = 0.34494054317474365 + 10.0 * 8.353846549987793
Epoch 1640, val loss: 0.39940011501312256
Epoch 1650, training loss: 83.87886047363281 = 0.34379181265830994 + 10.0 * 8.353507041931152
Epoch 1650, val loss: 0.3987445831298828
Epoch 1660, training loss: 83.87539672851562 = 0.34265169501304626 + 10.0 * 8.35327434539795
Epoch 1660, val loss: 0.3981013596057892
Epoch 1670, training loss: 83.8973388671875 = 0.3415224254131317 + 10.0 * 8.355581283569336
Epoch 1670, val loss: 0.39746543765068054
Epoch 1680, training loss: 83.87552642822266 = 0.3404080867767334 + 10.0 * 8.353511810302734
Epoch 1680, val loss: 0.3969021737575531
Epoch 1690, training loss: 83.86809539794922 = 0.33931776881217957 + 10.0 * 8.352877616882324
Epoch 1690, val loss: 0.3962877094745636
Epoch 1700, training loss: 83.8599853515625 = 0.33824464678764343 + 10.0 * 8.352174758911133
Epoch 1700, val loss: 0.39572209119796753
Epoch 1710, training loss: 83.8627700805664 = 0.33717992901802063 + 10.0 * 8.352559089660645
Epoch 1710, val loss: 0.3951733410358429
Epoch 1720, training loss: 83.86061096191406 = 0.3361269235610962 + 10.0 * 8.352448463439941
Epoch 1720, val loss: 0.39462053775787354
Epoch 1730, training loss: 83.85042572021484 = 0.3350987136363983 + 10.0 * 8.351532936096191
Epoch 1730, val loss: 0.39410436153411865
Epoch 1740, training loss: 83.84516143798828 = 0.33407679200172424 + 10.0 * 8.35110855102539
Epoch 1740, val loss: 0.3935910165309906
Epoch 1750, training loss: 83.84162902832031 = 0.33306384086608887 + 10.0 * 8.35085678100586
Epoch 1750, val loss: 0.3930862247943878
Epoch 1760, training loss: 83.83837127685547 = 0.3320580720901489 + 10.0 * 8.350630760192871
Epoch 1760, val loss: 0.3925940692424774
Epoch 1770, training loss: 83.85499572753906 = 0.33106058835983276 + 10.0 * 8.35239315032959
Epoch 1770, val loss: 0.3921166956424713
Epoch 1780, training loss: 83.83936309814453 = 0.33007967472076416 + 10.0 * 8.35092830657959
Epoch 1780, val loss: 0.39164096117019653
Epoch 1790, training loss: 83.83436584472656 = 0.3291192650794983 + 10.0 * 8.35052490234375
Epoch 1790, val loss: 0.39119547605514526
Epoch 1800, training loss: 83.82655334472656 = 0.3281725347042084 + 10.0 * 8.349838256835938
Epoch 1800, val loss: 0.39076584577560425
Epoch 1810, training loss: 83.8203125 = 0.3272334933280945 + 10.0 * 8.349308013916016
Epoch 1810, val loss: 0.39033767580986023
Epoch 1820, training loss: 83.81685638427734 = 0.3263038396835327 + 10.0 * 8.349055290222168
Epoch 1820, val loss: 0.38990798592567444
Epoch 1830, training loss: 83.81443786621094 = 0.3253763020038605 + 10.0 * 8.348905563354492
Epoch 1830, val loss: 0.3894937038421631
Epoch 1840, training loss: 83.86212158203125 = 0.32445064187049866 + 10.0 * 8.353767395019531
Epoch 1840, val loss: 0.38908514380455017
Epoch 1850, training loss: 83.82466888427734 = 0.3235495984554291 + 10.0 * 8.350111961364746
Epoch 1850, val loss: 0.3886776566505432
Epoch 1860, training loss: 83.80348205566406 = 0.3226526379585266 + 10.0 * 8.348082542419434
Epoch 1860, val loss: 0.388297438621521
Epoch 1870, training loss: 83.80223846435547 = 0.3217696249485016 + 10.0 * 8.348047256469727
Epoch 1870, val loss: 0.3879302740097046
Epoch 1880, training loss: 83.79705810546875 = 0.3208904564380646 + 10.0 * 8.347616195678711
Epoch 1880, val loss: 0.38755562901496887
Epoch 1890, training loss: 83.79401397705078 = 0.3200153708457947 + 10.0 * 8.347399711608887
Epoch 1890, val loss: 0.3871915340423584
Epoch 1900, training loss: 83.83649444580078 = 0.31914323568344116 + 10.0 * 8.35173511505127
Epoch 1900, val loss: 0.3868553042411804
Epoch 1910, training loss: 83.81056213378906 = 0.31828176975250244 + 10.0 * 8.349227905273438
Epoch 1910, val loss: 0.3864936828613281
Epoch 1920, training loss: 83.78598022460938 = 0.3174413740634918 + 10.0 * 8.346853256225586
Epoch 1920, val loss: 0.38614439964294434
Epoch 1930, training loss: 83.78118896484375 = 0.31660911440849304 + 10.0 * 8.346457481384277
Epoch 1930, val loss: 0.3858385682106018
Epoch 1940, training loss: 83.77787780761719 = 0.31578007340431213 + 10.0 * 8.346209526062012
Epoch 1940, val loss: 0.3855222761631012
Epoch 1950, training loss: 83.77359008789062 = 0.3149529993534088 + 10.0 * 8.345863342285156
Epoch 1950, val loss: 0.3851931393146515
Epoch 1960, training loss: 83.77050018310547 = 0.3141269385814667 + 10.0 * 8.345637321472168
Epoch 1960, val loss: 0.38487711548805237
Epoch 1970, training loss: 83.7761001586914 = 0.31330087780952454 + 10.0 * 8.346280097961426
Epoch 1970, val loss: 0.38455548882484436
Epoch 1980, training loss: 83.76766967773438 = 0.3124832808971405 + 10.0 * 8.345518112182617
Epoch 1980, val loss: 0.384268581867218
Epoch 1990, training loss: 83.77457427978516 = 0.311677485704422 + 10.0 * 8.34628963470459
Epoch 1990, val loss: 0.3839804530143738
Epoch 2000, training loss: 83.76252746582031 = 0.3108808100223541 + 10.0 * 8.34516429901123
Epoch 2000, val loss: 0.383709192276001
Epoch 2010, training loss: 83.75591278076172 = 0.3100886344909668 + 10.0 * 8.344582557678223
Epoch 2010, val loss: 0.3834535479545593
Epoch 2020, training loss: 83.75382232666016 = 0.3093006908893585 + 10.0 * 8.344451904296875
Epoch 2020, val loss: 0.3831903338432312
Epoch 2030, training loss: 83.75365447998047 = 0.30851495265960693 + 10.0 * 8.344513893127441
Epoch 2030, val loss: 0.3829505145549774
Epoch 2040, training loss: 83.75384521484375 = 0.30773216485977173 + 10.0 * 8.344611167907715
Epoch 2040, val loss: 0.38269755244255066
Epoch 2050, training loss: 83.74565124511719 = 0.3069572448730469 + 10.0 * 8.34386920928955
Epoch 2050, val loss: 0.38242974877357483
Epoch 2060, training loss: 83.74659729003906 = 0.3061850666999817 + 10.0 * 8.34404182434082
Epoch 2060, val loss: 0.38220131397247314
Epoch 2070, training loss: 83.74468231201172 = 0.30542340874671936 + 10.0 * 8.343926429748535
Epoch 2070, val loss: 0.3819809854030609
Epoch 2080, training loss: 83.76038360595703 = 0.3046625554561615 + 10.0 * 8.345571517944336
Epoch 2080, val loss: 0.3817257583141327
Epoch 2090, training loss: 83.7332992553711 = 0.30392178893089294 + 10.0 * 8.342937469482422
Epoch 2090, val loss: 0.3815166652202606
Epoch 2100, training loss: 83.72555541992188 = 0.30317792296409607 + 10.0 * 8.34223747253418
Epoch 2100, val loss: 0.38128018379211426
Epoch 2110, training loss: 83.72154998779297 = 0.30244264006614685 + 10.0 * 8.341910362243652
Epoch 2110, val loss: 0.38109543919563293
Epoch 2120, training loss: 83.7183837890625 = 0.3017074465751648 + 10.0 * 8.341668128967285
Epoch 2120, val loss: 0.380871057510376
Epoch 2130, training loss: 83.71637725830078 = 0.30097320675849915 + 10.0 * 8.341540336608887
Epoch 2130, val loss: 0.38067427277565
Epoch 2140, training loss: 83.75532531738281 = 0.3002431392669678 + 10.0 * 8.345508575439453
Epoch 2140, val loss: 0.3804643750190735
Epoch 2150, training loss: 83.7371826171875 = 0.2995132505893707 + 10.0 * 8.343767166137695
Epoch 2150, val loss: 0.3802902102470398
Epoch 2160, training loss: 83.70963287353516 = 0.2988072335720062 + 10.0 * 8.341082572937012
Epoch 2160, val loss: 0.38009750843048096
Epoch 2170, training loss: 83.70629119873047 = 0.29809990525245667 + 10.0 * 8.340818405151367
Epoch 2170, val loss: 0.3799506723880768
Epoch 2180, training loss: 83.70002746582031 = 0.29739460349082947 + 10.0 * 8.340263366699219
Epoch 2180, val loss: 0.3797743618488312
Epoch 2190, training loss: 83.69746398925781 = 0.2966900169849396 + 10.0 * 8.34007740020752
Epoch 2190, val loss: 0.3795911967754364
Epoch 2200, training loss: 83.69490051269531 = 0.2959868311882019 + 10.0 * 8.33989143371582
Epoch 2200, val loss: 0.3794253468513489
Epoch 2210, training loss: 83.70581817626953 = 0.29528331756591797 + 10.0 * 8.34105396270752
Epoch 2210, val loss: 0.3792572617530823
Epoch 2220, training loss: 83.69511413574219 = 0.2945835292339325 + 10.0 * 8.340052604675293
Epoch 2220, val loss: 0.37912845611572266
Epoch 2230, training loss: 83.68541717529297 = 0.2938923239707947 + 10.0 * 8.339152336120605
Epoch 2230, val loss: 0.3789578378200531
Epoch 2240, training loss: 83.68306732177734 = 0.29320409893989563 + 10.0 * 8.33898639678955
Epoch 2240, val loss: 0.378827303647995
Epoch 2250, training loss: 83.68084716796875 = 0.29251807928085327 + 10.0 * 8.33883285522461
Epoch 2250, val loss: 0.37867653369903564
Epoch 2260, training loss: 83.68690490722656 = 0.2918322682380676 + 10.0 * 8.339507102966309
Epoch 2260, val loss: 0.37852513790130615
Epoch 2270, training loss: 83.68919372558594 = 0.29115182161331177 + 10.0 * 8.339803695678711
Epoch 2270, val loss: 0.378401517868042
Epoch 2280, training loss: 83.68444061279297 = 0.2904774248600006 + 10.0 * 8.339396476745605
Epoch 2280, val loss: 0.3782913386821747
Epoch 2290, training loss: 83.67021942138672 = 0.2898060381412506 + 10.0 * 8.338041305541992
Epoch 2290, val loss: 0.37817445397377014
Epoch 2300, training loss: 83.66719818115234 = 0.28913864493370056 + 10.0 * 8.33780574798584
Epoch 2300, val loss: 0.378055214881897
Epoch 2310, training loss: 83.66472625732422 = 0.288471519947052 + 10.0 * 8.337625503540039
Epoch 2310, val loss: 0.3779314458370209
Epoch 2320, training loss: 83.66450500488281 = 0.28780293464660645 + 10.0 * 8.33767032623291
Epoch 2320, val loss: 0.3778277635574341
Epoch 2330, training loss: 83.675537109375 = 0.28713589906692505 + 10.0 * 8.33884048461914
Epoch 2330, val loss: 0.3777156174182892
Epoch 2340, training loss: 83.65752410888672 = 0.28647130727767944 + 10.0 * 8.337105751037598
Epoch 2340, val loss: 0.377601683139801
Epoch 2350, training loss: 83.65961456298828 = 0.28580909967422485 + 10.0 * 8.337380409240723
Epoch 2350, val loss: 0.3775113523006439
Epoch 2360, training loss: 83.68831634521484 = 0.28514519333839417 + 10.0 * 8.340316772460938
Epoch 2360, val loss: 0.37742722034454346
Epoch 2370, training loss: 83.65564727783203 = 0.28449565172195435 + 10.0 * 8.337115287780762
Epoch 2370, val loss: 0.3773101270198822
Epoch 2380, training loss: 83.64779663085938 = 0.2838481664657593 + 10.0 * 8.336394309997559
Epoch 2380, val loss: 0.3772171437740326
Epoch 2390, training loss: 83.64437103271484 = 0.28320008516311646 + 10.0 * 8.336116790771484
Epoch 2390, val loss: 0.37713733315467834
Epoch 2400, training loss: 83.64051818847656 = 0.28255367279052734 + 10.0 * 8.335796356201172
Epoch 2400, val loss: 0.377041220664978
Epoch 2410, training loss: 83.63823699951172 = 0.2819051146507263 + 10.0 * 8.335633277893066
Epoch 2410, val loss: 0.3769589960575104
Epoch 2420, training loss: 83.66828155517578 = 0.28125935792922974 + 10.0 * 8.338702201843262
Epoch 2420, val loss: 0.37686002254486084
Epoch 2430, training loss: 83.64543914794922 = 0.28061598539352417 + 10.0 * 8.336482048034668
Epoch 2430, val loss: 0.3768354654312134
Epoch 2440, training loss: 83.63668823242188 = 0.27998173236846924 + 10.0 * 8.335670471191406
Epoch 2440, val loss: 0.376719206571579
Epoch 2450, training loss: 83.62897491455078 = 0.2793479263782501 + 10.0 * 8.334962844848633
Epoch 2450, val loss: 0.37668976187705994
Epoch 2460, training loss: 83.63262939453125 = 0.278717041015625 + 10.0 * 8.3353910446167
Epoch 2460, val loss: 0.3766157925128937
Epoch 2470, training loss: 83.64366149902344 = 0.2780876159667969 + 10.0 * 8.336557388305664
Epoch 2470, val loss: 0.37659117579460144
Epoch 2480, training loss: 83.62763977050781 = 0.2774646580219269 + 10.0 * 8.335017204284668
Epoch 2480, val loss: 0.37653329968452454
Epoch 2490, training loss: 83.62094116210938 = 0.27684080600738525 + 10.0 * 8.334409713745117
Epoch 2490, val loss: 0.37645193934440613
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8493150684931506
0.8648119973918714
The final CL Acc:0.84982, 0.00041, The final GNN Acc:0.86397, 0.00095
Begin epxeriment: noisy_level: 0.2 cont_weight: 10 epoch: der1:0.3 der2:0.2 dfr1:0.4 dfr2:0.4
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=2500, cl_num_layers=2, cl_num_proj_hidden=64, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=10.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.2, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=32, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, noisy_level=0.2, num_hidden=64, num_proj_hidden=64, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=40, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 88648])
add edge: torch.Size([2, 106416])
remove edge: torch.Size([2, 70800])
updated graph: torch.Size([2, 88568])
=== Noisy graph ===
/home/project-robust-contrastive/Robust-Contrastive-Learning/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 106.91018676757812 = 1.0876731872558594 + 10.0 * 10.58225154876709
Epoch 0, val loss: 1.088793158531189
Epoch 10, training loss: 106.90129089355469 = 1.0836005210876465 + 10.0 * 10.581768989562988
Epoch 10, val loss: 1.084695816040039
Epoch 20, training loss: 106.87425994873047 = 1.0794565677642822 + 10.0 * 10.579480171203613
Epoch 20, val loss: 1.0804791450500488
Epoch 30, training loss: 106.7623062133789 = 1.0750324726104736 + 10.0 * 10.568727493286133
Epoch 30, val loss: 1.075943946838379
Epoch 40, training loss: 106.32719421386719 = 1.0702250003814697 + 10.0 * 10.525696754455566
Epoch 40, val loss: 1.070999026298523
Epoch 50, training loss: 104.89949035644531 = 1.065354347229004 + 10.0 * 10.383413314819336
Epoch 50, val loss: 1.0660816431045532
Epoch 60, training loss: 100.72904968261719 = 1.0610849857330322 + 10.0 * 9.966795921325684
Epoch 60, val loss: 1.0617746114730835
Epoch 70, training loss: 97.89124298095703 = 1.0564517974853516 + 10.0 * 9.683479309082031
Epoch 70, val loss: 1.0570944547653198
Epoch 80, training loss: 97.08988952636719 = 1.051984429359436 + 10.0 * 9.603790283203125
Epoch 80, val loss: 1.0526976585388184
Epoch 90, training loss: 96.18724060058594 = 1.0485388040542603 + 10.0 * 9.513870239257812
Epoch 90, val loss: 1.0493831634521484
Epoch 100, training loss: 94.4946517944336 = 1.0464240312576294 + 10.0 * 9.344822883605957
Epoch 100, val loss: 1.0474414825439453
Epoch 110, training loss: 92.57686614990234 = 1.0449683666229248 + 10.0 * 9.153189659118652
Epoch 110, val loss: 1.04616117477417
Epoch 120, training loss: 91.65792846679688 = 1.0435162782669067 + 10.0 * 9.061441421508789
Epoch 120, val loss: 1.0448172092437744
Epoch 130, training loss: 91.08494567871094 = 1.0414543151855469 + 10.0 * 9.004348754882812
Epoch 130, val loss: 1.0427907705307007
Epoch 140, training loss: 90.42769622802734 = 1.0391430854797363 + 10.0 * 8.938855171203613
Epoch 140, val loss: 1.040542721748352
Epoch 150, training loss: 90.007080078125 = 1.0369024276733398 + 10.0 * 8.897017478942871
Epoch 150, val loss: 1.038321852684021
Epoch 160, training loss: 89.61976623535156 = 1.034576416015625 + 10.0 * 8.858518600463867
Epoch 160, val loss: 1.035996675491333
Epoch 170, training loss: 89.27118682861328 = 1.0321376323699951 + 10.0 * 8.823904991149902
Epoch 170, val loss: 1.0336302518844604
Epoch 180, training loss: 89.02383422851562 = 1.0295405387878418 + 10.0 * 8.799428939819336
Epoch 180, val loss: 1.0311769247055054
Epoch 190, training loss: 88.81172180175781 = 1.0267060995101929 + 10.0 * 8.778501510620117
Epoch 190, val loss: 1.0284982919692993
Epoch 200, training loss: 88.64068603515625 = 1.023632526397705 + 10.0 * 8.76170539855957
Epoch 200, val loss: 1.0255540609359741
Epoch 210, training loss: 88.52383422851562 = 1.0203653573989868 + 10.0 * 8.750347137451172
Epoch 210, val loss: 1.022334337234497
Epoch 220, training loss: 88.38166046142578 = 1.0168840885162354 + 10.0 * 8.736477851867676
Epoch 220, val loss: 1.0190194845199585
Epoch 230, training loss: 88.25068664550781 = 1.0132763385772705 + 10.0 * 8.72374153137207
Epoch 230, val loss: 1.0155739784240723
Epoch 240, training loss: 88.10882568359375 = 1.0095373392105103 + 10.0 * 8.709928512573242
Epoch 240, val loss: 1.0120232105255127
Epoch 250, training loss: 87.96621704101562 = 1.0056432485580444 + 10.0 * 8.696057319641113
Epoch 250, val loss: 1.0082814693450928
Epoch 260, training loss: 87.8037109375 = 1.0015223026275635 + 10.0 * 8.680218696594238
Epoch 260, val loss: 1.0044095516204834
Epoch 270, training loss: 87.64310455322266 = 0.9972282648086548 + 10.0 * 8.664587020874023
Epoch 270, val loss: 1.0003076791763306
Epoch 280, training loss: 87.55445861816406 = 0.9926390647888184 + 10.0 * 8.656182289123535
Epoch 280, val loss: 0.9959837794303894
Epoch 290, training loss: 87.37855529785156 = 0.9876679182052612 + 10.0 * 8.63908863067627
Epoch 290, val loss: 0.9911925792694092
Epoch 300, training loss: 87.26066589355469 = 0.9823160767555237 + 10.0 * 8.627835273742676
Epoch 300, val loss: 0.9860938787460327
Epoch 310, training loss: 87.16252136230469 = 0.9765996932983398 + 10.0 * 8.618592262268066
Epoch 310, val loss: 0.9806252121925354
Epoch 320, training loss: 87.0716781616211 = 0.9704464077949524 + 10.0 * 8.610123634338379
Epoch 320, val loss: 0.974751353263855
Epoch 330, training loss: 87.0030517578125 = 0.9638400077819824 + 10.0 * 8.603921890258789
Epoch 330, val loss: 0.968471884727478
Epoch 340, training loss: 86.93134307861328 = 0.9567704796791077 + 10.0 * 8.597456932067871
Epoch 340, val loss: 0.9617094397544861
Epoch 350, training loss: 86.86144256591797 = 0.9492788910865784 + 10.0 * 8.591216087341309
Epoch 350, val loss: 0.954566240310669
Epoch 360, training loss: 86.79088592529297 = 0.9413616061210632 + 10.0 * 8.584952354431152
Epoch 360, val loss: 0.9470029473304749
Epoch 370, training loss: 86.75940704345703 = 0.9330340623855591 + 10.0 * 8.582636833190918
Epoch 370, val loss: 0.9390884041786194
Epoch 380, training loss: 86.69073486328125 = 0.9243133068084717 + 10.0 * 8.576642036437988
Epoch 380, val loss: 0.9307323694229126
Epoch 390, training loss: 86.62854766845703 = 0.9152140617370605 + 10.0 * 8.571332931518555
Epoch 390, val loss: 0.9220782518386841
Epoch 400, training loss: 86.57501220703125 = 0.9057670831680298 + 10.0 * 8.566924095153809
Epoch 400, val loss: 0.9130923748016357
Epoch 410, training loss: 86.52324676513672 = 0.8959619402885437 + 10.0 * 8.562727928161621
Epoch 410, val loss: 0.9037742614746094
Epoch 420, training loss: 86.5005874633789 = 0.8858711123466492 + 10.0 * 8.561471939086914
Epoch 420, val loss: 0.8942134380340576
Epoch 430, training loss: 86.43356323242188 = 0.8754895925521851 + 10.0 * 8.555807113647461
Epoch 430, val loss: 0.884337842464447
Epoch 440, training loss: 86.39680480957031 = 0.8648844361305237 + 10.0 * 8.553192138671875
Epoch 440, val loss: 0.8743060231208801
Epoch 450, training loss: 86.34504699707031 = 0.8541417717933655 + 10.0 * 8.549090385437012
Epoch 450, val loss: 0.8641126155853271
Epoch 460, training loss: 86.31230163574219 = 0.8432806730270386 + 10.0 * 8.546902656555176
Epoch 460, val loss: 0.8538340330123901
Epoch 470, training loss: 86.2628173828125 = 0.832321047782898 + 10.0 * 8.543049812316895
Epoch 470, val loss: 0.8435128927230835
Epoch 480, training loss: 86.22419738769531 = 0.8213308453559875 + 10.0 * 8.540287017822266
Epoch 480, val loss: 0.8331718444824219
Epoch 490, training loss: 86.1898422241211 = 0.810368537902832 + 10.0 * 8.537946701049805
Epoch 490, val loss: 0.8228460550308228
Epoch 500, training loss: 86.15491485595703 = 0.7994648218154907 + 10.0 * 8.535545349121094
Epoch 500, val loss: 0.8126168251037598
Epoch 510, training loss: 86.12650299072266 = 0.7886427640914917 + 10.0 * 8.533785820007324
Epoch 510, val loss: 0.8024936318397522
Epoch 520, training loss: 86.08928680419922 = 0.7780110239982605 + 10.0 * 8.5311279296875
Epoch 520, val loss: 0.7925654053688049
Epoch 530, training loss: 86.05479431152344 = 0.7676118016242981 + 10.0 * 8.528718948364258
Epoch 530, val loss: 0.7828969359397888
Epoch 540, training loss: 86.03158569335938 = 0.7574529647827148 + 10.0 * 8.527413368225098
Epoch 540, val loss: 0.7734734416007996
Epoch 550, training loss: 85.99131774902344 = 0.7475610971450806 + 10.0 * 8.524375915527344
Epoch 550, val loss: 0.7643435597419739
Epoch 560, training loss: 85.96070861816406 = 0.7380432486534119 + 10.0 * 8.522266387939453
Epoch 560, val loss: 0.7555776834487915
Epoch 570, training loss: 85.93048095703125 = 0.7288742661476135 + 10.0 * 8.520160675048828
Epoch 570, val loss: 0.7471755743026733
Epoch 580, training loss: 85.92723083496094 = 0.720059335231781 + 10.0 * 8.520716667175293
Epoch 580, val loss: 0.7391358613967896
Epoch 590, training loss: 85.88925170898438 = 0.7115479111671448 + 10.0 * 8.517770767211914
Epoch 590, val loss: 0.7314158082008362
Epoch 600, training loss: 85.86922454833984 = 0.7034295797348022 + 10.0 * 8.516579627990723
Epoch 600, val loss: 0.724088728427887
Epoch 610, training loss: 85.85124969482422 = 0.6956268548965454 + 10.0 * 8.515562057495117
Epoch 610, val loss: 0.7170432209968567
Epoch 620, training loss: 85.81422424316406 = 0.6882023215293884 + 10.0 * 8.512601852416992
Epoch 620, val loss: 0.7104483842849731
Epoch 630, training loss: 85.791259765625 = 0.6811692118644714 + 10.0 * 8.511009216308594
Epoch 630, val loss: 0.7042168378829956
Epoch 640, training loss: 85.76822662353516 = 0.674461841583252 + 10.0 * 8.509376525878906
Epoch 640, val loss: 0.6983107328414917
Epoch 650, training loss: 85.7474136352539 = 0.6680583953857422 + 10.0 * 8.507935523986816
Epoch 650, val loss: 0.6926981210708618
Epoch 660, training loss: 85.7294692993164 = 0.66192227602005 + 10.0 * 8.506754875183105
Epoch 660, val loss: 0.6873693466186523
Epoch 670, training loss: 85.77111053466797 = 0.6560471057891846 + 10.0 * 8.511507034301758
Epoch 670, val loss: 0.6822935342788696
Epoch 680, training loss: 85.70756530761719 = 0.6504275798797607 + 10.0 * 8.50571346282959
Epoch 680, val loss: 0.6774734258651733
Epoch 690, training loss: 85.67828369140625 = 0.6451061367988586 + 10.0 * 8.503317832946777
Epoch 690, val loss: 0.672991156578064
Epoch 700, training loss: 85.65914154052734 = 0.6400627493858337 + 10.0 * 8.501908302307129
Epoch 700, val loss: 0.6687554717063904
Epoch 710, training loss: 85.64222717285156 = 0.6352390646934509 + 10.0 * 8.500699043273926
Epoch 710, val loss: 0.6647471785545349
Epoch 720, training loss: 85.62802124023438 = 0.6306183338165283 + 10.0 * 8.499740600585938
Epoch 720, val loss: 0.6609364748001099
Epoch 730, training loss: 85.6761245727539 = 0.6261895298957825 + 10.0 * 8.504993438720703
Epoch 730, val loss: 0.6573118567466736
Epoch 740, training loss: 85.61204528808594 = 0.621917724609375 + 10.0 * 8.49901294708252
Epoch 740, val loss: 0.6538116335868835
Epoch 750, training loss: 85.58247375488281 = 0.6178666353225708 + 10.0 * 8.496460914611816
Epoch 750, val loss: 0.6505427360534668
Epoch 760, training loss: 85.57066345214844 = 0.614007294178009 + 10.0 * 8.495665550231934
Epoch 760, val loss: 0.6474931836128235
Epoch 770, training loss: 85.55797576904297 = 0.6103118658065796 + 10.0 * 8.494766235351562
Epoch 770, val loss: 0.6445618271827698
Epoch 780, training loss: 85.56297302246094 = 0.6067367196083069 + 10.0 * 8.495623588562012
Epoch 780, val loss: 0.6418105363845825
Epoch 790, training loss: 85.53072357177734 = 0.603305995464325 + 10.0 * 8.492741584777832
Epoch 790, val loss: 0.6391360759735107
Epoch 800, training loss: 85.51945495605469 = 0.6000400185585022 + 10.0 * 8.491941452026367
Epoch 800, val loss: 0.6366077065467834
Epoch 810, training loss: 85.50641632080078 = 0.5968837738037109 + 10.0 * 8.49095344543457
Epoch 810, val loss: 0.6342471241950989
Epoch 820, training loss: 85.5302734375 = 0.5938377380371094 + 10.0 * 8.493642807006836
Epoch 820, val loss: 0.6319692134857178
Epoch 830, training loss: 85.49686431884766 = 0.5908703804016113 + 10.0 * 8.490598678588867
Epoch 830, val loss: 0.6296887397766113
Epoch 840, training loss: 85.4755859375 = 0.5880274772644043 + 10.0 * 8.48875617980957
Epoch 840, val loss: 0.6276093125343323
Epoch 850, training loss: 85.50926971435547 = 0.5852718949317932 + 10.0 * 8.492399215698242
Epoch 850, val loss: 0.6255835294723511
Epoch 860, training loss: 85.46279907226562 = 0.5825952887535095 + 10.0 * 8.488019943237305
Epoch 860, val loss: 0.6236289739608765
Epoch 870, training loss: 85.44094848632812 = 0.5800295472145081 + 10.0 * 8.486091613769531
Epoch 870, val loss: 0.6217697858810425
Epoch 880, training loss: 85.42930603027344 = 0.5775492787361145 + 10.0 * 8.485175132751465
Epoch 880, val loss: 0.620013415813446
Epoch 890, training loss: 85.41806030273438 = 0.57514488697052 + 10.0 * 8.484292030334473
Epoch 890, val loss: 0.6183157563209534
Epoch 900, training loss: 85.40936279296875 = 0.5728031396865845 + 10.0 * 8.48365592956543
Epoch 900, val loss: 0.616649329662323
Epoch 910, training loss: 85.46992492675781 = 0.5704857110977173 + 10.0 * 8.489943504333496
Epoch 910, val loss: 0.6150486469268799
Epoch 920, training loss: 85.40902709960938 = 0.5681691765785217 + 10.0 * 8.484086036682129
Epoch 920, val loss: 0.6134189963340759
Epoch 930, training loss: 85.38298034667969 = 0.565990686416626 + 10.0 * 8.481698989868164
Epoch 930, val loss: 0.61186283826828
Epoch 940, training loss: 85.37127685546875 = 0.5638863444328308 + 10.0 * 8.480738639831543
Epoch 940, val loss: 0.6103906631469727
Epoch 950, training loss: 85.36101531982422 = 0.5618389844894409 + 10.0 * 8.479917526245117
Epoch 950, val loss: 0.6089897751808167
Epoch 960, training loss: 85.35147094726562 = 0.5598148107528687 + 10.0 * 8.479166030883789
Epoch 960, val loss: 0.6076166033744812
Epoch 970, training loss: 85.34217071533203 = 0.55781090259552 + 10.0 * 8.478436470031738
Epoch 970, val loss: 0.6062445044517517
Epoch 980, training loss: 85.33866882324219 = 0.5558333396911621 + 10.0 * 8.478283882141113
Epoch 980, val loss: 0.6048845052719116
Epoch 990, training loss: 85.33592224121094 = 0.5538305044174194 + 10.0 * 8.478208541870117
Epoch 990, val loss: 0.6035376787185669
Epoch 1000, training loss: 85.3333969116211 = 0.5518598556518555 + 10.0 * 8.478154182434082
Epoch 1000, val loss: 0.6021488904953003
Epoch 1010, training loss: 85.31371307373047 = 0.5499576330184937 + 10.0 * 8.476375579833984
Epoch 1010, val loss: 0.600828230381012
Epoch 1020, training loss: 85.30050659179688 = 0.5480931997299194 + 10.0 * 8.475240707397461
Epoch 1020, val loss: 0.5995872616767883
Epoch 1030, training loss: 85.29350280761719 = 0.5462572574615479 + 10.0 * 8.474724769592285
Epoch 1030, val loss: 0.598314642906189
Epoch 1040, training loss: 85.3431167602539 = 0.5444092750549316 + 10.0 * 8.479870796203613
Epoch 1040, val loss: 0.5970435738563538
Epoch 1050, training loss: 85.27979278564453 = 0.5425384044647217 + 10.0 * 8.473725318908691
Epoch 1050, val loss: 0.595739483833313
Epoch 1060, training loss: 85.27222442626953 = 0.5407462120056152 + 10.0 * 8.47314739227295
Epoch 1060, val loss: 0.5945406556129456
Epoch 1070, training loss: 85.25898742675781 = 0.5389741659164429 + 10.0 * 8.472002029418945
Epoch 1070, val loss: 0.5932999849319458
Epoch 1080, training loss: 85.25150299072266 = 0.5372096300125122 + 10.0 * 8.471429824829102
Epoch 1080, val loss: 0.5920700430870056
Epoch 1090, training loss: 85.24376678466797 = 0.5354376435279846 + 10.0 * 8.470832824707031
Epoch 1090, val loss: 0.5908833146095276
Epoch 1100, training loss: 85.2362060546875 = 0.5336694121360779 + 10.0 * 8.470253944396973
Epoch 1100, val loss: 0.5896598696708679
Epoch 1110, training loss: 85.23550415039062 = 0.5318942070007324 + 10.0 * 8.470361709594727
Epoch 1110, val loss: 0.5884284973144531
Epoch 1120, training loss: 85.2585678100586 = 0.5300789475440979 + 10.0 * 8.472848892211914
Epoch 1120, val loss: 0.5872043371200562
Epoch 1130, training loss: 85.22001647949219 = 0.5282763242721558 + 10.0 * 8.4691743850708
Epoch 1130, val loss: 0.5859423279762268
Epoch 1140, training loss: 85.21381378173828 = 0.5265069007873535 + 10.0 * 8.468730926513672
Epoch 1140, val loss: 0.5847799181938171
Epoch 1150, training loss: 85.20277404785156 = 0.5247361063957214 + 10.0 * 8.467803955078125
Epoch 1150, val loss: 0.5835636854171753
Epoch 1160, training loss: 85.19512939453125 = 0.5229620337486267 + 10.0 * 8.467216491699219
Epoch 1160, val loss: 0.582335889339447
Epoch 1170, training loss: 85.19689178466797 = 0.5211817026138306 + 10.0 * 8.467571258544922
Epoch 1170, val loss: 0.5811453461647034
Epoch 1180, training loss: 85.19306945800781 = 0.5193657875061035 + 10.0 * 8.467370986938477
Epoch 1180, val loss: 0.5798528790473938
Epoch 1190, training loss: 85.18148803710938 = 0.51755291223526 + 10.0 * 8.46639347076416
Epoch 1190, val loss: 0.5786555409431458
Epoch 1200, training loss: 85.17335510253906 = 0.5157449245452881 + 10.0 * 8.465761184692383
Epoch 1200, val loss: 0.5773699283599854
Epoch 1210, training loss: 85.16956329345703 = 0.5139440298080444 + 10.0 * 8.465561866760254
Epoch 1210, val loss: 0.5761384963989258
Epoch 1220, training loss: 85.20890808105469 = 0.5120955109596252 + 10.0 * 8.469681739807129
Epoch 1220, val loss: 0.5749046802520752
Epoch 1230, training loss: 85.16376495361328 = 0.5102486610412598 + 10.0 * 8.465352058410645
Epoch 1230, val loss: 0.57362961769104
Epoch 1240, training loss: 85.14958953857422 = 0.5084186792373657 + 10.0 * 8.464117050170898
Epoch 1240, val loss: 0.5723803043365479
Epoch 1250, training loss: 85.1424789428711 = 0.5066154599189758 + 10.0 * 8.463586807250977
Epoch 1250, val loss: 0.5711831450462341
Epoch 1260, training loss: 85.13610076904297 = 0.5047829151153564 + 10.0 * 8.46313190460205
Epoch 1260, val loss: 0.5699490308761597
Epoch 1270, training loss: 85.13033294677734 = 0.5029363632202148 + 10.0 * 8.462739944458008
Epoch 1270, val loss: 0.5687113404273987
Epoch 1280, training loss: 85.14202880859375 = 0.5010687708854675 + 10.0 * 8.464096069335938
Epoch 1280, val loss: 0.5674691796302795
Epoch 1290, training loss: 85.13338470458984 = 0.49914050102233887 + 10.0 * 8.463424682617188
Epoch 1290, val loss: 0.5661134123802185
Epoch 1300, training loss: 85.12017822265625 = 0.4972248673439026 + 10.0 * 8.462295532226562
Epoch 1300, val loss: 0.5648736357688904
Epoch 1310, training loss: 85.11196899414062 = 0.4953171908855438 + 10.0 * 8.461665153503418
Epoch 1310, val loss: 0.5635519027709961
Epoch 1320, training loss: 85.10404205322266 = 0.49340152740478516 + 10.0 * 8.461064338684082
Epoch 1320, val loss: 0.5622966885566711
Epoch 1330, training loss: 85.12788391113281 = 0.4914548397064209 + 10.0 * 8.463643074035645
Epoch 1330, val loss: 0.5609717965126038
Epoch 1340, training loss: 85.10221099853516 = 0.48947352170944214 + 10.0 * 8.461274147033691
Epoch 1340, val loss: 0.5597554445266724
Epoch 1350, training loss: 85.0934066772461 = 0.4874916672706604 + 10.0 * 8.460591316223145
Epoch 1350, val loss: 0.5583873987197876
Epoch 1360, training loss: 85.08480072021484 = 0.48551520705223083 + 10.0 * 8.459928512573242
Epoch 1360, val loss: 0.5571303367614746
Epoch 1370, training loss: 85.07666015625 = 0.4835357964038849 + 10.0 * 8.459312438964844
Epoch 1370, val loss: 0.5558663010597229
Epoch 1380, training loss: 85.07103729248047 = 0.48154759407043457 + 10.0 * 8.458949089050293
Epoch 1380, val loss: 0.5546102523803711
Epoch 1390, training loss: 85.07195281982422 = 0.4795460104942322 + 10.0 * 8.459240913391113
Epoch 1390, val loss: 0.5533722639083862
Epoch 1400, training loss: 85.11913299560547 = 0.4774867296218872 + 10.0 * 8.464164733886719
Epoch 1400, val loss: 0.5520763397216797
Epoch 1410, training loss: 85.06504821777344 = 0.4754016101360321 + 10.0 * 8.458964347839355
Epoch 1410, val loss: 0.550808846950531
Epoch 1420, training loss: 85.05706024169922 = 0.47335702180862427 + 10.0 * 8.458370208740234
Epoch 1420, val loss: 0.5495879054069519
Epoch 1430, training loss: 85.0451431274414 = 0.47131380438804626 + 10.0 * 8.457383155822754
Epoch 1430, val loss: 0.5483118295669556
Epoch 1440, training loss: 85.03997039794922 = 0.4692738652229309 + 10.0 * 8.457069396972656
Epoch 1440, val loss: 0.5471128225326538
Epoch 1450, training loss: 85.03375244140625 = 0.4672112464904785 + 10.0 * 8.45665454864502
Epoch 1450, val loss: 0.5458855032920837
Epoch 1460, training loss: 85.03160095214844 = 0.465137779712677 + 10.0 * 8.456645965576172
Epoch 1460, val loss: 0.5446829199790955
Epoch 1470, training loss: 85.07357025146484 = 0.4630460739135742 + 10.0 * 8.461052894592285
Epoch 1470, val loss: 0.5435006618499756
Epoch 1480, training loss: 85.03179168701172 = 0.4609073996543884 + 10.0 * 8.457088470458984
Epoch 1480, val loss: 0.5423030853271484
Epoch 1490, training loss: 85.0157241821289 = 0.45880812406539917 + 10.0 * 8.45569133758545
Epoch 1490, val loss: 0.5410833358764648
Epoch 1500, training loss: 85.01310729980469 = 0.4567263126373291 + 10.0 * 8.45563793182373
Epoch 1500, val loss: 0.5400388836860657
Epoch 1510, training loss: 85.03250122070312 = 0.45463985204696655 + 10.0 * 8.457785606384277
Epoch 1510, val loss: 0.538918137550354
Epoch 1520, training loss: 84.99871063232422 = 0.45252296328544617 + 10.0 * 8.454618453979492
Epoch 1520, val loss: 0.5377631783485413
Epoch 1530, training loss: 84.99658203125 = 0.4504076838493347 + 10.0 * 8.454617500305176
Epoch 1530, val loss: 0.536674439907074
Epoch 1540, training loss: 84.98876190185547 = 0.44830766320228577 + 10.0 * 8.454045295715332
Epoch 1540, val loss: 0.5356432795524597
Epoch 1550, training loss: 84.99041748046875 = 0.44619718194007874 + 10.0 * 8.454421997070312
Epoch 1550, val loss: 0.5346055626869202
Epoch 1560, training loss: 84.98539733886719 = 0.44407033920288086 + 10.0 * 8.454133033752441
Epoch 1560, val loss: 0.5335781574249268
Epoch 1570, training loss: 84.98310089111328 = 0.44194212555885315 + 10.0 * 8.454115867614746
Epoch 1570, val loss: 0.5325525999069214
Epoch 1580, training loss: 85.01453399658203 = 0.43980953097343445 + 10.0 * 8.45747184753418
Epoch 1580, val loss: 0.5315552353858948
Epoch 1590, training loss: 84.97901916503906 = 0.4376797378063202 + 10.0 * 8.454133987426758
Epoch 1590, val loss: 0.5305187702178955
Epoch 1600, training loss: 84.96333312988281 = 0.4355807304382324 + 10.0 * 8.452775955200195
Epoch 1600, val loss: 0.5296344757080078
Epoch 1610, training loss: 84.95404815673828 = 0.43349528312683105 + 10.0 * 8.452054977416992
Epoch 1610, val loss: 0.5287076234817505
Epoch 1620, training loss: 84.9517593383789 = 0.4314211905002594 + 10.0 * 8.452033996582031
Epoch 1620, val loss: 0.5278390049934387
Epoch 1630, training loss: 84.99148559570312 = 0.42933687567710876 + 10.0 * 8.456214904785156
Epoch 1630, val loss: 0.5270417928695679
Epoch 1640, training loss: 84.9547119140625 = 0.4272301495075226 + 10.0 * 8.45274829864502
Epoch 1640, val loss: 0.5260029435157776
Epoch 1650, training loss: 84.93817138671875 = 0.42517393827438354 + 10.0 * 8.451299667358398
Epoch 1650, val loss: 0.5252849459648132
Epoch 1660, training loss: 84.93759155273438 = 0.4231422245502472 + 10.0 * 8.451444625854492
Epoch 1660, val loss: 0.5244507789611816
Epoch 1670, training loss: 84.92792510986328 = 0.42111828923225403 + 10.0 * 8.45068073272705
Epoch 1670, val loss: 0.5236586928367615
Epoch 1680, training loss: 84.93391418457031 = 0.41909536719322205 + 10.0 * 8.451481819152832
Epoch 1680, val loss: 0.5228825807571411
Epoch 1690, training loss: 84.92872619628906 = 0.4170721769332886 + 10.0 * 8.451166152954102
Epoch 1690, val loss: 0.522171676158905
Epoch 1700, training loss: 84.91403198242188 = 0.41505688428878784 + 10.0 * 8.449897766113281
Epoch 1700, val loss: 0.5214211344718933
Epoch 1710, training loss: 84.91060638427734 = 0.41306522488594055 + 10.0 * 8.44975471496582
Epoch 1710, val loss: 0.5207518935203552
Epoch 1720, training loss: 84.90938568115234 = 0.41109612584114075 + 10.0 * 8.4498291015625
Epoch 1720, val loss: 0.5200797319412231
Epoch 1730, training loss: 84.9037094116211 = 0.40912193059921265 + 10.0 * 8.449459075927734
Epoch 1730, val loss: 0.5193840861320496
Epoch 1740, training loss: 84.89593505859375 = 0.4071647822856903 + 10.0 * 8.448877334594727
Epoch 1740, val loss: 0.5187767744064331
Epoch 1750, training loss: 84.9540786743164 = 0.4052049219608307 + 10.0 * 8.454887390136719
Epoch 1750, val loss: 0.5180931687355042
Epoch 1760, training loss: 84.8998031616211 = 0.4032616913318634 + 10.0 * 8.449654579162598
Epoch 1760, val loss: 0.5174869298934937
Epoch 1770, training loss: 84.87924194335938 = 0.40134817361831665 + 10.0 * 8.447789192199707
Epoch 1770, val loss: 0.516889214515686
Epoch 1780, training loss: 84.87095642089844 = 0.3994568884372711 + 10.0 * 8.447149276733398
Epoch 1780, val loss: 0.5163242816925049
Epoch 1790, training loss: 84.86607360839844 = 0.39757758378982544 + 10.0 * 8.446849822998047
Epoch 1790, val loss: 0.5158101916313171
Epoch 1800, training loss: 84.86293029785156 = 0.3956945538520813 + 10.0 * 8.446723937988281
Epoch 1800, val loss: 0.5152595639228821
Epoch 1810, training loss: 84.90293884277344 = 0.39382433891296387 + 10.0 * 8.450911521911621
Epoch 1810, val loss: 0.5147783756256104
Epoch 1820, training loss: 84.87613677978516 = 0.3919658064842224 + 10.0 * 8.448416709899902
Epoch 1820, val loss: 0.5143079161643982
Epoch 1830, training loss: 84.85363006591797 = 0.3901246190071106 + 10.0 * 8.446351051330566
Epoch 1830, val loss: 0.5139279961585999
Epoch 1840, training loss: 84.84542846679688 = 0.3883019983768463 + 10.0 * 8.445712089538574
Epoch 1840, val loss: 0.5134254097938538
Epoch 1850, training loss: 84.83850860595703 = 0.3865119218826294 + 10.0 * 8.445199966430664
Epoch 1850, val loss: 0.5130615830421448
Epoch 1860, training loss: 84.8546371459961 = 0.3847183585166931 + 10.0 * 8.446991920471191
Epoch 1860, val loss: 0.5126646161079407
Epoch 1870, training loss: 84.83707427978516 = 0.3829137086868286 + 10.0 * 8.445416450500488
Epoch 1870, val loss: 0.5123999118804932
Epoch 1880, training loss: 84.82396697998047 = 0.3811357021331787 + 10.0 * 8.444283485412598
Epoch 1880, val loss: 0.5119389891624451
Epoch 1890, training loss: 84.81893920898438 = 0.3793907165527344 + 10.0 * 8.443955421447754
Epoch 1890, val loss: 0.5117090344429016
Epoch 1900, training loss: 84.81503295898438 = 0.377655565738678 + 10.0 * 8.443737983703613
Epoch 1900, val loss: 0.511375904083252
Epoch 1910, training loss: 84.82838439941406 = 0.3759296238422394 + 10.0 * 8.445245742797852
Epoch 1910, val loss: 0.5111276507377625
Epoch 1920, training loss: 84.8072280883789 = 0.3742099404335022 + 10.0 * 8.443302154541016
Epoch 1920, val loss: 0.510776162147522
Epoch 1930, training loss: 84.80257415771484 = 0.37250491976737976 + 10.0 * 8.44300651550293
Epoch 1930, val loss: 0.5105295777320862
Epoch 1940, training loss: 84.80525207519531 = 0.3708176016807556 + 10.0 * 8.443443298339844
Epoch 1940, val loss: 0.5102779865264893
Epoch 1950, training loss: 84.8133316040039 = 0.3691508173942566 + 10.0 * 8.444417953491211
Epoch 1950, val loss: 0.5100514888763428
Epoch 1960, training loss: 84.79166412353516 = 0.36748436093330383 + 10.0 * 8.442418098449707
Epoch 1960, val loss: 0.5098941326141357
Epoch 1970, training loss: 84.7887191772461 = 0.365850031375885 + 10.0 * 8.442286491394043
Epoch 1970, val loss: 0.509688138961792
Epoch 1980, training loss: 84.78794860839844 = 0.3642316460609436 + 10.0 * 8.442371368408203
Epoch 1980, val loss: 0.5095584988594055
Epoch 1990, training loss: 84.80338287353516 = 0.362626314163208 + 10.0 * 8.444075584411621
Epoch 1990, val loss: 0.5094576478004456
Epoch 2000, training loss: 84.77852630615234 = 0.3610135018825531 + 10.0 * 8.441751480102539
Epoch 2000, val loss: 0.5094016790390015
Epoch 2010, training loss: 84.7664566040039 = 0.3594229221343994 + 10.0 * 8.440703392028809
Epoch 2010, val loss: 0.5091938376426697
Epoch 2020, training loss: 84.76285552978516 = 0.3578603267669678 + 10.0 * 8.440500259399414
Epoch 2020, val loss: 0.5090969800949097
Epoch 2030, training loss: 84.75686645507812 = 0.35630473494529724 + 10.0 * 8.440055847167969
Epoch 2030, val loss: 0.5090847611427307
Epoch 2040, training loss: 84.78627014160156 = 0.35474613308906555 + 10.0 * 8.44315242767334
Epoch 2040, val loss: 0.5090210437774658
Epoch 2050, training loss: 84.75965118408203 = 0.3531818687915802 + 10.0 * 8.44064712524414
Epoch 2050, val loss: 0.5090591907501221
Epoch 2060, training loss: 84.76119995117188 = 0.35164955258369446 + 10.0 * 8.44095516204834
Epoch 2060, val loss: 0.5091192126274109
Epoch 2070, training loss: 84.75138854980469 = 0.3501063585281372 + 10.0 * 8.440128326416016
Epoch 2070, val loss: 0.509067952632904
Epoch 2080, training loss: 84.74015808105469 = 0.34859028458595276 + 10.0 * 8.439157485961914
Epoch 2080, val loss: 0.5091062784194946
Epoch 2090, training loss: 84.73233032226562 = 0.3470885157585144 + 10.0 * 8.43852424621582
Epoch 2090, val loss: 0.509190022945404
Epoch 2100, training loss: 84.73094177246094 = 0.3455846607685089 + 10.0 * 8.438535690307617
Epoch 2100, val loss: 0.509232759475708
Epoch 2110, training loss: 84.75505065917969 = 0.3440851867198944 + 10.0 * 8.441096305847168
Epoch 2110, val loss: 0.5092570781707764
Epoch 2120, training loss: 84.73418426513672 = 0.3425966799259186 + 10.0 * 8.43915843963623
Epoch 2120, val loss: 0.5093119144439697
Epoch 2130, training loss: 84.7423324584961 = 0.3411155939102173 + 10.0 * 8.4401216506958
Epoch 2130, val loss: 0.5095512270927429
Epoch 2140, training loss: 84.71605682373047 = 0.3396218717098236 + 10.0 * 8.437643051147461
Epoch 2140, val loss: 0.5096657872200012
Epoch 2150, training loss: 84.71365356445312 = 0.3381507694721222 + 10.0 * 8.43755054473877
Epoch 2150, val loss: 0.5098901987075806
Epoch 2160, training loss: 84.74564361572266 = 0.3366891145706177 + 10.0 * 8.440895080566406
Epoch 2160, val loss: 0.5100197792053223
Epoch 2170, training loss: 84.71186065673828 = 0.3352277874946594 + 10.0 * 8.437663078308105
Epoch 2170, val loss: 0.5102823376655579
Epoch 2180, training loss: 84.70504760742188 = 0.33376970887184143 + 10.0 * 8.437128067016602
Epoch 2180, val loss: 0.5104942917823792
Epoch 2190, training loss: 84.69699096679688 = 0.33233365416526794 + 10.0 * 8.4364652633667
Epoch 2190, val loss: 0.5107333064079285
Epoch 2200, training loss: 84.69139862060547 = 0.3309020400047302 + 10.0 * 8.436049461364746
Epoch 2200, val loss: 0.5109435319900513
Epoch 2210, training loss: 84.69390869140625 = 0.3294755816459656 + 10.0 * 8.436443328857422
Epoch 2210, val loss: 0.511185884475708
Epoch 2220, training loss: 84.73544311523438 = 0.3280491828918457 + 10.0 * 8.440739631652832
Epoch 2220, val loss: 0.5115131735801697
Epoch 2230, training loss: 84.69563293457031 = 0.32662975788116455 + 10.0 * 8.43690013885498
Epoch 2230, val loss: 0.5115799307823181
Epoch 2240, training loss: 84.68168640136719 = 0.3252163827419281 + 10.0 * 8.435647010803223
Epoch 2240, val loss: 0.512093722820282
Epoch 2250, training loss: 84.67395782470703 = 0.32381558418273926 + 10.0 * 8.435014724731445
Epoch 2250, val loss: 0.5123050808906555
Epoch 2260, training loss: 84.6732177734375 = 0.32242053747177124 + 10.0 * 8.435079574584961
Epoch 2260, val loss: 0.5127328038215637
Epoch 2270, training loss: 84.7152328491211 = 0.3210415542125702 + 10.0 * 8.43941879272461
Epoch 2270, val loss: 0.5131773948669434
Epoch 2280, training loss: 84.68582153320312 = 0.31960976123809814 + 10.0 * 8.436620712280273
Epoch 2280, val loss: 0.5133919715881348
Epoch 2290, training loss: 84.66361999511719 = 0.31822484731674194 + 10.0 * 8.434539794921875
Epoch 2290, val loss: 0.5137758255004883
Epoch 2300, training loss: 84.65764617919922 = 0.3168381452560425 + 10.0 * 8.434080123901367
Epoch 2300, val loss: 0.5142042636871338
Epoch 2310, training loss: 84.65452575683594 = 0.3154541850090027 + 10.0 * 8.433907508850098
Epoch 2310, val loss: 0.5144739151000977
Epoch 2320, training loss: 84.67810821533203 = 0.314075767993927 + 10.0 * 8.436403274536133
Epoch 2320, val loss: 0.5148681402206421
Epoch 2330, training loss: 84.65151977539062 = 0.31268447637557983 + 10.0 * 8.433883666992188
Epoch 2330, val loss: 0.5152945518493652
Epoch 2340, training loss: 84.64691162109375 = 0.3112952709197998 + 10.0 * 8.433561325073242
Epoch 2340, val loss: 0.5156660676002502
Epoch 2350, training loss: 84.64221954345703 = 0.3099067807197571 + 10.0 * 8.433231353759766
Epoch 2350, val loss: 0.5160806179046631
Epoch 2360, training loss: 84.63902282714844 = 0.3085315227508545 + 10.0 * 8.433049201965332
Epoch 2360, val loss: 0.5165266990661621
Epoch 2370, training loss: 84.64900970458984 = 0.3071598708629608 + 10.0 * 8.434185028076172
Epoch 2370, val loss: 0.5169767737388611
Epoch 2380, training loss: 84.64547729492188 = 0.30579277873039246 + 10.0 * 8.433968544006348
Epoch 2380, val loss: 0.5174252986907959
Epoch 2390, training loss: 84.63430786132812 = 0.3044271469116211 + 10.0 * 8.432988166809082
Epoch 2390, val loss: 0.5180208086967468
Epoch 2400, training loss: 84.63069152832031 = 0.3030785322189331 + 10.0 * 8.432761192321777
Epoch 2400, val loss: 0.5183513760566711
Epoch 2410, training loss: 84.63438415527344 = 0.3017352223396301 + 10.0 * 8.43326473236084
Epoch 2410, val loss: 0.5189599394798279
Epoch 2420, training loss: 84.62405395507812 = 0.30038878321647644 + 10.0 * 8.432366371154785
Epoch 2420, val loss: 0.5193638801574707
Epoch 2430, training loss: 84.618896484375 = 0.29904627799987793 + 10.0 * 8.431984901428223
Epoch 2430, val loss: 0.5200175046920776
Epoch 2440, training loss: 84.65904998779297 = 0.29770538210868835 + 10.0 * 8.436134338378906
Epoch 2440, val loss: 0.5205758213996887
Epoch 2450, training loss: 84.61824035644531 = 0.29637420177459717 + 10.0 * 8.4321870803833
Epoch 2450, val loss: 0.5210816860198975
Epoch 2460, training loss: 84.60872650146484 = 0.2950354218482971 + 10.0 * 8.431368827819824
Epoch 2460, val loss: 0.5217341184616089
Epoch 2470, training loss: 84.60346221923828 = 0.29370877146720886 + 10.0 * 8.430974960327148
Epoch 2470, val loss: 0.5223100185394287
Epoch 2480, training loss: 84.603515625 = 0.29238927364349365 + 10.0 * 8.431112289428711
Epoch 2480, val loss: 0.5229938626289368
Epoch 2490, training loss: 84.62588500976562 = 0.2910647392272949 + 10.0 * 8.43348217010498
Epoch 2490, val loss: 0.5236908793449402
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8016235413495687
0.8198217778743752
=== training gcn model ===
Epoch 0, training loss: 106.9186019897461 = 1.0955424308776855 + 10.0 * 10.582305908203125
Epoch 0, val loss: 1.094114065170288
Epoch 10, training loss: 106.9122085571289 = 1.091123342514038 + 10.0 * 10.582108497619629
Epoch 10, val loss: 1.0897314548492432
Epoch 20, training loss: 106.8985824584961 = 1.086392879486084 + 10.0 * 10.581218719482422
Epoch 20, val loss: 1.08504319190979
Epoch 30, training loss: 106.85133361816406 = 1.0812820196151733 + 10.0 * 10.577005386352539
Epoch 30, val loss: 1.0800060033798218
Epoch 40, training loss: 106.66492462158203 = 1.0757967233657837 + 10.0 * 10.55891227722168
Epoch 40, val loss: 1.0746315717697144
Epoch 50, training loss: 106.05121612548828 = 1.0699442625045776 + 10.0 * 10.498126983642578
Epoch 50, val loss: 1.0689677000045776
Epoch 60, training loss: 104.42974090576172 = 1.0647252798080444 + 10.0 * 10.336501121520996
Epoch 60, val loss: 1.064107060432434
Epoch 70, training loss: 102.06755065917969 = 1.0601000785827637 + 10.0 * 10.10074520111084
Epoch 70, val loss: 1.0597813129425049
Epoch 80, training loss: 100.21630859375 = 1.056219220161438 + 10.0 * 9.916008949279785
Epoch 80, val loss: 1.0561668872833252
Epoch 90, training loss: 97.322265625 = 1.0523532629013062 + 10.0 * 9.626991271972656
Epoch 90, val loss: 1.052513837814331
Epoch 100, training loss: 95.30096435546875 = 1.048302412033081 + 10.0 * 9.42526626586914
Epoch 100, val loss: 1.0486443042755127
Epoch 110, training loss: 94.22212982177734 = 1.0444177389144897 + 10.0 * 9.317770957946777
Epoch 110, val loss: 1.0448392629623413
Epoch 120, training loss: 93.6475830078125 = 1.0406646728515625 + 10.0 * 9.26069164276123
Epoch 120, val loss: 1.0411899089813232
Epoch 130, training loss: 92.76594543457031 = 1.0378481149673462 + 10.0 * 9.172809600830078
Epoch 130, val loss: 1.0385600328445435
Epoch 140, training loss: 91.50163269042969 = 1.0358120203018188 + 10.0 * 9.046582221984863
Epoch 140, val loss: 1.036664605140686
Epoch 150, training loss: 90.66578674316406 = 1.0339016914367676 + 10.0 * 8.963188171386719
Epoch 150, val loss: 1.0347868204116821
Epoch 160, training loss: 90.135009765625 = 1.031477928161621 + 10.0 * 8.91035270690918
Epoch 160, val loss: 1.032369613647461
Epoch 170, training loss: 89.69847869873047 = 1.0284392833709717 + 10.0 * 8.867003440856934
Epoch 170, val loss: 1.0293461084365845
Epoch 180, training loss: 89.29425048828125 = 1.0250271558761597 + 10.0 * 8.826922416687012
Epoch 180, val loss: 1.026010513305664
Epoch 190, training loss: 88.96339416503906 = 1.0215712785720825 + 10.0 * 8.794182777404785
Epoch 190, val loss: 1.022673487663269
Epoch 200, training loss: 88.7137222290039 = 1.0179991722106934 + 10.0 * 8.769572257995605
Epoch 200, val loss: 1.0191656351089478
Epoch 210, training loss: 88.49502563476562 = 1.0141857862472534 + 10.0 * 8.74808406829834
Epoch 210, val loss: 1.0153782367706299
Epoch 220, training loss: 88.33012390136719 = 1.0100404024124146 + 10.0 * 8.73200798034668
Epoch 220, val loss: 1.0113041400909424
Epoch 230, training loss: 88.204345703125 = 1.0055493116378784 + 10.0 * 8.719880104064941
Epoch 230, val loss: 1.0069001913070679
Epoch 240, training loss: 88.09540557861328 = 1.0006897449493408 + 10.0 * 8.709471702575684
Epoch 240, val loss: 1.0021392107009888
Epoch 250, training loss: 88.01551055908203 = 0.9953858256340027 + 10.0 * 8.70201301574707
Epoch 250, val loss: 0.9969531893730164
Epoch 260, training loss: 87.9074478149414 = 0.989582896232605 + 10.0 * 8.69178581237793
Epoch 260, val loss: 0.9912922382354736
Epoch 270, training loss: 87.80509948730469 = 0.9834309220314026 + 10.0 * 8.682167053222656
Epoch 270, val loss: 0.9852932691574097
Epoch 280, training loss: 87.70226287841797 = 0.9769665002822876 + 10.0 * 8.672529220581055
Epoch 280, val loss: 0.9790003895759583
Epoch 290, training loss: 87.61817932128906 = 0.9701155424118042 + 10.0 * 8.664806365966797
Epoch 290, val loss: 0.9723131060600281
Epoch 300, training loss: 87.52532196044922 = 0.9627437591552734 + 10.0 * 8.656257629394531
Epoch 300, val loss: 0.9651583433151245
Epoch 310, training loss: 87.442138671875 = 0.9549326300621033 + 10.0 * 8.648720741271973
Epoch 310, val loss: 0.9575628042221069
Epoch 320, training loss: 87.35464477539062 = 0.9466109275817871 + 10.0 * 8.640803337097168
Epoch 320, val loss: 0.9494781494140625
Epoch 330, training loss: 87.27774047851562 = 0.9378235340118408 + 10.0 * 8.633992195129395
Epoch 330, val loss: 0.9409671425819397
Epoch 340, training loss: 87.19669342041016 = 0.9285464286804199 + 10.0 * 8.626814842224121
Epoch 340, val loss: 0.9319934248924255
Epoch 350, training loss: 87.12377166748047 = 0.9187878966331482 + 10.0 * 8.620498657226562
Epoch 350, val loss: 0.9225853085517883
Epoch 360, training loss: 87.04887390136719 = 0.9086118936538696 + 10.0 * 8.614026069641113
Epoch 360, val loss: 0.9127951860427856
Epoch 370, training loss: 86.98538970947266 = 0.8980806469917297 + 10.0 * 8.608731269836426
Epoch 370, val loss: 0.9026739001274109
Epoch 380, training loss: 86.90750885009766 = 0.8871490359306335 + 10.0 * 8.602036476135254
Epoch 380, val loss: 0.8922306895256042
Epoch 390, training loss: 86.83555603027344 = 0.875942051410675 + 10.0 * 8.595961570739746
Epoch 390, val loss: 0.8815293312072754
Epoch 400, training loss: 86.77523040771484 = 0.8645747303962708 + 10.0 * 8.591065406799316
Epoch 400, val loss: 0.8706848621368408
Epoch 410, training loss: 86.71500396728516 = 0.8529765009880066 + 10.0 * 8.586202621459961
Epoch 410, val loss: 0.8596876859664917
Epoch 420, training loss: 86.65240478515625 = 0.8412572741508484 + 10.0 * 8.581114768981934
Epoch 420, val loss: 0.8485731482505798
Epoch 430, training loss: 86.59489440917969 = 0.8295164108276367 + 10.0 * 8.5765380859375
Epoch 430, val loss: 0.837457001209259
Epoch 440, training loss: 86.5433349609375 = 0.8177912831306458 + 10.0 * 8.572553634643555
Epoch 440, val loss: 0.8263927102088928
Epoch 450, training loss: 86.4957275390625 = 0.8061060905456543 + 10.0 * 8.568962097167969
Epoch 450, val loss: 0.8153732419013977
Epoch 460, training loss: 86.44242095947266 = 0.7944847941398621 + 10.0 * 8.564793586730957
Epoch 460, val loss: 0.8044977188110352
Epoch 470, training loss: 86.39756774902344 = 0.7830156683921814 + 10.0 * 8.561455726623535
Epoch 470, val loss: 0.7937952876091003
Epoch 480, training loss: 86.35752868652344 = 0.7717424035072327 + 10.0 * 8.558578491210938
Epoch 480, val loss: 0.7833042144775391
Epoch 490, training loss: 86.33282470703125 = 0.7606225609779358 + 10.0 * 8.557220458984375
Epoch 490, val loss: 0.7729559540748596
Epoch 500, training loss: 86.296630859375 = 0.7497671842575073 + 10.0 * 8.554686546325684
Epoch 500, val loss: 0.762947142124176
Epoch 510, training loss: 86.24512481689453 = 0.7392776012420654 + 10.0 * 8.55058479309082
Epoch 510, val loss: 0.7532796263694763
Epoch 520, training loss: 86.20966339111328 = 0.7290971279144287 + 10.0 * 8.548056602478027
Epoch 520, val loss: 0.7439200282096863
Epoch 530, training loss: 86.20120239257812 = 0.7192496657371521 + 10.0 * 8.548195838928223
Epoch 530, val loss: 0.7349278926849365
Epoch 540, training loss: 86.19524383544922 = 0.7097225785255432 + 10.0 * 8.548551559448242
Epoch 540, val loss: 0.7262111306190491
Epoch 550, training loss: 86.12494659423828 = 0.7006050944328308 + 10.0 * 8.542433738708496
Epoch 550, val loss: 0.7179802656173706
Epoch 560, training loss: 86.08934020996094 = 0.6919130682945251 + 10.0 * 8.539743423461914
Epoch 560, val loss: 0.7101522088050842
Epoch 570, training loss: 86.06024932861328 = 0.683598518371582 + 10.0 * 8.537664413452148
Epoch 570, val loss: 0.7026841640472412
Epoch 580, training loss: 86.03338623046875 = 0.6756536960601807 + 10.0 * 8.535773277282715
Epoch 580, val loss: 0.6955990791320801
Epoch 590, training loss: 86.00726318359375 = 0.6680904626846313 + 10.0 * 8.533917427062988
Epoch 590, val loss: 0.6888984441757202
Epoch 600, training loss: 85.98384857177734 = 0.6608995199203491 + 10.0 * 8.532295227050781
Epoch 600, val loss: 0.6825456619262695
Epoch 610, training loss: 86.00055694580078 = 0.6540362238883972 + 10.0 * 8.534651756286621
Epoch 610, val loss: 0.6765157580375671
Epoch 620, training loss: 85.94676971435547 = 0.6474819183349609 + 10.0 * 8.529928207397461
Epoch 620, val loss: 0.6708226799964905
Epoch 630, training loss: 85.91816711425781 = 0.6413052082061768 + 10.0 * 8.52768611907959
Epoch 630, val loss: 0.6654947996139526
Epoch 640, training loss: 85.88980865478516 = 0.6354536414146423 + 10.0 * 8.525435447692871
Epoch 640, val loss: 0.6604713201522827
Epoch 650, training loss: 85.86869049072266 = 0.6298749446868896 + 10.0 * 8.523881912231445
Epoch 650, val loss: 0.6557616591453552
Epoch 660, training loss: 85.85547637939453 = 0.6245785355567932 + 10.0 * 8.523089408874512
Epoch 660, val loss: 0.651320219039917
Epoch 670, training loss: 85.85289764404297 = 0.6195046901702881 + 10.0 * 8.52333927154541
Epoch 670, val loss: 0.6470744609832764
Epoch 680, training loss: 85.82217407226562 = 0.614700198173523 + 10.0 * 8.520747184753418
Epoch 680, val loss: 0.6431311964988708
Epoch 690, training loss: 85.81372833251953 = 0.6101285815238953 + 10.0 * 8.520359992980957
Epoch 690, val loss: 0.6394124627113342
Epoch 700, training loss: 85.7801742553711 = 0.6058099269866943 + 10.0 * 8.517436027526855
Epoch 700, val loss: 0.6359195113182068
Epoch 710, training loss: 85.76140594482422 = 0.601707935333252 + 10.0 * 8.515970230102539
Epoch 710, val loss: 0.6326574087142944
Epoch 720, training loss: 85.74100494384766 = 0.5977990031242371 + 10.0 * 8.514320373535156
Epoch 720, val loss: 0.6295809149742126
Epoch 730, training loss: 85.73429107666016 = 0.5940720438957214 + 10.0 * 8.514021873474121
Epoch 730, val loss: 0.6266785860061646
Epoch 740, training loss: 85.73505401611328 = 0.5904877185821533 + 10.0 * 8.514456748962402
Epoch 740, val loss: 0.6239194869995117
Epoch 750, training loss: 85.70167541503906 = 0.587071418762207 + 10.0 * 8.511460304260254
Epoch 750, val loss: 0.621360182762146
Epoch 760, training loss: 85.68244171142578 = 0.5838341116905212 + 10.0 * 8.50986099243164
Epoch 760, val loss: 0.6189126968383789
Epoch 770, training loss: 85.66839599609375 = 0.580747663974762 + 10.0 * 8.508764266967773
Epoch 770, val loss: 0.6166397333145142
Epoch 780, training loss: 85.67465209960938 = 0.5777931213378906 + 10.0 * 8.509686470031738
Epoch 780, val loss: 0.6145010590553284
Epoch 790, training loss: 85.66709899902344 = 0.5749174952507019 + 10.0 * 8.509218215942383
Epoch 790, val loss: 0.6124235391616821
Epoch 800, training loss: 85.63917541503906 = 0.5721824765205383 + 10.0 * 8.506699562072754
Epoch 800, val loss: 0.6104751825332642
Epoch 810, training loss: 85.6148910522461 = 0.5695703625679016 + 10.0 * 8.504531860351562
Epoch 810, val loss: 0.6086247563362122
Epoch 820, training loss: 85.60269927978516 = 0.5670697689056396 + 10.0 * 8.503562927246094
Epoch 820, val loss: 0.6069107055664062
Epoch 830, training loss: 85.61357116699219 = 0.564655601978302 + 10.0 * 8.504891395568848
Epoch 830, val loss: 0.6052667498588562
Epoch 840, training loss: 85.57872009277344 = 0.5622991919517517 + 10.0 * 8.501642227172852
Epoch 840, val loss: 0.6036869287490845
Epoch 850, training loss: 85.57230377197266 = 0.5600351095199585 + 10.0 * 8.501226425170898
Epoch 850, val loss: 0.6021518707275391
Epoch 860, training loss: 85.55794525146484 = 0.5578653812408447 + 10.0 * 8.500007629394531
Epoch 860, val loss: 0.6007202863693237
Epoch 870, training loss: 85.54417419433594 = 0.5557676553726196 + 10.0 * 8.49884033203125
Epoch 870, val loss: 0.5993738174438477
Epoch 880, training loss: 85.5436019897461 = 0.5537419319152832 + 10.0 * 8.49898624420166
Epoch 880, val loss: 0.5980677008628845
Epoch 890, training loss: 85.5295639038086 = 0.5517510771751404 + 10.0 * 8.497781753540039
Epoch 890, val loss: 0.596809446811676
Epoch 900, training loss: 85.51919555664062 = 0.5498244762420654 + 10.0 * 8.496936798095703
Epoch 900, val loss: 0.5955889225006104
Epoch 910, training loss: 85.5046157836914 = 0.54796302318573 + 10.0 * 8.495664596557617
Epoch 910, val loss: 0.5944207906723022
Epoch 920, training loss: 85.49581146240234 = 0.546161949634552 + 10.0 * 8.494964599609375
Epoch 920, val loss: 0.5933239459991455
Epoch 930, training loss: 85.56421661376953 = 0.5443976521492004 + 10.0 * 8.501981735229492
Epoch 930, val loss: 0.5922424793243408
Epoch 940, training loss: 85.47732543945312 = 0.5426662564277649 + 10.0 * 8.4934663772583
Epoch 940, val loss: 0.5912346839904785
Epoch 950, training loss: 85.46633911132812 = 0.5409967303276062 + 10.0 * 8.492533683776855
Epoch 950, val loss: 0.5901791453361511
Epoch 960, training loss: 85.4544677734375 = 0.539380669593811 + 10.0 * 8.491508483886719
Epoch 960, val loss: 0.5892596244812012
Epoch 970, training loss: 85.44266510009766 = 0.5378039479255676 + 10.0 * 8.490486145019531
Epoch 970, val loss: 0.5883557796478271
Epoch 980, training loss: 85.43221282958984 = 0.5362603068351746 + 10.0 * 8.489595413208008
Epoch 980, val loss: 0.5874356627464294
Epoch 990, training loss: 85.46379852294922 = 0.5347471833229065 + 10.0 * 8.492905616760254
Epoch 990, val loss: 0.5865504741668701
Epoch 1000, training loss: 85.44542694091797 = 0.5332316756248474 + 10.0 * 8.491219520568848
Epoch 1000, val loss: 0.5857064127922058
Epoch 1010, training loss: 85.41325378417969 = 0.5317534804344177 + 10.0 * 8.488149642944336
Epoch 1010, val loss: 0.5848574042320251
Epoch 1020, training loss: 85.39987182617188 = 0.5303183197975159 + 10.0 * 8.486955642700195
Epoch 1020, val loss: 0.5840256214141846
Epoch 1030, training loss: 85.40181732177734 = 0.5289160013198853 + 10.0 * 8.487290382385254
Epoch 1030, val loss: 0.5832502245903015
Epoch 1040, training loss: 85.3975601196289 = 0.5275272130966187 + 10.0 * 8.487003326416016
Epoch 1040, val loss: 0.5824487805366516
Epoch 1050, training loss: 85.37657928466797 = 0.5261639356613159 + 10.0 * 8.485041618347168
Epoch 1050, val loss: 0.5816873908042908
Epoch 1060, training loss: 85.36665344238281 = 0.5248284935951233 + 10.0 * 8.484182357788086
Epoch 1060, val loss: 0.5809326767921448
Epoch 1070, training loss: 85.35660552978516 = 0.5235192775726318 + 10.0 * 8.483308792114258
Epoch 1070, val loss: 0.5802104473114014
Epoch 1080, training loss: 85.37996673583984 = 0.5222238302230835 + 10.0 * 8.485774040222168
Epoch 1080, val loss: 0.5794870853424072
Epoch 1090, training loss: 85.350830078125 = 0.5209370851516724 + 10.0 * 8.482989311218262
Epoch 1090, val loss: 0.5787317752838135
Epoch 1100, training loss: 85.36270141601562 = 0.5196680426597595 + 10.0 * 8.48430347442627
Epoch 1100, val loss: 0.5780203342437744
Epoch 1110, training loss: 85.33361053466797 = 0.5184124112129211 + 10.0 * 8.48151969909668
Epoch 1110, val loss: 0.5773142576217651
Epoch 1120, training loss: 85.31952667236328 = 0.5171884298324585 + 10.0 * 8.480234146118164
Epoch 1120, val loss: 0.5766242742538452
Epoch 1130, training loss: 85.31112670898438 = 0.5159888863563538 + 10.0 * 8.479513168334961
Epoch 1130, val loss: 0.5759774446487427
Epoch 1140, training loss: 85.30205535888672 = 0.5148070454597473 + 10.0 * 8.478724479675293
Epoch 1140, val loss: 0.5753251910209656
Epoch 1150, training loss: 85.2982406616211 = 0.5136355757713318 + 10.0 * 8.478460311889648
Epoch 1150, val loss: 0.5746871829032898
Epoch 1160, training loss: 85.35026550292969 = 0.5124600529670715 + 10.0 * 8.483780860900879
Epoch 1160, val loss: 0.5740217566490173
Epoch 1170, training loss: 85.30017852783203 = 0.5112907886505127 + 10.0 * 8.478888511657715
Epoch 1170, val loss: 0.5733685493469238
Epoch 1180, training loss: 85.28034210205078 = 0.5101319551467896 + 10.0 * 8.477021217346191
Epoch 1180, val loss: 0.5726834535598755
Epoch 1190, training loss: 85.26769256591797 = 0.5090040564537048 + 10.0 * 8.475869178771973
Epoch 1190, val loss: 0.5720506310462952
Epoch 1200, training loss: 85.25973510742188 = 0.507885217666626 + 10.0 * 8.475184440612793
Epoch 1200, val loss: 0.5714413523674011
Epoch 1210, training loss: 85.27554321289062 = 0.5067727565765381 + 10.0 * 8.476877212524414
Epoch 1210, val loss: 0.5708214044570923
Epoch 1220, training loss: 85.27801513671875 = 0.5056391358375549 + 10.0 * 8.477237701416016
Epoch 1220, val loss: 0.5701379179954529
Epoch 1230, training loss: 85.2423095703125 = 0.5045228004455566 + 10.0 * 8.47377872467041
Epoch 1230, val loss: 0.5695108771324158
Epoch 1240, training loss: 85.2385482788086 = 0.5034273862838745 + 10.0 * 8.473512649536133
Epoch 1240, val loss: 0.5688761472702026
Epoch 1250, training loss: 85.2365493774414 = 0.5023459792137146 + 10.0 * 8.473421096801758
Epoch 1250, val loss: 0.5682564973831177
Epoch 1260, training loss: 85.28215026855469 = 0.5012639760971069 + 10.0 * 8.47808837890625
Epoch 1260, val loss: 0.5676285624504089
Epoch 1270, training loss: 85.23686981201172 = 0.5001609921455383 + 10.0 * 8.473670959472656
Epoch 1270, val loss: 0.566970944404602
Epoch 1280, training loss: 85.21894073486328 = 0.49909284710884094 + 10.0 * 8.47198486328125
Epoch 1280, val loss: 0.5663242936134338
Epoch 1290, training loss: 85.20741271972656 = 0.4980233907699585 + 10.0 * 8.470938682556152
Epoch 1290, val loss: 0.565714955329895
Epoch 1300, training loss: 85.20170593261719 = 0.4969635307788849 + 10.0 * 8.470474243164062
Epoch 1300, val loss: 0.5650919079780579
Epoch 1310, training loss: 85.2177963256836 = 0.49590542912483215 + 10.0 * 8.472188949584961
Epoch 1310, val loss: 0.5644763708114624
Epoch 1320, training loss: 85.19740295410156 = 0.49482640624046326 + 10.0 * 8.470257759094238
Epoch 1320, val loss: 0.5638253092765808
Epoch 1330, training loss: 85.18879699707031 = 0.49376219511032104 + 10.0 * 8.469503402709961
Epoch 1330, val loss: 0.5632109045982361
Epoch 1340, training loss: 85.18187713623047 = 0.49270620942115784 + 10.0 * 8.468916893005371
Epoch 1340, val loss: 0.5626186728477478
Epoch 1350, training loss: 85.17617797851562 = 0.4916619658470154 + 10.0 * 8.468451499938965
Epoch 1350, val loss: 0.5620203018188477
Epoch 1360, training loss: 85.18577575683594 = 0.49061572551727295 + 10.0 * 8.469515800476074
Epoch 1360, val loss: 0.5614237189292908
Epoch 1370, training loss: 85.1905746459961 = 0.4895516037940979 + 10.0 * 8.470102310180664
Epoch 1370, val loss: 0.5608149766921997
Epoch 1380, training loss: 85.17933654785156 = 0.488467812538147 + 10.0 * 8.469087600708008
Epoch 1380, val loss: 0.5601716637611389
Epoch 1390, training loss: 85.16030883789062 = 0.4873996675014496 + 10.0 * 8.467290878295898
Epoch 1390, val loss: 0.5594993233680725
Epoch 1400, training loss: 85.15616607666016 = 0.48635557293891907 + 10.0 * 8.466980934143066
Epoch 1400, val loss: 0.5589005947113037
Epoch 1410, training loss: 85.14813995361328 = 0.485323041677475 + 10.0 * 8.46628189086914
Epoch 1410, val loss: 0.5583069324493408
Epoch 1420, training loss: 85.14310455322266 = 0.4842972457408905 + 10.0 * 8.465880393981934
Epoch 1420, val loss: 0.5577000379562378
Epoch 1430, training loss: 85.13843536376953 = 0.4832671582698822 + 10.0 * 8.465517044067383
Epoch 1430, val loss: 0.5571366548538208
Epoch 1440, training loss: 85.1346435546875 = 0.4822339117527008 + 10.0 * 8.465241432189941
Epoch 1440, val loss: 0.5565528869628906
Epoch 1450, training loss: 85.14747619628906 = 0.4811958968639374 + 10.0 * 8.466628074645996
Epoch 1450, val loss: 0.555953860282898
Epoch 1460, training loss: 85.14552307128906 = 0.4801325500011444 + 10.0 * 8.46653938293457
Epoch 1460, val loss: 0.5553491711616516
Epoch 1470, training loss: 85.12700653076172 = 0.47906631231307983 + 10.0 * 8.464794158935547
Epoch 1470, val loss: 0.5547139644622803
Epoch 1480, training loss: 85.11803436279297 = 0.4780206084251404 + 10.0 * 8.464001655578613
Epoch 1480, val loss: 0.5541041493415833
Epoch 1490, training loss: 85.11296844482422 = 0.47697678208351135 + 10.0 * 8.46359920501709
Epoch 1490, val loss: 0.5535096526145935
Epoch 1500, training loss: 85.11003112792969 = 0.4759325087070465 + 10.0 * 8.463410377502441
Epoch 1500, val loss: 0.5529077649116516
Epoch 1510, training loss: 85.13983917236328 = 0.47487765550613403 + 10.0 * 8.466496467590332
Epoch 1510, val loss: 0.5523041486740112
Epoch 1520, training loss: 85.12606811523438 = 0.4738067388534546 + 10.0 * 8.465226173400879
Epoch 1520, val loss: 0.5516798496246338
Epoch 1530, training loss: 85.09727478027344 = 0.47272834181785583 + 10.0 * 8.462454795837402
Epoch 1530, val loss: 0.5510443449020386
Epoch 1540, training loss: 85.09716796875 = 0.471661776304245 + 10.0 * 8.462550163269043
Epoch 1540, val loss: 0.5504245758056641
Epoch 1550, training loss: 85.09476470947266 = 0.4705962836742401 + 10.0 * 8.462416648864746
Epoch 1550, val loss: 0.5498142838478088
Epoch 1560, training loss: 85.11665344238281 = 0.46952489018440247 + 10.0 * 8.464712142944336
Epoch 1560, val loss: 0.5492121577262878
Epoch 1570, training loss: 85.08104705810547 = 0.4684298634529114 + 10.0 * 8.461261749267578
Epoch 1570, val loss: 0.5485852360725403
Epoch 1580, training loss: 85.08525085449219 = 0.46734726428985596 + 10.0 * 8.461790084838867
Epoch 1580, val loss: 0.5479499101638794
Epoch 1590, training loss: 85.0728988647461 = 0.46626541018486023 + 10.0 * 8.460663795471191
Epoch 1590, val loss: 0.5473425388336182
Epoch 1600, training loss: 85.08285522460938 = 0.4651813507080078 + 10.0 * 8.461767196655273
Epoch 1600, val loss: 0.5467113256454468
Epoch 1610, training loss: 85.07157135009766 = 0.4640718102455139 + 10.0 * 8.460749626159668
Epoch 1610, val loss: 0.5461023449897766
Epoch 1620, training loss: 85.06560516357422 = 0.46295925974845886 + 10.0 * 8.460264205932617
Epoch 1620, val loss: 0.5454444885253906
Epoch 1630, training loss: 85.07415008544922 = 0.4618474841117859 + 10.0 * 8.461230278015137
Epoch 1630, val loss: 0.5448458790779114
Epoch 1640, training loss: 85.0716552734375 = 0.46072670817375183 + 10.0 * 8.461092948913574
Epoch 1640, val loss: 0.5441972017288208
Epoch 1650, training loss: 85.06198120117188 = 0.4596104919910431 + 10.0 * 8.460237503051758
Epoch 1650, val loss: 0.5435559153556824
Epoch 1660, training loss: 85.05317687988281 = 0.4584871530532837 + 10.0 * 8.459468841552734
Epoch 1660, val loss: 0.5429710149765015
Epoch 1670, training loss: 85.05094146728516 = 0.4573640525341034 + 10.0 * 8.459357261657715
Epoch 1670, val loss: 0.5423594117164612
Epoch 1680, training loss: 85.04578399658203 = 0.4562370479106903 + 10.0 * 8.458954811096191
Epoch 1680, val loss: 0.5417566895484924
Epoch 1690, training loss: 85.0704574584961 = 0.4551011025905609 + 10.0 * 8.461535453796387
Epoch 1690, val loss: 0.5411604046821594
Epoch 1700, training loss: 85.0352554321289 = 0.4539511203765869 + 10.0 * 8.458130836486816
Epoch 1700, val loss: 0.5405429601669312
Epoch 1710, training loss: 85.03399658203125 = 0.4528074860572815 + 10.0 * 8.45811939239502
Epoch 1710, val loss: 0.5399408340454102
Epoch 1720, training loss: 85.02837371826172 = 0.451661080121994 + 10.0 * 8.457671165466309
Epoch 1720, val loss: 0.5393680930137634
Epoch 1730, training loss: 85.02154541015625 = 0.45051661133766174 + 10.0 * 8.45710277557373
Epoch 1730, val loss: 0.5387734770774841
Epoch 1740, training loss: 85.0286636352539 = 0.44936293363571167 + 10.0 * 8.457929611206055
Epoch 1740, val loss: 0.538179337978363
Epoch 1750, training loss: 85.04622650146484 = 0.44818824529647827 + 10.0 * 8.459803581237793
Epoch 1750, val loss: 0.5376115441322327
Epoch 1760, training loss: 85.02033233642578 = 0.44700416922569275 + 10.0 * 8.457332611083984
Epoch 1760, val loss: 0.5369874835014343
Epoch 1770, training loss: 85.00816345214844 = 0.4458262026309967 + 10.0 * 8.456233978271484
Epoch 1770, val loss: 0.5364248752593994
Epoch 1780, training loss: 85.00431060791016 = 0.4446474611759186 + 10.0 * 8.455965995788574
Epoch 1780, val loss: 0.5358303785324097
Epoch 1790, training loss: 84.9985122680664 = 0.44346532225608826 + 10.0 * 8.455504417419434
Epoch 1790, val loss: 0.5352638959884644
Epoch 1800, training loss: 84.99713897705078 = 0.4422740042209625 + 10.0 * 8.455486297607422
Epoch 1800, val loss: 0.5346993803977966
Epoch 1810, training loss: 85.0396728515625 = 0.44107088446617126 + 10.0 * 8.459859848022461
Epoch 1810, val loss: 0.5340983867645264
Epoch 1820, training loss: 85.02416229248047 = 0.43982118368148804 + 10.0 * 8.458434104919434
Epoch 1820, val loss: 0.5334985852241516
Epoch 1830, training loss: 84.99947357177734 = 0.4385765790939331 + 10.0 * 8.456089973449707
Epoch 1830, val loss: 0.5328485369682312
Epoch 1840, training loss: 84.9804916381836 = 0.4373474717140198 + 10.0 * 8.454314231872559
Epoch 1840, val loss: 0.532248854637146
Epoch 1850, training loss: 84.97820281982422 = 0.4361227750778198 + 10.0 * 8.454208374023438
Epoch 1850, val loss: 0.5316694974899292
Epoch 1860, training loss: 84.97400665283203 = 0.4348898231983185 + 10.0 * 8.453911781311035
Epoch 1860, val loss: 0.5311036109924316
Epoch 1870, training loss: 84.98129272460938 = 0.43364736437797546 + 10.0 * 8.454764366149902
Epoch 1870, val loss: 0.5305318832397461
Epoch 1880, training loss: 84.97892761230469 = 0.43237295746803284 + 10.0 * 8.454655647277832
Epoch 1880, val loss: 0.5299343466758728
Epoch 1890, training loss: 84.97371673583984 = 0.4310857653617859 + 10.0 * 8.454263687133789
Epoch 1890, val loss: 0.5293344855308533
Epoch 1900, training loss: 84.96206665039062 = 0.4298058748245239 + 10.0 * 8.453226089477539
Epoch 1900, val loss: 0.52876216173172
Epoch 1910, training loss: 84.95574951171875 = 0.4285290837287903 + 10.0 * 8.452722549438477
Epoch 1910, val loss: 0.5281542539596558
Epoch 1920, training loss: 84.95316314697266 = 0.42724767327308655 + 10.0 * 8.452591896057129
Epoch 1920, val loss: 0.5275768041610718
Epoch 1930, training loss: 84.97406768798828 = 0.4259577989578247 + 10.0 * 8.454811096191406
Epoch 1930, val loss: 0.5269860625267029
Epoch 1940, training loss: 84.9546127319336 = 0.4246251583099365 + 10.0 * 8.452999114990234
Epoch 1940, val loss: 0.5264254212379456
Epoch 1950, training loss: 84.94624328613281 = 0.4232920706272125 + 10.0 * 8.452295303344727
Epoch 1950, val loss: 0.5258137583732605
Epoch 1960, training loss: 84.93966674804688 = 0.4219644069671631 + 10.0 * 8.451769828796387
Epoch 1960, val loss: 0.5252272486686707
Epoch 1970, training loss: 84.9345703125 = 0.42063552141189575 + 10.0 * 8.451393127441406
Epoch 1970, val loss: 0.5246630311012268
Epoch 1980, training loss: 84.93193054199219 = 0.41929909586906433 + 10.0 * 8.451263427734375
Epoch 1980, val loss: 0.5241098999977112
Epoch 1990, training loss: 84.931396484375 = 0.4179517328739166 + 10.0 * 8.45134449005127
Epoch 1990, val loss: 0.5235238075256348
Epoch 2000, training loss: 84.9637222290039 = 0.4165908396244049 + 10.0 * 8.454713821411133
Epoch 2000, val loss: 0.5229406356811523
Epoch 2010, training loss: 84.94532012939453 = 0.4152110517024994 + 10.0 * 8.453010559082031
Epoch 2010, val loss: 0.5223894715309143
Epoch 2020, training loss: 84.9219741821289 = 0.41383096575737 + 10.0 * 8.450814247131348
Epoch 2020, val loss: 0.5218391418457031
Epoch 2030, training loss: 84.91543579101562 = 0.4124535918235779 + 10.0 * 8.450298309326172
Epoch 2030, val loss: 0.5212723016738892
Epoch 2040, training loss: 84.91744232177734 = 0.4110746681690216 + 10.0 * 8.450636863708496
Epoch 2040, val loss: 0.520756185054779
Epoch 2050, training loss: 84.95043182373047 = 0.40968450903892517 + 10.0 * 8.45407485961914
Epoch 2050, val loss: 0.520239531993866
Epoch 2060, training loss: 84.90341186523438 = 0.40828052163124084 + 10.0 * 8.44951343536377
Epoch 2060, val loss: 0.5196855068206787
Epoch 2070, training loss: 84.90618896484375 = 0.40687862038612366 + 10.0 * 8.449931144714355
Epoch 2070, val loss: 0.5191474556922913
Epoch 2080, training loss: 84.8978042602539 = 0.405472993850708 + 10.0 * 8.449233055114746
Epoch 2080, val loss: 0.5186286568641663
Epoch 2090, training loss: 84.89894104003906 = 0.40406739711761475 + 10.0 * 8.449487686157227
Epoch 2090, val loss: 0.5181290507316589
Epoch 2100, training loss: 84.91570281982422 = 0.40264832973480225 + 10.0 * 8.451305389404297
Epoch 2100, val loss: 0.5176365971565247
Epoch 2110, training loss: 84.8993148803711 = 0.40121176838874817 + 10.0 * 8.449810028076172
Epoch 2110, val loss: 0.5171021223068237
Epoch 2120, training loss: 84.89775848388672 = 0.399759978055954 + 10.0 * 8.449800491333008
Epoch 2120, val loss: 0.516597330570221
Epoch 2130, training loss: 84.88420104980469 = 0.3983010947704315 + 10.0 * 8.448590278625488
Epoch 2130, val loss: 0.5160863995552063
Epoch 2140, training loss: 84.8786849975586 = 0.39684560894966125 + 10.0 * 8.4481840133667
Epoch 2140, val loss: 0.5155915021896362
Epoch 2150, training loss: 84.87395477294922 = 0.39538758993148804 + 10.0 * 8.447856903076172
Epoch 2150, val loss: 0.5151235461235046
Epoch 2160, training loss: 84.89637756347656 = 0.393930047750473 + 10.0 * 8.450244903564453
Epoch 2160, val loss: 0.5146482586860657
Epoch 2170, training loss: 84.87061309814453 = 0.3924452066421509 + 10.0 * 8.447816848754883
Epoch 2170, val loss: 0.5141770839691162
Epoch 2180, training loss: 84.86640930175781 = 0.39096468687057495 + 10.0 * 8.44754409790039
Epoch 2180, val loss: 0.5137016773223877
Epoch 2190, training loss: 84.86347961425781 = 0.38948705792427063 + 10.0 * 8.447399139404297
Epoch 2190, val loss: 0.5132277607917786
Epoch 2200, training loss: 84.86743927001953 = 0.38800716400146484 + 10.0 * 8.447942733764648
Epoch 2200, val loss: 0.5127893686294556
Epoch 2210, training loss: 84.90921020507812 = 0.38651254773139954 + 10.0 * 8.452269554138184
Epoch 2210, val loss: 0.5123534798622131
Epoch 2220, training loss: 84.86453247070312 = 0.38500070571899414 + 10.0 * 8.447953224182129
Epoch 2220, val loss: 0.5119118094444275
Epoch 2230, training loss: 84.84960174560547 = 0.38349857926368713 + 10.0 * 8.446610450744629
Epoch 2230, val loss: 0.5115065574645996
Epoch 2240, training loss: 84.84577178955078 = 0.3820000886917114 + 10.0 * 8.44637680053711
Epoch 2240, val loss: 0.5110767483711243
Epoch 2250, training loss: 84.84081268310547 = 0.38049790263175964 + 10.0 * 8.44603157043457
Epoch 2250, val loss: 0.5106756687164307
Epoch 2260, training loss: 84.83784484863281 = 0.3789891302585602 + 10.0 * 8.44588565826416
Epoch 2260, val loss: 0.5103059411048889
Epoch 2270, training loss: 84.85346221923828 = 0.3774664103984833 + 10.0 * 8.447599411010742
Epoch 2270, val loss: 0.5099261403083801
Epoch 2280, training loss: 84.84790802001953 = 0.3759281635284424 + 10.0 * 8.447197914123535
Epoch 2280, val loss: 0.5095040202140808
Epoch 2290, training loss: 84.88429260253906 = 0.37439054250717163 + 10.0 * 8.450990676879883
Epoch 2290, val loss: 0.5091261267662048
Epoch 2300, training loss: 84.82622528076172 = 0.3728269040584564 + 10.0 * 8.445340156555176
Epoch 2300, val loss: 0.5087919235229492
Epoch 2310, training loss: 84.82475280761719 = 0.3712882995605469 + 10.0 * 8.445345878601074
Epoch 2310, val loss: 0.5084134340286255
Epoch 2320, training loss: 84.822021484375 = 0.3697492480278015 + 10.0 * 8.445226669311523
Epoch 2320, val loss: 0.5080787539482117
Epoch 2330, training loss: 84.8150634765625 = 0.36820128560066223 + 10.0 * 8.444685935974121
Epoch 2330, val loss: 0.5077604055404663
Epoch 2340, training loss: 84.81294250488281 = 0.3666561543941498 + 10.0 * 8.444628715515137
Epoch 2340, val loss: 0.5074619650840759
Epoch 2350, training loss: 84.81322479248047 = 0.3651025593280792 + 10.0 * 8.444811820983887
Epoch 2350, val loss: 0.5071502923965454
Epoch 2360, training loss: 84.8448257446289 = 0.36354196071624756 + 10.0 * 8.448128700256348
Epoch 2360, val loss: 0.5068670511245728
Epoch 2370, training loss: 84.80162811279297 = 0.3619796335697174 + 10.0 * 8.443964958190918
Epoch 2370, val loss: 0.5065330266952515
Epoch 2380, training loss: 84.80359649658203 = 0.3604208528995514 + 10.0 * 8.444317817687988
Epoch 2380, val loss: 0.506243109703064
Epoch 2390, training loss: 84.8271255493164 = 0.3588598966598511 + 10.0 * 8.446826934814453
Epoch 2390, val loss: 0.5059905648231506
Epoch 2400, training loss: 84.79576873779297 = 0.3572796583175659 + 10.0 * 8.443849563598633
Epoch 2400, val loss: 0.5057300329208374
Epoch 2410, training loss: 84.79495239257812 = 0.35571497678756714 + 10.0 * 8.443923950195312
Epoch 2410, val loss: 0.5054596662521362
Epoch 2420, training loss: 84.79080963134766 = 0.35415685176849365 + 10.0 * 8.443665504455566
Epoch 2420, val loss: 0.5052456855773926
Epoch 2430, training loss: 84.78572845458984 = 0.3526044189929962 + 10.0 * 8.44331169128418
Epoch 2430, val loss: 0.5049904584884644
Epoch 2440, training loss: 84.8089370727539 = 0.35104575753211975 + 10.0 * 8.445789337158203
Epoch 2440, val loss: 0.504767119884491
Epoch 2450, training loss: 84.77732849121094 = 0.3494742512702942 + 10.0 * 8.442785263061523
Epoch 2450, val loss: 0.5045557022094727
Epoch 2460, training loss: 84.77391815185547 = 0.34791040420532227 + 10.0 * 8.442601203918457
Epoch 2460, val loss: 0.5043259859085083
Epoch 2470, training loss: 84.77073669433594 = 0.34634289145469666 + 10.0 * 8.442439079284668
Epoch 2470, val loss: 0.5041423439979553
Epoch 2480, training loss: 84.7689208984375 = 0.3447820842266083 + 10.0 * 8.442414283752441
Epoch 2480, val loss: 0.5039501190185547
Epoch 2490, training loss: 84.8194351196289 = 0.3432222008705139 + 10.0 * 8.44762134552002
Epoch 2490, val loss: 0.5037586092948914
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8077118214104515
0.818445265521988
=== training gcn model ===
Epoch 0, training loss: 106.92232513427734 = 1.099352240562439 + 10.0 * 10.582297325134277
Epoch 0, val loss: 1.097698450088501
Epoch 10, training loss: 106.9153060913086 = 1.0943646430969238 + 10.0 * 10.582094192504883
Epoch 10, val loss: 1.0927579402923584
Epoch 20, training loss: 106.90125274658203 = 1.0888792276382446 + 10.0 * 10.581236839294434
Epoch 20, val loss: 1.087323784828186
Epoch 30, training loss: 106.85493469238281 = 1.082836627960205 + 10.0 * 10.57720947265625
Epoch 30, val loss: 1.0813722610473633
Epoch 40, training loss: 106.68004608154297 = 1.0762416124343872 + 10.0 * 10.560380935668945
Epoch 40, val loss: 1.0749763250350952
Epoch 50, training loss: 106.15928649902344 = 1.0693345069885254 + 10.0 * 10.508995056152344
Epoch 50, val loss: 1.068406581878662
Epoch 60, training loss: 104.84072875976562 = 1.0634994506835938 + 10.0 * 10.37772274017334
Epoch 60, val loss: 1.0631160736083984
Epoch 70, training loss: 102.076416015625 = 1.0589181184768677 + 10.0 * 10.101749420166016
Epoch 70, val loss: 1.0589089393615723
Epoch 80, training loss: 99.20862579345703 = 1.0551576614379883 + 10.0 * 9.815346717834473
Epoch 80, val loss: 1.0554430484771729
Epoch 90, training loss: 96.04571533203125 = 1.0512526035308838 + 10.0 * 9.499445915222168
Epoch 90, val loss: 1.0515892505645752
Epoch 100, training loss: 94.35105895996094 = 1.047057032585144 + 10.0 * 9.330400466918945
Epoch 100, val loss: 1.047391414642334
Epoch 110, training loss: 93.83221435546875 = 1.0430996417999268 + 10.0 * 9.278911590576172
Epoch 110, val loss: 1.0435575246810913
Epoch 120, training loss: 93.23236846923828 = 1.0397193431854248 + 10.0 * 9.21926498413086
Epoch 120, val loss: 1.0404201745986938
Epoch 130, training loss: 92.32355499267578 = 1.0371472835540771 + 10.0 * 9.128641128540039
Epoch 130, val loss: 1.038089394569397
Epoch 140, training loss: 91.47222137451172 = 1.035089373588562 + 10.0 * 9.043713569641113
Epoch 140, val loss: 1.0362056493759155
Epoch 150, training loss: 90.88241577148438 = 1.0330175161361694 + 10.0 * 8.984939575195312
Epoch 150, val loss: 1.0342265367507935
Epoch 160, training loss: 90.27185821533203 = 1.0308328866958618 + 10.0 * 8.924102783203125
Epoch 160, val loss: 1.0321451425552368
Epoch 170, training loss: 89.7641372680664 = 1.0286507606506348 + 10.0 * 8.87354850769043
Epoch 170, val loss: 1.0300843715667725
Epoch 180, training loss: 89.45565032958984 = 1.0260558128356934 + 10.0 * 8.8429594039917
Epoch 180, val loss: 1.0276168584823608
Epoch 190, training loss: 89.21914672851562 = 1.0229657888412476 + 10.0 * 8.819618225097656
Epoch 190, val loss: 1.0246913433074951
Epoch 200, training loss: 88.99894714355469 = 1.019559621810913 + 10.0 * 8.797938346862793
Epoch 200, val loss: 1.0214731693267822
Epoch 210, training loss: 88.75892639160156 = 1.015972375869751 + 10.0 * 8.77429485321045
Epoch 210, val loss: 1.0180951356887817
Epoch 220, training loss: 88.54908752441406 = 1.0122663974761963 + 10.0 * 8.753682136535645
Epoch 220, val loss: 1.0145819187164307
Epoch 230, training loss: 88.36034393310547 = 1.008202075958252 + 10.0 * 8.735214233398438
Epoch 230, val loss: 1.0107029676437378
Epoch 240, training loss: 88.17729949951172 = 1.0036920309066772 + 10.0 * 8.717360496520996
Epoch 240, val loss: 1.0064047574996948
Epoch 250, training loss: 88.02326202392578 = 0.9987900853157043 + 10.0 * 8.702447891235352
Epoch 250, val loss: 1.0017908811569214
Epoch 260, training loss: 87.88607025146484 = 0.9935386180877686 + 10.0 * 8.689252853393555
Epoch 260, val loss: 0.9968968033790588
Epoch 270, training loss: 87.76911926269531 = 0.9878841042518616 + 10.0 * 8.678123474121094
Epoch 270, val loss: 0.9916233420372009
Epoch 280, training loss: 87.67198181152344 = 0.9817488193511963 + 10.0 * 8.669023513793945
Epoch 280, val loss: 0.9859157204627991
Epoch 290, training loss: 87.58407592773438 = 0.975151002407074 + 10.0 * 8.660892486572266
Epoch 290, val loss: 0.979763388633728
Epoch 300, training loss: 87.50447845458984 = 0.9680782556533813 + 10.0 * 8.653639793395996
Epoch 300, val loss: 0.9731684923171997
Epoch 310, training loss: 87.4297103881836 = 0.9605501294136047 + 10.0 * 8.646916389465332
Epoch 310, val loss: 0.9661833047866821
Epoch 320, training loss: 87.35671997070312 = 0.9526354074478149 + 10.0 * 8.640408515930176
Epoch 320, val loss: 0.9588671922683716
Epoch 330, training loss: 87.28299713134766 = 0.9443387389183044 + 10.0 * 8.633866310119629
Epoch 330, val loss: 0.951223611831665
Epoch 340, training loss: 87.21924591064453 = 0.9356839060783386 + 10.0 * 8.628355979919434
Epoch 340, val loss: 0.9432592988014221
Epoch 350, training loss: 87.1438217163086 = 0.9266959428787231 + 10.0 * 8.621712684631348
Epoch 350, val loss: 0.9350306987762451
Epoch 360, training loss: 87.07794189453125 = 0.9174425601959229 + 10.0 * 8.616049766540527
Epoch 360, val loss: 0.9265926480293274
Epoch 370, training loss: 87.01512908935547 = 0.90788733959198 + 10.0 * 8.610723495483398
Epoch 370, val loss: 0.9178963303565979
Epoch 380, training loss: 87.005126953125 = 0.8980330228805542 + 10.0 * 8.610709190368652
Epoch 380, val loss: 0.9089428782463074
Epoch 390, training loss: 86.91719818115234 = 0.8879223465919495 + 10.0 * 8.602927207946777
Epoch 390, val loss: 0.8998122215270996
Epoch 400, training loss: 86.85368347167969 = 0.8777154684066772 + 10.0 * 8.597597122192383
Epoch 400, val loss: 0.8906408548355103
Epoch 410, training loss: 86.80165100097656 = 0.8674179911613464 + 10.0 * 8.593423843383789
Epoch 410, val loss: 0.8814156651496887
Epoch 420, training loss: 86.76507568359375 = 0.8570706844329834 + 10.0 * 8.590800285339355
Epoch 420, val loss: 0.8721681833267212
Epoch 430, training loss: 86.71153259277344 = 0.846660315990448 + 10.0 * 8.586487770080566
Epoch 430, val loss: 0.862943172454834
Epoch 440, training loss: 86.66002655029297 = 0.836317241191864 + 10.0 * 8.58237075805664
Epoch 440, val loss: 0.8538010120391846
Epoch 450, training loss: 86.61324310302734 = 0.8260442018508911 + 10.0 * 8.578720092773438
Epoch 450, val loss: 0.8447535037994385
Epoch 460, training loss: 86.56695556640625 = 0.8158311247825623 + 10.0 * 8.575112342834473
Epoch 460, val loss: 0.8357813954353333
Epoch 470, training loss: 86.55120086669922 = 0.8056907057762146 + 10.0 * 8.574551582336426
Epoch 470, val loss: 0.8269075155258179
Epoch 480, training loss: 86.49213409423828 = 0.7956069111824036 + 10.0 * 8.569652557373047
Epoch 480, val loss: 0.8180932402610779
Epoch 490, training loss: 86.43605041503906 = 0.7856873273849487 + 10.0 * 8.565035820007324
Epoch 490, val loss: 0.8095229268074036
Epoch 500, training loss: 86.39435577392578 = 0.775958776473999 + 10.0 * 8.561840057373047
Epoch 500, val loss: 0.8010953068733215
Epoch 510, training loss: 86.35256958007812 = 0.7663690447807312 + 10.0 * 8.558619499206543
Epoch 510, val loss: 0.7928011417388916
Epoch 520, training loss: 86.31346130371094 = 0.7568298578262329 + 10.0 * 8.555663108825684
Epoch 520, val loss: 0.7845588326454163
Epoch 530, training loss: 86.32747650146484 = 0.7474544048309326 + 10.0 * 8.558002471923828
Epoch 530, val loss: 0.7764982581138611
Epoch 540, training loss: 86.24628448486328 = 0.7381774187088013 + 10.0 * 8.550810813903809
Epoch 540, val loss: 0.7684868574142456
Epoch 550, training loss: 86.2047348022461 = 0.7290874719619751 + 10.0 * 8.547564506530762
Epoch 550, val loss: 0.7606810331344604
Epoch 560, training loss: 86.16899108886719 = 0.7201583981513977 + 10.0 * 8.544882774353027
Epoch 560, val loss: 0.7529829740524292
Epoch 570, training loss: 86.14401245117188 = 0.7113780975341797 + 10.0 * 8.54326343536377
Epoch 570, val loss: 0.7454729080200195
Epoch 580, training loss: 86.12687683105469 = 0.7028746008872986 + 10.0 * 8.542400360107422
Epoch 580, val loss: 0.7381860017776489
Epoch 590, training loss: 86.08164978027344 = 0.6946535110473633 + 10.0 * 8.53869915008545
Epoch 590, val loss: 0.7312101721763611
Epoch 600, training loss: 86.04712677001953 = 0.6867414712905884 + 10.0 * 8.536038398742676
Epoch 600, val loss: 0.7245002388954163
Epoch 610, training loss: 86.01808166503906 = 0.6791224479675293 + 10.0 * 8.533895492553711
Epoch 610, val loss: 0.7180505990982056
Epoch 620, training loss: 86.02299499511719 = 0.6717491745948792 + 10.0 * 8.535123825073242
Epoch 620, val loss: 0.7118043303489685
Epoch 630, training loss: 85.96775817871094 = 0.6646047234535217 + 10.0 * 8.530315399169922
Epoch 630, val loss: 0.7058030366897583
Epoch 640, training loss: 85.93991088867188 = 0.6577607989311218 + 10.0 * 8.528215408325195
Epoch 640, val loss: 0.7000675797462463
Epoch 650, training loss: 85.91596984863281 = 0.6511968970298767 + 10.0 * 8.526476860046387
Epoch 650, val loss: 0.6945841908454895
Epoch 660, training loss: 85.89155578613281 = 0.6449097394943237 + 10.0 * 8.524663925170898
Epoch 660, val loss: 0.6893593668937683
Epoch 670, training loss: 85.90580749511719 = 0.6388714909553528 + 10.0 * 8.526693344116211
Epoch 670, val loss: 0.6843768358230591
Epoch 680, training loss: 85.84941864013672 = 0.6330579519271851 + 10.0 * 8.521636009216309
Epoch 680, val loss: 0.6795457005500793
Epoch 690, training loss: 85.8270492553711 = 0.6275492906570435 + 10.0 * 8.519949913024902
Epoch 690, val loss: 0.6750867962837219
Epoch 700, training loss: 85.80403137207031 = 0.6223340630531311 + 10.0 * 8.518169403076172
Epoch 700, val loss: 0.670840322971344
Epoch 710, training loss: 85.78208923339844 = 0.6173661947250366 + 10.0 * 8.516472816467285
Epoch 710, val loss: 0.666865885257721
Epoch 720, training loss: 85.76361083984375 = 0.6126182079315186 + 10.0 * 8.51509952545166
Epoch 720, val loss: 0.6630821824073792
Epoch 730, training loss: 85.79086303710938 = 0.6080516576766968 + 10.0 * 8.518280982971191
Epoch 730, val loss: 0.6594666242599487
Epoch 740, training loss: 85.72313690185547 = 0.6036739945411682 + 10.0 * 8.511945724487305
Epoch 740, val loss: 0.6560015082359314
Epoch 750, training loss: 85.7086410522461 = 0.599536657333374 + 10.0 * 8.510910987854004
Epoch 750, val loss: 0.6527412533760071
Epoch 760, training loss: 85.68595886230469 = 0.5955871939659119 + 10.0 * 8.509037017822266
Epoch 760, val loss: 0.6496880054473877
Epoch 770, training loss: 85.67542266845703 = 0.591803252696991 + 10.0 * 8.50836181640625
Epoch 770, val loss: 0.6467950344085693
Epoch 780, training loss: 85.65705871582031 = 0.5881550908088684 + 10.0 * 8.506890296936035
Epoch 780, val loss: 0.64399653673172
Epoch 790, training loss: 85.63442993164062 = 0.5846542716026306 + 10.0 * 8.504977226257324
Epoch 790, val loss: 0.6413702964782715
Epoch 800, training loss: 85.6180191040039 = 0.5813278555870056 + 10.0 * 8.503668785095215
Epoch 800, val loss: 0.6388949751853943
Epoch 810, training loss: 85.60063934326172 = 0.5781441330909729 + 10.0 * 8.502248764038086
Epoch 810, val loss: 0.6365257501602173
Epoch 820, training loss: 85.59972381591797 = 0.5750434398651123 + 10.0 * 8.50246810913086
Epoch 820, val loss: 0.6341960430145264
Epoch 830, training loss: 85.57457733154297 = 0.5720535516738892 + 10.0 * 8.500252723693848
Epoch 830, val loss: 0.6320051550865173
Epoch 840, training loss: 85.55281829833984 = 0.5692084431648254 + 10.0 * 8.498361587524414
Epoch 840, val loss: 0.6299514770507812
Epoch 850, training loss: 85.53616333007812 = 0.5664802193641663 + 10.0 * 8.496968269348145
Epoch 850, val loss: 0.6279681921005249
Epoch 860, training loss: 85.51969909667969 = 0.5638361573219299 + 10.0 * 8.495586395263672
Epoch 860, val loss: 0.6260836124420166
Epoch 870, training loss: 85.54623413085938 = 0.561267614364624 + 10.0 * 8.498497009277344
Epoch 870, val loss: 0.6242820024490356
Epoch 880, training loss: 85.49357604980469 = 0.5587294697761536 + 10.0 * 8.493484497070312
Epoch 880, val loss: 0.6224738955497742
Epoch 890, training loss: 85.48519897460938 = 0.5562924146652222 + 10.0 * 8.492891311645508
Epoch 890, val loss: 0.6207281947135925
Epoch 900, training loss: 85.46527099609375 = 0.5539311766624451 + 10.0 * 8.491133689880371
Epoch 900, val loss: 0.6190522313117981
Epoch 910, training loss: 85.45004272460938 = 0.5516478419303894 + 10.0 * 8.489839553833008
Epoch 910, val loss: 0.6174273490905762
Epoch 920, training loss: 85.4536361694336 = 0.5494139194488525 + 10.0 * 8.490422248840332
Epoch 920, val loss: 0.6158356666564941
Epoch 930, training loss: 85.42285919189453 = 0.5472003817558289 + 10.0 * 8.487565994262695
Epoch 930, val loss: 0.6143191456794739
Epoch 940, training loss: 85.41336822509766 = 0.5450493097305298 + 10.0 * 8.486831665039062
Epoch 940, val loss: 0.6128136515617371
Epoch 950, training loss: 85.40591430664062 = 0.5429522395133972 + 10.0 * 8.486295700073242
Epoch 950, val loss: 0.6113282442092896
Epoch 960, training loss: 85.4028549194336 = 0.5408729910850525 + 10.0 * 8.486198425292969
Epoch 960, val loss: 0.6097938418388367
Epoch 970, training loss: 85.38680267333984 = 0.5388222932815552 + 10.0 * 8.484797477722168
Epoch 970, val loss: 0.6084427833557129
Epoch 980, training loss: 85.3691177368164 = 0.5368422865867615 + 10.0 * 8.483227729797363
Epoch 980, val loss: 0.6069650650024414
Epoch 990, training loss: 85.35877227783203 = 0.5349029302597046 + 10.0 * 8.482386589050293
Epoch 990, val loss: 0.6055849194526672
Epoch 1000, training loss: 85.36836242675781 = 0.5329936742782593 + 10.0 * 8.483536720275879
Epoch 1000, val loss: 0.6042802929878235
Epoch 1010, training loss: 85.35501098632812 = 0.5310704112052917 + 10.0 * 8.482394218444824
Epoch 1010, val loss: 0.602800190448761
Epoch 1020, training loss: 85.33509063720703 = 0.5291898250579834 + 10.0 * 8.480589866638184
Epoch 1020, val loss: 0.601487398147583
Epoch 1030, training loss: 85.32620239257812 = 0.5273854732513428 + 10.0 * 8.47988224029541
Epoch 1030, val loss: 0.6001796722412109
Epoch 1040, training loss: 85.31383514404297 = 0.5256214141845703 + 10.0 * 8.478821754455566
Epoch 1040, val loss: 0.5988975167274475
Epoch 1050, training loss: 85.3062515258789 = 0.5238867998123169 + 10.0 * 8.478236198425293
Epoch 1050, val loss: 0.5976324677467346
Epoch 1060, training loss: 85.31990051269531 = 0.5221549272537231 + 10.0 * 8.479774475097656
Epoch 1060, val loss: 0.5963958501815796
Epoch 1070, training loss: 85.29705810546875 = 0.5204392671585083 + 10.0 * 8.477662086486816
Epoch 1070, val loss: 0.5951011180877686
Epoch 1080, training loss: 85.30215454101562 = 0.5187419056892395 + 10.0 * 8.478341102600098
Epoch 1080, val loss: 0.5939290523529053
Epoch 1090, training loss: 85.28260803222656 = 0.5170637369155884 + 10.0 * 8.476553916931152
Epoch 1090, val loss: 0.592576801776886
Epoch 1100, training loss: 85.26776123046875 = 0.5154194235801697 + 10.0 * 8.475234031677246
Epoch 1100, val loss: 0.5914570689201355
Epoch 1110, training loss: 85.25738525390625 = 0.5138006806373596 + 10.0 * 8.474358558654785
Epoch 1110, val loss: 0.5902164578437805
Epoch 1120, training loss: 85.24884796142578 = 0.5122022032737732 + 10.0 * 8.473665237426758
Epoch 1120, val loss: 0.5890653729438782
Epoch 1130, training loss: 85.2439956665039 = 0.5106152296066284 + 10.0 * 8.47333812713623
Epoch 1130, val loss: 0.5878664255142212
Epoch 1140, training loss: 85.26312255859375 = 0.5090222358703613 + 10.0 * 8.475409507751465
Epoch 1140, val loss: 0.5867131948471069
Epoch 1150, training loss: 85.24109649658203 = 0.5074337720870972 + 10.0 * 8.473366737365723
Epoch 1150, val loss: 0.5854591131210327
Epoch 1160, training loss: 85.2255859375 = 0.5058794617652893 + 10.0 * 8.471970558166504
Epoch 1160, val loss: 0.5843790769577026
Epoch 1170, training loss: 85.21307373046875 = 0.5043497681617737 + 10.0 * 8.47087287902832
Epoch 1170, val loss: 0.5832401514053345
Epoch 1180, training loss: 85.20604705810547 = 0.5028368234634399 + 10.0 * 8.470320701599121
Epoch 1180, val loss: 0.5820791125297546
Epoch 1190, training loss: 85.2060775756836 = 0.5013352632522583 + 10.0 * 8.470474243164062
Epoch 1190, val loss: 0.580967903137207
Epoch 1200, training loss: 85.21057891845703 = 0.499812513589859 + 10.0 * 8.471076965332031
Epoch 1200, val loss: 0.5798326730728149
Epoch 1210, training loss: 85.18812561035156 = 0.4983057379722595 + 10.0 * 8.468981742858887
Epoch 1210, val loss: 0.5786603689193726
Epoch 1220, training loss: 85.17901611328125 = 0.49682319164276123 + 10.0 * 8.468218803405762
Epoch 1220, val loss: 0.577541708946228
Epoch 1230, training loss: 85.17285919189453 = 0.49535784125328064 + 10.0 * 8.46774959564209
Epoch 1230, val loss: 0.5764412879943848
Epoch 1240, training loss: 85.18698120117188 = 0.49389538168907166 + 10.0 * 8.469308853149414
Epoch 1240, val loss: 0.5753183960914612
Epoch 1250, training loss: 85.16852569580078 = 0.49242153763771057 + 10.0 * 8.467610359191895
Epoch 1250, val loss: 0.5741842985153198
Epoch 1260, training loss: 85.15852355957031 = 0.4909638464450836 + 10.0 * 8.466755867004395
Epoch 1260, val loss: 0.5731009244918823
Epoch 1270, training loss: 85.1490478515625 = 0.4895196557044983 + 10.0 * 8.46595287322998
Epoch 1270, val loss: 0.5719997882843018
Epoch 1280, training loss: 85.15084838867188 = 0.48808401823043823 + 10.0 * 8.466276168823242
Epoch 1280, val loss: 0.5709068775177002
Epoch 1290, training loss: 85.13697052001953 = 0.4866332709789276 + 10.0 * 8.465033531188965
Epoch 1290, val loss: 0.5698304772377014
Epoch 1300, training loss: 85.14015197753906 = 0.48519352078437805 + 10.0 * 8.465496063232422
Epoch 1300, val loss: 0.568778395652771
Epoch 1310, training loss: 85.13378143310547 = 0.4837363064289093 + 10.0 * 8.465004920959473
Epoch 1310, val loss: 0.5676268935203552
Epoch 1320, training loss: 85.12153625488281 = 0.48229748010635376 + 10.0 * 8.463923454284668
Epoch 1320, val loss: 0.5665620565414429
Epoch 1330, training loss: 85.11105346679688 = 0.48089051246643066 + 10.0 * 8.463016510009766
Epoch 1330, val loss: 0.5654373168945312
Epoch 1340, training loss: 85.10556030273438 = 0.4794907569885254 + 10.0 * 8.462606430053711
Epoch 1340, val loss: 0.5644305944442749
Epoch 1350, training loss: 85.09879302978516 = 0.47809845209121704 + 10.0 * 8.462069511413574
Epoch 1350, val loss: 0.5633183121681213
Epoch 1360, training loss: 85.10248565673828 = 0.4767005741596222 + 10.0 * 8.462578773498535
Epoch 1360, val loss: 0.5622483491897583
Epoch 1370, training loss: 85.09038543701172 = 0.47527557611465454 + 10.0 * 8.461511611938477
Epoch 1370, val loss: 0.561174750328064
Epoch 1380, training loss: 85.08846282958984 = 0.4738582670688629 + 10.0 * 8.46146011352539
Epoch 1380, val loss: 0.560065507888794
Epoch 1390, training loss: 85.0776596069336 = 0.4724639356136322 + 10.0 * 8.460519790649414
Epoch 1390, val loss: 0.5590066313743591
Epoch 1400, training loss: 85.06930541992188 = 0.47107556462287903 + 10.0 * 8.459822654724121
Epoch 1400, val loss: 0.5579515099525452
Epoch 1410, training loss: 85.0638198852539 = 0.46968892216682434 + 10.0 * 8.459413528442383
Epoch 1410, val loss: 0.556888997554779
Epoch 1420, training loss: 85.07089233398438 = 0.46829909086227417 + 10.0 * 8.460259437561035
Epoch 1420, val loss: 0.5557798147201538
Epoch 1430, training loss: 85.07902526855469 = 0.46686944365501404 + 10.0 * 8.461215019226074
Epoch 1430, val loss: 0.554825484752655
Epoch 1440, training loss: 85.06053161621094 = 0.4654339551925659 + 10.0 * 8.45950984954834
Epoch 1440, val loss: 0.5536025166511536
Epoch 1450, training loss: 85.04696655273438 = 0.46403542160987854 + 10.0 * 8.458292961120605
Epoch 1450, val loss: 0.552573025226593
Epoch 1460, training loss: 85.03714752197266 = 0.4626530110836029 + 10.0 * 8.457448959350586
Epoch 1460, val loss: 0.5515280365943909
Epoch 1470, training loss: 85.03044128417969 = 0.46127304434776306 + 10.0 * 8.456916809082031
Epoch 1470, val loss: 0.5504735112190247
Epoch 1480, training loss: 85.02489471435547 = 0.4598897695541382 + 10.0 * 8.456500053405762
Epoch 1480, val loss: 0.5494443774223328
Epoch 1490, training loss: 85.02095031738281 = 0.4584989845752716 + 10.0 * 8.456245422363281
Epoch 1490, val loss: 0.5483668446540833
Epoch 1500, training loss: 85.10015869140625 = 0.4570937752723694 + 10.0 * 8.464306831359863
Epoch 1500, val loss: 0.5472186207771301
Epoch 1510, training loss: 85.01024627685547 = 0.45563894510269165 + 10.0 * 8.455460548400879
Epoch 1510, val loss: 0.5461955070495605
Epoch 1520, training loss: 85.0109634399414 = 0.45422273874282837 + 10.0 * 8.455674171447754
Epoch 1520, val loss: 0.5451329350471497
Epoch 1530, training loss: 84.99983978271484 = 0.4528242349624634 + 10.0 * 8.45470142364502
Epoch 1530, val loss: 0.5440664887428284
Epoch 1540, training loss: 84.99286651611328 = 0.4514269232749939 + 10.0 * 8.454144477844238
Epoch 1540, val loss: 0.5430320501327515
Epoch 1550, training loss: 84.9902114868164 = 0.45002490282058716 + 10.0 * 8.454018592834473
Epoch 1550, val loss: 0.5419604778289795
Epoch 1560, training loss: 85.035400390625 = 0.44860100746154785 + 10.0 * 8.458680152893066
Epoch 1560, val loss: 0.5408740639686584
Epoch 1570, training loss: 84.98180389404297 = 0.4471537172794342 + 10.0 * 8.453465461730957
Epoch 1570, val loss: 0.5398656129837036
Epoch 1580, training loss: 84.97696685791016 = 0.44572874903678894 + 10.0 * 8.453123092651367
Epoch 1580, val loss: 0.5387499332427979
Epoch 1590, training loss: 84.96778869628906 = 0.4443158209323883 + 10.0 * 8.452347755432129
Epoch 1590, val loss: 0.5377398133277893
Epoch 1600, training loss: 84.96257019042969 = 0.44290342926979065 + 10.0 * 8.451967239379883
Epoch 1600, val loss: 0.5367306470870972
Epoch 1610, training loss: 84.95855712890625 = 0.4414846897125244 + 10.0 * 8.45170783996582
Epoch 1610, val loss: 0.5356957316398621
Epoch 1620, training loss: 85.0068588256836 = 0.44004955887794495 + 10.0 * 8.456681251525879
Epoch 1620, val loss: 0.5346611738204956
Epoch 1630, training loss: 84.95504760742188 = 0.43858447670936584 + 10.0 * 8.45164680480957
Epoch 1630, val loss: 0.5336602926254272
Epoch 1640, training loss: 84.94684600830078 = 0.4371441900730133 + 10.0 * 8.450970649719238
Epoch 1640, val loss: 0.5326104164123535
Epoch 1650, training loss: 84.93981170654297 = 0.43570414185523987 + 10.0 * 8.450410842895508
Epoch 1650, val loss: 0.5316076874732971
Epoch 1660, training loss: 84.97171783447266 = 0.43425726890563965 + 10.0 * 8.45374584197998
Epoch 1660, val loss: 0.5305851101875305
Epoch 1670, training loss: 84.9419937133789 = 0.43278175592422485 + 10.0 * 8.450921058654785
Epoch 1670, val loss: 0.5294908881187439
Epoch 1680, training loss: 84.92669677734375 = 0.4313231408596039 + 10.0 * 8.44953727722168
Epoch 1680, val loss: 0.5284638404846191
Epoch 1690, training loss: 84.91976928710938 = 0.429863840341568 + 10.0 * 8.448990821838379
Epoch 1690, val loss: 0.5274403095245361
Epoch 1700, training loss: 84.91426086425781 = 0.4283984899520874 + 10.0 * 8.448586463928223
Epoch 1700, val loss: 0.5264008641242981
Epoch 1710, training loss: 84.90965270996094 = 0.42692282795906067 + 10.0 * 8.448272705078125
Epoch 1710, val loss: 0.5253970623016357
Epoch 1720, training loss: 84.91180419921875 = 0.4254332184791565 + 10.0 * 8.448637008666992
Epoch 1720, val loss: 0.5243632197380066
Epoch 1730, training loss: 84.93867492675781 = 0.4239112436771393 + 10.0 * 8.451476097106934
Epoch 1730, val loss: 0.5233049392700195
Epoch 1740, training loss: 84.90058898925781 = 0.42237588763237 + 10.0 * 8.447820663452148
Epoch 1740, val loss: 0.5223044753074646
Epoch 1750, training loss: 84.89453887939453 = 0.42086082696914673 + 10.0 * 8.447367668151855
Epoch 1750, val loss: 0.5212985873222351
Epoch 1760, training loss: 84.88897705078125 = 0.41934236884117126 + 10.0 * 8.4469633102417
Epoch 1760, val loss: 0.5202854871749878
Epoch 1770, training loss: 84.88414764404297 = 0.41781705617904663 + 10.0 * 8.446633338928223
Epoch 1770, val loss: 0.5193210244178772
Epoch 1780, training loss: 84.91258239746094 = 0.4162830114364624 + 10.0 * 8.449629783630371
Epoch 1780, val loss: 0.5184147953987122
Epoch 1790, training loss: 84.8836441040039 = 0.41471239924430847 + 10.0 * 8.446893692016602
Epoch 1790, val loss: 0.5172773003578186
Epoch 1800, training loss: 84.87619018554688 = 0.41314828395843506 + 10.0 * 8.446304321289062
Epoch 1800, val loss: 0.5162696838378906
Epoch 1810, training loss: 84.87751770019531 = 0.41157960891723633 + 10.0 * 8.446593284606934
Epoch 1810, val loss: 0.5152878761291504
Epoch 1820, training loss: 84.86476135253906 = 0.40999969840049744 + 10.0 * 8.445476531982422
Epoch 1820, val loss: 0.5142979025840759
Epoch 1830, training loss: 84.85990905761719 = 0.40841853618621826 + 10.0 * 8.445149421691895
Epoch 1830, val loss: 0.513315737247467
Epoch 1840, training loss: 84.8668212890625 = 0.4068250358104706 + 10.0 * 8.446000099182129
Epoch 1840, val loss: 0.5123730301856995
Epoch 1850, training loss: 84.86245727539062 = 0.40521302819252014 + 10.0 * 8.445724487304688
Epoch 1850, val loss: 0.5113949179649353
Epoch 1860, training loss: 84.85009002685547 = 0.4035944938659668 + 10.0 * 8.444649696350098
Epoch 1860, val loss: 0.5104171633720398
Epoch 1870, training loss: 84.84642028808594 = 0.40196406841278076 + 10.0 * 8.444445610046387
Epoch 1870, val loss: 0.5094624161720276
Epoch 1880, training loss: 84.84967803955078 = 0.40032845735549927 + 10.0 * 8.444934844970703
Epoch 1880, val loss: 0.5085088610649109
Epoch 1890, training loss: 84.845703125 = 0.39867568016052246 + 10.0 * 8.444703102111816
Epoch 1890, val loss: 0.5075450539588928
Epoch 1900, training loss: 84.83615112304688 = 0.3970288634300232 + 10.0 * 8.443912506103516
Epoch 1900, val loss: 0.5066397786140442
Epoch 1910, training loss: 84.82935333251953 = 0.39537858963012695 + 10.0 * 8.443397521972656
Epoch 1910, val loss: 0.5057314038276672
Epoch 1920, training loss: 84.8270492553711 = 0.39372333884239197 + 10.0 * 8.44333267211914
Epoch 1920, val loss: 0.5048539638519287
Epoch 1930, training loss: 84.82691192626953 = 0.3920571208000183 + 10.0 * 8.443485260009766
Epoch 1930, val loss: 0.5039366483688354
Epoch 1940, training loss: 84.81845092773438 = 0.39039096236228943 + 10.0 * 8.442806243896484
Epoch 1940, val loss: 0.5030046701431274
Epoch 1950, training loss: 84.85125732421875 = 0.3887168765068054 + 10.0 * 8.446253776550293
Epoch 1950, val loss: 0.5022177696228027
Epoch 1960, training loss: 84.81400299072266 = 0.38703101873397827 + 10.0 * 8.442697525024414
Epoch 1960, val loss: 0.5012369751930237
Epoch 1970, training loss: 84.80408477783203 = 0.3853626847267151 + 10.0 * 8.441872596740723
Epoch 1970, val loss: 0.5004778504371643
Epoch 1980, training loss: 84.79806518554688 = 0.3836936354637146 + 10.0 * 8.441437721252441
Epoch 1980, val loss: 0.4996000826358795
Epoch 1990, training loss: 84.79248809814453 = 0.382019579410553 + 10.0 * 8.441046714782715
Epoch 1990, val loss: 0.49882233142852783
Epoch 2000, training loss: 84.78800201416016 = 0.3803403973579407 + 10.0 * 8.440766334533691
Epoch 2000, val loss: 0.4980168640613556
Epoch 2010, training loss: 84.78382110595703 = 0.37865206599235535 + 10.0 * 8.440516471862793
Epoch 2010, val loss: 0.4972156584262848
Epoch 2020, training loss: 84.78716278076172 = 0.3769589364528656 + 10.0 * 8.441020011901855
Epoch 2020, val loss: 0.4964500963687897
Epoch 2030, training loss: 84.81452178955078 = 0.3752345144748688 + 10.0 * 8.443928718566895
Epoch 2030, val loss: 0.49572300910949707
Epoch 2040, training loss: 84.79581451416016 = 0.373516321182251 + 10.0 * 8.442229270935059
Epoch 2040, val loss: 0.494869202375412
Epoch 2050, training loss: 84.76919555664062 = 0.37181273102760315 + 10.0 * 8.439738273620605
Epoch 2050, val loss: 0.49417832493782043
Epoch 2060, training loss: 84.76519012451172 = 0.3701242208480835 + 10.0 * 8.439506530761719
Epoch 2060, val loss: 0.4934985041618347
Epoch 2070, training loss: 84.76092529296875 = 0.3684445917606354 + 10.0 * 8.439248085021973
Epoch 2070, val loss: 0.4928370714187622
Epoch 2080, training loss: 84.7560806274414 = 0.36675623059272766 + 10.0 * 8.438932418823242
Epoch 2080, val loss: 0.49216213822364807
Epoch 2090, training loss: 84.7666244506836 = 0.3650650382041931 + 10.0 * 8.440155982971191
Epoch 2090, val loss: 0.4915020763874054
Epoch 2100, training loss: 84.75108337402344 = 0.36335116624832153 + 10.0 * 8.438773155212402
Epoch 2100, val loss: 0.4908829927444458
Epoch 2110, training loss: 84.75489807128906 = 0.36164945363998413 + 10.0 * 8.439325332641602
Epoch 2110, val loss: 0.4902314245700836
Epoch 2120, training loss: 84.74251556396484 = 0.3599579930305481 + 10.0 * 8.43825626373291
Epoch 2120, val loss: 0.4896264374256134
Epoch 2130, training loss: 84.73685455322266 = 0.3582686185836792 + 10.0 * 8.437858581542969
Epoch 2130, val loss: 0.4890708327293396
Epoch 2140, training loss: 84.73570251464844 = 0.3565822243690491 + 10.0 * 8.437911987304688
Epoch 2140, val loss: 0.48847702145576477
Epoch 2150, training loss: 84.77705383300781 = 0.3548966646194458 + 10.0 * 8.442215919494629
Epoch 2150, val loss: 0.48794785141944885
Epoch 2160, training loss: 84.73878479003906 = 0.3532070815563202 + 10.0 * 8.438557624816895
Epoch 2160, val loss: 0.4873914420604706
Epoch 2170, training loss: 84.72724151611328 = 0.3515256345272064 + 10.0 * 8.43757152557373
Epoch 2170, val loss: 0.4868365228176117
Epoch 2180, training loss: 84.71955871582031 = 0.3498533070087433 + 10.0 * 8.436970710754395
Epoch 2180, val loss: 0.4863988161087036
Epoch 2190, training loss: 84.71458435058594 = 0.3481746017932892 + 10.0 * 8.436640739440918
Epoch 2190, val loss: 0.4859040677547455
Epoch 2200, training loss: 84.71520233154297 = 0.3465028703212738 + 10.0 * 8.436869621276855
Epoch 2200, val loss: 0.48548510670661926
Epoch 2210, training loss: 84.73941802978516 = 0.3448263108730316 + 10.0 * 8.439458847045898
Epoch 2210, val loss: 0.48506078124046326
Epoch 2220, training loss: 84.71151733398438 = 0.34314656257629395 + 10.0 * 8.436837196350098
Epoch 2220, val loss: 0.4846954643726349
Epoch 2230, training loss: 84.70081329345703 = 0.3414851129055023 + 10.0 * 8.435933113098145
Epoch 2230, val loss: 0.48439058661460876
Epoch 2240, training loss: 84.69754028320312 = 0.33983203768730164 + 10.0 * 8.435770988464355
Epoch 2240, val loss: 0.4841325283050537
Epoch 2250, training loss: 84.695068359375 = 0.33818066120147705 + 10.0 * 8.435688972473145
Epoch 2250, val loss: 0.4838346540927887
Epoch 2260, training loss: 84.71084594726562 = 0.33653897047042847 + 10.0 * 8.437430381774902
Epoch 2260, val loss: 0.48360106348991394
Epoch 2270, training loss: 84.68548583984375 = 0.3348941504955292 + 10.0 * 8.435059547424316
Epoch 2270, val loss: 0.4832903742790222
Epoch 2280, training loss: 84.68589782714844 = 0.33325761556625366 + 10.0 * 8.435263633728027
Epoch 2280, val loss: 0.483090877532959
Epoch 2290, training loss: 84.67768859863281 = 0.33161911368370056 + 10.0 * 8.434606552124023
Epoch 2290, val loss: 0.48296594619750977
Epoch 2300, training loss: 84.67544555664062 = 0.329986035823822 + 10.0 * 8.434545516967773
Epoch 2300, val loss: 0.48282504081726074
Epoch 2310, training loss: 84.68079376220703 = 0.3283611834049225 + 10.0 * 8.435243606567383
Epoch 2310, val loss: 0.48273131251335144
Epoch 2320, training loss: 84.68109893798828 = 0.3267345726490021 + 10.0 * 8.435436248779297
Epoch 2320, val loss: 0.48265889286994934
Epoch 2330, training loss: 84.6776351928711 = 0.32511448860168457 + 10.0 * 8.43525218963623
Epoch 2330, val loss: 0.4826739430427551
Epoch 2340, training loss: 84.66168975830078 = 0.32349973917007446 + 10.0 * 8.433818817138672
Epoch 2340, val loss: 0.482611745595932
Epoch 2350, training loss: 84.65670776367188 = 0.3218962252140045 + 10.0 * 8.433481216430664
Epoch 2350, val loss: 0.48264080286026
Epoch 2360, training loss: 84.65654754638672 = 0.32030168175697327 + 10.0 * 8.433624267578125
Epoch 2360, val loss: 0.482703298330307
Epoch 2370, training loss: 84.67243194580078 = 0.31871917843818665 + 10.0 * 8.435371398925781
Epoch 2370, val loss: 0.48283693194389343
Epoch 2380, training loss: 84.65238952636719 = 0.31712785363197327 + 10.0 * 8.433526039123535
Epoch 2380, val loss: 0.48290523886680603
Epoch 2390, training loss: 84.64628601074219 = 0.31555479764938354 + 10.0 * 8.433073043823242
Epoch 2390, val loss: 0.4831429719924927
Epoch 2400, training loss: 84.65038299560547 = 0.3139828145503998 + 10.0 * 8.433640480041504
Epoch 2400, val loss: 0.4832633137702942
Epoch 2410, training loss: 84.65589904785156 = 0.31239473819732666 + 10.0 * 8.434350967407227
Epoch 2410, val loss: 0.48352155089378357
Epoch 2420, training loss: 84.63904571533203 = 0.3108116388320923 + 10.0 * 8.432823181152344
Epoch 2420, val loss: 0.4838338792324066
Epoch 2430, training loss: 84.63121795654297 = 0.30923816561698914 + 10.0 * 8.432198524475098
Epoch 2430, val loss: 0.48416370153427124
Epoch 2440, training loss: 84.62641143798828 = 0.30767470598220825 + 10.0 * 8.431873321533203
Epoch 2440, val loss: 0.48452356457710266
Epoch 2450, training loss: 84.62273406982422 = 0.30612725019454956 + 10.0 * 8.431660652160645
Epoch 2450, val loss: 0.4849267601966858
Epoch 2460, training loss: 84.63005065917969 = 0.30459973216056824 + 10.0 * 8.43254566192627
Epoch 2460, val loss: 0.4854119122028351
Epoch 2470, training loss: 84.63459014892578 = 0.3030717670917511 + 10.0 * 8.433152198791504
Epoch 2470, val loss: 0.48591652512550354
Epoch 2480, training loss: 84.61499786376953 = 0.3015597462654114 + 10.0 * 8.431344032287598
Epoch 2480, val loss: 0.48645323514938354
Epoch 2490, training loss: 84.61357116699219 = 0.30005499720573425 + 10.0 * 8.431351661682129
Epoch 2490, val loss: 0.48700225353240967
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8107559614408929
0.8188799536332682
The final CL Acc:0.80670, 0.00380, The final GNN Acc:0.81905, 0.00057
raw graph: torch.Size([2, 88648])
add edge: torch.Size([2, 111058])
remove edge: torch.Size([2, 66556])
updated graph: torch.Size([2, 88966])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 106.93635559082031 = 1.113633632659912 + 10.0 * 10.58227252960205
Epoch 0, val loss: 1.1144342422485352
Epoch 10, training loss: 106.9276123046875 = 1.1080645322799683 + 10.0 * 10.581954956054688
Epoch 10, val loss: 1.1089028120040894
Epoch 20, training loss: 106.90773010253906 = 1.102491021156311 + 10.0 * 10.580523490905762
Epoch 20, val loss: 1.1033101081848145
Epoch 30, training loss: 106.83641815185547 = 1.0965776443481445 + 10.0 * 10.573984146118164
Epoch 30, val loss: 1.0973695516586304
Epoch 40, training loss: 106.56480407714844 = 1.090336561203003 + 10.0 * 10.547446250915527
Epoch 40, val loss: 1.0910712480545044
Epoch 50, training loss: 105.75044250488281 = 1.083512544631958 + 10.0 * 10.466692924499512
Epoch 50, val loss: 1.0841768980026245
Epoch 60, training loss: 104.0040283203125 = 1.0765199661254883 + 10.0 * 10.292750358581543
Epoch 60, val loss: 1.0771793127059937
Epoch 70, training loss: 101.5069580078125 = 1.0699447393417358 + 10.0 * 10.043701171875
Epoch 70, val loss: 1.07056725025177
Epoch 80, training loss: 97.84392547607422 = 1.063240885734558 + 10.0 * 9.678068161010742
Epoch 80, val loss: 1.063818335533142
Epoch 90, training loss: 95.7718505859375 = 1.0566586256027222 + 10.0 * 9.471519470214844
Epoch 90, val loss: 1.0572543144226074
Epoch 100, training loss: 94.88125610351562 = 1.050227403640747 + 10.0 * 9.383103370666504
Epoch 100, val loss: 1.0509488582611084
Epoch 110, training loss: 94.05976104736328 = 1.044204831123352 + 10.0 * 9.301555633544922
Epoch 110, val loss: 1.0451087951660156
Epoch 120, training loss: 93.38407135009766 = 1.0391042232513428 + 10.0 * 9.2344970703125
Epoch 120, val loss: 1.0401268005371094
Epoch 130, training loss: 92.90314483642578 = 1.0344802141189575 + 10.0 * 9.186866760253906
Epoch 130, val loss: 1.0354468822479248
Epoch 140, training loss: 92.17152404785156 = 1.029681921005249 + 10.0 * 9.114184379577637
Epoch 140, val loss: 1.0306681394577026
Epoch 150, training loss: 91.18546295166016 = 1.0260483026504517 + 10.0 * 9.015941619873047
Epoch 150, val loss: 1.0271860361099243
Epoch 160, training loss: 90.5946044921875 = 1.0229052305221558 + 10.0 * 8.957170486450195
Epoch 160, val loss: 1.0239847898483276
Epoch 170, training loss: 89.94847106933594 = 1.0189329385757446 + 10.0 * 8.892953872680664
Epoch 170, val loss: 1.0200467109680176
Epoch 180, training loss: 89.3178482055664 = 1.0149680376052856 + 10.0 * 8.83028793334961
Epoch 180, val loss: 1.0162400007247925
Epoch 190, training loss: 88.8597640991211 = 1.0107265710830688 + 10.0 * 8.784903526306152
Epoch 190, val loss: 1.0120669603347778
Epoch 200, training loss: 88.5591049194336 = 1.005785346031189 + 10.0 * 8.755331993103027
Epoch 200, val loss: 1.0072638988494873
Epoch 210, training loss: 88.30805969238281 = 1.0003600120544434 + 10.0 * 8.730770111083984
Epoch 210, val loss: 1.0020431280136108
Epoch 220, training loss: 88.101318359375 = 0.9945862293243408 + 10.0 * 8.710673332214355
Epoch 220, val loss: 0.9964621663093567
Epoch 230, training loss: 87.89769744873047 = 0.9884026646614075 + 10.0 * 8.690929412841797
Epoch 230, val loss: 0.9905388355255127
Epoch 240, training loss: 87.73944091796875 = 0.9817622303962708 + 10.0 * 8.67576789855957
Epoch 240, val loss: 0.9841330051422119
Epoch 250, training loss: 87.58871459960938 = 0.974641740322113 + 10.0 * 8.661407470703125
Epoch 250, val loss: 0.9772711396217346
Epoch 260, training loss: 87.46749877929688 = 0.9670547246932983 + 10.0 * 8.650044441223145
Epoch 260, val loss: 0.9699887633323669
Epoch 270, training loss: 87.34989166259766 = 0.9589126110076904 + 10.0 * 8.639097213745117
Epoch 270, val loss: 0.9621260762214661
Epoch 280, training loss: 87.25166320800781 = 0.9501591920852661 + 10.0 * 8.63015079498291
Epoch 280, val loss: 0.953685998916626
Epoch 290, training loss: 87.15568542480469 = 0.9408482909202576 + 10.0 * 8.62148380279541
Epoch 290, val loss: 0.9447445273399353
Epoch 300, training loss: 87.05809783935547 = 0.9310961365699768 + 10.0 * 8.612699508666992
Epoch 300, val loss: 0.9354012608528137
Epoch 310, training loss: 86.97664642333984 = 0.9209049344062805 + 10.0 * 8.605573654174805
Epoch 310, val loss: 0.9256033897399902
Epoch 320, training loss: 86.87256622314453 = 0.9101967811584473 + 10.0 * 8.596237182617188
Epoch 320, val loss: 0.9153593182563782
Epoch 330, training loss: 86.78626251220703 = 0.8989757299423218 + 10.0 * 8.588727951049805
Epoch 330, val loss: 0.9045903086662292
Epoch 340, training loss: 86.7184829711914 = 0.8871565461158752 + 10.0 * 8.58313274383545
Epoch 340, val loss: 0.893225371837616
Epoch 350, training loss: 86.65485382080078 = 0.874752938747406 + 10.0 * 8.578009605407715
Epoch 350, val loss: 0.8813586235046387
Epoch 360, training loss: 86.57719421386719 = 0.8620292544364929 + 10.0 * 8.571516036987305
Epoch 360, val loss: 0.8691519498825073
Epoch 370, training loss: 86.50933837890625 = 0.848980188369751 + 10.0 * 8.566035270690918
Epoch 370, val loss: 0.8566527962684631
Epoch 380, training loss: 86.43733978271484 = 0.835699737071991 + 10.0 * 8.560163497924805
Epoch 380, val loss: 0.8439517617225647
Epoch 390, training loss: 86.361328125 = 0.822281002998352 + 10.0 * 8.55390453338623
Epoch 390, val loss: 0.8311196565628052
Epoch 400, training loss: 86.32701873779297 = 0.8086936473846436 + 10.0 * 8.55183219909668
Epoch 400, val loss: 0.8181532025337219
Epoch 410, training loss: 86.2116470336914 = 0.7950794696807861 + 10.0 * 8.541656494140625
Epoch 410, val loss: 0.8051284551620483
Epoch 420, training loss: 86.13172912597656 = 0.7816505432128906 + 10.0 * 8.535008430480957
Epoch 420, val loss: 0.7923321723937988
Epoch 430, training loss: 86.05350494384766 = 0.7681992053985596 + 10.0 * 8.528531074523926
Epoch 430, val loss: 0.7794628739356995
Epoch 440, training loss: 85.98362731933594 = 0.7547865509986877 + 10.0 * 8.522884368896484
Epoch 440, val loss: 0.7666547298431396
Epoch 450, training loss: 85.9511489868164 = 0.7413823008537292 + 10.0 * 8.520977020263672
Epoch 450, val loss: 0.7539334893226624
Epoch 460, training loss: 85.8523178100586 = 0.7282997965812683 + 10.0 * 8.512401580810547
Epoch 460, val loss: 0.7414602637290955
Epoch 470, training loss: 85.79084014892578 = 0.71547931432724 + 10.0 * 8.507535934448242
Epoch 470, val loss: 0.7292417883872986
Epoch 480, training loss: 85.733154296875 = 0.7028494477272034 + 10.0 * 8.503030776977539
Epoch 480, val loss: 0.7172386050224304
Epoch 490, training loss: 85.68070983886719 = 0.6905196309089661 + 10.0 * 8.499018669128418
Epoch 490, val loss: 0.7055420875549316
Epoch 500, training loss: 85.67351531982422 = 0.6785486936569214 + 10.0 * 8.499496459960938
Epoch 500, val loss: 0.6941757798194885
Epoch 510, training loss: 85.59759521484375 = 0.6668561100959778 + 10.0 * 8.493074417114258
Epoch 510, val loss: 0.6831454038619995
Epoch 520, training loss: 85.5566635131836 = 0.6556941866874695 + 10.0 * 8.490097045898438
Epoch 520, val loss: 0.6726175546646118
Epoch 530, training loss: 85.51836395263672 = 0.6449370384216309 + 10.0 * 8.487342834472656
Epoch 530, val loss: 0.6624824404716492
Epoch 540, training loss: 85.48487854003906 = 0.6346256732940674 + 10.0 * 8.485025405883789
Epoch 540, val loss: 0.6527988314628601
Epoch 550, training loss: 85.45622253417969 = 0.6247216463088989 + 10.0 * 8.483149528503418
Epoch 550, val loss: 0.6435224413871765
Epoch 560, training loss: 85.43982696533203 = 0.6152289509773254 + 10.0 * 8.482460021972656
Epoch 560, val loss: 0.6346787810325623
Epoch 570, training loss: 85.40202331542969 = 0.606269896030426 + 10.0 * 8.479575157165527
Epoch 570, val loss: 0.6263328790664673
Epoch 580, training loss: 85.3726806640625 = 0.597751796245575 + 10.0 * 8.477492332458496
Epoch 580, val loss: 0.618435263633728
Epoch 590, training loss: 85.34520721435547 = 0.5896509885787964 + 10.0 * 8.475555419921875
Epoch 590, val loss: 0.6109615564346313
Epoch 600, training loss: 85.31864929199219 = 0.5819447040557861 + 10.0 * 8.47367000579834
Epoch 600, val loss: 0.6038801670074463
Epoch 610, training loss: 85.32176971435547 = 0.5746049880981445 + 10.0 * 8.474716186523438
Epoch 610, val loss: 0.597148060798645
Epoch 620, training loss: 85.2736587524414 = 0.5675851106643677 + 10.0 * 8.470606803894043
Epoch 620, val loss: 0.5907976031303406
Epoch 630, training loss: 85.27139282226562 = 0.5609671473503113 + 10.0 * 8.47104263305664
Epoch 630, val loss: 0.5848028063774109
Epoch 640, training loss: 85.23199462890625 = 0.5546383261680603 + 10.0 * 8.467735290527344
Epoch 640, val loss: 0.5791823267936707
Epoch 650, training loss: 85.20323944091797 = 0.5486655235290527 + 10.0 * 8.46545696258545
Epoch 650, val loss: 0.5738300085067749
Epoch 660, training loss: 85.17793273925781 = 0.5429531335830688 + 10.0 * 8.46349811553955
Epoch 660, val loss: 0.5687835216522217
Epoch 670, training loss: 85.15840148925781 = 0.5374939441680908 + 10.0 * 8.462091445922852
Epoch 670, val loss: 0.5639932155609131
Epoch 680, training loss: 85.15961456298828 = 0.5322809815406799 + 10.0 * 8.462733268737793
Epoch 680, val loss: 0.5594562888145447
Epoch 690, training loss: 85.13785552978516 = 0.52728670835495 + 10.0 * 8.46105670928955
Epoch 690, val loss: 0.5550850629806519
Epoch 700, training loss: 85.0975341796875 = 0.5225266218185425 + 10.0 * 8.457500457763672
Epoch 700, val loss: 0.5510263442993164
Epoch 710, training loss: 85.0764389038086 = 0.5179986953735352 + 10.0 * 8.455843925476074
Epoch 710, val loss: 0.5471758246421814
Epoch 720, training loss: 85.05863952636719 = 0.5136532187461853 + 10.0 * 8.454498291015625
Epoch 720, val loss: 0.5435149669647217
Epoch 730, training loss: 85.06510925292969 = 0.5094729661941528 + 10.0 * 8.45556354522705
Epoch 730, val loss: 0.5400028228759766
Epoch 740, training loss: 85.02759552001953 = 0.5054176449775696 + 10.0 * 8.452218055725098
Epoch 740, val loss: 0.5366581678390503
Epoch 750, training loss: 85.0040512084961 = 0.5015745759010315 + 10.0 * 8.450247764587402
Epoch 750, val loss: 0.5335096120834351
Epoch 760, training loss: 84.98914337158203 = 0.4978539049625397 + 10.0 * 8.449129104614258
Epoch 760, val loss: 0.5304768085479736
Epoch 770, training loss: 84.99696350097656 = 0.4942464828491211 + 10.0 * 8.450271606445312
Epoch 770, val loss: 0.5275687575340271
Epoch 780, training loss: 84.96524810791016 = 0.4907606244087219 + 10.0 * 8.44744873046875
Epoch 780, val loss: 0.5247911810874939
Epoch 790, training loss: 84.94036865234375 = 0.4874252378940582 + 10.0 * 8.445294380187988
Epoch 790, val loss: 0.522139310836792
Epoch 800, training loss: 84.92571258544922 = 0.48420068621635437 + 10.0 * 8.444150924682617
Epoch 800, val loss: 0.5195814371109009
Epoch 810, training loss: 84.90956115722656 = 0.48108673095703125 + 10.0 * 8.44284725189209
Epoch 810, val loss: 0.5171889066696167
Epoch 820, training loss: 84.9154281616211 = 0.4780479669570923 + 10.0 * 8.443737983703613
Epoch 820, val loss: 0.5148129463195801
Epoch 830, training loss: 84.91736602783203 = 0.4750576913356781 + 10.0 * 8.444231033325195
Epoch 830, val loss: 0.5126011967658997
Epoch 840, training loss: 84.87673950195312 = 0.4721987247467041 + 10.0 * 8.440454483032227
Epoch 840, val loss: 0.5104193091392517
Epoch 850, training loss: 84.8558120727539 = 0.46943625807762146 + 10.0 * 8.438637733459473
Epoch 850, val loss: 0.5083215236663818
Epoch 860, training loss: 84.84200286865234 = 0.4667404890060425 + 10.0 * 8.437525749206543
Epoch 860, val loss: 0.5063116550445557
Epoch 870, training loss: 84.83499908447266 = 0.46411368250846863 + 10.0 * 8.437088966369629
Epoch 870, val loss: 0.5044054985046387
Epoch 880, training loss: 84.81914520263672 = 0.4615327715873718 + 10.0 * 8.435761451721191
Epoch 880, val loss: 0.5024785995483398
Epoch 890, training loss: 84.80477905273438 = 0.4590355157852173 + 10.0 * 8.434574127197266
Epoch 890, val loss: 0.5006914138793945
Epoch 900, training loss: 84.7922592163086 = 0.4566129148006439 + 10.0 * 8.433565139770508
Epoch 900, val loss: 0.4989090859889984
Epoch 910, training loss: 84.80264282226562 = 0.45424139499664307 + 10.0 * 8.434840202331543
Epoch 910, val loss: 0.49717697501182556
Epoch 920, training loss: 84.7821273803711 = 0.4518847167491913 + 10.0 * 8.433024406433105
Epoch 920, val loss: 0.4956004321575165
Epoch 930, training loss: 84.75664520263672 = 0.44962403178215027 + 10.0 * 8.430702209472656
Epoch 930, val loss: 0.4939741790294647
Epoch 940, training loss: 84.74539184570312 = 0.44741758704185486 + 10.0 * 8.429797172546387
Epoch 940, val loss: 0.49240216612815857
Epoch 950, training loss: 84.73121643066406 = 0.4452560842037201 + 10.0 * 8.428595542907715
Epoch 950, val loss: 0.49090465903282166
Epoch 960, training loss: 84.72026062011719 = 0.44313985109329224 + 10.0 * 8.427712440490723
Epoch 960, val loss: 0.4894634783267975
Epoch 970, training loss: 84.72171020507812 = 0.4410495162010193 + 10.0 * 8.42806625366211
Epoch 970, val loss: 0.48800036311149597
Epoch 980, training loss: 84.74026489257812 = 0.43896380066871643 + 10.0 * 8.430130004882812
Epoch 980, val loss: 0.4865725636482239
Epoch 990, training loss: 84.70321655273438 = 0.43694478273391724 + 10.0 * 8.426627159118652
Epoch 990, val loss: 0.4851701557636261
Epoch 1000, training loss: 84.68189239501953 = 0.43498465418815613 + 10.0 * 8.424691200256348
Epoch 1000, val loss: 0.48383983969688416
Epoch 1010, training loss: 84.66871643066406 = 0.43306779861450195 + 10.0 * 8.423564910888672
Epoch 1010, val loss: 0.48260095715522766
Epoch 1020, training loss: 84.65806579589844 = 0.43118807673454285 + 10.0 * 8.422687530517578
Epoch 1020, val loss: 0.4813477694988251
Epoch 1030, training loss: 84.67281341552734 = 0.4293311834335327 + 10.0 * 8.424348831176758
Epoch 1030, val loss: 0.4800776541233063
Epoch 1040, training loss: 84.64877319335938 = 0.4274715483188629 + 10.0 * 8.42212963104248
Epoch 1040, val loss: 0.47896525263786316
Epoch 1050, training loss: 84.63728332519531 = 0.42568641901016235 + 10.0 * 8.421159744262695
Epoch 1050, val loss: 0.47775617241859436
Epoch 1060, training loss: 84.61903381347656 = 0.4239336848258972 + 10.0 * 8.419509887695312
Epoch 1060, val loss: 0.47663673758506775
Epoch 1070, training loss: 84.60903930664062 = 0.42221224308013916 + 10.0 * 8.418683052062988
Epoch 1070, val loss: 0.4755680561065674
Epoch 1080, training loss: 84.60282897949219 = 0.4205141067504883 + 10.0 * 8.418231010437012
Epoch 1080, val loss: 0.4744797945022583
Epoch 1090, training loss: 84.60206604003906 = 0.4188217520713806 + 10.0 * 8.41832447052002
Epoch 1090, val loss: 0.4733678996562958
Epoch 1100, training loss: 84.59027862548828 = 0.41715875267982483 + 10.0 * 8.417311668395996
Epoch 1100, val loss: 0.47241446375846863
Epoch 1110, training loss: 84.57575225830078 = 0.4155378043651581 + 10.0 * 8.416021347045898
Epoch 1110, val loss: 0.47135910391807556
Epoch 1120, training loss: 84.56383514404297 = 0.41393986344337463 + 10.0 * 8.414989471435547
Epoch 1120, val loss: 0.47038084268569946
Epoch 1130, training loss: 84.58560180664062 = 0.4123579263687134 + 10.0 * 8.41732406616211
Epoch 1130, val loss: 0.46942609548568726
Epoch 1140, training loss: 84.5795669555664 = 0.41074898838996887 + 10.0 * 8.416881561279297
Epoch 1140, val loss: 0.46837952733039856
Epoch 1150, training loss: 84.54442596435547 = 0.4091995060443878 + 10.0 * 8.413522720336914
Epoch 1150, val loss: 0.4674287736415863
Epoch 1160, training loss: 84.52949523925781 = 0.4076862335205078 + 10.0 * 8.41218090057373
Epoch 1160, val loss: 0.46649909019470215
Epoch 1170, training loss: 84.52239227294922 = 0.40619957447052 + 10.0 * 8.411619186401367
Epoch 1170, val loss: 0.4656049907207489
Epoch 1180, training loss: 84.55652618408203 = 0.4047192931175232 + 10.0 * 8.415181159973145
Epoch 1180, val loss: 0.4646373689174652
Epoch 1190, training loss: 84.52347564697266 = 0.4032305181026459 + 10.0 * 8.41202449798584
Epoch 1190, val loss: 0.46389248967170715
Epoch 1200, training loss: 84.50082397460938 = 0.4017823040485382 + 10.0 * 8.409904479980469
Epoch 1200, val loss: 0.46293503046035767
Epoch 1210, training loss: 84.48780822753906 = 0.400363564491272 + 10.0 * 8.408744812011719
Epoch 1210, val loss: 0.4621099829673767
Epoch 1220, training loss: 84.47907257080078 = 0.3989546597003937 + 10.0 * 8.408011436462402
Epoch 1220, val loss: 0.46127039194107056
Epoch 1230, training loss: 84.47557067871094 = 0.39755186438560486 + 10.0 * 8.407801628112793
Epoch 1230, val loss: 0.46043601632118225
Epoch 1240, training loss: 84.48194885253906 = 0.39613303542137146 + 10.0 * 8.408581733703613
Epoch 1240, val loss: 0.459574818611145
Epoch 1250, training loss: 84.46599578857422 = 0.39473602175712585 + 10.0 * 8.407125473022461
Epoch 1250, val loss: 0.45879822969436646
Epoch 1260, training loss: 84.44712829589844 = 0.3933737277984619 + 10.0 * 8.405375480651855
Epoch 1260, val loss: 0.4579324424266815
Epoch 1270, training loss: 84.44071960449219 = 0.3920292854309082 + 10.0 * 8.404869079589844
Epoch 1270, val loss: 0.4571581482887268
Epoch 1280, training loss: 84.43312072753906 = 0.39068955183029175 + 10.0 * 8.404243469238281
Epoch 1280, val loss: 0.4563690721988678
Epoch 1290, training loss: 84.47550964355469 = 0.3893483877182007 + 10.0 * 8.408616065979004
Epoch 1290, val loss: 0.45551830530166626
Epoch 1300, training loss: 84.44217681884766 = 0.3879929780960083 + 10.0 * 8.405418395996094
Epoch 1300, val loss: 0.45490217208862305
Epoch 1310, training loss: 84.41279602050781 = 0.3866749703884125 + 10.0 * 8.402612686157227
Epoch 1310, val loss: 0.45398280024528503
Epoch 1320, training loss: 84.40264892578125 = 0.3853803873062134 + 10.0 * 8.401726722717285
Epoch 1320, val loss: 0.4532465934753418
Epoch 1330, training loss: 84.39705657958984 = 0.384088933467865 + 10.0 * 8.401296615600586
Epoch 1330, val loss: 0.4525180160999298
Epoch 1340, training loss: 84.43431854248047 = 0.3827975392341614 + 10.0 * 8.405152320861816
Epoch 1340, val loss: 0.4516896605491638
Epoch 1350, training loss: 84.39474487304688 = 0.3814995288848877 + 10.0 * 8.401324272155762
Epoch 1350, val loss: 0.45103341341018677
Epoch 1360, training loss: 84.3765640258789 = 0.3802264630794525 + 10.0 * 8.399633407592773
Epoch 1360, val loss: 0.4501934349536896
Epoch 1370, training loss: 84.36882019042969 = 0.3789665400981903 + 10.0 * 8.398984909057617
Epoch 1370, val loss: 0.44951942563056946
Epoch 1380, training loss: 84.36079406738281 = 0.37770769000053406 + 10.0 * 8.398308753967285
Epoch 1380, val loss: 0.4487447142601013
Epoch 1390, training loss: 84.35374450683594 = 0.37645432353019714 + 10.0 * 8.39772891998291
Epoch 1390, val loss: 0.44799908995628357
Epoch 1400, training loss: 84.35205078125 = 0.37519776821136475 + 10.0 * 8.397685050964355
Epoch 1400, val loss: 0.44727063179016113
Epoch 1410, training loss: 84.39102172851562 = 0.3739151358604431 + 10.0 * 8.401710510253906
Epoch 1410, val loss: 0.4465084671974182
Epoch 1420, training loss: 84.3597412109375 = 0.37263765931129456 + 10.0 * 8.398710250854492
Epoch 1420, val loss: 0.44577527046203613
Epoch 1430, training loss: 84.33780670166016 = 0.3713999390602112 + 10.0 * 8.39664077758789
Epoch 1430, val loss: 0.4449157416820526
Epoch 1440, training loss: 84.3258285522461 = 0.370169073343277 + 10.0 * 8.3955659866333
Epoch 1440, val loss: 0.4442557990550995
Epoch 1450, training loss: 84.317138671875 = 0.36894410848617554 + 10.0 * 8.394819259643555
Epoch 1450, val loss: 0.4434587359428406
Epoch 1460, training loss: 84.31180572509766 = 0.367717444896698 + 10.0 * 8.3944091796875
Epoch 1460, val loss: 0.4427196681499481
Epoch 1470, training loss: 84.35962677001953 = 0.3664773106575012 + 10.0 * 8.399314880371094
Epoch 1470, val loss: 0.4419265389442444
Epoch 1480, training loss: 84.30696868896484 = 0.3652360141277313 + 10.0 * 8.394173622131348
Epoch 1480, val loss: 0.4412429630756378
Epoch 1490, training loss: 84.29302978515625 = 0.3640133738517761 + 10.0 * 8.392901420593262
Epoch 1490, val loss: 0.44044652581214905
Epoch 1500, training loss: 84.28836059570312 = 0.3628010153770447 + 10.0 * 8.392556190490723
Epoch 1500, val loss: 0.4396934509277344
Epoch 1510, training loss: 84.28479766845703 = 0.3615914285182953 + 10.0 * 8.39232063293457
Epoch 1510, val loss: 0.4389137923717499
Epoch 1520, training loss: 84.31155395507812 = 0.36036843061447144 + 10.0 * 8.395118713378906
Epoch 1520, val loss: 0.43811818957328796
Epoch 1530, training loss: 84.27294158935547 = 0.35914263129234314 + 10.0 * 8.391379356384277
Epoch 1530, val loss: 0.4374193847179413
Epoch 1540, training loss: 84.27967071533203 = 0.35793402791023254 + 10.0 * 8.392173767089844
Epoch 1540, val loss: 0.43666762113571167
Epoch 1550, training loss: 84.26573181152344 = 0.3567216694355011 + 10.0 * 8.390901565551758
Epoch 1550, val loss: 0.4359029531478882
Epoch 1560, training loss: 84.25946807861328 = 0.35551875829696655 + 10.0 * 8.39039421081543
Epoch 1560, val loss: 0.4351089894771576
Epoch 1570, training loss: 84.24752044677734 = 0.35431718826293945 + 10.0 * 8.389320373535156
Epoch 1570, val loss: 0.434396892786026
Epoch 1580, training loss: 84.24241638183594 = 0.35311582684516907 + 10.0 * 8.38892936706543
Epoch 1580, val loss: 0.43361160159111023
Epoch 1590, training loss: 84.23869323730469 = 0.3519098162651062 + 10.0 * 8.388677597045898
Epoch 1590, val loss: 0.4328378736972809
Epoch 1600, training loss: 84.26228332519531 = 0.3506965637207031 + 10.0 * 8.391159057617188
Epoch 1600, val loss: 0.4320569932460785
Epoch 1610, training loss: 84.22601318359375 = 0.3494747281074524 + 10.0 * 8.387654304504395
Epoch 1610, val loss: 0.43133893609046936
Epoch 1620, training loss: 84.21864318847656 = 0.34826308488845825 + 10.0 * 8.38703727722168
Epoch 1620, val loss: 0.43051448464393616
Epoch 1630, training loss: 84.23538208007812 = 0.3470507264137268 + 10.0 * 8.388833045959473
Epoch 1630, val loss: 0.42979106307029724
Epoch 1640, training loss: 84.21143341064453 = 0.34583401679992676 + 10.0 * 8.386560440063477
Epoch 1640, val loss: 0.4289843738079071
Epoch 1650, training loss: 84.20297241210938 = 0.34461966156959534 + 10.0 * 8.385835647583008
Epoch 1650, val loss: 0.42826133966445923
Epoch 1660, training loss: 84.19853210449219 = 0.3434094786643982 + 10.0 * 8.385512351989746
Epoch 1660, val loss: 0.42745262384414673
Epoch 1670, training loss: 84.2203369140625 = 0.3421923518180847 + 10.0 * 8.38781452178955
Epoch 1670, val loss: 0.4266792833805084
Epoch 1680, training loss: 84.19468688964844 = 0.340955525636673 + 10.0 * 8.38537311553955
Epoch 1680, val loss: 0.4259468913078308
Epoch 1690, training loss: 84.19046783447266 = 0.33972853422164917 + 10.0 * 8.3850736618042
Epoch 1690, val loss: 0.4251408576965332
Epoch 1700, training loss: 84.17814636230469 = 0.3385073244571686 + 10.0 * 8.383963584899902
Epoch 1700, val loss: 0.4244368076324463
Epoch 1710, training loss: 84.17305755615234 = 0.33728599548339844 + 10.0 * 8.383577346801758
Epoch 1710, val loss: 0.4236316978931427
Epoch 1720, training loss: 84.19866943359375 = 0.33605894446372986 + 10.0 * 8.386260986328125
Epoch 1720, val loss: 0.4228818714618683
Epoch 1730, training loss: 84.16503143310547 = 0.3348156809806824 + 10.0 * 8.383021354675293
Epoch 1730, val loss: 0.4220481514930725
Epoch 1740, training loss: 84.16871643066406 = 0.33357739448547363 + 10.0 * 8.383513450622559
Epoch 1740, val loss: 0.4212448298931122
Epoch 1750, training loss: 84.15005493164062 = 0.33233991265296936 + 10.0 * 8.3817720413208
Epoch 1750, val loss: 0.420574814081192
Epoch 1760, training loss: 84.147216796875 = 0.33110833168029785 + 10.0 * 8.381610870361328
Epoch 1760, val loss: 0.41977816820144653
Epoch 1770, training loss: 84.1397705078125 = 0.32987555861473083 + 10.0 * 8.380990028381348
Epoch 1770, val loss: 0.4189833700656891
Epoch 1780, training loss: 84.14004516601562 = 0.32863473892211914 + 10.0 * 8.38114070892334
Epoch 1780, val loss: 0.418186753988266
Epoch 1790, training loss: 84.1475601196289 = 0.3273836076259613 + 10.0 * 8.382017135620117
Epoch 1790, val loss: 0.41743457317352295
Epoch 1800, training loss: 84.14212799072266 = 0.32612353563308716 + 10.0 * 8.381600379943848
Epoch 1800, val loss: 0.41668757796287537
Epoch 1810, training loss: 84.1256332397461 = 0.3248710334300995 + 10.0 * 8.38007640838623
Epoch 1810, val loss: 0.4158579111099243
Epoch 1820, training loss: 84.118896484375 = 0.3236244022846222 + 10.0 * 8.37952709197998
Epoch 1820, val loss: 0.4150715172290802
Epoch 1830, training loss: 84.12801361083984 = 0.3223837614059448 + 10.0 * 8.380563735961914
Epoch 1830, val loss: 0.41427409648895264
Epoch 1840, training loss: 84.12433624267578 = 0.3211325407028198 + 10.0 * 8.38032054901123
Epoch 1840, val loss: 0.41364586353302
Epoch 1850, training loss: 84.10242462158203 = 0.31988465785980225 + 10.0 * 8.378253936767578
Epoch 1850, val loss: 0.41290536522865295
Epoch 1860, training loss: 84.10045623779297 = 0.3186439573764801 + 10.0 * 8.378181457519531
Epoch 1860, val loss: 0.4121658205986023
Epoch 1870, training loss: 84.09569549560547 = 0.3173995018005371 + 10.0 * 8.377829551696777
Epoch 1870, val loss: 0.41150006651878357
Epoch 1880, training loss: 84.10759735107422 = 0.31615132093429565 + 10.0 * 8.379144668579102
Epoch 1880, val loss: 0.41077011823654175
Epoch 1890, training loss: 84.0865478515625 = 0.31489118933677673 + 10.0 * 8.377165794372559
Epoch 1890, val loss: 0.41006067395210266
Epoch 1900, training loss: 84.08232879638672 = 0.31363677978515625 + 10.0 * 8.376869201660156
Epoch 1900, val loss: 0.40935975313186646
Epoch 1910, training loss: 84.10443115234375 = 0.3123830556869507 + 10.0 * 8.379204750061035
Epoch 1910, val loss: 0.40871867537498474
Epoch 1920, training loss: 84.07262420654297 = 0.31111881136894226 + 10.0 * 8.376150131225586
Epoch 1920, val loss: 0.40802180767059326
Epoch 1930, training loss: 84.06781768798828 = 0.3098653554916382 + 10.0 * 8.375795364379883
Epoch 1930, val loss: 0.4073430597782135
Epoch 1940, training loss: 84.0631103515625 = 0.30861443281173706 + 10.0 * 8.375449180603027
Epoch 1940, val loss: 0.40670540928840637
Epoch 1950, training loss: 84.05874633789062 = 0.30736392736434937 + 10.0 * 8.375138282775879
Epoch 1950, val loss: 0.4060523808002472
Epoch 1960, training loss: 84.08631134033203 = 0.3061157166957855 + 10.0 * 8.378019332885742
Epoch 1960, val loss: 0.40532341599464417
Epoch 1970, training loss: 84.07088470458984 = 0.30484524369239807 + 10.0 * 8.376604080200195
Epoch 1970, val loss: 0.404936283826828
Epoch 1980, training loss: 84.0524673461914 = 0.30359968543052673 + 10.0 * 8.374887466430664
Epoch 1980, val loss: 0.4041464626789093
Epoch 1990, training loss: 84.04383087158203 = 0.30236324667930603 + 10.0 * 8.374147415161133
Epoch 1990, val loss: 0.40358567237854004
Epoch 2000, training loss: 84.0410385131836 = 0.30112388730049133 + 10.0 * 8.373991012573242
Epoch 2000, val loss: 0.403006911277771
Epoch 2010, training loss: 84.08416748046875 = 0.2998824119567871 + 10.0 * 8.37842845916748
Epoch 2010, val loss: 0.40240558981895447
Epoch 2020, training loss: 84.05174255371094 = 0.29862168431282043 + 10.0 * 8.375311851501465
Epoch 2020, val loss: 0.4019217789173126
Epoch 2030, training loss: 84.0341796875 = 0.29739025235176086 + 10.0 * 8.373678207397461
Epoch 2030, val loss: 0.4012591242790222
Epoch 2040, training loss: 84.02466583251953 = 0.29615306854248047 + 10.0 * 8.372851371765137
Epoch 2040, val loss: 0.40079909563064575
Epoch 2050, training loss: 84.01940155029297 = 0.29492539167404175 + 10.0 * 8.372447967529297
Epoch 2050, val loss: 0.40022987127304077
Epoch 2060, training loss: 84.02120208740234 = 0.29369473457336426 + 10.0 * 8.372751235961914
Epoch 2060, val loss: 0.39974281191825867
Epoch 2070, training loss: 84.04745483398438 = 0.2924615740776062 + 10.0 * 8.37549877166748
Epoch 2070, val loss: 0.39923179149627686
Epoch 2080, training loss: 84.01783752441406 = 0.29123252630233765 + 10.0 * 8.372660636901855
Epoch 2080, val loss: 0.3986367881298065
Epoch 2090, training loss: 84.00704193115234 = 0.2900083065032959 + 10.0 * 8.371703147888184
Epoch 2090, val loss: 0.3983149826526642
Epoch 2100, training loss: 84.00056457519531 = 0.28879088163375854 + 10.0 * 8.371177673339844
Epoch 2100, val loss: 0.3978002965450287
Epoch 2110, training loss: 83.99645233154297 = 0.2875726819038391 + 10.0 * 8.370887756347656
Epoch 2110, val loss: 0.3974131941795349
Epoch 2120, training loss: 83.99246215820312 = 0.2863621115684509 + 10.0 * 8.370610237121582
Epoch 2120, val loss: 0.39697685837745667
Epoch 2130, training loss: 83.9908218383789 = 0.2851523160934448 + 10.0 * 8.370567321777344
Epoch 2130, val loss: 0.39652886986732483
Epoch 2140, training loss: 84.0656967163086 = 0.28394001722335815 + 10.0 * 8.378175735473633
Epoch 2140, val loss: 0.39607223868370056
Epoch 2150, training loss: 83.99781799316406 = 0.2826988399028778 + 10.0 * 8.371511459350586
Epoch 2150, val loss: 0.3958509564399719
Epoch 2160, training loss: 83.98680877685547 = 0.28150391578674316 + 10.0 * 8.37053108215332
Epoch 2160, val loss: 0.3953917920589447
Epoch 2170, training loss: 83.9794692993164 = 0.2803100347518921 + 10.0 * 8.369915962219238
Epoch 2170, val loss: 0.3950868546962738
Epoch 2180, training loss: 83.97323608398438 = 0.2791256606578827 + 10.0 * 8.369410514831543
Epoch 2180, val loss: 0.39474549889564514
Epoch 2190, training loss: 83.96813201904297 = 0.27794280648231506 + 10.0 * 8.3690185546875
Epoch 2190, val loss: 0.39441803097724915
Epoch 2200, training loss: 83.96480560302734 = 0.2767605781555176 + 10.0 * 8.368803977966309
Epoch 2200, val loss: 0.3941119611263275
Epoch 2210, training loss: 83.96131134033203 = 0.2755798399448395 + 10.0 * 8.368573188781738
Epoch 2210, val loss: 0.3938235640525818
Epoch 2220, training loss: 83.96566009521484 = 0.27440106868743896 + 10.0 * 8.369126319885254
Epoch 2220, val loss: 0.3935544788837433
Epoch 2230, training loss: 83.98351287841797 = 0.27321404218673706 + 10.0 * 8.3710298538208
Epoch 2230, val loss: 0.39332136511802673
Epoch 2240, training loss: 83.97479248046875 = 0.27203691005706787 + 10.0 * 8.370275497436523
Epoch 2240, val loss: 0.3929918110370636
Epoch 2250, training loss: 83.95002746582031 = 0.2708735167980194 + 10.0 * 8.367915153503418
Epoch 2250, val loss: 0.39279717206954956
Epoch 2260, training loss: 83.9468765258789 = 0.2697201073169708 + 10.0 * 8.367715835571289
Epoch 2260, val loss: 0.3925210237503052
Epoch 2270, training loss: 83.94252014160156 = 0.2685699760913849 + 10.0 * 8.367395401000977
Epoch 2270, val loss: 0.3924226760864258
Epoch 2280, training loss: 83.938720703125 = 0.2674238383769989 + 10.0 * 8.3671293258667
Epoch 2280, val loss: 0.39221858978271484
Epoch 2290, training loss: 83.9355239868164 = 0.26627975702285767 + 10.0 * 8.366924285888672
Epoch 2290, val loss: 0.39207136631011963
Epoch 2300, training loss: 83.94327545166016 = 0.2651394009590149 + 10.0 * 8.367814064025879
Epoch 2300, val loss: 0.3919566571712494
Epoch 2310, training loss: 83.96057891845703 = 0.26399752497673035 + 10.0 * 8.369657516479492
Epoch 2310, val loss: 0.39179709553718567
Epoch 2320, training loss: 83.928955078125 = 0.262868732213974 + 10.0 * 8.366608619689941
Epoch 2320, val loss: 0.39170584082603455
Epoch 2330, training loss: 83.92839050292969 = 0.26175180077552795 + 10.0 * 8.366663932800293
Epoch 2330, val loss: 0.39153286814689636
Epoch 2340, training loss: 83.92141723632812 = 0.26063263416290283 + 10.0 * 8.36607837677002
Epoch 2340, val loss: 0.391520231962204
Epoch 2350, training loss: 83.91683959960938 = 0.2595255672931671 + 10.0 * 8.365731239318848
Epoch 2350, val loss: 0.39143210649490356
Epoch 2360, training loss: 83.91373443603516 = 0.25841793417930603 + 10.0 * 8.365531921386719
Epoch 2360, val loss: 0.3913506269454956
Epoch 2370, training loss: 83.91969299316406 = 0.2573164701461792 + 10.0 * 8.36623764038086
Epoch 2370, val loss: 0.39122045040130615
Epoch 2380, training loss: 83.92704772949219 = 0.25621068477630615 + 10.0 * 8.367083549499512
Epoch 2380, val loss: 0.39125141501426697
Epoch 2390, training loss: 83.9075927734375 = 0.2551126182079315 + 10.0 * 8.36524772644043
Epoch 2390, val loss: 0.39118102192878723
Epoch 2400, training loss: 83.90294647216797 = 0.25402048230171204 + 10.0 * 8.364892959594727
Epoch 2400, val loss: 0.3912186622619629
Epoch 2410, training loss: 83.89960479736328 = 0.2529370188713074 + 10.0 * 8.364666938781738
Epoch 2410, val loss: 0.3911477327346802
Epoch 2420, training loss: 83.89958953857422 = 0.25185805559158325 + 10.0 * 8.36477279663086
Epoch 2420, val loss: 0.3911598324775696
Epoch 2430, training loss: 83.95341491699219 = 0.2507854104042053 + 10.0 * 8.37026309967041
Epoch 2430, val loss: 0.39112403988838196
Epoch 2440, training loss: 83.90721130371094 = 0.24970851838588715 + 10.0 * 8.365750312805176
Epoch 2440, val loss: 0.39119967818260193
Epoch 2450, training loss: 83.89253997802734 = 0.2486475110054016 + 10.0 * 8.364389419555664
Epoch 2450, val loss: 0.391205370426178
Epoch 2460, training loss: 83.88536834716797 = 0.2475874423980713 + 10.0 * 8.363778114318848
Epoch 2460, val loss: 0.39123237133026123
Epoch 2470, training loss: 83.88066101074219 = 0.24653375148773193 + 10.0 * 8.363412857055664
Epoch 2470, val loss: 0.39127111434936523
Epoch 2480, training loss: 83.87832641601562 = 0.24548417329788208 + 10.0 * 8.36328411102295
Epoch 2480, val loss: 0.3913358747959137
Epoch 2490, training loss: 83.8926773071289 = 0.24444247782230377 + 10.0 * 8.364823341369629
Epoch 2490, val loss: 0.39133039116859436
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8376458650431253
0.8650293414475115
=== training gcn model ===
Epoch 0, training loss: 106.9167251586914 = 1.0937236547470093 + 10.0 * 10.582300186157227
Epoch 0, val loss: 1.0913662910461426
Epoch 10, training loss: 106.90911102294922 = 1.0887093544006348 + 10.0 * 10.582040786743164
Epoch 10, val loss: 1.0864471197128296
Epoch 20, training loss: 106.89104461669922 = 1.0833795070648193 + 10.0 * 10.580766677856445
Epoch 20, val loss: 1.0812422037124634
Epoch 30, training loss: 106.82415771484375 = 1.07766854763031 + 10.0 * 10.5746488571167
Epoch 30, val loss: 1.0756937265396118
Epoch 40, training loss: 106.56927490234375 = 1.0714293718338013 + 10.0 * 10.549784660339355
Epoch 40, val loss: 1.0696403980255127
Epoch 50, training loss: 105.76792907714844 = 1.0643409490585327 + 10.0 * 10.470358848571777
Epoch 50, val loss: 1.0627431869506836
Epoch 60, training loss: 103.78318786621094 = 1.0564732551574707 + 10.0 * 10.272671699523926
Epoch 60, val loss: 1.0551127195358276
Epoch 70, training loss: 100.27606201171875 = 1.0477699041366577 + 10.0 * 9.922829627990723
Epoch 70, val loss: 1.0466421842575073
Epoch 80, training loss: 98.22655487060547 = 1.0402758121490479 + 10.0 * 9.7186279296875
Epoch 80, val loss: 1.0397374629974365
Epoch 90, training loss: 97.17176055908203 = 1.0354570150375366 + 10.0 * 9.613630294799805
Epoch 90, val loss: 1.035248041152954
Epoch 100, training loss: 96.00269317626953 = 1.03195059299469 + 10.0 * 9.497074127197266
Epoch 100, val loss: 1.0318822860717773
Epoch 110, training loss: 94.60565948486328 = 1.0283979177474976 + 10.0 * 9.357726097106934
Epoch 110, val loss: 1.0283989906311035
Epoch 120, training loss: 93.6933822631836 = 1.0245076417922974 + 10.0 * 9.266887664794922
Epoch 120, val loss: 1.0245537757873535
Epoch 130, training loss: 93.34729766845703 = 1.0202739238739014 + 10.0 * 9.232702255249023
Epoch 130, val loss: 1.0203742980957031
Epoch 140, training loss: 93.012939453125 = 1.0162044763565063 + 10.0 * 9.199673652648926
Epoch 140, val loss: 1.0164214372634888
Epoch 150, training loss: 92.45775604248047 = 1.0119125843048096 + 10.0 * 9.144584655761719
Epoch 150, val loss: 1.0122946500778198
Epoch 160, training loss: 91.58839416503906 = 1.0082111358642578 + 10.0 * 9.058018684387207
Epoch 160, val loss: 1.0088794231414795
Epoch 170, training loss: 90.83999633789062 = 1.004805326461792 + 10.0 * 8.983518600463867
Epoch 170, val loss: 1.0054221153259277
Epoch 180, training loss: 90.4096908569336 = 0.9994290471076965 + 10.0 * 8.94102668762207
Epoch 180, val loss: 1.0000542402267456
Epoch 190, training loss: 89.89590454101562 = 0.9941614866256714 + 10.0 * 8.89017391204834
Epoch 190, val loss: 0.9950258731842041
Epoch 200, training loss: 89.54365539550781 = 0.9890157580375671 + 10.0 * 8.855463981628418
Epoch 200, val loss: 0.9899598956108093
Epoch 210, training loss: 89.25431060791016 = 0.9836033582687378 + 10.0 * 8.827070236206055
Epoch 210, val loss: 0.9846936464309692
Epoch 220, training loss: 88.8904037475586 = 0.9780465960502625 + 10.0 * 8.791234970092773
Epoch 220, val loss: 0.9792723059654236
Epoch 230, training loss: 88.473876953125 = 0.972723662853241 + 10.0 * 8.750115394592285
Epoch 230, val loss: 0.9740800261497498
Epoch 240, training loss: 88.16960906982422 = 0.9668434262275696 + 10.0 * 8.720276832580566
Epoch 240, val loss: 0.9681875705718994
Epoch 250, training loss: 87.9405746459961 = 0.9596384763717651 + 10.0 * 8.69809341430664
Epoch 250, val loss: 0.9611133933067322
Epoch 260, training loss: 87.73219299316406 = 0.9516080021858215 + 10.0 * 8.678058624267578
Epoch 260, val loss: 0.9532337188720703
Epoch 270, training loss: 87.54344177246094 = 0.9431573748588562 + 10.0 * 8.660028457641602
Epoch 270, val loss: 0.9449892044067383
Epoch 280, training loss: 87.35961151123047 = 0.9345337748527527 + 10.0 * 8.642507553100586
Epoch 280, val loss: 0.9366234540939331
Epoch 290, training loss: 87.22492218017578 = 0.9255020618438721 + 10.0 * 8.629941940307617
Epoch 290, val loss: 0.9278351664543152
Epoch 300, training loss: 87.05223083496094 = 0.9158728718757629 + 10.0 * 8.613636016845703
Epoch 300, val loss: 0.9184808731079102
Epoch 310, training loss: 86.91822814941406 = 0.9056916832923889 + 10.0 * 8.601253509521484
Epoch 310, val loss: 0.9086259007453918
Epoch 320, training loss: 86.80519104003906 = 0.8949652314186096 + 10.0 * 8.591022491455078
Epoch 320, val loss: 0.8982008695602417
Epoch 330, training loss: 86.69286346435547 = 0.8838271498680115 + 10.0 * 8.580904006958008
Epoch 330, val loss: 0.8874449729919434
Epoch 340, training loss: 86.580322265625 = 0.872448205947876 + 10.0 * 8.57078742980957
Epoch 340, val loss: 0.8765408396720886
Epoch 350, training loss: 86.50135803222656 = 0.8608418703079224 + 10.0 * 8.564051628112793
Epoch 350, val loss: 0.8654786348342896
Epoch 360, training loss: 86.37565612792969 = 0.8491196632385254 + 10.0 * 8.552653312683105
Epoch 360, val loss: 0.8542355895042419
Epoch 370, training loss: 86.28486633300781 = 0.8372989296913147 + 10.0 * 8.544756889343262
Epoch 370, val loss: 0.8428908586502075
Epoch 380, training loss: 86.229736328125 = 0.8252930641174316 + 10.0 * 8.540444374084473
Epoch 380, val loss: 0.8314892053604126
Epoch 390, training loss: 86.13645935058594 = 0.813021719455719 + 10.0 * 8.532343864440918
Epoch 390, val loss: 0.8198748230934143
Epoch 400, training loss: 86.0489501953125 = 0.8007509708404541 + 10.0 * 8.524820327758789
Epoch 400, val loss: 0.8082358241081238
Epoch 410, training loss: 86.01582336425781 = 0.7882821559906006 + 10.0 * 8.522753715515137
Epoch 410, val loss: 0.7965065240859985
Epoch 420, training loss: 85.92804718017578 = 0.7758036851882935 + 10.0 * 8.51522445678711
Epoch 420, val loss: 0.7846880555152893
Epoch 430, training loss: 85.86625671386719 = 0.7634736895561218 + 10.0 * 8.510278701782227
Epoch 430, val loss: 0.7731133699417114
Epoch 440, training loss: 85.81153869628906 = 0.7511965036392212 + 10.0 * 8.506033897399902
Epoch 440, val loss: 0.7615851163864136
Epoch 450, training loss: 85.77491760253906 = 0.7389999032020569 + 10.0 * 8.503591537475586
Epoch 450, val loss: 0.7501104474067688
Epoch 460, training loss: 85.73479461669922 = 0.726841390132904 + 10.0 * 8.500795364379883
Epoch 460, val loss: 0.7388937473297119
Epoch 470, training loss: 85.67709350585938 = 0.7149350047111511 + 10.0 * 8.4962158203125
Epoch 470, val loss: 0.7278027534484863
Epoch 480, training loss: 85.64277648925781 = 0.7032198309898376 + 10.0 * 8.493955612182617
Epoch 480, val loss: 0.7169167995452881
Epoch 490, training loss: 85.60540771484375 = 0.6916568279266357 + 10.0 * 8.491374969482422
Epoch 490, val loss: 0.7061797380447388
Epoch 500, training loss: 85.56857299804688 = 0.6803900003433228 + 10.0 * 8.488818168640137
Epoch 500, val loss: 0.695817232131958
Epoch 510, training loss: 85.53152465820312 = 0.6694228053092957 + 10.0 * 8.486209869384766
Epoch 510, val loss: 0.6856631636619568
Epoch 520, training loss: 85.49569702148438 = 0.6586798429489136 + 10.0 * 8.483701705932617
Epoch 520, val loss: 0.6758036613464355
Epoch 530, training loss: 85.4883041381836 = 0.6482338905334473 + 10.0 * 8.484006881713867
Epoch 530, val loss: 0.6662147045135498
Epoch 540, training loss: 85.43016052246094 = 0.6380479335784912 + 10.0 * 8.479211807250977
Epoch 540, val loss: 0.6568449139595032
Epoch 550, training loss: 85.39717864990234 = 0.6282132863998413 + 10.0 * 8.476896286010742
Epoch 550, val loss: 0.6478012800216675
Epoch 560, training loss: 85.38985443115234 = 0.6186877489089966 + 10.0 * 8.477116584777832
Epoch 560, val loss: 0.6390169858932495
Epoch 570, training loss: 85.3610610961914 = 0.6094297766685486 + 10.0 * 8.475163459777832
Epoch 570, val loss: 0.6306878924369812
Epoch 580, training loss: 85.30388641357422 = 0.6005837321281433 + 10.0 * 8.470330238342285
Epoch 580, val loss: 0.622511625289917
Epoch 590, training loss: 85.2702407836914 = 0.5920570492744446 + 10.0 * 8.467818260192871
Epoch 590, val loss: 0.6146949529647827
Epoch 600, training loss: 85.24105072021484 = 0.5838367342948914 + 10.0 * 8.465721130371094
Epoch 600, val loss: 0.6072046160697937
Epoch 610, training loss: 85.25930786132812 = 0.5759435296058655 + 10.0 * 8.46833610534668
Epoch 610, val loss: 0.5999565124511719
Epoch 620, training loss: 85.19230651855469 = 0.5682541131973267 + 10.0 * 8.46240520477295
Epoch 620, val loss: 0.5929915308952332
Epoch 630, training loss: 85.16303253173828 = 0.5610304474830627 + 10.0 * 8.460200309753418
Epoch 630, val loss: 0.5864310264587402
Epoch 640, training loss: 85.13417053222656 = 0.5540879368782043 + 10.0 * 8.458008766174316
Epoch 640, val loss: 0.5801581144332886
Epoch 650, training loss: 85.10765075683594 = 0.5474477410316467 + 10.0 * 8.45602035522461
Epoch 650, val loss: 0.5741639733314514
Epoch 660, training loss: 85.1214370727539 = 0.541062593460083 + 10.0 * 8.458037376403809
Epoch 660, val loss: 0.5683366060256958
Epoch 670, training loss: 85.0691909790039 = 0.5348702073097229 + 10.0 * 8.453432083129883
Epoch 670, val loss: 0.5628255605697632
Epoch 680, training loss: 85.04420471191406 = 0.5290442109107971 + 10.0 * 8.451516151428223
Epoch 680, val loss: 0.557658314704895
Epoch 690, training loss: 85.02132415771484 = 0.523490309715271 + 10.0 * 8.449783325195312
Epoch 690, val loss: 0.5527374148368835
Epoch 700, training loss: 85.00187683105469 = 0.5181674957275391 + 10.0 * 8.448370933532715
Epoch 700, val loss: 0.5480557084083557
Epoch 710, training loss: 84.98292541503906 = 0.5130861401557922 + 10.0 * 8.44698429107666
Epoch 710, val loss: 0.5435954332351685
Epoch 720, training loss: 85.01739501953125 = 0.5081847906112671 + 10.0 * 8.450921058654785
Epoch 720, val loss: 0.5393765568733215
Epoch 730, training loss: 84.96170806884766 = 0.5035178661346436 + 10.0 * 8.445818901062012
Epoch 730, val loss: 0.5352354049682617
Epoch 740, training loss: 84.9353256225586 = 0.49909764528274536 + 10.0 * 8.443622589111328
Epoch 740, val loss: 0.5314307808876038
Epoch 750, training loss: 84.92384338378906 = 0.4948747456073761 + 10.0 * 8.442896842956543
Epoch 750, val loss: 0.5278395414352417
Epoch 760, training loss: 84.90483093261719 = 0.4908362030982971 + 10.0 * 8.441399574279785
Epoch 760, val loss: 0.5244172215461731
Epoch 770, training loss: 84.88833618164062 = 0.4869750738143921 + 10.0 * 8.440135955810547
Epoch 770, val loss: 0.521155595779419
Epoch 780, training loss: 84.87215423583984 = 0.4833177626132965 + 10.0 * 8.438883781433105
Epoch 780, val loss: 0.518147885799408
Epoch 790, training loss: 84.85614013671875 = 0.4798051416873932 + 10.0 * 8.437633514404297
Epoch 790, val loss: 0.5152456760406494
Epoch 800, training loss: 84.84234619140625 = 0.476432740688324 + 10.0 * 8.436591148376465
Epoch 800, val loss: 0.5125094652175903
Epoch 810, training loss: 84.86360168457031 = 0.47318413853645325 + 10.0 * 8.439042091369629
Epoch 810, val loss: 0.509922981262207
Epoch 820, training loss: 84.82772064208984 = 0.47003939747810364 + 10.0 * 8.435768127441406
Epoch 820, val loss: 0.507296085357666
Epoch 830, training loss: 84.80787658691406 = 0.46706047654151917 + 10.0 * 8.434081077575684
Epoch 830, val loss: 0.5050022602081299
Epoch 840, training loss: 84.78984832763672 = 0.46421635150909424 + 10.0 * 8.432562828063965
Epoch 840, val loss: 0.5027543306350708
Epoch 850, training loss: 84.78356170654297 = 0.46147629618644714 + 10.0 * 8.432208061218262
Epoch 850, val loss: 0.5006220936775208
Epoch 860, training loss: 84.77398681640625 = 0.45880258083343506 + 10.0 * 8.4315185546875
Epoch 860, val loss: 0.4985160231590271
Epoch 870, training loss: 84.76366424560547 = 0.45622798800468445 + 10.0 * 8.430743217468262
Epoch 870, val loss: 0.4966253936290741
Epoch 880, training loss: 84.74170684814453 = 0.45377299189567566 + 10.0 * 8.428792953491211
Epoch 880, val loss: 0.49473416805267334
Epoch 890, training loss: 84.73082733154297 = 0.45139145851135254 + 10.0 * 8.427943229675293
Epoch 890, val loss: 0.4929468333721161
Epoch 900, training loss: 84.7315444946289 = 0.44905298948287964 + 10.0 * 8.42824935913086
Epoch 900, val loss: 0.4912894368171692
Epoch 910, training loss: 84.7221450805664 = 0.44672074913978577 + 10.0 * 8.427541732788086
Epoch 910, val loss: 0.4894477427005768
Epoch 920, training loss: 84.70072937011719 = 0.44447624683380127 + 10.0 * 8.425625801086426
Epoch 920, val loss: 0.4878646433353424
Epoch 930, training loss: 84.68834686279297 = 0.44232988357543945 + 10.0 * 8.424601554870605
Epoch 930, val loss: 0.48631614446640015
Epoch 940, training loss: 84.67327880859375 = 0.4402334690093994 + 10.0 * 8.423304557800293
Epoch 940, val loss: 0.48478999733924866
Epoch 950, training loss: 84.66291046142578 = 0.43817177414894104 + 10.0 * 8.422473907470703
Epoch 950, val loss: 0.4832548499107361
Epoch 960, training loss: 84.65142822265625 = 0.4361221492290497 + 10.0 * 8.421530723571777
Epoch 960, val loss: 0.4817717671394348
Epoch 970, training loss: 84.64073181152344 = 0.43411508202552795 + 10.0 * 8.420661926269531
Epoch 970, val loss: 0.48030710220336914
Epoch 980, training loss: 84.69817352294922 = 0.43212130665779114 + 10.0 * 8.426605224609375
Epoch 980, val loss: 0.47893279790878296
Epoch 990, training loss: 84.63946533203125 = 0.43005838990211487 + 10.0 * 8.420940399169922
Epoch 990, val loss: 0.4772995412349701
Epoch 1000, training loss: 84.62338256835938 = 0.42812874913215637 + 10.0 * 8.419525146484375
Epoch 1000, val loss: 0.4759540557861328
Epoch 1010, training loss: 84.6083755493164 = 0.4262565076351166 + 10.0 * 8.418211936950684
Epoch 1010, val loss: 0.4746301472187042
Epoch 1020, training loss: 84.5925064086914 = 0.42441487312316895 + 10.0 * 8.41680908203125
Epoch 1020, val loss: 0.47329461574554443
Epoch 1030, training loss: 84.58356475830078 = 0.4225656986236572 + 10.0 * 8.416099548339844
Epoch 1030, val loss: 0.47201982140541077
Epoch 1040, training loss: 84.57329559326172 = 0.4207095205783844 + 10.0 * 8.415258407592773
Epoch 1040, val loss: 0.4707488715648651
Epoch 1050, training loss: 84.56413269042969 = 0.41885146498680115 + 10.0 * 8.414527893066406
Epoch 1050, val loss: 0.4694701135158539
Epoch 1060, training loss: 84.55553436279297 = 0.4169999063014984 + 10.0 * 8.413853645324707
Epoch 1060, val loss: 0.46819978952407837
Epoch 1070, training loss: 84.61177062988281 = 0.41515055298805237 + 10.0 * 8.419661521911621
Epoch 1070, val loss: 0.46694985032081604
Epoch 1080, training loss: 84.55323028564453 = 0.41327595710754395 + 10.0 * 8.413995742797852
Epoch 1080, val loss: 0.4656243324279785
Epoch 1090, training loss: 84.53917694091797 = 0.41148513555526733 + 10.0 * 8.412769317626953
Epoch 1090, val loss: 0.46442362666130066
Epoch 1100, training loss: 84.52153015136719 = 0.40970882773399353 + 10.0 * 8.411182403564453
Epoch 1100, val loss: 0.4632159173488617
Epoch 1110, training loss: 84.51551818847656 = 0.4079226553440094 + 10.0 * 8.410759925842285
Epoch 1110, val loss: 0.4620378613471985
Epoch 1120, training loss: 84.57159423828125 = 0.40610700845718384 + 10.0 * 8.416548728942871
Epoch 1120, val loss: 0.46086350083351135
Epoch 1130, training loss: 84.51083374023438 = 0.4042849540710449 + 10.0 * 8.41065502166748
Epoch 1130, val loss: 0.45950889587402344
Epoch 1140, training loss: 84.49034881591797 = 0.40250155329704285 + 10.0 * 8.408784866333008
Epoch 1140, val loss: 0.45831266045570374
Epoch 1150, training loss: 84.4845962524414 = 0.4007224440574646 + 10.0 * 8.408388137817383
Epoch 1150, val loss: 0.45712903141975403
Epoch 1160, training loss: 84.4754867553711 = 0.39894387125968933 + 10.0 * 8.407654762268066
Epoch 1160, val loss: 0.45592641830444336
Epoch 1170, training loss: 84.46739196777344 = 0.39716836810112 + 10.0 * 8.407022476196289
Epoch 1170, val loss: 0.45477935671806335
Epoch 1180, training loss: 84.459716796875 = 0.39539533853530884 + 10.0 * 8.406432151794434
Epoch 1180, val loss: 0.4535777270793915
Epoch 1190, training loss: 84.46809387207031 = 0.3936241865158081 + 10.0 * 8.40744686126709
Epoch 1190, val loss: 0.45239466428756714
Epoch 1200, training loss: 84.50025177001953 = 0.3918229937553406 + 10.0 * 8.410842895507812
Epoch 1200, val loss: 0.4512871503829956
Epoch 1210, training loss: 84.45843505859375 = 0.39007309079170227 + 10.0 * 8.406835556030273
Epoch 1210, val loss: 0.4501335024833679
Epoch 1220, training loss: 84.44002532958984 = 0.3883577585220337 + 10.0 * 8.405166625976562
Epoch 1220, val loss: 0.4489786624908447
Epoch 1230, training loss: 84.42802429199219 = 0.3866421580314636 + 10.0 * 8.404138565063477
Epoch 1230, val loss: 0.44788238406181335
Epoch 1240, training loss: 84.41978454589844 = 0.3849380314350128 + 10.0 * 8.403484344482422
Epoch 1240, val loss: 0.44679346680641174
Epoch 1250, training loss: 84.41281127929688 = 0.3832317292690277 + 10.0 * 8.402957916259766
Epoch 1250, val loss: 0.4457502067089081
Epoch 1260, training loss: 84.4062271118164 = 0.3815271556377411 + 10.0 * 8.402469635009766
Epoch 1260, val loss: 0.44471004605293274
Epoch 1270, training loss: 84.40034484863281 = 0.37981775403022766 + 10.0 * 8.402052879333496
Epoch 1270, val loss: 0.44369447231292725
Epoch 1280, training loss: 84.43697357177734 = 0.3780978322029114 + 10.0 * 8.405887603759766
Epoch 1280, val loss: 0.44275009632110596
Epoch 1290, training loss: 84.41226196289062 = 0.3763574957847595 + 10.0 * 8.403590202331543
Epoch 1290, val loss: 0.44153568148612976
Epoch 1300, training loss: 84.3882827758789 = 0.3746504485607147 + 10.0 * 8.401363372802734
Epoch 1300, val loss: 0.4406653940677643
Epoch 1310, training loss: 84.37663269042969 = 0.37294599413871765 + 10.0 * 8.400368690490723
Epoch 1310, val loss: 0.43969646096229553
Epoch 1320, training loss: 84.37085723876953 = 0.37124088406562805 + 10.0 * 8.399961471557617
Epoch 1320, val loss: 0.4387208819389343
Epoch 1330, training loss: 84.36766052246094 = 0.3695423901081085 + 10.0 * 8.399811744689941
Epoch 1330, val loss: 0.4378960132598877
Epoch 1340, training loss: 84.39186096191406 = 0.36781975626945496 + 10.0 * 8.402403831481934
Epoch 1340, val loss: 0.436935156583786
Epoch 1350, training loss: 84.35462188720703 = 0.36611998081207275 + 10.0 * 8.398850440979004
Epoch 1350, val loss: 0.43599632382392883
Epoch 1360, training loss: 84.34923553466797 = 0.3644304871559143 + 10.0 * 8.398480415344238
Epoch 1360, val loss: 0.4351031184196472
Epoch 1370, training loss: 84.34161376953125 = 0.3627499043941498 + 10.0 * 8.397886276245117
Epoch 1370, val loss: 0.43422138690948486
Epoch 1380, training loss: 84.33515167236328 = 0.36106306314468384 + 10.0 * 8.397409439086914
Epoch 1380, val loss: 0.4333854913711548
Epoch 1390, training loss: 84.37007904052734 = 0.35939425230026245 + 10.0 * 8.401067733764648
Epoch 1390, val loss: 0.43261998891830444
Epoch 1400, training loss: 84.35013580322266 = 0.3576657474040985 + 10.0 * 8.399247169494629
Epoch 1400, val loss: 0.43156498670578003
Epoch 1410, training loss: 84.32042694091797 = 0.3560100197792053 + 10.0 * 8.396441459655762
Epoch 1410, val loss: 0.43083545565605164
Epoch 1420, training loss: 84.31553649902344 = 0.35435932874679565 + 10.0 * 8.396117210388184
Epoch 1420, val loss: 0.43006545305252075
Epoch 1430, training loss: 84.30793762207031 = 0.35271185636520386 + 10.0 * 8.395522117614746
Epoch 1430, val loss: 0.4292363226413727
Epoch 1440, training loss: 84.30255126953125 = 0.35104888677597046 + 10.0 * 8.395150184631348
Epoch 1440, val loss: 0.42844459414482117
Epoch 1450, training loss: 84.33728790283203 = 0.34940385818481445 + 10.0 * 8.398788452148438
Epoch 1450, val loss: 0.4275968074798584
Epoch 1460, training loss: 84.3056640625 = 0.3477235436439514 + 10.0 * 8.395793914794922
Epoch 1460, val loss: 0.4270344376564026
Epoch 1470, training loss: 84.29515075683594 = 0.34610864520072937 + 10.0 * 8.394904136657715
Epoch 1470, val loss: 0.4262142479419708
Epoch 1480, training loss: 84.28341674804688 = 0.3444749116897583 + 10.0 * 8.39389419555664
Epoch 1480, val loss: 0.4255985617637634
Epoch 1490, training loss: 84.27637481689453 = 0.34283924102783203 + 10.0 * 8.393353462219238
Epoch 1490, val loss: 0.42486563324928284
Epoch 1500, training loss: 84.2704849243164 = 0.34120070934295654 + 10.0 * 8.392928123474121
Epoch 1500, val loss: 0.42422136664390564
Epoch 1510, training loss: 84.26490020751953 = 0.339572936296463 + 10.0 * 8.392532348632812
Epoch 1510, val loss: 0.42355090379714966
Epoch 1520, training loss: 84.33443450927734 = 0.33795642852783203 + 10.0 * 8.39964771270752
Epoch 1520, val loss: 0.422899454832077
Epoch 1530, training loss: 84.28678894042969 = 0.3363252282142639 + 10.0 * 8.39504623413086
Epoch 1530, val loss: 0.4223520755767822
Epoch 1540, training loss: 84.26165771484375 = 0.33474093675613403 + 10.0 * 8.392691612243652
Epoch 1540, val loss: 0.4216223657131195
Epoch 1550, training loss: 84.24725341796875 = 0.33316361904144287 + 10.0 * 8.391408920288086
Epoch 1550, val loss: 0.42113685607910156
Epoch 1560, training loss: 84.2406234741211 = 0.331611305475235 + 10.0 * 8.390901565551758
Epoch 1560, val loss: 0.42053693532943726
Epoch 1570, training loss: 84.23441314697266 = 0.33007171750068665 + 10.0 * 8.390434265136719
Epoch 1570, val loss: 0.42007532715797424
Epoch 1580, training loss: 84.22943878173828 = 0.3285478949546814 + 10.0 * 8.39008903503418
Epoch 1580, val loss: 0.4195709526538849
Epoch 1590, training loss: 84.22465515136719 = 0.327040433883667 + 10.0 * 8.389760971069336
Epoch 1590, val loss: 0.4191533327102661
Epoch 1600, training loss: 84.27615356445312 = 0.32554665207862854 + 10.0 * 8.395060539245605
Epoch 1600, val loss: 0.4186306595802307
Epoch 1610, training loss: 84.2611312866211 = 0.3239987790584564 + 10.0 * 8.393712997436523
Epoch 1610, val loss: 0.41838309168815613
Epoch 1620, training loss: 84.21128845214844 = 0.3225283920764923 + 10.0 * 8.388875961303711
Epoch 1620, val loss: 0.4179462492465973
Epoch 1630, training loss: 84.2101821899414 = 0.3210606575012207 + 10.0 * 8.388912200927734
Epoch 1630, val loss: 0.41750454902648926
Epoch 1640, training loss: 84.20142364501953 = 0.31959405541419983 + 10.0 * 8.388182640075684
Epoch 1640, val loss: 0.4170929789543152
Epoch 1650, training loss: 84.19625854492188 = 0.3181333839893341 + 10.0 * 8.387812614440918
Epoch 1650, val loss: 0.41675931215286255
Epoch 1660, training loss: 84.1937026977539 = 0.31667569279670715 + 10.0 * 8.387702941894531
Epoch 1660, val loss: 0.41636592149734497
Epoch 1670, training loss: 84.24055480957031 = 0.31522247195243835 + 10.0 * 8.392533302307129
Epoch 1670, val loss: 0.41591864824295044
Epoch 1680, training loss: 84.19253540039062 = 0.31381508708000183 + 10.0 * 8.387872695922852
Epoch 1680, val loss: 0.41580870747566223
Epoch 1690, training loss: 84.17742156982422 = 0.3124125301837921 + 10.0 * 8.386500358581543
Epoch 1690, val loss: 0.41553544998168945
Epoch 1700, training loss: 84.17561340332031 = 0.31103864312171936 + 10.0 * 8.386457443237305
Epoch 1700, val loss: 0.41526275873184204
Epoch 1710, training loss: 84.20115661621094 = 0.3097060024738312 + 10.0 * 8.389144897460938
Epoch 1710, val loss: 0.4152083992958069
Epoch 1720, training loss: 84.1715316772461 = 0.3083162009716034 + 10.0 * 8.386321067810059
Epoch 1720, val loss: 0.4148563742637634
Epoch 1730, training loss: 84.16222381591797 = 0.30699896812438965 + 10.0 * 8.385522842407227
Epoch 1730, val loss: 0.4147912561893463
Epoch 1740, training loss: 84.15580749511719 = 0.30566316843032837 + 10.0 * 8.385014533996582
Epoch 1740, val loss: 0.41462498903274536
Epoch 1750, training loss: 84.15156555175781 = 0.3043490946292877 + 10.0 * 8.384721755981445
Epoch 1750, val loss: 0.41455045342445374
Epoch 1760, training loss: 84.15199279785156 = 0.303027480840683 + 10.0 * 8.384897232055664
Epoch 1760, val loss: 0.4144567847251892
Epoch 1770, training loss: 84.18586730957031 = 0.30170950293540955 + 10.0 * 8.388415336608887
Epoch 1770, val loss: 0.4144796133041382
Epoch 1780, training loss: 84.14837646484375 = 0.30039945244789124 + 10.0 * 8.384798049926758
Epoch 1780, val loss: 0.4142666757106781
Epoch 1790, training loss: 84.14009857177734 = 0.2991172671318054 + 10.0 * 8.384098052978516
Epoch 1790, val loss: 0.4143320322036743
Epoch 1800, training loss: 84.13111114501953 = 0.29784104228019714 + 10.0 * 8.383326530456543
Epoch 1800, val loss: 0.4142278730869293
Epoch 1810, training loss: 84.12711334228516 = 0.29658401012420654 + 10.0 * 8.383052825927734
Epoch 1810, val loss: 0.4142668843269348
Epoch 1820, training loss: 84.12346649169922 = 0.2953304946422577 + 10.0 * 8.382813453674316
Epoch 1820, val loss: 0.4142119884490967
Epoch 1830, training loss: 84.18032836914062 = 0.29407891631126404 + 10.0 * 8.38862419128418
Epoch 1830, val loss: 0.41420021653175354
Epoch 1840, training loss: 84.14405822753906 = 0.2928586006164551 + 10.0 * 8.385119438171387
Epoch 1840, val loss: 0.41422194242477417
Epoch 1850, training loss: 84.11563110351562 = 0.2916245460510254 + 10.0 * 8.382400512695312
Epoch 1850, val loss: 0.4141232967376709
Epoch 1860, training loss: 84.10777282714844 = 0.29043981432914734 + 10.0 * 8.381732940673828
Epoch 1860, val loss: 0.41422513127326965
Epoch 1870, training loss: 84.1053237915039 = 0.28926435112953186 + 10.0 * 8.381606101989746
Epoch 1870, val loss: 0.4142720699310303
Epoch 1880, training loss: 84.13712310791016 = 0.2880827486515045 + 10.0 * 8.384903907775879
Epoch 1880, val loss: 0.4142536222934723
Epoch 1890, training loss: 84.10838317871094 = 0.28693175315856934 + 10.0 * 8.382144927978516
Epoch 1890, val loss: 0.4144931733608246
Epoch 1900, training loss: 84.09850311279297 = 0.285774827003479 + 10.0 * 8.38127326965332
Epoch 1900, val loss: 0.41451022028923035
Epoch 1910, training loss: 84.08934020996094 = 0.2846335768699646 + 10.0 * 8.380471229553223
Epoch 1910, val loss: 0.4145718812942505
Epoch 1920, training loss: 84.08489990234375 = 0.2834877073764801 + 10.0 * 8.380141258239746
Epoch 1920, val loss: 0.41462597250938416
Epoch 1930, training loss: 84.09212493896484 = 0.2823486924171448 + 10.0 * 8.380977630615234
Epoch 1930, val loss: 0.4146132469177246
Epoch 1940, training loss: 84.09317016601562 = 0.28120800852775574 + 10.0 * 8.381196975708008
Epoch 1940, val loss: 0.41488128900527954
Epoch 1950, training loss: 84.08393096923828 = 0.2800772488117218 + 10.0 * 8.380385398864746
Epoch 1950, val loss: 0.41484081745147705
Epoch 1960, training loss: 84.0757064819336 = 0.2789861857891083 + 10.0 * 8.379672050476074
Epoch 1960, val loss: 0.4151509702205658
Epoch 1970, training loss: 84.06820678710938 = 0.27788248658180237 + 10.0 * 8.379032135009766
Epoch 1970, val loss: 0.4153214395046234
Epoch 1980, training loss: 84.06455993652344 = 0.2768000662326813 + 10.0 * 8.378775596618652
Epoch 1980, val loss: 0.41549795866012573
Epoch 1990, training loss: 84.06097412109375 = 0.2757234275341034 + 10.0 * 8.378524780273438
Epoch 1990, val loss: 0.41575783491134644
Epoch 2000, training loss: 84.07344818115234 = 0.2746540904045105 + 10.0 * 8.37987995147705
Epoch 2000, val loss: 0.4160482585430145
Epoch 2010, training loss: 84.05494689941406 = 0.27355679869651794 + 10.0 * 8.378138542175293
Epoch 2010, val loss: 0.41612252593040466
Epoch 2020, training loss: 84.05671691894531 = 0.2725021243095398 + 10.0 * 8.378421783447266
Epoch 2020, val loss: 0.41647687554359436
Epoch 2030, training loss: 84.05484008789062 = 0.2714415192604065 + 10.0 * 8.378339767456055
Epoch 2030, val loss: 0.4166504740715027
Epoch 2040, training loss: 84.07927703857422 = 0.2704056203365326 + 10.0 * 8.380887031555176
Epoch 2040, val loss: 0.41698184609413147
Epoch 2050, training loss: 84.04698944091797 = 0.2693423926830292 + 10.0 * 8.377764701843262
Epoch 2050, val loss: 0.417251318693161
Epoch 2060, training loss: 84.03861999511719 = 0.26832181215286255 + 10.0 * 8.377030372619629
Epoch 2060, val loss: 0.41755229234695435
Epoch 2070, training loss: 84.03564453125 = 0.2672911584377289 + 10.0 * 8.376835823059082
Epoch 2070, val loss: 0.41785532236099243
Epoch 2080, training loss: 84.03218841552734 = 0.26625239849090576 + 10.0 * 8.376593589782715
Epoch 2080, val loss: 0.41816842555999756
Epoch 2090, training loss: 84.05550384521484 = 0.26523086428642273 + 10.0 * 8.379027366638184
Epoch 2090, val loss: 0.41851451992988586
Epoch 2100, training loss: 84.04325103759766 = 0.26417937874794006 + 10.0 * 8.377906799316406
Epoch 2100, val loss: 0.41886672377586365
Epoch 2110, training loss: 84.03430938720703 = 0.26315852999687195 + 10.0 * 8.377115249633789
Epoch 2110, val loss: 0.4190784692764282
Epoch 2120, training loss: 84.02098846435547 = 0.2621479332447052 + 10.0 * 8.375884056091309
Epoch 2120, val loss: 0.41949012875556946
Epoch 2130, training loss: 84.01654815673828 = 0.2611449658870697 + 10.0 * 8.375539779663086
Epoch 2130, val loss: 0.419797420501709
Epoch 2140, training loss: 84.01264190673828 = 0.2601410448551178 + 10.0 * 8.375249862670898
Epoch 2140, val loss: 0.42012423276901245
Epoch 2150, training loss: 84.02117919921875 = 0.25913485884666443 + 10.0 * 8.376204490661621
Epoch 2150, val loss: 0.420442670583725
Epoch 2160, training loss: 84.02912139892578 = 0.25812265276908875 + 10.0 * 8.377099990844727
Epoch 2160, val loss: 0.42076587677001953
Epoch 2170, training loss: 84.01190185546875 = 0.25714269280433655 + 10.0 * 8.375475883483887
Epoch 2170, val loss: 0.42102620005607605
Epoch 2180, training loss: 84.00337219238281 = 0.2561572790145874 + 10.0 * 8.37472152709961
Epoch 2180, val loss: 0.42140817642211914
Epoch 2190, training loss: 83.99825286865234 = 0.2551870346069336 + 10.0 * 8.374306678771973
Epoch 2190, val loss: 0.42172476649284363
Epoch 2200, training loss: 83.99478912353516 = 0.25421634316444397 + 10.0 * 8.374056816101074
Epoch 2200, val loss: 0.42207062244415283
Epoch 2210, training loss: 84.03766632080078 = 0.253244549036026 + 10.0 * 8.378442764282227
Epoch 2210, val loss: 0.42228201031684875
Epoch 2220, training loss: 84.00334167480469 = 0.25230640172958374 + 10.0 * 8.375103950500488
Epoch 2220, val loss: 0.42288461327552795
Epoch 2230, training loss: 83.99371337890625 = 0.2513390779495239 + 10.0 * 8.374237060546875
Epoch 2230, val loss: 0.42301812767982483
Epoch 2240, training loss: 83.98365783691406 = 0.2504027783870697 + 10.0 * 8.37332534790039
Epoch 2240, val loss: 0.42349404096603394
Epoch 2250, training loss: 83.978515625 = 0.24944962561130524 + 10.0 * 8.372906684875488
Epoch 2250, val loss: 0.4237956702709198
Epoch 2260, training loss: 83.97518920898438 = 0.24852189421653748 + 10.0 * 8.37266731262207
Epoch 2260, val loss: 0.4241463243961334
Epoch 2270, training loss: 83.98458862304688 = 0.24759019911289215 + 10.0 * 8.373700141906738
Epoch 2270, val loss: 0.42448747158050537
Epoch 2280, training loss: 83.99015045166016 = 0.24665015935897827 + 10.0 * 8.374349594116211
Epoch 2280, val loss: 0.4249090552330017
Epoch 2290, training loss: 83.97235107421875 = 0.24571023881435394 + 10.0 * 8.372663497924805
Epoch 2290, val loss: 0.42528459429740906
Epoch 2300, training loss: 83.9676742553711 = 0.2447691708803177 + 10.0 * 8.37229061126709
Epoch 2300, val loss: 0.42571309208869934
Epoch 2310, training loss: 83.95936584472656 = 0.24384459853172302 + 10.0 * 8.371552467346191
Epoch 2310, val loss: 0.42612192034721375
Epoch 2320, training loss: 83.95699310302734 = 0.2429184913635254 + 10.0 * 8.371407508850098
Epoch 2320, val loss: 0.4265633523464203
Epoch 2330, training loss: 83.97010803222656 = 0.24201327562332153 + 10.0 * 8.372809410095215
Epoch 2330, val loss: 0.42710232734680176
Epoch 2340, training loss: 83.95752716064453 = 0.24106387794017792 + 10.0 * 8.3716459274292
Epoch 2340, val loss: 0.4273753762245178
Epoch 2350, training loss: 83.97396087646484 = 0.2401466965675354 + 10.0 * 8.373380661010742
Epoch 2350, val loss: 0.4278668463230133
Epoch 2360, training loss: 83.95050048828125 = 0.23922762274742126 + 10.0 * 8.371127128601074
Epoch 2360, val loss: 0.42807891964912415
Epoch 2370, training loss: 83.94230651855469 = 0.238322451710701 + 10.0 * 8.37039852142334
Epoch 2370, val loss: 0.4286142885684967
Epoch 2380, training loss: 83.93907165527344 = 0.23741236329078674 + 10.0 * 8.370165824890137
Epoch 2380, val loss: 0.42902228236198425
Epoch 2390, training loss: 83.93746185302734 = 0.2365046888589859 + 10.0 * 8.370096206665039
Epoch 2390, val loss: 0.42942163348197937
Epoch 2400, training loss: 83.94589233398438 = 0.23561064898967743 + 10.0 * 8.371027946472168
Epoch 2400, val loss: 0.4298282265663147
Epoch 2410, training loss: 83.95502471923828 = 0.23471006751060486 + 10.0 * 8.372031211853027
Epoch 2410, val loss: 0.43032166361808777
Epoch 2420, training loss: 83.94713592529297 = 0.23382005095481873 + 10.0 * 8.371332168579102
Epoch 2420, val loss: 0.43067657947540283
Epoch 2430, training loss: 83.9303970336914 = 0.2328995019197464 + 10.0 * 8.369749069213867
Epoch 2430, val loss: 0.43107807636260986
Epoch 2440, training loss: 83.92158508300781 = 0.23201142251491547 + 10.0 * 8.36895751953125
Epoch 2440, val loss: 0.4315283000469208
Epoch 2450, training loss: 83.91846466064453 = 0.2311069816350937 + 10.0 * 8.368735313415527
Epoch 2450, val loss: 0.4319514036178589
Epoch 2460, training loss: 83.91571807861328 = 0.23022674024105072 + 10.0 * 8.368549346923828
Epoch 2460, val loss: 0.4324486553668976
Epoch 2470, training loss: 83.94303131103516 = 0.2293442040681839 + 10.0 * 8.371368408203125
Epoch 2470, val loss: 0.43297043442726135
Epoch 2480, training loss: 83.92894744873047 = 0.2284325361251831 + 10.0 * 8.370051383972168
Epoch 2480, val loss: 0.4331802725791931
Epoch 2490, training loss: 83.91407012939453 = 0.22754697501659393 + 10.0 * 8.36865234375
Epoch 2490, val loss: 0.43365105986595154
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8346017250126838
0.8641599652249512
=== training gcn model ===
Epoch 0, training loss: 106.92605590820312 = 1.1033600568771362 + 10.0 * 10.582269668579102
Epoch 0, val loss: 1.1011213064193726
Epoch 10, training loss: 106.91770935058594 = 1.098504900932312 + 10.0 * 10.581920623779297
Epoch 10, val loss: 1.096373438835144
Epoch 20, training loss: 106.89611053466797 = 1.0938183069229126 + 10.0 * 10.580228805541992
Epoch 20, val loss: 1.0917726755142212
Epoch 30, training loss: 106.81338500976562 = 1.0890138149261475 + 10.0 * 10.572437286376953
Epoch 30, val loss: 1.0870418548583984
Epoch 40, training loss: 106.49901580810547 = 1.0838394165039062 + 10.0 * 10.54151725769043
Epoch 40, val loss: 1.0819343328475952
Epoch 50, training loss: 105.58084106445312 = 1.078153371810913 + 10.0 * 10.450268745422363
Epoch 50, val loss: 1.0763695240020752
Epoch 60, training loss: 103.5585708618164 = 1.072520136833191 + 10.0 * 10.248605728149414
Epoch 60, val loss: 1.0709115266799927
Epoch 70, training loss: 100.95359802246094 = 1.066585898399353 + 10.0 * 9.988700866699219
Epoch 70, val loss: 1.065132975578308
Epoch 80, training loss: 98.8606185913086 = 1.0601670742034912 + 10.0 * 9.780045509338379
Epoch 80, val loss: 1.058778166770935
Epoch 90, training loss: 97.10856628417969 = 1.0532302856445312 + 10.0 * 9.605533599853516
Epoch 90, val loss: 1.0520848035812378
Epoch 100, training loss: 95.05360412597656 = 1.04789400100708 + 10.0 * 9.4005708694458
Epoch 100, val loss: 1.0471994876861572
Epoch 110, training loss: 93.53475189208984 = 1.044764757156372 + 10.0 * 9.248998641967773
Epoch 110, val loss: 1.0442818403244019
Epoch 120, training loss: 92.98955535888672 = 1.0408926010131836 + 10.0 * 9.194866180419922
Epoch 120, val loss: 1.0404821634292603
Epoch 130, training loss: 92.18396759033203 = 1.0358983278274536 + 10.0 * 9.11480712890625
Epoch 130, val loss: 1.0356987714767456
Epoch 140, training loss: 91.19522857666016 = 1.0313953161239624 + 10.0 * 9.016383171081543
Epoch 140, val loss: 1.0314570665359497
Epoch 150, training loss: 90.4615707397461 = 1.0278223752975464 + 10.0 * 8.943374633789062
Epoch 150, val loss: 1.0280108451843262
Epoch 160, training loss: 89.90596771240234 = 1.0244836807250977 + 10.0 * 8.888148307800293
Epoch 160, val loss: 1.0247224569320679
Epoch 170, training loss: 89.46041107177734 = 1.020761251449585 + 10.0 * 8.843965530395508
Epoch 170, val loss: 1.0210025310516357
Epoch 180, training loss: 89.09890747070312 = 1.0162571668624878 + 10.0 * 8.80826473236084
Epoch 180, val loss: 1.0165669918060303
Epoch 190, training loss: 88.73859405517578 = 1.0117285251617432 + 10.0 * 8.772686958312988
Epoch 190, val loss: 1.0121883153915405
Epoch 200, training loss: 88.4921875 = 1.0071077346801758 + 10.0 * 8.748507499694824
Epoch 200, val loss: 1.0076422691345215
Epoch 210, training loss: 88.27018737792969 = 1.001699447631836 + 10.0 * 8.726848602294922
Epoch 210, val loss: 1.0023400783538818
Epoch 220, training loss: 88.07066345214844 = 0.9958803653717041 + 10.0 * 8.707478523254395
Epoch 220, val loss: 0.996695339679718
Epoch 230, training loss: 87.925537109375 = 0.9900345206260681 + 10.0 * 8.693550109863281
Epoch 230, val loss: 0.9909375309944153
Epoch 240, training loss: 87.80425262451172 = 0.9837973713874817 + 10.0 * 8.682045936584473
Epoch 240, val loss: 0.984830379486084
Epoch 250, training loss: 87.68305206298828 = 0.977002739906311 + 10.0 * 8.670604705810547
Epoch 250, val loss: 0.9781391620635986
Epoch 260, training loss: 87.56615447998047 = 0.969819962978363 + 10.0 * 8.65963363647461
Epoch 260, val loss: 0.9710627198219299
Epoch 270, training loss: 87.41973876953125 = 0.9624529480934143 + 10.0 * 8.64572811126709
Epoch 270, val loss: 0.9638673067092896
Epoch 280, training loss: 87.27198791503906 = 0.9549387693405151 + 10.0 * 8.631704330444336
Epoch 280, val loss: 0.9565024375915527
Epoch 290, training loss: 87.17838287353516 = 0.9471556544303894 + 10.0 * 8.623122215270996
Epoch 290, val loss: 0.9487950205802917
Epoch 300, training loss: 87.017578125 = 0.938694179058075 + 10.0 * 8.607888221740723
Epoch 300, val loss: 0.9405540227890015
Epoch 310, training loss: 86.89879608154297 = 0.9298540353775024 + 10.0 * 8.596894264221191
Epoch 310, val loss: 0.9319161176681519
Epoch 320, training loss: 86.79168701171875 = 0.9206335544586182 + 10.0 * 8.587105751037598
Epoch 320, val loss: 0.92286217212677
Epoch 330, training loss: 86.72904205322266 = 0.9110046029090881 + 10.0 * 8.581804275512695
Epoch 330, val loss: 0.9134109616279602
Epoch 340, training loss: 86.62503814697266 = 0.9009280800819397 + 10.0 * 8.57241153717041
Epoch 340, val loss: 0.9035419225692749
Epoch 350, training loss: 86.54920196533203 = 0.8905212879180908 + 10.0 * 8.565868377685547
Epoch 350, val loss: 0.8933590650558472
Epoch 360, training loss: 86.48936462402344 = 0.8797827959060669 + 10.0 * 8.560957908630371
Epoch 360, val loss: 0.8828542828559875
Epoch 370, training loss: 86.43578338623047 = 0.8687559366226196 + 10.0 * 8.556702613830566
Epoch 370, val loss: 0.8720951676368713
Epoch 380, training loss: 86.39303588867188 = 0.8574709296226501 + 10.0 * 8.553556442260742
Epoch 380, val loss: 0.8611088395118713
Epoch 390, training loss: 86.3443374633789 = 0.8460672497749329 + 10.0 * 8.549826622009277
Epoch 390, val loss: 0.8500566482543945
Epoch 400, training loss: 86.29438781738281 = 0.8347485065460205 + 10.0 * 8.545964241027832
Epoch 400, val loss: 0.8390620350837708
Epoch 410, training loss: 86.24034881591797 = 0.823371171951294 + 10.0 * 8.54169750213623
Epoch 410, val loss: 0.8280414938926697
Epoch 420, training loss: 86.23930358886719 = 0.8119906187057495 + 10.0 * 8.542731285095215
Epoch 420, val loss: 0.8169339299201965
Epoch 430, training loss: 86.1488265991211 = 0.8005316257476807 + 10.0 * 8.534830093383789
Epoch 430, val loss: 0.805908203125
Epoch 440, training loss: 86.0840835571289 = 0.7892789840698242 + 10.0 * 8.529480934143066
Epoch 440, val loss: 0.7950621247291565
Epoch 450, training loss: 86.01953125 = 0.7780518531799316 + 10.0 * 8.524147987365723
Epoch 450, val loss: 0.7842228412628174
Epoch 460, training loss: 85.95587921142578 = 0.7668135166168213 + 10.0 * 8.518906593322754
Epoch 460, val loss: 0.7734136581420898
Epoch 470, training loss: 85.91085815429688 = 0.7555834650993347 + 10.0 * 8.515527725219727
Epoch 470, val loss: 0.7625170350074768
Epoch 480, training loss: 85.83313751220703 = 0.7443618178367615 + 10.0 * 8.508877754211426
Epoch 480, val loss: 0.7517571449279785
Epoch 490, training loss: 85.76979064941406 = 0.7332010269165039 + 10.0 * 8.50365924835205
Epoch 490, val loss: 0.7410460710525513
Epoch 500, training loss: 85.76701354980469 = 0.7220722436904907 + 10.0 * 8.504494667053223
Epoch 500, val loss: 0.7302402853965759
Epoch 510, training loss: 85.67986297607422 = 0.7108012437820435 + 10.0 * 8.496906280517578
Epoch 510, val loss: 0.7194833159446716
Epoch 520, training loss: 85.62062072753906 = 0.6997378468513489 + 10.0 * 8.492088317871094
Epoch 520, val loss: 0.7089028358459473
Epoch 530, training loss: 85.57150268554688 = 0.6887961030006409 + 10.0 * 8.48827075958252
Epoch 530, val loss: 0.6984298229217529
Epoch 540, training loss: 85.54822540283203 = 0.6779609322547913 + 10.0 * 8.48702621459961
Epoch 540, val loss: 0.6880652904510498
Epoch 550, training loss: 85.50741577148438 = 0.6671435236930847 + 10.0 * 8.484026908874512
Epoch 550, val loss: 0.677727460861206
Epoch 560, training loss: 85.45690155029297 = 0.6566212177276611 + 10.0 * 8.48002815246582
Epoch 560, val loss: 0.6677007675170898
Epoch 570, training loss: 85.41825103759766 = 0.6463382840156555 + 10.0 * 8.477190971374512
Epoch 570, val loss: 0.6579290628433228
Epoch 580, training loss: 85.3819351196289 = 0.6362331509590149 + 10.0 * 8.474570274353027
Epoch 580, val loss: 0.6483572721481323
Epoch 590, training loss: 85.35018920898438 = 0.6263275146484375 + 10.0 * 8.472386360168457
Epoch 590, val loss: 0.6389964818954468
Epoch 600, training loss: 85.3453598022461 = 0.6166201829910278 + 10.0 * 8.47287368774414
Epoch 600, val loss: 0.6298251748085022
Epoch 610, training loss: 85.2892074584961 = 0.6071246266365051 + 10.0 * 8.468208312988281
Epoch 610, val loss: 0.6209608316421509
Epoch 620, training loss: 85.2573471069336 = 0.5979903340339661 + 10.0 * 8.465935707092285
Epoch 620, val loss: 0.6124375462532043
Epoch 630, training loss: 85.2281494140625 = 0.5891650319099426 + 10.0 * 8.463898658752441
Epoch 630, val loss: 0.6042176485061646
Epoch 640, training loss: 85.20213317871094 = 0.5806450843811035 + 10.0 * 8.462148666381836
Epoch 640, val loss: 0.5963073372840881
Epoch 650, training loss: 85.19479370117188 = 0.5723620057106018 + 10.0 * 8.46224308013916
Epoch 650, val loss: 0.5886609554290771
Epoch 660, training loss: 85.16665649414062 = 0.5643956661224365 + 10.0 * 8.460226058959961
Epoch 660, val loss: 0.5813598036766052
Epoch 670, training loss: 85.12712097167969 = 0.5569497346878052 + 10.0 * 8.457016944885254
Epoch 670, val loss: 0.5745564699172974
Epoch 680, training loss: 85.10432434082031 = 0.5497695803642273 + 10.0 * 8.455455780029297
Epoch 680, val loss: 0.5680205821990967
Epoch 690, training loss: 85.08071899414062 = 0.542877197265625 + 10.0 * 8.453783988952637
Epoch 690, val loss: 0.561800479888916
Epoch 700, training loss: 85.09861755371094 = 0.5362575650215149 + 10.0 * 8.456235885620117
Epoch 700, val loss: 0.5558636784553528
Epoch 710, training loss: 85.05062103271484 = 0.5299668312072754 + 10.0 * 8.452065467834473
Epoch 710, val loss: 0.5502288937568665
Epoch 720, training loss: 85.01871490478516 = 0.5240224599838257 + 10.0 * 8.449468612670898
Epoch 720, val loss: 0.5449719429016113
Epoch 730, training loss: 84.99544525146484 = 0.5183554291725159 + 10.0 * 8.447709083557129
Epoch 730, val loss: 0.5399972796440125
Epoch 740, training loss: 84.97469329833984 = 0.5129832625389099 + 10.0 * 8.446170806884766
Epoch 740, val loss: 0.5353212356567383
Epoch 750, training loss: 84.99427032470703 = 0.5078017711639404 + 10.0 * 8.448646545410156
Epoch 750, val loss: 0.530833899974823
Epoch 760, training loss: 84.93800354003906 = 0.5028730034828186 + 10.0 * 8.443512916564941
Epoch 760, val loss: 0.5266356468200684
Epoch 770, training loss: 84.91835021972656 = 0.4982697367668152 + 10.0 * 8.442008018493652
Epoch 770, val loss: 0.5227695107460022
Epoch 780, training loss: 84.900634765625 = 0.49383363127708435 + 10.0 * 8.440679550170898
Epoch 780, val loss: 0.5190900564193726
Epoch 790, training loss: 84.88455200195312 = 0.4896053969860077 + 10.0 * 8.439494132995605
Epoch 790, val loss: 0.515567421913147
Epoch 800, training loss: 84.88607025146484 = 0.48552680015563965 + 10.0 * 8.440053939819336
Epoch 800, val loss: 0.512233316898346
Epoch 810, training loss: 84.86703491210938 = 0.4816439151763916 + 10.0 * 8.438539505004883
Epoch 810, val loss: 0.509100615978241
Epoch 820, training loss: 84.840087890625 = 0.4779796898365021 + 10.0 * 8.436210632324219
Epoch 820, val loss: 0.5061754584312439
Epoch 830, training loss: 84.8207015991211 = 0.4744637906551361 + 10.0 * 8.434623718261719
Epoch 830, val loss: 0.5033720135688782
Epoch 840, training loss: 84.84870147705078 = 0.4710898697376251 + 10.0 * 8.437761306762695
Epoch 840, val loss: 0.5007631182670593
Epoch 850, training loss: 84.80706787109375 = 0.4677661061286926 + 10.0 * 8.433930397033691
Epoch 850, val loss: 0.49820858240127563
Epoch 860, training loss: 84.77710723876953 = 0.4647030532360077 + 10.0 * 8.43124008178711
Epoch 860, val loss: 0.49586087465286255
Epoch 870, training loss: 84.76480102539062 = 0.46173250675201416 + 10.0 * 8.430307388305664
Epoch 870, val loss: 0.4936416745185852
Epoch 880, training loss: 84.75064849853516 = 0.45885658264160156 + 10.0 * 8.429179191589355
Epoch 880, val loss: 0.4915030896663666
Epoch 890, training loss: 84.737548828125 = 0.456094890832901 + 10.0 * 8.428145408630371
Epoch 890, val loss: 0.4894629418849945
Epoch 900, training loss: 84.8075180053711 = 0.45342177152633667 + 10.0 * 8.435409545898438
Epoch 900, val loss: 0.4875513017177582
Epoch 910, training loss: 84.71651458740234 = 0.4507286250591278 + 10.0 * 8.426578521728516
Epoch 910, val loss: 0.48558786511421204
Epoch 920, training loss: 84.70423126220703 = 0.44824865460395813 + 10.0 * 8.42559814453125
Epoch 920, val loss: 0.4838085174560547
Epoch 930, training loss: 84.692626953125 = 0.4458385705947876 + 10.0 * 8.424678802490234
Epoch 930, val loss: 0.48208722472190857
Epoch 940, training loss: 84.68114471435547 = 0.4434707462787628 + 10.0 * 8.42376708984375
Epoch 940, val loss: 0.480444997549057
Epoch 950, training loss: 84.67060852050781 = 0.4411775469779968 + 10.0 * 8.422943115234375
Epoch 950, val loss: 0.478854775428772
Epoch 960, training loss: 84.67740631103516 = 0.4389384984970093 + 10.0 * 8.423846244812012
Epoch 960, val loss: 0.47729745507240295
Epoch 970, training loss: 84.67642974853516 = 0.4367145001888275 + 10.0 * 8.423971176147461
Epoch 970, val loss: 0.4757976830005646
Epoch 980, training loss: 84.64381408691406 = 0.4345921277999878 + 10.0 * 8.42092227935791
Epoch 980, val loss: 0.47435685992240906
Epoch 990, training loss: 84.63516235351562 = 0.43257272243499756 + 10.0 * 8.420259475708008
Epoch 990, val loss: 0.47299477458000183
Epoch 1000, training loss: 84.62374877929688 = 0.43059077858924866 + 10.0 * 8.419316291809082
Epoch 1000, val loss: 0.471684992313385
Epoch 1010, training loss: 84.61396026611328 = 0.42864587903022766 + 10.0 * 8.41853141784668
Epoch 1010, val loss: 0.47041961550712585
Epoch 1020, training loss: 84.60618591308594 = 0.4267385005950928 + 10.0 * 8.41794490814209
Epoch 1020, val loss: 0.46919944882392883
Epoch 1030, training loss: 84.6336898803711 = 0.4248700439929962 + 10.0 * 8.420881271362305
Epoch 1030, val loss: 0.4680268168449402
Epoch 1040, training loss: 84.58952331542969 = 0.423022598028183 + 10.0 * 8.416650772094727
Epoch 1040, val loss: 0.46673399209976196
Epoch 1050, training loss: 84.58079528808594 = 0.4212476909160614 + 10.0 * 8.41595458984375
Epoch 1050, val loss: 0.46553635597229004
Epoch 1060, training loss: 84.57038879394531 = 0.41949260234832764 + 10.0 * 8.41508960723877
Epoch 1060, val loss: 0.46444740891456604
Epoch 1070, training loss: 84.56097412109375 = 0.41778814792633057 + 10.0 * 8.414319038391113
Epoch 1070, val loss: 0.46332985162734985
Epoch 1080, training loss: 84.57568359375 = 0.4161053001880646 + 10.0 * 8.4159574508667
Epoch 1080, val loss: 0.4622325301170349
Epoch 1090, training loss: 84.56053924560547 = 0.4144130051136017 + 10.0 * 8.414612770080566
Epoch 1090, val loss: 0.46122267842292786
Epoch 1100, training loss: 84.54244232177734 = 0.4127870798110962 + 10.0 * 8.412965774536133
Epoch 1100, val loss: 0.46014469861984253
Epoch 1110, training loss: 84.53046417236328 = 0.4111955165863037 + 10.0 * 8.411927223205566
Epoch 1110, val loss: 0.4590887725353241
Epoch 1120, training loss: 84.5207748413086 = 0.4096309244632721 + 10.0 * 8.411114692687988
Epoch 1120, val loss: 0.45810309052467346
Epoch 1130, training loss: 84.53187561035156 = 0.40810149908065796 + 10.0 * 8.41237735748291
Epoch 1130, val loss: 0.4571745991706848
Epoch 1140, training loss: 84.52816009521484 = 0.4065595865249634 + 10.0 * 8.41215991973877
Epoch 1140, val loss: 0.45615556836128235
Epoch 1150, training loss: 84.4972152709961 = 0.4050775170326233 + 10.0 * 8.40921401977539
Epoch 1150, val loss: 0.4551909267902374
Epoch 1160, training loss: 84.48831176757812 = 0.4036239683628082 + 10.0 * 8.408468246459961
Epoch 1160, val loss: 0.45424774289131165
Epoch 1170, training loss: 84.4811019897461 = 0.4021921157836914 + 10.0 * 8.407891273498535
Epoch 1170, val loss: 0.4533541798591614
Epoch 1180, training loss: 84.47612762451172 = 0.4007919728755951 + 10.0 * 8.407533645629883
Epoch 1180, val loss: 0.4524499475955963
Epoch 1190, training loss: 84.49766540527344 = 0.39937251806259155 + 10.0 * 8.409829139709473
Epoch 1190, val loss: 0.4515426456928253
Epoch 1200, training loss: 84.47464752197266 = 0.3979978561401367 + 10.0 * 8.407665252685547
Epoch 1200, val loss: 0.45063814520835876
Epoch 1210, training loss: 84.45262908935547 = 0.39665892720222473 + 10.0 * 8.405596733093262
Epoch 1210, val loss: 0.44980624318122864
Epoch 1220, training loss: 84.44563293457031 = 0.39533039927482605 + 10.0 * 8.405030250549316
Epoch 1220, val loss: 0.44895514845848083
Epoch 1230, training loss: 84.43912506103516 = 0.3940235674381256 + 10.0 * 8.404510498046875
Epoch 1230, val loss: 0.4481191337108612
Epoch 1240, training loss: 84.4657974243164 = 0.3927217423915863 + 10.0 * 8.407307624816895
Epoch 1240, val loss: 0.44721418619155884
Epoch 1250, training loss: 84.44368743896484 = 0.391410768032074 + 10.0 * 8.405227661132812
Epoch 1250, val loss: 0.44652771949768066
Epoch 1260, training loss: 84.42213439941406 = 0.39015135169029236 + 10.0 * 8.4031982421875
Epoch 1260, val loss: 0.44563907384872437
Epoch 1270, training loss: 84.41321563720703 = 0.3889046013355255 + 10.0 * 8.402430534362793
Epoch 1270, val loss: 0.44484376907348633
Epoch 1280, training loss: 84.40673828125 = 0.3876674473285675 + 10.0 * 8.401906967163086
Epoch 1280, val loss: 0.44408711791038513
Epoch 1290, training loss: 84.40074157714844 = 0.38644900918006897 + 10.0 * 8.401429176330566
Epoch 1290, val loss: 0.44330140948295593
Epoch 1300, training loss: 84.4404296875 = 0.38522422313690186 + 10.0 * 8.40552043914795
Epoch 1300, val loss: 0.4425683915615082
Epoch 1310, training loss: 84.40538024902344 = 0.3839963972568512 + 10.0 * 8.402138710021973
Epoch 1310, val loss: 0.44167929887771606
Epoch 1320, training loss: 84.38280487060547 = 0.3827972710132599 + 10.0 * 8.40000057220459
Epoch 1320, val loss: 0.4409644901752472
Epoch 1330, training loss: 84.3763656616211 = 0.3816099762916565 + 10.0 * 8.399476051330566
Epoch 1330, val loss: 0.4402146637439728
Epoch 1340, training loss: 84.37483978271484 = 0.38044026494026184 + 10.0 * 8.399439811706543
Epoch 1340, val loss: 0.43947574496269226
Epoch 1350, training loss: 84.39469146728516 = 0.37926164269447327 + 10.0 * 8.401542663574219
Epoch 1350, val loss: 0.4387926757335663
Epoch 1360, training loss: 84.36299896240234 = 0.3781035244464874 + 10.0 * 8.398488998413086
Epoch 1360, val loss: 0.43796053528785706
Epoch 1370, training loss: 84.35371398925781 = 0.3769560158252716 + 10.0 * 8.397676467895508
Epoch 1370, val loss: 0.4372461438179016
Epoch 1380, training loss: 84.34832763671875 = 0.375824898481369 + 10.0 * 8.397250175476074
Epoch 1380, val loss: 0.4365696310997009
Epoch 1390, training loss: 84.34795379638672 = 0.374703049659729 + 10.0 * 8.39732551574707
Epoch 1390, val loss: 0.4358663260936737
Epoch 1400, training loss: 84.37007904052734 = 0.37357112765312195 + 10.0 * 8.399650573730469
Epoch 1400, val loss: 0.4351698160171509
Epoch 1410, training loss: 84.34844970703125 = 0.372465044260025 + 10.0 * 8.397598266601562
Epoch 1410, val loss: 0.4344605803489685
Epoch 1420, training loss: 84.3313980102539 = 0.37137094140052795 + 10.0 * 8.396002769470215
Epoch 1420, val loss: 0.4338527023792267
Epoch 1430, training loss: 84.32190704345703 = 0.37030357122421265 + 10.0 * 8.395160675048828
Epoch 1430, val loss: 0.43320900201797485
Epoch 1440, training loss: 84.31653594970703 = 0.3692442774772644 + 10.0 * 8.394728660583496
Epoch 1440, val loss: 0.43253692984580994
Epoch 1450, training loss: 84.31768798828125 = 0.3681904077529907 + 10.0 * 8.394949913024902
Epoch 1450, val loss: 0.431936115026474
Epoch 1460, training loss: 84.34281158447266 = 0.36711978912353516 + 10.0 * 8.39756965637207
Epoch 1460, val loss: 0.431350439786911
Epoch 1470, training loss: 84.30508422851562 = 0.36606141924858093 + 10.0 * 8.393902778625488
Epoch 1470, val loss: 0.4306004047393799
Epoch 1480, training loss: 84.29773712158203 = 0.36502113938331604 + 10.0 * 8.393271446228027
Epoch 1480, val loss: 0.43002843856811523
Epoch 1490, training loss: 84.29300689697266 = 0.3639901876449585 + 10.0 * 8.392901420593262
Epoch 1490, val loss: 0.4293961226940155
Epoch 1500, training loss: 84.28851318359375 = 0.3629635274410248 + 10.0 * 8.392555236816406
Epoch 1500, val loss: 0.428839772939682
Epoch 1510, training loss: 84.3551254272461 = 0.3619374632835388 + 10.0 * 8.39931869506836
Epoch 1510, val loss: 0.42833226919174194
Epoch 1520, training loss: 84.28426361083984 = 0.3608880341053009 + 10.0 * 8.392337799072266
Epoch 1520, val loss: 0.4276019334793091
Epoch 1530, training loss: 84.28111267089844 = 0.35986989736557007 + 10.0 * 8.39212417602539
Epoch 1530, val loss: 0.4269908666610718
Epoch 1540, training loss: 84.26795196533203 = 0.35886555910110474 + 10.0 * 8.390909194946289
Epoch 1540, val loss: 0.42642635107040405
Epoch 1550, training loss: 84.26460266113281 = 0.3578645586967468 + 10.0 * 8.390673637390137
Epoch 1550, val loss: 0.4258602559566498
Epoch 1560, training loss: 84.26468658447266 = 0.3568652868270874 + 10.0 * 8.390782356262207
Epoch 1560, val loss: 0.42527422308921814
Epoch 1570, training loss: 84.28176879882812 = 0.3558509647846222 + 10.0 * 8.39259147644043
Epoch 1570, val loss: 0.42469432950019836
Epoch 1580, training loss: 84.2520523071289 = 0.3548414409160614 + 10.0 * 8.389720916748047
Epoch 1580, val loss: 0.42409875988960266
Epoch 1590, training loss: 84.24650573730469 = 0.35384243726730347 + 10.0 * 8.389266014099121
Epoch 1590, val loss: 0.42356154322624207
Epoch 1600, training loss: 84.24574279785156 = 0.35284560918807983 + 10.0 * 8.389289855957031
Epoch 1600, val loss: 0.42297476530075073
Epoch 1610, training loss: 84.27994537353516 = 0.35184332728385925 + 10.0 * 8.392809867858887
Epoch 1610, val loss: 0.4223693907260895
Epoch 1620, training loss: 84.24439239501953 = 0.3508429229259491 + 10.0 * 8.389354705810547
Epoch 1620, val loss: 0.42180967330932617
Epoch 1630, training loss: 84.23004913330078 = 0.34984636306762695 + 10.0 * 8.388020515441895
Epoch 1630, val loss: 0.421273410320282
Epoch 1640, training loss: 84.22234344482422 = 0.34886109828948975 + 10.0 * 8.387348175048828
Epoch 1640, val loss: 0.42072343826293945
Epoch 1650, training loss: 84.2175064086914 = 0.3478798270225525 + 10.0 * 8.386962890625
Epoch 1650, val loss: 0.4201456308364868
Epoch 1660, training loss: 84.2203369140625 = 0.34689536690711975 + 10.0 * 8.387344360351562
Epoch 1660, val loss: 0.4195733368396759
Epoch 1670, training loss: 84.22784423828125 = 0.3458915650844574 + 10.0 * 8.388195037841797
Epoch 1670, val loss: 0.4190782904624939
Epoch 1680, training loss: 84.20470428466797 = 0.34489932656288147 + 10.0 * 8.385980606079102
Epoch 1680, val loss: 0.4184911847114563
Epoch 1690, training loss: 84.20162963867188 = 0.3439064025878906 + 10.0 * 8.385771751403809
Epoch 1690, val loss: 0.4180167317390442
Epoch 1700, training loss: 84.1949691772461 = 0.3429209887981415 + 10.0 * 8.385205268859863
Epoch 1700, val loss: 0.4174281358718872
Epoch 1710, training loss: 84.19003295898438 = 0.3419305682182312 + 10.0 * 8.384809494018555
Epoch 1710, val loss: 0.4169204831123352
Epoch 1720, training loss: 84.23196411132812 = 0.340941846370697 + 10.0 * 8.3891019821167
Epoch 1720, val loss: 0.41638830304145813
Epoch 1730, training loss: 84.20378112792969 = 0.3399190306663513 + 10.0 * 8.386385917663574
Epoch 1730, val loss: 0.4157779812812805
Epoch 1740, training loss: 84.18661499023438 = 0.33892807364463806 + 10.0 * 8.38476848602295
Epoch 1740, val loss: 0.41523250937461853
Epoch 1750, training loss: 84.17359924316406 = 0.3379325568675995 + 10.0 * 8.383566856384277
Epoch 1750, val loss: 0.4146973490715027
Epoch 1760, training loss: 84.1658706665039 = 0.3369508683681488 + 10.0 * 8.382891654968262
Epoch 1760, val loss: 0.4141218364238739
Epoch 1770, training loss: 84.16188049316406 = 0.33596399426460266 + 10.0 * 8.38259220123291
Epoch 1770, val loss: 0.4135947525501251
Epoch 1780, training loss: 84.20906066894531 = 0.3349777162075043 + 10.0 * 8.387408256530762
Epoch 1780, val loss: 0.413076788187027
Epoch 1790, training loss: 84.17631530761719 = 0.3339604139328003 + 10.0 * 8.384235382080078
Epoch 1790, val loss: 0.4124777019023895
Epoch 1800, training loss: 84.1537094116211 = 0.3329806625843048 + 10.0 * 8.382073402404785
Epoch 1800, val loss: 0.41191565990448
Epoch 1810, training loss: 84.14295959472656 = 0.3319930136203766 + 10.0 * 8.381096839904785
Epoch 1810, val loss: 0.4114035665988922
Epoch 1820, training loss: 84.13893127441406 = 0.33101725578308105 + 10.0 * 8.380791664123535
Epoch 1820, val loss: 0.4108981788158417
Epoch 1830, training loss: 84.13599395751953 = 0.3300332725048065 + 10.0 * 8.380596160888672
Epoch 1830, val loss: 0.41034188866615295
Epoch 1840, training loss: 84.20531463623047 = 0.3290320634841919 + 10.0 * 8.387628555297852
Epoch 1840, val loss: 0.4097828269004822
Epoch 1850, training loss: 84.12909698486328 = 0.3280295729637146 + 10.0 * 8.380106925964355
Epoch 1850, val loss: 0.4092903435230255
Epoch 1860, training loss: 84.12909698486328 = 0.3270490765571594 + 10.0 * 8.380205154418945
Epoch 1860, val loss: 0.40880683064460754
Epoch 1870, training loss: 84.11656951904297 = 0.32606229186058044 + 10.0 * 8.379050254821777
Epoch 1870, val loss: 0.40828874707221985
Epoch 1880, training loss: 84.11216735839844 = 0.32508158683776855 + 10.0 * 8.378708839416504
Epoch 1880, val loss: 0.40777650475502014
Epoch 1890, training loss: 84.1067123413086 = 0.3241022825241089 + 10.0 * 8.378260612487793
Epoch 1890, val loss: 0.4072868824005127
Epoch 1900, training loss: 84.10733032226562 = 0.3231174647808075 + 10.0 * 8.37842082977295
Epoch 1900, val loss: 0.4067589044570923
Epoch 1910, training loss: 84.1407699584961 = 0.322114497423172 + 10.0 * 8.381865501403809
Epoch 1910, val loss: 0.4062727987766266
Epoch 1920, training loss: 84.09819793701172 = 0.321117103099823 + 10.0 * 8.377708435058594
Epoch 1920, val loss: 0.4058089554309845
Epoch 1930, training loss: 84.09153747558594 = 0.3201175630092621 + 10.0 * 8.377141952514648
Epoch 1930, val loss: 0.4053991436958313
Epoch 1940, training loss: 84.08769989013672 = 0.3191198706626892 + 10.0 * 8.37685775756836
Epoch 1940, val loss: 0.4048594534397125
Epoch 1950, training loss: 84.0831069946289 = 0.3181237280368805 + 10.0 * 8.376498222351074
Epoch 1950, val loss: 0.40443137288093567
Epoch 1960, training loss: 84.117919921875 = 0.3171216547489166 + 10.0 * 8.38007926940918
Epoch 1960, val loss: 0.4039812684059143
Epoch 1970, training loss: 84.0887451171875 = 0.3160817325115204 + 10.0 * 8.377266883850098
Epoch 1970, val loss: 0.40340298414230347
Epoch 1980, training loss: 84.07609558105469 = 0.31508007645606995 + 10.0 * 8.37610149383545
Epoch 1980, val loss: 0.40294066071510315
Epoch 1990, training loss: 84.06629943847656 = 0.3140707015991211 + 10.0 * 8.375223159790039
Epoch 1990, val loss: 0.4025154113769531
Epoch 2000, training loss: 84.0624008178711 = 0.313072144985199 + 10.0 * 8.374933242797852
Epoch 2000, val loss: 0.40203240513801575
Epoch 2010, training loss: 84.05815887451172 = 0.31206655502319336 + 10.0 * 8.374608993530273
Epoch 2010, val loss: 0.40158703923225403
Epoch 2020, training loss: 84.07121276855469 = 0.3110613226890564 + 10.0 * 8.376015663146973
Epoch 2020, val loss: 0.4011326730251312
Epoch 2030, training loss: 84.05722045898438 = 0.3100351095199585 + 10.0 * 8.37471866607666
Epoch 2030, val loss: 0.4006887376308441
Epoch 2040, training loss: 84.04840850830078 = 0.30902785062789917 + 10.0 * 8.373937606811523
Epoch 2040, val loss: 0.4002762734889984
Epoch 2050, training loss: 84.04257202148438 = 0.30801883339881897 + 10.0 * 8.373455047607422
Epoch 2050, val loss: 0.39983752369880676
Epoch 2060, training loss: 84.0386962890625 = 0.3070138692855835 + 10.0 * 8.373167991638184
Epoch 2060, val loss: 0.3994174301624298
Epoch 2070, training loss: 84.03509521484375 = 0.3060058355331421 + 10.0 * 8.372908592224121
Epoch 2070, val loss: 0.39904260635375977
Epoch 2080, training loss: 84.03884887695312 = 0.3049955368041992 + 10.0 * 8.373385429382324
Epoch 2080, val loss: 0.39863818883895874
Epoch 2090, training loss: 84.04596710205078 = 0.30396053194999695 + 10.0 * 8.374200820922852
Epoch 2090, val loss: 0.39822089672088623
Epoch 2100, training loss: 84.0273666381836 = 0.302944153547287 + 10.0 * 8.372442245483398
Epoch 2100, val loss: 0.39775949716567993
Epoch 2110, training loss: 84.02640533447266 = 0.30192315578460693 + 10.0 * 8.372447967529297
Epoch 2110, val loss: 0.3973793089389801
Epoch 2120, training loss: 84.03019714355469 = 0.3009120225906372 + 10.0 * 8.372928619384766
Epoch 2120, val loss: 0.3968879282474518
Epoch 2130, training loss: 84.01910400390625 = 0.29989373683929443 + 10.0 * 8.371920585632324
Epoch 2130, val loss: 0.39656609296798706
Epoch 2140, training loss: 84.0130615234375 = 0.2988842725753784 + 10.0 * 8.371417999267578
Epoch 2140, val loss: 0.39622819423675537
Epoch 2150, training loss: 84.01182556152344 = 0.29787036776542664 + 10.0 * 8.3713960647583
Epoch 2150, val loss: 0.3958395719528198
Epoch 2160, training loss: 84.02918243408203 = 0.2968517243862152 + 10.0 * 8.3732328414917
Epoch 2160, val loss: 0.39542660117149353
Epoch 2170, training loss: 84.0105972290039 = 0.29584014415740967 + 10.0 * 8.371476173400879
Epoch 2170, val loss: 0.3950108289718628
Epoch 2180, training loss: 84.00102996826172 = 0.2948247492313385 + 10.0 * 8.370620727539062
Epoch 2180, val loss: 0.3946509063243866
Epoch 2190, training loss: 84.00298309326172 = 0.2938120663166046 + 10.0 * 8.370916366577148
Epoch 2190, val loss: 0.39425140619277954
Epoch 2200, training loss: 84.00151062011719 = 0.29279470443725586 + 10.0 * 8.370871543884277
Epoch 2200, val loss: 0.3938664495944977
Epoch 2210, training loss: 83.98602294921875 = 0.2917785942554474 + 10.0 * 8.369424819946289
Epoch 2210, val loss: 0.3934723734855652
Epoch 2220, training loss: 83.98777770996094 = 0.29076454043388367 + 10.0 * 8.369701385498047
Epoch 2220, val loss: 0.39307066798210144
Epoch 2230, training loss: 83.98905944824219 = 0.2897545099258423 + 10.0 * 8.369930267333984
Epoch 2230, val loss: 0.3926902115345001
Epoch 2240, training loss: 83.9847183227539 = 0.28874197602272034 + 10.0 * 8.369597434997559
Epoch 2240, val loss: 0.39241263270378113
Epoch 2250, training loss: 83.97421264648438 = 0.2877274751663208 + 10.0 * 8.368648529052734
Epoch 2250, val loss: 0.3920780420303345
Epoch 2260, training loss: 83.98125457763672 = 0.2867090106010437 + 10.0 * 8.369454383850098
Epoch 2260, val loss: 0.3916972279548645
Epoch 2270, training loss: 83.96788024902344 = 0.28569352626800537 + 10.0 * 8.368219375610352
Epoch 2270, val loss: 0.39135223627090454
Epoch 2280, training loss: 83.96450805664062 = 0.2846813201904297 + 10.0 * 8.367982864379883
Epoch 2280, val loss: 0.39098164439201355
Epoch 2290, training loss: 83.97430419921875 = 0.283667653799057 + 10.0 * 8.369063377380371
Epoch 2290, val loss: 0.390658438205719
Epoch 2300, training loss: 83.9582290649414 = 0.2826492190361023 + 10.0 * 8.367558479309082
Epoch 2300, val loss: 0.3903934061527252
Epoch 2310, training loss: 83.95293426513672 = 0.28163036704063416 + 10.0 * 8.367130279541016
Epoch 2310, val loss: 0.39011675119400024
Epoch 2320, training loss: 83.94961547851562 = 0.2806087136268616 + 10.0 * 8.366900444030762
Epoch 2320, val loss: 0.38980379700660706
Epoch 2330, training loss: 83.95539093017578 = 0.27958160638809204 + 10.0 * 8.367581367492676
Epoch 2330, val loss: 0.3895617723464966
Epoch 2340, training loss: 83.95375061035156 = 0.27855512499809265 + 10.0 * 8.36751937866211
Epoch 2340, val loss: 0.38929781317710876
Epoch 2350, training loss: 83.94584655761719 = 0.27752265334129333 + 10.0 * 8.366832733154297
Epoch 2350, val loss: 0.38897815346717834
Epoch 2360, training loss: 83.93989562988281 = 0.27648910880088806 + 10.0 * 8.366340637207031
Epoch 2360, val loss: 0.38873374462127686
Epoch 2370, training loss: 83.93765258789062 = 0.27546247839927673 + 10.0 * 8.366219520568848
Epoch 2370, val loss: 0.3885146677494049
Epoch 2380, training loss: 83.9523696899414 = 0.2744206190109253 + 10.0 * 8.36779499053955
Epoch 2380, val loss: 0.3882605731487274
Epoch 2390, training loss: 83.92536926269531 = 0.2733969986438751 + 10.0 * 8.36519718170166
Epoch 2390, val loss: 0.3880901336669922
Epoch 2400, training loss: 83.92097473144531 = 0.2723637521266937 + 10.0 * 8.364861488342285
Epoch 2400, val loss: 0.38790833950042725
Epoch 2410, training loss: 83.91635131835938 = 0.27133774757385254 + 10.0 * 8.364500999450684
Epoch 2410, val loss: 0.38765305280685425
Epoch 2420, training loss: 83.91219329833984 = 0.27030810713768005 + 10.0 * 8.364188194274902
Epoch 2420, val loss: 0.3875080347061157
Epoch 2430, training loss: 83.91199493408203 = 0.26927369832992554 + 10.0 * 8.364272117614746
Epoch 2430, val loss: 0.3873177170753479
Epoch 2440, training loss: 83.98099517822266 = 0.2682303190231323 + 10.0 * 8.37127685546875
Epoch 2440, val loss: 0.3871746063232422
Epoch 2450, training loss: 83.92737579345703 = 0.26716741919517517 + 10.0 * 8.366021156311035
Epoch 2450, val loss: 0.38695934414863586
Epoch 2460, training loss: 83.90420532226562 = 0.2661171853542328 + 10.0 * 8.363808631896973
Epoch 2460, val loss: 0.38683685660362244
Epoch 2470, training loss: 83.89697265625 = 0.26506882905960083 + 10.0 * 8.363190650939941
Epoch 2470, val loss: 0.38661834597587585
Epoch 2480, training loss: 83.8935775756836 = 0.26402944326400757 + 10.0 * 8.362955093383789
Epoch 2480, val loss: 0.38650086522102356
Epoch 2490, training loss: 83.89495086669922 = 0.26298630237579346 + 10.0 * 8.36319637298584
Epoch 2490, val loss: 0.38638436794281006
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8483003551496702
0.8624936607983772
The final CL Acc:0.84018, 0.00587, The final GNN Acc:0.86389, 0.00105
Begin epxeriment: noisy_level: 0.2 cont_weight: 10 epoch: der1:0.3 der2:0.2 dfr1:0.4 dfr2:0.3
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=2500, cl_num_layers=2, cl_num_proj_hidden=64, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=10.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.2, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=32, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, noisy_level=0.2, num_hidden=64, num_proj_hidden=64, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=40, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 88648])
add edge: torch.Size([2, 106286])
remove edge: torch.Size([2, 70936])
updated graph: torch.Size([2, 88574])
=== Noisy graph ===
/home/project-robust-contrastive/Robust-Contrastive-Learning/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 106.93244934082031 = 1.1098206043243408 + 10.0 * 10.582262992858887
Epoch 0, val loss: 1.11090087890625
Epoch 10, training loss: 106.9233627319336 = 1.104202151298523 + 10.0 * 10.581915855407715
Epoch 10, val loss: 1.105288028717041
Epoch 20, training loss: 106.9002914428711 = 1.0984325408935547 + 10.0 * 10.580185890197754
Epoch 20, val loss: 1.0994887351989746
Epoch 30, training loss: 106.80929565429688 = 1.0921612977981567 + 10.0 * 10.5717134475708
Epoch 30, val loss: 1.093159794807434
Epoch 40, training loss: 106.44296264648438 = 1.085185170173645 + 10.0 * 10.535778045654297
Epoch 40, val loss: 1.0861258506774902
Epoch 50, training loss: 105.24242401123047 = 1.0775411128997803 + 10.0 * 10.416488647460938
Epoch 50, val loss: 1.078438639640808
Epoch 60, training loss: 102.38268280029297 = 1.0697767734527588 + 10.0 * 10.131290435791016
Epoch 60, val loss: 1.0707225799560547
Epoch 70, training loss: 99.34113311767578 = 1.0625228881835938 + 10.0 * 9.827860832214355
Epoch 70, val loss: 1.0636762380599976
Epoch 80, training loss: 97.7454605102539 = 1.057944893836975 + 10.0 * 9.66875171661377
Epoch 80, val loss: 1.0593198537826538
Epoch 90, training loss: 96.95982360839844 = 1.054990291595459 + 10.0 * 9.590482711791992
Epoch 90, val loss: 1.0564749240875244
Epoch 100, training loss: 95.89547729492188 = 1.0523356199264526 + 10.0 * 9.48431396484375
Epoch 100, val loss: 1.0538514852523804
Epoch 110, training loss: 93.9563980102539 = 1.0499423742294312 + 10.0 * 9.290645599365234
Epoch 110, val loss: 1.051548719406128
Epoch 120, training loss: 92.28971099853516 = 1.0479174852371216 + 10.0 * 9.124178886413574
Epoch 120, val loss: 1.0495586395263672
Epoch 130, training loss: 91.287353515625 = 1.0464085340499878 + 10.0 * 9.024094581604004
Epoch 130, val loss: 1.0479966402053833
Epoch 140, training loss: 90.44723510742188 = 1.0446661710739136 + 10.0 * 8.94025707244873
Epoch 140, val loss: 1.04619562625885
Epoch 150, training loss: 89.76937103271484 = 1.0428082942962646 + 10.0 * 8.872655868530273
Epoch 150, val loss: 1.0442932844161987
Epoch 160, training loss: 89.28611755371094 = 1.0409189462661743 + 10.0 * 8.824520111083984
Epoch 160, val loss: 1.0423792600631714
Epoch 170, training loss: 88.92955017089844 = 1.0389093160629272 + 10.0 * 8.789064407348633
Epoch 170, val loss: 1.0403639078140259
Epoch 180, training loss: 88.65426635742188 = 1.0367753505706787 + 10.0 * 8.761749267578125
Epoch 180, val loss: 1.038236379623413
Epoch 190, training loss: 88.4249267578125 = 1.034498929977417 + 10.0 * 8.739042282104492
Epoch 190, val loss: 1.0359776020050049
Epoch 200, training loss: 88.2205810546875 = 1.0320894718170166 + 10.0 * 8.718849182128906
Epoch 200, val loss: 1.033591389656067
Epoch 210, training loss: 88.04682922363281 = 1.0295755863189697 + 10.0 * 8.701725006103516
Epoch 210, val loss: 1.0311473608016968
Epoch 220, training loss: 87.88349914550781 = 1.026976227760315 + 10.0 * 8.685651779174805
Epoch 220, val loss: 1.0286431312561035
Epoch 230, training loss: 87.7372055053711 = 1.0242844820022583 + 10.0 * 8.671292304992676
Epoch 230, val loss: 1.0260322093963623
Epoch 240, training loss: 87.64266967773438 = 1.021435022354126 + 10.0 * 8.66212272644043
Epoch 240, val loss: 1.0232641696929932
Epoch 250, training loss: 87.50977325439453 = 1.0183398723602295 + 10.0 * 8.64914321899414
Epoch 250, val loss: 1.0202652215957642
Epoch 260, training loss: 87.40824127197266 = 1.0150253772735596 + 10.0 * 8.639322280883789
Epoch 260, val loss: 1.017052412033081
Epoch 270, training loss: 87.32350158691406 = 1.0115147829055786 + 10.0 * 8.63119888305664
Epoch 270, val loss: 1.0136572122573853
Epoch 280, training loss: 87.25479125976562 = 1.0077320337295532 + 10.0 * 8.624706268310547
Epoch 280, val loss: 1.009992241859436
Epoch 290, training loss: 87.19315338134766 = 1.0036128759384155 + 10.0 * 8.618953704833984
Epoch 290, val loss: 1.0060327053070068
Epoch 300, training loss: 87.12144470214844 = 0.9992377758026123 + 10.0 * 8.612220764160156
Epoch 300, val loss: 1.0018147230148315
Epoch 310, training loss: 87.05892181396484 = 0.9945622682571411 + 10.0 * 8.606435775756836
Epoch 310, val loss: 0.9973283410072327
Epoch 320, training loss: 87.02880096435547 = 0.9895610809326172 + 10.0 * 8.603923797607422
Epoch 320, val loss: 0.9925440549850464
Epoch 330, training loss: 86.94660949707031 = 0.9842265248298645 + 10.0 * 8.596238136291504
Epoch 330, val loss: 0.9874379634857178
Epoch 340, training loss: 86.88469696044922 = 0.978560209274292 + 10.0 * 8.59061336517334
Epoch 340, val loss: 0.9820263981819153
Epoch 350, training loss: 86.82689666748047 = 0.9725484251976013 + 10.0 * 8.585434913635254
Epoch 350, val loss: 0.9762941598892212
Epoch 360, training loss: 86.77365112304688 = 0.9661661982536316 + 10.0 * 8.580748558044434
Epoch 360, val loss: 0.9702232480049133
Epoch 370, training loss: 86.73119354248047 = 0.9593954086303711 + 10.0 * 8.577179908752441
Epoch 370, val loss: 0.9637730717658997
Epoch 380, training loss: 86.68131256103516 = 0.9522189497947693 + 10.0 * 8.572909355163574
Epoch 380, val loss: 0.9569725394248962
Epoch 390, training loss: 86.63353729248047 = 0.9447125792503357 + 10.0 * 8.568882942199707
Epoch 390, val loss: 0.9498645067214966
Epoch 400, training loss: 86.58794403076172 = 0.9368487596511841 + 10.0 * 8.565109252929688
Epoch 400, val loss: 0.9424277544021606
Epoch 410, training loss: 86.59712219238281 = 0.9286332130432129 + 10.0 * 8.566848754882812
Epoch 410, val loss: 0.9346384406089783
Epoch 420, training loss: 86.51670837402344 = 0.9200260639190674 + 10.0 * 8.559667587280273
Epoch 420, val loss: 0.9265391826629639
Epoch 430, training loss: 86.4736328125 = 0.9111301898956299 + 10.0 * 8.556249618530273
Epoch 430, val loss: 0.9181874990463257
Epoch 440, training loss: 86.44134521484375 = 0.9019570350646973 + 10.0 * 8.553938865661621
Epoch 440, val loss: 0.9095874428749084
Epoch 450, training loss: 86.40615844726562 = 0.8924758434295654 + 10.0 * 8.55136775970459
Epoch 450, val loss: 0.9006991386413574
Epoch 460, training loss: 86.37679290771484 = 0.8827856779098511 + 10.0 * 8.54940128326416
Epoch 460, val loss: 0.8916491866111755
Epoch 470, training loss: 86.34341430664062 = 0.8729254007339478 + 10.0 * 8.547048568725586
Epoch 470, val loss: 0.8824430108070374
Epoch 480, training loss: 86.31236267089844 = 0.8628997802734375 + 10.0 * 8.544946670532227
Epoch 480, val loss: 0.8731124997138977
Epoch 490, training loss: 86.30278778076172 = 0.8527634739875793 + 10.0 * 8.545002937316895
Epoch 490, val loss: 0.8637107014656067
Epoch 500, training loss: 86.26154327392578 = 0.8425523042678833 + 10.0 * 8.541898727416992
Epoch 500, val loss: 0.8542351722717285
Epoch 510, training loss: 86.23082733154297 = 0.8323618769645691 + 10.0 * 8.539846420288086
Epoch 510, val loss: 0.8448474407196045
Epoch 520, training loss: 86.21058654785156 = 0.8222493529319763 + 10.0 * 8.538833618164062
Epoch 520, val loss: 0.8355242013931274
Epoch 530, training loss: 86.17107391357422 = 0.8121968507766724 + 10.0 * 8.535887718200684
Epoch 530, val loss: 0.8262993693351746
Epoch 540, training loss: 86.13687896728516 = 0.8022999167442322 + 10.0 * 8.53345775604248
Epoch 540, val loss: 0.8172419667243958
Epoch 550, training loss: 86.10757446289062 = 0.7925673127174377 + 10.0 * 8.531500816345215
Epoch 550, val loss: 0.8083665370941162
Epoch 560, training loss: 86.08370208740234 = 0.7830698490142822 + 10.0 * 8.530062675476074
Epoch 560, val loss: 0.7997121810913086
Epoch 570, training loss: 86.07510375976562 = 0.7737802267074585 + 10.0 * 8.530132293701172
Epoch 570, val loss: 0.7912737131118774
Epoch 580, training loss: 86.0341796875 = 0.7647255659103394 + 10.0 * 8.526945114135742
Epoch 580, val loss: 0.7831271886825562
Epoch 590, training loss: 85.99958038330078 = 0.7560200095176697 + 10.0 * 8.5243558883667
Epoch 590, val loss: 0.7752751708030701
Epoch 600, training loss: 85.98214721679688 = 0.7475928664207458 + 10.0 * 8.523455619812012
Epoch 600, val loss: 0.7677059769630432
Epoch 610, training loss: 85.94622802734375 = 0.7394880652427673 + 10.0 * 8.520673751831055
Epoch 610, val loss: 0.7604498863220215
Epoch 620, training loss: 85.91767883300781 = 0.7316915988922119 + 10.0 * 8.518598556518555
Epoch 620, val loss: 0.7534815073013306
Epoch 630, training loss: 85.89981079101562 = 0.7241907119750977 + 10.0 * 8.517561912536621
Epoch 630, val loss: 0.7467958331108093
Epoch 640, training loss: 85.88252258300781 = 0.716964602470398 + 10.0 * 8.516555786132812
Epoch 640, val loss: 0.7403907179832458
Epoch 650, training loss: 85.85074615478516 = 0.7100787162780762 + 10.0 * 8.514066696166992
Epoch 650, val loss: 0.7342385053634644
Epoch 660, training loss: 85.82630920410156 = 0.7035093307495117 + 10.0 * 8.512280464172363
Epoch 660, val loss: 0.7284300923347473
Epoch 670, training loss: 85.7962875366211 = 0.6972507238388062 + 10.0 * 8.509903907775879
Epoch 670, val loss: 0.722920835018158
Epoch 680, training loss: 85.77326965332031 = 0.6912813186645508 + 10.0 * 8.508198738098145
Epoch 680, val loss: 0.7176629900932312
Epoch 690, training loss: 85.7629165649414 = 0.6855649948120117 + 10.0 * 8.507735252380371
Epoch 690, val loss: 0.7126339673995972
Epoch 700, training loss: 85.80299377441406 = 0.6800572276115417 + 10.0 * 8.512293815612793
Epoch 700, val loss: 0.707785427570343
Epoch 710, training loss: 85.72012329101562 = 0.6748185753822327 + 10.0 * 8.50452995300293
Epoch 710, val loss: 0.7031590342521667
Epoch 720, training loss: 85.69087219238281 = 0.6698815226554871 + 10.0 * 8.50209903717041
Epoch 720, val loss: 0.698847234249115
Epoch 730, training loss: 85.66914367675781 = 0.6651785373687744 + 10.0 * 8.500396728515625
Epoch 730, val loss: 0.6947531700134277
Epoch 740, training loss: 85.6481704711914 = 0.6606776118278503 + 10.0 * 8.498749732971191
Epoch 740, val loss: 0.6908219456672668
Epoch 750, training loss: 85.62905883789062 = 0.656358003616333 + 10.0 * 8.497270584106445
Epoch 750, val loss: 0.6870569586753845
Epoch 760, training loss: 85.67206573486328 = 0.6521878838539124 + 10.0 * 8.50198745727539
Epoch 760, val loss: 0.6834543347358704
Epoch 770, training loss: 85.60708618164062 = 0.6481416821479797 + 10.0 * 8.495894432067871
Epoch 770, val loss: 0.6798526048660278
Epoch 780, training loss: 85.5851058959961 = 0.6443060040473938 + 10.0 * 8.494080543518066
Epoch 780, val loss: 0.6765316128730774
Epoch 790, training loss: 85.56198120117188 = 0.6406539678573608 + 10.0 * 8.492132186889648
Epoch 790, val loss: 0.6733608245849609
Epoch 800, training loss: 85.54823303222656 = 0.6371565461158752 + 10.0 * 8.491107940673828
Epoch 800, val loss: 0.6703141927719116
Epoch 810, training loss: 85.53086853027344 = 0.6337621808052063 + 10.0 * 8.489710807800293
Epoch 810, val loss: 0.6673455834388733
Epoch 820, training loss: 85.52499389648438 = 0.6304991841316223 + 10.0 * 8.489449501037598
Epoch 820, val loss: 0.6644977927207947
Epoch 830, training loss: 85.5009765625 = 0.6273784637451172 + 10.0 * 8.487360000610352
Epoch 830, val loss: 0.6617947220802307
Epoch 840, training loss: 85.48373413085938 = 0.6243806481361389 + 10.0 * 8.48593521118164
Epoch 840, val loss: 0.6591867208480835
Epoch 850, training loss: 85.47022247314453 = 0.6214845776557922 + 10.0 * 8.48487377166748
Epoch 850, val loss: 0.6566667556762695
Epoch 860, training loss: 85.51011657714844 = 0.6186733245849609 + 10.0 * 8.489144325256348
Epoch 860, val loss: 0.6542397141456604
Epoch 870, training loss: 85.46839141845703 = 0.6158992648124695 + 10.0 * 8.485249519348145
Epoch 870, val loss: 0.651768684387207
Epoch 880, training loss: 85.43604278564453 = 0.6132617592811584 + 10.0 * 8.482278823852539
Epoch 880, val loss: 0.6494647860527039
Epoch 890, training loss: 85.42091369628906 = 0.6107231974601746 + 10.0 * 8.481019020080566
Epoch 890, val loss: 0.6472716331481934
Epoch 900, training loss: 85.41905975341797 = 0.6082738637924194 + 10.0 * 8.481078147888184
Epoch 900, val loss: 0.6451148390769958
Epoch 910, training loss: 85.39894104003906 = 0.6058626770973206 + 10.0 * 8.479307174682617
Epoch 910, val loss: 0.6430343389511108
Epoch 920, training loss: 85.38645935058594 = 0.6035177707672119 + 10.0 * 8.478294372558594
Epoch 920, val loss: 0.6409767270088196
Epoch 930, training loss: 85.37380981445312 = 0.6012559533119202 + 10.0 * 8.477254867553711
Epoch 930, val loss: 0.6390246748924255
Epoch 940, training loss: 85.36107635498047 = 0.5990424156188965 + 10.0 * 8.476202964782715
Epoch 940, val loss: 0.6370881199836731
Epoch 950, training loss: 85.35157775878906 = 0.5968678593635559 + 10.0 * 8.475470542907715
Epoch 950, val loss: 0.635201096534729
Epoch 960, training loss: 85.39652252197266 = 0.5947211980819702 + 10.0 * 8.480180740356445
Epoch 960, val loss: 0.6333039999008179
Epoch 970, training loss: 85.34188079833984 = 0.5925819873809814 + 10.0 * 8.474929809570312
Epoch 970, val loss: 0.6314370036125183
Epoch 980, training loss: 85.3228759765625 = 0.5905240178108215 + 10.0 * 8.473235130310059
Epoch 980, val loss: 0.6296411752700806
Epoch 990, training loss: 85.31562042236328 = 0.5885218381881714 + 10.0 * 8.472709655761719
Epoch 990, val loss: 0.6279081702232361
Epoch 1000, training loss: 85.31832122802734 = 0.5865581631660461 + 10.0 * 8.473176956176758
Epoch 1000, val loss: 0.6261705756187439
Epoch 1010, training loss: 85.29534912109375 = 0.5846200585365295 + 10.0 * 8.471073150634766
Epoch 1010, val loss: 0.6245165467262268
Epoch 1020, training loss: 85.31182098388672 = 0.5827116966247559 + 10.0 * 8.47291088104248
Epoch 1020, val loss: 0.622850239276886
Epoch 1030, training loss: 85.2835693359375 = 0.5808189511299133 + 10.0 * 8.470274925231934
Epoch 1030, val loss: 0.6211286187171936
Epoch 1040, training loss: 85.26725769042969 = 0.5789803266525269 + 10.0 * 8.468828201293945
Epoch 1040, val loss: 0.6195494532585144
Epoch 1050, training loss: 85.25568389892578 = 0.5771701335906982 + 10.0 * 8.467851638793945
Epoch 1050, val loss: 0.6179484724998474
Epoch 1060, training loss: 85.24915313720703 = 0.5753816366195679 + 10.0 * 8.467377662658691
Epoch 1060, val loss: 0.6163922548294067
Epoch 1070, training loss: 85.29065704345703 = 0.5735915899276733 + 10.0 * 8.47170639038086
Epoch 1070, val loss: 0.6148079037666321
Epoch 1080, training loss: 85.25254821777344 = 0.5717753767967224 + 10.0 * 8.468076705932617
Epoch 1080, val loss: 0.6131775975227356
Epoch 1090, training loss: 85.22745513916016 = 0.5700255036354065 + 10.0 * 8.465743064880371
Epoch 1090, val loss: 0.6116445660591125
Epoch 1100, training loss: 85.21546936035156 = 0.568318247795105 + 10.0 * 8.464715003967285
Epoch 1100, val loss: 0.6101299524307251
Epoch 1110, training loss: 85.20695495605469 = 0.5666499733924866 + 10.0 * 8.464030265808105
Epoch 1110, val loss: 0.608648419380188
Epoch 1120, training loss: 85.1987533569336 = 0.5649957656860352 + 10.0 * 8.46337604522705
Epoch 1120, val loss: 0.6071678400039673
Epoch 1130, training loss: 85.19158172607422 = 0.5633466839790344 + 10.0 * 8.462823867797852
Epoch 1130, val loss: 0.6057069897651672
Epoch 1140, training loss: 85.18838500976562 = 0.5617060661315918 + 10.0 * 8.462667465209961
Epoch 1140, val loss: 0.6042501330375671
Epoch 1150, training loss: 85.24347686767578 = 0.5600518584251404 + 10.0 * 8.468342781066895
Epoch 1150, val loss: 0.6027916669845581
Epoch 1160, training loss: 85.17295837402344 = 0.5583673119544983 + 10.0 * 8.461459159851074
Epoch 1160, val loss: 0.6012648344039917
Epoch 1170, training loss: 85.17285919189453 = 0.556743323802948 + 10.0 * 8.4616117477417
Epoch 1170, val loss: 0.5998547077178955
Epoch 1180, training loss: 85.15912628173828 = 0.5551541447639465 + 10.0 * 8.460397720336914
Epoch 1180, val loss: 0.5984401106834412
Epoch 1190, training loss: 85.15218353271484 = 0.5535826683044434 + 10.0 * 8.459859848022461
Epoch 1190, val loss: 0.5970200896263123
Epoch 1200, training loss: 85.14456176757812 = 0.5520169734954834 + 10.0 * 8.459254264831543
Epoch 1200, val loss: 0.5956196784973145
Epoch 1210, training loss: 85.13937377929688 = 0.5504509210586548 + 10.0 * 8.458891868591309
Epoch 1210, val loss: 0.5942084789276123
Epoch 1220, training loss: 85.18104553222656 = 0.5488757491111755 + 10.0 * 8.463216781616211
Epoch 1220, val loss: 0.5927364230155945
Epoch 1230, training loss: 85.14788055419922 = 0.5472792983055115 + 10.0 * 8.460060119628906
Epoch 1230, val loss: 0.5913946628570557
Epoch 1240, training loss: 85.1263656616211 = 0.5457046627998352 + 10.0 * 8.4580659866333
Epoch 1240, val loss: 0.5899326205253601
Epoch 1250, training loss: 85.11627197265625 = 0.5441551804542542 + 10.0 * 8.4572114944458
Epoch 1250, val loss: 0.5885521769523621
Epoch 1260, training loss: 85.1401596069336 = 0.5426115393638611 + 10.0 * 8.459754943847656
Epoch 1260, val loss: 0.5871641635894775
Epoch 1270, training loss: 85.11585998535156 = 0.5410438776016235 + 10.0 * 8.457481384277344
Epoch 1270, val loss: 0.585713803768158
Epoch 1280, training loss: 85.104248046875 = 0.5394976735115051 + 10.0 * 8.456475257873535
Epoch 1280, val loss: 0.5843372941017151
Epoch 1290, training loss: 85.09465789794922 = 0.5379601716995239 + 10.0 * 8.455669403076172
Epoch 1290, val loss: 0.5829371809959412
Epoch 1300, training loss: 85.08832550048828 = 0.536428689956665 + 10.0 * 8.45518970489502
Epoch 1300, val loss: 0.5815541744232178
Epoch 1310, training loss: 85.09291076660156 = 0.5348914861679077 + 10.0 * 8.455801963806152
Epoch 1310, val loss: 0.5801649689674377
Epoch 1320, training loss: 85.08077239990234 = 0.5333364605903625 + 10.0 * 8.454744338989258
Epoch 1320, val loss: 0.5787479877471924
Epoch 1330, training loss: 85.08097076416016 = 0.5317874550819397 + 10.0 * 8.45491886138916
Epoch 1330, val loss: 0.5773329138755798
Epoch 1340, training loss: 85.09077453613281 = 0.5302397012710571 + 10.0 * 8.456052780151367
Epoch 1340, val loss: 0.5759375095367432
Epoch 1350, training loss: 85.06653594970703 = 0.5286917090415955 + 10.0 * 8.453783988952637
Epoch 1350, val loss: 0.5745501518249512
Epoch 1360, training loss: 85.0610580444336 = 0.527152419090271 + 10.0 * 8.453390121459961
Epoch 1360, val loss: 0.5731385350227356
Epoch 1370, training loss: 85.05571746826172 = 0.5256146192550659 + 10.0 * 8.453010559082031
Epoch 1370, val loss: 0.5717217922210693
Epoch 1380, training loss: 85.07073974609375 = 0.5240693688392639 + 10.0 * 8.454667091369629
Epoch 1380, val loss: 0.570268452167511
Epoch 1390, training loss: 85.046142578125 = 0.5224952101707458 + 10.0 * 8.452364921569824
Epoch 1390, val loss: 0.5688785314559937
Epoch 1400, training loss: 85.04110717773438 = 0.5209354162216187 + 10.0 * 8.452016830444336
Epoch 1400, val loss: 0.5674281120300293
Epoch 1410, training loss: 85.04095458984375 = 0.5193841457366943 + 10.0 * 8.452157020568848
Epoch 1410, val loss: 0.5660078525543213
Epoch 1420, training loss: 85.06661224365234 = 0.5178216099739075 + 10.0 * 8.454878807067871
Epoch 1420, val loss: 0.5645495653152466
Epoch 1430, training loss: 85.03583526611328 = 0.5162559747695923 + 10.0 * 8.451957702636719
Epoch 1430, val loss: 0.5631716847419739
Epoch 1440, training loss: 85.02327728271484 = 0.5146925449371338 + 10.0 * 8.450858116149902
Epoch 1440, val loss: 0.5617107152938843
Epoch 1450, training loss: 85.0177001953125 = 0.5131366848945618 + 10.0 * 8.450456619262695
Epoch 1450, val loss: 0.5602982640266418
Epoch 1460, training loss: 85.01244354248047 = 0.5115775465965271 + 10.0 * 8.45008659362793
Epoch 1460, val loss: 0.5588661432266235
Epoch 1470, training loss: 85.01081085205078 = 0.5100095868110657 + 10.0 * 8.450079917907715
Epoch 1470, val loss: 0.5574387311935425
Epoch 1480, training loss: 85.04837799072266 = 0.5084249377250671 + 10.0 * 8.453995704650879
Epoch 1480, val loss: 0.5559889674186707
Epoch 1490, training loss: 85.03585052490234 = 0.5067967772483826 + 10.0 * 8.452905654907227
Epoch 1490, val loss: 0.5544562339782715
Epoch 1500, training loss: 85.00418090820312 = 0.5051944851875305 + 10.0 * 8.449898719787598
Epoch 1500, val loss: 0.5530321598052979
Epoch 1510, training loss: 84.99166870117188 = 0.5036056637763977 + 10.0 * 8.448805809020996
Epoch 1510, val loss: 0.5515636801719666
Epoch 1520, training loss: 84.98760223388672 = 0.5020239353179932 + 10.0 * 8.44855785369873
Epoch 1520, val loss: 0.550123929977417
Epoch 1530, training loss: 84.9825210571289 = 0.5004432201385498 + 10.0 * 8.44820785522461
Epoch 1530, val loss: 0.5487035512924194
Epoch 1540, training loss: 85.02017211914062 = 0.4988514184951782 + 10.0 * 8.452132225036621
Epoch 1540, val loss: 0.5472924709320068
Epoch 1550, training loss: 84.9943618774414 = 0.49720990657806396 + 10.0 * 8.449715614318848
Epoch 1550, val loss: 0.5457302331924438
Epoch 1560, training loss: 84.97004699707031 = 0.49559521675109863 + 10.0 * 8.447444915771484
Epoch 1560, val loss: 0.5442785024642944
Epoch 1570, training loss: 84.96766662597656 = 0.4939884543418884 + 10.0 * 8.447367668151855
Epoch 1570, val loss: 0.5428355932235718
Epoch 1580, training loss: 84.96024322509766 = 0.49238669872283936 + 10.0 * 8.446785926818848
Epoch 1580, val loss: 0.5413654446601868
Epoch 1590, training loss: 84.96427154541016 = 0.4907801151275635 + 10.0 * 8.447348594665527
Epoch 1590, val loss: 0.5398669838905334
Epoch 1600, training loss: 84.97018432617188 = 0.48915034532546997 + 10.0 * 8.448102951049805
Epoch 1600, val loss: 0.5383973121643066
Epoch 1610, training loss: 84.95072174072266 = 0.48752424120903015 + 10.0 * 8.446319580078125
Epoch 1610, val loss: 0.536960244178772
Epoch 1620, training loss: 84.94573211669922 = 0.4859052300453186 + 10.0 * 8.445981979370117
Epoch 1620, val loss: 0.5355029106140137
Epoch 1630, training loss: 84.94218444824219 = 0.48429378867149353 + 10.0 * 8.445789337158203
Epoch 1630, val loss: 0.5340338945388794
Epoch 1640, training loss: 84.982421875 = 0.48267266154289246 + 10.0 * 8.44997501373291
Epoch 1640, val loss: 0.5325869917869568
Epoch 1650, training loss: 84.94462585449219 = 0.48103368282318115 + 10.0 * 8.446359634399414
Epoch 1650, val loss: 0.5310844779014587
Epoch 1660, training loss: 84.93294525146484 = 0.4794060289859772 + 10.0 * 8.445353507995605
Epoch 1660, val loss: 0.5296526551246643
Epoch 1670, training loss: 84.92500305175781 = 0.47779005765914917 + 10.0 * 8.444721221923828
Epoch 1670, val loss: 0.5282159447669983
Epoch 1680, training loss: 84.91984558105469 = 0.47617557644844055 + 10.0 * 8.444367408752441
Epoch 1680, val loss: 0.5267755389213562
Epoch 1690, training loss: 84.91520690917969 = 0.4745591878890991 + 10.0 * 8.44406509399414
Epoch 1690, val loss: 0.5253550410270691
Epoch 1700, training loss: 84.9121322631836 = 0.47294139862060547 + 10.0 * 8.44391918182373
Epoch 1700, val loss: 0.5239399075508118
Epoch 1710, training loss: 84.94506072998047 = 0.4713211953639984 + 10.0 * 8.44737434387207
Epoch 1710, val loss: 0.5225512385368347
Epoch 1720, training loss: 84.925048828125 = 0.46966710686683655 + 10.0 * 8.445538520812988
Epoch 1720, val loss: 0.5210437178611755
Epoch 1730, training loss: 84.92253875732422 = 0.4680366814136505 + 10.0 * 8.445449829101562
Epoch 1730, val loss: 0.5196660757064819
Epoch 1740, training loss: 84.89707946777344 = 0.46641919016838074 + 10.0 * 8.443066596984863
Epoch 1740, val loss: 0.518220841884613
Epoch 1750, training loss: 84.89494323730469 = 0.4648228883743286 + 10.0 * 8.443012237548828
Epoch 1750, val loss: 0.5168246626853943
Epoch 1760, training loss: 84.88819122314453 = 0.46323487162590027 + 10.0 * 8.442495346069336
Epoch 1760, val loss: 0.5154638290405273
Epoch 1770, training loss: 84.8841323852539 = 0.46164804697036743 + 10.0 * 8.442248344421387
Epoch 1770, val loss: 0.5140901803970337
Epoch 1780, training loss: 84.89335632324219 = 0.46005943417549133 + 10.0 * 8.443329811096191
Epoch 1780, val loss: 0.512711763381958
Epoch 1790, training loss: 84.886962890625 = 0.45845383405685425 + 10.0 * 8.442851066589355
Epoch 1790, val loss: 0.5113424062728882
Epoch 1800, training loss: 84.8928451538086 = 0.4568580389022827 + 10.0 * 8.443598747253418
Epoch 1800, val loss: 0.5100247859954834
Epoch 1810, training loss: 84.88267517089844 = 0.4552784860134125 + 10.0 * 8.442739486694336
Epoch 1810, val loss: 0.508697509765625
Epoch 1820, training loss: 84.87086486816406 = 0.45371583104133606 + 10.0 * 8.441715240478516
Epoch 1820, val loss: 0.5074202418327332
Epoch 1830, training loss: 84.8624038696289 = 0.45216822624206543 + 10.0 * 8.441022872924805
Epoch 1830, val loss: 0.5061188340187073
Epoch 1840, training loss: 84.85945892333984 = 0.45062533020973206 + 10.0 * 8.44088363647461
Epoch 1840, val loss: 0.5048370361328125
Epoch 1850, training loss: 84.86939239501953 = 0.4490841329097748 + 10.0 * 8.442030906677246
Epoch 1850, val loss: 0.5035518407821655
Epoch 1860, training loss: 84.85469818115234 = 0.44754138588905334 + 10.0 * 8.440715789794922
Epoch 1860, val loss: 0.5023218393325806
Epoch 1870, training loss: 84.8524398803711 = 0.44600987434387207 + 10.0 * 8.440643310546875
Epoch 1870, val loss: 0.5011130571365356
Epoch 1880, training loss: 84.8877944946289 = 0.4444797933101654 + 10.0 * 8.444331169128418
Epoch 1880, val loss: 0.49991509318351746
Epoch 1890, training loss: 84.84883117675781 = 0.44294652342796326 + 10.0 * 8.440587997436523
Epoch 1890, val loss: 0.49866190552711487
Epoch 1900, training loss: 84.83890533447266 = 0.44143691658973694 + 10.0 * 8.439746856689453
Epoch 1900, val loss: 0.4975360631942749
Epoch 1910, training loss: 84.83279418945312 = 0.4399392008781433 + 10.0 * 8.439285278320312
Epoch 1910, val loss: 0.4963424801826477
Epoch 1920, training loss: 84.83104705810547 = 0.4384501278400421 + 10.0 * 8.43925952911377
Epoch 1920, val loss: 0.49521055817604065
Epoch 1930, training loss: 84.87909698486328 = 0.43696558475494385 + 10.0 * 8.444212913513184
Epoch 1930, val loss: 0.49418044090270996
Epoch 1940, training loss: 84.84488677978516 = 0.43546441197395325 + 10.0 * 8.440942764282227
Epoch 1940, val loss: 0.49287867546081543
Epoch 1950, training loss: 84.82610321044922 = 0.43399497866630554 + 10.0 * 8.439210891723633
Epoch 1950, val loss: 0.4918929636478424
Epoch 1960, training loss: 84.81623840332031 = 0.43253451585769653 + 10.0 * 8.438370704650879
Epoch 1960, val loss: 0.4907723665237427
Epoch 1970, training loss: 84.81551361083984 = 0.4310867190361023 + 10.0 * 8.438443183898926
Epoch 1970, val loss: 0.489714115858078
Epoch 1980, training loss: 84.85581970214844 = 0.42963966727256775 + 10.0 * 8.442617416381836
Epoch 1980, val loss: 0.4886721968650818
Epoch 1990, training loss: 84.81558990478516 = 0.42818620800971985 + 10.0 * 8.438740730285645
Epoch 1990, val loss: 0.48763391375541687
Epoch 2000, training loss: 84.80406188964844 = 0.4267573356628418 + 10.0 * 8.43773078918457
Epoch 2000, val loss: 0.48662883043289185
Epoch 2010, training loss: 84.79801940917969 = 0.4253411293029785 + 10.0 * 8.437268257141113
Epoch 2010, val loss: 0.48562371730804443
Epoch 2020, training loss: 84.7953109741211 = 0.42393362522125244 + 10.0 * 8.437137603759766
Epoch 2020, val loss: 0.48466524481773376
Epoch 2030, training loss: 84.82599639892578 = 0.42252862453460693 + 10.0 * 8.440346717834473
Epoch 2030, val loss: 0.4836949408054352
Epoch 2040, training loss: 84.80573272705078 = 0.42112407088279724 + 10.0 * 8.438460350036621
Epoch 2040, val loss: 0.4828023314476013
Epoch 2050, training loss: 84.7896499633789 = 0.4197344183921814 + 10.0 * 8.436991691589355
Epoch 2050, val loss: 0.48189499974250793
Epoch 2060, training loss: 84.78424835205078 = 0.4183562994003296 + 10.0 * 8.436589241027832
Epoch 2060, val loss: 0.4810085594654083
Epoch 2070, training loss: 84.7840347290039 = 0.416982501745224 + 10.0 * 8.436704635620117
Epoch 2070, val loss: 0.48012790083885193
Epoch 2080, training loss: 84.802734375 = 0.4156111478805542 + 10.0 * 8.438712120056152
Epoch 2080, val loss: 0.47923070192337036
Epoch 2090, training loss: 84.7899398803711 = 0.4142398238182068 + 10.0 * 8.437570571899414
Epoch 2090, val loss: 0.4783429503440857
Epoch 2100, training loss: 84.78093719482422 = 0.41287851333618164 + 10.0 * 8.436805725097656
Epoch 2100, val loss: 0.47753021121025085
Epoch 2110, training loss: 84.77474975585938 = 0.41153424978256226 + 10.0 * 8.436321258544922
Epoch 2110, val loss: 0.4767940640449524
Epoch 2120, training loss: 84.77389526367188 = 0.41020041704177856 + 10.0 * 8.436368942260742
Epoch 2120, val loss: 0.47593528032302856
Epoch 2130, training loss: 84.76588439941406 = 0.40887150168418884 + 10.0 * 8.435701370239258
Epoch 2130, val loss: 0.4751576781272888
Epoch 2140, training loss: 84.75575256347656 = 0.40755048394203186 + 10.0 * 8.434820175170898
Epoch 2140, val loss: 0.4743984639644623
Epoch 2150, training loss: 84.75527954101562 = 0.4062362611293793 + 10.0 * 8.434904098510742
Epoch 2150, val loss: 0.4736393094062805
Epoch 2160, training loss: 84.77128601074219 = 0.40492403507232666 + 10.0 * 8.436635971069336
Epoch 2160, val loss: 0.4728671610355377
Epoch 2170, training loss: 84.75419616699219 = 0.40361472964286804 + 10.0 * 8.435057640075684
Epoch 2170, val loss: 0.4720957577228546
Epoch 2180, training loss: 84.75885009765625 = 0.40231481194496155 + 10.0 * 8.435653686523438
Epoch 2180, val loss: 0.47138112783432007
Epoch 2190, training loss: 84.74471282958984 = 0.40102091431617737 + 10.0 * 8.434369087219238
Epoch 2190, val loss: 0.4707235097885132
Epoch 2200, training loss: 84.74505615234375 = 0.39974111318588257 + 10.0 * 8.434531211853027
Epoch 2200, val loss: 0.4700518250465393
Epoch 2210, training loss: 84.7489242553711 = 0.39846035838127136 + 10.0 * 8.435046195983887
Epoch 2210, val loss: 0.4693687856197357
Epoch 2220, training loss: 84.74920654296875 = 0.3971855044364929 + 10.0 * 8.435201644897461
Epoch 2220, val loss: 0.46870115399360657
Epoch 2230, training loss: 84.73213195800781 = 0.3959217071533203 + 10.0 * 8.433621406555176
Epoch 2230, val loss: 0.468135803937912
Epoch 2240, training loss: 84.7306900024414 = 0.3946669399738312 + 10.0 * 8.433602333068848
Epoch 2240, val loss: 0.46755677461624146
Epoch 2250, training loss: 84.7514877319336 = 0.39341726899147034 + 10.0 * 8.435807228088379
Epoch 2250, val loss: 0.4669330418109894
Epoch 2260, training loss: 84.72950744628906 = 0.3921663165092468 + 10.0 * 8.433733940124512
Epoch 2260, val loss: 0.466321736574173
Epoch 2270, training loss: 84.71939086914062 = 0.3909335732460022 + 10.0 * 8.432846069335938
Epoch 2270, val loss: 0.465792715549469
Epoch 2280, training loss: 84.71900939941406 = 0.3897084593772888 + 10.0 * 8.432929992675781
Epoch 2280, val loss: 0.46524369716644287
Epoch 2290, training loss: 84.72428131103516 = 0.3884868025779724 + 10.0 * 8.433579444885254
Epoch 2290, val loss: 0.46464917063713074
Epoch 2300, training loss: 84.72892761230469 = 0.38727104663848877 + 10.0 * 8.434165954589844
Epoch 2300, val loss: 0.4641076624393463
Epoch 2310, training loss: 84.71063232421875 = 0.38606229424476624 + 10.0 * 8.432456970214844
Epoch 2310, val loss: 0.46371668577194214
Epoch 2320, training loss: 84.70781707763672 = 0.3848581910133362 + 10.0 * 8.432295799255371
Epoch 2320, val loss: 0.46318933367729187
Epoch 2330, training loss: 84.7169189453125 = 0.38365671038627625 + 10.0 * 8.43332576751709
Epoch 2330, val loss: 0.46271011233329773
Epoch 2340, training loss: 84.6998291015625 = 0.3824596703052521 + 10.0 * 8.431736946105957
Epoch 2340, val loss: 0.4622882306575775
Epoch 2350, training loss: 84.70477294921875 = 0.38127031922340393 + 10.0 * 8.432350158691406
Epoch 2350, val loss: 0.46186742186546326
Epoch 2360, training loss: 84.78366088867188 = 0.38008031249046326 + 10.0 * 8.44035816192627
Epoch 2360, val loss: 0.4614723026752472
Epoch 2370, training loss: 84.69755554199219 = 0.3789023756980896 + 10.0 * 8.431865692138672
Epoch 2370, val loss: 0.46095332503318787
Epoch 2380, training loss: 84.68938446044922 = 0.37774011492729187 + 10.0 * 8.431164741516113
Epoch 2380, val loss: 0.46062588691711426
Epoch 2390, training loss: 84.68655395507812 = 0.3765891492366791 + 10.0 * 8.430996894836426
Epoch 2390, val loss: 0.4602726995944977
Epoch 2400, training loss: 84.68084716796875 = 0.37544429302215576 + 10.0 * 8.430540084838867
Epoch 2400, val loss: 0.45989370346069336
Epoch 2410, training loss: 84.67743682861328 = 0.3743007183074951 + 10.0 * 8.430314064025879
Epoch 2410, val loss: 0.45951205492019653
Epoch 2420, training loss: 84.6778793334961 = 0.3731584846973419 + 10.0 * 8.430471420288086
Epoch 2420, val loss: 0.45917871594429016
Epoch 2430, training loss: 84.75189208984375 = 0.37201619148254395 + 10.0 * 8.437987327575684
Epoch 2430, val loss: 0.4589492082595825
Epoch 2440, training loss: 84.6934814453125 = 0.37086221575737 + 10.0 * 8.43226146697998
Epoch 2440, val loss: 0.4583580791950226
Epoch 2450, training loss: 84.66944122314453 = 0.3697287142276764 + 10.0 * 8.429971694946289
Epoch 2450, val loss: 0.4581601619720459
Epoch 2460, training loss: 84.6637954711914 = 0.36859801411628723 + 10.0 * 8.429519653320312
Epoch 2460, val loss: 0.45784857869148254
Epoch 2470, training loss: 84.66102600097656 = 0.3674693703651428 + 10.0 * 8.42935562133789
Epoch 2470, val loss: 0.45753803849220276
Epoch 2480, training loss: 84.65898895263672 = 0.36634355783462524 + 10.0 * 8.429265022277832
Epoch 2480, val loss: 0.4572505056858063
Epoch 2490, training loss: 84.70256042480469 = 0.36521950364112854 + 10.0 * 8.433733940124512
Epoch 2490, val loss: 0.4568677544593811
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7879249112125823
0.8160544809099471
=== training gcn model ===
Epoch 0, training loss: 106.92849731445312 = 1.1054741144180298 + 10.0 * 10.58230209350586
Epoch 0, val loss: 1.1044926643371582
Epoch 10, training loss: 106.92071533203125 = 1.0997216701507568 + 10.0 * 10.582098960876465
Epoch 10, val loss: 1.0987980365753174
Epoch 20, training loss: 106.90489959716797 = 1.0936607122421265 + 10.0 * 10.581124305725098
Epoch 20, val loss: 1.0927783250808716
Epoch 30, training loss: 106.84996795654297 = 1.0871024131774902 + 10.0 * 10.576286315917969
Epoch 30, val loss: 1.086266279220581
Epoch 40, training loss: 106.6515884399414 = 1.0799363851547241 + 10.0 * 10.557165145874023
Epoch 40, val loss: 1.0791901350021362
Epoch 50, training loss: 106.13285827636719 = 1.0723142623901367 + 10.0 * 10.506054878234863
Epoch 50, val loss: 1.0717147588729858
Epoch 60, training loss: 105.03269958496094 = 1.0653334856033325 + 10.0 * 10.396737098693848
Epoch 60, val loss: 1.065035343170166
Epoch 70, training loss: 103.00462341308594 = 1.0591752529144287 + 10.0 * 10.194544792175293
Epoch 70, val loss: 1.058871865272522
Epoch 80, training loss: 101.20880889892578 = 1.0527424812316895 + 10.0 * 10.015606880187988
Epoch 80, val loss: 1.0525743961334229
Epoch 90, training loss: 98.9859619140625 = 1.0469539165496826 + 10.0 * 9.793901443481445
Epoch 90, val loss: 1.0469542741775513
Epoch 100, training loss: 97.33055114746094 = 1.0420253276824951 + 10.0 * 9.628852844238281
Epoch 100, val loss: 1.042173981666565
Epoch 110, training loss: 96.1767578125 = 1.0375683307647705 + 10.0 * 9.51391887664795
Epoch 110, val loss: 1.0377538204193115
Epoch 120, training loss: 94.84432983398438 = 1.0334874391555786 + 10.0 * 9.381084442138672
Epoch 120, val loss: 1.0337268114089966
Epoch 130, training loss: 93.91636657714844 = 1.0308434963226318 + 10.0 * 9.288552284240723
Epoch 130, val loss: 1.0312176942825317
Epoch 140, training loss: 93.41768646240234 = 1.029409646987915 + 10.0 * 9.2388277053833
Epoch 140, val loss: 1.029795527458191
Epoch 150, training loss: 92.7262191772461 = 1.0271611213684082 + 10.0 * 9.169905662536621
Epoch 150, val loss: 1.0274908542633057
Epoch 160, training loss: 91.63388061523438 = 1.0252346992492676 + 10.0 * 9.060864448547363
Epoch 160, val loss: 1.0257689952850342
Epoch 170, training loss: 90.822509765625 = 1.0246554613113403 + 10.0 * 8.979784965515137
Epoch 170, val loss: 1.0252920389175415
Epoch 180, training loss: 90.23863983154297 = 1.0228735208511353 + 10.0 * 8.921576499938965
Epoch 180, val loss: 1.0234559774398804
Epoch 190, training loss: 89.78523254394531 = 1.0203907489776611 + 10.0 * 8.876483917236328
Epoch 190, val loss: 1.0211001634597778
Epoch 200, training loss: 89.3907241821289 = 1.0180714130401611 + 10.0 * 8.837265014648438
Epoch 200, val loss: 1.018875002861023
Epoch 210, training loss: 88.986572265625 = 1.016106128692627 + 10.0 * 8.797046661376953
Epoch 210, val loss: 1.0169553756713867
Epoch 220, training loss: 88.66773986816406 = 1.0139884948730469 + 10.0 * 8.765375137329102
Epoch 220, val loss: 1.014788269996643
Epoch 230, training loss: 88.4389877319336 = 1.0112333297729492 + 10.0 * 8.742775917053223
Epoch 230, val loss: 1.012000560760498
Epoch 240, training loss: 88.265625 = 1.0078715085983276 + 10.0 * 8.725774765014648
Epoch 240, val loss: 1.0086331367492676
Epoch 250, training loss: 88.13980865478516 = 1.0040394067764282 + 10.0 * 8.713577270507812
Epoch 250, val loss: 1.0048458576202393
Epoch 260, training loss: 88.04190826416016 = 1.000192642211914 + 10.0 * 8.704172134399414
Epoch 260, val loss: 1.0010299682617188
Epoch 270, training loss: 87.95268249511719 = 0.9961155652999878 + 10.0 * 8.695656776428223
Epoch 270, val loss: 0.9970179200172424
Epoch 280, training loss: 87.871337890625 = 0.9917976260185242 + 10.0 * 8.68795394897461
Epoch 280, val loss: 0.9927869439125061
Epoch 290, training loss: 87.79573822021484 = 0.9872990250587463 + 10.0 * 8.6808443069458
Epoch 290, val loss: 0.9883517622947693
Epoch 300, training loss: 87.70245361328125 = 0.982586681842804 + 10.0 * 8.67198657989502
Epoch 300, val loss: 0.9837270379066467
Epoch 310, training loss: 87.61561584472656 = 0.9776968359947205 + 10.0 * 8.66379165649414
Epoch 310, val loss: 0.9789443016052246
Epoch 320, training loss: 87.51358032226562 = 0.9726108312606812 + 10.0 * 8.654096603393555
Epoch 320, val loss: 0.9738844037055969
Epoch 330, training loss: 87.42143249511719 = 0.9672769904136658 + 10.0 * 8.645415306091309
Epoch 330, val loss: 0.9686393141746521
Epoch 340, training loss: 87.33548736572266 = 0.9616610407829285 + 10.0 * 8.637382507324219
Epoch 340, val loss: 0.9631101489067078
Epoch 350, training loss: 87.27562713623047 = 0.9557002186775208 + 10.0 * 8.63199234008789
Epoch 350, val loss: 0.9571989178657532
Epoch 360, training loss: 87.18397521972656 = 0.9492295384407043 + 10.0 * 8.623475074768066
Epoch 360, val loss: 0.9508704543113708
Epoch 370, training loss: 87.09532165527344 = 0.9425110220909119 + 10.0 * 8.615281105041504
Epoch 370, val loss: 0.9442832469940186
Epoch 380, training loss: 87.0132064819336 = 0.9355419278144836 + 10.0 * 8.607767105102539
Epoch 380, val loss: 0.937439501285553
Epoch 390, training loss: 86.97927856445312 = 0.9282432794570923 + 10.0 * 8.605103492736816
Epoch 390, val loss: 0.9301993250846863
Epoch 400, training loss: 86.8678970336914 = 0.9205464124679565 + 10.0 * 8.594735145568848
Epoch 400, val loss: 0.922735333442688
Epoch 410, training loss: 86.78239440917969 = 0.9126535654067993 + 10.0 * 8.586974143981934
Epoch 410, val loss: 0.9150533080101013
Epoch 420, training loss: 86.70975494384766 = 0.9045030474662781 + 10.0 * 8.580525398254395
Epoch 420, val loss: 0.9071118831634521
Epoch 430, training loss: 86.64337158203125 = 0.8960675597190857 + 10.0 * 8.57473087310791
Epoch 430, val loss: 0.8988957405090332
Epoch 440, training loss: 86.67327880859375 = 0.8872600793838501 + 10.0 * 8.578601837158203
Epoch 440, val loss: 0.8904417753219604
Epoch 450, training loss: 86.54254913330078 = 0.87813401222229 + 10.0 * 8.566441535949707
Epoch 450, val loss: 0.8814992904663086
Epoch 460, training loss: 86.48249053955078 = 0.868998110294342 + 10.0 * 8.561349868774414
Epoch 460, val loss: 0.8726309537887573
Epoch 470, training loss: 86.43049621582031 = 0.859705924987793 + 10.0 * 8.557079315185547
Epoch 470, val loss: 0.8636536598205566
Epoch 480, training loss: 86.383056640625 = 0.8502612709999084 + 10.0 * 8.553279876708984
Epoch 480, val loss: 0.8545313477516174
Epoch 490, training loss: 86.33671569824219 = 0.8406490683555603 + 10.0 * 8.549606323242188
Epoch 490, val loss: 0.8452916145324707
Epoch 500, training loss: 86.29158782958984 = 0.8309666514396667 + 10.0 * 8.546062469482422
Epoch 500, val loss: 0.8359839916229248
Epoch 510, training loss: 86.26051330566406 = 0.8212268352508545 + 10.0 * 8.543928146362305
Epoch 510, val loss: 0.8266255259513855
Epoch 520, training loss: 86.20512390136719 = 0.8114506006240845 + 10.0 * 8.53936767578125
Epoch 520, val loss: 0.8173174262046814
Epoch 530, training loss: 86.16138458251953 = 0.8018193244934082 + 10.0 * 8.535956382751465
Epoch 530, val loss: 0.8081336617469788
Epoch 540, training loss: 86.11874389648438 = 0.7922670245170593 + 10.0 * 8.532648086547852
Epoch 540, val loss: 0.7990495562553406
Epoch 550, training loss: 86.11417388916016 = 0.7827996015548706 + 10.0 * 8.533137321472168
Epoch 550, val loss: 0.7900260090827942
Epoch 560, training loss: 86.03820037841797 = 0.773390531539917 + 10.0 * 8.526480674743652
Epoch 560, val loss: 0.7811926603317261
Epoch 570, training loss: 85.99663543701172 = 0.7642420530319214 + 10.0 * 8.523239135742188
Epoch 570, val loss: 0.7726312875747681
Epoch 580, training loss: 85.95651245117188 = 0.7552821636199951 + 10.0 * 8.520123481750488
Epoch 580, val loss: 0.764239490032196
Epoch 590, training loss: 85.91879272460938 = 0.7465274333953857 + 10.0 * 8.517226219177246
Epoch 590, val loss: 0.7560381293296814
Epoch 600, training loss: 85.90789794921875 = 0.7379258275032043 + 10.0 * 8.516997337341309
Epoch 600, val loss: 0.7480846643447876
Epoch 610, training loss: 85.8565444946289 = 0.7295610308647156 + 10.0 * 8.51269817352295
Epoch 610, val loss: 0.740261435508728
Epoch 620, training loss: 85.8192367553711 = 0.721471905708313 + 10.0 * 8.50977611541748
Epoch 620, val loss: 0.7328309416770935
Epoch 630, training loss: 85.78724670410156 = 0.7136522531509399 + 10.0 * 8.507359504699707
Epoch 630, val loss: 0.7256410121917725
Epoch 640, training loss: 85.7609634399414 = 0.7060412168502808 + 10.0 * 8.505492210388184
Epoch 640, val loss: 0.718682050704956
Epoch 650, training loss: 85.74034118652344 = 0.69859379529953 + 10.0 * 8.504175186157227
Epoch 650, val loss: 0.7118704319000244
Epoch 660, training loss: 85.71267700195312 = 0.6914253234863281 + 10.0 * 8.502124786376953
Epoch 660, val loss: 0.7053777575492859
Epoch 670, training loss: 85.68185424804688 = 0.6846121549606323 + 10.0 * 8.499724388122559
Epoch 670, val loss: 0.6991745233535767
Epoch 680, training loss: 85.6577377319336 = 0.6780030131340027 + 10.0 * 8.497973442077637
Epoch 680, val loss: 0.6932347416877747
Epoch 690, training loss: 85.63736724853516 = 0.671613335609436 + 10.0 * 8.496575355529785
Epoch 690, val loss: 0.6875243186950684
Epoch 700, training loss: 85.64842224121094 = 0.6654072403907776 + 10.0 * 8.49830150604248
Epoch 700, val loss: 0.6819648742675781
Epoch 710, training loss: 85.60175323486328 = 0.6594166159629822 + 10.0 * 8.494234085083008
Epoch 710, val loss: 0.6766404509544373
Epoch 720, training loss: 85.58275604248047 = 0.6537445187568665 + 10.0 * 8.492900848388672
Epoch 720, val loss: 0.6716477274894714
Epoch 730, training loss: 85.56311798095703 = 0.6482712626457214 + 10.0 * 8.491484642028809
Epoch 730, val loss: 0.6668447256088257
Epoch 740, training loss: 85.5444107055664 = 0.6429845690727234 + 10.0 * 8.490142822265625
Epoch 740, val loss: 0.6622282862663269
Epoch 750, training loss: 85.52787780761719 = 0.6378772854804993 + 10.0 * 8.48900032043457
Epoch 750, val loss: 0.6577916741371155
Epoch 760, training loss: 85.51495361328125 = 0.6329460144042969 + 10.0 * 8.488201141357422
Epoch 760, val loss: 0.6535005569458008
Epoch 770, training loss: 85.50231170654297 = 0.6280929446220398 + 10.0 * 8.487421989440918
Epoch 770, val loss: 0.6493353247642517
Epoch 780, training loss: 85.48643493652344 = 0.6234197020530701 + 10.0 * 8.48630142211914
Epoch 780, val loss: 0.6452988386154175
Epoch 790, training loss: 85.4694595336914 = 0.6190090775489807 + 10.0 * 8.485044479370117
Epoch 790, val loss: 0.6415343880653381
Epoch 800, training loss: 85.45547485351562 = 0.6147418022155762 + 10.0 * 8.484073638916016
Epoch 800, val loss: 0.6379067897796631
Epoch 810, training loss: 85.44207000732422 = 0.6106008291244507 + 10.0 * 8.483146667480469
Epoch 810, val loss: 0.6343777179718018
Epoch 820, training loss: 85.42903900146484 = 0.6065809726715088 + 10.0 * 8.482245445251465
Epoch 820, val loss: 0.6309670805931091
Epoch 830, training loss: 85.416748046875 = 0.6026603579521179 + 10.0 * 8.481409072875977
Epoch 830, val loss: 0.6276509761810303
Epoch 840, training loss: 85.4475326538086 = 0.5988158583641052 + 10.0 * 8.484871864318848
Epoch 840, val loss: 0.6244163513183594
Epoch 850, training loss: 85.40122985839844 = 0.5950591564178467 + 10.0 * 8.480616569519043
Epoch 850, val loss: 0.6212008595466614
Epoch 860, training loss: 85.38143920898438 = 0.5914646983146667 + 10.0 * 8.478998184204102
Epoch 860, val loss: 0.6182007789611816
Epoch 870, training loss: 85.36725616455078 = 0.5879671573638916 + 10.0 * 8.47792911529541
Epoch 870, val loss: 0.6152707934379578
Epoch 880, training loss: 85.3547592163086 = 0.5845442414283752 + 10.0 * 8.477022171020508
Epoch 880, val loss: 0.6123921275138855
Epoch 890, training loss: 85.36337280273438 = 0.5811761021614075 + 10.0 * 8.478219985961914
Epoch 890, val loss: 0.6096164584159851
Epoch 900, training loss: 85.33587646484375 = 0.5778342485427856 + 10.0 * 8.475804328918457
Epoch 900, val loss: 0.60673588514328
Epoch 910, training loss: 85.32134246826172 = 0.5746033787727356 + 10.0 * 8.474674224853516
Epoch 910, val loss: 0.6040860414505005
Epoch 920, training loss: 85.3061294555664 = 0.5714325904846191 + 10.0 * 8.473469734191895
Epoch 920, val loss: 0.6014211773872375
Epoch 930, training loss: 85.29244232177734 = 0.5683196783065796 + 10.0 * 8.472412109375
Epoch 930, val loss: 0.5988489389419556
Epoch 940, training loss: 85.28117370605469 = 0.5652529001235962 + 10.0 * 8.47159194946289
Epoch 940, val loss: 0.5963113903999329
Epoch 950, training loss: 85.29721069335938 = 0.562224805355072 + 10.0 * 8.473498344421387
Epoch 950, val loss: 0.5937850475311279
Epoch 960, training loss: 85.27569580078125 = 0.5591998100280762 + 10.0 * 8.471650123596191
Epoch 960, val loss: 0.5912617444992065
Epoch 970, training loss: 85.24357604980469 = 0.5562703013420105 + 10.0 * 8.468730926513672
Epoch 970, val loss: 0.5888564586639404
Epoch 980, training loss: 85.23239135742188 = 0.5533953309059143 + 10.0 * 8.467899322509766
Epoch 980, val loss: 0.5864834189414978
Epoch 990, training loss: 85.22919464111328 = 0.5505536198616028 + 10.0 * 8.467864036560059
Epoch 990, val loss: 0.5841474533081055
Epoch 1000, training loss: 85.21247863769531 = 0.547728955745697 + 10.0 * 8.466474533081055
Epoch 1000, val loss: 0.5817934274673462
Epoch 1010, training loss: 85.19795227050781 = 0.5449722409248352 + 10.0 * 8.46529769897461
Epoch 1010, val loss: 0.5794802308082581
Epoch 1020, training loss: 85.18585205078125 = 0.5422670245170593 + 10.0 * 8.46435832977295
Epoch 1020, val loss: 0.5772500038146973
Epoch 1030, training loss: 85.17466735839844 = 0.5395969152450562 + 10.0 * 8.463506698608398
Epoch 1030, val loss: 0.5750194191932678
Epoch 1040, training loss: 85.17707824707031 = 0.5369471311569214 + 10.0 * 8.46401309967041
Epoch 1040, val loss: 0.5728031396865845
Epoch 1050, training loss: 85.15922546386719 = 0.5343133807182312 + 10.0 * 8.462491035461426
Epoch 1050, val loss: 0.5707015991210938
Epoch 1060, training loss: 85.14386749267578 = 0.5317169427871704 + 10.0 * 8.461215019226074
Epoch 1060, val loss: 0.568544864654541
Epoch 1070, training loss: 85.15353393554688 = 0.5291600227355957 + 10.0 * 8.462437629699707
Epoch 1070, val loss: 0.5664469003677368
Epoch 1080, training loss: 85.14263916015625 = 0.5266181230545044 + 10.0 * 8.461602210998535
Epoch 1080, val loss: 0.5644177794456482
Epoch 1090, training loss: 85.1182632446289 = 0.5241008400917053 + 10.0 * 8.459416389465332
Epoch 1090, val loss: 0.5623186826705933
Epoch 1100, training loss: 85.10674285888672 = 0.5216467976570129 + 10.0 * 8.45850944519043
Epoch 1100, val loss: 0.5602783560752869
Epoch 1110, training loss: 85.09410858154297 = 0.5192196369171143 + 10.0 * 8.457489013671875
Epoch 1110, val loss: 0.5583478808403015
Epoch 1120, training loss: 85.08616638183594 = 0.5168043375015259 + 10.0 * 8.45693588256836
Epoch 1120, val loss: 0.5563953518867493
Epoch 1130, training loss: 85.10215759277344 = 0.5144006013870239 + 10.0 * 8.458775520324707
Epoch 1130, val loss: 0.5545094013214111
Epoch 1140, training loss: 85.07247924804688 = 0.5119951367378235 + 10.0 * 8.456048965454102
Epoch 1140, val loss: 0.5524529218673706
Epoch 1150, training loss: 85.06658172607422 = 0.5096337795257568 + 10.0 * 8.455694198608398
Epoch 1150, val loss: 0.5506261587142944
Epoch 1160, training loss: 85.05128479003906 = 0.5072926878929138 + 10.0 * 8.454399108886719
Epoch 1160, val loss: 0.5486804246902466
Epoch 1170, training loss: 85.0421142578125 = 0.5049756765365601 + 10.0 * 8.453714370727539
Epoch 1170, val loss: 0.546853244304657
Epoch 1180, training loss: 85.04154968261719 = 0.5026726722717285 + 10.0 * 8.453887939453125
Epoch 1180, val loss: 0.5450183153152466
Epoch 1190, training loss: 85.03436279296875 = 0.5003657937049866 + 10.0 * 8.453399658203125
Epoch 1190, val loss: 0.5431505441665649
Epoch 1200, training loss: 85.02365112304688 = 0.4980836510658264 + 10.0 * 8.452556610107422
Epoch 1200, val loss: 0.5413218140602112
Epoch 1210, training loss: 85.0146255493164 = 0.49581339955329895 + 10.0 * 8.451881408691406
Epoch 1210, val loss: 0.5395463705062866
Epoch 1220, training loss: 85.02722930908203 = 0.493548721075058 + 10.0 * 8.453368186950684
Epoch 1220, val loss: 0.5377205610275269
Epoch 1230, training loss: 85.00009155273438 = 0.4912741184234619 + 10.0 * 8.450881958007812
Epoch 1230, val loss: 0.5360036492347717
Epoch 1240, training loss: 84.99022674560547 = 0.48903876543045044 + 10.0 * 8.450119018554688
Epoch 1240, val loss: 0.534295380115509
Epoch 1250, training loss: 84.98181915283203 = 0.4868241548538208 + 10.0 * 8.449499130249023
Epoch 1250, val loss: 0.5325539708137512
Epoch 1260, training loss: 84.97590637207031 = 0.48462849855422974 + 10.0 * 8.449128150939941
Epoch 1260, val loss: 0.5308963656425476
Epoch 1270, training loss: 84.99420166015625 = 0.4824391305446625 + 10.0 * 8.451176643371582
Epoch 1270, val loss: 0.5292474031448364
Epoch 1280, training loss: 84.9852294921875 = 0.4802389442920685 + 10.0 * 8.450498580932617
Epoch 1280, val loss: 0.5274614691734314
Epoch 1290, training loss: 84.95867919921875 = 0.4780798554420471 + 10.0 * 8.448060035705566
Epoch 1290, val loss: 0.5258637070655823
Epoch 1300, training loss: 84.94856262207031 = 0.4759482443332672 + 10.0 * 8.447261810302734
Epoch 1300, val loss: 0.5242565274238586
Epoch 1310, training loss: 84.94306945800781 = 0.473850280046463 + 10.0 * 8.446921348571777
Epoch 1310, val loss: 0.5226436853408813
Epoch 1320, training loss: 84.96508026123047 = 0.47176119685173035 + 10.0 * 8.449331283569336
Epoch 1320, val loss: 0.5210203528404236
Epoch 1330, training loss: 84.93572235107422 = 0.4696561098098755 + 10.0 * 8.446606636047363
Epoch 1330, val loss: 0.5195915699005127
Epoch 1340, training loss: 84.92729187011719 = 0.46760183572769165 + 10.0 * 8.445968627929688
Epoch 1340, val loss: 0.5180127024650574
Epoch 1350, training loss: 84.91838073730469 = 0.465561181306839 + 10.0 * 8.445281982421875
Epoch 1350, val loss: 0.5165459513664246
Epoch 1360, training loss: 84.91156768798828 = 0.4635365307331085 + 10.0 * 8.444803237915039
Epoch 1360, val loss: 0.5150968432426453
Epoch 1370, training loss: 84.93070220947266 = 0.461523175239563 + 10.0 * 8.446917533874512
Epoch 1370, val loss: 0.5136508345603943
Epoch 1380, training loss: 84.9112548828125 = 0.4595089256763458 + 10.0 * 8.445174217224121
Epoch 1380, val loss: 0.5121755003929138
Epoch 1390, training loss: 84.89923858642578 = 0.4575258195400238 + 10.0 * 8.444170951843262
Epoch 1390, val loss: 0.5108142495155334
Epoch 1400, training loss: 84.8892822265625 = 0.4555777311325073 + 10.0 * 8.443370819091797
Epoch 1400, val loss: 0.5094010233879089
Epoch 1410, training loss: 84.88282775878906 = 0.4536554515361786 + 10.0 * 8.442916870117188
Epoch 1410, val loss: 0.5081230998039246
Epoch 1420, training loss: 84.87755584716797 = 0.45174574851989746 + 10.0 * 8.442581176757812
Epoch 1420, val loss: 0.5067832469940186
Epoch 1430, training loss: 84.8924789428711 = 0.44984713196754456 + 10.0 * 8.444263458251953
Epoch 1430, val loss: 0.5055589079856873
Epoch 1440, training loss: 84.87118530273438 = 0.4479498565196991 + 10.0 * 8.442323684692383
Epoch 1440, val loss: 0.5041143298149109
Epoch 1450, training loss: 84.86724090576172 = 0.44607236981391907 + 10.0 * 8.442116737365723
Epoch 1450, val loss: 0.5029383301734924
Epoch 1460, training loss: 84.85967254638672 = 0.44422540068626404 + 10.0 * 8.441544532775879
Epoch 1460, val loss: 0.5016578435897827
Epoch 1470, training loss: 84.85310363769531 = 0.4423961341381073 + 10.0 * 8.441070556640625
Epoch 1470, val loss: 0.5004849433898926
Epoch 1480, training loss: 84.86672973632812 = 0.4405813217163086 + 10.0 * 8.442614555358887
Epoch 1480, val loss: 0.49932923913002014
Epoch 1490, training loss: 84.84288024902344 = 0.43877139687538147 + 10.0 * 8.440410614013672
Epoch 1490, val loss: 0.49815019965171814
Epoch 1500, training loss: 84.83812713623047 = 0.4369924068450928 + 10.0 * 8.44011402130127
Epoch 1500, val loss: 0.49708786606788635
Epoch 1510, training loss: 84.8317642211914 = 0.4352388083934784 + 10.0 * 8.439652442932129
Epoch 1510, val loss: 0.4959644675254822
Epoch 1520, training loss: 84.8260726928711 = 0.43350750207901 + 10.0 * 8.43925666809082
Epoch 1520, val loss: 0.49494504928588867
Epoch 1530, training loss: 84.825439453125 = 0.43179506063461304 + 10.0 * 8.439364433288574
Epoch 1530, val loss: 0.4938963055610657
Epoch 1540, training loss: 84.8337631225586 = 0.4300861358642578 + 10.0 * 8.440367698669434
Epoch 1540, val loss: 0.4928545653820038
Epoch 1550, training loss: 84.81856536865234 = 0.4283939599990845 + 10.0 * 8.439017295837402
Epoch 1550, val loss: 0.49194422364234924
Epoch 1560, training loss: 84.80868530273438 = 0.42673414945602417 + 10.0 * 8.43819522857666
Epoch 1560, val loss: 0.4910476505756378
Epoch 1570, training loss: 84.80375671386719 = 0.4250924289226532 + 10.0 * 8.4378662109375
Epoch 1570, val loss: 0.4901346266269684
Epoch 1580, training loss: 84.81124114990234 = 0.42345932126045227 + 10.0 * 8.438777923583984
Epoch 1580, val loss: 0.48924684524536133
Epoch 1590, training loss: 84.7995376586914 = 0.421836793422699 + 10.0 * 8.437769889831543
Epoch 1590, val loss: 0.4883624017238617
Epoch 1600, training loss: 84.81979370117188 = 0.42022427916526794 + 10.0 * 8.439956665039062
Epoch 1600, val loss: 0.48748278617858887
Epoch 1610, training loss: 84.78910827636719 = 0.4186159074306488 + 10.0 * 8.43704891204834
Epoch 1610, val loss: 0.4867793917655945
Epoch 1620, training loss: 84.7796401977539 = 0.41704824566841125 + 10.0 * 8.436259269714355
Epoch 1620, val loss: 0.4859706461429596
Epoch 1630, training loss: 84.77477264404297 = 0.41550275683403015 + 10.0 * 8.43592643737793
Epoch 1630, val loss: 0.4852212965488434
Epoch 1640, training loss: 84.7701187133789 = 0.41397207975387573 + 10.0 * 8.435614585876465
Epoch 1640, val loss: 0.48448821902275085
Epoch 1650, training loss: 84.76866149902344 = 0.4124477803707123 + 10.0 * 8.43562126159668
Epoch 1650, val loss: 0.4837890565395355
Epoch 1660, training loss: 84.76881408691406 = 0.4109317362308502 + 10.0 * 8.43578815460205
Epoch 1660, val loss: 0.4831072688102722
Epoch 1670, training loss: 84.761962890625 = 0.4094254970550537 + 10.0 * 8.435254096984863
Epoch 1670, val loss: 0.48250308632850647
Epoch 1680, training loss: 84.7646713256836 = 0.40793636441230774 + 10.0 * 8.435673713684082
Epoch 1680, val loss: 0.481878399848938
Epoch 1690, training loss: 84.7558822631836 = 0.4064471423625946 + 10.0 * 8.434943199157715
Epoch 1690, val loss: 0.4812307059764862
Epoch 1700, training loss: 84.76770782470703 = 0.40500813722610474 + 10.0 * 8.436269760131836
Epoch 1700, val loss: 0.4807411730289459
Epoch 1710, training loss: 84.74641418457031 = 0.4035802483558655 + 10.0 * 8.434283256530762
Epoch 1710, val loss: 0.4801502525806427
Epoch 1720, training loss: 84.73643493652344 = 0.40218913555145264 + 10.0 * 8.43342399597168
Epoch 1720, val loss: 0.4796353876590729
Epoch 1730, training loss: 84.73301696777344 = 0.40079042315483093 + 10.0 * 8.433222770690918
Epoch 1730, val loss: 0.47915053367614746
Epoch 1740, training loss: 84.72821044921875 = 0.3993992209434509 + 10.0 * 8.432881355285645
Epoch 1740, val loss: 0.4786585569381714
Epoch 1750, training loss: 84.7242431640625 = 0.3980124294757843 + 10.0 * 8.432622909545898
Epoch 1750, val loss: 0.4781702756881714
Epoch 1760, training loss: 84.72015380859375 = 0.3966345489025116 + 10.0 * 8.432352066040039
Epoch 1760, val loss: 0.47772330045700073
Epoch 1770, training loss: 84.72386169433594 = 0.39526185393333435 + 10.0 * 8.432859420776367
Epoch 1770, val loss: 0.4773275554180145
Epoch 1780, training loss: 84.72126007080078 = 0.3938802182674408 + 10.0 * 8.432737350463867
Epoch 1780, val loss: 0.47689542174339294
Epoch 1790, training loss: 84.72184753417969 = 0.39253589510917664 + 10.0 * 8.432931900024414
Epoch 1790, val loss: 0.4765433371067047
Epoch 1800, training loss: 84.71157836914062 = 0.3912128806114197 + 10.0 * 8.432036399841309
Epoch 1800, val loss: 0.4761318862438202
Epoch 1810, training loss: 84.70247650146484 = 0.3899036943912506 + 10.0 * 8.431257247924805
Epoch 1810, val loss: 0.4758736193180084
Epoch 1820, training loss: 84.6972427368164 = 0.38860392570495605 + 10.0 * 8.430864334106445
Epoch 1820, val loss: 0.47553372383117676
Epoch 1830, training loss: 84.69364929199219 = 0.38730818033218384 + 10.0 * 8.430634498596191
Epoch 1830, val loss: 0.47521716356277466
Epoch 1840, training loss: 84.68966674804688 = 0.38601410388946533 + 10.0 * 8.430364608764648
Epoch 1840, val loss: 0.47490260004997253
Epoch 1850, training loss: 84.68586730957031 = 0.38472849130630493 + 10.0 * 8.430113792419434
Epoch 1850, val loss: 0.4746069312095642
Epoch 1860, training loss: 84.68470764160156 = 0.3834506571292877 + 10.0 * 8.43012523651123
Epoch 1860, val loss: 0.4742911756038666
Epoch 1870, training loss: 84.69905090332031 = 0.382169246673584 + 10.0 * 8.43168830871582
Epoch 1870, val loss: 0.47406840324401855
Epoch 1880, training loss: 84.7329330444336 = 0.3808869421482086 + 10.0 * 8.43520450592041
Epoch 1880, val loss: 0.4739242494106293
Epoch 1890, training loss: 84.68789672851562 = 0.3795912265777588 + 10.0 * 8.430830001831055
Epoch 1890, val loss: 0.47334641218185425
Epoch 1900, training loss: 84.67350769042969 = 0.3783092498779297 + 10.0 * 8.429519653320312
Epoch 1900, val loss: 0.4733637869358063
Epoch 1910, training loss: 84.66527557373047 = 0.3770546615123749 + 10.0 * 8.42882251739502
Epoch 1910, val loss: 0.4730820953845978
Epoch 1920, training loss: 84.66022491455078 = 0.3758021593093872 + 10.0 * 8.428442001342773
Epoch 1920, val loss: 0.4730522632598877
Epoch 1930, training loss: 84.65677642822266 = 0.3745609223842621 + 10.0 * 8.428221702575684
Epoch 1930, val loss: 0.4729648530483246
Epoch 1940, training loss: 84.65703582763672 = 0.3733193576335907 + 10.0 * 8.42837142944336
Epoch 1940, val loss: 0.472756564617157
Epoch 1950, training loss: 84.67403411865234 = 0.37208181619644165 + 10.0 * 8.430194854736328
Epoch 1950, val loss: 0.4726678133010864
Epoch 1960, training loss: 84.65680694580078 = 0.37087103724479675 + 10.0 * 8.428593635559082
Epoch 1960, val loss: 0.4724271595478058
Epoch 1970, training loss: 84.66527557373047 = 0.36965736746788025 + 10.0 * 8.429561614990234
Epoch 1970, val loss: 0.47228631377220154
Epoch 1980, training loss: 84.6436996459961 = 0.3684593737125397 + 10.0 * 8.427523612976074
Epoch 1980, val loss: 0.47220906615257263
Epoch 1990, training loss: 84.64054870605469 = 0.36726635694503784 + 10.0 * 8.427328109741211
Epoch 1990, val loss: 0.472220778465271
Epoch 2000, training loss: 84.6326675415039 = 0.3660867512226105 + 10.0 * 8.426657676696777
Epoch 2000, val loss: 0.4720703959465027
Epoch 2010, training loss: 84.6321792602539 = 0.36490628123283386 + 10.0 * 8.426727294921875
Epoch 2010, val loss: 0.4719894826412201
Epoch 2020, training loss: 84.64759826660156 = 0.3637301027774811 + 10.0 * 8.428386688232422
Epoch 2020, val loss: 0.47205832600593567
Epoch 2030, training loss: 84.62821960449219 = 0.36255812644958496 + 10.0 * 8.426566123962402
Epoch 2030, val loss: 0.4718450605869293
Epoch 2040, training loss: 84.6229019165039 = 0.3613954186439514 + 10.0 * 8.42615032196045
Epoch 2040, val loss: 0.47182270884513855
Epoch 2050, training loss: 84.61746215820312 = 0.3602415919303894 + 10.0 * 8.425722122192383
Epoch 2050, val loss: 0.47184842824935913
Epoch 2060, training loss: 84.61174774169922 = 0.3590894639492035 + 10.0 * 8.425265312194824
Epoch 2060, val loss: 0.47177672386169434
Epoch 2070, training loss: 84.60968780517578 = 0.3579460084438324 + 10.0 * 8.42517375946045
Epoch 2070, val loss: 0.4717940390110016
Epoch 2080, training loss: 84.63776397705078 = 0.3568069636821747 + 10.0 * 8.428095817565918
Epoch 2080, val loss: 0.4719812273979187
Epoch 2090, training loss: 84.61973571777344 = 0.3556632697582245 + 10.0 * 8.426407814025879
Epoch 2090, val loss: 0.47185349464416504
Epoch 2100, training loss: 84.60514068603516 = 0.35453906655311584 + 10.0 * 8.425060272216797
Epoch 2100, val loss: 0.471885085105896
Epoch 2110, training loss: 84.59709167480469 = 0.3534230887889862 + 10.0 * 8.42436695098877
Epoch 2110, val loss: 0.47196710109710693
Epoch 2120, training loss: 84.59480285644531 = 0.3523222804069519 + 10.0 * 8.424247741699219
Epoch 2120, val loss: 0.4720156192779541
Epoch 2130, training loss: 84.61517333984375 = 0.35122406482696533 + 10.0 * 8.42639446258545
Epoch 2130, val loss: 0.472019225358963
Epoch 2140, training loss: 84.5853042602539 = 0.35012173652648926 + 10.0 * 8.423518180847168
Epoch 2140, val loss: 0.4722338616847992
Epoch 2150, training loss: 84.58534240722656 = 0.34903109073638916 + 10.0 * 8.42363166809082
Epoch 2150, val loss: 0.47233331203460693
Epoch 2160, training loss: 84.58172607421875 = 0.34795188903808594 + 10.0 * 8.42337703704834
Epoch 2160, val loss: 0.4724416136741638
Epoch 2170, training loss: 84.57662200927734 = 0.34687697887420654 + 10.0 * 8.422974586486816
Epoch 2170, val loss: 0.4725266098976135
Epoch 2180, training loss: 84.59635925292969 = 0.3458026647567749 + 10.0 * 8.425055503845215
Epoch 2180, val loss: 0.472598135471344
Epoch 2190, training loss: 84.5857925415039 = 0.34473565220832825 + 10.0 * 8.424105644226074
Epoch 2190, val loss: 0.47277769446372986
Epoch 2200, training loss: 84.58889770507812 = 0.34367793798446655 + 10.0 * 8.424521446228027
Epoch 2200, val loss: 0.4727914333343506
Epoch 2210, training loss: 84.57142639160156 = 0.34261393547058105 + 10.0 * 8.422881126403809
Epoch 2210, val loss: 0.47319966554641724
Epoch 2220, training loss: 84.56367492675781 = 0.34157153964042664 + 10.0 * 8.422210693359375
Epoch 2220, val loss: 0.4733003079891205
Epoch 2230, training loss: 84.55833435058594 = 0.34053799510002136 + 10.0 * 8.42177963256836
Epoch 2230, val loss: 0.473458468914032
Epoch 2240, training loss: 84.55622863769531 = 0.33951041102409363 + 10.0 * 8.421671867370605
Epoch 2240, val loss: 0.47363921999931335
Epoch 2250, training loss: 84.58512115478516 = 0.3384840190410614 + 10.0 * 8.424663543701172
Epoch 2250, val loss: 0.473690927028656
Epoch 2260, training loss: 84.56562805175781 = 0.33743593096733093 + 10.0 * 8.422819137573242
Epoch 2260, val loss: 0.4740206301212311
Epoch 2270, training loss: 84.56358337402344 = 0.3364107608795166 + 10.0 * 8.422717094421387
Epoch 2270, val loss: 0.4741149842739105
Epoch 2280, training loss: 84.54387664794922 = 0.33538368344306946 + 10.0 * 8.420849800109863
Epoch 2280, val loss: 0.4742344617843628
Epoch 2290, training loss: 84.54147338867188 = 0.3343521058559418 + 10.0 * 8.4207124710083
Epoch 2290, val loss: 0.4745044410228729
Epoch 2300, training loss: 84.53816986083984 = 0.3333231806755066 + 10.0 * 8.42048454284668
Epoch 2300, val loss: 0.47470027208328247
Epoch 2310, training loss: 84.53466796875 = 0.3322935104370117 + 10.0 * 8.42023754119873
Epoch 2310, val loss: 0.47497498989105225
Epoch 2320, training loss: 84.53174591064453 = 0.33127301931381226 + 10.0 * 8.42004680633545
Epoch 2320, val loss: 0.4751530587673187
Epoch 2330, training loss: 84.54542541503906 = 0.33025306463241577 + 10.0 * 8.421517372131348
Epoch 2330, val loss: 0.4753110110759735
Epoch 2340, training loss: 84.53471374511719 = 0.329227089881897 + 10.0 * 8.420549392700195
Epoch 2340, val loss: 0.47574612498283386
Epoch 2350, training loss: 84.53471374511719 = 0.32821911573410034 + 10.0 * 8.420649528503418
Epoch 2350, val loss: 0.4759460687637329
Epoch 2360, training loss: 84.52557373046875 = 0.3272285461425781 + 10.0 * 8.41983413696289
Epoch 2360, val loss: 0.476081520318985
Epoch 2370, training loss: 84.5176773071289 = 0.32624003291130066 + 10.0 * 8.419143676757812
Epoch 2370, val loss: 0.476441353559494
Epoch 2380, training loss: 84.51569366455078 = 0.3252488374710083 + 10.0 * 8.419044494628906
Epoch 2380, val loss: 0.4767388105392456
Epoch 2390, training loss: 84.51219940185547 = 0.3242623805999756 + 10.0 * 8.418793678283691
Epoch 2390, val loss: 0.4770561158657074
Epoch 2400, training loss: 84.51056671142578 = 0.32326844334602356 + 10.0 * 8.418729782104492
Epoch 2400, val loss: 0.4772997796535492
Epoch 2410, training loss: 84.53833770751953 = 0.32228556275367737 + 10.0 * 8.421605110168457
Epoch 2410, val loss: 0.477585107088089
Epoch 2420, training loss: 84.56317138671875 = 0.3213067948818207 + 10.0 * 8.424186706542969
Epoch 2420, val loss: 0.4777350425720215
Epoch 2430, training loss: 84.5146713256836 = 0.3203175663948059 + 10.0 * 8.419435501098633
Epoch 2430, val loss: 0.47817814350128174
Epoch 2440, training loss: 84.50331115722656 = 0.31935563683509827 + 10.0 * 8.418395042419434
Epoch 2440, val loss: 0.4784550070762634
Epoch 2450, training loss: 84.49798583984375 = 0.31838786602020264 + 10.0 * 8.417959213256836
Epoch 2450, val loss: 0.47884848713874817
Epoch 2460, training loss: 84.49373626708984 = 0.3174237906932831 + 10.0 * 8.417631149291992
Epoch 2460, val loss: 0.47914573550224304
Epoch 2470, training loss: 84.49137878417969 = 0.3164611756801605 + 10.0 * 8.417491912841797
Epoch 2470, val loss: 0.47945258021354675
Epoch 2480, training loss: 84.4891128540039 = 0.31549280881881714 + 10.0 * 8.417362213134766
Epoch 2480, val loss: 0.47985339164733887
Epoch 2490, training loss: 84.5073013305664 = 0.3145209848880768 + 10.0 * 8.419278144836426
Epoch 2490, val loss: 0.48037096858024597
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.791476407914764
0.8169963051510541
=== training gcn model ===
Epoch 0, training loss: 106.9239501953125 = 1.100900411605835 + 10.0 * 10.582304954528809
Epoch 0, val loss: 1.0996477603912354
Epoch 10, training loss: 106.91699981689453 = 1.0959993600845337 + 10.0 * 10.582099914550781
Epoch 10, val loss: 1.0948041677474976
Epoch 20, training loss: 106.90257263183594 = 1.0909227132797241 + 10.0 * 10.581165313720703
Epoch 20, val loss: 1.089769721031189
Epoch 30, training loss: 106.85501861572266 = 1.0853652954101562 + 10.0 * 10.57696533203125
Epoch 30, val loss: 1.0842636823654175
Epoch 40, training loss: 106.67980194091797 = 1.0791513919830322 + 10.0 * 10.560064315795898
Epoch 40, val loss: 1.0781431198120117
Epoch 50, training loss: 106.09992218017578 = 1.0721079111099243 + 10.0 * 10.502781867980957
Epoch 50, val loss: 1.0711984634399414
Epoch 60, training loss: 104.36214447021484 = 1.06429922580719 + 10.0 * 10.329784393310547
Epoch 60, val loss: 1.063564419746399
Epoch 70, training loss: 100.56399536132812 = 1.0557185411453247 + 10.0 * 9.950827598571777
Epoch 70, val loss: 1.0550951957702637
Epoch 80, training loss: 98.38894653320312 = 1.048338770866394 + 10.0 * 9.734060287475586
Epoch 80, val loss: 1.0481592416763306
Epoch 90, training loss: 97.71334075927734 = 1.0438947677612305 + 10.0 * 9.66694450378418
Epoch 90, val loss: 1.0439636707305908
Epoch 100, training loss: 97.09855651855469 = 1.0405540466308594 + 10.0 * 9.60580062866211
Epoch 100, val loss: 1.040622353553772
Epoch 110, training loss: 95.94959259033203 = 1.0371038913726807 + 10.0 * 9.491249084472656
Epoch 110, val loss: 1.0371510982513428
Epoch 120, training loss: 94.17561340332031 = 1.0337817668914795 + 10.0 * 9.314183235168457
Epoch 120, val loss: 1.0338722467422485
Epoch 130, training loss: 92.88378143310547 = 1.0311226844787598 + 10.0 * 9.185266494750977
Epoch 130, val loss: 1.0312713384628296
Epoch 140, training loss: 92.20687866210938 = 1.0284066200256348 + 10.0 * 9.117847442626953
Epoch 140, val loss: 1.0284885168075562
Epoch 150, training loss: 91.87461853027344 = 1.0249385833740234 + 10.0 * 9.084967613220215
Epoch 150, val loss: 1.0248392820358276
Epoch 160, training loss: 91.6320571899414 = 1.0207098722457886 + 10.0 * 9.061135292053223
Epoch 160, val loss: 1.020499348640442
Epoch 170, training loss: 91.31769561767578 = 1.0168389081954956 + 10.0 * 9.030085563659668
Epoch 170, val loss: 1.0166820287704468
Epoch 180, training loss: 90.8605728149414 = 1.0140045881271362 + 10.0 * 8.98465633392334
Epoch 180, val loss: 1.0139455795288086
Epoch 190, training loss: 90.35527801513672 = 1.0117419958114624 + 10.0 * 8.934353828430176
Epoch 190, val loss: 1.0116500854492188
Epoch 200, training loss: 90.0591812133789 = 1.0090898275375366 + 10.0 * 8.905009269714355
Epoch 200, val loss: 1.008754849433899
Epoch 210, training loss: 89.78225708007812 = 1.005439281463623 + 10.0 * 8.877681732177734
Epoch 210, val loss: 1.0049947500228882
Epoch 220, training loss: 89.42850494384766 = 1.0020002126693726 + 10.0 * 8.842650413513184
Epoch 220, val loss: 1.001679539680481
Epoch 230, training loss: 89.0417251586914 = 0.9992154836654663 + 10.0 * 8.804250717163086
Epoch 230, val loss: 0.9989041686058044
Epoch 240, training loss: 88.74667358398438 = 0.9962738156318665 + 10.0 * 8.775039672851562
Epoch 240, val loss: 0.995961606502533
Epoch 250, training loss: 88.57360076904297 = 0.9924340844154358 + 10.0 * 8.758116722106934
Epoch 250, val loss: 0.9920363426208496
Epoch 260, training loss: 88.43657684326172 = 0.9876608848571777 + 10.0 * 8.744891166687012
Epoch 260, val loss: 0.9871859550476074
Epoch 270, training loss: 88.29754638671875 = 0.982479989528656 + 10.0 * 8.73150634765625
Epoch 270, val loss: 0.9819895625114441
Epoch 280, training loss: 88.1673355102539 = 0.9772253632545471 + 10.0 * 8.719011306762695
Epoch 280, val loss: 0.9767316579818726
Epoch 290, training loss: 88.04178619384766 = 0.9717376828193665 + 10.0 * 8.70700454711914
Epoch 290, val loss: 0.9712778925895691
Epoch 300, training loss: 87.94477844238281 = 0.9659110307693481 + 10.0 * 8.69788646697998
Epoch 300, val loss: 0.9653545618057251
Epoch 310, training loss: 87.85838317871094 = 0.9595620632171631 + 10.0 * 8.689882278442383
Epoch 310, val loss: 0.9589622616767883
Epoch 320, training loss: 87.77999877929688 = 0.9528539180755615 + 10.0 * 8.682714462280273
Epoch 320, val loss: 0.9522066712379456
Epoch 330, training loss: 87.692626953125 = 0.9459137916564941 + 10.0 * 8.674671173095703
Epoch 330, val loss: 0.9453309178352356
Epoch 340, training loss: 87.60752868652344 = 0.9388378262519836 + 10.0 * 8.666869163513184
Epoch 340, val loss: 0.9382481575012207
Epoch 350, training loss: 87.50314331054688 = 0.9315276145935059 + 10.0 * 8.657161712646484
Epoch 350, val loss: 0.9310163259506226
Epoch 360, training loss: 87.40733337402344 = 0.9240298271179199 + 10.0 * 8.648330688476562
Epoch 360, val loss: 0.9235270023345947
Epoch 370, training loss: 87.33019256591797 = 0.9162542223930359 + 10.0 * 8.641393661499023
Epoch 370, val loss: 0.9157775044441223
Epoch 380, training loss: 87.23804473876953 = 0.9081549048423767 + 10.0 * 8.632988929748535
Epoch 380, val loss: 0.9077634215354919
Epoch 390, training loss: 87.16521453857422 = 0.8998450040817261 + 10.0 * 8.626537322998047
Epoch 390, val loss: 0.8995142579078674
Epoch 400, training loss: 87.07661437988281 = 0.8912763595581055 + 10.0 * 8.618534088134766
Epoch 400, val loss: 0.8911292552947998
Epoch 410, training loss: 86.99935150146484 = 0.8825770020484924 + 10.0 * 8.611677169799805
Epoch 410, val loss: 0.8825693726539612
Epoch 420, training loss: 86.9295654296875 = 0.8737478852272034 + 10.0 * 8.605581283569336
Epoch 420, val loss: 0.8739149570465088
Epoch 430, training loss: 86.87141418457031 = 0.8648046851158142 + 10.0 * 8.60066032409668
Epoch 430, val loss: 0.8650526404380798
Epoch 440, training loss: 86.80926513671875 = 0.8557299971580505 + 10.0 * 8.595354080200195
Epoch 440, val loss: 0.8562682867050171
Epoch 450, training loss: 86.75831604003906 = 0.8466827869415283 + 10.0 * 8.591163635253906
Epoch 450, val loss: 0.8474765419960022
Epoch 460, training loss: 86.72100067138672 = 0.8376063108444214 + 10.0 * 8.588338851928711
Epoch 460, val loss: 0.8387178182601929
Epoch 470, training loss: 86.67855072021484 = 0.8285598754882812 + 10.0 * 8.584999084472656
Epoch 470, val loss: 0.8299147486686707
Epoch 480, training loss: 86.62012481689453 = 0.8195450901985168 + 10.0 * 8.580058097839355
Epoch 480, val loss: 0.8212969899177551
Epoch 490, training loss: 86.5770263671875 = 0.8106535077095032 + 10.0 * 8.576637268066406
Epoch 490, val loss: 0.8128277063369751
Epoch 500, training loss: 86.53334045410156 = 0.801893413066864 + 10.0 * 8.573144912719727
Epoch 500, val loss: 0.8044838905334473
Epoch 510, training loss: 86.4931640625 = 0.7931708693504333 + 10.0 * 8.569999694824219
Epoch 510, val loss: 0.7962358593940735
Epoch 520, training loss: 86.45000457763672 = 0.7845828533172607 + 10.0 * 8.56654167175293
Epoch 520, val loss: 0.7881343364715576
Epoch 530, training loss: 86.41277313232422 = 0.7762171030044556 + 10.0 * 8.563655853271484
Epoch 530, val loss: 0.7803123593330383
Epoch 540, training loss: 86.37547302246094 = 0.7679844498634338 + 10.0 * 8.560749053955078
Epoch 540, val loss: 0.7726311087608337
Epoch 550, training loss: 86.35452270507812 = 0.7598584890365601 + 10.0 * 8.559466361999512
Epoch 550, val loss: 0.7651061415672302
Epoch 560, training loss: 86.30750274658203 = 0.7518681287765503 + 10.0 * 8.555562973022461
Epoch 560, val loss: 0.7578040957450867
Epoch 570, training loss: 86.27533721923828 = 0.7441655397415161 + 10.0 * 8.553117752075195
Epoch 570, val loss: 0.750795304775238
Epoch 580, training loss: 86.24602508544922 = 0.7366799116134644 + 10.0 * 8.550934791564941
Epoch 580, val loss: 0.7439911961555481
Epoch 590, training loss: 86.21795654296875 = 0.7293722629547119 + 10.0 * 8.548858642578125
Epoch 590, val loss: 0.7373937368392944
Epoch 600, training loss: 86.22039031982422 = 0.7222515940666199 + 10.0 * 8.549814224243164
Epoch 600, val loss: 0.731019914150238
Epoch 610, training loss: 86.17013549804688 = 0.7153429985046387 + 10.0 * 8.545479774475098
Epoch 610, val loss: 0.72486811876297
Epoch 620, training loss: 86.1446533203125 = 0.7087006568908691 + 10.0 * 8.543595314025879
Epoch 620, val loss: 0.719023585319519
Epoch 630, training loss: 86.1183853149414 = 0.7022897601127625 + 10.0 * 8.541608810424805
Epoch 630, val loss: 0.7134186029434204
Epoch 640, training loss: 86.09484100341797 = 0.6960949897766113 + 10.0 * 8.539874076843262
Epoch 640, val loss: 0.7080068588256836
Epoch 650, training loss: 86.10023498535156 = 0.6900830864906311 + 10.0 * 8.541014671325684
Epoch 650, val loss: 0.702833890914917
Epoch 660, training loss: 86.04963684082031 = 0.6843308806419373 + 10.0 * 8.536530494689941
Epoch 660, val loss: 0.6978880167007446
Epoch 670, training loss: 86.02220153808594 = 0.6788390278816223 + 10.0 * 8.53433609008789
Epoch 670, val loss: 0.6932067275047302
Epoch 680, training loss: 85.99826049804688 = 0.6735702157020569 + 10.0 * 8.532468795776367
Epoch 680, val loss: 0.6887266635894775
Epoch 690, training loss: 85.9742660522461 = 0.6684713959693909 + 10.0 * 8.530579566955566
Epoch 690, val loss: 0.684444010257721
Epoch 700, training loss: 85.9544677734375 = 0.6635440587997437 + 10.0 * 8.529092788696289
Epoch 700, val loss: 0.6803271174430847
Epoch 710, training loss: 85.94313049316406 = 0.6587420105934143 + 10.0 * 8.528438568115234
Epoch 710, val loss: 0.6763138771057129
Epoch 720, training loss: 85.91413116455078 = 0.6541249752044678 + 10.0 * 8.5260009765625
Epoch 720, val loss: 0.6725087761878967
Epoch 730, training loss: 85.89007568359375 = 0.6496909856796265 + 10.0 * 8.524038314819336
Epoch 730, val loss: 0.6688072681427002
Epoch 740, training loss: 85.87203216552734 = 0.6454004049301147 + 10.0 * 8.522663116455078
Epoch 740, val loss: 0.6652709245681763
Epoch 750, training loss: 85.88552856445312 = 0.6412200927734375 + 10.0 * 8.524431228637695
Epoch 750, val loss: 0.661812961101532
Epoch 760, training loss: 85.84876251220703 = 0.6371049284934998 + 10.0 * 8.52116584777832
Epoch 760, val loss: 0.6585098505020142
Epoch 770, training loss: 85.82740783691406 = 0.6331714391708374 + 10.0 * 8.519423484802246
Epoch 770, val loss: 0.6553145051002502
Epoch 780, training loss: 85.80864715576172 = 0.6293583512306213 + 10.0 * 8.517929077148438
Epoch 780, val loss: 0.6522151827812195
Epoch 790, training loss: 85.7958755493164 = 0.6256558895111084 + 10.0 * 8.517022132873535
Epoch 790, val loss: 0.6492317914962769
Epoch 800, training loss: 85.80533599853516 = 0.6220365762710571 + 10.0 * 8.518329620361328
Epoch 800, val loss: 0.6463289260864258
Epoch 810, training loss: 85.77220916748047 = 0.6184821724891663 + 10.0 * 8.515372276306152
Epoch 810, val loss: 0.6434602737426758
Epoch 820, training loss: 85.75923919677734 = 0.6150711178779602 + 10.0 * 8.514416694641113
Epoch 820, val loss: 0.6407254338264465
Epoch 830, training loss: 85.74482727050781 = 0.6117562055587769 + 10.0 * 8.513307571411133
Epoch 830, val loss: 0.6380870342254639
Epoch 840, training loss: 85.7334213256836 = 0.6085296273231506 + 10.0 * 8.512489318847656
Epoch 840, val loss: 0.63553786277771
Epoch 850, training loss: 85.7355728149414 = 0.6053766012191772 + 10.0 * 8.513019561767578
Epoch 850, val loss: 0.6330015659332275
Epoch 860, training loss: 85.7227783203125 = 0.6022716760635376 + 10.0 * 8.51205062866211
Epoch 860, val loss: 0.6305845379829407
Epoch 870, training loss: 85.70169067382812 = 0.599267303943634 + 10.0 * 8.510242462158203
Epoch 870, val loss: 0.6281610131263733
Epoch 880, training loss: 85.68514251708984 = 0.5963544249534607 + 10.0 * 8.508878707885742
Epoch 880, val loss: 0.625887930393219
Epoch 890, training loss: 85.67388916015625 = 0.5935137867927551 + 10.0 * 8.508037567138672
Epoch 890, val loss: 0.623643159866333
Epoch 900, training loss: 85.68280029296875 = 0.5907323360443115 + 10.0 * 8.509206771850586
Epoch 900, val loss: 0.6214384436607361
Epoch 910, training loss: 85.65983581542969 = 0.5879577994346619 + 10.0 * 8.507187843322754
Epoch 910, val loss: 0.619247555732727
Epoch 920, training loss: 85.64369201660156 = 0.5852999687194824 + 10.0 * 8.505839347839355
Epoch 920, val loss: 0.6171556711196899
Epoch 930, training loss: 85.62591552734375 = 0.5827051401138306 + 10.0 * 8.504321098327637
Epoch 930, val loss: 0.6151227951049805
Epoch 940, training loss: 85.61962890625 = 0.5801686644554138 + 10.0 * 8.503946304321289
Epoch 940, val loss: 0.6130948066711426
Epoch 950, training loss: 85.6041259765625 = 0.5776491761207581 + 10.0 * 8.502647399902344
Epoch 950, val loss: 0.6111637949943542
Epoch 960, training loss: 85.5909194946289 = 0.5751926302909851 + 10.0 * 8.501572608947754
Epoch 960, val loss: 0.6092082262039185
Epoch 970, training loss: 85.57980346679688 = 0.572790801525116 + 10.0 * 8.500700950622559
Epoch 970, val loss: 0.6073448061943054
Epoch 980, training loss: 85.56645965576172 = 0.5704324245452881 + 10.0 * 8.499602317810059
Epoch 980, val loss: 0.605477511882782
Epoch 990, training loss: 85.58442687988281 = 0.5680967569351196 + 10.0 * 8.501632690429688
Epoch 990, val loss: 0.6036144495010376
Epoch 1000, training loss: 85.56251525878906 = 0.5657506585121155 + 10.0 * 8.499676704406738
Epoch 1000, val loss: 0.6018097400665283
Epoch 1010, training loss: 85.53958892822266 = 0.5634711384773254 + 10.0 * 8.497611999511719
Epoch 1010, val loss: 0.6000232100486755
Epoch 1020, training loss: 85.52340698242188 = 0.5612559914588928 + 10.0 * 8.496214866638184
Epoch 1020, val loss: 0.5982882380485535
Epoch 1030, training loss: 85.51174926757812 = 0.5590697526931763 + 10.0 * 8.495267868041992
Epoch 1030, val loss: 0.5965716242790222
Epoch 1040, training loss: 85.51008605957031 = 0.5569025278091431 + 10.0 * 8.495318412780762
Epoch 1040, val loss: 0.5948442220687866
Epoch 1050, training loss: 85.49437713623047 = 0.5547184348106384 + 10.0 * 8.493966102600098
Epoch 1050, val loss: 0.5931479930877686
Epoch 1060, training loss: 85.48269653320312 = 0.5525652766227722 + 10.0 * 8.493013381958008
Epoch 1060, val loss: 0.5914344191551208
Epoch 1070, training loss: 85.47132873535156 = 0.5504524111747742 + 10.0 * 8.492087364196777
Epoch 1070, val loss: 0.5897677540779114
Epoch 1080, training loss: 85.46031951904297 = 0.5483729839324951 + 10.0 * 8.491194725036621
Epoch 1080, val loss: 0.5881187915802002
Epoch 1090, training loss: 85.48888397216797 = 0.5462912321090698 + 10.0 * 8.49425983428955
Epoch 1090, val loss: 0.5864676833152771
Epoch 1100, training loss: 85.46196746826172 = 0.544202446937561 + 10.0 * 8.491776466369629
Epoch 1100, val loss: 0.5848406553268433
Epoch 1110, training loss: 85.42833709716797 = 0.5421522855758667 + 10.0 * 8.488618850708008
Epoch 1110, val loss: 0.5831927061080933
Epoch 1120, training loss: 85.42081451416016 = 0.5401341915130615 + 10.0 * 8.488068580627441
Epoch 1120, val loss: 0.5815736651420593
Epoch 1130, training loss: 85.42105865478516 = 0.5381309986114502 + 10.0 * 8.488292694091797
Epoch 1130, val loss: 0.5800120830535889
Epoch 1140, training loss: 85.3991470336914 = 0.5361084342002869 + 10.0 * 8.486303329467773
Epoch 1140, val loss: 0.5783363580703735
Epoch 1150, training loss: 85.39170837402344 = 0.5341119766235352 + 10.0 * 8.485759735107422
Epoch 1150, val loss: 0.5767592787742615
Epoch 1160, training loss: 85.37884521484375 = 0.5321351289749146 + 10.0 * 8.484670639038086
Epoch 1160, val loss: 0.575163722038269
Epoch 1170, training loss: 85.37086486816406 = 0.5301640033721924 + 10.0 * 8.48406982421875
Epoch 1170, val loss: 0.5735818147659302
Epoch 1180, training loss: 85.3824234008789 = 0.528176486492157 + 10.0 * 8.485424995422363
Epoch 1180, val loss: 0.5720106959342957
Epoch 1190, training loss: 85.35563659667969 = 0.5261875987052917 + 10.0 * 8.482945442199707
Epoch 1190, val loss: 0.5703719854354858
Epoch 1200, training loss: 85.33686828613281 = 0.5242567658424377 + 10.0 * 8.481261253356934
Epoch 1200, val loss: 0.5688597559928894
Epoch 1210, training loss: 85.32866668701172 = 0.522339940071106 + 10.0 * 8.480632781982422
Epoch 1210, val loss: 0.5673227906227112
Epoch 1220, training loss: 85.31832885742188 = 0.52042156457901 + 10.0 * 8.479790687561035
Epoch 1220, val loss: 0.5657846927642822
Epoch 1230, training loss: 85.34224700927734 = 0.5184996128082275 + 10.0 * 8.48237419128418
Epoch 1230, val loss: 0.5642614960670471
Epoch 1240, training loss: 85.31336212158203 = 0.5165372490882874 + 10.0 * 8.479681968688965
Epoch 1240, val loss: 0.5625995993614197
Epoch 1250, training loss: 85.29422760009766 = 0.514623761177063 + 10.0 * 8.477960586547852
Epoch 1250, val loss: 0.5610832571983337
Epoch 1260, training loss: 85.28181457519531 = 0.5127158761024475 + 10.0 * 8.476909637451172
Epoch 1260, val loss: 0.5595558881759644
Epoch 1270, training loss: 85.28044128417969 = 0.5108126997947693 + 10.0 * 8.47696304321289
Epoch 1270, val loss: 0.5580172538757324
Epoch 1280, training loss: 85.26844024658203 = 0.5088908672332764 + 10.0 * 8.47595500946045
Epoch 1280, val loss: 0.556458055973053
Epoch 1290, training loss: 85.25919342041016 = 0.5069777965545654 + 10.0 * 8.475221633911133
Epoch 1290, val loss: 0.5549164414405823
Epoch 1300, training loss: 85.24971771240234 = 0.5050804018974304 + 10.0 * 8.47446346282959
Epoch 1300, val loss: 0.5533543825149536
Epoch 1310, training loss: 85.24393463134766 = 0.5031846165657043 + 10.0 * 8.474075317382812
Epoch 1310, val loss: 0.5518303513526917
Epoch 1320, training loss: 85.26905059814453 = 0.5012706518173218 + 10.0 * 8.476778030395508
Epoch 1320, val loss: 0.550308346748352
Epoch 1330, training loss: 85.23538208007812 = 0.4993661940097809 + 10.0 * 8.473601341247559
Epoch 1330, val loss: 0.5487076640129089
Epoch 1340, training loss: 85.21931457519531 = 0.49747318029403687 + 10.0 * 8.472184181213379
Epoch 1340, val loss: 0.5472084283828735
Epoch 1350, training loss: 85.2100601196289 = 0.4955959916114807 + 10.0 * 8.47144603729248
Epoch 1350, val loss: 0.5456806421279907
Epoch 1360, training loss: 85.20344543457031 = 0.4937131404876709 + 10.0 * 8.470973014831543
Epoch 1360, val loss: 0.5441786646842957
Epoch 1370, training loss: 85.23328399658203 = 0.4918140769004822 + 10.0 * 8.474146842956543
Epoch 1370, val loss: 0.542637288570404
Epoch 1380, training loss: 85.20857238769531 = 0.4899156093597412 + 10.0 * 8.4718656539917
Epoch 1380, val loss: 0.5412004590034485
Epoch 1390, training loss: 85.19058990478516 = 0.4880264401435852 + 10.0 * 8.470255851745605
Epoch 1390, val loss: 0.5396806597709656
Epoch 1400, training loss: 85.17662048339844 = 0.48615506291389465 + 10.0 * 8.469046592712402
Epoch 1400, val loss: 0.5382383465766907
Epoch 1410, training loss: 85.1712875366211 = 0.48429396748542786 + 10.0 * 8.46869945526123
Epoch 1410, val loss: 0.536783754825592
Epoch 1420, training loss: 85.16813659667969 = 0.482440322637558 + 10.0 * 8.4685697555542
Epoch 1420, val loss: 0.5353473424911499
Epoch 1430, training loss: 85.20001983642578 = 0.4805617928504944 + 10.0 * 8.471945762634277
Epoch 1430, val loss: 0.533909797668457
Epoch 1440, training loss: 85.15650939941406 = 0.4786979854106903 + 10.0 * 8.467781066894531
Epoch 1440, val loss: 0.5324593186378479
Epoch 1450, training loss: 85.14738464355469 = 0.47685617208480835 + 10.0 * 8.467053413391113
Epoch 1450, val loss: 0.5310304760932922
Epoch 1460, training loss: 85.14295959472656 = 0.4750272333621979 + 10.0 * 8.466793060302734
Epoch 1460, val loss: 0.52971351146698
Epoch 1470, training loss: 85.14216613769531 = 0.4732012450695038 + 10.0 * 8.466897010803223
Epoch 1470, val loss: 0.5283293724060059
Epoch 1480, training loss: 85.14412689208984 = 0.4713634252548218 + 10.0 * 8.467275619506836
Epoch 1480, val loss: 0.5269988179206848
Epoch 1490, training loss: 85.134521484375 = 0.46952947974205017 + 10.0 * 8.466499328613281
Epoch 1490, val loss: 0.525640606880188
Epoch 1500, training loss: 85.11935424804688 = 0.4677053391933441 + 10.0 * 8.465165138244629
Epoch 1500, val loss: 0.524290919303894
Epoch 1510, training loss: 85.11673736572266 = 0.4658965468406677 + 10.0 * 8.465084075927734
Epoch 1510, val loss: 0.5230185389518738
Epoch 1520, training loss: 85.11612701416016 = 0.4640858769416809 + 10.0 * 8.465204238891602
Epoch 1520, val loss: 0.5217344760894775
Epoch 1530, training loss: 85.11268615722656 = 0.46227702498435974 + 10.0 * 8.46504020690918
Epoch 1530, val loss: 0.5204704999923706
Epoch 1540, training loss: 85.09788513183594 = 0.4604797959327698 + 10.0 * 8.463740348815918
Epoch 1540, val loss: 0.5192538499832153
Epoch 1550, training loss: 85.0964126586914 = 0.4586959779262543 + 10.0 * 8.46377182006836
Epoch 1550, val loss: 0.5180195569992065
Epoch 1560, training loss: 85.103515625 = 0.4569109380245209 + 10.0 * 8.46466064453125
Epoch 1560, val loss: 0.5168173909187317
Epoch 1570, training loss: 85.09272003173828 = 0.4551180899143219 + 10.0 * 8.463760375976562
Epoch 1570, val loss: 0.5155729651451111
Epoch 1580, training loss: 85.08409881591797 = 0.45335131883621216 + 10.0 * 8.463074684143066
Epoch 1580, val loss: 0.5144261121749878
Epoch 1590, training loss: 85.08303833007812 = 0.45159798860549927 + 10.0 * 8.463144302368164
Epoch 1590, val loss: 0.5132889747619629
Epoch 1600, training loss: 85.07130432128906 = 0.4498368203639984 + 10.0 * 8.462146759033203
Epoch 1600, val loss: 0.5120893120765686
Epoch 1610, training loss: 85.06339263916016 = 0.4480985999107361 + 10.0 * 8.461529731750488
Epoch 1610, val loss: 0.5109745860099792
Epoch 1620, training loss: 85.07283020019531 = 0.44636762142181396 + 10.0 * 8.462646484375
Epoch 1620, val loss: 0.5098627805709839
Epoch 1630, training loss: 85.05378723144531 = 0.4446393549442291 + 10.0 * 8.460914611816406
Epoch 1630, val loss: 0.508765459060669
Epoch 1640, training loss: 85.0469970703125 = 0.4429335296154022 + 10.0 * 8.460406303405762
Epoch 1640, val loss: 0.5076837539672852
Epoch 1650, training loss: 85.0626449584961 = 0.4412321150302887 + 10.0 * 8.462141036987305
Epoch 1650, val loss: 0.5066301822662354
Epoch 1660, training loss: 85.03679656982422 = 0.4395206570625305 + 10.0 * 8.45972728729248
Epoch 1660, val loss: 0.5056347846984863
Epoch 1670, training loss: 85.04169464111328 = 0.43783172965049744 + 10.0 * 8.460386276245117
Epoch 1670, val loss: 0.5046050548553467
Epoch 1680, training loss: 85.02741241455078 = 0.43612903356552124 + 10.0 * 8.459128379821777
Epoch 1680, val loss: 0.5036153197288513
Epoch 1690, training loss: 85.02218627929688 = 0.4344632029533386 + 10.0 * 8.458772659301758
Epoch 1690, val loss: 0.5026875734329224
Epoch 1700, training loss: 85.01554870605469 = 0.4327920973300934 + 10.0 * 8.45827579498291
Epoch 1700, val loss: 0.5017497539520264
Epoch 1710, training loss: 85.017822265625 = 0.4311390519142151 + 10.0 * 8.45866870880127
Epoch 1710, val loss: 0.500816285610199
Epoch 1720, training loss: 85.0089340209961 = 0.42947375774383545 + 10.0 * 8.457945823669434
Epoch 1720, val loss: 0.49988463521003723
Epoch 1730, training loss: 84.99981689453125 = 0.4278295338153839 + 10.0 * 8.457199096679688
Epoch 1730, val loss: 0.49900388717651367
Epoch 1740, training loss: 84.9969482421875 = 0.42619338631629944 + 10.0 * 8.457075119018555
Epoch 1740, val loss: 0.49819666147232056
Epoch 1750, training loss: 85.02559661865234 = 0.4245557188987732 + 10.0 * 8.460103988647461
Epoch 1750, val loss: 0.49734947085380554
Epoch 1760, training loss: 84.98588562011719 = 0.42293116450309753 + 10.0 * 8.45629596710205
Epoch 1760, val loss: 0.4964987337589264
Epoch 1770, training loss: 84.97984313964844 = 0.4213292896747589 + 10.0 * 8.455851554870605
Epoch 1770, val loss: 0.49570998549461365
Epoch 1780, training loss: 84.9727783203125 = 0.4197457730770111 + 10.0 * 8.455303192138672
Epoch 1780, val loss: 0.4949817359447479
Epoch 1790, training loss: 84.9674072265625 = 0.4181612730026245 + 10.0 * 8.454924583435059
Epoch 1790, val loss: 0.4941904544830322
Epoch 1800, training loss: 84.96834564208984 = 0.4165809154510498 + 10.0 * 8.45517635345459
Epoch 1800, val loss: 0.49344155192375183
Epoch 1810, training loss: 84.98416137695312 = 0.4149916470050812 + 10.0 * 8.456916809082031
Epoch 1810, val loss: 0.4927148222923279
Epoch 1820, training loss: 84.9567642211914 = 0.4134247303009033 + 10.0 * 8.454334259033203
Epoch 1820, val loss: 0.4920436143875122
Epoch 1830, training loss: 84.94966888427734 = 0.41186651587486267 + 10.0 * 8.453780174255371
Epoch 1830, val loss: 0.4913831055164337
Epoch 1840, training loss: 84.97064208984375 = 0.4103229343891144 + 10.0 * 8.456031799316406
Epoch 1840, val loss: 0.49072661995887756
Epoch 1850, training loss: 84.94092559814453 = 0.40877780318260193 + 10.0 * 8.453214645385742
Epoch 1850, val loss: 0.4900782108306885
Epoch 1860, training loss: 84.93635559082031 = 0.40725067257881165 + 10.0 * 8.452910423278809
Epoch 1860, val loss: 0.4894624352455139
Epoch 1870, training loss: 84.93119812011719 = 0.4057379364967346 + 10.0 * 8.452546119689941
Epoch 1870, val loss: 0.4889088571071625
Epoch 1880, training loss: 84.92601776123047 = 0.4042280316352844 + 10.0 * 8.452178955078125
Epoch 1880, val loss: 0.4882844388484955
Epoch 1890, training loss: 84.921875 = 0.40272077918052673 + 10.0 * 8.451915740966797
Epoch 1890, val loss: 0.4877282977104187
Epoch 1900, training loss: 84.9476089477539 = 0.4012182950973511 + 10.0 * 8.454639434814453
Epoch 1900, val loss: 0.4871358871459961
Epoch 1910, training loss: 84.9402084350586 = 0.3996851146221161 + 10.0 * 8.454051971435547
Epoch 1910, val loss: 0.4866635501384735
Epoch 1920, training loss: 84.91020965576172 = 0.3981964886188507 + 10.0 * 8.451201438903809
Epoch 1920, val loss: 0.48610126972198486
Epoch 1930, training loss: 84.90889739990234 = 0.39672228693962097 + 10.0 * 8.451217651367188
Epoch 1930, val loss: 0.4856148362159729
Epoch 1940, training loss: 84.90113830566406 = 0.39524686336517334 + 10.0 * 8.450589179992676
Epoch 1940, val loss: 0.48514536023139954
Epoch 1950, training loss: 84.89729309082031 = 0.3937757909297943 + 10.0 * 8.45035171508789
Epoch 1950, val loss: 0.4847075939178467
Epoch 1960, training loss: 84.89605712890625 = 0.3923075795173645 + 10.0 * 8.450374603271484
Epoch 1960, val loss: 0.48425188660621643
Epoch 1970, training loss: 84.9593734741211 = 0.39083409309387207 + 10.0 * 8.456853866577148
Epoch 1970, val loss: 0.4838564097881317
Epoch 1980, training loss: 84.89421081542969 = 0.3893469572067261 + 10.0 * 8.45048713684082
Epoch 1980, val loss: 0.4834614098072052
Epoch 1990, training loss: 84.8834457397461 = 0.38789790868759155 + 10.0 * 8.449554443359375
Epoch 1990, val loss: 0.48310428857803345
Epoch 2000, training loss: 84.88028717041016 = 0.3864532709121704 + 10.0 * 8.449383735656738
Epoch 2000, val loss: 0.4826989471912384
Epoch 2010, training loss: 84.87397003173828 = 0.385018527507782 + 10.0 * 8.448895454406738
Epoch 2010, val loss: 0.48235055804252625
Epoch 2020, training loss: 84.87135314941406 = 0.38358619809150696 + 10.0 * 8.448777198791504
Epoch 2020, val loss: 0.48199567198753357
Epoch 2030, training loss: 84.90812683105469 = 0.3821500539779663 + 10.0 * 8.452597618103027
Epoch 2030, val loss: 0.481610506772995
Epoch 2040, training loss: 84.88320922851562 = 0.3807087540626526 + 10.0 * 8.450250625610352
Epoch 2040, val loss: 0.4813025891780853
Epoch 2050, training loss: 84.86033630371094 = 0.37928900122642517 + 10.0 * 8.448104858398438
Epoch 2050, val loss: 0.4809684455394745
Epoch 2060, training loss: 84.85751342773438 = 0.3778836131095886 + 10.0 * 8.447962760925293
Epoch 2060, val loss: 0.4807230234146118
Epoch 2070, training loss: 84.85301971435547 = 0.37647196650505066 + 10.0 * 8.447654724121094
Epoch 2070, val loss: 0.48046207427978516
Epoch 2080, training loss: 84.85391235351562 = 0.37505772709846497 + 10.0 * 8.447885513305664
Epoch 2080, val loss: 0.4802372455596924
Epoch 2090, training loss: 84.8607177734375 = 0.3736438453197479 + 10.0 * 8.448707580566406
Epoch 2090, val loss: 0.48000261187553406
Epoch 2100, training loss: 84.844482421875 = 0.37222638726234436 + 10.0 * 8.447225570678711
Epoch 2100, val loss: 0.4796934127807617
Epoch 2110, training loss: 84.83779907226562 = 0.37082451581954956 + 10.0 * 8.446697235107422
Epoch 2110, val loss: 0.4794800877571106
Epoch 2120, training loss: 84.83799743652344 = 0.36942535638809204 + 10.0 * 8.446857452392578
Epoch 2120, val loss: 0.47927093505859375
Epoch 2130, training loss: 84.8509750366211 = 0.36803197860717773 + 10.0 * 8.448293685913086
Epoch 2130, val loss: 0.47905728220939636
Epoch 2140, training loss: 84.82860565185547 = 0.36663931608200073 + 10.0 * 8.446196556091309
Epoch 2140, val loss: 0.4788025915622711
Epoch 2150, training loss: 84.82511138916016 = 0.3652650713920593 + 10.0 * 8.445984840393066
Epoch 2150, val loss: 0.4786301851272583
Epoch 2160, training loss: 84.84063720703125 = 0.36389636993408203 + 10.0 * 8.447673797607422
Epoch 2160, val loss: 0.4785241484642029
Epoch 2170, training loss: 84.81971740722656 = 0.36250898241996765 + 10.0 * 8.445720672607422
Epoch 2170, val loss: 0.47833994030952454
Epoch 2180, training loss: 84.81389617919922 = 0.36114615201950073 + 10.0 * 8.44527530670166
Epoch 2180, val loss: 0.4781923294067383
Epoch 2190, training loss: 84.8073959350586 = 0.3597925901412964 + 10.0 * 8.4447603225708
Epoch 2190, val loss: 0.4781104028224945
Epoch 2200, training loss: 84.8115234375 = 0.3584458827972412 + 10.0 * 8.445307731628418
Epoch 2200, val loss: 0.4780026078224182
Epoch 2210, training loss: 84.81524658203125 = 0.35709348320961 + 10.0 * 8.445815086364746
Epoch 2210, val loss: 0.47793859243392944
Epoch 2220, training loss: 84.80793762207031 = 0.35573863983154297 + 10.0 * 8.445219993591309
Epoch 2220, val loss: 0.4777287244796753
Epoch 2230, training loss: 84.795166015625 = 0.3544120192527771 + 10.0 * 8.444075584411621
Epoch 2230, val loss: 0.477718323469162
Epoch 2240, training loss: 84.7913818359375 = 0.353076696395874 + 10.0 * 8.443830490112305
Epoch 2240, val loss: 0.47763508558273315
Epoch 2250, training loss: 84.83380126953125 = 0.3517477214336395 + 10.0 * 8.448205947875977
Epoch 2250, val loss: 0.4776017665863037
Epoch 2260, training loss: 84.80644226074219 = 0.35040608048439026 + 10.0 * 8.44560432434082
Epoch 2260, val loss: 0.47767722606658936
Epoch 2270, training loss: 84.78176879882812 = 0.3490770161151886 + 10.0 * 8.443269729614258
Epoch 2270, val loss: 0.47756290435791016
Epoch 2280, training loss: 84.77734375 = 0.3477664589881897 + 10.0 * 8.442957878112793
Epoch 2280, val loss: 0.477649450302124
Epoch 2290, training loss: 84.77108001708984 = 0.34645912051200867 + 10.0 * 8.442461967468262
Epoch 2290, val loss: 0.47758740186691284
Epoch 2300, training loss: 84.76620483398438 = 0.3451559543609619 + 10.0 * 8.442105293273926
Epoch 2300, val loss: 0.47764483094215393
Epoch 2310, training loss: 84.76271057128906 = 0.34385278820991516 + 10.0 * 8.441885948181152
Epoch 2310, val loss: 0.47772669792175293
Epoch 2320, training loss: 84.76168823242188 = 0.3425476849079132 + 10.0 * 8.441914558410645
Epoch 2320, val loss: 0.4777947664260864
Epoch 2330, training loss: 84.80716705322266 = 0.3412403464317322 + 10.0 * 8.446592330932617
Epoch 2330, val loss: 0.47789883613586426
Epoch 2340, training loss: 84.76705169677734 = 0.33993101119995117 + 10.0 * 8.442712783813477
Epoch 2340, val loss: 0.4779427945613861
Epoch 2350, training loss: 84.75538635253906 = 0.33862578868865967 + 10.0 * 8.441676139831543
Epoch 2350, val loss: 0.4781366288661957
Epoch 2360, training loss: 84.74896240234375 = 0.3373388946056366 + 10.0 * 8.441162109375
Epoch 2360, val loss: 0.4781964123249054
Epoch 2370, training loss: 84.77507019042969 = 0.3360500633716583 + 10.0 * 8.443902015686035
Epoch 2370, val loss: 0.4783102571964264
Epoch 2380, training loss: 84.74771118164062 = 0.3347599506378174 + 10.0 * 8.44129467010498
Epoch 2380, val loss: 0.4784417152404785
Epoch 2390, training loss: 84.73916625976562 = 0.33348289132118225 + 10.0 * 8.440568923950195
Epoch 2390, val loss: 0.478621244430542
Epoch 2400, training loss: 84.73291778564453 = 0.3322078287601471 + 10.0 * 8.440071105957031
Epoch 2400, val loss: 0.47870856523513794
Epoch 2410, training loss: 84.73017120361328 = 0.33094078302383423 + 10.0 * 8.439923286437988
Epoch 2410, val loss: 0.47887861728668213
Epoch 2420, training loss: 84.77464294433594 = 0.3296760320663452 + 10.0 * 8.444497108459473
Epoch 2420, val loss: 0.4789694547653198
Epoch 2430, training loss: 84.73831939697266 = 0.32842448353767395 + 10.0 * 8.44098949432373
Epoch 2430, val loss: 0.47927823662757874
Epoch 2440, training loss: 84.722412109375 = 0.3271624743938446 + 10.0 * 8.43952465057373
Epoch 2440, val loss: 0.4794178009033203
Epoch 2450, training loss: 84.71538543701172 = 0.32591789960861206 + 10.0 * 8.438946723937988
Epoch 2450, val loss: 0.47956523299217224
Epoch 2460, training loss: 84.71123504638672 = 0.32467666268348694 + 10.0 * 8.438655853271484
Epoch 2460, val loss: 0.4797600507736206
Epoch 2470, training loss: 84.71613311767578 = 0.3234339952468872 + 10.0 * 8.43927001953125
Epoch 2470, val loss: 0.4799010157585144
Epoch 2480, training loss: 84.72622680664062 = 0.3221914768218994 + 10.0 * 8.440403938293457
Epoch 2480, val loss: 0.4801263213157654
Epoch 2490, training loss: 84.70587921142578 = 0.3209547996520996 + 10.0 * 8.438492774963379
Epoch 2490, val loss: 0.4804052412509918
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8026382546930492
0.8173585452437877
The final CL Acc:0.79401, 0.00627, The final GNN Acc:0.81680, 0.00055
raw graph: torch.Size([2, 88648])
add edge: torch.Size([2, 110678])
remove edge: torch.Size([2, 66788])
updated graph: torch.Size([2, 88818])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 106.92716979980469 = 1.1041712760925293 + 10.0 * 10.582300186157227
Epoch 0, val loss: 1.1030957698822021
Epoch 10, training loss: 106.91943359375 = 1.0987827777862549 + 10.0 * 10.582064628601074
Epoch 10, val loss: 1.0977481603622437
Epoch 20, training loss: 106.9027099609375 = 1.0931731462478638 + 10.0 * 10.580953598022461
Epoch 20, val loss: 1.0921909809112549
Epoch 30, training loss: 106.8440933227539 = 1.087195634841919 + 10.0 * 10.575689315795898
Epoch 30, val loss: 1.086246132850647
Epoch 40, training loss: 106.61715698242188 = 1.0806726217269897 + 10.0 * 10.553647994995117
Epoch 40, val loss: 1.079776406288147
Epoch 50, training loss: 105.9246597290039 = 1.0736981630325317 + 10.0 * 10.485095977783203
Epoch 50, val loss: 1.0728431940078735
Epoch 60, training loss: 104.33512115478516 = 1.0664445161819458 + 10.0 * 10.326868057250977
Epoch 60, val loss: 1.0657051801681519
Epoch 70, training loss: 101.8648910522461 = 1.0594905614852905 + 10.0 * 10.08053970336914
Epoch 70, val loss: 1.0587725639343262
Epoch 80, training loss: 99.55029296875 = 1.0530256032943726 + 10.0 * 9.849726676940918
Epoch 80, val loss: 1.052474856376648
Epoch 90, training loss: 97.8608627319336 = 1.0476279258728027 + 10.0 * 9.681323051452637
Epoch 90, val loss: 1.0473421812057495
Epoch 100, training loss: 95.7286376953125 = 1.0427480936050415 + 10.0 * 9.468588829040527
Epoch 100, val loss: 1.0426928997039795
Epoch 110, training loss: 93.80474853515625 = 1.038326621055603 + 10.0 * 9.276641845703125
Epoch 110, val loss: 1.0385007858276367
Epoch 120, training loss: 93.11640930175781 = 1.034271478652954 + 10.0 * 9.208213806152344
Epoch 120, val loss: 1.0345968008041382
Epoch 130, training loss: 92.6282958984375 = 1.0302586555480957 + 10.0 * 9.15980339050293
Epoch 130, val loss: 1.0307289361953735
Epoch 140, training loss: 91.85325622558594 = 1.0269062519073486 + 10.0 * 9.082634925842285
Epoch 140, val loss: 1.0275574922561646
Epoch 150, training loss: 90.87650299072266 = 1.0248891115188599 + 10.0 * 8.985161781311035
Epoch 150, val loss: 1.0256906747817993
Epoch 160, training loss: 89.99044036865234 = 1.0236972570419312 + 10.0 * 8.896674156188965
Epoch 160, val loss: 1.0245524644851685
Epoch 170, training loss: 89.26814270019531 = 1.0222477912902832 + 10.0 * 8.824589729309082
Epoch 170, val loss: 1.023045301437378
Epoch 180, training loss: 88.77232360839844 = 1.0197982788085938 + 10.0 * 8.775252342224121
Epoch 180, val loss: 1.0206482410430908
Epoch 190, training loss: 88.46046447753906 = 1.0164254903793335 + 10.0 * 8.744403839111328
Epoch 190, val loss: 1.0173890590667725
Epoch 200, training loss: 88.23432159423828 = 1.0127249956130981 + 10.0 * 8.722159385681152
Epoch 200, val loss: 1.0138144493103027
Epoch 210, training loss: 88.02335357666016 = 1.009191870689392 + 10.0 * 8.701416015625
Epoch 210, val loss: 1.010387659072876
Epoch 220, training loss: 87.83486938476562 = 1.0058143138885498 + 10.0 * 8.682905197143555
Epoch 220, val loss: 1.0070725679397583
Epoch 230, training loss: 87.67918395996094 = 1.00217604637146 + 10.0 * 8.66770076751709
Epoch 230, val loss: 1.0034780502319336
Epoch 240, training loss: 87.55425262451172 = 0.9980961680412292 + 10.0 * 8.65561580657959
Epoch 240, val loss: 0.999464213848114
Epoch 250, training loss: 87.4515609741211 = 0.9936262369155884 + 10.0 * 8.645792961120605
Epoch 250, val loss: 0.995069682598114
Epoch 260, training loss: 87.36158752441406 = 0.9887704849243164 + 10.0 * 8.63728141784668
Epoch 260, val loss: 0.9903236031532288
Epoch 270, training loss: 87.27693176269531 = 0.9836090803146362 + 10.0 * 8.629331588745117
Epoch 270, val loss: 0.9852665066719055
Epoch 280, training loss: 87.19022369384766 = 0.9782460331916809 + 10.0 * 8.621197700500488
Epoch 280, val loss: 0.9800492525100708
Epoch 290, training loss: 87.09686279296875 = 0.9726835489273071 + 10.0 * 8.612417221069336
Epoch 290, val loss: 0.9746143221855164
Epoch 300, training loss: 86.99999237060547 = 0.9668309688568115 + 10.0 * 8.603316307067871
Epoch 300, val loss: 0.9689078330993652
Epoch 310, training loss: 86.93643188476562 = 0.9607046842575073 + 10.0 * 8.597573280334473
Epoch 310, val loss: 0.9629737138748169
Epoch 320, training loss: 86.8090591430664 = 0.9543337821960449 + 10.0 * 8.58547306060791
Epoch 320, val loss: 0.9567403793334961
Epoch 330, training loss: 86.70392608642578 = 0.9477314352989197 + 10.0 * 8.5756196975708
Epoch 330, val loss: 0.950276255607605
Epoch 340, training loss: 86.6084213256836 = 0.9407781362533569 + 10.0 * 8.566763877868652
Epoch 340, val loss: 0.9434771537780762
Epoch 350, training loss: 86.52310180664062 = 0.9334263205528259 + 10.0 * 8.558967590332031
Epoch 350, val loss: 0.936259925365448
Epoch 360, training loss: 86.43595886230469 = 0.9256328344345093 + 10.0 * 8.551032066345215
Epoch 360, val loss: 0.9286158084869385
Epoch 370, training loss: 86.37001037597656 = 0.917428731918335 + 10.0 * 8.545258522033691
Epoch 370, val loss: 0.9205975532531738
Epoch 380, training loss: 86.28462219238281 = 0.9088472723960876 + 10.0 * 8.537577629089355
Epoch 380, val loss: 0.9122342467308044
Epoch 390, training loss: 86.21273040771484 = 0.8999637365341187 + 10.0 * 8.53127670288086
Epoch 390, val loss: 0.9035626649856567
Epoch 400, training loss: 86.1484375 = 0.8907496929168701 + 10.0 * 8.525769233703613
Epoch 400, val loss: 0.8945329189300537
Epoch 410, training loss: 86.09684753417969 = 0.8811720013618469 + 10.0 * 8.521567344665527
Epoch 410, val loss: 0.8851791620254517
Epoch 420, training loss: 86.02057647705078 = 0.8712758421897888 + 10.0 * 8.51492977142334
Epoch 420, val loss: 0.875595211982727
Epoch 430, training loss: 85.96060943603516 = 0.8612010478973389 + 10.0 * 8.509941101074219
Epoch 430, val loss: 0.8658014535903931
Epoch 440, training loss: 85.90399169921875 = 0.8508272767066956 + 10.0 * 8.505315780639648
Epoch 440, val loss: 0.8556843400001526
Epoch 450, training loss: 85.86329650878906 = 0.8401399254798889 + 10.0 * 8.502315521240234
Epoch 450, val loss: 0.8452839255332947
Epoch 460, training loss: 85.82955169677734 = 0.8291218280792236 + 10.0 * 8.500042915344238
Epoch 460, val loss: 0.8346368670463562
Epoch 470, training loss: 85.76527404785156 = 0.8179867267608643 + 10.0 * 8.494729042053223
Epoch 470, val loss: 0.8237801790237427
Epoch 480, training loss: 85.71785736083984 = 0.8067101836204529 + 10.0 * 8.491114616394043
Epoch 480, val loss: 0.812901496887207
Epoch 490, training loss: 85.6769027709961 = 0.7952978610992432 + 10.0 * 8.488161087036133
Epoch 490, val loss: 0.801869809627533
Epoch 500, training loss: 85.63671875 = 0.7837644815444946 + 10.0 * 8.485295295715332
Epoch 500, val loss: 0.7907746434211731
Epoch 510, training loss: 85.63832092285156 = 0.7721543312072754 + 10.0 * 8.486616134643555
Epoch 510, val loss: 0.7796233892440796
Epoch 520, training loss: 85.5768051147461 = 0.7605063319206238 + 10.0 * 8.481630325317383
Epoch 520, val loss: 0.7683775424957275
Epoch 530, training loss: 85.5295181274414 = 0.7489995956420898 + 10.0 * 8.478052139282227
Epoch 530, val loss: 0.7574160099029541
Epoch 540, training loss: 85.49568176269531 = 0.7376099824905396 + 10.0 * 8.475807189941406
Epoch 540, val loss: 0.7465064525604248
Epoch 550, training loss: 85.46155548095703 = 0.7262998223304749 + 10.0 * 8.473525047302246
Epoch 550, val loss: 0.7357128262519836
Epoch 560, training loss: 85.4582748413086 = 0.7151079177856445 + 10.0 * 8.474316596984863
Epoch 560, val loss: 0.7249840497970581
Epoch 570, training loss: 85.4066162109375 = 0.7039389610290527 + 10.0 * 8.470267295837402
Epoch 570, val loss: 0.7144728899002075
Epoch 580, training loss: 85.36870574951172 = 0.6931468844413757 + 10.0 * 8.46755599975586
Epoch 580, val loss: 0.7041776180267334
Epoch 590, training loss: 85.33677673339844 = 0.6825067400932312 + 10.0 * 8.465426445007324
Epoch 590, val loss: 0.6941238045692444
Epoch 600, training loss: 85.30577850341797 = 0.6721097826957703 + 10.0 * 8.463366508483887
Epoch 600, val loss: 0.6842703223228455
Epoch 610, training loss: 85.28060913085938 = 0.6619281768798828 + 10.0 * 8.461868286132812
Epoch 610, val loss: 0.6746240258216858
Epoch 620, training loss: 85.25598907470703 = 0.6518792510032654 + 10.0 * 8.460411071777344
Epoch 620, val loss: 0.6651358604431152
Epoch 630, training loss: 85.23629760742188 = 0.6421780586242676 + 10.0 * 8.45941162109375
Epoch 630, val loss: 0.6560168862342834
Epoch 640, training loss: 85.20500946044922 = 0.6329020261764526 + 10.0 * 8.457210540771484
Epoch 640, val loss: 0.6472853422164917
Epoch 650, training loss: 85.17669677734375 = 0.623856782913208 + 10.0 * 8.455284118652344
Epoch 650, val loss: 0.6387929916381836
Epoch 660, training loss: 85.16846466064453 = 0.6150767803192139 + 10.0 * 8.455339431762695
Epoch 660, val loss: 0.6305924654006958
Epoch 670, training loss: 85.13239288330078 = 0.6066186428070068 + 10.0 * 8.452577590942383
Epoch 670, val loss: 0.622643232345581
Epoch 680, training loss: 85.10678100585938 = 0.5984840989112854 + 10.0 * 8.45082950592041
Epoch 680, val loss: 0.6150460839271545
Epoch 690, training loss: 85.08489990234375 = 0.5906368494033813 + 10.0 * 8.449426651000977
Epoch 690, val loss: 0.6077272295951843
Epoch 700, training loss: 85.06669616699219 = 0.5830927491188049 + 10.0 * 8.448360443115234
Epoch 700, val loss: 0.6006864309310913
Epoch 710, training loss: 85.06536865234375 = 0.5757458806037903 + 10.0 * 8.448962211608887
Epoch 710, val loss: 0.593903660774231
Epoch 720, training loss: 85.02780151367188 = 0.5687310099601746 + 10.0 * 8.445906639099121
Epoch 720, val loss: 0.5874502658843994
Epoch 730, training loss: 85.00374603271484 = 0.5620618462562561 + 10.0 * 8.444168090820312
Epoch 730, val loss: 0.581251323223114
Epoch 740, training loss: 84.98475646972656 = 0.5556235909461975 + 10.0 * 8.442913055419922
Epoch 740, val loss: 0.5753153562545776
Epoch 750, training loss: 84.97335815429688 = 0.5494396090507507 + 10.0 * 8.442392349243164
Epoch 750, val loss: 0.5696186423301697
Epoch 760, training loss: 84.94843292236328 = 0.5434358716011047 + 10.0 * 8.440500259399414
Epoch 760, val loss: 0.564129650592804
Epoch 770, training loss: 84.92676544189453 = 0.5376929640769958 + 10.0 * 8.438907623291016
Epoch 770, val loss: 0.558890700340271
Epoch 780, training loss: 84.90937042236328 = 0.5321937799453735 + 10.0 * 8.43771743774414
Epoch 780, val loss: 0.5538656711578369
Epoch 790, training loss: 84.92911529541016 = 0.5268785357475281 + 10.0 * 8.440223693847656
Epoch 790, val loss: 0.5490192770957947
Epoch 800, training loss: 84.87647247314453 = 0.521763026714325 + 10.0 * 8.435470581054688
Epoch 800, val loss: 0.5443712472915649
Epoch 810, training loss: 84.86029815673828 = 0.5168730020523071 + 10.0 * 8.434342384338379
Epoch 810, val loss: 0.5399571657180786
Epoch 820, training loss: 84.84638977050781 = 0.512157678604126 + 10.0 * 8.433423042297363
Epoch 820, val loss: 0.5357075929641724
Epoch 830, training loss: 84.86795806884766 = 0.5075757503509521 + 10.0 * 8.43603801727295
Epoch 830, val loss: 0.5316017270088196
Epoch 840, training loss: 84.82551574707031 = 0.5031487345695496 + 10.0 * 8.432236671447754
Epoch 840, val loss: 0.5275818109512329
Epoch 850, training loss: 84.80061340332031 = 0.4988861382007599 + 10.0 * 8.43017292022705
Epoch 850, val loss: 0.5237882733345032
Epoch 860, training loss: 84.78392028808594 = 0.4947919249534607 + 10.0 * 8.428913116455078
Epoch 860, val loss: 0.5201106071472168
Epoch 870, training loss: 84.77249145507812 = 0.4907953143119812 + 10.0 * 8.428169250488281
Epoch 870, val loss: 0.516570508480072
Epoch 880, training loss: 84.76763153076172 = 0.4868907332420349 + 10.0 * 8.42807388305664
Epoch 880, val loss: 0.5130927562713623
Epoch 890, training loss: 84.74364471435547 = 0.48308250308036804 + 10.0 * 8.426055908203125
Epoch 890, val loss: 0.5097331404685974
Epoch 900, training loss: 84.72875213623047 = 0.479445219039917 + 10.0 * 8.424930572509766
Epoch 900, val loss: 0.5064937472343445
Epoch 910, training loss: 84.73334503173828 = 0.4758816063404083 + 10.0 * 8.425745964050293
Epoch 910, val loss: 0.5033622980117798
Epoch 920, training loss: 84.6997299194336 = 0.47239550948143005 + 10.0 * 8.422733306884766
Epoch 920, val loss: 0.5002873539924622
Epoch 930, training loss: 84.68645477294922 = 0.46905016899108887 + 10.0 * 8.421740531921387
Epoch 930, val loss: 0.4973372519016266
Epoch 940, training loss: 84.6738052368164 = 0.4657837152481079 + 10.0 * 8.420802116394043
Epoch 940, val loss: 0.49447163939476013
Epoch 950, training loss: 84.66114807128906 = 0.46259748935699463 + 10.0 * 8.419855117797852
Epoch 950, val loss: 0.49168625473976135
Epoch 960, training loss: 84.68307495117188 = 0.45945802330970764 + 10.0 * 8.422361373901367
Epoch 960, val loss: 0.4889594316482544
Epoch 970, training loss: 84.6417007446289 = 0.4563971161842346 + 10.0 * 8.418530464172363
Epoch 970, val loss: 0.48627835512161255
Epoch 980, training loss: 84.62201690673828 = 0.45346716046333313 + 10.0 * 8.416854858398438
Epoch 980, val loss: 0.48373669385910034
Epoch 990, training loss: 84.6102523803711 = 0.4506186544895172 + 10.0 * 8.415964126586914
Epoch 990, val loss: 0.48125529289245605
Epoch 1000, training loss: 84.59759521484375 = 0.4478262960910797 + 10.0 * 8.414977073669434
Epoch 1000, val loss: 0.4788505733013153
Epoch 1010, training loss: 84.63849639892578 = 0.4450741112232208 + 10.0 * 8.419342041015625
Epoch 1010, val loss: 0.47649914026260376
Epoch 1020, training loss: 84.58891296386719 = 0.44237756729125977 + 10.0 * 8.414653778076172
Epoch 1020, val loss: 0.47414758801460266
Epoch 1030, training loss: 84.56253814697266 = 0.439771443605423 + 10.0 * 8.412276268005371
Epoch 1030, val loss: 0.47190621495246887
Epoch 1040, training loss: 84.55155944824219 = 0.43722429871559143 + 10.0 * 8.411433219909668
Epoch 1040, val loss: 0.469739705324173
Epoch 1050, training loss: 84.54157257080078 = 0.4347304105758667 + 10.0 * 8.410684585571289
Epoch 1050, val loss: 0.4676125943660736
Epoch 1060, training loss: 84.55435180664062 = 0.43227168917655945 + 10.0 * 8.41220760345459
Epoch 1060, val loss: 0.4655367434024811
Epoch 1070, training loss: 84.53107452392578 = 0.429831326007843 + 10.0 * 8.410123825073242
Epoch 1070, val loss: 0.4634730815887451
Epoch 1080, training loss: 84.51307678222656 = 0.4274781048297882 + 10.0 * 8.408559799194336
Epoch 1080, val loss: 0.4614703953266144
Epoch 1090, training loss: 84.50332641601562 = 0.4251713752746582 + 10.0 * 8.407815933227539
Epoch 1090, val loss: 0.4595471918582916
Epoch 1100, training loss: 84.49373626708984 = 0.4229050278663635 + 10.0 * 8.407083511352539
Epoch 1100, val loss: 0.45766356587409973
Epoch 1110, training loss: 84.48774719238281 = 0.42068544030189514 + 10.0 * 8.406705856323242
Epoch 1110, val loss: 0.455779492855072
Epoch 1120, training loss: 84.51380920410156 = 0.4184929132461548 + 10.0 * 8.409531593322754
Epoch 1120, val loss: 0.45394912362098694
Epoch 1130, training loss: 84.47795867919922 = 0.41632720828056335 + 10.0 * 8.406163215637207
Epoch 1130, val loss: 0.4521855413913727
Epoch 1140, training loss: 84.4609146118164 = 0.41422414779663086 + 10.0 * 8.404668807983398
Epoch 1140, val loss: 0.45041877031326294
Epoch 1150, training loss: 84.45360565185547 = 0.41216492652893066 + 10.0 * 8.404144287109375
Epoch 1150, val loss: 0.44872796535491943
Epoch 1160, training loss: 84.4488754272461 = 0.4101461172103882 + 10.0 * 8.4038724899292
Epoch 1160, val loss: 0.44705796241760254
Epoch 1170, training loss: 84.43827056884766 = 0.40814778208732605 + 10.0 * 8.4030122756958
Epoch 1170, val loss: 0.44540488719940186
Epoch 1180, training loss: 84.48754119873047 = 0.4061788022518158 + 10.0 * 8.408136367797852
Epoch 1180, val loss: 0.4437827169895172
Epoch 1190, training loss: 84.4292221069336 = 0.40422120690345764 + 10.0 * 8.40250015258789
Epoch 1190, val loss: 0.4422203302383423
Epoch 1200, training loss: 84.41475677490234 = 0.4023434519767761 + 10.0 * 8.401241302490234
Epoch 1200, val loss: 0.4406483471393585
Epoch 1210, training loss: 84.40674591064453 = 0.4004893898963928 + 10.0 * 8.400625228881836
Epoch 1210, val loss: 0.43916410207748413
Epoch 1220, training loss: 84.39785766601562 = 0.3986709415912628 + 10.0 * 8.399918556213379
Epoch 1220, val loss: 0.43768739700317383
Epoch 1230, training loss: 84.39048767089844 = 0.39687660336494446 + 10.0 * 8.399361610412598
Epoch 1230, val loss: 0.4362410306930542
Epoch 1240, training loss: 84.396484375 = 0.3951004445552826 + 10.0 * 8.400137901306152
Epoch 1240, val loss: 0.43483766913414
Epoch 1250, training loss: 84.39093780517578 = 0.3933246433734894 + 10.0 * 8.399761199951172
Epoch 1250, val loss: 0.43341246247291565
Epoch 1260, training loss: 84.38052368164062 = 0.39160457253456116 + 10.0 * 8.398892402648926
Epoch 1260, val loss: 0.432050883769989
Epoch 1270, training loss: 84.36454010009766 = 0.38992220163345337 + 10.0 * 8.397461891174316
Epoch 1270, val loss: 0.43072789907455444
Epoch 1280, training loss: 84.35798645019531 = 0.38827207684516907 + 10.0 * 8.396970748901367
Epoch 1280, val loss: 0.42944303154945374
Epoch 1290, training loss: 84.35073852539062 = 0.38664737343788147 + 10.0 * 8.396409034729004
Epoch 1290, val loss: 0.4281714856624603
Epoch 1300, training loss: 84.34491729736328 = 0.3850420117378235 + 10.0 * 8.395987510681152
Epoch 1300, val loss: 0.4269406199455261
Epoch 1310, training loss: 84.38021850585938 = 0.383451908826828 + 10.0 * 8.399676322937012
Epoch 1310, val loss: 0.42572829127311707
Epoch 1320, training loss: 84.36395263671875 = 0.38186413049697876 + 10.0 * 8.398208618164062
Epoch 1320, val loss: 0.42451488971710205
Epoch 1330, training loss: 84.33720397949219 = 0.3803243637084961 + 10.0 * 8.3956880569458
Epoch 1330, val loss: 0.4233226776123047
Epoch 1340, training loss: 84.32183074951172 = 0.3788257837295532 + 10.0 * 8.39430046081543
Epoch 1340, val loss: 0.4221820533275604
Epoch 1350, training loss: 84.31644439697266 = 0.3773545026779175 + 10.0 * 8.393908500671387
Epoch 1350, val loss: 0.42110806703567505
Epoch 1360, training loss: 84.31073760986328 = 0.3759082555770874 + 10.0 * 8.39348316192627
Epoch 1360, val loss: 0.4200022518634796
Epoch 1370, training loss: 84.30472564697266 = 0.3744770586490631 + 10.0 * 8.393025398254395
Epoch 1370, val loss: 0.4189726412296295
Epoch 1380, training loss: 84.2993392944336 = 0.37305909395217896 + 10.0 * 8.392627716064453
Epoch 1380, val loss: 0.41791340708732605
Epoch 1390, training loss: 84.29421997070312 = 0.3716565668582916 + 10.0 * 8.392255783081055
Epoch 1390, val loss: 0.41689223051071167
Epoch 1400, training loss: 84.32211303710938 = 0.37027013301849365 + 10.0 * 8.395184516906738
Epoch 1400, val loss: 0.41589096188545227
Epoch 1410, training loss: 84.3046875 = 0.3688790500164032 + 10.0 * 8.393580436706543
Epoch 1410, val loss: 0.41489383578300476
Epoch 1420, training loss: 84.28772735595703 = 0.36753398180007935 + 10.0 * 8.392019271850586
Epoch 1420, val loss: 0.41390690207481384
Epoch 1430, training loss: 84.27783966064453 = 0.36620989441871643 + 10.0 * 8.391162872314453
Epoch 1430, val loss: 0.4129810929298401
Epoch 1440, training loss: 84.27983856201172 = 0.3649062514305115 + 10.0 * 8.39149284362793
Epoch 1440, val loss: 0.41206836700439453
Epoch 1450, training loss: 84.27066040039062 = 0.36361411213874817 + 10.0 * 8.390704154968262
Epoch 1450, val loss: 0.4111681878566742
Epoch 1460, training loss: 84.25990295410156 = 0.36234402656555176 + 10.0 * 8.389756202697754
Epoch 1460, val loss: 0.41029906272888184
Epoch 1470, training loss: 84.25871276855469 = 0.3610928952693939 + 10.0 * 8.389761924743652
Epoch 1470, val loss: 0.40942272543907166
Epoch 1480, training loss: 84.25215911865234 = 0.35985612869262695 + 10.0 * 8.389230728149414
Epoch 1480, val loss: 0.40857964754104614
Epoch 1490, training loss: 84.247802734375 = 0.35863634943962097 + 10.0 * 8.388916969299316
Epoch 1490, val loss: 0.4077402651309967
Epoch 1500, training loss: 84.25102233886719 = 0.35742923617362976 + 10.0 * 8.389359474182129
Epoch 1500, val loss: 0.4069342613220215
Epoch 1510, training loss: 84.23360443115234 = 0.3562363088130951 + 10.0 * 8.387736320495605
Epoch 1510, val loss: 0.40612298250198364
Epoch 1520, training loss: 84.24388122558594 = 0.35507112741470337 + 10.0 * 8.388880729675293
Epoch 1520, val loss: 0.40532320737838745
Epoch 1530, training loss: 84.23240661621094 = 0.35389959812164307 + 10.0 * 8.387850761413574
Epoch 1530, val loss: 0.40456485748291016
Epoch 1540, training loss: 84.22574615478516 = 0.35277000069618225 + 10.0 * 8.387297630310059
Epoch 1540, val loss: 0.4038222134113312
Epoch 1550, training loss: 84.21451568603516 = 0.351644903421402 + 10.0 * 8.386286735534668
Epoch 1550, val loss: 0.40308380126953125
Epoch 1560, training loss: 84.20950317382812 = 0.35053884983062744 + 10.0 * 8.385896682739258
Epoch 1560, val loss: 0.4023718535900116
Epoch 1570, training loss: 84.21893310546875 = 0.3494417667388916 + 10.0 * 8.38694953918457
Epoch 1570, val loss: 0.4016777575016022
Epoch 1580, training loss: 84.20787048339844 = 0.3483539819717407 + 10.0 * 8.38595199584961
Epoch 1580, val loss: 0.4009813666343689
Epoch 1590, training loss: 84.19054412841797 = 0.34728214144706726 + 10.0 * 8.384325981140137
Epoch 1590, val loss: 0.40028345584869385
Epoch 1600, training loss: 84.1882095336914 = 0.34622836112976074 + 10.0 * 8.384198188781738
Epoch 1600, val loss: 0.39961889386177063
Epoch 1610, training loss: 84.18657684326172 = 0.3451879322528839 + 10.0 * 8.384139060974121
Epoch 1610, val loss: 0.3989628851413727
Epoch 1620, training loss: 84.20692443847656 = 0.34415164589881897 + 10.0 * 8.386277198791504
Epoch 1620, val loss: 0.39832577109336853
Epoch 1630, training loss: 84.17455291748047 = 0.34311985969543457 + 10.0 * 8.383143424987793
Epoch 1630, val loss: 0.39770570397377014
Epoch 1640, training loss: 84.16427612304688 = 0.34211355447769165 + 10.0 * 8.38221549987793
Epoch 1640, val loss: 0.3970632553100586
Epoch 1650, training loss: 84.16290283203125 = 0.3411179184913635 + 10.0 * 8.38217830657959
Epoch 1650, val loss: 0.39647096395492554
Epoch 1660, training loss: 84.16670227050781 = 0.3401328921318054 + 10.0 * 8.382657051086426
Epoch 1660, val loss: 0.3958800733089447
Epoch 1670, training loss: 84.15617370605469 = 0.33914944529533386 + 10.0 * 8.381702423095703
Epoch 1670, val loss: 0.39531058073043823
Epoch 1680, training loss: 84.15412902832031 = 0.33818235993385315 + 10.0 * 8.38159465789795
Epoch 1680, val loss: 0.3947126865386963
Epoch 1690, training loss: 84.14708709716797 = 0.3372311294078827 + 10.0 * 8.380985260009766
Epoch 1690, val loss: 0.39415135979652405
Epoch 1700, training loss: 84.14452362060547 = 0.3362925946712494 + 10.0 * 8.380823135375977
Epoch 1700, val loss: 0.39360401034355164
Epoch 1710, training loss: 84.1520004272461 = 0.33535781502723694 + 10.0 * 8.381664276123047
Epoch 1710, val loss: 0.3930703103542328
Epoch 1720, training loss: 84.1280288696289 = 0.33442938327789307 + 10.0 * 8.37936019897461
Epoch 1720, val loss: 0.39250364899635315
Epoch 1730, training loss: 84.12789154052734 = 0.3335210084915161 + 10.0 * 8.379437446594238
Epoch 1730, val loss: 0.39196744561195374
Epoch 1740, training loss: 84.11839294433594 = 0.3326134979724884 + 10.0 * 8.378578186035156
Epoch 1740, val loss: 0.39146849513053894
Epoch 1750, training loss: 84.11746215820312 = 0.3317181169986725 + 10.0 * 8.37857437133789
Epoch 1750, val loss: 0.39095115661621094
Epoch 1760, training loss: 84.13702392578125 = 0.3308248519897461 + 10.0 * 8.380620002746582
Epoch 1760, val loss: 0.39045196771621704
Epoch 1770, training loss: 84.11096954345703 = 0.329935222864151 + 10.0 * 8.378103256225586
Epoch 1770, val loss: 0.3899719715118408
Epoch 1780, training loss: 84.10541534423828 = 0.32906222343444824 + 10.0 * 8.37763500213623
Epoch 1780, val loss: 0.38947534561157227
Epoch 1790, training loss: 84.09854125976562 = 0.32819992303848267 + 10.0 * 8.377034187316895
Epoch 1790, val loss: 0.38902705907821655
Epoch 1800, training loss: 84.1050796508789 = 0.32734060287475586 + 10.0 * 8.377774238586426
Epoch 1800, val loss: 0.38856402039527893
Epoch 1810, training loss: 84.10614776611328 = 0.3264777660369873 + 10.0 * 8.37796688079834
Epoch 1810, val loss: 0.3881103992462158
Epoch 1820, training loss: 84.0868911743164 = 0.32562872767448425 + 10.0 * 8.376126289367676
Epoch 1820, val loss: 0.3876420855522156
Epoch 1830, training loss: 84.08296966552734 = 0.3247925341129303 + 10.0 * 8.375818252563477
Epoch 1830, val loss: 0.3872309923171997
Epoch 1840, training loss: 84.07975769042969 = 0.3239600956439972 + 10.0 * 8.375579833984375
Epoch 1840, val loss: 0.38680410385131836
Epoch 1850, training loss: 84.12104797363281 = 0.32313552498817444 + 10.0 * 8.379791259765625
Epoch 1850, val loss: 0.38642191886901855
Epoch 1860, training loss: 84.08162689208984 = 0.3223000466823578 + 10.0 * 8.375932693481445
Epoch 1860, val loss: 0.38597193360328674
Epoch 1870, training loss: 84.0679931640625 = 0.32149016857147217 + 10.0 * 8.374650955200195
Epoch 1870, val loss: 0.38556525111198425
Epoch 1880, training loss: 84.06098937988281 = 0.3206859230995178 + 10.0 * 8.374030113220215
Epoch 1880, val loss: 0.38518205285072327
Epoch 1890, training loss: 84.05654907226562 = 0.3198852837085724 + 10.0 * 8.373666763305664
Epoch 1890, val loss: 0.38479793071746826
Epoch 1900, training loss: 84.06218719482422 = 0.3190893828868866 + 10.0 * 8.374309539794922
Epoch 1900, val loss: 0.3844016492366791
Epoch 1910, training loss: 84.07842254638672 = 0.3182898461818695 + 10.0 * 8.376012802124023
Epoch 1910, val loss: 0.38404810428619385
Epoch 1920, training loss: 84.0505142211914 = 0.3174949288368225 + 10.0 * 8.37330150604248
Epoch 1920, val loss: 0.3836638629436493
Epoch 1930, training loss: 84.04818725585938 = 0.31672635674476624 + 10.0 * 8.373146057128906
Epoch 1930, val loss: 0.38330328464508057
Epoch 1940, training loss: 84.0395278930664 = 0.31596288084983826 + 10.0 * 8.372356414794922
Epoch 1940, val loss: 0.38298144936561584
Epoch 1950, training loss: 84.03487396240234 = 0.31520792841911316 + 10.0 * 8.371966361999512
Epoch 1950, val loss: 0.3826121389865875
Epoch 1960, training loss: 84.03134155273438 = 0.3144530653953552 + 10.0 * 8.371688842773438
Epoch 1960, val loss: 0.3822956383228302
Epoch 1970, training loss: 84.02867889404297 = 0.31370043754577637 + 10.0 * 8.371498107910156
Epoch 1970, val loss: 0.38194364309310913
Epoch 1980, training loss: 84.04798126220703 = 0.312947154045105 + 10.0 * 8.373502731323242
Epoch 1980, val loss: 0.3816138803958893
Epoch 1990, training loss: 84.03175354003906 = 0.31220364570617676 + 10.0 * 8.371954917907715
Epoch 1990, val loss: 0.38130810856819153
Epoch 2000, training loss: 84.08446502685547 = 0.3114592432975769 + 10.0 * 8.377300262451172
Epoch 2000, val loss: 0.38097435235977173
Epoch 2010, training loss: 84.0323715209961 = 0.3107220530509949 + 10.0 * 8.372164726257324
Epoch 2010, val loss: 0.38067540526390076
Epoch 2020, training loss: 84.0190200805664 = 0.31000176072120667 + 10.0 * 8.370901107788086
Epoch 2020, val loss: 0.38034605979919434
Epoch 2030, training loss: 84.01018524169922 = 0.3092844784259796 + 10.0 * 8.370089530944824
Epoch 2030, val loss: 0.38007745146751404
Epoch 2040, training loss: 84.0056381225586 = 0.308575302362442 + 10.0 * 8.369706153869629
Epoch 2040, val loss: 0.37976786494255066
Epoch 2050, training loss: 84.00200653076172 = 0.30786803364753723 + 10.0 * 8.369413375854492
Epoch 2050, val loss: 0.3795014023780823
Epoch 2060, training loss: 83.99886322021484 = 0.30716243386268616 + 10.0 * 8.369170188903809
Epoch 2060, val loss: 0.379226952791214
Epoch 2070, training loss: 83.99699401855469 = 0.3064610958099365 + 10.0 * 8.369053840637207
Epoch 2070, val loss: 0.37895745038986206
Epoch 2080, training loss: 84.03035736083984 = 0.3057597875595093 + 10.0 * 8.372459411621094
Epoch 2080, val loss: 0.3786911368370056
Epoch 2090, training loss: 83.9930648803711 = 0.30505675077438354 + 10.0 * 8.36880111694336
Epoch 2090, val loss: 0.3784683346748352
Epoch 2100, training loss: 83.99102783203125 = 0.30435892939567566 + 10.0 * 8.368666648864746
Epoch 2100, val loss: 0.37816283106803894
Epoch 2110, training loss: 83.98660278320312 = 0.3036755323410034 + 10.0 * 8.368292808532715
Epoch 2110, val loss: 0.3779737055301666
Epoch 2120, training loss: 83.98210144042969 = 0.3029935359954834 + 10.0 * 8.367910385131836
Epoch 2120, val loss: 0.3777022361755371
Epoch 2130, training loss: 83.98326873779297 = 0.302316814661026 + 10.0 * 8.368095397949219
Epoch 2130, val loss: 0.3774968385696411
Epoch 2140, training loss: 83.99958801269531 = 0.30163899064064026 + 10.0 * 8.369794845581055
Epoch 2140, val loss: 0.37727028131484985
Epoch 2150, training loss: 83.9784927368164 = 0.3009631335735321 + 10.0 * 8.367753028869629
Epoch 2150, val loss: 0.37704089283943176
Epoch 2160, training loss: 83.97284698486328 = 0.3002970516681671 + 10.0 * 8.367254257202148
Epoch 2160, val loss: 0.3768378496170044
Epoch 2170, training loss: 83.96772766113281 = 0.29963770508766174 + 10.0 * 8.366808891296387
Epoch 2170, val loss: 0.3766249418258667
Epoch 2180, training loss: 83.96483612060547 = 0.29898110032081604 + 10.0 * 8.366585731506348
Epoch 2180, val loss: 0.3764148950576782
Epoch 2190, training loss: 83.96158599853516 = 0.29832568764686584 + 10.0 * 8.366326332092285
Epoch 2190, val loss: 0.37621769309043884
Epoch 2200, training loss: 83.96066284179688 = 0.2976728677749634 + 10.0 * 8.36629867553711
Epoch 2200, val loss: 0.37601765990257263
Epoch 2210, training loss: 84.04190063476562 = 0.2970234751701355 + 10.0 * 8.37448787689209
Epoch 2210, val loss: 0.3758581876754761
Epoch 2220, training loss: 83.97612762451172 = 0.2963571846485138 + 10.0 * 8.367977142333984
Epoch 2220, val loss: 0.37566661834716797
Epoch 2230, training loss: 83.95894622802734 = 0.29571136832237244 + 10.0 * 8.366323471069336
Epoch 2230, val loss: 0.3754168748855591
Epoch 2240, training loss: 83.95030975341797 = 0.29507890343666077 + 10.0 * 8.365522384643555
Epoch 2240, val loss: 0.37531358003616333
Epoch 2250, training loss: 83.9467544555664 = 0.29445064067840576 + 10.0 * 8.365230560302734
Epoch 2250, val loss: 0.3751163184642792
Epoch 2260, training loss: 83.9439468383789 = 0.29382550716400146 + 10.0 * 8.365012168884277
Epoch 2260, val loss: 0.3749762773513794
Epoch 2270, training loss: 83.94112396240234 = 0.2932005822658539 + 10.0 * 8.364792823791504
Epoch 2270, val loss: 0.37480348348617554
Epoch 2280, training loss: 83.93853759765625 = 0.2925727665424347 + 10.0 * 8.364596366882324
Epoch 2280, val loss: 0.3746543526649475
Epoch 2290, training loss: 83.93603515625 = 0.2919435501098633 + 10.0 * 8.364408493041992
Epoch 2290, val loss: 0.37449726462364197
Epoch 2300, training loss: 83.9347152709961 = 0.2913142144680023 + 10.0 * 8.364339828491211
Epoch 2300, val loss: 0.3743341565132141
Epoch 2310, training loss: 83.97660827636719 = 0.2906884253025055 + 10.0 * 8.368592262268066
Epoch 2310, val loss: 0.3741849660873413
Epoch 2320, training loss: 83.94168853759766 = 0.29006052017211914 + 10.0 * 8.36516284942627
Epoch 2320, val loss: 0.3740670084953308
Epoch 2330, training loss: 83.93109130859375 = 0.2894362509250641 + 10.0 * 8.364165306091309
Epoch 2330, val loss: 0.3738628029823303
Epoch 2340, training loss: 83.92681121826172 = 0.28882524371147156 + 10.0 * 8.363798141479492
Epoch 2340, val loss: 0.37376347184181213
Epoch 2350, training loss: 83.92151641845703 = 0.28821468353271484 + 10.0 * 8.363329887390137
Epoch 2350, val loss: 0.3736000955104828
Epoch 2360, training loss: 83.91939544677734 = 0.28760671615600586 + 10.0 * 8.363179206848145
Epoch 2360, val loss: 0.37347352504730225
Epoch 2370, training loss: 83.91986083984375 = 0.28699901700019836 + 10.0 * 8.363286018371582
Epoch 2370, val loss: 0.3733426034450531
Epoch 2380, training loss: 83.957763671875 = 0.2863906919956207 + 10.0 * 8.36713695526123
Epoch 2380, val loss: 0.37322500348091125
Epoch 2390, training loss: 83.91771697998047 = 0.28578200936317444 + 10.0 * 8.36319351196289
Epoch 2390, val loss: 0.3731110692024231
Epoch 2400, training loss: 83.9116439819336 = 0.285176545381546 + 10.0 * 8.362646102905273
Epoch 2400, val loss: 0.3729960024356842
Epoch 2410, training loss: 83.91266632080078 = 0.2845785319805145 + 10.0 * 8.362809181213379
Epoch 2410, val loss: 0.37285974621772766
Epoch 2420, training loss: 83.92239379882812 = 0.28398266434669495 + 10.0 * 8.36384105682373
Epoch 2420, val loss: 0.3728005290031433
Epoch 2430, training loss: 83.90803527832031 = 0.2833876609802246 + 10.0 * 8.362464904785156
Epoch 2430, val loss: 0.37267640233039856
Epoch 2440, training loss: 83.90323638916016 = 0.28279268741607666 + 10.0 * 8.362044334411621
Epoch 2440, val loss: 0.37259751558303833
Epoch 2450, training loss: 83.9013671875 = 0.2822016775608063 + 10.0 * 8.361916542053223
Epoch 2450, val loss: 0.37249431014060974
Epoch 2460, training loss: 83.91194915771484 = 0.2816097140312195 + 10.0 * 8.36303424835205
Epoch 2460, val loss: 0.3724009096622467
Epoch 2470, training loss: 83.90381622314453 = 0.28101664781570435 + 10.0 * 8.362279891967773
Epoch 2470, val loss: 0.3723054826259613
Epoch 2480, training loss: 83.89415740966797 = 0.2804241180419922 + 10.0 * 8.361372947692871
Epoch 2480, val loss: 0.37218737602233887
Epoch 2490, training loss: 83.8916244506836 = 0.279835969209671 + 10.0 * 8.361178398132324
Epoch 2490, val loss: 0.3720981478691101
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.854388635210553
0.8629283489096574
=== training gcn model ===
Epoch 0, training loss: 106.9138412475586 = 1.090963363647461 + 10.0 * 10.582287788391113
Epoch 0, val loss: 1.0918339490890503
Epoch 10, training loss: 106.9069595336914 = 1.0868456363677979 + 10.0 * 10.582011222839355
Epoch 10, val loss: 1.0877418518066406
Epoch 20, training loss: 106.89138793945312 = 1.0826411247253418 + 10.0 * 10.5808744430542
Epoch 20, val loss: 1.083562970161438
Epoch 30, training loss: 106.83833312988281 = 1.078252911567688 + 10.0 * 10.576007843017578
Epoch 30, val loss: 1.0791980028152466
Epoch 40, training loss: 106.63683319091797 = 1.0736268758773804 + 10.0 * 10.556321144104004
Epoch 40, val loss: 1.0745644569396973
Epoch 50, training loss: 105.974853515625 = 1.0686088800430298 + 10.0 * 10.49062442779541
Epoch 50, val loss: 1.0694996118545532
Epoch 60, training loss: 104.23895263671875 = 1.0630536079406738 + 10.0 * 10.31758975982666
Epoch 60, val loss: 1.0638974905014038
Epoch 70, training loss: 100.70279693603516 = 1.0567280054092407 + 10.0 * 9.964607238769531
Epoch 70, val loss: 1.057477593421936
Epoch 80, training loss: 96.68350219726562 = 1.0500656366348267 + 10.0 * 9.56334400177002
Epoch 80, val loss: 1.0510919094085693
Epoch 90, training loss: 95.0732650756836 = 1.045013666152954 + 10.0 * 9.402825355529785
Epoch 90, val loss: 1.0462747812271118
Epoch 100, training loss: 93.56978607177734 = 1.0401732921600342 + 10.0 * 9.252961158752441
Epoch 100, val loss: 1.0415375232696533
Epoch 110, training loss: 92.87120819091797 = 1.0353922843933105 + 10.0 * 9.183581352233887
Epoch 110, val loss: 1.0367718935012817
Epoch 120, training loss: 92.2920150756836 = 1.0310758352279663 + 10.0 * 9.126093864440918
Epoch 120, val loss: 1.0324816703796387
Epoch 130, training loss: 91.59415435791016 = 1.0275019407272339 + 10.0 * 9.056665420532227
Epoch 130, val loss: 1.0289753675460815
Epoch 140, training loss: 90.99066162109375 = 1.0244003534317017 + 10.0 * 8.996625900268555
Epoch 140, val loss: 1.0259298086166382
Epoch 150, training loss: 90.59310913085938 = 1.0212645530700684 + 10.0 * 8.957184791564941
Epoch 150, val loss: 1.0228525400161743
Epoch 160, training loss: 90.15281677246094 = 1.0177220106124878 + 10.0 * 8.913509368896484
Epoch 160, val loss: 1.0194135904312134
Epoch 170, training loss: 89.62266540527344 = 1.0142173767089844 + 10.0 * 8.860844612121582
Epoch 170, val loss: 1.0161049365997314
Epoch 180, training loss: 89.24362182617188 = 1.0108835697174072 + 10.0 * 8.823273658752441
Epoch 180, val loss: 1.0128865242004395
Epoch 190, training loss: 88.90940856933594 = 1.0068179368972778 + 10.0 * 8.790258407592773
Epoch 190, val loss: 1.0089211463928223
Epoch 200, training loss: 88.6449966430664 = 1.0021368265151978 + 10.0 * 8.764286041259766
Epoch 200, val loss: 1.0044258832931519
Epoch 210, training loss: 88.49724578857422 = 0.9973207712173462 + 10.0 * 8.749992370605469
Epoch 210, val loss: 0.9998865127563477
Epoch 220, training loss: 88.34971618652344 = 0.9924737811088562 + 10.0 * 8.735723495483398
Epoch 220, val loss: 0.9953210949897766
Epoch 230, training loss: 88.18269348144531 = 0.9875941872596741 + 10.0 * 8.719510078430176
Epoch 230, val loss: 0.9907812476158142
Epoch 240, training loss: 87.99363708496094 = 0.9827413558959961 + 10.0 * 8.701089859008789
Epoch 240, val loss: 0.9863100051879883
Epoch 250, training loss: 87.81424713134766 = 0.9779127240180969 + 10.0 * 8.683633804321289
Epoch 250, val loss: 0.9818620681762695
Epoch 260, training loss: 87.63768768310547 = 0.9729386568069458 + 10.0 * 8.666475296020508
Epoch 260, val loss: 0.9772815704345703
Epoch 270, training loss: 87.48108673095703 = 0.9677230715751648 + 10.0 * 8.651336669921875
Epoch 270, val loss: 0.9724883437156677
Epoch 280, training loss: 87.33109283447266 = 0.962302565574646 + 10.0 * 8.636878967285156
Epoch 280, val loss: 0.9675326347351074
Epoch 290, training loss: 87.13334655761719 = 0.9566718935966492 + 10.0 * 8.617667198181152
Epoch 290, val loss: 0.9623532891273499
Epoch 300, training loss: 87.00112915039062 = 0.9506950974464417 + 10.0 * 8.605043411254883
Epoch 300, val loss: 0.9568836688995361
Epoch 310, training loss: 86.83351135253906 = 0.9441497325897217 + 10.0 * 8.588935852050781
Epoch 310, val loss: 0.9508166313171387
Epoch 320, training loss: 86.70671844482422 = 0.9370213747024536 + 10.0 * 8.576970100402832
Epoch 320, val loss: 0.9442078471183777
Epoch 330, training loss: 86.6031494140625 = 0.9294069409370422 + 10.0 * 8.567374229431152
Epoch 330, val loss: 0.9371597170829773
Epoch 340, training loss: 86.51832580566406 = 0.9213254451751709 + 10.0 * 8.559700012207031
Epoch 340, val loss: 0.9297404289245605
Epoch 350, training loss: 86.43418884277344 = 0.9129118919372559 + 10.0 * 8.552127838134766
Epoch 350, val loss: 0.9219789505004883
Epoch 360, training loss: 86.3563461303711 = 0.9042131900787354 + 10.0 * 8.54521369934082
Epoch 360, val loss: 0.9139842987060547
Epoch 370, training loss: 86.2857437133789 = 0.8952317833900452 + 10.0 * 8.539051055908203
Epoch 370, val loss: 0.9057222008705139
Epoch 380, training loss: 86.22174835205078 = 0.8859698176383972 + 10.0 * 8.533577919006348
Epoch 380, val loss: 0.8972265124320984
Epoch 390, training loss: 86.1798324584961 = 0.8765038251876831 + 10.0 * 8.530332565307617
Epoch 390, val loss: 0.8885431885719299
Epoch 400, training loss: 86.11681365966797 = 0.8669333457946777 + 10.0 * 8.524988174438477
Epoch 400, val loss: 0.879749596118927
Epoch 410, training loss: 86.06918334960938 = 0.8571762442588806 + 10.0 * 8.521200180053711
Epoch 410, val loss: 0.8708098530769348
Epoch 420, training loss: 86.02598571777344 = 0.8472862839698792 + 10.0 * 8.51786994934082
Epoch 420, val loss: 0.8617586493492126
Epoch 430, training loss: 85.98433685302734 = 0.8372735977172852 + 10.0 * 8.5147066116333
Epoch 430, val loss: 0.8525959253311157
Epoch 440, training loss: 85.94219207763672 = 0.8272132277488708 + 10.0 * 8.51149845123291
Epoch 440, val loss: 0.8433979153633118
Epoch 450, training loss: 85.89973449707031 = 0.8171531558036804 + 10.0 * 8.508257865905762
Epoch 450, val loss: 0.8342094421386719
Epoch 460, training loss: 85.868408203125 = 0.8071014285087585 + 10.0 * 8.506131172180176
Epoch 460, val loss: 0.8250377774238586
Epoch 470, training loss: 85.81586456298828 = 0.7971387505531311 + 10.0 * 8.501872062683105
Epoch 470, val loss: 0.8159448504447937
Epoch 480, training loss: 85.77680206298828 = 0.7873010039329529 + 10.0 * 8.498950004577637
Epoch 480, val loss: 0.8069861531257629
Epoch 490, training loss: 85.73368072509766 = 0.7775206565856934 + 10.0 * 8.49561595916748
Epoch 490, val loss: 0.79808509349823
Epoch 500, training loss: 85.7244644165039 = 0.7678345441818237 + 10.0 * 8.495662689208984
Epoch 500, val loss: 0.7892327904701233
Epoch 510, training loss: 85.6705093383789 = 0.7581428289413452 + 10.0 * 8.491236686706543
Epoch 510, val loss: 0.780427098274231
Epoch 520, training loss: 85.625244140625 = 0.7486574649810791 + 10.0 * 8.487658500671387
Epoch 520, val loss: 0.7718042731285095
Epoch 530, training loss: 85.59635925292969 = 0.739352822303772 + 10.0 * 8.485700607299805
Epoch 530, val loss: 0.7633498907089233
Epoch 540, training loss: 85.54776763916016 = 0.7301994562149048 + 10.0 * 8.481756210327148
Epoch 540, val loss: 0.7550732493400574
Epoch 550, training loss: 85.50788116455078 = 0.7213246822357178 + 10.0 * 8.478655815124512
Epoch 550, val loss: 0.7470414042472839
Epoch 560, training loss: 85.47408294677734 = 0.7126885652542114 + 10.0 * 8.476139068603516
Epoch 560, val loss: 0.7392295598983765
Epoch 570, training loss: 85.43031311035156 = 0.7042887210845947 + 10.0 * 8.472601890563965
Epoch 570, val loss: 0.7316563725471497
Epoch 580, training loss: 85.41175079345703 = 0.6961372494697571 + 10.0 * 8.471561431884766
Epoch 580, val loss: 0.7243053913116455
Epoch 590, training loss: 85.36007690429688 = 0.6882067322731018 + 10.0 * 8.46718692779541
Epoch 590, val loss: 0.7171953320503235
Epoch 600, training loss: 85.3100357055664 = 0.6805651187896729 + 10.0 * 8.462946891784668
Epoch 600, val loss: 0.7103378772735596
Epoch 610, training loss: 85.26348114013672 = 0.673133373260498 + 10.0 * 8.45903491973877
Epoch 610, val loss: 0.7036983966827393
Epoch 620, training loss: 85.22765350341797 = 0.6659016013145447 + 10.0 * 8.456174850463867
Epoch 620, val loss: 0.6972226500511169
Epoch 630, training loss: 85.26548767089844 = 0.6587777733802795 + 10.0 * 8.460671424865723
Epoch 630, val loss: 0.6908042430877686
Epoch 640, training loss: 85.17742156982422 = 0.651727557182312 + 10.0 * 8.452569961547852
Epoch 640, val loss: 0.6845335960388184
Epoch 650, training loss: 85.13440704345703 = 0.6449544429779053 + 10.0 * 8.448945999145508
Epoch 650, val loss: 0.6785048842430115
Epoch 660, training loss: 85.11051940917969 = 0.6383693218231201 + 10.0 * 8.44721508026123
Epoch 660, val loss: 0.6726064085960388
Epoch 670, training loss: 85.08368682861328 = 0.6319411993026733 + 10.0 * 8.445174217224121
Epoch 670, val loss: 0.6668665409088135
Epoch 680, training loss: 85.0609130859375 = 0.6256715059280396 + 10.0 * 8.443524360656738
Epoch 680, val loss: 0.6612758040428162
Epoch 690, training loss: 85.05148315429688 = 0.6195641756057739 + 10.0 * 8.443191528320312
Epoch 690, val loss: 0.6558088064193726
Epoch 700, training loss: 85.04483032226562 = 0.6135947704315186 + 10.0 * 8.443123817443848
Epoch 700, val loss: 0.6504740715026855
Epoch 710, training loss: 85.0043716430664 = 0.6077925562858582 + 10.0 * 8.439657211303711
Epoch 710, val loss: 0.645354688167572
Epoch 720, training loss: 84.98004913330078 = 0.6022241115570068 + 10.0 * 8.437782287597656
Epoch 720, val loss: 0.6403884887695312
Epoch 730, training loss: 84.9593276977539 = 0.5968315005302429 + 10.0 * 8.436249732971191
Epoch 730, val loss: 0.6355937719345093
Epoch 740, training loss: 84.9381103515625 = 0.5915855169296265 + 10.0 * 8.434652328491211
Epoch 740, val loss: 0.6309203505516052
Epoch 750, training loss: 84.92019653320312 = 0.5864781141281128 + 10.0 * 8.433371543884277
Epoch 750, val loss: 0.6263682246208191
Epoch 760, training loss: 84.966796875 = 0.5814955234527588 + 10.0 * 8.438529968261719
Epoch 760, val loss: 0.6218832731246948
Epoch 770, training loss: 84.88912963867188 = 0.5765798687934875 + 10.0 * 8.431255340576172
Epoch 770, val loss: 0.6175441145896912
Epoch 780, training loss: 84.8707046508789 = 0.5719056129455566 + 10.0 * 8.429880142211914
Epoch 780, val loss: 0.6133995652198792
Epoch 790, training loss: 84.85250091552734 = 0.5673867464065552 + 10.0 * 8.428510665893555
Epoch 790, val loss: 0.6093617081642151
Epoch 800, training loss: 84.83499908447266 = 0.5629911422729492 + 10.0 * 8.427201271057129
Epoch 800, val loss: 0.6054527759552002
Epoch 810, training loss: 84.81880950927734 = 0.55870121717453 + 10.0 * 8.426011085510254
Epoch 810, val loss: 0.6016277074813843
Epoch 820, training loss: 84.80236053466797 = 0.5545102953910828 + 10.0 * 8.424784660339355
Epoch 820, val loss: 0.597894012928009
Epoch 830, training loss: 84.78742980957031 = 0.5504125356674194 + 10.0 * 8.423701286315918
Epoch 830, val loss: 0.5942367315292358
Epoch 840, training loss: 84.81946563720703 = 0.5463906526565552 + 10.0 * 8.42730712890625
Epoch 840, val loss: 0.5906239748001099
Epoch 850, training loss: 84.76490020751953 = 0.542413055896759 + 10.0 * 8.422248840332031
Epoch 850, val loss: 0.5871265530586243
Epoch 860, training loss: 84.74411010742188 = 0.5385940670967102 + 10.0 * 8.420551300048828
Epoch 860, val loss: 0.5836922526359558
Epoch 870, training loss: 84.72760009765625 = 0.5348732471466064 + 10.0 * 8.419272422790527
Epoch 870, val loss: 0.5803900957107544
Epoch 880, training loss: 84.71625518798828 = 0.5312383770942688 + 10.0 * 8.418501853942871
Epoch 880, val loss: 0.5771609544754028
Epoch 890, training loss: 84.70934295654297 = 0.5276377201080322 + 10.0 * 8.418169975280762
Epoch 890, val loss: 0.5739391446113586
Epoch 900, training loss: 84.69410705566406 = 0.524103581905365 + 10.0 * 8.417000770568848
Epoch 900, val loss: 0.5707873702049255
Epoch 910, training loss: 84.67635345458984 = 0.5206621885299683 + 10.0 * 8.415569305419922
Epoch 910, val loss: 0.5677052140235901
Epoch 920, training loss: 84.67462158203125 = 0.517282247543335 + 10.0 * 8.41573429107666
Epoch 920, val loss: 0.5646616816520691
Epoch 930, training loss: 84.65254211425781 = 0.5139344334602356 + 10.0 * 8.413861274719238
Epoch 930, val loss: 0.5617139935493469
Epoch 940, training loss: 84.64178466796875 = 0.510668933391571 + 10.0 * 8.413111686706543
Epoch 940, val loss: 0.5588139891624451
Epoch 950, training loss: 84.63092041015625 = 0.5074705481529236 + 10.0 * 8.412344932556152
Epoch 950, val loss: 0.5559300184249878
Epoch 960, training loss: 84.6187515258789 = 0.5043228268623352 + 10.0 * 8.411442756652832
Epoch 960, val loss: 0.5531411170959473
Epoch 970, training loss: 84.61735534667969 = 0.5012200474739075 + 10.0 * 8.411613464355469
Epoch 970, val loss: 0.550366222858429
Epoch 980, training loss: 84.62663269042969 = 0.4981444180011749 + 10.0 * 8.412848472595215
Epoch 980, val loss: 0.5475170612335205
Epoch 990, training loss: 84.59368133544922 = 0.4951038062572479 + 10.0 * 8.409857749938965
Epoch 990, val loss: 0.5448828339576721
Epoch 1000, training loss: 84.57918548583984 = 0.49215131998062134 + 10.0 * 8.408703804016113
Epoch 1000, val loss: 0.5422018766403198
Epoch 1010, training loss: 84.56875610351562 = 0.48925310373306274 + 10.0 * 8.407950401306152
Epoch 1010, val loss: 0.5395861268043518
Epoch 1020, training loss: 84.56004333496094 = 0.4863918125629425 + 10.0 * 8.407365798950195
Epoch 1020, val loss: 0.537051260471344
Epoch 1030, training loss: 84.55706024169922 = 0.4835681915283203 + 10.0 * 8.407349586486816
Epoch 1030, val loss: 0.5345107316970825
Epoch 1040, training loss: 84.57675170898438 = 0.4807591140270233 + 10.0 * 8.409599304199219
Epoch 1040, val loss: 0.5319141745567322
Epoch 1050, training loss: 84.54463195800781 = 0.4779793918132782 + 10.0 * 8.406664848327637
Epoch 1050, val loss: 0.5295392274856567
Epoch 1060, training loss: 84.52459716796875 = 0.47528278827667236 + 10.0 * 8.404932022094727
Epoch 1060, val loss: 0.5270448327064514
Epoch 1070, training loss: 84.5133056640625 = 0.4726376533508301 + 10.0 * 8.40406608581543
Epoch 1070, val loss: 0.5246899127960205
Epoch 1080, training loss: 84.50363159179688 = 0.47003084421157837 + 10.0 * 8.403360366821289
Epoch 1080, val loss: 0.5223641395568848
Epoch 1090, training loss: 84.49520111083984 = 0.4674469530582428 + 10.0 * 8.402775764465332
Epoch 1090, val loss: 0.5200299620628357
Epoch 1100, training loss: 84.50044250488281 = 0.46488305926322937 + 10.0 * 8.403555870056152
Epoch 1100, val loss: 0.5176911354064941
Epoch 1110, training loss: 84.4910888671875 = 0.46231162548065186 + 10.0 * 8.402877807617188
Epoch 1110, val loss: 0.5154944062232971
Epoch 1120, training loss: 84.4769287109375 = 0.4597861170768738 + 10.0 * 8.401714324951172
Epoch 1120, val loss: 0.5131699442863464
Epoch 1130, training loss: 84.45954132080078 = 0.4573192000389099 + 10.0 * 8.400221824645996
Epoch 1130, val loss: 0.5109769105911255
Epoch 1140, training loss: 84.45075988769531 = 0.4548933804035187 + 10.0 * 8.39958667755127
Epoch 1140, val loss: 0.5088522434234619
Epoch 1150, training loss: 84.45177459716797 = 0.4524930417537689 + 10.0 * 8.399928092956543
Epoch 1150, val loss: 0.5067404508590698
Epoch 1160, training loss: 84.43833923339844 = 0.45007243752479553 + 10.0 * 8.398826599121094
Epoch 1160, val loss: 0.50457763671875
Epoch 1170, training loss: 84.4344482421875 = 0.44770586490631104 + 10.0 * 8.398674011230469
Epoch 1170, val loss: 0.5024785399436951
Epoch 1180, training loss: 84.41607666015625 = 0.4454055428504944 + 10.0 * 8.397067070007324
Epoch 1180, val loss: 0.5004495978355408
Epoch 1190, training loss: 84.40879821777344 = 0.4431335926055908 + 10.0 * 8.396566390991211
Epoch 1190, val loss: 0.498469740152359
Epoch 1200, training loss: 84.39844512939453 = 0.4408850073814392 + 10.0 * 8.395755767822266
Epoch 1200, val loss: 0.4965164363384247
Epoch 1210, training loss: 84.39030456542969 = 0.4386531114578247 + 10.0 * 8.39516544342041
Epoch 1210, val loss: 0.4945523142814636
Epoch 1220, training loss: 84.40989685058594 = 0.43642255663871765 + 10.0 * 8.397347450256348
Epoch 1220, val loss: 0.4925423860549927
Epoch 1230, training loss: 84.39603424072266 = 0.43419480323791504 + 10.0 * 8.396183967590332
Epoch 1230, val loss: 0.4907623827457428
Epoch 1240, training loss: 84.36805725097656 = 0.4320073425769806 + 10.0 * 8.39360523223877
Epoch 1240, val loss: 0.4888469874858856
Epoch 1250, training loss: 84.35951232910156 = 0.4298555552959442 + 10.0 * 8.392965316772461
Epoch 1250, val loss: 0.48698320984840393
Epoch 1260, training loss: 84.3580093383789 = 0.42773425579071045 + 10.0 * 8.393027305603027
Epoch 1260, val loss: 0.4851701557636261
Epoch 1270, training loss: 84.3698501586914 = 0.4256046712398529 + 10.0 * 8.394424438476562
Epoch 1270, val loss: 0.48337167501449585
Epoch 1280, training loss: 84.33728790283203 = 0.4235023856163025 + 10.0 * 8.391378402709961
Epoch 1280, val loss: 0.48156866431236267
Epoch 1290, training loss: 84.33045196533203 = 0.42144471406936646 + 10.0 * 8.390900611877441
Epoch 1290, val loss: 0.479817658662796
Epoch 1300, training loss: 84.32321166992188 = 0.4194127917289734 + 10.0 * 8.390379905700684
Epoch 1300, val loss: 0.47811779379844666
Epoch 1310, training loss: 84.31542205810547 = 0.4174037575721741 + 10.0 * 8.389801979064941
Epoch 1310, val loss: 0.4764333665370941
Epoch 1320, training loss: 84.30827331542969 = 0.4154062867164612 + 10.0 * 8.389286994934082
Epoch 1320, val loss: 0.47475746273994446
Epoch 1330, training loss: 84.3047866821289 = 0.4134169816970825 + 10.0 * 8.389137268066406
Epoch 1330, val loss: 0.4730875790119171
Epoch 1340, training loss: 84.31758117675781 = 0.4114246666431427 + 10.0 * 8.390615463256836
Epoch 1340, val loss: 0.47146111726760864
Epoch 1350, training loss: 84.29410552978516 = 0.40944021940231323 + 10.0 * 8.388466835021973
Epoch 1350, val loss: 0.46977949142456055
Epoch 1360, training loss: 84.28514099121094 = 0.4074941873550415 + 10.0 * 8.387764930725098
Epoch 1360, val loss: 0.4681995213031769
Epoch 1370, training loss: 84.27742004394531 = 0.405568391084671 + 10.0 * 8.387185096740723
Epoch 1370, val loss: 0.46658098697662354
Epoch 1380, training loss: 84.28734588623047 = 0.40366131067276 + 10.0 * 8.388368606567383
Epoch 1380, val loss: 0.46500957012176514
Epoch 1390, training loss: 84.26564025878906 = 0.40173956751823425 + 10.0 * 8.38638973236084
Epoch 1390, val loss: 0.4634706974029541
Epoch 1400, training loss: 84.2606201171875 = 0.3998568058013916 + 10.0 * 8.386075973510742
Epoch 1400, val loss: 0.46192437410354614
Epoch 1410, training loss: 84.25599670410156 = 0.39800578355789185 + 10.0 * 8.385799407958984
Epoch 1410, val loss: 0.4604369103908539
Epoch 1420, training loss: 84.24891662597656 = 0.3961774706840515 + 10.0 * 8.385273933410645
Epoch 1420, val loss: 0.458954781293869
Epoch 1430, training loss: 84.24224853515625 = 0.39436301589012146 + 10.0 * 8.384788513183594
Epoch 1430, val loss: 0.4575059115886688
Epoch 1440, training loss: 84.23675537109375 = 0.39255908131599426 + 10.0 * 8.384419441223145
Epoch 1440, val loss: 0.4560532867908478
Epoch 1450, training loss: 84.25554656982422 = 0.3907630443572998 + 10.0 * 8.386478424072266
Epoch 1450, val loss: 0.45465779304504395
Epoch 1460, training loss: 84.24362182617188 = 0.3889544904232025 + 10.0 * 8.385466575622559
Epoch 1460, val loss: 0.45312511920928955
Epoch 1470, training loss: 84.22301483154297 = 0.3871936500072479 + 10.0 * 8.38358211517334
Epoch 1470, val loss: 0.4517313838005066
Epoch 1480, training loss: 84.21809387207031 = 0.38545483350753784 + 10.0 * 8.383264541625977
Epoch 1480, val loss: 0.4503486454486847
Epoch 1490, training loss: 84.22087860107422 = 0.38373225927352905 + 10.0 * 8.38371467590332
Epoch 1490, val loss: 0.4489307403564453
Epoch 1500, training loss: 84.20576477050781 = 0.38202735781669617 + 10.0 * 8.382373809814453
Epoch 1500, val loss: 0.44764018058776855
Epoch 1510, training loss: 84.21693420410156 = 0.3803434669971466 + 10.0 * 8.383659362792969
Epoch 1510, val loss: 0.44632580876350403
Epoch 1520, training loss: 84.19709777832031 = 0.3786625564098358 + 10.0 * 8.381843566894531
Epoch 1520, val loss: 0.4449741244316101
Epoch 1530, training loss: 84.19178771972656 = 0.37701818346977234 + 10.0 * 8.381476402282715
Epoch 1530, val loss: 0.4436884820461273
Epoch 1540, training loss: 84.18560791015625 = 0.3753930628299713 + 10.0 * 8.381021499633789
Epoch 1540, val loss: 0.44242191314697266
Epoch 1550, training loss: 84.17969512939453 = 0.3737858235836029 + 10.0 * 8.380590438842773
Epoch 1550, val loss: 0.44117608666419983
Epoch 1560, training loss: 84.17486572265625 = 0.3721916079521179 + 10.0 * 8.380267143249512
Epoch 1560, val loss: 0.4399297833442688
Epoch 1570, training loss: 84.1737060546875 = 0.37060853838920593 + 10.0 * 8.38031005859375
Epoch 1570, val loss: 0.43874841928482056
Epoch 1580, training loss: 84.2103271484375 = 0.36901965737342834 + 10.0 * 8.384130477905273
Epoch 1580, val loss: 0.4375309646129608
Epoch 1590, training loss: 84.16378784179688 = 0.3674357831478119 + 10.0 * 8.379634857177734
Epoch 1590, val loss: 0.43631914258003235
Epoch 1600, training loss: 84.15967559814453 = 0.3658972978591919 + 10.0 * 8.379377365112305
Epoch 1600, val loss: 0.4350561797618866
Epoch 1610, training loss: 84.15178680419922 = 0.3643922805786133 + 10.0 * 8.378739356994629
Epoch 1610, val loss: 0.4339018762111664
Epoch 1620, training loss: 84.14689636230469 = 0.36290496587753296 + 10.0 * 8.378398895263672
Epoch 1620, val loss: 0.4328162968158722
Epoch 1630, training loss: 84.1414794921875 = 0.36142489314079285 + 10.0 * 8.378005027770996
Epoch 1630, val loss: 0.43167850375175476
Epoch 1640, training loss: 84.13687896728516 = 0.3599525988101959 + 10.0 * 8.377692222595215
Epoch 1640, val loss: 0.43056827783584595
Epoch 1650, training loss: 84.1332015991211 = 0.3584895730018616 + 10.0 * 8.377470970153809
Epoch 1650, val loss: 0.42950373888015747
Epoch 1660, training loss: 84.17874145507812 = 0.35703545808792114 + 10.0 * 8.382170677185059
Epoch 1660, val loss: 0.42848455905914307
Epoch 1670, training loss: 84.14215087890625 = 0.35557177662849426 + 10.0 * 8.378657341003418
Epoch 1670, val loss: 0.4273161292076111
Epoch 1680, training loss: 84.14146423339844 = 0.35413858294487 + 10.0 * 8.378732681274414
Epoch 1680, val loss: 0.4262685775756836
Epoch 1690, training loss: 84.11913299560547 = 0.35273897647857666 + 10.0 * 8.376639366149902
Epoch 1690, val loss: 0.4252746105194092
Epoch 1700, training loss: 84.11088562011719 = 0.3513645827770233 + 10.0 * 8.375951766967773
Epoch 1700, val loss: 0.4242815673351288
Epoch 1710, training loss: 84.10639953613281 = 0.3500073254108429 + 10.0 * 8.375638961791992
Epoch 1710, val loss: 0.4233139455318451
Epoch 1720, training loss: 84.10215759277344 = 0.34865906834602356 + 10.0 * 8.375349998474121
Epoch 1720, val loss: 0.42234066128730774
Epoch 1730, training loss: 84.09703063964844 = 0.34731775522232056 + 10.0 * 8.374971389770508
Epoch 1730, val loss: 0.4214009940624237
Epoch 1740, training loss: 84.09297180175781 = 0.34598207473754883 + 10.0 * 8.374698638916016
Epoch 1740, val loss: 0.4204619526863098
Epoch 1750, training loss: 84.09524536132812 = 0.34465137124061584 + 10.0 * 8.375059127807617
Epoch 1750, val loss: 0.4195266664028168
Epoch 1760, training loss: 84.12763977050781 = 0.34331732988357544 + 10.0 * 8.378432273864746
Epoch 1760, val loss: 0.4186035692691803
Epoch 1770, training loss: 84.08262634277344 = 0.34199145436286926 + 10.0 * 8.374063491821289
Epoch 1770, val loss: 0.41766542196273804
Epoch 1780, training loss: 84.0787353515625 = 0.340702086687088 + 10.0 * 8.37380313873291
Epoch 1780, val loss: 0.4167879521846771
Epoch 1790, training loss: 84.07411193847656 = 0.33943137526512146 + 10.0 * 8.373468399047852
Epoch 1790, val loss: 0.4159097373485565
Epoch 1800, training loss: 84.06771850585938 = 0.3381730616092682 + 10.0 * 8.372954368591309
Epoch 1800, val loss: 0.4150615632534027
Epoch 1810, training loss: 84.0633773803711 = 0.33692100644111633 + 10.0 * 8.372645378112793
Epoch 1810, val loss: 0.41421839594841003
Epoch 1820, training loss: 84.0644302368164 = 0.3356754779815674 + 10.0 * 8.372875213623047
Epoch 1820, val loss: 0.4134126901626587
Epoch 1830, training loss: 84.08759307861328 = 0.3344267010688782 + 10.0 * 8.375316619873047
Epoch 1830, val loss: 0.4126017987728119
Epoch 1840, training loss: 84.05552673339844 = 0.33318841457366943 + 10.0 * 8.372233390808105
Epoch 1840, val loss: 0.4117516875267029
Epoch 1850, training loss: 84.05046844482422 = 0.3319721519947052 + 10.0 * 8.37185001373291
Epoch 1850, val loss: 0.4109605848789215
Epoch 1860, training loss: 84.04460906982422 = 0.3307695984840393 + 10.0 * 8.371383666992188
Epoch 1860, val loss: 0.41018879413604736
Epoch 1870, training loss: 84.03910827636719 = 0.32957616448402405 + 10.0 * 8.370953559875488
Epoch 1870, val loss: 0.4094234108924866
Epoch 1880, training loss: 84.03521728515625 = 0.3283909261226654 + 10.0 * 8.370682716369629
Epoch 1880, val loss: 0.40866827964782715
Epoch 1890, training loss: 84.03355407714844 = 0.32721251249313354 + 10.0 * 8.370634078979492
Epoch 1890, val loss: 0.40790265798568726
Epoch 1900, training loss: 84.0459213256836 = 0.32603970170021057 + 10.0 * 8.371988296508789
Epoch 1900, val loss: 0.40718671679496765
Epoch 1910, training loss: 84.05034637451172 = 0.32486772537231445 + 10.0 * 8.37254810333252
Epoch 1910, val loss: 0.40653854608535767
Epoch 1920, training loss: 84.02989196777344 = 0.323712021112442 + 10.0 * 8.370617866516113
Epoch 1920, val loss: 0.405755877494812
Epoch 1930, training loss: 84.01966857910156 = 0.3225859999656677 + 10.0 * 8.369708061218262
Epoch 1930, val loss: 0.40506598353385925
Epoch 1940, training loss: 84.01343536376953 = 0.32147911190986633 + 10.0 * 8.369195938110352
Epoch 1940, val loss: 0.4044318199157715
Epoch 1950, training loss: 84.0096435546875 = 0.32037726044654846 + 10.0 * 8.368927001953125
Epoch 1950, val loss: 0.4037354588508606
Epoch 1960, training loss: 84.00527954101562 = 0.31928107142448425 + 10.0 * 8.368599891662598
Epoch 1960, val loss: 0.40311571955680847
Epoch 1970, training loss: 84.00151062011719 = 0.3181861340999603 + 10.0 * 8.368332862854004
Epoch 1970, val loss: 0.4024754464626312
Epoch 1980, training loss: 83.99801635742188 = 0.31709548830986023 + 10.0 * 8.36809253692627
Epoch 1980, val loss: 0.40185311436653137
Epoch 1990, training loss: 84.0038833618164 = 0.31601375341415405 + 10.0 * 8.368786811828613
Epoch 1990, val loss: 0.4013335406780243
Epoch 2000, training loss: 83.99908447265625 = 0.31491774320602417 + 10.0 * 8.368416786193848
Epoch 2000, val loss: 0.4005699157714844
Epoch 2010, training loss: 83.99736022949219 = 0.31385156512260437 + 10.0 * 8.368350982666016
Epoch 2010, val loss: 0.4000515341758728
Epoch 2020, training loss: 83.98734283447266 = 0.3127940595149994 + 10.0 * 8.367454528808594
Epoch 2020, val loss: 0.399474561214447
Epoch 2030, training loss: 83.9820785522461 = 0.311745822429657 + 10.0 * 8.367033004760742
Epoch 2030, val loss: 0.39887335896492004
Epoch 2040, training loss: 83.9801025390625 = 0.3107079565525055 + 10.0 * 8.366939544677734
Epoch 2040, val loss: 0.3983094096183777
Epoch 2050, training loss: 84.01941680908203 = 0.30967098474502563 + 10.0 * 8.37097454071045
Epoch 2050, val loss: 0.39781302213668823
Epoch 2060, training loss: 83.9808120727539 = 0.30862677097320557 + 10.0 * 8.367218971252441
Epoch 2060, val loss: 0.3972126543521881
Epoch 2070, training loss: 83.97289276123047 = 0.3076077699661255 + 10.0 * 8.366528511047363
Epoch 2070, val loss: 0.3967321515083313
Epoch 2080, training loss: 83.96672821044922 = 0.30659300088882446 + 10.0 * 8.366013526916504
Epoch 2080, val loss: 0.39620450139045715
Epoch 2090, training loss: 83.961669921875 = 0.3055899143218994 + 10.0 * 8.365608215332031
Epoch 2090, val loss: 0.39573147892951965
Epoch 2100, training loss: 83.95816040039062 = 0.3045894503593445 + 10.0 * 8.365357398986816
Epoch 2100, val loss: 0.3952479660511017
Epoch 2110, training loss: 83.95894622802734 = 0.3035915195941925 + 10.0 * 8.365535736083984
Epoch 2110, val loss: 0.3947857916355133
Epoch 2120, training loss: 83.98326873779297 = 0.30259251594543457 + 10.0 * 8.368067741394043
Epoch 2120, val loss: 0.3943111300468445
Epoch 2130, training loss: 83.95359802246094 = 0.3015972673892975 + 10.0 * 8.36520004272461
Epoch 2130, val loss: 0.39385852217674255
Epoch 2140, training loss: 83.94815826416016 = 0.3006177246570587 + 10.0 * 8.364753723144531
Epoch 2140, val loss: 0.39341625571250916
Epoch 2150, training loss: 83.944580078125 = 0.2996496558189392 + 10.0 * 8.364492416381836
Epoch 2150, val loss: 0.3929777145385742
Epoch 2160, training loss: 83.9450912475586 = 0.29868873953819275 + 10.0 * 8.364640235900879
Epoch 2160, val loss: 0.39258265495300293
Epoch 2170, training loss: 83.96532440185547 = 0.2977293133735657 + 10.0 * 8.366759300231934
Epoch 2170, val loss: 0.3921695649623871
Epoch 2180, training loss: 83.94366455078125 = 0.29677262902259827 + 10.0 * 8.364688873291016
Epoch 2180, val loss: 0.39169368147850037
Epoch 2190, training loss: 83.93070983886719 = 0.29582834243774414 + 10.0 * 8.36348819732666
Epoch 2190, val loss: 0.3913877010345459
Epoch 2200, training loss: 83.92664337158203 = 0.2948920428752899 + 10.0 * 8.363175392150879
Epoch 2200, val loss: 0.3909851610660553
Epoch 2210, training loss: 83.92660522460938 = 0.29396072030067444 + 10.0 * 8.363264083862305
Epoch 2210, val loss: 0.3906234800815582
Epoch 2220, training loss: 83.95513916015625 = 0.2930331826210022 + 10.0 * 8.3662109375
Epoch 2220, val loss: 0.39033108949661255
Epoch 2230, training loss: 83.9249267578125 = 0.2921007573604584 + 10.0 * 8.363283157348633
Epoch 2230, val loss: 0.3898333013057709
Epoch 2240, training loss: 83.91946411132812 = 0.2911866009235382 + 10.0 * 8.362828254699707
Epoch 2240, val loss: 0.3895905315876007
Epoch 2250, training loss: 83.91283416748047 = 0.2902754247188568 + 10.0 * 8.362256050109863
Epoch 2250, val loss: 0.38920989632606506
Epoch 2260, training loss: 83.90834045410156 = 0.2893710136413574 + 10.0 * 8.361897468566895
Epoch 2260, val loss: 0.38892558217048645
Epoch 2270, training loss: 83.90526580810547 = 0.28846657276153564 + 10.0 * 8.361680030822754
Epoch 2270, val loss: 0.3886178135871887
Epoch 2280, training loss: 83.94478607177734 = 0.2875644862651825 + 10.0 * 8.365721702575684
Epoch 2280, val loss: 0.38836008310317993
Epoch 2290, training loss: 83.9193115234375 = 0.28665369749069214 + 10.0 * 8.363265991210938
Epoch 2290, val loss: 0.3880118131637573
Epoch 2300, training loss: 83.90432739257812 = 0.2857649624347687 + 10.0 * 8.361856460571289
Epoch 2300, val loss: 0.387776643037796
Epoch 2310, training loss: 83.89476013183594 = 0.2848831117153168 + 10.0 * 8.360987663269043
Epoch 2310, val loss: 0.387469083070755
Epoch 2320, training loss: 83.89091491699219 = 0.28401127457618713 + 10.0 * 8.360690116882324
Epoch 2320, val loss: 0.3872377872467041
Epoch 2330, training loss: 83.88650512695312 = 0.28313878178596497 + 10.0 * 8.360336303710938
Epoch 2330, val loss: 0.38696786761283875
Epoch 2340, training loss: 83.8837661743164 = 0.2822675406932831 + 10.0 * 8.360150337219238
Epoch 2340, val loss: 0.38674643635749817
Epoch 2350, training loss: 83.90642547607422 = 0.281399667263031 + 10.0 * 8.362502098083496
Epoch 2350, val loss: 0.3866119682788849
Epoch 2360, training loss: 83.88750457763672 = 0.2805233299732208 + 10.0 * 8.360697746276855
Epoch 2360, val loss: 0.38622087240219116
Epoch 2370, training loss: 83.87960052490234 = 0.27966591715812683 + 10.0 * 8.359993934631348
Epoch 2370, val loss: 0.38609960675239563
Epoch 2380, training loss: 83.87236785888672 = 0.2788079082965851 + 10.0 * 8.359355926513672
Epoch 2380, val loss: 0.38585779070854187
Epoch 2390, training loss: 83.86972045898438 = 0.277957558631897 + 10.0 * 8.359176635742188
Epoch 2390, val loss: 0.38569679856300354
Epoch 2400, training loss: 83.91427612304688 = 0.27711185812950134 + 10.0 * 8.363716125488281
Epoch 2400, val loss: 0.3856286108493805
Epoch 2410, training loss: 83.88128662109375 = 0.2762509286403656 + 10.0 * 8.360503196716309
Epoch 2410, val loss: 0.38519442081451416
Epoch 2420, training loss: 83.86419677734375 = 0.2754146456718445 + 10.0 * 8.358878135681152
Epoch 2420, val loss: 0.3851461112499237
Epoch 2430, training loss: 83.85707092285156 = 0.27457883954048157 + 10.0 * 8.358248710632324
Epoch 2430, val loss: 0.3849467933177948
Epoch 2440, training loss: 83.85436248779297 = 0.2737496495246887 + 10.0 * 8.358060836791992
Epoch 2440, val loss: 0.38473832607269287
Epoch 2450, training loss: 83.85205078125 = 0.2729220688343048 + 10.0 * 8.35791301727295
Epoch 2450, val loss: 0.3846055865287781
Epoch 2460, training loss: 83.87135314941406 = 0.27209368348121643 + 10.0 * 8.359926223754883
Epoch 2460, val loss: 0.3844282329082489
Epoch 2470, training loss: 83.8487319946289 = 0.2712642252445221 + 10.0 * 8.357747077941895
Epoch 2470, val loss: 0.38436421751976013
Epoch 2480, training loss: 83.84513092041016 = 0.2704402208328247 + 10.0 * 8.35746955871582
Epoch 2480, val loss: 0.3842012584209442
Epoch 2490, training loss: 83.8404769897461 = 0.26962336897850037 + 10.0 * 8.357085227966309
Epoch 2490, val loss: 0.3841404318809509
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8508371385083713
0.8642324132434979
=== training gcn model ===
Epoch 0, training loss: 106.91885375976562 = 1.0961031913757324 + 10.0 * 10.582275390625
Epoch 0, val loss: 1.094886302947998
Epoch 10, training loss: 106.91044616699219 = 1.0913817882537842 + 10.0 * 10.58190631866455
Epoch 10, val loss: 1.0901962518692017
Epoch 20, training loss: 106.8886947631836 = 1.0863910913467407 + 10.0 * 10.580230712890625
Epoch 20, val loss: 1.0852651596069336
Epoch 30, training loss: 106.79145050048828 = 1.0812628269195557 + 10.0 * 10.571019172668457
Epoch 30, val loss: 1.0802111625671387
Epoch 40, training loss: 106.3465576171875 = 1.0758177042007446 + 10.0 * 10.527073860168457
Epoch 40, val loss: 1.074823021888733
Epoch 50, training loss: 104.83574676513672 = 1.0701136589050293 + 10.0 * 10.37656307220459
Epoch 50, val loss: 1.0692461729049683
Epoch 60, training loss: 101.1907730102539 = 1.0646647214889526 + 10.0 * 10.01261043548584
Epoch 60, val loss: 1.0638368129730225
Epoch 70, training loss: 97.05459594726562 = 1.0590031147003174 + 10.0 * 9.59955883026123
Epoch 70, val loss: 1.0582753419876099
Epoch 80, training loss: 94.18472290039062 = 1.0532091856002808 + 10.0 * 9.313151359558105
Epoch 80, val loss: 1.05259108543396
Epoch 90, training loss: 92.61471557617188 = 1.0476487874984741 + 10.0 * 9.156706809997559
Epoch 90, val loss: 1.0473123788833618
Epoch 100, training loss: 91.64165496826172 = 1.0431920289993286 + 10.0 * 9.059846878051758
Epoch 100, val loss: 1.0431355237960815
Epoch 110, training loss: 90.74522399902344 = 1.0400152206420898 + 10.0 * 8.970520973205566
Epoch 110, val loss: 1.0401650667190552
Epoch 120, training loss: 90.04529571533203 = 1.037688970565796 + 10.0 * 8.900760650634766
Epoch 120, val loss: 1.0379602909088135
Epoch 130, training loss: 89.50565338134766 = 1.0356355905532837 + 10.0 * 8.847002029418945
Epoch 130, val loss: 1.0359610319137573
Epoch 140, training loss: 89.03687286376953 = 1.033449411392212 + 10.0 * 8.800342559814453
Epoch 140, val loss: 1.0338484048843384
Epoch 150, training loss: 88.65906524658203 = 1.0311247110366821 + 10.0 * 8.76279354095459
Epoch 150, val loss: 1.0316013097763062
Epoch 160, training loss: 88.35224914550781 = 1.028631567955017 + 10.0 * 8.732361793518066
Epoch 160, val loss: 1.0291495323181152
Epoch 170, training loss: 88.07887268066406 = 1.0258663892745972 + 10.0 * 8.705301284790039
Epoch 170, val loss: 1.0264211893081665
Epoch 180, training loss: 87.82416534423828 = 1.0228643417358398 + 10.0 * 8.680130004882812
Epoch 180, val loss: 1.0234959125518799
Epoch 190, training loss: 87.64327239990234 = 1.0195693969726562 + 10.0 * 8.662370681762695
Epoch 190, val loss: 1.020256757736206
Epoch 200, training loss: 87.45974731445312 = 1.0160253047943115 + 10.0 * 8.644372940063477
Epoch 200, val loss: 1.0167871713638306
Epoch 210, training loss: 87.28767395019531 = 1.0123372077941895 + 10.0 * 8.627533912658691
Epoch 210, val loss: 1.013214349746704
Epoch 220, training loss: 87.12622833251953 = 1.0085147619247437 + 10.0 * 8.611771583557129
Epoch 220, val loss: 1.0094797611236572
Epoch 230, training loss: 86.97533416748047 = 1.0044920444488525 + 10.0 * 8.597084045410156
Epoch 230, val loss: 1.0055515766143799
Epoch 240, training loss: 86.86058044433594 = 1.000166654586792 + 10.0 * 8.586041450500488
Epoch 240, val loss: 1.0012775659561157
Epoch 250, training loss: 86.7347640991211 = 0.995316207408905 + 10.0 * 8.573945045471191
Epoch 250, val loss: 0.9965294003486633
Epoch 260, training loss: 86.64030456542969 = 0.9900999665260315 + 10.0 * 8.565020561218262
Epoch 260, val loss: 0.9914499521255493
Epoch 270, training loss: 86.55133056640625 = 0.984502375125885 + 10.0 * 8.556682586669922
Epoch 270, val loss: 0.9860038161277771
Epoch 280, training loss: 86.48487091064453 = 0.978489875793457 + 10.0 * 8.550638198852539
Epoch 280, val loss: 0.9801344275474548
Epoch 290, training loss: 86.41385650634766 = 0.9720668792724609 + 10.0 * 8.54417896270752
Epoch 290, val loss: 0.9738671183586121
Epoch 300, training loss: 86.34791564941406 = 0.9652543663978577 + 10.0 * 8.5382661819458
Epoch 300, val loss: 0.9672281742095947
Epoch 310, training loss: 86.29828643798828 = 0.9581089019775391 + 10.0 * 8.534017562866211
Epoch 310, val loss: 0.9602627158164978
Epoch 320, training loss: 86.22865295410156 = 0.9505613446235657 + 10.0 * 8.527809143066406
Epoch 320, val loss: 0.9529218077659607
Epoch 330, training loss: 86.16802215576172 = 0.9426577687263489 + 10.0 * 8.522536277770996
Epoch 330, val loss: 0.9452375173568726
Epoch 340, training loss: 86.1272964477539 = 0.9344068765640259 + 10.0 * 8.519289016723633
Epoch 340, val loss: 0.9371879696846008
Epoch 350, training loss: 86.05647277832031 = 0.9257633090019226 + 10.0 * 8.513071060180664
Epoch 350, val loss: 0.9288017749786377
Epoch 360, training loss: 86.00402069091797 = 0.9168475866317749 + 10.0 * 8.50871753692627
Epoch 360, val loss: 0.9201334714889526
Epoch 370, training loss: 85.94974517822266 = 0.9076030850410461 + 10.0 * 8.5042142868042
Epoch 370, val loss: 0.9111759066581726
Epoch 380, training loss: 85.92115020751953 = 0.8980703949928284 + 10.0 * 8.502307891845703
Epoch 380, val loss: 0.9019567966461182
Epoch 390, training loss: 85.8714599609375 = 0.8882263898849487 + 10.0 * 8.498323440551758
Epoch 390, val loss: 0.8923980593681335
Epoch 400, training loss: 85.80158233642578 = 0.8781810998916626 + 10.0 * 8.492340087890625
Epoch 400, val loss: 0.8826844692230225
Epoch 410, training loss: 85.74364471435547 = 0.8680683970451355 + 10.0 * 8.487558364868164
Epoch 410, val loss: 0.8728780746459961
Epoch 420, training loss: 85.69188690185547 = 0.8578085899353027 + 10.0 * 8.483407974243164
Epoch 420, val loss: 0.8629772663116455
Epoch 430, training loss: 85.66138458251953 = 0.8473621606826782 + 10.0 * 8.481402397155762
Epoch 430, val loss: 0.8528879284858704
Epoch 440, training loss: 85.60926055908203 = 0.8367049694061279 + 10.0 * 8.477254867553711
Epoch 440, val loss: 0.8425611853599548
Epoch 450, training loss: 85.54654693603516 = 0.8260044455528259 + 10.0 * 8.472054481506348
Epoch 450, val loss: 0.8322805762290955
Epoch 460, training loss: 85.52022552490234 = 0.8152711987495422 + 10.0 * 8.470495223999023
Epoch 460, val loss: 0.8219200372695923
Epoch 470, training loss: 85.45323181152344 = 0.804402232170105 + 10.0 * 8.464882850646973
Epoch 470, val loss: 0.8115349411964417
Epoch 480, training loss: 85.40785217285156 = 0.7935791611671448 + 10.0 * 8.461427688598633
Epoch 480, val loss: 0.8011971116065979
Epoch 490, training loss: 85.36819458007812 = 0.7827796936035156 + 10.0 * 8.458541870117188
Epoch 490, val loss: 0.7908576726913452
Epoch 500, training loss: 85.32991790771484 = 0.7718459367752075 + 10.0 * 8.45580768585205
Epoch 500, val loss: 0.7804405093193054
Epoch 510, training loss: 85.28675842285156 = 0.7610588669776917 + 10.0 * 8.452569961547852
Epoch 510, val loss: 0.770173966884613
Epoch 520, training loss: 85.2464599609375 = 0.7504308223724365 + 10.0 * 8.449603080749512
Epoch 520, val loss: 0.7601062059402466
Epoch 530, training loss: 85.20482635498047 = 0.7398850917816162 + 10.0 * 8.446494102478027
Epoch 530, val loss: 0.7501445412635803
Epoch 540, training loss: 85.1686782836914 = 0.7294723987579346 + 10.0 * 8.443921089172363
Epoch 540, val loss: 0.7403440475463867
Epoch 550, training loss: 85.17862701416016 = 0.7191347479820251 + 10.0 * 8.44594955444336
Epoch 550, val loss: 0.7306196689605713
Epoch 560, training loss: 85.1025619506836 = 0.708905816078186 + 10.0 * 8.43936538696289
Epoch 560, val loss: 0.7210915684700012
Epoch 570, training loss: 85.07191467285156 = 0.6990088224411011 + 10.0 * 8.437291145324707
Epoch 570, val loss: 0.7118858695030212
Epoch 580, training loss: 85.04290771484375 = 0.689345121383667 + 10.0 * 8.435356140136719
Epoch 580, val loss: 0.7029306888580322
Epoch 590, training loss: 85.03889465332031 = 0.6799280643463135 + 10.0 * 8.435895919799805
Epoch 590, val loss: 0.6942250728607178
Epoch 600, training loss: 84.99205780029297 = 0.6706785559654236 + 10.0 * 8.432138442993164
Epoch 600, val loss: 0.6857981085777283
Epoch 610, training loss: 84.95836639404297 = 0.6618138551712036 + 10.0 * 8.429655075073242
Epoch 610, val loss: 0.6776867508888245
Epoch 620, training loss: 84.93285369873047 = 0.6531779170036316 + 10.0 * 8.42796802520752
Epoch 620, val loss: 0.6698811054229736
Epoch 630, training loss: 84.91211700439453 = 0.644830048084259 + 10.0 * 8.426729202270508
Epoch 630, val loss: 0.6623533368110657
Epoch 640, training loss: 84.89893341064453 = 0.6367309093475342 + 10.0 * 8.426219940185547
Epoch 640, val loss: 0.6551334857940674
Epoch 650, training loss: 84.86845397949219 = 0.6289495825767517 + 10.0 * 8.4239501953125
Epoch 650, val loss: 0.6482311487197876
Epoch 660, training loss: 84.85301208496094 = 0.6214389801025391 + 10.0 * 8.423157691955566
Epoch 660, val loss: 0.6416376233100891
Epoch 670, training loss: 84.82208251953125 = 0.6142878532409668 + 10.0 * 8.42077922821045
Epoch 670, val loss: 0.6354155540466309
Epoch 680, training loss: 84.80274963378906 = 0.6074291467666626 + 10.0 * 8.41953182220459
Epoch 680, val loss: 0.6295070052146912
Epoch 690, training loss: 84.78407287597656 = 0.600844144821167 + 10.0 * 8.418322563171387
Epoch 690, val loss: 0.6238676905632019
Epoch 700, training loss: 84.78629302978516 = 0.5945104956626892 + 10.0 * 8.419178009033203
Epoch 700, val loss: 0.6184963583946228
Epoch 710, training loss: 84.77212524414062 = 0.5883344411849976 + 10.0 * 8.418378829956055
Epoch 710, val loss: 0.6134012937545776
Epoch 720, training loss: 84.73550415039062 = 0.5825279951095581 + 10.0 * 8.415297508239746
Epoch 720, val loss: 0.6085777282714844
Epoch 730, training loss: 84.72145080566406 = 0.5769449472427368 + 10.0 * 8.414450645446777
Epoch 730, val loss: 0.6040239334106445
Epoch 740, training loss: 84.71479034423828 = 0.5715786218643188 + 10.0 * 8.414320945739746
Epoch 740, val loss: 0.5996921062469482
Epoch 750, training loss: 84.69284057617188 = 0.5664103627204895 + 10.0 * 8.412642478942871
Epoch 750, val loss: 0.5955498814582825
Epoch 760, training loss: 84.68230438232422 = 0.5614550709724426 + 10.0 * 8.412084579467773
Epoch 760, val loss: 0.5916171669960022
Epoch 770, training loss: 84.66419219970703 = 0.556710422039032 + 10.0 * 8.410748481750488
Epoch 770, val loss: 0.587921142578125
Epoch 780, training loss: 84.66582489013672 = 0.5521337985992432 + 10.0 * 8.411369323730469
Epoch 780, val loss: 0.5843761563301086
Epoch 790, training loss: 84.63573455810547 = 0.5476844310760498 + 10.0 * 8.408804893493652
Epoch 790, val loss: 0.5810092091560364
Epoch 800, training loss: 84.62411499023438 = 0.5434510707855225 + 10.0 * 8.408066749572754
Epoch 800, val loss: 0.57779860496521
Epoch 810, training loss: 84.61019897460938 = 0.5393906831741333 + 10.0 * 8.40708065032959
Epoch 810, val loss: 0.5747610330581665
Epoch 820, training loss: 84.5968246459961 = 0.5354530215263367 + 10.0 * 8.406137466430664
Epoch 820, val loss: 0.5718717575073242
Epoch 830, training loss: 84.5971908569336 = 0.5316471457481384 + 10.0 * 8.406554222106934
Epoch 830, val loss: 0.5690739750862122
Epoch 840, training loss: 84.58226776123047 = 0.5279220342636108 + 10.0 * 8.405434608459473
Epoch 840, val loss: 0.5663573741912842
Epoch 850, training loss: 84.57579803466797 = 0.5243266820907593 + 10.0 * 8.405146598815918
Epoch 850, val loss: 0.5637850165367126
Epoch 860, training loss: 84.55120849609375 = 0.520887553691864 + 10.0 * 8.403032302856445
Epoch 860, val loss: 0.5613088607788086
Epoch 870, training loss: 84.54081726074219 = 0.5175604820251465 + 10.0 * 8.402325630187988
Epoch 870, val loss: 0.5589302182197571
Epoch 880, training loss: 84.5455551147461 = 0.514307975769043 + 10.0 * 8.403124809265137
Epoch 880, val loss: 0.5566079616546631
Epoch 890, training loss: 84.51985931396484 = 0.511070966720581 + 10.0 * 8.40087890625
Epoch 890, val loss: 0.5543803572654724
Epoch 900, training loss: 84.5078353881836 = 0.5079749822616577 + 10.0 * 8.399986267089844
Epoch 900, val loss: 0.5521682500839233
Epoch 910, training loss: 84.49688720703125 = 0.5049563646316528 + 10.0 * 8.399192810058594
Epoch 910, val loss: 0.5500727295875549
Epoch 920, training loss: 84.48947143554688 = 0.5020032525062561 + 10.0 * 8.398746490478516
Epoch 920, val loss: 0.5480104088783264
Epoch 930, training loss: 84.50321197509766 = 0.4990849792957306 + 10.0 * 8.400412559509277
Epoch 930, val loss: 0.5459682941436768
Epoch 940, training loss: 84.4697265625 = 0.4962134063243866 + 10.0 * 8.397351264953613
Epoch 940, val loss: 0.5440074801445007
Epoch 950, training loss: 84.48084259033203 = 0.49341705441474915 + 10.0 * 8.39874267578125
Epoch 950, val loss: 0.54206383228302
Epoch 960, training loss: 84.45735168457031 = 0.49063822627067566 + 10.0 * 8.396671295166016
Epoch 960, val loss: 0.5401396751403809
Epoch 970, training loss: 84.44485473632812 = 0.48792994022369385 + 10.0 * 8.395692825317383
Epoch 970, val loss: 0.5383002758026123
Epoch 980, training loss: 84.4329605102539 = 0.4852796196937561 + 10.0 * 8.394767761230469
Epoch 980, val loss: 0.5364648103713989
Epoch 990, training loss: 84.4516830444336 = 0.48264163732528687 + 10.0 * 8.396903991699219
Epoch 990, val loss: 0.5347152948379517
Epoch 1000, training loss: 84.41685485839844 = 0.4799710214138031 + 10.0 * 8.393688201904297
Epoch 1000, val loss: 0.5327911376953125
Epoch 1010, training loss: 84.40762329101562 = 0.47740107774734497 + 10.0 * 8.393022537231445
Epoch 1010, val loss: 0.5310479402542114
Epoch 1020, training loss: 84.3970947265625 = 0.47486722469329834 + 10.0 * 8.39222240447998
Epoch 1020, val loss: 0.5292813181877136
Epoch 1030, training loss: 84.38745880126953 = 0.4723632037639618 + 10.0 * 8.391509056091309
Epoch 1030, val loss: 0.5275673270225525
Epoch 1040, training loss: 84.39024353027344 = 0.4698792099952698 + 10.0 * 8.392036437988281
Epoch 1040, val loss: 0.5258106589317322
Epoch 1050, training loss: 84.3771743774414 = 0.46737000346183777 + 10.0 * 8.39098072052002
Epoch 1050, val loss: 0.5241607427597046
Epoch 1060, training loss: 84.36650848388672 = 0.4649180769920349 + 10.0 * 8.390158653259277
Epoch 1060, val loss: 0.5224136710166931
Epoch 1070, training loss: 84.36307525634766 = 0.4624796211719513 + 10.0 * 8.390059471130371
Epoch 1070, val loss: 0.5207727551460266
Epoch 1080, training loss: 84.3785629272461 = 0.4600151777267456 + 10.0 * 8.391855239868164
Epoch 1080, val loss: 0.51907879114151
Epoch 1090, training loss: 84.34749603271484 = 0.4575777053833008 + 10.0 * 8.388991355895996
Epoch 1090, val loss: 0.5172924399375916
Epoch 1100, training loss: 84.33438110351562 = 0.45520344376564026 + 10.0 * 8.387918472290039
Epoch 1100, val loss: 0.5156270861625671
Epoch 1110, training loss: 84.32178497314453 = 0.45282429456710815 + 10.0 * 8.386896133422852
Epoch 1110, val loss: 0.5139647722244263
Epoch 1120, training loss: 84.31481170654297 = 0.4504525065422058 + 10.0 * 8.386435508728027
Epoch 1120, val loss: 0.5122879147529602
Epoch 1130, training loss: 84.30872344970703 = 0.4480823874473572 + 10.0 * 8.386064529418945
Epoch 1130, val loss: 0.5105894804000854
Epoch 1140, training loss: 84.34967803955078 = 0.44568678736686707 + 10.0 * 8.390398979187012
Epoch 1140, val loss: 0.5088379383087158
Epoch 1150, training loss: 84.31189727783203 = 0.4432615041732788 + 10.0 * 8.386863708496094
Epoch 1150, val loss: 0.5072116851806641
Epoch 1160, training loss: 84.28665924072266 = 0.44089338183403015 + 10.0 * 8.384576797485352
Epoch 1160, val loss: 0.5054180026054382
Epoch 1170, training loss: 84.27826690673828 = 0.4385301172733307 + 10.0 * 8.383974075317383
Epoch 1170, val loss: 0.503739595413208
Epoch 1180, training loss: 84.27324676513672 = 0.43616747856140137 + 10.0 * 8.383708000183105
Epoch 1180, val loss: 0.5020325183868408
Epoch 1190, training loss: 84.31362915039062 = 0.43378087878227234 + 10.0 * 8.387984275817871
Epoch 1190, val loss: 0.5003679990768433
Epoch 1200, training loss: 84.27222442626953 = 0.43135958909988403 + 10.0 * 8.384086608886719
Epoch 1200, val loss: 0.4984445869922638
Epoch 1210, training loss: 84.2541275024414 = 0.42899253964424133 + 10.0 * 8.382513046264648
Epoch 1210, val loss: 0.49674123525619507
Epoch 1220, training loss: 84.24551391601562 = 0.4266163408756256 + 10.0 * 8.381890296936035
Epoch 1220, val loss: 0.4949580729007721
Epoch 1230, training loss: 84.2664566040039 = 0.42422765493392944 + 10.0 * 8.384222984313965
Epoch 1230, val loss: 0.49322211742401123
Epoch 1240, training loss: 84.2386703491211 = 0.42182856798171997 + 10.0 * 8.381684303283691
Epoch 1240, val loss: 0.49139484763145447
Epoch 1250, training loss: 84.22827911376953 = 0.4194453954696655 + 10.0 * 8.38088321685791
Epoch 1250, val loss: 0.4896019399166107
Epoch 1260, training loss: 84.21796417236328 = 0.4170810282230377 + 10.0 * 8.380087852478027
Epoch 1260, val loss: 0.4878372848033905
Epoch 1270, training loss: 84.21148681640625 = 0.4147048592567444 + 10.0 * 8.379678726196289
Epoch 1270, val loss: 0.48606669902801514
Epoch 1280, training loss: 84.21263122558594 = 0.41232559084892273 + 10.0 * 8.380030632019043
Epoch 1280, val loss: 0.4842834174633026
Epoch 1290, training loss: 84.229736328125 = 0.40991681814193726 + 10.0 * 8.38198184967041
Epoch 1290, val loss: 0.48254501819610596
Epoch 1300, training loss: 84.19819641113281 = 0.40751948952674866 + 10.0 * 8.379068374633789
Epoch 1300, val loss: 0.4805821180343628
Epoch 1310, training loss: 84.18746948242188 = 0.4051574766635895 + 10.0 * 8.378231048583984
Epoch 1310, val loss: 0.47889000177383423
Epoch 1320, training loss: 84.18192291259766 = 0.4027855396270752 + 10.0 * 8.377913475036621
Epoch 1320, val loss: 0.4771062433719635
Epoch 1330, training loss: 84.17665100097656 = 0.40042367577552795 + 10.0 * 8.377622604370117
Epoch 1330, val loss: 0.4753261208534241
Epoch 1340, training loss: 84.1993637084961 = 0.3980485796928406 + 10.0 * 8.380131721496582
Epoch 1340, val loss: 0.4735322892665863
Epoch 1350, training loss: 84.17206573486328 = 0.3956582546234131 + 10.0 * 8.377640724182129
Epoch 1350, val loss: 0.47173747420310974
Epoch 1360, training loss: 84.16093444824219 = 0.3932971656322479 + 10.0 * 8.376764297485352
Epoch 1360, val loss: 0.470007985830307
Epoch 1370, training loss: 84.15272521972656 = 0.39094701409339905 + 10.0 * 8.376177787780762
Epoch 1370, val loss: 0.4682287573814392
Epoch 1380, training loss: 84.14656829833984 = 0.388606995344162 + 10.0 * 8.3757963180542
Epoch 1380, val loss: 0.4665409028530121
Epoch 1390, training loss: 84.15939331054688 = 0.38626912236213684 + 10.0 * 8.377312660217285
Epoch 1390, val loss: 0.4648706912994385
Epoch 1400, training loss: 84.16255950927734 = 0.38386762142181396 + 10.0 * 8.377869606018066
Epoch 1400, val loss: 0.4631243944168091
Epoch 1410, training loss: 84.14513397216797 = 0.38155579566955566 + 10.0 * 8.376358032226562
Epoch 1410, val loss: 0.46136555075645447
Epoch 1420, training loss: 84.12779998779297 = 0.37928491830825806 + 10.0 * 8.37485122680664
Epoch 1420, val loss: 0.45976507663726807
Epoch 1430, training loss: 84.12100219726562 = 0.37702232599258423 + 10.0 * 8.374398231506348
Epoch 1430, val loss: 0.4581427574157715
Epoch 1440, training loss: 84.11553192138672 = 0.3747805058956146 + 10.0 * 8.374074935913086
Epoch 1440, val loss: 0.4565465748310089
Epoch 1450, training loss: 84.11016082763672 = 0.3725472092628479 + 10.0 * 8.373761177062988
Epoch 1450, val loss: 0.45497092604637146
Epoch 1460, training loss: 84.10506439208984 = 0.3703352212905884 + 10.0 * 8.373472213745117
Epoch 1460, val loss: 0.45343950390815735
Epoch 1470, training loss: 84.10074615478516 = 0.36813464760780334 + 10.0 * 8.373261451721191
Epoch 1470, val loss: 0.4518866240978241
Epoch 1480, training loss: 84.1148452758789 = 0.36595019698143005 + 10.0 * 8.374889373779297
Epoch 1480, val loss: 0.45037034153938293
Epoch 1490, training loss: 84.13378143310547 = 0.3637566864490509 + 10.0 * 8.377002716064453
Epoch 1490, val loss: 0.44883307814598083
Epoch 1500, training loss: 84.09627532958984 = 0.36158105731010437 + 10.0 * 8.373469352722168
Epoch 1500, val loss: 0.44734492897987366
Epoch 1510, training loss: 84.08807373046875 = 0.359479159116745 + 10.0 * 8.372859001159668
Epoch 1510, val loss: 0.44597357511520386
Epoch 1520, training loss: 84.08197784423828 = 0.3573959171772003 + 10.0 * 8.372457504272461
Epoch 1520, val loss: 0.44455796480178833
Epoch 1530, training loss: 84.07909393310547 = 0.35534024238586426 + 10.0 * 8.37237548828125
Epoch 1530, val loss: 0.44319456815719604
Epoch 1540, training loss: 84.07219696044922 = 0.35330888628959656 + 10.0 * 8.371889114379883
Epoch 1540, val loss: 0.44184720516204834
Epoch 1550, training loss: 84.06519317626953 = 0.35129812359809875 + 10.0 * 8.371389389038086
Epoch 1550, val loss: 0.44052737951278687
Epoch 1560, training loss: 84.06504821777344 = 0.34931501746177673 + 10.0 * 8.371573448181152
Epoch 1560, val loss: 0.4392543435096741
Epoch 1570, training loss: 84.08040618896484 = 0.34734347462654114 + 10.0 * 8.373306274414062
Epoch 1570, val loss: 0.43803897500038147
Epoch 1580, training loss: 84.0528335571289 = 0.3454085886478424 + 10.0 * 8.370742797851562
Epoch 1580, val loss: 0.4367136061191559
Epoch 1590, training loss: 84.04853057861328 = 0.34350651502609253 + 10.0 * 8.370502471923828
Epoch 1590, val loss: 0.4355752170085907
Epoch 1600, training loss: 84.04527282714844 = 0.3415999114513397 + 10.0 * 8.370367050170898
Epoch 1600, val loss: 0.43441829085350037
Epoch 1610, training loss: 84.0617904663086 = 0.33970844745635986 + 10.0 * 8.372208595275879
Epoch 1610, val loss: 0.433326780796051
Epoch 1620, training loss: 84.04489135742188 = 0.3378124237060547 + 10.0 * 8.370707511901855
Epoch 1620, val loss: 0.43209946155548096
Epoch 1630, training loss: 84.03490447998047 = 0.335978239774704 + 10.0 * 8.369893074035645
Epoch 1630, val loss: 0.43106767535209656
Epoch 1640, training loss: 84.02706909179688 = 0.33415403962135315 + 10.0 * 8.369291305541992
Epoch 1640, val loss: 0.42995935678482056
Epoch 1650, training loss: 84.02584838867188 = 0.3323523998260498 + 10.0 * 8.369349479675293
Epoch 1650, val loss: 0.4289652109146118
Epoch 1660, training loss: 84.05476379394531 = 0.3305581510066986 + 10.0 * 8.372420310974121
Epoch 1660, val loss: 0.42800310254096985
Epoch 1670, training loss: 84.01800537109375 = 0.32877105474472046 + 10.0 * 8.36892318725586
Epoch 1670, val loss: 0.42685651779174805
Epoch 1680, training loss: 84.00782012939453 = 0.32702696323394775 + 10.0 * 8.36807918548584
Epoch 1680, val loss: 0.42594102025032043
Epoch 1690, training loss: 84.00543212890625 = 0.32530999183654785 + 10.0 * 8.368012428283691
Epoch 1690, val loss: 0.4250437617301941
Epoch 1700, training loss: 84.00404357910156 = 0.32360783219337463 + 10.0 * 8.368043899536133
Epoch 1700, val loss: 0.4241308867931366
Epoch 1710, training loss: 84.02877044677734 = 0.3219122588634491 + 10.0 * 8.370685577392578
Epoch 1710, val loss: 0.42330440878868103
Epoch 1720, training loss: 84.00713348388672 = 0.3202579617500305 + 10.0 * 8.368687629699707
Epoch 1720, val loss: 0.4225076138973236
Epoch 1730, training loss: 83.99014282226562 = 0.3186127841472626 + 10.0 * 8.36715316772461
Epoch 1730, val loss: 0.42162469029426575
Epoch 1740, training loss: 83.9850845336914 = 0.3170054852962494 + 10.0 * 8.36680793762207
Epoch 1740, val loss: 0.42089033126831055
Epoch 1750, training loss: 83.98699951171875 = 0.3153950572013855 + 10.0 * 8.36716079711914
Epoch 1750, val loss: 0.4201466739177704
Epoch 1760, training loss: 84.01632690429688 = 0.3137931823730469 + 10.0 * 8.370253562927246
Epoch 1760, val loss: 0.419475257396698
Epoch 1770, training loss: 83.9837417602539 = 0.3122202754020691 + 10.0 * 8.367152214050293
Epoch 1770, val loss: 0.41864603757858276
Epoch 1780, training loss: 83.9712905883789 = 0.3106763958930969 + 10.0 * 8.366061210632324
Epoch 1780, val loss: 0.4179823100566864
Epoch 1790, training loss: 83.96382141113281 = 0.3091583549976349 + 10.0 * 8.365466117858887
Epoch 1790, val loss: 0.41735920310020447
Epoch 1800, training loss: 83.96094512939453 = 0.3076378107070923 + 10.0 * 8.365330696105957
Epoch 1800, val loss: 0.4167320132255554
Epoch 1810, training loss: 83.95726013183594 = 0.30614498257637024 + 10.0 * 8.365111351013184
Epoch 1810, val loss: 0.41614091396331787
Epoch 1820, training loss: 83.98115539550781 = 0.30467185378074646 + 10.0 * 8.367648124694824
Epoch 1820, val loss: 0.41564974188804626
Epoch 1830, training loss: 83.9705581665039 = 0.3031916618347168 + 10.0 * 8.36673641204834
Epoch 1830, val loss: 0.4150446951389313
Epoch 1840, training loss: 83.96424102783203 = 0.3017410933971405 + 10.0 * 8.366250038146973
Epoch 1840, val loss: 0.41444921493530273
Epoch 1850, training loss: 83.94271087646484 = 0.3003178536891937 + 10.0 * 8.364239692687988
Epoch 1850, val loss: 0.41395989060401917
Epoch 1860, training loss: 83.93821716308594 = 0.29891493916511536 + 10.0 * 8.363930702209473
Epoch 1860, val loss: 0.4134571850299835
Epoch 1870, training loss: 83.93708801269531 = 0.29752328991889954 + 10.0 * 8.363956451416016
Epoch 1870, val loss: 0.412982314825058
Epoch 1880, training loss: 83.93950653076172 = 0.29613810777664185 + 10.0 * 8.364336967468262
Epoch 1880, val loss: 0.41253283619880676
Epoch 1890, training loss: 83.92942810058594 = 0.2947704493999481 + 10.0 * 8.363466262817383
Epoch 1890, val loss: 0.41214442253112793
Epoch 1900, training loss: 83.96454620361328 = 0.293416827917099 + 10.0 * 8.36711311340332
Epoch 1900, val loss: 0.4118143916130066
Epoch 1910, training loss: 83.92752075195312 = 0.29204291105270386 + 10.0 * 8.363547325134277
Epoch 1910, val loss: 0.4112444221973419
Epoch 1920, training loss: 83.91899108886719 = 0.29072239995002747 + 10.0 * 8.362826347351074
Epoch 1920, val loss: 0.41096606850624084
Epoch 1930, training loss: 83.91183471679688 = 0.28940385580062866 + 10.0 * 8.362242698669434
Epoch 1930, val loss: 0.41060870885849
Epoch 1940, training loss: 83.9127197265625 = 0.28809747099876404 + 10.0 * 8.362462043762207
Epoch 1940, val loss: 0.41027122735977173
Epoch 1950, training loss: 83.92987823486328 = 0.2868054509162903 + 10.0 * 8.364307403564453
Epoch 1950, val loss: 0.41008222103118896
Epoch 1960, training loss: 83.90315246582031 = 0.2855165898799896 + 10.0 * 8.361763954162598
Epoch 1960, val loss: 0.4097197651863098
Epoch 1970, training loss: 83.89781951904297 = 0.2842792868614197 + 10.0 * 8.361353874206543
Epoch 1970, val loss: 0.4095226526260376
Epoch 1980, training loss: 83.89491271972656 = 0.28303584456443787 + 10.0 * 8.361187934875488
Epoch 1980, val loss: 0.40931251645088196
Epoch 1990, training loss: 83.92179870605469 = 0.28179875016212463 + 10.0 * 8.36400032043457
Epoch 1990, val loss: 0.40914613008499146
Epoch 2000, training loss: 83.88839721679688 = 0.28052958846092224 + 10.0 * 8.360786437988281
Epoch 2000, val loss: 0.4088766276836395
Epoch 2010, training loss: 83.88606262207031 = 0.2792949378490448 + 10.0 * 8.360676765441895
Epoch 2010, val loss: 0.4086562395095825
Epoch 2020, training loss: 83.88133239746094 = 0.27809274196624756 + 10.0 * 8.360323905944824
Epoch 2020, val loss: 0.40857475996017456
Epoch 2030, training loss: 83.87638092041016 = 0.2768982946872711 + 10.0 * 8.35994815826416
Epoch 2030, val loss: 0.4084785580635071
Epoch 2040, training loss: 83.87225341796875 = 0.2757198214530945 + 10.0 * 8.35965347290039
Epoch 2040, val loss: 0.40840771794319153
Epoch 2050, training loss: 83.87030792236328 = 0.274555504322052 + 10.0 * 8.359575271606445
Epoch 2050, val loss: 0.4083649516105652
Epoch 2060, training loss: 83.9073486328125 = 0.2733907401561737 + 10.0 * 8.363395690917969
Epoch 2060, val loss: 0.4083767235279083
Epoch 2070, training loss: 83.88291931152344 = 0.2722181975841522 + 10.0 * 8.36107063293457
Epoch 2070, val loss: 0.4083067774772644
Epoch 2080, training loss: 83.86998748779297 = 0.2710579037666321 + 10.0 * 8.359892845153809
Epoch 2080, val loss: 0.4082496166229248
Epoch 2090, training loss: 83.85771179199219 = 0.26990756392478943 + 10.0 * 8.358780860900879
Epoch 2090, val loss: 0.4081944525241852
Epoch 2100, training loss: 83.85367584228516 = 0.2687877118587494 + 10.0 * 8.358488082885742
Epoch 2100, val loss: 0.40811824798583984
Epoch 2110, training loss: 83.85118865966797 = 0.26766911149024963 + 10.0 * 8.358351707458496
Epoch 2110, val loss: 0.4080997407436371
Epoch 2120, training loss: 83.8901138305664 = 0.26654672622680664 + 10.0 * 8.362356185913086
Epoch 2120, val loss: 0.40799155831336975
Epoch 2130, training loss: 83.8844985961914 = 0.26546356081962585 + 10.0 * 8.361903190612793
Epoch 2130, val loss: 0.40823182463645935
Epoch 2140, training loss: 83.8450927734375 = 0.26433995366096497 + 10.0 * 8.358075141906738
Epoch 2140, val loss: 0.40821486711502075
Epoch 2150, training loss: 83.83951568603516 = 0.26326873898506165 + 10.0 * 8.357625007629395
Epoch 2150, val loss: 0.40826094150543213
Epoch 2160, training loss: 83.8343505859375 = 0.26220008730888367 + 10.0 * 8.35721492767334
Epoch 2160, val loss: 0.40835481882095337
Epoch 2170, training loss: 83.83250427246094 = 0.2611353099346161 + 10.0 * 8.357136726379395
Epoch 2170, val loss: 0.40838325023651123
Epoch 2180, training loss: 83.85076141357422 = 0.2600696086883545 + 10.0 * 8.359068870544434
Epoch 2180, val loss: 0.4084556996822357
Epoch 2190, training loss: 83.82966613769531 = 0.2589954733848572 + 10.0 * 8.357067108154297
Epoch 2190, val loss: 0.40858644247055054
Epoch 2200, training loss: 83.83307647705078 = 0.2579508125782013 + 10.0 * 8.357512474060059
Epoch 2200, val loss: 0.4086909294128418
Epoch 2210, training loss: 83.83295440673828 = 0.2569141387939453 + 10.0 * 8.357604026794434
Epoch 2210, val loss: 0.4087805449962616
Epoch 2220, training loss: 83.81535339355469 = 0.2558879256248474 + 10.0 * 8.35594654083252
Epoch 2220, val loss: 0.4089862108230591
Epoch 2230, training loss: 83.81372833251953 = 0.2548788785934448 + 10.0 * 8.35588550567627
Epoch 2230, val loss: 0.40915802121162415
Epoch 2240, training loss: 83.81365203857422 = 0.25388103723526 + 10.0 * 8.355977058410645
Epoch 2240, val loss: 0.409383624792099
Epoch 2250, training loss: 83.81847381591797 = 0.2528878152370453 + 10.0 * 8.356557846069336
Epoch 2250, val loss: 0.40961989760398865
Epoch 2260, training loss: 83.8332748413086 = 0.2518911063671112 + 10.0 * 8.358138084411621
Epoch 2260, val loss: 0.4098142981529236
Epoch 2270, training loss: 83.80546569824219 = 0.250921368598938 + 10.0 * 8.355454444885254
Epoch 2270, val loss: 0.41011351346969604
Epoch 2280, training loss: 83.80311584472656 = 0.24994368851184845 + 10.0 * 8.355317115783691
Epoch 2280, val loss: 0.41032087802886963
Epoch 2290, training loss: 83.83048248291016 = 0.2489771842956543 + 10.0 * 8.358150482177734
Epoch 2290, val loss: 0.4106886684894562
Epoch 2300, training loss: 83.79241180419922 = 0.24797621369361877 + 10.0 * 8.354443550109863
Epoch 2300, val loss: 0.410773366689682
Epoch 2310, training loss: 83.78984832763672 = 0.24702045321464539 + 10.0 * 8.354283332824707
Epoch 2310, val loss: 0.4110903739929199
Epoch 2320, training loss: 83.78604888916016 = 0.24607278406620026 + 10.0 * 8.353998184204102
Epoch 2320, val loss: 0.4113890528678894
Epoch 2330, training loss: 83.78225708007812 = 0.24512243270874023 + 10.0 * 8.353713035583496
Epoch 2330, val loss: 0.41162437200546265
Epoch 2340, training loss: 83.78276824951172 = 0.24417489767074585 + 10.0 * 8.353859901428223
Epoch 2340, val loss: 0.41189441084861755
Epoch 2350, training loss: 83.80609130859375 = 0.2432214319705963 + 10.0 * 8.356287002563477
Epoch 2350, val loss: 0.4121909737586975
Epoch 2360, training loss: 83.79544830322266 = 0.24227042496204376 + 10.0 * 8.355318069458008
Epoch 2360, val loss: 0.4125451147556305
Epoch 2370, training loss: 83.77793884277344 = 0.2413322776556015 + 10.0 * 8.353660583496094
Epoch 2370, val loss: 0.4127771258354187
Epoch 2380, training loss: 83.76905822753906 = 0.24039585888385773 + 10.0 * 8.352866172790527
Epoch 2380, val loss: 0.4129694104194641
Epoch 2390, training loss: 83.7674560546875 = 0.2394673377275467 + 10.0 * 8.352799415588379
Epoch 2390, val loss: 0.41320011019706726
Epoch 2400, training loss: 83.7821044921875 = 0.2385542243719101 + 10.0 * 8.354354858398438
Epoch 2400, val loss: 0.41345295310020447
Epoch 2410, training loss: 83.77287292480469 = 0.23765233159065247 + 10.0 * 8.353521347045898
Epoch 2410, val loss: 0.4137946665287018
Epoch 2420, training loss: 83.76148986816406 = 0.23677124083042145 + 10.0 * 8.352472305297852
Epoch 2420, val loss: 0.4141072630882263
Epoch 2430, training loss: 83.7573471069336 = 0.23587745428085327 + 10.0 * 8.352147102355957
Epoch 2430, val loss: 0.4143134355545044
Epoch 2440, training loss: 83.75390625 = 0.23500080406665802 + 10.0 * 8.351890563964844
Epoch 2440, val loss: 0.41458746790885925
Epoch 2450, training loss: 83.77491760253906 = 0.23411695659160614 + 10.0 * 8.354080200195312
Epoch 2450, val loss: 0.4149589538574219
Epoch 2460, training loss: 83.75669860839844 = 0.23323382437229156 + 10.0 * 8.352346420288086
Epoch 2460, val loss: 0.41526469588279724
Epoch 2470, training loss: 83.74984741210938 = 0.23234768211841583 + 10.0 * 8.351750373840332
Epoch 2470, val loss: 0.4155312478542328
Epoch 2480, training loss: 83.7666244506836 = 0.23148316144943237 + 10.0 * 8.353513717651367
Epoch 2480, val loss: 0.4158702492713928
Epoch 2490, training loss: 83.74213409423828 = 0.2306242734193802 + 10.0 * 8.351151466369629
Epoch 2490, val loss: 0.4162129759788513
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8396752917300863
0.8640150691878578
The final CL Acc:0.84830, 0.00627, The final GNN Acc:0.86373, 0.00057
Begin epxeriment: noisy_level: 0.2 cont_weight: 10 epoch: der1:0.3 der2:0.2 dfr1:0.4 dfr2:0.5
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=2500, cl_num_layers=2, cl_num_proj_hidden=64, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=10.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.2, drop_feat_rate_1=0.4, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=32, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, noisy_level=0.2, num_hidden=64, num_proj_hidden=64, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=40, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 88648])
add edge: torch.Size([2, 106368])
remove edge: torch.Size([2, 70872])
updated graph: torch.Size([2, 88592])
=== Noisy graph ===
/home/project-robust-contrastive/Robust-Contrastive-Learning/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 106.94590759277344 = 1.122771143913269 + 10.0 * 10.582313537597656
Epoch 0, val loss: 1.1214940547943115
Epoch 10, training loss: 106.93827819824219 = 1.1165885925292969 + 10.0 * 10.582169532775879
Epoch 10, val loss: 1.1153340339660645
Epoch 20, training loss: 106.9260482788086 = 1.1099038124084473 + 10.0 * 10.58161449432373
Epoch 20, val loss: 1.108650803565979
Epoch 30, training loss: 106.89414978027344 = 1.1024361848831177 + 10.0 * 10.579171180725098
Epoch 30, val loss: 1.1011826992034912
Epoch 40, training loss: 106.77667999267578 = 1.0939922332763672 + 10.0 * 10.568268775939941
Epoch 40, val loss: 1.092752456665039
Epoch 50, training loss: 106.3580322265625 = 1.0845601558685303 + 10.0 * 10.527347564697266
Epoch 50, val loss: 1.0834062099456787
Epoch 60, training loss: 105.0537109375 = 1.074815034866333 + 10.0 * 10.397890090942383
Epoch 60, val loss: 1.0739332437515259
Epoch 70, training loss: 101.89218139648438 = 1.0656996965408325 + 10.0 * 10.082648277282715
Epoch 70, val loss: 1.0649865865707397
Epoch 80, training loss: 98.85166931152344 = 1.0573253631591797 + 10.0 * 9.779434204101562
Epoch 80, val loss: 1.0569708347320557
Epoch 90, training loss: 97.38251495361328 = 1.0506365299224854 + 10.0 * 9.633188247680664
Epoch 90, val loss: 1.0506553649902344
Epoch 100, training loss: 96.0589828491211 = 1.04515540599823 + 10.0 * 9.501382827758789
Epoch 100, val loss: 1.0455327033996582
Epoch 110, training loss: 94.87686157226562 = 1.040635108947754 + 10.0 * 9.383623123168945
Epoch 110, val loss: 1.0413421392440796
Epoch 120, training loss: 94.28492736816406 = 1.0363291501998901 + 10.0 * 9.324859619140625
Epoch 120, val loss: 1.037251353263855
Epoch 130, training loss: 93.6694107055664 = 1.0319774150848389 + 10.0 * 9.26374340057373
Epoch 130, val loss: 1.0331385135650635
Epoch 140, training loss: 92.86783599853516 = 1.0280776023864746 + 10.0 * 9.183976173400879
Epoch 140, val loss: 1.029465913772583
Epoch 150, training loss: 92.23375701904297 = 1.024728775024414 + 10.0 * 9.120903015136719
Epoch 150, val loss: 1.0262541770935059
Epoch 160, training loss: 91.85257720947266 = 1.021019458770752 + 10.0 * 9.083155632019043
Epoch 160, val loss: 1.022611379623413
Epoch 170, training loss: 91.44083404541016 = 1.016622543334961 + 10.0 * 9.042421340942383
Epoch 170, val loss: 1.0183402299880981
Epoch 180, training loss: 90.93331909179688 = 1.0122343301773071 + 10.0 * 8.992108345031738
Epoch 180, val loss: 1.0141808986663818
Epoch 190, training loss: 90.4501724243164 = 1.0080337524414062 + 10.0 * 8.9442138671875
Epoch 190, val loss: 1.0100996494293213
Epoch 200, training loss: 90.00845336914062 = 1.003590703010559 + 10.0 * 8.90048599243164
Epoch 200, val loss: 1.0058038234710693
Epoch 210, training loss: 89.64315032958984 = 0.9989272356033325 + 10.0 * 8.864422798156738
Epoch 210, val loss: 1.001241683959961
Epoch 220, training loss: 89.31077575683594 = 0.9940343499183655 + 10.0 * 8.831674575805664
Epoch 220, val loss: 0.9964611530303955
Epoch 230, training loss: 89.08155822753906 = 0.9887243509292603 + 10.0 * 8.809283256530762
Epoch 230, val loss: 0.9912303686141968
Epoch 240, training loss: 88.87179565429688 = 0.9828966856002808 + 10.0 * 8.78888988494873
Epoch 240, val loss: 0.9855884909629822
Epoch 250, training loss: 88.71773529052734 = 0.9767395853996277 + 10.0 * 8.774099349975586
Epoch 250, val loss: 0.9795936346054077
Epoch 260, training loss: 88.56999206542969 = 0.9701668620109558 + 10.0 * 8.759982109069824
Epoch 260, val loss: 0.9732570648193359
Epoch 270, training loss: 88.42237854003906 = 0.9632065296173096 + 10.0 * 8.745917320251465
Epoch 270, val loss: 0.9665256142616272
Epoch 280, training loss: 88.28048706054688 = 0.9558581709861755 + 10.0 * 8.732462882995605
Epoch 280, val loss: 0.9594316482543945
Epoch 290, training loss: 88.15569305419922 = 0.9480936527252197 + 10.0 * 8.720759391784668
Epoch 290, val loss: 0.9519515633583069
Epoch 300, training loss: 88.04370880126953 = 0.9398961067199707 + 10.0 * 8.710381507873535
Epoch 300, val loss: 0.9440616369247437
Epoch 310, training loss: 87.94025421142578 = 0.9312662482261658 + 10.0 * 8.700899124145508
Epoch 310, val loss: 0.935768723487854
Epoch 320, training loss: 87.86766815185547 = 0.922214686870575 + 10.0 * 8.694544792175293
Epoch 320, val loss: 0.9270716309547424
Epoch 330, training loss: 87.76351928710938 = 0.9127464890480042 + 10.0 * 8.685076713562012
Epoch 330, val loss: 0.9179938435554504
Epoch 340, training loss: 87.67645263671875 = 0.9029395580291748 + 10.0 * 8.677350997924805
Epoch 340, val loss: 0.9085851311683655
Epoch 350, training loss: 87.60914611816406 = 0.8928012251853943 + 10.0 * 8.671634674072266
Epoch 350, val loss: 0.8988795876502991
Epoch 360, training loss: 87.54347229003906 = 0.8823578357696533 + 10.0 * 8.666111946105957
Epoch 360, val loss: 0.8889010548591614
Epoch 370, training loss: 87.4800796508789 = 0.8716790080070496 + 10.0 * 8.660840034484863
Epoch 370, val loss: 0.8787268400192261
Epoch 380, training loss: 87.41741180419922 = 0.8608520030975342 + 10.0 * 8.655655860900879
Epoch 380, val loss: 0.8684472441673279
Epoch 390, training loss: 87.35536193847656 = 0.8499329686164856 + 10.0 * 8.650543212890625
Epoch 390, val loss: 0.8580692410469055
Epoch 400, training loss: 87.30728149414062 = 0.8389120697975159 + 10.0 * 8.64683723449707
Epoch 400, val loss: 0.8476212024688721
Epoch 410, training loss: 87.2454833984375 = 0.8278636932373047 + 10.0 * 8.641761779785156
Epoch 410, val loss: 0.8371389508247375
Epoch 420, training loss: 87.1813735961914 = 0.8168613314628601 + 10.0 * 8.63645076751709
Epoch 420, val loss: 0.8267219662666321
Epoch 430, training loss: 87.15242767333984 = 0.8059156537055969 + 10.0 * 8.634651184082031
Epoch 430, val loss: 0.8163521885871887
Epoch 440, training loss: 87.0784683227539 = 0.795011579990387 + 10.0 * 8.628345489501953
Epoch 440, val loss: 0.806047260761261
Epoch 450, training loss: 87.02337646484375 = 0.7842169404029846 + 10.0 * 8.623915672302246
Epoch 450, val loss: 0.7958323955535889
Epoch 460, training loss: 86.97207641601562 = 0.7735654711723328 + 10.0 * 8.619851112365723
Epoch 460, val loss: 0.7857449650764465
Epoch 470, training loss: 86.96734619140625 = 0.7630571126937866 + 10.0 * 8.620429039001465
Epoch 470, val loss: 0.7757946848869324
Epoch 480, training loss: 86.88392639160156 = 0.7527129650115967 + 10.0 * 8.613121032714844
Epoch 480, val loss: 0.7660102248191833
Epoch 490, training loss: 86.83665466308594 = 0.742612361907959 + 10.0 * 8.609403610229492
Epoch 490, val loss: 0.7564508318901062
Epoch 500, training loss: 86.79305267333984 = 0.7327520847320557 + 10.0 * 8.606030464172363
Epoch 500, val loss: 0.7471251487731934
Epoch 510, training loss: 86.7750015258789 = 0.7231290936470032 + 10.0 * 8.60518741607666
Epoch 510, val loss: 0.7379968762397766
Epoch 520, training loss: 86.73275756835938 = 0.7137452960014343 + 10.0 * 8.601901054382324
Epoch 520, val loss: 0.7291723489761353
Epoch 530, training loss: 86.68400573730469 = 0.7046838998794556 + 10.0 * 8.597932815551758
Epoch 530, val loss: 0.7206255793571472
Epoch 540, training loss: 86.6413345336914 = 0.6959357857704163 + 10.0 * 8.594539642333984
Epoch 540, val loss: 0.7123734951019287
Epoch 550, training loss: 86.60388946533203 = 0.6874958276748657 + 10.0 * 8.591639518737793
Epoch 550, val loss: 0.7044215202331543
Epoch 560, training loss: 86.59794616699219 = 0.6793820858001709 + 10.0 * 8.591856002807617
Epoch 560, val loss: 0.6968012452125549
Epoch 570, training loss: 86.54529571533203 = 0.6715644598007202 + 10.0 * 8.587373733520508
Epoch 570, val loss: 0.6894426941871643
Epoch 580, training loss: 86.50483703613281 = 0.6641188263893127 + 10.0 * 8.58407211303711
Epoch 580, val loss: 0.6824736595153809
Epoch 590, training loss: 86.47100830078125 = 0.6570111513137817 + 10.0 * 8.581399917602539
Epoch 590, val loss: 0.6758435368537903
Epoch 600, training loss: 86.46894836425781 = 0.6502097249031067 + 10.0 * 8.581873893737793
Epoch 600, val loss: 0.6695050001144409
Epoch 610, training loss: 86.41753387451172 = 0.6437214016914368 + 10.0 * 8.577381134033203
Epoch 610, val loss: 0.6635131239891052
Epoch 620, training loss: 86.38227081298828 = 0.6375594735145569 + 10.0 * 8.574471473693848
Epoch 620, val loss: 0.6578207612037659
Epoch 630, training loss: 86.35443878173828 = 0.6316934823989868 + 10.0 * 8.572275161743164
Epoch 630, val loss: 0.6524229645729065
Epoch 640, training loss: 86.32595825195312 = 0.6261090636253357 + 10.0 * 8.569985389709473
Epoch 640, val loss: 0.6473119854927063
Epoch 650, training loss: 86.30752563476562 = 0.6208107471466064 + 10.0 * 8.568671226501465
Epoch 650, val loss: 0.6425142288208008
Epoch 660, training loss: 86.272216796875 = 0.6157738566398621 + 10.0 * 8.565644264221191
Epoch 660, val loss: 0.6379271745681763
Epoch 670, training loss: 86.24650573730469 = 0.6110132336616516 + 10.0 * 8.563549041748047
Epoch 670, val loss: 0.6336510181427002
Epoch 680, training loss: 86.22782897949219 = 0.6064972877502441 + 10.0 * 8.562132835388184
Epoch 680, val loss: 0.6296324133872986
Epoch 690, training loss: 86.21330261230469 = 0.6021903157234192 + 10.0 * 8.561111450195312
Epoch 690, val loss: 0.6258389949798584
Epoch 700, training loss: 86.19100189208984 = 0.5981038808822632 + 10.0 * 8.559289932250977
Epoch 700, val loss: 0.6222870349884033
Epoch 710, training loss: 86.15819549560547 = 0.5942370295524597 + 10.0 * 8.556395530700684
Epoch 710, val loss: 0.6189199686050415
Epoch 720, training loss: 86.14208221435547 = 0.5905674695968628 + 10.0 * 8.555150985717773
Epoch 720, val loss: 0.6157497763633728
Epoch 730, training loss: 86.14717102050781 = 0.5870647430419922 + 10.0 * 8.556010246276855
Epoch 730, val loss: 0.6127700209617615
Epoch 740, training loss: 86.1093521118164 = 0.5837317109107971 + 10.0 * 8.55256175994873
Epoch 740, val loss: 0.6100515127182007
Epoch 750, training loss: 86.08700561523438 = 0.5805515646934509 + 10.0 * 8.55064582824707
Epoch 750, val loss: 0.607417106628418
Epoch 760, training loss: 86.06890106201172 = 0.5775189399719238 + 10.0 * 8.549138069152832
Epoch 760, val loss: 0.6049757599830627
Epoch 770, training loss: 86.06401062011719 = 0.5746139883995056 + 10.0 * 8.54893970489502
Epoch 770, val loss: 0.6026536226272583
Epoch 780, training loss: 86.0547103881836 = 0.5718271732330322 + 10.0 * 8.548288345336914
Epoch 780, val loss: 0.6004902720451355
Epoch 790, training loss: 86.03068542480469 = 0.5691428780555725 + 10.0 * 8.546154022216797
Epoch 790, val loss: 0.5984054207801819
Epoch 800, training loss: 86.01030731201172 = 0.5665908455848694 + 10.0 * 8.544371604919434
Epoch 800, val loss: 0.5964841246604919
Epoch 810, training loss: 85.99717712402344 = 0.5641371607780457 + 10.0 * 8.543303489685059
Epoch 810, val loss: 0.5946893692016602
Epoch 820, training loss: 85.99147033691406 = 0.5617725253105164 + 10.0 * 8.542969703674316
Epoch 820, val loss: 0.5930051803588867
Epoch 830, training loss: 85.97809600830078 = 0.5594773292541504 + 10.0 * 8.541861534118652
Epoch 830, val loss: 0.59135502576828
Epoch 840, training loss: 85.96755981445312 = 0.5572670698165894 + 10.0 * 8.54102897644043
Epoch 840, val loss: 0.5898202657699585
Epoch 850, training loss: 85.95536804199219 = 0.5551284551620483 + 10.0 * 8.540023803710938
Epoch 850, val loss: 0.5882968902587891
Epoch 860, training loss: 85.94133758544922 = 0.553078830242157 + 10.0 * 8.538825988769531
Epoch 860, val loss: 0.5869844555854797
Epoch 870, training loss: 85.92615509033203 = 0.5510871410369873 + 10.0 * 8.537507057189941
Epoch 870, val loss: 0.5856647491455078
Epoch 880, training loss: 85.91529846191406 = 0.5491641759872437 + 10.0 * 8.536613464355469
Epoch 880, val loss: 0.5844399929046631
Epoch 890, training loss: 85.92035675048828 = 0.547290027141571 + 10.0 * 8.537306785583496
Epoch 890, val loss: 0.5832725763320923
Epoch 900, training loss: 85.91132354736328 = 0.545455276966095 + 10.0 * 8.53658676147461
Epoch 900, val loss: 0.5822006464004517
Epoch 910, training loss: 85.89376068115234 = 0.5436772108078003 + 10.0 * 8.535008430480957
Epoch 910, val loss: 0.5811026096343994
Epoch 920, training loss: 85.877685546875 = 0.5419527292251587 + 10.0 * 8.533573150634766
Epoch 920, val loss: 0.5801519155502319
Epoch 930, training loss: 85.86878967285156 = 0.5402739644050598 + 10.0 * 8.532851219177246
Epoch 930, val loss: 0.5792129039764404
Epoch 940, training loss: 85.88333129882812 = 0.5386300683021545 + 10.0 * 8.534470558166504
Epoch 940, val loss: 0.5783119797706604
Epoch 950, training loss: 85.86190795898438 = 0.5370028018951416 + 10.0 * 8.532490730285645
Epoch 950, val loss: 0.5773875117301941
Epoch 960, training loss: 85.85161590576172 = 0.535423755645752 + 10.0 * 8.53161907196045
Epoch 960, val loss: 0.5766273140907288
Epoch 970, training loss: 85.83577728271484 = 0.5338882207870483 + 10.0 * 8.53018856048584
Epoch 970, val loss: 0.575829267501831
Epoch 980, training loss: 85.82208251953125 = 0.5323817133903503 + 10.0 * 8.528970718383789
Epoch 980, val loss: 0.5750983357429504
Epoch 990, training loss: 85.82295227050781 = 0.530903697013855 + 10.0 * 8.529204368591309
Epoch 990, val loss: 0.5743983387947083
Epoch 1000, training loss: 85.8089828491211 = 0.5294468402862549 + 10.0 * 8.527953147888184
Epoch 1000, val loss: 0.5736627578735352
Epoch 1010, training loss: 85.79779815673828 = 0.5280187726020813 + 10.0 * 8.526978492736816
Epoch 1010, val loss: 0.573055624961853
Epoch 1020, training loss: 85.79197692871094 = 0.5266333818435669 + 10.0 * 8.526534080505371
Epoch 1020, val loss: 0.5724036693572998
Epoch 1030, training loss: 85.7894058227539 = 0.5252627730369568 + 10.0 * 8.52641487121582
Epoch 1030, val loss: 0.5718014240264893
Epoch 1040, training loss: 85.77030944824219 = 0.5239067673683167 + 10.0 * 8.524640083312988
Epoch 1040, val loss: 0.5711817145347595
Epoch 1050, training loss: 85.75505065917969 = 0.5225936770439148 + 10.0 * 8.523245811462402
Epoch 1050, val loss: 0.5706056356430054
Epoch 1060, training loss: 85.75572967529297 = 0.5212944746017456 + 10.0 * 8.523443222045898
Epoch 1060, val loss: 0.5700519680976868
Epoch 1070, training loss: 85.76566314697266 = 0.5199967622756958 + 10.0 * 8.524566650390625
Epoch 1070, val loss: 0.569512665271759
Epoch 1080, training loss: 85.73796081542969 = 0.5187293887138367 + 10.0 * 8.521923065185547
Epoch 1080, val loss: 0.569036602973938
Epoch 1090, training loss: 85.71923065185547 = 0.5174765586853027 + 10.0 * 8.520174980163574
Epoch 1090, val loss: 0.5685253739356995
Epoch 1100, training loss: 85.70478057861328 = 0.5162549018859863 + 10.0 * 8.518852233886719
Epoch 1100, val loss: 0.5680745244026184
Epoch 1110, training loss: 85.73224639892578 = 0.515040934085846 + 10.0 * 8.521720886230469
Epoch 1110, val loss: 0.5676199197769165
Epoch 1120, training loss: 85.70983123779297 = 0.5138319730758667 + 10.0 * 8.519599914550781
Epoch 1120, val loss: 0.5671500563621521
Epoch 1130, training loss: 85.67424774169922 = 0.512630045413971 + 10.0 * 8.516161918640137
Epoch 1130, val loss: 0.5667048692703247
Epoch 1140, training loss: 85.6644058227539 = 0.5114538669586182 + 10.0 * 8.515295028686523
Epoch 1140, val loss: 0.5662698745727539
Epoch 1150, training loss: 85.65351104736328 = 0.5102937817573547 + 10.0 * 8.514322280883789
Epoch 1150, val loss: 0.5658463835716248
Epoch 1160, training loss: 85.64498138427734 = 0.5091431736946106 + 10.0 * 8.51358413696289
Epoch 1160, val loss: 0.565460741519928
Epoch 1170, training loss: 85.70225524902344 = 0.5080018639564514 + 10.0 * 8.519425392150879
Epoch 1170, val loss: 0.5651021599769592
Epoch 1180, training loss: 85.62486267089844 = 0.506824254989624 + 10.0 * 8.511804580688477
Epoch 1180, val loss: 0.5645900368690491
Epoch 1190, training loss: 85.6214828491211 = 0.5056840181350708 + 10.0 * 8.511579513549805
Epoch 1190, val loss: 0.564175546169281
Epoch 1200, training loss: 85.60604095458984 = 0.5045677423477173 + 10.0 * 8.510147094726562
Epoch 1200, val loss: 0.5637978315353394
Epoch 1210, training loss: 85.61872100830078 = 0.5034548044204712 + 10.0 * 8.511526107788086
Epoch 1210, val loss: 0.563417911529541
Epoch 1220, training loss: 85.60811614990234 = 0.5023252964019775 + 10.0 * 8.510579109191895
Epoch 1220, val loss: 0.5630138516426086
Epoch 1230, training loss: 85.58817291259766 = 0.5012044310569763 + 10.0 * 8.508696556091309
Epoch 1230, val loss: 0.5625917911529541
Epoch 1240, training loss: 85.57740020751953 = 0.5001097917556763 + 10.0 * 8.507729530334473
Epoch 1240, val loss: 0.5622211694717407
Epoch 1250, training loss: 85.56358337402344 = 0.4990275800228119 + 10.0 * 8.506455421447754
Epoch 1250, val loss: 0.5618942379951477
Epoch 1260, training loss: 85.55635833740234 = 0.49794769287109375 + 10.0 * 8.505841255187988
Epoch 1260, val loss: 0.5615366101264954
Epoch 1270, training loss: 85.54798889160156 = 0.49686816334724426 + 10.0 * 8.505111694335938
Epoch 1270, val loss: 0.5611651539802551
Epoch 1280, training loss: 85.54837036132812 = 0.495792955160141 + 10.0 * 8.505257606506348
Epoch 1280, val loss: 0.560791015625
Epoch 1290, training loss: 85.5464859008789 = 0.4946952164173126 + 10.0 * 8.505178451538086
Epoch 1290, val loss: 0.5604767799377441
Epoch 1300, training loss: 85.52666473388672 = 0.4936008155345917 + 10.0 * 8.50330638885498
Epoch 1300, val loss: 0.5600562691688538
Epoch 1310, training loss: 85.51946258544922 = 0.4925217926502228 + 10.0 * 8.502694129943848
Epoch 1310, val loss: 0.559678316116333
Epoch 1320, training loss: 85.51167297363281 = 0.4914480745792389 + 10.0 * 8.502022743225098
Epoch 1320, val loss: 0.5593271851539612
Epoch 1330, training loss: 85.5046157836914 = 0.4903765916824341 + 10.0 * 8.501423835754395
Epoch 1330, val loss: 0.5589540004730225
Epoch 1340, training loss: 85.52001953125 = 0.4893043637275696 + 10.0 * 8.503071784973145
Epoch 1340, val loss: 0.55854731798172
Epoch 1350, training loss: 85.51101684570312 = 0.4882098138332367 + 10.0 * 8.502280235290527
Epoch 1350, val loss: 0.5582756400108337
Epoch 1360, training loss: 85.49031829833984 = 0.48713043332099915 + 10.0 * 8.50031852722168
Epoch 1360, val loss: 0.5578685998916626
Epoch 1370, training loss: 85.47941589355469 = 0.486051470041275 + 10.0 * 8.499336242675781
Epoch 1370, val loss: 0.5574886202812195
Epoch 1380, training loss: 85.47660827636719 = 0.48497235774993896 + 10.0 * 8.499163627624512
Epoch 1380, val loss: 0.5571699142456055
Epoch 1390, training loss: 85.5058822631836 = 0.4838767647743225 + 10.0 * 8.50220012664795
Epoch 1390, val loss: 0.5568028092384338
Epoch 1400, training loss: 85.46216583251953 = 0.48278096318244934 + 10.0 * 8.49793815612793
Epoch 1400, val loss: 0.556438684463501
Epoch 1410, training loss: 85.45048522949219 = 0.481687992811203 + 10.0 * 8.496879577636719
Epoch 1410, val loss: 0.556061863899231
Epoch 1420, training loss: 85.44413757324219 = 0.48059821128845215 + 10.0 * 8.496354103088379
Epoch 1420, val loss: 0.5557131767272949
Epoch 1430, training loss: 85.44026184082031 = 0.47950562834739685 + 10.0 * 8.496075630187988
Epoch 1430, val loss: 0.5553354024887085
Epoch 1440, training loss: 85.48919677734375 = 0.4783974885940552 + 10.0 * 8.501079559326172
Epoch 1440, val loss: 0.5549362301826477
Epoch 1450, training loss: 85.43109130859375 = 0.4772786498069763 + 10.0 * 8.495381355285645
Epoch 1450, val loss: 0.5545803308486938
Epoch 1460, training loss: 85.42167663574219 = 0.4761662185192108 + 10.0 * 8.494550704956055
Epoch 1460, val loss: 0.5542053580284119
Epoch 1470, training loss: 85.41410827636719 = 0.4750521183013916 + 10.0 * 8.493906021118164
Epoch 1470, val loss: 0.5538102984428406
Epoch 1480, training loss: 85.4072494506836 = 0.4739353060722351 + 10.0 * 8.493330955505371
Epoch 1480, val loss: 0.5534318089485168
Epoch 1490, training loss: 85.40135192871094 = 0.4728102385997772 + 10.0 * 8.492854118347168
Epoch 1490, val loss: 0.5530396699905396
Epoch 1500, training loss: 85.4277114868164 = 0.47167104482650757 + 10.0 * 8.495603561401367
Epoch 1500, val loss: 0.5526202917098999
Epoch 1510, training loss: 85.40908813476562 = 0.4705057442188263 + 10.0 * 8.493858337402344
Epoch 1510, val loss: 0.5522598028182983
Epoch 1520, training loss: 85.39788055419922 = 0.46933475136756897 + 10.0 * 8.492854118347168
Epoch 1520, val loss: 0.551800012588501
Epoch 1530, training loss: 85.38134002685547 = 0.4681776165962219 + 10.0 * 8.491315841674805
Epoch 1530, val loss: 0.5513922572135925
Epoch 1540, training loss: 85.3755111694336 = 0.4670194983482361 + 10.0 * 8.490849494934082
Epoch 1540, val loss: 0.5510321855545044
Epoch 1550, training loss: 85.36805725097656 = 0.4658483564853668 + 10.0 * 8.49022102355957
Epoch 1550, val loss: 0.550582766532898
Epoch 1560, training loss: 85.3627700805664 = 0.4646698236465454 + 10.0 * 8.4898099899292
Epoch 1560, val loss: 0.5501892566680908
Epoch 1570, training loss: 85.38542175292969 = 0.46347931027412415 + 10.0 * 8.492194175720215
Epoch 1570, val loss: 0.5497948527336121
Epoch 1580, training loss: 85.39344787597656 = 0.46226438879966736 + 10.0 * 8.493118286132812
Epoch 1580, val loss: 0.549253523349762
Epoch 1590, training loss: 85.35015869140625 = 0.4610420763492584 + 10.0 * 8.488911628723145
Epoch 1590, val loss: 0.5488272905349731
Epoch 1600, training loss: 85.3440933227539 = 0.459829181432724 + 10.0 * 8.488426208496094
Epoch 1600, val loss: 0.5484057068824768
Epoch 1610, training loss: 85.33891296386719 = 0.4586118459701538 + 10.0 * 8.488030433654785
Epoch 1610, val loss: 0.5479941368103027
Epoch 1620, training loss: 85.33303833007812 = 0.4573822021484375 + 10.0 * 8.487565994262695
Epoch 1620, val loss: 0.547516405582428
Epoch 1630, training loss: 85.3704605102539 = 0.45613735914230347 + 10.0 * 8.491432189941406
Epoch 1630, val loss: 0.547041118144989
Epoch 1640, training loss: 85.34607696533203 = 0.4548690915107727 + 10.0 * 8.489120483398438
Epoch 1640, val loss: 0.5465770363807678
Epoch 1650, training loss: 85.32259368896484 = 0.4535928964614868 + 10.0 * 8.486900329589844
Epoch 1650, val loss: 0.5460730791091919
Epoch 1660, training loss: 85.31273651123047 = 0.4523189663887024 + 10.0 * 8.486042022705078
Epoch 1660, val loss: 0.5456002354621887
Epoch 1670, training loss: 85.309326171875 = 0.4510384500026703 + 10.0 * 8.485828399658203
Epoch 1670, val loss: 0.5451400279998779
Epoch 1680, training loss: 85.3182144165039 = 0.4497460722923279 + 10.0 * 8.486846923828125
Epoch 1680, val loss: 0.5446257591247559
Epoch 1690, training loss: 85.33902740478516 = 0.4484330415725708 + 10.0 * 8.489059448242188
Epoch 1690, val loss: 0.5441215634346008
Epoch 1700, training loss: 85.3041000366211 = 0.44710540771484375 + 10.0 * 8.485699653625488
Epoch 1700, val loss: 0.5436626672744751
Epoch 1710, training loss: 85.29287719726562 = 0.4457719624042511 + 10.0 * 8.484710693359375
Epoch 1710, val loss: 0.5430930256843567
Epoch 1720, training loss: 85.28453063964844 = 0.4444335401058197 + 10.0 * 8.484009742736816
Epoch 1720, val loss: 0.5426396131515503
Epoch 1730, training loss: 85.27898406982422 = 0.44308075308799744 + 10.0 * 8.483591079711914
Epoch 1730, val loss: 0.5421141982078552
Epoch 1740, training loss: 85.27383422851562 = 0.4417126476764679 + 10.0 * 8.4832124710083
Epoch 1740, val loss: 0.5416115522384644
Epoch 1750, training loss: 85.27752685546875 = 0.44033345580101013 + 10.0 * 8.483719825744629
Epoch 1750, val loss: 0.5411152839660645
Epoch 1760, training loss: 85.27637481689453 = 0.43892771005630493 + 10.0 * 8.483744621276855
Epoch 1760, val loss: 0.5405778288841248
Epoch 1770, training loss: 85.26448059082031 = 0.43750807642936707 + 10.0 * 8.482697486877441
Epoch 1770, val loss: 0.5400334596633911
Epoch 1780, training loss: 85.2592544555664 = 0.4360887408256531 + 10.0 * 8.482316970825195
Epoch 1780, val loss: 0.5395238995552063
Epoch 1790, training loss: 85.25537109375 = 0.4346650540828705 + 10.0 * 8.482070922851562
Epoch 1790, val loss: 0.5390098690986633
Epoch 1800, training loss: 85.32242584228516 = 0.43321675062179565 + 10.0 * 8.488920211791992
Epoch 1800, val loss: 0.5384509563446045
Epoch 1810, training loss: 85.2429428100586 = 0.4317419230937958 + 10.0 * 8.481120109558105
Epoch 1810, val loss: 0.5379016995429993
Epoch 1820, training loss: 85.24436950683594 = 0.43026548624038696 + 10.0 * 8.481410026550293
Epoch 1820, val loss: 0.5373737812042236
Epoch 1830, training loss: 85.23493957519531 = 0.42878565192222595 + 10.0 * 8.480615615844727
Epoch 1830, val loss: 0.5368258357048035
Epoch 1840, training loss: 85.22859191894531 = 0.42729347944259644 + 10.0 * 8.480130195617676
Epoch 1840, val loss: 0.5363122224807739
Epoch 1850, training loss: 85.2245101928711 = 0.4257844090461731 + 10.0 * 8.479872703552246
Epoch 1850, val loss: 0.5357794165611267
Epoch 1860, training loss: 85.24771118164062 = 0.4242638945579529 + 10.0 * 8.482344627380371
Epoch 1860, val loss: 0.5352467894554138
Epoch 1870, training loss: 85.22004699707031 = 0.42271584272384644 + 10.0 * 8.47973346710205
Epoch 1870, val loss: 0.5347356200218201
Epoch 1880, training loss: 85.21876525878906 = 0.42116695642471313 + 10.0 * 8.47976016998291
Epoch 1880, val loss: 0.5342254638671875
Epoch 1890, training loss: 85.2154312133789 = 0.41960790753364563 + 10.0 * 8.479581832885742
Epoch 1890, val loss: 0.5336967706680298
Epoch 1900, training loss: 85.21524810791016 = 0.4180379807949066 + 10.0 * 8.479721069335938
Epoch 1900, val loss: 0.5331895351409912
Epoch 1910, training loss: 85.19971466064453 = 0.4164547026157379 + 10.0 * 8.478325843811035
Epoch 1910, val loss: 0.5327175855636597
Epoch 1920, training loss: 85.19754791259766 = 0.4148651659488678 + 10.0 * 8.47826862335205
Epoch 1920, val loss: 0.5322204232215881
Epoch 1930, training loss: 85.1924819946289 = 0.41326451301574707 + 10.0 * 8.477922439575195
Epoch 1930, val loss: 0.5317320823669434
Epoch 1940, training loss: 85.1928939819336 = 0.4116462767124176 + 10.0 * 8.478124618530273
Epoch 1940, val loss: 0.5312395691871643
Epoch 1950, training loss: 85.21736907958984 = 0.410012423992157 + 10.0 * 8.480735778808594
Epoch 1950, val loss: 0.5306661128997803
Epoch 1960, training loss: 85.20580291748047 = 0.40835699439048767 + 10.0 * 8.479744911193848
Epoch 1960, val loss: 0.5301348567008972
Epoch 1970, training loss: 85.18375396728516 = 0.40669679641723633 + 10.0 * 8.477705001831055
Epoch 1970, val loss: 0.5296770930290222
Epoch 1980, training loss: 85.17282104492188 = 0.40502631664276123 + 10.0 * 8.476778984069824
Epoch 1980, val loss: 0.5291324853897095
Epoch 1990, training loss: 85.16677856445312 = 0.40334734320640564 + 10.0 * 8.476343154907227
Epoch 1990, val loss: 0.5286676287651062
Epoch 2000, training loss: 85.16259765625 = 0.4016590118408203 + 10.0 * 8.476094245910645
Epoch 2000, val loss: 0.5281780362129211
Epoch 2010, training loss: 85.16465759277344 = 0.39996373653411865 + 10.0 * 8.476469039916992
Epoch 2010, val loss: 0.527732789516449
Epoch 2020, training loss: 85.19915771484375 = 0.3982526659965515 + 10.0 * 8.480090141296387
Epoch 2020, val loss: 0.5272475481033325
Epoch 2030, training loss: 85.15403747558594 = 0.39653030037879944 + 10.0 * 8.475750923156738
Epoch 2030, val loss: 0.5267887115478516
Epoch 2040, training loss: 85.15596771240234 = 0.3948146402835846 + 10.0 * 8.476115226745605
Epoch 2040, val loss: 0.5263186097145081
Epoch 2050, training loss: 85.15291595458984 = 0.39310115575790405 + 10.0 * 8.475980758666992
Epoch 2050, val loss: 0.5259067416191101
Epoch 2060, training loss: 85.14585876464844 = 0.3913780450820923 + 10.0 * 8.475447654724121
Epoch 2060, val loss: 0.5254589915275574
Epoch 2070, training loss: 85.13607788085938 = 0.38964831829071045 + 10.0 * 8.474642753601074
Epoch 2070, val loss: 0.5250287652015686
Epoch 2080, training loss: 85.13220977783203 = 0.3879067897796631 + 10.0 * 8.474430084228516
Epoch 2080, val loss: 0.5245974063873291
Epoch 2090, training loss: 85.13858032226562 = 0.3861626386642456 + 10.0 * 8.475241661071777
Epoch 2090, val loss: 0.5241676568984985
Epoch 2100, training loss: 85.14910888671875 = 0.38440772891044617 + 10.0 * 8.476469993591309
Epoch 2100, val loss: 0.5237209796905518
Epoch 2110, training loss: 85.1230697631836 = 0.38263702392578125 + 10.0 * 8.474042892456055
Epoch 2110, val loss: 0.5232637524604797
Epoch 2120, training loss: 85.11595153808594 = 0.3808794915676117 + 10.0 * 8.473506927490234
Epoch 2120, val loss: 0.5228939652442932
Epoch 2130, training loss: 85.11019134521484 = 0.3791137933731079 + 10.0 * 8.473108291625977
Epoch 2130, val loss: 0.5224804878234863
Epoch 2140, training loss: 85.1058578491211 = 0.3773438632488251 + 10.0 * 8.472851753234863
Epoch 2140, val loss: 0.5220710635185242
Epoch 2150, training loss: 85.1249008178711 = 0.37557023763656616 + 10.0 * 8.474932670593262
Epoch 2150, val loss: 0.5217583775520325
Epoch 2160, training loss: 85.10486602783203 = 0.3737616240978241 + 10.0 * 8.47311019897461
Epoch 2160, val loss: 0.5211991667747498
Epoch 2170, training loss: 85.10438537597656 = 0.3719756007194519 + 10.0 * 8.473240852355957
Epoch 2170, val loss: 0.5209783911705017
Epoch 2180, training loss: 85.09046173095703 = 0.370159775018692 + 10.0 * 8.472029685974121
Epoch 2180, val loss: 0.5205334424972534
Epoch 2190, training loss: 85.08710479736328 = 0.36836186051368713 + 10.0 * 8.471874237060547
Epoch 2190, val loss: 0.520199179649353
Epoch 2200, training loss: 85.1400375366211 = 0.3665570020675659 + 10.0 * 8.477348327636719
Epoch 2200, val loss: 0.5198848843574524
Epoch 2210, training loss: 85.09303283691406 = 0.36473166942596436 + 10.0 * 8.472829818725586
Epoch 2210, val loss: 0.5196108222007751
Epoch 2220, training loss: 85.07906341552734 = 0.3629138171672821 + 10.0 * 8.471614837646484
Epoch 2220, val loss: 0.5192874670028687
Epoch 2230, training loss: 85.07061004638672 = 0.3611093759536743 + 10.0 * 8.47095012664795
Epoch 2230, val loss: 0.5190768241882324
Epoch 2240, training loss: 85.0698013305664 = 0.35929855704307556 + 10.0 * 8.471050262451172
Epoch 2240, val loss: 0.5187609195709229
Epoch 2250, training loss: 85.10498809814453 = 0.3574850559234619 + 10.0 * 8.474750518798828
Epoch 2250, val loss: 0.518554151058197
Epoch 2260, training loss: 85.06416320800781 = 0.35567018389701843 + 10.0 * 8.470849990844727
Epoch 2260, val loss: 0.5182963609695435
Epoch 2270, training loss: 85.05219268798828 = 0.3538505434989929 + 10.0 * 8.469834327697754
Epoch 2270, val loss: 0.5180969834327698
Epoch 2280, training loss: 85.04705810546875 = 0.35204029083251953 + 10.0 * 8.469501495361328
Epoch 2280, val loss: 0.5179073810577393
Epoch 2290, training loss: 85.04270935058594 = 0.3502349257469177 + 10.0 * 8.469247817993164
Epoch 2290, val loss: 0.5177615284919739
Epoch 2300, training loss: 85.05876922607422 = 0.34842726588249207 + 10.0 * 8.471034049987793
Epoch 2300, val loss: 0.5176026821136475
Epoch 2310, training loss: 85.03944396972656 = 0.3466036915779114 + 10.0 * 8.469284057617188
Epoch 2310, val loss: 0.5174664258956909
Epoch 2320, training loss: 85.04824829101562 = 0.34479430317878723 + 10.0 * 8.470345497131348
Epoch 2320, val loss: 0.5174129605293274
Epoch 2330, training loss: 85.02742767333984 = 0.3429889380931854 + 10.0 * 8.468443870544434
Epoch 2330, val loss: 0.5173302292823792
Epoch 2340, training loss: 85.02359008789062 = 0.34118637442588806 + 10.0 * 8.468240737915039
Epoch 2340, val loss: 0.5172873735427856
Epoch 2350, training loss: 85.02105712890625 = 0.33939114212989807 + 10.0 * 8.46816635131836
Epoch 2350, val loss: 0.5172813534736633
Epoch 2360, training loss: 85.08238220214844 = 0.3376147449016571 + 10.0 * 8.47447681427002
Epoch 2360, val loss: 0.5172757506370544
Epoch 2370, training loss: 85.02849578857422 = 0.33580300211906433 + 10.0 * 8.469269752502441
Epoch 2370, val loss: 0.517368733882904
Epoch 2380, training loss: 85.00875854492188 = 0.3340167701244354 + 10.0 * 8.467473983764648
Epoch 2380, val loss: 0.5173744559288025
Epoch 2390, training loss: 85.0039291381836 = 0.33223405480384827 + 10.0 * 8.467168807983398
Epoch 2390, val loss: 0.5174964070320129
Epoch 2400, training loss: 84.9988784790039 = 0.33045023679733276 + 10.0 * 8.466842651367188
Epoch 2400, val loss: 0.5175958275794983
Epoch 2410, training loss: 85.00366973876953 = 0.32867738604545593 + 10.0 * 8.467499732971191
Epoch 2410, val loss: 0.5177959203720093
Epoch 2420, training loss: 85.02558898925781 = 0.3269036114215851 + 10.0 * 8.469868659973145
Epoch 2420, val loss: 0.5179259181022644
Epoch 2430, training loss: 84.98993682861328 = 0.3251315653324127 + 10.0 * 8.466480255126953
Epoch 2430, val loss: 0.5181102156639099
Epoch 2440, training loss: 84.9835433959961 = 0.32336971163749695 + 10.0 * 8.46601676940918
Epoch 2440, val loss: 0.5183513760566711
Epoch 2450, training loss: 84.97967529296875 = 0.32160961627960205 + 10.0 * 8.46580696105957
Epoch 2450, val loss: 0.518570065498352
Epoch 2460, training loss: 84.97676086425781 = 0.31984663009643555 + 10.0 * 8.465691566467285
Epoch 2460, val loss: 0.5188607573509216
Epoch 2470, training loss: 85.0114974975586 = 0.3180827796459198 + 10.0 * 8.469341278076172
Epoch 2470, val loss: 0.5191305875778198
Epoch 2480, training loss: 84.97238159179688 = 0.31629645824432373 + 10.0 * 8.465608596801758
Epoch 2480, val loss: 0.5194182991981506
Epoch 2490, training loss: 84.9677963256836 = 0.3145160675048828 + 10.0 * 8.465328216552734
Epoch 2490, val loss: 0.5197434425354004
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7995941146626078
0.8139534883720931
=== training gcn model ===
Epoch 0, training loss: 106.9578857421875 = 1.1348912715911865 + 10.0 * 10.582300186157227
Epoch 0, val loss: 1.1337321996688843
Epoch 10, training loss: 106.94902801513672 = 1.1278469562530518 + 10.0 * 10.582118034362793
Epoch 10, val loss: 1.1267521381378174
Epoch 20, training loss: 106.93362426757812 = 1.1205700635910034 + 10.0 * 10.581305503845215
Epoch 20, val loss: 1.1194812059402466
Epoch 30, training loss: 106.88819885253906 = 1.112708330154419 + 10.0 * 10.57754898071289
Epoch 30, val loss: 1.1116374731063843
Epoch 40, training loss: 106.71963500976562 = 1.1041853427886963 + 10.0 * 10.561544418334961
Epoch 40, val loss: 1.103152871131897
Epoch 50, training loss: 106.16991424560547 = 1.094783067703247 + 10.0 * 10.507513046264648
Epoch 50, val loss: 1.093780755996704
Epoch 60, training loss: 104.68985748291016 = 1.0847283601760864 + 10.0 * 10.360512733459473
Epoch 60, val loss: 1.0839143991470337
Epoch 70, training loss: 101.46414184570312 = 1.0746890306472778 + 10.0 * 10.038945198059082
Epoch 70, val loss: 1.0740004777908325
Epoch 80, training loss: 98.56482696533203 = 1.064893364906311 + 10.0 * 9.749993324279785
Epoch 80, val loss: 1.0647203922271729
Epoch 90, training loss: 97.12735748291016 = 1.0583906173706055 + 10.0 * 9.606897354125977
Epoch 90, val loss: 1.0585784912109375
Epoch 100, training loss: 96.03463745117188 = 1.0530526638031006 + 10.0 * 9.49815845489502
Epoch 100, val loss: 1.0533769130706787
Epoch 110, training loss: 94.95596313476562 = 1.0478157997131348 + 10.0 * 9.390814781188965
Epoch 110, val loss: 1.0482749938964844
Epoch 120, training loss: 94.29854583740234 = 1.0425968170166016 + 10.0 * 9.325594902038574
Epoch 120, val loss: 1.0431931018829346
Epoch 130, training loss: 93.9767074584961 = 1.0380066633224487 + 10.0 * 9.293869972229004
Epoch 130, val loss: 1.0387126207351685
Epoch 140, training loss: 93.58545684814453 = 1.0339901447296143 + 10.0 * 9.255146980285645
Epoch 140, val loss: 1.0347543954849243
Epoch 150, training loss: 92.96781921386719 = 1.0304145812988281 + 10.0 * 9.193740844726562
Epoch 150, val loss: 1.0312397480010986
Epoch 160, training loss: 92.08950805664062 = 1.0273456573486328 + 10.0 * 9.106216430664062
Epoch 160, val loss: 1.0282230377197266
Epoch 170, training loss: 91.22981262207031 = 1.0243529081344604 + 10.0 * 9.020545959472656
Epoch 170, val loss: 1.0252652168273926
Epoch 180, training loss: 90.69660949707031 = 1.021002173423767 + 10.0 * 8.967560768127441
Epoch 180, val loss: 1.0218833684921265
Epoch 190, training loss: 90.38862609863281 = 1.0169782638549805 + 10.0 * 8.937165260314941
Epoch 190, val loss: 1.017838716506958
Epoch 200, training loss: 90.11847686767578 = 1.012526273727417 + 10.0 * 8.910594940185547
Epoch 200, val loss: 1.0134037733078003
Epoch 210, training loss: 89.84515380859375 = 1.0079164505004883 + 10.0 * 8.883723258972168
Epoch 210, val loss: 1.0088303089141846
Epoch 220, training loss: 89.5783462524414 = 1.0031813383102417 + 10.0 * 8.857516288757324
Epoch 220, val loss: 1.004154920578003
Epoch 230, training loss: 89.344482421875 = 0.9981552362442017 + 10.0 * 8.834632873535156
Epoch 230, val loss: 0.9991304874420166
Epoch 240, training loss: 89.16668701171875 = 0.9927250146865845 + 10.0 * 8.81739616394043
Epoch 240, val loss: 0.9936825633049011
Epoch 250, training loss: 88.99882507324219 = 0.9868752956390381 + 10.0 * 8.80119514465332
Epoch 250, val loss: 0.9878325462341309
Epoch 260, training loss: 88.82321166992188 = 0.980724573135376 + 10.0 * 8.784248352050781
Epoch 260, val loss: 0.981702983379364
Epoch 270, training loss: 88.65233612060547 = 0.9742696285247803 + 10.0 * 8.767807006835938
Epoch 270, val loss: 0.9752249121665955
Epoch 280, training loss: 88.49687194824219 = 0.9674529433250427 + 10.0 * 8.752942085266113
Epoch 280, val loss: 0.9684128165245056
Epoch 290, training loss: 88.36710357666016 = 0.9602516293525696 + 10.0 * 8.74068546295166
Epoch 290, val loss: 0.9611860513687134
Epoch 300, training loss: 88.260009765625 = 0.9526156783103943 + 10.0 * 8.73073959350586
Epoch 300, val loss: 0.953498125076294
Epoch 310, training loss: 88.11992645263672 = 0.9445599913597107 + 10.0 * 8.717536926269531
Epoch 310, val loss: 0.945419430732727
Epoch 320, training loss: 87.99436950683594 = 0.9361959099769592 + 10.0 * 8.705817222595215
Epoch 320, val loss: 0.937034547328949
Epoch 330, training loss: 87.86158752441406 = 0.9275240898132324 + 10.0 * 8.69340705871582
Epoch 330, val loss: 0.9283416867256165
Epoch 340, training loss: 87.7552261352539 = 0.918495774269104 + 10.0 * 8.683672904968262
Epoch 340, val loss: 0.9192702174186707
Epoch 350, training loss: 87.63471984863281 = 0.9090889096260071 + 10.0 * 8.672563552856445
Epoch 350, val loss: 0.9098336696624756
Epoch 360, training loss: 87.52450561523438 = 0.8993351459503174 + 10.0 * 8.662516593933105
Epoch 360, val loss: 0.900099515914917
Epoch 370, training loss: 87.42000579833984 = 0.8892821669578552 + 10.0 * 8.653072357177734
Epoch 370, val loss: 0.8900386095046997
Epoch 380, training loss: 87.32389831542969 = 0.8789371252059937 + 10.0 * 8.644495964050293
Epoch 380, val loss: 0.8796819448471069
Epoch 390, training loss: 87.30147552490234 = 0.8682993054389954 + 10.0 * 8.643317222595215
Epoch 390, val loss: 0.869105339050293
Epoch 400, training loss: 87.18515014648438 = 0.8574040532112122 + 10.0 * 8.632774353027344
Epoch 400, val loss: 0.8581806421279907
Epoch 410, training loss: 87.10873413085938 = 0.8463742136955261 + 10.0 * 8.626235961914062
Epoch 410, val loss: 0.8471593260765076
Epoch 420, training loss: 87.04224395751953 = 0.835181474685669 + 10.0 * 8.620706558227539
Epoch 420, val loss: 0.8360293507575989
Epoch 430, training loss: 86.9825668334961 = 0.8238637447357178 + 10.0 * 8.615870475769043
Epoch 430, val loss: 0.8247547149658203
Epoch 440, training loss: 86.97098541259766 = 0.8124191164970398 + 10.0 * 8.615857124328613
Epoch 440, val loss: 0.8133923411369324
Epoch 450, training loss: 86.88961791992188 = 0.8008949756622314 + 10.0 * 8.608872413635254
Epoch 450, val loss: 0.8019655346870422
Epoch 460, training loss: 86.83677673339844 = 0.7893494367599487 + 10.0 * 8.604742050170898
Epoch 460, val loss: 0.7905357480049133
Epoch 470, training loss: 86.788818359375 = 0.7777994871139526 + 10.0 * 8.601101875305176
Epoch 470, val loss: 0.7791164517402649
Epoch 480, training loss: 86.74433135986328 = 0.7662394642829895 + 10.0 * 8.597808837890625
Epoch 480, val loss: 0.7677040696144104
Epoch 490, training loss: 86.75321960449219 = 0.7546938061714172 + 10.0 * 8.599852561950684
Epoch 490, val loss: 0.756339967250824
Epoch 500, training loss: 86.67390441894531 = 0.7432085275650024 + 10.0 * 8.593069076538086
Epoch 500, val loss: 0.7450472712516785
Epoch 510, training loss: 86.62222290039062 = 0.7318843603134155 + 10.0 * 8.589033126831055
Epoch 510, val loss: 0.7339624762535095
Epoch 520, training loss: 86.58097839355469 = 0.7206931710243225 + 10.0 * 8.586028099060059
Epoch 520, val loss: 0.7230274677276611
Epoch 530, training loss: 86.5403060913086 = 0.7096648812294006 + 10.0 * 8.583064079284668
Epoch 530, val loss: 0.7123029828071594
Epoch 540, training loss: 86.50563049316406 = 0.6988410949707031 + 10.0 * 8.580678939819336
Epoch 540, val loss: 0.7018360495567322
Epoch 550, training loss: 86.46420288085938 = 0.688290536403656 + 10.0 * 8.577590942382812
Epoch 550, val loss: 0.6916484832763672
Epoch 560, training loss: 86.4178695678711 = 0.678040087223053 + 10.0 * 8.573983192443848
Epoch 560, val loss: 0.681779682636261
Epoch 570, training loss: 86.37330627441406 = 0.6680888533592224 + 10.0 * 8.570521354675293
Epoch 570, val loss: 0.6722413301467896
Epoch 580, training loss: 86.3379135131836 = 0.6584551930427551 + 10.0 * 8.56794548034668
Epoch 580, val loss: 0.663008451461792
Epoch 590, training loss: 86.30194854736328 = 0.649131715297699 + 10.0 * 8.565281867980957
Epoch 590, val loss: 0.6542276740074158
Epoch 600, training loss: 86.25332641601562 = 0.6401965022087097 + 10.0 * 8.561312675476074
Epoch 600, val loss: 0.6457428932189941
Epoch 610, training loss: 86.2203369140625 = 0.6316394209861755 + 10.0 * 8.558870315551758
Epoch 610, val loss: 0.6376665830612183
Epoch 620, training loss: 86.19035339355469 = 0.6234427094459534 + 10.0 * 8.55669116973877
Epoch 620, val loss: 0.6299936771392822
Epoch 630, training loss: 86.1580810546875 = 0.6156312227249146 + 10.0 * 8.554244995117188
Epoch 630, val loss: 0.6227497458457947
Epoch 640, training loss: 86.11820220947266 = 0.6082078218460083 + 10.0 * 8.550999641418457
Epoch 640, val loss: 0.6158870458602905
Epoch 650, training loss: 86.08948516845703 = 0.6011587977409363 + 10.0 * 8.548832893371582
Epoch 650, val loss: 0.6093915700912476
Epoch 660, training loss: 86.06282043457031 = 0.5944661498069763 + 10.0 * 8.546834945678711
Epoch 660, val loss: 0.6033061742782593
Epoch 670, training loss: 86.03838348388672 = 0.5881677865982056 + 10.0 * 8.545022010803223
Epoch 670, val loss: 0.5976040363311768
Epoch 680, training loss: 86.00325012207031 = 0.5822498798370361 + 10.0 * 8.542099952697754
Epoch 680, val loss: 0.5922930836677551
Epoch 690, training loss: 85.97748565673828 = 0.5766764879226685 + 10.0 * 8.540081024169922
Epoch 690, val loss: 0.5873510837554932
Epoch 700, training loss: 85.9542465209961 = 0.5714210271835327 + 10.0 * 8.53828239440918
Epoch 700, val loss: 0.5827211141586304
Epoch 710, training loss: 85.94129180908203 = 0.5664583444595337 + 10.0 * 8.537483215332031
Epoch 710, val loss: 0.5783606171607971
Epoch 720, training loss: 85.9222640991211 = 0.5617755055427551 + 10.0 * 8.536048889160156
Epoch 720, val loss: 0.5743151903152466
Epoch 730, training loss: 85.88572692871094 = 0.5573752522468567 + 10.0 * 8.532835006713867
Epoch 730, val loss: 0.5705644488334656
Epoch 740, training loss: 85.86366271972656 = 0.5532308220863342 + 10.0 * 8.53104305267334
Epoch 740, val loss: 0.5670815110206604
Epoch 750, training loss: 85.84122467041016 = 0.5493249297142029 + 10.0 * 8.529190063476562
Epoch 750, val loss: 0.563805878162384
Epoch 760, training loss: 85.82141876220703 = 0.5456252098083496 + 10.0 * 8.527579307556152
Epoch 760, val loss: 0.5607330799102783
Epoch 770, training loss: 85.8128890991211 = 0.5421144366264343 + 10.0 * 8.527077674865723
Epoch 770, val loss: 0.557822048664093
Epoch 780, training loss: 85.81807708740234 = 0.5387501120567322 + 10.0 * 8.527933120727539
Epoch 780, val loss: 0.5552190542221069
Epoch 790, training loss: 85.77896881103516 = 0.5355581045150757 + 10.0 * 8.524340629577637
Epoch 790, val loss: 0.5526405572891235
Epoch 800, training loss: 85.76025390625 = 0.5325431823730469 + 10.0 * 8.522770881652832
Epoch 800, val loss: 0.5502510070800781
Epoch 810, training loss: 85.74362182617188 = 0.5296666026115417 + 10.0 * 8.521395683288574
Epoch 810, val loss: 0.548007607460022
Epoch 820, training loss: 85.73015594482422 = 0.526913046836853 + 10.0 * 8.520323753356934
Epoch 820, val loss: 0.5458863973617554
Epoch 830, training loss: 85.76073455810547 = 0.5242618322372437 + 10.0 * 8.52364730834961
Epoch 830, val loss: 0.5438658595085144
Epoch 840, training loss: 85.71253967285156 = 0.5217136144638062 + 10.0 * 8.519083023071289
Epoch 840, val loss: 0.541973888874054
Epoch 850, training loss: 85.69746398925781 = 0.5192752480506897 + 10.0 * 8.51781940460205
Epoch 850, val loss: 0.5401957631111145
Epoch 860, training loss: 85.68096160888672 = 0.5169377326965332 + 10.0 * 8.516402244567871
Epoch 860, val loss: 0.5384899973869324
Epoch 870, training loss: 85.6668930053711 = 0.5146813988685608 + 10.0 * 8.51522159576416
Epoch 870, val loss: 0.5368647575378418
Epoch 880, training loss: 85.67754364013672 = 0.5125017166137695 + 10.0 * 8.516504287719727
Epoch 880, val loss: 0.5353134274482727
Epoch 890, training loss: 85.65396881103516 = 0.5103759765625 + 10.0 * 8.514359474182129
Epoch 890, val loss: 0.5338354706764221
Epoch 900, training loss: 85.63803100585938 = 0.5083339810371399 + 10.0 * 8.512969970703125
Epoch 900, val loss: 0.5324231386184692
Epoch 910, training loss: 85.62002563476562 = 0.5063575506210327 + 10.0 * 8.511366844177246
Epoch 910, val loss: 0.5310772657394409
Epoch 920, training loss: 85.60821533203125 = 0.5044451951980591 + 10.0 * 8.510376930236816
Epoch 920, val loss: 0.5298031568527222
Epoch 930, training loss: 85.61681365966797 = 0.5025845766067505 + 10.0 * 8.511423110961914
Epoch 930, val loss: 0.5285919308662415
Epoch 940, training loss: 85.61112976074219 = 0.5007372498512268 + 10.0 * 8.511038780212402
Epoch 940, val loss: 0.5273104906082153
Epoch 950, training loss: 85.58245849609375 = 0.49895909428596497 + 10.0 * 8.508349418640137
Epoch 950, val loss: 0.5261801481246948
Epoch 960, training loss: 85.56755065917969 = 0.4972459375858307 + 10.0 * 8.507030487060547
Epoch 960, val loss: 0.5250729918479919
Epoch 970, training loss: 85.55506134033203 = 0.4955834448337555 + 10.0 * 8.505948066711426
Epoch 970, val loss: 0.5240128040313721
Epoch 980, training loss: 85.54447174072266 = 0.49396035075187683 + 10.0 * 8.505051612854004
Epoch 980, val loss: 0.5230072140693665
Epoch 990, training loss: 85.54570007324219 = 0.4923747777938843 + 10.0 * 8.505331993103027
Epoch 990, val loss: 0.522027313709259
Epoch 1000, training loss: 85.52649688720703 = 0.4907971918582916 + 10.0 * 8.503569602966309
Epoch 1000, val loss: 0.5210221409797668
Epoch 1010, training loss: 85.52754974365234 = 0.489265501499176 + 10.0 * 8.503828048706055
Epoch 1010, val loss: 0.5201078057289124
Epoch 1020, training loss: 85.50524139404297 = 0.4877818822860718 + 10.0 * 8.501745223999023
Epoch 1020, val loss: 0.51918625831604
Epoch 1030, training loss: 85.49777221679688 = 0.48633623123168945 + 10.0 * 8.501143455505371
Epoch 1030, val loss: 0.518341064453125
Epoch 1040, training loss: 85.51837158203125 = 0.4849171042442322 + 10.0 * 8.503345489501953
Epoch 1040, val loss: 0.517533540725708
Epoch 1050, training loss: 85.48992919921875 = 0.4834904074668884 + 10.0 * 8.500643730163574
Epoch 1050, val loss: 0.5166400671005249
Epoch 1060, training loss: 85.47056579589844 = 0.4821203052997589 + 10.0 * 8.498845100402832
Epoch 1060, val loss: 0.5158917903900146
Epoch 1070, training loss: 85.45962524414062 = 0.4807695746421814 + 10.0 * 8.497885704040527
Epoch 1070, val loss: 0.5151074528694153
Epoch 1080, training loss: 85.48908996582031 = 0.4794429540634155 + 10.0 * 8.500964164733887
Epoch 1080, val loss: 0.514397382736206
Epoch 1090, training loss: 85.44822692871094 = 0.47813042998313904 + 10.0 * 8.49700927734375
Epoch 1090, val loss: 0.5136118531227112
Epoch 1100, training loss: 85.43054962158203 = 0.47684794664382935 + 10.0 * 8.495370864868164
Epoch 1100, val loss: 0.5128795504570007
Epoch 1110, training loss: 85.4218978881836 = 0.47559216618537903 + 10.0 * 8.494630813598633
Epoch 1110, val loss: 0.51222825050354
Epoch 1120, training loss: 85.41468048095703 = 0.47435688972473145 + 10.0 * 8.49403190612793
Epoch 1120, val loss: 0.5115315914154053
Epoch 1130, training loss: 85.45941925048828 = 0.4731219708919525 + 10.0 * 8.498629570007324
Epoch 1130, val loss: 0.5108703374862671
Epoch 1140, training loss: 85.40788269042969 = 0.47189560532569885 + 10.0 * 8.493598937988281
Epoch 1140, val loss: 0.5102044939994812
Epoch 1150, training loss: 85.39009857177734 = 0.4707024097442627 + 10.0 * 8.491939544677734
Epoch 1150, val loss: 0.5095893740653992
Epoch 1160, training loss: 85.3823013305664 = 0.4695340394973755 + 10.0 * 8.491276741027832
Epoch 1160, val loss: 0.5089982151985168
Epoch 1170, training loss: 85.37580108642578 = 0.4683798849582672 + 10.0 * 8.490742683410645
Epoch 1170, val loss: 0.5084027647972107
Epoch 1180, training loss: 85.3839111328125 = 0.46722880005836487 + 10.0 * 8.491667747497559
Epoch 1180, val loss: 0.5078103542327881
Epoch 1190, training loss: 85.36415100097656 = 0.46607494354248047 + 10.0 * 8.489808082580566
Epoch 1190, val loss: 0.5072708129882812
Epoch 1200, training loss: 85.3661117553711 = 0.46492964029312134 + 10.0 * 8.490118026733398
Epoch 1200, val loss: 0.5066661238670349
Epoch 1210, training loss: 85.34908294677734 = 0.46381324529647827 + 10.0 * 8.488527297973633
Epoch 1210, val loss: 0.5061326026916504
Epoch 1220, training loss: 85.3432388305664 = 0.4627077579498291 + 10.0 * 8.488053321838379
Epoch 1220, val loss: 0.5056178569793701
Epoch 1230, training loss: 85.36071014404297 = 0.461607426404953 + 10.0 * 8.489910125732422
Epoch 1230, val loss: 0.5051329135894775
Epoch 1240, training loss: 85.3522720336914 = 0.46050381660461426 + 10.0 * 8.489176750183105
Epoch 1240, val loss: 0.5045671463012695
Epoch 1250, training loss: 85.33180236816406 = 0.459407776594162 + 10.0 * 8.487239837646484
Epoch 1250, val loss: 0.5040646195411682
Epoch 1260, training loss: 85.32181549072266 = 0.4583408832550049 + 10.0 * 8.486347198486328
Epoch 1260, val loss: 0.5035958290100098
Epoch 1270, training loss: 85.3154525756836 = 0.45728668570518494 + 10.0 * 8.485816955566406
Epoch 1270, val loss: 0.5031317472457886
Epoch 1280, training loss: 85.30896759033203 = 0.4562326967716217 + 10.0 * 8.485273361206055
Epoch 1280, val loss: 0.5026947259902954
Epoch 1290, training loss: 85.31355285644531 = 0.45518386363983154 + 10.0 * 8.48583698272705
Epoch 1290, val loss: 0.5022754669189453
Epoch 1300, training loss: 85.30049133300781 = 0.4541199803352356 + 10.0 * 8.484637260437012
Epoch 1300, val loss: 0.5017850399017334
Epoch 1310, training loss: 85.30235290527344 = 0.4530637860298157 + 10.0 * 8.484929084777832
Epoch 1310, val loss: 0.5014016032218933
Epoch 1320, training loss: 85.2900390625 = 0.4520268142223358 + 10.0 * 8.483800888061523
Epoch 1320, val loss: 0.5009366869926453
Epoch 1330, training loss: 85.28349304199219 = 0.45099735260009766 + 10.0 * 8.48324966430664
Epoch 1330, val loss: 0.5005294680595398
Epoch 1340, training loss: 85.27842712402344 = 0.44997483491897583 + 10.0 * 8.482845306396484
Epoch 1340, val loss: 0.5001518726348877
Epoch 1350, training loss: 85.2733154296875 = 0.4489513635635376 + 10.0 * 8.482436180114746
Epoch 1350, val loss: 0.49972060322761536
Epoch 1360, training loss: 85.29747009277344 = 0.4479205012321472 + 10.0 * 8.484954833984375
Epoch 1360, val loss: 0.4992988407611847
Epoch 1370, training loss: 85.29559326171875 = 0.4468928873538971 + 10.0 * 8.484869956970215
Epoch 1370, val loss: 0.49897488951683044
Epoch 1380, training loss: 85.26028442382812 = 0.44584891200065613 + 10.0 * 8.481443405151367
Epoch 1380, val loss: 0.4985663890838623
Epoch 1390, training loss: 85.25526428222656 = 0.444827675819397 + 10.0 * 8.481043815612793
Epoch 1390, val loss: 0.49821022152900696
Epoch 1400, training loss: 85.2495346069336 = 0.4438183903694153 + 10.0 * 8.480571746826172
Epoch 1400, val loss: 0.49783995747566223
Epoch 1410, training loss: 85.24337005615234 = 0.44281190633773804 + 10.0 * 8.480055809020996
Epoch 1410, val loss: 0.497496098279953
Epoch 1420, training loss: 85.23841857910156 = 0.44180363416671753 + 10.0 * 8.47966194152832
Epoch 1420, val loss: 0.4971628189086914
Epoch 1430, training loss: 85.2550048828125 = 0.440790057182312 + 10.0 * 8.48142147064209
Epoch 1430, val loss: 0.4968758821487427
Epoch 1440, training loss: 85.24152374267578 = 0.43975383043289185 + 10.0 * 8.48017692565918
Epoch 1440, val loss: 0.49645471572875977
Epoch 1450, training loss: 85.23236846923828 = 0.4387316107749939 + 10.0 * 8.479364395141602
Epoch 1450, val loss: 0.49612727761268616
Epoch 1460, training loss: 85.22164916992188 = 0.4377291202545166 + 10.0 * 8.478391647338867
Epoch 1460, val loss: 0.49580955505371094
Epoch 1470, training loss: 85.21475219726562 = 0.4367285966873169 + 10.0 * 8.477802276611328
Epoch 1470, val loss: 0.49548429250717163
Epoch 1480, training loss: 85.20912170410156 = 0.435722678899765 + 10.0 * 8.477339744567871
Epoch 1480, val loss: 0.49517735838890076
Epoch 1490, training loss: 85.20453643798828 = 0.43471574783325195 + 10.0 * 8.476982116699219
Epoch 1490, val loss: 0.4948672950267792
Epoch 1500, training loss: 85.24276733398438 = 0.43369778990745544 + 10.0 * 8.48090648651123
Epoch 1500, val loss: 0.494551420211792
Epoch 1510, training loss: 85.23314666748047 = 0.4326631426811218 + 10.0 * 8.480048179626465
Epoch 1510, val loss: 0.4941648244857788
Epoch 1520, training loss: 85.19337463378906 = 0.43163245916366577 + 10.0 * 8.476174354553223
Epoch 1520, val loss: 0.4938583970069885
Epoch 1530, training loss: 85.18761444091797 = 0.43061116337776184 + 10.0 * 8.475700378417969
Epoch 1530, val loss: 0.49354469776153564
Epoch 1540, training loss: 85.18275451660156 = 0.4295957088470459 + 10.0 * 8.475316047668457
Epoch 1540, val loss: 0.49322330951690674
Epoch 1550, training loss: 85.17681884765625 = 0.42858392000198364 + 10.0 * 8.474823951721191
Epoch 1550, val loss: 0.492912232875824
Epoch 1560, training loss: 85.18267822265625 = 0.4275628626346588 + 10.0 * 8.47551155090332
Epoch 1560, val loss: 0.4925624430179596
Epoch 1570, training loss: 85.16942596435547 = 0.4265187680721283 + 10.0 * 8.47429084777832
Epoch 1570, val loss: 0.4923054575920105
Epoch 1580, training loss: 85.17001342773438 = 0.4254770576953888 + 10.0 * 8.474453926086426
Epoch 1580, val loss: 0.49193477630615234
Epoch 1590, training loss: 85.1612319946289 = 0.42444637417793274 + 10.0 * 8.473678588867188
Epoch 1590, val loss: 0.4916878640651703
Epoch 1600, training loss: 85.15555572509766 = 0.4234079122543335 + 10.0 * 8.473215103149414
Epoch 1600, val loss: 0.4913766384124756
Epoch 1610, training loss: 85.17857360839844 = 0.42237091064453125 + 10.0 * 8.47562026977539
Epoch 1610, val loss: 0.4910696744918823
Epoch 1620, training loss: 85.15186309814453 = 0.42129847407341003 + 10.0 * 8.47305679321289
Epoch 1620, val loss: 0.4907960891723633
Epoch 1630, training loss: 85.1464614868164 = 0.42024216055870056 + 10.0 * 8.47262191772461
Epoch 1630, val loss: 0.49047765135765076
Epoch 1640, training loss: 85.1395034790039 = 0.4191797971725464 + 10.0 * 8.47203254699707
Epoch 1640, val loss: 0.49019289016723633
Epoch 1650, training loss: 85.13427734375 = 0.41811874508857727 + 10.0 * 8.4716157913208
Epoch 1650, val loss: 0.48990723490715027
Epoch 1660, training loss: 85.12919616699219 = 0.4170447885990143 + 10.0 * 8.47121524810791
Epoch 1660, val loss: 0.4896290898323059
Epoch 1670, training loss: 85.15298461914062 = 0.41597190499305725 + 10.0 * 8.473701477050781
Epoch 1670, val loss: 0.48929134011268616
Epoch 1680, training loss: 85.1363525390625 = 0.41485998034477234 + 10.0 * 8.472148895263672
Epoch 1680, val loss: 0.48906204104423523
Epoch 1690, training loss: 85.12576293945312 = 0.41376569867134094 + 10.0 * 8.471199989318848
Epoch 1690, val loss: 0.48872169852256775
Epoch 1700, training loss: 85.11300659179688 = 0.4126777648925781 + 10.0 * 8.470032691955566
Epoch 1700, val loss: 0.48845967650413513
Epoch 1710, training loss: 85.10865020751953 = 0.41158655285835266 + 10.0 * 8.469706535339355
Epoch 1710, val loss: 0.48818251490592957
Epoch 1720, training loss: 85.10295867919922 = 0.4104892909526825 + 10.0 * 8.469246864318848
Epoch 1720, val loss: 0.4878961741924286
Epoch 1730, training loss: 85.11154174804688 = 0.40938204526901245 + 10.0 * 8.470215797424316
Epoch 1730, val loss: 0.48758596181869507
Epoch 1740, training loss: 85.0960464477539 = 0.40825173258781433 + 10.0 * 8.468779563903809
Epoch 1740, val loss: 0.48730388283729553
Epoch 1750, training loss: 85.10407257080078 = 0.40711888670921326 + 10.0 * 8.469695091247559
Epoch 1750, val loss: 0.48696163296699524
Epoch 1760, training loss: 85.08758544921875 = 0.40600359439849854 + 10.0 * 8.468157768249512
Epoch 1760, val loss: 0.48670172691345215
Epoch 1770, training loss: 85.08137512207031 = 0.4048800766468048 + 10.0 * 8.467649459838867
Epoch 1770, val loss: 0.4863993525505066
Epoch 1780, training loss: 85.07695007324219 = 0.40375441312789917 + 10.0 * 8.46731948852539
Epoch 1780, val loss: 0.48609086871147156
Epoch 1790, training loss: 85.07223510742188 = 0.40262430906295776 + 10.0 * 8.466960906982422
Epoch 1790, val loss: 0.4857964515686035
Epoch 1800, training loss: 85.09027862548828 = 0.40147864818573 + 10.0 * 8.468879699707031
Epoch 1800, val loss: 0.48544225096702576
Epoch 1810, training loss: 85.08171081542969 = 0.4003148674964905 + 10.0 * 8.4681396484375
Epoch 1810, val loss: 0.4852708876132965
Epoch 1820, training loss: 85.07012176513672 = 0.39914989471435547 + 10.0 * 8.467097282409668
Epoch 1820, val loss: 0.4848474860191345
Epoch 1830, training loss: 85.0571517944336 = 0.3980015218257904 + 10.0 * 8.465914726257324
Epoch 1830, val loss: 0.4845246970653534
Epoch 1840, training loss: 85.05018615722656 = 0.3968503177165985 + 10.0 * 8.465333938598633
Epoch 1840, val loss: 0.4842911958694458
Epoch 1850, training loss: 85.0445556640625 = 0.39568912982940674 + 10.0 * 8.464886665344238
Epoch 1850, val loss: 0.48402491211891174
Epoch 1860, training loss: 85.04737091064453 = 0.39452171325683594 + 10.0 * 8.46528434753418
Epoch 1860, val loss: 0.48377907276153564
Epoch 1870, training loss: 85.05985260009766 = 0.39333558082580566 + 10.0 * 8.466651916503906
Epoch 1870, val loss: 0.4835149943828583
Epoch 1880, training loss: 85.03495788574219 = 0.39215153455734253 + 10.0 * 8.46428108215332
Epoch 1880, val loss: 0.4832111895084381
Epoch 1890, training loss: 85.03131866455078 = 0.3909655511379242 + 10.0 * 8.464035034179688
Epoch 1890, val loss: 0.4829557538032532
Epoch 1900, training loss: 85.03264617919922 = 0.3897843062877655 + 10.0 * 8.464285850524902
Epoch 1900, val loss: 0.48273372650146484
Epoch 1910, training loss: 85.01812744140625 = 0.3885969817638397 + 10.0 * 8.462953567504883
Epoch 1910, val loss: 0.48248377442359924
Epoch 1920, training loss: 85.02434539794922 = 0.38741129636764526 + 10.0 * 8.463693618774414
Epoch 1920, val loss: 0.4822843074798584
Epoch 1930, training loss: 85.03679656982422 = 0.38621026277542114 + 10.0 * 8.465059280395508
Epoch 1930, val loss: 0.482026070356369
Epoch 1940, training loss: 85.00753784179688 = 0.3849947452545166 + 10.0 * 8.462254524230957
Epoch 1940, val loss: 0.48181605339050293
Epoch 1950, training loss: 85.00597381591797 = 0.3837987184524536 + 10.0 * 8.462217330932617
Epoch 1950, val loss: 0.48158904910087585
Epoch 1960, training loss: 84.99665069580078 = 0.38259488344192505 + 10.0 * 8.461405754089355
Epoch 1960, val loss: 0.4814046621322632
Epoch 1970, training loss: 84.99092102050781 = 0.38138625025749207 + 10.0 * 8.460953712463379
Epoch 1970, val loss: 0.48122063279151917
Epoch 1980, training loss: 84.98737335205078 = 0.3801727592945099 + 10.0 * 8.46072006225586
Epoch 1980, val loss: 0.48102301359176636
Epoch 1990, training loss: 85.00442504882812 = 0.3789524435997009 + 10.0 * 8.462547302246094
Epoch 1990, val loss: 0.48084309697151184
Epoch 2000, training loss: 84.97865295410156 = 0.3777182102203369 + 10.0 * 8.46009349822998
Epoch 2000, val loss: 0.4806559979915619
Epoch 2010, training loss: 84.9765853881836 = 0.3764871060848236 + 10.0 * 8.460009574890137
Epoch 2010, val loss: 0.4804641604423523
Epoch 2020, training loss: 84.97884368896484 = 0.37525486946105957 + 10.0 * 8.460359573364258
Epoch 2020, val loss: 0.4803321957588196
Epoch 2030, training loss: 84.97125244140625 = 0.37402480840682983 + 10.0 * 8.459722518920898
Epoch 2030, val loss: 0.4801958203315735
Epoch 2040, training loss: 84.96968841552734 = 0.37279683351516724 + 10.0 * 8.459689140319824
Epoch 2040, val loss: 0.48008713126182556
Epoch 2050, training loss: 84.96151733398438 = 0.3715515732765198 + 10.0 * 8.458996772766113
Epoch 2050, val loss: 0.4799271523952484
Epoch 2060, training loss: 84.95747375488281 = 0.37030649185180664 + 10.0 * 8.45871639251709
Epoch 2060, val loss: 0.4798145592212677
Epoch 2070, training loss: 84.97457122802734 = 0.3690590560436249 + 10.0 * 8.460551261901855
Epoch 2070, val loss: 0.47973722219467163
Epoch 2080, training loss: 84.94757843017578 = 0.3678092062473297 + 10.0 * 8.457977294921875
Epoch 2080, val loss: 0.4796110689640045
Epoch 2090, training loss: 84.95732116699219 = 0.36656054854393005 + 10.0 * 8.459075927734375
Epoch 2090, val loss: 0.47959843277931213
Epoch 2100, training loss: 84.94749450683594 = 0.3652975559234619 + 10.0 * 8.458219528198242
Epoch 2100, val loss: 0.479532390832901
Epoch 2110, training loss: 84.93865203857422 = 0.36405155062675476 + 10.0 * 8.457460403442383
Epoch 2110, val loss: 0.479526162147522
Epoch 2120, training loss: 84.93341064453125 = 0.3628010153770447 + 10.0 * 8.457060813903809
Epoch 2120, val loss: 0.4794647693634033
Epoch 2130, training loss: 84.948974609375 = 0.36155396699905396 + 10.0 * 8.458742141723633
Epoch 2130, val loss: 0.47941166162490845
Epoch 2140, training loss: 84.92420196533203 = 0.36029931902885437 + 10.0 * 8.456390380859375
Epoch 2140, val loss: 0.4794864058494568
Epoch 2150, training loss: 84.91992950439453 = 0.35904639959335327 + 10.0 * 8.456088066101074
Epoch 2150, val loss: 0.4795205295085907
Epoch 2160, training loss: 84.9161605834961 = 0.35779455304145813 + 10.0 * 8.455836296081543
Epoch 2160, val loss: 0.4795401096343994
Epoch 2170, training loss: 84.91731262207031 = 0.3565429151058197 + 10.0 * 8.456076622009277
Epoch 2170, val loss: 0.47959989309310913
Epoch 2180, training loss: 84.950927734375 = 0.35528549551963806 + 10.0 * 8.459564208984375
Epoch 2180, val loss: 0.47958025336265564
Epoch 2190, training loss: 84.91374969482422 = 0.35400959849357605 + 10.0 * 8.455973625183105
Epoch 2190, val loss: 0.47966131567955017
Epoch 2200, training loss: 84.9083251953125 = 0.3527456521987915 + 10.0 * 8.455557823181152
Epoch 2200, val loss: 0.4796622395515442
Epoch 2210, training loss: 84.90640258789062 = 0.3514893054962158 + 10.0 * 8.45549201965332
Epoch 2210, val loss: 0.47978073358535767
Epoch 2220, training loss: 84.8967514038086 = 0.3502296805381775 + 10.0 * 8.454652786254883
Epoch 2220, val loss: 0.4798552095890045
Epoch 2230, training loss: 84.8924560546875 = 0.34896329045295715 + 10.0 * 8.454349517822266
Epoch 2230, val loss: 0.4799458384513855
Epoch 2240, training loss: 84.89895629882812 = 0.3477005958557129 + 10.0 * 8.45512580871582
Epoch 2240, val loss: 0.480050265789032
Epoch 2250, training loss: 84.90044403076172 = 0.34644171595573425 + 10.0 * 8.455400466918945
Epoch 2250, val loss: 0.4801505506038666
Epoch 2260, training loss: 84.89531707763672 = 0.34519001841545105 + 10.0 * 8.455012321472168
Epoch 2260, val loss: 0.48028475046157837
Epoch 2270, training loss: 84.8894271850586 = 0.3439200520515442 + 10.0 * 8.454550743103027
Epoch 2270, val loss: 0.4804103672504425
Epoch 2280, training loss: 84.87476348876953 = 0.342647522687912 + 10.0 * 8.453211784362793
Epoch 2280, val loss: 0.4806215763092041
Epoch 2290, training loss: 84.87728881835938 = 0.34137162566185 + 10.0 * 8.453592300415039
Epoch 2290, val loss: 0.4808133542537689
Epoch 2300, training loss: 84.88069915771484 = 0.3400956392288208 + 10.0 * 8.454060554504395
Epoch 2300, val loss: 0.48101574182510376
Epoch 2310, training loss: 84.87290954589844 = 0.3388236463069916 + 10.0 * 8.453409194946289
Epoch 2310, val loss: 0.48126232624053955
Epoch 2320, training loss: 84.87076568603516 = 0.33754095435142517 + 10.0 * 8.453322410583496
Epoch 2320, val loss: 0.4814332127571106
Epoch 2330, training loss: 84.86479949951172 = 0.33624839782714844 + 10.0 * 8.452855110168457
Epoch 2330, val loss: 0.4816109836101532
Epoch 2340, training loss: 84.85895538330078 = 0.3349658250808716 + 10.0 * 8.452398300170898
Epoch 2340, val loss: 0.48182186484336853
Epoch 2350, training loss: 84.86033630371094 = 0.33367329835891724 + 10.0 * 8.452666282653809
Epoch 2350, val loss: 0.4820176362991333
Epoch 2360, training loss: 84.85688781738281 = 0.33238014578819275 + 10.0 * 8.4524507522583
Epoch 2360, val loss: 0.4822002947330475
Epoch 2370, training loss: 84.84986114501953 = 0.3310930132865906 + 10.0 * 8.451876640319824
Epoch 2370, val loss: 0.4823574423789978
Epoch 2380, training loss: 84.87162780761719 = 0.3298046588897705 + 10.0 * 8.454182624816895
Epoch 2380, val loss: 0.4825485348701477
Epoch 2390, training loss: 84.8479232788086 = 0.3284984230995178 + 10.0 * 8.451942443847656
Epoch 2390, val loss: 0.4828692078590393
Epoch 2400, training loss: 84.87931823730469 = 0.32721173763275146 + 10.0 * 8.45521068572998
Epoch 2400, val loss: 0.48310285806655884
Epoch 2410, training loss: 84.84429931640625 = 0.3259153366088867 + 10.0 * 8.451838493347168
Epoch 2410, val loss: 0.4832570552825928
Epoch 2420, training loss: 84.8318862915039 = 0.32462114095687866 + 10.0 * 8.450726509094238
Epoch 2420, val loss: 0.48349741101264954
Epoch 2430, training loss: 84.82635498046875 = 0.32332736253738403 + 10.0 * 8.450303077697754
Epoch 2430, val loss: 0.4837891161441803
Epoch 2440, training loss: 84.82267761230469 = 0.3220368027687073 + 10.0 * 8.450063705444336
Epoch 2440, val loss: 0.48403993248939514
Epoch 2450, training loss: 84.8187255859375 = 0.320740282535553 + 10.0 * 8.449798583984375
Epoch 2450, val loss: 0.4843369722366333
Epoch 2460, training loss: 84.82306671142578 = 0.31943997740745544 + 10.0 * 8.450362205505371
Epoch 2460, val loss: 0.48462727665901184
Epoch 2470, training loss: 84.85083770751953 = 0.31812912225723267 + 10.0 * 8.45327091217041
Epoch 2470, val loss: 0.4850161671638489
Epoch 2480, training loss: 84.82503509521484 = 0.3168180286884308 + 10.0 * 8.450821876525879
Epoch 2480, val loss: 0.485303670167923
Epoch 2490, training loss: 84.80894470214844 = 0.31551986932754517 + 10.0 * 8.449342727661133
Epoch 2490, val loss: 0.485776424407959
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7945205479452054
0.818445265521988
=== training gcn model ===
Epoch 0, training loss: 106.93560791015625 = 1.112563967704773 + 10.0 * 10.582304000854492
Epoch 0, val loss: 1.1095818281173706
Epoch 10, training loss: 106.92817687988281 = 1.107375979423523 + 10.0 * 10.582079887390137
Epoch 10, val loss: 1.1044381856918335
Epoch 20, training loss: 106.9125747680664 = 1.101844310760498 + 10.0 * 10.581072807312012
Epoch 20, val loss: 1.0989618301391602
Epoch 30, training loss: 106.85929107666016 = 1.0958189964294434 + 10.0 * 10.576347351074219
Epoch 30, val loss: 1.0930497646331787
Epoch 40, training loss: 106.65261840820312 = 1.0891989469528198 + 10.0 * 10.556342124938965
Epoch 40, val loss: 1.0865806341171265
Epoch 50, training loss: 105.99605560302734 = 1.0815832614898682 + 10.0 * 10.491447448730469
Epoch 50, val loss: 1.0791809558868408
Epoch 60, training loss: 104.31951904296875 = 1.07367742061615 + 10.0 * 10.324584007263184
Epoch 60, val loss: 1.071682095527649
Epoch 70, training loss: 102.05711364746094 = 1.065965175628662 + 10.0 * 10.099115371704102
Epoch 70, val loss: 1.0643905401229858
Epoch 80, training loss: 99.9356918334961 = 1.059383749961853 + 10.0 * 9.887630462646484
Epoch 80, val loss: 1.0581856966018677
Epoch 90, training loss: 96.76705169677734 = 1.0538508892059326 + 10.0 * 9.571320533752441
Epoch 90, val loss: 1.0529804229736328
Epoch 100, training loss: 94.8560791015625 = 1.0496950149536133 + 10.0 * 9.380638122558594
Epoch 100, val loss: 1.0490602254867554
Epoch 110, training loss: 93.87220764160156 = 1.0464192628860474 + 10.0 * 9.28257942199707
Epoch 110, val loss: 1.0459643602371216
Epoch 120, training loss: 93.10289001464844 = 1.0431439876556396 + 10.0 * 9.205974578857422
Epoch 120, val loss: 1.0428239107131958
Epoch 130, training loss: 92.30850982666016 = 1.039894461631775 + 10.0 * 9.126861572265625
Epoch 130, val loss: 1.0396591424942017
Epoch 140, training loss: 91.8908920288086 = 1.0363662242889404 + 10.0 * 9.08545207977295
Epoch 140, val loss: 1.0362119674682617
Epoch 150, training loss: 91.4857177734375 = 1.0326428413391113 + 10.0 * 9.045307159423828
Epoch 150, val loss: 1.0326077938079834
Epoch 160, training loss: 90.9767074584961 = 1.029317021369934 + 10.0 * 8.994738578796387
Epoch 160, val loss: 1.0294508934020996
Epoch 170, training loss: 90.47550201416016 = 1.026384711265564 + 10.0 * 8.94491195678711
Epoch 170, val loss: 1.0265120267868042
Epoch 180, training loss: 90.02432250976562 = 1.022979736328125 + 10.0 * 8.900134086608887
Epoch 180, val loss: 1.02313232421875
Epoch 190, training loss: 89.64727020263672 = 1.0194346904754639 + 10.0 * 8.862783432006836
Epoch 190, val loss: 1.0196359157562256
Epoch 200, training loss: 89.25428009033203 = 1.015539526939392 + 10.0 * 8.823873519897461
Epoch 200, val loss: 1.0157004594802856
Epoch 210, training loss: 88.99038696289062 = 1.0111095905303955 + 10.0 * 8.797927856445312
Epoch 210, val loss: 1.011256217956543
Epoch 220, training loss: 88.7873764038086 = 1.0060951709747314 + 10.0 * 8.778127670288086
Epoch 220, val loss: 1.0062347650527954
Epoch 230, training loss: 88.61277770996094 = 1.0006129741668701 + 10.0 * 8.76121711730957
Epoch 230, val loss: 1.0007634162902832
Epoch 240, training loss: 88.44633483886719 = 0.9948558807373047 + 10.0 * 8.745147705078125
Epoch 240, val loss: 0.9950317740440369
Epoch 250, training loss: 88.3083724975586 = 0.9888225197792053 + 10.0 * 8.731954574584961
Epoch 250, val loss: 0.9890047907829285
Epoch 260, training loss: 88.16927337646484 = 0.982448935508728 + 10.0 * 8.718682289123535
Epoch 260, val loss: 0.9826050996780396
Epoch 270, training loss: 88.03482818603516 = 0.975665271282196 + 10.0 * 8.705916404724121
Epoch 270, val loss: 0.975849449634552
Epoch 280, training loss: 87.91942596435547 = 0.9684356451034546 + 10.0 * 8.695098876953125
Epoch 280, val loss: 0.9686474800109863
Epoch 290, training loss: 87.8133544921875 = 0.9606946110725403 + 10.0 * 8.685266494750977
Epoch 290, val loss: 0.960974931716919
Epoch 300, training loss: 87.71565246582031 = 0.9524871706962585 + 10.0 * 8.67631721496582
Epoch 300, val loss: 0.9528473615646362
Epoch 310, training loss: 87.6400375366211 = 0.943797767162323 + 10.0 * 8.669624328613281
Epoch 310, val loss: 0.9442809820175171
Epoch 320, training loss: 87.55059051513672 = 0.934610903263092 + 10.0 * 8.661598205566406
Epoch 320, val loss: 0.9352253675460815
Epoch 330, training loss: 87.46843719482422 = 0.9250050783157349 + 10.0 * 8.654343605041504
Epoch 330, val loss: 0.9257702827453613
Epoch 340, training loss: 87.39591979980469 = 0.9150074124336243 + 10.0 * 8.648091316223145
Epoch 340, val loss: 0.9159579277038574
Epoch 350, training loss: 87.32916259765625 = 0.9046123027801514 + 10.0 * 8.642455101013184
Epoch 350, val loss: 0.9058225750923157
Epoch 360, training loss: 87.259521484375 = 0.8939253091812134 + 10.0 * 8.63655948638916
Epoch 360, val loss: 0.8953867554664612
Epoch 370, training loss: 87.19181060791016 = 0.8829793930053711 + 10.0 * 8.63088321685791
Epoch 370, val loss: 0.8847351670265198
Epoch 380, training loss: 87.1344985961914 = 0.8718004822731018 + 10.0 * 8.626269340515137
Epoch 380, val loss: 0.8738750219345093
Epoch 390, training loss: 87.07303619384766 = 0.8604635000228882 + 10.0 * 8.621256828308105
Epoch 390, val loss: 0.8628975749015808
Epoch 400, training loss: 87.01203155517578 = 0.8490249514579773 + 10.0 * 8.616300582885742
Epoch 400, val loss: 0.8518399596214294
Epoch 410, training loss: 86.99687194824219 = 0.83749920129776 + 10.0 * 8.615937232971191
Epoch 410, val loss: 0.8406898975372314
Epoch 420, training loss: 86.90221405029297 = 0.8258770704269409 + 10.0 * 8.607633590698242
Epoch 420, val loss: 0.829565167427063
Epoch 430, training loss: 86.8530044555664 = 0.814326822757721 + 10.0 * 8.60386848449707
Epoch 430, val loss: 0.8185067772865295
Epoch 440, training loss: 86.80643463134766 = 0.802836537361145 + 10.0 * 8.600359916687012
Epoch 440, val loss: 0.8075010180473328
Epoch 450, training loss: 86.7619857788086 = 0.7913769483566284 + 10.0 * 8.597061157226562
Epoch 450, val loss: 0.7965511679649353
Epoch 460, training loss: 86.7117919921875 = 0.7800413370132446 + 10.0 * 8.593174934387207
Epoch 460, val loss: 0.7857508659362793
Epoch 470, training loss: 86.67069244384766 = 0.7688656449317932 + 10.0 * 8.590182304382324
Epoch 470, val loss: 0.7751163244247437
Epoch 480, training loss: 86.62664031982422 = 0.757839560508728 + 10.0 * 8.58687973022461
Epoch 480, val loss: 0.7646440863609314
Epoch 490, training loss: 86.59546661376953 = 0.7469712495803833 + 10.0 * 8.58484935760498
Epoch 490, val loss: 0.7543390989303589
Epoch 500, training loss: 86.55964660644531 = 0.7363266348838806 + 10.0 * 8.582331657409668
Epoch 500, val loss: 0.7442572116851807
Epoch 510, training loss: 86.50973510742188 = 0.7259043455123901 + 10.0 * 8.57838249206543
Epoch 510, val loss: 0.7344299554824829
Epoch 520, training loss: 86.47419738769531 = 0.7157626152038574 + 10.0 * 8.575843811035156
Epoch 520, val loss: 0.7248706221580505
Epoch 530, training loss: 86.43731689453125 = 0.7058857679367065 + 10.0 * 8.573143005371094
Epoch 530, val loss: 0.715593159198761
Epoch 540, training loss: 86.40010833740234 = 0.6963069438934326 + 10.0 * 8.570380210876465
Epoch 540, val loss: 0.7065805792808533
Epoch 550, training loss: 86.36463165283203 = 0.6869907379150391 + 10.0 * 8.567764282226562
Epoch 550, val loss: 0.6978536248207092
Epoch 560, training loss: 86.382568359375 = 0.6779775619506836 + 10.0 * 8.570459365844727
Epoch 560, val loss: 0.6894196271896362
Epoch 570, training loss: 86.31355285644531 = 0.669205904006958 + 10.0 * 8.564435005187988
Epoch 570, val loss: 0.6812456846237183
Epoch 580, training loss: 86.27825927734375 = 0.6608221530914307 + 10.0 * 8.56174373626709
Epoch 580, val loss: 0.6734679341316223
Epoch 590, training loss: 86.24777221679688 = 0.6528038382530212 + 10.0 * 8.559496879577637
Epoch 590, val loss: 0.6659898161888123
Epoch 600, training loss: 86.22593688964844 = 0.645107090473175 + 10.0 * 8.558083534240723
Epoch 600, val loss: 0.6588884592056274
Epoch 610, training loss: 86.20848083496094 = 0.6377170085906982 + 10.0 * 8.557076454162598
Epoch 610, val loss: 0.652050793170929
Epoch 620, training loss: 86.17803955078125 = 0.6306576132774353 + 10.0 * 8.55473804473877
Epoch 620, val loss: 0.6455735564231873
Epoch 630, training loss: 86.15421295166016 = 0.6239664554595947 + 10.0 * 8.553024291992188
Epoch 630, val loss: 0.6394619941711426
Epoch 640, training loss: 86.12921142578125 = 0.6175928711891174 + 10.0 * 8.551161766052246
Epoch 640, val loss: 0.6336670517921448
Epoch 650, training loss: 86.13228607177734 = 0.6115412712097168 + 10.0 * 8.552074432373047
Epoch 650, val loss: 0.6282185912132263
Epoch 660, training loss: 86.10918426513672 = 0.6057793498039246 + 10.0 * 8.55034065246582
Epoch 660, val loss: 0.6229625940322876
Epoch 670, training loss: 86.07866668701172 = 0.6003488302230835 + 10.0 * 8.547831535339355
Epoch 670, val loss: 0.6180907487869263
Epoch 680, training loss: 86.0716781616211 = 0.5952100157737732 + 10.0 * 8.547647476196289
Epoch 680, val loss: 0.6135644316673279
Epoch 690, training loss: 86.04248809814453 = 0.5903869271278381 + 10.0 * 8.545209884643555
Epoch 690, val loss: 0.609294056892395
Epoch 700, training loss: 86.02422332763672 = 0.585837185382843 + 10.0 * 8.543838500976562
Epoch 700, val loss: 0.6053012609481812
Epoch 710, training loss: 86.01557159423828 = 0.5815514922142029 + 10.0 * 8.543401718139648
Epoch 710, val loss: 0.6015900373458862
Epoch 720, training loss: 85.99604034423828 = 0.5774891972541809 + 10.0 * 8.541854858398438
Epoch 720, val loss: 0.598090410232544
Epoch 730, training loss: 85.97659301757812 = 0.5736744999885559 + 10.0 * 8.540291786193848
Epoch 730, val loss: 0.594832718372345
Epoch 740, training loss: 85.96234130859375 = 0.5700840950012207 + 10.0 * 8.539225578308105
Epoch 740, val loss: 0.591797947883606
Epoch 750, training loss: 85.97167205810547 = 0.5666967034339905 + 10.0 * 8.540497779846191
Epoch 750, val loss: 0.5889401435852051
Epoch 760, training loss: 85.94170379638672 = 0.5634832978248596 + 10.0 * 8.537821769714355
Epoch 760, val loss: 0.5863572955131531
Epoch 770, training loss: 85.919921875 = 0.5604654550552368 + 10.0 * 8.535945892333984
Epoch 770, val loss: 0.5838568210601807
Epoch 780, training loss: 85.90003204345703 = 0.5576229691505432 + 10.0 * 8.53424072265625
Epoch 780, val loss: 0.5816274881362915
Epoch 790, training loss: 85.89070892333984 = 0.5549330711364746 + 10.0 * 8.533577919006348
Epoch 790, val loss: 0.579490602016449
Epoch 800, training loss: 85.88858032226562 = 0.5523536205291748 + 10.0 * 8.533622741699219
Epoch 800, val loss: 0.577461838722229
Epoch 810, training loss: 85.86219024658203 = 0.5499054789543152 + 10.0 * 8.531229019165039
Epoch 810, val loss: 0.5756089687347412
Epoch 820, training loss: 85.84199523925781 = 0.547584056854248 + 10.0 * 8.529440879821777
Epoch 820, val loss: 0.5738517045974731
Epoch 830, training loss: 85.82894134521484 = 0.5453757047653198 + 10.0 * 8.528356552124023
Epoch 830, val loss: 0.5722145438194275
Epoch 840, training loss: 85.82647705078125 = 0.5432419180870056 + 10.0 * 8.52832317352295
Epoch 840, val loss: 0.5706788301467896
Epoch 850, training loss: 85.81048583984375 = 0.5411869883537292 + 10.0 * 8.52692985534668
Epoch 850, val loss: 0.5691556930541992
Epoch 860, training loss: 85.78549194335938 = 0.5392265319824219 + 10.0 * 8.524626731872559
Epoch 860, val loss: 0.5677366852760315
Epoch 870, training loss: 85.76935577392578 = 0.5373579263687134 + 10.0 * 8.523199081420898
Epoch 870, val loss: 0.5664419531822205
Epoch 880, training loss: 85.75294494628906 = 0.5355491638183594 + 10.0 * 8.52173900604248
Epoch 880, val loss: 0.5651969909667969
Epoch 890, training loss: 85.78974151611328 = 0.5337910652160645 + 10.0 * 8.525594711303711
Epoch 890, val loss: 0.5640848278999329
Epoch 900, training loss: 85.74259948730469 = 0.5320607423782349 + 10.0 * 8.5210542678833
Epoch 900, val loss: 0.5628333687782288
Epoch 910, training loss: 85.71915435791016 = 0.5303910374641418 + 10.0 * 8.518877029418945
Epoch 910, val loss: 0.5616975426673889
Epoch 920, training loss: 85.70718383789062 = 0.5287925601005554 + 10.0 * 8.517839431762695
Epoch 920, val loss: 0.5606361031532288
Epoch 930, training loss: 85.68863677978516 = 0.5272408723831177 + 10.0 * 8.516139030456543
Epoch 930, val loss: 0.5596301555633545
Epoch 940, training loss: 85.67676544189453 = 0.5257306098937988 + 10.0 * 8.515103340148926
Epoch 940, val loss: 0.5586645603179932
Epoch 950, training loss: 85.72158813476562 = 0.5242476463317871 + 10.0 * 8.519734382629395
Epoch 950, val loss: 0.5576531887054443
Epoch 960, training loss: 85.67665100097656 = 0.5227628946304321 + 10.0 * 8.515388488769531
Epoch 960, val loss: 0.5568127632141113
Epoch 970, training loss: 85.64552307128906 = 0.5213266611099243 + 10.0 * 8.512419700622559
Epoch 970, val loss: 0.5559189915657043
Epoch 980, training loss: 85.63343048095703 = 0.519935667514801 + 10.0 * 8.51134967803955
Epoch 980, val loss: 0.5550249218940735
Epoch 990, training loss: 85.6208724975586 = 0.5185747146606445 + 10.0 * 8.510229110717773
Epoch 990, val loss: 0.5541728734970093
Epoch 1000, training loss: 85.61380767822266 = 0.5172315239906311 + 10.0 * 8.50965690612793
Epoch 1000, val loss: 0.5533893704414368
Epoch 1010, training loss: 85.66250610351562 = 0.5158846378326416 + 10.0 * 8.51466178894043
Epoch 1010, val loss: 0.552550196647644
Epoch 1020, training loss: 85.60832214355469 = 0.5145487785339355 + 10.0 * 8.509377479553223
Epoch 1020, val loss: 0.5516675114631653
Epoch 1030, training loss: 85.58942413330078 = 0.5132479667663574 + 10.0 * 8.507617950439453
Epoch 1030, val loss: 0.5510046482086182
Epoch 1040, training loss: 85.57323455810547 = 0.5119746923446655 + 10.0 * 8.506125450134277
Epoch 1040, val loss: 0.5502302646636963
Epoch 1050, training loss: 85.56525421142578 = 0.5107166767120361 + 10.0 * 8.505453109741211
Epoch 1050, val loss: 0.5494420528411865
Epoch 1060, training loss: 85.57262420654297 = 0.5094702243804932 + 10.0 * 8.506315231323242
Epoch 1060, val loss: 0.5487170219421387
Epoch 1070, training loss: 85.54957580566406 = 0.5082089900970459 + 10.0 * 8.50413703918457
Epoch 1070, val loss: 0.5480147004127502
Epoch 1080, training loss: 85.54293823242188 = 0.5069700479507446 + 10.0 * 8.503596305847168
Epoch 1080, val loss: 0.5473019480705261
Epoch 1090, training loss: 85.53221130371094 = 0.5057549476623535 + 10.0 * 8.502645492553711
Epoch 1090, val loss: 0.5465993285179138
Epoch 1100, training loss: 85.52351379394531 = 0.5045479536056519 + 10.0 * 8.501896858215332
Epoch 1100, val loss: 0.5459001064300537
Epoch 1110, training loss: 85.52639770507812 = 0.5033480525016785 + 10.0 * 8.502305030822754
Epoch 1110, val loss: 0.5451903939247131
Epoch 1120, training loss: 85.53973388671875 = 0.5021321177482605 + 10.0 * 8.50376033782959
Epoch 1120, val loss: 0.5445050001144409
Epoch 1130, training loss: 85.5149154663086 = 0.5009154677391052 + 10.0 * 8.501399993896484
Epoch 1130, val loss: 0.5438078045845032
Epoch 1140, training loss: 85.49345397949219 = 0.4997289776802063 + 10.0 * 8.499372482299805
Epoch 1140, val loss: 0.543140709400177
Epoch 1150, training loss: 85.48424530029297 = 0.49855881929397583 + 10.0 * 8.498568534851074
Epoch 1150, val loss: 0.5424588918685913
Epoch 1160, training loss: 85.48947143554688 = 0.49739402532577515 + 10.0 * 8.499208450317383
Epoch 1160, val loss: 0.5417701601982117
Epoch 1170, training loss: 85.47270965576172 = 0.49620941281318665 + 10.0 * 8.497650146484375
Epoch 1170, val loss: 0.5411131978034973
Epoch 1180, training loss: 85.46263885498047 = 0.49503228068351746 + 10.0 * 8.496760368347168
Epoch 1180, val loss: 0.540433406829834
Epoch 1190, training loss: 85.45596313476562 = 0.49386924505233765 + 10.0 * 8.496210098266602
Epoch 1190, val loss: 0.5397739410400391
Epoch 1200, training loss: 85.45051574707031 = 0.4927162826061249 + 10.0 * 8.495779991149902
Epoch 1200, val loss: 0.5391400456428528
Epoch 1210, training loss: 85.51277160644531 = 0.4915561378002167 + 10.0 * 8.502121925354004
Epoch 1210, val loss: 0.5385605096817017
Epoch 1220, training loss: 85.44984436035156 = 0.4903561472892761 + 10.0 * 8.495948791503906
Epoch 1220, val loss: 0.5376980304718018
Epoch 1230, training loss: 85.43632507324219 = 0.4891812801361084 + 10.0 * 8.494714736938477
Epoch 1230, val loss: 0.5370796322822571
Epoch 1240, training loss: 85.42488861083984 = 0.4880264401435852 + 10.0 * 8.493685722351074
Epoch 1240, val loss: 0.5364084839820862
Epoch 1250, training loss: 85.41583251953125 = 0.4868667721748352 + 10.0 * 8.49289608001709
Epoch 1250, val loss: 0.5357832908630371
Epoch 1260, training loss: 85.44496154785156 = 0.48571068048477173 + 10.0 * 8.495924949645996
Epoch 1260, val loss: 0.5351752638816833
Epoch 1270, training loss: 85.44132232666016 = 0.48449674248695374 + 10.0 * 8.495682716369629
Epoch 1270, val loss: 0.5343577861785889
Epoch 1280, training loss: 85.40702056884766 = 0.48331308364868164 + 10.0 * 8.49237060546875
Epoch 1280, val loss: 0.5337479114532471
Epoch 1290, training loss: 85.39051818847656 = 0.48214325308799744 + 10.0 * 8.490838050842285
Epoch 1290, val loss: 0.5330696702003479
Epoch 1300, training loss: 85.38438415527344 = 0.48098060488700867 + 10.0 * 8.490340232849121
Epoch 1300, val loss: 0.5324257016181946
Epoch 1310, training loss: 85.37985229492188 = 0.4798101484775543 + 10.0 * 8.49000358581543
Epoch 1310, val loss: 0.5317732095718384
Epoch 1320, training loss: 85.41880798339844 = 0.4786249101161957 + 10.0 * 8.4940185546875
Epoch 1320, val loss: 0.5310370922088623
Epoch 1330, training loss: 85.38140106201172 = 0.4774267077445984 + 10.0 * 8.490397453308105
Epoch 1330, val loss: 0.5304432511329651
Epoch 1340, training loss: 85.36546325683594 = 0.47623124718666077 + 10.0 * 8.488923072814941
Epoch 1340, val loss: 0.5297086834907532
Epoch 1350, training loss: 85.3690185546875 = 0.47505006194114685 + 10.0 * 8.489397048950195
Epoch 1350, val loss: 0.5290684103965759
Epoch 1360, training loss: 85.35285186767578 = 0.4738435447216034 + 10.0 * 8.487900733947754
Epoch 1360, val loss: 0.5283735990524292
Epoch 1370, training loss: 85.34229278564453 = 0.47264355421066284 + 10.0 * 8.48696517944336
Epoch 1370, val loss: 0.527754545211792
Epoch 1380, training loss: 85.33354187011719 = 0.4714427590370178 + 10.0 * 8.486209869384766
Epoch 1380, val loss: 0.5270388126373291
Epoch 1390, training loss: 85.32698059082031 = 0.470246821641922 + 10.0 * 8.485673904418945
Epoch 1390, val loss: 0.5263931155204773
Epoch 1400, training loss: 85.34880065917969 = 0.4690421223640442 + 10.0 * 8.48797607421875
Epoch 1400, val loss: 0.5257180333137512
Epoch 1410, training loss: 85.33165740966797 = 0.4678133726119995 + 10.0 * 8.486384391784668
Epoch 1410, val loss: 0.5250725150108337
Epoch 1420, training loss: 85.32447814941406 = 0.4665850102901459 + 10.0 * 8.48578929901123
Epoch 1420, val loss: 0.5244023203849792
Epoch 1430, training loss: 85.30145263671875 = 0.46535974740982056 + 10.0 * 8.483609199523926
Epoch 1430, val loss: 0.523687481880188
Epoch 1440, training loss: 85.2923583984375 = 0.4641420245170593 + 10.0 * 8.482821464538574
Epoch 1440, val loss: 0.5230215191841125
Epoch 1450, training loss: 85.29866790771484 = 0.46291637420654297 + 10.0 * 8.483575820922852
Epoch 1450, val loss: 0.5223113298416138
Epoch 1460, training loss: 85.3013916015625 = 0.46165698766708374 + 10.0 * 8.483973503112793
Epoch 1460, val loss: 0.5216138362884521
Epoch 1470, training loss: 85.27552032470703 = 0.46039649844169617 + 10.0 * 8.481512069702148
Epoch 1470, val loss: 0.5209530591964722
Epoch 1480, training loss: 85.26924896240234 = 0.45914751291275024 + 10.0 * 8.481010437011719
Epoch 1480, val loss: 0.520268976688385
Epoch 1490, training loss: 85.26233673095703 = 0.4578953981399536 + 10.0 * 8.480443954467773
Epoch 1490, val loss: 0.5195558667182922
Epoch 1500, training loss: 85.25788879394531 = 0.4566307067871094 + 10.0 * 8.48012638092041
Epoch 1500, val loss: 0.5188848376274109
Epoch 1510, training loss: 85.32098388671875 = 0.4553375840187073 + 10.0 * 8.486564636230469
Epoch 1510, val loss: 0.5181581377983093
Epoch 1520, training loss: 85.2470703125 = 0.45403748750686646 + 10.0 * 8.479303359985352
Epoch 1520, val loss: 0.5174756646156311
Epoch 1530, training loss: 85.24198150634766 = 0.45273539423942566 + 10.0 * 8.478924751281738
Epoch 1530, val loss: 0.5167734026908875
Epoch 1540, training loss: 85.23350524902344 = 0.4514332711696625 + 10.0 * 8.4782075881958
Epoch 1540, val loss: 0.5160377025604248
Epoch 1550, training loss: 85.22547149658203 = 0.4501299262046814 + 10.0 * 8.477534294128418
Epoch 1550, val loss: 0.515328049659729
Epoch 1560, training loss: 85.22760009765625 = 0.4488077461719513 + 10.0 * 8.477879524230957
Epoch 1560, val loss: 0.514593243598938
Epoch 1570, training loss: 85.25920867919922 = 0.44745588302612305 + 10.0 * 8.481175422668457
Epoch 1570, val loss: 0.5137941837310791
Epoch 1580, training loss: 85.23541259765625 = 0.4460952579975128 + 10.0 * 8.478931427001953
Epoch 1580, val loss: 0.5131773948669434
Epoch 1590, training loss: 85.21436309814453 = 0.4447408616542816 + 10.0 * 8.476962089538574
Epoch 1590, val loss: 0.5124287009239197
Epoch 1600, training loss: 85.20011138916016 = 0.44339650869369507 + 10.0 * 8.475671768188477
Epoch 1600, val loss: 0.5117278099060059
Epoch 1610, training loss: 85.1988525390625 = 0.44204288721084595 + 10.0 * 8.47568130493164
Epoch 1610, val loss: 0.5110145211219788
Epoch 1620, training loss: 85.25942993164062 = 0.44067198038101196 + 10.0 * 8.4818754196167
Epoch 1620, val loss: 0.5103659629821777
Epoch 1630, training loss: 85.20536804199219 = 0.43927183747291565 + 10.0 * 8.47661018371582
Epoch 1630, val loss: 0.5095756649971008
Epoch 1640, training loss: 85.18412780761719 = 0.43788740038871765 + 10.0 * 8.474623680114746
Epoch 1640, val loss: 0.5088998675346375
Epoch 1650, training loss: 85.17569732666016 = 0.4364989101886749 + 10.0 * 8.473919868469238
Epoch 1650, val loss: 0.5081519484519958
Epoch 1660, training loss: 85.16998291015625 = 0.4351072609424591 + 10.0 * 8.473487854003906
Epoch 1660, val loss: 0.5074535012245178
Epoch 1670, training loss: 85.16586303710938 = 0.43370118737220764 + 10.0 * 8.47321605682373
Epoch 1670, val loss: 0.5067583322525024
Epoch 1680, training loss: 85.18659973144531 = 0.4322798550128937 + 10.0 * 8.475431442260742
Epoch 1680, val loss: 0.5060179829597473
Epoch 1690, training loss: 85.17166137695312 = 0.43084269762039185 + 10.0 * 8.474081993103027
Epoch 1690, val loss: 0.5052890777587891
Epoch 1700, training loss: 85.1628189086914 = 0.42939355969429016 + 10.0 * 8.473342895507812
Epoch 1700, val loss: 0.5045821666717529
Epoch 1710, training loss: 85.15020751953125 = 0.42795854806900024 + 10.0 * 8.472225189208984
Epoch 1710, val loss: 0.5038508772850037
Epoch 1720, training loss: 85.15970611572266 = 0.4265187680721283 + 10.0 * 8.473318099975586
Epoch 1720, val loss: 0.5031405687332153
Epoch 1730, training loss: 85.15213775634766 = 0.425054669380188 + 10.0 * 8.472707748413086
Epoch 1730, val loss: 0.502429187297821
Epoch 1740, training loss: 85.14147186279297 = 0.423594206571579 + 10.0 * 8.47178840637207
Epoch 1740, val loss: 0.5018132925033569
Epoch 1750, training loss: 85.13203430175781 = 0.4221315383911133 + 10.0 * 8.470990180969238
Epoch 1750, val loss: 0.5011123418807983
Epoch 1760, training loss: 85.13397979736328 = 0.4206593334674835 + 10.0 * 8.471331596374512
Epoch 1760, val loss: 0.500464916229248
Epoch 1770, training loss: 85.15604400634766 = 0.41917160153388977 + 10.0 * 8.473687171936035
Epoch 1770, val loss: 0.49978840351104736
Epoch 1780, training loss: 85.12811279296875 = 0.4176529347896576 + 10.0 * 8.47104549407959
Epoch 1780, val loss: 0.49899715185165405
Epoch 1790, training loss: 85.12356567382812 = 0.41616183519363403 + 10.0 * 8.47074031829834
Epoch 1790, val loss: 0.49837297201156616
Epoch 1800, training loss: 85.11328125 = 0.41465362906455994 + 10.0 * 8.469862937927246
Epoch 1800, val loss: 0.4977062940597534
Epoch 1810, training loss: 85.10686492919922 = 0.41314345598220825 + 10.0 * 8.469371795654297
Epoch 1810, val loss: 0.4970950186252594
Epoch 1820, training loss: 85.10970306396484 = 0.41162988543510437 + 10.0 * 8.469807624816895
Epoch 1820, val loss: 0.4965280294418335
Epoch 1830, training loss: 85.12033081054688 = 0.41009944677352905 + 10.0 * 8.471022605895996
Epoch 1830, val loss: 0.49590784311294556
Epoch 1840, training loss: 85.10025787353516 = 0.4085545539855957 + 10.0 * 8.469170570373535
Epoch 1840, val loss: 0.49522778391838074
Epoch 1850, training loss: 85.09730529785156 = 0.4070141911506653 + 10.0 * 8.469029426574707
Epoch 1850, val loss: 0.4946189224720001
Epoch 1860, training loss: 85.09274291992188 = 0.4054702818393707 + 10.0 * 8.468727111816406
Epoch 1860, val loss: 0.49408283829689026
Epoch 1870, training loss: 85.08394622802734 = 0.4039151668548584 + 10.0 * 8.468003273010254
Epoch 1870, val loss: 0.4934895932674408
Epoch 1880, training loss: 85.10884094238281 = 0.4023546576499939 + 10.0 * 8.470648765563965
Epoch 1880, val loss: 0.4928787052631378
Epoch 1890, training loss: 85.08443450927734 = 0.4007973372936249 + 10.0 * 8.468363761901855
Epoch 1890, val loss: 0.4924689531326294
Epoch 1900, training loss: 85.06904602050781 = 0.3992164731025696 + 10.0 * 8.4669828414917
Epoch 1900, val loss: 0.49181413650512695
Epoch 1910, training loss: 85.06352233886719 = 0.3976520895957947 + 10.0 * 8.46658706665039
Epoch 1910, val loss: 0.49131539463996887
Epoch 1920, training loss: 85.0666275024414 = 0.3960818946361542 + 10.0 * 8.46705436706543
Epoch 1920, val loss: 0.4907645881175995
Epoch 1930, training loss: 85.0890884399414 = 0.39449217915534973 + 10.0 * 8.469459533691406
Epoch 1930, val loss: 0.4901644289493561
Epoch 1940, training loss: 85.06781768798828 = 0.39291009306907654 + 10.0 * 8.467490196228027
Epoch 1940, val loss: 0.48973768949508667
Epoch 1950, training loss: 85.053955078125 = 0.39132240414619446 + 10.0 * 8.466263771057129
Epoch 1950, val loss: 0.48915061354637146
Epoch 1960, training loss: 85.04501342773438 = 0.3897377848625183 + 10.0 * 8.465527534484863
Epoch 1960, val loss: 0.4886220097541809
Epoch 1970, training loss: 85.04071044921875 = 0.38816750049591064 + 10.0 * 8.465253829956055
Epoch 1970, val loss: 0.48816531896591187
Epoch 1980, training loss: 85.0703353881836 = 0.3865838050842285 + 10.0 * 8.468375205993652
Epoch 1980, val loss: 0.4876910150051117
Epoch 1990, training loss: 85.04190826416016 = 0.38498765230178833 + 10.0 * 8.465692520141602
Epoch 1990, val loss: 0.4872373044490814
Epoch 2000, training loss: 85.04795837402344 = 0.383394330739975 + 10.0 * 8.466456413269043
Epoch 2000, val loss: 0.4867361783981323
Epoch 2010, training loss: 85.03316497802734 = 0.38181570172309875 + 10.0 * 8.46513557434082
Epoch 2010, val loss: 0.48638930916786194
Epoch 2020, training loss: 85.022216796875 = 0.3802355229854584 + 10.0 * 8.464198112487793
Epoch 2020, val loss: 0.4859882891178131
Epoch 2030, training loss: 85.01742553710938 = 0.3786601424217224 + 10.0 * 8.463876724243164
Epoch 2030, val loss: 0.48563313484191895
Epoch 2040, training loss: 85.01799774169922 = 0.3770778179168701 + 10.0 * 8.464092254638672
Epoch 2040, val loss: 0.4852871894836426
Epoch 2050, training loss: 85.05633544921875 = 0.37550026178359985 + 10.0 * 8.468083381652832
Epoch 2050, val loss: 0.4849998950958252
Epoch 2060, training loss: 85.03128051757812 = 0.3738988935947418 + 10.0 * 8.465738296508789
Epoch 2060, val loss: 0.4846442937850952
Epoch 2070, training loss: 85.00981140136719 = 0.3722971975803375 + 10.0 * 8.463750839233398
Epoch 2070, val loss: 0.4842813014984131
Epoch 2080, training loss: 84.99823760986328 = 0.37073323130607605 + 10.0 * 8.462750434875488
Epoch 2080, val loss: 0.48406118154525757
Epoch 2090, training loss: 84.99256896972656 = 0.369162380695343 + 10.0 * 8.462340354919434
Epoch 2090, val loss: 0.4838171601295471
Epoch 2100, training loss: 84.99408721923828 = 0.3675897717475891 + 10.0 * 8.46264934539795
Epoch 2100, val loss: 0.4835692346096039
Epoch 2110, training loss: 85.039794921875 = 0.3659999966621399 + 10.0 * 8.467379570007324
Epoch 2110, val loss: 0.4832492470741272
Epoch 2120, training loss: 84.9883804321289 = 0.36443963646888733 + 10.0 * 8.462393760681152
Epoch 2120, val loss: 0.483209490776062
Epoch 2130, training loss: 84.98011779785156 = 0.36287781596183777 + 10.0 * 8.461724281311035
Epoch 2130, val loss: 0.48302769660949707
Epoch 2140, training loss: 84.98307037353516 = 0.3613179922103882 + 10.0 * 8.462175369262695
Epoch 2140, val loss: 0.48284024000167847
Epoch 2150, training loss: 84.98799133300781 = 0.3597889542579651 + 10.0 * 8.462820053100586
Epoch 2150, val loss: 0.4827859699726105
Epoch 2160, training loss: 84.96768188476562 = 0.358227401971817 + 10.0 * 8.460945129394531
Epoch 2160, val loss: 0.4825991094112396
Epoch 2170, training loss: 84.96349334716797 = 0.35668113827705383 + 10.0 * 8.460680961608887
Epoch 2170, val loss: 0.4824712574481964
Epoch 2180, training loss: 84.96383666992188 = 0.35514187812805176 + 10.0 * 8.460869789123535
Epoch 2180, val loss: 0.4823821783065796
Epoch 2190, training loss: 84.98440551757812 = 0.35360223054885864 + 10.0 * 8.463080406188965
Epoch 2190, val loss: 0.4822309911251068
Epoch 2200, training loss: 84.97035217285156 = 0.3520594537258148 + 10.0 * 8.46182918548584
Epoch 2200, val loss: 0.4821367561817169
Epoch 2210, training loss: 84.95379638671875 = 0.35053738951683044 + 10.0 * 8.460325241088867
Epoch 2210, val loss: 0.48224061727523804
Epoch 2220, training loss: 84.9473648071289 = 0.34900641441345215 + 10.0 * 8.45983600616455
Epoch 2220, val loss: 0.4821261465549469
Epoch 2230, training loss: 84.94525909423828 = 0.34749364852905273 + 10.0 * 8.459775924682617
Epoch 2230, val loss: 0.48216405510902405
Epoch 2240, training loss: 84.9422836303711 = 0.3459828794002533 + 10.0 * 8.459630012512207
Epoch 2240, val loss: 0.48219606280326843
Epoch 2250, training loss: 84.93836975097656 = 0.34446945786476135 + 10.0 * 8.459390640258789
Epoch 2250, val loss: 0.48224085569381714
Epoch 2260, training loss: 84.93605041503906 = 0.34296175837516785 + 10.0 * 8.459308624267578
Epoch 2260, val loss: 0.48231038451194763
Epoch 2270, training loss: 84.95175170898438 = 0.34145307540893555 + 10.0 * 8.461030006408691
Epoch 2270, val loss: 0.4823324680328369
Epoch 2280, training loss: 84.93423461914062 = 0.3399510383605957 + 10.0 * 8.459428787231445
Epoch 2280, val loss: 0.4824904203414917
Epoch 2290, training loss: 84.93061065673828 = 0.3384665250778198 + 10.0 * 8.45921516418457
Epoch 2290, val loss: 0.4826738238334656
Epoch 2300, training loss: 84.94002532958984 = 0.33697283267974854 + 10.0 * 8.460305213928223
Epoch 2300, val loss: 0.48281440138816833
Epoch 2310, training loss: 84.91230773925781 = 0.33546820282936096 + 10.0 * 8.457684516906738
Epoch 2310, val loss: 0.4827677309513092
Epoch 2320, training loss: 84.91291046142578 = 0.3339919149875641 + 10.0 * 8.457891464233398
Epoch 2320, val loss: 0.4828684628009796
Epoch 2330, training loss: 84.9061279296875 = 0.3325210213661194 + 10.0 * 8.457361221313477
Epoch 2330, val loss: 0.4830399751663208
Epoch 2340, training loss: 84.91081237792969 = 0.3310513198375702 + 10.0 * 8.457975387573242
Epoch 2340, val loss: 0.48319098353385925
Epoch 2350, training loss: 84.9271240234375 = 0.3295861482620239 + 10.0 * 8.459753036499023
Epoch 2350, val loss: 0.48347732424736023
Epoch 2360, training loss: 84.90939331054688 = 0.3281617760658264 + 10.0 * 8.458123207092285
Epoch 2360, val loss: 0.48392826318740845
Epoch 2370, training loss: 84.90116882324219 = 0.3266987204551697 + 10.0 * 8.457447052001953
Epoch 2370, val loss: 0.4841195344924927
Epoch 2380, training loss: 84.88856506347656 = 0.32526567578315735 + 10.0 * 8.456330299377441
Epoch 2380, val loss: 0.48435401916503906
Epoch 2390, training loss: 84.88385772705078 = 0.3238375782966614 + 10.0 * 8.456002235412598
Epoch 2390, val loss: 0.48469680547714233
Epoch 2400, training loss: 84.88362121582031 = 0.32240599393844604 + 10.0 * 8.456121444702148
Epoch 2400, val loss: 0.4849173128604889
Epoch 2410, training loss: 84.95343017578125 = 0.3209746479988098 + 10.0 * 8.463245391845703
Epoch 2410, val loss: 0.48516613245010376
Epoch 2420, training loss: 84.89849853515625 = 0.3195631206035614 + 10.0 * 8.457893371582031
Epoch 2420, val loss: 0.48571786284446716
Epoch 2430, training loss: 84.88172912597656 = 0.31812527775764465 + 10.0 * 8.456360816955566
Epoch 2430, val loss: 0.485914945602417
Epoch 2440, training loss: 84.87606048583984 = 0.31672433018684387 + 10.0 * 8.455933570861816
Epoch 2440, val loss: 0.48638731241226196
Epoch 2450, training loss: 84.89558410644531 = 0.3153125047683716 + 10.0 * 8.458026885986328
Epoch 2450, val loss: 0.4867421090602875
Epoch 2460, training loss: 84.86407470703125 = 0.3138962984085083 + 10.0 * 8.455018043518066
Epoch 2460, val loss: 0.48707541823387146
Epoch 2470, training loss: 84.85911560058594 = 0.3124946057796478 + 10.0 * 8.454662322998047
Epoch 2470, val loss: 0.4874618351459503
Epoch 2480, training loss: 84.85726165771484 = 0.3111027181148529 + 10.0 * 8.454615592956543
Epoch 2480, val loss: 0.48788565397262573
Epoch 2490, training loss: 84.88468933105469 = 0.3097068667411804 + 10.0 * 8.457498550415039
Epoch 2490, val loss: 0.48827511072158813
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7919837645865042
0.8167065130768674
The final CL Acc:0.79537, 0.00316, The final GNN Acc:0.81637, 0.00185
raw graph: torch.Size([2, 88648])
add edge: torch.Size([2, 111108])
remove edge: torch.Size([2, 66314])
updated graph: torch.Size([2, 88774])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 106.90852355957031 = 1.0853428840637207 + 10.0 * 10.582318305969238
Epoch 0, val loss: 1.0857183933258057
Epoch 10, training loss: 106.90225982666016 = 1.0811901092529297 + 10.0 * 10.582106590270996
Epoch 10, val loss: 1.0816112756729126
Epoch 20, training loss: 106.88760375976562 = 1.0770150423049927 + 10.0 * 10.581058502197266
Epoch 20, val loss: 1.0774649381637573
Epoch 30, training loss: 106.83354187011719 = 1.0727518796920776 + 10.0 * 10.576078414916992
Epoch 30, val loss: 1.0732345581054688
Epoch 40, training loss: 106.62897491455078 = 1.068452000617981 + 10.0 * 10.556052207946777
Epoch 40, val loss: 1.068941354751587
Epoch 50, training loss: 106.03592681884766 = 1.064042091369629 + 10.0 * 10.497188568115234
Epoch 50, val loss: 1.0645471811294556
Epoch 60, training loss: 104.8381118774414 = 1.059720516204834 + 10.0 * 10.377839088439941
Epoch 60, val loss: 1.060314655303955
Epoch 70, training loss: 103.2950210571289 = 1.0557981729507446 + 10.0 * 10.223921775817871
Epoch 70, val loss: 1.0564336776733398
Epoch 80, training loss: 102.28710174560547 = 1.051831841468811 + 10.0 * 10.123526573181152
Epoch 80, val loss: 1.0524346828460693
Epoch 90, training loss: 100.51252746582031 = 1.0470794439315796 + 10.0 * 9.946544647216797
Epoch 90, val loss: 1.047655463218689
Epoch 100, training loss: 97.60062408447266 = 1.0414992570877075 + 10.0 * 9.655912399291992
Epoch 100, val loss: 1.0421303510665894
Epoch 110, training loss: 96.24217987060547 = 1.0356425046920776 + 10.0 * 9.52065372467041
Epoch 110, val loss: 1.0365568399429321
Epoch 120, training loss: 95.09262084960938 = 1.030623197555542 + 10.0 * 9.40619945526123
Epoch 120, val loss: 1.0317362546920776
Epoch 130, training loss: 94.32513427734375 = 1.0258724689483643 + 10.0 * 9.329926490783691
Epoch 130, val loss: 1.0269594192504883
Epoch 140, training loss: 93.6373291015625 = 1.0209797620773315 + 10.0 * 9.261634826660156
Epoch 140, val loss: 1.0222152471542358
Epoch 150, training loss: 93.20475769042969 = 1.016952633857727 + 10.0 * 9.218780517578125
Epoch 150, val loss: 1.0181455612182617
Epoch 160, training loss: 92.65139770507812 = 1.0117915868759155 + 10.0 * 9.163960456848145
Epoch 160, val loss: 1.0130788087844849
Epoch 170, training loss: 91.84039306640625 = 1.006666660308838 + 10.0 * 9.083372116088867
Epoch 170, val loss: 1.008276343345642
Epoch 180, training loss: 91.06729888916016 = 1.0024089813232422 + 10.0 * 9.006488800048828
Epoch 180, val loss: 1.0042332410812378
Epoch 190, training loss: 90.50221252441406 = 0.9982441663742065 + 10.0 * 8.950396537780762
Epoch 190, val loss: 1.0000545978546143
Epoch 200, training loss: 90.01909637451172 = 0.9929311275482178 + 10.0 * 8.902616500854492
Epoch 200, val loss: 0.994662344455719
Epoch 210, training loss: 89.74055480957031 = 0.9863497018814087 + 10.0 * 8.875420570373535
Epoch 210, val loss: 0.9880626201629639
Epoch 220, training loss: 89.4576644897461 = 0.9792238473892212 + 10.0 * 8.847844123840332
Epoch 220, val loss: 0.9810357689857483
Epoch 230, training loss: 89.16488647460938 = 0.9720627069473267 + 10.0 * 8.819282531738281
Epoch 230, val loss: 0.9740440845489502
Epoch 240, training loss: 88.90425872802734 = 0.9646500945091248 + 10.0 * 8.793960571289062
Epoch 240, val loss: 0.9667943716049194
Epoch 250, training loss: 88.72118377685547 = 0.9565079212188721 + 10.0 * 8.776468276977539
Epoch 250, val loss: 0.9587356448173523
Epoch 260, training loss: 88.57118225097656 = 0.9473656415939331 + 10.0 * 8.762381553649902
Epoch 260, val loss: 0.9497973918914795
Epoch 270, training loss: 88.41543579101562 = 0.9378077983856201 + 10.0 * 8.747762680053711
Epoch 270, val loss: 0.9404734373092651
Epoch 280, training loss: 88.24681091308594 = 0.9279966950416565 + 10.0 * 8.731882095336914
Epoch 280, val loss: 0.930962324142456
Epoch 290, training loss: 88.15955352783203 = 0.9179551005363464 + 10.0 * 8.724160194396973
Epoch 290, val loss: 0.9212611317634583
Epoch 300, training loss: 87.90877532958984 = 0.9073548316955566 + 10.0 * 8.700141906738281
Epoch 300, val loss: 0.910811722278595
Epoch 310, training loss: 87.71509552001953 = 0.8963258266448975 + 10.0 * 8.681877136230469
Epoch 310, val loss: 0.9000953435897827
Epoch 320, training loss: 87.55709075927734 = 0.8847610950469971 + 10.0 * 8.66723346710205
Epoch 320, val loss: 0.8888334631919861
Epoch 330, training loss: 87.40367889404297 = 0.8726860880851746 + 10.0 * 8.653099060058594
Epoch 330, val loss: 0.8770659565925598
Epoch 340, training loss: 87.2762680053711 = 0.8600565791130066 + 10.0 * 8.641621589660645
Epoch 340, val loss: 0.8647941946983337
Epoch 350, training loss: 87.1531982421875 = 0.8467068672180176 + 10.0 * 8.630648612976074
Epoch 350, val loss: 0.8517809510231018
Epoch 360, training loss: 87.02193450927734 = 0.8328931927680969 + 10.0 * 8.618904113769531
Epoch 360, val loss: 0.8384026288986206
Epoch 370, training loss: 86.9061050415039 = 0.818678081035614 + 10.0 * 8.608742713928223
Epoch 370, val loss: 0.82461017370224
Epoch 380, training loss: 86.79957580566406 = 0.804133415222168 + 10.0 * 8.599544525146484
Epoch 380, val loss: 0.8105840086936951
Epoch 390, training loss: 86.72169494628906 = 0.7892287373542786 + 10.0 * 8.593246459960938
Epoch 390, val loss: 0.7962049841880798
Epoch 400, training loss: 86.63075256347656 = 0.774275004863739 + 10.0 * 8.585647583007812
Epoch 400, val loss: 0.7818741798400879
Epoch 410, training loss: 86.5425033569336 = 0.7594567537307739 + 10.0 * 8.578304290771484
Epoch 410, val loss: 0.7676634788513184
Epoch 420, training loss: 86.46900177001953 = 0.7445427179336548 + 10.0 * 8.5724458694458
Epoch 420, val loss: 0.753436267375946
Epoch 430, training loss: 86.39967346191406 = 0.7298932075500488 + 10.0 * 8.566977500915527
Epoch 430, val loss: 0.7394666075706482
Epoch 440, training loss: 86.33531951904297 = 0.7154635787010193 + 10.0 * 8.561985969543457
Epoch 440, val loss: 0.7257471680641174
Epoch 450, training loss: 86.29290771484375 = 0.7012650966644287 + 10.0 * 8.559164047241211
Epoch 450, val loss: 0.7122789025306702
Epoch 460, training loss: 86.22696685791016 = 0.6874895095825195 + 10.0 * 8.553947448730469
Epoch 460, val loss: 0.6993280649185181
Epoch 470, training loss: 86.1668930053711 = 0.674234926700592 + 10.0 * 8.54926586151123
Epoch 470, val loss: 0.6868760585784912
Epoch 480, training loss: 86.11422729492188 = 0.6613439917564392 + 10.0 * 8.5452880859375
Epoch 480, val loss: 0.6748313903808594
Epoch 490, training loss: 86.06251525878906 = 0.6489630937576294 + 10.0 * 8.54135513305664
Epoch 490, val loss: 0.663297712802887
Epoch 500, training loss: 86.01730346679688 = 0.6370383501052856 + 10.0 * 8.538026809692383
Epoch 500, val loss: 0.6522495150566101
Epoch 510, training loss: 85.99554443359375 = 0.6255790591239929 + 10.0 * 8.536996841430664
Epoch 510, val loss: 0.6416476964950562
Epoch 520, training loss: 85.93246459960938 = 0.614709198474884 + 10.0 * 8.53177547454834
Epoch 520, val loss: 0.6316949129104614
Epoch 530, training loss: 85.88945007324219 = 0.6044742465019226 + 10.0 * 8.528497695922852
Epoch 530, val loss: 0.622342586517334
Epoch 540, training loss: 85.85033416748047 = 0.5947092771530151 + 10.0 * 8.525562286376953
Epoch 540, val loss: 0.6134934425354004
Epoch 550, training loss: 85.840576171875 = 0.5854093432426453 + 10.0 * 8.525516510009766
Epoch 550, val loss: 0.6051411628723145
Epoch 560, training loss: 85.78421020507812 = 0.5766229629516602 + 10.0 * 8.520758628845215
Epoch 560, val loss: 0.5972026586532593
Epoch 570, training loss: 85.73736572265625 = 0.5682641863822937 + 10.0 * 8.5169095993042
Epoch 570, val loss: 0.5898208618164062
Epoch 580, training loss: 85.69168090820312 = 0.5603840947151184 + 10.0 * 8.513129234313965
Epoch 580, val loss: 0.5828426480293274
Epoch 590, training loss: 85.66557312011719 = 0.5529359579086304 + 10.0 * 8.511263847351074
Epoch 590, val loss: 0.576294481754303
Epoch 600, training loss: 85.67625427246094 = 0.5457726716995239 + 10.0 * 8.51304817199707
Epoch 600, val loss: 0.5701412558555603
Epoch 610, training loss: 85.58152770996094 = 0.5390694737434387 + 10.0 * 8.50424575805664
Epoch 610, val loss: 0.5643165111541748
Epoch 620, training loss: 85.54644012451172 = 0.5327915549278259 + 10.0 * 8.501364707946777
Epoch 620, val loss: 0.5589317083358765
Epoch 630, training loss: 85.50982666015625 = 0.5267914533615112 + 10.0 * 8.498303413391113
Epoch 630, val loss: 0.5538892149925232
Epoch 640, training loss: 85.47777557373047 = 0.5211121439933777 + 10.0 * 8.49566650390625
Epoch 640, val loss: 0.5490840077400208
Epoch 650, training loss: 85.4491958618164 = 0.5156937837600708 + 10.0 * 8.4933500289917
Epoch 650, val loss: 0.5446153879165649
Epoch 660, training loss: 85.46845245361328 = 0.5104784369468689 + 10.0 * 8.495798110961914
Epoch 660, val loss: 0.5403836965560913
Epoch 670, training loss: 85.40790557861328 = 0.5054744482040405 + 10.0 * 8.490242958068848
Epoch 670, val loss: 0.5362585186958313
Epoch 680, training loss: 85.37277221679688 = 0.5007847547531128 + 10.0 * 8.487198829650879
Epoch 680, val loss: 0.5324987173080444
Epoch 690, training loss: 85.34911346435547 = 0.4962844252586365 + 10.0 * 8.485282897949219
Epoch 690, val loss: 0.5289807915687561
Epoch 700, training loss: 85.33749389648438 = 0.4920330047607422 + 10.0 * 8.484545707702637
Epoch 700, val loss: 0.5256086587905884
Epoch 710, training loss: 85.31145477294922 = 0.4879052937030792 + 10.0 * 8.482355117797852
Epoch 710, val loss: 0.5224378705024719
Epoch 720, training loss: 85.29638671875 = 0.48403090238571167 + 10.0 * 8.48123550415039
Epoch 720, val loss: 0.5194542407989502
Epoch 730, training loss: 85.2741928100586 = 0.4803245961666107 + 10.0 * 8.479387283325195
Epoch 730, val loss: 0.5166807770729065
Epoch 740, training loss: 85.25614929199219 = 0.47678959369659424 + 10.0 * 8.477935791015625
Epoch 740, val loss: 0.514083981513977
Epoch 750, training loss: 85.24420928955078 = 0.4734037220478058 + 10.0 * 8.477080345153809
Epoch 750, val loss: 0.5115920305252075
Epoch 760, training loss: 85.25712585449219 = 0.47012755274772644 + 10.0 * 8.478699684143066
Epoch 760, val loss: 0.5092650055885315
Epoch 770, training loss: 85.21198272705078 = 0.4670248329639435 + 10.0 * 8.474495887756348
Epoch 770, val loss: 0.5070163607597351
Epoch 780, training loss: 85.19509887695312 = 0.46405303478240967 + 10.0 * 8.473104476928711
Epoch 780, val loss: 0.5049352049827576
Epoch 790, training loss: 85.17943572998047 = 0.46119067072868347 + 10.0 * 8.471824645996094
Epoch 790, val loss: 0.5030379891395569
Epoch 800, training loss: 85.1655502319336 = 0.4584604799747467 + 10.0 * 8.470708847045898
Epoch 800, val loss: 0.5011466145515442
Epoch 810, training loss: 85.20178985595703 = 0.45583978295326233 + 10.0 * 8.474595069885254
Epoch 810, val loss: 0.49949872493743896
Epoch 820, training loss: 85.14723205566406 = 0.4532451927661896 + 10.0 * 8.469398498535156
Epoch 820, val loss: 0.49770623445510864
Epoch 830, training loss: 85.1263198852539 = 0.4508330821990967 + 10.0 * 8.467548370361328
Epoch 830, val loss: 0.49614065885543823
Epoch 840, training loss: 85.11109161376953 = 0.4484979510307312 + 10.0 * 8.466259002685547
Epoch 840, val loss: 0.4947511553764343
Epoch 850, training loss: 85.09894561767578 = 0.44624659419059753 + 10.0 * 8.465270042419434
Epoch 850, val loss: 0.49333518743515015
Epoch 860, training loss: 85.145751953125 = 0.44405409693717957 + 10.0 * 8.470170021057129
Epoch 860, val loss: 0.49201494455337524
Epoch 870, training loss: 85.09191131591797 = 0.4418894350528717 + 10.0 * 8.465002059936523
Epoch 870, val loss: 0.4906637668609619
Epoch 880, training loss: 85.06574249267578 = 0.4398784041404724 + 10.0 * 8.462586402893066
Epoch 880, val loss: 0.48953649401664734
Epoch 890, training loss: 85.04971313476562 = 0.43789559602737427 + 10.0 * 8.461181640625
Epoch 890, val loss: 0.48840075731277466
Epoch 900, training loss: 85.04011535644531 = 0.4359932243824005 + 10.0 * 8.46041202545166
Epoch 900, val loss: 0.48724210262298584
Epoch 910, training loss: 85.05878448486328 = 0.4341196119785309 + 10.0 * 8.4624662399292
Epoch 910, val loss: 0.4862028956413269
Epoch 920, training loss: 85.0194320678711 = 0.43229910731315613 + 10.0 * 8.45871353149414
Epoch 920, val loss: 0.48514625430107117
Epoch 930, training loss: 85.01243591308594 = 0.43055427074432373 + 10.0 * 8.4581880569458
Epoch 930, val loss: 0.48423996567726135
Epoch 940, training loss: 85.02295684814453 = 0.42883896827697754 + 10.0 * 8.45941162109375
Epoch 940, val loss: 0.4832972586154938
Epoch 950, training loss: 84.99018096923828 = 0.4271588623523712 + 10.0 * 8.45630168914795
Epoch 950, val loss: 0.482349157333374
Epoch 960, training loss: 84.98052978515625 = 0.42555657029151917 + 10.0 * 8.455496788024902
Epoch 960, val loss: 0.481519877910614
Epoch 970, training loss: 84.9717025756836 = 0.423982709646225 + 10.0 * 8.454771995544434
Epoch 970, val loss: 0.48072829842567444
Epoch 980, training loss: 84.96347045898438 = 0.422468900680542 + 10.0 * 8.454099655151367
Epoch 980, val loss: 0.4799107015132904
Epoch 990, training loss: 84.95771026611328 = 0.4209789037704468 + 10.0 * 8.453672409057617
Epoch 990, val loss: 0.4791741669178009
Epoch 1000, training loss: 84.98333740234375 = 0.41951507329940796 + 10.0 * 8.456381797790527
Epoch 1000, val loss: 0.4784134328365326
Epoch 1010, training loss: 84.94690704345703 = 0.41805970668792725 + 10.0 * 8.452884674072266
Epoch 1010, val loss: 0.4776649475097656
Epoch 1020, training loss: 84.93717193603516 = 0.41668596863746643 + 10.0 * 8.452048301696777
Epoch 1020, val loss: 0.47697126865386963
Epoch 1030, training loss: 84.92244720458984 = 0.41534116864204407 + 10.0 * 8.45071029663086
Epoch 1030, val loss: 0.4763307273387909
Epoch 1040, training loss: 84.91793823242188 = 0.4140379726886749 + 10.0 * 8.450389862060547
Epoch 1040, val loss: 0.4756886661052704
Epoch 1050, training loss: 84.93572998046875 = 0.4127543270587921 + 10.0 * 8.45229721069336
Epoch 1050, val loss: 0.47513675689697266
Epoch 1060, training loss: 84.91180419921875 = 0.4114704430103302 + 10.0 * 8.450033187866211
Epoch 1060, val loss: 0.4744037389755249
Epoch 1070, training loss: 84.89999389648438 = 0.4102463722229004 + 10.0 * 8.448974609375
Epoch 1070, val loss: 0.4738697409629822
Epoch 1080, training loss: 84.88856506347656 = 0.409038245677948 + 10.0 * 8.447953224182129
Epoch 1080, val loss: 0.47327494621276855
Epoch 1090, training loss: 84.88352966308594 = 0.4078603684902191 + 10.0 * 8.447566986083984
Epoch 1090, val loss: 0.47274065017700195
Epoch 1100, training loss: 84.9195785522461 = 0.40668344497680664 + 10.0 * 8.451289176940918
Epoch 1100, val loss: 0.47218137979507446
Epoch 1110, training loss: 84.880615234375 = 0.40554943680763245 + 10.0 * 8.44750690460205
Epoch 1110, val loss: 0.471663236618042
Epoch 1120, training loss: 84.86347198486328 = 0.4044262766838074 + 10.0 * 8.445904731750488
Epoch 1120, val loss: 0.4711810052394867
Epoch 1130, training loss: 84.85202026367188 = 0.4033392667770386 + 10.0 * 8.444868087768555
Epoch 1130, val loss: 0.4706419110298157
Epoch 1140, training loss: 84.86676788330078 = 0.402267724275589 + 10.0 * 8.446450233459473
Epoch 1140, val loss: 0.47011610865592957
Epoch 1150, training loss: 84.8504409790039 = 0.4011983871459961 + 10.0 * 8.444924354553223
Epoch 1150, val loss: 0.46980446577072144
Epoch 1160, training loss: 84.83506774902344 = 0.4001635015010834 + 10.0 * 8.443490982055664
Epoch 1160, val loss: 0.4692227244377136
Epoch 1170, training loss: 84.82511901855469 = 0.39914974570274353 + 10.0 * 8.442597389221191
Epoch 1170, val loss: 0.46882230043411255
Epoch 1180, training loss: 84.89895629882812 = 0.3981483578681946 + 10.0 * 8.450080871582031
Epoch 1180, val loss: 0.4685325622558594
Epoch 1190, training loss: 84.83917236328125 = 0.3971119225025177 + 10.0 * 8.444206237792969
Epoch 1190, val loss: 0.467786580324173
Epoch 1200, training loss: 84.80755615234375 = 0.39615529775619507 + 10.0 * 8.441140174865723
Epoch 1200, val loss: 0.4674608111381531
Epoch 1210, training loss: 84.79786682128906 = 0.39520126581192017 + 10.0 * 8.440266609191895
Epoch 1210, val loss: 0.4670620560646057
Epoch 1220, training loss: 84.79000854492188 = 0.3942669630050659 + 10.0 * 8.439574241638184
Epoch 1220, val loss: 0.4666934013366699
Epoch 1230, training loss: 84.78521728515625 = 0.3933420181274414 + 10.0 * 8.439188003540039
Epoch 1230, val loss: 0.46628791093826294
Epoch 1240, training loss: 84.80923461914062 = 0.39240992069244385 + 10.0 * 8.441682815551758
Epoch 1240, val loss: 0.46593937277793884
Epoch 1250, training loss: 84.76839447021484 = 0.39149031043052673 + 10.0 * 8.437690734863281
Epoch 1250, val loss: 0.4654504060745239
Epoch 1260, training loss: 84.76576232910156 = 0.3905947804450989 + 10.0 * 8.437517166137695
Epoch 1260, val loss: 0.4650703966617584
Epoch 1270, training loss: 84.77324676513672 = 0.38971590995788574 + 10.0 * 8.438352584838867
Epoch 1270, val loss: 0.46473628282546997
Epoch 1280, training loss: 84.75724792480469 = 0.3888023793697357 + 10.0 * 8.436844825744629
Epoch 1280, val loss: 0.46434909105300903
Epoch 1290, training loss: 84.74288177490234 = 0.38793548941612244 + 10.0 * 8.435495376586914
Epoch 1290, val loss: 0.46386557817459106
Epoch 1300, training loss: 84.73603820800781 = 0.38708171248435974 + 10.0 * 8.434895515441895
Epoch 1300, val loss: 0.46358466148376465
Epoch 1310, training loss: 84.72818756103516 = 0.3862386643886566 + 10.0 * 8.434194564819336
Epoch 1310, val loss: 0.4631637930870056
Epoch 1320, training loss: 84.720947265625 = 0.3854043781757355 + 10.0 * 8.433553695678711
Epoch 1320, val loss: 0.4628206193447113
Epoch 1330, training loss: 84.74713134765625 = 0.3845614790916443 + 10.0 * 8.436257362365723
Epoch 1330, val loss: 0.4624136984348297
Epoch 1340, training loss: 84.72531127929688 = 0.3837200999259949 + 10.0 * 8.434159278869629
Epoch 1340, val loss: 0.46215009689331055
Epoch 1350, training loss: 84.7506332397461 = 0.38287216424942017 + 10.0 * 8.436776161193848
Epoch 1350, val loss: 0.4618738293647766
Epoch 1360, training loss: 84.70433044433594 = 0.3820095956325531 + 10.0 * 8.432231903076172
Epoch 1360, val loss: 0.4612613916397095
Epoch 1370, training loss: 84.68902587890625 = 0.3811943233013153 + 10.0 * 8.43078327178955
Epoch 1370, val loss: 0.46110087633132935
Epoch 1380, training loss: 84.68230438232422 = 0.38037732243537903 + 10.0 * 8.430192947387695
Epoch 1380, val loss: 0.46062952280044556
Epoch 1390, training loss: 84.67611694335938 = 0.3795645534992218 + 10.0 * 8.429655075073242
Epoch 1390, val loss: 0.46036043763160706
Epoch 1400, training loss: 84.71280670166016 = 0.3787468373775482 + 10.0 * 8.433405876159668
Epoch 1400, val loss: 0.4600064754486084
Epoch 1410, training loss: 84.66316223144531 = 0.37789613008499146 + 10.0 * 8.428525924682617
Epoch 1410, val loss: 0.45950672030448914
Epoch 1420, training loss: 84.65110778808594 = 0.37708744406700134 + 10.0 * 8.427401542663574
Epoch 1420, val loss: 0.4592795670032501
Epoch 1430, training loss: 84.64620971679688 = 0.3762848377227783 + 10.0 * 8.426992416381836
Epoch 1430, val loss: 0.4588989019393921
Epoch 1440, training loss: 84.64512634277344 = 0.37548747658729553 + 10.0 * 8.426963806152344
Epoch 1440, val loss: 0.4586523473262787
Epoch 1450, training loss: 84.65644073486328 = 0.37468212842941284 + 10.0 * 8.428175926208496
Epoch 1450, val loss: 0.4583229422569275
Epoch 1460, training loss: 84.63743591308594 = 0.3738585412502289 + 10.0 * 8.426358222961426
Epoch 1460, val loss: 0.4578130543231964
Epoch 1470, training loss: 84.63334655761719 = 0.3730607330799103 + 10.0 * 8.42602825164795
Epoch 1470, val loss: 0.45754870772361755
Epoch 1480, training loss: 84.64796447753906 = 0.3722578287124634 + 10.0 * 8.427570343017578
Epoch 1480, val loss: 0.45724576711654663
Epoch 1490, training loss: 84.61029052734375 = 0.37144365906715393 + 10.0 * 8.423884391784668
Epoch 1490, val loss: 0.4567107558250427
Epoch 1500, training loss: 84.60353088378906 = 0.3706510365009308 + 10.0 * 8.423288345336914
Epoch 1500, val loss: 0.45649585127830505
Epoch 1510, training loss: 84.59497833251953 = 0.36986663937568665 + 10.0 * 8.422511100769043
Epoch 1510, val loss: 0.4560982882976532
Epoch 1520, training loss: 84.60431671142578 = 0.3690790832042694 + 10.0 * 8.423523902893066
Epoch 1520, val loss: 0.45579108595848083
Epoch 1530, training loss: 84.60961151123047 = 0.36826691031455994 + 10.0 * 8.424135208129883
Epoch 1530, val loss: 0.4554300010204315
Epoch 1540, training loss: 84.58634948730469 = 0.3674648404121399 + 10.0 * 8.42188835144043
Epoch 1540, val loss: 0.4549863040447235
Epoch 1550, training loss: 84.57421112060547 = 0.36668404936790466 + 10.0 * 8.42075252532959
Epoch 1550, val loss: 0.45468637347221375
Epoch 1560, training loss: 84.56843566894531 = 0.36590301990509033 + 10.0 * 8.420252799987793
Epoch 1560, val loss: 0.4543134570121765
Epoch 1570, training loss: 84.5738754272461 = 0.3651241958141327 + 10.0 * 8.42087459564209
Epoch 1570, val loss: 0.45394042134284973
Epoch 1580, training loss: 84.5805892944336 = 0.36431995034217834 + 10.0 * 8.421627044677734
Epoch 1580, val loss: 0.45357778668403625
Epoch 1590, training loss: 84.56721496582031 = 0.3635315001010895 + 10.0 * 8.420368194580078
Epoch 1590, val loss: 0.4532376825809479
Epoch 1600, training loss: 84.54827117919922 = 0.36274999380111694 + 10.0 * 8.41855239868164
Epoch 1600, val loss: 0.452872097492218
Epoch 1610, training loss: 84.54576873779297 = 0.36197221279144287 + 10.0 * 8.418379783630371
Epoch 1610, val loss: 0.45246919989585876
Epoch 1620, training loss: 84.54988098144531 = 0.36119702458381653 + 10.0 * 8.418868064880371
Epoch 1620, val loss: 0.45209959149360657
Epoch 1630, training loss: 84.59147644042969 = 0.3604085445404053 + 10.0 * 8.423107147216797
Epoch 1630, val loss: 0.4519006013870239
Epoch 1640, training loss: 84.5470199584961 = 0.35959893465042114 + 10.0 * 8.418742179870605
Epoch 1640, val loss: 0.451246052980423
Epoch 1650, training loss: 84.53131103515625 = 0.35882723331451416 + 10.0 * 8.417248725891113
Epoch 1650, val loss: 0.4511139392852783
Epoch 1660, training loss: 84.52099609375 = 0.3580443561077118 + 10.0 * 8.416295051574707
Epoch 1660, val loss: 0.45063239336013794
Epoch 1670, training loss: 84.51490020751953 = 0.3572731912136078 + 10.0 * 8.415761947631836
Epoch 1670, val loss: 0.45031923055648804
Epoch 1680, training loss: 84.51043701171875 = 0.35649776458740234 + 10.0 * 8.415393829345703
Epoch 1680, val loss: 0.44997134804725647
Epoch 1690, training loss: 84.50820922851562 = 0.35571718215942383 + 10.0 * 8.41524887084961
Epoch 1690, val loss: 0.4496004581451416
Epoch 1700, training loss: 84.58904266357422 = 0.35493001341819763 + 10.0 * 8.42341136932373
Epoch 1700, val loss: 0.4492937922477722
Epoch 1710, training loss: 84.52864837646484 = 0.35408177971839905 + 10.0 * 8.41745662689209
Epoch 1710, val loss: 0.4486052393913269
Epoch 1720, training loss: 84.50125122070312 = 0.35329195857048035 + 10.0 * 8.414795875549316
Epoch 1720, val loss: 0.4484366178512573
Epoch 1730, training loss: 84.49434661865234 = 0.35249578952789307 + 10.0 * 8.414185523986816
Epoch 1730, val loss: 0.4479849338531494
Epoch 1740, training loss: 84.48600769042969 = 0.3517129421234131 + 10.0 * 8.413429260253906
Epoch 1740, val loss: 0.44760826230049133
Epoch 1750, training loss: 84.4808349609375 = 0.35091885924339294 + 10.0 * 8.412991523742676
Epoch 1750, val loss: 0.4472052752971649
Epoch 1760, training loss: 84.47669219970703 = 0.3501221537590027 + 10.0 * 8.412656784057617
Epoch 1760, val loss: 0.4468097984790802
Epoch 1770, training loss: 84.48193359375 = 0.3493219316005707 + 10.0 * 8.413261413574219
Epoch 1770, val loss: 0.4464881122112274
Epoch 1780, training loss: 84.48171997070312 = 0.3484908640384674 + 10.0 * 8.413323402404785
Epoch 1780, val loss: 0.44600313901901245
Epoch 1790, training loss: 84.47350311279297 = 0.3476802110671997 + 10.0 * 8.412582397460938
Epoch 1790, val loss: 0.44566115736961365
Epoch 1800, training loss: 84.48627471923828 = 0.3468646705150604 + 10.0 * 8.413941383361816
Epoch 1800, val loss: 0.44509464502334595
Epoch 1810, training loss: 84.46196746826172 = 0.3460543751716614 + 10.0 * 8.411591529846191
Epoch 1810, val loss: 0.4447329044342041
Epoch 1820, training loss: 84.4588851928711 = 0.3452610373497009 + 10.0 * 8.411362648010254
Epoch 1820, val loss: 0.4444984793663025
Epoch 1830, training loss: 84.45407104492188 = 0.3444536328315735 + 10.0 * 8.410962104797363
Epoch 1830, val loss: 0.4440568685531616
Epoch 1840, training loss: 84.45127868652344 = 0.34364402294158936 + 10.0 * 8.41076374053955
Epoch 1840, val loss: 0.44365689158439636
Epoch 1850, training loss: 84.45935821533203 = 0.3428279161453247 + 10.0 * 8.411653518676758
Epoch 1850, val loss: 0.4432726800441742
Epoch 1860, training loss: 84.44422149658203 = 0.34200432896614075 + 10.0 * 8.410222053527832
Epoch 1860, val loss: 0.4428038001060486
Epoch 1870, training loss: 84.46907043457031 = 0.34117716550827026 + 10.0 * 8.412789344787598
Epoch 1870, val loss: 0.44248852133750916
Epoch 1880, training loss: 84.4308853149414 = 0.3403385579586029 + 10.0 * 8.40905475616455
Epoch 1880, val loss: 0.4419301152229309
Epoch 1890, training loss: 84.42596435546875 = 0.33951684832572937 + 10.0 * 8.408644676208496
Epoch 1890, val loss: 0.44158074259757996
Epoch 1900, training loss: 84.42084503173828 = 0.3386921286582947 + 10.0 * 8.408215522766113
Epoch 1900, val loss: 0.4411875307559967
Epoch 1910, training loss: 84.426025390625 = 0.33786341547966003 + 10.0 * 8.40881633758545
Epoch 1910, val loss: 0.4408263564109802
Epoch 1920, training loss: 84.45142364501953 = 0.3370158076286316 + 10.0 * 8.4114408493042
Epoch 1920, val loss: 0.4404539167881012
Epoch 1930, training loss: 84.41261291503906 = 0.3361552059650421 + 10.0 * 8.407645225524902
Epoch 1930, val loss: 0.43992388248443604
Epoch 1940, training loss: 84.40569305419922 = 0.33530619740486145 + 10.0 * 8.407038688659668
Epoch 1940, val loss: 0.4395601451396942
Epoch 1950, training loss: 84.4085464477539 = 0.3344607353210449 + 10.0 * 8.407408714294434
Epoch 1950, val loss: 0.43923720717430115
Epoch 1960, training loss: 84.40115356445312 = 0.33360061049461365 + 10.0 * 8.406755447387695
Epoch 1960, val loss: 0.43881747126579285
Epoch 1970, training loss: 84.39241790771484 = 0.33274045586586 + 10.0 * 8.405967712402344
Epoch 1970, val loss: 0.43836089968681335
Epoch 1980, training loss: 84.3907699584961 = 0.3318891227245331 + 10.0 * 8.405888557434082
Epoch 1980, val loss: 0.4380149245262146
Epoch 1990, training loss: 84.44747161865234 = 0.3310316503047943 + 10.0 * 8.411643981933594
Epoch 1990, val loss: 0.4376937448978424
Epoch 2000, training loss: 84.41983795166016 = 0.3301186263561249 + 10.0 * 8.408971786499023
Epoch 2000, val loss: 0.43695491552352905
Epoch 2010, training loss: 84.3832778930664 = 0.32925546169281006 + 10.0 * 8.405402183532715
Epoch 2010, val loss: 0.4368046522140503
Epoch 2020, training loss: 84.37324523925781 = 0.3283810019493103 + 10.0 * 8.404485702514648
Epoch 2020, val loss: 0.4363306760787964
Epoch 2030, training loss: 84.36795806884766 = 0.3275068998336792 + 10.0 * 8.404045104980469
Epoch 2030, val loss: 0.4358961582183838
Epoch 2040, training loss: 84.36318969726562 = 0.32663220167160034 + 10.0 * 8.403656005859375
Epoch 2040, val loss: 0.4355056583881378
Epoch 2050, training loss: 84.36553955078125 = 0.3257450759410858 + 10.0 * 8.403979301452637
Epoch 2050, val loss: 0.4350939691066742
Epoch 2060, training loss: 84.43035888671875 = 0.32482853531837463 + 10.0 * 8.410552978515625
Epoch 2060, val loss: 0.43465614318847656
Epoch 2070, training loss: 84.3756103515625 = 0.32390767335891724 + 10.0 * 8.405170440673828
Epoch 2070, val loss: 0.4342338740825653
Epoch 2080, training loss: 84.35720825195312 = 0.32299840450286865 + 10.0 * 8.403421401977539
Epoch 2080, val loss: 0.433883398771286
Epoch 2090, training loss: 84.34650421142578 = 0.32208916544914246 + 10.0 * 8.402441024780273
Epoch 2090, val loss: 0.43346405029296875
Epoch 2100, training loss: 84.33983612060547 = 0.32117772102355957 + 10.0 * 8.40186595916748
Epoch 2100, val loss: 0.43308401107788086
Epoch 2110, training loss: 84.3426284790039 = 0.32025575637817383 + 10.0 * 8.402236938476562
Epoch 2110, val loss: 0.4326297640800476
Epoch 2120, training loss: 84.3898696899414 = 0.31930118799209595 + 10.0 * 8.40705680847168
Epoch 2120, val loss: 0.4322276711463928
Epoch 2130, training loss: 84.33525848388672 = 0.31837213039398193 + 10.0 * 8.401688575744629
Epoch 2130, val loss: 0.4318646490573883
Epoch 2140, training loss: 84.32562255859375 = 0.31743767857551575 + 10.0 * 8.400818824768066
Epoch 2140, val loss: 0.4315004050731659
Epoch 2150, training loss: 84.32308959960938 = 0.3165018558502197 + 10.0 * 8.40065860748291
Epoch 2150, val loss: 0.4310450255870819
Epoch 2160, training loss: 84.31710052490234 = 0.31556594371795654 + 10.0 * 8.400153160095215
Epoch 2160, val loss: 0.4306888282299042
Epoch 2170, training loss: 84.32017517089844 = 0.3146176040172577 + 10.0 * 8.400555610656738
Epoch 2170, val loss: 0.430245965719223
Epoch 2180, training loss: 84.38533020019531 = 0.31364622712135315 + 10.0 * 8.4071683883667
Epoch 2180, val loss: 0.4298652410507202
Epoch 2190, training loss: 84.32949829101562 = 0.3126862049102783 + 10.0 * 8.401681900024414
Epoch 2190, val loss: 0.42950019240379333
Epoch 2200, training loss: 84.30803680419922 = 0.3117140531539917 + 10.0 * 8.399632453918457
Epoch 2200, val loss: 0.4290647506713867
Epoch 2210, training loss: 84.29844665527344 = 0.31075426936149597 + 10.0 * 8.39876937866211
Epoch 2210, val loss: 0.428661048412323
Epoch 2220, training loss: 84.29618835449219 = 0.30978813767433167 + 10.0 * 8.398639678955078
Epoch 2220, val loss: 0.4283128082752228
Epoch 2230, training loss: 84.35206604003906 = 0.3088195025920868 + 10.0 * 8.404324531555176
Epoch 2230, val loss: 0.4281531274318695
Epoch 2240, training loss: 84.30693054199219 = 0.30781176686286926 + 10.0 * 8.399911880493164
Epoch 2240, val loss: 0.4273133873939514
Epoch 2250, training loss: 84.28563690185547 = 0.30684423446655273 + 10.0 * 8.397878646850586
Epoch 2250, val loss: 0.4271291494369507
Epoch 2260, training loss: 84.28263854980469 = 0.30586594343185425 + 10.0 * 8.397677421569824
Epoch 2260, val loss: 0.4267377555370331
Epoch 2270, training loss: 84.31026458740234 = 0.3048931360244751 + 10.0 * 8.400537490844727
Epoch 2270, val loss: 0.4264335334300995
Epoch 2280, training loss: 84.2752685546875 = 0.3038817346096039 + 10.0 * 8.397138595581055
Epoch 2280, val loss: 0.4258420169353485
Epoch 2290, training loss: 84.27523040771484 = 0.3028993010520935 + 10.0 * 8.397233009338379
Epoch 2290, val loss: 0.4255725145339966
Epoch 2300, training loss: 84.26852416992188 = 0.30191534757614136 + 10.0 * 8.396660804748535
Epoch 2300, val loss: 0.4252159297466278
Epoch 2310, training loss: 84.2596664428711 = 0.30092853307724 + 10.0 * 8.3958740234375
Epoch 2310, val loss: 0.42486169934272766
Epoch 2320, training loss: 84.2581558227539 = 0.2999425232410431 + 10.0 * 8.395821571350098
Epoch 2320, val loss: 0.42454037070274353
Epoch 2330, training loss: 84.27099609375 = 0.2989455461502075 + 10.0 * 8.397205352783203
Epoch 2330, val loss: 0.4241314232349396
Epoch 2340, training loss: 84.28226470947266 = 0.29792699217796326 + 10.0 * 8.398433685302734
Epoch 2340, val loss: 0.42375582456588745
Epoch 2350, training loss: 84.26337432861328 = 0.2969364523887634 + 10.0 * 8.39664363861084
Epoch 2350, val loss: 0.4235674738883972
Epoch 2360, training loss: 84.24813079833984 = 0.2959241569042206 + 10.0 * 8.395220756530762
Epoch 2360, val loss: 0.4231775104999542
Epoch 2370, training loss: 84.24891662597656 = 0.29492056369781494 + 10.0 * 8.39539909362793
Epoch 2370, val loss: 0.42287033796310425
Epoch 2380, training loss: 84.25120544433594 = 0.29389649629592896 + 10.0 * 8.395730972290039
Epoch 2380, val loss: 0.42240840196609497
Epoch 2390, training loss: 84.23773956298828 = 0.29287731647491455 + 10.0 * 8.394486427307129
Epoch 2390, val loss: 0.4220597445964813
Epoch 2400, training loss: 84.23011779785156 = 0.29187169671058655 + 10.0 * 8.393824577331543
Epoch 2400, val loss: 0.42179715633392334
Epoch 2410, training loss: 84.22805786132812 = 0.29086026549339294 + 10.0 * 8.393719673156738
Epoch 2410, val loss: 0.4214307367801666
Epoch 2420, training loss: 84.2474594116211 = 0.2898450195789337 + 10.0 * 8.395761489868164
Epoch 2420, val loss: 0.42107611894607544
Epoch 2430, training loss: 84.24774932861328 = 0.2888169586658478 + 10.0 * 8.395893096923828
Epoch 2430, val loss: 0.42080816626548767
Epoch 2440, training loss: 84.23088073730469 = 0.287780225276947 + 10.0 * 8.394309997558594
Epoch 2440, val loss: 0.4204857647418976
Epoch 2450, training loss: 84.22294616699219 = 0.286779522895813 + 10.0 * 8.393616676330566
Epoch 2450, val loss: 0.42041993141174316
Epoch 2460, training loss: 84.21305084228516 = 0.2857504189014435 + 10.0 * 8.392729759216309
Epoch 2460, val loss: 0.41999319195747375
Epoch 2470, training loss: 84.204833984375 = 0.2847371995449066 + 10.0 * 8.392009735107422
Epoch 2470, val loss: 0.41985705494880676
Epoch 2480, training loss: 84.20211791992188 = 0.2837219834327698 + 10.0 * 8.391839981079102
Epoch 2480, val loss: 0.4195919930934906
Epoch 2490, training loss: 84.2010726928711 = 0.28270086646080017 + 10.0 * 8.391837120056152
Epoch 2490, val loss: 0.41940316557884216
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8361237950279046
0.8643048612620445
=== training gcn model ===
Epoch 0, training loss: 106.91577911376953 = 1.0927810668945312 + 10.0 * 10.582300186157227
Epoch 0, val loss: 1.0907740592956543
Epoch 10, training loss: 106.90909576416016 = 1.0885729789733887 + 10.0 * 10.582052230834961
Epoch 10, val loss: 1.0866235494613647
Epoch 20, training loss: 106.89220428466797 = 1.0842703580856323 + 10.0 * 10.580793380737305
Epoch 20, val loss: 1.082366704940796
Epoch 30, training loss: 106.82811737060547 = 1.079514980316162 + 10.0 * 10.574860572814941
Epoch 30, val loss: 1.0776519775390625
Epoch 40, training loss: 106.59135437011719 = 1.0739880800247192 + 10.0 * 10.551736831665039
Epoch 40, val loss: 1.0721960067749023
Epoch 50, training loss: 105.93070220947266 = 1.0677307844161987 + 10.0 * 10.486296653747559
Epoch 50, val loss: 1.0660427808761597
Epoch 60, training loss: 104.5073013305664 = 1.0611852407455444 + 10.0 * 10.344611167907715
Epoch 60, val loss: 1.0596805810928345
Epoch 70, training loss: 102.09687042236328 = 1.0539450645446777 + 10.0 * 10.104291915893555
Epoch 70, val loss: 1.0524265766143799
Epoch 80, training loss: 99.0370101928711 = 1.0448026657104492 + 10.0 * 9.79922103881836
Epoch 80, val loss: 1.04352867603302
Epoch 90, training loss: 97.80663299560547 = 1.03783118724823 + 10.0 * 9.6768798828125
Epoch 90, val loss: 1.0370278358459473
Epoch 100, training loss: 97.06661224365234 = 1.032157063484192 + 10.0 * 9.603445053100586
Epoch 100, val loss: 1.0316321849822998
Epoch 110, training loss: 96.45927429199219 = 1.028069257736206 + 10.0 * 9.543120384216309
Epoch 110, val loss: 1.0277135372161865
Epoch 120, training loss: 95.8485107421875 = 1.0241984128952026 + 10.0 * 9.482431411743164
Epoch 120, val loss: 1.0239700078964233
Epoch 130, training loss: 95.19892120361328 = 1.019963026046753 + 10.0 * 9.417895317077637
Epoch 130, val loss: 1.0198346376419067
Epoch 140, training loss: 94.50022888183594 = 1.0154898166656494 + 10.0 * 9.348474502563477
Epoch 140, val loss: 1.0154975652694702
Epoch 150, training loss: 93.78533935546875 = 1.0111489295959473 + 10.0 * 9.277419090270996
Epoch 150, val loss: 1.0113766193389893
Epoch 160, training loss: 93.2106704711914 = 1.006916880607605 + 10.0 * 9.220375061035156
Epoch 160, val loss: 1.007263422012329
Epoch 170, training loss: 92.71046447753906 = 1.0011730194091797 + 10.0 * 9.170928955078125
Epoch 170, val loss: 1.0015186071395874
Epoch 180, training loss: 91.9519271850586 = 0.994249701499939 + 10.0 * 9.095767974853516
Epoch 180, val loss: 0.994913637638092
Epoch 190, training loss: 91.12809753417969 = 0.9888284206390381 + 10.0 * 9.01392650604248
Epoch 190, val loss: 0.9899476170539856
Epoch 200, training loss: 90.6583023071289 = 0.983881413936615 + 10.0 * 8.967442512512207
Epoch 200, val loss: 0.9851515293121338
Epoch 210, training loss: 90.12074279785156 = 0.9776765704154968 + 10.0 * 8.914306640625
Epoch 210, val loss: 0.9792415499687195
Epoch 220, training loss: 89.5577392578125 = 0.9715074896812439 + 10.0 * 8.858623504638672
Epoch 220, val loss: 0.9733349084854126
Epoch 230, training loss: 89.18043518066406 = 0.964474618434906 + 10.0 * 8.821596145629883
Epoch 230, val loss: 0.9663832783699036
Epoch 240, training loss: 88.93888854980469 = 0.9556390643119812 + 10.0 * 8.798324584960938
Epoch 240, val loss: 0.9576185345649719
Epoch 250, training loss: 88.77806091308594 = 0.9458616375923157 + 10.0 * 8.783220291137695
Epoch 250, val loss: 0.9482311010360718
Epoch 260, training loss: 88.6646499633789 = 0.9354443550109863 + 10.0 * 8.772920608520508
Epoch 260, val loss: 0.9382462501525879
Epoch 270, training loss: 88.56499481201172 = 0.9245715737342834 + 10.0 * 8.764042854309082
Epoch 270, val loss: 0.9277994632720947
Epoch 280, training loss: 88.46693420410156 = 0.913650393486023 + 10.0 * 8.755328178405762
Epoch 280, val loss: 0.9172870516777039
Epoch 290, training loss: 88.36966705322266 = 0.9027065634727478 + 10.0 * 8.746695518493652
Epoch 290, val loss: 0.9068092703819275
Epoch 300, training loss: 88.2584457397461 = 0.8916712999343872 + 10.0 * 8.736677169799805
Epoch 300, val loss: 0.896220326423645
Epoch 310, training loss: 88.12273406982422 = 0.880497395992279 + 10.0 * 8.724223136901855
Epoch 310, val loss: 0.8854551315307617
Epoch 320, training loss: 87.96693420410156 = 0.8694259524345398 + 10.0 * 8.70975112915039
Epoch 320, val loss: 0.8748527765274048
Epoch 330, training loss: 87.79397583007812 = 0.8585363626480103 + 10.0 * 8.693544387817383
Epoch 330, val loss: 0.8644689321517944
Epoch 340, training loss: 87.65299224853516 = 0.8475068807601929 + 10.0 * 8.680548667907715
Epoch 340, val loss: 0.8537224531173706
Epoch 350, training loss: 87.50384521484375 = 0.835993766784668 + 10.0 * 8.66678524017334
Epoch 350, val loss: 0.8428075909614563
Epoch 360, training loss: 87.36734008789062 = 0.8245359063148499 + 10.0 * 8.654279708862305
Epoch 360, val loss: 0.83188396692276
Epoch 370, training loss: 87.22098541259766 = 0.8134044408798218 + 10.0 * 8.64075756072998
Epoch 370, val loss: 0.8212915062904358
Epoch 380, training loss: 87.10408782958984 = 0.8026501536369324 + 10.0 * 8.630144119262695
Epoch 380, val loss: 0.8109251260757446
Epoch 390, training loss: 86.94782257080078 = 0.7916468977928162 + 10.0 * 8.615617752075195
Epoch 390, val loss: 0.8004809021949768
Epoch 400, training loss: 86.81622314453125 = 0.7804741859436035 + 10.0 * 8.603574752807617
Epoch 400, val loss: 0.7898240089416504
Epoch 410, training loss: 86.70975494384766 = 0.7688631415367126 + 10.0 * 8.59408950805664
Epoch 410, val loss: 0.7786738872528076
Epoch 420, training loss: 86.6347427368164 = 0.7566691040992737 + 10.0 * 8.587807655334473
Epoch 420, val loss: 0.7671342492103577
Epoch 430, training loss: 86.53671264648438 = 0.744317889213562 + 10.0 * 8.579239845275879
Epoch 430, val loss: 0.7553306221961975
Epoch 440, training loss: 86.45361328125 = 0.7319506406784058 + 10.0 * 8.572166442871094
Epoch 440, val loss: 0.7435261011123657
Epoch 450, training loss: 86.39473724365234 = 0.7194753885269165 + 10.0 * 8.567525863647461
Epoch 450, val loss: 0.7316790819168091
Epoch 460, training loss: 86.29806518554688 = 0.7069373726844788 + 10.0 * 8.559112548828125
Epoch 460, val loss: 0.7198019027709961
Epoch 470, training loss: 86.21881103515625 = 0.6946109533309937 + 10.0 * 8.552419662475586
Epoch 470, val loss: 0.7081447839736938
Epoch 480, training loss: 86.1629867553711 = 0.6822939515113831 + 10.0 * 8.54806900024414
Epoch 480, val loss: 0.6964960694313049
Epoch 490, training loss: 86.07710266113281 = 0.6700635552406311 + 10.0 * 8.540703773498535
Epoch 490, val loss: 0.6849985718727112
Epoch 500, training loss: 86.00111389160156 = 0.6580649614334106 + 10.0 * 8.53430461883545
Epoch 500, val loss: 0.6736383438110352
Epoch 510, training loss: 85.93675994873047 = 0.6462207436561584 + 10.0 * 8.529054641723633
Epoch 510, val loss: 0.6624066829681396
Epoch 520, training loss: 85.8957290649414 = 0.6345163583755493 + 10.0 * 8.526121139526367
Epoch 520, val loss: 0.651421844959259
Epoch 530, training loss: 85.82135009765625 = 0.6230559945106506 + 10.0 * 8.519829750061035
Epoch 530, val loss: 0.6407603621482849
Epoch 540, training loss: 85.768798828125 = 0.6120623350143433 + 10.0 * 8.515673637390137
Epoch 540, val loss: 0.6304925084114075
Epoch 550, training loss: 85.71453094482422 = 0.6014503240585327 + 10.0 * 8.511308670043945
Epoch 550, val loss: 0.6206136345863342
Epoch 560, training loss: 85.67280578613281 = 0.5911557674407959 + 10.0 * 8.50816535949707
Epoch 560, val loss: 0.611076295375824
Epoch 570, training loss: 85.61390686035156 = 0.5812978744506836 + 10.0 * 8.503260612487793
Epoch 570, val loss: 0.6020317077636719
Epoch 580, training loss: 85.56986236572266 = 0.5719896554946899 + 10.0 * 8.499787330627441
Epoch 580, val loss: 0.5934566855430603
Epoch 590, training loss: 85.52730560302734 = 0.5630823969841003 + 10.0 * 8.49642276763916
Epoch 590, val loss: 0.5853677988052368
Epoch 600, training loss: 85.48785400390625 = 0.5546250343322754 + 10.0 * 8.493322372436523
Epoch 600, val loss: 0.5777603983879089
Epoch 610, training loss: 85.45707702636719 = 0.546578586101532 + 10.0 * 8.491049766540527
Epoch 610, val loss: 0.5705347061157227
Epoch 620, training loss: 85.42781066894531 = 0.5388906598091125 + 10.0 * 8.488892555236816
Epoch 620, val loss: 0.5637211203575134
Epoch 630, training loss: 85.39331817626953 = 0.531675398349762 + 10.0 * 8.486164093017578
Epoch 630, val loss: 0.5573411583900452
Epoch 640, training loss: 85.36114501953125 = 0.5248992443084717 + 10.0 * 8.483624458312988
Epoch 640, val loss: 0.5514135956764221
Epoch 650, training loss: 85.33263397216797 = 0.5184831023216248 + 10.0 * 8.481414794921875
Epoch 650, val loss: 0.5458719730377197
Epoch 660, training loss: 85.3085708618164 = 0.5124395489692688 + 10.0 * 8.479613304138184
Epoch 660, val loss: 0.5407112836837769
Epoch 670, training loss: 85.2913818359375 = 0.5066726803779602 + 10.0 * 8.478470802307129
Epoch 670, val loss: 0.5358468294143677
Epoch 680, training loss: 85.25370788574219 = 0.5012348294258118 + 10.0 * 8.475247383117676
Epoch 680, val loss: 0.531272292137146
Epoch 690, training loss: 85.22901916503906 = 0.4961104989051819 + 10.0 * 8.473291397094727
Epoch 690, val loss: 0.5270481705665588
Epoch 700, training loss: 85.20256805419922 = 0.49125128984451294 + 10.0 * 8.471132278442383
Epoch 700, val loss: 0.52311110496521
Epoch 710, training loss: 85.26573944091797 = 0.4866489768028259 + 10.0 * 8.477909088134766
Epoch 710, val loss: 0.5193239450454712
Epoch 720, training loss: 85.17539978027344 = 0.48219138383865356 + 10.0 * 8.469320297241211
Epoch 720, val loss: 0.5158812403678894
Epoch 730, training loss: 85.13751220703125 = 0.4780866503715515 + 10.0 * 8.4659423828125
Epoch 730, val loss: 0.5126917958259583
Epoch 740, training loss: 85.10719299316406 = 0.47417834401130676 + 10.0 * 8.463300704956055
Epoch 740, val loss: 0.509765625
Epoch 750, training loss: 85.08473205566406 = 0.47047093510627747 + 10.0 * 8.46142578125
Epoch 750, val loss: 0.5069442391395569
Epoch 760, training loss: 85.0650405883789 = 0.4669301509857178 + 10.0 * 8.459811210632324
Epoch 760, val loss: 0.5043177604675293
Epoch 770, training loss: 85.07190704345703 = 0.4635162949562073 + 10.0 * 8.46083927154541
Epoch 770, val loss: 0.5018201470375061
Epoch 780, training loss: 85.03593444824219 = 0.46024468541145325 + 10.0 * 8.457569122314453
Epoch 780, val loss: 0.4994329810142517
Epoch 790, training loss: 85.00554656982422 = 0.4571629464626312 + 10.0 * 8.454838752746582
Epoch 790, val loss: 0.4972640573978424
Epoch 800, training loss: 84.9878921508789 = 0.4541949927806854 + 10.0 * 8.453370094299316
Epoch 800, val loss: 0.4952599108219147
Epoch 810, training loss: 84.98396301269531 = 0.4513496458530426 + 10.0 * 8.453261375427246
Epoch 810, val loss: 0.4933006465435028
Epoch 820, training loss: 84.96656799316406 = 0.44857555627822876 + 10.0 * 8.451799392700195
Epoch 820, val loss: 0.4914371967315674
Epoch 830, training loss: 84.94285583496094 = 0.44593435525894165 + 10.0 * 8.449691772460938
Epoch 830, val loss: 0.4896547198295593
Epoch 840, training loss: 84.92399597167969 = 0.44339826703071594 + 10.0 * 8.448060035705566
Epoch 840, val loss: 0.48805516958236694
Epoch 850, training loss: 84.91321563720703 = 0.4409457743167877 + 10.0 * 8.447226524353027
Epoch 850, val loss: 0.48648613691329956
Epoch 860, training loss: 84.90741729736328 = 0.4385482668876648 + 10.0 * 8.446887016296387
Epoch 860, val loss: 0.4849501848220825
Epoch 870, training loss: 84.88623046875 = 0.4362265169620514 + 10.0 * 8.445000648498535
Epoch 870, val loss: 0.48345473408699036
Epoch 880, training loss: 84.86915588378906 = 0.4340074062347412 + 10.0 * 8.443514823913574
Epoch 880, val loss: 0.48208025097846985
Epoch 890, training loss: 84.8541488647461 = 0.43185174465179443 + 10.0 * 8.442229270935059
Epoch 890, val loss: 0.4807673394680023
Epoch 900, training loss: 84.84920501708984 = 0.4297628104686737 + 10.0 * 8.441944122314453
Epoch 900, val loss: 0.4794963002204895
Epoch 910, training loss: 84.83507537841797 = 0.4276999235153198 + 10.0 * 8.4407377243042
Epoch 910, val loss: 0.4783104658126831
Epoch 920, training loss: 84.8171157836914 = 0.42572030425071716 + 10.0 * 8.439139366149902
Epoch 920, val loss: 0.4770837128162384
Epoch 930, training loss: 84.80545043945312 = 0.42378416657447815 + 10.0 * 8.438166618347168
Epoch 930, val loss: 0.4759804904460907
Epoch 940, training loss: 84.7926254272461 = 0.42189207673072815 + 10.0 * 8.437073707580566
Epoch 940, val loss: 0.4749096930027008
Epoch 950, training loss: 84.78292846679688 = 0.4200437068939209 + 10.0 * 8.436288833618164
Epoch 950, val loss: 0.4738573431968689
Epoch 960, training loss: 84.80290985107422 = 0.418224960565567 + 10.0 * 8.438467979431152
Epoch 960, val loss: 0.4729037880897522
Epoch 970, training loss: 84.77288818359375 = 0.41642525792121887 + 10.0 * 8.435646057128906
Epoch 970, val loss: 0.47185787558555603
Epoch 980, training loss: 84.75428771972656 = 0.4146839380264282 + 10.0 * 8.433960914611816
Epoch 980, val loss: 0.47089695930480957
Epoch 990, training loss: 84.74002075195312 = 0.4129927456378937 + 10.0 * 8.432703018188477
Epoch 990, val loss: 0.4699831008911133
Epoch 1000, training loss: 84.7413101196289 = 0.41133204102516174 + 10.0 * 8.432997703552246
Epoch 1000, val loss: 0.46912047266960144
Epoch 1010, training loss: 84.72315979003906 = 0.40968120098114014 + 10.0 * 8.431347846984863
Epoch 1010, val loss: 0.4682740867137909
Epoch 1020, training loss: 84.71142578125 = 0.4080795347690582 + 10.0 * 8.430334091186523
Epoch 1020, val loss: 0.4674454927444458
Epoch 1030, training loss: 84.70231628417969 = 0.40651267766952515 + 10.0 * 8.429580688476562
Epoch 1030, val loss: 0.4666571021080017
Epoch 1040, training loss: 84.70674133300781 = 0.40497052669525146 + 10.0 * 8.430177688598633
Epoch 1040, val loss: 0.46585187315940857
Epoch 1050, training loss: 84.69019317626953 = 0.40343549847602844 + 10.0 * 8.428675651550293
Epoch 1050, val loss: 0.46510371565818787
Epoch 1060, training loss: 84.67747497558594 = 0.40194395184516907 + 10.0 * 8.427553176879883
Epoch 1060, val loss: 0.46436434984207153
Epoch 1070, training loss: 84.67534637451172 = 0.4004710018634796 + 10.0 * 8.42748737335205
Epoch 1070, val loss: 0.46365243196487427
Epoch 1080, training loss: 84.65766143798828 = 0.3990122675895691 + 10.0 * 8.425865173339844
Epoch 1080, val loss: 0.462943434715271
Epoch 1090, training loss: 84.66913604736328 = 0.3975716829299927 + 10.0 * 8.427156448364258
Epoch 1090, val loss: 0.4622001051902771
Epoch 1100, training loss: 84.64069366455078 = 0.3961479067802429 + 10.0 * 8.424454689025879
Epoch 1100, val loss: 0.4615749716758728
Epoch 1110, training loss: 84.62911224365234 = 0.394751638174057 + 10.0 * 8.423436164855957
Epoch 1110, val loss: 0.46093931794166565
Epoch 1120, training loss: 84.62068939208984 = 0.3933766186237335 + 10.0 * 8.422731399536133
Epoch 1120, val loss: 0.4602927267551422
Epoch 1130, training loss: 84.61793518066406 = 0.3920150101184845 + 10.0 * 8.422592163085938
Epoch 1130, val loss: 0.45968762040138245
Epoch 1140, training loss: 84.6290512084961 = 0.39065226912498474 + 10.0 * 8.423839569091797
Epoch 1140, val loss: 0.45914486050605774
Epoch 1150, training loss: 84.60535430908203 = 0.3893038034439087 + 10.0 * 8.421605110168457
Epoch 1150, val loss: 0.45850375294685364
Epoch 1160, training loss: 84.58560180664062 = 0.3879721760749817 + 10.0 * 8.419763565063477
Epoch 1160, val loss: 0.4579027593135834
Epoch 1170, training loss: 84.57714080810547 = 0.38666367530822754 + 10.0 * 8.419047355651855
Epoch 1170, val loss: 0.457298219203949
Epoch 1180, training loss: 84.58150482177734 = 0.38536790013313293 + 10.0 * 8.4196138381958
Epoch 1180, val loss: 0.4567623436450958
Epoch 1190, training loss: 84.57188415527344 = 0.3840619921684265 + 10.0 * 8.418782234191895
Epoch 1190, val loss: 0.45627373456954956
Epoch 1200, training loss: 84.5590591430664 = 0.38277530670166016 + 10.0 * 8.417628288269043
Epoch 1200, val loss: 0.45562440156936646
Epoch 1210, training loss: 84.54398345947266 = 0.38150471448898315 + 10.0 * 8.416247367858887
Epoch 1210, val loss: 0.4551245868206024
Epoch 1220, training loss: 84.55158233642578 = 0.3802449703216553 + 10.0 * 8.417134284973145
Epoch 1220, val loss: 0.45458298921585083
Epoch 1230, training loss: 84.53375244140625 = 0.3789805471897125 + 10.0 * 8.41547679901123
Epoch 1230, val loss: 0.45405858755111694
Epoch 1240, training loss: 84.53016662597656 = 0.37772971391677856 + 10.0 * 8.415243148803711
Epoch 1240, val loss: 0.4534933269023895
Epoch 1250, training loss: 84.53922271728516 = 0.37648478150367737 + 10.0 * 8.41627311706543
Epoch 1250, val loss: 0.4529615640640259
Epoch 1260, training loss: 84.51222229003906 = 0.37525424361228943 + 10.0 * 8.413697242736816
Epoch 1260, val loss: 0.4525020122528076
Epoch 1270, training loss: 84.50057983398438 = 0.37403473258018494 + 10.0 * 8.412654876708984
Epoch 1270, val loss: 0.45196425914764404
Epoch 1280, training loss: 84.49028015136719 = 0.372829794883728 + 10.0 * 8.411745071411133
Epoch 1280, val loss: 0.45148712396621704
Epoch 1290, training loss: 84.48313903808594 = 0.3716287314891815 + 10.0 * 8.411150932312012
Epoch 1290, val loss: 0.4510054588317871
Epoch 1300, training loss: 84.51649475097656 = 0.37042462825775146 + 10.0 * 8.414607048034668
Epoch 1300, val loss: 0.45046788454055786
Epoch 1310, training loss: 84.49838256835938 = 0.36919140815734863 + 10.0 * 8.412919044494629
Epoch 1310, val loss: 0.44997158646583557
Epoch 1320, training loss: 84.46489715576172 = 0.3679905831813812 + 10.0 * 8.409690856933594
Epoch 1320, val loss: 0.44950249791145325
Epoch 1330, training loss: 84.45928955078125 = 0.3668060004711151 + 10.0 * 8.409248352050781
Epoch 1330, val loss: 0.4489879012107849
Epoch 1340, training loss: 84.44998931884766 = 0.3656303584575653 + 10.0 * 8.408435821533203
Epoch 1340, val loss: 0.448530912399292
Epoch 1350, training loss: 84.44320678710938 = 0.364455908536911 + 10.0 * 8.407875061035156
Epoch 1350, val loss: 0.4480401277542114
Epoch 1360, training loss: 84.46923065185547 = 0.3632754981517792 + 10.0 * 8.410595893859863
Epoch 1360, val loss: 0.4475807845592499
Epoch 1370, training loss: 84.4544448852539 = 0.3620842695236206 + 10.0 * 8.409235954284668
Epoch 1370, val loss: 0.44702762365341187
Epoch 1380, training loss: 84.43069458007812 = 0.36090826988220215 + 10.0 * 8.406978607177734
Epoch 1380, val loss: 0.4466799199581146
Epoch 1390, training loss: 84.42086791992188 = 0.35973790287971497 + 10.0 * 8.406112670898438
Epoch 1390, val loss: 0.4462074935436249
Epoch 1400, training loss: 84.41779327392578 = 0.35857120156288147 + 10.0 * 8.405921936035156
Epoch 1400, val loss: 0.445768266916275
Epoch 1410, training loss: 84.42633819580078 = 0.3574008643627167 + 10.0 * 8.406893730163574
Epoch 1410, val loss: 0.4453333616256714
Epoch 1420, training loss: 84.4076919555664 = 0.35622453689575195 + 10.0 * 8.405146598815918
Epoch 1420, val loss: 0.4448133707046509
Epoch 1430, training loss: 84.40221405029297 = 0.3550538420677185 + 10.0 * 8.404715538024902
Epoch 1430, val loss: 0.44438618421554565
Epoch 1440, training loss: 84.39623260498047 = 0.3538868725299835 + 10.0 * 8.404233932495117
Epoch 1440, val loss: 0.4438827633857727
Epoch 1450, training loss: 84.39187622070312 = 0.352726548910141 + 10.0 * 8.403914451599121
Epoch 1450, val loss: 0.4434565305709839
Epoch 1460, training loss: 84.41983032226562 = 0.35156071186065674 + 10.0 * 8.406826972961426
Epoch 1460, val loss: 0.44306641817092896
Epoch 1470, training loss: 84.39059448242188 = 0.3503783345222473 + 10.0 * 8.404021263122559
Epoch 1470, val loss: 0.442508339881897
Epoch 1480, training loss: 84.37711334228516 = 0.3492223024368286 + 10.0 * 8.402789115905762
Epoch 1480, val loss: 0.4420783817768097
Epoch 1490, training loss: 84.3686752319336 = 0.3480737507343292 + 10.0 * 8.402059555053711
Epoch 1490, val loss: 0.4416389763355255
Epoch 1500, training loss: 84.36360931396484 = 0.3469320237636566 + 10.0 * 8.401667594909668
Epoch 1500, val loss: 0.44121065735816956
Epoch 1510, training loss: 84.36685180664062 = 0.34579676389694214 + 10.0 * 8.402105331420898
Epoch 1510, val loss: 0.44081011414527893
Epoch 1520, training loss: 84.35869598388672 = 0.3446424901485443 + 10.0 * 8.401405334472656
Epoch 1520, val loss: 0.4402971863746643
Epoch 1530, training loss: 84.35626220703125 = 0.34350404143333435 + 10.0 * 8.401275634765625
Epoch 1530, val loss: 0.43991631269454956
Epoch 1540, training loss: 84.34549713134766 = 0.3423638939857483 + 10.0 * 8.400313377380371
Epoch 1540, val loss: 0.4394235610961914
Epoch 1550, training loss: 84.33866119384766 = 0.3412306010723114 + 10.0 * 8.39974308013916
Epoch 1550, val loss: 0.4390089213848114
Epoch 1560, training loss: 84.34444427490234 = 0.3400993049144745 + 10.0 * 8.400434494018555
Epoch 1560, val loss: 0.4386497437953949
Epoch 1570, training loss: 84.35001373291016 = 0.3389384150505066 + 10.0 * 8.401107788085938
Epoch 1570, val loss: 0.43819230794906616
Epoch 1580, training loss: 84.33670806884766 = 0.33779868483543396 + 10.0 * 8.399890899658203
Epoch 1580, val loss: 0.4377451539039612
Epoch 1590, training loss: 84.32289123535156 = 0.3366735577583313 + 10.0 * 8.398622512817383
Epoch 1590, val loss: 0.43728944659233093
Epoch 1600, training loss: 84.31700897216797 = 0.3355615437030792 + 10.0 * 8.398144721984863
Epoch 1600, val loss: 0.4369240999221802
Epoch 1610, training loss: 84.31149291992188 = 0.3344503343105316 + 10.0 * 8.397704124450684
Epoch 1610, val loss: 0.43655404448509216
Epoch 1620, training loss: 84.30615234375 = 0.333336740732193 + 10.0 * 8.397281646728516
Epoch 1620, val loss: 0.4361821711063385
Epoch 1630, training loss: 84.30855560302734 = 0.3322211503982544 + 10.0 * 8.39763355255127
Epoch 1630, val loss: 0.43579068779945374
Epoch 1640, training loss: 84.30313110351562 = 0.33108577132225037 + 10.0 * 8.397204399108887
Epoch 1640, val loss: 0.43540212512016296
Epoch 1650, training loss: 84.29560089111328 = 0.32996004819869995 + 10.0 * 8.396563529968262
Epoch 1650, val loss: 0.4350152611732483
Epoch 1660, training loss: 84.29676818847656 = 0.32883960008621216 + 10.0 * 8.3967924118042
Epoch 1660, val loss: 0.4345940053462982
Epoch 1670, training loss: 84.30905151367188 = 0.32771608233451843 + 10.0 * 8.398134231567383
Epoch 1670, val loss: 0.434222549200058
Epoch 1680, training loss: 84.28935241699219 = 0.32659754157066345 + 10.0 * 8.396275520324707
Epoch 1680, val loss: 0.4339074194431305
Epoch 1690, training loss: 84.27576446533203 = 0.32547685503959656 + 10.0 * 8.395029067993164
Epoch 1690, val loss: 0.4334966242313385
Epoch 1700, training loss: 84.26966094970703 = 0.32436102628707886 + 10.0 * 8.394529342651367
Epoch 1700, val loss: 0.43313542008399963
Epoch 1710, training loss: 84.26712036132812 = 0.3232453763484955 + 10.0 * 8.394388198852539
Epoch 1710, val loss: 0.4327984154224396
Epoch 1720, training loss: 84.29566192626953 = 0.32212764024734497 + 10.0 * 8.397353172302246
Epoch 1720, val loss: 0.43246716260910034
Epoch 1730, training loss: 84.27242279052734 = 0.32099688053131104 + 10.0 * 8.395142555236816
Epoch 1730, val loss: 0.4321407675743103
Epoch 1740, training loss: 84.25709533691406 = 0.31987226009368896 + 10.0 * 8.393722534179688
Epoch 1740, val loss: 0.43169206380844116
Epoch 1750, training loss: 84.24833679199219 = 0.3187568485736847 + 10.0 * 8.39295768737793
Epoch 1750, val loss: 0.4314202666282654
Epoch 1760, training loss: 84.24192810058594 = 0.3176446557044983 + 10.0 * 8.392428398132324
Epoch 1760, val loss: 0.4310601055622101
Epoch 1770, training loss: 84.2570571899414 = 0.31653523445129395 + 10.0 * 8.394052505493164
Epoch 1770, val loss: 0.4306975305080414
Epoch 1780, training loss: 84.23436737060547 = 0.3154055178165436 + 10.0 * 8.39189624786377
Epoch 1780, val loss: 0.4304089844226837
Epoch 1790, training loss: 84.2364273071289 = 0.3142986595630646 + 10.0 * 8.392212867736816
Epoch 1790, val loss: 0.4301087260246277
Epoch 1800, training loss: 84.22437286376953 = 0.3132021725177765 + 10.0 * 8.391117095947266
Epoch 1800, val loss: 0.4297815263271332
Epoch 1810, training loss: 84.21817016601562 = 0.3121047616004944 + 10.0 * 8.390606880187988
Epoch 1810, val loss: 0.4295051693916321
Epoch 1820, training loss: 84.23652648925781 = 0.31101056933403015 + 10.0 * 8.39255142211914
Epoch 1820, val loss: 0.42925554513931274
Epoch 1830, training loss: 84.21627044677734 = 0.30989670753479004 + 10.0 * 8.390637397766113
Epoch 1830, val loss: 0.4288054406642914
Epoch 1840, training loss: 84.21285247802734 = 0.3088029623031616 + 10.0 * 8.390405654907227
Epoch 1840, val loss: 0.4286375343799591
Epoch 1850, training loss: 84.20231628417969 = 0.30770808458328247 + 10.0 * 8.389460563659668
Epoch 1850, val loss: 0.42821773886680603
Epoch 1860, training loss: 84.19658660888672 = 0.3066188097000122 + 10.0 * 8.388997077941895
Epoch 1860, val loss: 0.42798447608947754
Epoch 1870, training loss: 84.19563293457031 = 0.3055250942707062 + 10.0 * 8.389010429382324
Epoch 1870, val loss: 0.42765378952026367
Epoch 1880, training loss: 84.22713470458984 = 0.3044193983078003 + 10.0 * 8.392271041870117
Epoch 1880, val loss: 0.42732980847358704
Epoch 1890, training loss: 84.20018768310547 = 0.3033037781715393 + 10.0 * 8.389688491821289
Epoch 1890, val loss: 0.42709240317344666
Epoch 1900, training loss: 84.1827392578125 = 0.3021995425224304 + 10.0 * 8.388053894042969
Epoch 1900, val loss: 0.42677006125450134
Epoch 1910, training loss: 84.18004608154297 = 0.30110135674476624 + 10.0 * 8.387894630432129
Epoch 1910, val loss: 0.42654478549957275
Epoch 1920, training loss: 84.17312622070312 = 0.30000248551368713 + 10.0 * 8.387311935424805
Epoch 1920, val loss: 0.42628729343414307
Epoch 1930, training loss: 84.18165588378906 = 0.29890209436416626 + 10.0 * 8.388275146484375
Epoch 1930, val loss: 0.42604613304138184
Epoch 1940, training loss: 84.16947937011719 = 0.2977921962738037 + 10.0 * 8.387168884277344
Epoch 1940, val loss: 0.4257630705833435
Epoch 1950, training loss: 84.1628189086914 = 0.2966912090778351 + 10.0 * 8.386612892150879
Epoch 1950, val loss: 0.42557117342948914
Epoch 1960, training loss: 84.1658935546875 = 0.2955942749977112 + 10.0 * 8.387029647827148
Epoch 1960, val loss: 0.4253934919834137
Epoch 1970, training loss: 84.17366790771484 = 0.2944856286048889 + 10.0 * 8.387918472290039
Epoch 1970, val loss: 0.42511656880378723
Epoch 1980, training loss: 84.15445709228516 = 0.2933734059333801 + 10.0 * 8.3861083984375
Epoch 1980, val loss: 0.42481985688209534
Epoch 1990, training loss: 84.1463623046875 = 0.2922692596912384 + 10.0 * 8.385409355163574
Epoch 1990, val loss: 0.4246436357498169
Epoch 2000, training loss: 84.14106750488281 = 0.29116567969322205 + 10.0 * 8.384989738464355
Epoch 2000, val loss: 0.4244660437107086
Epoch 2010, training loss: 84.14120483398438 = 0.2900552749633789 + 10.0 * 8.385114669799805
Epoch 2010, val loss: 0.424287348985672
Epoch 2020, training loss: 84.16132354736328 = 0.2889421880245209 + 10.0 * 8.387238502502441
Epoch 2020, val loss: 0.4242071211338043
Epoch 2030, training loss: 84.13919067382812 = 0.2878127694129944 + 10.0 * 8.385137557983398
Epoch 2030, val loss: 0.4239089787006378
Epoch 2040, training loss: 84.13045501708984 = 0.28669196367263794 + 10.0 * 8.384376525878906
Epoch 2040, val loss: 0.4236907958984375
Epoch 2050, training loss: 84.14641571044922 = 0.28557345271110535 + 10.0 * 8.386083602905273
Epoch 2050, val loss: 0.42359861731529236
Epoch 2060, training loss: 84.12467193603516 = 0.28445038199424744 + 10.0 * 8.38402271270752
Epoch 2060, val loss: 0.42334532737731934
Epoch 2070, training loss: 84.11920928955078 = 0.2833389937877655 + 10.0 * 8.383586883544922
Epoch 2070, val loss: 0.42330607771873474
Epoch 2080, training loss: 84.11348724365234 = 0.2822279632091522 + 10.0 * 8.383126258850098
Epoch 2080, val loss: 0.4230809211730957
Epoch 2090, training loss: 84.11004638671875 = 0.28112179040908813 + 10.0 * 8.382892608642578
Epoch 2090, val loss: 0.4229646921157837
Epoch 2100, training loss: 84.11625671386719 = 0.28001606464385986 + 10.0 * 8.383624076843262
Epoch 2100, val loss: 0.4228295087814331
Epoch 2110, training loss: 84.1067123413086 = 0.2789100408554077 + 10.0 * 8.382780075073242
Epoch 2110, val loss: 0.4227421283721924
Epoch 2120, training loss: 84.10227966308594 = 0.27780601382255554 + 10.0 * 8.382447242736816
Epoch 2120, val loss: 0.42265111207962036
Epoch 2130, training loss: 84.10369110107422 = 0.2766987681388855 + 10.0 * 8.382699966430664
Epoch 2130, val loss: 0.42255714535713196
Epoch 2140, training loss: 84.1034164428711 = 0.2755886912345886 + 10.0 * 8.382782936096191
Epoch 2140, val loss: 0.4224601686000824
Epoch 2150, training loss: 84.09355163574219 = 0.2744741141796112 + 10.0 * 8.38190746307373
Epoch 2150, val loss: 0.4222963750362396
Epoch 2160, training loss: 84.15582275390625 = 0.2733590304851532 + 10.0 * 8.388246536254883
Epoch 2160, val loss: 0.42216500639915466
Epoch 2170, training loss: 84.10527038574219 = 0.2722487449645996 + 10.0 * 8.383302688598633
Epoch 2170, val loss: 0.4222595989704132
Epoch 2180, training loss: 84.08293151855469 = 0.2711329758167267 + 10.0 * 8.381179809570312
Epoch 2180, val loss: 0.42206159234046936
Epoch 2190, training loss: 84.07467651367188 = 0.2700331509113312 + 10.0 * 8.380464553833008
Epoch 2190, val loss: 0.4220503866672516
Epoch 2200, training loss: 84.07062530517578 = 0.268937349319458 + 10.0 * 8.380168914794922
Epoch 2200, val loss: 0.4219992160797119
Epoch 2210, training loss: 84.06652069091797 = 0.267843633890152 + 10.0 * 8.379867553710938
Epoch 2210, val loss: 0.4219459891319275
Epoch 2220, training loss: 84.06336975097656 = 0.2667466402053833 + 10.0 * 8.37966251373291
Epoch 2220, val loss: 0.42192572355270386
Epoch 2230, training loss: 84.080322265625 = 0.2656528055667877 + 10.0 * 8.38146686553955
Epoch 2230, val loss: 0.42199763655662537
Epoch 2240, training loss: 84.05967712402344 = 0.2645442485809326 + 10.0 * 8.37951374053955
Epoch 2240, val loss: 0.42176270484924316
Epoch 2250, training loss: 84.0628890991211 = 0.26345112919807434 + 10.0 * 8.37994384765625
Epoch 2250, val loss: 0.42192861437797546
Epoch 2260, training loss: 84.0581283569336 = 0.26235267519950867 + 10.0 * 8.37957763671875
Epoch 2260, val loss: 0.4218190610408783
Epoch 2270, training loss: 84.05976104736328 = 0.26125600934028625 + 10.0 * 8.379850387573242
Epoch 2270, val loss: 0.4218379855155945
Epoch 2280, training loss: 84.052001953125 = 0.2601582407951355 + 10.0 * 8.37918472290039
Epoch 2280, val loss: 0.42186421155929565
Epoch 2290, training loss: 84.04166412353516 = 0.2590598464012146 + 10.0 * 8.378260612487793
Epoch 2290, val loss: 0.4218375086784363
Epoch 2300, training loss: 84.0495376586914 = 0.2579655647277832 + 10.0 * 8.379157066345215
Epoch 2300, val loss: 0.42183247208595276
Epoch 2310, training loss: 84.03750610351562 = 0.25687089562416077 + 10.0 * 8.378063201904297
Epoch 2310, val loss: 0.4218885004520416
Epoch 2320, training loss: 84.03009796142578 = 0.255776584148407 + 10.0 * 8.377431869506836
Epoch 2320, val loss: 0.4218900501728058
Epoch 2330, training loss: 84.04300689697266 = 0.2546803653240204 + 10.0 * 8.378832817077637
Epoch 2330, val loss: 0.42193150520324707
Epoch 2340, training loss: 84.03952026367188 = 0.25357651710510254 + 10.0 * 8.378594398498535
Epoch 2340, val loss: 0.42201051115989685
Epoch 2350, training loss: 84.02603912353516 = 0.2524787485599518 + 10.0 * 8.377355575561523
Epoch 2350, val loss: 0.42217135429382324
Epoch 2360, training loss: 84.01875305175781 = 0.251373291015625 + 10.0 * 8.376737594604492
Epoch 2360, val loss: 0.42212510108947754
Epoch 2370, training loss: 84.01361846923828 = 0.25027137994766235 + 10.0 * 8.376334190368652
Epoch 2370, val loss: 0.4222412407398224
Epoch 2380, training loss: 84.03487396240234 = 0.24917097389698029 + 10.0 * 8.378570556640625
Epoch 2380, val loss: 0.4222272038459778
Epoch 2390, training loss: 84.01374053955078 = 0.2480662614107132 + 10.0 * 8.376566886901855
Epoch 2390, val loss: 0.4224686622619629
Epoch 2400, training loss: 84.0054702758789 = 0.24695731699466705 + 10.0 * 8.37585163116455
Epoch 2400, val loss: 0.4224853515625
Epoch 2410, training loss: 84.00080871582031 = 0.24585255980491638 + 10.0 * 8.375495910644531
Epoch 2410, val loss: 0.4226324260234833
Epoch 2420, training loss: 84.00932312011719 = 0.244742751121521 + 10.0 * 8.376458168029785
Epoch 2420, val loss: 0.4227240979671478
Epoch 2430, training loss: 84.02482604980469 = 0.24362513422966003 + 10.0 * 8.378120422363281
Epoch 2430, val loss: 0.4227437674999237
Epoch 2440, training loss: 83.99270629882812 = 0.24250788986682892 + 10.0 * 8.375020027160645
Epoch 2440, val loss: 0.42288345098495483
Epoch 2450, training loss: 83.98959350585938 = 0.24139197170734406 + 10.0 * 8.3748197555542
Epoch 2450, val loss: 0.422951340675354
Epoch 2460, training loss: 83.98424530029297 = 0.24027757346630096 + 10.0 * 8.374396324157715
Epoch 2460, val loss: 0.4230435788631439
Epoch 2470, training loss: 83.9791030883789 = 0.2391587197780609 + 10.0 * 8.373994827270508
Epoch 2470, val loss: 0.4231349229812622
Epoch 2480, training loss: 83.99407958984375 = 0.23803666234016418 + 10.0 * 8.375604629516602
Epoch 2480, val loss: 0.4233682453632355
Epoch 2490, training loss: 83.97521209716797 = 0.23690074682235718 + 10.0 * 8.373830795288086
Epoch 2490, val loss: 0.4232676923274994
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8366311516996447
0.8659711656886185
=== training gcn model ===
Epoch 0, training loss: 106.92273712158203 = 1.099613904953003 + 10.0 * 10.582311630249023
Epoch 0, val loss: 1.0972269773483276
Epoch 10, training loss: 106.9164047241211 = 1.0950429439544678 + 10.0 * 10.582136154174805
Epoch 10, val loss: 1.0927568674087524
Epoch 20, training loss: 106.90296936035156 = 1.0903297662734985 + 10.0 * 10.581263542175293
Epoch 20, val loss: 1.0880967378616333
Epoch 30, training loss: 106.8551025390625 = 1.0850796699523926 + 10.0 * 10.577001571655273
Epoch 30, val loss: 1.0829296112060547
Epoch 40, training loss: 106.66901397705078 = 1.079310417175293 + 10.0 * 10.55897045135498
Epoch 40, val loss: 1.0773080587387085
Epoch 50, training loss: 106.1241455078125 = 1.0729985237121582 + 10.0 * 10.505114555358887
Epoch 50, val loss: 1.0711748600006104
Epoch 60, training loss: 105.02010345458984 = 1.0667465925216675 + 10.0 * 10.39533519744873
Epoch 60, val loss: 1.065234899520874
Epoch 70, training loss: 103.44569396972656 = 1.0615150928497314 + 10.0 * 10.238417625427246
Epoch 70, val loss: 1.0601691007614136
Epoch 80, training loss: 102.36267852783203 = 1.0560808181762695 + 10.0 * 10.130659103393555
Epoch 80, val loss: 1.0547950267791748
Epoch 90, training loss: 100.64583587646484 = 1.0487762689590454 + 10.0 * 9.95970630645752
Epoch 90, val loss: 1.0475597381591797
Epoch 100, training loss: 96.2344970703125 = 1.0390280485153198 + 10.0 * 9.519547462463379
Epoch 100, val loss: 1.0378755331039429
Epoch 110, training loss: 94.43685913085938 = 1.029719591140747 + 10.0 * 9.340714454650879
Epoch 110, val loss: 1.0290359258651733
Epoch 120, training loss: 93.66319274902344 = 1.021754264831543 + 10.0 * 9.264143943786621
Epoch 120, val loss: 1.0214909315109253
Epoch 130, training loss: 93.30490112304688 = 1.0144683122634888 + 10.0 * 9.229043960571289
Epoch 130, val loss: 1.0145233869552612
Epoch 140, training loss: 93.0374984741211 = 1.00752854347229 + 10.0 * 9.202997207641602
Epoch 140, val loss: 1.0078418254852295
Epoch 150, training loss: 92.71485137939453 = 1.0008286237716675 + 10.0 * 9.171401977539062
Epoch 150, val loss: 1.0014249086380005
Epoch 160, training loss: 92.3270263671875 = 0.9945058226585388 + 10.0 * 9.133252143859863
Epoch 160, val loss: 0.9954221248626709
Epoch 170, training loss: 91.85826110839844 = 0.9885110259056091 + 10.0 * 9.08697509765625
Epoch 170, val loss: 0.9897572994232178
Epoch 180, training loss: 91.36693572998047 = 0.9828643202781677 + 10.0 * 9.038407325744629
Epoch 180, val loss: 0.9844648241996765
Epoch 190, training loss: 90.98019409179688 = 0.977300763130188 + 10.0 * 9.000288963317871
Epoch 190, val loss: 0.9791837930679321
Epoch 200, training loss: 90.6468505859375 = 0.9710643887519836 + 10.0 * 8.967578887939453
Epoch 200, val loss: 0.9731826186180115
Epoch 210, training loss: 90.25397491455078 = 0.9640735387802124 + 10.0 * 8.928990364074707
Epoch 210, val loss: 0.9665170311927795
Epoch 220, training loss: 89.89501190185547 = 0.95681232213974 + 10.0 * 8.893819808959961
Epoch 220, val loss: 0.9596540927886963
Epoch 230, training loss: 89.68614196777344 = 0.9491497874259949 + 10.0 * 8.873699188232422
Epoch 230, val loss: 0.9523202180862427
Epoch 240, training loss: 89.54644012451172 = 0.9406766891479492 + 10.0 * 8.860576629638672
Epoch 240, val loss: 0.9441742897033691
Epoch 250, training loss: 89.4043960571289 = 0.9317894577980042 + 10.0 * 8.847260475158691
Epoch 250, val loss: 0.9357048869132996
Epoch 260, training loss: 89.24239349365234 = 0.9230917692184448 + 10.0 * 8.831930160522461
Epoch 260, val loss: 0.9274998307228088
Epoch 270, training loss: 89.03126525878906 = 0.9147292375564575 + 10.0 * 8.811654090881348
Epoch 270, val loss: 0.919678270816803
Epoch 280, training loss: 88.74823760986328 = 0.9065759181976318 + 10.0 * 8.78416633605957
Epoch 280, val loss: 0.912102997303009
Epoch 290, training loss: 88.42981719970703 = 0.898601233959198 + 10.0 * 8.753122329711914
Epoch 290, val loss: 0.9046816825866699
Epoch 300, training loss: 88.191650390625 = 0.8903884887695312 + 10.0 * 8.73012638092041
Epoch 300, val loss: 0.8970010280609131
Epoch 310, training loss: 87.95474243164062 = 0.8814356923103333 + 10.0 * 8.707330703735352
Epoch 310, val loss: 0.888563334941864
Epoch 320, training loss: 87.77511596679688 = 0.8717990517616272 + 10.0 * 8.690332412719727
Epoch 320, val loss: 0.8794938325881958
Epoch 330, training loss: 87.62454223632812 = 0.8615326285362244 + 10.0 * 8.676301002502441
Epoch 330, val loss: 0.8698379993438721
Epoch 340, training loss: 87.47941589355469 = 0.850851833820343 + 10.0 * 8.662856101989746
Epoch 340, val loss: 0.8598518967628479
Epoch 350, training loss: 87.37481689453125 = 0.840081512928009 + 10.0 * 8.653473854064941
Epoch 350, val loss: 0.8498185276985168
Epoch 360, training loss: 87.23955535888672 = 0.829255223274231 + 10.0 * 8.641030311584473
Epoch 360, val loss: 0.8397794365882874
Epoch 370, training loss: 87.12724304199219 = 0.8184083700180054 + 10.0 * 8.630884170532227
Epoch 370, val loss: 0.8296965956687927
Epoch 380, training loss: 87.0247802734375 = 0.8074027895927429 + 10.0 * 8.621737480163574
Epoch 380, val loss: 0.8194811344146729
Epoch 390, training loss: 86.92639923095703 = 0.7962411046028137 + 10.0 * 8.613016128540039
Epoch 390, val loss: 0.809077799320221
Epoch 400, training loss: 86.83087921142578 = 0.785011351108551 + 10.0 * 8.604586601257324
Epoch 400, val loss: 0.7987030744552612
Epoch 410, training loss: 86.74183654785156 = 0.7737396359443665 + 10.0 * 8.596809387207031
Epoch 410, val loss: 0.7882725596427917
Epoch 420, training loss: 86.67203521728516 = 0.7623906135559082 + 10.0 * 8.590964317321777
Epoch 420, val loss: 0.7777541875839233
Epoch 430, training loss: 86.59027099609375 = 0.7509687542915344 + 10.0 * 8.583930015563965
Epoch 430, val loss: 0.7672163844108582
Epoch 440, training loss: 86.51273345947266 = 0.7396632432937622 + 10.0 * 8.577306747436523
Epoch 440, val loss: 0.7567947506904602
Epoch 450, training loss: 86.44181823730469 = 0.7285043001174927 + 10.0 * 8.571331024169922
Epoch 450, val loss: 0.746515691280365
Epoch 460, training loss: 86.36482238769531 = 0.7175269722938538 + 10.0 * 8.564729690551758
Epoch 460, val loss: 0.7364404797554016
Epoch 470, training loss: 86.29336547851562 = 0.7067140340805054 + 10.0 * 8.55866527557373
Epoch 470, val loss: 0.7265033721923828
Epoch 480, training loss: 86.21086883544922 = 0.6960529088973999 + 10.0 * 8.551481246948242
Epoch 480, val loss: 0.716697633266449
Epoch 490, training loss: 86.14080047607422 = 0.6855925917625427 + 10.0 * 8.545520782470703
Epoch 490, val loss: 0.7071347832679749
Epoch 500, training loss: 86.08694458007812 = 0.6752777099609375 + 10.0 * 8.541166305541992
Epoch 500, val loss: 0.6976678371429443
Epoch 510, training loss: 86.00485229492188 = 0.6651144027709961 + 10.0 * 8.533973693847656
Epoch 510, val loss: 0.6883228421211243
Epoch 520, training loss: 85.94816589355469 = 0.6551634669303894 + 10.0 * 8.52929973602295
Epoch 520, val loss: 0.6791935563087463
Epoch 530, training loss: 85.90380859375 = 0.6453579664230347 + 10.0 * 8.525845527648926
Epoch 530, val loss: 0.670184850692749
Epoch 540, training loss: 85.85264587402344 = 0.6357054114341736 + 10.0 * 8.52169418334961
Epoch 540, val loss: 0.6613591909408569
Epoch 550, training loss: 85.80599212646484 = 0.626333475112915 + 10.0 * 8.517965316772461
Epoch 550, val loss: 0.652777910232544
Epoch 560, training loss: 85.76739501953125 = 0.6172147393226624 + 10.0 * 8.51501750946045
Epoch 560, val loss: 0.6444273591041565
Epoch 570, training loss: 85.73300170898438 = 0.6083236932754517 + 10.0 * 8.512468338012695
Epoch 570, val loss: 0.6362602710723877
Epoch 580, training loss: 85.70858764648438 = 0.5996868014335632 + 10.0 * 8.510890007019043
Epoch 580, val loss: 0.6283391118049622
Epoch 590, training loss: 85.66758728027344 = 0.5913375616073608 + 10.0 * 8.507624626159668
Epoch 590, val loss: 0.6207200288772583
Epoch 600, training loss: 85.6387939453125 = 0.5833175182342529 + 10.0 * 8.505547523498535
Epoch 600, val loss: 0.613394558429718
Epoch 610, training loss: 85.60904693603516 = 0.5755906701087952 + 10.0 * 8.503345489501953
Epoch 610, val loss: 0.6063472032546997
Epoch 620, training loss: 85.60415649414062 = 0.5681482553482056 + 10.0 * 8.50360107421875
Epoch 620, val loss: 0.5995460152626038
Epoch 630, training loss: 85.55905151367188 = 0.5609448552131653 + 10.0 * 8.499811172485352
Epoch 630, val loss: 0.5930406451225281
Epoch 640, training loss: 85.53343963623047 = 0.5540850758552551 + 10.0 * 8.49793529510498
Epoch 640, val loss: 0.5868170261383057
Epoch 650, training loss: 85.50934600830078 = 0.5474918484687805 + 10.0 * 8.496185302734375
Epoch 650, val loss: 0.5808403491973877
Epoch 660, training loss: 85.5332260131836 = 0.5411549806594849 + 10.0 * 8.499207496643066
Epoch 660, val loss: 0.5750948190689087
Epoch 670, training loss: 85.4797134399414 = 0.5350542068481445 + 10.0 * 8.494465827941895
Epoch 670, val loss: 0.5696256756782532
Epoch 680, training loss: 85.45307922363281 = 0.5292803645133972 + 10.0 * 8.492380142211914
Epoch 680, val loss: 0.5644360184669495
Epoch 690, training loss: 85.43022155761719 = 0.523752748966217 + 10.0 * 8.490647315979004
Epoch 690, val loss: 0.559513509273529
Epoch 700, training loss: 85.4113998413086 = 0.5184730887413025 + 10.0 * 8.489293098449707
Epoch 700, val loss: 0.5548231601715088
Epoch 710, training loss: 85.39363861083984 = 0.5134278535842896 + 10.0 * 8.488020896911621
Epoch 710, val loss: 0.5503671765327454
Epoch 720, training loss: 85.38301849365234 = 0.5085934400558472 + 10.0 * 8.487442970275879
Epoch 720, val loss: 0.5460891723632812
Epoch 730, training loss: 85.37285614013672 = 0.5039307475090027 + 10.0 * 8.486892700195312
Epoch 730, val loss: 0.5420748591423035
Epoch 740, training loss: 85.34561920166016 = 0.4995407164096832 + 10.0 * 8.484607696533203
Epoch 740, val loss: 0.5382599234580994
Epoch 750, training loss: 85.3270492553711 = 0.4953593611717224 + 10.0 * 8.483168601989746
Epoch 750, val loss: 0.5346250534057617
Epoch 760, training loss: 85.30964660644531 = 0.49138373136520386 + 10.0 * 8.481825828552246
Epoch 760, val loss: 0.5312424898147583
Epoch 770, training loss: 85.2925796508789 = 0.4875844419002533 + 10.0 * 8.480499267578125
Epoch 770, val loss: 0.5280372500419617
Epoch 780, training loss: 85.31460571289062 = 0.4839426577091217 + 10.0 * 8.48306655883789
Epoch 780, val loss: 0.5249560475349426
Epoch 790, training loss: 85.2606201171875 = 0.48042353987693787 + 10.0 * 8.478019714355469
Epoch 790, val loss: 0.5220451354980469
Epoch 800, training loss: 85.24484252929688 = 0.47710853815078735 + 10.0 * 8.476773262023926
Epoch 800, val loss: 0.519334077835083
Epoch 810, training loss: 85.22312927246094 = 0.47395163774490356 + 10.0 * 8.4749174118042
Epoch 810, val loss: 0.5167421698570251
Epoch 820, training loss: 85.20288848876953 = 0.4709368944168091 + 10.0 * 8.47319507598877
Epoch 820, val loss: 0.5143315196037292
Epoch 830, training loss: 85.21864318847656 = 0.4680432379245758 + 10.0 * 8.47506046295166
Epoch 830, val loss: 0.5120027661323547
Epoch 840, training loss: 85.19015502929688 = 0.4652038514614105 + 10.0 * 8.472495079040527
Epoch 840, val loss: 0.5098100900650024
Epoch 850, training loss: 85.14936065673828 = 0.46253693103790283 + 10.0 * 8.468682289123535
Epoch 850, val loss: 0.5077683925628662
Epoch 860, training loss: 85.12895202636719 = 0.45999637246131897 + 10.0 * 8.46689510345459
Epoch 860, val loss: 0.5058220028877258
Epoch 870, training loss: 85.10636901855469 = 0.45755109190940857 + 10.0 * 8.464881896972656
Epoch 870, val loss: 0.5039942264556885
Epoch 880, training loss: 85.0866470336914 = 0.45518890023231506 + 10.0 * 8.46314525604248
Epoch 880, val loss: 0.5022630095481873
Epoch 890, training loss: 85.13005065917969 = 0.4528875946998596 + 10.0 * 8.467716217041016
Epoch 890, val loss: 0.5005936622619629
Epoch 900, training loss: 85.06710052490234 = 0.45062458515167236 + 10.0 * 8.461647987365723
Epoch 900, val loss: 0.498960018157959
Epoch 910, training loss: 85.03511810302734 = 0.44847163558006287 + 10.0 * 8.458664894104004
Epoch 910, val loss: 0.4974175691604614
Epoch 920, training loss: 85.0144271850586 = 0.4463980495929718 + 10.0 * 8.456803321838379
Epoch 920, val loss: 0.4959670901298523
Epoch 930, training loss: 84.99759674072266 = 0.4443856179714203 + 10.0 * 8.455320358276367
Epoch 930, val loss: 0.4945848882198334
Epoch 940, training loss: 85.00685119628906 = 0.4424141049385071 + 10.0 * 8.456443786621094
Epoch 940, val loss: 0.493196040391922
Epoch 950, training loss: 84.98692321777344 = 0.44040584564208984 + 10.0 * 8.454651832580566
Epoch 950, val loss: 0.4918978810310364
Epoch 960, training loss: 84.96426391601562 = 0.4385034739971161 + 10.0 * 8.45257568359375
Epoch 960, val loss: 0.49066466093063354
Epoch 970, training loss: 84.94566345214844 = 0.43667447566986084 + 10.0 * 8.450899124145508
Epoch 970, val loss: 0.489421546459198
Epoch 980, training loss: 84.93177032470703 = 0.4348925054073334 + 10.0 * 8.449687957763672
Epoch 980, val loss: 0.4883076548576355
Epoch 990, training loss: 84.9195556640625 = 0.4331360161304474 + 10.0 * 8.448641777038574
Epoch 990, val loss: 0.4871661365032196
Epoch 1000, training loss: 84.90840148925781 = 0.4314012825489044 + 10.0 * 8.447699546813965
Epoch 1000, val loss: 0.48609498143196106
Epoch 1010, training loss: 84.89778900146484 = 0.4296817481517792 + 10.0 * 8.446810722351074
Epoch 1010, val loss: 0.4850017726421356
Epoch 1020, training loss: 84.88748168945312 = 0.42797908186912537 + 10.0 * 8.445950508117676
Epoch 1020, val loss: 0.48396772146224976
Epoch 1030, training loss: 84.879638671875 = 0.4262957274913788 + 10.0 * 8.445334434509277
Epoch 1030, val loss: 0.482922226190567
Epoch 1040, training loss: 84.91818237304688 = 0.42461302876472473 + 10.0 * 8.449357032775879
Epoch 1040, val loss: 0.4818555414676666
Epoch 1050, training loss: 84.88198852539062 = 0.4229409098625183 + 10.0 * 8.445904731750488
Epoch 1050, val loss: 0.480929970741272
Epoch 1060, training loss: 84.85699462890625 = 0.4213484227657318 + 10.0 * 8.443564414978027
Epoch 1060, val loss: 0.47992146015167236
Epoch 1070, training loss: 84.84268951416016 = 0.4197949171066284 + 10.0 * 8.442289352416992
Epoch 1070, val loss: 0.47901710867881775
Epoch 1080, training loss: 84.83202362060547 = 0.4182716906070709 + 10.0 * 8.441374778747559
Epoch 1080, val loss: 0.4781598448753357
Epoch 1090, training loss: 84.82318115234375 = 0.41676539182662964 + 10.0 * 8.440641403198242
Epoch 1090, val loss: 0.47728344798088074
Epoch 1100, training loss: 84.8148193359375 = 0.4152701497077942 + 10.0 * 8.43995475769043
Epoch 1100, val loss: 0.4764285087585449
Epoch 1110, training loss: 84.86845397949219 = 0.41379109025001526 + 10.0 * 8.445466995239258
Epoch 1110, val loss: 0.4755192995071411
Epoch 1120, training loss: 84.80345916748047 = 0.41228941082954407 + 10.0 * 8.439116477966309
Epoch 1120, val loss: 0.4747640788555145
Epoch 1130, training loss: 84.79150390625 = 0.41085758805274963 + 10.0 * 8.438064575195312
Epoch 1130, val loss: 0.47396308183670044
Epoch 1140, training loss: 84.78053283691406 = 0.40945950150489807 + 10.0 * 8.43710708618164
Epoch 1140, val loss: 0.4732121229171753
Epoch 1150, training loss: 84.76997375488281 = 0.4080776274204254 + 10.0 * 8.436189651489258
Epoch 1150, val loss: 0.4724811017513275
Epoch 1160, training loss: 84.76116943359375 = 0.4067022502422333 + 10.0 * 8.435446739196777
Epoch 1160, val loss: 0.47175052762031555
Epoch 1170, training loss: 84.78092956542969 = 0.40533143281936646 + 10.0 * 8.437559127807617
Epoch 1170, val loss: 0.4710601270198822
Epoch 1180, training loss: 84.75504302978516 = 0.4039650559425354 + 10.0 * 8.435107231140137
Epoch 1180, val loss: 0.47030332684516907
Epoch 1190, training loss: 84.73493957519531 = 0.40262413024902344 + 10.0 * 8.433231353759766
Epoch 1190, val loss: 0.4696197211742401
Epoch 1200, training loss: 84.72647857666016 = 0.40130966901779175 + 10.0 * 8.432517051696777
Epoch 1200, val loss: 0.46892958879470825
Epoch 1210, training loss: 84.71501922607422 = 0.4000047743320465 + 10.0 * 8.431501388549805
Epoch 1210, val loss: 0.4683015048503876
Epoch 1220, training loss: 84.70919799804688 = 0.3987123966217041 + 10.0 * 8.431048393249512
Epoch 1220, val loss: 0.46766018867492676
Epoch 1230, training loss: 84.74590301513672 = 0.39740678668022156 + 10.0 * 8.434849739074707
Epoch 1230, val loss: 0.4670264720916748
Epoch 1240, training loss: 84.69153594970703 = 0.39611291885375977 + 10.0 * 8.429542541503906
Epoch 1240, val loss: 0.46639886498451233
Epoch 1250, training loss: 84.68486022949219 = 0.39485934376716614 + 10.0 * 8.428999900817871
Epoch 1250, val loss: 0.4657024145126343
Epoch 1260, training loss: 84.67100524902344 = 0.3936217427253723 + 10.0 * 8.427738189697266
Epoch 1260, val loss: 0.4651646614074707
Epoch 1270, training loss: 84.66333770751953 = 0.39238983392715454 + 10.0 * 8.427095413208008
Epoch 1270, val loss: 0.4645761251449585
Epoch 1280, training loss: 84.68438720703125 = 0.3911619484424591 + 10.0 * 8.429323196411133
Epoch 1280, val loss: 0.46405285596847534
Epoch 1290, training loss: 84.66313171386719 = 0.389920711517334 + 10.0 * 8.42732048034668
Epoch 1290, val loss: 0.4633830785751343
Epoch 1300, training loss: 84.63858795166016 = 0.3887040317058563 + 10.0 * 8.424988746643066
Epoch 1300, val loss: 0.46283644437789917
Epoch 1310, training loss: 84.63432312011719 = 0.3875056505203247 + 10.0 * 8.424681663513184
Epoch 1310, val loss: 0.4623109996318817
Epoch 1320, training loss: 84.65608978271484 = 0.38630595803260803 + 10.0 * 8.42697811126709
Epoch 1320, val loss: 0.4617665410041809
Epoch 1330, training loss: 84.62198638916016 = 0.3851224184036255 + 10.0 * 8.423686027526855
Epoch 1330, val loss: 0.46120157837867737
Epoch 1340, training loss: 84.60880279541016 = 0.38394874334335327 + 10.0 * 8.4224853515625
Epoch 1340, val loss: 0.46068617701530457
Epoch 1350, training loss: 84.5999526977539 = 0.3827890157699585 + 10.0 * 8.421716690063477
Epoch 1350, val loss: 0.460208535194397
Epoch 1360, training loss: 84.59263610839844 = 0.3816329836845398 + 10.0 * 8.421100616455078
Epoch 1360, val loss: 0.4596920311450958
Epoch 1370, training loss: 84.6059799194336 = 0.3804710805416107 + 10.0 * 8.422551155090332
Epoch 1370, val loss: 0.459231436252594
Epoch 1380, training loss: 84.5876235961914 = 0.3792967200279236 + 10.0 * 8.420832633972168
Epoch 1380, val loss: 0.458687424659729
Epoch 1390, training loss: 84.57684326171875 = 0.3781406879425049 + 10.0 * 8.419870376586914
Epoch 1390, val loss: 0.45820140838623047
Epoch 1400, training loss: 84.5632553100586 = 0.37700462341308594 + 10.0 * 8.418624877929688
Epoch 1400, val loss: 0.45770296454429626
Epoch 1410, training loss: 84.55599212646484 = 0.3758739233016968 + 10.0 * 8.418011665344238
Epoch 1410, val loss: 0.45725929737091064
Epoch 1420, training loss: 84.5510482788086 = 0.3747478425502777 + 10.0 * 8.417630195617676
Epoch 1420, val loss: 0.4567858576774597
Epoch 1430, training loss: 84.57369232177734 = 0.3736197054386139 + 10.0 * 8.420007705688477
Epoch 1430, val loss: 0.45631706714630127
Epoch 1440, training loss: 84.55314636230469 = 0.3724806010723114 + 10.0 * 8.418066024780273
Epoch 1440, val loss: 0.4558643400669098
Epoch 1450, training loss: 84.55271911621094 = 0.37135687470436096 + 10.0 * 8.418136596679688
Epoch 1450, val loss: 0.455379456281662
Epoch 1460, training loss: 84.53289031982422 = 0.37023308873176575 + 10.0 * 8.416265487670898
Epoch 1460, val loss: 0.4550197720527649
Epoch 1470, training loss: 84.51854705810547 = 0.3691340982913971 + 10.0 * 8.414941787719727
Epoch 1470, val loss: 0.45455124974250793
Epoch 1480, training loss: 84.5097885131836 = 0.36803606152534485 + 10.0 * 8.414175033569336
Epoch 1480, val loss: 0.45417264103889465
Epoch 1490, training loss: 84.5220718383789 = 0.36693790555000305 + 10.0 * 8.41551399230957
Epoch 1490, val loss: 0.4538004994392395
Epoch 1500, training loss: 84.50682830810547 = 0.36582499742507935 + 10.0 * 8.414100646972656
Epoch 1500, val loss: 0.45333439111709595
Epoch 1510, training loss: 84.49748992919922 = 0.36473187804222107 + 10.0 * 8.413275718688965
Epoch 1510, val loss: 0.4529401659965515
Epoch 1520, training loss: 84.48604583740234 = 0.3636507987976074 + 10.0 * 8.412240028381348
Epoch 1520, val loss: 0.4525802433490753
Epoch 1530, training loss: 84.50012969970703 = 0.36257433891296387 + 10.0 * 8.413755416870117
Epoch 1530, val loss: 0.45214712619781494
Epoch 1540, training loss: 84.47557067871094 = 0.3614770472049713 + 10.0 * 8.411409378051758
Epoch 1540, val loss: 0.451827734708786
Epoch 1550, training loss: 84.46939086914062 = 0.36040040850639343 + 10.0 * 8.41089916229248
Epoch 1550, val loss: 0.4514271914958954
Epoch 1560, training loss: 84.46226501464844 = 0.35933271050453186 + 10.0 * 8.410293579101562
Epoch 1560, val loss: 0.451093465089798
Epoch 1570, training loss: 84.45608520507812 = 0.35826724767684937 + 10.0 * 8.409781455993652
Epoch 1570, val loss: 0.4507540166378021
Epoch 1580, training loss: 84.4493179321289 = 0.3572036921977997 + 10.0 * 8.409212112426758
Epoch 1580, val loss: 0.4504082202911377
Epoch 1590, training loss: 84.45158386230469 = 0.35613909363746643 + 10.0 * 8.409543991088867
Epoch 1590, val loss: 0.45006299018859863
Epoch 1600, training loss: 84.4611587524414 = 0.3550611138343811 + 10.0 * 8.410609245300293
Epoch 1600, val loss: 0.4496820271015167
Epoch 1610, training loss: 84.4454574584961 = 0.3539840579032898 + 10.0 * 8.409147262573242
Epoch 1610, val loss: 0.4493865966796875
Epoch 1620, training loss: 84.42862701416016 = 0.35293880105018616 + 10.0 * 8.40756893157959
Epoch 1620, val loss: 0.4490446448326111
Epoch 1630, training loss: 84.42362213134766 = 0.35190004110336304 + 10.0 * 8.407172203063965
Epoch 1630, val loss: 0.4487541615962982
Epoch 1640, training loss: 84.41923522949219 = 0.35086601972579956 + 10.0 * 8.40683650970459
Epoch 1640, val loss: 0.4484533369541168
Epoch 1650, training loss: 84.44048309326172 = 0.34982389211654663 + 10.0 * 8.409066200256348
Epoch 1650, val loss: 0.4481695890426636
Epoch 1660, training loss: 84.41297149658203 = 0.34878024458885193 + 10.0 * 8.40641975402832
Epoch 1660, val loss: 0.4478721022605896
Epoch 1670, training loss: 84.403076171875 = 0.3477467894554138 + 10.0 * 8.405532836914062
Epoch 1670, val loss: 0.4475511908531189
Epoch 1680, training loss: 84.40106964111328 = 0.34671878814697266 + 10.0 * 8.405435562133789
Epoch 1680, val loss: 0.44729575514793396
Epoch 1690, training loss: 84.4378662109375 = 0.3456883132457733 + 10.0 * 8.409217834472656
Epoch 1690, val loss: 0.44702962040901184
Epoch 1700, training loss: 84.40169525146484 = 0.3446589708328247 + 10.0 * 8.4057035446167
Epoch 1700, val loss: 0.4467053711414337
Epoch 1710, training loss: 84.39482879638672 = 0.34363314509391785 + 10.0 * 8.405118942260742
Epoch 1710, val loss: 0.446433424949646
Epoch 1720, training loss: 84.38851928710938 = 0.3426167368888855 + 10.0 * 8.404590606689453
Epoch 1720, val loss: 0.4461854100227356
Epoch 1730, training loss: 84.37786865234375 = 0.34159770607948303 + 10.0 * 8.403627395629883
Epoch 1730, val loss: 0.44593122601509094
Epoch 1740, training loss: 84.37023162841797 = 0.34058526158332825 + 10.0 * 8.40296459197998
Epoch 1740, val loss: 0.4456871747970581
Epoch 1750, training loss: 84.36611938476562 = 0.339570015668869 + 10.0 * 8.402654647827148
Epoch 1750, val loss: 0.44542396068573
Epoch 1760, training loss: 84.37164306640625 = 0.3385501801967621 + 10.0 * 8.40330982208252
Epoch 1760, val loss: 0.44519996643066406
Epoch 1770, training loss: 84.37197875976562 = 0.33751797676086426 + 10.0 * 8.403446197509766
Epoch 1770, val loss: 0.4449533522129059
Epoch 1780, training loss: 84.35604858398438 = 0.3364894986152649 + 10.0 * 8.401956558227539
Epoch 1780, val loss: 0.4446692168712616
Epoch 1790, training loss: 84.35417175292969 = 0.335477352142334 + 10.0 * 8.40186882019043
Epoch 1790, val loss: 0.4444288909435272
Epoch 1800, training loss: 84.34776306152344 = 0.33446744084358215 + 10.0 * 8.40132999420166
Epoch 1800, val loss: 0.4442001283168793
Epoch 1810, training loss: 84.35735321044922 = 0.33345773816108704 + 10.0 * 8.402389526367188
Epoch 1810, val loss: 0.4439740478992462
Epoch 1820, training loss: 84.3470687866211 = 0.332438200712204 + 10.0 * 8.401463508605957
Epoch 1820, val loss: 0.4437415301799774
Epoch 1830, training loss: 84.3343276977539 = 0.33142220973968506 + 10.0 * 8.400290489196777
Epoch 1830, val loss: 0.4435296952724457
Epoch 1840, training loss: 84.33314514160156 = 0.33041661977767944 + 10.0 * 8.400273323059082
Epoch 1840, val loss: 0.4433392882347107
Epoch 1850, training loss: 84.3309326171875 = 0.3294071853160858 + 10.0 * 8.400152206420898
Epoch 1850, val loss: 0.44312945008277893
Epoch 1860, training loss: 84.34127044677734 = 0.3283950388431549 + 10.0 * 8.401288032531738
Epoch 1860, val loss: 0.4429347515106201
Epoch 1870, training loss: 84.32001495361328 = 0.32738161087036133 + 10.0 * 8.399263381958008
Epoch 1870, val loss: 0.44268885254859924
Epoch 1880, training loss: 84.31893157958984 = 0.32637569308280945 + 10.0 * 8.399255752563477
Epoch 1880, val loss: 0.4424642026424408
Epoch 1890, training loss: 84.31214141845703 = 0.325370192527771 + 10.0 * 8.398676872253418
Epoch 1890, val loss: 0.44229549169540405
Epoch 1900, training loss: 84.33146667480469 = 0.3243667781352997 + 10.0 * 8.400710105895996
Epoch 1900, val loss: 0.44210028648376465
Epoch 1910, training loss: 84.31501770019531 = 0.32336241006851196 + 10.0 * 8.399165153503418
Epoch 1910, val loss: 0.44193780422210693
Epoch 1920, training loss: 84.30477142333984 = 0.32236388325691223 + 10.0 * 8.39824104309082
Epoch 1920, val loss: 0.44173353910446167
Epoch 1930, training loss: 84.30400848388672 = 0.32137277722358704 + 10.0 * 8.398263931274414
Epoch 1930, val loss: 0.4415540397167206
Epoch 1940, training loss: 84.3261947631836 = 0.3203760087490082 + 10.0 * 8.400582313537598
Epoch 1940, val loss: 0.4413643181324005
Epoch 1950, training loss: 84.29569244384766 = 0.319367915391922 + 10.0 * 8.397632598876953
Epoch 1950, val loss: 0.44122982025146484
Epoch 1960, training loss: 84.28789520263672 = 0.3183838427066803 + 10.0 * 8.396951675415039
Epoch 1960, val loss: 0.4410475194454193
Epoch 1970, training loss: 84.28237915039062 = 0.3173980712890625 + 10.0 * 8.39649772644043
Epoch 1970, val loss: 0.4408826231956482
Epoch 1980, training loss: 84.27874755859375 = 0.31641289591789246 + 10.0 * 8.396233558654785
Epoch 1980, val loss: 0.4407326877117157
Epoch 1990, training loss: 84.30841064453125 = 0.3154243528842926 + 10.0 * 8.399298667907715
Epoch 1990, val loss: 0.440592885017395
Epoch 2000, training loss: 84.28522491455078 = 0.3144247829914093 + 10.0 * 8.397080421447754
Epoch 2000, val loss: 0.44043412804603577
Epoch 2010, training loss: 84.27230072021484 = 0.3134397268295288 + 10.0 * 8.395886421203613
Epoch 2010, val loss: 0.44026756286621094
Epoch 2020, training loss: 84.27345275878906 = 0.31245654821395874 + 10.0 * 8.396100044250488
Epoch 2020, val loss: 0.44013461470603943
Epoch 2030, training loss: 84.26918029785156 = 0.3114723563194275 + 10.0 * 8.395771026611328
Epoch 2030, val loss: 0.43998050689697266
Epoch 2040, training loss: 84.256103515625 = 0.3104943037033081 + 10.0 * 8.394560813903809
Epoch 2040, val loss: 0.4397980570793152
Epoch 2050, training loss: 84.25621032714844 = 0.30951640009880066 + 10.0 * 8.394669532775879
Epoch 2050, val loss: 0.43967512249946594
Epoch 2060, training loss: 84.25286865234375 = 0.3085365295410156 + 10.0 * 8.39443302154541
Epoch 2060, val loss: 0.43951913714408875
Epoch 2070, training loss: 84.25144958496094 = 0.30755823850631714 + 10.0 * 8.394389152526855
Epoch 2070, val loss: 0.43937909603118896
Epoch 2080, training loss: 84.25 = 0.3065800666809082 + 10.0 * 8.394342422485352
Epoch 2080, val loss: 0.43923285603523254
Epoch 2090, training loss: 84.26443481445312 = 0.30560368299484253 + 10.0 * 8.395883560180664
Epoch 2090, val loss: 0.4391256272792816
Epoch 2100, training loss: 84.24591827392578 = 0.3046157956123352 + 10.0 * 8.394129753112793
Epoch 2100, val loss: 0.43902844190597534
Epoch 2110, training loss: 84.23389434814453 = 0.303650438785553 + 10.0 * 8.393024444580078
Epoch 2110, val loss: 0.4388483464717865
Epoch 2120, training loss: 84.2257308959961 = 0.30268964171409607 + 10.0 * 8.392304420471191
Epoch 2120, val loss: 0.4387549161911011
Epoch 2130, training loss: 84.22198486328125 = 0.3017333447933197 + 10.0 * 8.392024993896484
Epoch 2130, val loss: 0.438657283782959
Epoch 2140, training loss: 84.21934509277344 = 0.30077749490737915 + 10.0 * 8.39185619354248
Epoch 2140, val loss: 0.4385424852371216
Epoch 2150, training loss: 84.2550048828125 = 0.2998177707195282 + 10.0 * 8.39551830291748
Epoch 2150, val loss: 0.43846291303634644
Epoch 2160, training loss: 84.23027038574219 = 0.2988464832305908 + 10.0 * 8.393142700195312
Epoch 2160, val loss: 0.43833285570144653
Epoch 2170, training loss: 84.21992492675781 = 0.2978827655315399 + 10.0 * 8.392204284667969
Epoch 2170, val loss: 0.4381788372993469
Epoch 2180, training loss: 84.20574188232422 = 0.296937495470047 + 10.0 * 8.390880584716797
Epoch 2180, val loss: 0.43810024857521057
Epoch 2190, training loss: 84.20233154296875 = 0.2959935963153839 + 10.0 * 8.390634536743164
Epoch 2190, val loss: 0.4380165636539459
Epoch 2200, training loss: 84.19892120361328 = 0.29504886269569397 + 10.0 * 8.390386581420898
Epoch 2200, val loss: 0.43791672587394714
Epoch 2210, training loss: 84.20291900634766 = 0.29410600662231445 + 10.0 * 8.390881538391113
Epoch 2210, val loss: 0.43782398104667664
Epoch 2220, training loss: 84.2042007446289 = 0.29315489530563354 + 10.0 * 8.391104698181152
Epoch 2220, val loss: 0.43773403763771057
Epoch 2230, training loss: 84.1903305053711 = 0.29221177101135254 + 10.0 * 8.389811515808105
Epoch 2230, val loss: 0.4375775456428528
Epoch 2240, training loss: 84.18646240234375 = 0.29127466678619385 + 10.0 * 8.389518737792969
Epoch 2240, val loss: 0.43754318356513977
Epoch 2250, training loss: 84.18970489501953 = 0.2903403341770172 + 10.0 * 8.389936447143555
Epoch 2250, val loss: 0.4374745786190033
Epoch 2260, training loss: 84.18632507324219 = 0.28940048813819885 + 10.0 * 8.389692306518555
Epoch 2260, val loss: 0.4373905658721924
Epoch 2270, training loss: 84.18209075927734 = 0.28846511244773865 + 10.0 * 8.389362335205078
Epoch 2270, val loss: 0.4372847378253937
Epoch 2280, training loss: 84.19256591796875 = 0.28753232955932617 + 10.0 * 8.390503883361816
Epoch 2280, val loss: 0.437226265668869
Epoch 2290, training loss: 84.17388153076172 = 0.28659340739250183 + 10.0 * 8.388729095458984
Epoch 2290, val loss: 0.43721678853034973
Epoch 2300, training loss: 84.17131805419922 = 0.28566330671310425 + 10.0 * 8.388566017150879
Epoch 2300, val loss: 0.4371422529220581
Epoch 2310, training loss: 84.16448211669922 = 0.284736692905426 + 10.0 * 8.387974739074707
Epoch 2310, val loss: 0.4370969831943512
Epoch 2320, training loss: 84.1624755859375 = 0.28380993008613586 + 10.0 * 8.387866020202637
Epoch 2320, val loss: 0.4370427429676056
Epoch 2330, training loss: 84.19237518310547 = 0.2828778028488159 + 10.0 * 8.390950202941895
Epoch 2330, val loss: 0.4370288848876953
Epoch 2340, training loss: 84.16725158691406 = 0.28194350004196167 + 10.0 * 8.388530731201172
Epoch 2340, val loss: 0.4369218349456787
Epoch 2350, training loss: 84.15827178955078 = 0.28101447224617004 + 10.0 * 8.387725830078125
Epoch 2350, val loss: 0.43684595823287964
Epoch 2360, training loss: 84.14949798583984 = 0.2800920903682709 + 10.0 * 8.386940956115723
Epoch 2360, val loss: 0.4368174076080322
Epoch 2370, training loss: 84.14569854736328 = 0.27917030453681946 + 10.0 * 8.386652946472168
Epoch 2370, val loss: 0.436773419380188
Epoch 2380, training loss: 84.14266967773438 = 0.2782432734966278 + 10.0 * 8.386442184448242
Epoch 2380, val loss: 0.43675363063812256
Epoch 2390, training loss: 84.14811706542969 = 0.2773131728172302 + 10.0 * 8.387080192565918
Epoch 2390, val loss: 0.43672746419906616
Epoch 2400, training loss: 84.16534423828125 = 0.2763744592666626 + 10.0 * 8.388896942138672
Epoch 2400, val loss: 0.43669217824935913
Epoch 2410, training loss: 84.14644622802734 = 0.27543333172798157 + 10.0 * 8.387101173400879
Epoch 2410, val loss: 0.43663647770881653
Epoch 2420, training loss: 84.13379669189453 = 0.2744970917701721 + 10.0 * 8.385930061340332
Epoch 2420, val loss: 0.4366076588630676
Epoch 2430, training loss: 84.13054656982422 = 0.2735702097415924 + 10.0 * 8.385698318481445
Epoch 2430, val loss: 0.4366207420825958
Epoch 2440, training loss: 84.12554931640625 = 0.2726472020149231 + 10.0 * 8.385290145874023
Epoch 2440, val loss: 0.4366089105606079
Epoch 2450, training loss: 84.12200164794922 = 0.2717239558696747 + 10.0 * 8.385027885437012
Epoch 2450, val loss: 0.43661338090896606
Epoch 2460, training loss: 84.13028717041016 = 0.27079975605010986 + 10.0 * 8.38594913482666
Epoch 2460, val loss: 0.4366225600242615
Epoch 2470, training loss: 84.14149475097656 = 0.2698661684989929 + 10.0 * 8.387163162231445
Epoch 2470, val loss: 0.4366613030433655
Epoch 2480, training loss: 84.11886596679688 = 0.26892799139022827 + 10.0 * 8.384993553161621
Epoch 2480, val loss: 0.43666887283325195
Epoch 2490, training loss: 84.11128997802734 = 0.2679978311061859 + 10.0 * 8.384328842163086
Epoch 2490, val loss: 0.4367472231388092
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.828513444951801
0.8641599652249512
The final CL Acc:0.83376, 0.00371, The final GNN Acc:0.86481, 0.00082
Begin epxeriment: noisy_level: 0.2 cont_weight: 10 epoch: der1:0.3 der2:0.3 dfr1:0.4 dfr2:0
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=2500, cl_num_layers=2, cl_num_proj_hidden=64, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=10.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.3, drop_feat_rate_1=0.4, drop_feat_rate_2=0.0, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=32, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, noisy_level=0.2, num_hidden=64, num_proj_hidden=64, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=40, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 88648])
add edge: torch.Size([2, 106242])
remove edge: torch.Size([2, 70474])
updated graph: torch.Size([2, 88068])
=== Noisy graph ===
/home/project-robust-contrastive/Robust-Contrastive-Learning/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 101.47358703613281 = 1.0971860885620117 + 10.0 * 10.037640571594238
Epoch 0, val loss: 1.0986326932907104
Epoch 10, training loss: 97.32199096679688 = 1.0937532186508179 + 10.0 * 9.622823715209961
Epoch 10, val loss: 1.0952049493789673
Epoch 20, training loss: 95.76600646972656 = 1.0904815196990967 + 10.0 * 9.467552185058594
Epoch 20, val loss: 1.0919218063354492
Epoch 30, training loss: 94.59394073486328 = 1.0873792171478271 + 10.0 * 9.350656509399414
Epoch 30, val loss: 1.0888150930404663
Epoch 40, training loss: 93.66371154785156 = 1.0844738483428955 + 10.0 * 9.25792407989502
Epoch 40, val loss: 1.0858991146087646
Epoch 50, training loss: 92.90731048583984 = 1.0817400217056274 + 10.0 * 9.182557106018066
Epoch 50, val loss: 1.0831563472747803
Epoch 60, training loss: 92.2706298828125 = 1.0791897773742676 + 10.0 * 9.11914348602295
Epoch 60, val loss: 1.0805991888046265
Epoch 70, training loss: 91.72329711914062 = 1.076789140701294 + 10.0 * 9.064650535583496
Epoch 70, val loss: 1.0781913995742798
Epoch 80, training loss: 91.25408172607422 = 1.0745494365692139 + 10.0 * 9.017953872680664
Epoch 80, val loss: 1.0759435892105103
Epoch 90, training loss: 90.8409652709961 = 1.0724538564682007 + 10.0 * 8.976850509643555
Epoch 90, val loss: 1.0738378763198853
Epoch 100, training loss: 90.47504425048828 = 1.0705066919326782 + 10.0 * 8.940454483032227
Epoch 100, val loss: 1.0718774795532227
Epoch 110, training loss: 90.17268371582031 = 1.0687103271484375 + 10.0 * 8.91039752960205
Epoch 110, val loss: 1.0700658559799194
Epoch 120, training loss: 89.89138793945312 = 1.0670477151870728 + 10.0 * 8.882433891296387
Epoch 120, val loss: 1.0683844089508057
Epoch 130, training loss: 89.63619232177734 = 1.0655144453048706 + 10.0 * 8.857068061828613
Epoch 130, val loss: 1.0668340921401978
Epoch 140, training loss: 89.43649291992188 = 1.0641155242919922 + 10.0 * 8.837237358093262
Epoch 140, val loss: 1.0654155015945435
Epoch 150, training loss: 89.25796508789062 = 1.0628504753112793 + 10.0 * 8.819511413574219
Epoch 150, val loss: 1.0641288757324219
Epoch 160, training loss: 89.1055679321289 = 1.0617129802703857 + 10.0 * 8.8043851852417
Epoch 160, val loss: 1.0629688501358032
Epoch 170, training loss: 89.02952575683594 = 1.0606606006622314 + 10.0 * 8.796886444091797
Epoch 170, val loss: 1.0618818998336792
Epoch 180, training loss: 88.91674041748047 = 1.0598225593566895 + 10.0 * 8.78569221496582
Epoch 180, val loss: 1.0610120296478271
Epoch 190, training loss: 88.75286102294922 = 1.0590089559555054 + 10.0 * 8.76938533782959
Epoch 190, val loss: 1.060179352760315
Epoch 200, training loss: 88.67886352539062 = 1.0583281517028809 + 10.0 * 8.762053489685059
Epoch 200, val loss: 1.0594663619995117
Epoch 210, training loss: 88.61148834228516 = 1.0577598810195923 + 10.0 * 8.755373001098633
Epoch 210, val loss: 1.0588583946228027
Epoch 220, training loss: 88.58346557617188 = 1.0572538375854492 + 10.0 * 8.7526216506958
Epoch 220, val loss: 1.058325171470642
Epoch 230, training loss: 88.48656463623047 = 1.0568169355392456 + 10.0 * 8.742975234985352
Epoch 230, val loss: 1.057848334312439
Epoch 240, training loss: 88.46049499511719 = 1.0565029382705688 + 10.0 * 8.740399360656738
Epoch 240, val loss: 1.057511806488037
Epoch 250, training loss: 88.40139770507812 = 1.056207299232483 + 10.0 * 8.734519004821777
Epoch 250, val loss: 1.0571985244750977
Epoch 260, training loss: 88.3305892944336 = 1.0559643507003784 + 10.0 * 8.727462768554688
Epoch 260, val loss: 1.0569225549697876
Epoch 270, training loss: 88.28726196289062 = 1.0557769536972046 + 10.0 * 8.723148345947266
Epoch 270, val loss: 1.0567187070846558
Epoch 280, training loss: 88.27674102783203 = 1.0556329488754272 + 10.0 * 8.722110748291016
Epoch 280, val loss: 1.0565330982208252
Epoch 290, training loss: 88.28096771240234 = 1.0555129051208496 + 10.0 * 8.722545623779297
Epoch 290, val loss: 1.0563955307006836
Epoch 300, training loss: 88.2425308227539 = 1.0554099082946777 + 10.0 * 8.718711853027344
Epoch 300, val loss: 1.0562777519226074
Epoch 310, training loss: 88.18277740478516 = 1.05532705783844 + 10.0 * 8.71274471282959
Epoch 310, val loss: 1.0561747550964355
Epoch 320, training loss: 88.1719741821289 = 1.0552606582641602 + 10.0 * 8.711671829223633
Epoch 320, val loss: 1.056097388267517
Epoch 330, training loss: 88.16449737548828 = 1.055205225944519 + 10.0 * 8.710928916931152
Epoch 330, val loss: 1.0560263395309448
Epoch 340, training loss: 88.17655944824219 = 1.0551751852035522 + 10.0 * 8.712138175964355
Epoch 340, val loss: 1.0559748411178589
Epoch 350, training loss: 88.15145111083984 = 1.055132269859314 + 10.0 * 8.70963191986084
Epoch 350, val loss: 1.0559287071228027
Epoch 360, training loss: 88.1539535522461 = 1.0551142692565918 + 10.0 * 8.709883689880371
Epoch 360, val loss: 1.0558958053588867
Epoch 370, training loss: 88.15568542480469 = 1.055095911026001 + 10.0 * 8.710058212280273
Epoch 370, val loss: 1.055859923362732
Epoch 380, training loss: 88.13143920898438 = 1.055058240890503 + 10.0 * 8.707637786865234
Epoch 380, val loss: 1.0558239221572876
Epoch 390, training loss: 88.12728118896484 = 1.055026650428772 + 10.0 * 8.707225799560547
Epoch 390, val loss: 1.0557973384857178
Epoch 400, training loss: 88.0955581665039 = 1.0549956560134888 + 10.0 * 8.704056739807129
Epoch 400, val loss: 1.0557552576065063
Epoch 410, training loss: 88.10511016845703 = 1.0549750328063965 + 10.0 * 8.705013275146484
Epoch 410, val loss: 1.0557360649108887
Epoch 420, training loss: 88.13491821289062 = 1.0549848079681396 + 10.0 * 8.707993507385254
Epoch 420, val loss: 1.055719017982483
Epoch 430, training loss: 88.11737823486328 = 1.054965853691101 + 10.0 * 8.706241607666016
Epoch 430, val loss: 1.055711269378662
Epoch 440, training loss: 88.12383270263672 = 1.054955005645752 + 10.0 * 8.706888198852539
Epoch 440, val loss: 1.0556960105895996
Epoch 450, training loss: 88.14977264404297 = 1.0549424886703491 + 10.0 * 8.70948314666748
Epoch 450, val loss: 1.0556871891021729
Epoch 460, training loss: 88.18492126464844 = 1.054917335510254 + 10.0 * 8.713000297546387
Epoch 460, val loss: 1.0556557178497314
Epoch 470, training loss: 88.17976379394531 = 1.054901361465454 + 10.0 * 8.712486267089844
Epoch 470, val loss: 1.0556389093399048
Epoch 480, training loss: 88.17053985595703 = 1.0548770427703857 + 10.0 * 8.711565971374512
Epoch 480, val loss: 1.055615782737732
Epoch 490, training loss: 88.18942260742188 = 1.0548537969589233 + 10.0 * 8.713457107543945
Epoch 490, val loss: 1.0555940866470337
Epoch 500, training loss: 88.20382690429688 = 1.0548417568206787 + 10.0 * 8.714899063110352
Epoch 500, val loss: 1.05559241771698
Epoch 510, training loss: 88.19293212890625 = 1.0548089742660522 + 10.0 * 8.713811874389648
Epoch 510, val loss: 1.0555540323257446
Epoch 520, training loss: 88.12939453125 = 1.054749608039856 + 10.0 * 8.707464218139648
Epoch 520, val loss: 1.0555124282836914
Epoch 530, training loss: 88.22928619384766 = 1.054794192314148 + 10.0 * 8.717449188232422
Epoch 530, val loss: 1.0555278062820435
Epoch 540, training loss: 88.22813415527344 = 1.0547773838043213 + 10.0 * 8.71733570098877
Epoch 540, val loss: 1.0555155277252197
Epoch 550, training loss: 88.27932739257812 = 1.054776668548584 + 10.0 * 8.722455024719238
Epoch 550, val loss: 1.055508017539978
Epoch 560, training loss: 88.29045104980469 = 1.0547603368759155 + 10.0 * 8.7235689163208
Epoch 560, val loss: 1.0554656982421875
Epoch 570, training loss: 88.33734893798828 = 1.0547072887420654 + 10.0 * 8.728263854980469
Epoch 570, val loss: 1.0554544925689697
Epoch 580, training loss: 88.17774963378906 = 1.0546501874923706 + 10.0 * 8.712309837341309
Epoch 580, val loss: 1.0553829669952393
Epoch 590, training loss: 88.28157806396484 = 1.054692029953003 + 10.0 * 8.722688674926758
Epoch 590, val loss: 1.0554311275482178
Epoch 600, training loss: 88.27286529541016 = 1.0546616315841675 + 10.0 * 8.721819877624512
Epoch 600, val loss: 1.0554012060165405
Epoch 610, training loss: 88.32699584960938 = 1.0546602010726929 + 10.0 * 8.72723388671875
Epoch 610, val loss: 1.0554068088531494
Epoch 620, training loss: 88.33440399169922 = 1.0546245574951172 + 10.0 * 8.727977752685547
Epoch 620, val loss: 1.0553672313690186
Epoch 630, training loss: 88.34786224365234 = 1.054619312286377 + 10.0 * 8.729324340820312
Epoch 630, val loss: 1.0553566217422485
Epoch 640, training loss: 88.38732147216797 = 1.05460524559021 + 10.0 * 8.733271598815918
Epoch 640, val loss: 1.0553436279296875
Epoch 650, training loss: 88.42131042480469 = 1.0545905828475952 + 10.0 * 8.736672401428223
Epoch 650, val loss: 1.0553361177444458
Epoch 660, training loss: 88.46076202392578 = 1.0545850992202759 + 10.0 * 8.740617752075195
Epoch 660, val loss: 1.0553230047225952
Epoch 670, training loss: 88.43193817138672 = 1.0545401573181152 + 10.0 * 8.737739562988281
Epoch 670, val loss: 1.0552743673324585
Epoch 680, training loss: 88.45342254638672 = 1.054525375366211 + 10.0 * 8.739889144897461
Epoch 680, val loss: 1.0552691221237183
Epoch 690, training loss: 88.42942810058594 = 1.0544909238815308 + 10.0 * 8.737493515014648
Epoch 690, val loss: 1.0552421808242798
Epoch 700, training loss: 88.59646606445312 = 1.0545227527618408 + 10.0 * 8.754194259643555
Epoch 700, val loss: 1.05527663230896
Epoch 710, training loss: 88.56076049804688 = 1.0544990301132202 + 10.0 * 8.750626564025879
Epoch 710, val loss: 1.0552605390548706
Epoch 720, training loss: 88.61414337158203 = 1.0545114278793335 + 10.0 * 8.755963325500488
Epoch 720, val loss: 1.0552709102630615
Epoch 730, training loss: 88.67347717285156 = 1.0545181035995483 + 10.0 * 8.761896133422852
Epoch 730, val loss: 1.0552630424499512
Epoch 740, training loss: 88.65764617919922 = 1.0544896125793457 + 10.0 * 8.760315895080566
Epoch 740, val loss: 1.0552403926849365
Epoch 750, training loss: 88.70115661621094 = 1.0544813871383667 + 10.0 * 8.764667510986328
Epoch 750, val loss: 1.0552302598953247
Epoch 760, training loss: 88.68852996826172 = 1.0544514656066895 + 10.0 * 8.763407707214355
Epoch 760, val loss: 1.0552113056182861
Epoch 770, training loss: 88.73391723632812 = 1.0544381141662598 + 10.0 * 8.767948150634766
Epoch 770, val loss: 1.055201768875122
Epoch 780, training loss: 88.76156616210938 = 1.0544278621673584 + 10.0 * 8.770713806152344
Epoch 780, val loss: 1.0551837682724
Epoch 790, training loss: 88.80609893798828 = 1.0544143915176392 + 10.0 * 8.775168418884277
Epoch 790, val loss: 1.0551589727401733
Epoch 800, training loss: 88.79163360595703 = 1.0543996095657349 + 10.0 * 8.773723602294922
Epoch 800, val loss: 1.0551469326019287
Epoch 810, training loss: 88.8417739868164 = 1.0543829202651978 + 10.0 * 8.778738975524902
Epoch 810, val loss: 1.0551341772079468
Epoch 820, training loss: 88.82559204101562 = 1.0543569326400757 + 10.0 * 8.77712345123291
Epoch 820, val loss: 1.0551034212112427
Epoch 830, training loss: 88.83927154541016 = 1.0543290376663208 + 10.0 * 8.778493881225586
Epoch 830, val loss: 1.0550950765609741
Epoch 840, training loss: 88.85157012939453 = 1.0542999505996704 + 10.0 * 8.7797269821167
Epoch 840, val loss: 1.0550466775894165
Epoch 850, training loss: 88.91761779785156 = 1.0542936325073242 + 10.0 * 8.786333084106445
Epoch 850, val loss: 1.0550462007522583
Epoch 860, training loss: 88.92935180664062 = 1.0542802810668945 + 10.0 * 8.787507057189941
Epoch 860, val loss: 1.0550471544265747
Epoch 870, training loss: 88.9961929321289 = 1.0542857646942139 + 10.0 * 8.794191360473633
Epoch 870, val loss: 1.0550495386123657
Epoch 880, training loss: 88.97615051269531 = 1.0542508363723755 + 10.0 * 8.792189598083496
Epoch 880, val loss: 1.055010437965393
Epoch 890, training loss: 88.99943542480469 = 1.054246425628662 + 10.0 * 8.794519424438477
Epoch 890, val loss: 1.054998755455017
Epoch 900, training loss: 88.93663787841797 = 1.0541878938674927 + 10.0 * 8.788244247436523
Epoch 900, val loss: 1.0549508333206177
Epoch 910, training loss: 88.96852111816406 = 1.0541847944259644 + 10.0 * 8.791433334350586
Epoch 910, val loss: 1.0549532175064087
Epoch 920, training loss: 89.02718353271484 = 1.0541796684265137 + 10.0 * 8.797300338745117
Epoch 920, val loss: 1.0549466609954834
Epoch 930, training loss: 89.00933074951172 = 1.054132103919983 + 10.0 * 8.795519828796387
Epoch 930, val loss: 1.0548980236053467
Epoch 940, training loss: 89.04737854003906 = 1.0541303157806396 + 10.0 * 8.799324989318848
Epoch 940, val loss: 1.0548903942108154
Epoch 950, training loss: 89.05062103271484 = 1.0541002750396729 + 10.0 * 8.799652099609375
Epoch 950, val loss: 1.0548673868179321
Epoch 960, training loss: 89.05502319335938 = 1.0540823936462402 + 10.0 * 8.800093650817871
Epoch 960, val loss: 1.0548410415649414
Epoch 970, training loss: 89.08943939208984 = 1.054067850112915 + 10.0 * 8.803537368774414
Epoch 970, val loss: 1.0548371076583862
Epoch 980, training loss: 89.09957885742188 = 1.054045557975769 + 10.0 * 8.804553031921387
Epoch 980, val loss: 1.054802656173706
Epoch 990, training loss: 89.11026763916016 = 1.0540165901184082 + 10.0 * 8.805624961853027
Epoch 990, val loss: 1.0547878742218018
Epoch 1000, training loss: 89.1978530883789 = 1.0540000200271606 + 10.0 * 8.814385414123535
Epoch 1000, val loss: 1.054772138595581
Epoch 1010, training loss: 89.11004638671875 = 1.0539448261260986 + 10.0 * 8.805609703063965
Epoch 1010, val loss: 1.0547199249267578
Epoch 1020, training loss: 89.10792541503906 = 1.0538945198059082 + 10.0 * 8.805402755737305
Epoch 1020, val loss: 1.054666519165039
Epoch 1030, training loss: 89.08586883544922 = 1.0538700819015503 + 10.0 * 8.803199768066406
Epoch 1030, val loss: 1.0546491146087646
Epoch 1040, training loss: 89.06214141845703 = 1.0538265705108643 + 10.0 * 8.80083179473877
Epoch 1040, val loss: 1.0545973777770996
Epoch 1050, training loss: 89.13512420654297 = 1.053835153579712 + 10.0 * 8.80812931060791
Epoch 1050, val loss: 1.054619550704956
Epoch 1060, training loss: 89.18987274169922 = 1.053828239440918 + 10.0 * 8.813604354858398
Epoch 1060, val loss: 1.054620623588562
Epoch 1070, training loss: 89.24913787841797 = 1.0538393259048462 + 10.0 * 8.81952953338623
Epoch 1070, val loss: 1.0546246767044067
Epoch 1080, training loss: 89.26280975341797 = 1.0538119077682495 + 10.0 * 8.820899963378906
Epoch 1080, val loss: 1.0546045303344727
Epoch 1090, training loss: 89.27725982666016 = 1.0537816286087036 + 10.0 * 8.822347640991211
Epoch 1090, val loss: 1.0545729398727417
Epoch 1100, training loss: 89.29661560058594 = 1.0537588596343994 + 10.0 * 8.824285507202148
Epoch 1100, val loss: 1.0545501708984375
Epoch 1110, training loss: 89.31324005126953 = 1.0537383556365967 + 10.0 * 8.825949668884277
Epoch 1110, val loss: 1.0545284748077393
Epoch 1120, training loss: 89.31361389160156 = 1.0536974668502808 + 10.0 * 8.8259916305542
Epoch 1120, val loss: 1.0544952154159546
Epoch 1130, training loss: 89.32401275634766 = 1.0536751747131348 + 10.0 * 8.827033996582031
Epoch 1130, val loss: 1.0544788837432861
Epoch 1140, training loss: 89.33479309082031 = 1.0536514520645142 + 10.0 * 8.82811450958252
Epoch 1140, val loss: 1.0544575452804565
Epoch 1150, training loss: 89.35121154785156 = 1.0536305904388428 + 10.0 * 8.829758644104004
Epoch 1150, val loss: 1.054445743560791
Epoch 1160, training loss: 89.36864471435547 = 1.0536011457443237 + 10.0 * 8.831503868103027
Epoch 1160, val loss: 1.0544085502624512
Epoch 1170, training loss: 89.3206558227539 = 1.0535247325897217 + 10.0 * 8.826712608337402
Epoch 1170, val loss: 1.0543503761291504
Epoch 1180, training loss: 89.34764099121094 = 1.053512454032898 + 10.0 * 8.829412460327148
Epoch 1180, val loss: 1.0543471574783325
Epoch 1190, training loss: 89.42265319824219 = 1.0535074472427368 + 10.0 * 8.836915016174316
Epoch 1190, val loss: 1.054341435432434
Epoch 1200, training loss: 89.4409408569336 = 1.0534801483154297 + 10.0 * 8.838746070861816
Epoch 1200, val loss: 1.0543159246444702
Epoch 1210, training loss: 89.48426055908203 = 1.0534591674804688 + 10.0 * 8.843080520629883
Epoch 1210, val loss: 1.0542856454849243
Epoch 1220, training loss: 89.49211883544922 = 1.0534113645553589 + 10.0 * 8.843870162963867
Epoch 1220, val loss: 1.054236888885498
Epoch 1230, training loss: 89.45207977294922 = 1.0533759593963623 + 10.0 * 8.83987045288086
Epoch 1230, val loss: 1.0542023181915283
Epoch 1240, training loss: 89.47867584228516 = 1.0533477067947388 + 10.0 * 8.842533111572266
Epoch 1240, val loss: 1.0541908740997314
Epoch 1250, training loss: 89.53704071044922 = 1.0533243417739868 + 10.0 * 8.848371505737305
Epoch 1250, val loss: 1.0541776418685913
Epoch 1260, training loss: 89.53619384765625 = 1.0532851219177246 + 10.0 * 8.848291397094727
Epoch 1260, val loss: 1.0541340112686157
Epoch 1270, training loss: 89.54536437988281 = 1.0532441139221191 + 10.0 * 8.849211692810059
Epoch 1270, val loss: 1.0540711879730225
Epoch 1280, training loss: 89.49048614501953 = 1.0531775951385498 + 10.0 * 8.843730926513672
Epoch 1280, val loss: 1.0540207624435425
Epoch 1290, training loss: 89.50633239746094 = 1.0531163215637207 + 10.0 * 8.845321655273438
Epoch 1290, val loss: 1.053989291191101
Epoch 1300, training loss: 89.58338165283203 = 1.0531271696090698 + 10.0 * 8.853025436401367
Epoch 1300, val loss: 1.0540152788162231
Epoch 1310, training loss: 89.64318084716797 = 1.053123116493225 + 10.0 * 8.85900592803955
Epoch 1310, val loss: 1.0540046691894531
Epoch 1320, training loss: 89.60248565673828 = 1.0530518293380737 + 10.0 * 8.85494327545166
Epoch 1320, val loss: 1.0539391040802002
Epoch 1330, training loss: 89.63298034667969 = 1.0530303716659546 + 10.0 * 8.85799503326416
Epoch 1330, val loss: 1.0539145469665527
Epoch 1340, training loss: 89.6655502319336 = 1.0530160665512085 + 10.0 * 8.86125373840332
Epoch 1340, val loss: 1.0539029836654663
Epoch 1350, training loss: 89.7148666381836 = 1.0529900789260864 + 10.0 * 8.86618709564209
Epoch 1350, val loss: 1.0538705587387085
Epoch 1360, training loss: 89.64203643798828 = 1.0529063940048218 + 10.0 * 8.858912467956543
Epoch 1360, val loss: 1.0538132190704346
Epoch 1370, training loss: 89.6812744140625 = 1.0528841018676758 + 10.0 * 8.862838745117188
Epoch 1370, val loss: 1.0537933111190796
Epoch 1380, training loss: 89.73155212402344 = 1.052870512008667 + 10.0 * 8.867868423461914
Epoch 1380, val loss: 1.0537827014923096
Epoch 1390, training loss: 89.72570037841797 = 1.0528333187103271 + 10.0 * 8.867286682128906
Epoch 1390, val loss: 1.0537415742874146
Epoch 1400, training loss: 89.74913787841797 = 1.0528054237365723 + 10.0 * 8.869633674621582
Epoch 1400, val loss: 1.053701639175415
Epoch 1410, training loss: 89.75658416748047 = 1.0527522563934326 + 10.0 * 8.870383262634277
Epoch 1410, val loss: 1.0536777973175049
Epoch 1420, training loss: 89.75443267822266 = 1.052686333656311 + 10.0 * 8.870174407958984
Epoch 1420, val loss: 1.0536260604858398
Epoch 1430, training loss: 89.35847473144531 = 1.0519816875457764 + 10.0 * 8.830649375915527
Epoch 1430, val loss: 1.0529941320419312
Epoch 1440, training loss: 89.70850372314453 = 1.0524159669876099 + 10.0 * 8.865609169006348
Epoch 1440, val loss: 1.0533196926116943
Epoch 1450, training loss: 89.56519317626953 = 1.052383303642273 + 10.0 * 8.85128116607666
Epoch 1450, val loss: 1.0533711910247803
Epoch 1460, training loss: 89.62409210205078 = 1.0524044036865234 + 10.0 * 8.857168197631836
Epoch 1460, val loss: 1.0533615350723267
Epoch 1470, training loss: 89.68975067138672 = 1.052423357963562 + 10.0 * 8.863733291625977
Epoch 1470, val loss: 1.053389549255371
Epoch 1480, training loss: 89.74359893798828 = 1.052449107170105 + 10.0 * 8.869114875793457
Epoch 1480, val loss: 1.0534228086471558
Epoch 1490, training loss: 89.81468963623047 = 1.0524557828903198 + 10.0 * 8.87622356414795
Epoch 1490, val loss: 1.0534318685531616
Epoch 1500, training loss: 89.86511993408203 = 1.0524498224258423 + 10.0 * 8.881266593933105
Epoch 1500, val loss: 1.053417444229126
Epoch 1510, training loss: 89.89624786376953 = 1.052417278289795 + 10.0 * 8.884383201599121
Epoch 1510, val loss: 1.0533995628356934
Epoch 1520, training loss: 89.9214859008789 = 1.0523864030838013 + 10.0 * 8.886910438537598
Epoch 1520, val loss: 1.0533696413040161
Epoch 1530, training loss: 89.94770050048828 = 1.0523579120635986 + 10.0 * 8.889533996582031
Epoch 1530, val loss: 1.0533419847488403
Epoch 1540, training loss: 89.94696807861328 = 1.0522958040237427 + 10.0 * 8.889467239379883
Epoch 1540, val loss: 1.0532917976379395
Epoch 1550, training loss: 89.99624633789062 = 1.052280068397522 + 10.0 * 8.894396781921387
Epoch 1550, val loss: 1.053279995918274
Epoch 1560, training loss: 90.03121948242188 = 1.0522468090057373 + 10.0 * 8.897897720336914
Epoch 1560, val loss: 1.0532476902008057
Epoch 1570, training loss: 90.0389633178711 = 1.0521936416625977 + 10.0 * 8.898676872253418
Epoch 1570, val loss: 1.0531970262527466
Epoch 1580, training loss: 90.01897430419922 = 1.0521339178085327 + 10.0 * 8.896684646606445
Epoch 1580, val loss: 1.0531564950942993
Epoch 1590, training loss: 90.03899383544922 = 1.0520826578140259 + 10.0 * 8.898691177368164
Epoch 1590, val loss: 1.0531131029129028
Epoch 1600, training loss: 90.10166931152344 = 1.0520647764205933 + 10.0 * 8.904960632324219
Epoch 1600, val loss: 1.0530974864959717
Epoch 1610, training loss: 90.12044525146484 = 1.0520341396331787 + 10.0 * 8.906841278076172
Epoch 1610, val loss: 1.0530486106872559
Epoch 1620, training loss: 90.10198974609375 = 1.0519646406173706 + 10.0 * 8.90500259399414
Epoch 1620, val loss: 1.053000807762146
Epoch 1630, training loss: 90.128662109375 = 1.0519087314605713 + 10.0 * 8.907674789428711
Epoch 1630, val loss: 1.0529693365097046
Epoch 1640, training loss: 90.18222045898438 = 1.0519015789031982 + 10.0 * 8.913031578063965
Epoch 1640, val loss: 1.0529447793960571
Epoch 1650, training loss: 90.21856689453125 = 1.051862359046936 + 10.0 * 8.916669845581055
Epoch 1650, val loss: 1.0529124736785889
Epoch 1660, training loss: 90.22039031982422 = 1.0518085956573486 + 10.0 * 8.916857719421387
Epoch 1660, val loss: 1.0528531074523926
Epoch 1670, training loss: 90.2190170288086 = 1.0517369508743286 + 10.0 * 8.916728019714355
Epoch 1670, val loss: 1.0528054237365723
Epoch 1680, training loss: 90.24411010742188 = 1.0517144203186035 + 10.0 * 8.91923999786377
Epoch 1680, val loss: 1.0527955293655396
Epoch 1690, training loss: 90.2840576171875 = 1.051684021949768 + 10.0 * 8.923237800598145
Epoch 1690, val loss: 1.0527654886245728
Epoch 1700, training loss: 90.22999572753906 = 1.0515737533569336 + 10.0 * 8.917841911315918
Epoch 1700, val loss: 1.0526728630065918
Epoch 1710, training loss: 90.30287170410156 = 1.0515482425689697 + 10.0 * 8.925131797790527
Epoch 1710, val loss: 1.0526416301727295
Epoch 1720, training loss: 90.30208587646484 = 1.0514826774597168 + 10.0 * 8.925060272216797
Epoch 1720, val loss: 1.0525975227355957
Epoch 1730, training loss: 90.3399887084961 = 1.0514715909957886 + 10.0 * 8.928852081298828
Epoch 1730, val loss: 1.0525827407836914
Epoch 1740, training loss: 90.33716583251953 = 1.0514026880264282 + 10.0 * 8.928576469421387
Epoch 1740, val loss: 1.0525175333023071
Epoch 1750, training loss: 90.31383514404297 = 1.0513206720352173 + 10.0 * 8.926251411437988
Epoch 1750, val loss: 1.0524369478225708
Epoch 1760, training loss: 90.31360626220703 = 1.0512640476226807 + 10.0 * 8.926234245300293
Epoch 1760, val loss: 1.052371859550476
Epoch 1770, training loss: 90.32127380371094 = 1.0512058734893799 + 10.0 * 8.927006721496582
Epoch 1770, val loss: 1.0523513555526733
Epoch 1780, training loss: 90.35326385498047 = 1.051189661026001 + 10.0 * 8.930207252502441
Epoch 1780, val loss: 1.0523325204849243
Epoch 1790, training loss: 90.392333984375 = 1.0511326789855957 + 10.0 * 8.934120178222656
Epoch 1790, val loss: 1.0522747039794922
Epoch 1800, training loss: 90.35588073730469 = 1.0510419607162476 + 10.0 * 8.9304838180542
Epoch 1800, val loss: 1.052207112312317
Epoch 1810, training loss: 90.40599060058594 = 1.0510281324386597 + 10.0 * 8.93549633026123
Epoch 1810, val loss: 1.0521858930587769
Epoch 1820, training loss: 90.45760345458984 = 1.0510053634643555 + 10.0 * 8.94066047668457
Epoch 1820, val loss: 1.0521786212921143
Epoch 1830, training loss: 90.43966674804688 = 1.0509228706359863 + 10.0 * 8.938874244689941
Epoch 1830, val loss: 1.0521116256713867
Epoch 1840, training loss: 90.4736099243164 = 1.0508911609649658 + 10.0 * 8.942272186279297
Epoch 1840, val loss: 1.052071452140808
Epoch 1850, training loss: 90.47901916503906 = 1.050824761390686 + 10.0 * 8.942819595336914
Epoch 1850, val loss: 1.0520145893096924
Epoch 1860, training loss: 90.43851470947266 = 1.0507328510284424 + 10.0 * 8.938777923583984
Epoch 1860, val loss: 1.0519503355026245
Epoch 1870, training loss: 90.49832916259766 = 1.050704836845398 + 10.0 * 8.944762229919434
Epoch 1870, val loss: 1.051913857460022
Epoch 1880, training loss: 90.51513671875 = 1.0506619215011597 + 10.0 * 8.946447372436523
Epoch 1880, val loss: 1.051878571510315
Epoch 1890, training loss: 90.51142883300781 = 1.0505694150924683 + 10.0 * 8.946085929870605
Epoch 1890, val loss: 1.0517899990081787
Epoch 1900, training loss: 90.36579895019531 = 1.0503458976745605 + 10.0 * 8.93154525756836
Epoch 1900, val loss: 1.0516189336776733
Epoch 1910, training loss: 90.38302612304688 = 1.0503389835357666 + 10.0 * 8.933268547058105
Epoch 1910, val loss: 1.0515968799591064
Epoch 1920, training loss: 90.42910766601562 = 1.05032479763031 + 10.0 * 8.937878608703613
Epoch 1920, val loss: 1.0515460968017578
Epoch 1930, training loss: 90.4579849243164 = 1.050270915031433 + 10.0 * 8.940771102905273
Epoch 1930, val loss: 1.0515235662460327
Epoch 1940, training loss: 90.53203582763672 = 1.0502660274505615 + 10.0 * 8.948177337646484
Epoch 1940, val loss: 1.0515296459197998
Epoch 1950, training loss: 90.54542541503906 = 1.0502220392227173 + 10.0 * 8.949520111083984
Epoch 1950, val loss: 1.0515011548995972
Epoch 1960, training loss: 90.52760314941406 = 1.0501246452331543 + 10.0 * 8.947748184204102
Epoch 1960, val loss: 1.0514073371887207
Epoch 1970, training loss: 90.55262756347656 = 1.050079107284546 + 10.0 * 8.950254440307617
Epoch 1970, val loss: 1.0513572692871094
Epoch 1980, training loss: 90.58735656738281 = 1.0500377416610718 + 10.0 * 8.953731536865234
Epoch 1980, val loss: 1.0513317584991455
Epoch 1990, training loss: 90.5952377319336 = 1.049952507019043 + 10.0 * 8.95452880859375
Epoch 1990, val loss: 1.0512754917144775
Epoch 2000, training loss: 90.60517883300781 = 1.0498627424240112 + 10.0 * 8.955531120300293
Epoch 2000, val loss: 1.0511685609817505
Epoch 2010, training loss: 90.60836791992188 = 1.0498050451278687 + 10.0 * 8.955856323242188
Epoch 2010, val loss: 1.0511354207992554
Epoch 2020, training loss: 90.66214752197266 = 1.0497630834579468 + 10.0 * 8.961237907409668
Epoch 2020, val loss: 1.051111102104187
Epoch 2030, training loss: 90.65734100341797 = 1.0496894121170044 + 10.0 * 8.96076488494873
Epoch 2030, val loss: 1.0510634183883667
Epoch 2040, training loss: 90.66423034667969 = 1.0496329069137573 + 10.0 * 8.96146011352539
Epoch 2040, val loss: 1.0509746074676514
Epoch 2050, training loss: 90.65190124511719 = 1.0495270490646362 + 10.0 * 8.960237503051758
Epoch 2050, val loss: 1.050909399986267
Epoch 2060, training loss: 90.67183685302734 = 1.0494734048843384 + 10.0 * 8.962236404418945
Epoch 2060, val loss: 1.0508490800857544
Epoch 2070, training loss: 90.69921875 = 1.049419641494751 + 10.0 * 8.96497917175293
Epoch 2070, val loss: 1.0508065223693848
Epoch 2080, training loss: 90.71711730957031 = 1.0493463277816772 + 10.0 * 8.966776847839355
Epoch 2080, val loss: 1.0507014989852905
Epoch 2090, training loss: 90.70490264892578 = 1.0492490530014038 + 10.0 * 8.96556568145752
Epoch 2090, val loss: 1.0506951808929443
Epoch 2100, training loss: 90.68579864501953 = 1.0491386651992798 + 10.0 * 8.963665962219238
Epoch 2100, val loss: 1.0505865812301636
Epoch 2110, training loss: 90.66648864746094 = 1.0490437746047974 + 10.0 * 8.96174430847168
Epoch 2110, val loss: 1.0504817962646484
Epoch 2120, training loss: 90.72100830078125 = 1.0490226745605469 + 10.0 * 8.967198371887207
Epoch 2120, val loss: 1.050490379333496
Epoch 2130, training loss: 90.76239776611328 = 1.0490022897720337 + 10.0 * 8.971339225769043
Epoch 2130, val loss: 1.0504698753356934
Epoch 2140, training loss: 90.74873352050781 = 1.0488853454589844 + 10.0 * 8.969985008239746
Epoch 2140, val loss: 1.050372838973999
Epoch 2150, training loss: 90.75425720214844 = 1.0488362312316895 + 10.0 * 8.970541954040527
Epoch 2150, val loss: 1.050329327583313
Epoch 2160, training loss: 90.78903198242188 = 1.0487693548202515 + 10.0 * 8.974026679992676
Epoch 2160, val loss: 1.0502814054489136
Epoch 2170, training loss: 90.76282501220703 = 1.0486668348312378 + 10.0 * 8.971415519714355
Epoch 2170, val loss: 1.0501636266708374
Epoch 2180, training loss: 90.77125549316406 = 1.0486007928848267 + 10.0 * 8.972265243530273
Epoch 2180, val loss: 1.0501261949539185
Epoch 2190, training loss: 90.81827545166016 = 1.0485594272613525 + 10.0 * 8.976971626281738
Epoch 2190, val loss: 1.0500891208648682
Epoch 2200, training loss: 90.79949951171875 = 1.048461675643921 + 10.0 * 8.975103378295898
Epoch 2200, val loss: 1.0500214099884033
Epoch 2210, training loss: 90.8304214477539 = 1.0483943223953247 + 10.0 * 8.978202819824219
Epoch 2210, val loss: 1.049897313117981
Epoch 2220, training loss: 90.83174133300781 = 1.0483007431030273 + 10.0 * 8.978343963623047
Epoch 2220, val loss: 1.0498552322387695
Epoch 2230, training loss: 90.84033966064453 = 1.0482362508773804 + 10.0 * 8.97921085357666
Epoch 2230, val loss: 1.0498167276382446
Epoch 2240, training loss: 90.82893371582031 = 1.0481208562850952 + 10.0 * 8.978081703186035
Epoch 2240, val loss: 1.0497187376022339
Epoch 2250, training loss: 90.79830932617188 = 1.0480115413665771 + 10.0 * 8.975029945373535
Epoch 2250, val loss: 1.0496208667755127
Epoch 2260, training loss: 90.74842834472656 = 1.0478357076644897 + 10.0 * 8.970059394836426
Epoch 2260, val loss: 1.0493897199630737
Epoch 2270, training loss: 90.75923156738281 = 1.0477001667022705 + 10.0 * 8.971153259277344
Epoch 2270, val loss: 1.0492956638336182
Epoch 2280, training loss: 90.72699737548828 = 1.0475534200668335 + 10.0 * 8.967944145202637
Epoch 2280, val loss: 1.0492057800292969
Epoch 2290, training loss: 90.71698760986328 = 1.0474650859832764 + 10.0 * 8.966952323913574
Epoch 2290, val loss: 1.049183964729309
Epoch 2300, training loss: 90.78761291503906 = 1.0475027561187744 + 10.0 * 8.974011421203613
Epoch 2300, val loss: 1.049221158027649
Epoch 2310, training loss: 90.84352111816406 = 1.0474982261657715 + 10.0 * 8.979601860046387
Epoch 2310, val loss: 1.0491981506347656
Epoch 2320, training loss: 90.9044189453125 = 1.0474733114242554 + 10.0 * 8.985694885253906
Epoch 2320, val loss: 1.0491822957992554
Epoch 2330, training loss: 90.8819580078125 = 1.047369122505188 + 10.0 * 8.983458518981934
Epoch 2330, val loss: 1.0490800142288208
Epoch 2340, training loss: 90.91124725341797 = 1.0472949743270874 + 10.0 * 8.986394882202148
Epoch 2340, val loss: 1.049010157585144
Epoch 2350, training loss: 90.9283676147461 = 1.0472280979156494 + 10.0 * 8.988114356994629
Epoch 2350, val loss: 1.0489633083343506
Epoch 2360, training loss: 90.93194580078125 = 1.0471227169036865 + 10.0 * 8.988482475280762
Epoch 2360, val loss: 1.0488693714141846
Epoch 2370, training loss: 90.89885711669922 = 1.0469902753829956 + 10.0 * 8.985186576843262
Epoch 2370, val loss: 1.0487552881240845
Epoch 2380, training loss: 90.9494400024414 = 1.0469403266906738 + 10.0 * 8.990249633789062
Epoch 2380, val loss: 1.0487359762191772
Epoch 2390, training loss: 90.92176055908203 = 1.046824336051941 + 10.0 * 8.987493515014648
Epoch 2390, val loss: 1.0486189126968384
Epoch 2400, training loss: 90.92242431640625 = 1.0467209815979004 + 10.0 * 8.987569808959961
Epoch 2400, val loss: 1.0485020875930786
Epoch 2410, training loss: 90.9454574584961 = 1.0466593503952026 + 10.0 * 8.989879608154297
Epoch 2410, val loss: 1.048475980758667
Epoch 2420, training loss: 90.96719360351562 = 1.0465737581253052 + 10.0 * 8.992061614990234
Epoch 2420, val loss: 1.0484312772750854
Epoch 2430, training loss: 90.96739196777344 = 1.0464816093444824 + 10.0 * 8.992091178894043
Epoch 2430, val loss: 1.048351526260376
Epoch 2440, training loss: 90.97913360595703 = 1.0463778972625732 + 10.0 * 8.99327564239502
Epoch 2440, val loss: 1.0482701063156128
Epoch 2450, training loss: 91.00271606445312 = 1.0463217496871948 + 10.0 * 8.99563980102539
Epoch 2450, val loss: 1.0482369661331177
Epoch 2460, training loss: 90.99466705322266 = 1.0462112426757812 + 10.0 * 8.994845390319824
Epoch 2460, val loss: 1.0480973720550537
Epoch 2470, training loss: 90.9496078491211 = 1.0460065603256226 + 10.0 * 8.990360260009766
Epoch 2470, val loss: 1.047989845275879
Epoch 2480, training loss: 90.97259521484375 = 1.045949101448059 + 10.0 * 8.992664337158203
Epoch 2480, val loss: 1.0479363203048706
Epoch 2490, training loss: 91.03190612792969 = 1.0459080934524536 + 10.0 * 8.998600006103516
Epoch 2490, val loss: 1.0479083061218262
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.4152173913043478
0.8084474389625445
=== training gcn model ===
Epoch 0, training loss: 101.92073059082031 = 1.0959635972976685 + 10.0 * 10.082476615905762
Epoch 0, val loss: 1.0971657037734985
Epoch 10, training loss: 97.1925277709961 = 1.0925160646438599 + 10.0 * 9.610001564025879
Epoch 10, val loss: 1.0937597751617432
Epoch 20, training loss: 95.60883331298828 = 1.0892386436462402 + 10.0 * 9.451959609985352
Epoch 20, val loss: 1.0905089378356934
Epoch 30, training loss: 94.48096466064453 = 1.0861095190048218 + 10.0 * 9.339485168457031
Epoch 30, val loss: 1.0874131917953491
Epoch 40, training loss: 93.5974349975586 = 1.0831542015075684 + 10.0 * 9.251428604125977
Epoch 40, val loss: 1.0844827890396118
Epoch 50, training loss: 92.87138366699219 = 1.0803847312927246 + 10.0 * 9.179100036621094
Epoch 50, val loss: 1.0817314386367798
Epoch 60, training loss: 92.24759674072266 = 1.0777918100357056 + 10.0 * 9.11698055267334
Epoch 60, val loss: 1.0791510343551636
Epoch 70, training loss: 91.70053100585938 = 1.0753626823425293 + 10.0 * 9.062517166137695
Epoch 70, val loss: 1.0767287015914917
Epoch 80, training loss: 91.22435760498047 = 1.0731019973754883 + 10.0 * 9.015125274658203
Epoch 80, val loss: 1.0744714736938477
Epoch 90, training loss: 90.80140686035156 = 1.0710121393203735 + 10.0 * 8.973039627075195
Epoch 90, val loss: 1.0723800659179688
Epoch 100, training loss: 90.42660522460938 = 1.0690864324569702 + 10.0 * 8.935751914978027
Epoch 100, val loss: 1.0704470872879028
Epoch 110, training loss: 90.09555053710938 = 1.067322850227356 + 10.0 * 8.902822494506836
Epoch 110, val loss: 1.0686711072921753
Epoch 120, training loss: 89.81844329833984 = 1.0657103061676025 + 10.0 * 8.875272750854492
Epoch 120, val loss: 1.067046046257019
Epoch 130, training loss: 89.55626678466797 = 1.0642445087432861 + 10.0 * 8.849202156066895
Epoch 130, val loss: 1.0655609369277954
Epoch 140, training loss: 89.35120391845703 = 1.0629252195358276 + 10.0 * 8.828827857971191
Epoch 140, val loss: 1.0642207860946655
Epoch 150, training loss: 89.16419982910156 = 1.061749815940857 + 10.0 * 8.8102445602417
Epoch 150, val loss: 1.0630167722702026
Epoch 160, training loss: 89.0196762084961 = 1.0606884956359863 + 10.0 * 8.7958984375
Epoch 160, val loss: 1.0619362592697144
Epoch 170, training loss: 88.87715911865234 = 1.0597937107086182 + 10.0 * 8.781736373901367
Epoch 170, val loss: 1.0610020160675049
Epoch 180, training loss: 88.7429428100586 = 1.059003472328186 + 10.0 * 8.768393516540527
Epoch 180, val loss: 1.0601818561553955
Epoch 190, training loss: 88.61851501464844 = 1.0583114624023438 + 10.0 * 8.756020545959473
Epoch 190, val loss: 1.0594576597213745
Epoch 200, training loss: 88.52294158935547 = 1.0577291250228882 + 10.0 * 8.74652099609375
Epoch 200, val loss: 1.0588409900665283
Epoch 210, training loss: 88.44387817382812 = 1.0572376251220703 + 10.0 * 8.738664627075195
Epoch 210, val loss: 1.0583149194717407
Epoch 220, training loss: 88.37696075439453 = 1.05683434009552 + 10.0 * 8.732012748718262
Epoch 220, val loss: 1.0578762292861938
Epoch 230, training loss: 88.28605651855469 = 1.056490182876587 + 10.0 * 8.722956657409668
Epoch 230, val loss: 1.0575064420700073
Epoch 240, training loss: 88.22978973388672 = 1.0562076568603516 + 10.0 * 8.717358589172363
Epoch 240, val loss: 1.0571964979171753
Epoch 250, training loss: 88.1845703125 = 1.0559797286987305 + 10.0 * 8.712859153747559
Epoch 250, val loss: 1.0569438934326172
Epoch 260, training loss: 88.14764404296875 = 1.055816411972046 + 10.0 * 8.709182739257812
Epoch 260, val loss: 1.0567384958267212
Epoch 270, training loss: 88.08470153808594 = 1.0556631088256836 + 10.0 * 8.702903747558594
Epoch 270, val loss: 1.0565739870071411
Epoch 280, training loss: 88.0318832397461 = 1.0555511713027954 + 10.0 * 8.697633743286133
Epoch 280, val loss: 1.056442141532898
Epoch 290, training loss: 87.99776458740234 = 1.0554635524749756 + 10.0 * 8.694230079650879
Epoch 290, val loss: 1.0563305616378784
Epoch 300, training loss: 87.96788024902344 = 1.0553910732269287 + 10.0 * 8.691248893737793
Epoch 300, val loss: 1.056248426437378
Epoch 310, training loss: 87.93629455566406 = 1.0553348064422607 + 10.0 * 8.688096046447754
Epoch 310, val loss: 1.0561777353286743
Epoch 320, training loss: 87.9029769897461 = 1.055286169052124 + 10.0 * 8.684769630432129
Epoch 320, val loss: 1.056113362312317
Epoch 330, training loss: 87.91142272949219 = 1.055254578590393 + 10.0 * 8.685617446899414
Epoch 330, val loss: 1.056071162223816
Epoch 340, training loss: 87.90132141113281 = 1.055228352546692 + 10.0 * 8.684609413146973
Epoch 340, val loss: 1.0560367107391357
Epoch 350, training loss: 87.88338470458984 = 1.0551992654800415 + 10.0 * 8.682818412780762
Epoch 350, val loss: 1.0559978485107422
Epoch 360, training loss: 87.8458023071289 = 1.0551565885543823 + 10.0 * 8.679064750671387
Epoch 360, val loss: 1.0559550523757935
Epoch 370, training loss: 87.83586883544922 = 1.05513596534729 + 10.0 * 8.678072929382324
Epoch 370, val loss: 1.0559273958206177
Epoch 380, training loss: 87.8209457397461 = 1.0551161766052246 + 10.0 * 8.676583290100098
Epoch 380, val loss: 1.0559062957763672
Epoch 390, training loss: 87.83956146240234 = 1.0551034212112427 + 10.0 * 8.678445816040039
Epoch 390, val loss: 1.0559169054031372
Epoch 400, training loss: 88.19397735595703 = 1.054405927658081 + 10.0 * 8.713956832885742
Epoch 400, val loss: 1.0551657676696777
Epoch 410, training loss: 88.23155975341797 = 1.0548536777496338 + 10.0 * 8.717670440673828
Epoch 410, val loss: 1.0556929111480713
Epoch 420, training loss: 88.31742095947266 = 1.0549906492233276 + 10.0 * 8.726243019104004
Epoch 420, val loss: 1.055738091468811
Epoch 430, training loss: 88.0323486328125 = 1.0548962354660034 + 10.0 * 8.697745323181152
Epoch 430, val loss: 1.0556652545928955
Epoch 440, training loss: 88.06450653076172 = 1.0549259185791016 + 10.0 * 8.700958251953125
Epoch 440, val loss: 1.0557072162628174
Epoch 450, training loss: 88.07124328613281 = 1.0549143552780151 + 10.0 * 8.701632499694824
Epoch 450, val loss: 1.0556870698928833
Epoch 460, training loss: 88.12271881103516 = 1.0549226999282837 + 10.0 * 8.706779479980469
Epoch 460, val loss: 1.0556975603103638
Epoch 470, training loss: 88.15056610107422 = 1.0549147129058838 + 10.0 * 8.709565162658691
Epoch 470, val loss: 1.055692434310913
Epoch 480, training loss: 88.16986846923828 = 1.0549060106277466 + 10.0 * 8.711496353149414
Epoch 480, val loss: 1.0556838512420654
Epoch 490, training loss: 88.16139221191406 = 1.0548840761184692 + 10.0 * 8.710650444030762
Epoch 490, val loss: 1.0556583404541016
Epoch 500, training loss: 88.20553588867188 = 1.0548771619796753 + 10.0 * 8.715065956115723
Epoch 500, val loss: 1.0556532144546509
Epoch 510, training loss: 88.22355651855469 = 1.0548710823059082 + 10.0 * 8.71686840057373
Epoch 510, val loss: 1.0556470155715942
Epoch 520, training loss: 88.21204376220703 = 1.0548487901687622 + 10.0 * 8.715719223022461
Epoch 520, val loss: 1.0556281805038452
Epoch 530, training loss: 88.24958801269531 = 1.0548427104949951 + 10.0 * 8.719474792480469
Epoch 530, val loss: 1.0556201934814453
Epoch 540, training loss: 88.18522644042969 = 1.0547858476638794 + 10.0 * 8.713044166564941
Epoch 540, val loss: 1.0555726289749146
Epoch 550, training loss: 88.24728393554688 = 1.054789662361145 + 10.0 * 8.719249725341797
Epoch 550, val loss: 1.0555713176727295
Epoch 560, training loss: 88.28148651123047 = 1.0547858476638794 + 10.0 * 8.72266960144043
Epoch 560, val loss: 1.0555628538131714
Epoch 570, training loss: 88.29759979248047 = 1.0547726154327393 + 10.0 * 8.724283218383789
Epoch 570, val loss: 1.0555530786514282
Epoch 580, training loss: 88.33181762695312 = 1.0547617673873901 + 10.0 * 8.727705001831055
Epoch 580, val loss: 1.0555427074432373
Epoch 590, training loss: 88.32379913330078 = 1.054734230041504 + 10.0 * 8.726906776428223
Epoch 590, val loss: 1.055516004562378
Epoch 600, training loss: 88.13893127441406 = 1.0545122623443604 + 10.0 * 8.708441734313965
Epoch 600, val loss: 1.0552866458892822
Epoch 610, training loss: 88.61280059814453 = 1.0547430515289307 + 10.0 * 8.755805969238281
Epoch 610, val loss: 1.0555251836776733
Epoch 620, training loss: 88.3019790649414 = 1.0545519590377808 + 10.0 * 8.724742889404297
Epoch 620, val loss: 1.0553276538848877
Epoch 630, training loss: 88.38199615478516 = 1.0546534061431885 + 10.0 * 8.732733726501465
Epoch 630, val loss: 1.0554392337799072
Epoch 640, training loss: 88.3119125366211 = 1.0546185970306396 + 10.0 * 8.725728988647461
Epoch 640, val loss: 1.0554001331329346
Epoch 650, training loss: 88.39434051513672 = 1.0546354055404663 + 10.0 * 8.733970642089844
Epoch 650, val loss: 1.0554143190383911
Epoch 660, training loss: 88.4559555053711 = 1.0546321868896484 + 10.0 * 8.740132331848145
Epoch 660, val loss: 1.0554152727127075
Epoch 670, training loss: 88.46007537841797 = 1.0546154975891113 + 10.0 * 8.740545272827148
Epoch 670, val loss: 1.055396556854248
Epoch 680, training loss: 88.50100708007812 = 1.054600715637207 + 10.0 * 8.744640350341797
Epoch 680, val loss: 1.05538010597229
Epoch 690, training loss: 88.50115203857422 = 1.0545767545700073 + 10.0 * 8.744657516479492
Epoch 690, val loss: 1.0553652048110962
Epoch 700, training loss: 88.50032806396484 = 1.0545620918273926 + 10.0 * 8.744576454162598
Epoch 700, val loss: 1.0553439855575562
Epoch 710, training loss: 88.52332305908203 = 1.0545434951782227 + 10.0 * 8.746877670288086
Epoch 710, val loss: 1.0553271770477295
Epoch 720, training loss: 88.57577514648438 = 1.0545258522033691 + 10.0 * 8.752124786376953
Epoch 720, val loss: 1.0553135871887207
Epoch 730, training loss: 88.5677719116211 = 1.0545072555541992 + 10.0 * 8.751326560974121
Epoch 730, val loss: 1.0552927255630493
Epoch 740, training loss: 88.61797332763672 = 1.0544990301132202 + 10.0 * 8.75634765625
Epoch 740, val loss: 1.0552806854248047
Epoch 750, training loss: 88.63848876953125 = 1.0544795989990234 + 10.0 * 8.758400917053223
Epoch 750, val loss: 1.0552631616592407
Epoch 760, training loss: 88.66922760009766 = 1.054466962814331 + 10.0 * 8.761476516723633
Epoch 760, val loss: 1.0552517175674438
Epoch 770, training loss: 88.69161224365234 = 1.054451823234558 + 10.0 * 8.763715744018555
Epoch 770, val loss: 1.0552245378494263
Epoch 780, training loss: 88.67202758789062 = 1.054418921470642 + 10.0 * 8.761760711669922
Epoch 780, val loss: 1.0552045106887817
Epoch 790, training loss: 88.70271301269531 = 1.0543992519378662 + 10.0 * 8.76483154296875
Epoch 790, val loss: 1.0551869869232178
Epoch 800, training loss: 88.72957611083984 = 1.0543948411941528 + 10.0 * 8.767518043518066
Epoch 800, val loss: 1.0551800727844238
Epoch 810, training loss: 88.76502227783203 = 1.0543674230575562 + 10.0 * 8.771065711975098
Epoch 810, val loss: 1.0551565885543823
Epoch 820, training loss: 88.7843017578125 = 1.0543478727340698 + 10.0 * 8.772995948791504
Epoch 820, val loss: 1.055144190788269
Epoch 830, training loss: 88.80953216552734 = 1.0543239116668701 + 10.0 * 8.775521278381348
Epoch 830, val loss: 1.0551191568374634
Epoch 840, training loss: 88.7934799194336 = 1.0542986392974854 + 10.0 * 8.773918151855469
Epoch 840, val loss: 1.0550882816314697
Epoch 850, training loss: 88.79800415039062 = 1.0542746782302856 + 10.0 * 8.774373054504395
Epoch 850, val loss: 1.0550731420516968
Epoch 860, training loss: 88.82728576660156 = 1.054264783859253 + 10.0 * 8.777301788330078
Epoch 860, val loss: 1.0550552606582642
Epoch 870, training loss: 88.82807159423828 = 1.0542429685592651 + 10.0 * 8.777382850646973
Epoch 870, val loss: 1.0550329685211182
Epoch 880, training loss: 89.10708618164062 = 1.0542004108428955 + 10.0 * 8.805288314819336
Epoch 880, val loss: 1.0549414157867432
Epoch 890, training loss: 88.4565200805664 = 1.0538617372512817 + 10.0 * 8.740265846252441
Epoch 890, val loss: 1.0547305345535278
Epoch 900, training loss: 88.69281005859375 = 1.054064393043518 + 10.0 * 8.763875007629395
Epoch 900, val loss: 1.05483078956604
Epoch 910, training loss: 88.69419860839844 = 1.05404794216156 + 10.0 * 8.764015197753906
Epoch 910, val loss: 1.0548686981201172
Epoch 920, training loss: 88.64716339111328 = 1.0540201663970947 + 10.0 * 8.759313583374023
Epoch 920, val loss: 1.0548319816589355
Epoch 930, training loss: 88.77930450439453 = 1.0540564060211182 + 10.0 * 8.7725248336792
Epoch 930, val loss: 1.054864525794983
Epoch 940, training loss: 88.8659896850586 = 1.054072618484497 + 10.0 * 8.7811918258667
Epoch 940, val loss: 1.0548774003982544
Epoch 950, training loss: 88.87528228759766 = 1.054047703742981 + 10.0 * 8.782123565673828
Epoch 950, val loss: 1.054861307144165
Epoch 960, training loss: 88.93168640136719 = 1.0540368556976318 + 10.0 * 8.787764549255371
Epoch 960, val loss: 1.0548429489135742
Epoch 970, training loss: 88.94525909423828 = 1.0540152788162231 + 10.0 * 8.789124488830566
Epoch 970, val loss: 1.0548259019851685
Epoch 980, training loss: 88.95292663574219 = 1.0539894104003906 + 10.0 * 8.78989315032959
Epoch 980, val loss: 1.0548008680343628
Epoch 990, training loss: 88.9854507446289 = 1.0539684295654297 + 10.0 * 8.793148040771484
Epoch 990, val loss: 1.054783582687378
Epoch 1000, training loss: 89.03257751464844 = 1.0539517402648926 + 10.0 * 8.79786205291748
Epoch 1000, val loss: 1.0547447204589844
Epoch 1010, training loss: 88.96775817871094 = 1.0538928508758545 + 10.0 * 8.791386604309082
Epoch 1010, val loss: 1.0547150373458862
Epoch 1020, training loss: 89.02946472167969 = 1.0538876056671143 + 10.0 * 8.797557830810547
Epoch 1020, val loss: 1.05470609664917
Epoch 1030, training loss: 89.07823181152344 = 1.0538727045059204 + 10.0 * 8.802435874938965
Epoch 1030, val loss: 1.0546901226043701
Epoch 1040, training loss: 89.09103393554688 = 1.0538438558578491 + 10.0 * 8.803719520568848
Epoch 1040, val loss: 1.0546681880950928
Epoch 1050, training loss: 89.1246337890625 = 1.0538311004638672 + 10.0 * 8.807080268859863
Epoch 1050, val loss: 1.0546538829803467
Epoch 1060, training loss: 89.05467224121094 = 1.0537550449371338 + 10.0 * 8.800091743469238
Epoch 1060, val loss: 1.0545672178268433
Epoch 1070, training loss: 89.1139144897461 = 1.0537580251693726 + 10.0 * 8.806015968322754
Epoch 1070, val loss: 1.054581642150879
Epoch 1080, training loss: 89.1736068725586 = 1.0537654161453247 + 10.0 * 8.811984062194824
Epoch 1080, val loss: 1.0545941591262817
Epoch 1090, training loss: 89.18653106689453 = 1.0537323951721191 + 10.0 * 8.81328010559082
Epoch 1090, val loss: 1.0545763969421387
Epoch 1100, training loss: 89.20142364501953 = 1.0537070035934448 + 10.0 * 8.81477165222168
Epoch 1100, val loss: 1.0545507669448853
Epoch 1110, training loss: 89.20398712158203 = 1.0536694526672363 + 10.0 * 8.815031051635742
Epoch 1110, val loss: 1.0545158386230469
Epoch 1120, training loss: 89.25204467773438 = 1.0536500215530396 + 10.0 * 8.819839477539062
Epoch 1120, val loss: 1.0544930696487427
Epoch 1130, training loss: 89.25499725341797 = 1.0536190271377563 + 10.0 * 8.820137977600098
Epoch 1130, val loss: 1.0544679164886475
Epoch 1140, training loss: 89.26054382324219 = 1.0535871982574463 + 10.0 * 8.820695877075195
Epoch 1140, val loss: 1.0544365644454956
Epoch 1150, training loss: 89.29197692871094 = 1.0535650253295898 + 10.0 * 8.823841094970703
Epoch 1150, val loss: 1.054419755935669
Epoch 1160, training loss: 89.32597351074219 = 1.0535424947738647 + 10.0 * 8.827242851257324
Epoch 1160, val loss: 1.0543997287750244
Epoch 1170, training loss: 89.36124420166016 = 1.0535205602645874 + 10.0 * 8.830772399902344
Epoch 1170, val loss: 1.0543785095214844
Epoch 1180, training loss: 89.33358764648438 = 1.0534809827804565 + 10.0 * 8.828010559082031
Epoch 1180, val loss: 1.054349422454834
Epoch 1190, training loss: 89.36410522460938 = 1.053467035293579 + 10.0 * 8.831064224243164
Epoch 1190, val loss: 1.054337978363037
Epoch 1200, training loss: 89.32024383544922 = 1.0534058809280396 + 10.0 * 8.82668399810791
Epoch 1200, val loss: 1.0542725324630737
Epoch 1210, training loss: 89.36363220214844 = 1.053389310836792 + 10.0 * 8.831024169921875
Epoch 1210, val loss: 1.054266095161438
Epoch 1220, training loss: 89.39553833007812 = 1.0533666610717773 + 10.0 * 8.834217071533203
Epoch 1220, val loss: 1.054237723350525
Epoch 1230, training loss: 89.53333282470703 = 1.053290843963623 + 10.0 * 8.848004341125488
Epoch 1230, val loss: 1.0541298389434814
Epoch 1240, training loss: 89.40850067138672 = 1.0532326698303223 + 10.0 * 8.835527420043945
Epoch 1240, val loss: 1.0541017055511475
Epoch 1250, training loss: 89.30673217773438 = 1.0531625747680664 + 10.0 * 8.825357437133789
Epoch 1250, val loss: 1.0540436506271362
Epoch 1260, training loss: 89.43401336669922 = 1.0531930923461914 + 10.0 * 8.838082313537598
Epoch 1260, val loss: 1.0540910959243774
Epoch 1270, training loss: 89.46340942382812 = 1.0531865358352661 + 10.0 * 8.841022491455078
Epoch 1270, val loss: 1.0540878772735596
Epoch 1280, training loss: 89.54776763916016 = 1.0531994104385376 + 10.0 * 8.849456787109375
Epoch 1280, val loss: 1.0541033744812012
Epoch 1290, training loss: 89.58879089355469 = 1.0531833171844482 + 10.0 * 8.853560447692871
Epoch 1290, val loss: 1.0540895462036133
Epoch 1300, training loss: 89.59689331054688 = 1.053152322769165 + 10.0 * 8.854373931884766
Epoch 1300, val loss: 1.0540589094161987
Epoch 1310, training loss: 89.63877868652344 = 1.053127408027649 + 10.0 * 8.858564376831055
Epoch 1310, val loss: 1.0540322065353394
Epoch 1320, training loss: 89.6601333618164 = 1.053096055984497 + 10.0 * 8.86070442199707
Epoch 1320, val loss: 1.0540125370025635
Epoch 1330, training loss: 89.65396881103516 = 1.0530452728271484 + 10.0 * 8.860092163085938
Epoch 1330, val loss: 1.0539560317993164
Epoch 1340, training loss: 89.69464111328125 = 1.0530383586883545 + 10.0 * 8.864160537719727
Epoch 1340, val loss: 1.0539604425430298
Epoch 1350, training loss: 89.68301391601562 = 1.0529934167861938 + 10.0 * 8.863001823425293
Epoch 1350, val loss: 1.0539109706878662
Epoch 1360, training loss: 89.68624877929688 = 1.052948236465454 + 10.0 * 8.863329887390137
Epoch 1360, val loss: 1.0538870096206665
Epoch 1370, training loss: 89.7448501586914 = 1.0529489517211914 + 10.0 * 8.869190216064453
Epoch 1370, val loss: 1.053891658782959
Epoch 1380, training loss: 89.77597045898438 = 1.0529165267944336 + 10.0 * 8.872304916381836
Epoch 1380, val loss: 1.0538429021835327
Epoch 1390, training loss: 89.77536010742188 = 1.052870512008667 + 10.0 * 8.872248649597168
Epoch 1390, val loss: 1.0538173913955688
Epoch 1400, training loss: 89.7777328491211 = 1.0528355836868286 + 10.0 * 8.872489929199219
Epoch 1400, val loss: 1.0537869930267334
Epoch 1410, training loss: 89.79533386230469 = 1.0528072118759155 + 10.0 * 8.874252319335938
Epoch 1410, val loss: 1.0537712574005127
Epoch 1420, training loss: 89.8260269165039 = 1.052785873413086 + 10.0 * 8.877324104309082
Epoch 1420, val loss: 1.0537432432174683
Epoch 1430, training loss: 89.84101867675781 = 1.0527364015579224 + 10.0 * 8.878828048706055
Epoch 1430, val loss: 1.053713321685791
Epoch 1440, training loss: 89.84384155273438 = 1.0526846647262573 + 10.0 * 8.87911605834961
Epoch 1440, val loss: 1.05364990234375
Epoch 1450, training loss: 89.8192138671875 = 1.0526460409164429 + 10.0 * 8.876657485961914
Epoch 1450, val loss: 1.0536106824874878
Epoch 1460, training loss: 89.83968353271484 = 1.052619218826294 + 10.0 * 8.878705978393555
Epoch 1460, val loss: 1.0535939931869507
Epoch 1470, training loss: 89.89978790283203 = 1.0526046752929688 + 10.0 * 8.88471794128418
Epoch 1470, val loss: 1.0535861253738403
Epoch 1480, training loss: 89.85844421386719 = 1.0525351762771606 + 10.0 * 8.880590438842773
Epoch 1480, val loss: 1.053527593612671
Epoch 1490, training loss: 89.90260314941406 = 1.0525327920913696 + 10.0 * 8.88500690460205
Epoch 1490, val loss: 1.0535246133804321
Epoch 1500, training loss: 89.94344329833984 = 1.052514672279358 + 10.0 * 8.889093399047852
Epoch 1500, val loss: 1.0534952878952026
Epoch 1510, training loss: 89.9336929321289 = 1.052456021308899 + 10.0 * 8.888123512268066
Epoch 1510, val loss: 1.0534576177597046
Epoch 1520, training loss: 89.94932556152344 = 1.0524272918701172 + 10.0 * 8.889689445495605
Epoch 1520, val loss: 1.0534342527389526
Epoch 1530, training loss: 89.97364807128906 = 1.0524003505706787 + 10.0 * 8.892125129699707
Epoch 1530, val loss: 1.0534090995788574
Epoch 1540, training loss: 89.96260070800781 = 1.052334189414978 + 10.0 * 8.891026496887207
Epoch 1540, val loss: 1.053350806236267
Epoch 1550, training loss: 89.98810577392578 = 1.0522996187210083 + 10.0 * 8.893580436706543
Epoch 1550, val loss: 1.0533239841461182
Epoch 1560, training loss: 89.99333190917969 = 1.0522558689117432 + 10.0 * 8.894107818603516
Epoch 1560, val loss: 1.0532946586608887
Epoch 1570, training loss: 90.0243911743164 = 1.0522290468215942 + 10.0 * 8.897215843200684
Epoch 1570, val loss: 1.053261160850525
Epoch 1580, training loss: 90.0382308959961 = 1.0521814823150635 + 10.0 * 8.898604393005371
Epoch 1580, val loss: 1.0532283782958984
Epoch 1590, training loss: 90.05926513671875 = 1.0521448850631714 + 10.0 * 8.900712013244629
Epoch 1590, val loss: 1.0531835556030273
Epoch 1600, training loss: 90.09681701660156 = 1.0521292686462402 + 10.0 * 8.904468536376953
Epoch 1600, val loss: 1.0531681776046753
Epoch 1610, training loss: 90.08295440673828 = 1.0520685911178589 + 10.0 * 8.903088569641113
Epoch 1610, val loss: 1.0531325340270996
Epoch 1620, training loss: 90.08953094482422 = 1.0520069599151611 + 10.0 * 8.903752326965332
Epoch 1620, val loss: 1.0530612468719482
Epoch 1630, training loss: 90.07585144042969 = 1.051962971687317 + 10.0 * 8.902388572692871
Epoch 1630, val loss: 1.0530227422714233
Epoch 1640, training loss: 90.042724609375 = 1.0518667697906494 + 10.0 * 8.899085998535156
Epoch 1640, val loss: 1.0529541969299316
Epoch 1650, training loss: 90.0079116821289 = 1.051807165145874 + 10.0 * 8.895610809326172
Epoch 1650, val loss: 1.052898645401001
Epoch 1660, training loss: 90.01214599609375 = 1.0517659187316895 + 10.0 * 8.896038055419922
Epoch 1660, val loss: 1.052873134613037
Epoch 1670, training loss: 90.0800552368164 = 1.0517674684524536 + 10.0 * 8.90282917022705
Epoch 1670, val loss: 1.0528736114501953
Epoch 1680, training loss: 90.16938781738281 = 1.0517902374267578 + 10.0 * 8.911760330200195
Epoch 1680, val loss: 1.0528987646102905
Epoch 1690, training loss: 90.24361419677734 = 1.0517828464508057 + 10.0 * 8.919183731079102
Epoch 1690, val loss: 1.0529086589813232
Epoch 1700, training loss: 90.22628784179688 = 1.051723837852478 + 10.0 * 8.917455673217773
Epoch 1700, val loss: 1.0528455972671509
Epoch 1710, training loss: 90.22349548339844 = 1.0516672134399414 + 10.0 * 8.917182922363281
Epoch 1710, val loss: 1.052804946899414
Epoch 1720, training loss: 90.25929260253906 = 1.0516363382339478 + 10.0 * 8.92076587677002
Epoch 1720, val loss: 1.0527839660644531
Epoch 1730, training loss: 90.26810455322266 = 1.0515691041946411 + 10.0 * 8.921653747558594
Epoch 1730, val loss: 1.0527323484420776
Epoch 1740, training loss: 90.27304077148438 = 1.0515472888946533 + 10.0 * 8.922149658203125
Epoch 1740, val loss: 1.0526942014694214
Epoch 1750, training loss: 90.31542205810547 = 1.0515191555023193 + 10.0 * 8.926389694213867
Epoch 1750, val loss: 1.0526936054229736
Epoch 1760, training loss: 90.30941772460938 = 1.051439642906189 + 10.0 * 8.925798416137695
Epoch 1760, val loss: 1.0526188611984253
Epoch 1770, training loss: 90.33843994140625 = 1.0514191389083862 + 10.0 * 8.928701400756836
Epoch 1770, val loss: 1.0525957345962524
Epoch 1780, training loss: 90.34435272216797 = 1.051369309425354 + 10.0 * 8.929298400878906
Epoch 1780, val loss: 1.052551507949829
Epoch 1790, training loss: 90.35173034667969 = 1.0513163805007935 + 10.0 * 8.930041313171387
Epoch 1790, val loss: 1.0525095462799072
Epoch 1800, training loss: 90.33268737792969 = 1.0512425899505615 + 10.0 * 8.928144454956055
Epoch 1800, val loss: 1.052415132522583
Epoch 1810, training loss: 90.318115234375 = 1.0511517524719238 + 10.0 * 8.926695823669434
Epoch 1810, val loss: 1.0523732900619507
Epoch 1820, training loss: 90.34930419921875 = 1.0511380434036255 + 10.0 * 8.929816246032715
Epoch 1820, val loss: 1.0523614883422852
Epoch 1830, training loss: 90.39329528808594 = 1.0511090755462646 + 10.0 * 8.934218406677246
Epoch 1830, val loss: 1.052344560623169
Epoch 1840, training loss: 90.36050415039062 = 1.0510395765304565 + 10.0 * 8.930946350097656
Epoch 1840, val loss: 1.0522764921188354
Epoch 1850, training loss: 90.40978240966797 = 1.051024317741394 + 10.0 * 8.93587589263916
Epoch 1850, val loss: 1.0522633790969849
Epoch 1860, training loss: 90.40155029296875 = 1.050937533378601 + 10.0 * 8.93506145477295
Epoch 1860, val loss: 1.0522068738937378
Epoch 1870, training loss: 90.41644287109375 = 1.0508915185928345 + 10.0 * 8.936555862426758
Epoch 1870, val loss: 1.052148699760437
Epoch 1880, training loss: 90.43248748779297 = 1.0508673191070557 + 10.0 * 8.938161849975586
Epoch 1880, val loss: 1.0521290302276611
Epoch 1890, training loss: 90.44242858886719 = 1.0508054494857788 + 10.0 * 8.939162254333496
Epoch 1890, val loss: 1.052056908607483
Epoch 1900, training loss: 90.4560775756836 = 1.0507121086120605 + 10.0 * 8.940536499023438
Epoch 1900, val loss: 1.051986575126648
Epoch 1910, training loss: 90.4440689086914 = 1.0506699085235596 + 10.0 * 8.939340591430664
Epoch 1910, val loss: 1.0519523620605469
Epoch 1920, training loss: 90.48951721191406 = 1.0506422519683838 + 10.0 * 8.943887710571289
Epoch 1920, val loss: 1.051946759223938
Epoch 1930, training loss: 90.4935302734375 = 1.050567865371704 + 10.0 * 8.944295883178711
Epoch 1930, val loss: 1.0518615245819092
Epoch 1940, training loss: 90.48631286621094 = 1.0505070686340332 + 10.0 * 8.943580627441406
Epoch 1940, val loss: 1.051829218864441
Epoch 1950, training loss: 90.531005859375 = 1.0504910945892334 + 10.0 * 8.948051452636719
Epoch 1950, val loss: 1.0518059730529785
Epoch 1960, training loss: 90.5286865234375 = 1.0504131317138672 + 10.0 * 8.947827339172363
Epoch 1960, val loss: 1.051730990409851
Epoch 1970, training loss: 90.55065155029297 = 1.05036461353302 + 10.0 * 8.950029373168945
Epoch 1970, val loss: 1.0517010688781738
Epoch 1980, training loss: 90.54963684082031 = 1.0503133535385132 + 10.0 * 8.949932098388672
Epoch 1980, val loss: 1.051659107208252
Epoch 1990, training loss: 90.53843688964844 = 1.0502357482910156 + 10.0 * 8.948820114135742
Epoch 1990, val loss: 1.0515737533569336
Epoch 2000, training loss: 90.55705261230469 = 1.0501766204833984 + 10.0 * 8.950687408447266
Epoch 2000, val loss: 1.0515385866165161
Epoch 2010, training loss: 90.58798217773438 = 1.0501298904418945 + 10.0 * 8.953784942626953
Epoch 2010, val loss: 1.0515053272247314
Epoch 2020, training loss: 90.61573791503906 = 1.050085425376892 + 10.0 * 8.956564903259277
Epoch 2020, val loss: 1.0514689683914185
Epoch 2030, training loss: 90.59302520751953 = 1.0500235557556152 + 10.0 * 8.954299926757812
Epoch 2030, val loss: 1.0513825416564941
Epoch 2040, training loss: 90.607177734375 = 1.0499389171600342 + 10.0 * 8.955723762512207
Epoch 2040, val loss: 1.0513230562210083
Epoch 2050, training loss: 90.62309265136719 = 1.0498857498168945 + 10.0 * 8.957320213317871
Epoch 2050, val loss: 1.0512908697128296
Epoch 2060, training loss: 90.62791442871094 = 1.0498195886611938 + 10.0 * 8.957809448242188
Epoch 2060, val loss: 1.0512317419052124
Epoch 2070, training loss: 90.5871810913086 = 1.0496939420700073 + 10.0 * 8.95374870300293
Epoch 2070, val loss: 1.0511029958724976
Epoch 2080, training loss: 90.57533264160156 = 1.0496197938919067 + 10.0 * 8.952570915222168
Epoch 2080, val loss: 1.0510666370391846
Epoch 2090, training loss: 90.63621520996094 = 1.0496134757995605 + 10.0 * 8.958660125732422
Epoch 2090, val loss: 1.0510642528533936
Epoch 2100, training loss: 90.69637298583984 = 1.049575924873352 + 10.0 * 8.964679718017578
Epoch 2100, val loss: 1.0510390996932983
Epoch 2110, training loss: 90.65672302246094 = 1.049476981163025 + 10.0 * 8.960724830627441
Epoch 2110, val loss: 1.0509344339370728
Epoch 2120, training loss: 90.69467163085938 = 1.049433708190918 + 10.0 * 8.964524269104004
Epoch 2120, val loss: 1.0509032011032104
Epoch 2130, training loss: 90.73848724365234 = 1.0493873357772827 + 10.0 * 8.968910217285156
Epoch 2130, val loss: 1.050842523574829
Epoch 2140, training loss: 90.6801986694336 = 1.0492770671844482 + 10.0 * 8.963091850280762
Epoch 2140, val loss: 1.0507872104644775
Epoch 2150, training loss: 90.72017669677734 = 1.0492404699325562 + 10.0 * 8.967093467712402
Epoch 2150, val loss: 1.0507500171661377
Epoch 2160, training loss: 90.74641418457031 = 1.0491608381271362 + 10.0 * 8.969724655151367
Epoch 2160, val loss: 1.0506742000579834
Epoch 2170, training loss: 90.6911392211914 = 1.049045205116272 + 10.0 * 8.96420955657959
Epoch 2170, val loss: 1.0505540370941162
Epoch 2180, training loss: 90.67423248291016 = 1.048945665359497 + 10.0 * 8.962529182434082
Epoch 2180, val loss: 1.0505053997039795
Epoch 2190, training loss: 90.70780944824219 = 1.0489203929901123 + 10.0 * 8.965888977050781
Epoch 2190, val loss: 1.0504682064056396
Epoch 2200, training loss: 90.75343322753906 = 1.048903226852417 + 10.0 * 8.970453262329102
Epoch 2200, val loss: 1.0504616498947144
Epoch 2210, training loss: 90.79315185546875 = 1.0488495826721191 + 10.0 * 8.974430084228516
Epoch 2210, val loss: 1.0504335165023804
Epoch 2220, training loss: 90.75624084472656 = 1.0487405061721802 + 10.0 * 8.970749855041504
Epoch 2220, val loss: 1.0503129959106445
Epoch 2230, training loss: 90.77458953857422 = 1.0486838817596436 + 10.0 * 8.972590446472168
Epoch 2230, val loss: 1.0502713918685913
Epoch 2240, training loss: 90.81050109863281 = 1.0486565828323364 + 10.0 * 8.976183891296387
Epoch 2240, val loss: 1.0502537488937378
Epoch 2250, training loss: 90.80998229980469 = 1.048546552658081 + 10.0 * 8.976143836975098
Epoch 2250, val loss: 1.0501532554626465
Epoch 2260, training loss: 90.79430389404297 = 1.0484552383422852 + 10.0 * 8.974584579467773
Epoch 2260, val loss: 1.050060749053955
Epoch 2270, training loss: 90.81363677978516 = 1.0484062433242798 + 10.0 * 8.976522445678711
Epoch 2270, val loss: 1.050042748451233
Epoch 2280, training loss: 90.84495544433594 = 1.0483366250991821 + 10.0 * 8.97966194152832
Epoch 2280, val loss: 1.0499752759933472
Epoch 2290, training loss: 90.84257507324219 = 1.0482585430145264 + 10.0 * 8.979432106018066
Epoch 2290, val loss: 1.0499104261398315
Epoch 2300, training loss: 90.73222351074219 = 1.0479720830917358 + 10.0 * 8.968424797058105
Epoch 2300, val loss: 1.0495537519454956
Epoch 2310, training loss: 90.67279815673828 = 1.04784095287323 + 10.0 * 8.962495803833008
Epoch 2310, val loss: 1.0495071411132812
Epoch 2320, training loss: 90.74526977539062 = 1.0478520393371582 + 10.0 * 8.969741821289062
Epoch 2320, val loss: 1.0495257377624512
Epoch 2330, training loss: 90.68306732177734 = 1.0477162599563599 + 10.0 * 8.96353530883789
Epoch 2330, val loss: 1.049446940422058
Epoch 2340, training loss: 90.75350952148438 = 1.0476512908935547 + 10.0 * 8.970585823059082
Epoch 2340, val loss: 1.0493700504302979
Epoch 2350, training loss: 90.7739486694336 = 1.0476444959640503 + 10.0 * 8.972630500793457
Epoch 2350, val loss: 1.0493727922439575
Epoch 2360, training loss: 90.84676361083984 = 1.0476430654525757 + 10.0 * 8.979911804199219
Epoch 2360, val loss: 1.0493780374526978
Epoch 2370, training loss: 90.91033935546875 = 1.0476242303848267 + 10.0 * 8.986271858215332
Epoch 2370, val loss: 1.049360990524292
Epoch 2380, training loss: 90.85741424560547 = 1.047460913658142 + 10.0 * 8.980995178222656
Epoch 2380, val loss: 1.049214482307434
Epoch 2390, training loss: 90.89043426513672 = 1.0474016666412354 + 10.0 * 8.98430347442627
Epoch 2390, val loss: 1.0491783618927002
Epoch 2400, training loss: 90.94034576416016 = 1.0473688840866089 + 10.0 * 8.989297866821289
Epoch 2400, val loss: 1.0491615533828735
Epoch 2410, training loss: 90.97657775878906 = 1.0473109483718872 + 10.0 * 8.992926597595215
Epoch 2410, val loss: 1.0490920543670654
Epoch 2420, training loss: 90.84425354003906 = 1.0470879077911377 + 10.0 * 8.979716300964355
Epoch 2420, val loss: 1.0489091873168945
Epoch 2430, training loss: 90.85852813720703 = 1.0469611883163452 + 10.0 * 8.981157302856445
Epoch 2430, val loss: 1.0487946271896362
Epoch 2440, training loss: 90.89803314208984 = 1.0469194650650024 + 10.0 * 8.985111236572266
Epoch 2440, val loss: 1.048799991607666
Epoch 2450, training loss: 90.95674896240234 = 1.0469337701797485 + 10.0 * 8.990981101989746
Epoch 2450, val loss: 1.0487958192825317
Epoch 2460, training loss: 90.99180603027344 = 1.0468628406524658 + 10.0 * 8.994494438171387
Epoch 2460, val loss: 1.0487401485443115
Epoch 2470, training loss: 90.95481872558594 = 1.0467442274093628 + 10.0 * 8.99080753326416
Epoch 2470, val loss: 1.0486162900924683
Epoch 2480, training loss: 90.98055267333984 = 1.0466643571853638 + 10.0 * 8.993389129638672
Epoch 2480, val loss: 1.0485748052597046
Epoch 2490, training loss: 91.02462768554688 = 1.0466089248657227 + 10.0 * 8.997801780700684
Epoch 2490, val loss: 1.0485389232635498
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.4082608695652174
0.8104759834818518
=== training gcn model ===
Epoch 0, training loss: 102.46652221679688 = 1.0875978469848633 + 10.0 * 10.13789176940918
Epoch 0, val loss: 1.089185357093811
Epoch 10, training loss: 97.23193359375 = 1.0848053693771362 + 10.0 * 9.614712715148926
Epoch 10, val loss: 1.0864200592041016
Epoch 20, training loss: 95.4946517944336 = 1.0822744369506836 + 10.0 * 9.441237449645996
Epoch 20, val loss: 1.0838673114776611
Epoch 30, training loss: 94.3406982421875 = 1.079833745956421 + 10.0 * 9.326086044311523
Epoch 30, val loss: 1.081421136856079
Epoch 40, training loss: 93.4522933959961 = 1.0775657892227173 + 10.0 * 9.237472534179688
Epoch 40, val loss: 1.0791497230529785
Epoch 50, training loss: 92.74763488769531 = 1.0754413604736328 + 10.0 * 9.167219161987305
Epoch 50, val loss: 1.0770111083984375
Epoch 60, training loss: 92.14017486572266 = 1.0734444856643677 + 10.0 * 9.106672286987305
Epoch 60, val loss: 1.074997901916504
Epoch 70, training loss: 91.61129760742188 = 1.071567177772522 + 10.0 * 9.053973197937012
Epoch 70, val loss: 1.0731009244918823
Epoch 80, training loss: 91.15884399414062 = 1.0698132514953613 + 10.0 * 9.008902549743652
Epoch 80, val loss: 1.0713237524032593
Epoch 90, training loss: 90.76321411132812 = 1.0681747198104858 + 10.0 * 8.969503402709961
Epoch 90, val loss: 1.069661021232605
Epoch 100, training loss: 90.40731811523438 = 1.0666519403457642 + 10.0 * 8.934066772460938
Epoch 100, val loss: 1.0681134462356567
Epoch 110, training loss: 90.0947494506836 = 1.065242886543274 + 10.0 * 8.902950286865234
Epoch 110, val loss: 1.0666779279708862
Epoch 120, training loss: 89.82342529296875 = 1.063952088356018 + 10.0 * 8.875947952270508
Epoch 120, val loss: 1.0653570890426636
Epoch 130, training loss: 89.57868957519531 = 1.062767744064331 + 10.0 * 8.851592063903809
Epoch 130, val loss: 1.0641429424285889
Epoch 140, training loss: 89.38223266601562 = 1.0617045164108276 + 10.0 * 8.832052230834961
Epoch 140, val loss: 1.0630425214767456
Epoch 150, training loss: 89.20542907714844 = 1.060738444328308 + 10.0 * 8.814469337463379
Epoch 150, val loss: 1.0620428323745728
Epoch 160, training loss: 89.04486083984375 = 1.0598899126052856 + 10.0 * 8.798497200012207
Epoch 160, val loss: 1.0611522197723389
Epoch 170, training loss: 88.89244842529297 = 1.0591342449188232 + 10.0 * 8.783330917358398
Epoch 170, val loss: 1.0603578090667725
Epoch 180, training loss: 88.75886535644531 = 1.0584734678268433 + 10.0 * 8.770039558410645
Epoch 180, val loss: 1.059655785560608
Epoch 190, training loss: 88.65068054199219 = 1.0579030513763428 + 10.0 * 8.759278297424316
Epoch 190, val loss: 1.0590436458587646
Epoch 200, training loss: 88.53919982910156 = 1.0574263334274292 + 10.0 * 8.748177528381348
Epoch 200, val loss: 1.0585273504257202
Epoch 210, training loss: 88.45729064941406 = 1.0570242404937744 + 10.0 * 8.740026473999023
Epoch 210, val loss: 1.0580801963806152
Epoch 220, training loss: 88.38236999511719 = 1.0566906929016113 + 10.0 * 8.73256778717041
Epoch 220, val loss: 1.0577067136764526
Epoch 230, training loss: 88.29132843017578 = 1.0564138889312744 + 10.0 * 8.723491668701172
Epoch 230, val loss: 1.0573952198028564
Epoch 240, training loss: 88.23339080810547 = 1.0561864376068115 + 10.0 * 8.717720985412598
Epoch 240, val loss: 1.0571348667144775
Epoch 250, training loss: 88.19203186035156 = 1.05599844455719 + 10.0 * 8.713603019714355
Epoch 250, val loss: 1.056916356086731
Epoch 260, training loss: 88.11248779296875 = 1.055842399597168 + 10.0 * 8.70566463470459
Epoch 260, val loss: 1.056731939315796
Epoch 270, training loss: 88.07572937011719 = 1.0557266473770142 + 10.0 * 8.702000617980957
Epoch 270, val loss: 1.0565950870513916
Epoch 280, training loss: 88.04959869384766 = 1.055643081665039 + 10.0 * 8.699396133422852
Epoch 280, val loss: 1.0564788579940796
Epoch 290, training loss: 88.03978729248047 = 1.0555769205093384 + 10.0 * 8.698420524597168
Epoch 290, val loss: 1.0563908815383911
Epoch 300, training loss: 88.03734588623047 = 1.055497169494629 + 10.0 * 8.698184967041016
Epoch 300, val loss: 1.0563008785247803
Epoch 310, training loss: 87.9673080444336 = 1.0554453134536743 + 10.0 * 8.691186904907227
Epoch 310, val loss: 1.0562396049499512
Epoch 320, training loss: 87.94568634033203 = 1.055413842201233 + 10.0 * 8.689027786254883
Epoch 320, val loss: 1.0561884641647339
Epoch 330, training loss: 87.93328094482422 = 1.0553810596466064 + 10.0 * 8.687789916992188
Epoch 330, val loss: 1.0561463832855225
Epoch 340, training loss: 87.94168090820312 = 1.0553545951843262 + 10.0 * 8.68863296508789
Epoch 340, val loss: 1.0561093091964722
Epoch 350, training loss: 87.92937469482422 = 1.055328130722046 + 10.0 * 8.68740463256836
Epoch 350, val loss: 1.0560592412948608
Epoch 360, training loss: 87.91793823242188 = 1.0552926063537598 + 10.0 * 8.686264991760254
Epoch 360, val loss: 1.0560327768325806
Epoch 370, training loss: 87.90027618408203 = 1.0552732944488525 + 10.0 * 8.684499740600586
Epoch 370, val loss: 1.056004285812378
Epoch 380, training loss: 87.88595581054688 = 1.0552549362182617 + 10.0 * 8.683070182800293
Epoch 380, val loss: 1.0559839010238647
Epoch 390, training loss: 87.87179565429688 = 1.055224895477295 + 10.0 * 8.681657791137695
Epoch 390, val loss: 1.0559529066085815
Epoch 400, training loss: 87.8790283203125 = 1.0552070140838623 + 10.0 * 8.682382583618164
Epoch 400, val loss: 1.0559295415878296
Epoch 410, training loss: 87.8492431640625 = 1.0551867485046387 + 10.0 * 8.67940616607666
Epoch 410, val loss: 1.0559053421020508
Epoch 420, training loss: 87.84896850585938 = 1.0551618337631226 + 10.0 * 8.679380416870117
Epoch 420, val loss: 1.0558816194534302
Epoch 430, training loss: 87.87146759033203 = 1.0551527738571167 + 10.0 * 8.681631088256836
Epoch 430, val loss: 1.0558708906173706
Epoch 440, training loss: 87.83472442626953 = 1.055121898651123 + 10.0 * 8.677960395812988
Epoch 440, val loss: 1.0558420419692993
Epoch 450, training loss: 87.82787322998047 = 1.0551010370254517 + 10.0 * 8.677277565002441
Epoch 450, val loss: 1.0558165311813354
Epoch 460, training loss: 87.84184265136719 = 1.0550843477249146 + 10.0 * 8.678675651550293
Epoch 460, val loss: 1.0557923316955566
Epoch 470, training loss: 87.87330627441406 = 1.0550673007965088 + 10.0 * 8.68182373046875
Epoch 470, val loss: 1.0557708740234375
Epoch 480, training loss: 87.8629379272461 = 1.0550408363342285 + 10.0 * 8.680789947509766
Epoch 480, val loss: 1.055745244026184
Epoch 490, training loss: 87.8589859008789 = 1.0549867153167725 + 10.0 * 8.680399894714355
Epoch 490, val loss: 1.0556834936141968
Epoch 500, training loss: 87.90717315673828 = 1.055017352104187 + 10.0 * 8.685215950012207
Epoch 500, val loss: 1.0557136535644531
Epoch 510, training loss: 87.89993286132812 = 1.0549863576889038 + 10.0 * 8.684494972229004
Epoch 510, val loss: 1.0556941032409668
Epoch 520, training loss: 87.90203094482422 = 1.0549726486206055 + 10.0 * 8.68470573425293
Epoch 520, val loss: 1.0556800365447998
Epoch 530, training loss: 87.90153503417969 = 1.0549513101577759 + 10.0 * 8.68465805053711
Epoch 530, val loss: 1.055656909942627
Epoch 540, training loss: 87.91600036621094 = 1.054929256439209 + 10.0 * 8.68610668182373
Epoch 540, val loss: 1.0556375980377197
Epoch 550, training loss: 87.93903350830078 = 1.0549074411392212 + 10.0 * 8.6884126663208
Epoch 550, val loss: 1.0556178092956543
Epoch 560, training loss: 87.95153045654297 = 1.0548961162567139 + 10.0 * 8.689663887023926
Epoch 560, val loss: 1.055601954460144
Epoch 570, training loss: 87.95413208007812 = 1.0548726320266724 + 10.0 * 8.689926147460938
Epoch 570, val loss: 1.0555872917175293
Epoch 580, training loss: 87.97463989257812 = 1.0548450946807861 + 10.0 * 8.69197940826416
Epoch 580, val loss: 1.0555511713027954
Epoch 590, training loss: 88.00408172607422 = 1.0548341274261475 + 10.0 * 8.694925308227539
Epoch 590, val loss: 1.0555344820022583
Epoch 600, training loss: 87.97036743164062 = 1.054808259010315 + 10.0 * 8.691555976867676
Epoch 600, val loss: 1.0555148124694824
Epoch 610, training loss: 88.01834106445312 = 1.0548015832901 + 10.0 * 8.696353912353516
Epoch 610, val loss: 1.0555088520050049
Epoch 620, training loss: 87.99662017822266 = 1.054772973060608 + 10.0 * 8.694185256958008
Epoch 620, val loss: 1.0554859638214111
Epoch 630, training loss: 88.03353118896484 = 1.0547618865966797 + 10.0 * 8.697876930236816
Epoch 630, val loss: 1.0554624795913696
Epoch 640, training loss: 87.92901611328125 = 1.0545889139175415 + 10.0 * 8.687442779541016
Epoch 640, val loss: 1.0552728176116943
Epoch 650, training loss: 88.15647888183594 = 1.0547329187393188 + 10.0 * 8.710174560546875
Epoch 650, val loss: 1.0554251670837402
Epoch 660, training loss: 87.99132537841797 = 1.0546478033065796 + 10.0 * 8.6936674118042
Epoch 660, val loss: 1.055364966392517
Epoch 670, training loss: 87.98136138916016 = 1.0546438694000244 + 10.0 * 8.692671775817871
Epoch 670, val loss: 1.0553414821624756
Epoch 680, training loss: 87.98188781738281 = 1.0546205043792725 + 10.0 * 8.692727088928223
Epoch 680, val loss: 1.055330514907837
Epoch 690, training loss: 88.03375244140625 = 1.0546226501464844 + 10.0 * 8.69791316986084
Epoch 690, val loss: 1.055334448814392
Epoch 700, training loss: 88.06077575683594 = 1.0546127557754517 + 10.0 * 8.700616836547852
Epoch 700, val loss: 1.0553233623504639
Epoch 710, training loss: 88.08683013916016 = 1.0546005964279175 + 10.0 * 8.703222274780273
Epoch 710, val loss: 1.0553120374679565
Epoch 720, training loss: 88.08171844482422 = 1.0545765161514282 + 10.0 * 8.702714920043945
Epoch 720, val loss: 1.0552929639816284
Epoch 730, training loss: 88.12771606445312 = 1.054553747177124 + 10.0 * 8.707316398620605
Epoch 730, val loss: 1.0552668571472168
Epoch 740, training loss: 88.14521026611328 = 1.0545425415039062 + 10.0 * 8.709066390991211
Epoch 740, val loss: 1.0552558898925781
Epoch 750, training loss: 88.1636734008789 = 1.0545217990875244 + 10.0 * 8.710915565490723
Epoch 750, val loss: 1.0552287101745605
Epoch 760, training loss: 88.17498779296875 = 1.054490566253662 + 10.0 * 8.71204948425293
Epoch 760, val loss: 1.0552064180374146
Epoch 770, training loss: 88.19001770019531 = 1.054471492767334 + 10.0 * 8.713554382324219
Epoch 770, val loss: 1.055187702178955
Epoch 780, training loss: 88.21731567382812 = 1.0544557571411133 + 10.0 * 8.716285705566406
Epoch 780, val loss: 1.0551695823669434
Epoch 790, training loss: 88.17950439453125 = 1.0544097423553467 + 10.0 * 8.712509155273438
Epoch 790, val loss: 1.055118441581726
Epoch 800, training loss: 88.27655792236328 = 1.0543895959854126 + 10.0 * 8.722216606140137
Epoch 800, val loss: 1.0550978183746338
Epoch 810, training loss: 88.18114471435547 = 1.0543447732925415 + 10.0 * 8.712679862976074
Epoch 810, val loss: 1.0550671815872192
Epoch 820, training loss: 88.19009399414062 = 1.054334282875061 + 10.0 * 8.71357536315918
Epoch 820, val loss: 1.0550512075424194
Epoch 830, training loss: 88.22804260253906 = 1.0543146133422852 + 10.0 * 8.71737289428711
Epoch 830, val loss: 1.0550287961959839
Epoch 840, training loss: 88.24042510986328 = 1.0543168783187866 + 10.0 * 8.718610763549805
Epoch 840, val loss: 1.0550349950790405
Epoch 850, training loss: 88.30671691894531 = 1.0543075799942017 + 10.0 * 8.725240707397461
Epoch 850, val loss: 1.0550278425216675
Epoch 860, training loss: 88.34883880615234 = 1.0542891025543213 + 10.0 * 8.72945499420166
Epoch 860, val loss: 1.0550063848495483
Epoch 870, training loss: 88.34431457519531 = 1.054258108139038 + 10.0 * 8.729005813598633
Epoch 870, val loss: 1.054971694946289
Epoch 880, training loss: 88.3583984375 = 1.0542399883270264 + 10.0 * 8.730416297912598
Epoch 880, val loss: 1.0549578666687012
Epoch 890, training loss: 88.33811950683594 = 1.0541990995407104 + 10.0 * 8.728391647338867
Epoch 890, val loss: 1.0549218654632568
Epoch 900, training loss: 88.37652587890625 = 1.0541746616363525 + 10.0 * 8.732234954833984
Epoch 900, val loss: 1.0549075603485107
Epoch 910, training loss: 88.37837219238281 = 1.0541667938232422 + 10.0 * 8.732419967651367
Epoch 910, val loss: 1.054894208908081
Epoch 920, training loss: 88.38291931152344 = 1.054141879081726 + 10.0 * 8.732877731323242
Epoch 920, val loss: 1.0548745393753052
Epoch 930, training loss: 88.40656280517578 = 1.054109811782837 + 10.0 * 8.735245704650879
Epoch 930, val loss: 1.0548491477966309
Epoch 940, training loss: 88.3968734741211 = 1.0540677309036255 + 10.0 * 8.734280586242676
Epoch 940, val loss: 1.054805874824524
Epoch 950, training loss: 88.44786071777344 = 1.0540636777877808 + 10.0 * 8.7393798828125
Epoch 950, val loss: 1.0547997951507568
Epoch 960, training loss: 88.45597076416016 = 1.0540387630462646 + 10.0 * 8.740193367004395
Epoch 960, val loss: 1.0547815561294556
Epoch 970, training loss: 88.45269775390625 = 1.0540064573287964 + 10.0 * 8.739869117736816
Epoch 970, val loss: 1.0547456741333008
Epoch 980, training loss: 88.51105499267578 = 1.0540012121200562 + 10.0 * 8.745705604553223
Epoch 980, val loss: 1.0547362565994263
Epoch 990, training loss: 88.49945068359375 = 1.0539735555648804 + 10.0 * 8.744547843933105
Epoch 990, val loss: 1.0547069311141968
Epoch 1000, training loss: 88.5038070678711 = 1.0539394617080688 + 10.0 * 8.744986534118652
Epoch 1000, val loss: 1.0546817779541016
Epoch 1010, training loss: 88.53445434570312 = 1.0539195537567139 + 10.0 * 8.748053550720215
Epoch 1010, val loss: 1.0546512603759766
Epoch 1020, training loss: 88.47652435302734 = 1.0538102388381958 + 10.0 * 8.742271423339844
Epoch 1020, val loss: 1.0545936822891235
Epoch 1030, training loss: 88.45915985107422 = 1.053773045539856 + 10.0 * 8.740538597106934
Epoch 1030, val loss: 1.054526686668396
Epoch 1040, training loss: 88.42206573486328 = 1.0537463426589966 + 10.0 * 8.736831665039062
Epoch 1040, val loss: 1.0545014142990112
Epoch 1050, training loss: 88.49127197265625 = 1.053763747215271 + 10.0 * 8.74375057220459
Epoch 1050, val loss: 1.0545192956924438
Epoch 1060, training loss: 88.56362915039062 = 1.0537761449813843 + 10.0 * 8.750985145568848
Epoch 1060, val loss: 1.0545289516448975
Epoch 1070, training loss: 88.60359191894531 = 1.0537663698196411 + 10.0 * 8.754982948303223
Epoch 1070, val loss: 1.0545108318328857
Epoch 1080, training loss: 88.55616760253906 = 1.0537129640579224 + 10.0 * 8.750246047973633
Epoch 1080, val loss: 1.054470419883728
Epoch 1090, training loss: 88.60553741455078 = 1.0536941289901733 + 10.0 * 8.755184173583984
Epoch 1090, val loss: 1.0544519424438477
Epoch 1100, training loss: 88.65576171875 = 1.0536861419677734 + 10.0 * 8.760207176208496
Epoch 1100, val loss: 1.0544426441192627
Epoch 1110, training loss: 88.64331817626953 = 1.0536431074142456 + 10.0 * 8.758967399597168
Epoch 1110, val loss: 1.0544036626815796
Epoch 1120, training loss: 88.67530822753906 = 1.053617238998413 + 10.0 * 8.762168884277344
Epoch 1120, val loss: 1.0543707609176636
Epoch 1130, training loss: 88.67540740966797 = 1.0535832643508911 + 10.0 * 8.762182235717773
Epoch 1130, val loss: 1.054347038269043
Epoch 1140, training loss: 88.70158386230469 = 1.0535590648651123 + 10.0 * 8.764802932739258
Epoch 1140, val loss: 1.0543184280395508
Epoch 1150, training loss: 88.7054672241211 = 1.0535268783569336 + 10.0 * 8.765193939208984
Epoch 1150, val loss: 1.05428946018219
Epoch 1160, training loss: 88.75591278076172 = 1.053516149520874 + 10.0 * 8.77023983001709
Epoch 1160, val loss: 1.0542783737182617
Epoch 1170, training loss: 88.75675201416016 = 1.0534634590148926 + 10.0 * 8.770328521728516
Epoch 1170, val loss: 1.054234504699707
Epoch 1180, training loss: 88.77340698242188 = 1.0534394979476929 + 10.0 * 8.771997451782227
Epoch 1180, val loss: 1.0542162656784058
Epoch 1190, training loss: 88.81349182128906 = 1.0534168481826782 + 10.0 * 8.776007652282715
Epoch 1190, val loss: 1.0541988611221313
Epoch 1200, training loss: 88.85201263427734 = 1.053398847579956 + 10.0 * 8.779861450195312
Epoch 1200, val loss: 1.0541646480560303
Epoch 1210, training loss: 88.81200408935547 = 1.0533360242843628 + 10.0 * 8.775866508483887
Epoch 1210, val loss: 1.0540926456451416
Epoch 1220, training loss: 88.7773208618164 = 1.05327308177948 + 10.0 * 8.772404670715332
Epoch 1220, val loss: 1.0540425777435303
Epoch 1230, training loss: 88.82645416259766 = 1.0532479286193848 + 10.0 * 8.777320861816406
Epoch 1230, val loss: 1.0540415048599243
Epoch 1240, training loss: 88.87794494628906 = 1.0532513856887817 + 10.0 * 8.782468795776367
Epoch 1240, val loss: 1.0540428161621094
Epoch 1250, training loss: 88.9182357788086 = 1.0532399415969849 + 10.0 * 8.786499977111816
Epoch 1250, val loss: 1.0540249347686768
Epoch 1260, training loss: 88.93360900878906 = 1.0532033443450928 + 10.0 * 8.788041114807129
Epoch 1260, val loss: 1.0539871454238892
Epoch 1270, training loss: 88.93502044677734 = 1.053167700767517 + 10.0 * 8.788185119628906
Epoch 1270, val loss: 1.053958535194397
Epoch 1280, training loss: 88.95431518554688 = 1.0531232357025146 + 10.0 * 8.790119171142578
Epoch 1280, val loss: 1.0539164543151855
Epoch 1290, training loss: 88.98541259765625 = 1.0530996322631836 + 10.0 * 8.793231010437012
Epoch 1290, val loss: 1.0538963079452515
Epoch 1300, training loss: 88.98809814453125 = 1.0530558824539185 + 10.0 * 8.79350471496582
Epoch 1300, val loss: 1.0538418292999268
Epoch 1310, training loss: 88.97511291503906 = 1.052977204322815 + 10.0 * 8.792213439941406
Epoch 1310, val loss: 1.0537853240966797
Epoch 1320, training loss: 88.97240447998047 = 1.0529711246490479 + 10.0 * 8.791943550109863
Epoch 1320, val loss: 1.0537673234939575
Epoch 1330, training loss: 89.041748046875 = 1.0529494285583496 + 10.0 * 8.798879623413086
Epoch 1330, val loss: 1.053758144378662
Epoch 1340, training loss: 89.00736236572266 = 1.0528881549835205 + 10.0 * 8.79544734954834
Epoch 1340, val loss: 1.053713321685791
Epoch 1350, training loss: 89.04823303222656 = 1.0528663396835327 + 10.0 * 8.79953670501709
Epoch 1350, val loss: 1.0536879301071167
Epoch 1360, training loss: 89.11991119384766 = 1.052856206893921 + 10.0 * 8.806705474853516
Epoch 1360, val loss: 1.0536741018295288
Epoch 1370, training loss: 89.13610076904297 = 1.0528192520141602 + 10.0 * 8.808328628540039
Epoch 1370, val loss: 1.0536330938339233
Epoch 1380, training loss: 89.08917236328125 = 1.052740454673767 + 10.0 * 8.803643226623535
Epoch 1380, val loss: 1.053571105003357
Epoch 1390, training loss: 89.0766830444336 = 1.052714228630066 + 10.0 * 8.802396774291992
Epoch 1390, val loss: 1.0535396337509155
Epoch 1400, training loss: 89.112060546875 = 1.0526623725891113 + 10.0 * 8.805939674377441
Epoch 1400, val loss: 1.0534917116165161
Epoch 1410, training loss: 89.14906311035156 = 1.052646279335022 + 10.0 * 8.80964183807373
Epoch 1410, val loss: 1.0534954071044922
Epoch 1420, training loss: 89.15331268310547 = 1.0526036024093628 + 10.0 * 8.810070991516113
Epoch 1420, val loss: 1.0534310340881348
Epoch 1430, training loss: 89.13477325439453 = 1.0525609254837036 + 10.0 * 8.808221817016602
Epoch 1430, val loss: 1.0533816814422607
Epoch 1440, training loss: 89.1415023803711 = 1.05251145362854 + 10.0 * 8.80889892578125
Epoch 1440, val loss: 1.0533353090286255
Epoch 1450, training loss: 89.17922973632812 = 1.0524905920028687 + 10.0 * 8.812673568725586
Epoch 1450, val loss: 1.0533267259597778
Epoch 1460, training loss: 89.1528549194336 = 1.0524306297302246 + 10.0 * 8.810042381286621
Epoch 1460, val loss: 1.0532640218734741
Epoch 1470, training loss: 89.20547485351562 = 1.0523918867111206 + 10.0 * 8.815308570861816
Epoch 1470, val loss: 1.0532450675964355
Epoch 1480, training loss: 89.240478515625 = 1.052390456199646 + 10.0 * 8.818808555603027
Epoch 1480, val loss: 1.0532420873641968
Epoch 1490, training loss: 89.2660140991211 = 1.0523556470870972 + 10.0 * 8.821366310119629
Epoch 1490, val loss: 1.0531930923461914
Epoch 1500, training loss: 89.2590103149414 = 1.0523121356964111 + 10.0 * 8.820669174194336
Epoch 1500, val loss: 1.0531606674194336
Epoch 1510, training loss: 89.31098175048828 = 1.0522874593734741 + 10.0 * 8.8258695602417
Epoch 1510, val loss: 1.0531455278396606
Epoch 1520, training loss: 89.32159423828125 = 1.0522384643554688 + 10.0 * 8.826935768127441
Epoch 1520, val loss: 1.053087830543518
Epoch 1530, training loss: 89.32070922851562 = 1.0522072315216064 + 10.0 * 8.826849937438965
Epoch 1530, val loss: 1.053060531616211
Epoch 1540, training loss: 89.36138153076172 = 1.0521705150604248 + 10.0 * 8.830921173095703
Epoch 1540, val loss: 1.0530433654785156
Epoch 1550, training loss: 89.38343811035156 = 1.0521327257156372 + 10.0 * 8.833130836486816
Epoch 1550, val loss: 1.0529874563217163
Epoch 1560, training loss: 89.2945556640625 = 1.0519922971725464 + 10.0 * 8.82425594329834
Epoch 1560, val loss: 1.0528748035430908
Epoch 1570, training loss: 89.33551025390625 = 1.0519556999206543 + 10.0 * 8.82835578918457
Epoch 1570, val loss: 1.0528488159179688
Epoch 1580, training loss: 89.39945983886719 = 1.0519599914550781 + 10.0 * 8.834750175476074
Epoch 1580, val loss: 1.0528359413146973
Epoch 1590, training loss: 89.41829681396484 = 1.0519362688064575 + 10.0 * 8.836636543273926
Epoch 1590, val loss: 1.0528249740600586
Epoch 1600, training loss: 89.38967895507812 = 1.0517246723175049 + 10.0 * 8.833795547485352
Epoch 1600, val loss: 1.05263352394104
Epoch 1610, training loss: 89.3470687866211 = 1.0516870021820068 + 10.0 * 8.829538345336914
Epoch 1610, val loss: 1.052601933479309
Epoch 1620, training loss: 89.3719482421875 = 1.051682710647583 + 10.0 * 8.832026481628418
Epoch 1620, val loss: 1.0525963306427002
Epoch 1630, training loss: 89.4120864868164 = 1.0516794919967651 + 10.0 * 8.836040496826172
Epoch 1630, val loss: 1.0526007413864136
Epoch 1640, training loss: 89.48448944091797 = 1.0516926050186157 + 10.0 * 8.843279838562012
Epoch 1640, val loss: 1.0525894165039062
Epoch 1650, training loss: 89.46671295166016 = 1.0516341924667358 + 10.0 * 8.841507911682129
Epoch 1650, val loss: 1.0525563955307007
Epoch 1660, training loss: 89.49833679199219 = 1.0516128540039062 + 10.0 * 8.844672203063965
Epoch 1660, val loss: 1.0525280237197876
Epoch 1670, training loss: 89.51522064208984 = 1.05158269405365 + 10.0 * 8.84636402130127
Epoch 1670, val loss: 1.052499771118164
Epoch 1680, training loss: 89.50981903076172 = 1.051503300666809 + 10.0 * 8.845830917358398
Epoch 1680, val loss: 1.0524377822875977
Epoch 1690, training loss: 89.57142639160156 = 1.051490068435669 + 10.0 * 8.851993560791016
Epoch 1690, val loss: 1.052407145500183
Epoch 1700, training loss: 89.58018493652344 = 1.0514293909072876 + 10.0 * 8.852875709533691
Epoch 1700, val loss: 1.0523481369018555
Epoch 1710, training loss: 89.58763885498047 = 1.0513843297958374 + 10.0 * 8.853625297546387
Epoch 1710, val loss: 1.052316665649414
Epoch 1720, training loss: 89.60565185546875 = 1.0513461828231812 + 10.0 * 8.855430603027344
Epoch 1720, val loss: 1.0522828102111816
Epoch 1730, training loss: 89.53779602050781 = 1.0511800050735474 + 10.0 * 8.848661422729492
Epoch 1730, val loss: 1.0521392822265625
Epoch 1740, training loss: 89.49075317382812 = 1.0511274337768555 + 10.0 * 8.843962669372559
Epoch 1740, val loss: 1.0520672798156738
Epoch 1750, training loss: 89.53348541259766 = 1.0510941743850708 + 10.0 * 8.848238945007324
Epoch 1750, val loss: 1.052031397819519
Epoch 1760, training loss: 89.58842468261719 = 1.0510691404342651 + 10.0 * 8.853734970092773
Epoch 1760, val loss: 1.0520367622375488
Epoch 1770, training loss: 89.65717315673828 = 1.0510731935501099 + 10.0 * 8.860610008239746
Epoch 1770, val loss: 1.0520446300506592
Epoch 1780, training loss: 89.68779754638672 = 1.0510159730911255 + 10.0 * 8.863677978515625
Epoch 1780, val loss: 1.051963448524475
Epoch 1790, training loss: 89.66950225830078 = 1.0509510040283203 + 10.0 * 8.861855506896973
Epoch 1790, val loss: 1.0519275665283203
Epoch 1800, training loss: 89.73444366455078 = 1.050931453704834 + 10.0 * 8.868350982666016
Epoch 1800, val loss: 1.0519030094146729
Epoch 1810, training loss: 89.7545394897461 = 1.0508829355239868 + 10.0 * 8.870366096496582
Epoch 1810, val loss: 1.0518537759780884
Epoch 1820, training loss: 89.71365356445312 = 1.0507686138153076 + 10.0 * 8.866289138793945
Epoch 1820, val loss: 1.051783561706543
Epoch 1830, training loss: 89.75434112548828 = 1.0507566928863525 + 10.0 * 8.87035846710205
Epoch 1830, val loss: 1.0517429113388062
Epoch 1840, training loss: 89.79680633544922 = 1.050735592842102 + 10.0 * 8.87460708618164
Epoch 1840, val loss: 1.0517330169677734
Epoch 1850, training loss: 89.80374145507812 = 1.0506598949432373 + 10.0 * 8.8753080368042
Epoch 1850, val loss: 1.0516388416290283
Epoch 1860, training loss: 89.71692657470703 = 1.0505326986312866 + 10.0 * 8.866640090942383
Epoch 1860, val loss: 1.0515612363815308
Epoch 1870, training loss: 89.79985046386719 = 1.0505226850509644 + 10.0 * 8.874933242797852
Epoch 1870, val loss: 1.051557183265686
Epoch 1880, training loss: 89.84603881835938 = 1.0505011081695557 + 10.0 * 8.87955379486084
Epoch 1880, val loss: 1.0515116453170776
Epoch 1890, training loss: 89.84309387207031 = 1.050408959388733 + 10.0 * 8.879268646240234
Epoch 1890, val loss: 1.0514342784881592
Epoch 1900, training loss: 89.79962921142578 = 1.0503473281860352 + 10.0 * 8.87492847442627
Epoch 1900, val loss: 1.0513144731521606
Epoch 1910, training loss: 89.8404541015625 = 1.0502815246582031 + 10.0 * 8.87901782989502
Epoch 1910, val loss: 1.051302433013916
Epoch 1920, training loss: 89.88028717041016 = 1.0502604246139526 + 10.0 * 8.883002281188965
Epoch 1920, val loss: 1.0512992143630981
Epoch 1930, training loss: 89.83023071289062 = 1.0501590967178345 + 10.0 * 8.878007888793945
Epoch 1930, val loss: 1.0512313842773438
Epoch 1940, training loss: 89.87532043457031 = 1.050143837928772 + 10.0 * 8.88251781463623
Epoch 1940, val loss: 1.0511767864227295
Epoch 1950, training loss: 89.91545867919922 = 1.0500969886779785 + 10.0 * 8.886536598205566
Epoch 1950, val loss: 1.0511491298675537
Epoch 1960, training loss: 89.90523529052734 = 1.050032138824463 + 10.0 * 8.885519981384277
Epoch 1960, val loss: 1.0510759353637695
Epoch 1970, training loss: 89.92000579833984 = 1.0499825477600098 + 10.0 * 8.887002944946289
Epoch 1970, val loss: 1.0510462522506714
Epoch 1980, training loss: 89.94426727294922 = 1.0499049425125122 + 10.0 * 8.889436721801758
Epoch 1980, val loss: 1.0509967803955078
Epoch 1990, training loss: 89.94695281982422 = 1.0498440265655518 + 10.0 * 8.889711380004883
Epoch 1990, val loss: 1.0508999824523926
Epoch 2000, training loss: 89.94119262695312 = 1.0497748851776123 + 10.0 * 8.889142036437988
Epoch 2000, val loss: 1.050844430923462
Epoch 2010, training loss: 89.96382141113281 = 1.0497221946716309 + 10.0 * 8.891409873962402
Epoch 2010, val loss: 1.0508204698562622
Epoch 2020, training loss: 89.99556732177734 = 1.0496747493743896 + 10.0 * 8.8945894241333
Epoch 2020, val loss: 1.0507627725601196
Epoch 2030, training loss: 89.96076965332031 = 1.0495697259902954 + 10.0 * 8.891119956970215
Epoch 2030, val loss: 1.050680160522461
Epoch 2040, training loss: 89.9840087890625 = 1.0495140552520752 + 10.0 * 8.893449783325195
Epoch 2040, val loss: 1.0506410598754883
Epoch 2050, training loss: 90.00935363769531 = 1.0494321584701538 + 10.0 * 8.895992279052734
Epoch 2050, val loss: 1.0505439043045044
Epoch 2060, training loss: 89.83836364746094 = 1.0491851568222046 + 10.0 * 8.878917694091797
Epoch 2060, val loss: 1.0503381490707397
Epoch 2070, training loss: 89.84794616699219 = 1.0491559505462646 + 10.0 * 8.879878997802734
Epoch 2070, val loss: 1.0502837896347046
Epoch 2080, training loss: 89.91206359863281 = 1.0491477251052856 + 10.0 * 8.88629150390625
Epoch 2080, val loss: 1.0502923727035522
Epoch 2090, training loss: 89.97698211669922 = 1.0491483211517334 + 10.0 * 8.892783164978027
Epoch 2090, val loss: 1.0503076314926147
Epoch 2100, training loss: 90.0232162475586 = 1.049151062965393 + 10.0 * 8.897406578063965
Epoch 2100, val loss: 1.0502995252609253
Epoch 2110, training loss: 90.01335906982422 = 1.049079179763794 + 10.0 * 8.896428108215332
Epoch 2110, val loss: 1.0502253770828247
Epoch 2120, training loss: 90.0243911743164 = 1.049000859260559 + 10.0 * 8.897539138793945
Epoch 2120, val loss: 1.0501703023910522
Epoch 2130, training loss: 90.07078552246094 = 1.0489625930786133 + 10.0 * 8.902181625366211
Epoch 2130, val loss: 1.0501514673233032
Epoch 2140, training loss: 90.04094696044922 = 1.0488251447677612 + 10.0 * 8.899211883544922
Epoch 2140, val loss: 1.0500344038009644
Epoch 2150, training loss: 90.03828430175781 = 1.0487834215164185 + 10.0 * 8.898950576782227
Epoch 2150, val loss: 1.0499972105026245
Epoch 2160, training loss: 90.07926940917969 = 1.0487490892410278 + 10.0 * 8.903051376342773
Epoch 2160, val loss: 1.0499674081802368
Epoch 2170, training loss: 90.11934661865234 = 1.0487143993377686 + 10.0 * 8.907063484191895
Epoch 2170, val loss: 1.0499167442321777
Epoch 2180, training loss: 90.10009765625 = 1.048588752746582 + 10.0 * 8.905150413513184
Epoch 2180, val loss: 1.049824833869934
Epoch 2190, training loss: 90.14129638671875 = 1.0485191345214844 + 10.0 * 8.909276962280273
Epoch 2190, val loss: 1.0497653484344482
Epoch 2200, training loss: 90.11003875732422 = 1.0484241247177124 + 10.0 * 8.906161308288574
Epoch 2200, val loss: 1.049673318862915
Epoch 2210, training loss: 90.12665557861328 = 1.0483644008636475 + 10.0 * 8.907829284667969
Epoch 2210, val loss: 1.0496186017990112
Epoch 2220, training loss: 90.1501693725586 = 1.0483335256576538 + 10.0 * 8.910183906555176
Epoch 2220, val loss: 1.0496076345443726
Epoch 2230, training loss: 90.18692016601562 = 1.048279881477356 + 10.0 * 8.913864135742188
Epoch 2230, val loss: 1.049554705619812
Epoch 2240, training loss: 90.15106964111328 = 1.0480735301971436 + 10.0 * 8.910299301147461
Epoch 2240, val loss: 1.049344778060913
Epoch 2250, training loss: 90.11907958984375 = 1.0479676723480225 + 10.0 * 8.907111167907715
Epoch 2250, val loss: 1.0492483377456665
Epoch 2260, training loss: 90.16539764404297 = 1.0479767322540283 + 10.0 * 8.911742210388184
Epoch 2260, val loss: 1.0492910146713257
Epoch 2270, training loss: 90.21310424804688 = 1.0479416847229004 + 10.0 * 8.916516304016113
Epoch 2270, val loss: 1.0492490530014038
Epoch 2280, training loss: 90.19269561767578 = 1.0478661060333252 + 10.0 * 8.914483070373535
Epoch 2280, val loss: 1.049181580543518
Epoch 2290, training loss: 90.19584655761719 = 1.0477722883224487 + 10.0 * 8.914807319641113
Epoch 2290, val loss: 1.0490964651107788
Epoch 2300, training loss: 90.19808959960938 = 1.047693133354187 + 10.0 * 8.915040016174316
Epoch 2300, val loss: 1.0490450859069824
Epoch 2310, training loss: 90.24310302734375 = 1.047634482383728 + 10.0 * 8.919546127319336
Epoch 2310, val loss: 1.0489747524261475
Epoch 2320, training loss: 90.15962982177734 = 1.0474047660827637 + 10.0 * 8.911222457885742
Epoch 2320, val loss: 1.0487428903579712
Epoch 2330, training loss: 90.0086898803711 = 1.0467798709869385 + 10.0 * 8.896190643310547
Epoch 2330, val loss: 1.0481133460998535
Epoch 2340, training loss: 90.06542205810547 = 1.0468393564224243 + 10.0 * 8.90185832977295
Epoch 2340, val loss: 1.0481479167938232
Epoch 2350, training loss: 90.1473617553711 = 1.0469001531600952 + 10.0 * 8.910046577453613
Epoch 2350, val loss: 1.0482820272445679
Epoch 2360, training loss: 90.20825958251953 = 1.0469485521316528 + 10.0 * 8.916131019592285
Epoch 2360, val loss: 1.0483975410461426
Epoch 2370, training loss: 90.24723815917969 = 1.0469160079956055 + 10.0 * 8.920032501220703
Epoch 2370, val loss: 1.0484073162078857
Epoch 2380, training loss: 90.30793762207031 = 1.0469635725021362 + 10.0 * 8.92609691619873
Epoch 2380, val loss: 1.0484484434127808
Epoch 2390, training loss: 90.35325622558594 = 1.0469588041305542 + 10.0 * 8.93062973022461
Epoch 2390, val loss: 1.0484435558319092
Epoch 2400, training loss: 90.38035583496094 = 1.0469297170639038 + 10.0 * 8.933342933654785
Epoch 2400, val loss: 1.048430323600769
Epoch 2410, training loss: 90.3701171875 = 1.0468460321426392 + 10.0 * 8.932327270507812
Epoch 2410, val loss: 1.0483534336090088
Epoch 2420, training loss: 90.38823699951172 = 1.0467684268951416 + 10.0 * 8.934146881103516
Epoch 2420, val loss: 1.048282265663147
Epoch 2430, training loss: 90.39298248291016 = 1.0466862916946411 + 10.0 * 8.934629440307617
Epoch 2430, val loss: 1.04822838306427
Epoch 2440, training loss: 90.39695739746094 = 1.0465857982635498 + 10.0 * 8.935037612915039
Epoch 2440, val loss: 1.0481562614440918
Epoch 2450, training loss: 90.42318725585938 = 1.046553373336792 + 10.0 * 8.937663078308105
Epoch 2450, val loss: 1.0481255054473877
Epoch 2460, training loss: 90.41588592529297 = 1.0464189052581787 + 10.0 * 8.936946868896484
Epoch 2460, val loss: 1.048007845878601
Epoch 2470, training loss: 90.41178894042969 = 1.0463223457336426 + 10.0 * 8.936546325683594
Epoch 2470, val loss: 1.047933578491211
Epoch 2480, training loss: 90.45845031738281 = 1.0462919473648071 + 10.0 * 8.941215515136719
Epoch 2480, val loss: 1.047893762588501
Epoch 2490, training loss: 90.46514892578125 = 1.0462054014205933 + 10.0 * 8.94189453125
Epoch 2490, val loss: 1.047841191291809
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.40840579710144925
0.8104035354633051
The final CL Acc:0.41063, 0.00325, The final GNN Acc:0.80978, 0.00094
raw graph: torch.Size([2, 88648])
add edge: torch.Size([2, 110494])
remove edge: torch.Size([2, 66408])
updated graph: torch.Size([2, 88254])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 102.130126953125 = 1.1269755363464355 + 10.0 * 10.10031509399414
Epoch 0, val loss: 1.1260533332824707
Epoch 10, training loss: 97.50658416748047 = 1.1219229698181152 + 10.0 * 9.638465881347656
Epoch 10, val loss: 1.1210333108901978
Epoch 20, training loss: 95.36962890625 = 1.1172499656677246 + 10.0 * 9.425237655639648
Epoch 20, val loss: 1.116391897201538
Epoch 30, training loss: 93.98262023925781 = 1.1127190589904785 + 10.0 * 9.28699016571045
Epoch 30, val loss: 1.1118897199630737
Epoch 40, training loss: 92.90176391601562 = 1.1083686351776123 + 10.0 * 9.179339408874512
Epoch 40, val loss: 1.107580542564392
Epoch 50, training loss: 92.06076049804688 = 1.1042296886444092 + 10.0 * 9.09565258026123
Epoch 50, val loss: 1.103480577468872
Epoch 60, training loss: 91.36685180664062 = 1.1002858877182007 + 10.0 * 9.026656150817871
Epoch 60, val loss: 1.0995783805847168
Epoch 70, training loss: 90.77912902832031 = 1.0965287685394287 + 10.0 * 8.968259811401367
Epoch 70, val loss: 1.095861554145813
Epoch 80, training loss: 90.27496337890625 = 1.0929549932479858 + 10.0 * 8.918200492858887
Epoch 80, val loss: 1.0923293828964233
Epoch 90, training loss: 89.84391784667969 = 1.0895707607269287 + 10.0 * 8.875434875488281
Epoch 90, val loss: 1.0889862775802612
Epoch 100, training loss: 89.47042846679688 = 1.0863676071166992 + 10.0 * 8.838406562805176
Epoch 100, val loss: 1.0858248472213745
Epoch 110, training loss: 89.15348815917969 = 1.0833414793014526 + 10.0 * 8.807014465332031
Epoch 110, val loss: 1.0828425884246826
Epoch 120, training loss: 88.88817596435547 = 1.0804957151412964 + 10.0 * 8.780767440795898
Epoch 120, val loss: 1.0800403356552124
Epoch 130, training loss: 88.65129852294922 = 1.0778290033340454 + 10.0 * 8.757347106933594
Epoch 130, val loss: 1.077422022819519
Epoch 140, training loss: 88.44436645507812 = 1.0753304958343506 + 10.0 * 8.736903190612793
Epoch 140, val loss: 1.074973225593567
Epoch 150, training loss: 88.2657699584961 = 1.073019027709961 + 10.0 * 8.719274520874023
Epoch 150, val loss: 1.0727139711380005
Epoch 160, training loss: 88.1263656616211 = 1.0708845853805542 + 10.0 * 8.705548286437988
Epoch 160, val loss: 1.0706344842910767
Epoch 170, training loss: 88.00544738769531 = 1.0689210891723633 + 10.0 * 8.693652153015137
Epoch 170, val loss: 1.0687216520309448
Epoch 180, training loss: 87.88041687011719 = 1.0671266317367554 + 10.0 * 8.681329727172852
Epoch 180, val loss: 1.0669821500778198
Epoch 190, training loss: 87.78475952148438 = 1.0654938220977783 + 10.0 * 8.671926498413086
Epoch 190, val loss: 1.0654083490371704
Epoch 200, training loss: 87.75028991699219 = 1.0639960765838623 + 10.0 * 8.66862964630127
Epoch 200, val loss: 1.0639617443084717
Epoch 210, training loss: 87.66749572753906 = 1.062726378440857 + 10.0 * 8.660476684570312
Epoch 210, val loss: 1.0627514123916626
Epoch 220, training loss: 87.53286743164062 = 1.0615230798721313 + 10.0 * 8.647134780883789
Epoch 220, val loss: 1.0616106986999512
Epoch 230, training loss: 87.47405242919922 = 1.0605106353759766 + 10.0 * 8.64135456085205
Epoch 230, val loss: 1.0606517791748047
Epoch 240, training loss: 87.41459655761719 = 1.0596007108688354 + 10.0 * 8.635499954223633
Epoch 240, val loss: 1.05979585647583
Epoch 250, training loss: 87.3864974975586 = 1.058793306350708 + 10.0 * 8.632770538330078
Epoch 250, val loss: 1.059059739112854
Epoch 260, training loss: 87.393310546875 = 1.0581547021865845 + 10.0 * 8.633516311645508
Epoch 260, val loss: 1.0584594011306763
Epoch 270, training loss: 87.3470230102539 = 1.0575740337371826 + 10.0 * 8.628945350646973
Epoch 270, val loss: 1.0579339265823364
Epoch 280, training loss: 87.35089874267578 = 1.0570918321609497 + 10.0 * 8.62938117980957
Epoch 280, val loss: 1.057492971420288
Epoch 290, training loss: 87.3047866821289 = 1.0566844940185547 + 10.0 * 8.624810218811035
Epoch 290, val loss: 1.0571378469467163
Epoch 300, training loss: 87.55855560302734 = 1.05617094039917 + 10.0 * 8.650238990783691
Epoch 300, val loss: 1.0566359758377075
Epoch 310, training loss: 87.33389282226562 = 1.0559040307998657 + 10.0 * 8.627799034118652
Epoch 310, val loss: 1.0564554929733276
Epoch 320, training loss: 87.84300994873047 = 1.0557647943496704 + 10.0 * 8.67872428894043
Epoch 320, val loss: 1.056294322013855
Epoch 330, training loss: 87.39524841308594 = 1.0553869009017944 + 10.0 * 8.63398551940918
Epoch 330, val loss: 1.0560168027877808
Epoch 340, training loss: 87.52603149414062 = 1.0553905963897705 + 10.0 * 8.647064208984375
Epoch 340, val loss: 1.0560283660888672
Epoch 350, training loss: 87.46106719970703 = 1.0552585124969482 + 10.0 * 8.640581130981445
Epoch 350, val loss: 1.0559077262878418
Epoch 360, training loss: 87.4518814086914 = 1.055166244506836 + 10.0 * 8.639671325683594
Epoch 360, val loss: 1.0558422803878784
Epoch 370, training loss: 87.44149780273438 = 1.0550992488861084 + 10.0 * 8.638639450073242
Epoch 370, val loss: 1.0557920932769775
Epoch 380, training loss: 87.45862579345703 = 1.0550404787063599 + 10.0 * 8.640358924865723
Epoch 380, val loss: 1.055750846862793
Epoch 390, training loss: 87.46633911132812 = 1.0549741983413696 + 10.0 * 8.641136169433594
Epoch 390, val loss: 1.055703043937683
Epoch 400, training loss: 87.41252136230469 = 1.054919719696045 + 10.0 * 8.635760307312012
Epoch 400, val loss: 1.0556676387786865
Epoch 410, training loss: 87.45137023925781 = 1.0548862218856812 + 10.0 * 8.6396484375
Epoch 410, val loss: 1.0556347370147705
Epoch 420, training loss: 87.46526336669922 = 1.0548601150512695 + 10.0 * 8.641039848327637
Epoch 420, val loss: 1.055619478225708
Epoch 430, training loss: 87.48703002929688 = 1.0548317432403564 + 10.0 * 8.643219947814941
Epoch 430, val loss: 1.0556007623672485
Epoch 440, training loss: 87.51277923583984 = 1.054795742034912 + 10.0 * 8.645798683166504
Epoch 440, val loss: 1.0555757284164429
Epoch 450, training loss: 87.53781127929688 = 1.0547806024551392 + 10.0 * 8.648303031921387
Epoch 450, val loss: 1.0555658340454102
Epoch 460, training loss: 87.55516052246094 = 1.0547562837600708 + 10.0 * 8.650040626525879
Epoch 460, val loss: 1.0555425882339478
Epoch 470, training loss: 87.58868408203125 = 1.0547459125518799 + 10.0 * 8.653393745422363
Epoch 470, val loss: 1.0555349588394165
Epoch 480, training loss: 87.57865905761719 = 1.0547205209732056 + 10.0 * 8.65239429473877
Epoch 480, val loss: 1.0555158853530884
Epoch 490, training loss: 87.59268188476562 = 1.0546993017196655 + 10.0 * 8.65379810333252
Epoch 490, val loss: 1.055495023727417
Epoch 500, training loss: 87.65489196777344 = 1.0547126531600952 + 10.0 * 8.660017967224121
Epoch 500, val loss: 1.055510401725769
Epoch 510, training loss: 87.65885162353516 = 1.0547044277191162 + 10.0 * 8.660414695739746
Epoch 510, val loss: 1.0555096864700317
Epoch 520, training loss: 87.62748718261719 = 1.0546451807022095 + 10.0 * 8.6572847366333
Epoch 520, val loss: 1.055458426475525
Epoch 530, training loss: 87.64945220947266 = 1.0546449422836304 + 10.0 * 8.659481048583984
Epoch 530, val loss: 1.0554548501968384
Epoch 540, training loss: 87.67324829101562 = 1.054645299911499 + 10.0 * 8.661860466003418
Epoch 540, val loss: 1.0554530620574951
Epoch 550, training loss: 87.71063995361328 = 1.0546331405639648 + 10.0 * 8.665600776672363
Epoch 550, val loss: 1.0554354190826416
Epoch 560, training loss: 87.75076293945312 = 1.05463445186615 + 10.0 * 8.669612884521484
Epoch 560, val loss: 1.0554379224777222
Epoch 570, training loss: 87.79235076904297 = 1.0546224117279053 + 10.0 * 8.673772811889648
Epoch 570, val loss: 1.055432915687561
Epoch 580, training loss: 87.80386352539062 = 1.0546094179153442 + 10.0 * 8.674924850463867
Epoch 580, val loss: 1.055416464805603
Epoch 590, training loss: 87.83515167236328 = 1.0545916557312012 + 10.0 * 8.678056716918945
Epoch 590, val loss: 1.0554015636444092
Epoch 600, training loss: 87.87355041503906 = 1.0545892715454102 + 10.0 * 8.681896209716797
Epoch 600, val loss: 1.0553876161575317
Epoch 610, training loss: 87.81924438476562 = 1.0545384883880615 + 10.0 * 8.676470756530762
Epoch 610, val loss: 1.0553523302078247
Epoch 620, training loss: 87.64622497558594 = 1.0543439388275146 + 10.0 * 8.659188270568848
Epoch 620, val loss: 1.0551615953445435
Epoch 630, training loss: 88.0289077758789 = 1.0545432567596436 + 10.0 * 8.697436332702637
Epoch 630, val loss: 1.055362343788147
Epoch 640, training loss: 87.9255599975586 = 1.0544928312301636 + 10.0 * 8.68710708618164
Epoch 640, val loss: 1.0553123950958252
Epoch 650, training loss: 87.92594146728516 = 1.0544936656951904 + 10.0 * 8.68714427947998
Epoch 650, val loss: 1.0553197860717773
Epoch 660, training loss: 87.90251159667969 = 1.0544735193252563 + 10.0 * 8.68480396270752
Epoch 660, val loss: 1.0553046464920044
Epoch 670, training loss: 87.95687103271484 = 1.0544792413711548 + 10.0 * 8.690238952636719
Epoch 670, val loss: 1.0553085803985596
Epoch 680, training loss: 88.00566101074219 = 1.05447256565094 + 10.0 * 8.69511890411377
Epoch 680, val loss: 1.0552982091903687
Epoch 690, training loss: 88.03194427490234 = 1.0544629096984863 + 10.0 * 8.697748184204102
Epoch 690, val loss: 1.0552871227264404
Epoch 700, training loss: 88.0617446899414 = 1.0544464588165283 + 10.0 * 8.700730323791504
Epoch 700, val loss: 1.0552760362625122
Epoch 710, training loss: 88.10490417480469 = 1.0544443130493164 + 10.0 * 8.705045700073242
Epoch 710, val loss: 1.0552685260772705
Epoch 720, training loss: 88.11880493164062 = 1.0544281005859375 + 10.0 * 8.706438064575195
Epoch 720, val loss: 1.055250883102417
Epoch 730, training loss: 88.10972595214844 = 1.054395318031311 + 10.0 * 8.705533027648926
Epoch 730, val loss: 1.0552245378494263
Epoch 740, training loss: 88.14908599853516 = 1.0543991327285767 + 10.0 * 8.709468841552734
Epoch 740, val loss: 1.0552226305007935
Epoch 750, training loss: 88.1875991821289 = 1.0543898344039917 + 10.0 * 8.7133207321167
Epoch 750, val loss: 1.055214762687683
Epoch 760, training loss: 88.18917083740234 = 1.0543737411499023 + 10.0 * 8.713479995727539
Epoch 760, val loss: 1.055202841758728
Epoch 770, training loss: 88.2112808227539 = 1.0543608665466309 + 10.0 * 8.715692520141602
Epoch 770, val loss: 1.055185317993164
Epoch 780, training loss: 88.23685455322266 = 1.054344892501831 + 10.0 * 8.71825122833252
Epoch 780, val loss: 1.055173635482788
Epoch 790, training loss: 88.23587799072266 = 1.0543246269226074 + 10.0 * 8.718155860900879
Epoch 790, val loss: 1.0551567077636719
Epoch 800, training loss: 88.26360321044922 = 1.0543183088302612 + 10.0 * 8.720928192138672
Epoch 800, val loss: 1.0551477670669556
Epoch 810, training loss: 88.27849578857422 = 1.054305076599121 + 10.0 * 8.722418785095215
Epoch 810, val loss: 1.0551257133483887
Epoch 820, training loss: 88.30857849121094 = 1.0542900562286377 + 10.0 * 8.725428581237793
Epoch 820, val loss: 1.055108666419983
Epoch 830, training loss: 88.29117584228516 = 1.054250717163086 + 10.0 * 8.723691940307617
Epoch 830, val loss: 1.0550919771194458
Epoch 840, training loss: 88.34383392333984 = 1.0542620420455933 + 10.0 * 8.728957176208496
Epoch 840, val loss: 1.0550907850265503
Epoch 850, training loss: 88.33454895019531 = 1.0541893243789673 + 10.0 * 8.728035926818848
Epoch 850, val loss: 1.0549877882003784
Epoch 860, training loss: 88.42105102539062 = 1.054200291633606 + 10.0 * 8.736684799194336
Epoch 860, val loss: 1.055060863494873
Epoch 870, training loss: 88.34355163574219 = 1.0541715621948242 + 10.0 * 8.728938102722168
Epoch 870, val loss: 1.0550038814544678
Epoch 880, training loss: 88.34664916992188 = 1.0541619062423706 + 10.0 * 8.729249000549316
Epoch 880, val loss: 1.0550040006637573
Epoch 890, training loss: 88.48448944091797 = 1.0541924238204956 + 10.0 * 8.743029594421387
Epoch 890, val loss: 1.0550246238708496
Epoch 900, training loss: 88.54216003417969 = 1.0541967153549194 + 10.0 * 8.748796463012695
Epoch 900, val loss: 1.0550336837768555
Epoch 910, training loss: 88.6217269897461 = 1.0541936159133911 + 10.0 * 8.756753921508789
Epoch 910, val loss: 1.0550273656845093
Epoch 920, training loss: 88.62876892089844 = 1.0541850328445435 + 10.0 * 8.757458686828613
Epoch 920, val loss: 1.0550192594528198
Epoch 930, training loss: 88.63215637207031 = 1.054165005683899 + 10.0 * 8.75779914855957
Epoch 930, val loss: 1.0550013780593872
Epoch 940, training loss: 88.67161560058594 = 1.0541565418243408 + 10.0 * 8.761746406555176
Epoch 940, val loss: 1.054994821548462
Epoch 950, training loss: 88.68932342529297 = 1.0541409254074097 + 10.0 * 8.763518333435059
Epoch 950, val loss: 1.0549731254577637
Epoch 960, training loss: 88.73389434814453 = 1.054133653640747 + 10.0 * 8.767976760864258
Epoch 960, val loss: 1.054969072341919
Epoch 970, training loss: 88.7577896118164 = 1.0541203022003174 + 10.0 * 8.770366668701172
Epoch 970, val loss: 1.0549527406692505
Epoch 980, training loss: 88.78832244873047 = 1.0541013479232788 + 10.0 * 8.773422241210938
Epoch 980, val loss: 1.054938554763794
Epoch 990, training loss: 88.7682113647461 = 1.0540695190429688 + 10.0 * 8.771413803100586
Epoch 990, val loss: 1.054906964302063
Epoch 1000, training loss: 88.8251724243164 = 1.0540727376937866 + 10.0 * 8.77711009979248
Epoch 1000, val loss: 1.0549085140228271
Epoch 1010, training loss: 88.84136199951172 = 1.054046869277954 + 10.0 * 8.778731346130371
Epoch 1010, val loss: 1.0548824071884155
Epoch 1020, training loss: 88.87351989746094 = 1.054036021232605 + 10.0 * 8.78194808959961
Epoch 1020, val loss: 1.054866909980774
Epoch 1030, training loss: 88.87612915039062 = 1.054020881652832 + 10.0 * 8.782210350036621
Epoch 1030, val loss: 1.0548522472381592
Epoch 1040, training loss: 88.89912414550781 = 1.0540087223052979 + 10.0 * 8.78451156616211
Epoch 1040, val loss: 1.0548471212387085
Epoch 1050, training loss: 88.92718505859375 = 1.0539928674697876 + 10.0 * 8.78731918334961
Epoch 1050, val loss: 1.054826259613037
Epoch 1060, training loss: 88.87828826904297 = 1.0539149045944214 + 10.0 * 8.782437324523926
Epoch 1060, val loss: 1.0547294616699219
Epoch 1070, training loss: 88.89482879638672 = 1.053908348083496 + 10.0 * 8.78409194946289
Epoch 1070, val loss: 1.0547425746917725
Epoch 1080, training loss: 88.91421508789062 = 1.0539181232452393 + 10.0 * 8.786029815673828
Epoch 1080, val loss: 1.0547521114349365
Epoch 1090, training loss: 88.97460174560547 = 1.0539134740829468 + 10.0 * 8.792068481445312
Epoch 1090, val loss: 1.0547457933425903
Epoch 1100, training loss: 88.98197174072266 = 1.0538966655731201 + 10.0 * 8.792807579040527
Epoch 1100, val loss: 1.0547327995300293
Epoch 1110, training loss: 88.99724578857422 = 1.0538948774337769 + 10.0 * 8.79433536529541
Epoch 1110, val loss: 1.054726481437683
Epoch 1120, training loss: 89.05142974853516 = 1.053879737854004 + 10.0 * 8.799755096435547
Epoch 1120, val loss: 1.0547083616256714
Epoch 1130, training loss: 89.0228042602539 = 1.053837776184082 + 10.0 * 8.796895980834961
Epoch 1130, val loss: 1.0546598434448242
Epoch 1140, training loss: 89.0502700805664 = 1.0538299083709717 + 10.0 * 8.799643516540527
Epoch 1140, val loss: 1.0546607971191406
Epoch 1150, training loss: 89.08952331542969 = 1.0538251399993896 + 10.0 * 8.803569793701172
Epoch 1150, val loss: 1.054659366607666
Epoch 1160, training loss: 89.09805297851562 = 1.053816318511963 + 10.0 * 8.804423332214355
Epoch 1160, val loss: 1.0546467304229736
Epoch 1170, training loss: 89.12622833251953 = 1.0537928342819214 + 10.0 * 8.807243347167969
Epoch 1170, val loss: 1.0546305179595947
Epoch 1180, training loss: 89.162353515625 = 1.0537902116775513 + 10.0 * 8.810856819152832
Epoch 1180, val loss: 1.0546249151229858
Epoch 1190, training loss: 89.13337707519531 = 1.053757667541504 + 10.0 * 8.807962417602539
Epoch 1190, val loss: 1.0545949935913086
Epoch 1200, training loss: 89.19383239746094 = 1.0537561178207397 + 10.0 * 8.814007759094238
Epoch 1200, val loss: 1.0545902252197266
Epoch 1210, training loss: 89.22032165527344 = 1.053740382194519 + 10.0 * 8.816658020019531
Epoch 1210, val loss: 1.0545620918273926
Epoch 1220, training loss: 89.20054626464844 = 1.0536917448043823 + 10.0 * 8.814685821533203
Epoch 1220, val loss: 1.0545390844345093
Epoch 1230, training loss: 89.2392807006836 = 1.0536876916885376 + 10.0 * 8.818559646606445
Epoch 1230, val loss: 1.0545258522033691
Epoch 1240, training loss: 89.26549530029297 = 1.0536789894104004 + 10.0 * 8.821181297302246
Epoch 1240, val loss: 1.0545165538787842
Epoch 1250, training loss: 89.30451202392578 = 1.0536644458770752 + 10.0 * 8.825084686279297
Epoch 1250, val loss: 1.0544992685317993
Epoch 1260, training loss: 89.32064819335938 = 1.053650140762329 + 10.0 * 8.826700210571289
Epoch 1260, val loss: 1.0544859170913696
Epoch 1270, training loss: 89.33892822265625 = 1.0536304712295532 + 10.0 * 8.828530311584473
Epoch 1270, val loss: 1.0544641017913818
Epoch 1280, training loss: 89.34329986572266 = 1.053614854812622 + 10.0 * 8.82896900177002
Epoch 1280, val loss: 1.0544480085372925
Epoch 1290, training loss: 89.36724090576172 = 1.0536044836044312 + 10.0 * 8.831363677978516
Epoch 1290, val loss: 1.054427981376648
Epoch 1300, training loss: 89.1600341796875 = 1.0531595945358276 + 10.0 * 8.810687065124512
Epoch 1300, val loss: 1.0539875030517578
Epoch 1310, training loss: 89.36083984375 = 1.0534600019454956 + 10.0 * 8.830738067626953
Epoch 1310, val loss: 1.0542793273925781
Epoch 1320, training loss: 89.28646087646484 = 1.05339777469635 + 10.0 * 8.8233060836792
Epoch 1320, val loss: 1.0542384386062622
Epoch 1330, training loss: 89.39588928222656 = 1.0534642934799194 + 10.0 * 8.83424186706543
Epoch 1330, val loss: 1.0543004274368286
Epoch 1340, training loss: 89.4457015991211 = 1.0534615516662598 + 10.0 * 8.839223861694336
Epoch 1340, val loss: 1.0543049573898315
Epoch 1350, training loss: 89.48844146728516 = 1.0534675121307373 + 10.0 * 8.843497276306152
Epoch 1350, val loss: 1.0543031692504883
Epoch 1360, training loss: 89.51766204833984 = 1.0534602403640747 + 10.0 * 8.846420288085938
Epoch 1360, val loss: 1.054291844367981
Epoch 1370, training loss: 89.55744934082031 = 1.0534536838531494 + 10.0 * 8.8503999710083
Epoch 1370, val loss: 1.054284930229187
Epoch 1380, training loss: 89.55654907226562 = 1.0534191131591797 + 10.0 * 8.850313186645508
Epoch 1380, val loss: 1.0542458295822144
Epoch 1390, training loss: 89.56575012207031 = 1.0533950328826904 + 10.0 * 8.851235389709473
Epoch 1390, val loss: 1.0542283058166504
Epoch 1400, training loss: 89.60201263427734 = 1.053390622138977 + 10.0 * 8.854862213134766
Epoch 1400, val loss: 1.0542250871658325
Epoch 1410, training loss: 89.63599395751953 = 1.0533716678619385 + 10.0 * 8.858262062072754
Epoch 1410, val loss: 1.054209589958191
Epoch 1420, training loss: 89.63655090332031 = 1.0533416271209717 + 10.0 * 8.858320236206055
Epoch 1420, val loss: 1.05418062210083
Epoch 1430, training loss: 89.66081237792969 = 1.0533180236816406 + 10.0 * 8.860750198364258
Epoch 1430, val loss: 1.0541530847549438
Epoch 1440, training loss: 89.67182922363281 = 1.0532951354980469 + 10.0 * 8.86185359954834
Epoch 1440, val loss: 1.0541311502456665
Epoch 1450, training loss: 89.70455932617188 = 1.0532824993133545 + 10.0 * 8.865127563476562
Epoch 1450, val loss: 1.0541231632232666
Epoch 1460, training loss: 89.71357727050781 = 1.0532565116882324 + 10.0 * 8.866032600402832
Epoch 1460, val loss: 1.0540971755981445
Epoch 1470, training loss: 89.70647430419922 = 1.053207278251648 + 10.0 * 8.865326881408691
Epoch 1470, val loss: 1.0540456771850586
Epoch 1480, training loss: 89.72930145263672 = 1.0531902313232422 + 10.0 * 8.867610931396484
Epoch 1480, val loss: 1.0540351867675781
Epoch 1490, training loss: 89.77631378173828 = 1.0531961917877197 + 10.0 * 8.87231159210205
Epoch 1490, val loss: 1.0540425777435303
Epoch 1500, training loss: 89.76329040527344 = 1.053161382675171 + 10.0 * 8.871012687683105
Epoch 1500, val loss: 1.0540028810501099
Epoch 1510, training loss: 89.79452514648438 = 1.0531443357467651 + 10.0 * 8.874137878417969
Epoch 1510, val loss: 1.053986668586731
Epoch 1520, training loss: 89.81210327148438 = 1.053117275238037 + 10.0 * 8.875898361206055
Epoch 1520, val loss: 1.0539588928222656
Epoch 1530, training loss: 89.81565856933594 = 1.0530873537063599 + 10.0 * 8.876256942749023
Epoch 1530, val loss: 1.0539315938949585
Epoch 1540, training loss: 89.84880828857422 = 1.0530716180801392 + 10.0 * 8.879573822021484
Epoch 1540, val loss: 1.053918480873108
Epoch 1550, training loss: 89.87001037597656 = 1.0530542135238647 + 10.0 * 8.881695747375488
Epoch 1550, val loss: 1.0538983345031738
Epoch 1560, training loss: 89.86589050292969 = 1.0530166625976562 + 10.0 * 8.881287574768066
Epoch 1560, val loss: 1.0538727045059204
Epoch 1570, training loss: 89.87652587890625 = 1.0529842376708984 + 10.0 * 8.882353782653809
Epoch 1570, val loss: 1.0538357496261597
Epoch 1580, training loss: 89.8318862915039 = 1.052929401397705 + 10.0 * 8.87789535522461
Epoch 1580, val loss: 1.05375075340271
Epoch 1590, training loss: 89.83402252197266 = 1.0528756380081177 + 10.0 * 8.878114700317383
Epoch 1590, val loss: 1.0537269115447998
Epoch 1600, training loss: 89.87947845458984 = 1.052872896194458 + 10.0 * 8.882660865783691
Epoch 1600, val loss: 1.0537307262420654
Epoch 1610, training loss: 89.924072265625 = 1.052870273590088 + 10.0 * 8.887120246887207
Epoch 1610, val loss: 1.053729772567749
Epoch 1620, training loss: 89.96868133544922 = 1.0528643131256104 + 10.0 * 8.891581535339355
Epoch 1620, val loss: 1.0537123680114746
Epoch 1630, training loss: 89.96227264404297 = 1.0528223514556885 + 10.0 * 8.890944480895996
Epoch 1630, val loss: 1.053682565689087
Epoch 1640, training loss: 89.98871612548828 = 1.0528029203414917 + 10.0 * 8.893590927124023
Epoch 1640, val loss: 1.0536634922027588
Epoch 1650, training loss: 90.01593017578125 = 1.0527820587158203 + 10.0 * 8.89631462097168
Epoch 1650, val loss: 1.05362868309021
Epoch 1660, training loss: 90.0040283203125 = 1.0527276992797852 + 10.0 * 8.895130157470703
Epoch 1660, val loss: 1.053597092628479
Epoch 1670, training loss: 90.03783416748047 = 1.0527129173278809 + 10.0 * 8.89851188659668
Epoch 1670, val loss: 1.0535775423049927
Epoch 1680, training loss: 90.03915405273438 = 1.0526810884475708 + 10.0 * 8.89864730834961
Epoch 1680, val loss: 1.0535472631454468
Epoch 1690, training loss: 90.06034851074219 = 1.052635669708252 + 10.0 * 8.900771141052246
Epoch 1690, val loss: 1.053495168685913
Epoch 1700, training loss: 90.06324005126953 = 1.0526050329208374 + 10.0 * 8.901063919067383
Epoch 1700, val loss: 1.0534647703170776
Epoch 1710, training loss: 90.09896087646484 = 1.0525941848754883 + 10.0 * 8.90463638305664
Epoch 1710, val loss: 1.0534578561782837
Epoch 1720, training loss: 90.105712890625 = 1.0525646209716797 + 10.0 * 8.905314445495605
Epoch 1720, val loss: 1.0534300804138184
Epoch 1730, training loss: 90.12762451171875 = 1.052535891532898 + 10.0 * 8.907508850097656
Epoch 1730, val loss: 1.053403615951538
Epoch 1740, training loss: 90.12594604492188 = 1.0525039434432983 + 10.0 * 8.907343864440918
Epoch 1740, val loss: 1.0533581972122192
Epoch 1750, training loss: 90.1001968383789 = 1.052432656288147 + 10.0 * 8.904776573181152
Epoch 1750, val loss: 1.053300142288208
Epoch 1760, training loss: 90.12250518798828 = 1.0524120330810547 + 10.0 * 8.90700912475586
Epoch 1760, val loss: 1.0532969236373901
Epoch 1770, training loss: 90.17807006835938 = 1.0524073839187622 + 10.0 * 8.912566184997559
Epoch 1770, val loss: 1.053270697593689
Epoch 1780, training loss: 90.18807983398438 = 1.0523712635040283 + 10.0 * 8.91357135772705
Epoch 1780, val loss: 1.0532501935958862
Epoch 1790, training loss: 90.18321228027344 = 1.052337408065796 + 10.0 * 8.913087844848633
Epoch 1790, val loss: 1.0532200336456299
Epoch 1800, training loss: 90.19017028808594 = 1.0523004531860352 + 10.0 * 8.913786888122559
Epoch 1800, val loss: 1.0531784296035767
Epoch 1810, training loss: 90.21783447265625 = 1.052277684211731 + 10.0 * 8.916555404663086
Epoch 1810, val loss: 1.0531648397445679
Epoch 1820, training loss: 90.2383804321289 = 1.0522555112838745 + 10.0 * 8.918612480163574
Epoch 1820, val loss: 1.0531234741210938
Epoch 1830, training loss: 90.20018768310547 = 1.0521948337554932 + 10.0 * 8.914799690246582
Epoch 1830, val loss: 1.0530627965927124
Epoch 1840, training loss: 90.14705657958984 = 1.0521135330200195 + 10.0 * 8.909494400024414
Epoch 1840, val loss: 1.0530059337615967
Epoch 1850, training loss: 90.14581298828125 = 1.0520654916763306 + 10.0 * 8.909375190734863
Epoch 1850, val loss: 1.0529613494873047
Epoch 1860, training loss: 90.10763549804688 = 1.0519839525222778 + 10.0 * 8.90556526184082
Epoch 1860, val loss: 1.052885890007019
Epoch 1870, training loss: 90.1254653930664 = 1.051945686340332 + 10.0 * 8.90735149383545
Epoch 1870, val loss: 1.0528571605682373
Epoch 1880, training loss: 90.15088653564453 = 1.0519343614578247 + 10.0 * 8.909894943237305
Epoch 1880, val loss: 1.0528323650360107
Epoch 1890, training loss: 90.19818115234375 = 1.0519471168518066 + 10.0 * 8.914623260498047
Epoch 1890, val loss: 1.0528470277786255
Epoch 1900, training loss: 90.25806427001953 = 1.0519471168518066 + 10.0 * 8.920611381530762
Epoch 1900, val loss: 1.0528430938720703
Epoch 1910, training loss: 90.29103088378906 = 1.0519176721572876 + 10.0 * 8.923911094665527
Epoch 1910, val loss: 1.05280339717865
Epoch 1920, training loss: 90.29437255859375 = 1.0518674850463867 + 10.0 * 8.924250602722168
Epoch 1920, val loss: 1.0527478456497192
Epoch 1930, training loss: 90.29450988769531 = 1.0518330335617065 + 10.0 * 8.924267768859863
Epoch 1930, val loss: 1.0527187585830688
Epoch 1940, training loss: 90.31688690185547 = 1.051794409751892 + 10.0 * 8.926508903503418
Epoch 1940, val loss: 1.0526882410049438
Epoch 1950, training loss: 90.35647583007812 = 1.0517768859863281 + 10.0 * 8.93047046661377
Epoch 1950, val loss: 1.0526634454727173
Epoch 1960, training loss: 90.35635375976562 = 1.051712989807129 + 10.0 * 8.930463790893555
Epoch 1960, val loss: 1.0526074171066284
Epoch 1970, training loss: 90.30744171142578 = 1.0516154766082764 + 10.0 * 8.925582885742188
Epoch 1970, val loss: 1.052520513534546
Epoch 1980, training loss: 90.33883666992188 = 1.051611304283142 + 10.0 * 8.928722381591797
Epoch 1980, val loss: 1.0525115728378296
Epoch 1990, training loss: 90.36370849609375 = 1.0515921115875244 + 10.0 * 8.931211471557617
Epoch 1990, val loss: 1.0524929761886597
Epoch 2000, training loss: 90.37297821044922 = 1.0515490770339966 + 10.0 * 8.93214225769043
Epoch 2000, val loss: 1.0524522066116333
Epoch 2010, training loss: 90.39026641845703 = 1.0515109300613403 + 10.0 * 8.93387508392334
Epoch 2010, val loss: 1.0524075031280518
Epoch 2020, training loss: 90.39105224609375 = 1.0514678955078125 + 10.0 * 8.933958053588867
Epoch 2020, val loss: 1.052380919456482
Epoch 2030, training loss: 90.43726348876953 = 1.0514342784881592 + 10.0 * 8.938582420349121
Epoch 2030, val loss: 1.0523297786712646
Epoch 2040, training loss: 90.39212036132812 = 1.0513684749603271 + 10.0 * 8.934075355529785
Epoch 2040, val loss: 1.052280068397522
Epoch 2050, training loss: 90.40975189208984 = 1.0513427257537842 + 10.0 * 8.935840606689453
Epoch 2050, val loss: 1.052251935005188
Epoch 2060, training loss: 90.42875671386719 = 1.0512974262237549 + 10.0 * 8.937746047973633
Epoch 2060, val loss: 1.052212119102478
Epoch 2070, training loss: 90.44063568115234 = 1.0512430667877197 + 10.0 * 8.938939094543457
Epoch 2070, val loss: 1.0521665811538696
Epoch 2080, training loss: 90.44831085205078 = 1.0511951446533203 + 10.0 * 8.939711570739746
Epoch 2080, val loss: 1.0521178245544434
Epoch 2090, training loss: 90.39446258544922 = 1.0509742498397827 + 10.0 * 8.934349060058594
Epoch 2090, val loss: 1.051839828491211
Epoch 2100, training loss: 89.95260620117188 = 1.050295352935791 + 10.0 * 8.890231132507324
Epoch 2100, val loss: 1.0512827634811401
Epoch 2110, training loss: 90.17363739013672 = 1.0505988597869873 + 10.0 * 8.912303924560547
Epoch 2110, val loss: 1.0515239238739014
Epoch 2120, training loss: 90.04518127441406 = 1.050566554069519 + 10.0 * 8.89946174621582
Epoch 2120, val loss: 1.05152428150177
Epoch 2130, training loss: 90.09878540039062 = 1.0505985021591187 + 10.0 * 8.904818534851074
Epoch 2130, val loss: 1.0515602827072144
Epoch 2140, training loss: 90.1665267944336 = 1.050602674484253 + 10.0 * 8.911592483520508
Epoch 2140, val loss: 1.0515425205230713
Epoch 2150, training loss: 90.26091003417969 = 1.0506452322006226 + 10.0 * 8.921026229858398
Epoch 2150, val loss: 1.0515882968902588
Epoch 2160, training loss: 90.32559204101562 = 1.0506564378738403 + 10.0 * 8.92749309539795
Epoch 2160, val loss: 1.0516120195388794
Epoch 2170, training loss: 90.36965942382812 = 1.0506449937820435 + 10.0 * 8.931901931762695
Epoch 2170, val loss: 1.0516020059585571
Epoch 2180, training loss: 90.41120147705078 = 1.050610065460205 + 10.0 * 8.93605899810791
Epoch 2180, val loss: 1.0515656471252441
Epoch 2190, training loss: 90.43399047851562 = 1.0505821704864502 + 10.0 * 8.93834114074707
Epoch 2190, val loss: 1.0515278577804565
Epoch 2200, training loss: 90.44841003417969 = 1.050536870956421 + 10.0 * 8.939786911010742
Epoch 2200, val loss: 1.0514973402023315
Epoch 2210, training loss: 90.47518157958984 = 1.0505013465881348 + 10.0 * 8.942468643188477
Epoch 2210, val loss: 1.0514620542526245
Epoch 2220, training loss: 90.50212860107422 = 1.0504554510116577 + 10.0 * 8.945167541503906
Epoch 2220, val loss: 1.0514174699783325
Epoch 2230, training loss: 90.50780487060547 = 1.050395131111145 + 10.0 * 8.945741653442383
Epoch 2230, val loss: 1.0513434410095215
Epoch 2240, training loss: 90.51473236083984 = 1.0503478050231934 + 10.0 * 8.946438789367676
Epoch 2240, val loss: 1.0513108968734741
Epoch 2250, training loss: 90.5399398803711 = 1.0503019094467163 + 10.0 * 8.94896411895752
Epoch 2250, val loss: 1.0512721538543701
Epoch 2260, training loss: 90.5701904296875 = 1.0502727031707764 + 10.0 * 8.95199203491211
Epoch 2260, val loss: 1.0512408018112183
Epoch 2270, training loss: 90.55670928955078 = 1.0501857995986938 + 10.0 * 8.950652122497559
Epoch 2270, val loss: 1.0511507987976074
Epoch 2280, training loss: 90.57682800292969 = 1.0501458644866943 + 10.0 * 8.952668190002441
Epoch 2280, val loss: 1.0511163473129272
Epoch 2290, training loss: 90.5864028930664 = 1.050078272819519 + 10.0 * 8.953632354736328
Epoch 2290, val loss: 1.0510528087615967
Epoch 2300, training loss: 90.57845306396484 = 1.0500224828720093 + 10.0 * 8.952842712402344
Epoch 2300, val loss: 1.0510094165802002
Epoch 2310, training loss: 90.60285186767578 = 1.0499922037124634 + 10.0 * 8.955286026000977
Epoch 2310, val loss: 1.0509682893753052
Epoch 2320, training loss: 90.62543487548828 = 1.0499460697174072 + 10.0 * 8.957548141479492
Epoch 2320, val loss: 1.0509181022644043
Epoch 2330, training loss: 90.61693572998047 = 1.0498677492141724 + 10.0 * 8.956707000732422
Epoch 2330, val loss: 1.0508556365966797
Epoch 2340, training loss: 90.625732421875 = 1.049816370010376 + 10.0 * 8.95759105682373
Epoch 2340, val loss: 1.0508077144622803
Epoch 2350, training loss: 90.63077545166016 = 1.0497496128082275 + 10.0 * 8.958102226257324
Epoch 2350, val loss: 1.050726294517517
Epoch 2360, training loss: 90.65087127685547 = 1.0496901273727417 + 10.0 * 8.960118293762207
Epoch 2360, val loss: 1.050687551498413
Epoch 2370, training loss: 90.65415954589844 = 1.0496296882629395 + 10.0 * 8.960453033447266
Epoch 2370, val loss: 1.0506230592727661
Epoch 2380, training loss: 90.68112182617188 = 1.0495812892913818 + 10.0 * 8.963153839111328
Epoch 2380, val loss: 1.050586462020874
Epoch 2390, training loss: 90.68234252929688 = 1.049557089805603 + 10.0 * 8.963277816772461
Epoch 2390, val loss: 1.050545334815979
Epoch 2400, training loss: 90.69558715820312 = 1.0494372844696045 + 10.0 * 8.964614868164062
Epoch 2400, val loss: 1.0504419803619385
Epoch 2410, training loss: 90.69985961914062 = 1.0493942499160767 + 10.0 * 8.965046882629395
Epoch 2410, val loss: 1.0503990650177002
Epoch 2420, training loss: 90.7175064086914 = 1.0493524074554443 + 10.0 * 8.966814994812012
Epoch 2420, val loss: 1.0503634214401245
Epoch 2430, training loss: 90.69827270507812 = 1.0492660999298096 + 10.0 * 8.964900970458984
Epoch 2430, val loss: 1.0502643585205078
Epoch 2440, training loss: 90.70418548583984 = 1.0491832494735718 + 10.0 * 8.965499877929688
Epoch 2440, val loss: 1.0502015352249146
Epoch 2450, training loss: 90.72249603271484 = 1.049148678779602 + 10.0 * 8.967334747314453
Epoch 2450, val loss: 1.0501725673675537
Epoch 2460, training loss: 90.73233795166016 = 1.0490810871124268 + 10.0 * 8.9683256149292
Epoch 2460, val loss: 1.0500941276550293
Epoch 2470, training loss: 90.72525787353516 = 1.0489897727966309 + 10.0 * 8.967626571655273
Epoch 2470, val loss: 1.0500184297561646
Epoch 2480, training loss: 90.76573181152344 = 1.048963189125061 + 10.0 * 8.97167682647705
Epoch 2480, val loss: 1.0499846935272217
Epoch 2490, training loss: 90.80032348632812 = 1.0489258766174316 + 10.0 * 8.975139617919922
Epoch 2490, val loss: 1.0499482154846191
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.4023188405797101
0.8641599652249512
=== training gcn model ===
Epoch 0, training loss: 100.82728576660156 = 1.1131629943847656 + 10.0 * 9.97141170501709
Epoch 0, val loss: 1.1118861436843872
Epoch 10, training loss: 96.17523956298828 = 1.1091570854187012 + 10.0 * 9.506608963012695
Epoch 10, val loss: 1.1078732013702393
Epoch 20, training loss: 94.35359191894531 = 1.1051762104034424 + 10.0 * 9.324841499328613
Epoch 20, val loss: 1.1039187908172607
Epoch 30, training loss: 93.04261779785156 = 1.101356863975525 + 10.0 * 9.19412612915039
Epoch 30, val loss: 1.100131630897522
Epoch 40, training loss: 92.0508804321289 = 1.0977349281311035 + 10.0 * 9.095314979553223
Epoch 40, val loss: 1.0965490341186523
Epoch 50, training loss: 91.24793243408203 = 1.0943214893341064 + 10.0 * 9.015360832214355
Epoch 50, val loss: 1.0931735038757324
Epoch 60, training loss: 90.57862091064453 = 1.0910773277282715 + 10.0 * 8.94875431060791
Epoch 60, val loss: 1.0899666547775269
Epoch 70, training loss: 90.0213394165039 = 1.088006854057312 + 10.0 * 8.893333435058594
Epoch 70, val loss: 1.086934208869934
Epoch 80, training loss: 89.55987548828125 = 1.08510160446167 + 10.0 * 8.847477912902832
Epoch 80, val loss: 1.084072232246399
Epoch 90, training loss: 89.17390441894531 = 1.0823566913604736 + 10.0 * 8.809154510498047
Epoch 90, val loss: 1.0813707113265991
Epoch 100, training loss: 88.83316040039062 = 1.0797568559646606 + 10.0 * 8.77534008026123
Epoch 100, val loss: 1.0788193941116333
Epoch 110, training loss: 88.55282592773438 = 1.0773239135742188 + 10.0 * 8.747550010681152
Epoch 110, val loss: 1.0764355659484863
Epoch 120, training loss: 88.30007934570312 = 1.0750383138656616 + 10.0 * 8.722504615783691
Epoch 120, val loss: 1.0742052793502808
Epoch 130, training loss: 88.07737731933594 = 1.0728963613510132 + 10.0 * 8.700448036193848
Epoch 130, val loss: 1.072121500968933
Epoch 140, training loss: 87.88396453857422 = 1.0709103345870972 + 10.0 * 8.681305885314941
Epoch 140, val loss: 1.0702003240585327
Epoch 150, training loss: 87.70365142822266 = 1.0690586566925049 + 10.0 * 8.663458824157715
Epoch 150, val loss: 1.068416714668274
Epoch 160, training loss: 87.55821990966797 = 1.0673578977584839 + 10.0 * 8.649085998535156
Epoch 160, val loss: 1.0667833089828491
Epoch 170, training loss: 87.43123626708984 = 1.065801739692688 + 10.0 * 8.636543273925781
Epoch 170, val loss: 1.065304160118103
Epoch 180, training loss: 87.32965087890625 = 1.0643779039382935 + 10.0 * 8.626527786254883
Epoch 180, val loss: 1.0639609098434448
Epoch 190, training loss: 87.2558364868164 = 1.0630930662155151 + 10.0 * 8.619274139404297
Epoch 190, val loss: 1.0627477169036865
Epoch 200, training loss: 87.16053771972656 = 1.061946153640747 + 10.0 * 8.609859466552734
Epoch 200, val loss: 1.0616847276687622
Epoch 210, training loss: 87.09933471679688 = 1.0609288215637207 + 10.0 * 8.603840827941895
Epoch 210, val loss: 1.0607472658157349
Epoch 220, training loss: 87.03443908691406 = 1.0600188970565796 + 10.0 * 8.597441673278809
Epoch 220, val loss: 1.0599184036254883
Epoch 230, training loss: 86.98252868652344 = 1.0592241287231445 + 10.0 * 8.592329978942871
Epoch 230, val loss: 1.0592001676559448
Epoch 240, training loss: 86.93660736083984 = 1.058523178100586 + 10.0 * 8.587808609008789
Epoch 240, val loss: 1.0585711002349854
Epoch 250, training loss: 86.90234375 = 1.0579113960266113 + 10.0 * 8.584443092346191
Epoch 250, val loss: 1.0580390691757202
Epoch 260, training loss: 86.85871887207031 = 1.0574215650558472 + 10.0 * 8.580129623413086
Epoch 260, val loss: 1.0576057434082031
Epoch 270, training loss: 86.82179260253906 = 1.0569849014282227 + 10.0 * 8.576480865478516
Epoch 270, val loss: 1.0572391748428345
Epoch 280, training loss: 86.81757354736328 = 1.0566318035125732 + 10.0 * 8.576093673706055
Epoch 280, val loss: 1.0569427013397217
Epoch 290, training loss: 86.781005859375 = 1.056317925453186 + 10.0 * 8.572468757629395
Epoch 290, val loss: 1.0566790103912354
Epoch 300, training loss: 86.7840347290039 = 1.056077480316162 + 10.0 * 8.572795867919922
Epoch 300, val loss: 1.0564863681793213
Epoch 310, training loss: 86.7111587524414 = 1.0558414459228516 + 10.0 * 8.565531730651855
Epoch 310, val loss: 1.056304931640625
Epoch 320, training loss: 86.76484680175781 = 1.055694580078125 + 10.0 * 8.570915222167969
Epoch 320, val loss: 1.0561903715133667
Epoch 330, training loss: 86.70465087890625 = 1.0555588006973267 + 10.0 * 8.564908981323242
Epoch 330, val loss: 1.0560894012451172
Epoch 340, training loss: 86.73184204101562 = 1.0554563999176025 + 10.0 * 8.567638397216797
Epoch 340, val loss: 1.0560189485549927
Epoch 350, training loss: 86.71639251708984 = 1.0553642511367798 + 10.0 * 8.566102981567383
Epoch 350, val loss: 1.0559536218643188
Epoch 360, training loss: 86.70600128173828 = 1.0552808046340942 + 10.0 * 8.565072059631348
Epoch 360, val loss: 1.055890679359436
Epoch 370, training loss: 86.68448638916016 = 1.0552209615707397 + 10.0 * 8.562926292419434
Epoch 370, val loss: 1.0558472871780396
Epoch 380, training loss: 86.68401336669922 = 1.0551570653915405 + 10.0 * 8.562885284423828
Epoch 380, val loss: 1.0558016300201416
Epoch 390, training loss: 86.67929077148438 = 1.055116057395935 + 10.0 * 8.562417984008789
Epoch 390, val loss: 1.055768370628357
Epoch 400, training loss: 86.730712890625 = 1.0550893545150757 + 10.0 * 8.567562103271484
Epoch 400, val loss: 1.0557581186294556
Epoch 410, training loss: 86.71956634521484 = 1.0550646781921387 + 10.0 * 8.566450119018555
Epoch 410, val loss: 1.0557518005371094
Epoch 420, training loss: 86.7088623046875 = 1.0550388097763062 + 10.0 * 8.56538200378418
Epoch 420, val loss: 1.0557420253753662
Epoch 430, training loss: 86.70268249511719 = 1.0550051927566528 + 10.0 * 8.564767837524414
Epoch 430, val loss: 1.0557153224945068
Epoch 440, training loss: 86.71096801757812 = 1.0549832582473755 + 10.0 * 8.565598487854004
Epoch 440, val loss: 1.0557008981704712
Epoch 450, training loss: 86.75210571289062 = 1.0549613237380981 + 10.0 * 8.569714546203613
Epoch 450, val loss: 1.055681586265564
Epoch 460, training loss: 86.7217788696289 = 1.0549312829971313 + 10.0 * 8.56668472290039
Epoch 460, val loss: 1.0556652545928955
Epoch 470, training loss: 86.74569702148438 = 1.0549191236495972 + 10.0 * 8.56907844543457
Epoch 470, val loss: 1.0556538105010986
Epoch 480, training loss: 86.76820373535156 = 1.0548937320709229 + 10.0 * 8.571331024169922
Epoch 480, val loss: 1.0556252002716064
Epoch 490, training loss: 86.75993347167969 = 1.0548702478408813 + 10.0 * 8.57050609588623
Epoch 490, val loss: 1.0556081533432007
Epoch 500, training loss: 86.79461669921875 = 1.0548546314239502 + 10.0 * 8.573976516723633
Epoch 500, val loss: 1.0555994510650635
Epoch 510, training loss: 86.82405853271484 = 1.0548474788665771 + 10.0 * 8.576921463012695
Epoch 510, val loss: 1.0555903911590576
Epoch 520, training loss: 86.84697723388672 = 1.0548279285430908 + 10.0 * 8.579215049743652
Epoch 520, val loss: 1.0555707216262817
Epoch 530, training loss: 86.85028839111328 = 1.0548064708709717 + 10.0 * 8.579547882080078
Epoch 530, val loss: 1.055553913116455
Epoch 540, training loss: 86.85381317138672 = 1.0547846555709839 + 10.0 * 8.579902648925781
Epoch 540, val loss: 1.0555318593978882
Epoch 550, training loss: 86.87400817871094 = 1.054774284362793 + 10.0 * 8.581923484802246
Epoch 550, val loss: 1.0555272102355957
Epoch 560, training loss: 86.83674621582031 = 1.0547207593917847 + 10.0 * 8.578203201293945
Epoch 560, val loss: 1.0554780960083008
Epoch 570, training loss: 86.88749694824219 = 1.0547298192977905 + 10.0 * 8.583276748657227
Epoch 570, val loss: 1.0554832220077515
Epoch 580, training loss: 86.9333724975586 = 1.0547325611114502 + 10.0 * 8.58786392211914
Epoch 580, val loss: 1.0554838180541992
Epoch 590, training loss: 86.91865539550781 = 1.0547096729278564 + 10.0 * 8.586394309997559
Epoch 590, val loss: 1.0554598569869995
Epoch 600, training loss: 86.96537017822266 = 1.0546966791152954 + 10.0 * 8.59106731414795
Epoch 600, val loss: 1.055448293685913
Epoch 610, training loss: 86.93089294433594 = 1.0546658039093018 + 10.0 * 8.58762264251709
Epoch 610, val loss: 1.055419921875
Epoch 620, training loss: 86.92589569091797 = 1.0546530485153198 + 10.0 * 8.587124824523926
Epoch 620, val loss: 1.0554007291793823
Epoch 630, training loss: 86.980224609375 = 1.0546550750732422 + 10.0 * 8.592556953430176
Epoch 630, val loss: 1.055406093597412
Epoch 640, training loss: 86.99942779541016 = 1.0546334981918335 + 10.0 * 8.59447956085205
Epoch 640, val loss: 1.0553901195526123
Epoch 650, training loss: 87.01914978027344 = 1.0546241998672485 + 10.0 * 8.596452713012695
Epoch 650, val loss: 1.0553786754608154
Epoch 660, training loss: 87.0455093383789 = 1.054614543914795 + 10.0 * 8.599089622497559
Epoch 660, val loss: 1.0553638935089111
Epoch 670, training loss: 87.04176330566406 = 1.0545859336853027 + 10.0 * 8.59871768951416
Epoch 670, val loss: 1.0553381443023682
Epoch 680, training loss: 87.086181640625 = 1.0545822381973267 + 10.0 * 8.60315990447998
Epoch 680, val loss: 1.0553300380706787
Epoch 690, training loss: 87.10472106933594 = 1.0545622110366821 + 10.0 * 8.605015754699707
Epoch 690, val loss: 1.0553168058395386
Epoch 700, training loss: 87.12879943847656 = 1.0545722246170044 + 10.0 * 8.607422828674316
Epoch 700, val loss: 1.0553113222122192
Epoch 710, training loss: 87.1050033569336 = 1.0544826984405518 + 10.0 * 8.60505199432373
Epoch 710, val loss: 1.0552339553833008
Epoch 720, training loss: 86.9671630859375 = 1.0543805360794067 + 10.0 * 8.591278076171875
Epoch 720, val loss: 1.055185317993164
Epoch 730, training loss: 87.11705017089844 = 1.054207444190979 + 10.0 * 8.606284141540527
Epoch 730, val loss: 1.0548399686813354
Epoch 740, training loss: 86.94051361083984 = 1.0541696548461914 + 10.0 * 8.588634490966797
Epoch 740, val loss: 1.0549994707107544
Epoch 750, training loss: 87.14595794677734 = 1.0543360710144043 + 10.0 * 8.609162330627441
Epoch 750, val loss: 1.055076241493225
Epoch 760, training loss: 86.97049713134766 = 1.0542932748794556 + 10.0 * 8.591620445251465
Epoch 760, val loss: 1.0550742149353027
Epoch 770, training loss: 86.95025634765625 = 1.054271936416626 + 10.0 * 8.589597702026367
Epoch 770, val loss: 1.0550339221954346
Epoch 780, training loss: 87.01600646972656 = 1.054302453994751 + 10.0 * 8.596170425415039
Epoch 780, val loss: 1.055068850517273
Epoch 790, training loss: 87.06603240966797 = 1.0543078184127808 + 10.0 * 8.60117244720459
Epoch 790, val loss: 1.0550734996795654
Epoch 800, training loss: 87.1001205444336 = 1.0543015003204346 + 10.0 * 8.604581832885742
Epoch 800, val loss: 1.055062174797058
Epoch 810, training loss: 87.1370620727539 = 1.0542962551116943 + 10.0 * 8.6082763671875
Epoch 810, val loss: 1.0550590753555298
Epoch 820, training loss: 87.18279266357422 = 1.0542848110198975 + 10.0 * 8.6128511428833
Epoch 820, val loss: 1.0550481081008911
Epoch 830, training loss: 87.19359588623047 = 1.054258942604065 + 10.0 * 8.613933563232422
Epoch 830, val loss: 1.0550274848937988
Epoch 840, training loss: 87.20674896240234 = 1.0542346239089966 + 10.0 * 8.615251541137695
Epoch 840, val loss: 1.0549813508987427
Epoch 850, training loss: 87.22240447998047 = 1.0541876554489136 + 10.0 * 8.616822242736816
Epoch 850, val loss: 1.0549408197402954
Epoch 860, training loss: 87.26897430419922 = 1.0541889667510986 + 10.0 * 8.621478080749512
Epoch 860, val loss: 1.0549496412277222
Epoch 870, training loss: 87.26329040527344 = 1.0541725158691406 + 10.0 * 8.620912551879883
Epoch 870, val loss: 1.054939866065979
Epoch 880, training loss: 87.2175521850586 = 1.0541139841079712 + 10.0 * 8.61634349822998
Epoch 880, val loss: 1.054884433746338
Epoch 890, training loss: 87.29251861572266 = 1.0541213750839233 + 10.0 * 8.623839378356934
Epoch 890, val loss: 1.0548895597457886
Epoch 900, training loss: 87.30272674560547 = 1.0540966987609863 + 10.0 * 8.624862670898438
Epoch 900, val loss: 1.0548712015151978
Epoch 910, training loss: 87.32964324951172 = 1.054072618484497 + 10.0 * 8.627557754516602
Epoch 910, val loss: 1.0548515319824219
Epoch 920, training loss: 87.36170959472656 = 1.054060459136963 + 10.0 * 8.630764961242676
Epoch 920, val loss: 1.0548309087753296
Epoch 930, training loss: 87.38728332519531 = 1.0540415048599243 + 10.0 * 8.63332462310791
Epoch 930, val loss: 1.054818868637085
Epoch 940, training loss: 87.385986328125 = 1.0540101528167725 + 10.0 * 8.633197784423828
Epoch 940, val loss: 1.0547912120819092
Epoch 950, training loss: 87.38058471679688 = 1.0539671182632446 + 10.0 * 8.632661819458008
Epoch 950, val loss: 1.0547534227371216
Epoch 960, training loss: 87.40823364257812 = 1.053943395614624 + 10.0 * 8.635429382324219
Epoch 960, val loss: 1.0547349452972412
Epoch 970, training loss: 87.4518051147461 = 1.0539463758468628 + 10.0 * 8.639785766601562
Epoch 970, val loss: 1.0547298192977905
Epoch 980, training loss: 87.43563079833984 = 1.053910255432129 + 10.0 * 8.638172149658203
Epoch 980, val loss: 1.0546948909759521
Epoch 990, training loss: 87.47579193115234 = 1.0538945198059082 + 10.0 * 8.642189979553223
Epoch 990, val loss: 1.054681658744812
Epoch 1000, training loss: 87.49098205566406 = 1.0538716316223145 + 10.0 * 8.64371109008789
Epoch 1000, val loss: 1.0546613931655884
Epoch 1010, training loss: 87.52882385253906 = 1.0538609027862549 + 10.0 * 8.647496223449707
Epoch 1010, val loss: 1.0546530485153198
Epoch 1020, training loss: 87.53598022460938 = 1.0538315773010254 + 10.0 * 8.648214340209961
Epoch 1020, val loss: 1.0546263456344604
Epoch 1030, training loss: 87.5563735961914 = 1.053809642791748 + 10.0 * 8.650256156921387
Epoch 1030, val loss: 1.0546073913574219
Epoch 1040, training loss: 87.53924560546875 = 1.0537599325180054 + 10.0 * 8.64854907989502
Epoch 1040, val loss: 1.0545648336410522
Epoch 1050, training loss: 87.57747650146484 = 1.053751826286316 + 10.0 * 8.652372360229492
Epoch 1050, val loss: 1.0545501708984375
Epoch 1060, training loss: 87.5907974243164 = 1.0537338256835938 + 10.0 * 8.653706550598145
Epoch 1060, val loss: 1.0545289516448975
Epoch 1070, training loss: 87.59197998046875 = 1.053698182106018 + 10.0 * 8.653828620910645
Epoch 1070, val loss: 1.0545084476470947
Epoch 1080, training loss: 87.60904693603516 = 1.0536848306655884 + 10.0 * 8.655535697937012
Epoch 1080, val loss: 1.054489254951477
Epoch 1090, training loss: 87.67569732666016 = 1.0536566972732544 + 10.0 * 8.662203788757324
Epoch 1090, val loss: 1.0544378757476807
Epoch 1100, training loss: 87.64570617675781 = 1.0536152124404907 + 10.0 * 8.659209251403809
Epoch 1100, val loss: 1.054410457611084
Epoch 1110, training loss: 87.65933990478516 = 1.053593635559082 + 10.0 * 8.660573959350586
Epoch 1110, val loss: 1.0543937683105469
Epoch 1120, training loss: 87.70652770996094 = 1.0535870790481567 + 10.0 * 8.66529369354248
Epoch 1120, val loss: 1.0543882846832275
Epoch 1130, training loss: 87.77066040039062 = 1.053571343421936 + 10.0 * 8.671709060668945
Epoch 1130, val loss: 1.0543729066848755
Epoch 1140, training loss: 87.73839569091797 = 1.0535277128219604 + 10.0 * 8.668486595153809
Epoch 1140, val loss: 1.0543409585952759
Epoch 1150, training loss: 87.64996337890625 = 1.0534324645996094 + 10.0 * 8.659652709960938
Epoch 1150, val loss: 1.05422842502594
Epoch 1160, training loss: 87.69269561767578 = 1.0534433126449585 + 10.0 * 8.663925170898438
Epoch 1160, val loss: 1.0542430877685547
Epoch 1170, training loss: 87.72563934326172 = 1.05342698097229 + 10.0 * 8.667221069335938
Epoch 1170, val loss: 1.0542316436767578
Epoch 1180, training loss: 87.7815170288086 = 1.0534175634384155 + 10.0 * 8.672809600830078
Epoch 1180, val loss: 1.0542194843292236
Epoch 1190, training loss: 87.7742691040039 = 1.05337655544281 + 10.0 * 8.672089576721191
Epoch 1190, val loss: 1.0541883707046509
Epoch 1200, training loss: 87.79215240478516 = 1.0533435344696045 + 10.0 * 8.673880577087402
Epoch 1200, val loss: 1.0541636943817139
Epoch 1210, training loss: 87.8223648071289 = 1.0533311367034912 + 10.0 * 8.67690372467041
Epoch 1210, val loss: 1.0541430711746216
Epoch 1220, training loss: 87.81731414794922 = 1.0532736778259277 + 10.0 * 8.676403999328613
Epoch 1220, val loss: 1.0540902614593506
Epoch 1230, training loss: 87.83576965332031 = 1.0532530546188354 + 10.0 * 8.678251266479492
Epoch 1230, val loss: 1.0540752410888672
Epoch 1240, training loss: 87.87834167480469 = 1.0532305240631104 + 10.0 * 8.682511329650879
Epoch 1240, val loss: 1.054053544998169
Epoch 1250, training loss: 87.89038848876953 = 1.0532007217407227 + 10.0 * 8.68371868133545
Epoch 1250, val loss: 1.0540122985839844
Epoch 1260, training loss: 87.86275482177734 = 1.053134799003601 + 10.0 * 8.680962562561035
Epoch 1260, val loss: 1.0539544820785522
Epoch 1270, training loss: 87.88372039794922 = 1.0531278848648071 + 10.0 * 8.683058738708496
Epoch 1270, val loss: 1.053938627243042
Epoch 1280, training loss: 87.90496826171875 = 1.0530835390090942 + 10.0 * 8.685188293457031
Epoch 1280, val loss: 1.053904414176941
Epoch 1290, training loss: 87.94393157958984 = 1.0530798435211182 + 10.0 * 8.689085006713867
Epoch 1290, val loss: 1.0539042949676514
Epoch 1300, training loss: 87.95945739746094 = 1.053043007850647 + 10.0 * 8.690641403198242
Epoch 1300, val loss: 1.053873062133789
Epoch 1310, training loss: 87.96324920654297 = 1.053004503250122 + 10.0 * 8.691024780273438
Epoch 1310, val loss: 1.0538243055343628
Epoch 1320, training loss: 87.97705078125 = 1.052966594696045 + 10.0 * 8.692408561706543
Epoch 1320, val loss: 1.0538008213043213
Epoch 1330, training loss: 88.01838684082031 = 1.0529453754425049 + 10.0 * 8.69654369354248
Epoch 1330, val loss: 1.0537734031677246
Epoch 1340, training loss: 88.01969146728516 = 1.0528967380523682 + 10.0 * 8.696680068969727
Epoch 1340, val loss: 1.053723931312561
Epoch 1350, training loss: 87.98020935058594 = 1.05282461643219 + 10.0 * 8.69273853302002
Epoch 1350, val loss: 1.0536595582962036
Epoch 1360, training loss: 87.9964599609375 = 1.0528051853179932 + 10.0 * 8.694365501403809
Epoch 1360, val loss: 1.0536434650421143
Epoch 1370, training loss: 88.0347671508789 = 1.0528008937835693 + 10.0 * 8.698196411132812
Epoch 1370, val loss: 1.053632378578186
Epoch 1380, training loss: 88.06526184082031 = 1.0527796745300293 + 10.0 * 8.701248168945312
Epoch 1380, val loss: 1.0536166429519653
Epoch 1390, training loss: 88.07962036132812 = 1.0527487993240356 + 10.0 * 8.70268726348877
Epoch 1390, val loss: 1.0535881519317627
Epoch 1400, training loss: 88.08391571044922 = 1.0527056455612183 + 10.0 * 8.703121185302734
Epoch 1400, val loss: 1.0535383224487305
Epoch 1410, training loss: 88.07923126220703 = 1.0526374578475952 + 10.0 * 8.702659606933594
Epoch 1410, val loss: 1.0534754991531372
Epoch 1420, training loss: 88.09491729736328 = 1.0526105165481567 + 10.0 * 8.704230308532715
Epoch 1420, val loss: 1.053450107574463
Epoch 1430, training loss: 88.08197784423828 = 1.0525591373443604 + 10.0 * 8.70294189453125
Epoch 1430, val loss: 1.0533970594406128
Epoch 1440, training loss: 87.97166442871094 = 1.0523343086242676 + 10.0 * 8.691932678222656
Epoch 1440, val loss: 1.053173303604126
Epoch 1450, training loss: 87.91529083251953 = 1.0522631406784058 + 10.0 * 8.68630313873291
Epoch 1450, val loss: 1.053101658821106
Epoch 1460, training loss: 88.07310485839844 = 1.0523245334625244 + 10.0 * 8.702077865600586
Epoch 1460, val loss: 1.0531755685806274
Epoch 1470, training loss: 88.06426239013672 = 1.0523202419281006 + 10.0 * 8.701193809509277
Epoch 1470, val loss: 1.0531738996505737
Epoch 1480, training loss: 88.11515045166016 = 1.0523301362991333 + 10.0 * 8.706281661987305
Epoch 1480, val loss: 1.0531786680221558
Epoch 1490, training loss: 88.1632308959961 = 1.0523346662521362 + 10.0 * 8.711089134216309
Epoch 1490, val loss: 1.0531848669052124
Epoch 1500, training loss: 88.19196319580078 = 1.052308440208435 + 10.0 * 8.71396541595459
Epoch 1500, val loss: 1.0531548261642456
Epoch 1510, training loss: 88.2071533203125 = 1.0522727966308594 + 10.0 * 8.71548843383789
Epoch 1510, val loss: 1.0531179904937744
Epoch 1520, training loss: 88.23477935791016 = 1.052250623703003 + 10.0 * 8.718252182006836
Epoch 1520, val loss: 1.0530955791473389
Epoch 1530, training loss: 88.21591186523438 = 1.052180528640747 + 10.0 * 8.716373443603516
Epoch 1530, val loss: 1.053026556968689
Epoch 1540, training loss: 88.25626373291016 = 1.0521482229232788 + 10.0 * 8.72041130065918
Epoch 1540, val loss: 1.0530016422271729
Epoch 1550, training loss: 88.26345825195312 = 1.0521148443222046 + 10.0 * 8.721134185791016
Epoch 1550, val loss: 1.0529658794403076
Epoch 1560, training loss: 88.29264068603516 = 1.052070140838623 + 10.0 * 8.7240571975708
Epoch 1560, val loss: 1.0529283285140991
Epoch 1570, training loss: 88.29042053222656 = 1.052022933959961 + 10.0 * 8.72383975982666
Epoch 1570, val loss: 1.0528773069381714
Epoch 1580, training loss: 88.31961059570312 = 1.0519884824752808 + 10.0 * 8.726762771606445
Epoch 1580, val loss: 1.0528416633605957
Epoch 1590, training loss: 88.33821105957031 = 1.0519376993179321 + 10.0 * 8.72862720489502
Epoch 1590, val loss: 1.052787184715271
Epoch 1600, training loss: 88.3249740600586 = 1.051875352859497 + 10.0 * 8.727310180664062
Epoch 1600, val loss: 1.052730679512024
Epoch 1610, training loss: 88.37870788574219 = 1.0518525838851929 + 10.0 * 8.732686042785645
Epoch 1610, val loss: 1.0527105331420898
Epoch 1620, training loss: 88.39148712158203 = 1.0518075227737427 + 10.0 * 8.733967781066895
Epoch 1620, val loss: 1.05266273021698
Epoch 1630, training loss: 88.22123718261719 = 1.051552176475525 + 10.0 * 8.716968536376953
Epoch 1630, val loss: 1.05247163772583
Epoch 1640, training loss: 88.12057495117188 = 1.0514111518859863 + 10.0 * 8.706915855407715
Epoch 1640, val loss: 1.0523087978363037
Epoch 1650, training loss: 88.1827392578125 = 1.0514397621154785 + 10.0 * 8.713129997253418
Epoch 1650, val loss: 1.0523130893707275
Epoch 1660, training loss: 88.26315307617188 = 1.051476001739502 + 10.0 * 8.72116756439209
Epoch 1660, val loss: 1.0523483753204346
Epoch 1670, training loss: 88.31696319580078 = 1.0514909029006958 + 10.0 * 8.726547241210938
Epoch 1670, val loss: 1.0523688793182373
Epoch 1680, training loss: 88.3423843383789 = 1.0514774322509766 + 10.0 * 8.729090690612793
Epoch 1680, val loss: 1.0523512363433838
Epoch 1690, training loss: 88.37710571289062 = 1.0514496564865112 + 10.0 * 8.732564926147461
Epoch 1690, val loss: 1.052320122718811
Epoch 1700, training loss: 88.37901306152344 = 1.0514061450958252 + 10.0 * 8.732760429382324
Epoch 1700, val loss: 1.0522785186767578
Epoch 1710, training loss: 88.388671875 = 1.0513577461242676 + 10.0 * 8.733731269836426
Epoch 1710, val loss: 1.0522292852401733
Epoch 1720, training loss: 88.41064453125 = 1.0513026714324951 + 10.0 * 8.735934257507324
Epoch 1720, val loss: 1.052178978919983
Epoch 1730, training loss: 88.42781829833984 = 1.0512632131576538 + 10.0 * 8.737655639648438
Epoch 1730, val loss: 1.052147388458252
Epoch 1740, training loss: 88.4310302734375 = 1.0512007474899292 + 10.0 * 8.737982749938965
Epoch 1740, val loss: 1.0520856380462646
Epoch 1750, training loss: 88.44905853271484 = 1.0511523485183716 + 10.0 * 8.739789962768555
Epoch 1750, val loss: 1.0520397424697876
Epoch 1760, training loss: 88.4708480834961 = 1.0511082410812378 + 10.0 * 8.741973876953125
Epoch 1760, val loss: 1.0520025491714478
Epoch 1770, training loss: 88.4886474609375 = 1.0510615110397339 + 10.0 * 8.743758201599121
Epoch 1770, val loss: 1.051958680152893
Epoch 1780, training loss: 88.47605895996094 = 1.0509719848632812 + 10.0 * 8.742508888244629
Epoch 1780, val loss: 1.0518747568130493
Epoch 1790, training loss: 88.49526977539062 = 1.0509300231933594 + 10.0 * 8.744433403015137
Epoch 1790, val loss: 1.051823377609253
Epoch 1800, training loss: 88.52630615234375 = 1.0508906841278076 + 10.0 * 8.747541427612305
Epoch 1800, val loss: 1.0517923831939697
Epoch 1810, training loss: 88.53272247314453 = 1.0508458614349365 + 10.0 * 8.748188018798828
Epoch 1810, val loss: 1.051750659942627
Epoch 1820, training loss: 88.53182983398438 = 1.0507577657699585 + 10.0 * 8.748106956481934
Epoch 1820, val loss: 1.051659345626831
Epoch 1830, training loss: 88.5395278930664 = 1.050707221031189 + 10.0 * 8.748882293701172
Epoch 1830, val loss: 1.0516177415847778
Epoch 1840, training loss: 88.56439971923828 = 1.0506638288497925 + 10.0 * 8.751373291015625
Epoch 1840, val loss: 1.051571249961853
Epoch 1850, training loss: 88.57904815673828 = 1.0506025552749634 + 10.0 * 8.752843856811523
Epoch 1850, val loss: 1.051511287689209
Epoch 1860, training loss: 88.59615325927734 = 1.050560712814331 + 10.0 * 8.754559516906738
Epoch 1860, val loss: 1.0514556169509888
Epoch 1870, training loss: 88.52540588378906 = 1.0504019260406494 + 10.0 * 8.7475004196167
Epoch 1870, val loss: 1.0513097047805786
Epoch 1880, training loss: 88.57656860351562 = 1.050386667251587 + 10.0 * 8.752618789672852
Epoch 1880, val loss: 1.051308035850525
Epoch 1890, training loss: 88.59423828125 = 1.050333857536316 + 10.0 * 8.754390716552734
Epoch 1890, val loss: 1.0512622594833374
Epoch 1900, training loss: 88.6329574584961 = 1.0503123998641968 + 10.0 * 8.758264541625977
Epoch 1900, val loss: 1.0512338876724243
Epoch 1910, training loss: 88.64715576171875 = 1.050252079963684 + 10.0 * 8.759690284729004
Epoch 1910, val loss: 1.0511581897735596
Epoch 1920, training loss: 88.6397933959961 = 1.0501784086227417 + 10.0 * 8.75896167755127
Epoch 1920, val loss: 1.0511071681976318
Epoch 1930, training loss: 88.67851257324219 = 1.0501235723495483 + 10.0 * 8.762838363647461
Epoch 1930, val loss: 1.0510483980178833
Epoch 1940, training loss: 88.66706848144531 = 1.0500394105911255 + 10.0 * 8.761702537536621
Epoch 1940, val loss: 1.0509761571884155
Epoch 1950, training loss: 88.66556549072266 = 1.0499674081802368 + 10.0 * 8.761560440063477
Epoch 1950, val loss: 1.0509003400802612
Epoch 1960, training loss: 88.68955993652344 = 1.0499141216278076 + 10.0 * 8.763964653015137
Epoch 1960, val loss: 1.0508500337600708
Epoch 1970, training loss: 88.68984985351562 = 1.0498517751693726 + 10.0 * 8.763999938964844
Epoch 1970, val loss: 1.0507996082305908
Epoch 1980, training loss: 88.72982025146484 = 1.049813985824585 + 10.0 * 8.768000602722168
Epoch 1980, val loss: 1.050758719444275
Epoch 1990, training loss: 88.75131225585938 = 1.0497487783432007 + 10.0 * 8.770155906677246
Epoch 1990, val loss: 1.050682544708252
Epoch 2000, training loss: 88.75284576416016 = 1.0496026277542114 + 10.0 * 8.770323753356934
Epoch 2000, val loss: 1.0504764318466187
Epoch 2010, training loss: 88.5755386352539 = 1.0493971109390259 + 10.0 * 8.75261402130127
Epoch 2010, val loss: 1.0502537488937378
Epoch 2020, training loss: 88.5459976196289 = 1.0492489337921143 + 10.0 * 8.749674797058105
Epoch 2020, val loss: 1.0502065420150757
Epoch 2030, training loss: 88.58805084228516 = 1.0492315292358398 + 10.0 * 8.753881454467773
Epoch 2030, val loss: 1.0501813888549805
Epoch 2040, training loss: 88.62347412109375 = 1.0491982698440552 + 10.0 * 8.757427215576172
Epoch 2040, val loss: 1.0501925945281982
Epoch 2050, training loss: 88.65874481201172 = 1.0491852760314941 + 10.0 * 8.760955810546875
Epoch 2050, val loss: 1.050169587135315
Epoch 2060, training loss: 88.70185852050781 = 1.0491644144058228 + 10.0 * 8.76526927947998
Epoch 2060, val loss: 1.0501506328582764
Epoch 2070, training loss: 88.729736328125 = 1.0491312742233276 + 10.0 * 8.768060684204102
Epoch 2070, val loss: 1.05010986328125
Epoch 2080, training loss: 88.73719024658203 = 1.049054503440857 + 10.0 * 8.768813133239746
Epoch 2080, val loss: 1.0500354766845703
Epoch 2090, training loss: 88.73792266845703 = 1.0489763021469116 + 10.0 * 8.768895149230957
Epoch 2090, val loss: 1.0499697923660278
Epoch 2100, training loss: 88.761962890625 = 1.0489022731781006 + 10.0 * 8.771306037902832
Epoch 2100, val loss: 1.0499063730239868
Epoch 2110, training loss: 88.7778091430664 = 1.0488560199737549 + 10.0 * 8.772894859313965
Epoch 2110, val loss: 1.0498648881912231
Epoch 2120, training loss: 88.79522705078125 = 1.0487706661224365 + 10.0 * 8.774645805358887
Epoch 2120, val loss: 1.0497838258743286
Epoch 2130, training loss: 88.8042221069336 = 1.0486961603164673 + 10.0 * 8.775552749633789
Epoch 2130, val loss: 1.049699306488037
Epoch 2140, training loss: 88.81157684326172 = 1.048625111579895 + 10.0 * 8.77629566192627
Epoch 2140, val loss: 1.0496395826339722
Epoch 2150, training loss: 88.82465362548828 = 1.048557162284851 + 10.0 * 8.777609825134277
Epoch 2150, val loss: 1.0495760440826416
Epoch 2160, training loss: 88.826416015625 = 1.0484626293182373 + 10.0 * 8.777795791625977
Epoch 2160, val loss: 1.0494871139526367
Epoch 2170, training loss: 88.8375015258789 = 1.0483884811401367 + 10.0 * 8.778911590576172
Epoch 2170, val loss: 1.0494128465652466
Epoch 2180, training loss: 88.8633041381836 = 1.048330545425415 + 10.0 * 8.78149700164795
Epoch 2180, val loss: 1.0493388175964355
Epoch 2190, training loss: 88.85870361328125 = 1.0482475757598877 + 10.0 * 8.781045913696289
Epoch 2190, val loss: 1.0492722988128662
Epoch 2200, training loss: 88.87699127197266 = 1.0482062101364136 + 10.0 * 8.782878875732422
Epoch 2200, val loss: 1.0492408275604248
Epoch 2210, training loss: 88.92362213134766 = 1.0481805801391602 + 10.0 * 8.787544250488281
Epoch 2210, val loss: 1.0492043495178223
Epoch 2220, training loss: 88.92910766601562 = 1.048099398612976 + 10.0 * 8.788101196289062
Epoch 2220, val loss: 1.049090027809143
Epoch 2230, training loss: 88.89986419677734 = 1.0479837656021118 + 10.0 * 8.785188674926758
Epoch 2230, val loss: 1.0489813089370728
Epoch 2240, training loss: 88.85726928710938 = 1.047827959060669 + 10.0 * 8.780943870544434
Epoch 2240, val loss: 1.0488440990447998
Epoch 2250, training loss: 88.87751770019531 = 1.0477098226547241 + 10.0 * 8.782980918884277
Epoch 2250, val loss: 1.0487390756607056
Epoch 2260, training loss: 88.887939453125 = 1.0476341247558594 + 10.0 * 8.784029960632324
Epoch 2260, val loss: 1.0487024784088135
Epoch 2270, training loss: 88.93063354492188 = 1.0476305484771729 + 10.0 * 8.788300514221191
Epoch 2270, val loss: 1.0486823320388794
Epoch 2280, training loss: 88.9621353149414 = 1.0475664138793945 + 10.0 * 8.79145622253418
Epoch 2280, val loss: 1.0486115217208862
Epoch 2290, training loss: 88.97158813476562 = 1.0474791526794434 + 10.0 * 8.792410850524902
Epoch 2290, val loss: 1.0485248565673828
Epoch 2300, training loss: 88.98577880859375 = 1.0473986864089966 + 10.0 * 8.793837547302246
Epoch 2300, val loss: 1.0484431982040405
Epoch 2310, training loss: 89.0019302368164 = 1.0473004579544067 + 10.0 * 8.795462608337402
Epoch 2310, val loss: 1.0483489036560059
Epoch 2320, training loss: 89.0155029296875 = 1.0472385883331299 + 10.0 * 8.796826362609863
Epoch 2320, val loss: 1.0482920408248901
Epoch 2330, training loss: 88.97553253173828 = 1.0470736026763916 + 10.0 * 8.792845726013184
Epoch 2330, val loss: 1.0481771230697632
Epoch 2340, training loss: 89.01372528076172 = 1.047028660774231 + 10.0 * 8.796669960021973
Epoch 2340, val loss: 1.0481053590774536
Epoch 2350, training loss: 89.04422760009766 = 1.0469874143600464 + 10.0 * 8.799723625183105
Epoch 2350, val loss: 1.0480667352676392
Epoch 2360, training loss: 89.05416870117188 = 1.046905755996704 + 10.0 * 8.800725936889648
Epoch 2360, val loss: 1.0479823350906372
Epoch 2370, training loss: 89.06504821777344 = 1.0468134880065918 + 10.0 * 8.801823616027832
Epoch 2370, val loss: 1.047873616218567
Epoch 2380, training loss: 88.94725036621094 = 1.04629385471344 + 10.0 * 8.790095329284668
Epoch 2380, val loss: 1.0474880933761597
Epoch 2390, training loss: 88.8678207397461 = 1.046121597290039 + 10.0 * 8.782170295715332
Epoch 2390, val loss: 1.0472872257232666
Epoch 2400, training loss: 88.8929443359375 = 1.0461398363113403 + 10.0 * 8.784680366516113
Epoch 2400, val loss: 1.0472192764282227
Epoch 2410, training loss: 88.89046478271484 = 1.046013593673706 + 10.0 * 8.784444808959961
Epoch 2410, val loss: 1.0470064878463745
Epoch 2420, training loss: 88.98564147949219 = 1.0459665060043335 + 10.0 * 8.793967247009277
Epoch 2420, val loss: 1.047029733657837
Epoch 2430, training loss: 88.7599105834961 = 1.045403242111206 + 10.0 * 8.771450996398926
Epoch 2430, val loss: 1.04656982421875
Epoch 2440, training loss: 88.87760925292969 = 1.045614242553711 + 10.0 * 8.783199310302734
Epoch 2440, val loss: 1.046651840209961
Epoch 2450, training loss: 88.83667755126953 = 1.0455336570739746 + 10.0 * 8.779114723205566
Epoch 2450, val loss: 1.0466854572296143
Epoch 2460, training loss: 88.8792953491211 = 1.0454798936843872 + 10.0 * 8.783381462097168
Epoch 2460, val loss: 1.0466171503067017
Epoch 2470, training loss: 88.93688201904297 = 1.0454977750778198 + 10.0 * 8.789138793945312
Epoch 2470, val loss: 1.0466351509094238
Epoch 2480, training loss: 88.97663879394531 = 1.0454967021942139 + 10.0 * 8.79311466217041
Epoch 2480, val loss: 1.0466264486312866
Epoch 2490, training loss: 89.01605224609375 = 1.0454660654067993 + 10.0 * 8.797059059143066
Epoch 2490, val loss: 1.0465983152389526
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.41884057971014493
0.8632905890023909
=== training gcn model ===
Epoch 0, training loss: 102.15448760986328 = 1.1187660694122314 + 10.0 * 10.103571891784668
Epoch 0, val loss: 1.1169596910476685
Epoch 10, training loss: 96.85567474365234 = 1.114106297492981 + 10.0 * 9.574156761169434
Epoch 10, val loss: 1.1123552322387695
Epoch 20, training loss: 94.90592193603516 = 1.1096755266189575 + 10.0 * 9.37962532043457
Epoch 20, val loss: 1.1079905033111572
Epoch 30, training loss: 93.60591125488281 = 1.1054438352584839 + 10.0 * 9.250046730041504
Epoch 30, val loss: 1.1038163900375366
Epoch 40, training loss: 92.65528106689453 = 1.101406216621399 + 10.0 * 9.155386924743652
Epoch 40, val loss: 1.0998426675796509
Epoch 50, training loss: 91.8931884765625 = 1.0975823402404785 + 10.0 * 9.079561233520508
Epoch 50, val loss: 1.096086025238037
Epoch 60, training loss: 91.25106048583984 = 1.0939562320709229 + 10.0 * 9.015710830688477
Epoch 60, val loss: 1.0925272703170776
Epoch 70, training loss: 90.70565032958984 = 1.0905191898345947 + 10.0 * 8.961512565612793
Epoch 70, val loss: 1.0891621112823486
Epoch 80, training loss: 90.23249053955078 = 1.0872715711593628 + 10.0 * 8.914522171020508
Epoch 80, val loss: 1.0859891176223755
Epoch 90, training loss: 89.81478118896484 = 1.0842067003250122 + 10.0 * 8.87305736541748
Epoch 90, val loss: 1.0830003023147583
Epoch 100, training loss: 89.45314025878906 = 1.0813255310058594 + 10.0 * 8.837181091308594
Epoch 100, val loss: 1.0801992416381836
Epoch 110, training loss: 89.13139343261719 = 1.0786075592041016 + 10.0 * 8.805278778076172
Epoch 110, val loss: 1.0775671005249023
Epoch 120, training loss: 88.87318420410156 = 1.0760798454284668 + 10.0 * 8.77971076965332
Epoch 120, val loss: 1.0751274824142456
Epoch 130, training loss: 88.64120483398438 = 1.0737290382385254 + 10.0 * 8.756747245788574
Epoch 130, val loss: 1.0728645324707031
Epoch 140, training loss: 88.4550552368164 = 1.071550965309143 + 10.0 * 8.738350868225098
Epoch 140, val loss: 1.070777416229248
Epoch 150, training loss: 88.26490020751953 = 1.0695366859436035 + 10.0 * 8.719536781311035
Epoch 150, val loss: 1.0688562393188477
Epoch 160, training loss: 88.10899353027344 = 1.0676865577697754 + 10.0 * 8.704130172729492
Epoch 160, val loss: 1.067104697227478
Epoch 170, training loss: 87.98889923095703 = 1.0660325288772583 + 10.0 * 8.692286491394043
Epoch 170, val loss: 1.065542459487915
Epoch 180, training loss: 87.84689331054688 = 1.0645136833190918 + 10.0 * 8.678237915039062
Epoch 180, val loss: 1.064120888710022
Epoch 190, training loss: 87.75203704833984 = 1.063177466392517 + 10.0 * 8.668886184692383
Epoch 190, val loss: 1.0628777742385864
Epoch 200, training loss: 87.65873718261719 = 1.0619698762893677 + 10.0 * 8.659676551818848
Epoch 200, val loss: 1.0617634057998657
Epoch 210, training loss: 87.59922790527344 = 1.0609158277511597 + 10.0 * 8.653831481933594
Epoch 210, val loss: 1.0607969760894775
Epoch 220, training loss: 87.50996398925781 = 1.0599881410598755 + 10.0 * 8.644997596740723
Epoch 220, val loss: 1.059956669807434
Epoch 230, training loss: 87.46978759765625 = 1.0591850280761719 + 10.0 * 8.641059875488281
Epoch 230, val loss: 1.059227466583252
Epoch 240, training loss: 87.44355010986328 = 1.058501124382019 + 10.0 * 8.638504981994629
Epoch 240, val loss: 1.0586237907409668
Epoch 250, training loss: 87.37371063232422 = 1.0579036474227905 + 10.0 * 8.631580352783203
Epoch 250, val loss: 1.0581010580062866
Epoch 260, training loss: 87.32617950439453 = 1.057403564453125 + 10.0 * 8.626877784729004
Epoch 260, val loss: 1.0576649904251099
Epoch 270, training loss: 87.27384185791016 = 1.0569850206375122 + 10.0 * 8.621685981750488
Epoch 270, val loss: 1.0573070049285889
Epoch 280, training loss: 87.24144744873047 = 1.0566319227218628 + 10.0 * 8.618481636047363
Epoch 280, val loss: 1.057003140449524
Epoch 290, training loss: 87.22650909423828 = 1.056348443031311 + 10.0 * 8.617015838623047
Epoch 290, val loss: 1.0567694902420044
Epoch 300, training loss: 87.20098114013672 = 1.0560967922210693 + 10.0 * 8.61448860168457
Epoch 300, val loss: 1.0565733909606934
Epoch 310, training loss: 87.16041564941406 = 1.0559040307998657 + 10.0 * 8.610451698303223
Epoch 310, val loss: 1.0564206838607788
Epoch 320, training loss: 87.14788818359375 = 1.0557514429092407 + 10.0 * 8.609213829040527
Epoch 320, val loss: 1.0562983751296997
Epoch 330, training loss: 87.12390899658203 = 1.0556237697601318 + 10.0 * 8.606828689575195
Epoch 330, val loss: 1.0561997890472412
Epoch 340, training loss: 87.03239440917969 = 1.055467963218689 + 10.0 * 8.597692489624023
Epoch 340, val loss: 1.0560718774795532
Epoch 350, training loss: 87.05738067626953 = 1.0554128885269165 + 10.0 * 8.600196838378906
Epoch 350, val loss: 1.0560272932052612
Epoch 360, training loss: 87.11825561523438 = 1.0553675889968872 + 10.0 * 8.60628890991211
Epoch 360, val loss: 1.0560163259506226
Epoch 370, training loss: 87.05925750732422 = 1.0552902221679688 + 10.0 * 8.600397109985352
Epoch 370, val loss: 1.055962085723877
Epoch 380, training loss: 87.07272338867188 = 1.055256962776184 + 10.0 * 8.601746559143066
Epoch 380, val loss: 1.0559417009353638
Epoch 390, training loss: 87.05338287353516 = 1.0552140474319458 + 10.0 * 8.599817276000977
Epoch 390, val loss: 1.0559146404266357
Epoch 400, training loss: 87.05333709716797 = 1.0551824569702148 + 10.0 * 8.599815368652344
Epoch 400, val loss: 1.0558927059173584
Epoch 410, training loss: 87.05703735351562 = 1.0551546812057495 + 10.0 * 8.600188255310059
Epoch 410, val loss: 1.0558754205703735
Epoch 420, training loss: 87.07174682617188 = 1.055129051208496 + 10.0 * 8.601661682128906
Epoch 420, val loss: 1.0558547973632812
Epoch 430, training loss: 87.13557434082031 = 1.0551042556762695 + 10.0 * 8.608046531677246
Epoch 430, val loss: 1.0558314323425293
Epoch 440, training loss: 87.07230377197266 = 1.0550482273101807 + 10.0 * 8.601725578308105
Epoch 440, val loss: 1.0558104515075684
Epoch 450, training loss: 87.08075714111328 = 1.0550673007965088 + 10.0 * 8.602568626403809
Epoch 450, val loss: 1.0558158159255981
Epoch 460, training loss: 87.08234405517578 = 1.055030345916748 + 10.0 * 8.602731704711914
Epoch 460, val loss: 1.0557934045791626
Epoch 470, training loss: 87.081298828125 = 1.0550267696380615 + 10.0 * 8.602627754211426
Epoch 470, val loss: 1.0557864904403687
Epoch 480, training loss: 87.09944152832031 = 1.0550260543823242 + 10.0 * 8.60444164276123
Epoch 480, val loss: 1.0557904243469238
Epoch 490, training loss: 87.12942504882812 = 1.0550166368484497 + 10.0 * 8.607440948486328
Epoch 490, val loss: 1.0557793378829956
Epoch 500, training loss: 87.12071228027344 = 1.055010199546814 + 10.0 * 8.60657024383545
Epoch 500, val loss: 1.055776834487915
Epoch 510, training loss: 87.12650299072266 = 1.0549914836883545 + 10.0 * 8.60715103149414
Epoch 510, val loss: 1.0557632446289062
Epoch 520, training loss: 87.13579559326172 = 1.0549849271774292 + 10.0 * 8.608080863952637
Epoch 520, val loss: 1.0557605028152466
Epoch 530, training loss: 87.1269302368164 = 1.0549601316452026 + 10.0 * 8.607196807861328
Epoch 530, val loss: 1.0557384490966797
Epoch 540, training loss: 87.0755844116211 = 1.0549290180206299 + 10.0 * 8.602065086364746
Epoch 540, val loss: 1.055709719657898
Epoch 550, training loss: 87.08051300048828 = 1.0549215078353882 + 10.0 * 8.602559089660645
Epoch 550, val loss: 1.0556907653808594
Epoch 560, training loss: 87.13727569580078 = 1.0549296140670776 + 10.0 * 8.608234405517578
Epoch 560, val loss: 1.0557067394256592
Epoch 570, training loss: 87.19414520263672 = 1.0549262762069702 + 10.0 * 8.613922119140625
Epoch 570, val loss: 1.055702567100525
Epoch 580, training loss: 87.19498443603516 = 1.0549204349517822 + 10.0 * 8.614006042480469
Epoch 580, val loss: 1.0556979179382324
Epoch 590, training loss: 87.22965240478516 = 1.0549174547195435 + 10.0 * 8.617473602294922
Epoch 590, val loss: 1.0556918382644653
Epoch 600, training loss: 87.24976348876953 = 1.0549067258834839 + 10.0 * 8.619485855102539
Epoch 600, val loss: 1.0556820631027222
Epoch 610, training loss: 87.24462890625 = 1.0548888444900513 + 10.0 * 8.618974685668945
Epoch 610, val loss: 1.055666446685791
Epoch 620, training loss: 87.28462982177734 = 1.0548814535140991 + 10.0 * 8.62297534942627
Epoch 620, val loss: 1.0556604862213135
Epoch 630, training loss: 87.24386596679688 = 1.054854154586792 + 10.0 * 8.618901252746582
Epoch 630, val loss: 1.0556424856185913
Epoch 640, training loss: 87.2491226196289 = 1.0548301935195923 + 10.0 * 8.619428634643555
Epoch 640, val loss: 1.055609107017517
Epoch 650, training loss: 87.25543212890625 = 1.0548210144042969 + 10.0 * 8.620060920715332
Epoch 650, val loss: 1.0555897951126099
Epoch 660, training loss: 87.29220581054688 = 1.05482816696167 + 10.0 * 8.623738288879395
Epoch 660, val loss: 1.055609941482544
Epoch 670, training loss: 87.31649017333984 = 1.0548267364501953 + 10.0 * 8.626166343688965
Epoch 670, val loss: 1.055607557296753
Epoch 680, training loss: 87.3381118774414 = 1.0548099279403687 + 10.0 * 8.62833023071289
Epoch 680, val loss: 1.0555845499038696
Epoch 690, training loss: 87.3470687866211 = 1.05479896068573 + 10.0 * 8.629226684570312
Epoch 690, val loss: 1.0555751323699951
Epoch 700, training loss: 87.37406158447266 = 1.0547841787338257 + 10.0 * 8.631927490234375
Epoch 700, val loss: 1.0555707216262817
Epoch 710, training loss: 87.39128112792969 = 1.0547740459442139 + 10.0 * 8.633650779724121
Epoch 710, val loss: 1.055556058883667
Epoch 720, training loss: 87.38311767578125 = 1.0547544956207275 + 10.0 * 8.63283634185791
Epoch 720, val loss: 1.0555413961410522
Epoch 730, training loss: 87.41133880615234 = 1.0547518730163574 + 10.0 * 8.635659217834473
Epoch 730, val loss: 1.055541753768921
Epoch 740, training loss: 87.43753051757812 = 1.0547425746917725 + 10.0 * 8.63827896118164
Epoch 740, val loss: 1.0555177927017212
Epoch 750, training loss: 87.40950012207031 = 1.0547101497650146 + 10.0 * 8.635478973388672
Epoch 750, val loss: 1.055496335029602
Epoch 760, training loss: 87.42327880859375 = 1.0546702146530151 + 10.0 * 8.636860847473145
Epoch 760, val loss: 1.0554780960083008
Epoch 770, training loss: 87.4048080444336 = 1.0546696186065674 + 10.0 * 8.635013580322266
Epoch 770, val loss: 1.055471420288086
Epoch 780, training loss: 87.42301940917969 = 1.054656982421875 + 10.0 * 8.636836051940918
Epoch 780, val loss: 1.0554555654525757
Epoch 790, training loss: 87.47473907470703 = 1.054664134979248 + 10.0 * 8.642007827758789
Epoch 790, val loss: 1.0554643869400024
Epoch 800, training loss: 87.51075744628906 = 1.0546692609786987 + 10.0 * 8.645608901977539
Epoch 800, val loss: 1.0554571151733398
Epoch 810, training loss: 87.52519989013672 = 1.0546483993530273 + 10.0 * 8.647054672241211
Epoch 810, val loss: 1.0554416179656982
Epoch 820, training loss: 87.56281280517578 = 1.0546468496322632 + 10.0 * 8.650815963745117
Epoch 820, val loss: 1.0554441213607788
Epoch 830, training loss: 87.56101989746094 = 1.0546354055404663 + 10.0 * 8.650638580322266
Epoch 830, val loss: 1.0554306507110596
Epoch 840, training loss: 87.54176330566406 = 1.0546081066131592 + 10.0 * 8.648715019226074
Epoch 840, val loss: 1.055393934249878
Epoch 850, training loss: 87.57479095458984 = 1.0546027421951294 + 10.0 * 8.652018547058105
Epoch 850, val loss: 1.0553972721099854
Epoch 860, training loss: 87.59366607666016 = 1.0546009540557861 + 10.0 * 8.653905868530273
Epoch 860, val loss: 1.055392861366272
Epoch 870, training loss: 87.62582397460938 = 1.0545943975448608 + 10.0 * 8.657122611999512
Epoch 870, val loss: 1.0553828477859497
Epoch 880, training loss: 87.63805389404297 = 1.0545850992202759 + 10.0 * 8.658346176147461
Epoch 880, val loss: 1.05538010597229
Epoch 890, training loss: 87.63410186767578 = 1.0545597076416016 + 10.0 * 8.657954216003418
Epoch 890, val loss: 1.0553500652313232
Epoch 900, training loss: 87.68264770507812 = 1.0545520782470703 + 10.0 * 8.662809371948242
Epoch 900, val loss: 1.0553462505340576
Epoch 910, training loss: 87.70142364501953 = 1.0545566082000732 + 10.0 * 8.66468620300293
Epoch 910, val loss: 1.0553499460220337
Epoch 920, training loss: 87.73863983154297 = 1.0545419454574585 + 10.0 * 8.66840934753418
Epoch 920, val loss: 1.055334210395813
Epoch 930, training loss: 87.72107696533203 = 1.054518461227417 + 10.0 * 8.666655540466309
Epoch 930, val loss: 1.055321216583252
Epoch 940, training loss: 87.73297119140625 = 1.0545059442520142 + 10.0 * 8.6678466796875
Epoch 940, val loss: 1.0553096532821655
Epoch 950, training loss: 87.79190826416016 = 1.0545018911361694 + 10.0 * 8.67374038696289
Epoch 950, val loss: 1.0553067922592163
Epoch 960, training loss: 87.81754302978516 = 1.0544953346252441 + 10.0 * 8.676304817199707
Epoch 960, val loss: 1.0552997589111328
Epoch 970, training loss: 87.79070281982422 = 1.0544792413711548 + 10.0 * 8.673622131347656
Epoch 970, val loss: 1.055286169052124
Epoch 980, training loss: 87.84527587890625 = 1.0544674396514893 + 10.0 * 8.679080963134766
Epoch 980, val loss: 1.0552587509155273
Epoch 990, training loss: 87.84491729736328 = 1.0544493198394775 + 10.0 * 8.679046630859375
Epoch 990, val loss: 1.0552539825439453
Epoch 1000, training loss: 87.86305236816406 = 1.0544406175613403 + 10.0 * 8.68086051940918
Epoch 1000, val loss: 1.0552467107772827
Epoch 1010, training loss: 87.85094451904297 = 1.0544135570526123 + 10.0 * 8.67965316772461
Epoch 1010, val loss: 1.0552247762680054
Epoch 1020, training loss: 87.884521484375 = 1.0543965101242065 + 10.0 * 8.683012008666992
Epoch 1020, val loss: 1.0552095174789429
Epoch 1030, training loss: 87.89617156982422 = 1.054388403892517 + 10.0 * 8.684178352355957
Epoch 1030, val loss: 1.0552005767822266
Epoch 1040, training loss: 87.92035675048828 = 1.054374098777771 + 10.0 * 8.68659782409668
Epoch 1040, val loss: 1.0551822185516357
Epoch 1050, training loss: 87.92605590820312 = 1.0543686151504517 + 10.0 * 8.687169075012207
Epoch 1050, val loss: 1.0551806688308716
Epoch 1060, training loss: 87.96025848388672 = 1.054365634918213 + 10.0 * 8.69058895111084
Epoch 1060, val loss: 1.055172085762024
Epoch 1070, training loss: 87.9703369140625 = 1.0543532371520996 + 10.0 * 8.691598892211914
Epoch 1070, val loss: 1.0551577806472778
Epoch 1080, training loss: 87.98272705078125 = 1.0543330907821655 + 10.0 * 8.692838668823242
Epoch 1080, val loss: 1.0551453828811646
Epoch 1090, training loss: 87.97901153564453 = 1.0543134212493896 + 10.0 * 8.692469596862793
Epoch 1090, val loss: 1.055112600326538
Epoch 1100, training loss: 87.9952163696289 = 1.0543010234832764 + 10.0 * 8.694091796875
Epoch 1100, val loss: 1.0551114082336426
Epoch 1110, training loss: 88.04715728759766 = 1.0542978048324585 + 10.0 * 8.699285507202148
Epoch 1110, val loss: 1.0551023483276367
Epoch 1120, training loss: 88.0369644165039 = 1.0542665719985962 + 10.0 * 8.698269844055176
Epoch 1120, val loss: 1.0550864934921265
Epoch 1130, training loss: 88.0756607055664 = 1.0542621612548828 + 10.0 * 8.702139854431152
Epoch 1130, val loss: 1.0550756454467773
Epoch 1140, training loss: 88.09382629394531 = 1.0542494058609009 + 10.0 * 8.703957557678223
Epoch 1140, val loss: 1.0550564527511597
Epoch 1150, training loss: 88.01243591308594 = 1.0541832447052002 + 10.0 * 8.695825576782227
Epoch 1150, val loss: 1.0550001859664917
Epoch 1160, training loss: 88.02639770507812 = 1.0541576147079468 + 10.0 * 8.697223663330078
Epoch 1160, val loss: 1.0549715757369995
Epoch 1170, training loss: 88.00553131103516 = 1.0540965795516968 + 10.0 * 8.69514274597168
Epoch 1170, val loss: 1.0549229383468628
Epoch 1180, training loss: 87.99718475341797 = 1.0540884733200073 + 10.0 * 8.694310188293457
Epoch 1180, val loss: 1.0549260377883911
Epoch 1190, training loss: 88.02054595947266 = 1.054097294807434 + 10.0 * 8.69664478302002
Epoch 1190, val loss: 1.0549322366714478
Epoch 1200, training loss: 88.05587768554688 = 1.0541081428527832 + 10.0 * 8.700177192687988
Epoch 1200, val loss: 1.0549302101135254
Epoch 1210, training loss: 88.1075210571289 = 1.0541170835494995 + 10.0 * 8.705340385437012
Epoch 1210, val loss: 1.0549376010894775
Epoch 1220, training loss: 88.13983154296875 = 1.0541207790374756 + 10.0 * 8.70857048034668
Epoch 1220, val loss: 1.0549298524856567
Epoch 1230, training loss: 88.12005615234375 = 1.0540812015533447 + 10.0 * 8.706597328186035
Epoch 1230, val loss: 1.054908275604248
Epoch 1240, training loss: 88.15400695800781 = 1.0540788173675537 + 10.0 * 8.709993362426758
Epoch 1240, val loss: 1.0549010038375854
Epoch 1250, training loss: 88.17488861083984 = 1.0540671348571777 + 10.0 * 8.712081909179688
Epoch 1250, val loss: 1.0548874139785767
Epoch 1260, training loss: 88.17581176757812 = 1.0540416240692139 + 10.0 * 8.712177276611328
Epoch 1260, val loss: 1.0548546314239502
Epoch 1270, training loss: 88.20063781738281 = 1.0540275573730469 + 10.0 * 8.71466064453125
Epoch 1270, val loss: 1.0548497438430786
Epoch 1280, training loss: 88.19841003417969 = 1.0540080070495605 + 10.0 * 8.71444034576416
Epoch 1280, val loss: 1.0548255443572998
Epoch 1290, training loss: 88.20647430419922 = 1.0539872646331787 + 10.0 * 8.715249061584473
Epoch 1290, val loss: 1.0548110008239746
Epoch 1300, training loss: 88.25212860107422 = 1.053981065750122 + 10.0 * 8.719815254211426
Epoch 1300, val loss: 1.0547986030578613
Epoch 1310, training loss: 88.23735046386719 = 1.0539530515670776 + 10.0 * 8.718339920043945
Epoch 1310, val loss: 1.0547754764556885
Epoch 1320, training loss: 88.27210998535156 = 1.0539300441741943 + 10.0 * 8.721817970275879
Epoch 1320, val loss: 1.0547528266906738
Epoch 1330, training loss: 88.07131958007812 = 1.0535423755645752 + 10.0 * 8.701777458190918
Epoch 1330, val loss: 1.054336428642273
Epoch 1340, training loss: 88.2225112915039 = 1.053733468055725 + 10.0 * 8.716877937316895
Epoch 1340, val loss: 1.0545505285263062
Epoch 1350, training loss: 88.11605834960938 = 1.0536460876464844 + 10.0 * 8.7062406539917
Epoch 1350, val loss: 1.0545003414154053
Epoch 1360, training loss: 88.14934539794922 = 1.0537059307098389 + 10.0 * 8.709564208984375
Epoch 1360, val loss: 1.0545408725738525
Epoch 1370, training loss: 88.27603912353516 = 1.0537563562393188 + 10.0 * 8.722228050231934
Epoch 1370, val loss: 1.0545779466629028
Epoch 1380, training loss: 88.31648254394531 = 1.0537540912628174 + 10.0 * 8.726272583007812
Epoch 1380, val loss: 1.054579734802246
Epoch 1390, training loss: 88.36653137207031 = 1.0537577867507935 + 10.0 * 8.731277465820312
Epoch 1390, val loss: 1.0545817613601685
Epoch 1400, training loss: 88.39411926269531 = 1.0537315607070923 + 10.0 * 8.734038352966309
Epoch 1400, val loss: 1.054557204246521
Epoch 1410, training loss: 88.44363403320312 = 1.0537275075912476 + 10.0 * 8.738990783691406
Epoch 1410, val loss: 1.0545545816421509
Epoch 1420, training loss: 88.44535827636719 = 1.053697109222412 + 10.0 * 8.739166259765625
Epoch 1420, val loss: 1.0545283555984497
Epoch 1430, training loss: 88.49883270263672 = 1.0536845922470093 + 10.0 * 8.744514465332031
Epoch 1430, val loss: 1.054514765739441
Epoch 1440, training loss: 88.509765625 = 1.053662896156311 + 10.0 * 8.745610237121582
Epoch 1440, val loss: 1.054491400718689
Epoch 1450, training loss: 88.51587677001953 = 1.0536401271820068 + 10.0 * 8.746223449707031
Epoch 1450, val loss: 1.0544697046279907
Epoch 1460, training loss: 88.55657196044922 = 1.0536247491836548 + 10.0 * 8.75029468536377
Epoch 1460, val loss: 1.054455041885376
Epoch 1470, training loss: 88.59385681152344 = 1.0536036491394043 + 10.0 * 8.75402545928955
Epoch 1470, val loss: 1.0544307231903076
Epoch 1480, training loss: 88.609130859375 = 1.0535829067230225 + 10.0 * 8.755555152893066
Epoch 1480, val loss: 1.0544114112854004
Epoch 1490, training loss: 88.6028823852539 = 1.0535391569137573 + 10.0 * 8.754934310913086
Epoch 1490, val loss: 1.0543711185455322
Epoch 1500, training loss: 88.62767028808594 = 1.0535215139389038 + 10.0 * 8.757414817810059
Epoch 1500, val loss: 1.054356336593628
Epoch 1510, training loss: 88.6813735961914 = 1.0535157918930054 + 10.0 * 8.762785911560059
Epoch 1510, val loss: 1.054345965385437
Epoch 1520, training loss: 88.65446472167969 = 1.053470492362976 + 10.0 * 8.760099411010742
Epoch 1520, val loss: 1.054309368133545
Epoch 1530, training loss: 88.70097351074219 = 1.0534602403640747 + 10.0 * 8.764751434326172
Epoch 1530, val loss: 1.054292917251587
Epoch 1540, training loss: 88.7350845336914 = 1.0534404516220093 + 10.0 * 8.768163681030273
Epoch 1540, val loss: 1.05427885055542
Epoch 1550, training loss: 88.73625183105469 = 1.053403377532959 + 10.0 * 8.768284797668457
Epoch 1550, val loss: 1.0542361736297607
Epoch 1560, training loss: 88.70903778076172 = 1.0533666610717773 + 10.0 * 8.7655668258667
Epoch 1560, val loss: 1.054205060005188
Epoch 1570, training loss: 88.75403594970703 = 1.053354263305664 + 10.0 * 8.770068168640137
Epoch 1570, val loss: 1.0541965961456299
Epoch 1580, training loss: 88.79672241210938 = 1.0533376932144165 + 10.0 * 8.774338722229004
Epoch 1580, val loss: 1.05417001247406
Epoch 1590, training loss: 88.77619171142578 = 1.0532985925674438 + 10.0 * 8.772289276123047
Epoch 1590, val loss: 1.0541412830352783
Epoch 1600, training loss: 88.77159881591797 = 1.0532549619674683 + 10.0 * 8.771834373474121
Epoch 1600, val loss: 1.0541125535964966
Epoch 1610, training loss: 88.82150268554688 = 1.0532407760620117 + 10.0 * 8.776826858520508
Epoch 1610, val loss: 1.0540813207626343
Epoch 1620, training loss: 88.82402038574219 = 1.0532069206237793 + 10.0 * 8.777081489562988
Epoch 1620, val loss: 1.054036021232605
Epoch 1630, training loss: 88.72168731689453 = 1.0531260967254639 + 10.0 * 8.76685619354248
Epoch 1630, val loss: 1.0539665222167969
Epoch 1640, training loss: 88.6816177368164 = 1.0530502796173096 + 10.0 * 8.762857437133789
Epoch 1640, val loss: 1.0538806915283203
Epoch 1650, training loss: 88.68071746826172 = 1.0530303716659546 + 10.0 * 8.762768745422363
Epoch 1650, val loss: 1.0538811683654785
Epoch 1660, training loss: 88.61534118652344 = 1.0529342889785767 + 10.0 * 8.756240844726562
Epoch 1660, val loss: 1.0538002252578735
Epoch 1670, training loss: 88.72673797607422 = 1.052955985069275 + 10.0 * 8.767377853393555
Epoch 1670, val loss: 1.0537986755371094
Epoch 1680, training loss: 88.77376556396484 = 1.0529814958572388 + 10.0 * 8.772078514099121
Epoch 1680, val loss: 1.0538325309753418
Epoch 1690, training loss: 88.80697631835938 = 1.0529621839523315 + 10.0 * 8.77540111541748
Epoch 1690, val loss: 1.0538188219070435
Epoch 1700, training loss: 88.87307739257812 = 1.052947759628296 + 10.0 * 8.782012939453125
Epoch 1700, val loss: 1.0538030862808228
Epoch 1710, training loss: 88.87769317626953 = 1.05292546749115 + 10.0 * 8.782476425170898
Epoch 1710, val loss: 1.0537751913070679
Epoch 1720, training loss: 88.88933563232422 = 1.0528944730758667 + 10.0 * 8.78364372253418
Epoch 1720, val loss: 1.0537465810775757
Epoch 1730, training loss: 88.87517547607422 = 1.0528353452682495 + 10.0 * 8.782234191894531
Epoch 1730, val loss: 1.0536882877349854
Epoch 1740, training loss: 88.8685073852539 = 1.0527904033660889 + 10.0 * 8.781572341918945
Epoch 1740, val loss: 1.053652048110962
Epoch 1750, training loss: 88.89274597167969 = 1.0527846813201904 + 10.0 * 8.783995628356934
Epoch 1750, val loss: 1.05364191532135
Epoch 1760, training loss: 88.93453216552734 = 1.052767276763916 + 10.0 * 8.788176536560059
Epoch 1760, val loss: 1.0536307096481323
Epoch 1770, training loss: 88.91202545166016 = 1.052692174911499 + 10.0 * 8.785933494567871
Epoch 1770, val loss: 1.053559422492981
Epoch 1780, training loss: 88.9358139038086 = 1.0526560544967651 + 10.0 * 8.788315773010254
Epoch 1780, val loss: 1.0535335540771484
Epoch 1790, training loss: 88.9695053100586 = 1.052639365196228 + 10.0 * 8.791686058044434
Epoch 1790, val loss: 1.0535211563110352
Epoch 1800, training loss: 88.99447631835938 = 1.052621841430664 + 10.0 * 8.794185638427734
Epoch 1800, val loss: 1.0534961223602295
Epoch 1810, training loss: 89.00967407226562 = 1.0525912046432495 + 10.0 * 8.795708656311035
Epoch 1810, val loss: 1.0534594058990479
Epoch 1820, training loss: 89.01748657226562 = 1.0525370836257935 + 10.0 * 8.79649543762207
Epoch 1820, val loss: 1.0534124374389648
Epoch 1830, training loss: 89.01057434082031 = 1.0524901151657104 + 10.0 * 8.795808792114258
Epoch 1830, val loss: 1.0533517599105835
Epoch 1840, training loss: 89.0163345336914 = 1.0524488687515259 + 10.0 * 8.796388626098633
Epoch 1840, val loss: 1.053328275680542
Epoch 1850, training loss: 89.04751586914062 = 1.052415132522583 + 10.0 * 8.79951000213623
Epoch 1850, val loss: 1.0532978773117065
Epoch 1860, training loss: 89.05148315429688 = 1.0523799657821655 + 10.0 * 8.799909591674805
Epoch 1860, val loss: 1.0532629489898682
Epoch 1870, training loss: 89.05763244628906 = 1.0523316860198975 + 10.0 * 8.800530433654785
Epoch 1870, val loss: 1.0532269477844238
Epoch 1880, training loss: 89.08665466308594 = 1.0522942543029785 + 10.0 * 8.803436279296875
Epoch 1880, val loss: 1.0531854629516602
Epoch 1890, training loss: 89.10326385498047 = 1.0522594451904297 + 10.0 * 8.805100440979004
Epoch 1890, val loss: 1.0531628131866455
Epoch 1900, training loss: 89.11396026611328 = 1.052219033241272 + 10.0 * 8.806174278259277
Epoch 1900, val loss: 1.0531147718429565
Epoch 1910, training loss: 89.05522918701172 = 1.0521305799484253 + 10.0 * 8.800310134887695
Epoch 1910, val loss: 1.053015112876892
Epoch 1920, training loss: 89.05632781982422 = 1.05201256275177 + 10.0 * 8.800432205200195
Epoch 1920, val loss: 1.0529747009277344
Epoch 1930, training loss: 88.86361694335938 = 1.051692008972168 + 10.0 * 8.781192779541016
Epoch 1930, val loss: 1.052564263343811
Epoch 1940, training loss: 88.90083312988281 = 1.0517768859863281 + 10.0 * 8.784905433654785
Epoch 1940, val loss: 1.0526827573776245
Epoch 1950, training loss: 88.96141815185547 = 1.0518147945404053 + 10.0 * 8.790960311889648
Epoch 1950, val loss: 1.0527114868164062
Epoch 1960, training loss: 89.0096664428711 = 1.0518149137496948 + 10.0 * 8.795785903930664
Epoch 1960, val loss: 1.0527230501174927
Epoch 1970, training loss: 89.08504486083984 = 1.0518121719360352 + 10.0 * 8.803323745727539
Epoch 1970, val loss: 1.0527310371398926
Epoch 1980, training loss: 89.13380432128906 = 1.0518035888671875 + 10.0 * 8.808199882507324
Epoch 1980, val loss: 1.0527228116989136
Epoch 1990, training loss: 89.18872833251953 = 1.0517886877059937 + 10.0 * 8.81369400024414
Epoch 1990, val loss: 1.0527055263519287
Epoch 2000, training loss: 89.20667266845703 = 1.0517514944076538 + 10.0 * 8.815492630004883
Epoch 2000, val loss: 1.052669644355774
Epoch 2010, training loss: 89.2123031616211 = 1.05170476436615 + 10.0 * 8.816060066223145
Epoch 2010, val loss: 1.0526255369186401
Epoch 2020, training loss: 89.20089721679688 = 1.0516315698623657 + 10.0 * 8.814927101135254
Epoch 2020, val loss: 1.0525729656219482
Epoch 2030, training loss: 89.20394134521484 = 1.051594853401184 + 10.0 * 8.815234184265137
Epoch 2030, val loss: 1.0525248050689697
Epoch 2040, training loss: 89.198974609375 = 1.0515369176864624 + 10.0 * 8.814743995666504
Epoch 2040, val loss: 1.0524866580963135
Epoch 2050, training loss: 89.25143432617188 = 1.0515111684799194 + 10.0 * 8.819992065429688
Epoch 2050, val loss: 1.0524554252624512
Epoch 2060, training loss: 89.2753677368164 = 1.0514847040176392 + 10.0 * 8.822388648986816
Epoch 2060, val loss: 1.0524256229400635
Epoch 2070, training loss: 89.26566314697266 = 1.0514092445373535 + 10.0 * 8.821425437927246
Epoch 2070, val loss: 1.052359700202942
Epoch 2080, training loss: 89.26677703857422 = 1.0513564348220825 + 10.0 * 8.821542739868164
Epoch 2080, val loss: 1.0522891283035278
Epoch 2090, training loss: 89.25611114501953 = 1.051275372505188 + 10.0 * 8.820483207702637
Epoch 2090, val loss: 1.0522332191467285
Epoch 2100, training loss: 89.27335357666016 = 1.0512340068817139 + 10.0 * 8.822212219238281
Epoch 2100, val loss: 1.0521929264068604
Epoch 2110, training loss: 89.29360961914062 = 1.0511932373046875 + 10.0 * 8.824241638183594
Epoch 2110, val loss: 1.0521701574325562
Epoch 2120, training loss: 89.31238555908203 = 1.0511434078216553 + 10.0 * 8.82612419128418
Epoch 2120, val loss: 1.0521162748336792
Epoch 2130, training loss: 89.32721710205078 = 1.0510859489440918 + 10.0 * 8.82761287689209
Epoch 2130, val loss: 1.0520665645599365
Epoch 2140, training loss: 89.30977630615234 = 1.0509949922561646 + 10.0 * 8.825878143310547
Epoch 2140, val loss: 1.0519944429397583
Epoch 2150, training loss: 89.36112213134766 = 1.0509886741638184 + 10.0 * 8.831013679504395
Epoch 2150, val loss: 1.0519760847091675
Epoch 2160, training loss: 89.37712860107422 = 1.0509450435638428 + 10.0 * 8.832618713378906
Epoch 2160, val loss: 1.051941156387329
Epoch 2170, training loss: 89.37850189208984 = 1.0508966445922852 + 10.0 * 8.83276081085205
Epoch 2170, val loss: 1.0518931150436401
Epoch 2180, training loss: 89.30061340332031 = 1.0507497787475586 + 10.0 * 8.824986457824707
Epoch 2180, val loss: 1.0517734289169312
Epoch 2190, training loss: 89.34610748291016 = 1.0507360696792603 + 10.0 * 8.829537391662598
Epoch 2190, val loss: 1.0517332553863525
Epoch 2200, training loss: 89.37215423583984 = 1.0506941080093384 + 10.0 * 8.832145690917969
Epoch 2200, val loss: 1.0517165660858154
Epoch 2210, training loss: 89.4140625 = 1.0506737232208252 + 10.0 * 8.836338996887207
Epoch 2210, val loss: 1.0516960620880127
Epoch 2220, training loss: 89.42366790771484 = 1.0506091117858887 + 10.0 * 8.837306022644043
Epoch 2220, val loss: 1.0516375303268433
Epoch 2230, training loss: 89.3998031616211 = 1.0505071878433228 + 10.0 * 8.834929466247559
Epoch 2230, val loss: 1.0515474081039429
Epoch 2240, training loss: 89.30632019042969 = 1.0503699779510498 + 10.0 * 8.825594902038574
Epoch 2240, val loss: 1.0514100790023804
Epoch 2250, training loss: 89.27223205566406 = 1.0502513647079468 + 10.0 * 8.822197914123535
Epoch 2250, val loss: 1.051284670829773
Epoch 2260, training loss: 89.33042907714844 = 1.0502488613128662 + 10.0 * 8.828018188476562
Epoch 2260, val loss: 1.0512969493865967
Epoch 2270, training loss: 89.40181732177734 = 1.0502517223358154 + 10.0 * 8.835156440734863
Epoch 2270, val loss: 1.0512887239456177
Epoch 2280, training loss: 89.45511627197266 = 1.0502127408981323 + 10.0 * 8.840490341186523
Epoch 2280, val loss: 1.0512607097625732
Epoch 2290, training loss: 89.46180725097656 = 1.0501550436019897 + 10.0 * 8.841165542602539
Epoch 2290, val loss: 1.051204800605774
Epoch 2300, training loss: 89.46956634521484 = 1.0500693321228027 + 10.0 * 8.841949462890625
Epoch 2300, val loss: 1.051116943359375
Epoch 2310, training loss: 89.33882904052734 = 1.0497972965240479 + 10.0 * 8.828903198242188
Epoch 2310, val loss: 1.0508301258087158
Epoch 2320, training loss: 88.87496185302734 = 1.0489437580108643 + 10.0 * 8.782602310180664
Epoch 2320, val loss: 1.0499231815338135
Epoch 2330, training loss: 89.07051849365234 = 1.049136996269226 + 10.0 * 8.802138328552246
Epoch 2330, val loss: 1.0502153635025024
Epoch 2340, training loss: 88.9642562866211 = 1.0489566326141357 + 10.0 * 8.791529655456543
Epoch 2340, val loss: 1.0500918626785278
Epoch 2350, training loss: 89.04220581054688 = 1.0491337776184082 + 10.0 * 8.799306869506836
Epoch 2350, val loss: 1.0501915216445923
Epoch 2360, training loss: 89.15098571777344 = 1.0492205619812012 + 10.0 * 8.810176849365234
Epoch 2360, val loss: 1.0503019094467163
Epoch 2370, training loss: 89.22921752929688 = 1.0492281913757324 + 10.0 * 8.817998886108398
Epoch 2370, val loss: 1.0503281354904175
Epoch 2380, training loss: 89.30812072753906 = 1.0492582321166992 + 10.0 * 8.825886726379395
Epoch 2380, val loss: 1.0503565073013306
Epoch 2390, training loss: 89.36719512939453 = 1.049257516860962 + 10.0 * 8.831793785095215
Epoch 2390, val loss: 1.0503720045089722
Epoch 2400, training loss: 89.3983154296875 = 1.0492218732833862 + 10.0 * 8.834909439086914
Epoch 2400, val loss: 1.0503418445587158
Epoch 2410, training loss: 89.44422912597656 = 1.0491849184036255 + 10.0 * 8.83950424194336
Epoch 2410, val loss: 1.0503159761428833
Epoch 2420, training loss: 89.44965362548828 = 1.049123764038086 + 10.0 * 8.840052604675293
Epoch 2420, val loss: 1.0502604246139526
Epoch 2430, training loss: 89.45564270019531 = 1.0490418672561646 + 10.0 * 8.840660095214844
Epoch 2430, val loss: 1.050192952156067
Epoch 2440, training loss: 89.4901351928711 = 1.04899001121521 + 10.0 * 8.844114303588867
Epoch 2440, val loss: 1.050148367881775
Epoch 2450, training loss: 89.50164794921875 = 1.0489269495010376 + 10.0 * 8.845272064208984
Epoch 2450, val loss: 1.0500905513763428
Epoch 2460, training loss: 89.48330688476562 = 1.048822045326233 + 10.0 * 8.843448638916016
Epoch 2460, val loss: 1.0500019788742065
Epoch 2470, training loss: 89.51156616210938 = 1.0487747192382812 + 10.0 * 8.84627914428711
Epoch 2470, val loss: 1.049959421157837
Epoch 2480, training loss: 89.54206085205078 = 1.0487185716629028 + 10.0 * 8.849333763122559
Epoch 2480, val loss: 1.049911379814148
Epoch 2490, training loss: 89.55118560791016 = 1.0486352443695068 + 10.0 * 8.850255012512207
Epoch 2490, val loss: 1.049842119216919
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.40492753623188404
0.8627110048540173
The final CL Acc:0.40870, 0.00725, The final GNN Acc:0.86339, 0.00060
Begin epxeriment: noisy_level: 0.2 cont_weight: 10 epoch: der1:0.3 der2:0.3 dfr1:0.4 dfr2:0.4
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=2500, cl_num_layers=2, cl_num_proj_hidden=64, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=10.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.3, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=32, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, noisy_level=0.2, num_hidden=64, num_proj_hidden=64, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=40, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 88648])
add edge: torch.Size([2, 106520])
remove edge: torch.Size([2, 71132])
updated graph: torch.Size([2, 89004])
=== Noisy graph ===
/home/project-robust-contrastive/Robust-Contrastive-Learning/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 103.90168762207031 = 1.0893070697784424 + 10.0 * 10.281237602233887
Epoch 0, val loss: 1.0908156633377075
Epoch 10, training loss: 99.59716033935547 = 1.0861568450927734 + 10.0 * 9.851099967956543
Epoch 10, val loss: 1.0876590013504028
Epoch 20, training loss: 97.6670150756836 = 1.0831857919692993 + 10.0 * 9.6583833694458
Epoch 20, val loss: 1.0846738815307617
Epoch 30, training loss: 96.30691528320312 = 1.080356240272522 + 10.0 * 9.522656440734863
Epoch 30, val loss: 1.0818309783935547
Epoch 40, training loss: 95.28279876708984 = 1.0776985883712769 + 10.0 * 9.420510292053223
Epoch 40, val loss: 1.0791617631912231
Epoch 50, training loss: 94.43507385253906 = 1.0752363204956055 + 10.0 * 9.335984230041504
Epoch 50, val loss: 1.0766823291778564
Epoch 60, training loss: 93.71275329589844 = 1.0729222297668457 + 10.0 * 9.263982772827148
Epoch 60, val loss: 1.0743517875671387
Epoch 70, training loss: 93.07599639892578 = 1.0707515478134155 + 10.0 * 9.20052433013916
Epoch 70, val loss: 1.072164535522461
Epoch 80, training loss: 92.52046966552734 = 1.0687108039855957 + 10.0 * 9.14517593383789
Epoch 80, val loss: 1.0701031684875488
Epoch 90, training loss: 92.04591369628906 = 1.0667959451675415 + 10.0 * 9.097911834716797
Epoch 90, val loss: 1.0681657791137695
Epoch 100, training loss: 91.61595916748047 = 1.064995288848877 + 10.0 * 9.055096626281738
Epoch 100, val loss: 1.0663429498672485
Epoch 110, training loss: 91.2464370727539 = 1.063288927078247 + 10.0 * 9.018315315246582
Epoch 110, val loss: 1.0646156072616577
Epoch 120, training loss: 90.92603302001953 = 1.06168532371521 + 10.0 * 8.986434936523438
Epoch 120, val loss: 1.0629757642745972
Epoch 130, training loss: 90.64282989501953 = 1.0601471662521362 + 10.0 * 8.958268165588379
Epoch 130, val loss: 1.0614171028137207
Epoch 140, training loss: 90.41021728515625 = 1.0586875677108765 + 10.0 * 8.935153007507324
Epoch 140, val loss: 1.059920072555542
Epoch 150, training loss: 90.20860290527344 = 1.0572950839996338 + 10.0 * 8.915130615234375
Epoch 150, val loss: 1.0584962368011475
Epoch 160, training loss: 90.03260803222656 = 1.0559064149856567 + 10.0 * 8.897669792175293
Epoch 160, val loss: 1.057074785232544
Epoch 170, training loss: 89.8397216796875 = 1.0545188188552856 + 10.0 * 8.878520011901855
Epoch 170, val loss: 1.0556442737579346
Epoch 180, training loss: 89.72308349609375 = 1.0533102750778198 + 10.0 * 8.86697769165039
Epoch 180, val loss: 1.0543904304504395
Epoch 190, training loss: 89.57569122314453 = 1.0519399642944336 + 10.0 * 8.852375030517578
Epoch 190, val loss: 1.052993893623352
Epoch 200, training loss: 89.53797149658203 = 1.050429105758667 + 10.0 * 8.848753929138184
Epoch 200, val loss: 1.051403284072876
Epoch 210, training loss: 89.61992645263672 = 1.0494118928909302 + 10.0 * 8.857050895690918
Epoch 210, val loss: 1.0503793954849243
Epoch 220, training loss: 89.35652923583984 = 1.0479074716567993 + 10.0 * 8.830862045288086
Epoch 220, val loss: 1.048808217048645
Epoch 230, training loss: 89.26345825195312 = 1.0465068817138672 + 10.0 * 8.821695327758789
Epoch 230, val loss: 1.0473878383636475
Epoch 240, training loss: 89.21376037597656 = 1.0450326204299927 + 10.0 * 8.816872596740723
Epoch 240, val loss: 1.0458849668502808
Epoch 250, training loss: 89.15876007080078 = 1.0434315204620361 + 10.0 * 8.811532974243164
Epoch 250, val loss: 1.0442558526992798
Epoch 260, training loss: 89.11026763916016 = 1.0417429208755493 + 10.0 * 8.806852340698242
Epoch 260, val loss: 1.0425126552581787
Epoch 270, training loss: 89.11422729492188 = 1.0399575233459473 + 10.0 * 8.807427406311035
Epoch 270, val loss: 1.0407170057296753
Epoch 280, training loss: 89.06986236572266 = 1.0379756689071655 + 10.0 * 8.80318832397461
Epoch 280, val loss: 1.0387097597122192
Epoch 290, training loss: 89.02703857421875 = 1.035845398902893 + 10.0 * 8.79911994934082
Epoch 290, val loss: 1.036558747291565
Epoch 300, training loss: 88.99166107177734 = 1.033589482307434 + 10.0 * 8.795806884765625
Epoch 300, val loss: 1.0343177318572998
Epoch 310, training loss: 88.95668029785156 = 1.0310779809951782 + 10.0 * 8.792560577392578
Epoch 310, val loss: 1.03178071975708
Epoch 320, training loss: 88.9439468383789 = 1.0284380912780762 + 10.0 * 8.79155158996582
Epoch 320, val loss: 1.0290969610214233
Epoch 330, training loss: 88.95297241210938 = 1.0255916118621826 + 10.0 * 8.79273796081543
Epoch 330, val loss: 1.0262454748153687
Epoch 340, training loss: 88.87112426757812 = 1.0222992897033691 + 10.0 * 8.784882545471191
Epoch 340, val loss: 1.0229955911636353
Epoch 350, training loss: 88.90310668945312 = 1.0190542936325073 + 10.0 * 8.788405418395996
Epoch 350, val loss: 1.0196707248687744
Epoch 360, training loss: 88.89717102050781 = 1.0154894590377808 + 10.0 * 8.788167953491211
Epoch 360, val loss: 1.016055941581726
Epoch 370, training loss: 88.90718841552734 = 1.0116312503814697 + 10.0 * 8.789555549621582
Epoch 370, val loss: 1.0120747089385986
Epoch 380, training loss: 88.91792297363281 = 1.007630705833435 + 10.0 * 8.79102897644043
Epoch 380, val loss: 1.0083177089691162
Epoch 390, training loss: 88.77000427246094 = 1.002978801727295 + 10.0 * 8.776702880859375
Epoch 390, val loss: 1.0035417079925537
Epoch 400, training loss: 88.89512634277344 = 0.9986882209777832 + 10.0 * 8.789644241333008
Epoch 400, val loss: 0.9992105960845947
Epoch 410, training loss: 88.87647247314453 = 0.9936746954917908 + 10.0 * 8.78827953338623
Epoch 410, val loss: 0.9942358136177063
Epoch 420, training loss: 88.91769409179688 = 0.9885724186897278 + 10.0 * 8.792912483215332
Epoch 420, val loss: 0.9891054034233093
Epoch 430, training loss: 88.90687561035156 = 0.9830245971679688 + 10.0 * 8.79238510131836
Epoch 430, val loss: 0.983568549156189
Epoch 440, training loss: 88.9296875 = 0.9772339463233948 + 10.0 * 8.795245170593262
Epoch 440, val loss: 0.9777889847755432
Epoch 450, training loss: 88.92904663085938 = 0.9712955951690674 + 10.0 * 8.795774459838867
Epoch 450, val loss: 0.9717938899993896
Epoch 460, training loss: 88.93561553955078 = 0.964868426322937 + 10.0 * 8.797075271606445
Epoch 460, val loss: 0.9654417037963867
Epoch 470, training loss: 88.9472885131836 = 0.9581298232078552 + 10.0 * 8.79891586303711
Epoch 470, val loss: 0.9587481617927551
Epoch 480, training loss: 88.9486312866211 = 0.9512341022491455 + 10.0 * 8.799739837646484
Epoch 480, val loss: 0.9518508315086365
Epoch 490, training loss: 88.9536361694336 = 0.943989098072052 + 10.0 * 8.80096435546875
Epoch 490, val loss: 0.9446905255317688
Epoch 500, training loss: 88.96791076660156 = 0.9365156292915344 + 10.0 * 8.803139686584473
Epoch 500, val loss: 0.9371901154518127
Epoch 510, training loss: 88.98741149902344 = 0.9288580417633057 + 10.0 * 8.805855751037598
Epoch 510, val loss: 0.9296072721481323
Epoch 520, training loss: 88.98426818847656 = 0.9207911491394043 + 10.0 * 8.806347846984863
Epoch 520, val loss: 0.9217110276222229
Epoch 530, training loss: 88.947998046875 = 0.9127470850944519 + 10.0 * 8.8035249710083
Epoch 530, val loss: 0.9138035774230957
Epoch 540, training loss: 88.9722671508789 = 0.9045257568359375 + 10.0 * 8.806774139404297
Epoch 540, val loss: 0.9056724309921265
Epoch 550, training loss: 88.99078369140625 = 0.8961163759231567 + 10.0 * 8.809466361999512
Epoch 550, val loss: 0.8974581956863403
Epoch 560, training loss: 88.9744873046875 = 0.8875435590744019 + 10.0 * 8.808694839477539
Epoch 560, val loss: 0.8890851140022278
Epoch 570, training loss: 88.94537353515625 = 0.8785845637321472 + 10.0 * 8.806678771972656
Epoch 570, val loss: 0.8803746700286865
Epoch 580, training loss: 88.9571304321289 = 0.8697054982185364 + 10.0 * 8.80874252319336
Epoch 580, val loss: 0.8717647194862366
Epoch 590, training loss: 88.98234558105469 = 0.861018717288971 + 10.0 * 8.812132835388184
Epoch 590, val loss: 0.8632745742797852
Epoch 600, training loss: 89.01151275634766 = 0.8522757291793823 + 10.0 * 8.815923690795898
Epoch 600, val loss: 0.8548305034637451
Epoch 610, training loss: 89.05059051513672 = 0.8432235717773438 + 10.0 * 8.8207368850708
Epoch 610, val loss: 0.8462020754814148
Epoch 620, training loss: 89.05376434326172 = 0.8341124653816223 + 10.0 * 8.821965217590332
Epoch 620, val loss: 0.8375753164291382
Epoch 630, training loss: 89.0547103881836 = 0.8251063823699951 + 10.0 * 8.82296085357666
Epoch 630, val loss: 0.82899409532547
Epoch 640, training loss: 88.9948501586914 = 0.815801203250885 + 10.0 * 8.817904472351074
Epoch 640, val loss: 0.8202235698699951
Epoch 650, training loss: 88.9787368774414 = 0.8067753314971924 + 10.0 * 8.817195892333984
Epoch 650, val loss: 0.8117700219154358
Epoch 660, training loss: 89.04608154296875 = 0.798052966594696 + 10.0 * 8.824803352355957
Epoch 660, val loss: 0.8036710619926453
Epoch 670, training loss: 89.08755493164062 = 0.7891289591789246 + 10.0 * 8.829842567443848
Epoch 670, val loss: 0.7953140139579773
Epoch 680, training loss: 89.08930206298828 = 0.7801461815834045 + 10.0 * 8.830915451049805
Epoch 680, val loss: 0.7870281338691711
Epoch 690, training loss: 89.13201141357422 = 0.7713717222213745 + 10.0 * 8.836064338684082
Epoch 690, val loss: 0.7789493203163147
Epoch 700, training loss: 89.12307739257812 = 0.7625683546066284 + 10.0 * 8.836050987243652
Epoch 700, val loss: 0.7707691192626953
Epoch 710, training loss: 89.16788482666016 = 0.7539011836051941 + 10.0 * 8.841398239135742
Epoch 710, val loss: 0.762904942035675
Epoch 720, training loss: 89.17353820800781 = 0.7455340027809143 + 10.0 * 8.84280014038086
Epoch 720, val loss: 0.7551958560943604
Epoch 730, training loss: 89.2237777709961 = 0.7372130751609802 + 10.0 * 8.84865665435791
Epoch 730, val loss: 0.7477889060974121
Epoch 740, training loss: 89.2388687133789 = 0.7290768027305603 + 10.0 * 8.85097885131836
Epoch 740, val loss: 0.7404274344444275
Epoch 750, training loss: 89.24887084960938 = 0.7211146354675293 + 10.0 * 8.852775573730469
Epoch 750, val loss: 0.7332133650779724
Epoch 760, training loss: 89.26335906982422 = 0.7132797241210938 + 10.0 * 8.855008125305176
Epoch 760, val loss: 0.7263197898864746
Epoch 770, training loss: 89.29369354248047 = 0.705622136592865 + 10.0 * 8.858807563781738
Epoch 770, val loss: 0.719529390335083
Epoch 780, training loss: 89.30184173583984 = 0.6980790495872498 + 10.0 * 8.860376358032227
Epoch 780, val loss: 0.7129232287406921
Epoch 790, training loss: 89.36248779296875 = 0.6912254095077515 + 10.0 * 8.86712646484375
Epoch 790, val loss: 0.7070479393005371
Epoch 800, training loss: 89.10911560058594 = 0.6841613054275513 + 10.0 * 8.842495918273926
Epoch 800, val loss: 0.7015390992164612
Epoch 810, training loss: 89.328369140625 = 0.6780967712402344 + 10.0 * 8.86502742767334
Epoch 810, val loss: 0.6954955458641052
Epoch 820, training loss: 89.30040740966797 = 0.6716442108154297 + 10.0 * 8.862875938415527
Epoch 820, val loss: 0.690148651599884
Epoch 830, training loss: 89.31281280517578 = 0.6654289364814758 + 10.0 * 8.864738464355469
Epoch 830, val loss: 0.6850044131278992
Epoch 840, training loss: 89.35553741455078 = 0.6595508456230164 + 10.0 * 8.869598388671875
Epoch 840, val loss: 0.6800691485404968
Epoch 850, training loss: 89.35807800292969 = 0.6536892652511597 + 10.0 * 8.870439529418945
Epoch 850, val loss: 0.6750707626342773
Epoch 860, training loss: 89.1231918334961 = 0.6476841568946838 + 10.0 * 8.847551345825195
Epoch 860, val loss: 0.6703991293907166
Epoch 870, training loss: 89.51124572753906 = 0.642878532409668 + 10.0 * 8.886837005615234
Epoch 870, val loss: 0.6659380197525024
Epoch 880, training loss: 89.39891052246094 = 0.6378771066665649 + 10.0 * 8.876103401184082
Epoch 880, val loss: 0.6622838377952576
Epoch 890, training loss: 89.42485046386719 = 0.6328968405723572 + 10.0 * 8.879195213317871
Epoch 890, val loss: 0.658224880695343
Epoch 900, training loss: 89.49632263183594 = 0.6283869743347168 + 10.0 * 8.88679313659668
Epoch 900, val loss: 0.6545395255088806
Epoch 910, training loss: 89.48760986328125 = 0.6238166689872742 + 10.0 * 8.88637924194336
Epoch 910, val loss: 0.6509135365486145
Epoch 920, training loss: 89.51581573486328 = 0.6195667386054993 + 10.0 * 8.88962459564209
Epoch 920, val loss: 0.6476683616638184
Epoch 930, training loss: 89.55621337890625 = 0.6154924035072327 + 10.0 * 8.894071578979492
Epoch 930, val loss: 0.6445015072822571
Epoch 940, training loss: 89.53966522216797 = 0.6115342974662781 + 10.0 * 8.892812728881836
Epoch 940, val loss: 0.6415274143218994
Epoch 950, training loss: 89.60394287109375 = 0.6077700853347778 + 10.0 * 8.899617195129395
Epoch 950, val loss: 0.6387101411819458
Epoch 960, training loss: 89.63461303710938 = 0.6040674448013306 + 10.0 * 8.903055191040039
Epoch 960, val loss: 0.6360292434692383
Epoch 970, training loss: 89.63582611083984 = 0.6005901098251343 + 10.0 * 8.903523445129395
Epoch 970, val loss: 0.6334378719329834
Epoch 980, training loss: 89.65186309814453 = 0.5971916913986206 + 10.0 * 8.90546703338623
Epoch 980, val loss: 0.6310130953788757
Epoch 990, training loss: 89.6201400756836 = 0.5939679741859436 + 10.0 * 8.902616500854492
Epoch 990, val loss: 0.6286144256591797
Epoch 1000, training loss: 89.6521987915039 = 0.5908002853393555 + 10.0 * 8.906140327453613
Epoch 1000, val loss: 0.6264588236808777
Epoch 1010, training loss: 89.67225646972656 = 0.5878194570541382 + 10.0 * 8.908443450927734
Epoch 1010, val loss: 0.624271810054779
Epoch 1020, training loss: 89.72759246826172 = 0.5847995281219482 + 10.0 * 8.914278984069824
Epoch 1020, val loss: 0.6222808361053467
Epoch 1030, training loss: 89.71469116210938 = 0.5818938612937927 + 10.0 * 8.91327953338623
Epoch 1030, val loss: 0.620303750038147
Epoch 1040, training loss: 89.74816131591797 = 0.5790671706199646 + 10.0 * 8.916910171508789
Epoch 1040, val loss: 0.6182602643966675
Epoch 1050, training loss: 89.70436096191406 = 0.576255202293396 + 10.0 * 8.912810325622559
Epoch 1050, val loss: 0.6165868639945984
Epoch 1060, training loss: 89.71940612792969 = 0.5735212564468384 + 10.0 * 8.91458797454834
Epoch 1060, val loss: 0.6147704124450684
Epoch 1070, training loss: 89.7650146484375 = 0.5709449052810669 + 10.0 * 8.91940689086914
Epoch 1070, val loss: 0.6130884885787964
Epoch 1080, training loss: 89.77501678466797 = 0.5683149695396423 + 10.0 * 8.920670509338379
Epoch 1080, val loss: 0.6113578081130981
Epoch 1090, training loss: 89.78990173339844 = 0.5657334327697754 + 10.0 * 8.922416687011719
Epoch 1090, val loss: 0.6098214983940125
Epoch 1100, training loss: 89.85326385498047 = 0.5632691383361816 + 10.0 * 8.928998947143555
Epoch 1100, val loss: 0.6084431409835815
Epoch 1110, training loss: 89.82664489746094 = 0.560833215713501 + 10.0 * 8.926580429077148
Epoch 1110, val loss: 0.6065030097961426
Epoch 1120, training loss: 89.87645721435547 = 0.5583147406578064 + 10.0 * 8.931814193725586
Epoch 1120, val loss: 0.6053575277328491
Epoch 1130, training loss: 89.97290802001953 = 0.5558573603630066 + 10.0 * 8.941705703735352
Epoch 1130, val loss: 0.6036439538002014
Epoch 1140, training loss: 89.90447998046875 = 0.5535519123077393 + 10.0 * 8.93509292602539
Epoch 1140, val loss: 0.6023567318916321
Epoch 1150, training loss: 89.92131042480469 = 0.5511952638626099 + 10.0 * 8.93701171875
Epoch 1150, val loss: 0.6008850336074829
Epoch 1160, training loss: 89.94102478027344 = 0.5487815737724304 + 10.0 * 8.939224243164062
Epoch 1160, val loss: 0.5997897386550903
Epoch 1170, training loss: 90.02228546142578 = 0.5463975071907043 + 10.0 * 8.947588920593262
Epoch 1170, val loss: 0.598032534122467
Epoch 1180, training loss: 89.97996520996094 = 0.5440332293510437 + 10.0 * 8.94359302520752
Epoch 1180, val loss: 0.5969347953796387
Epoch 1190, training loss: 89.99170684814453 = 0.5416311025619507 + 10.0 * 8.94500732421875
Epoch 1190, val loss: 0.5952858924865723
Epoch 1200, training loss: 89.93446350097656 = 0.5394003391265869 + 10.0 * 8.939506530761719
Epoch 1200, val loss: 0.5942630767822266
Epoch 1210, training loss: 89.9671630859375 = 0.5370168089866638 + 10.0 * 8.943014144897461
Epoch 1210, val loss: 0.5926462411880493
Epoch 1220, training loss: 90.00811004638672 = 0.5346843600273132 + 10.0 * 8.947342872619629
Epoch 1220, val loss: 0.5918077230453491
Epoch 1230, training loss: 90.06409454345703 = 0.5323691368103027 + 10.0 * 8.95317268371582
Epoch 1230, val loss: 0.5901690125465393
Epoch 1240, training loss: 90.08076477050781 = 0.5300291180610657 + 10.0 * 8.955073356628418
Epoch 1240, val loss: 0.5888808965682983
Epoch 1250, training loss: 90.07151794433594 = 0.5277209877967834 + 10.0 * 8.95438003540039
Epoch 1250, val loss: 0.5873486399650574
Epoch 1260, training loss: 90.04669952392578 = 0.5253816843032837 + 10.0 * 8.952131271362305
Epoch 1260, val loss: 0.5860859155654907
Epoch 1270, training loss: 90.07627868652344 = 0.5231607556343079 + 10.0 * 8.95531177520752
Epoch 1270, val loss: 0.5849659442901611
Epoch 1280, training loss: 90.15223693847656 = 0.5208192467689514 + 10.0 * 8.963141441345215
Epoch 1280, val loss: 0.5837195515632629
Epoch 1290, training loss: 90.17972564697266 = 0.5184537768363953 + 10.0 * 8.966127395629883
Epoch 1290, val loss: 0.5822341442108154
Epoch 1300, training loss: 90.21109008789062 = 0.5161204934120178 + 10.0 * 8.969496726989746
Epoch 1300, val loss: 0.5809029340744019
Epoch 1310, training loss: 90.2573471069336 = 0.5139111280441284 + 10.0 * 8.974344253540039
Epoch 1310, val loss: 0.5793720483779907
Epoch 1320, training loss: 90.093505859375 = 0.5114882588386536 + 10.0 * 8.95820140838623
Epoch 1320, val loss: 0.5783278346061707
Epoch 1330, training loss: 90.13011932373047 = 0.509209394454956 + 10.0 * 8.962091445922852
Epoch 1330, val loss: 0.577122688293457
Epoch 1340, training loss: 90.16329956054688 = 0.5068942308425903 + 10.0 * 8.9656400680542
Epoch 1340, val loss: 0.5755437612533569
Epoch 1350, training loss: 90.25065612792969 = 0.50455242395401 + 10.0 * 8.974610328674316
Epoch 1350, val loss: 0.5745689272880554
Epoch 1360, training loss: 90.15577697753906 = 0.5021683573722839 + 10.0 * 8.965360641479492
Epoch 1360, val loss: 0.5732104778289795
Epoch 1370, training loss: 90.15758514404297 = 0.4999059736728668 + 10.0 * 8.965767860412598
Epoch 1370, val loss: 0.5716677308082581
Epoch 1380, training loss: 90.18839263916016 = 0.4976503551006317 + 10.0 * 8.969074249267578
Epoch 1380, val loss: 0.5708028674125671
Epoch 1390, training loss: 90.27699279785156 = 0.495344340801239 + 10.0 * 8.978164672851562
Epoch 1390, val loss: 0.569122850894928
Epoch 1400, training loss: 90.36084747314453 = 0.4929712414741516 + 10.0 * 8.986787796020508
Epoch 1400, val loss: 0.5679109692573547
Epoch 1410, training loss: 90.37647247314453 = 0.4905509650707245 + 10.0 * 8.988592147827148
Epoch 1410, val loss: 0.5664389133453369
Epoch 1420, training loss: 90.36092376708984 = 0.4881068170070648 + 10.0 * 8.987281799316406
Epoch 1420, val loss: 0.5650433897972107
Epoch 1430, training loss: 90.41800689697266 = 0.4857137203216553 + 10.0 * 8.993229866027832
Epoch 1430, val loss: 0.56360924243927
Epoch 1440, training loss: 90.38639068603516 = 0.48325449228286743 + 10.0 * 8.990313529968262
Epoch 1440, val loss: 0.5621888041496277
Epoch 1450, training loss: 90.10344696044922 = 0.4810652732849121 + 10.0 * 8.962238311767578
Epoch 1450, val loss: 0.5610119104385376
Epoch 1460, training loss: 90.17582702636719 = 0.478731632232666 + 10.0 * 8.969709396362305
Epoch 1460, val loss: 0.5589327216148376
Epoch 1470, training loss: 90.35108947753906 = 0.47637051343917847 + 10.0 * 8.987471580505371
Epoch 1470, val loss: 0.5592505931854248
Epoch 1480, training loss: 90.35830688476562 = 0.47383105754852295 + 10.0 * 8.988447189331055
Epoch 1480, val loss: 0.5570363402366638
Epoch 1490, training loss: 90.41768646240234 = 0.4713812470436096 + 10.0 * 8.994630813598633
Epoch 1490, val loss: 0.555471658706665
Epoch 1500, training loss: 90.4996109008789 = 0.468977689743042 + 10.0 * 9.003063201904297
Epoch 1500, val loss: 0.554298996925354
Epoch 1510, training loss: 90.49365234375 = 0.4664487838745117 + 10.0 * 9.002720832824707
Epoch 1510, val loss: 0.5527048707008362
Epoch 1520, training loss: 90.46432495117188 = 0.46398818492889404 + 10.0 * 9.000033378601074
Epoch 1520, val loss: 0.5516976714134216
Epoch 1530, training loss: 90.54075622558594 = 0.4615494906902313 + 10.0 * 9.00792121887207
Epoch 1530, val loss: 0.5503393411636353
Epoch 1540, training loss: 90.56026458740234 = 0.4590610861778259 + 10.0 * 9.010120391845703
Epoch 1540, val loss: 0.5488477945327759
Epoch 1550, training loss: 90.54316711425781 = 0.45655861496925354 + 10.0 * 9.008661270141602
Epoch 1550, val loss: 0.547724187374115
Epoch 1560, training loss: 90.57978057861328 = 0.4540633261203766 + 10.0 * 9.012571334838867
Epoch 1560, val loss: 0.5462357997894287
Epoch 1570, training loss: 90.59449768066406 = 0.4515703022480011 + 10.0 * 9.01429271697998
Epoch 1570, val loss: 0.5448400974273682
Epoch 1580, training loss: 90.59781646728516 = 0.44905775785446167 + 10.0 * 9.014875411987305
Epoch 1580, val loss: 0.5437354445457458
Epoch 1590, training loss: 90.6113052368164 = 0.4465530812740326 + 10.0 * 9.016474723815918
Epoch 1590, val loss: 0.5424631237983704
Epoch 1600, training loss: 90.63809967041016 = 0.4441679120063782 + 10.0 * 9.019392967224121
Epoch 1600, val loss: 0.5409005284309387
Epoch 1610, training loss: 90.6855697631836 = 0.4417245388031006 + 10.0 * 9.024384498596191
Epoch 1610, val loss: 0.5398999452590942
Epoch 1620, training loss: 90.7186050415039 = 0.4392898976802826 + 10.0 * 9.027931213378906
Epoch 1620, val loss: 0.5387075543403625
Epoch 1630, training loss: 90.73947143554688 = 0.43681636452674866 + 10.0 * 9.030265808105469
Epoch 1630, val loss: 0.5375803709030151
Epoch 1640, training loss: 90.73104858398438 = 0.43435171246528625 + 10.0 * 9.029669761657715
Epoch 1640, val loss: 0.536302387714386
Epoch 1650, training loss: 90.68890380859375 = 0.43192845582962036 + 10.0 * 9.025697708129883
Epoch 1650, val loss: 0.5350098013877869
Epoch 1660, training loss: 90.72122192382812 = 0.42950111627578735 + 10.0 * 9.02917194366455
Epoch 1660, val loss: 0.5337732434272766
Epoch 1670, training loss: 90.76876068115234 = 0.42710211873054504 + 10.0 * 9.03416633605957
Epoch 1670, val loss: 0.5327090620994568
Epoch 1680, training loss: 90.65879821777344 = 0.4246676564216614 + 10.0 * 9.023412704467773
Epoch 1680, val loss: 0.5318561792373657
Epoch 1690, training loss: 90.67236328125 = 0.4223332107067108 + 10.0 * 9.025003433227539
Epoch 1690, val loss: 0.5309235453605652
Epoch 1700, training loss: 90.75323486328125 = 0.41998594999313354 + 10.0 * 9.0333251953125
Epoch 1700, val loss: 0.5290884375572205
Epoch 1710, training loss: 90.84358215332031 = 0.41755497455596924 + 10.0 * 9.0426025390625
Epoch 1710, val loss: 0.5282292366027832
Epoch 1720, training loss: 90.81670379638672 = 0.41512370109558105 + 10.0 * 9.04015827178955
Epoch 1720, val loss: 0.5270784497261047
Epoch 1730, training loss: 90.84075164794922 = 0.41274845600128174 + 10.0 * 9.042799949645996
Epoch 1730, val loss: 0.526018500328064
Epoch 1740, training loss: 90.87296295166016 = 0.4103788733482361 + 10.0 * 9.046258926391602
Epoch 1740, val loss: 0.5250148773193359
Epoch 1750, training loss: 90.875 = 0.4080633819103241 + 10.0 * 9.046693801879883
Epoch 1750, val loss: 0.5239388942718506
Epoch 1760, training loss: 90.90911102294922 = 0.4057239592075348 + 10.0 * 9.050338745117188
Epoch 1760, val loss: 0.5229524970054626
Epoch 1770, training loss: 90.92111206054688 = 0.4033774733543396 + 10.0 * 9.051774024963379
Epoch 1770, val loss: 0.5219741463661194
Epoch 1780, training loss: 90.93052673339844 = 0.40103065967559814 + 10.0 * 9.052949905395508
Epoch 1780, val loss: 0.5209625363349915
Epoch 1790, training loss: 90.90196228027344 = 0.3986983299255371 + 10.0 * 9.050326347351074
Epoch 1790, val loss: 0.5198345184326172
Epoch 1800, training loss: 90.96709442138672 = 0.39655420184135437 + 10.0 * 9.05705451965332
Epoch 1800, val loss: 0.5200949907302856
Epoch 1810, training loss: 90.1614990234375 = 0.3947480618953705 + 10.0 * 8.976675033569336
Epoch 1810, val loss: 0.5181640982627869
Epoch 1820, training loss: 90.53986358642578 = 0.39279794692993164 + 10.0 * 9.0147066116333
Epoch 1820, val loss: 0.517401933670044
Epoch 1830, training loss: 90.58882904052734 = 0.39060381054878235 + 10.0 * 9.01982307434082
Epoch 1830, val loss: 0.5172496438026428
Epoch 1840, training loss: 90.66241455078125 = 0.38821929693222046 + 10.0 * 9.027419090270996
Epoch 1840, val loss: 0.5163443088531494
Epoch 1850, training loss: 90.76396179199219 = 0.38595911860466003 + 10.0 * 9.037800788879395
Epoch 1850, val loss: 0.5149857401847839
Epoch 1860, training loss: 90.8872299194336 = 0.38360342383384705 + 10.0 * 9.050362586975098
Epoch 1860, val loss: 0.5142133235931396
Epoch 1870, training loss: 90.95337677001953 = 0.38131028413772583 + 10.0 * 9.057207107543945
Epoch 1870, val loss: 0.5136890411376953
Epoch 1880, training loss: 91.0211410522461 = 0.37898021936416626 + 10.0 * 9.064215660095215
Epoch 1880, val loss: 0.5127568244934082
Epoch 1890, training loss: 91.01618957519531 = 0.37667736411094666 + 10.0 * 9.06395149230957
Epoch 1890, val loss: 0.5119665861129761
Epoch 1900, training loss: 91.07110595703125 = 0.3743914067745209 + 10.0 * 9.069671630859375
Epoch 1900, val loss: 0.5112290978431702
Epoch 1910, training loss: 91.068359375 = 0.3721108138561249 + 10.0 * 9.069624900817871
Epoch 1910, val loss: 0.5106456875801086
Epoch 1920, training loss: 91.03522491455078 = 0.3698277175426483 + 10.0 * 9.066539764404297
Epoch 1920, val loss: 0.5099390149116516
Epoch 1930, training loss: 91.12545013427734 = 0.36755549907684326 + 10.0 * 9.075789451599121
Epoch 1930, val loss: 0.5093663930892944
Epoch 1940, training loss: 91.18403625488281 = 0.3653343915939331 + 10.0 * 9.081870079040527
Epoch 1940, val loss: 0.5088118314743042
Epoch 1950, training loss: 91.13682556152344 = 0.3630986511707306 + 10.0 * 9.077372550964355
Epoch 1950, val loss: 0.5083723664283752
Epoch 1960, training loss: 91.17294311523438 = 0.36093640327453613 + 10.0 * 9.08120059967041
Epoch 1960, val loss: 0.507923424243927
Epoch 1970, training loss: 91.18635559082031 = 0.35875967144966125 + 10.0 * 9.082759857177734
Epoch 1970, val loss: 0.5076494812965393
Epoch 1980, training loss: 91.18621826171875 = 0.35664600133895874 + 10.0 * 9.08295726776123
Epoch 1980, val loss: 0.5069963335990906
Epoch 1990, training loss: 91.23328399658203 = 0.35453423857688904 + 10.0 * 9.087874412536621
Epoch 1990, val loss: 0.5066462159156799
Epoch 2000, training loss: 91.2587661743164 = 0.352411150932312 + 10.0 * 9.090635299682617
Epoch 2000, val loss: 0.5063759088516235
Epoch 2010, training loss: 91.22655487060547 = 0.35033443570137024 + 10.0 * 9.087621688842773
Epoch 2010, val loss: 0.5063439011573792
Epoch 2020, training loss: 91.25303649902344 = 0.3482525944709778 + 10.0 * 9.090478897094727
Epoch 2020, val loss: 0.5058839917182922
Epoch 2030, training loss: 91.30412292480469 = 0.34620699286460876 + 10.0 * 9.095791816711426
Epoch 2030, val loss: 0.5057194828987122
Epoch 2040, training loss: 91.3005142211914 = 0.34416747093200684 + 10.0 * 9.095634460449219
Epoch 2040, val loss: 0.5054258108139038
Epoch 2050, training loss: 91.28530883789062 = 0.3421213626861572 + 10.0 * 9.094318389892578
Epoch 2050, val loss: 0.5052659511566162
Epoch 2060, training loss: 91.34225463867188 = 0.34012869000434875 + 10.0 * 9.100213050842285
Epoch 2060, val loss: 0.5052223801612854
Epoch 2070, training loss: 91.17735290527344 = 0.33828842639923096 + 10.0 * 9.083906173706055
Epoch 2070, val loss: 0.5054796934127808
Epoch 2080, training loss: 91.16268920898438 = 0.3364546298980713 + 10.0 * 9.082623481750488
Epoch 2080, val loss: 0.5042184591293335
Epoch 2090, training loss: 91.1208267211914 = 0.33466216921806335 + 10.0 * 9.07861614227295
Epoch 2090, val loss: 0.5051971673965454
Epoch 2100, training loss: 91.09901428222656 = 0.3327006995677948 + 10.0 * 9.076631546020508
Epoch 2100, val loss: 0.504790186882019
Epoch 2110, training loss: 91.21774291992188 = 0.33082064986228943 + 10.0 * 9.088692665100098
Epoch 2110, val loss: 0.505164623260498
Epoch 2120, training loss: 91.31922149658203 = 0.3288552761077881 + 10.0 * 9.09903621673584
Epoch 2120, val loss: 0.5056285262107849
Epoch 2130, training loss: 91.37974548339844 = 0.3268957734107971 + 10.0 * 9.105284690856934
Epoch 2130, val loss: 0.5057793855667114
Epoch 2140, training loss: 91.37178039550781 = 0.32494789361953735 + 10.0 * 9.104682922363281
Epoch 2140, val loss: 0.5058432817459106
Epoch 2150, training loss: 91.17140197753906 = 0.32309791445732117 + 10.0 * 9.084830284118652
Epoch 2150, val loss: 0.5060656666755676
Epoch 2160, training loss: 91.27962493896484 = 0.32117629051208496 + 10.0 * 9.095845222473145
Epoch 2160, val loss: 0.5057556629180908
Epoch 2170, training loss: 91.35440826416016 = 0.31935662031173706 + 10.0 * 9.10350513458252
Epoch 2170, val loss: 0.5057807564735413
Epoch 2180, training loss: 91.43000030517578 = 0.3174908459186554 + 10.0 * 9.111250877380371
Epoch 2180, val loss: 0.5063080787658691
Epoch 2190, training loss: 91.4329833984375 = 0.3155937194824219 + 10.0 * 9.111738204956055
Epoch 2190, val loss: 0.5066953897476196
Epoch 2200, training loss: 91.43692779541016 = 0.313714861869812 + 10.0 * 9.112321853637695
Epoch 2200, val loss: 0.5073515176773071
Epoch 2210, training loss: 91.50279235839844 = 0.3118573725223541 + 10.0 * 9.11909294128418
Epoch 2210, val loss: 0.5073786377906799
Epoch 2220, training loss: 91.54378509521484 = 0.30998891592025757 + 10.0 * 9.123379707336426
Epoch 2220, val loss: 0.5076424479484558
Epoch 2230, training loss: 91.57936096191406 = 0.30812355875968933 + 10.0 * 9.127123832702637
Epoch 2230, val loss: 0.5082419514656067
Epoch 2240, training loss: 91.15480041503906 = 0.3066393733024597 + 10.0 * 9.084815979003906
Epoch 2240, val loss: 0.5070200562477112
Epoch 2250, training loss: 91.59137725830078 = 0.3047909438610077 + 10.0 * 9.128658294677734
Epoch 2250, val loss: 0.5086751580238342
Epoch 2260, training loss: 91.6160659790039 = 0.3030501902103424 + 10.0 * 9.131301879882812
Epoch 2260, val loss: 0.508812427520752
Epoch 2270, training loss: 91.7328872680664 = 0.3012711703777313 + 10.0 * 9.14316177368164
Epoch 2270, val loss: 0.5088375806808472
Epoch 2280, training loss: 91.77549743652344 = 0.29939958453178406 + 10.0 * 9.14760971069336
Epoch 2280, val loss: 0.5097845792770386
Epoch 2290, training loss: 91.8879623413086 = 0.29757821559906006 + 10.0 * 9.159038543701172
Epoch 2290, val loss: 0.5103691816329956
Epoch 2300, training loss: 91.92049407958984 = 0.29574066400527954 + 10.0 * 9.1624755859375
Epoch 2300, val loss: 0.5109722018241882
Epoch 2310, training loss: 91.93135833740234 = 0.2939058542251587 + 10.0 * 9.163744926452637
Epoch 2310, val loss: 0.5115876793861389
Epoch 2320, training loss: 91.96841430664062 = 0.29209521412849426 + 10.0 * 9.167631149291992
Epoch 2320, val loss: 0.5121121406555176
Epoch 2330, training loss: 91.95227813720703 = 0.29027578234672546 + 10.0 * 9.166200637817383
Epoch 2330, val loss: 0.5128341913223267
Epoch 2340, training loss: 91.93339538574219 = 0.2884969413280487 + 10.0 * 9.16448974609375
Epoch 2340, val loss: 0.5131075978279114
Epoch 2350, training loss: 91.98753356933594 = 0.28670576214790344 + 10.0 * 9.170083045959473
Epoch 2350, val loss: 0.5138526558876038
Epoch 2360, training loss: 92.05615997314453 = 0.2849085032939911 + 10.0 * 9.177124977111816
Epoch 2360, val loss: 0.5142644643783569
Epoch 2370, training loss: 92.03567504882812 = 0.28311020135879517 + 10.0 * 9.175256729125977
Epoch 2370, val loss: 0.5151603817939758
Epoch 2380, training loss: 92.08914184570312 = 0.2813253700733185 + 10.0 * 9.180781364440918
Epoch 2380, val loss: 0.5158959031105042
Epoch 2390, training loss: 92.0625991821289 = 0.2795241177082062 + 10.0 * 9.17830753326416
Epoch 2390, val loss: 0.5163906812667847
Epoch 2400, training loss: 92.06258392333984 = 0.2777698040008545 + 10.0 * 9.178481101989746
Epoch 2400, val loss: 0.5168867707252502
Epoch 2410, training loss: 92.05296325683594 = 0.2760014832019806 + 10.0 * 9.177696228027344
Epoch 2410, val loss: 0.5183815956115723
Epoch 2420, training loss: 92.10353088378906 = 0.2742128372192383 + 10.0 * 9.182931900024414
Epoch 2420, val loss: 0.5188543796539307
Epoch 2430, training loss: 92.10601043701172 = 0.2724359929561615 + 10.0 * 9.183357238769531
Epoch 2430, val loss: 0.5195363163948059
Epoch 2440, training loss: 92.13858795166016 = 0.27063798904418945 + 10.0 * 9.186795234680176
Epoch 2440, val loss: 0.520513117313385
Epoch 2450, training loss: 92.18030548095703 = 0.2688371539115906 + 10.0 * 9.191146850585938
Epoch 2450, val loss: 0.5211820602416992
Epoch 2460, training loss: 92.14749145507812 = 0.26708605885505676 + 10.0 * 9.188039779663086
Epoch 2460, val loss: 0.5225992798805237
Epoch 2470, training loss: 92.15914916992188 = 0.26531657576560974 + 10.0 * 9.189382553100586
Epoch 2470, val loss: 0.523124635219574
Epoch 2480, training loss: 92.1826400756836 = 0.2635355293750763 + 10.0 * 9.191910743713379
Epoch 2480, val loss: 0.5241827368736267
Epoch 2490, training loss: 92.20022583007812 = 0.2617620825767517 + 10.0 * 9.193845748901367
Epoch 2490, val loss: 0.5252726674079895
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8015942028985507
0.8125769760197059
=== training gcn model ===
Epoch 0, training loss: 104.05713653564453 = 1.0902273654937744 + 10.0 * 10.296690940856934
Epoch 0, val loss: 1.0892081260681152
Epoch 10, training loss: 100.28009796142578 = 1.0869070291519165 + 10.0 * 9.919319152832031
Epoch 10, val loss: 1.0859839916229248
Epoch 20, training loss: 98.37726593017578 = 1.083936095237732 + 10.0 * 9.72933292388916
Epoch 20, val loss: 1.083069086074829
Epoch 30, training loss: 96.9131088256836 = 1.0811513662338257 + 10.0 * 9.583195686340332
Epoch 30, val loss: 1.080336093902588
Epoch 40, training loss: 95.74149322509766 = 1.078557014465332 + 10.0 * 9.466293334960938
Epoch 40, val loss: 1.0777961015701294
Epoch 50, training loss: 94.77738952636719 = 1.076138973236084 + 10.0 * 9.370124816894531
Epoch 50, val loss: 1.0754338502883911
Epoch 60, training loss: 93.9542236328125 = 1.0738599300384521 + 10.0 * 9.288036346435547
Epoch 60, val loss: 1.0732169151306152
Epoch 70, training loss: 93.25193786621094 = 1.0717275142669678 + 10.0 * 9.218021392822266
Epoch 70, val loss: 1.071149468421936
Epoch 80, training loss: 92.65138244628906 = 1.0697189569473267 + 10.0 * 9.158166885375977
Epoch 80, val loss: 1.0692088603973389
Epoch 90, training loss: 92.14022064208984 = 1.0678186416625977 + 10.0 * 9.107240676879883
Epoch 90, val loss: 1.0673816204071045
Epoch 100, training loss: 91.7020263671875 = 1.0660330057144165 + 10.0 * 9.063599586486816
Epoch 100, val loss: 1.0656709671020508
Epoch 110, training loss: 91.31625366210938 = 1.064335823059082 + 10.0 * 9.025191307067871
Epoch 110, val loss: 1.0640451908111572
Epoch 120, training loss: 90.98733520507812 = 1.062711477279663 + 10.0 * 8.992462158203125
Epoch 120, val loss: 1.0624947547912598
Epoch 130, training loss: 90.70819091796875 = 1.0611600875854492 + 10.0 * 8.964703559875488
Epoch 130, val loss: 1.0610202550888062
Epoch 140, training loss: 90.47103118896484 = 1.0596665143966675 + 10.0 * 8.941136360168457
Epoch 140, val loss: 1.059593915939331
Epoch 150, training loss: 90.2656478881836 = 1.0582183599472046 + 10.0 * 8.920742988586426
Epoch 150, val loss: 1.0582222938537598
Epoch 160, training loss: 90.08799743652344 = 1.0568166971206665 + 10.0 * 8.903118133544922
Epoch 160, val loss: 1.056901454925537
Epoch 170, training loss: 89.9363784790039 = 1.055413842201233 + 10.0 * 8.888096809387207
Epoch 170, val loss: 1.0555692911148071
Epoch 180, training loss: 89.8204345703125 = 1.0540472269058228 + 10.0 * 8.876638412475586
Epoch 180, val loss: 1.0542618036270142
Epoch 190, training loss: 89.68014526367188 = 1.052606463432312 + 10.0 * 8.862753868103027
Epoch 190, val loss: 1.0529178380966187
Epoch 200, training loss: 89.61908721923828 = 1.051236867904663 + 10.0 * 8.85678482055664
Epoch 200, val loss: 1.0516074895858765
Epoch 210, training loss: 89.54291534423828 = 1.0497881174087524 + 10.0 * 8.849312782287598
Epoch 210, val loss: 1.0502222776412964
Epoch 220, training loss: 89.47130584716797 = 1.0482667684555054 + 10.0 * 8.842304229736328
Epoch 220, val loss: 1.048780083656311
Epoch 230, training loss: 89.38227844238281 = 1.046639323234558 + 10.0 * 8.833563804626465
Epoch 230, val loss: 1.047220230102539
Epoch 240, training loss: 89.3407974243164 = 1.0449799299240112 + 10.0 * 8.829581260681152
Epoch 240, val loss: 1.0456279516220093
Epoch 250, training loss: 89.29145050048828 = 1.0432653427124023 + 10.0 * 8.82481861114502
Epoch 250, val loss: 1.0439709424972534
Epoch 260, training loss: 89.23462677001953 = 1.041386604309082 + 10.0 * 8.819323539733887
Epoch 260, val loss: 1.0421719551086426
Epoch 270, training loss: 89.24481964111328 = 1.039414405822754 + 10.0 * 8.820540428161621
Epoch 270, val loss: 1.04026460647583
Epoch 280, training loss: 89.18315124511719 = 1.0372763872146606 + 10.0 * 8.814587593078613
Epoch 280, val loss: 1.0381349325180054
Epoch 290, training loss: 89.19506072998047 = 1.0350406169891357 + 10.0 * 8.816001892089844
Epoch 290, val loss: 1.035969614982605
Epoch 300, training loss: 89.18791961669922 = 1.0325132608413696 + 10.0 * 8.815540313720703
Epoch 300, val loss: 1.0335549116134644
Epoch 310, training loss: 89.12822723388672 = 1.0298439264297485 + 10.0 * 8.80983829498291
Epoch 310, val loss: 1.0309308767318726
Epoch 320, training loss: 89.16309356689453 = 1.027022361755371 + 10.0 * 8.813607215881348
Epoch 320, val loss: 1.0282059907913208
Epoch 330, training loss: 89.1397476196289 = 1.0239171981811523 + 10.0 * 8.811582565307617
Epoch 330, val loss: 1.0251778364181519
Epoch 340, training loss: 89.11845397949219 = 1.020495057106018 + 10.0 * 8.809796333312988
Epoch 340, val loss: 1.021883487701416
Epoch 350, training loss: 89.12213897705078 = 1.0167644023895264 + 10.0 * 8.810537338256836
Epoch 350, val loss: 1.0182522535324097
Epoch 360, training loss: 89.14185333251953 = 1.0127952098846436 + 10.0 * 8.812906265258789
Epoch 360, val loss: 1.0143290758132935
Epoch 370, training loss: 89.13431549072266 = 1.008406400680542 + 10.0 * 8.812590599060059
Epoch 370, val loss: 1.010063648223877
Epoch 380, training loss: 89.15020751953125 = 1.0037410259246826 + 10.0 * 8.81464672088623
Epoch 380, val loss: 1.005477786064148
Epoch 390, training loss: 89.19727325439453 = 0.9984358549118042 + 10.0 * 8.819883346557617
Epoch 390, val loss: 1.0004725456237793
Epoch 400, training loss: 89.16061401367188 = 0.9928721189498901 + 10.0 * 8.816774368286133
Epoch 400, val loss: 0.9948447942733765
Epoch 410, training loss: 89.44850158691406 = 0.987612783908844 + 10.0 * 8.846089363098145
Epoch 410, val loss: 0.9898349046707153
Epoch 420, training loss: 89.1419677734375 = 0.9808001518249512 + 10.0 * 8.816117286682129
Epoch 420, val loss: 0.983271598815918
Epoch 430, training loss: 89.25660705566406 = 0.9745206832885742 + 10.0 * 8.828208923339844
Epoch 430, val loss: 0.9771016240119934
Epoch 440, training loss: 89.32568359375 = 0.9676110744476318 + 10.0 * 8.835806846618652
Epoch 440, val loss: 0.9704760909080505
Epoch 450, training loss: 89.33980560302734 = 0.9602743983268738 + 10.0 * 8.837953567504883
Epoch 450, val loss: 0.9634238481521606
Epoch 460, training loss: 89.36397552490234 = 0.9525209665298462 + 10.0 * 8.841145515441895
Epoch 460, val loss: 0.9559171795845032
Epoch 470, training loss: 89.40950775146484 = 0.9445530772209167 + 10.0 * 8.846495628356934
Epoch 470, val loss: 0.948305070400238
Epoch 480, training loss: 89.40228271484375 = 0.9361048340797424 + 10.0 * 8.846617698669434
Epoch 480, val loss: 0.9403114318847656
Epoch 490, training loss: 89.36366271972656 = 0.9274915456771851 + 10.0 * 8.84361743927002
Epoch 490, val loss: 0.9320940375328064
Epoch 500, training loss: 89.39965057373047 = 0.9186936616897583 + 10.0 * 8.848095893859863
Epoch 500, val loss: 0.9236453175544739
Epoch 510, training loss: 89.43647766113281 = 0.909860372543335 + 10.0 * 8.852662086486816
Epoch 510, val loss: 0.9153238534927368
Epoch 520, training loss: 89.42444610595703 = 0.9008026123046875 + 10.0 * 8.852364540100098
Epoch 520, val loss: 0.9068721532821655
Epoch 530, training loss: 89.42506408691406 = 0.891435444355011 + 10.0 * 8.853363037109375
Epoch 530, val loss: 0.8979613184928894
Epoch 540, training loss: 89.43867492675781 = 0.88219153881073 + 10.0 * 8.855648040771484
Epoch 540, val loss: 0.889430582523346
Epoch 550, training loss: 89.48551940917969 = 0.8730807304382324 + 10.0 * 8.861244201660156
Epoch 550, val loss: 0.8810031414031982
Epoch 560, training loss: 89.44011688232422 = 0.8639087677001953 + 10.0 * 8.857621192932129
Epoch 560, val loss: 0.872488260269165
Epoch 570, training loss: 89.44633483886719 = 0.8548053503036499 + 10.0 * 8.859152793884277
Epoch 570, val loss: 0.8642051815986633
Epoch 580, training loss: 89.51737976074219 = 0.8461498022079468 + 10.0 * 8.867122650146484
Epoch 580, val loss: 0.8562372922897339
Epoch 590, training loss: 89.5323257446289 = 0.837563157081604 + 10.0 * 8.869476318359375
Epoch 590, val loss: 0.848461925983429
Epoch 600, training loss: 89.55108642578125 = 0.8292240500450134 + 10.0 * 8.872186660766602
Epoch 600, val loss: 0.8409087061882019
Epoch 610, training loss: 89.56229400634766 = 0.821075439453125 + 10.0 * 8.87412166595459
Epoch 610, val loss: 0.8335320353507996
Epoch 620, training loss: 89.57052612304688 = 0.8132082223892212 + 10.0 * 8.875731468200684
Epoch 620, val loss: 0.8265457153320312
Epoch 630, training loss: 89.5990219116211 = 0.8056058883666992 + 10.0 * 8.879342079162598
Epoch 630, val loss: 0.8197669982910156
Epoch 640, training loss: 89.59722900390625 = 0.7983146905899048 + 10.0 * 8.879891395568848
Epoch 640, val loss: 0.8132483959197998
Epoch 650, training loss: 89.64388275146484 = 0.7912322878837585 + 10.0 * 8.885265350341797
Epoch 650, val loss: 0.8069308996200562
Epoch 660, training loss: 89.64752197265625 = 0.7843155860900879 + 10.0 * 8.886320114135742
Epoch 660, val loss: 0.8008501529693604
Epoch 670, training loss: 89.6521224975586 = 0.777596116065979 + 10.0 * 8.887453079223633
Epoch 670, val loss: 0.7949281930923462
Epoch 680, training loss: 89.68243408203125 = 0.7712136507034302 + 10.0 * 8.891121864318848
Epoch 680, val loss: 0.7893092632293701
Epoch 690, training loss: 89.63883209228516 = 0.7648888826370239 + 10.0 * 8.887393951416016
Epoch 690, val loss: 0.7836977243423462
Epoch 700, training loss: 89.67259979248047 = 0.758887767791748 + 10.0 * 8.89137077331543
Epoch 700, val loss: 0.7784639596939087
Epoch 710, training loss: 89.64700317382812 = 0.752836287021637 + 10.0 * 8.889416694641113
Epoch 710, val loss: 0.7731294631958008
Epoch 720, training loss: 89.68159484863281 = 0.7472261786460876 + 10.0 * 8.893437385559082
Epoch 720, val loss: 0.768185019493103
Epoch 730, training loss: 89.70176696777344 = 0.7416089177131653 + 10.0 * 8.896016120910645
Epoch 730, val loss: 0.763244092464447
Epoch 740, training loss: 89.72344207763672 = 0.7361332178115845 + 10.0 * 8.898731231689453
Epoch 740, val loss: 0.7583653330802917
Epoch 750, training loss: 89.74189758300781 = 0.7306603193283081 + 10.0 * 8.901124000549316
Epoch 750, val loss: 0.753566324710846
Epoch 760, training loss: 89.78560638427734 = 0.7253298759460449 + 10.0 * 8.906027793884277
Epoch 760, val loss: 0.748884379863739
Epoch 770, training loss: 89.7401123046875 = 0.7201131582260132 + 10.0 * 8.901999473571777
Epoch 770, val loss: 0.7443474531173706
Epoch 780, training loss: 89.76849365234375 = 0.7146880030632019 + 10.0 * 8.905380249023438
Epoch 780, val loss: 0.7394582629203796
Epoch 790, training loss: 89.75836944580078 = 0.7098892331123352 + 10.0 * 8.904848098754883
Epoch 790, val loss: 0.7352418303489685
Epoch 800, training loss: 89.75749969482422 = 0.7048851847648621 + 10.0 * 8.905261039733887
Epoch 800, val loss: 0.7307747602462769
Epoch 810, training loss: 89.78836059570312 = 0.6999245882034302 + 10.0 * 8.908843040466309
Epoch 810, val loss: 0.7263426780700684
Epoch 820, training loss: 89.8302230834961 = 0.6951236128807068 + 10.0 * 8.9135103225708
Epoch 820, val loss: 0.7219749093055725
Epoch 830, training loss: 89.88758087158203 = 0.6903144717216492 + 10.0 * 8.919726371765137
Epoch 830, val loss: 0.7176805734634399
Epoch 840, training loss: 89.90005493164062 = 0.6855859160423279 + 10.0 * 8.921446800231934
Epoch 840, val loss: 0.7134155035018921
Epoch 850, training loss: 89.96530151367188 = 0.6808457374572754 + 10.0 * 8.928445816040039
Epoch 850, val loss: 0.7090965509414673
Epoch 860, training loss: 89.97969818115234 = 0.6761481165885925 + 10.0 * 8.930355072021484
Epoch 860, val loss: 0.7048467397689819
Epoch 870, training loss: 89.98407745361328 = 0.6715461015701294 + 10.0 * 8.931253433227539
Epoch 870, val loss: 0.7006622552871704
Epoch 880, training loss: 90.00831604003906 = 0.667005717754364 + 10.0 * 8.934130668640137
Epoch 880, val loss: 0.6964836120605469
Epoch 890, training loss: 90.03105926513672 = 0.6624855399131775 + 10.0 * 8.936857223510742
Epoch 890, val loss: 0.692337691783905
Epoch 900, training loss: 90.06818389892578 = 0.6580001711845398 + 10.0 * 8.941019058227539
Epoch 900, val loss: 0.6883001327514648
Epoch 910, training loss: 90.09734344482422 = 0.6536024808883667 + 10.0 * 8.944374084472656
Epoch 910, val loss: 0.6842606663703918
Epoch 920, training loss: 90.10592651367188 = 0.6492305994033813 + 10.0 * 8.945669174194336
Epoch 920, val loss: 0.6803249716758728
Epoch 930, training loss: 90.16641235351562 = 0.6448706388473511 + 10.0 * 8.952154159545898
Epoch 930, val loss: 0.6763758659362793
Epoch 940, training loss: 90.1915512084961 = 0.6405073404312134 + 10.0 * 8.955103874206543
Epoch 940, val loss: 0.6724764108657837
Epoch 950, training loss: 90.15606689453125 = 0.6363416314125061 + 10.0 * 8.951972007751465
Epoch 950, val loss: 0.6687033176422119
Epoch 960, training loss: 90.15547943115234 = 0.632192075252533 + 10.0 * 8.9523286819458
Epoch 960, val loss: 0.6650272607803345
Epoch 970, training loss: 90.14619445800781 = 0.6281090378761292 + 10.0 * 8.951807975769043
Epoch 970, val loss: 0.6613376140594482
Epoch 980, training loss: 90.23541259765625 = 0.6241445541381836 + 10.0 * 8.961126327514648
Epoch 980, val loss: 0.6578818559646606
Epoch 990, training loss: 90.2903823852539 = 0.6202195882797241 + 10.0 * 8.967016220092773
Epoch 990, val loss: 0.6544076204299927
Epoch 1000, training loss: 90.45381164550781 = 0.6163025498390198 + 10.0 * 8.98375129699707
Epoch 1000, val loss: 0.6511520743370056
Epoch 1010, training loss: 90.45750427246094 = 0.6125344634056091 + 10.0 * 8.9844970703125
Epoch 1010, val loss: 0.6477075219154358
Epoch 1020, training loss: 90.54039001464844 = 0.6087552309036255 + 10.0 * 8.993163108825684
Epoch 1020, val loss: 0.6444767117500305
Epoch 1030, training loss: 90.61024475097656 = 0.6051276326179504 + 10.0 * 9.00051212310791
Epoch 1030, val loss: 0.641391932964325
Epoch 1040, training loss: 90.6495590209961 = 0.6015684604644775 + 10.0 * 9.004798889160156
Epoch 1040, val loss: 0.6382721662521362
Epoch 1050, training loss: 90.65486907958984 = 0.5981072187423706 + 10.0 * 9.00567626953125
Epoch 1050, val loss: 0.635330855846405
Epoch 1060, training loss: 90.68268585205078 = 0.5947557687759399 + 10.0 * 9.008792877197266
Epoch 1060, val loss: 0.6326128840446472
Epoch 1070, training loss: 90.63934326171875 = 0.5914450883865356 + 10.0 * 9.004789352416992
Epoch 1070, val loss: 0.6298598647117615
Epoch 1080, training loss: 90.68171691894531 = 0.5882619619369507 + 10.0 * 9.009345054626465
Epoch 1080, val loss: 0.6271849870681763
Epoch 1090, training loss: 90.77056121826172 = 0.5851773023605347 + 10.0 * 9.018538475036621
Epoch 1090, val loss: 0.6246770024299622
Epoch 1100, training loss: 90.77510833740234 = 0.5820062756538391 + 10.0 * 9.019309997558594
Epoch 1100, val loss: 0.6221151947975159
Epoch 1110, training loss: 90.7715072631836 = 0.5788552761077881 + 10.0 * 9.019265174865723
Epoch 1110, val loss: 0.6195542812347412
Epoch 1120, training loss: 90.8038330078125 = 0.5758857131004333 + 10.0 * 9.022794723510742
Epoch 1120, val loss: 0.6170974969863892
Epoch 1130, training loss: 90.8520278930664 = 0.5728669762611389 + 10.0 * 9.027915954589844
Epoch 1130, val loss: 0.6147058010101318
Epoch 1140, training loss: 90.85285949707031 = 0.5698195099830627 + 10.0 * 9.028304100036621
Epoch 1140, val loss: 0.61235511302948
Epoch 1150, training loss: 90.82229614257812 = 0.5669582486152649 + 10.0 * 9.025533676147461
Epoch 1150, val loss: 0.6100161075592041
Epoch 1160, training loss: 90.9114990234375 = 0.5641059875488281 + 10.0 * 9.03473949432373
Epoch 1160, val loss: 0.6076979041099548
Epoch 1170, training loss: 90.92630004882812 = 0.5612185597419739 + 10.0 * 9.036508560180664
Epoch 1170, val loss: 0.6054551601409912
Epoch 1180, training loss: 90.9608383178711 = 0.558348536491394 + 10.0 * 9.04024887084961
Epoch 1180, val loss: 0.6031988859176636
Epoch 1190, training loss: 90.95420837402344 = 0.5556933879852295 + 10.0 * 9.039851188659668
Epoch 1190, val loss: 0.6011543273925781
Epoch 1200, training loss: 91.02133178710938 = 0.5529347658157349 + 10.0 * 9.046839714050293
Epoch 1200, val loss: 0.5989755988121033
Epoch 1210, training loss: 91.070556640625 = 0.5501641035079956 + 10.0 * 9.05203914642334
Epoch 1210, val loss: 0.5967792868614197
Epoch 1220, training loss: 90.97132110595703 = 0.547295093536377 + 10.0 * 9.042402267456055
Epoch 1220, val loss: 0.5945637226104736
Epoch 1230, training loss: 91.03177642822266 = 0.5446346402168274 + 10.0 * 9.048714637756348
Epoch 1230, val loss: 0.5924451947212219
Epoch 1240, training loss: 91.07533264160156 = 0.5419446229934692 + 10.0 * 9.053339004516602
Epoch 1240, val loss: 0.5904420018196106
Epoch 1250, training loss: 91.11663818359375 = 0.539237916469574 + 10.0 * 9.057740211486816
Epoch 1250, val loss: 0.5883716344833374
Epoch 1260, training loss: 91.15677642822266 = 0.5365117192268372 + 10.0 * 9.062026023864746
Epoch 1260, val loss: 0.5862187147140503
Epoch 1270, training loss: 91.1520767211914 = 0.5337821841239929 + 10.0 * 9.061829566955566
Epoch 1270, val loss: 0.5840973854064941
Epoch 1280, training loss: 91.1269760131836 = 0.5310778617858887 + 10.0 * 9.059590339660645
Epoch 1280, val loss: 0.582001805305481
Epoch 1290, training loss: 91.1934814453125 = 0.5284114480018616 + 10.0 * 9.066507339477539
Epoch 1290, val loss: 0.5799728035926819
Epoch 1300, training loss: 91.14130401611328 = 0.5257515907287598 + 10.0 * 9.061555862426758
Epoch 1300, val loss: 0.5778317451477051
Epoch 1310, training loss: 91.14094543457031 = 0.5231795310974121 + 10.0 * 9.061777114868164
Epoch 1310, val loss: 0.575838029384613
Epoch 1320, training loss: 91.15933227539062 = 0.5205267667770386 + 10.0 * 9.063880920410156
Epoch 1320, val loss: 0.5739144682884216
Epoch 1330, training loss: 91.258056640625 = 0.5178203582763672 + 10.0 * 9.074023246765137
Epoch 1330, val loss: 0.5717368125915527
Epoch 1340, training loss: 91.24706268310547 = 0.5150063037872314 + 10.0 * 9.073205947875977
Epoch 1340, val loss: 0.5696499943733215
Epoch 1350, training loss: 91.26129150390625 = 0.512253999710083 + 10.0 * 9.07490348815918
Epoch 1350, val loss: 0.5675248503684998
Epoch 1360, training loss: 91.30194091796875 = 0.5095470547676086 + 10.0 * 9.079239845275879
Epoch 1360, val loss: 0.5654007196426392
Epoch 1370, training loss: 91.35045623779297 = 0.5067991018295288 + 10.0 * 9.084365844726562
Epoch 1370, val loss: 0.5634052157402039
Epoch 1380, training loss: 91.31949615478516 = 0.5039472579956055 + 10.0 * 9.081555366516113
Epoch 1380, val loss: 0.5611210465431213
Epoch 1390, training loss: 91.3416976928711 = 0.5011960864067078 + 10.0 * 9.084050178527832
Epoch 1390, val loss: 0.5591177344322205
Epoch 1400, training loss: 91.30875396728516 = 0.49837324023246765 + 10.0 * 9.081037521362305
Epoch 1400, val loss: 0.5569383502006531
Epoch 1410, training loss: 91.36759948730469 = 0.49551287293434143 + 10.0 * 9.08720874786377
Epoch 1410, val loss: 0.5546552538871765
Epoch 1420, training loss: 91.41510772705078 = 0.49262842535972595 + 10.0 * 9.09224796295166
Epoch 1420, val loss: 0.5524987578392029
Epoch 1430, training loss: 91.30707550048828 = 0.4898071587085724 + 10.0 * 9.081727027893066
Epoch 1430, val loss: 0.550271213054657
Epoch 1440, training loss: 91.33513641357422 = 0.4869622588157654 + 10.0 * 9.084817886352539
Epoch 1440, val loss: 0.5482356548309326
Epoch 1450, training loss: 91.35134887695312 = 0.48405468463897705 + 10.0 * 9.086729049682617
Epoch 1450, val loss: 0.546133816242218
Epoch 1460, training loss: 91.33636474609375 = 0.48118409514427185 + 10.0 * 9.085517883300781
Epoch 1460, val loss: 0.5439031720161438
Epoch 1470, training loss: 91.4128646850586 = 0.47825461626052856 + 10.0 * 9.093461036682129
Epoch 1470, val loss: 0.5417068004608154
Epoch 1480, training loss: 91.49755859375 = 0.47528398036956787 + 10.0 * 9.102228164672852
Epoch 1480, val loss: 0.5395330190658569
Epoch 1490, training loss: 91.53580474853516 = 0.47233423590660095 + 10.0 * 9.10634708404541
Epoch 1490, val loss: 0.5374107360839844
Epoch 1500, training loss: 91.5478744506836 = 0.469343900680542 + 10.0 * 9.107852935791016
Epoch 1500, val loss: 0.5352485775947571
Epoch 1510, training loss: 91.5398941040039 = 0.4663723111152649 + 10.0 * 9.107352256774902
Epoch 1510, val loss: 0.5330700278282166
Epoch 1520, training loss: 91.56428527832031 = 0.4634073078632355 + 10.0 * 9.110087394714355
Epoch 1520, val loss: 0.530910313129425
Epoch 1530, training loss: 91.58061981201172 = 0.4604438841342926 + 10.0 * 9.112017631530762
Epoch 1530, val loss: 0.5288122296333313
Epoch 1540, training loss: 91.62330627441406 = 0.45744815468788147 + 10.0 * 9.116585731506348
Epoch 1540, val loss: 0.5266213417053223
Epoch 1550, training loss: 91.61015319824219 = 0.4545178711414337 + 10.0 * 9.11556339263916
Epoch 1550, val loss: 0.5244747400283813
Epoch 1560, training loss: 91.55467987060547 = 0.45177268981933594 + 10.0 * 9.11029052734375
Epoch 1560, val loss: 0.5227304100990295
Epoch 1570, training loss: 91.56999969482422 = 0.44890955090522766 + 10.0 * 9.112109184265137
Epoch 1570, val loss: 0.5208255648612976
Epoch 1580, training loss: 91.65188598632812 = 0.44598904252052307 + 10.0 * 9.120589256286621
Epoch 1580, val loss: 0.518869936466217
Epoch 1590, training loss: 91.63946533203125 = 0.4429885149002075 + 10.0 * 9.119647979736328
Epoch 1590, val loss: 0.5166376233100891
Epoch 1600, training loss: 91.62321472167969 = 0.4400006830692291 + 10.0 * 9.118321418762207
Epoch 1600, val loss: 0.5146982073783875
Epoch 1610, training loss: 91.68013000488281 = 0.4370708167552948 + 10.0 * 9.124305725097656
Epoch 1610, val loss: 0.5127207636833191
Epoch 1620, training loss: 91.68647766113281 = 0.4341076612472534 + 10.0 * 9.125237464904785
Epoch 1620, val loss: 0.5107870101928711
Epoch 1630, training loss: 91.72916412353516 = 0.4311619699001312 + 10.0 * 9.129800796508789
Epoch 1630, val loss: 0.5088058710098267
Epoch 1640, training loss: 91.71922302246094 = 0.42825058102607727 + 10.0 * 9.129096984863281
Epoch 1640, val loss: 0.5069180130958557
Epoch 1650, training loss: 91.7799072265625 = 0.42537087202072144 + 10.0 * 9.135454177856445
Epoch 1650, val loss: 0.5052297711372375
Epoch 1660, training loss: 91.77814483642578 = 0.4225049614906311 + 10.0 * 9.135563850402832
Epoch 1660, val loss: 0.5033970475196838
Epoch 1670, training loss: 91.7969970703125 = 0.4196644127368927 + 10.0 * 9.137733459472656
Epoch 1670, val loss: 0.5015266537666321
Epoch 1680, training loss: 91.80091857910156 = 0.41686591506004333 + 10.0 * 9.138405799865723
Epoch 1680, val loss: 0.4999622404575348
Epoch 1690, training loss: 91.82898712158203 = 0.41405338048934937 + 10.0 * 9.14149284362793
Epoch 1690, val loss: 0.4981399476528168
Epoch 1700, training loss: 91.83024597167969 = 0.41129690408706665 + 10.0 * 9.141894340515137
Epoch 1700, val loss: 0.4966222643852234
Epoch 1710, training loss: 91.65882110595703 = 0.4089023470878601 + 10.0 * 9.124991416931152
Epoch 1710, val loss: 0.49555251002311707
Epoch 1720, training loss: 91.51530456542969 = 0.4063877761363983 + 10.0 * 9.110891342163086
Epoch 1720, val loss: 0.4934191107749939
Epoch 1730, training loss: 91.6180648803711 = 0.4042265713214874 + 10.0 * 9.121383666992188
Epoch 1730, val loss: 0.4928731620311737
Epoch 1740, training loss: 91.61164093017578 = 0.40156030654907227 + 10.0 * 9.121007919311523
Epoch 1740, val loss: 0.491610586643219
Epoch 1750, training loss: 91.57073211669922 = 0.3989400267601013 + 10.0 * 9.117178916931152
Epoch 1750, val loss: 0.4889744818210602
Epoch 1760, training loss: 91.6776123046875 = 0.3962920308113098 + 10.0 * 9.128131866455078
Epoch 1760, val loss: 0.4881573021411896
Epoch 1770, training loss: 91.7723388671875 = 0.39374154806137085 + 10.0 * 9.137860298156738
Epoch 1770, val loss: 0.486894428730011
Epoch 1780, training loss: 91.84878540039062 = 0.3911570906639099 + 10.0 * 9.14576244354248
Epoch 1780, val loss: 0.4855942726135254
Epoch 1790, training loss: 91.8566665649414 = 0.3886001706123352 + 10.0 * 9.146806716918945
Epoch 1790, val loss: 0.4843652844429016
Epoch 1800, training loss: 91.89163970947266 = 0.3860979676246643 + 10.0 * 9.150553703308105
Epoch 1800, val loss: 0.48322248458862305
Epoch 1810, training loss: 91.8922348022461 = 0.383635550737381 + 10.0 * 9.150859832763672
Epoch 1810, val loss: 0.48201748728752136
Epoch 1820, training loss: 91.89702606201172 = 0.38121768832206726 + 10.0 * 9.151580810546875
Epoch 1820, val loss: 0.48098719120025635
Epoch 1830, training loss: 91.92790222167969 = 0.3788374066352844 + 10.0 * 9.154906272888184
Epoch 1830, val loss: 0.4800381362438202
Epoch 1840, training loss: 91.86071014404297 = 0.37650012969970703 + 10.0 * 9.148420333862305
Epoch 1840, val loss: 0.47915294766426086
Epoch 1850, training loss: 91.89301300048828 = 0.3742896020412445 + 10.0 * 9.151872634887695
Epoch 1850, val loss: 0.4778634309768677
Epoch 1860, training loss: 91.98109436035156 = 0.3720216155052185 + 10.0 * 9.160906791687012
Epoch 1860, val loss: 0.4773681163787842
Epoch 1870, training loss: 92.00827026367188 = 0.3697911202907562 + 10.0 * 9.163847923278809
Epoch 1870, val loss: 0.47627413272857666
Epoch 1880, training loss: 92.0284652709961 = 0.3675394654273987 + 10.0 * 9.166092872619629
Epoch 1880, val loss: 0.47571608424186707
Epoch 1890, training loss: 92.05245971679688 = 0.36532777547836304 + 10.0 * 9.168713569641113
Epoch 1890, val loss: 0.4749457836151123
Epoch 1900, training loss: 92.05451202392578 = 0.36313706636428833 + 10.0 * 9.169137954711914
Epoch 1900, val loss: 0.47414839267730713
Epoch 1910, training loss: 91.91464233398438 = 0.36109963059425354 + 10.0 * 9.155354499816895
Epoch 1910, val loss: 0.4744282066822052
Epoch 1920, training loss: 91.85665130615234 = 0.359110027551651 + 10.0 * 9.149754524230957
Epoch 1920, val loss: 0.4729958474636078
Epoch 1930, training loss: 91.92044067382812 = 0.3570619821548462 + 10.0 * 9.15633773803711
Epoch 1930, val loss: 0.4723949134349823
Epoch 1940, training loss: 92.00000762939453 = 0.35504481196403503 + 10.0 * 9.164496421813965
Epoch 1940, val loss: 0.4719354808330536
Epoch 1950, training loss: 92.08158111572266 = 0.3529842793941498 + 10.0 * 9.172860145568848
Epoch 1950, val loss: 0.4715704023838043
Epoch 1960, training loss: 92.12657928466797 = 0.35090991854667664 + 10.0 * 9.177567481994629
Epoch 1960, val loss: 0.4710427224636078
Epoch 1970, training loss: 92.13774871826172 = 0.3488735556602478 + 10.0 * 9.178887367248535
Epoch 1970, val loss: 0.47066280245780945
Epoch 1980, training loss: 92.1302719116211 = 0.34685850143432617 + 10.0 * 9.17834186553955
Epoch 1980, val loss: 0.47034627199172974
Epoch 1990, training loss: 92.16011810302734 = 0.34486889839172363 + 10.0 * 9.181524276733398
Epoch 1990, val loss: 0.4701187014579773
Epoch 2000, training loss: 92.1785659790039 = 0.3428898751735687 + 10.0 * 9.183568000793457
Epoch 2000, val loss: 0.4697470963001251
Epoch 2010, training loss: 92.15699768066406 = 0.3409387767314911 + 10.0 * 9.181605339050293
Epoch 2010, val loss: 0.46959376335144043
Epoch 2020, training loss: 92.15643310546875 = 0.3390251100063324 + 10.0 * 9.181740760803223
Epoch 2020, val loss: 0.46932074427604675
Epoch 2030, training loss: 92.17489624023438 = 0.33713647723197937 + 10.0 * 9.183775901794434
Epoch 2030, val loss: 0.4690943658351898
Epoch 2040, training loss: 92.21864318847656 = 0.33525681495666504 + 10.0 * 9.188338279724121
Epoch 2040, val loss: 0.46899035573005676
Epoch 2050, training loss: 92.17095947265625 = 0.3333838880062103 + 10.0 * 9.183757781982422
Epoch 2050, val loss: 0.4689789116382599
Epoch 2060, training loss: 92.1471176147461 = 0.3315538763999939 + 10.0 * 9.181556701660156
Epoch 2060, val loss: 0.46904274821281433
Epoch 2070, training loss: 92.21822357177734 = 0.3297887444496155 + 10.0 * 9.188843727111816
Epoch 2070, val loss: 0.4690518081188202
Epoch 2080, training loss: 92.2930908203125 = 0.3279559910297394 + 10.0 * 9.196513175964355
Epoch 2080, val loss: 0.46881452202796936
Epoch 2090, training loss: 92.26610565185547 = 0.32609859108924866 + 10.0 * 9.194001197814941
Epoch 2090, val loss: 0.46890315413475037
Epoch 2100, training loss: 92.23001861572266 = 0.32424357533454895 + 10.0 * 9.190577507019043
Epoch 2100, val loss: 0.4692317545413971
Epoch 2110, training loss: 92.2551498413086 = 0.3224429190158844 + 10.0 * 9.193270683288574
Epoch 2110, val loss: 0.46934565901756287
Epoch 2120, training loss: 92.31377410888672 = 0.3206295371055603 + 10.0 * 9.19931411743164
Epoch 2120, val loss: 0.46925273537635803
Epoch 2130, training loss: 92.36016845703125 = 0.3188125491142273 + 10.0 * 9.20413589477539
Epoch 2130, val loss: 0.4693279564380646
Epoch 2140, training loss: 92.29281616210938 = 0.31703633069992065 + 10.0 * 9.197577476501465
Epoch 2140, val loss: 0.46998804807662964
Epoch 2150, training loss: 92.08502960205078 = 0.3153434097766876 + 10.0 * 9.176968574523926
Epoch 2150, val loss: 0.4689179062843323
Epoch 2160, training loss: 92.14773559570312 = 0.3142124116420746 + 10.0 * 9.18335247039795
Epoch 2160, val loss: 0.4685869514942169
Epoch 2170, training loss: 92.1616439819336 = 0.31214407086372375 + 10.0 * 9.18494987487793
Epoch 2170, val loss: 0.4693622589111328
Epoch 2180, training loss: 92.1799087524414 = 0.3104689419269562 + 10.0 * 9.186944007873535
Epoch 2180, val loss: 0.46999430656433105
Epoch 2190, training loss: 92.09382629394531 = 0.30894801020622253 + 10.0 * 9.178487777709961
Epoch 2190, val loss: 0.4697260558605194
Epoch 2200, training loss: 92.10551452636719 = 0.30723831057548523 + 10.0 * 9.179827690124512
Epoch 2200, val loss: 0.47025391459465027
Epoch 2210, training loss: 92.26631927490234 = 0.30545201897621155 + 10.0 * 9.196086883544922
Epoch 2210, val loss: 0.4708169102668762
Epoch 2220, training loss: 92.30903625488281 = 0.30369266867637634 + 10.0 * 9.200533866882324
Epoch 2220, val loss: 0.4712981879711151
Epoch 2230, training loss: 92.3296890258789 = 0.3019217252731323 + 10.0 * 9.202776908874512
Epoch 2230, val loss: 0.4716630280017853
Epoch 2240, training loss: 92.36934661865234 = 0.30016234517097473 + 10.0 * 9.206918716430664
Epoch 2240, val loss: 0.4720171391963959
Epoch 2250, training loss: 92.36485290527344 = 0.2984418570995331 + 10.0 * 9.20664119720459
Epoch 2250, val loss: 0.47219178080558777
Epoch 2260, training loss: 92.40322875976562 = 0.2967095375061035 + 10.0 * 9.210652351379395
Epoch 2260, val loss: 0.47268354892730713
Epoch 2270, training loss: 92.37661743164062 = 0.29502585530281067 + 10.0 * 9.208158493041992
Epoch 2270, val loss: 0.47323769330978394
Epoch 2280, training loss: 92.38558959960938 = 0.2933116853237152 + 10.0 * 9.209227561950684
Epoch 2280, val loss: 0.47368255257606506
Epoch 2290, training loss: 92.41471862792969 = 0.2915998101234436 + 10.0 * 9.212311744689941
Epoch 2290, val loss: 0.4742538332939148
Epoch 2300, training loss: 92.44454956054688 = 0.2898750305175781 + 10.0 * 9.21546745300293
Epoch 2300, val loss: 0.4748516082763672
Epoch 2310, training loss: 92.42584228515625 = 0.28816482424736023 + 10.0 * 9.213768005371094
Epoch 2310, val loss: 0.47524821758270264
Epoch 2320, training loss: 92.41239166259766 = 0.28647279739379883 + 10.0 * 9.212591171264648
Epoch 2320, val loss: 0.4761769473552704
Epoch 2330, training loss: 92.43730926513672 = 0.28479090332984924 + 10.0 * 9.215251922607422
Epoch 2330, val loss: 0.4764927625656128
Epoch 2340, training loss: 92.47891998291016 = 0.2831009030342102 + 10.0 * 9.219581604003906
Epoch 2340, val loss: 0.47702062129974365
Epoch 2350, training loss: 92.49107360839844 = 0.28141334652900696 + 10.0 * 9.220966339111328
Epoch 2350, val loss: 0.47749266028404236
Epoch 2360, training loss: 92.48938751220703 = 0.2797448933124542 + 10.0 * 9.220964431762695
Epoch 2360, val loss: 0.4782406687736511
Epoch 2370, training loss: 92.50525665283203 = 0.27806583046913147 + 10.0 * 9.222719192504883
Epoch 2370, val loss: 0.47877416014671326
Epoch 2380, training loss: 92.49998474121094 = 0.276390016078949 + 10.0 * 9.222359657287598
Epoch 2380, val loss: 0.47942429780960083
Epoch 2390, training loss: 92.51895141601562 = 0.27474600076675415 + 10.0 * 9.224420547485352
Epoch 2390, val loss: 0.4803321361541748
Epoch 2400, training loss: 92.5557632446289 = 0.27307766675949097 + 10.0 * 9.22826862335205
Epoch 2400, val loss: 0.480841726064682
Epoch 2410, training loss: 92.5574722290039 = 0.2714032530784607 + 10.0 * 9.228607177734375
Epoch 2410, val loss: 0.48130783438682556
Epoch 2420, training loss: 92.53799438476562 = 0.2697395384311676 + 10.0 * 9.226825714111328
Epoch 2420, val loss: 0.48236867785453796
Epoch 2430, training loss: 92.48825073242188 = 0.2683994174003601 + 10.0 * 9.22198486328125
Epoch 2430, val loss: 0.48346248269081116
Epoch 2440, training loss: 92.46956634521484 = 0.2667959928512573 + 10.0 * 9.220277786254883
Epoch 2440, val loss: 0.48372602462768555
Epoch 2450, training loss: 92.5333023071289 = 0.26508527994155884 + 10.0 * 9.226821899414062
Epoch 2450, val loss: 0.48495015501976013
Epoch 2460, training loss: 92.55131530761719 = 0.26339566707611084 + 10.0 * 9.228792190551758
Epoch 2460, val loss: 0.48536595702171326
Epoch 2470, training loss: 92.57882690429688 = 0.26171359419822693 + 10.0 * 9.231711387634277
Epoch 2470, val loss: 0.48612967133522034
Epoch 2480, training loss: 92.59341430664062 = 0.26003721356391907 + 10.0 * 9.23333740234375
Epoch 2480, val loss: 0.4870733618736267
Epoch 2490, training loss: 92.55829620361328 = 0.2583784759044647 + 10.0 * 9.229991912841797
Epoch 2490, val loss: 0.48781776428222656
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8059420289855073
0.8106933275374919
=== training gcn model ===
Epoch 0, training loss: 103.68392944335938 = 1.1200600862503052 + 10.0 * 10.256386756896973
Epoch 0, val loss: 1.12025785446167
Epoch 10, training loss: 99.85381317138672 = 1.115094542503357 + 10.0 * 9.873871803283691
Epoch 10, val loss: 1.1153022050857544
Epoch 20, training loss: 97.83021545410156 = 1.1104693412780762 + 10.0 * 9.671975135803223
Epoch 20, val loss: 1.1106737852096558
Epoch 30, training loss: 96.41574096679688 = 1.1060279607772827 + 10.0 * 9.53097152709961
Epoch 30, val loss: 1.1062356233596802
Epoch 40, training loss: 95.31245422363281 = 1.1017976999282837 + 10.0 * 9.421065330505371
Epoch 40, val loss: 1.1020159721374512
Epoch 50, training loss: 94.4073486328125 = 1.0977654457092285 + 10.0 * 9.330958366394043
Epoch 50, val loss: 1.0979927778244019
Epoch 60, training loss: 93.6300277709961 = 1.0938998460769653 + 10.0 * 9.253612518310547
Epoch 60, val loss: 1.0941442251205444
Epoch 70, training loss: 92.97647857666016 = 1.090214729309082 + 10.0 * 9.188626289367676
Epoch 70, val loss: 1.0904821157455444
Epoch 80, training loss: 92.41972351074219 = 1.086696743965149 + 10.0 * 9.133302688598633
Epoch 80, val loss: 1.086988925933838
Epoch 90, training loss: 91.93766784667969 = 1.0833287239074707 + 10.0 * 9.085433959960938
Epoch 90, val loss: 1.0836546421051025
Epoch 100, training loss: 91.51448059082031 = 1.0801057815551758 + 10.0 * 9.043437004089355
Epoch 100, val loss: 1.0804616212844849
Epoch 110, training loss: 91.15447998046875 = 1.0770422220230103 + 10.0 * 9.007743835449219
Epoch 110, val loss: 1.0774383544921875
Epoch 120, training loss: 90.84276580810547 = 1.0740886926651 + 10.0 * 8.97686767578125
Epoch 120, val loss: 1.0745298862457275
Epoch 130, training loss: 90.57901763916016 = 1.0713058710098267 + 10.0 * 8.95077133178711
Epoch 130, val loss: 1.0717895030975342
Epoch 140, training loss: 90.3404769897461 = 1.0686575174331665 + 10.0 * 8.9271821975708
Epoch 140, val loss: 1.0691962242126465
Epoch 150, training loss: 90.23624420166016 = 1.0661417245864868 + 10.0 * 8.917010307312012
Epoch 150, val loss: 1.066725492477417
Epoch 160, training loss: 90.02413177490234 = 1.0637062788009644 + 10.0 * 8.896042823791504
Epoch 160, val loss: 1.064332365989685
Epoch 170, training loss: 89.88027954101562 = 1.061448335647583 + 10.0 * 8.88188362121582
Epoch 170, val loss: 1.0621148347854614
Epoch 180, training loss: 89.72102355957031 = 1.0592551231384277 + 10.0 * 8.86617660522461
Epoch 180, val loss: 1.059975266456604
Epoch 190, training loss: 89.60037231445312 = 1.057173490524292 + 10.0 * 8.85431957244873
Epoch 190, val loss: 1.0579198598861694
Epoch 200, training loss: 89.56930541992188 = 1.0551884174346924 + 10.0 * 8.851411819458008
Epoch 200, val loss: 1.0559804439544678
Epoch 210, training loss: 89.43792724609375 = 1.053265929222107 + 10.0 * 8.838465690612793
Epoch 210, val loss: 1.0541009902954102
Epoch 220, training loss: 89.34928894042969 = 1.0514256954193115 + 10.0 * 8.82978630065918
Epoch 220, val loss: 1.0523103475570679
Epoch 230, training loss: 89.29302215576172 = 1.0496776103973389 + 10.0 * 8.824335098266602
Epoch 230, val loss: 1.0505964756011963
Epoch 240, training loss: 89.3930892944336 = 1.0479414463043213 + 10.0 * 8.834514617919922
Epoch 240, val loss: 1.0488911867141724
Epoch 250, training loss: 89.33123779296875 = 1.0462920665740967 + 10.0 * 8.82849407196045
Epoch 250, val loss: 1.0472877025604248
Epoch 260, training loss: 89.28205108642578 = 1.0445661544799805 + 10.0 * 8.823748588562012
Epoch 260, val loss: 1.0456215143203735
Epoch 270, training loss: 89.2677001953125 = 1.042860746383667 + 10.0 * 8.822484016418457
Epoch 270, val loss: 1.0439460277557373
Epoch 280, training loss: 89.24178314208984 = 1.041139006614685 + 10.0 * 8.820064544677734
Epoch 280, val loss: 1.0422589778900146
Epoch 290, training loss: 89.18059539794922 = 1.0393872261047363 + 10.0 * 8.814120292663574
Epoch 290, val loss: 1.0405919551849365
Epoch 300, training loss: 89.16075134277344 = 1.0375933647155762 + 10.0 * 8.812315940856934
Epoch 300, val loss: 1.0388514995574951
Epoch 310, training loss: 89.1574478149414 = 1.035685420036316 + 10.0 * 8.812176704406738
Epoch 310, val loss: 1.037002444267273
Epoch 320, training loss: 89.12765502929688 = 1.0337756872177124 + 10.0 * 8.809388160705566
Epoch 320, val loss: 1.0351449251174927
Epoch 330, training loss: 89.12767028808594 = 1.0317493677139282 + 10.0 * 8.809592247009277
Epoch 330, val loss: 1.0332220792770386
Epoch 340, training loss: 89.1130142211914 = 1.0296382904052734 + 10.0 * 8.808337211608887
Epoch 340, val loss: 1.0311698913574219
Epoch 350, training loss: 89.11529541015625 = 1.0274516344070435 + 10.0 * 8.808784484863281
Epoch 350, val loss: 1.0290675163269043
Epoch 360, training loss: 89.1448974609375 = 1.0251140594482422 + 10.0 * 8.811978340148926
Epoch 360, val loss: 1.0267999172210693
Epoch 370, training loss: 89.125244140625 = 1.0227068662643433 + 10.0 * 8.810254096984863
Epoch 370, val loss: 1.0244431495666504
Epoch 380, training loss: 89.159423828125 = 1.0201380252838135 + 10.0 * 8.813928604125977
Epoch 380, val loss: 1.0219627618789673
Epoch 390, training loss: 89.175537109375 = 1.0174357891082764 + 10.0 * 8.815810203552246
Epoch 390, val loss: 1.0193500518798828
Epoch 400, training loss: 89.18363189697266 = 1.0146516561508179 + 10.0 * 8.816898345947266
Epoch 400, val loss: 1.0166500806808472
Epoch 410, training loss: 89.17008209228516 = 1.0115466117858887 + 10.0 * 8.8158540725708
Epoch 410, val loss: 1.0137050151824951
Epoch 420, training loss: 89.17501831054688 = 1.0083945989608765 + 10.0 * 8.816662788391113
Epoch 420, val loss: 1.0106401443481445
Epoch 430, training loss: 89.16690063476562 = 1.0050621032714844 + 10.0 * 8.816184043884277
Epoch 430, val loss: 1.0074049234390259
Epoch 440, training loss: 89.20499420166016 = 1.00162672996521 + 10.0 * 8.820337295532227
Epoch 440, val loss: 1.0041173696517944
Epoch 450, training loss: 89.22218322753906 = 0.9979112148284912 + 10.0 * 8.822427749633789
Epoch 450, val loss: 1.0005674362182617
Epoch 460, training loss: 89.18373107910156 = 0.9938700199127197 + 10.0 * 8.818985939025879
Epoch 460, val loss: 0.9967063665390015
Epoch 470, training loss: 89.18611145019531 = 0.9896069765090942 + 10.0 * 8.819650650024414
Epoch 470, val loss: 0.9926994442939758
Epoch 480, training loss: 89.24090576171875 = 0.9852836728096008 + 10.0 * 8.825562477111816
Epoch 480, val loss: 0.9884466528892517
Epoch 490, training loss: 89.23648071289062 = 0.9805790781974792 + 10.0 * 8.825590133666992
Epoch 490, val loss: 0.983957827091217
Epoch 500, training loss: 89.27561950683594 = 0.9757696390151978 + 10.0 * 8.829984664916992
Epoch 500, val loss: 0.9793490171432495
Epoch 510, training loss: 89.26240539550781 = 0.9704467058181763 + 10.0 * 8.829195976257324
Epoch 510, val loss: 0.9743011593818665
Epoch 520, training loss: 89.24370574951172 = 0.9649561047554016 + 10.0 * 8.827875137329102
Epoch 520, val loss: 0.9690853357315063
Epoch 530, training loss: 89.25146484375 = 0.9591946601867676 + 10.0 * 8.82922649383545
Epoch 530, val loss: 0.9636337161064148
Epoch 540, training loss: 89.15423583984375 = 0.9529019594192505 + 10.0 * 8.820133209228516
Epoch 540, val loss: 0.9576504826545715
Epoch 550, training loss: 89.3045425415039 = 0.9468041062355042 + 10.0 * 8.835773468017578
Epoch 550, val loss: 0.9518699049949646
Epoch 560, training loss: 89.2426528930664 = 0.940130352973938 + 10.0 * 8.830251693725586
Epoch 560, val loss: 0.9455243349075317
Epoch 570, training loss: 89.3576889038086 = 0.9334216117858887 + 10.0 * 8.842427253723145
Epoch 570, val loss: 0.9393177032470703
Epoch 580, training loss: 89.2079849243164 = 0.92574542760849 + 10.0 * 8.828224182128906
Epoch 580, val loss: 0.9320012331008911
Epoch 590, training loss: 89.3182144165039 = 0.918241024017334 + 10.0 * 8.839997291564941
Epoch 590, val loss: 0.9250367879867554
Epoch 600, training loss: 89.26261901855469 = 0.9100735187530518 + 10.0 * 8.835254669189453
Epoch 600, val loss: 0.9174301028251648
Epoch 610, training loss: 89.3076171875 = 0.901826024055481 + 10.0 * 8.84057903289795
Epoch 610, val loss: 0.9096943140029907
Epoch 620, training loss: 89.3196029663086 = 0.8930233120918274 + 10.0 * 8.842658042907715
Epoch 620, val loss: 0.9015570282936096
Epoch 630, training loss: 89.36278533935547 = 0.8840941786766052 + 10.0 * 8.847868919372559
Epoch 630, val loss: 0.8932046890258789
Epoch 640, training loss: 89.1767578125 = 0.8741940259933472 + 10.0 * 8.830256462097168
Epoch 640, val loss: 0.8838916420936584
Epoch 650, training loss: 89.37654876708984 = 0.865225076675415 + 10.0 * 8.8511323928833
Epoch 650, val loss: 0.8755513429641724
Epoch 660, training loss: 89.37582397460938 = 0.8555259704589844 + 10.0 * 8.852029800415039
Epoch 660, val loss: 0.8667216300964355
Epoch 670, training loss: 89.3707504272461 = 0.8455752730369568 + 10.0 * 8.852518081665039
Epoch 670, val loss: 0.8574534058570862
Epoch 680, training loss: 89.45166778564453 = 0.8355185985565186 + 10.0 * 8.861615180969238
Epoch 680, val loss: 0.8481999039649963
Epoch 690, training loss: 89.48550415039062 = 0.8252941370010376 + 10.0 * 8.866021156311035
Epoch 690, val loss: 0.8387743234634399
Epoch 700, training loss: 89.5125732421875 = 0.8151117563247681 + 10.0 * 8.869746208190918
Epoch 700, val loss: 0.8295102715492249
Epoch 710, training loss: 89.51654815673828 = 0.8049510717391968 + 10.0 * 8.871159553527832
Epoch 710, val loss: 0.8202158808708191
Epoch 720, training loss: 89.53829193115234 = 0.7947421669960022 + 10.0 * 8.87435531616211
Epoch 720, val loss: 0.8109077215194702
Epoch 730, training loss: 89.5188217163086 = 0.7845699787139893 + 10.0 * 8.873425483703613
Epoch 730, val loss: 0.80149906873703
Epoch 740, training loss: 89.5531997680664 = 0.7745525240898132 + 10.0 * 8.877864837646484
Epoch 740, val loss: 0.7924862504005432
Epoch 750, training loss: 89.54264831542969 = 0.764722466468811 + 10.0 * 8.877792358398438
Epoch 750, val loss: 0.7834175825119019
Epoch 760, training loss: 89.58145141601562 = 0.7550771236419678 + 10.0 * 8.882637977600098
Epoch 760, val loss: 0.7746424078941345
Epoch 770, training loss: 89.59774780273438 = 0.7454502582550049 + 10.0 * 8.885229110717773
Epoch 770, val loss: 0.7658503651618958
Epoch 780, training loss: 89.62055969238281 = 0.7360436320304871 + 10.0 * 8.88845157623291
Epoch 780, val loss: 0.7573198676109314
Epoch 790, training loss: 89.66731262207031 = 0.7268941402435303 + 10.0 * 8.894042015075684
Epoch 790, val loss: 0.7489624619483948
Epoch 800, training loss: 89.61246490478516 = 0.7177693247795105 + 10.0 * 8.889470100402832
Epoch 800, val loss: 0.7407492995262146
Epoch 810, training loss: 89.60379791259766 = 0.708907961845398 + 10.0 * 8.88948917388916
Epoch 810, val loss: 0.732725203037262
Epoch 820, training loss: 89.63206481933594 = 0.700178861618042 + 10.0 * 8.8931884765625
Epoch 820, val loss: 0.724632740020752
Epoch 830, training loss: 89.68701934814453 = 0.6920031309127808 + 10.0 * 8.89950180053711
Epoch 830, val loss: 0.7172844409942627
Epoch 840, training loss: 89.6708984375 = 0.6841092109680176 + 10.0 * 8.89867877960205
Epoch 840, val loss: 0.7101230621337891
Epoch 850, training loss: 89.67781066894531 = 0.6764614582061768 + 10.0 * 8.900135040283203
Epoch 850, val loss: 0.7032912969589233
Epoch 860, training loss: 89.72166442871094 = 0.6692935228347778 + 10.0 * 8.905237197875977
Epoch 860, val loss: 0.6968660354614258
Epoch 870, training loss: 89.77627563476562 = 0.6623403429985046 + 10.0 * 8.911394119262695
Epoch 870, val loss: 0.6906107664108276
Epoch 880, training loss: 89.76172637939453 = 0.6556196212768555 + 10.0 * 8.910611152648926
Epoch 880, val loss: 0.6846460700035095
Epoch 890, training loss: 89.77566528320312 = 0.6492958068847656 + 10.0 * 8.912636756896973
Epoch 890, val loss: 0.6790427565574646
Epoch 900, training loss: 89.72232055664062 = 0.6432211995124817 + 10.0 * 8.907910346984863
Epoch 900, val loss: 0.6737279295921326
Epoch 910, training loss: 89.66584014892578 = 0.6375600099563599 + 10.0 * 8.902828216552734
Epoch 910, val loss: 0.6689467430114746
Epoch 920, training loss: 89.71540069580078 = 0.6320852637290955 + 10.0 * 8.908330917358398
Epoch 920, val loss: 0.6642370820045471
Epoch 930, training loss: 89.82019805908203 = 0.6267772912979126 + 10.0 * 8.919342041015625
Epoch 930, val loss: 0.6589950323104858
Epoch 940, training loss: 89.97552490234375 = 0.6221248507499695 + 10.0 * 8.93533992767334
Epoch 940, val loss: 0.6556985378265381
Epoch 950, training loss: 89.77310180664062 = 0.616973340511322 + 10.0 * 8.915613174438477
Epoch 950, val loss: 0.6510653495788574
Epoch 960, training loss: 89.79053497314453 = 0.6122653484344482 + 10.0 * 8.917826652526855
Epoch 960, val loss: 0.6472389698028564
Epoch 970, training loss: 89.87648010253906 = 0.6079200506210327 + 10.0 * 8.92685604095459
Epoch 970, val loss: 0.643767237663269
Epoch 980, training loss: 89.8978271484375 = 0.6035908460617065 + 10.0 * 8.929423332214355
Epoch 980, val loss: 0.6401014924049377
Epoch 990, training loss: 89.89715576171875 = 0.5995139479637146 + 10.0 * 8.929764747619629
Epoch 990, val loss: 0.6369274258613586
Epoch 1000, training loss: 89.95311737060547 = 0.595651388168335 + 10.0 * 8.935747146606445
Epoch 1000, val loss: 0.6339068412780762
Epoch 1010, training loss: 89.97823333740234 = 0.5919080972671509 + 10.0 * 8.938632011413574
Epoch 1010, val loss: 0.6309522986412048
Epoch 1020, training loss: 89.9919204711914 = 0.5882405042648315 + 10.0 * 8.940367698669434
Epoch 1020, val loss: 0.6281490921974182
Epoch 1030, training loss: 89.99227142333984 = 0.5847333073616028 + 10.0 * 8.940753936767578
Epoch 1030, val loss: 0.6254895329475403
Epoch 1040, training loss: 89.98880767822266 = 0.5813104510307312 + 10.0 * 8.940749168395996
Epoch 1040, val loss: 0.6229847073554993
Epoch 1050, training loss: 89.67768096923828 = 0.5778024792671204 + 10.0 * 8.909987449645996
Epoch 1050, val loss: 0.6201563477516174
Epoch 1060, training loss: 89.94293212890625 = 0.5750392079353333 + 10.0 * 8.936788558959961
Epoch 1060, val loss: 0.6182194948196411
Epoch 1070, training loss: 89.98792266845703 = 0.5720736384391785 + 10.0 * 8.941584587097168
Epoch 1070, val loss: 0.6165512800216675
Epoch 1080, training loss: 90.00891876220703 = 0.5691144466400146 + 10.0 * 8.94398021697998
Epoch 1080, val loss: 0.6141679883003235
Epoch 1090, training loss: 90.07145690917969 = 0.5661774277687073 + 10.0 * 8.950528144836426
Epoch 1090, val loss: 0.6122207045555115
Epoch 1100, training loss: 90.08039855957031 = 0.5633035898208618 + 10.0 * 8.951709747314453
Epoch 1100, val loss: 0.6103237867355347
Epoch 1110, training loss: 90.12007141113281 = 0.5605065822601318 + 10.0 * 8.95595645904541
Epoch 1110, val loss: 0.6083979606628418
Epoch 1120, training loss: 90.11553192138672 = 0.557770848274231 + 10.0 * 8.95577621459961
Epoch 1120, val loss: 0.6065630912780762
Epoch 1130, training loss: 90.14203643798828 = 0.5551506280899048 + 10.0 * 8.958688735961914
Epoch 1130, val loss: 0.6048746109008789
Epoch 1140, training loss: 90.19110870361328 = 0.5525771379470825 + 10.0 * 8.96385383605957
Epoch 1140, val loss: 0.6032621264457703
Epoch 1150, training loss: 90.18161010742188 = 0.5499975085258484 + 10.0 * 8.96316146850586
Epoch 1150, val loss: 0.601500928401947
Epoch 1160, training loss: 90.22992706298828 = 0.5475043058395386 + 10.0 * 8.968242645263672
Epoch 1160, val loss: 0.6000723242759705
Epoch 1170, training loss: 90.23892974853516 = 0.5449811220169067 + 10.0 * 8.96939468383789
Epoch 1170, val loss: 0.5985127091407776
Epoch 1180, training loss: 90.25698852539062 = 0.5425238013267517 + 10.0 * 8.97144603729248
Epoch 1180, val loss: 0.5969527363777161
Epoch 1190, training loss: 90.25933837890625 = 0.5401126742362976 + 10.0 * 8.971921920776367
Epoch 1190, val loss: 0.5955854058265686
Epoch 1200, training loss: 90.30657196044922 = 0.5377610325813293 + 10.0 * 8.97688102722168
Epoch 1200, val loss: 0.5941904783248901
Epoch 1210, training loss: 90.28028106689453 = 0.5353943705558777 + 10.0 * 8.974489212036133
Epoch 1210, val loss: 0.5926320552825928
Epoch 1220, training loss: 90.26705932617188 = 0.5330972671508789 + 10.0 * 8.973396301269531
Epoch 1220, val loss: 0.5915068984031677
Epoch 1230, training loss: 90.29681396484375 = 0.5308070182800293 + 10.0 * 8.976600646972656
Epoch 1230, val loss: 0.5900738835334778
Epoch 1240, training loss: 90.34962463378906 = 0.5285223722457886 + 10.0 * 8.982110977172852
Epoch 1240, val loss: 0.5887428522109985
Epoch 1250, training loss: 90.34759521484375 = 0.5262176990509033 + 10.0 * 8.982137680053711
Epoch 1250, val loss: 0.5874966382980347
Epoch 1260, training loss: 90.36900329589844 = 0.5239560604095459 + 10.0 * 8.984504699707031
Epoch 1260, val loss: 0.5862641334533691
Epoch 1270, training loss: 90.37937927246094 = 0.521729052066803 + 10.0 * 8.98576545715332
Epoch 1270, val loss: 0.5850613117218018
Epoch 1280, training loss: 90.4232177734375 = 0.5195326805114746 + 10.0 * 8.990368843078613
Epoch 1280, val loss: 0.5838631987571716
Epoch 1290, training loss: 90.46204376220703 = 0.5173244476318359 + 10.0 * 8.994471549987793
Epoch 1290, val loss: 0.5827428698539734
Epoch 1300, training loss: 90.42940521240234 = 0.5151203274726868 + 10.0 * 8.99142837524414
Epoch 1300, val loss: 0.5814332365989685
Epoch 1310, training loss: 90.46711730957031 = 0.5129612684249878 + 10.0 * 8.995415687561035
Epoch 1310, val loss: 0.5802211165428162
Epoch 1320, training loss: 90.49649810791016 = 0.5107674598693848 + 10.0 * 8.998573303222656
Epoch 1320, val loss: 0.5790510177612305
Epoch 1330, training loss: 90.49392700195312 = 0.5085709691047668 + 10.0 * 8.998536109924316
Epoch 1330, val loss: 0.5779013633728027
Epoch 1340, training loss: 90.54652404785156 = 0.5064101815223694 + 10.0 * 9.004011154174805
Epoch 1340, val loss: 0.5768146514892578
Epoch 1350, training loss: 90.56861114501953 = 0.5042301416397095 + 10.0 * 9.006438255310059
Epoch 1350, val loss: 0.5756462216377258
Epoch 1360, training loss: 90.55496978759766 = 0.502051055431366 + 10.0 * 9.005291938781738
Epoch 1360, val loss: 0.5745466351509094
Epoch 1370, training loss: 90.5939712524414 = 0.4998959004878998 + 10.0 * 9.009407997131348
Epoch 1370, val loss: 0.5734274983406067
Epoch 1380, training loss: 90.56257629394531 = 0.49775609374046326 + 10.0 * 9.006482124328613
Epoch 1380, val loss: 0.5724272131919861
Epoch 1390, training loss: 90.62162780761719 = 0.4956616461277008 + 10.0 * 9.01259708404541
Epoch 1390, val loss: 0.5713196396827698
Epoch 1400, training loss: 90.59563446044922 = 0.49348577857017517 + 10.0 * 9.010214805603027
Epoch 1400, val loss: 0.5703029036521912
Epoch 1410, training loss: 90.65062713623047 = 0.491330623626709 + 10.0 * 9.015929222106934
Epoch 1410, val loss: 0.5692798495292664
Epoch 1420, training loss: 90.66858673095703 = 0.48920533061027527 + 10.0 * 9.017938613891602
Epoch 1420, val loss: 0.5681154727935791
Epoch 1430, training loss: 90.67833709716797 = 0.4870617687702179 + 10.0 * 9.01912784576416
Epoch 1430, val loss: 0.5671689510345459
Epoch 1440, training loss: 90.6968994140625 = 0.484939843416214 + 10.0 * 9.021196365356445
Epoch 1440, val loss: 0.5661473274230957
Epoch 1450, training loss: 90.73808288574219 = 0.48279300332069397 + 10.0 * 9.025528907775879
Epoch 1450, val loss: 0.5649852156639099
Epoch 1460, training loss: 90.63043212890625 = 0.4806191623210907 + 10.0 * 9.014981269836426
Epoch 1460, val loss: 0.5633907318115234
Epoch 1470, training loss: 90.5503921508789 = 0.47851333022117615 + 10.0 * 9.007187843322754
Epoch 1470, val loss: 0.5629896521568298
Epoch 1480, training loss: 90.60398864746094 = 0.4765370786190033 + 10.0 * 9.012744903564453
Epoch 1480, val loss: 0.5619392395019531
Epoch 1490, training loss: 90.5804672241211 = 0.4744660556316376 + 10.0 * 9.010600090026855
Epoch 1490, val loss: 0.5609316229820251
Epoch 1500, training loss: 90.64940643310547 = 0.47243648767471313 + 10.0 * 9.01769733428955
Epoch 1500, val loss: 0.5601587295532227
Epoch 1510, training loss: 90.73454284667969 = 0.47024914622306824 + 10.0 * 9.026430130004883
Epoch 1510, val loss: 0.5591710209846497
Epoch 1520, training loss: 90.76637268066406 = 0.46804720163345337 + 10.0 * 9.02983283996582
Epoch 1520, val loss: 0.5580585598945618
Epoch 1530, training loss: 90.66512298583984 = 0.46582427620887756 + 10.0 * 9.019929885864258
Epoch 1530, val loss: 0.5569545030593872
Epoch 1540, training loss: 90.71634674072266 = 0.46373796463012695 + 10.0 * 9.025260925292969
Epoch 1540, val loss: 0.5561128258705139
Epoch 1550, training loss: 90.7815933227539 = 0.4616245627403259 + 10.0 * 9.031996726989746
Epoch 1550, val loss: 0.5551650524139404
Epoch 1560, training loss: 90.82877349853516 = 0.45943641662597656 + 10.0 * 9.036933898925781
Epoch 1560, val loss: 0.5543254017829895
Epoch 1570, training loss: 90.79642486572266 = 0.45721858739852905 + 10.0 * 9.033920288085938
Epoch 1570, val loss: 0.5533655881881714
Epoch 1580, training loss: 90.81922912597656 = 0.45504051446914673 + 10.0 * 9.036418914794922
Epoch 1580, val loss: 0.5523037910461426
Epoch 1590, training loss: 90.85955047607422 = 0.45291784405708313 + 10.0 * 9.04066276550293
Epoch 1590, val loss: 0.5514914989471436
Epoch 1600, training loss: 90.85040283203125 = 0.4507320821285248 + 10.0 * 9.03996753692627
Epoch 1600, val loss: 0.5505251884460449
Epoch 1610, training loss: 90.86371612548828 = 0.44860711693763733 + 10.0 * 9.041510581970215
Epoch 1610, val loss: 0.549679160118103
Epoch 1620, training loss: 90.94165802001953 = 0.44646137952804565 + 10.0 * 9.049519538879395
Epoch 1620, val loss: 0.548761248588562
Epoch 1630, training loss: 90.93890380859375 = 0.4442913830280304 + 10.0 * 9.049461364746094
Epoch 1630, val loss: 0.5476850271224976
Epoch 1640, training loss: 90.95060729980469 = 0.4421425759792328 + 10.0 * 9.050846099853516
Epoch 1640, val loss: 0.5469481348991394
Epoch 1650, training loss: 90.94575500488281 = 0.44001927971839905 + 10.0 * 9.050573348999023
Epoch 1650, val loss: 0.5458013415336609
Epoch 1660, training loss: 90.96465301513672 = 0.43787240982055664 + 10.0 * 9.052678108215332
Epoch 1660, val loss: 0.544942319393158
Epoch 1670, training loss: 90.99237060546875 = 0.4357214868068695 + 10.0 * 9.055665016174316
Epoch 1670, val loss: 0.5441534519195557
Epoch 1680, training loss: 91.03248596191406 = 0.4335740804672241 + 10.0 * 9.059891700744629
Epoch 1680, val loss: 0.5432221293449402
Epoch 1690, training loss: 90.97026824951172 = 0.4314125180244446 + 10.0 * 9.053885459899902
Epoch 1690, val loss: 0.5423643589019775
Epoch 1700, training loss: 90.93172454833984 = 0.4293704628944397 + 10.0 * 9.050235748291016
Epoch 1700, val loss: 0.5416810512542725
Epoch 1710, training loss: 90.89257049560547 = 0.4273317754268646 + 10.0 * 9.046524047851562
Epoch 1710, val loss: 0.541232705116272
Epoch 1720, training loss: 90.94026947021484 = 0.4253216087818146 + 10.0 * 9.051494598388672
Epoch 1720, val loss: 0.5399977564811707
Epoch 1730, training loss: 91.01133728027344 = 0.42318594455718994 + 10.0 * 9.058815002441406
Epoch 1730, val loss: 0.5395511984825134
Epoch 1740, training loss: 91.05667877197266 = 0.42100539803504944 + 10.0 * 9.063567161560059
Epoch 1740, val loss: 0.5380935072898865
Epoch 1750, training loss: 91.0394515991211 = 0.41882744431495667 + 10.0 * 9.06206226348877
Epoch 1750, val loss: 0.5375696420669556
Epoch 1760, training loss: 91.04116821289062 = 0.4167131185531616 + 10.0 * 9.062445640563965
Epoch 1760, val loss: 0.5366019010543823
Epoch 1770, training loss: 91.10238647460938 = 0.4145779311656952 + 10.0 * 9.068780899047852
Epoch 1770, val loss: 0.5358956456184387
Epoch 1780, training loss: 91.03201293945312 = 0.41242021322250366 + 10.0 * 9.061959266662598
Epoch 1780, val loss: 0.5351458191871643
Epoch 1790, training loss: 91.03683471679688 = 0.4103529751300812 + 10.0 * 9.062647819519043
Epoch 1790, val loss: 0.5341212749481201
Epoch 1800, training loss: 91.10093688964844 = 0.40827009081840515 + 10.0 * 9.069266319274902
Epoch 1800, val loss: 0.5334779620170593
Epoch 1810, training loss: 91.16433715820312 = 0.4061664640903473 + 10.0 * 9.075817108154297
Epoch 1810, val loss: 0.5327296853065491
Epoch 1820, training loss: 91.15953063964844 = 0.4040261209011078 + 10.0 * 9.075550079345703
Epoch 1820, val loss: 0.5321112871170044
Epoch 1830, training loss: 91.12229919433594 = 0.4019175171852112 + 10.0 * 9.072038650512695
Epoch 1830, val loss: 0.531233012676239
Epoch 1840, training loss: 91.1579360961914 = 0.39980635046958923 + 10.0 * 9.075813293457031
Epoch 1840, val loss: 0.5306470990180969
Epoch 1850, training loss: 91.2194595336914 = 0.39769360423088074 + 10.0 * 9.08217716217041
Epoch 1850, val loss: 0.5299727916717529
Epoch 1860, training loss: 91.19033813476562 = 0.39553648233413696 + 10.0 * 9.079480171203613
Epoch 1860, val loss: 0.5293872952461243
Epoch 1870, training loss: 91.17879486083984 = 0.39341965317726135 + 10.0 * 9.078537940979004
Epoch 1870, val loss: 0.5284993052482605
Epoch 1880, training loss: 91.24665832519531 = 0.39132142066955566 + 10.0 * 9.08553409576416
Epoch 1880, val loss: 0.527913510799408
Epoch 1890, training loss: 91.2904052734375 = 0.38918590545654297 + 10.0 * 9.09012222290039
Epoch 1890, val loss: 0.5271952748298645
Epoch 1900, training loss: 91.25799560546875 = 0.38708361983299255 + 10.0 * 9.087091445922852
Epoch 1900, val loss: 0.526530921459198
Epoch 1910, training loss: 91.25042724609375 = 0.3849904537200928 + 10.0 * 9.086544036865234
Epoch 1910, val loss: 0.5260341763496399
Epoch 1920, training loss: 91.30415344238281 = 0.3829067647457123 + 10.0 * 9.092124938964844
Epoch 1920, val loss: 0.5254828333854675
Epoch 1930, training loss: 91.28421783447266 = 0.3807762563228607 + 10.0 * 9.090344429016113
Epoch 1930, val loss: 0.5248456001281738
Epoch 1940, training loss: 91.26390075683594 = 0.3786526024341583 + 10.0 * 9.08852481842041
Epoch 1940, val loss: 0.5241588354110718
Epoch 1950, training loss: 91.3088607788086 = 0.37658318877220154 + 10.0 * 9.09322738647461
Epoch 1950, val loss: 0.5238136053085327
Epoch 1960, training loss: 91.3558349609375 = 0.37447479367256165 + 10.0 * 9.098135948181152
Epoch 1960, val loss: 0.5232689380645752
Epoch 1970, training loss: 91.16213989257812 = 0.37243279814720154 + 10.0 * 9.078969955444336
Epoch 1970, val loss: 0.5234363675117493
Epoch 1980, training loss: 91.11481475830078 = 0.37050098180770874 + 10.0 * 9.074431419372559
Epoch 1980, val loss: 0.5217791795730591
Epoch 1990, training loss: 91.3628158569336 = 0.3688412010669708 + 10.0 * 9.099397659301758
Epoch 1990, val loss: 0.5216853618621826
Epoch 2000, training loss: 91.26468658447266 = 0.36709681153297424 + 10.0 * 9.08975887298584
Epoch 2000, val loss: 0.523330569267273
Epoch 2010, training loss: 91.19275665283203 = 0.36482951045036316 + 10.0 * 9.082792282104492
Epoch 2010, val loss: 0.5217389464378357
Epoch 2020, training loss: 91.22074127197266 = 0.36281153559684753 + 10.0 * 9.085793495178223
Epoch 2020, val loss: 0.5208051204681396
Epoch 2030, training loss: 91.29841613769531 = 0.36077600717544556 + 10.0 * 9.09376335144043
Epoch 2030, val loss: 0.5201269388198853
Epoch 2040, training loss: 91.39590454101562 = 0.3586874008178711 + 10.0 * 9.103721618652344
Epoch 2040, val loss: 0.5200032591819763
Epoch 2050, training loss: 91.45156860351562 = 0.3565565049648285 + 10.0 * 9.109500885009766
Epoch 2050, val loss: 0.5195454359054565
Epoch 2060, training loss: 91.47884368896484 = 0.35444098711013794 + 10.0 * 9.11244010925293
Epoch 2060, val loss: 0.5193598866462708
Epoch 2070, training loss: 91.49150085449219 = 0.35234132409095764 + 10.0 * 9.113916397094727
Epoch 2070, val loss: 0.5188642144203186
Epoch 2080, training loss: 91.48426055908203 = 0.3502460718154907 + 10.0 * 9.113401412963867
Epoch 2080, val loss: 0.5187720656394958
Epoch 2090, training loss: 91.52346801757812 = 0.3481542766094208 + 10.0 * 9.117531776428223
Epoch 2090, val loss: 0.5184557437896729
Epoch 2100, training loss: 91.546630859375 = 0.3460599482059479 + 10.0 * 9.120057106018066
Epoch 2100, val loss: 0.5181419253349304
Epoch 2110, training loss: 91.53374481201172 = 0.34398218989372253 + 10.0 * 9.118976593017578
Epoch 2110, val loss: 0.5178165435791016
Epoch 2120, training loss: 91.57882690429688 = 0.34190237522125244 + 10.0 * 9.123692512512207
Epoch 2120, val loss: 0.5174371600151062
Epoch 2130, training loss: 91.58560180664062 = 0.3398067355155945 + 10.0 * 9.124579429626465
Epoch 2130, val loss: 0.5172411203384399
Epoch 2140, training loss: 91.55233001708984 = 0.33772560954093933 + 10.0 * 9.121460914611816
Epoch 2140, val loss: 0.5169386863708496
Epoch 2150, training loss: 91.6004409790039 = 0.3356323540210724 + 10.0 * 9.126481056213379
Epoch 2150, val loss: 0.5167880058288574
Epoch 2160, training loss: 91.65443420410156 = 0.33351612091064453 + 10.0 * 9.132091522216797
Epoch 2160, val loss: 0.5165320038795471
Epoch 2170, training loss: 91.62397003173828 = 0.3314127027988434 + 10.0 * 9.129255294799805
Epoch 2170, val loss: 0.5164985656738281
Epoch 2180, training loss: 91.6424789428711 = 0.3293394446372986 + 10.0 * 9.131314277648926
Epoch 2180, val loss: 0.5164544582366943
Epoch 2190, training loss: 91.64563751220703 = 0.3272346258163452 + 10.0 * 9.131840705871582
Epoch 2190, val loss: 0.5163878798484802
Epoch 2200, training loss: 91.67216491699219 = 0.32519495487213135 + 10.0 * 9.134696960449219
Epoch 2200, val loss: 0.5160425305366516
Epoch 2210, training loss: 91.72003173828125 = 0.32311853766441345 + 10.0 * 9.139691352844238
Epoch 2210, val loss: 0.5159449577331543
Epoch 2220, training loss: 91.71804809570312 = 0.32103127241134644 + 10.0 * 9.139701843261719
Epoch 2220, val loss: 0.5160161256790161
Epoch 2230, training loss: 91.71173858642578 = 0.31895703077316284 + 10.0 * 9.139278411865234
Epoch 2230, val loss: 0.5160475969314575
Epoch 2240, training loss: 91.72584533691406 = 0.3168957829475403 + 10.0 * 9.140894889831543
Epoch 2240, val loss: 0.5157336592674255
Epoch 2250, training loss: 91.71917724609375 = 0.3148208558559418 + 10.0 * 9.140436172485352
Epoch 2250, val loss: 0.5158414840698242
Epoch 2260, training loss: 91.74907684326172 = 0.31273648142814636 + 10.0 * 9.143633842468262
Epoch 2260, val loss: 0.5157487392425537
Epoch 2270, training loss: 91.7486801147461 = 0.31066185235977173 + 10.0 * 9.14380168914795
Epoch 2270, val loss: 0.5160686373710632
Epoch 2280, training loss: 91.63716125488281 = 0.3088024854660034 + 10.0 * 9.13283634185791
Epoch 2280, val loss: 0.516606867313385
Epoch 2290, training loss: 91.59317779541016 = 0.30681994557380676 + 10.0 * 9.12863540649414
Epoch 2290, val loss: 0.5163723230361938
Epoch 2300, training loss: 91.62205505371094 = 0.30484652519226074 + 10.0 * 9.131720542907715
Epoch 2300, val loss: 0.5164777636528015
Epoch 2310, training loss: 91.73193359375 = 0.30285266041755676 + 10.0 * 9.142908096313477
Epoch 2310, val loss: 0.5168406367301941
Epoch 2320, training loss: 91.78353118896484 = 0.3007490336894989 + 10.0 * 9.14827823638916
Epoch 2320, val loss: 0.5167005658149719
Epoch 2330, training loss: 91.76107025146484 = 0.2986632287502289 + 10.0 * 9.146241188049316
Epoch 2330, val loss: 0.516868531703949
Epoch 2340, training loss: 91.79296875 = 0.29660049080848694 + 10.0 * 9.149637222290039
Epoch 2340, val loss: 0.5172365307807922
Epoch 2350, training loss: 91.80924987792969 = 0.2945270240306854 + 10.0 * 9.151472091674805
Epoch 2350, val loss: 0.5174369812011719
Epoch 2360, training loss: 91.79978942871094 = 0.29248106479644775 + 10.0 * 9.150731086730957
Epoch 2360, val loss: 0.5176842212677002
Epoch 2370, training loss: 91.81238555908203 = 0.29046130180358887 + 10.0 * 9.152193069458008
Epoch 2370, val loss: 0.5181614756584167
Epoch 2380, training loss: 91.84674835205078 = 0.28841641545295715 + 10.0 * 9.15583324432373
Epoch 2380, val loss: 0.5184829235076904
Epoch 2390, training loss: 91.85359954833984 = 0.28636255860328674 + 10.0 * 9.156723976135254
Epoch 2390, val loss: 0.5190950036048889
Epoch 2400, training loss: 91.83043670654297 = 0.2843515872955322 + 10.0 * 9.154607772827148
Epoch 2400, val loss: 0.5193250775337219
Epoch 2410, training loss: 91.82791900634766 = 0.28232282400131226 + 10.0 * 9.154559135437012
Epoch 2410, val loss: 0.5198485255241394
Epoch 2420, training loss: 91.86680603027344 = 0.2802850604057312 + 10.0 * 9.158651351928711
Epoch 2420, val loss: 0.5205820202827454
Epoch 2430, training loss: 91.87866973876953 = 0.27826985716819763 + 10.0 * 9.160039901733398
Epoch 2430, val loss: 0.5209808945655823
Epoch 2440, training loss: 91.6915283203125 = 0.2762807011604309 + 10.0 * 9.141524314880371
Epoch 2440, val loss: 0.5219190120697021
Epoch 2450, training loss: 91.61849212646484 = 0.274493545293808 + 10.0 * 9.134400367736816
Epoch 2450, val loss: 0.5238518714904785
Epoch 2460, training loss: 91.64730834960938 = 0.2725564241409302 + 10.0 * 9.13747501373291
Epoch 2460, val loss: 0.5226820111274719
Epoch 2470, training loss: 91.52901458740234 = 0.27072837948799133 + 10.0 * 9.125828742980957
Epoch 2470, val loss: 0.5226274132728577
Epoch 2480, training loss: 91.56424713134766 = 0.26877275109291077 + 10.0 * 9.129547119140625
Epoch 2480, val loss: 0.5237695574760437
Epoch 2490, training loss: 91.59197235107422 = 0.2669461965560913 + 10.0 * 9.132502555847168
Epoch 2490, val loss: 0.5253390073776245
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7968115942028985
0.8125769760197059
The final CL Acc:0.80145, 0.00373, The final GNN Acc:0.81195, 0.00089
raw graph: torch.Size([2, 88648])
add edge: torch.Size([2, 110682])
remove edge: torch.Size([2, 66564])
updated graph: torch.Size([2, 88598])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 103.11150360107422 = 1.0955076217651367 + 10.0 * 10.201600074768066
Epoch 0, val loss: 1.0957272052764893
Epoch 10, training loss: 98.57678985595703 = 1.0916571617126465 + 10.0 * 9.748513221740723
Epoch 10, val loss: 1.0919487476348877
Epoch 20, training loss: 96.44947052001953 = 1.0880999565124512 + 10.0 * 9.536137580871582
Epoch 20, val loss: 1.088427186012268
Epoch 30, training loss: 94.89512634277344 = 1.0847617387771606 + 10.0 * 9.381036758422852
Epoch 30, val loss: 1.08512544631958
Epoch 40, training loss: 93.71529388427734 = 1.0816131830215454 + 10.0 * 9.263368606567383
Epoch 40, val loss: 1.0820047855377197
Epoch 50, training loss: 92.79912567138672 = 1.0786561965942383 + 10.0 * 9.172046661376953
Epoch 50, val loss: 1.079074501991272
Epoch 60, training loss: 92.05220031738281 = 1.075860857963562 + 10.0 * 9.097634315490723
Epoch 60, val loss: 1.0763050317764282
Epoch 70, training loss: 91.43190002441406 = 1.0732442140579224 + 10.0 * 9.035865783691406
Epoch 70, val loss: 1.0737097263336182
Epoch 80, training loss: 90.91191864013672 = 1.0707955360412598 + 10.0 * 8.984112739562988
Epoch 80, val loss: 1.0712822675704956
Epoch 90, training loss: 90.46985626220703 = 1.068471908569336 + 10.0 * 8.94013786315918
Epoch 90, val loss: 1.0689815282821655
Epoch 100, training loss: 90.08535766601562 = 1.066300392150879 + 10.0 * 8.90190601348877
Epoch 100, val loss: 1.0668293237686157
Epoch 110, training loss: 89.76172637939453 = 1.064245343208313 + 10.0 * 8.86974811553955
Epoch 110, val loss: 1.0647950172424316
Epoch 120, training loss: 89.47607421875 = 1.0623003244400024 + 10.0 * 8.841377258300781
Epoch 120, val loss: 1.0628665685653687
Epoch 130, training loss: 89.27432250976562 = 1.0604416131973267 + 10.0 * 8.821388244628906
Epoch 130, val loss: 1.0610300302505493
Epoch 140, training loss: 89.0458755493164 = 1.058696985244751 + 10.0 * 8.798717498779297
Epoch 140, val loss: 1.0592962503433228
Epoch 150, training loss: 88.8636245727539 = 1.0569982528686523 + 10.0 * 8.780662536621094
Epoch 150, val loss: 1.057610034942627
Epoch 160, training loss: 88.68558502197266 = 1.055320143699646 + 10.0 * 8.763026237487793
Epoch 160, val loss: 1.0559378862380981
Epoch 170, training loss: 88.55402374267578 = 1.053716778755188 + 10.0 * 8.750030517578125
Epoch 170, val loss: 1.0543453693389893
Epoch 180, training loss: 88.41439056396484 = 1.0520893335342407 + 10.0 * 8.736230850219727
Epoch 180, val loss: 1.0527141094207764
Epoch 190, training loss: 88.31394958496094 = 1.0504722595214844 + 10.0 * 8.726346969604492
Epoch 190, val loss: 1.0511056184768677
Epoch 200, training loss: 88.24237060546875 = 1.0488355159759521 + 10.0 * 8.719353675842285
Epoch 200, val loss: 1.049445390701294
Epoch 210, training loss: 88.18403625488281 = 1.0471949577331543 + 10.0 * 8.71368408203125
Epoch 210, val loss: 1.0478136539459229
Epoch 220, training loss: 88.09991455078125 = 1.045430302619934 + 10.0 * 8.705448150634766
Epoch 220, val loss: 1.0460559129714966
Epoch 230, training loss: 88.0160140991211 = 1.0435599088668823 + 10.0 * 8.697245597839355
Epoch 230, val loss: 1.0441672801971436
Epoch 240, training loss: 87.96138000488281 = 1.041749119758606 + 10.0 * 8.691963195800781
Epoch 240, val loss: 1.042338490486145
Epoch 250, training loss: 87.93549346923828 = 1.0397869348526 + 10.0 * 8.689570426940918
Epoch 250, val loss: 1.040397047996521
Epoch 260, training loss: 87.88356018066406 = 1.0375967025756836 + 10.0 * 8.684596061706543
Epoch 260, val loss: 1.0381911993026733
Epoch 270, training loss: 87.8332290649414 = 1.035333275794983 + 10.0 * 8.679789543151855
Epoch 270, val loss: 1.035910964012146
Epoch 280, training loss: 87.83856201171875 = 1.0329755544662476 + 10.0 * 8.680559158325195
Epoch 280, val loss: 1.0335428714752197
Epoch 290, training loss: 87.80370330810547 = 1.0303311347961426 + 10.0 * 8.677336692810059
Epoch 290, val loss: 1.030920147895813
Epoch 300, training loss: 87.71820831298828 = 1.027421236038208 + 10.0 * 8.669078826904297
Epoch 300, val loss: 1.0280327796936035
Epoch 310, training loss: 87.71219635009766 = 1.0245850086212158 + 10.0 * 8.668761253356934
Epoch 310, val loss: 1.0251222848892212
Epoch 320, training loss: 87.6775894165039 = 1.0212514400482178 + 10.0 * 8.665634155273438
Epoch 320, val loss: 1.0217740535736084
Epoch 330, training loss: 87.74102783203125 = 1.0179922580718994 + 10.0 * 8.672304153442383
Epoch 330, val loss: 1.0184998512268066
Epoch 340, training loss: 87.764892578125 = 1.0143102407455444 + 10.0 * 8.675058364868164
Epoch 340, val loss: 1.0147978067398071
Epoch 350, training loss: 87.74823760986328 = 1.0103752613067627 + 10.0 * 8.673786163330078
Epoch 350, val loss: 1.0108882188796997
Epoch 360, training loss: 87.75298309326172 = 1.0062363147735596 + 10.0 * 8.674674987792969
Epoch 360, val loss: 1.0067322254180908
Epoch 370, training loss: 87.73055267333984 = 1.0017274618148804 + 10.0 * 8.672883033752441
Epoch 370, val loss: 1.002210021018982
Epoch 380, training loss: 87.72128295898438 = 0.9969470500946045 + 10.0 * 8.672433853149414
Epoch 380, val loss: 0.9974321126937866
Epoch 390, training loss: 87.69580841064453 = 0.9917902946472168 + 10.0 * 8.670401573181152
Epoch 390, val loss: 0.9922512769699097
Epoch 400, training loss: 87.72261810302734 = 0.9865344762802124 + 10.0 * 8.673608779907227
Epoch 400, val loss: 0.9870589375495911
Epoch 410, training loss: 87.73885345458984 = 0.9808268547058105 + 10.0 * 8.675802230834961
Epoch 410, val loss: 0.9813780188560486
Epoch 420, training loss: 87.7003402709961 = 0.9746825098991394 + 10.0 * 8.672565460205078
Epoch 420, val loss: 0.9751517176628113
Epoch 430, training loss: 87.67083740234375 = 0.9684910178184509 + 10.0 * 8.670234680175781
Epoch 430, val loss: 0.9688420295715332
Epoch 440, training loss: 87.7542724609375 = 0.9618412852287292 + 10.0 * 8.679243087768555
Epoch 440, val loss: 0.9622264504432678
Epoch 450, training loss: 87.80046081542969 = 0.955066442489624 + 10.0 * 8.684539794921875
Epoch 450, val loss: 0.955915093421936
Epoch 460, training loss: 87.66495513916016 = 0.9477338790893555 + 10.0 * 8.671722412109375
Epoch 460, val loss: 0.9482222199440002
Epoch 470, training loss: 87.69294738769531 = 0.940187394618988 + 10.0 * 8.675275802612305
Epoch 470, val loss: 0.940715491771698
Epoch 480, training loss: 87.70021057128906 = 0.9321271777153015 + 10.0 * 8.67680835723877
Epoch 480, val loss: 0.9327629804611206
Epoch 490, training loss: 87.76105499267578 = 0.9238329529762268 + 10.0 * 8.683721542358398
Epoch 490, val loss: 0.9244486093521118
Epoch 500, training loss: 87.76714324951172 = 0.9151957631111145 + 10.0 * 8.68519401550293
Epoch 500, val loss: 0.9158321619033813
Epoch 510, training loss: 87.8021011352539 = 0.9063016176223755 + 10.0 * 8.689579963684082
Epoch 510, val loss: 0.9070123434066772
Epoch 520, training loss: 87.82756805419922 = 0.897261381149292 + 10.0 * 8.69303035736084
Epoch 520, val loss: 0.8980362415313721
Epoch 530, training loss: 87.82064056396484 = 0.8878925442695618 + 10.0 * 8.69327449798584
Epoch 530, val loss: 0.8887065649032593
Epoch 540, training loss: 87.84768676757812 = 0.8785704970359802 + 10.0 * 8.696911811828613
Epoch 540, val loss: 0.8795023560523987
Epoch 550, training loss: 87.8436279296875 = 0.8689748644828796 + 10.0 * 8.697465896606445
Epoch 550, val loss: 0.8701533675193787
Epoch 560, training loss: 87.83992767333984 = 0.8592116832733154 + 10.0 * 8.698071479797363
Epoch 560, val loss: 0.8605223298072815
Epoch 570, training loss: 87.86051177978516 = 0.8493227958679199 + 10.0 * 8.701119422912598
Epoch 570, val loss: 0.8508336544036865
Epoch 580, training loss: 87.8758316040039 = 0.8396304845809937 + 10.0 * 8.703619956970215
Epoch 580, val loss: 0.8413503170013428
Epoch 590, training loss: 87.91748809814453 = 0.8297699093818665 + 10.0 * 8.708771705627441
Epoch 590, val loss: 0.8316588401794434
Epoch 600, training loss: 87.9256591796875 = 0.8198385834693909 + 10.0 * 8.71058177947998
Epoch 600, val loss: 0.8219811916351318
Epoch 610, training loss: 87.93293762207031 = 0.8098764419555664 + 10.0 * 8.712306022644043
Epoch 610, val loss: 0.8124309182167053
Epoch 620, training loss: 87.93708038330078 = 0.7999927401542664 + 10.0 * 8.713708877563477
Epoch 620, val loss: 0.8028750419616699
Epoch 630, training loss: 87.89962768554688 = 0.7899198532104492 + 10.0 * 8.710970878601074
Epoch 630, val loss: 0.793195366859436
Epoch 640, training loss: 87.89965057373047 = 0.7800246477127075 + 10.0 * 8.711962699890137
Epoch 640, val loss: 0.7837144732475281
Epoch 650, training loss: 87.91719055175781 = 0.7703090310096741 + 10.0 * 8.714688301086426
Epoch 650, val loss: 0.7744790315628052
Epoch 660, training loss: 87.91142272949219 = 0.7605123519897461 + 10.0 * 8.71509075164795
Epoch 660, val loss: 0.7652341723442078
Epoch 670, training loss: 87.9289779663086 = 0.7507374882698059 + 10.0 * 8.71782398223877
Epoch 670, val loss: 0.7561789155006409
Epoch 680, training loss: 87.97343444824219 = 0.7410547733306885 + 10.0 * 8.723237991333008
Epoch 680, val loss: 0.7470828890800476
Epoch 690, training loss: 88.01054382324219 = 0.7314960956573486 + 10.0 * 8.727904319763184
Epoch 690, val loss: 0.7381477355957031
Epoch 700, training loss: 87.98907470703125 = 0.7219291925430298 + 10.0 * 8.726714134216309
Epoch 700, val loss: 0.7293251156806946
Epoch 710, training loss: 87.99480438232422 = 0.7124588489532471 + 10.0 * 8.728235244750977
Epoch 710, val loss: 0.7206286191940308
Epoch 720, training loss: 88.05680847167969 = 0.7030946016311646 + 10.0 * 8.735371589660645
Epoch 720, val loss: 0.7120487689971924
Epoch 730, training loss: 88.01477813720703 = 0.693747341632843 + 10.0 * 8.73210334777832
Epoch 730, val loss: 0.7035885453224182
Epoch 740, training loss: 88.02751159667969 = 0.6847137212753296 + 10.0 * 8.73427963256836
Epoch 740, val loss: 0.6953988075256348
Epoch 750, training loss: 88.07982635498047 = 0.6756411790847778 + 10.0 * 8.740418434143066
Epoch 750, val loss: 0.687158465385437
Epoch 760, training loss: 88.07427978515625 = 0.6666141748428345 + 10.0 * 8.740766525268555
Epoch 760, val loss: 0.6790595054626465
Epoch 770, training loss: 88.1112060546875 = 0.6578325033187866 + 10.0 * 8.74533748626709
Epoch 770, val loss: 0.6711325645446777
Epoch 780, training loss: 88.10424041748047 = 0.6491385102272034 + 10.0 * 8.74551010131836
Epoch 780, val loss: 0.6634871959686279
Epoch 790, training loss: 88.0165786743164 = 0.6406785845756531 + 10.0 * 8.737589836120605
Epoch 790, val loss: 0.6559732556343079
Epoch 800, training loss: 88.05023193359375 = 0.6323296427726746 + 10.0 * 8.741789817810059
Epoch 800, val loss: 0.6486279964447021
Epoch 810, training loss: 88.11143493652344 = 0.6242992281913757 + 10.0 * 8.748713493347168
Epoch 810, val loss: 0.641459584236145
Epoch 820, training loss: 88.11815643310547 = 0.6163403987884521 + 10.0 * 8.750181198120117
Epoch 820, val loss: 0.6345059275627136
Epoch 830, training loss: 88.16095733642578 = 0.6084774732589722 + 10.0 * 8.755248069763184
Epoch 830, val loss: 0.6277284622192383
Epoch 840, training loss: 88.12022399902344 = 0.6006097197532654 + 10.0 * 8.751961708068848
Epoch 840, val loss: 0.6209539175033569
Epoch 850, training loss: 88.1415023803711 = 0.5932261943817139 + 10.0 * 8.754827499389648
Epoch 850, val loss: 0.6145575046539307
Epoch 860, training loss: 88.18714904785156 = 0.585951030254364 + 10.0 * 8.760119438171387
Epoch 860, val loss: 0.6083218455314636
Epoch 870, training loss: 88.21501159667969 = 0.5787550210952759 + 10.0 * 8.763625144958496
Epoch 870, val loss: 0.6021538376808167
Epoch 880, training loss: 88.26367950439453 = 0.571837842464447 + 10.0 * 8.769184112548828
Epoch 880, val loss: 0.5962068438529968
Epoch 890, training loss: 88.22686004638672 = 0.5649573802947998 + 10.0 * 8.766190528869629
Epoch 890, val loss: 0.5904037952423096
Epoch 900, training loss: 88.23413848876953 = 0.5583550930023193 + 10.0 * 8.767578125
Epoch 900, val loss: 0.5849319696426392
Epoch 910, training loss: 88.30259704589844 = 0.5521826148033142 + 10.0 * 8.775041580200195
Epoch 910, val loss: 0.5796273350715637
Epoch 920, training loss: 88.30020904541016 = 0.5460096001625061 + 10.0 * 8.775419235229492
Epoch 920, val loss: 0.5743919014930725
Epoch 930, training loss: 88.32157135009766 = 0.5401067733764648 + 10.0 * 8.778146743774414
Epoch 930, val loss: 0.569552481174469
Epoch 940, training loss: 88.21173095703125 = 0.5340606570243835 + 10.0 * 8.767766952514648
Epoch 940, val loss: 0.5645624399185181
Epoch 950, training loss: 88.29716491699219 = 0.5286407470703125 + 10.0 * 8.77685260772705
Epoch 950, val loss: 0.5599951148033142
Epoch 960, training loss: 88.3328857421875 = 0.5233553647994995 + 10.0 * 8.780953407287598
Epoch 960, val loss: 0.5556329488754272
Epoch 970, training loss: 88.39229583740234 = 0.518120288848877 + 10.0 * 8.7874174118042
Epoch 970, val loss: 0.551316499710083
Epoch 980, training loss: 88.40714263916016 = 0.5129364132881165 + 10.0 * 8.789420127868652
Epoch 980, val loss: 0.5471897721290588
Epoch 990, training loss: 88.35731506347656 = 0.5079239010810852 + 10.0 * 8.78493881225586
Epoch 990, val loss: 0.5431708097457886
Epoch 1000, training loss: 88.37993621826172 = 0.5029566884040833 + 10.0 * 8.787697792053223
Epoch 1000, val loss: 0.5392615795135498
Epoch 1010, training loss: 88.44093322753906 = 0.49824634194374084 + 10.0 * 8.794268608093262
Epoch 1010, val loss: 0.5355210304260254
Epoch 1020, training loss: 88.47798156738281 = 0.49361950159072876 + 10.0 * 8.798436164855957
Epoch 1020, val loss: 0.5318566560745239
Epoch 1030, training loss: 88.42310333251953 = 0.48906242847442627 + 10.0 * 8.793404579162598
Epoch 1030, val loss: 0.5284119844436646
Epoch 1040, training loss: 88.4755859375 = 0.4847448766231537 + 10.0 * 8.799084663391113
Epoch 1040, val loss: 0.5250112414360046
Epoch 1050, training loss: 88.50370025634766 = 0.48050442337989807 + 10.0 * 8.802319526672363
Epoch 1050, val loss: 0.5217076539993286
Epoch 1060, training loss: 88.4776840209961 = 0.47640082240104675 + 10.0 * 8.800127983093262
Epoch 1060, val loss: 0.518559455871582
Epoch 1070, training loss: 88.5374984741211 = 0.47250261902809143 + 10.0 * 8.806499481201172
Epoch 1070, val loss: 0.5156405568122864
Epoch 1080, training loss: 88.48925018310547 = 0.468658983707428 + 10.0 * 8.802059173583984
Epoch 1080, val loss: 0.512836217880249
Epoch 1090, training loss: 88.44709014892578 = 0.4649890959262848 + 10.0 * 8.798210144042969
Epoch 1090, val loss: 0.5099416971206665
Epoch 1100, training loss: 88.43549346923828 = 0.461699903011322 + 10.0 * 8.797379493713379
Epoch 1100, val loss: 0.507395327091217
Epoch 1110, training loss: 88.32736206054688 = 0.4581598937511444 + 10.0 * 8.786920547485352
Epoch 1110, val loss: 0.5048553943634033
Epoch 1120, training loss: 88.41739654541016 = 0.45490992069244385 + 10.0 * 8.796248435974121
Epoch 1120, val loss: 0.5025470852851868
Epoch 1130, training loss: 88.50740814208984 = 0.4515901803970337 + 10.0 * 8.805582046508789
Epoch 1130, val loss: 0.4999955892562866
Epoch 1140, training loss: 88.54472351074219 = 0.44840312004089355 + 10.0 * 8.809632301330566
Epoch 1140, val loss: 0.49756211042404175
Epoch 1150, training loss: 88.568603515625 = 0.44525036215782166 + 10.0 * 8.812335014343262
Epoch 1150, val loss: 0.49523892998695374
Epoch 1160, training loss: 88.57215118408203 = 0.44229811429977417 + 10.0 * 8.81298542022705
Epoch 1160, val loss: 0.49309784173965454
Epoch 1170, training loss: 88.62318420410156 = 0.4393767714500427 + 10.0 * 8.818380355834961
Epoch 1170, val loss: 0.49107012152671814
Epoch 1180, training loss: 88.54527282714844 = 0.4364200532436371 + 10.0 * 8.810885429382324
Epoch 1180, val loss: 0.488781213760376
Epoch 1190, training loss: 88.56253814697266 = 0.43380123376846313 + 10.0 * 8.812873840332031
Epoch 1190, val loss: 0.48693227767944336
Epoch 1200, training loss: 88.63523864746094 = 0.431106299161911 + 10.0 * 8.820413589477539
Epoch 1200, val loss: 0.48519167304039
Epoch 1210, training loss: 88.68584442138672 = 0.42848461866378784 + 10.0 * 8.825736045837402
Epoch 1210, val loss: 0.4832710921764374
Epoch 1220, training loss: 88.70978546142578 = 0.4258531332015991 + 10.0 * 8.828393936157227
Epoch 1220, val loss: 0.4814724326133728
Epoch 1230, training loss: 88.71595001220703 = 0.4233212172985077 + 10.0 * 8.829262733459473
Epoch 1230, val loss: 0.4798242151737213
Epoch 1240, training loss: 88.76726531982422 = 0.4208680987358093 + 10.0 * 8.834639549255371
Epoch 1240, val loss: 0.4780595898628235
Epoch 1250, training loss: 88.75159454345703 = 0.41846662759780884 + 10.0 * 8.83331298828125
Epoch 1250, val loss: 0.4764496684074402
Epoch 1260, training loss: 88.78426361083984 = 0.4161585867404938 + 10.0 * 8.836810111999512
Epoch 1260, val loss: 0.47481781244277954
Epoch 1270, training loss: 88.80386352539062 = 0.41388770937919617 + 10.0 * 8.838997840881348
Epoch 1270, val loss: 0.4732245206832886
Epoch 1280, training loss: 88.4600601196289 = 0.41149842739105225 + 10.0 * 8.804856300354004
Epoch 1280, val loss: 0.4717651605606079
Epoch 1290, training loss: 88.57613372802734 = 0.40994831919670105 + 10.0 * 8.816617965698242
Epoch 1290, val loss: 0.4704899787902832
Epoch 1300, training loss: 88.31717681884766 = 0.4078447222709656 + 10.0 * 8.790933609008789
Epoch 1300, val loss: 0.4701884388923645
Epoch 1310, training loss: 88.52436065673828 = 0.4065138101577759 + 10.0 * 8.811784744262695
Epoch 1310, val loss: 0.4678271412849426
Epoch 1320, training loss: 88.53319549560547 = 0.4043102562427521 + 10.0 * 8.812888145446777
Epoch 1320, val loss: 0.4664388597011566
Epoch 1330, training loss: 88.46076202392578 = 0.4023053050041199 + 10.0 * 8.805845260620117
Epoch 1330, val loss: 0.4653073251247406
Epoch 1340, training loss: 88.56812286376953 = 0.40047284960746765 + 10.0 * 8.816764831542969
Epoch 1340, val loss: 0.4641321897506714
Epoch 1350, training loss: 88.66535949707031 = 0.39848941564559937 + 10.0 * 8.82668685913086
Epoch 1350, val loss: 0.46261224150657654
Epoch 1360, training loss: 88.72982025146484 = 0.39653921127319336 + 10.0 * 8.833328247070312
Epoch 1360, val loss: 0.4613296389579773
Epoch 1370, training loss: 88.75292205810547 = 0.3945993483066559 + 10.0 * 8.835832595825195
Epoch 1370, val loss: 0.4600245952606201
Epoch 1380, training loss: 88.728271484375 = 0.3926994800567627 + 10.0 * 8.83355712890625
Epoch 1380, val loss: 0.45879220962524414
Epoch 1390, training loss: 88.79950714111328 = 0.39092621207237244 + 10.0 * 8.840858459472656
Epoch 1390, val loss: 0.45763784646987915
Epoch 1400, training loss: 88.82569885253906 = 0.38912060856819153 + 10.0 * 8.843657493591309
Epoch 1400, val loss: 0.45653584599494934
Epoch 1410, training loss: 88.83427429199219 = 0.38734641671180725 + 10.0 * 8.844693183898926
Epoch 1410, val loss: 0.4553532004356384
Epoch 1420, training loss: 88.8717041015625 = 0.38560613989830017 + 10.0 * 8.848609924316406
Epoch 1420, val loss: 0.454209566116333
Epoch 1430, training loss: 88.90654754638672 = 0.38391223549842834 + 10.0 * 8.852263450622559
Epoch 1430, val loss: 0.4531811773777008
Epoch 1440, training loss: 88.9251708984375 = 0.3822159171104431 + 10.0 * 8.85429573059082
Epoch 1440, val loss: 0.4521196186542511
Epoch 1450, training loss: 88.9485855102539 = 0.3805372714996338 + 10.0 * 8.856804847717285
Epoch 1450, val loss: 0.45103660225868225
Epoch 1460, training loss: 88.96873474121094 = 0.37888798117637634 + 10.0 * 8.858983993530273
Epoch 1460, val loss: 0.44999104738235474
Epoch 1470, training loss: 88.97002410888672 = 0.37729594111442566 + 10.0 * 8.859272956848145
Epoch 1470, val loss: 0.4491218328475952
Epoch 1480, training loss: 88.98970031738281 = 0.37568747997283936 + 10.0 * 8.861401557922363
Epoch 1480, val loss: 0.4481773376464844
Epoch 1490, training loss: 89.04401397705078 = 0.37411510944366455 + 10.0 * 8.866990089416504
Epoch 1490, val loss: 0.44711655378341675
Epoch 1500, training loss: 89.03206634521484 = 0.37253743410110474 + 10.0 * 8.86595344543457
Epoch 1500, val loss: 0.4462519884109497
Epoch 1510, training loss: 89.07540893554688 = 0.3709932565689087 + 10.0 * 8.870441436767578
Epoch 1510, val loss: 0.44535717368125916
Epoch 1520, training loss: 89.03954315185547 = 0.3694136440753937 + 10.0 * 8.867012977600098
Epoch 1520, val loss: 0.4445411264896393
Epoch 1530, training loss: 89.0937271118164 = 0.3678801655769348 + 10.0 * 8.872584342956543
Epoch 1530, val loss: 0.4435758590698242
Epoch 1540, training loss: 89.11811828613281 = 0.3663570284843445 + 10.0 * 8.875176429748535
Epoch 1540, val loss: 0.44269537925720215
Epoch 1550, training loss: 89.12223052978516 = 0.36483708024024963 + 10.0 * 8.875739097595215
Epoch 1550, val loss: 0.44182872772216797
Epoch 1560, training loss: 89.12406921386719 = 0.36331140995025635 + 10.0 * 8.876075744628906
Epoch 1560, val loss: 0.4409864544868469
Epoch 1570, training loss: 89.13477325439453 = 0.3618195354938507 + 10.0 * 8.87729549407959
Epoch 1570, val loss: 0.4402003288269043
Epoch 1580, training loss: 89.14813995361328 = 0.36037203669548035 + 10.0 * 8.878776550292969
Epoch 1580, val loss: 0.43945929408073425
Epoch 1590, training loss: 89.19027709960938 = 0.3588862419128418 + 10.0 * 8.883138656616211
Epoch 1590, val loss: 0.438622385263443
Epoch 1600, training loss: 89.21073913574219 = 0.3573758602142334 + 10.0 * 8.885335922241211
Epoch 1600, val loss: 0.4377748966217041
Epoch 1610, training loss: 89.19486999511719 = 0.3559168875217438 + 10.0 * 8.883894920349121
Epoch 1610, val loss: 0.43703949451446533
Epoch 1620, training loss: 89.20449829101562 = 0.3544789254665375 + 10.0 * 8.885002136230469
Epoch 1620, val loss: 0.43631717562675476
Epoch 1630, training loss: 89.23274993896484 = 0.35303759574890137 + 10.0 * 8.887971878051758
Epoch 1630, val loss: 0.43556317687034607
Epoch 1640, training loss: 89.21305847167969 = 0.3515934944152832 + 10.0 * 8.886146545410156
Epoch 1640, val loss: 0.43476349115371704
Epoch 1650, training loss: 89.22840881347656 = 0.3501649498939514 + 10.0 * 8.887824058532715
Epoch 1650, val loss: 0.43407538533210754
Epoch 1660, training loss: 89.26063537597656 = 0.3487311005592346 + 10.0 * 8.891190528869629
Epoch 1660, val loss: 0.43346065282821655
Epoch 1670, training loss: 89.26071166992188 = 0.3473500609397888 + 10.0 * 8.891336441040039
Epoch 1670, val loss: 0.4328248202800751
Epoch 1680, training loss: 89.21915435791016 = 0.3459687829017639 + 10.0 * 8.88731861114502
Epoch 1680, val loss: 0.43184173107147217
Epoch 1690, training loss: 89.25959777832031 = 0.34461596608161926 + 10.0 * 8.891497611999512
Epoch 1690, val loss: 0.4313410222530365
Epoch 1700, training loss: 89.33256530761719 = 0.34319233894348145 + 10.0 * 8.898937225341797
Epoch 1700, val loss: 0.43064454197883606
Epoch 1710, training loss: 89.33684539794922 = 0.3417719602584839 + 10.0 * 8.899507522583008
Epoch 1710, val loss: 0.4299449026584625
Epoch 1720, training loss: 89.341552734375 = 0.3403207063674927 + 10.0 * 8.90012264251709
Epoch 1720, val loss: 0.4292028248310089
Epoch 1730, training loss: 89.34740447998047 = 0.3389018476009369 + 10.0 * 8.900850296020508
Epoch 1730, val loss: 0.4285474419593811
Epoch 1740, training loss: 89.3396224975586 = 0.3374840319156647 + 10.0 * 8.900213241577148
Epoch 1740, val loss: 0.4278614819049835
Epoch 1750, training loss: 89.35894012451172 = 0.3361460268497467 + 10.0 * 8.9022798538208
Epoch 1750, val loss: 0.42720696330070496
Epoch 1760, training loss: 89.36592864990234 = 0.33470526337623596 + 10.0 * 8.903122901916504
Epoch 1760, val loss: 0.42662954330444336
Epoch 1770, training loss: 89.4164810180664 = 0.3333224058151245 + 10.0 * 8.908315658569336
Epoch 1770, val loss: 0.42578554153442383
Epoch 1780, training loss: 89.3332748413086 = 0.33189138770103455 + 10.0 * 8.900137901306152
Epoch 1780, val loss: 0.42520493268966675
Epoch 1790, training loss: 89.38360595703125 = 0.3305378556251526 + 10.0 * 8.905306816101074
Epoch 1790, val loss: 0.4245121479034424
Epoch 1800, training loss: 89.41200256347656 = 0.32914644479751587 + 10.0 * 8.908285140991211
Epoch 1800, val loss: 0.42389240860939026
Epoch 1810, training loss: 89.47161102294922 = 0.32775235176086426 + 10.0 * 8.914385795593262
Epoch 1810, val loss: 0.4232502579689026
Epoch 1820, training loss: 89.5028305053711 = 0.3263357877731323 + 10.0 * 8.91765022277832
Epoch 1820, val loss: 0.4225742518901825
Epoch 1830, training loss: 89.33619689941406 = 0.3249984383583069 + 10.0 * 8.90112018585205
Epoch 1830, val loss: 0.4223276674747467
Epoch 1840, training loss: 89.24606323242188 = 0.3237476646900177 + 10.0 * 8.892231941223145
Epoch 1840, val loss: 0.4212784767150879
Epoch 1850, training loss: 89.27454376220703 = 0.322624534368515 + 10.0 * 8.89519214630127
Epoch 1850, val loss: 0.4216744899749756
Epoch 1860, training loss: 89.33863067626953 = 0.3212803304195404 + 10.0 * 8.901735305786133
Epoch 1860, val loss: 0.4204584062099457
Epoch 1870, training loss: 89.4200439453125 = 0.31990236043930054 + 10.0 * 8.910014152526855
Epoch 1870, val loss: 0.4199989140033722
Epoch 1880, training loss: 89.48320007324219 = 0.3185103237628937 + 10.0 * 8.916468620300293
Epoch 1880, val loss: 0.41935062408447266
Epoch 1890, training loss: 89.49142456054688 = 0.3171413540840149 + 10.0 * 8.917428970336914
Epoch 1890, val loss: 0.418815016746521
Epoch 1900, training loss: 89.52214050292969 = 0.31574463844299316 + 10.0 * 8.920639991760254
Epoch 1900, val loss: 0.418296754360199
Epoch 1910, training loss: 89.51850128173828 = 0.3143962323665619 + 10.0 * 8.92041015625
Epoch 1910, val loss: 0.41766583919525146
Epoch 1920, training loss: 89.52019500732422 = 0.3130382001399994 + 10.0 * 8.92071533203125
Epoch 1920, val loss: 0.41722097992897034
Epoch 1930, training loss: 89.52949523925781 = 0.3116951882839203 + 10.0 * 8.92177963256836
Epoch 1930, val loss: 0.41667112708091736
Epoch 1940, training loss: 89.54759979248047 = 0.3103543519973755 + 10.0 * 8.923724174499512
Epoch 1940, val loss: 0.416127473115921
Epoch 1950, training loss: 89.57733917236328 = 0.30900418758392334 + 10.0 * 8.926833152770996
Epoch 1950, val loss: 0.4157189130783081
Epoch 1960, training loss: 89.59548950195312 = 0.30767783522605896 + 10.0 * 8.928781509399414
Epoch 1960, val loss: 0.41517096757888794
Epoch 1970, training loss: 89.59387969970703 = 0.3063422441482544 + 10.0 * 8.928753852844238
Epoch 1970, val loss: 0.41471365094184875
Epoch 1980, training loss: 89.6204833984375 = 0.30502429604530334 + 10.0 * 8.931546211242676
Epoch 1980, val loss: 0.4143001437187195
Epoch 1990, training loss: 89.60980987548828 = 0.303740531206131 + 10.0 * 8.930606842041016
Epoch 1990, val loss: 0.4138809144496918
Epoch 2000, training loss: 89.64143371582031 = 0.3024327754974365 + 10.0 * 8.933900833129883
Epoch 2000, val loss: 0.413333535194397
Epoch 2010, training loss: 89.66756439208984 = 0.3011036217212677 + 10.0 * 8.936646461486816
Epoch 2010, val loss: 0.4130610227584839
Epoch 2020, training loss: 89.65397644042969 = 0.2997618317604065 + 10.0 * 8.93542194366455
Epoch 2020, val loss: 0.41253694891929626
Epoch 2030, training loss: 89.62905883789062 = 0.2985033094882965 + 10.0 * 8.933055877685547
Epoch 2030, val loss: 0.4121890962123871
Epoch 2040, training loss: 89.65135192871094 = 0.2972111403942108 + 10.0 * 8.93541431427002
Epoch 2040, val loss: 0.41183042526245117
Epoch 2050, training loss: 89.7012710571289 = 0.29590579867362976 + 10.0 * 8.940536499023438
Epoch 2050, val loss: 0.41147270798683167
Epoch 2060, training loss: 89.73955535888672 = 0.29458168148994446 + 10.0 * 8.944498062133789
Epoch 2060, val loss: 0.4110061526298523
Epoch 2070, training loss: 89.71366119384766 = 0.29328182339668274 + 10.0 * 8.942037582397461
Epoch 2070, val loss: 0.4105967879295349
Epoch 2080, training loss: 89.70338439941406 = 0.2919955253601074 + 10.0 * 8.941139221191406
Epoch 2080, val loss: 0.41029325127601624
Epoch 2090, training loss: 89.71849822998047 = 0.2906879186630249 + 10.0 * 8.942781448364258
Epoch 2090, val loss: 0.4098462164402008
Epoch 2100, training loss: 89.77191162109375 = 0.289386510848999 + 10.0 * 8.94825267791748
Epoch 2100, val loss: 0.4094388782978058
Epoch 2110, training loss: 89.75060272216797 = 0.2880498170852661 + 10.0 * 8.946255683898926
Epoch 2110, val loss: 0.40906310081481934
Epoch 2120, training loss: 89.74007415771484 = 0.286827951669693 + 10.0 * 8.945324897766113
Epoch 2120, val loss: 0.4088360071182251
Epoch 2130, training loss: 89.74559020996094 = 0.28552567958831787 + 10.0 * 8.946006774902344
Epoch 2130, val loss: 0.4085995554924011
Epoch 2140, training loss: 89.80622863769531 = 0.28422385454177856 + 10.0 * 8.952199935913086
Epoch 2140, val loss: 0.40821734070777893
Epoch 2150, training loss: 89.82649230957031 = 0.2829239070415497 + 10.0 * 8.954357147216797
Epoch 2150, val loss: 0.4077203571796417
Epoch 2160, training loss: 89.73208618164062 = 0.28162682056427 + 10.0 * 8.945046424865723
Epoch 2160, val loss: 0.4075809419155121
Epoch 2170, training loss: 89.79688262939453 = 0.2803989350795746 + 10.0 * 8.951648712158203
Epoch 2170, val loss: 0.40730202198028564
Epoch 2180, training loss: 89.84864044189453 = 0.27913111448287964 + 10.0 * 8.956951141357422
Epoch 2180, val loss: 0.40699389576911926
Epoch 2190, training loss: 89.85281372070312 = 0.27781224250793457 + 10.0 * 8.957500457763672
Epoch 2190, val loss: 0.40673384070396423
Epoch 2200, training loss: 89.85997009277344 = 0.27651914954185486 + 10.0 * 8.958345413208008
Epoch 2200, val loss: 0.40635648369789124
Epoch 2210, training loss: 89.86343383789062 = 0.27520638704299927 + 10.0 * 8.958822250366211
Epoch 2210, val loss: 0.40611031651496887
Epoch 2220, training loss: 89.87734985351562 = 0.2738925814628601 + 10.0 * 8.960345268249512
Epoch 2220, val loss: 0.40591010451316833
Epoch 2230, training loss: 89.86149597167969 = 0.27260565757751465 + 10.0 * 8.95888900756836
Epoch 2230, val loss: 0.4058116376399994
Epoch 2240, training loss: 89.86422729492188 = 0.27136939764022827 + 10.0 * 8.959285736083984
Epoch 2240, val loss: 0.4055534601211548
Epoch 2250, training loss: 89.88134002685547 = 0.27008703351020813 + 10.0 * 8.961125373840332
Epoch 2250, val loss: 0.4054785668849945
Epoch 2260, training loss: 89.90585327148438 = 0.26879626512527466 + 10.0 * 8.963705062866211
Epoch 2260, val loss: 0.4052119851112366
Epoch 2270, training loss: 89.9261474609375 = 0.2675149440765381 + 10.0 * 8.965863227844238
Epoch 2270, val loss: 0.40521368384361267
Epoch 2280, training loss: 89.89399719238281 = 0.26623257994651794 + 10.0 * 8.962776184082031
Epoch 2280, val loss: 0.4049539566040039
Epoch 2290, training loss: 89.90995025634766 = 0.26498764753341675 + 10.0 * 8.964496612548828
Epoch 2290, val loss: 0.40498995780944824
Epoch 2300, training loss: 89.94416046142578 = 0.2637380063533783 + 10.0 * 8.968042373657227
Epoch 2300, val loss: 0.40480858087539673
Epoch 2310, training loss: 89.95410919189453 = 0.262449711561203 + 10.0 * 8.969165802001953
Epoch 2310, val loss: 0.40465840697288513
Epoch 2320, training loss: 89.97559356689453 = 0.2611754834651947 + 10.0 * 8.971441268920898
Epoch 2320, val loss: 0.40469321608543396
Epoch 2330, training loss: 89.9945297241211 = 0.2598904073238373 + 10.0 * 8.973464012145996
Epoch 2330, val loss: 0.4044598340988159
Epoch 2340, training loss: 89.96614837646484 = 0.2586511969566345 + 10.0 * 8.970749855041504
Epoch 2340, val loss: 0.40457358956336975
Epoch 2350, training loss: 89.88436126708984 = 0.2576760947704315 + 10.0 * 8.962668418884277
Epoch 2350, val loss: 0.4056370258331299
Epoch 2360, training loss: 89.59198760986328 = 0.25666025280952454 + 10.0 * 8.93353271484375
Epoch 2360, val loss: 0.40453872084617615
Epoch 2370, training loss: 89.81456756591797 = 0.2559663951396942 + 10.0 * 8.955860137939453
Epoch 2370, val loss: 0.403464674949646
Epoch 2380, training loss: 89.55055236816406 = 0.25481465458869934 + 10.0 * 8.929574012756348
Epoch 2380, val loss: 0.406046062707901
Epoch 2390, training loss: 89.6733627319336 = 0.25349941849708557 + 10.0 * 8.941986083984375
Epoch 2390, val loss: 0.4052921235561371
Epoch 2400, training loss: 89.78907012939453 = 0.2522127330303192 + 10.0 * 8.953685760498047
Epoch 2400, val loss: 0.40550899505615234
Epoch 2410, training loss: 89.85771942138672 = 0.2509247660636902 + 10.0 * 8.96068000793457
Epoch 2410, val loss: 0.4053722023963928
Epoch 2420, training loss: 89.90499877929688 = 0.2496134340763092 + 10.0 * 8.96553897857666
Epoch 2420, val loss: 0.40522560477256775
Epoch 2430, training loss: 89.99378204345703 = 0.24831263720989227 + 10.0 * 8.974546432495117
Epoch 2430, val loss: 0.4051569104194641
Epoch 2440, training loss: 89.42279815673828 = 0.24750758707523346 + 10.0 * 8.917529106140137
Epoch 2440, val loss: 0.4045465588569641
Epoch 2450, training loss: 89.72862243652344 = 0.24618512392044067 + 10.0 * 8.948244094848633
Epoch 2450, val loss: 0.40458202362060547
Epoch 2460, training loss: 89.68413543701172 = 0.24483650922775269 + 10.0 * 8.943929672241211
Epoch 2460, val loss: 0.4058564603328705
Epoch 2470, training loss: 89.83126068115234 = 0.24363166093826294 + 10.0 * 8.958763122558594
Epoch 2470, val loss: 0.4052577614784241
Epoch 2480, training loss: 89.9078369140625 = 0.24233824014663696 + 10.0 * 8.96654987335205
Epoch 2480, val loss: 0.4055963158607483
Epoch 2490, training loss: 89.96416473388672 = 0.24102246761322021 + 10.0 * 8.972314834594727
Epoch 2490, val loss: 0.4054129719734192
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8543478260869565
0.8635803810765776
=== training gcn model ===
Epoch 0, training loss: 102.30963897705078 = 1.1020455360412598 + 10.0 * 10.120759963989258
Epoch 0, val loss: 1.0998892784118652
Epoch 10, training loss: 98.27189636230469 = 1.0982439517974854 + 10.0 * 9.717365264892578
Epoch 10, val loss: 1.0961588621139526
Epoch 20, training loss: 96.2548599243164 = 1.0947513580322266 + 10.0 * 9.516011238098145
Epoch 20, val loss: 1.0927282571792603
Epoch 30, training loss: 94.74195861816406 = 1.0913636684417725 + 10.0 * 9.365059852600098
Epoch 30, val loss: 1.0894125699996948
Epoch 40, training loss: 93.586669921875 = 1.0881158113479614 + 10.0 * 9.249855041503906
Epoch 40, val loss: 1.0862525701522827
Epoch 50, training loss: 92.6678237915039 = 1.0850025415420532 + 10.0 * 9.158282279968262
Epoch 50, val loss: 1.0832321643829346
Epoch 60, training loss: 91.91106414794922 = 1.0820056200027466 + 10.0 * 9.082905769348145
Epoch 60, val loss: 1.0803412199020386
Epoch 70, training loss: 91.2813491821289 = 1.0790995359420776 + 10.0 * 9.020224571228027
Epoch 70, val loss: 1.0775532722473145
Epoch 80, training loss: 90.75186920166016 = 1.0762888193130493 + 10.0 * 8.967557907104492
Epoch 80, val loss: 1.0748670101165771
Epoch 90, training loss: 90.30660247802734 = 1.0735723972320557 + 10.0 * 8.923303604125977
Epoch 90, val loss: 1.0722899436950684
Epoch 100, training loss: 89.93526458740234 = 1.0709397792816162 + 10.0 * 8.886432647705078
Epoch 100, val loss: 1.069793939590454
Epoch 110, training loss: 89.60408782958984 = 1.0683702230453491 + 10.0 * 8.853571891784668
Epoch 110, val loss: 1.0673726797103882
Epoch 120, training loss: 89.32006072998047 = 1.0658966302871704 + 10.0 * 8.825416564941406
Epoch 120, val loss: 1.0650357007980347
Epoch 130, training loss: 89.07988739013672 = 1.063482642173767 + 10.0 * 8.801640510559082
Epoch 130, val loss: 1.0627572536468506
Epoch 140, training loss: 88.87661743164062 = 1.0611519813537598 + 10.0 * 8.781546592712402
Epoch 140, val loss: 1.0605578422546387
Epoch 150, training loss: 88.6764907836914 = 1.0588058233261108 + 10.0 * 8.761768341064453
Epoch 150, val loss: 1.0583175420761108
Epoch 160, training loss: 88.51387023925781 = 1.0566216707229614 + 10.0 * 8.74572467803955
Epoch 160, val loss: 1.0562676191329956
Epoch 170, training loss: 88.38399505615234 = 1.0543642044067383 + 10.0 * 8.732962608337402
Epoch 170, val loss: 1.0541549921035767
Epoch 180, training loss: 88.25724792480469 = 1.0522873401641846 + 10.0 * 8.72049617767334
Epoch 180, val loss: 1.0521706342697144
Epoch 190, training loss: 88.1318588256836 = 1.0501012802124023 + 10.0 * 8.708175659179688
Epoch 190, val loss: 1.050123929977417
Epoch 200, training loss: 88.07644653320312 = 1.0479400157928467 + 10.0 * 8.702850341796875
Epoch 200, val loss: 1.0480432510375977
Epoch 210, training loss: 87.97702026367188 = 1.0457977056503296 + 10.0 * 8.693121910095215
Epoch 210, val loss: 1.046034336090088
Epoch 220, training loss: 87.87251281738281 = 1.043481707572937 + 10.0 * 8.682903289794922
Epoch 220, val loss: 1.0438276529312134
Epoch 230, training loss: 87.83494567871094 = 1.0412386655807495 + 10.0 * 8.679370880126953
Epoch 230, val loss: 1.0417065620422363
Epoch 240, training loss: 87.7546615600586 = 1.0388720035552979 + 10.0 * 8.671579360961914
Epoch 240, val loss: 1.0394370555877686
Epoch 250, training loss: 87.69609832763672 = 1.0364017486572266 + 10.0 * 8.665969848632812
Epoch 250, val loss: 1.0370372533798218
Epoch 260, training loss: 87.66133880615234 = 1.0337573289871216 + 10.0 * 8.662757873535156
Epoch 260, val loss: 1.0344644784927368
Epoch 270, training loss: 87.65159606933594 = 1.0310447216033936 + 10.0 * 8.662055015563965
Epoch 270, val loss: 1.0318286418914795
Epoch 280, training loss: 87.61610412597656 = 1.0281857252120972 + 10.0 * 8.658792495727539
Epoch 280, val loss: 1.02908456325531
Epoch 290, training loss: 87.57381439208984 = 1.0250649452209473 + 10.0 * 8.654874801635742
Epoch 290, val loss: 1.0261141061782837
Epoch 300, training loss: 87.56838989257812 = 1.0217536687850952 + 10.0 * 8.654664039611816
Epoch 300, val loss: 1.0228943824768066
Epoch 310, training loss: 87.49420166015625 = 1.0181245803833008 + 10.0 * 8.647607803344727
Epoch 310, val loss: 1.0193825960159302
Epoch 320, training loss: 87.47613525390625 = 1.014451503753662 + 10.0 * 8.64616870880127
Epoch 320, val loss: 1.0157679319381714
Epoch 330, training loss: 87.48542022705078 = 1.0104268789291382 + 10.0 * 8.647499084472656
Epoch 330, val loss: 1.011865258216858
Epoch 340, training loss: 87.41673278808594 = 1.0059765577316284 + 10.0 * 8.64107608795166
Epoch 340, val loss: 1.0074270963668823
Epoch 350, training loss: 87.44676208496094 = 1.0014516115188599 + 10.0 * 8.64453125
Epoch 350, val loss: 1.0030330419540405
Epoch 360, training loss: 87.47404479980469 = 0.9966039061546326 + 10.0 * 8.647744178771973
Epoch 360, val loss: 0.9983428716659546
Epoch 370, training loss: 87.4746322631836 = 0.9913235306739807 + 10.0 * 8.648330688476562
Epoch 370, val loss: 0.9932177662849426
Epoch 380, training loss: 87.48148345947266 = 0.9857487082481384 + 10.0 * 8.64957332611084
Epoch 380, val loss: 0.9877886176109314
Epoch 390, training loss: 87.51673126220703 = 0.9797315001487732 + 10.0 * 8.65369987487793
Epoch 390, val loss: 0.9819793105125427
Epoch 400, training loss: 87.43284606933594 = 0.9734112024307251 + 10.0 * 8.645943641662598
Epoch 400, val loss: 0.9757935404777527
Epoch 410, training loss: 87.4951171875 = 0.9670975208282471 + 10.0 * 8.652802467346191
Epoch 410, val loss: 0.9696617722511292
Epoch 420, training loss: 87.47982788085938 = 0.9601006507873535 + 10.0 * 8.651972770690918
Epoch 420, val loss: 0.9629213213920593
Epoch 430, training loss: 87.484375 = 0.9527640342712402 + 10.0 * 8.65316104888916
Epoch 430, val loss: 0.95582115650177
Epoch 440, training loss: 87.5093765258789 = 0.94523024559021 + 10.0 * 8.656414985656738
Epoch 440, val loss: 0.9485481977462769
Epoch 450, training loss: 87.51249694824219 = 0.9371755719184875 + 10.0 * 8.657532691955566
Epoch 450, val loss: 0.9407175183296204
Epoch 460, training loss: 87.55496978759766 = 0.9291273951530457 + 10.0 * 8.66258430480957
Epoch 460, val loss: 0.9328853487968445
Epoch 470, training loss: 87.54304504394531 = 0.9203044176101685 + 10.0 * 8.662274360656738
Epoch 470, val loss: 0.9244391322135925
Epoch 480, training loss: 87.55885314941406 = 0.911163330078125 + 10.0 * 8.664769172668457
Epoch 480, val loss: 0.9156972765922546
Epoch 490, training loss: 87.56742858886719 = 0.9017087817192078 + 10.0 * 8.666571617126465
Epoch 490, val loss: 0.9065260887145996
Epoch 500, training loss: 87.51172637939453 = 0.8919950127601624 + 10.0 * 8.661972999572754
Epoch 500, val loss: 0.897197961807251
Epoch 510, training loss: 87.49258422851562 = 0.8822192549705505 + 10.0 * 8.661036491394043
Epoch 510, val loss: 0.8877435326576233
Epoch 520, training loss: 87.50226593017578 = 0.8721320033073425 + 10.0 * 8.663013458251953
Epoch 520, val loss: 0.8780776858329773
Epoch 530, training loss: 87.54487609863281 = 0.8618282675743103 + 10.0 * 8.668304443359375
Epoch 530, val loss: 0.8682045936584473
Epoch 540, training loss: 87.54011535644531 = 0.8511848449707031 + 10.0 * 8.668892860412598
Epoch 540, val loss: 0.8579432368278503
Epoch 550, training loss: 87.54379272460938 = 0.8403337001800537 + 10.0 * 8.6703462600708
Epoch 550, val loss: 0.8474161624908447
Epoch 560, training loss: 87.5689697265625 = 0.8293758630752563 + 10.0 * 8.673959732055664
Epoch 560, val loss: 0.83689945936203
Epoch 570, training loss: 87.55233001708984 = 0.8181334137916565 + 10.0 * 8.673419952392578
Epoch 570, val loss: 0.826160192489624
Epoch 580, training loss: 87.57032775878906 = 0.8069074153900146 + 10.0 * 8.676342010498047
Epoch 580, val loss: 0.8153079152107239
Epoch 590, training loss: 87.58358001708984 = 0.7955933809280396 + 10.0 * 8.67879867553711
Epoch 590, val loss: 0.8044295907020569
Epoch 600, training loss: 87.5256118774414 = 0.783860445022583 + 10.0 * 8.674175262451172
Epoch 600, val loss: 0.7932270765304565
Epoch 610, training loss: 87.5827865600586 = 0.7722591161727905 + 10.0 * 8.681052207946777
Epoch 610, val loss: 0.7821938395500183
Epoch 620, training loss: 87.53682708740234 = 0.7607179284095764 + 10.0 * 8.677610397338867
Epoch 620, val loss: 0.7712721824645996
Epoch 630, training loss: 87.58563995361328 = 0.7495957612991333 + 10.0 * 8.68360424041748
Epoch 630, val loss: 0.7605984807014465
Epoch 640, training loss: 87.62786102294922 = 0.7384229302406311 + 10.0 * 8.688943862915039
Epoch 640, val loss: 0.7497900724411011
Epoch 650, training loss: 87.63594818115234 = 0.7269643545150757 + 10.0 * 8.690897941589355
Epoch 650, val loss: 0.7389698028564453
Epoch 660, training loss: 87.66458129882812 = 0.7157655954360962 + 10.0 * 8.694881439208984
Epoch 660, val loss: 0.7283194661140442
Epoch 670, training loss: 87.69965362548828 = 0.7046616673469543 + 10.0 * 8.699499130249023
Epoch 670, val loss: 0.7178770899772644
Epoch 680, training loss: 87.7850570678711 = 0.6934901475906372 + 10.0 * 8.70915699005127
Epoch 680, val loss: 0.7073005437850952
Epoch 690, training loss: 87.556884765625 = 0.6825286746025085 + 10.0 * 8.6874361038208
Epoch 690, val loss: 0.6971137523651123
Epoch 700, training loss: 87.69123840332031 = 0.6724240183830261 + 10.0 * 8.701881408691406
Epoch 700, val loss: 0.6874144673347473
Epoch 710, training loss: 87.77831268310547 = 0.6623706221580505 + 10.0 * 8.711594581604004
Epoch 710, val loss: 0.6779009103775024
Epoch 720, training loss: 87.76907348632812 = 0.6518457531929016 + 10.0 * 8.711722373962402
Epoch 720, val loss: 0.6681254506111145
Epoch 730, training loss: 87.80162048339844 = 0.6419644951820374 + 10.0 * 8.715965270996094
Epoch 730, val loss: 0.6589399576187134
Epoch 740, training loss: 87.82762145996094 = 0.6323469877243042 + 10.0 * 8.719527244567871
Epoch 740, val loss: 0.649907112121582
Epoch 750, training loss: 87.81599426269531 = 0.622952401638031 + 10.0 * 8.719304084777832
Epoch 750, val loss: 0.6411973237991333
Epoch 760, training loss: 87.85680389404297 = 0.6140362024307251 + 10.0 * 8.724276542663574
Epoch 760, val loss: 0.6329299211502075
Epoch 770, training loss: 87.91384887695312 = 0.6054522395133972 + 10.0 * 8.730839729309082
Epoch 770, val loss: 0.6248330473899841
Epoch 780, training loss: 87.8803482055664 = 0.5970653295516968 + 10.0 * 8.728327751159668
Epoch 780, val loss: 0.6169978976249695
Epoch 790, training loss: 87.77853393554688 = 0.5889074802398682 + 10.0 * 8.718962669372559
Epoch 790, val loss: 0.609843909740448
Epoch 800, training loss: 87.69468688964844 = 0.5813418626785278 + 10.0 * 8.711334228515625
Epoch 800, val loss: 0.602762758731842
Epoch 810, training loss: 87.84068298339844 = 0.5743894577026367 + 10.0 * 8.726629257202148
Epoch 810, val loss: 0.5962084531784058
Epoch 820, training loss: 87.81629943847656 = 0.5674588680267334 + 10.0 * 8.724884033203125
Epoch 820, val loss: 0.5899025797843933
Epoch 830, training loss: 87.85619354248047 = 0.5608910322189331 + 10.0 * 8.729530334472656
Epoch 830, val loss: 0.5839689373970032
Epoch 840, training loss: 87.93597412109375 = 0.5545975565910339 + 10.0 * 8.738138198852539
Epoch 840, val loss: 0.5783899426460266
Epoch 850, training loss: 87.96983337402344 = 0.5484163761138916 + 10.0 * 8.742141723632812
Epoch 850, val loss: 0.5728715062141418
Epoch 860, training loss: 87.97200012207031 = 0.5425288081169128 + 10.0 * 8.742947578430176
Epoch 860, val loss: 0.5676245093345642
Epoch 870, training loss: 87.8521957397461 = 0.5368732213973999 + 10.0 * 8.731532096862793
Epoch 870, val loss: 0.5628663897514343
Epoch 880, training loss: 87.86311340332031 = 0.5315655469894409 + 10.0 * 8.733155250549316
Epoch 880, val loss: 0.5581617951393127
Epoch 890, training loss: 87.95755767822266 = 0.5266655683517456 + 10.0 * 8.74308967590332
Epoch 890, val loss: 0.5537036061286926
Epoch 900, training loss: 88.02896118164062 = 0.5217297673225403 + 10.0 * 8.750722885131836
Epoch 900, val loss: 0.549427330493927
Epoch 910, training loss: 88.01018524169922 = 0.5169923305511475 + 10.0 * 8.749319076538086
Epoch 910, val loss: 0.5454542636871338
Epoch 920, training loss: 88.07327270507812 = 0.5124596357345581 + 10.0 * 8.756081581115723
Epoch 920, val loss: 0.5415312051773071
Epoch 930, training loss: 88.06605529785156 = 0.5079923868179321 + 10.0 * 8.755805969238281
Epoch 930, val loss: 0.5379364490509033
Epoch 940, training loss: 88.08409118652344 = 0.5038877725601196 + 10.0 * 8.758020401000977
Epoch 940, val loss: 0.534430742263794
Epoch 950, training loss: 88.12232971191406 = 0.49989256262779236 + 10.0 * 8.762243270874023
Epoch 950, val loss: 0.5310995578765869
Epoch 960, training loss: 88.1300277709961 = 0.496145099401474 + 10.0 * 8.763387680053711
Epoch 960, val loss: 0.5279403924942017
Epoch 970, training loss: 88.1241455078125 = 0.49250081181526184 + 10.0 * 8.763164520263672
Epoch 970, val loss: 0.5250119566917419
Epoch 980, training loss: 88.18302154541016 = 0.4889276921749115 + 10.0 * 8.7694091796875
Epoch 980, val loss: 0.5220550298690796
Epoch 990, training loss: 88.19715118408203 = 0.48539671301841736 + 10.0 * 8.771175384521484
Epoch 990, val loss: 0.5191485285758972
Epoch 1000, training loss: 88.2164535522461 = 0.4820516109466553 + 10.0 * 8.77344036102295
Epoch 1000, val loss: 0.516600489616394
Epoch 1010, training loss: 88.23542785644531 = 0.4787273705005646 + 10.0 * 8.775670051574707
Epoch 1010, val loss: 0.5139815211296082
Epoch 1020, training loss: 88.2640151977539 = 0.47552987933158875 + 10.0 * 8.778848648071289
Epoch 1020, val loss: 0.511451244354248
Epoch 1030, training loss: 88.31334686279297 = 0.47238487005233765 + 10.0 * 8.784096717834473
Epoch 1030, val loss: 0.5089843273162842
Epoch 1040, training loss: 88.2618637084961 = 0.4693416655063629 + 10.0 * 8.779252052307129
Epoch 1040, val loss: 0.5065744519233704
Epoch 1050, training loss: 88.41825103759766 = 0.4666578471660614 + 10.0 * 8.795159339904785
Epoch 1050, val loss: 0.5043014883995056
Epoch 1060, training loss: 88.3578872680664 = 0.4637812376022339 + 10.0 * 8.789410591125488
Epoch 1060, val loss: 0.5024828314781189
Epoch 1070, training loss: 88.4239501953125 = 0.4609277546405792 + 10.0 * 8.79630184173584
Epoch 1070, val loss: 0.5000542402267456
Epoch 1080, training loss: 88.44799041748047 = 0.4580988585948944 + 10.0 * 8.798989295959473
Epoch 1080, val loss: 0.49798813462257385
Epoch 1090, training loss: 88.3803939819336 = 0.45534735918045044 + 10.0 * 8.792505264282227
Epoch 1090, val loss: 0.4958982467651367
Epoch 1100, training loss: 88.38713073730469 = 0.4526243209838867 + 10.0 * 8.793451309204102
Epoch 1100, val loss: 0.4940996766090393
Epoch 1110, training loss: 88.44985961914062 = 0.450052946805954 + 10.0 * 8.799981117248535
Epoch 1110, val loss: 0.49203839898109436
Epoch 1120, training loss: 88.49312591552734 = 0.4475032687187195 + 10.0 * 8.80456256866455
Epoch 1120, val loss: 0.4904285669326782
Epoch 1130, training loss: 88.50106048583984 = 0.44555193185806274 + 10.0 * 8.805551528930664
Epoch 1130, val loss: 0.4884762763977051
Epoch 1140, training loss: 88.37639617919922 = 0.44307875633239746 + 10.0 * 8.79333209991455
Epoch 1140, val loss: 0.4871692955493927
Epoch 1150, training loss: 88.399658203125 = 0.4405384361743927 + 10.0 * 8.79591178894043
Epoch 1150, val loss: 0.4849916100502014
Epoch 1160, training loss: 88.43163299560547 = 0.4377143085002899 + 10.0 * 8.799391746520996
Epoch 1160, val loss: 0.4830904006958008
Epoch 1170, training loss: 88.50702667236328 = 0.435055673122406 + 10.0 * 8.807196617126465
Epoch 1170, val loss: 0.48104560375213623
Epoch 1180, training loss: 88.53826904296875 = 0.4324060380458832 + 10.0 * 8.810586929321289
Epoch 1180, val loss: 0.47903507947921753
Epoch 1190, training loss: 88.52220916748047 = 0.4298473596572876 + 10.0 * 8.809236526489258
Epoch 1190, val loss: 0.477093368768692
Epoch 1200, training loss: 88.52584838867188 = 0.42732173204421997 + 10.0 * 8.809852600097656
Epoch 1200, val loss: 0.47544899582862854
Epoch 1210, training loss: 88.60700988769531 = 0.4248594045639038 + 10.0 * 8.818215370178223
Epoch 1210, val loss: 0.4735703766345978
Epoch 1220, training loss: 88.59190368652344 = 0.422360897064209 + 10.0 * 8.816953659057617
Epoch 1220, val loss: 0.47180166840553284
Epoch 1230, training loss: 88.56513977050781 = 0.4199279844760895 + 10.0 * 8.814520835876465
Epoch 1230, val loss: 0.47017595171928406
Epoch 1240, training loss: 88.58161163330078 = 0.41758018732070923 + 10.0 * 8.81640338897705
Epoch 1240, val loss: 0.4685669541358948
Epoch 1250, training loss: 88.60356903076172 = 0.41518479585647583 + 10.0 * 8.818838119506836
Epoch 1250, val loss: 0.46678316593170166
Epoch 1260, training loss: 88.66680908203125 = 0.4127829968929291 + 10.0 * 8.825403213500977
Epoch 1260, val loss: 0.46524888277053833
Epoch 1270, training loss: 88.68388366699219 = 0.4104804992675781 + 10.0 * 8.827340126037598
Epoch 1270, val loss: 0.46341124176979065
Epoch 1280, training loss: 88.61431121826172 = 0.40835195779800415 + 10.0 * 8.820595741271973
Epoch 1280, val loss: 0.46213385462760925
Epoch 1290, training loss: 88.57293701171875 = 0.40616515278816223 + 10.0 * 8.81667709350586
Epoch 1290, val loss: 0.46023938059806824
Epoch 1300, training loss: 88.67918395996094 = 0.4039161801338196 + 10.0 * 8.827527046203613
Epoch 1300, val loss: 0.45871856808662415
Epoch 1310, training loss: 88.75009155273438 = 0.4016338884830475 + 10.0 * 8.834845542907715
Epoch 1310, val loss: 0.4570065438747406
Epoch 1320, training loss: 88.73950958251953 = 0.39929690957069397 + 10.0 * 8.834020614624023
Epoch 1320, val loss: 0.4555913507938385
Epoch 1330, training loss: 88.76490783691406 = 0.39705613255500793 + 10.0 * 8.836785316467285
Epoch 1330, val loss: 0.4538501799106598
Epoch 1340, training loss: 88.83470153808594 = 0.39482730627059937 + 10.0 * 8.843987464904785
Epoch 1340, val loss: 0.45239537954330444
Epoch 1350, training loss: 88.79959869384766 = 0.39258676767349243 + 10.0 * 8.84070110321045
Epoch 1350, val loss: 0.45069053769111633
Epoch 1360, training loss: 88.86759948730469 = 0.3904443681240082 + 10.0 * 8.847715377807617
Epoch 1360, val loss: 0.449309378862381
Epoch 1370, training loss: 88.82484436035156 = 0.3883250653743744 + 10.0 * 8.84365177154541
Epoch 1370, val loss: 0.4477602243423462
Epoch 1380, training loss: 88.8580093383789 = 0.38618966937065125 + 10.0 * 8.84718132019043
Epoch 1380, val loss: 0.4463472068309784
Epoch 1390, training loss: 88.86029052734375 = 0.3840266764163971 + 10.0 * 8.847626686096191
Epoch 1390, val loss: 0.4449586868286133
Epoch 1400, training loss: 88.87635040283203 = 0.382001668214798 + 10.0 * 8.849434852600098
Epoch 1400, val loss: 0.4434677064418793
Epoch 1410, training loss: 88.93467712402344 = 0.3799039125442505 + 10.0 * 8.855477333068848
Epoch 1410, val loss: 0.44220107793807983
Epoch 1420, training loss: 88.9830551147461 = 0.3777749240398407 + 10.0 * 8.860527992248535
Epoch 1420, val loss: 0.44068512320518494
Epoch 1430, training loss: 88.90433502197266 = 0.3757276237010956 + 10.0 * 8.852861404418945
Epoch 1430, val loss: 0.4392701983451843
Epoch 1440, training loss: 88.9476547241211 = 0.3736564815044403 + 10.0 * 8.857399940490723
Epoch 1440, val loss: 0.4381183385848999
Epoch 1450, training loss: 88.99220275878906 = 0.3716066777706146 + 10.0 * 8.862059593200684
Epoch 1450, val loss: 0.43676134943962097
Epoch 1460, training loss: 89.01936340332031 = 0.3695455491542816 + 10.0 * 8.864981651306152
Epoch 1460, val loss: 0.4353429973125458
Epoch 1470, training loss: 88.99786376953125 = 0.3674604594707489 + 10.0 * 8.86303997039795
Epoch 1470, val loss: 0.43403059244155884
Epoch 1480, training loss: 89.01300811767578 = 0.36545366048812866 + 10.0 * 8.864755630493164
Epoch 1480, val loss: 0.4326699376106262
Epoch 1490, training loss: 89.03347778320312 = 0.36342403292655945 + 10.0 * 8.867005348205566
Epoch 1490, val loss: 0.43166881799697876
Epoch 1500, training loss: 89.0644302368164 = 0.36142489314079285 + 10.0 * 8.87030029296875
Epoch 1500, val loss: 0.4300740659236908
Epoch 1510, training loss: 88.99979400634766 = 0.35948577523231506 + 10.0 * 8.864030838012695
Epoch 1510, val loss: 0.42897671461105347
Epoch 1520, training loss: 89.02671813964844 = 0.35755082964897156 + 10.0 * 8.86691665649414
Epoch 1520, val loss: 0.42734575271606445
Epoch 1530, training loss: 89.05628967285156 = 0.3556109666824341 + 10.0 * 8.870067596435547
Epoch 1530, val loss: 0.42622435092926025
Epoch 1540, training loss: 89.10462951660156 = 0.35362935066223145 + 10.0 * 8.875100135803223
Epoch 1540, val loss: 0.42492833733558655
Epoch 1550, training loss: 89.10273742675781 = 0.35163983702659607 + 10.0 * 8.875109672546387
Epoch 1550, val loss: 0.42369285225868225
Epoch 1560, training loss: 89.12287139892578 = 0.3496832847595215 + 10.0 * 8.877318382263184
Epoch 1560, val loss: 0.422465056180954
Epoch 1570, training loss: 89.12704467773438 = 0.34771811962127686 + 10.0 * 8.87793254852295
Epoch 1570, val loss: 0.42131224274635315
Epoch 1580, training loss: 89.00558471679688 = 0.34577029943466187 + 10.0 * 8.865981101989746
Epoch 1580, val loss: 0.42020076513290405
Epoch 1590, training loss: 88.81977081298828 = 0.3440603017807007 + 10.0 * 8.847570419311523
Epoch 1590, val loss: 0.41885730624198914
Epoch 1600, training loss: 89.03926849365234 = 0.3429398536682129 + 10.0 * 8.869632720947266
Epoch 1600, val loss: 0.4181252717971802
Epoch 1610, training loss: 88.83859252929688 = 0.3412403464317322 + 10.0 * 8.849735260009766
Epoch 1610, val loss: 0.417883962392807
Epoch 1620, training loss: 88.7608642578125 = 0.33941754698753357 + 10.0 * 8.842144966125488
Epoch 1620, val loss: 0.41548293828964233
Epoch 1630, training loss: 88.919189453125 = 0.33772873878479004 + 10.0 * 8.858145713806152
Epoch 1630, val loss: 0.4160829782485962
Epoch 1640, training loss: 89.05587768554688 = 0.3358159363269806 + 10.0 * 8.8720064163208
Epoch 1640, val loss: 0.41425156593322754
Epoch 1650, training loss: 88.98605346679688 = 0.33398061990737915 + 10.0 * 8.865206718444824
Epoch 1650, val loss: 0.4138221740722656
Epoch 1660, training loss: 89.02104187011719 = 0.33195748925209045 + 10.0 * 8.868908882141113
Epoch 1660, val loss: 0.4123392701148987
Epoch 1670, training loss: 89.0926513671875 = 0.3301517069339752 + 10.0 * 8.876249313354492
Epoch 1670, val loss: 0.4109400808811188
Epoch 1680, training loss: 89.1700210571289 = 0.32828134298324585 + 10.0 * 8.884174346923828
Epoch 1680, val loss: 0.41059014201164246
Epoch 1690, training loss: 89.21475219726562 = 0.32642000913619995 + 10.0 * 8.888833045959473
Epoch 1690, val loss: 0.40949273109436035
Epoch 1700, training loss: 89.24156951904297 = 0.3245669901371002 + 10.0 * 8.89169979095459
Epoch 1700, val loss: 0.40849053859710693
Epoch 1710, training loss: 89.2160873413086 = 0.322733998298645 + 10.0 * 8.889335632324219
Epoch 1710, val loss: 0.40762102603912354
Epoch 1720, training loss: 89.26897430419922 = 0.32090672850608826 + 10.0 * 8.894806861877441
Epoch 1720, val loss: 0.4067462980747223
Epoch 1730, training loss: 89.31484985351562 = 0.31909269094467163 + 10.0 * 8.899576187133789
Epoch 1730, val loss: 0.40583863854408264
Epoch 1740, training loss: 89.32447052001953 = 0.31732746958732605 + 10.0 * 8.900713920593262
Epoch 1740, val loss: 0.40481218695640564
Epoch 1750, training loss: 89.33165740966797 = 0.31554552912712097 + 10.0 * 8.901611328125
Epoch 1750, val loss: 0.40421077609062195
Epoch 1760, training loss: 89.36872100830078 = 0.3137628436088562 + 10.0 * 8.905495643615723
Epoch 1760, val loss: 0.4034218490123749
Epoch 1770, training loss: 89.37481689453125 = 0.3120150566101074 + 10.0 * 8.906280517578125
Epoch 1770, val loss: 0.40252551436424255
Epoch 1780, training loss: 89.34593200683594 = 0.31027737259864807 + 10.0 * 8.903565406799316
Epoch 1780, val loss: 0.4017307162284851
Epoch 1790, training loss: 89.40229034423828 = 0.3085748255252838 + 10.0 * 8.909371376037598
Epoch 1790, val loss: 0.4010540843009949
Epoch 1800, training loss: 89.43052673339844 = 0.3068726658821106 + 10.0 * 8.912365913391113
Epoch 1800, val loss: 0.4003295302391052
Epoch 1810, training loss: 89.39348602294922 = 0.3051307797431946 + 10.0 * 8.908835411071777
Epoch 1810, val loss: 0.39962026476860046
Epoch 1820, training loss: 89.42900848388672 = 0.30342432856559753 + 10.0 * 8.912558555603027
Epoch 1820, val loss: 0.39887744188308716
Epoch 1830, training loss: 89.47149658203125 = 0.3017664849758148 + 10.0 * 8.916973114013672
Epoch 1830, val loss: 0.39821386337280273
Epoch 1840, training loss: 89.0455093383789 = 0.30008718371391296 + 10.0 * 8.874542236328125
Epoch 1840, val loss: 0.39592817425727844
Epoch 1850, training loss: 88.05329895019531 = 0.3015662431716919 + 10.0 * 8.77517318725586
Epoch 1850, val loss: 0.400322288274765
Epoch 1860, training loss: 88.9344253540039 = 0.29898571968078613 + 10.0 * 8.863543510437012
Epoch 1860, val loss: 0.39882612228393555
Epoch 1870, training loss: 88.37698364257812 = 0.29670095443725586 + 10.0 * 8.808028221130371
Epoch 1870, val loss: 0.3960345983505249
Epoch 1880, training loss: 88.84195709228516 = 0.29544299840927124 + 10.0 * 8.85465145111084
Epoch 1880, val loss: 0.3976459503173828
Epoch 1890, training loss: 88.71510314941406 = 0.2936271131038666 + 10.0 * 8.842147827148438
Epoch 1890, val loss: 0.3959348499774933
Epoch 1900, training loss: 88.81826782226562 = 0.292077898979187 + 10.0 * 8.852619171142578
Epoch 1900, val loss: 0.39515042304992676
Epoch 1910, training loss: 88.92346954345703 = 0.29044243693351746 + 10.0 * 8.863302230834961
Epoch 1910, val loss: 0.39415550231933594
Epoch 1920, training loss: 88.99462890625 = 0.28878819942474365 + 10.0 * 8.870584487915039
Epoch 1920, val loss: 0.39398038387298584
Epoch 1930, training loss: 89.09607696533203 = 0.2871798276901245 + 10.0 * 8.880889892578125
Epoch 1930, val loss: 0.39328140020370483
Epoch 1940, training loss: 89.18899536132812 = 0.2855249345302582 + 10.0 * 8.890347480773926
Epoch 1940, val loss: 0.3927766680717468
Epoch 1950, training loss: 89.18789672851562 = 0.2839173376560211 + 10.0 * 8.890398025512695
Epoch 1950, val loss: 0.3922799527645111
Epoch 1960, training loss: 89.269287109375 = 0.2823367118835449 + 10.0 * 8.89869499206543
Epoch 1960, val loss: 0.39195436239242554
Epoch 1970, training loss: 89.30636596679688 = 0.28076258301734924 + 10.0 * 8.902560234069824
Epoch 1970, val loss: 0.39161229133605957
Epoch 1980, training loss: 89.34004974365234 = 0.2792125344276428 + 10.0 * 8.906084060668945
Epoch 1980, val loss: 0.3911136984825134
Epoch 1990, training loss: 89.34465789794922 = 0.2776866853237152 + 10.0 * 8.906697273254395
Epoch 1990, val loss: 0.3906775712966919
Epoch 2000, training loss: 89.3619613647461 = 0.2761319875717163 + 10.0 * 8.90858268737793
Epoch 2000, val loss: 0.3904241919517517
Epoch 2010, training loss: 89.4190444946289 = 0.2746143639087677 + 10.0 * 8.914443016052246
Epoch 2010, val loss: 0.3901534676551819
Epoch 2020, training loss: 89.38731384277344 = 0.27309098839759827 + 10.0 * 8.911421775817871
Epoch 2020, val loss: 0.38980984687805176
Epoch 2030, training loss: 89.37541198730469 = 0.2716338336467743 + 10.0 * 8.910377502441406
Epoch 2030, val loss: 0.38954755663871765
Epoch 2040, training loss: 89.45672607421875 = 0.27012982964515686 + 10.0 * 8.918660163879395
Epoch 2040, val loss: 0.3891546428203583
Epoch 2050, training loss: 89.48184967041016 = 0.2686545252799988 + 10.0 * 8.921319961547852
Epoch 2050, val loss: 0.3887340724468231
Epoch 2060, training loss: 89.50682067871094 = 0.2671746611595154 + 10.0 * 8.923964500427246
Epoch 2060, val loss: 0.3885430097579956
Epoch 2070, training loss: 89.50473022460938 = 0.2657134532928467 + 10.0 * 8.923901557922363
Epoch 2070, val loss: 0.3881806433200836
Epoch 2080, training loss: 89.46187591552734 = 0.2642635107040405 + 10.0 * 8.919760704040527
Epoch 2080, val loss: 0.388075590133667
Epoch 2090, training loss: 89.54065704345703 = 0.262808233499527 + 10.0 * 8.92778491973877
Epoch 2090, val loss: 0.38783589005470276
Epoch 2100, training loss: 89.5760726928711 = 0.261347234249115 + 10.0 * 8.931472778320312
Epoch 2100, val loss: 0.38762176036834717
Epoch 2110, training loss: 89.48725128173828 = 0.2598861753940582 + 10.0 * 8.922736167907715
Epoch 2110, val loss: 0.38745248317718506
Epoch 2120, training loss: 89.55791473388672 = 0.25848475098609924 + 10.0 * 8.929943084716797
Epoch 2120, val loss: 0.3870403468608856
Epoch 2130, training loss: 89.60411834716797 = 0.2570260167121887 + 10.0 * 8.934709548950195
Epoch 2130, val loss: 0.386771559715271
Epoch 2140, training loss: 89.64201354980469 = 0.25556591153144836 + 10.0 * 8.938644409179688
Epoch 2140, val loss: 0.3867020905017853
Epoch 2150, training loss: 89.62654876708984 = 0.254131555557251 + 10.0 * 8.937241554260254
Epoch 2150, val loss: 0.3864966928958893
Epoch 2160, training loss: 89.60323333740234 = 0.2527996599674225 + 10.0 * 8.935043334960938
Epoch 2160, val loss: 0.38614800572395325
Epoch 2170, training loss: 89.64351654052734 = 0.2513962984085083 + 10.0 * 8.93921184539795
Epoch 2170, val loss: 0.38653531670570374
Epoch 2180, training loss: 89.66185760498047 = 0.24993516504764557 + 10.0 * 8.941191673278809
Epoch 2180, val loss: 0.3862755000591278
Epoch 2190, training loss: 89.70868682861328 = 0.2484944611787796 + 10.0 * 8.946019172668457
Epoch 2190, val loss: 0.3861973285675049
Epoch 2200, training loss: 89.66461944580078 = 0.24707478284835815 + 10.0 * 8.941754341125488
Epoch 2200, val loss: 0.3859500586986542
Epoch 2210, training loss: 89.68511962890625 = 0.24568204581737518 + 10.0 * 8.943943977355957
Epoch 2210, val loss: 0.3859867751598358
Epoch 2220, training loss: 89.70755767822266 = 0.24425822496414185 + 10.0 * 8.946330070495605
Epoch 2220, val loss: 0.3860970735549927
Epoch 2230, training loss: 89.73615264892578 = 0.2428199052810669 + 10.0 * 8.949333190917969
Epoch 2230, val loss: 0.3859390914440155
Epoch 2240, training loss: 89.7149887084961 = 0.24140186607837677 + 10.0 * 8.947359085083008
Epoch 2240, val loss: 0.38582849502563477
Epoch 2250, training loss: 89.7579574584961 = 0.2399885356426239 + 10.0 * 8.951796531677246
Epoch 2250, val loss: 0.3857974112033844
Epoch 2260, training loss: 89.76461029052734 = 0.23856545984745026 + 10.0 * 8.952604293823242
Epoch 2260, val loss: 0.385888010263443
Epoch 2270, training loss: 89.77290344238281 = 0.2371569722890854 + 10.0 * 8.953574180603027
Epoch 2270, val loss: 0.3858143091201782
Epoch 2280, training loss: 89.81453704833984 = 0.236099973320961 + 10.0 * 8.957843780517578
Epoch 2280, val loss: 0.3854392468929291
Epoch 2290, training loss: 89.61386108398438 = 0.2346695512533188 + 10.0 * 8.937918663024902
Epoch 2290, val loss: 0.3862306773662567
Epoch 2300, training loss: 89.57467651367188 = 0.23356229066848755 + 10.0 * 8.934111595153809
Epoch 2300, val loss: 0.3858097195625305
Epoch 2310, training loss: 89.62928771972656 = 0.23224179446697235 + 10.0 * 8.939704895019531
Epoch 2310, val loss: 0.38610365986824036
Epoch 2320, training loss: 89.73980712890625 = 0.23086656630039215 + 10.0 * 8.950894355773926
Epoch 2320, val loss: 0.38638925552368164
Epoch 2330, training loss: 89.82036590576172 = 0.22941632568836212 + 10.0 * 8.959095001220703
Epoch 2330, val loss: 0.38632309436798096
Epoch 2340, training loss: 89.88600158691406 = 0.2279842495918274 + 10.0 * 8.965802192687988
Epoch 2340, val loss: 0.3863527178764343
Epoch 2350, training loss: 89.9095230102539 = 0.22658367455005646 + 10.0 * 8.968294143676758
Epoch 2350, val loss: 0.3863990306854248
Epoch 2360, training loss: 89.89332580566406 = 0.22517091035842896 + 10.0 * 8.966814994812012
Epoch 2360, val loss: 0.38651326298713684
Epoch 2370, training loss: 89.94497680664062 = 0.22378812730312347 + 10.0 * 8.972119331359863
Epoch 2370, val loss: 0.38667890429496765
Epoch 2380, training loss: 89.95930480957031 = 0.22239702939987183 + 10.0 * 8.9736909866333
Epoch 2380, val loss: 0.3868766725063324
Epoch 2390, training loss: 89.96027374267578 = 0.22100581228733063 + 10.0 * 8.973926544189453
Epoch 2390, val loss: 0.3870854079723358
Epoch 2400, training loss: 89.95851135253906 = 0.21963508427143097 + 10.0 * 8.97388744354248
Epoch 2400, val loss: 0.3874395191669464
Epoch 2410, training loss: 90.01968383789062 = 0.2182614505290985 + 10.0 * 8.980142593383789
Epoch 2410, val loss: 0.38756904006004333
Epoch 2420, training loss: 90.01931762695312 = 0.2168915718793869 + 10.0 * 8.980242729187012
Epoch 2420, val loss: 0.3878110349178314
Epoch 2430, training loss: 90.0059814453125 = 0.21552257239818573 + 10.0 * 8.979045867919922
Epoch 2430, val loss: 0.38807231187820435
Epoch 2440, training loss: 90.06460571289062 = 0.21417094767093658 + 10.0 * 8.9850435256958
Epoch 2440, val loss: 0.38829249143600464
Epoch 2450, training loss: 90.09163665771484 = 0.21279676258563995 + 10.0 * 8.987883567810059
Epoch 2450, val loss: 0.3886895477771759
Epoch 2460, training loss: 90.03810119628906 = 0.21144668757915497 + 10.0 * 8.982665061950684
Epoch 2460, val loss: 0.3888566792011261
Epoch 2470, training loss: 90.0909194946289 = 0.21009409427642822 + 10.0 * 8.988082885742188
Epoch 2470, val loss: 0.3893697261810303
Epoch 2480, training loss: 90.12627410888672 = 0.20874200761318207 + 10.0 * 8.991753578186035
Epoch 2480, val loss: 0.38964059948921204
Epoch 2490, training loss: 90.0968017578125 = 0.2074095904827118 + 10.0 * 8.98893928527832
Epoch 2490, val loss: 0.3900117576122284
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8578260869565217
0.8647395493733248
=== training gcn model ===
Epoch 0, training loss: 103.34138488769531 = 1.1023764610290527 + 10.0 * 10.22390079498291
Epoch 0, val loss: 1.1009478569030762
Epoch 10, training loss: 98.60162353515625 = 1.098390817642212 + 10.0 * 9.750323295593262
Epoch 10, val loss: 1.0970079898834229
Epoch 20, training loss: 96.51113891601562 = 1.0946820974349976 + 10.0 * 9.541646003723145
Epoch 20, val loss: 1.0933456420898438
Epoch 30, training loss: 95.02940368652344 = 1.0911071300506592 + 10.0 * 9.393829345703125
Epoch 30, val loss: 1.089824914932251
Epoch 40, training loss: 93.8880844116211 = 1.0877317190170288 + 10.0 * 9.280035018920898
Epoch 40, val loss: 1.0865154266357422
Epoch 50, training loss: 92.96115112304688 = 1.0845372676849365 + 10.0 * 9.187662124633789
Epoch 50, val loss: 1.0833933353424072
Epoch 60, training loss: 92.18680572509766 = 1.081504464149475 + 10.0 * 9.110529899597168
Epoch 60, val loss: 1.0804389715194702
Epoch 70, training loss: 91.52581787109375 = 1.0786170959472656 + 10.0 * 9.044720649719238
Epoch 70, val loss: 1.0776340961456299
Epoch 80, training loss: 90.97396087646484 = 1.0758711099624634 + 10.0 * 8.989809036254883
Epoch 80, val loss: 1.0749672651290894
Epoch 90, training loss: 90.52547454833984 = 1.0732234716415405 + 10.0 * 8.94522476196289
Epoch 90, val loss: 1.0724163055419922
Epoch 100, training loss: 90.14683532714844 = 1.0707236528396606 + 10.0 * 8.907610893249512
Epoch 100, val loss: 1.0700018405914307
Epoch 110, training loss: 89.80168151855469 = 1.0682768821716309 + 10.0 * 8.873340606689453
Epoch 110, val loss: 1.0676591396331787
Epoch 120, training loss: 89.51651763916016 = 1.0659253597259521 + 10.0 * 8.845059394836426
Epoch 120, val loss: 1.065401315689087
Epoch 130, training loss: 89.25947570800781 = 1.063639521598816 + 10.0 * 8.819583892822266
Epoch 130, val loss: 1.0632160902023315
Epoch 140, training loss: 89.0382080078125 = 1.0614049434661865 + 10.0 * 8.797680854797363
Epoch 140, val loss: 1.061086893081665
Epoch 150, training loss: 88.85267639160156 = 1.0592151880264282 + 10.0 * 8.779346466064453
Epoch 150, val loss: 1.0589934587478638
Epoch 160, training loss: 88.69261932373047 = 1.057063102722168 + 10.0 * 8.763555526733398
Epoch 160, val loss: 1.0569449663162231
Epoch 170, training loss: 88.52848815917969 = 1.0548882484436035 + 10.0 * 8.747360229492188
Epoch 170, val loss: 1.0548673868179321
Epoch 180, training loss: 88.40094757080078 = 1.0527877807617188 + 10.0 * 8.73481559753418
Epoch 180, val loss: 1.0528407096862793
Epoch 190, training loss: 88.31857299804688 = 1.0506519079208374 + 10.0 * 8.726792335510254
Epoch 190, val loss: 1.0507969856262207
Epoch 200, training loss: 88.18620300292969 = 1.048408031463623 + 10.0 * 8.71377944946289
Epoch 200, val loss: 1.0486468076705933
Epoch 210, training loss: 88.09584045410156 = 1.0461722612380981 + 10.0 * 8.70496654510498
Epoch 210, val loss: 1.0464917421340942
Epoch 220, training loss: 88.02437591552734 = 1.0439001321792603 + 10.0 * 8.698047637939453
Epoch 220, val loss: 1.0443071126937866
Epoch 230, training loss: 87.94950103759766 = 1.041454553604126 + 10.0 * 8.690804481506348
Epoch 230, val loss: 1.0419626235961914
Epoch 240, training loss: 87.93045043945312 = 1.0388814210891724 + 10.0 * 8.689157485961914
Epoch 240, val loss: 1.0394302606582642
Epoch 250, training loss: 87.85075378417969 = 1.0361745357513428 + 10.0 * 8.681458473205566
Epoch 250, val loss: 1.036866545677185
Epoch 260, training loss: 87.74089050292969 = 1.0331989526748657 + 10.0 * 8.670769691467285
Epoch 260, val loss: 1.0339382886886597
Epoch 270, training loss: 87.74995422363281 = 1.0301640033721924 + 10.0 * 8.671978950500488
Epoch 270, val loss: 1.030982494354248
Epoch 280, training loss: 87.6933364868164 = 1.0268323421478271 + 10.0 * 8.666650772094727
Epoch 280, val loss: 1.0277200937271118
Epoch 290, training loss: 87.63558959960938 = 1.0232075452804565 + 10.0 * 8.661237716674805
Epoch 290, val loss: 1.0242160558700562
Epoch 300, training loss: 87.63204193115234 = 1.0194034576416016 + 10.0 * 8.661264419555664
Epoch 300, val loss: 1.0204378366470337
Epoch 310, training loss: 87.63697052001953 = 1.0151326656341553 + 10.0 * 8.66218376159668
Epoch 310, val loss: 1.0163311958312988
Epoch 320, training loss: 87.62686920166016 = 1.010574460029602 + 10.0 * 8.661629676818848
Epoch 320, val loss: 1.0117243528366089
Epoch 330, training loss: 87.61170196533203 = 1.0056967735290527 + 10.0 * 8.660600662231445
Epoch 330, val loss: 1.0070503950119019
Epoch 340, training loss: 87.5468521118164 = 1.0004572868347168 + 10.0 * 8.65463924407959
Epoch 340, val loss: 1.0018868446350098
Epoch 350, training loss: 87.50836181640625 = 0.9946597218513489 + 10.0 * 8.65137004852295
Epoch 350, val loss: 0.9962477684020996
Epoch 360, training loss: 87.49358367919922 = 0.9885021448135376 + 10.0 * 8.650507926940918
Epoch 360, val loss: 0.9902116656303406
Epoch 370, training loss: 87.45838165283203 = 0.9819086790084839 + 10.0 * 8.6476469039917
Epoch 370, val loss: 0.9837426543235779
Epoch 380, training loss: 87.49837493896484 = 0.9750470519065857 + 10.0 * 8.65233325958252
Epoch 380, val loss: 0.9770222306251526
Epoch 390, training loss: 87.37704467773438 = 0.9669960141181946 + 10.0 * 8.64100456237793
Epoch 390, val loss: 0.9690865874290466
Epoch 400, training loss: 87.53203582763672 = 0.959659218788147 + 10.0 * 8.657238006591797
Epoch 400, val loss: 0.9619798064231873
Epoch 410, training loss: 87.43256378173828 = 0.9506272673606873 + 10.0 * 8.648193359375
Epoch 410, val loss: 0.9530695676803589
Epoch 420, training loss: 87.46562194824219 = 0.9418659210205078 + 10.0 * 8.652376174926758
Epoch 420, val loss: 0.9443991184234619
Epoch 430, training loss: 87.4565658569336 = 0.93205726146698 + 10.0 * 8.652450561523438
Epoch 430, val loss: 0.9349472522735596
Epoch 440, training loss: 87.4631576538086 = 0.9218323230743408 + 10.0 * 8.654132843017578
Epoch 440, val loss: 0.924854576587677
Epoch 450, training loss: 87.44059753417969 = 0.9109368324279785 + 10.0 * 8.652966499328613
Epoch 450, val loss: 0.9142361879348755
Epoch 460, training loss: 87.46612548828125 = 0.8995704650878906 + 10.0 * 8.656656265258789
Epoch 460, val loss: 0.9030547142028809
Epoch 470, training loss: 87.46683502197266 = 0.8874261975288391 + 10.0 * 8.657940864562988
Epoch 470, val loss: 0.8911774158477783
Epoch 480, training loss: 87.45402526855469 = 0.8747219443321228 + 10.0 * 8.657930374145508
Epoch 480, val loss: 0.8786411285400391
Epoch 490, training loss: 87.45597839355469 = 0.861289918422699 + 10.0 * 8.659468650817871
Epoch 490, val loss: 0.8655415773391724
Epoch 500, training loss: 87.43073272705078 = 0.84718257188797 + 10.0 * 8.658354759216309
Epoch 500, val loss: 0.8518174886703491
Epoch 510, training loss: 87.4410400390625 = 0.8329029083251953 + 10.0 * 8.66081428527832
Epoch 510, val loss: 0.8378607630729675
Epoch 520, training loss: 87.450927734375 = 0.8180557489395142 + 10.0 * 8.663287162780762
Epoch 520, val loss: 0.8234246373176575
Epoch 530, training loss: 87.43828582763672 = 0.8027079701423645 + 10.0 * 8.663557052612305
Epoch 530, val loss: 0.8085297346115112
Epoch 540, training loss: 87.39373016357422 = 0.7867454290390015 + 10.0 * 8.660698890686035
Epoch 540, val loss: 0.7931551933288574
Epoch 550, training loss: 87.4731674194336 = 0.77069491147995 + 10.0 * 8.670247077941895
Epoch 550, val loss: 0.7777132391929626
Epoch 560, training loss: 87.49174499511719 = 0.7546718716621399 + 10.0 * 8.673707962036133
Epoch 560, val loss: 0.762147068977356
Epoch 570, training loss: 87.39500427246094 = 0.7390068769454956 + 10.0 * 8.665599822998047
Epoch 570, val loss: 0.747309148311615
Epoch 580, training loss: 87.33119201660156 = 0.7223450541496277 + 10.0 * 8.660884857177734
Epoch 580, val loss: 0.7315772175788879
Epoch 590, training loss: 87.35958099365234 = 0.7066001296043396 + 10.0 * 8.665298461914062
Epoch 590, val loss: 0.7165738344192505
Epoch 600, training loss: 87.35501098632812 = 0.690946102142334 + 10.0 * 8.666406631469727
Epoch 600, val loss: 0.7017907500267029
Epoch 610, training loss: 87.37993621826172 = 0.6754524111747742 + 10.0 * 8.670448303222656
Epoch 610, val loss: 0.6872177124023438
Epoch 620, training loss: 87.39559936523438 = 0.6604183316230774 + 10.0 * 8.673518180847168
Epoch 620, val loss: 0.6731232404708862
Epoch 630, training loss: 87.39350891113281 = 0.645885705947876 + 10.0 * 8.674761772155762
Epoch 630, val loss: 0.6595463752746582
Epoch 640, training loss: 87.39256286621094 = 0.6319416165351868 + 10.0 * 8.676061630249023
Epoch 640, val loss: 0.6465383172035217
Epoch 650, training loss: 87.39867401123047 = 0.618664562702179 + 10.0 * 8.678000450134277
Epoch 650, val loss: 0.634236752986908
Epoch 660, training loss: 87.41946411132812 = 0.6060381531715393 + 10.0 * 8.681342124938965
Epoch 660, val loss: 0.622588574886322
Epoch 670, training loss: 87.3838119506836 = 0.5939252972602844 + 10.0 * 8.678988456726074
Epoch 670, val loss: 0.6114423274993896
Epoch 680, training loss: 87.37062072753906 = 0.5827431082725525 + 10.0 * 8.678788185119629
Epoch 680, val loss: 0.6011211276054382
Epoch 690, training loss: 87.41954803466797 = 0.5722560882568359 + 10.0 * 8.684728622436523
Epoch 690, val loss: 0.5914609432220459
Epoch 700, training loss: 87.44821166992188 = 0.5621911883354187 + 10.0 * 8.68860149383545
Epoch 700, val loss: 0.5823527574539185
Epoch 710, training loss: 87.44344329833984 = 0.5527940988540649 + 10.0 * 8.689064979553223
Epoch 710, val loss: 0.5737997889518738
Epoch 720, training loss: 87.44477081298828 = 0.543868362903595 + 10.0 * 8.69009017944336
Epoch 720, val loss: 0.5657511949539185
Epoch 730, training loss: 87.48883056640625 = 0.5357143878936768 + 10.0 * 8.695311546325684
Epoch 730, val loss: 0.5583413243293762
Epoch 740, training loss: 87.51944732666016 = 0.5279749631881714 + 10.0 * 8.69914722442627
Epoch 740, val loss: 0.5514047741889954
Epoch 750, training loss: 87.49701690673828 = 0.5206392407417297 + 10.0 * 8.697637557983398
Epoch 750, val loss: 0.5448824167251587
Epoch 760, training loss: 87.50240325927734 = 0.5138040781021118 + 10.0 * 8.698860168457031
Epoch 760, val loss: 0.5388047099113464
Epoch 770, training loss: 87.53810119628906 = 0.5075192451477051 + 10.0 * 8.703058242797852
Epoch 770, val loss: 0.5332713723182678
Epoch 780, training loss: 87.50727081298828 = 0.5013637542724609 + 10.0 * 8.700590133666992
Epoch 780, val loss: 0.5278786420822144
Epoch 790, training loss: 87.57563018798828 = 0.49570101499557495 + 10.0 * 8.707992553710938
Epoch 790, val loss: 0.5228312611579895
Epoch 800, training loss: 87.5967025756836 = 0.49031540751457214 + 10.0 * 8.710638046264648
Epoch 800, val loss: 0.5181838870048523
Epoch 810, training loss: 87.60982513427734 = 0.48520153760910034 + 10.0 * 8.712462425231934
Epoch 810, val loss: 0.5138818025588989
Epoch 820, training loss: 87.58755493164062 = 0.4805501699447632 + 10.0 * 8.710700035095215
Epoch 820, val loss: 0.5098513960838318
Epoch 830, training loss: 87.62999725341797 = 0.4760948121547699 + 10.0 * 8.7153902053833
Epoch 830, val loss: 0.5061152577400208
Epoch 840, training loss: 87.62393188476562 = 0.47178414463996887 + 10.0 * 8.715214729309082
Epoch 840, val loss: 0.5026155710220337
Epoch 850, training loss: 87.64241027832031 = 0.46790799498558044 + 10.0 * 8.717450141906738
Epoch 850, val loss: 0.4994235336780548
Epoch 860, training loss: 87.67736053466797 = 0.46414196491241455 + 10.0 * 8.721322059631348
Epoch 860, val loss: 0.49632155895233154
Epoch 870, training loss: 87.70652770996094 = 0.4606003165245056 + 10.0 * 8.724592208862305
Epoch 870, val loss: 0.4934842586517334
Epoch 880, training loss: 87.60344696044922 = 0.45713043212890625 + 10.0 * 8.714632034301758
Epoch 880, val loss: 0.4908009171485901
Epoch 890, training loss: 87.61674499511719 = 0.4539061486721039 + 10.0 * 8.716283798217773
Epoch 890, val loss: 0.4880725145339966
Epoch 900, training loss: 87.62139892578125 = 0.45077186822891235 + 10.0 * 8.717061996459961
Epoch 900, val loss: 0.4855041205883026
Epoch 910, training loss: 87.74124145507812 = 0.44827550649642944 + 10.0 * 8.729296684265137
Epoch 910, val loss: 0.48387250304222107
Epoch 920, training loss: 87.51043701171875 = 0.44530242681503296 + 10.0 * 8.706513404846191
Epoch 920, val loss: 0.48113521933555603
Epoch 930, training loss: 87.47772979736328 = 0.4424956440925598 + 10.0 * 8.703523635864258
Epoch 930, val loss: 0.47946688532829285
Epoch 940, training loss: 87.60112762451172 = 0.4402269124984741 + 10.0 * 8.716090202331543
Epoch 940, val loss: 0.4773349165916443
Epoch 950, training loss: 87.61691284179688 = 0.4375362694263458 + 10.0 * 8.717937469482422
Epoch 950, val loss: 0.47596970200538635
Epoch 960, training loss: 87.58467864990234 = 0.43520858883857727 + 10.0 * 8.714946746826172
Epoch 960, val loss: 0.4738411009311676
Epoch 970, training loss: 87.68125915527344 = 0.4327865242958069 + 10.0 * 8.724847793579102
Epoch 970, val loss: 0.4721046984195709
Epoch 980, training loss: 87.68553924560547 = 0.43037548661231995 + 10.0 * 8.725516319274902
Epoch 980, val loss: 0.4703449308872223
Epoch 990, training loss: 87.7365493774414 = 0.4281516373157501 + 10.0 * 8.730839729309082
Epoch 990, val loss: 0.4688563048839569
Epoch 1000, training loss: 87.74230194091797 = 0.4259439706802368 + 10.0 * 8.731636047363281
Epoch 1000, val loss: 0.4672245383262634
Epoch 1010, training loss: 87.77664184570312 = 0.4238055944442749 + 10.0 * 8.735283851623535
Epoch 1010, val loss: 0.4657074213027954
Epoch 1020, training loss: 87.81159973144531 = 0.4216761887073517 + 10.0 * 8.738992691040039
Epoch 1020, val loss: 0.46425890922546387
Epoch 1030, training loss: 87.81660461425781 = 0.41959628462791443 + 10.0 * 8.739701271057129
Epoch 1030, val loss: 0.4629017412662506
Epoch 1040, training loss: 87.81189727783203 = 0.41759827733039856 + 10.0 * 8.73943042755127
Epoch 1040, val loss: 0.4615595042705536
Epoch 1050, training loss: 87.85430908203125 = 0.4156600534915924 + 10.0 * 8.743865013122559
Epoch 1050, val loss: 0.4601428806781769
Epoch 1060, training loss: 87.91129302978516 = 0.41373252868652344 + 10.0 * 8.749755859375
Epoch 1060, val loss: 0.458813339471817
Epoch 1070, training loss: 87.80538940429688 = 0.4118158221244812 + 10.0 * 8.739356994628906
Epoch 1070, val loss: 0.4576717019081116
Epoch 1080, training loss: 87.92382049560547 = 0.4100017845630646 + 10.0 * 8.751381874084473
Epoch 1080, val loss: 0.45643889904022217
Epoch 1090, training loss: 87.81272888183594 = 0.408184289932251 + 10.0 * 8.740453720092773
Epoch 1090, val loss: 0.45544499158859253
Epoch 1100, training loss: 87.80470275878906 = 0.40641599893569946 + 10.0 * 8.739828109741211
Epoch 1100, val loss: 0.4539550542831421
Epoch 1110, training loss: 87.89659881591797 = 0.40461838245391846 + 10.0 * 8.749197959899902
Epoch 1110, val loss: 0.4531295597553253
Epoch 1120, training loss: 87.95437622070312 = 0.4028368592262268 + 10.0 * 8.75515365600586
Epoch 1120, val loss: 0.45197388529777527
Epoch 1130, training loss: 88.00677490234375 = 0.4010753929615021 + 10.0 * 8.76056957244873
Epoch 1130, val loss: 0.4506884813308716
Epoch 1140, training loss: 87.98757934570312 = 0.39932212233543396 + 10.0 * 8.758825302124023
Epoch 1140, val loss: 0.44967296719551086
Epoch 1150, training loss: 87.99921417236328 = 0.39760082960128784 + 10.0 * 8.760161399841309
Epoch 1150, val loss: 0.448457807302475
Epoch 1160, training loss: 88.01935577392578 = 0.39591920375823975 + 10.0 * 8.762343406677246
Epoch 1160, val loss: 0.4474910795688629
Epoch 1170, training loss: 88.0503158569336 = 0.3942062556743622 + 10.0 * 8.76561164855957
Epoch 1170, val loss: 0.44655555486679077
Epoch 1180, training loss: 88.06112670898438 = 0.3925068974494934 + 10.0 * 8.766861915588379
Epoch 1180, val loss: 0.44542640447616577
Epoch 1190, training loss: 88.10706329345703 = 0.39085981249809265 + 10.0 * 8.77161979675293
Epoch 1190, val loss: 0.4444478750228882
Epoch 1200, training loss: 88.15157318115234 = 0.3891897201538086 + 10.0 * 8.776238441467285
Epoch 1200, val loss: 0.44342339038848877
Epoch 1210, training loss: 88.1434326171875 = 0.38753625750541687 + 10.0 * 8.775589942932129
Epoch 1210, val loss: 0.4425197243690491
Epoch 1220, training loss: 88.12836456298828 = 0.38589951395988464 + 10.0 * 8.774246215820312
Epoch 1220, val loss: 0.44155633449554443
Epoch 1230, training loss: 88.17306518554688 = 0.38429540395736694 + 10.0 * 8.778877258300781
Epoch 1230, val loss: 0.44051533937454224
Epoch 1240, training loss: 88.18205261230469 = 0.3826453685760498 + 10.0 * 8.779940605163574
Epoch 1240, val loss: 0.4397985637187958
Epoch 1250, training loss: 88.1800308227539 = 0.381045401096344 + 10.0 * 8.779898643493652
Epoch 1250, val loss: 0.43888747692108154
Epoch 1260, training loss: 88.22314453125 = 0.37948477268218994 + 10.0 * 8.7843656539917
Epoch 1260, val loss: 0.43788400292396545
Epoch 1270, training loss: 88.27499389648438 = 0.37788933515548706 + 10.0 * 8.78971004486084
Epoch 1270, val loss: 0.436996191740036
Epoch 1280, training loss: 88.25068664550781 = 0.3762846887111664 + 10.0 * 8.787440299987793
Epoch 1280, val loss: 0.43606099486351013
Epoch 1290, training loss: 88.2757568359375 = 0.3747066259384155 + 10.0 * 8.790104866027832
Epoch 1290, val loss: 0.4353126883506775
Epoch 1300, training loss: 88.31513214111328 = 0.37314921617507935 + 10.0 * 8.794198989868164
Epoch 1300, val loss: 0.434487521648407
Epoch 1310, training loss: 88.30805969238281 = 0.371581107378006 + 10.0 * 8.793647766113281
Epoch 1310, val loss: 0.4336166977882385
Epoch 1320, training loss: 88.29843139648438 = 0.37005701661109924 + 10.0 * 8.792837142944336
Epoch 1320, val loss: 0.4327484369277954
Epoch 1330, training loss: 88.37077331542969 = 0.3685401678085327 + 10.0 * 8.800223350524902
Epoch 1330, val loss: 0.4321560561656952
Epoch 1340, training loss: 88.40900421142578 = 0.3670058250427246 + 10.0 * 8.804200172424316
Epoch 1340, val loss: 0.43140092492103577
Epoch 1350, training loss: 88.31246948242188 = 0.3654903471469879 + 10.0 * 8.794697761535645
Epoch 1350, val loss: 0.4307810068130493
Epoch 1360, training loss: 88.36357116699219 = 0.3640851378440857 + 10.0 * 8.799948692321777
Epoch 1360, val loss: 0.42990753054618835
Epoch 1370, training loss: 88.39203643798828 = 0.36257404088974 + 10.0 * 8.802946090698242
Epoch 1370, val loss: 0.42923128604888916
Epoch 1380, training loss: 88.42680358886719 = 0.3610961139202118 + 10.0 * 8.806570053100586
Epoch 1380, val loss: 0.4284975230693817
Epoch 1390, training loss: 88.4439926147461 = 0.35959622263908386 + 10.0 * 8.808439254760742
Epoch 1390, val loss: 0.4277927577495575
Epoch 1400, training loss: 88.41333770751953 = 0.3580993413925171 + 10.0 * 8.805523872375488
Epoch 1400, val loss: 0.42707887291908264
Epoch 1410, training loss: 88.46605682373047 = 0.3566383421421051 + 10.0 * 8.810941696166992
Epoch 1410, val loss: 0.42644089460372925
Epoch 1420, training loss: 88.46363067626953 = 0.3551482558250427 + 10.0 * 8.810848236083984
Epoch 1420, val loss: 0.4258899688720703
Epoch 1430, training loss: 88.50484466552734 = 0.35368984937667847 + 10.0 * 8.815114974975586
Epoch 1430, val loss: 0.4252300262451172
Epoch 1440, training loss: 88.44302368164062 = 0.3521665334701538 + 10.0 * 8.809085845947266
Epoch 1440, val loss: 0.4243520498275757
Epoch 1450, training loss: 88.48346710205078 = 0.3507603406906128 + 10.0 * 8.813270568847656
Epoch 1450, val loss: 0.42373308539390564
Epoch 1460, training loss: 88.52699279785156 = 0.3493114113807678 + 10.0 * 8.817768096923828
Epoch 1460, val loss: 0.42318904399871826
Epoch 1470, training loss: 88.46061706542969 = 0.347832053899765 + 10.0 * 8.811278343200684
Epoch 1470, val loss: 0.422504723072052
Epoch 1480, training loss: 88.52743530273438 = 0.3465161919593811 + 10.0 * 8.81809139251709
Epoch 1480, val loss: 0.42192932963371277
Epoch 1490, training loss: 88.4748306274414 = 0.3450372517108917 + 10.0 * 8.812978744506836
Epoch 1490, val loss: 0.4210045039653778
Epoch 1500, training loss: 88.53470611572266 = 0.343627005815506 + 10.0 * 8.819108009338379
Epoch 1500, val loss: 0.42078232765197754
Epoch 1510, training loss: 88.55801391601562 = 0.3421883285045624 + 10.0 * 8.821582794189453
Epoch 1510, val loss: 0.41998720169067383
Epoch 1520, training loss: 88.58165740966797 = 0.34077998995780945 + 10.0 * 8.824087142944336
Epoch 1520, val loss: 0.4195147156715393
Epoch 1530, training loss: 88.6049575805664 = 0.33932945132255554 + 10.0 * 8.826562881469727
Epoch 1530, val loss: 0.4188579022884369
Epoch 1540, training loss: 88.63811492919922 = 0.3379167914390564 + 10.0 * 8.8300199508667
Epoch 1540, val loss: 0.4182928502559662
Epoch 1550, training loss: 88.65074920654297 = 0.3364928960800171 + 10.0 * 8.831425666809082
Epoch 1550, val loss: 0.4177107810974121
Epoch 1560, training loss: 88.66159057617188 = 0.3350708782672882 + 10.0 * 8.83265209197998
Epoch 1560, val loss: 0.41716882586479187
Epoch 1570, training loss: 88.65010070800781 = 0.3336685597896576 + 10.0 * 8.831643104553223
Epoch 1570, val loss: 0.4166826903820038
Epoch 1580, training loss: 88.69901275634766 = 0.3322659432888031 + 10.0 * 8.836674690246582
Epoch 1580, val loss: 0.41605842113494873
Epoch 1590, training loss: 88.53853607177734 = 0.33079880475997925 + 10.0 * 8.82077407836914
Epoch 1590, val loss: 0.4156308174133301
Epoch 1600, training loss: 88.58627319335938 = 0.32959476113319397 + 10.0 * 8.825667381286621
Epoch 1600, val loss: 0.41531991958618164
Epoch 1610, training loss: 88.54911041259766 = 0.3282507359981537 + 10.0 * 8.822086334228516
Epoch 1610, val loss: 0.4146120250225067
Epoch 1620, training loss: 88.58890533447266 = 0.32695144414901733 + 10.0 * 8.82619571685791
Epoch 1620, val loss: 0.4139791429042816
Epoch 1630, training loss: 88.65885162353516 = 0.32553914189338684 + 10.0 * 8.833331108093262
Epoch 1630, val loss: 0.4136557877063751
Epoch 1640, training loss: 88.7225570678711 = 0.3241146504878998 + 10.0 * 8.839844703674316
Epoch 1640, val loss: 0.41308993101119995
Epoch 1650, training loss: 88.76275634765625 = 0.32269322872161865 + 10.0 * 8.844006538391113
Epoch 1650, val loss: 0.4126065671443939
Epoch 1660, training loss: 88.68708801269531 = 0.3213040828704834 + 10.0 * 8.836578369140625
Epoch 1660, val loss: 0.4121360182762146
Epoch 1670, training loss: 88.73910522460938 = 0.31994736194610596 + 10.0 * 8.84191608428955
Epoch 1670, val loss: 0.41166621446609497
Epoch 1680, training loss: 88.80387878417969 = 0.3185482323169708 + 10.0 * 8.848532676696777
Epoch 1680, val loss: 0.411198228597641
Epoch 1690, training loss: 88.81258392333984 = 0.3171287477016449 + 10.0 * 8.8495454788208
Epoch 1690, val loss: 0.41066882014274597
Epoch 1700, training loss: 88.78565979003906 = 0.3157331645488739 + 10.0 * 8.846992492675781
Epoch 1700, val loss: 0.410197377204895
Epoch 1710, training loss: 88.7943115234375 = 0.31431472301483154 + 10.0 * 8.847999572753906
Epoch 1710, val loss: 0.40972062945365906
Epoch 1720, training loss: 88.8282241821289 = 0.31291884183883667 + 10.0 * 8.851530075073242
Epoch 1720, val loss: 0.40936005115509033
Epoch 1730, training loss: 88.85423278808594 = 0.3115100860595703 + 10.0 * 8.854272842407227
Epoch 1730, val loss: 0.40901148319244385
Epoch 1740, training loss: 88.87824249267578 = 0.3101123571395874 + 10.0 * 8.856813430786133
Epoch 1740, val loss: 0.4085559546947479
Epoch 1750, training loss: 88.88037109375 = 0.3087054491043091 + 10.0 * 8.857166290283203
Epoch 1750, val loss: 0.4081324636936188
Epoch 1760, training loss: 88.8536148071289 = 0.30731701850891113 + 10.0 * 8.854629516601562
Epoch 1760, val loss: 0.40777847170829773
Epoch 1770, training loss: 88.8779296875 = 0.30598634481430054 + 10.0 * 8.857194900512695
Epoch 1770, val loss: 0.40731197595596313
Epoch 1780, training loss: 88.9229736328125 = 0.3045808672904968 + 10.0 * 8.861839294433594
Epoch 1780, val loss: 0.4070577323436737
Epoch 1790, training loss: 88.92784118652344 = 0.3031834661960602 + 10.0 * 8.862465858459473
Epoch 1790, val loss: 0.406559020280838
Epoch 1800, training loss: 88.8936538696289 = 0.3018079996109009 + 10.0 * 8.859184265136719
Epoch 1800, val loss: 0.40613114833831787
Epoch 1810, training loss: 88.91426086425781 = 0.3004446029663086 + 10.0 * 8.861381530761719
Epoch 1810, val loss: 0.40598875284194946
Epoch 1820, training loss: 88.9503173828125 = 0.29906049370765686 + 10.0 * 8.86512565612793
Epoch 1820, val loss: 0.40562358498573303
Epoch 1830, training loss: 88.97901153564453 = 0.29766592383384705 + 10.0 * 8.868134498596191
Epoch 1830, val loss: 0.405201256275177
Epoch 1840, training loss: 88.95382690429688 = 0.2962496280670166 + 10.0 * 8.865757942199707
Epoch 1840, val loss: 0.4048461318016052
Epoch 1850, training loss: 88.95875549316406 = 0.2948547601699829 + 10.0 * 8.866390228271484
Epoch 1850, val loss: 0.4045993983745575
Epoch 1860, training loss: 88.93637084960938 = 0.29351985454559326 + 10.0 * 8.864285469055176
Epoch 1860, val loss: 0.40422260761260986
Epoch 1870, training loss: 88.95721435546875 = 0.2921665608882904 + 10.0 * 8.866504669189453
Epoch 1870, val loss: 0.40410053730010986
Epoch 1880, training loss: 89.00112915039062 = 0.2908058762550354 + 10.0 * 8.871031761169434
Epoch 1880, val loss: 0.4038407802581787
Epoch 1890, training loss: 89.01779174804688 = 0.2894006669521332 + 10.0 * 8.872838973999023
Epoch 1890, val loss: 0.403480589389801
Epoch 1900, training loss: 89.02877807617188 = 0.28801003098487854 + 10.0 * 8.874076843261719
Epoch 1900, val loss: 0.40320858359336853
Epoch 1910, training loss: 89.008056640625 = 0.2866522967815399 + 10.0 * 8.872140884399414
Epoch 1910, val loss: 0.40297576785087585
Epoch 1920, training loss: 88.84307861328125 = 0.28551092743873596 + 10.0 * 8.855756759643555
Epoch 1920, val loss: 0.402131050825119
Epoch 1930, training loss: 88.72412109375 = 0.284390926361084 + 10.0 * 8.843973159790039
Epoch 1930, val loss: 0.40210533142089844
Epoch 1940, training loss: 88.77926635742188 = 0.28301671147346497 + 10.0 * 8.849624633789062
Epoch 1940, val loss: 0.40251609683036804
Epoch 1950, training loss: 88.87175750732422 = 0.2816358506679535 + 10.0 * 8.85901165008545
Epoch 1950, val loss: 0.4020775854587555
Epoch 1960, training loss: 88.95132446289062 = 0.28025081753730774 + 10.0 * 8.867107391357422
Epoch 1960, val loss: 0.4017486274242401
Epoch 1970, training loss: 88.99496459960938 = 0.27882257103919983 + 10.0 * 8.871614456176758
Epoch 1970, val loss: 0.40163758397102356
Epoch 1980, training loss: 89.00785827636719 = 0.2773914635181427 + 10.0 * 8.873046875
Epoch 1980, val loss: 0.40142643451690674
Epoch 1990, training loss: 88.9920425415039 = 0.27598097920417786 + 10.0 * 8.871606826782227
Epoch 1990, val loss: 0.4012712240219116
Epoch 2000, training loss: 89.02118682861328 = 0.2745920419692993 + 10.0 * 8.874659538269043
Epoch 2000, val loss: 0.4011395275592804
Epoch 2010, training loss: 89.04540252685547 = 0.27320656180381775 + 10.0 * 8.877219200134277
Epoch 2010, val loss: 0.4010085463523865
Epoch 2020, training loss: 89.07029724121094 = 0.27181345224380493 + 10.0 * 8.87984848022461
Epoch 2020, val loss: 0.40092769265174866
Epoch 2030, training loss: 89.07329559326172 = 0.2704344689846039 + 10.0 * 8.88028621673584
Epoch 2030, val loss: 0.4008907079696655
Epoch 2040, training loss: 89.08949279785156 = 0.2690545916557312 + 10.0 * 8.882043838500977
Epoch 2040, val loss: 0.40067699551582336
Epoch 2050, training loss: 89.09257507324219 = 0.26768356561660767 + 10.0 * 8.882489204406738
Epoch 2050, val loss: 0.40054377913475037
Epoch 2060, training loss: 89.10574340820312 = 0.2662992775440216 + 10.0 * 8.883944511413574
Epoch 2060, val loss: 0.40053483843803406
Epoch 2070, training loss: 89.1148452758789 = 0.26493266224861145 + 10.0 * 8.884991645812988
Epoch 2070, val loss: 0.40049272775650024
Epoch 2080, training loss: 89.1702651977539 = 0.2636812627315521 + 10.0 * 8.890658378601074
Epoch 2080, val loss: 0.4008272588253021
Epoch 2090, training loss: 88.87493133544922 = 0.26301974058151245 + 10.0 * 8.861190795898438
Epoch 2090, val loss: 0.400672048330307
Epoch 2100, training loss: 88.52904510498047 = 0.2616174817085266 + 10.0 * 8.826742172241211
Epoch 2100, val loss: 0.4004047214984894
Epoch 2110, training loss: 88.51009368896484 = 0.260438472032547 + 10.0 * 8.824965476989746
Epoch 2110, val loss: 0.39945077896118164
Epoch 2120, training loss: 88.7649154663086 = 0.2593134641647339 + 10.0 * 8.850560188293457
Epoch 2120, val loss: 0.40166258811950684
Epoch 2130, training loss: 88.78536224365234 = 0.25772595405578613 + 10.0 * 8.852763175964355
Epoch 2130, val loss: 0.40023380517959595
Epoch 2140, training loss: 88.87628173828125 = 0.2562277019023895 + 10.0 * 8.862005233764648
Epoch 2140, val loss: 0.3998992145061493
Epoch 2150, training loss: 88.9251937866211 = 0.2548423409461975 + 10.0 * 8.867034912109375
Epoch 2150, val loss: 0.40032339096069336
Epoch 2160, training loss: 88.97838592529297 = 0.2534404397010803 + 10.0 * 8.8724946975708
Epoch 2160, val loss: 0.4002144932746887
Epoch 2170, training loss: 89.00172424316406 = 0.2520398795604706 + 10.0 * 8.874968528747559
Epoch 2170, val loss: 0.4003760814666748
Epoch 2180, training loss: 89.03828430175781 = 0.2506526708602905 + 10.0 * 8.878763198852539
Epoch 2180, val loss: 0.4002918303012848
Epoch 2190, training loss: 89.03456115722656 = 0.24927379190921783 + 10.0 * 8.878528594970703
Epoch 2190, val loss: 0.4004059433937073
Epoch 2200, training loss: 89.07917785644531 = 0.24790872633457184 + 10.0 * 8.883127212524414
Epoch 2200, val loss: 0.40032973885536194
Epoch 2210, training loss: 89.04505920410156 = 0.24654841423034668 + 10.0 * 8.879850387573242
Epoch 2210, val loss: 0.4004586637020111
Epoch 2220, training loss: 89.08567810058594 = 0.24521088600158691 + 10.0 * 8.88404655456543
Epoch 2220, val loss: 0.40059247612953186
Epoch 2230, training loss: 89.08373260498047 = 0.24386762082576752 + 10.0 * 8.883986473083496
Epoch 2230, val loss: 0.40066128969192505
Epoch 2240, training loss: 89.07167053222656 = 0.2425352782011032 + 10.0 * 8.882913589477539
Epoch 2240, val loss: 0.40074989199638367
Epoch 2250, training loss: 89.09844970703125 = 0.24119246006011963 + 10.0 * 8.885725021362305
Epoch 2250, val loss: 0.4007112979888916
Epoch 2260, training loss: 89.10063934326172 = 0.2398434281349182 + 10.0 * 8.886079788208008
Epoch 2260, val loss: 0.40092530846595764
Epoch 2270, training loss: 89.08525085449219 = 0.23851650953292847 + 10.0 * 8.884673118591309
Epoch 2270, val loss: 0.40099868178367615
Epoch 2280, training loss: 89.11824035644531 = 0.23719659447669983 + 10.0 * 8.888104438781738
Epoch 2280, val loss: 0.40123414993286133
Epoch 2290, training loss: 89.15264892578125 = 0.235862135887146 + 10.0 * 8.891678810119629
Epoch 2290, val loss: 0.40127769112586975
Epoch 2300, training loss: 89.15238952636719 = 0.23451361060142517 + 10.0 * 8.8917875289917
Epoch 2300, val loss: 0.40137049555778503
Epoch 2310, training loss: 89.16397094726562 = 0.23317095637321472 + 10.0 * 8.89307975769043
Epoch 2310, val loss: 0.40157124400138855
Epoch 2320, training loss: 89.20977783203125 = 0.23182980716228485 + 10.0 * 8.897794723510742
Epoch 2320, val loss: 0.4016665518283844
Epoch 2330, training loss: 89.21932983398438 = 0.23048081994056702 + 10.0 * 8.898884773254395
Epoch 2330, val loss: 0.4017323851585388
Epoch 2340, training loss: 89.22561645507812 = 0.2291383147239685 + 10.0 * 8.89964771270752
Epoch 2340, val loss: 0.40195387601852417
Epoch 2350, training loss: 89.21099853515625 = 0.22780856490135193 + 10.0 * 8.898319244384766
Epoch 2350, val loss: 0.4022040367126465
Epoch 2360, training loss: 89.2337417602539 = 0.22647668421268463 + 10.0 * 8.900726318359375
Epoch 2360, val loss: 0.4022738039493561
Epoch 2370, training loss: 89.23808288574219 = 0.2251404970884323 + 10.0 * 8.901293754577637
Epoch 2370, val loss: 0.4023548662662506
Epoch 2380, training loss: 89.25042724609375 = 0.22382360696792603 + 10.0 * 8.902660369873047
Epoch 2380, val loss: 0.40266409516334534
Epoch 2390, training loss: 89.25452423095703 = 0.2225082963705063 + 10.0 * 8.90320110321045
Epoch 2390, val loss: 0.40278589725494385
Epoch 2400, training loss: 89.27711486816406 = 0.22119438648223877 + 10.0 * 8.90559196472168
Epoch 2400, val loss: 0.4031272530555725
Epoch 2410, training loss: 89.24531555175781 = 0.21988925337791443 + 10.0 * 8.902543067932129
Epoch 2410, val loss: 0.4034135043621063
Epoch 2420, training loss: 89.26690673828125 = 0.2186080515384674 + 10.0 * 8.904829978942871
Epoch 2420, val loss: 0.4034319519996643
Epoch 2430, training loss: 89.28775024414062 = 0.21730780601501465 + 10.0 * 8.907044410705566
Epoch 2430, val loss: 0.40362998843193054
Epoch 2440, training loss: 89.33743286132812 = 0.21600569784641266 + 10.0 * 8.912142753601074
Epoch 2440, val loss: 0.40399909019470215
Epoch 2450, training loss: 89.33779907226562 = 0.21469171345233917 + 10.0 * 8.912310600280762
Epoch 2450, val loss: 0.4041873812675476
Epoch 2460, training loss: 89.29380798339844 = 0.21341527998447418 + 10.0 * 8.908039093017578
Epoch 2460, val loss: 0.4044956862926483
Epoch 2470, training loss: 89.3006820678711 = 0.21217761933803558 + 10.0 * 8.908849716186523
Epoch 2470, val loss: 0.40487006306648254
Epoch 2480, training loss: 89.3249740600586 = 0.21091443300247192 + 10.0 * 8.911405563354492
Epoch 2480, val loss: 0.4051738679409027
Epoch 2490, training loss: 89.35099029541016 = 0.20961612462997437 + 10.0 * 8.91413688659668
Epoch 2490, val loss: 0.40534502267837524
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8485507246376811
0.8648844454104181
The final CL Acc:0.85357, 0.00383, The final GNN Acc:0.86440, 0.00058
Begin epxeriment: noisy_level: 0.2 cont_weight: 10 epoch: der1:0.3 der2:0.3 dfr1:0.4 dfr2:0.3
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=2500, cl_num_layers=2, cl_num_proj_hidden=64, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=10.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.3, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=32, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, noisy_level=0.2, num_hidden=64, num_proj_hidden=64, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=40, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 88648])
add edge: torch.Size([2, 106480])
remove edge: torch.Size([2, 71116])
updated graph: torch.Size([2, 88948])
=== Noisy graph ===
/home/project-robust-contrastive/Robust-Contrastive-Learning/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 103.62728118896484 = 1.1029155254364014 + 10.0 * 10.252436637878418
Epoch 0, val loss: 1.1025689840316772
Epoch 10, training loss: 99.43518829345703 = 1.099227786064148 + 10.0 * 9.833596229553223
Epoch 10, val loss: 1.0989537239074707
Epoch 20, training loss: 97.63420867919922 = 1.0958201885223389 + 10.0 * 9.653839111328125
Epoch 20, val loss: 1.0955653190612793
Epoch 30, training loss: 96.23625183105469 = 1.0925090312957764 + 10.0 * 9.514374732971191
Epoch 30, val loss: 1.0922882556915283
Epoch 40, training loss: 95.13409423828125 = 1.0893577337265015 + 10.0 * 9.404474258422852
Epoch 40, val loss: 1.0891746282577515
Epoch 50, training loss: 94.25135803222656 = 1.0863423347473145 + 10.0 * 9.31650161743164
Epoch 50, val loss: 1.0861924886703491
Epoch 60, training loss: 93.50364685058594 = 1.0834578275680542 + 10.0 * 9.242018699645996
Epoch 60, val loss: 1.083345651626587
Epoch 70, training loss: 92.85752868652344 = 1.0807032585144043 + 10.0 * 9.177682876586914
Epoch 70, val loss: 1.080627679824829
Epoch 80, training loss: 92.29502868652344 = 1.0780665874481201 + 10.0 * 9.121696472167969
Epoch 80, val loss: 1.0780296325683594
Epoch 90, training loss: 91.79917907714844 = 1.0755298137664795 + 10.0 * 9.072364807128906
Epoch 90, val loss: 1.0755304098129272
Epoch 100, training loss: 91.38372802734375 = 1.0731040239334106 + 10.0 * 9.031062126159668
Epoch 100, val loss: 1.0731425285339355
Epoch 110, training loss: 91.04131317138672 = 1.0708059072494507 + 10.0 * 8.997050285339355
Epoch 110, val loss: 1.07088303565979
Epoch 120, training loss: 90.73448181152344 = 1.0685901641845703 + 10.0 * 8.966588973999023
Epoch 120, val loss: 1.0687154531478882
Epoch 130, training loss: 90.46280670166016 = 1.0664869546890259 + 10.0 * 8.939631462097168
Epoch 130, val loss: 1.0666544437408447
Epoch 140, training loss: 90.23625946044922 = 1.0644571781158447 + 10.0 * 8.917180061340332
Epoch 140, val loss: 1.0646796226501465
Epoch 150, training loss: 90.0293197631836 = 1.0625497102737427 + 10.0 * 8.896677017211914
Epoch 150, val loss: 1.0628200769424438
Epoch 160, training loss: 89.85467529296875 = 1.060719609260559 + 10.0 * 8.879395484924316
Epoch 160, val loss: 1.0610482692718506
Epoch 170, training loss: 89.77008056640625 = 1.0589772462844849 + 10.0 * 8.871110916137695
Epoch 170, val loss: 1.0593401193618774
Epoch 180, training loss: 89.60762023925781 = 1.057319164276123 + 10.0 * 8.855030059814453
Epoch 180, val loss: 1.057777762413025
Epoch 190, training loss: 89.46247863769531 = 1.0557719469070435 + 10.0 * 8.840670585632324
Epoch 190, val loss: 1.0562337636947632
Epoch 200, training loss: 89.3878402709961 = 1.0542809963226318 + 10.0 * 8.833355903625488
Epoch 200, val loss: 1.0548319816589355
Epoch 210, training loss: 89.2935791015625 = 1.0528308153152466 + 10.0 * 8.824074745178223
Epoch 210, val loss: 1.0534247159957886
Epoch 220, training loss: 89.22891998291016 = 1.0514605045318604 + 10.0 * 8.81774616241455
Epoch 220, val loss: 1.0521007776260376
Epoch 230, training loss: 89.15287017822266 = 1.0500900745391846 + 10.0 * 8.810277938842773
Epoch 230, val loss: 1.0508126020431519
Epoch 240, training loss: 89.10467529296875 = 1.0487838983535767 + 10.0 * 8.80558967590332
Epoch 240, val loss: 1.0495513677597046
Epoch 250, training loss: 89.05404663085938 = 1.0474554300308228 + 10.0 * 8.8006591796875
Epoch 250, val loss: 1.0482693910598755
Epoch 260, training loss: 89.00237274169922 = 1.0461835861206055 + 10.0 * 8.795619010925293
Epoch 260, val loss: 1.0471323728561401
Epoch 270, training loss: 88.94676971435547 = 1.044746994972229 + 10.0 * 8.790202140808105
Epoch 270, val loss: 1.0457607507705688
Epoch 280, training loss: 88.91766357421875 = 1.0434404611587524 + 10.0 * 8.787422180175781
Epoch 280, val loss: 1.0444949865341187
Epoch 290, training loss: 88.89659118652344 = 1.0420080423355103 + 10.0 * 8.7854585647583
Epoch 290, val loss: 1.0431302785873413
Epoch 300, training loss: 88.94496154785156 = 1.0405371189117432 + 10.0 * 8.79044246673584
Epoch 300, val loss: 1.0417066812515259
Epoch 310, training loss: 88.89440155029297 = 1.0390088558197021 + 10.0 * 8.785539627075195
Epoch 310, val loss: 1.0402590036392212
Epoch 320, training loss: 88.83855438232422 = 1.037347674369812 + 10.0 * 8.780120849609375
Epoch 320, val loss: 1.0387012958526611
Epoch 330, training loss: 88.80703735351562 = 1.0355974435806274 + 10.0 * 8.777143478393555
Epoch 330, val loss: 1.037043571472168
Epoch 340, training loss: 88.82184600830078 = 1.033840298652649 + 10.0 * 8.778800010681152
Epoch 340, val loss: 1.0353453159332275
Epoch 350, training loss: 88.8526611328125 = 1.0319050550460815 + 10.0 * 8.782075881958008
Epoch 350, val loss: 1.0334956645965576
Epoch 360, training loss: 88.87714385986328 = 1.0298603773117065 + 10.0 * 8.784728050231934
Epoch 360, val loss: 1.031545639038086
Epoch 370, training loss: 88.84963989257812 = 1.0276836156845093 + 10.0 * 8.782195091247559
Epoch 370, val loss: 1.029476523399353
Epoch 380, training loss: 88.86395263671875 = 1.0254510641098022 + 10.0 * 8.783849716186523
Epoch 380, val loss: 1.0273438692092896
Epoch 390, training loss: 88.87334442138672 = 1.02296781539917 + 10.0 * 8.785037994384766
Epoch 390, val loss: 1.0250051021575928
Epoch 400, training loss: 88.90147399902344 = 1.020385980606079 + 10.0 * 8.788108825683594
Epoch 400, val loss: 1.0225474834442139
Epoch 410, training loss: 88.9283447265625 = 1.0175952911376953 + 10.0 * 8.791074752807617
Epoch 410, val loss: 1.0198743343353271
Epoch 420, training loss: 88.93193054199219 = 1.0145554542541504 + 10.0 * 8.79173755645752
Epoch 420, val loss: 1.016961932182312
Epoch 430, training loss: 88.91751098632812 = 1.0113940238952637 + 10.0 * 8.79061222076416
Epoch 430, val loss: 1.0140224695205688
Epoch 440, training loss: 88.96719360351562 = 1.0081467628479004 + 10.0 * 8.795904159545898
Epoch 440, val loss: 1.010938048362732
Epoch 450, training loss: 88.87130737304688 = 1.0044993162155151 + 10.0 * 8.786680221557617
Epoch 450, val loss: 1.0074310302734375
Epoch 460, training loss: 88.93307495117188 = 1.000985860824585 + 10.0 * 8.793209075927734
Epoch 460, val loss: 1.0040431022644043
Epoch 470, training loss: 88.92092895507812 = 0.9969949722290039 + 10.0 * 8.792393684387207
Epoch 470, val loss: 1.0003111362457275
Epoch 480, training loss: 88.93901062011719 = 0.9928305745124817 + 10.0 * 8.794618606567383
Epoch 480, val loss: 0.9964009523391724
Epoch 490, training loss: 88.99160766601562 = 0.9884176254272461 + 10.0 * 8.800318717956543
Epoch 490, val loss: 0.9922385811805725
Epoch 500, training loss: 88.99710845947266 = 0.9837357997894287 + 10.0 * 8.801337242126465
Epoch 500, val loss: 0.9878345131874084
Epoch 510, training loss: 89.00326538085938 = 0.9788345694541931 + 10.0 * 8.80244255065918
Epoch 510, val loss: 0.9832160472869873
Epoch 520, training loss: 88.9777603149414 = 0.9735735058784485 + 10.0 * 8.800418853759766
Epoch 520, val loss: 0.978287935256958
Epoch 530, training loss: 89.0035629272461 = 0.96821129322052 + 10.0 * 8.803535461425781
Epoch 530, val loss: 0.9732425212860107
Epoch 540, training loss: 89.04716491699219 = 0.9626811742782593 + 10.0 * 8.80844783782959
Epoch 540, val loss: 0.9680256247520447
Epoch 550, training loss: 89.03791809082031 = 0.9566943049430847 + 10.0 * 8.808122634887695
Epoch 550, val loss: 0.9624898433685303
Epoch 560, training loss: 89.04053497314453 = 0.9504281282424927 + 10.0 * 8.80901050567627
Epoch 560, val loss: 0.9566476345062256
Epoch 570, training loss: 89.09083557128906 = 0.9439007639884949 + 10.0 * 8.814693450927734
Epoch 570, val loss: 0.9505729079246521
Epoch 580, training loss: 89.096435546875 = 0.9370344877243042 + 10.0 * 8.815939903259277
Epoch 580, val loss: 0.9441696405410767
Epoch 590, training loss: 89.10157012939453 = 0.9299556016921997 + 10.0 * 8.817161560058594
Epoch 590, val loss: 0.9375549554824829
Epoch 600, training loss: 89.13127136230469 = 0.9229270219802856 + 10.0 * 8.820834159851074
Epoch 600, val loss: 0.9310342669487
Epoch 610, training loss: 89.11537170410156 = 0.9152374863624573 + 10.0 * 8.820013046264648
Epoch 610, val loss: 0.9239388704299927
Epoch 620, training loss: 89.14193725585938 = 0.9076387882232666 + 10.0 * 8.823430061340332
Epoch 620, val loss: 0.9169793725013733
Epoch 630, training loss: 89.1541748046875 = 0.8999240398406982 + 10.0 * 8.825425148010254
Epoch 630, val loss: 0.9098315238952637
Epoch 640, training loss: 89.13745880126953 = 0.8919495344161987 + 10.0 * 8.82455062866211
Epoch 640, val loss: 0.9025079011917114
Epoch 650, training loss: 89.18238830566406 = 0.8840664029121399 + 10.0 * 8.829832077026367
Epoch 650, val loss: 0.895482063293457
Epoch 660, training loss: 89.13865661621094 = 0.8761682510375977 + 10.0 * 8.826249122619629
Epoch 660, val loss: 0.888323962688446
Epoch 670, training loss: 89.13548278808594 = 0.8682020902633667 + 10.0 * 8.826727867126465
Epoch 670, val loss: 0.8809792995452881
Epoch 680, training loss: 89.16596221923828 = 0.8602575063705444 + 10.0 * 8.830570220947266
Epoch 680, val loss: 0.8738110065460205
Epoch 690, training loss: 89.17922973632812 = 0.8522449731826782 + 10.0 * 8.832698822021484
Epoch 690, val loss: 0.8665784597396851
Epoch 700, training loss: 89.17096710205078 = 0.8442051410675049 + 10.0 * 8.83267593383789
Epoch 700, val loss: 0.8593453168869019
Epoch 710, training loss: 89.20828247070312 = 0.8362712264060974 + 10.0 * 8.837201118469238
Epoch 710, val loss: 0.8522441387176514
Epoch 720, training loss: 89.24898529052734 = 0.828412652015686 + 10.0 * 8.842057228088379
Epoch 720, val loss: 0.8451574444770813
Epoch 730, training loss: 89.21920776367188 = 0.8203824162483215 + 10.0 * 8.839882850646973
Epoch 730, val loss: 0.8380319476127625
Epoch 740, training loss: 89.23172760009766 = 0.8125194311141968 + 10.0 * 8.841920852661133
Epoch 740, val loss: 0.8310423493385315
Epoch 750, training loss: 89.25543975830078 = 0.804904580116272 + 10.0 * 8.845053672790527
Epoch 750, val loss: 0.8242325186729431
Epoch 760, training loss: 89.24022674560547 = 0.797010064125061 + 10.0 * 8.844321250915527
Epoch 760, val loss: 0.8172885179519653
Epoch 770, training loss: 89.25877380371094 = 0.7894167900085449 + 10.0 * 8.846936225891113
Epoch 770, val loss: 0.8105292916297913
Epoch 780, training loss: 89.27013397216797 = 0.7818321585655212 + 10.0 * 8.848830223083496
Epoch 780, val loss: 0.803767740726471
Epoch 790, training loss: 89.30303955078125 = 0.7743381261825562 + 10.0 * 8.852869987487793
Epoch 790, val loss: 0.7970343828201294
Epoch 800, training loss: 89.3302993774414 = 0.7668028473854065 + 10.0 * 8.85634994506836
Epoch 800, val loss: 0.7902454733848572
Epoch 810, training loss: 89.34160614013672 = 0.7593380212783813 + 10.0 * 8.858226776123047
Epoch 810, val loss: 0.7836859226226807
Epoch 820, training loss: 89.35092163085938 = 0.7520561218261719 + 10.0 * 8.859886169433594
Epoch 820, val loss: 0.7771758437156677
Epoch 830, training loss: 89.30810546875 = 0.7446893453598022 + 10.0 * 8.856341361999512
Epoch 830, val loss: 0.7706337571144104
Epoch 840, training loss: 89.3526840209961 = 0.7374259233474731 + 10.0 * 8.861525535583496
Epoch 840, val loss: 0.7642031311988831
Epoch 850, training loss: 89.3681869506836 = 0.730448842048645 + 10.0 * 8.863774299621582
Epoch 850, val loss: 0.757885754108429
Epoch 860, training loss: 89.39324951171875 = 0.7235185503959656 + 10.0 * 8.866972923278809
Epoch 860, val loss: 0.7516465187072754
Epoch 870, training loss: 89.45489501953125 = 0.7168287634849548 + 10.0 * 8.873806953430176
Epoch 870, val loss: 0.7456803917884827
Epoch 880, training loss: 89.45525360107422 = 0.7100554704666138 + 10.0 * 8.874520301818848
Epoch 880, val loss: 0.7396689057350159
Epoch 890, training loss: 89.45150756835938 = 0.7033867835998535 + 10.0 * 8.874812126159668
Epoch 890, val loss: 0.7337327599525452
Epoch 900, training loss: 89.46673583984375 = 0.696918249130249 + 10.0 * 8.876981735229492
Epoch 900, val loss: 0.7279071807861328
Epoch 910, training loss: 89.48281860351562 = 0.690510630607605 + 10.0 * 8.879230499267578
Epoch 910, val loss: 0.7222574353218079
Epoch 920, training loss: 89.52420043945312 = 0.6843975782394409 + 10.0 * 8.883980751037598
Epoch 920, val loss: 0.7164906859397888
Epoch 930, training loss: 89.6087646484375 = 0.6786350607872009 + 10.0 * 8.893013000488281
Epoch 930, val loss: 0.7116973996162415
Epoch 940, training loss: 89.48844909667969 = 0.6727533936500549 + 10.0 * 8.881569862365723
Epoch 940, val loss: 0.706659734249115
Epoch 950, training loss: 89.5734634399414 = 0.6668629050254822 + 10.0 * 8.890660285949707
Epoch 950, val loss: 0.7013608813285828
Epoch 960, training loss: 89.63737487792969 = 0.6611468195915222 + 10.0 * 8.897623062133789
Epoch 960, val loss: 0.6962404847145081
Epoch 970, training loss: 89.64815521240234 = 0.6554526090621948 + 10.0 * 8.899271011352539
Epoch 970, val loss: 0.691303014755249
Epoch 980, training loss: 89.6443862915039 = 0.6500135660171509 + 10.0 * 8.899436950683594
Epoch 980, val loss: 0.6865217685699463
Epoch 990, training loss: 89.6203384399414 = 0.6446747779846191 + 10.0 * 8.897565841674805
Epoch 990, val loss: 0.6816760301589966
Epoch 1000, training loss: 89.67916870117188 = 0.6393713355064392 + 10.0 * 8.903979301452637
Epoch 1000, val loss: 0.6774097084999084
Epoch 1010, training loss: 89.65924835205078 = 0.6348280310630798 + 10.0 * 8.90244197845459
Epoch 1010, val loss: 0.6731997132301331
Epoch 1020, training loss: 89.62959289550781 = 0.6299289464950562 + 10.0 * 8.8999662399292
Epoch 1020, val loss: 0.6690773367881775
Epoch 1030, training loss: 89.72343444824219 = 0.6252567768096924 + 10.0 * 8.909817695617676
Epoch 1030, val loss: 0.6649613976478577
Epoch 1040, training loss: 89.70114135742188 = 0.6205874681472778 + 10.0 * 8.908055305480957
Epoch 1040, val loss: 0.6610325574874878
Epoch 1050, training loss: 89.7707290649414 = 0.6162561178207397 + 10.0 * 8.915447235107422
Epoch 1050, val loss: 0.6573026776313782
Epoch 1060, training loss: 89.75613403320312 = 0.6119385957717896 + 10.0 * 8.914419174194336
Epoch 1060, val loss: 0.6536771655082703
Epoch 1070, training loss: 89.79656982421875 = 0.6077183485031128 + 10.0 * 8.918885231018066
Epoch 1070, val loss: 0.6499131321907043
Epoch 1080, training loss: 89.82223510742188 = 0.603646457195282 + 10.0 * 8.921858787536621
Epoch 1080, val loss: 0.6466368436813354
Epoch 1090, training loss: 89.80901336669922 = 0.5996562838554382 + 10.0 * 8.92093563079834
Epoch 1090, val loss: 0.6432598829269409
Epoch 1100, training loss: 89.7967758178711 = 0.595909595489502 + 10.0 * 8.920086860656738
Epoch 1100, val loss: 0.6401274800300598
Epoch 1110, training loss: 89.80711364746094 = 0.5924100279808044 + 10.0 * 8.921470642089844
Epoch 1110, val loss: 0.637330949306488
Epoch 1120, training loss: 89.84675598144531 = 0.5888387560844421 + 10.0 * 8.92579174041748
Epoch 1120, val loss: 0.6343160271644592
Epoch 1130, training loss: 89.74051666259766 = 0.5851538181304932 + 10.0 * 8.915536880493164
Epoch 1130, val loss: 0.6312645077705383
Epoch 1140, training loss: 89.77507019042969 = 0.5819118618965149 + 10.0 * 8.919316291809082
Epoch 1140, val loss: 0.6287259459495544
Epoch 1150, training loss: 89.85457611083984 = 0.5786802768707275 + 10.0 * 8.927589416503906
Epoch 1150, val loss: 0.6261367797851562
Epoch 1160, training loss: 89.91292572021484 = 0.575537383556366 + 10.0 * 8.933738708496094
Epoch 1160, val loss: 0.6235831379890442
Epoch 1170, training loss: 89.8912353515625 = 0.5724316835403442 + 10.0 * 8.931879997253418
Epoch 1170, val loss: 0.6211813688278198
Epoch 1180, training loss: 89.90020751953125 = 0.5695405602455139 + 10.0 * 8.933066368103027
Epoch 1180, val loss: 0.6188352704048157
Epoch 1190, training loss: 89.90103149414062 = 0.5666354894638062 + 10.0 * 8.933439254760742
Epoch 1190, val loss: 0.616784930229187
Epoch 1200, training loss: 89.91527557373047 = 0.5639845132827759 + 10.0 * 8.935129165649414
Epoch 1200, val loss: 0.6145220994949341
Epoch 1210, training loss: 89.97415924072266 = 0.5611029267311096 + 10.0 * 8.941305160522461
Epoch 1210, val loss: 0.6122727394104004
Epoch 1220, training loss: 89.97329711914062 = 0.5582313537597656 + 10.0 * 8.941506385803223
Epoch 1220, val loss: 0.6101366877555847
Epoch 1230, training loss: 90.0340805053711 = 0.555166482925415 + 10.0 * 8.947891235351562
Epoch 1230, val loss: 0.6074060201644897
Epoch 1240, training loss: 90.0076904296875 = 0.5520968437194824 + 10.0 * 8.94555950164795
Epoch 1240, val loss: 0.6050282120704651
Epoch 1250, training loss: 90.0177993774414 = 0.5492717027664185 + 10.0 * 8.946852684020996
Epoch 1250, val loss: 0.6032004952430725
Epoch 1260, training loss: 90.05857849121094 = 0.5465441942214966 + 10.0 * 8.951203346252441
Epoch 1260, val loss: 0.6012313961982727
Epoch 1270, training loss: 90.04905700683594 = 0.5438665747642517 + 10.0 * 8.950518608093262
Epoch 1270, val loss: 0.5992180705070496
Epoch 1280, training loss: 90.04328155517578 = 0.5413411855697632 + 10.0 * 8.950193405151367
Epoch 1280, val loss: 0.5973420143127441
Epoch 1290, training loss: 90.08380889892578 = 0.5388249158859253 + 10.0 * 8.954498291015625
Epoch 1290, val loss: 0.5955963730812073
Epoch 1300, training loss: 90.0558853149414 = 0.5362955331802368 + 10.0 * 8.951959609985352
Epoch 1300, val loss: 0.5938649773597717
Epoch 1310, training loss: 90.11268615722656 = 0.533967137336731 + 10.0 * 8.95787239074707
Epoch 1310, val loss: 0.5921820998191833
Epoch 1320, training loss: 90.10820770263672 = 0.5315456986427307 + 10.0 * 8.957666397094727
Epoch 1320, val loss: 0.590410053730011
Epoch 1330, training loss: 90.0887680053711 = 0.5292271375656128 + 10.0 * 8.955953598022461
Epoch 1330, val loss: 0.5888209342956543
Epoch 1340, training loss: 90.16287994384766 = 0.5270410776138306 + 10.0 * 8.963583946228027
Epoch 1340, val loss: 0.5872498750686646
Epoch 1350, training loss: 90.1130142211914 = 0.5250047445297241 + 10.0 * 8.95880126953125
Epoch 1350, val loss: 0.5860221982002258
Epoch 1360, training loss: 90.14021301269531 = 0.5228878259658813 + 10.0 * 8.961732864379883
Epoch 1360, val loss: 0.5845311880111694
Epoch 1370, training loss: 90.1825942993164 = 0.5206876397132874 + 10.0 * 8.966190338134766
Epoch 1370, val loss: 0.5829104781150818
Epoch 1380, training loss: 90.1923828125 = 0.5185059905052185 + 10.0 * 8.967387199401855
Epoch 1380, val loss: 0.5813316106796265
Epoch 1390, training loss: 90.16043853759766 = 0.5163981914520264 + 10.0 * 8.964404106140137
Epoch 1390, val loss: 0.5798885226249695
Epoch 1400, training loss: 90.24454498291016 = 0.5143900513648987 + 10.0 * 8.973015785217285
Epoch 1400, val loss: 0.578491747379303
Epoch 1410, training loss: 90.27467346191406 = 0.5123444199562073 + 10.0 * 8.976232528686523
Epoch 1410, val loss: 0.5772466659545898
Epoch 1420, training loss: 90.21704864501953 = 0.5103082060813904 + 10.0 * 8.970674514770508
Epoch 1420, val loss: 0.5757073760032654
Epoch 1430, training loss: 90.26551055908203 = 0.508348286151886 + 10.0 * 8.975716590881348
Epoch 1430, val loss: 0.5744280815124512
Epoch 1440, training loss: 90.32797241210938 = 0.506337583065033 + 10.0 * 8.982163429260254
Epoch 1440, val loss: 0.5731074213981628
Epoch 1450, training loss: 90.23530578613281 = 0.5042812824249268 + 10.0 * 8.973102569580078
Epoch 1450, val loss: 0.571704626083374
Epoch 1460, training loss: 90.27608489990234 = 0.5024444460868835 + 10.0 * 8.977364540100098
Epoch 1460, val loss: 0.5703510046005249
Epoch 1470, training loss: 90.33769989013672 = 0.5006167888641357 + 10.0 * 8.983708381652832
Epoch 1470, val loss: 0.5691538453102112
Epoch 1480, training loss: 90.40410614013672 = 0.4987262785434723 + 10.0 * 8.990537643432617
Epoch 1480, val loss: 0.5681248307228088
Epoch 1490, training loss: 90.38033294677734 = 0.49680325388908386 + 10.0 * 8.98835277557373
Epoch 1490, val loss: 0.5663416385650635
Epoch 1500, training loss: 90.3807373046875 = 0.49490463733673096 + 10.0 * 8.9885835647583
Epoch 1500, val loss: 0.565473198890686
Epoch 1510, training loss: 90.42508697509766 = 0.4930282533168793 + 10.0 * 8.993206024169922
Epoch 1510, val loss: 0.563951313495636
Epoch 1520, training loss: 90.39918518066406 = 0.49111098051071167 + 10.0 * 8.99080753326416
Epoch 1520, val loss: 0.5627719759941101
Epoch 1530, training loss: 90.43348693847656 = 0.48930734395980835 + 10.0 * 8.994418144226074
Epoch 1530, val loss: 0.5614996552467346
Epoch 1540, training loss: 90.44357299804688 = 0.4875134527683258 + 10.0 * 8.995606422424316
Epoch 1540, val loss: 0.5603684186935425
Epoch 1550, training loss: 90.44751739501953 = 0.4856953024864197 + 10.0 * 8.996182441711426
Epoch 1550, val loss: 0.5590088367462158
Epoch 1560, training loss: 90.47000885009766 = 0.48384591937065125 + 10.0 * 8.998616218566895
Epoch 1560, val loss: 0.5579103827476501
Epoch 1570, training loss: 90.48320770263672 = 0.48203015327453613 + 10.0 * 9.000117301940918
Epoch 1570, val loss: 0.5567008852958679
Epoch 1580, training loss: 90.51914978027344 = 0.48023009300231934 + 10.0 * 9.003891944885254
Epoch 1580, val loss: 0.5555046200752258
Epoch 1590, training loss: 90.32579803466797 = 0.4783518612384796 + 10.0 * 8.98474407196045
Epoch 1590, val loss: 0.5542835593223572
Epoch 1600, training loss: 90.31316375732422 = 0.4768962264060974 + 10.0 * 8.983626365661621
Epoch 1600, val loss: 0.5535514950752258
Epoch 1610, training loss: 90.3910903930664 = 0.47530457377433777 + 10.0 * 8.991579055786133
Epoch 1610, val loss: 0.5522545576095581
Epoch 1620, training loss: 90.47151947021484 = 0.47357484698295593 + 10.0 * 8.999794960021973
Epoch 1620, val loss: 0.5511200428009033
Epoch 1630, training loss: 90.5464096069336 = 0.4717923104763031 + 10.0 * 9.007461547851562
Epoch 1630, val loss: 0.5498858094215393
Epoch 1640, training loss: 90.45956420898438 = 0.4699438810348511 + 10.0 * 8.99896240234375
Epoch 1640, val loss: 0.5487938523292542
Epoch 1650, training loss: 90.4684066772461 = 0.4682362973690033 + 10.0 * 9.000017166137695
Epoch 1650, val loss: 0.5478408932685852
Epoch 1660, training loss: 90.54302215576172 = 0.4665021300315857 + 10.0 * 9.007652282714844
Epoch 1660, val loss: 0.546666145324707
Epoch 1670, training loss: 90.54241943359375 = 0.46475547552108765 + 10.0 * 9.007766723632812
Epoch 1670, val loss: 0.5456834435462952
Epoch 1680, training loss: 90.57316589355469 = 0.4630143642425537 + 10.0 * 9.011014938354492
Epoch 1680, val loss: 0.5445318222045898
Epoch 1690, training loss: 90.5880355834961 = 0.46123307943344116 + 10.0 * 9.012680053710938
Epoch 1690, val loss: 0.5434060096740723
Epoch 1700, training loss: 90.60952758789062 = 0.4594908058643341 + 10.0 * 9.01500415802002
Epoch 1700, val loss: 0.5424076318740845
Epoch 1710, training loss: 90.59667205810547 = 0.4577476382255554 + 10.0 * 9.01389217376709
Epoch 1710, val loss: 0.5412656664848328
Epoch 1720, training loss: 90.61739349365234 = 0.4560726583003998 + 10.0 * 9.016132354736328
Epoch 1720, val loss: 0.54026859998703
Epoch 1730, training loss: 90.67242431640625 = 0.45436134934425354 + 10.0 * 9.021806716918945
Epoch 1730, val loss: 0.5393171310424805
Epoch 1740, training loss: 90.62537384033203 = 0.45260077714920044 + 10.0 * 9.017277717590332
Epoch 1740, val loss: 0.5383097529411316
Epoch 1750, training loss: 90.6511001586914 = 0.4509861469268799 + 10.0 * 9.020010948181152
Epoch 1750, val loss: 0.5373625159263611
Epoch 1760, training loss: 90.69137573242188 = 0.4493030309677124 + 10.0 * 9.02420711517334
Epoch 1760, val loss: 0.5363351702690125
Epoch 1770, training loss: 90.73675537109375 = 0.4475898742675781 + 10.0 * 9.02891731262207
Epoch 1770, val loss: 0.5353146195411682
Epoch 1780, training loss: 90.69678497314453 = 0.4459729790687561 + 10.0 * 9.025080680847168
Epoch 1780, val loss: 0.5342937111854553
Epoch 1790, training loss: 90.63836669921875 = 0.4444349706172943 + 10.0 * 9.019392967224121
Epoch 1790, val loss: 0.5338116884231567
Epoch 1800, training loss: 90.6317138671875 = 0.442806214094162 + 10.0 * 9.018891334533691
Epoch 1800, val loss: 0.5328140258789062
Epoch 1810, training loss: 90.72452545166016 = 0.44118669629096985 + 10.0 * 9.02833366394043
Epoch 1810, val loss: 0.5317781567573547
Epoch 1820, training loss: 90.77111053466797 = 0.43950048089027405 + 10.0 * 9.033161163330078
Epoch 1820, val loss: 0.5310011506080627
Epoch 1830, training loss: 90.76469421386719 = 0.4377823770046234 + 10.0 * 9.03269100189209
Epoch 1830, val loss: 0.5300820469856262
Epoch 1840, training loss: 90.79215240478516 = 0.4361211061477661 + 10.0 * 9.035603523254395
Epoch 1840, val loss: 0.5292575359344482
Epoch 1850, training loss: 90.84678649902344 = 0.43441566824913025 + 10.0 * 9.041236877441406
Epoch 1850, val loss: 0.5283149480819702
Epoch 1860, training loss: 90.79985809326172 = 0.4327152967453003 + 10.0 * 9.036714553833008
Epoch 1860, val loss: 0.5274724960327148
Epoch 1870, training loss: 90.89848327636719 = 0.4311161935329437 + 10.0 * 9.046736717224121
Epoch 1870, val loss: 0.5267900228500366
Epoch 1880, training loss: 90.81690979003906 = 0.4298219680786133 + 10.0 * 9.038708686828613
Epoch 1880, val loss: 0.5255551934242249
Epoch 1890, training loss: 90.74015808105469 = 0.4284178912639618 + 10.0 * 9.031173706054688
Epoch 1890, val loss: 0.5256828665733337
Epoch 1900, training loss: 90.83269500732422 = 0.42687639594078064 + 10.0 * 9.040581703186035
Epoch 1900, val loss: 0.5243586301803589
Epoch 1910, training loss: 90.9259033203125 = 0.4251501262187958 + 10.0 * 9.05007553100586
Epoch 1910, val loss: 0.5238577127456665
Epoch 1920, training loss: 90.95263671875 = 0.4233890771865845 + 10.0 * 9.052925109863281
Epoch 1920, val loss: 0.5228465795516968
Epoch 1930, training loss: 91.00753784179688 = 0.42164257168769836 + 10.0 * 9.058588981628418
Epoch 1930, val loss: 0.521896481513977
Epoch 1940, training loss: 90.99186706542969 = 0.4199010133743286 + 10.0 * 9.057196617126465
Epoch 1940, val loss: 0.5211857557296753
Epoch 1950, training loss: 91.02887725830078 = 0.4182162582874298 + 10.0 * 9.061066627502441
Epoch 1950, val loss: 0.5203293561935425
Epoch 1960, training loss: 90.95159149169922 = 0.4164733588695526 + 10.0 * 9.053511619567871
Epoch 1960, val loss: 0.5197075605392456
Epoch 1970, training loss: 91.00286102294922 = 0.4148638844490051 + 10.0 * 9.058799743652344
Epoch 1970, val loss: 0.5189452767372131
Epoch 1980, training loss: 91.04541778564453 = 0.41326195001602173 + 10.0 * 9.063215255737305
Epoch 1980, val loss: 0.5182672739028931
Epoch 1990, training loss: 91.12530517578125 = 0.4115844666957855 + 10.0 * 9.071372032165527
Epoch 1990, val loss: 0.5176045298576355
Epoch 2000, training loss: 91.10956573486328 = 0.40985986590385437 + 10.0 * 9.069971084594727
Epoch 2000, val loss: 0.5167770385742188
Epoch 2010, training loss: 91.07013702392578 = 0.408182293176651 + 10.0 * 9.066195487976074
Epoch 2010, val loss: 0.516330361366272
Epoch 2020, training loss: 91.14148712158203 = 0.40653929114341736 + 10.0 * 9.073494911193848
Epoch 2020, val loss: 0.5157358646392822
Epoch 2030, training loss: 91.16795349121094 = 0.40487566590309143 + 10.0 * 9.07630729675293
Epoch 2030, val loss: 0.5150831937789917
Epoch 2040, training loss: 91.14736938476562 = 0.4031863212585449 + 10.0 * 9.074419021606445
Epoch 2040, val loss: 0.5143895745277405
Epoch 2050, training loss: 91.1639175415039 = 0.4015422761440277 + 10.0 * 9.076237678527832
Epoch 2050, val loss: 0.5137821435928345
Epoch 2060, training loss: 91.20658111572266 = 0.39989081025123596 + 10.0 * 9.080669403076172
Epoch 2060, val loss: 0.5133245587348938
Epoch 2070, training loss: 91.20767974853516 = 0.3982173502445221 + 10.0 * 9.08094596862793
Epoch 2070, val loss: 0.5125671029090881
Epoch 2080, training loss: 91.21501159667969 = 0.3965713679790497 + 10.0 * 9.081844329833984
Epoch 2080, val loss: 0.5121670961380005
Epoch 2090, training loss: 91.2091064453125 = 0.3949401080608368 + 10.0 * 9.081416130065918
Epoch 2090, val loss: 0.5116169452667236
Epoch 2100, training loss: 91.26708221435547 = 0.39328068494796753 + 10.0 * 9.087380409240723
Epoch 2100, val loss: 0.5110222101211548
Epoch 2110, training loss: 91.20664978027344 = 0.3916783034801483 + 10.0 * 9.081497192382812
Epoch 2110, val loss: 0.5105782747268677
Epoch 2120, training loss: 91.14093780517578 = 0.390156090259552 + 10.0 * 9.075078010559082
Epoch 2120, val loss: 0.5098049640655518
Epoch 2130, training loss: 91.16999053955078 = 0.38871198892593384 + 10.0 * 9.07812786102295
Epoch 2130, val loss: 0.5094070434570312
Epoch 2140, training loss: 91.22197723388672 = 0.38715434074401855 + 10.0 * 9.08348274230957
Epoch 2140, val loss: 0.5089144110679626
Epoch 2150, training loss: 91.2729263305664 = 0.3855195641517639 + 10.0 * 9.088740348815918
Epoch 2150, val loss: 0.508492648601532
Epoch 2160, training loss: 91.32230377197266 = 0.38381871581077576 + 10.0 * 9.09384822845459
Epoch 2160, val loss: 0.5080307722091675
Epoch 2170, training loss: 91.28697204589844 = 0.3821171224117279 + 10.0 * 9.090485572814941
Epoch 2170, val loss: 0.5075552463531494
Epoch 2180, training loss: 91.3097915649414 = 0.3804551959037781 + 10.0 * 9.092933654785156
Epoch 2180, val loss: 0.5070168375968933
Epoch 2190, training loss: 91.3452377319336 = 0.3787843883037567 + 10.0 * 9.09664535522461
Epoch 2190, val loss: 0.5065627098083496
Epoch 2200, training loss: 91.35548400878906 = 0.3770817816257477 + 10.0 * 9.097840309143066
Epoch 2200, val loss: 0.5061333179473877
Epoch 2210, training loss: 91.29975891113281 = 0.3754553198814392 + 10.0 * 9.092430114746094
Epoch 2210, val loss: 0.5058503150939941
Epoch 2220, training loss: 91.376220703125 = 0.37380826473236084 + 10.0 * 9.100240707397461
Epoch 2220, val loss: 0.5054051280021667
Epoch 2230, training loss: 91.38825988769531 = 0.37213486433029175 + 10.0 * 9.10161304473877
Epoch 2230, val loss: 0.5049600005149841
Epoch 2240, training loss: 91.37834167480469 = 0.370491087436676 + 10.0 * 9.100785255432129
Epoch 2240, val loss: 0.504601776599884
Epoch 2250, training loss: 91.3812484741211 = 0.3688451647758484 + 10.0 * 9.101240158081055
Epoch 2250, val loss: 0.5042378306388855
Epoch 2260, training loss: 91.38654327392578 = 0.3671971261501312 + 10.0 * 9.101934432983398
Epoch 2260, val loss: 0.5039292573928833
Epoch 2270, training loss: 91.37736511230469 = 0.3655734062194824 + 10.0 * 9.101179122924805
Epoch 2270, val loss: 0.503798246383667
Epoch 2280, training loss: 91.43372344970703 = 0.3639366328716278 + 10.0 * 9.106978416442871
Epoch 2280, val loss: 0.5035707950592041
Epoch 2290, training loss: 91.41004943847656 = 0.3622346520423889 + 10.0 * 9.104781150817871
Epoch 2290, val loss: 0.5034030079841614
Epoch 2300, training loss: 91.40355682373047 = 0.3606126308441162 + 10.0 * 9.104294776916504
Epoch 2300, val loss: 0.5031806826591492
Epoch 2310, training loss: 91.37855529785156 = 0.35905054211616516 + 10.0 * 9.101950645446777
Epoch 2310, val loss: 0.5030198693275452
Epoch 2320, training loss: 91.40140533447266 = 0.3574632406234741 + 10.0 * 9.104394912719727
Epoch 2320, val loss: 0.5027447938919067
Epoch 2330, training loss: 91.469482421875 = 0.35580170154571533 + 10.0 * 9.111368179321289
Epoch 2330, val loss: 0.5026803612709045
Epoch 2340, training loss: 91.49156188964844 = 0.3540807366371155 + 10.0 * 9.113748550415039
Epoch 2340, val loss: 0.5025342702865601
Epoch 2350, training loss: 91.46125030517578 = 0.3523945212364197 + 10.0 * 9.110885620117188
Epoch 2350, val loss: 0.5023704171180725
Epoch 2360, training loss: 91.47817993164062 = 0.35077768564224243 + 10.0 * 9.112740516662598
Epoch 2360, val loss: 0.502128005027771
Epoch 2370, training loss: 91.47494506835938 = 0.34914442896842957 + 10.0 * 9.112580299377441
Epoch 2370, val loss: 0.502276599407196
Epoch 2380, training loss: 91.50299835205078 = 0.3475240170955658 + 10.0 * 9.115547180175781
Epoch 2380, val loss: 0.5021671652793884
Epoch 2390, training loss: 91.4955062866211 = 0.34586209058761597 + 10.0 * 9.114964485168457
Epoch 2390, val loss: 0.5021471381187439
Epoch 2400, training loss: 91.51657104492188 = 0.344249427318573 + 10.0 * 9.117232322692871
Epoch 2400, val loss: 0.502197265625
Epoch 2410, training loss: 91.51455688476562 = 0.3426235616207123 + 10.0 * 9.117193222045898
Epoch 2410, val loss: 0.5023640394210815
Epoch 2420, training loss: 91.54706573486328 = 0.3409932255744934 + 10.0 * 9.120607376098633
Epoch 2420, val loss: 0.502416729927063
Epoch 2430, training loss: 91.57588195800781 = 0.3393493890762329 + 10.0 * 9.123653411865234
Epoch 2430, val loss: 0.5024172067642212
Epoch 2440, training loss: 91.54888153076172 = 0.33767765760421753 + 10.0 * 9.12112045288086
Epoch 2440, val loss: 0.502602756023407
Epoch 2450, training loss: 91.54170989990234 = 0.3361058831214905 + 10.0 * 9.120560646057129
Epoch 2450, val loss: 0.5026992559432983
Epoch 2460, training loss: 91.58668518066406 = 0.3345116674900055 + 10.0 * 9.12521743774414
Epoch 2460, val loss: 0.5027297139167786
Epoch 2470, training loss: 91.60419464111328 = 0.332877516746521 + 10.0 * 9.127131462097168
Epoch 2470, val loss: 0.5029633641242981
Epoch 2480, training loss: 91.59028625488281 = 0.33126720786094666 + 10.0 * 9.12590217590332
Epoch 2480, val loss: 0.5030578374862671
Epoch 2490, training loss: 91.62551879882812 = 0.3296552300453186 + 10.0 * 9.129586219787598
Epoch 2490, val loss: 0.503336489200592
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8039130434782609
0.8185901615590815
=== training gcn model ===
Epoch 0, training loss: 103.17607879638672 = 1.110307216644287 + 10.0 * 10.20657730102539
Epoch 0, val loss: 1.1099576950073242
Epoch 10, training loss: 98.88175201416016 = 1.1056901216506958 + 10.0 * 9.777606010437012
Epoch 10, val loss: 1.1054128408432007
Epoch 20, training loss: 97.06483459472656 = 1.101426601409912 + 10.0 * 9.596341133117676
Epoch 20, val loss: 1.1012029647827148
Epoch 30, training loss: 95.78062438964844 = 1.0973563194274902 + 10.0 * 9.468326568603516
Epoch 30, val loss: 1.097192645072937
Epoch 40, training loss: 94.76073455810547 = 1.0935628414154053 + 10.0 * 9.366717338562012
Epoch 40, val loss: 1.0934600830078125
Epoch 50, training loss: 93.91708374023438 = 1.0899978876113892 + 10.0 * 9.282709121704102
Epoch 50, val loss: 1.0899527072906494
Epoch 60, training loss: 93.19925689697266 = 1.086639404296875 + 10.0 * 9.211261749267578
Epoch 60, val loss: 1.086645245552063
Epoch 70, training loss: 92.58023834228516 = 1.083460807800293 + 10.0 * 9.149678230285645
Epoch 70, val loss: 1.0835236310958862
Epoch 80, training loss: 92.0496597290039 = 1.0804696083068848 + 10.0 * 9.096919059753418
Epoch 80, val loss: 1.0805858373641968
Epoch 90, training loss: 91.58702087402344 = 1.0776476860046387 + 10.0 * 9.05093765258789
Epoch 90, val loss: 1.0778162479400635
Epoch 100, training loss: 91.17272186279297 = 1.0749903917312622 + 10.0 * 9.009773254394531
Epoch 100, val loss: 1.0752151012420654
Epoch 110, training loss: 90.80355834960938 = 1.0724848508834839 + 10.0 * 8.97310733795166
Epoch 110, val loss: 1.0727612972259521
Epoch 120, training loss: 90.50059509277344 = 1.0700950622558594 + 10.0 * 8.943049430847168
Epoch 120, val loss: 1.0704280138015747
Epoch 130, training loss: 90.23578643798828 = 1.0679056644439697 + 10.0 * 8.916788101196289
Epoch 130, val loss: 1.0682823657989502
Epoch 140, training loss: 89.98249816894531 = 1.0657979249954224 + 10.0 * 8.891670227050781
Epoch 140, val loss: 1.0662364959716797
Epoch 150, training loss: 89.76739501953125 = 1.0638272762298584 + 10.0 * 8.870356559753418
Epoch 150, val loss: 1.064302921295166
Epoch 160, training loss: 89.5792465209961 = 1.0619888305664062 + 10.0 * 8.851725578308105
Epoch 160, val loss: 1.0625100135803223
Epoch 170, training loss: 89.40913391113281 = 1.0602506399154663 + 10.0 * 8.834888458251953
Epoch 170, val loss: 1.060813069343567
Epoch 180, training loss: 89.26612091064453 = 1.0586143732070923 + 10.0 * 8.82075023651123
Epoch 180, val loss: 1.0592260360717773
Epoch 190, training loss: 89.13512420654297 = 1.0570627450942993 + 10.0 * 8.807806015014648
Epoch 190, val loss: 1.0577203035354614
Epoch 200, training loss: 89.01614379882812 = 1.0555753707885742 + 10.0 * 8.796056747436523
Epoch 200, val loss: 1.0562453269958496
Epoch 210, training loss: 88.9143295288086 = 1.054159164428711 + 10.0 * 8.786016464233398
Epoch 210, val loss: 1.0549010038375854
Epoch 220, training loss: 88.8208999633789 = 1.0527712106704712 + 10.0 * 8.776812553405762
Epoch 220, val loss: 1.0535653829574585
Epoch 230, training loss: 88.7149887084961 = 1.0514565706253052 + 10.0 * 8.766352653503418
Epoch 230, val loss: 1.0522810220718384
Epoch 240, training loss: 88.64363098144531 = 1.0501759052276611 + 10.0 * 8.759345054626465
Epoch 240, val loss: 1.0510221719741821
Epoch 250, training loss: 88.57621765136719 = 1.048865795135498 + 10.0 * 8.752735137939453
Epoch 250, val loss: 1.0497305393218994
Epoch 260, training loss: 88.50303649902344 = 1.0475218296051025 + 10.0 * 8.745551109313965
Epoch 260, val loss: 1.0484626293182373
Epoch 270, training loss: 88.54734802246094 = 1.046343445777893 + 10.0 * 8.750101089477539
Epoch 270, val loss: 1.0472460985183716
Epoch 280, training loss: 88.43418884277344 = 1.0450503826141357 + 10.0 * 8.738913536071777
Epoch 280, val loss: 1.0459682941436768
Epoch 290, training loss: 88.41387939453125 = 1.043733835220337 + 10.0 * 8.737014770507812
Epoch 290, val loss: 1.0447165966033936
Epoch 300, training loss: 88.41471862792969 = 1.0424003601074219 + 10.0 * 8.737232208251953
Epoch 300, val loss: 1.0433982610702515
Epoch 310, training loss: 88.368896484375 = 1.040911316871643 + 10.0 * 8.73279857635498
Epoch 310, val loss: 1.0419408082962036
Epoch 320, training loss: 88.35105895996094 = 1.0394293069839478 + 10.0 * 8.731163024902344
Epoch 320, val loss: 1.0404659509658813
Epoch 330, training loss: 88.32990264892578 = 1.037811040878296 + 10.0 * 8.729208946228027
Epoch 330, val loss: 1.038892388343811
Epoch 340, training loss: 88.32675170898438 = 1.036138653755188 + 10.0 * 8.729061126708984
Epoch 340, val loss: 1.0372518301010132
Epoch 350, training loss: 88.2902603149414 = 1.0344096422195435 + 10.0 * 8.725584983825684
Epoch 350, val loss: 1.035523772239685
Epoch 360, training loss: 88.27239227294922 = 1.0325472354888916 + 10.0 * 8.723984718322754
Epoch 360, val loss: 1.033663034439087
Epoch 370, training loss: 88.24966430664062 = 1.0305399894714355 + 10.0 * 8.721912384033203
Epoch 370, val loss: 1.0316920280456543
Epoch 380, training loss: 88.25286102294922 = 1.0283949375152588 + 10.0 * 8.72244644165039
Epoch 380, val loss: 1.0295823812484741
Epoch 390, training loss: 88.21673583984375 = 1.0261764526367188 + 10.0 * 8.719056129455566
Epoch 390, val loss: 1.0273836851119995
Epoch 400, training loss: 88.23967742919922 = 1.0239185094833374 + 10.0 * 8.721575736999512
Epoch 400, val loss: 1.0251803398132324
Epoch 410, training loss: 88.19660949707031 = 1.0213449001312256 + 10.0 * 8.71752643585205
Epoch 410, val loss: 1.0226662158966064
Epoch 420, training loss: 88.20166778564453 = 1.0186556577682495 + 10.0 * 8.718301773071289
Epoch 420, val loss: 1.0200027227401733
Epoch 430, training loss: 88.24220275878906 = 1.0158352851867676 + 10.0 * 8.722636222839355
Epoch 430, val loss: 1.0171986818313599
Epoch 440, training loss: 88.2332534790039 = 1.0128090381622314 + 10.0 * 8.722043991088867
Epoch 440, val loss: 1.0142534971237183
Epoch 450, training loss: 88.21583557128906 = 1.0095653533935547 + 10.0 * 8.720626831054688
Epoch 450, val loss: 1.0110785961151123
Epoch 460, training loss: 88.23775482177734 = 1.006140947341919 + 10.0 * 8.723161697387695
Epoch 460, val loss: 1.0077643394470215
Epoch 470, training loss: 88.2319564819336 = 1.002567172050476 + 10.0 * 8.722939491271973
Epoch 470, val loss: 1.004234790802002
Epoch 480, training loss: 88.23539733886719 = 0.9986711740493774 + 10.0 * 8.723672866821289
Epoch 480, val loss: 1.0004364252090454
Epoch 490, training loss: 88.23143005371094 = 0.9946775436401367 + 10.0 * 8.723675727844238
Epoch 490, val loss: 0.9965651631355286
Epoch 500, training loss: 88.22882843017578 = 0.9904351830482483 + 10.0 * 8.72383975982666
Epoch 500, val loss: 0.9924303293228149
Epoch 510, training loss: 88.22076416015625 = 0.9859310984611511 + 10.0 * 8.723483085632324
Epoch 510, val loss: 0.9879966378211975
Epoch 520, training loss: 88.21530151367188 = 0.9811223149299622 + 10.0 * 8.723417282104492
Epoch 520, val loss: 0.9834238290786743
Epoch 530, training loss: 88.17140197753906 = 0.9758148193359375 + 10.0 * 8.719558715820312
Epoch 530, val loss: 0.9783669114112854
Epoch 540, training loss: 88.2861099243164 = 0.9710139036178589 + 10.0 * 8.7315092086792
Epoch 540, val loss: 0.9736078381538391
Epoch 550, training loss: 88.32565307617188 = 0.9655575752258301 + 10.0 * 8.73600959777832
Epoch 550, val loss: 0.9683698415756226
Epoch 560, training loss: 88.1957015991211 = 0.9595681428909302 + 10.0 * 8.723612785339355
Epoch 560, val loss: 0.9626235365867615
Epoch 570, training loss: 88.22272491455078 = 0.9538227319717407 + 10.0 * 8.726890563964844
Epoch 570, val loss: 0.957065999507904
Epoch 580, training loss: 88.21934509277344 = 0.947542667388916 + 10.0 * 8.727180480957031
Epoch 580, val loss: 0.9510802030563354
Epoch 590, training loss: 88.20132446289062 = 0.9409465193748474 + 10.0 * 8.726037979125977
Epoch 590, val loss: 0.9447852373123169
Epoch 600, training loss: 88.2164306640625 = 0.9343688488006592 + 10.0 * 8.728205680847168
Epoch 600, val loss: 0.9384046196937561
Epoch 610, training loss: 88.23589324951172 = 0.9273589253425598 + 10.0 * 8.730853080749512
Epoch 610, val loss: 0.9317073225975037
Epoch 620, training loss: 88.24687957763672 = 0.9203073382377625 + 10.0 * 8.732656478881836
Epoch 620, val loss: 0.9249445796012878
Epoch 630, training loss: 88.28585052490234 = 0.9129269123077393 + 10.0 * 8.737292289733887
Epoch 630, val loss: 0.9179117679595947
Epoch 640, training loss: 88.2957992553711 = 0.9053451418876648 + 10.0 * 8.739046096801758
Epoch 640, val loss: 0.9106655120849609
Epoch 650, training loss: 88.26754760742188 = 0.8974743485450745 + 10.0 * 8.737007141113281
Epoch 650, val loss: 0.9032396078109741
Epoch 660, training loss: 88.30132293701172 = 0.8896130919456482 + 10.0 * 8.741170883178711
Epoch 660, val loss: 0.8957995176315308
Epoch 670, training loss: 88.3161849975586 = 0.8817166090011597 + 10.0 * 8.743447303771973
Epoch 670, val loss: 0.8883904218673706
Epoch 680, training loss: 88.33446502685547 = 0.8737547397613525 + 10.0 * 8.746070861816406
Epoch 680, val loss: 0.8810053467750549
Epoch 690, training loss: 88.33096313476562 = 0.8655624389648438 + 10.0 * 8.746540069580078
Epoch 690, val loss: 0.8731842041015625
Epoch 700, training loss: 88.31403350830078 = 0.8572100400924683 + 10.0 * 8.745682716369629
Epoch 700, val loss: 0.8653560876846313
Epoch 710, training loss: 88.34063720703125 = 0.8489211797714233 + 10.0 * 8.749171257019043
Epoch 710, val loss: 0.8577100038528442
Epoch 720, training loss: 88.30265808105469 = 0.8408408761024475 + 10.0 * 8.74618148803711
Epoch 720, val loss: 0.850123405456543
Epoch 730, training loss: 88.31698608398438 = 0.8325629234313965 + 10.0 * 8.748441696166992
Epoch 730, val loss: 0.8424246907234192
Epoch 740, training loss: 88.3936538696289 = 0.8248656392097473 + 10.0 * 8.756878852844238
Epoch 740, val loss: 0.8353015780448914
Epoch 750, training loss: 88.3646011352539 = 0.816910445690155 + 10.0 * 8.754769325256348
Epoch 750, val loss: 0.8279785513877869
Epoch 760, training loss: 88.40045928955078 = 0.8090882897377014 + 10.0 * 8.759137153625488
Epoch 760, val loss: 0.8207748532295227
Epoch 770, training loss: 88.40970611572266 = 0.8012501001358032 + 10.0 * 8.760846138000488
Epoch 770, val loss: 0.8135942816734314
Epoch 780, training loss: 88.40763854980469 = 0.7936489582061768 + 10.0 * 8.761399269104004
Epoch 780, val loss: 0.8066918253898621
Epoch 790, training loss: 88.36015319824219 = 0.7860726118087769 + 10.0 * 8.757408142089844
Epoch 790, val loss: 0.7996456623077393
Epoch 800, training loss: 88.40975952148438 = 0.7787660956382751 + 10.0 * 8.763099670410156
Epoch 800, val loss: 0.7930643558502197
Epoch 810, training loss: 88.43265533447266 = 0.7713889479637146 + 10.0 * 8.76612663269043
Epoch 810, val loss: 0.7864720821380615
Epoch 820, training loss: 88.42888641357422 = 0.7643477916717529 + 10.0 * 8.766453742980957
Epoch 820, val loss: 0.7800372242927551
Epoch 830, training loss: 88.46993255615234 = 0.7571530938148499 + 10.0 * 8.77127742767334
Epoch 830, val loss: 0.7735304236412048
Epoch 840, training loss: 88.48722076416016 = 0.7501357793807983 + 10.0 * 8.77370834350586
Epoch 840, val loss: 0.7671918869018555
Epoch 850, training loss: 88.46810913085938 = 0.743057906627655 + 10.0 * 8.772504806518555
Epoch 850, val loss: 0.7607471942901611
Epoch 860, training loss: 88.4550552368164 = 0.7362537980079651 + 10.0 * 8.771880149841309
Epoch 860, val loss: 0.7545928359031677
Epoch 870, training loss: 88.50575256347656 = 0.7295324802398682 + 10.0 * 8.77762222290039
Epoch 870, val loss: 0.7487603425979614
Epoch 880, training loss: 88.5434341430664 = 0.7228376865386963 + 10.0 * 8.782059669494629
Epoch 880, val loss: 0.7425826787948608
Epoch 890, training loss: 88.56184387207031 = 0.7161186933517456 + 10.0 * 8.78457260131836
Epoch 890, val loss: 0.7366719841957092
Epoch 900, training loss: 88.60498809814453 = 0.7096688151359558 + 10.0 * 8.789531707763672
Epoch 900, val loss: 0.7307664752006531
Epoch 910, training loss: 88.58938598632812 = 0.703277051448822 + 10.0 * 8.788610458374023
Epoch 910, val loss: 0.7250809073448181
Epoch 920, training loss: 88.59130096435547 = 0.6968231201171875 + 10.0 * 8.789447784423828
Epoch 920, val loss: 0.7193126082420349
Epoch 930, training loss: 88.626953125 = 0.690849781036377 + 10.0 * 8.793610572814941
Epoch 930, val loss: 0.7140792012214661
Epoch 940, training loss: 88.6081314086914 = 0.6847758889198303 + 10.0 * 8.792335510253906
Epoch 940, val loss: 0.7085973024368286
Epoch 950, training loss: 88.63353729248047 = 0.6789389848709106 + 10.0 * 8.795459747314453
Epoch 950, val loss: 0.7035250663757324
Epoch 960, training loss: 88.68215942382812 = 0.6731911897659302 + 10.0 * 8.800896644592285
Epoch 960, val loss: 0.6983113884925842
Epoch 970, training loss: 88.68824768066406 = 0.6674718260765076 + 10.0 * 8.802077293395996
Epoch 970, val loss: 0.6933053135871887
Epoch 980, training loss: 88.63985443115234 = 0.6618055105209351 + 10.0 * 8.797804832458496
Epoch 980, val loss: 0.6885210275650024
Epoch 990, training loss: 88.62798309326172 = 0.6562430262565613 + 10.0 * 8.797174453735352
Epoch 990, val loss: 0.6834450364112854
Epoch 1000, training loss: 88.6576919555664 = 0.6512076258659363 + 10.0 * 8.80064868927002
Epoch 1000, val loss: 0.6790432929992676
Epoch 1010, training loss: 88.69959259033203 = 0.6459577083587646 + 10.0 * 8.805363655090332
Epoch 1010, val loss: 0.674517035484314
Epoch 1020, training loss: 88.6704330444336 = 0.6408804059028625 + 10.0 * 8.802955627441406
Epoch 1020, val loss: 0.6702700853347778
Epoch 1030, training loss: 88.6942367553711 = 0.6359973549842834 + 10.0 * 8.805824279785156
Epoch 1030, val loss: 0.6659271717071533
Epoch 1040, training loss: 88.69859313964844 = 0.6310803294181824 + 10.0 * 8.806751251220703
Epoch 1040, val loss: 0.6616978645324707
Epoch 1050, training loss: 88.72200012207031 = 0.6263618469238281 + 10.0 * 8.809563636779785
Epoch 1050, val loss: 0.6577101349830627
Epoch 1060, training loss: 88.7270736694336 = 0.6216843724250793 + 10.0 * 8.810539245605469
Epoch 1060, val loss: 0.6536766290664673
Epoch 1070, training loss: 88.68885040283203 = 0.6169893741607666 + 10.0 * 8.807186126708984
Epoch 1070, val loss: 0.6498653888702393
Epoch 1080, training loss: 88.69194793701172 = 0.6128474473953247 + 10.0 * 8.807909965515137
Epoch 1080, val loss: 0.64618319272995
Epoch 1090, training loss: 88.7280044555664 = 0.6083621382713318 + 10.0 * 8.81196403503418
Epoch 1090, val loss: 0.6425405144691467
Epoch 1100, training loss: 88.77237701416016 = 0.6040399074554443 + 10.0 * 8.81683349609375
Epoch 1100, val loss: 0.6390253305435181
Epoch 1110, training loss: 88.76750946044922 = 0.5998680591583252 + 10.0 * 8.816763877868652
Epoch 1110, val loss: 0.6356027722358704
Epoch 1120, training loss: 88.7598648071289 = 0.5959132313728333 + 10.0 * 8.816394805908203
Epoch 1120, val loss: 0.6323506236076355
Epoch 1130, training loss: 88.80089569091797 = 0.5920175313949585 + 10.0 * 8.820887565612793
Epoch 1130, val loss: 0.6292703747749329
Epoch 1140, training loss: 88.83399963378906 = 0.5881602168083191 + 10.0 * 8.824584007263184
Epoch 1140, val loss: 0.6259967088699341
Epoch 1150, training loss: 88.82472229003906 = 0.5844582319259644 + 10.0 * 8.824026107788086
Epoch 1150, val loss: 0.6230291128158569
Epoch 1160, training loss: 88.80323791503906 = 0.5807426571846008 + 10.0 * 8.822249412536621
Epoch 1160, val loss: 0.6201975345611572
Epoch 1170, training loss: 88.83377075195312 = 0.5771504044532776 + 10.0 * 8.825662612915039
Epoch 1170, val loss: 0.6173459887504578
Epoch 1180, training loss: 88.85277557373047 = 0.5736439824104309 + 10.0 * 8.827913284301758
Epoch 1180, val loss: 0.6145633459091187
Epoch 1190, training loss: 88.87201690673828 = 0.5701575875282288 + 10.0 * 8.830185890197754
Epoch 1190, val loss: 0.6118286848068237
Epoch 1200, training loss: 88.85653686523438 = 0.5667100548744202 + 10.0 * 8.82898235321045
Epoch 1200, val loss: 0.6092050671577454
Epoch 1210, training loss: 88.88114166259766 = 0.5634141564369202 + 10.0 * 8.831772804260254
Epoch 1210, val loss: 0.606968343257904
Epoch 1220, training loss: 88.91283416748047 = 0.5601871013641357 + 10.0 * 8.835264205932617
Epoch 1220, val loss: 0.6044704914093018
Epoch 1230, training loss: 88.89175415039062 = 0.556792140007019 + 10.0 * 8.83349609375
Epoch 1230, val loss: 0.6017765998840332
Epoch 1240, training loss: 88.87361145019531 = 0.5534902811050415 + 10.0 * 8.832012176513672
Epoch 1240, val loss: 0.5992822051048279
Epoch 1250, training loss: 88.90061950683594 = 0.5503855347633362 + 10.0 * 8.835023880004883
Epoch 1250, val loss: 0.5970124006271362
Epoch 1260, training loss: 88.93954467773438 = 0.5473917126655579 + 10.0 * 8.839215278625488
Epoch 1260, val loss: 0.5948741436004639
Epoch 1270, training loss: 88.95064544677734 = 0.5443668961524963 + 10.0 * 8.840627670288086
Epoch 1270, val loss: 0.5925812125205994
Epoch 1280, training loss: 88.96036529541016 = 0.5413417220115662 + 10.0 * 8.841901779174805
Epoch 1280, val loss: 0.5903870463371277
Epoch 1290, training loss: 88.98033142089844 = 0.5383574366569519 + 10.0 * 8.844197273254395
Epoch 1290, val loss: 0.5882529616355896
Epoch 1300, training loss: 89.00460052490234 = 0.5354263782501221 + 10.0 * 8.846918106079102
Epoch 1300, val loss: 0.586145281791687
Epoch 1310, training loss: 88.9946517944336 = 0.5326969623565674 + 10.0 * 8.846195220947266
Epoch 1310, val loss: 0.5841753482818604
Epoch 1320, training loss: 89.02092742919922 = 0.5299442410469055 + 10.0 * 8.849098205566406
Epoch 1320, val loss: 0.5822205543518066
Epoch 1330, training loss: 89.03092193603516 = 0.527190625667572 + 10.0 * 8.850373268127441
Epoch 1330, val loss: 0.5801830291748047
Epoch 1340, training loss: 89.04227447509766 = 0.5245558619499207 + 10.0 * 8.851771354675293
Epoch 1340, val loss: 0.5783818364143372
Epoch 1350, training loss: 89.07301330566406 = 0.5219516158103943 + 10.0 * 8.855106353759766
Epoch 1350, val loss: 0.5766481161117554
Epoch 1360, training loss: 89.05384826660156 = 0.5193337202072144 + 10.0 * 8.8534517288208
Epoch 1360, val loss: 0.5746791958808899
Epoch 1370, training loss: 89.09249114990234 = 0.5167081952095032 + 10.0 * 8.85757827758789
Epoch 1370, val loss: 0.5730763673782349
Epoch 1380, training loss: 89.12969970703125 = 0.5141929388046265 + 10.0 * 8.861551284790039
Epoch 1380, val loss: 0.5711859464645386
Epoch 1390, training loss: 89.16024017333984 = 0.5116280317306519 + 10.0 * 8.864861488342285
Epoch 1390, val loss: 0.5696020722389221
Epoch 1400, training loss: 89.13460540771484 = 0.5091864466667175 + 10.0 * 8.862542152404785
Epoch 1400, val loss: 0.567821204662323
Epoch 1410, training loss: 89.14539337158203 = 0.5067918300628662 + 10.0 * 8.863860130310059
Epoch 1410, val loss: 0.5660954117774963
Epoch 1420, training loss: 89.1393814086914 = 0.5043638348579407 + 10.0 * 8.86350154876709
Epoch 1420, val loss: 0.564497172832489
Epoch 1430, training loss: 89.13584899902344 = 0.5019194483757019 + 10.0 * 8.86339282989502
Epoch 1430, val loss: 0.5628920197486877
Epoch 1440, training loss: 89.18138122558594 = 0.4995928704738617 + 10.0 * 8.868178367614746
Epoch 1440, val loss: 0.5613145232200623
Epoch 1450, training loss: 89.1968002319336 = 0.497239887714386 + 10.0 * 8.869956016540527
Epoch 1450, val loss: 0.5596697330474854
Epoch 1460, training loss: 89.22377014160156 = 0.49491190910339355 + 10.0 * 8.872885704040527
Epoch 1460, val loss: 0.5580369830131531
Epoch 1470, training loss: 89.19902801513672 = 0.49265924096107483 + 10.0 * 8.870636940002441
Epoch 1470, val loss: 0.5564113259315491
Epoch 1480, training loss: 89.21817779541016 = 0.4903450906276703 + 10.0 * 8.872782707214355
Epoch 1480, val loss: 0.5551925897598267
Epoch 1490, training loss: 89.24244689941406 = 0.4880969524383545 + 10.0 * 8.875434875488281
Epoch 1490, val loss: 0.5537498593330383
Epoch 1500, training loss: 89.21039581298828 = 0.4858015179634094 + 10.0 * 8.872459411621094
Epoch 1500, val loss: 0.5522236227989197
Epoch 1510, training loss: 89.2127914428711 = 0.4837566316127777 + 10.0 * 8.872903823852539
Epoch 1510, val loss: 0.5509081482887268
Epoch 1520, training loss: 89.24005889892578 = 0.4815009832382202 + 10.0 * 8.875856399536133
Epoch 1520, val loss: 0.5488895773887634
Epoch 1530, training loss: 89.23088836669922 = 0.47937431931495667 + 10.0 * 8.875150680541992
Epoch 1530, val loss: 0.5477941632270813
Epoch 1540, training loss: 89.28746795654297 = 0.4772282540798187 + 10.0 * 8.881024360656738
Epoch 1540, val loss: 0.5464807748794556
Epoch 1550, training loss: 89.26515197753906 = 0.47526925802230835 + 10.0 * 8.878988265991211
Epoch 1550, val loss: 0.5452808141708374
Epoch 1560, training loss: 89.198486328125 = 0.4732634425163269 + 10.0 * 8.872522354125977
Epoch 1560, val loss: 0.544197678565979
Epoch 1570, training loss: 89.2135238647461 = 0.47150591015815735 + 10.0 * 8.874201774597168
Epoch 1570, val loss: 0.5439526438713074
Epoch 1580, training loss: 89.13150024414062 = 0.46927690505981445 + 10.0 * 8.866222381591797
Epoch 1580, val loss: 0.5411918759346008
Epoch 1590, training loss: 89.23788452148438 = 0.467252641916275 + 10.0 * 8.877062797546387
Epoch 1590, val loss: 0.5407919883728027
Epoch 1600, training loss: 89.23140716552734 = 0.4651237726211548 + 10.0 * 8.876627922058105
Epoch 1600, val loss: 0.53915935754776
Epoch 1610, training loss: 89.26792907714844 = 0.4628719687461853 + 10.0 * 8.880505561828613
Epoch 1610, val loss: 0.5378462672233582
Epoch 1620, training loss: 89.33065795898438 = 0.46061578392982483 + 10.0 * 8.887003898620605
Epoch 1620, val loss: 0.5366460084915161
Epoch 1630, training loss: 89.34583282470703 = 0.45827385783195496 + 10.0 * 8.888755798339844
Epoch 1630, val loss: 0.5352744460105896
Epoch 1640, training loss: 89.34080505371094 = 0.4560393691062927 + 10.0 * 8.888476371765137
Epoch 1640, val loss: 0.5340684652328491
Epoch 1650, training loss: 89.35553741455078 = 0.45374947786331177 + 10.0 * 8.890178680419922
Epoch 1650, val loss: 0.5326533913612366
Epoch 1660, training loss: 89.37250518798828 = 0.4515930712223053 + 10.0 * 8.892091751098633
Epoch 1660, val loss: 0.5318021774291992
Epoch 1670, training loss: 89.38926696777344 = 0.44940879940986633 + 10.0 * 8.893985748291016
Epoch 1670, val loss: 0.530546247959137
Epoch 1680, training loss: 89.40359497070312 = 0.4472196400165558 + 10.0 * 8.895637512207031
Epoch 1680, val loss: 0.5293883085250854
Epoch 1690, training loss: 89.38956451416016 = 0.4450918436050415 + 10.0 * 8.894447326660156
Epoch 1690, val loss: 0.5281158089637756
Epoch 1700, training loss: 89.38577270507812 = 0.4429777264595032 + 10.0 * 8.894279479980469
Epoch 1700, val loss: 0.527083158493042
Epoch 1710, training loss: 89.43309783935547 = 0.4409385621547699 + 10.0 * 8.899215698242188
Epoch 1710, val loss: 0.5262343883514404
Epoch 1720, training loss: 89.503173828125 = 0.43887969851493835 + 10.0 * 8.906429290771484
Epoch 1720, val loss: 0.5251787304878235
Epoch 1730, training loss: 89.40711975097656 = 0.43708547949790955 + 10.0 * 8.897003173828125
Epoch 1730, val loss: 0.5243455767631531
Epoch 1740, training loss: 89.38215637207031 = 0.4349435567855835 + 10.0 * 8.894721031188965
Epoch 1740, val loss: 0.5230047702789307
Epoch 1750, training loss: 89.44351196289062 = 0.43306228518486023 + 10.0 * 8.901044845581055
Epoch 1750, val loss: 0.5222687125205994
Epoch 1760, training loss: 89.5077896118164 = 0.43107008934020996 + 10.0 * 8.907671928405762
Epoch 1760, val loss: 0.5213475823402405
Epoch 1770, training loss: 89.55017852783203 = 0.4290435016155243 + 10.0 * 8.912113189697266
Epoch 1770, val loss: 0.5204048156738281
Epoch 1780, training loss: 89.53631591796875 = 0.42717328667640686 + 10.0 * 8.910914421081543
Epoch 1780, val loss: 0.5194941759109497
Epoch 1790, training loss: 89.5683364868164 = 0.4251236021518707 + 10.0 * 8.914320945739746
Epoch 1790, val loss: 0.5187117457389832
Epoch 1800, training loss: 89.57434844970703 = 0.42310696840286255 + 10.0 * 8.915124893188477
Epoch 1800, val loss: 0.5174703598022461
Epoch 1810, training loss: 89.53907012939453 = 0.4211861491203308 + 10.0 * 8.911787986755371
Epoch 1810, val loss: 0.5168402791023254
Epoch 1820, training loss: 89.62975311279297 = 0.4193134307861328 + 10.0 * 8.92104434967041
Epoch 1820, val loss: 0.5159276127815247
Epoch 1830, training loss: 89.63800811767578 = 0.41737833619117737 + 10.0 * 8.922062873840332
Epoch 1830, val loss: 0.5151541829109192
Epoch 1840, training loss: 89.54722595214844 = 0.41556140780448914 + 10.0 * 8.913166999816895
Epoch 1840, val loss: 0.5140291452407837
Epoch 1850, training loss: 89.55399322509766 = 0.4137475788593292 + 10.0 * 8.914024353027344
Epoch 1850, val loss: 0.5133621692657471
Epoch 1860, training loss: 89.61433410644531 = 0.41192397475242615 + 10.0 * 8.92024040222168
Epoch 1860, val loss: 0.5128148198127747
Epoch 1870, training loss: 89.66303253173828 = 0.41007229685783386 + 10.0 * 8.92529582977295
Epoch 1870, val loss: 0.5119522213935852
Epoch 1880, training loss: 89.6784896850586 = 0.40827634930610657 + 10.0 * 8.927021026611328
Epoch 1880, val loss: 0.5111457109451294
Epoch 1890, training loss: 89.51563262939453 = 0.40702199935913086 + 10.0 * 8.910861015319824
Epoch 1890, val loss: 0.5129916071891785
Epoch 1900, training loss: 89.64422607421875 = 0.4060512185096741 + 10.0 * 8.92381763458252
Epoch 1900, val loss: 0.5093449950218201
Epoch 1910, training loss: 89.5916976928711 = 0.4039066731929779 + 10.0 * 8.918779373168945
Epoch 1910, val loss: 0.5102533102035522
Epoch 1920, training loss: 89.60366821289062 = 0.40208765864372253 + 10.0 * 8.920158386230469
Epoch 1920, val loss: 0.5091981887817383
Epoch 1930, training loss: 89.6780014038086 = 0.40024533867836 + 10.0 * 8.927775382995605
Epoch 1930, val loss: 0.5087473392486572
Epoch 1940, training loss: 89.7634506225586 = 0.3984427750110626 + 10.0 * 8.936500549316406
Epoch 1940, val loss: 0.5080207586288452
Epoch 1950, training loss: 89.78884887695312 = 0.39652520418167114 + 10.0 * 8.93923282623291
Epoch 1950, val loss: 0.5072587728500366
Epoch 1960, training loss: 89.81660461425781 = 0.3947189450263977 + 10.0 * 8.942188262939453
Epoch 1960, val loss: 0.506866991519928
Epoch 1970, training loss: 89.83695220947266 = 0.39290785789489746 + 10.0 * 8.944404602050781
Epoch 1970, val loss: 0.5063269734382629
Epoch 1980, training loss: 89.85263061523438 = 0.39114731550216675 + 10.0 * 8.946148872375488
Epoch 1980, val loss: 0.5057770013809204
Epoch 1990, training loss: 89.82805633544922 = 0.38939014077186584 + 10.0 * 8.943866729736328
Epoch 1990, val loss: 0.5054330229759216
Epoch 2000, training loss: 89.81947326660156 = 0.3876170516014099 + 10.0 * 8.943185806274414
Epoch 2000, val loss: 0.5047559142112732
Epoch 2010, training loss: 89.8882827758789 = 0.38587409257888794 + 10.0 * 8.950241088867188
Epoch 2010, val loss: 0.5042782425880432
Epoch 2020, training loss: 89.916259765625 = 0.3841612935066223 + 10.0 * 8.95320987701416
Epoch 2020, val loss: 0.5038487315177917
Epoch 2030, training loss: 89.92735290527344 = 0.38247737288475037 + 10.0 * 8.954487800598145
Epoch 2030, val loss: 0.5035288333892822
Epoch 2040, training loss: 89.95211791992188 = 0.3807327449321747 + 10.0 * 8.957139015197754
Epoch 2040, val loss: 0.5030663013458252
Epoch 2050, training loss: 89.91175079345703 = 0.3789593279361725 + 10.0 * 8.953279495239258
Epoch 2050, val loss: 0.5025193095207214
Epoch 2060, training loss: 89.92715454101562 = 0.3773011565208435 + 10.0 * 8.954984664916992
Epoch 2060, val loss: 0.502028226852417
Epoch 2070, training loss: 89.97948455810547 = 0.3756415843963623 + 10.0 * 8.960384368896484
Epoch 2070, val loss: 0.5017626881599426
Epoch 2080, training loss: 90.00845336914062 = 0.3739369213581085 + 10.0 * 8.963451385498047
Epoch 2080, val loss: 0.5011951923370361
Epoch 2090, training loss: 90.00911712646484 = 0.37223947048187256 + 10.0 * 8.963687896728516
Epoch 2090, val loss: 0.5008165836334229
Epoch 2100, training loss: 89.9971923828125 = 0.37061014771461487 + 10.0 * 8.962657928466797
Epoch 2100, val loss: 0.5007295608520508
Epoch 2110, training loss: 89.8884048461914 = 0.36922088265419006 + 10.0 * 8.95191764831543
Epoch 2110, val loss: 0.5011877417564392
Epoch 2120, training loss: 89.72649383544922 = 0.36786434054374695 + 10.0 * 8.93586254119873
Epoch 2120, val loss: 0.5015146136283875
Epoch 2130, training loss: 89.81879425048828 = 0.36667725443840027 + 10.0 * 8.945211410522461
Epoch 2130, val loss: 0.4994205832481384
Epoch 2140, training loss: 89.84207916259766 = 0.3651578724384308 + 10.0 * 8.947691917419434
Epoch 2140, val loss: 0.49976226687431335
Epoch 2150, training loss: 89.84523010253906 = 0.363582044839859 + 10.0 * 8.948164939880371
Epoch 2150, val loss: 0.500291645526886
Epoch 2160, training loss: 89.92744445800781 = 0.3618934154510498 + 10.0 * 8.956555366516113
Epoch 2160, val loss: 0.4994904100894928
Epoch 2170, training loss: 89.9972915649414 = 0.3601728677749634 + 10.0 * 8.963711738586426
Epoch 2170, val loss: 0.49878692626953125
Epoch 2180, training loss: 90.04962158203125 = 0.3584906756877899 + 10.0 * 8.96911334991455
Epoch 2180, val loss: 0.49877429008483887
Epoch 2190, training loss: 89.9801025390625 = 0.35693520307540894 + 10.0 * 8.962316513061523
Epoch 2190, val loss: 0.498940110206604
Epoch 2200, training loss: 90.0403060913086 = 0.35524430871009827 + 10.0 * 8.968505859375
Epoch 2200, val loss: 0.49867013096809387
Epoch 2210, training loss: 90.0887451171875 = 0.3536202609539032 + 10.0 * 8.973512649536133
Epoch 2210, val loss: 0.4983544647693634
Epoch 2220, training loss: 90.08207702636719 = 0.35198938846588135 + 10.0 * 8.97300910949707
Epoch 2220, val loss: 0.49801400303840637
Epoch 2230, training loss: 90.10710906982422 = 0.3503813147544861 + 10.0 * 8.975672721862793
Epoch 2230, val loss: 0.49800872802734375
Epoch 2240, training loss: 90.10853576660156 = 0.34883013367652893 + 10.0 * 8.975970268249512
Epoch 2240, val loss: 0.49795961380004883
Epoch 2250, training loss: 90.11705780029297 = 0.34725356101989746 + 10.0 * 8.976980209350586
Epoch 2250, val loss: 0.4976225197315216
Epoch 2260, training loss: 90.14889526367188 = 0.3457174301147461 + 10.0 * 8.980318069458008
Epoch 2260, val loss: 0.4976171851158142
Epoch 2270, training loss: 90.16908264160156 = 0.34413114190101624 + 10.0 * 8.982495307922363
Epoch 2270, val loss: 0.49753427505493164
Epoch 2280, training loss: 90.1700668334961 = 0.34255048632621765 + 10.0 * 8.982751846313477
Epoch 2280, val loss: 0.4974490702152252
Epoch 2290, training loss: 90.20455932617188 = 0.34103840589523315 + 10.0 * 8.98635196685791
Epoch 2290, val loss: 0.4973175823688507
Epoch 2300, training loss: 90.26241302490234 = 0.3394419252872467 + 10.0 * 8.992297172546387
Epoch 2300, val loss: 0.49728450179100037
Epoch 2310, training loss: 90.26153564453125 = 0.33791226148605347 + 10.0 * 8.992362022399902
Epoch 2310, val loss: 0.4973703622817993
Epoch 2320, training loss: 90.31461334228516 = 0.33638930320739746 + 10.0 * 8.997822761535645
Epoch 2320, val loss: 0.4973219037055969
Epoch 2330, training loss: 90.27503204345703 = 0.3348519504070282 + 10.0 * 8.994017601013184
Epoch 2330, val loss: 0.4970256984233856
Epoch 2340, training loss: 89.67166900634766 = 0.3338971734046936 + 10.0 * 8.93377685546875
Epoch 2340, val loss: 0.49778443574905396
Epoch 2350, training loss: 90.06889343261719 = 0.33290860056877136 + 10.0 * 8.97359848022461
Epoch 2350, val loss: 0.49749404191970825
Epoch 2360, training loss: 89.9603500366211 = 0.33160826563835144 + 10.0 * 8.962873458862305
Epoch 2360, val loss: 0.49916234612464905
Epoch 2370, training loss: 90.10400390625 = 0.3301442265510559 + 10.0 * 8.977385520935059
Epoch 2370, val loss: 0.49841052293777466
Epoch 2380, training loss: 90.19461822509766 = 0.32849597930908203 + 10.0 * 8.986612319946289
Epoch 2380, val loss: 0.49823740124702454
Epoch 2390, training loss: 90.28003692626953 = 0.3269760310649872 + 10.0 * 8.995306015014648
Epoch 2390, val loss: 0.49777230620384216
Epoch 2400, training loss: 90.34925842285156 = 0.32541972398757935 + 10.0 * 9.002384185791016
Epoch 2400, val loss: 0.49760890007019043
Epoch 2410, training loss: 90.37672424316406 = 0.32385507225990295 + 10.0 * 9.005287170410156
Epoch 2410, val loss: 0.49766552448272705
Epoch 2420, training loss: 90.37055206298828 = 0.32227128744125366 + 10.0 * 9.004827499389648
Epoch 2420, val loss: 0.4980929493904114
Epoch 2430, training loss: 90.401123046875 = 0.3207857310771942 + 10.0 * 9.008033752441406
Epoch 2430, val loss: 0.498102605342865
Epoch 2440, training loss: 90.41753387451172 = 0.31923583149909973 + 10.0 * 9.0098295211792
Epoch 2440, val loss: 0.4984508752822876
Epoch 2450, training loss: 90.46245574951172 = 0.3177206814289093 + 10.0 * 9.014473915100098
Epoch 2450, val loss: 0.4984278976917267
Epoch 2460, training loss: 90.43816375732422 = 0.3161734640598297 + 10.0 * 9.012199401855469
Epoch 2460, val loss: 0.49857544898986816
Epoch 2470, training loss: 90.43457794189453 = 0.3147168755531311 + 10.0 * 9.011985778808594
Epoch 2470, val loss: 0.4986293315887451
Epoch 2480, training loss: 90.47773742675781 = 0.3132614195346832 + 10.0 * 9.016447067260742
Epoch 2480, val loss: 0.498875230550766
Epoch 2490, training loss: 90.48809814453125 = 0.31170666217803955 + 10.0 * 9.01763916015625
Epoch 2490, val loss: 0.49989476799964905
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8088405797101449
0.819387089763095
=== training gcn model ===
Epoch 0, training loss: 103.43093872070312 = 1.1106866598129272 + 10.0 * 10.232025146484375
Epoch 0, val loss: 1.1112873554229736
Epoch 10, training loss: 99.3001708984375 = 1.1067205667495728 + 10.0 * 9.819345474243164
Epoch 10, val loss: 1.1073733568191528
Epoch 20, training loss: 97.514892578125 = 1.103013515472412 + 10.0 * 9.64118766784668
Epoch 20, val loss: 1.1036715507507324
Epoch 30, training loss: 96.14717102050781 = 1.0994151830673218 + 10.0 * 9.504775047302246
Epoch 30, val loss: 1.10009765625
Epoch 40, training loss: 95.0631103515625 = 1.0960168838500977 + 10.0 * 9.396709442138672
Epoch 40, val loss: 1.0967189073562622
Epoch 50, training loss: 94.1612548828125 = 1.0927754640579224 + 10.0 * 9.306848526000977
Epoch 50, val loss: 1.0934951305389404
Epoch 60, training loss: 93.40303802490234 = 1.0896799564361572 + 10.0 * 9.231335639953613
Epoch 60, val loss: 1.090417742729187
Epoch 70, training loss: 92.7590103149414 = 1.0867154598236084 + 10.0 * 9.167229652404785
Epoch 70, val loss: 1.087464690208435
Epoch 80, training loss: 92.20258331298828 = 1.0838539600372314 + 10.0 * 9.111872673034668
Epoch 80, val loss: 1.084618330001831
Epoch 90, training loss: 91.70848083496094 = 1.0811001062393188 + 10.0 * 9.062738418579102
Epoch 90, val loss: 1.0818794965744019
Epoch 100, training loss: 91.27935791015625 = 1.078446388244629 + 10.0 * 9.02009105682373
Epoch 100, val loss: 1.079244613647461
Epoch 110, training loss: 90.9051742553711 = 1.0758742094039917 + 10.0 * 8.982930183410645
Epoch 110, val loss: 1.0766853094100952
Epoch 120, training loss: 90.59258270263672 = 1.0733970403671265 + 10.0 * 8.951918601989746
Epoch 120, val loss: 1.0742210149765015
Epoch 130, training loss: 90.31171417236328 = 1.0709962844848633 + 10.0 * 8.924071311950684
Epoch 130, val loss: 1.0718390941619873
Epoch 140, training loss: 90.0942611694336 = 1.0686784982681274 + 10.0 * 8.902558326721191
Epoch 140, val loss: 1.0695388317108154
Epoch 150, training loss: 89.86595153808594 = 1.066422462463379 + 10.0 * 8.879953384399414
Epoch 150, val loss: 1.067315936088562
Epoch 160, training loss: 89.68177795410156 = 1.0642544031143188 + 10.0 * 8.8617525100708
Epoch 160, val loss: 1.0651601552963257
Epoch 170, training loss: 89.5191879272461 = 1.062155842781067 + 10.0 * 8.845703125
Epoch 170, val loss: 1.0630837678909302
Epoch 180, training loss: 89.38622283935547 = 1.0601462125778198 + 10.0 * 8.832608222961426
Epoch 180, val loss: 1.061081886291504
Epoch 190, training loss: 89.27873229980469 = 1.0581778287887573 + 10.0 * 8.82205581665039
Epoch 190, val loss: 1.059157133102417
Epoch 200, training loss: 89.1735610961914 = 1.056273102760315 + 10.0 * 8.811728477478027
Epoch 200, val loss: 1.0572842359542847
Epoch 210, training loss: 89.08051300048828 = 1.0544087886810303 + 10.0 * 8.802610397338867
Epoch 210, val loss: 1.0554440021514893
Epoch 220, training loss: 89.0358657836914 = 1.0526176691055298 + 10.0 * 8.798324584960938
Epoch 220, val loss: 1.0536837577819824
Epoch 230, training loss: 88.95938110351562 = 1.0508209466934204 + 10.0 * 8.79085636138916
Epoch 230, val loss: 1.0519160032272339
Epoch 240, training loss: 88.87959289550781 = 1.0490139722824097 + 10.0 * 8.783058166503906
Epoch 240, val loss: 1.0501642227172852
Epoch 250, training loss: 88.87451934814453 = 1.0472233295440674 + 10.0 * 8.782729148864746
Epoch 250, val loss: 1.0483958721160889
Epoch 260, training loss: 88.81024932861328 = 1.0454394817352295 + 10.0 * 8.776480674743652
Epoch 260, val loss: 1.0466411113739014
Epoch 270, training loss: 88.7762680053711 = 1.0435593128204346 + 10.0 * 8.773271560668945
Epoch 270, val loss: 1.0448178052902222
Epoch 280, training loss: 88.73009490966797 = 1.0416322946548462 + 10.0 * 8.76884651184082
Epoch 280, val loss: 1.042913794517517
Epoch 290, training loss: 88.7156982421875 = 1.0397309064865112 + 10.0 * 8.767596244812012
Epoch 290, val loss: 1.0410640239715576
Epoch 300, training loss: 88.6987533569336 = 1.037689208984375 + 10.0 * 8.766106605529785
Epoch 300, val loss: 1.039082646369934
Epoch 310, training loss: 88.6810302734375 = 1.0355736017227173 + 10.0 * 8.764545440673828
Epoch 310, val loss: 1.0369720458984375
Epoch 320, training loss: 88.65435028076172 = 1.033286690711975 + 10.0 * 8.762105941772461
Epoch 320, val loss: 1.0346492528915405
Epoch 330, training loss: 88.6729736328125 = 1.030961513519287 + 10.0 * 8.764201164245605
Epoch 330, val loss: 1.0324921607971191
Epoch 340, training loss: 88.63614654541016 = 1.0282597541809082 + 10.0 * 8.760788917541504
Epoch 340, val loss: 1.0299025774002075
Epoch 350, training loss: 88.57947540283203 = 1.0257298946380615 + 10.0 * 8.755374908447266
Epoch 350, val loss: 1.0273157358169556
Epoch 360, training loss: 88.64497375488281 = 1.0229665040969849 + 10.0 * 8.762201309204102
Epoch 360, val loss: 1.0246399641036987
Epoch 370, training loss: 88.61734008789062 = 1.019960641860962 + 10.0 * 8.759737968444824
Epoch 370, val loss: 1.021728277206421
Epoch 380, training loss: 88.61720275878906 = 1.0167535543441772 + 10.0 * 8.760045051574707
Epoch 380, val loss: 1.0186249017715454
Epoch 390, training loss: 88.60077667236328 = 1.0134488344192505 + 10.0 * 8.758732795715332
Epoch 390, val loss: 1.0153634548187256
Epoch 400, training loss: 88.60177612304688 = 1.0097633600234985 + 10.0 * 8.759201049804688
Epoch 400, val loss: 1.0118181705474854
Epoch 410, training loss: 88.60902404785156 = 1.0059479475021362 + 10.0 * 8.760307312011719
Epoch 410, val loss: 1.0080633163452148
Epoch 420, training loss: 88.61046600341797 = 1.0018837451934814 + 10.0 * 8.760858535766602
Epoch 420, val loss: 1.004090666770935
Epoch 430, training loss: 88.63675689697266 = 0.9975503087043762 + 10.0 * 8.763920783996582
Epoch 430, val loss: 0.9998524188995361
Epoch 440, training loss: 88.62049102783203 = 0.9928238987922668 + 10.0 * 8.76276683807373
Epoch 440, val loss: 0.9952841401100159
Epoch 450, training loss: 88.64253234863281 = 0.9878664612770081 + 10.0 * 8.765466690063477
Epoch 450, val loss: 0.9904261231422424
Epoch 460, training loss: 88.64203643798828 = 0.9826422929763794 + 10.0 * 8.765939712524414
Epoch 460, val loss: 0.9852135181427002
Epoch 470, training loss: 88.67585754394531 = 0.9769561886787415 + 10.0 * 8.769889831542969
Epoch 470, val loss: 0.9797699451446533
Epoch 480, training loss: 88.6905517578125 = 0.9710524082183838 + 10.0 * 8.771949768066406
Epoch 480, val loss: 0.9740864634513855
Epoch 490, training loss: 88.69817352294922 = 0.9645571708679199 + 10.0 * 8.773362159729004
Epoch 490, val loss: 0.9678443670272827
Epoch 500, training loss: 88.68373107910156 = 0.9576594829559326 + 10.0 * 8.772607803344727
Epoch 500, val loss: 0.9612023234367371
Epoch 510, training loss: 88.70545196533203 = 0.9504454731941223 + 10.0 * 8.775500297546387
Epoch 510, val loss: 0.954189121723175
Epoch 520, training loss: 88.6919174194336 = 0.9426340460777283 + 10.0 * 8.774928092956543
Epoch 520, val loss: 0.9466881155967712
Epoch 530, training loss: 88.68463134765625 = 0.9343996644020081 + 10.0 * 8.775022506713867
Epoch 530, val loss: 0.9388445615768433
Epoch 540, training loss: 88.71307373046875 = 0.9259607195854187 + 10.0 * 8.778711318969727
Epoch 540, val loss: 0.9307888150215149
Epoch 550, training loss: 88.76571655273438 = 0.9170433282852173 + 10.0 * 8.784867286682129
Epoch 550, val loss: 0.9223399758338928
Epoch 560, training loss: 88.77095031738281 = 0.907818615436554 + 10.0 * 8.7863130569458
Epoch 560, val loss: 0.91353440284729
Epoch 570, training loss: 88.7483901977539 = 0.8982407450675964 + 10.0 * 8.785015106201172
Epoch 570, val loss: 0.9043429493904114
Epoch 580, training loss: 88.75325012207031 = 0.8884590268135071 + 10.0 * 8.786478996276855
Epoch 580, val loss: 0.8950906991958618
Epoch 590, training loss: 88.73822021484375 = 0.8784272074699402 + 10.0 * 8.785979270935059
Epoch 590, val loss: 0.8855937123298645
Epoch 600, training loss: 88.70225524902344 = 0.8681183457374573 + 10.0 * 8.783413887023926
Epoch 600, val loss: 0.8760424852371216
Epoch 610, training loss: 88.78691101074219 = 0.8580340147018433 + 10.0 * 8.792887687683105
Epoch 610, val loss: 0.8665417432785034
Epoch 620, training loss: 88.80473327636719 = 0.847801148891449 + 10.0 * 8.795693397521973
Epoch 620, val loss: 0.8570236563682556
Epoch 630, training loss: 88.7939453125 = 0.837396502494812 + 10.0 * 8.795655250549316
Epoch 630, val loss: 0.8473420143127441
Epoch 640, training loss: 88.78933715820312 = 0.8270429372787476 + 10.0 * 8.796229362487793
Epoch 640, val loss: 0.8377149701118469
Epoch 650, training loss: 88.82538604736328 = 0.8169516324996948 + 10.0 * 8.800844192504883
Epoch 650, val loss: 0.8284135460853577
Epoch 660, training loss: 88.84174346923828 = 0.8066924214363098 + 10.0 * 8.803504943847656
Epoch 660, val loss: 0.8189119696617126
Epoch 670, training loss: 88.85774230957031 = 0.7965984344482422 + 10.0 * 8.806114196777344
Epoch 670, val loss: 0.8096792101860046
Epoch 680, training loss: 88.85856628417969 = 0.7866093516349792 + 10.0 * 8.807195663452148
Epoch 680, val loss: 0.8004539608955383
Epoch 690, training loss: 88.84298706054688 = 0.7768505215644836 + 10.0 * 8.80661392211914
Epoch 690, val loss: 0.7915188670158386
Epoch 700, training loss: 88.86343383789062 = 0.7672311663627625 + 10.0 * 8.809619903564453
Epoch 700, val loss: 0.7828416228294373
Epoch 710, training loss: 88.84046936035156 = 0.7572042942047119 + 10.0 * 8.808326721191406
Epoch 710, val loss: 0.7734301090240479
Epoch 720, training loss: 89.06060791015625 = 0.7491316795349121 + 10.0 * 8.831148147583008
Epoch 720, val loss: 0.7666220664978027
Epoch 730, training loss: 88.78285217285156 = 0.7395018339157104 + 10.0 * 8.80433464050293
Epoch 730, val loss: 0.7576431035995483
Epoch 740, training loss: 88.71746826171875 = 0.7305437326431274 + 10.0 * 8.79869270324707
Epoch 740, val loss: 0.7495129108428955
Epoch 750, training loss: 88.81334686279297 = 0.7220290899276733 + 10.0 * 8.809131622314453
Epoch 750, val loss: 0.7419361472129822
Epoch 760, training loss: 88.8384780883789 = 0.7134702801704407 + 10.0 * 8.812500953674316
Epoch 760, val loss: 0.7343407273292542
Epoch 770, training loss: 88.86101531982422 = 0.7049922943115234 + 10.0 * 8.81560230255127
Epoch 770, val loss: 0.7266904711723328
Epoch 780, training loss: 88.87169647216797 = 0.6966909766197205 + 10.0 * 8.817500114440918
Epoch 780, val loss: 0.7190977334976196
Epoch 790, training loss: 88.9041519165039 = 0.6886817812919617 + 10.0 * 8.82154655456543
Epoch 790, val loss: 0.71200031042099
Epoch 800, training loss: 88.90448760986328 = 0.6809315085411072 + 10.0 * 8.822355270385742
Epoch 800, val loss: 0.7050930857658386
Epoch 810, training loss: 88.87248229980469 = 0.6734766364097595 + 10.0 * 8.819900512695312
Epoch 810, val loss: 0.6982662677764893
Epoch 820, training loss: 88.89376831054688 = 0.6662023663520813 + 10.0 * 8.82275676727295
Epoch 820, val loss: 0.6919932961463928
Epoch 830, training loss: 88.9124755859375 = 0.6595256924629211 + 10.0 * 8.825295448303223
Epoch 830, val loss: 0.6859930157661438
Epoch 840, training loss: 88.97608947753906 = 0.652895450592041 + 10.0 * 8.832319259643555
Epoch 840, val loss: 0.6802605390548706
Epoch 850, training loss: 88.960205078125 = 0.6465610265731812 + 10.0 * 8.831364631652832
Epoch 850, val loss: 0.6746467351913452
Epoch 860, training loss: 89.00116729736328 = 0.6405075192451477 + 10.0 * 8.836065292358398
Epoch 860, val loss: 0.669428288936615
Epoch 870, training loss: 89.01616668701172 = 0.6348980069160461 + 10.0 * 8.838127136230469
Epoch 870, val loss: 0.6645834445953369
Epoch 880, training loss: 89.00288391113281 = 0.629352867603302 + 10.0 * 8.837352752685547
Epoch 880, val loss: 0.6597149968147278
Epoch 890, training loss: 89.01591491699219 = 0.6240499019622803 + 10.0 * 8.839186668395996
Epoch 890, val loss: 0.655174970626831
Epoch 900, training loss: 89.04157257080078 = 0.6191796660423279 + 10.0 * 8.842239379882812
Epoch 900, val loss: 0.651049792766571
Epoch 910, training loss: 89.0668716430664 = 0.6145049333572388 + 10.0 * 8.845236778259277
Epoch 910, val loss: 0.6470850110054016
Epoch 920, training loss: 89.08090209960938 = 0.6100348830223083 + 10.0 * 8.847086906433105
Epoch 920, val loss: 0.6432875990867615
Epoch 930, training loss: 89.08618927001953 = 0.6056224703788757 + 10.0 * 8.84805679321289
Epoch 930, val loss: 0.6395342946052551
Epoch 940, training loss: 89.11051940917969 = 0.6014920473098755 + 10.0 * 8.850902557373047
Epoch 940, val loss: 0.6361460089683533
Epoch 950, training loss: 89.15214538574219 = 0.5975476503372192 + 10.0 * 8.855459213256836
Epoch 950, val loss: 0.6328102350234985
Epoch 960, training loss: 89.10359191894531 = 0.5936899185180664 + 10.0 * 8.850990295410156
Epoch 960, val loss: 0.629727840423584
Epoch 970, training loss: 89.0943603515625 = 0.5900354385375977 + 10.0 * 8.850432395935059
Epoch 970, val loss: 0.6268749237060547
Epoch 980, training loss: 89.14148712158203 = 0.5865805745124817 + 10.0 * 8.855490684509277
Epoch 980, val loss: 0.6240925788879395
Epoch 990, training loss: 89.09970092773438 = 0.5830956697463989 + 10.0 * 8.851659774780273
Epoch 990, val loss: 0.6213310956954956
Epoch 1000, training loss: 89.11546325683594 = 0.5799533128738403 + 10.0 * 8.853550910949707
Epoch 1000, val loss: 0.61888188123703
Epoch 1010, training loss: 89.10513305664062 = 0.5767974257469177 + 10.0 * 8.85283374786377
Epoch 1010, val loss: 0.6164712309837341
Epoch 1020, training loss: 89.19392395019531 = 0.5738880634307861 + 10.0 * 8.862003326416016
Epoch 1020, val loss: 0.6142703294754028
Epoch 1030, training loss: 89.23310089111328 = 0.5710487961769104 + 10.0 * 8.866205215454102
Epoch 1030, val loss: 0.6121435761451721
Epoch 1040, training loss: 89.22679138183594 = 0.5682045221328735 + 10.0 * 8.86585807800293
Epoch 1040, val loss: 0.610055148601532
Epoch 1050, training loss: 89.2536392211914 = 0.5655324459075928 + 10.0 * 8.868810653686523
Epoch 1050, val loss: 0.6080365777015686
Epoch 1060, training loss: 89.25946044921875 = 0.5628592371940613 + 10.0 * 8.869660377502441
Epoch 1060, val loss: 0.6061338782310486
Epoch 1070, training loss: 89.24541473388672 = 0.5602350234985352 + 10.0 * 8.868517875671387
Epoch 1070, val loss: 0.6043189764022827
Epoch 1080, training loss: 89.23399353027344 = 0.5576907396316528 + 10.0 * 8.867630004882812
Epoch 1080, val loss: 0.6024104952812195
Epoch 1090, training loss: 89.30439758300781 = 0.5552673935890198 + 10.0 * 8.874913215637207
Epoch 1090, val loss: 0.6006133556365967
Epoch 1100, training loss: 89.23898315429688 = 0.5529274940490723 + 10.0 * 8.868605613708496
Epoch 1100, val loss: 0.5991256237030029
Epoch 1110, training loss: 89.2806396484375 = 0.5504822731018066 + 10.0 * 8.873015403747559
Epoch 1110, val loss: 0.5976934432983398
Epoch 1120, training loss: 89.33904266357422 = 0.5481650829315186 + 10.0 * 8.879087448120117
Epoch 1120, val loss: 0.5957911610603333
Epoch 1130, training loss: 89.32543182373047 = 0.545769453048706 + 10.0 * 8.877965927124023
Epoch 1130, val loss: 0.5942395925521851
Epoch 1140, training loss: 89.39955139160156 = 0.543532133102417 + 10.0 * 8.885601997375488
Epoch 1140, val loss: 0.5927228927612305
Epoch 1150, training loss: 89.4208984375 = 0.541233241558075 + 10.0 * 8.88796615600586
Epoch 1150, val loss: 0.591161847114563
Epoch 1160, training loss: 89.48297119140625 = 0.5390672087669373 + 10.0 * 8.894390106201172
Epoch 1160, val loss: 0.5900599360466003
Epoch 1170, training loss: 89.3465347290039 = 0.5367131233215332 + 10.0 * 8.880982398986816
Epoch 1170, val loss: 0.5881397724151611
Epoch 1180, training loss: 89.4229965209961 = 0.534756064414978 + 10.0 * 8.888823509216309
Epoch 1180, val loss: 0.5870091319084167
Epoch 1190, training loss: 89.48085021972656 = 0.5325590372085571 + 10.0 * 8.894828796386719
Epoch 1190, val loss: 0.5856794714927673
Epoch 1200, training loss: 89.5226821899414 = 0.5304093360900879 + 10.0 * 8.899227142333984
Epoch 1200, val loss: 0.5842810869216919
Epoch 1210, training loss: 89.54904174804688 = 0.5281873345375061 + 10.0 * 8.902085304260254
Epoch 1210, val loss: 0.582935094833374
Epoch 1220, training loss: 89.19219207763672 = 0.5257307887077332 + 10.0 * 8.866645812988281
Epoch 1220, val loss: 0.5811266899108887
Epoch 1230, training loss: 89.52424621582031 = 0.5237823128700256 + 10.0 * 8.900046348571777
Epoch 1230, val loss: 0.5806006193161011
Epoch 1240, training loss: 89.56643676757812 = 0.5218324065208435 + 10.0 * 8.904459953308105
Epoch 1240, val loss: 0.5788646936416626
Epoch 1250, training loss: 89.59156036376953 = 0.5197417140007019 + 10.0 * 8.907181739807129
Epoch 1250, val loss: 0.577850878238678
Epoch 1260, training loss: 89.66036987304688 = 0.5175594091415405 + 10.0 * 8.914280891418457
Epoch 1260, val loss: 0.5764575004577637
Epoch 1270, training loss: 89.6488265991211 = 0.5153154134750366 + 10.0 * 8.913351058959961
Epoch 1270, val loss: 0.5752384662628174
Epoch 1280, training loss: 89.69004821777344 = 0.513094961643219 + 10.0 * 8.917695999145508
Epoch 1280, val loss: 0.5738988518714905
Epoch 1290, training loss: 89.72502136230469 = 0.5109257698059082 + 10.0 * 8.921409606933594
Epoch 1290, val loss: 0.5725278258323669
Epoch 1300, training loss: 89.72824096679688 = 0.5087222456932068 + 10.0 * 8.921952247619629
Epoch 1300, val loss: 0.5711783766746521
Epoch 1310, training loss: 89.74364471435547 = 0.5065125823020935 + 10.0 * 8.923712730407715
Epoch 1310, val loss: 0.569980800151825
Epoch 1320, training loss: 89.64124298095703 = 0.5046155452728271 + 10.0 * 8.913662910461426
Epoch 1320, val loss: 0.5688095092773438
Epoch 1330, training loss: 89.57510375976562 = 0.5023328065872192 + 10.0 * 8.90727710723877
Epoch 1330, val loss: 0.5674331188201904
Epoch 1340, training loss: 89.7418441772461 = 0.5003906488418579 + 10.0 * 8.924145698547363
Epoch 1340, val loss: 0.5666709542274475
Epoch 1350, training loss: 89.82070922851562 = 0.4983871281147003 + 10.0 * 8.932231903076172
Epoch 1350, val loss: 0.5652820467948914
Epoch 1360, training loss: 89.84451293945312 = 0.496207594871521 + 10.0 * 8.934830665588379
Epoch 1360, val loss: 0.564267635345459
Epoch 1370, training loss: 89.85164642333984 = 0.49393177032470703 + 10.0 * 8.935770988464355
Epoch 1370, val loss: 0.5628014802932739
Epoch 1380, training loss: 89.89013671875 = 0.4916976988315582 + 10.0 * 8.939844131469727
Epoch 1380, val loss: 0.5615477561950684
Epoch 1390, training loss: 89.92072296142578 = 0.4895244836807251 + 10.0 * 8.943120002746582
Epoch 1390, val loss: 0.5603675246238708
Epoch 1400, training loss: 89.88907623291016 = 0.4872901141643524 + 10.0 * 8.940178871154785
Epoch 1400, val loss: 0.5590149164199829
Epoch 1410, training loss: 89.91206359863281 = 0.485042542219162 + 10.0 * 8.942702293395996
Epoch 1410, val loss: 0.557934045791626
Epoch 1420, training loss: 89.95243072509766 = 0.48281747102737427 + 10.0 * 8.946961402893066
Epoch 1420, val loss: 0.5565975308418274
Epoch 1430, training loss: 89.96065521240234 = 0.48056191205978394 + 10.0 * 8.948009490966797
Epoch 1430, val loss: 0.555362343788147
Epoch 1440, training loss: 89.96176147460938 = 0.47834718227386475 + 10.0 * 8.948341369628906
Epoch 1440, val loss: 0.5541533827781677
Epoch 1450, training loss: 89.97228240966797 = 0.4761195182800293 + 10.0 * 8.949616432189941
Epoch 1450, val loss: 0.5529831051826477
Epoch 1460, training loss: 89.98444366455078 = 0.4739859700202942 + 10.0 * 8.951045989990234
Epoch 1460, val loss: 0.5517492294311523
Epoch 1470, training loss: 90.02122497558594 = 0.47170203924179077 + 10.0 * 8.954952239990234
Epoch 1470, val loss: 0.5505321025848389
Epoch 1480, training loss: 90.01180267333984 = 0.469525545835495 + 10.0 * 8.954227447509766
Epoch 1480, val loss: 0.5492238402366638
Epoch 1490, training loss: 90.05479431152344 = 0.46726810932159424 + 10.0 * 8.958752632141113
Epoch 1490, val loss: 0.5481567978858948
Epoch 1500, training loss: 90.06487274169922 = 0.4650026857852936 + 10.0 * 8.959986686706543
Epoch 1500, val loss: 0.5467379093170166
Epoch 1510, training loss: 89.00172424316406 = 0.46350574493408203 + 10.0 * 8.853821754455566
Epoch 1510, val loss: 0.5435304641723633
Epoch 1520, training loss: 89.07750701904297 = 0.462922066450119 + 10.0 * 8.861458778381348
Epoch 1520, val loss: 0.5477432608604431
Epoch 1530, training loss: 89.65763092041016 = 0.46012839674949646 + 10.0 * 8.919750213623047
Epoch 1530, val loss: 0.543997049331665
Epoch 1540, training loss: 89.67724609375 = 0.45769256353378296 + 10.0 * 8.921955108642578
Epoch 1540, val loss: 0.5423247218132019
Epoch 1550, training loss: 89.77942657470703 = 0.4555462896823883 + 10.0 * 8.932388305664062
Epoch 1550, val loss: 0.5423678755760193
Epoch 1560, training loss: 89.7982177734375 = 0.453351229429245 + 10.0 * 8.934486389160156
Epoch 1560, val loss: 0.5403279662132263
Epoch 1570, training loss: 89.9176025390625 = 0.4512498378753662 + 10.0 * 8.946635246276855
Epoch 1570, val loss: 0.5400660037994385
Epoch 1580, training loss: 90.04743957519531 = 0.44911694526672363 + 10.0 * 8.959832191467285
Epoch 1580, val loss: 0.5381473302841187
Epoch 1590, training loss: 90.10551452636719 = 0.44679275155067444 + 10.0 * 8.965871810913086
Epoch 1590, val loss: 0.5372358560562134
Epoch 1600, training loss: 90.2217025756836 = 0.44459113478660583 + 10.0 * 8.97771167755127
Epoch 1600, val loss: 0.5360837578773499
Epoch 1610, training loss: 90.18273162841797 = 0.442278653383255 + 10.0 * 8.974045753479004
Epoch 1610, val loss: 0.5348209142684937
Epoch 1620, training loss: 90.26730346679688 = 0.4401121139526367 + 10.0 * 8.982719421386719
Epoch 1620, val loss: 0.5339676737785339
Epoch 1630, training loss: 90.32604217529297 = 0.43793198466300964 + 10.0 * 8.988810539245605
Epoch 1630, val loss: 0.5328816771507263
Epoch 1640, training loss: 90.30628967285156 = 0.4357731342315674 + 10.0 * 8.987051010131836
Epoch 1640, val loss: 0.5319202542304993
Epoch 1650, training loss: 90.37671661376953 = 0.4336450695991516 + 10.0 * 8.994306564331055
Epoch 1650, val loss: 0.5309281945228577
Epoch 1660, training loss: 90.41305541992188 = 0.431440144777298 + 10.0 * 8.998161315917969
Epoch 1660, val loss: 0.5299918055534363
Epoch 1670, training loss: 90.42632293701172 = 0.42925888299942017 + 10.0 * 8.999706268310547
Epoch 1670, val loss: 0.5287885069847107
Epoch 1680, training loss: 90.44718933105469 = 0.42712679505348206 + 10.0 * 9.002006530761719
Epoch 1680, val loss: 0.527705192565918
Epoch 1690, training loss: 90.47236633300781 = 0.4249885678291321 + 10.0 * 9.004737854003906
Epoch 1690, val loss: 0.5269333720207214
Epoch 1700, training loss: 90.48898315429688 = 0.4228518307209015 + 10.0 * 9.006612777709961
Epoch 1700, val loss: 0.5259860157966614
Epoch 1710, training loss: 90.51985931396484 = 0.4207551181316376 + 10.0 * 9.009910583496094
Epoch 1710, val loss: 0.5252621173858643
Epoch 1720, training loss: 90.56594848632812 = 0.41868293285369873 + 10.0 * 9.014726638793945
Epoch 1720, val loss: 0.5245189666748047
Epoch 1730, training loss: 90.58019256591797 = 0.4166540205478668 + 10.0 * 9.016353607177734
Epoch 1730, val loss: 0.5237126350402832
Epoch 1740, training loss: 90.4333724975586 = 0.4145311415195465 + 10.0 * 9.001884460449219
Epoch 1740, val loss: 0.5229796171188354
Epoch 1750, training loss: 90.47605895996094 = 0.41281822323799133 + 10.0 * 9.00632381439209
Epoch 1750, val loss: 0.5220937728881836
Epoch 1760, training loss: 90.38135528564453 = 0.410893976688385 + 10.0 * 8.997045516967773
Epoch 1760, val loss: 0.5223097801208496
Epoch 1770, training loss: 90.45516967773438 = 0.4089359939098358 + 10.0 * 9.004623413085938
Epoch 1770, val loss: 0.5208025574684143
Epoch 1780, training loss: 90.49092102050781 = 0.40692415833473206 + 10.0 * 9.008399963378906
Epoch 1780, val loss: 0.520246148109436
Epoch 1790, training loss: 90.58527374267578 = 0.4049554467201233 + 10.0 * 9.01803207397461
Epoch 1790, val loss: 0.5195086598396301
Epoch 1800, training loss: 90.6495590209961 = 0.40301409363746643 + 10.0 * 9.024654388427734
Epoch 1800, val loss: 0.5189973711967468
Epoch 1810, training loss: 90.65280151367188 = 0.40103578567504883 + 10.0 * 9.025176048278809
Epoch 1810, val loss: 0.5181373953819275
Epoch 1820, training loss: 90.69849395751953 = 0.3990897536277771 + 10.0 * 9.029940605163574
Epoch 1820, val loss: 0.5179188847541809
Epoch 1830, training loss: 90.76007843017578 = 0.39712971448898315 + 10.0 * 9.036294937133789
Epoch 1830, val loss: 0.5174600481987
Epoch 1840, training loss: 90.74397277832031 = 0.39516472816467285 + 10.0 * 9.034880638122559
Epoch 1840, val loss: 0.5169730186462402
Epoch 1850, training loss: 90.7724838256836 = 0.39324450492858887 + 10.0 * 9.037923812866211
Epoch 1850, val loss: 0.5164772272109985
Epoch 1860, training loss: 90.7448501586914 = 0.39131370186805725 + 10.0 * 9.035353660583496
Epoch 1860, val loss: 0.5158343315124512
Epoch 1870, training loss: 90.75773620605469 = 0.389494389295578 + 10.0 * 9.036824226379395
Epoch 1870, val loss: 0.5150308609008789
Epoch 1880, training loss: 90.83222198486328 = 0.3876267969608307 + 10.0 * 9.044459342956543
Epoch 1880, val loss: 0.5151535868644714
Epoch 1890, training loss: 90.86920928955078 = 0.38570544123649597 + 10.0 * 9.04835033416748
Epoch 1890, val loss: 0.5145466327667236
Epoch 1900, training loss: 90.83920288085938 = 0.38383814692497253 + 10.0 * 9.045536994934082
Epoch 1900, val loss: 0.5143630504608154
Epoch 1910, training loss: 90.88834381103516 = 0.38191813230514526 + 10.0 * 9.050642013549805
Epoch 1910, val loss: 0.5137057304382324
Epoch 1920, training loss: 90.93112182617188 = 0.3799457252025604 + 10.0 * 9.0551176071167
Epoch 1920, val loss: 0.5132598876953125
Epoch 1930, training loss: 90.93641662597656 = 0.3779568076133728 + 10.0 * 9.055845260620117
Epoch 1930, val loss: 0.5125613808631897
Epoch 1940, training loss: 90.86600494384766 = 0.3760285973548889 + 10.0 * 9.04899787902832
Epoch 1940, val loss: 0.5119979977607727
Epoch 1950, training loss: 90.91655731201172 = 0.37412309646606445 + 10.0 * 9.054243087768555
Epoch 1950, val loss: 0.5116878747940063
Epoch 1960, training loss: 90.9847183227539 = 0.37222257256507874 + 10.0 * 9.061249732971191
Epoch 1960, val loss: 0.5115611553192139
Epoch 1970, training loss: 91.00224304199219 = 0.3703120946884155 + 10.0 * 9.063192367553711
Epoch 1970, val loss: 0.5108733773231506
Epoch 1980, training loss: 90.66529846191406 = 0.36869460344314575 + 10.0 * 9.02966022491455
Epoch 1980, val loss: 0.5101222395896912
Epoch 1990, training loss: 90.80411529541016 = 0.3670283854007721 + 10.0 * 9.043708801269531
Epoch 1990, val loss: 0.5093148350715637
Epoch 2000, training loss: 90.83719635009766 = 0.3651559054851532 + 10.0 * 9.04720401763916
Epoch 2000, val loss: 0.5107923746109009
Epoch 2010, training loss: 90.86679077148438 = 0.36326974630355835 + 10.0 * 9.050352096557617
Epoch 2010, val loss: 0.5098341107368469
Epoch 2020, training loss: 90.93453979492188 = 0.3614068925380707 + 10.0 * 9.057313919067383
Epoch 2020, val loss: 0.5094721913337708
Epoch 2030, training loss: 90.98834228515625 = 0.3595946133136749 + 10.0 * 9.062874794006348
Epoch 2030, val loss: 0.5088669657707214
Epoch 2040, training loss: 91.02863311767578 = 0.3577120900154114 + 10.0 * 9.067091941833496
Epoch 2040, val loss: 0.5084797143936157
Epoch 2050, training loss: 91.05166625976562 = 0.3558191657066345 + 10.0 * 9.069584846496582
Epoch 2050, val loss: 0.5080625414848328
Epoch 2060, training loss: 91.05970764160156 = 0.35397282242774963 + 10.0 * 9.070573806762695
Epoch 2060, val loss: 0.5079131126403809
Epoch 2070, training loss: 91.05843353271484 = 0.3521474003791809 + 10.0 * 9.07062816619873
Epoch 2070, val loss: 0.5076178312301636
Epoch 2080, training loss: 91.09454345703125 = 0.35032355785369873 + 10.0 * 9.074421882629395
Epoch 2080, val loss: 0.5073565244674683
Epoch 2090, training loss: 91.11299133300781 = 0.34846410155296326 + 10.0 * 9.076452255249023
Epoch 2090, val loss: 0.5069116353988647
Epoch 2100, training loss: 91.11026763916016 = 0.34665682911872864 + 10.0 * 9.076360702514648
Epoch 2100, val loss: 0.5068562030792236
Epoch 2110, training loss: 91.13286590576172 = 0.34487009048461914 + 10.0 * 9.0787992477417
Epoch 2110, val loss: 0.5065497159957886
Epoch 2120, training loss: 91.15921783447266 = 0.3430703282356262 + 10.0 * 9.08161449432373
Epoch 2120, val loss: 0.5064754486083984
Epoch 2130, training loss: 91.16177368164062 = 0.3413037955760956 + 10.0 * 9.082047462463379
Epoch 2130, val loss: 0.5062369108200073
Epoch 2140, training loss: 91.19483947753906 = 0.3395097553730011 + 10.0 * 9.085533142089844
Epoch 2140, val loss: 0.5062135457992554
Epoch 2150, training loss: 91.2133560180664 = 0.337702214717865 + 10.0 * 9.087565422058105
Epoch 2150, val loss: 0.5058218240737915
Epoch 2160, training loss: 91.21102142333984 = 0.3358968198299408 + 10.0 * 9.087512016296387
Epoch 2160, val loss: 0.5058655142784119
Epoch 2170, training loss: 91.2071762084961 = 0.33410748839378357 + 10.0 * 9.08730697631836
Epoch 2170, val loss: 0.5061688423156738
Epoch 2180, training loss: 91.20413208007812 = 0.33238640427589417 + 10.0 * 9.087174415588379
Epoch 2180, val loss: 0.5056943297386169
Epoch 2190, training loss: 91.25513458251953 = 0.3306182026863098 + 10.0 * 9.092451095581055
Epoch 2190, val loss: 0.5055531859397888
Epoch 2200, training loss: 91.28833770751953 = 0.32881852984428406 + 10.0 * 9.095952033996582
Epoch 2200, val loss: 0.5055390000343323
Epoch 2210, training loss: 91.21832275390625 = 0.327084481716156 + 10.0 * 9.089123725891113
Epoch 2210, val loss: 0.505695104598999
Epoch 2220, training loss: 91.21512603759766 = 0.3253188133239746 + 10.0 * 9.088980674743652
Epoch 2220, val loss: 0.50639808177948
Epoch 2230, training loss: 91.25147247314453 = 0.32362207770347595 + 10.0 * 9.092784881591797
Epoch 2230, val loss: 0.5064643621444702
Epoch 2240, training loss: 91.27890014648438 = 0.321850061416626 + 10.0 * 9.095705032348633
Epoch 2240, val loss: 0.5057634711265564
Epoch 2250, training loss: 91.26885223388672 = 0.3200753927230835 + 10.0 * 9.094877243041992
Epoch 2250, val loss: 0.5060252547264099
Epoch 2260, training loss: 91.3065414428711 = 0.31833702325820923 + 10.0 * 9.098820686340332
Epoch 2260, val loss: 0.5058256983757019
Epoch 2270, training loss: 91.3452377319336 = 0.31653764843940735 + 10.0 * 9.102869987487793
Epoch 2270, val loss: 0.5061913132667542
Epoch 2280, training loss: 91.36106872558594 = 0.31477639079093933 + 10.0 * 9.104629516601562
Epoch 2280, val loss: 0.5058357119560242
Epoch 2290, training loss: 91.30672454833984 = 0.3131482005119324 + 10.0 * 9.099357604980469
Epoch 2290, val loss: 0.505604088306427
Epoch 2300, training loss: 90.95537567138672 = 0.31276029348373413 + 10.0 * 9.064261436462402
Epoch 2300, val loss: 0.5051141977310181
Epoch 2310, training loss: 90.69688415527344 = 0.31061169505119324 + 10.0 * 9.038627624511719
Epoch 2310, val loss: 0.5057321190834045
Epoch 2320, training loss: 90.92974853515625 = 0.30929428339004517 + 10.0 * 9.062045097351074
Epoch 2320, val loss: 0.5061646103858948
Epoch 2330, training loss: 90.9654541015625 = 0.30734604597091675 + 10.0 * 9.065811157226562
Epoch 2330, val loss: 0.5055935978889465
Epoch 2340, training loss: 91.11124420166016 = 0.3056379556655884 + 10.0 * 9.080560684204102
Epoch 2340, val loss: 0.5066653490066528
Epoch 2350, training loss: 91.15113830566406 = 0.3037586510181427 + 10.0 * 9.084737777709961
Epoch 2350, val loss: 0.5065003037452698
Epoch 2360, training loss: 91.24852752685547 = 0.30199486017227173 + 10.0 * 9.094653129577637
Epoch 2360, val loss: 0.5069889426231384
Epoch 2370, training loss: 91.28744506835938 = 0.3002198040485382 + 10.0 * 9.098722457885742
Epoch 2370, val loss: 0.5075135231018066
Epoch 2380, training loss: 91.31361389160156 = 0.2984135150909424 + 10.0 * 9.101519584655762
Epoch 2380, val loss: 0.507713258266449
Epoch 2390, training loss: 91.37338256835938 = 0.2966480553150177 + 10.0 * 9.107673645019531
Epoch 2390, val loss: 0.5080009698867798
Epoch 2400, training loss: 91.3802719116211 = 0.294871985912323 + 10.0 * 9.108540534973145
Epoch 2400, val loss: 0.5085422992706299
Epoch 2410, training loss: 91.40362548828125 = 0.2930926978588104 + 10.0 * 9.111053466796875
Epoch 2410, val loss: 0.5090158581733704
Epoch 2420, training loss: 91.45001983642578 = 0.2913467586040497 + 10.0 * 9.115867614746094
Epoch 2420, val loss: 0.5094379186630249
Epoch 2430, training loss: 91.45410919189453 = 0.28959277272224426 + 10.0 * 9.116451263427734
Epoch 2430, val loss: 0.5099984407424927
Epoch 2440, training loss: 91.43807983398438 = 0.28785353899002075 + 10.0 * 9.115022659301758
Epoch 2440, val loss: 0.5101490616798401
Epoch 2450, training loss: 91.43907928466797 = 0.28610938787460327 + 10.0 * 9.115297317504883
Epoch 2450, val loss: 0.510978102684021
Epoch 2460, training loss: 91.46798706054688 = 0.28436800837516785 + 10.0 * 9.118361473083496
Epoch 2460, val loss: 0.5116295218467712
Epoch 2470, training loss: 91.51039123535156 = 0.2826087772846222 + 10.0 * 9.122777938842773
Epoch 2470, val loss: 0.5123539566993713
Epoch 2480, training loss: 91.54624938964844 = 0.2808856666088104 + 10.0 * 9.12653636932373
Epoch 2480, val loss: 0.5127962231636047
Epoch 2490, training loss: 91.51387023925781 = 0.2791648507118225 + 10.0 * 9.123470306396484
Epoch 2490, val loss: 0.5134585499763489
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8052173913043478
0.8169963051510541
The final CL Acc:0.80599, 0.00208, The final GNN Acc:0.81832, 0.00099
raw graph: torch.Size([2, 88648])
add edge: torch.Size([2, 110608])
remove edge: torch.Size([2, 66544])
updated graph: torch.Size([2, 88504])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 103.69046783447266 = 1.1088757514953613 + 10.0 * 10.258158683776855
Epoch 0, val loss: 1.108115553855896
Epoch 10, training loss: 99.38433074951172 = 1.1048543453216553 + 10.0 * 9.827947616577148
Epoch 10, val loss: 1.1041337251663208
Epoch 20, training loss: 97.06058502197266 = 1.100886583328247 + 10.0 * 9.595970153808594
Epoch 20, val loss: 1.100218653678894
Epoch 30, training loss: 95.48001861572266 = 1.0970914363861084 + 10.0 * 9.438292503356934
Epoch 30, val loss: 1.0964748859405518
Epoch 40, training loss: 94.26237487792969 = 1.0934914350509644 + 10.0 * 9.316888809204102
Epoch 40, val loss: 1.0929301977157593
Epoch 50, training loss: 93.26973724365234 = 1.090089201927185 + 10.0 * 9.217965126037598
Epoch 50, val loss: 1.0895779132843018
Epoch 60, training loss: 92.42955017089844 = 1.0868607759475708 + 10.0 * 9.134268760681152
Epoch 60, val loss: 1.0863978862762451
Epoch 70, training loss: 91.72602081298828 = 1.0837970972061157 + 10.0 * 9.06422233581543
Epoch 70, val loss: 1.083382248878479
Epoch 80, training loss: 91.14311981201172 = 1.0808578729629517 + 10.0 * 9.006226539611816
Epoch 80, val loss: 1.0804917812347412
Epoch 90, training loss: 90.66112518310547 = 1.078065037727356 + 10.0 * 8.958306312561035
Epoch 90, val loss: 1.0777424573898315
Epoch 100, training loss: 90.25577545166016 = 1.0753980875015259 + 10.0 * 8.918037414550781
Epoch 100, val loss: 1.0751245021820068
Epoch 110, training loss: 89.89857482910156 = 1.0728278160095215 + 10.0 * 8.882574081420898
Epoch 110, val loss: 1.072605013847351
Epoch 120, training loss: 89.590576171875 = 1.0703911781311035 + 10.0 * 8.852018356323242
Epoch 120, val loss: 1.0702216625213623
Epoch 130, training loss: 89.3163833618164 = 1.0680499076843262 + 10.0 * 8.824833869934082
Epoch 130, val loss: 1.0679335594177246
Epoch 140, training loss: 89.10319519042969 = 1.0657918453216553 + 10.0 * 8.803740501403809
Epoch 140, val loss: 1.0657438039779663
Epoch 150, training loss: 88.88229370117188 = 1.063620924949646 + 10.0 * 8.781867027282715
Epoch 150, val loss: 1.0636301040649414
Epoch 160, training loss: 88.7149658203125 = 1.0615471601486206 + 10.0 * 8.765341758728027
Epoch 160, val loss: 1.061621069908142
Epoch 170, training loss: 88.56979370117188 = 1.0595539808273315 + 10.0 * 8.75102424621582
Epoch 170, val loss: 1.059694766998291
Epoch 180, training loss: 88.45907592773438 = 1.0576210021972656 + 10.0 * 8.740145683288574
Epoch 180, val loss: 1.0578328371047974
Epoch 190, training loss: 88.35889434814453 = 1.055763602256775 + 10.0 * 8.730313301086426
Epoch 190, val loss: 1.0560393333435059
Epoch 200, training loss: 88.25416564941406 = 1.0539106130599976 + 10.0 * 8.720026016235352
Epoch 200, val loss: 1.0542906522750854
Epoch 210, training loss: 88.16292572021484 = 1.0520998239517212 + 10.0 * 8.711082458496094
Epoch 210, val loss: 1.0525400638580322
Epoch 220, training loss: 88.10395050048828 = 1.050336480140686 + 10.0 * 8.705361366271973
Epoch 220, val loss: 1.050850510597229
Epoch 230, training loss: 88.04862213134766 = 1.0485451221466064 + 10.0 * 8.700007438659668
Epoch 230, val loss: 1.0491539239883423
Epoch 240, training loss: 88.05560302734375 = 1.046773910522461 + 10.0 * 8.700882911682129
Epoch 240, val loss: 1.0474483966827393
Epoch 250, training loss: 87.94954681396484 = 1.0449914932250977 + 10.0 * 8.690455436706543
Epoch 250, val loss: 1.045727252960205
Epoch 260, training loss: 87.90904235839844 = 1.0431650876998901 + 10.0 * 8.6865873336792
Epoch 260, val loss: 1.0439850091934204
Epoch 270, training loss: 87.89659881591797 = 1.0412509441375732 + 10.0 * 8.685534477233887
Epoch 270, val loss: 1.04216730594635
Epoch 280, training loss: 87.8506851196289 = 1.0392807722091675 + 10.0 * 8.681139945983887
Epoch 280, val loss: 1.0402742624282837
Epoch 290, training loss: 87.86666870117188 = 1.037177324295044 + 10.0 * 8.68294906616211
Epoch 290, val loss: 1.038277268409729
Epoch 300, training loss: 87.84684753417969 = 1.0350340604782104 + 10.0 * 8.681180953979492
Epoch 300, val loss: 1.0361895561218262
Epoch 310, training loss: 87.82554626464844 = 1.03267502784729 + 10.0 * 8.67928695678711
Epoch 310, val loss: 1.033930778503418
Epoch 320, training loss: 87.8310775756836 = 1.0300748348236084 + 10.0 * 8.680100440979004
Epoch 320, val loss: 1.03150475025177
Epoch 330, training loss: 87.813232421875 = 1.0274567604064941 + 10.0 * 8.678577423095703
Epoch 330, val loss: 1.028938889503479
Epoch 340, training loss: 87.79354858398438 = 1.0245543718338013 + 10.0 * 8.676899909973145
Epoch 340, val loss: 1.0261491537094116
Epoch 350, training loss: 87.83342742919922 = 1.0215344429016113 + 10.0 * 8.681188583374023
Epoch 350, val loss: 1.0231657028198242
Epoch 360, training loss: 87.81978607177734 = 1.0181889533996582 + 10.0 * 8.680159568786621
Epoch 360, val loss: 1.0199854373931885
Epoch 370, training loss: 87.84139251708984 = 1.0146502256393433 + 10.0 * 8.682674407958984
Epoch 370, val loss: 1.0165281295776367
Epoch 380, training loss: 87.86543273925781 = 1.0108342170715332 + 10.0 * 8.685460090637207
Epoch 380, val loss: 1.0128377676010132
Epoch 390, training loss: 87.8587646484375 = 1.0067355632781982 + 10.0 * 8.685202598571777
Epoch 390, val loss: 1.0088847875595093
Epoch 400, training loss: 87.78350830078125 = 1.0020365715026855 + 10.0 * 8.678147315979004
Epoch 400, val loss: 1.0042564868927002
Epoch 410, training loss: 87.84609985351562 = 0.9975911378860474 + 10.0 * 8.684850692749023
Epoch 410, val loss: 0.99996018409729
Epoch 420, training loss: 87.86302947998047 = 0.9927014708518982 + 10.0 * 8.687032699584961
Epoch 420, val loss: 0.9953524470329285
Epoch 430, training loss: 87.81018829345703 = 0.9874600172042847 + 10.0 * 8.682272911071777
Epoch 430, val loss: 0.9903078079223633
Epoch 440, training loss: 87.84699249267578 = 0.9819529056549072 + 10.0 * 8.686503410339355
Epoch 440, val loss: 0.9849932789802551
Epoch 450, training loss: 87.88744354248047 = 0.9761391282081604 + 10.0 * 8.691129684448242
Epoch 450, val loss: 0.9794578552246094
Epoch 460, training loss: 87.86571502685547 = 0.969857931137085 + 10.0 * 8.68958568572998
Epoch 460, val loss: 0.9733119010925293
Epoch 470, training loss: 87.81050872802734 = 0.9634220004081726 + 10.0 * 8.684708595275879
Epoch 470, val loss: 0.9671692252159119
Epoch 480, training loss: 87.87841796875 = 0.9557972550392151 + 10.0 * 8.692262649536133
Epoch 480, val loss: 0.9597278833389282
Epoch 490, training loss: 87.93444061279297 = 0.9494156837463379 + 10.0 * 8.698502540588379
Epoch 490, val loss: 0.9539122581481934
Epoch 500, training loss: 87.83368682861328 = 0.9420939087867737 + 10.0 * 8.689159393310547
Epoch 500, val loss: 0.9466004967689514
Epoch 510, training loss: 87.80794525146484 = 0.9342072606086731 + 10.0 * 8.687374114990234
Epoch 510, val loss: 0.9391797184944153
Epoch 520, training loss: 87.79985809326172 = 0.926325261592865 + 10.0 * 8.687353134155273
Epoch 520, val loss: 0.9316520690917969
Epoch 530, training loss: 87.83097839355469 = 0.9182459712028503 + 10.0 * 8.69127368927002
Epoch 530, val loss: 0.9239356517791748
Epoch 540, training loss: 87.84627532958984 = 0.9096613526344299 + 10.0 * 8.6936616897583
Epoch 540, val loss: 0.9157044291496277
Epoch 550, training loss: 87.86438751220703 = 0.9009353518486023 + 10.0 * 8.696345329284668
Epoch 550, val loss: 0.9073590040206909
Epoch 560, training loss: 87.88422393798828 = 0.891822338104248 + 10.0 * 8.699239730834961
Epoch 560, val loss: 0.8986995220184326
Epoch 570, training loss: 87.9049072265625 = 0.882217526435852 + 10.0 * 8.702268600463867
Epoch 570, val loss: 0.8892281651496887
Epoch 580, training loss: 87.98369598388672 = 0.8731297850608826 + 10.0 * 8.71105670928955
Epoch 580, val loss: 0.8806411623954773
Epoch 590, training loss: 87.92698669433594 = 0.8635241985321045 + 10.0 * 8.70634651184082
Epoch 590, val loss: 0.8718790411949158
Epoch 600, training loss: 87.89061737060547 = 0.8530797362327576 + 10.0 * 8.703753471374512
Epoch 600, val loss: 0.8618146777153015
Epoch 610, training loss: 87.95687103271484 = 0.8430323004722595 + 10.0 * 8.711383819580078
Epoch 610, val loss: 0.852245569229126
Epoch 620, training loss: 87.98231506347656 = 0.8326255679130554 + 10.0 * 8.71496868133545
Epoch 620, val loss: 0.8423970341682434
Epoch 630, training loss: 87.99034881591797 = 0.8220744729042053 + 10.0 * 8.716827392578125
Epoch 630, val loss: 0.8324486613273621
Epoch 640, training loss: 88.01192474365234 = 0.8114451169967651 + 10.0 * 8.720047950744629
Epoch 640, val loss: 0.8223869800567627
Epoch 650, training loss: 88.06405639648438 = 0.8006720542907715 + 10.0 * 8.726338386535645
Epoch 650, val loss: 0.8122670650482178
Epoch 660, training loss: 88.04315185546875 = 0.7897418737411499 + 10.0 * 8.725340843200684
Epoch 660, val loss: 0.8019170761108398
Epoch 670, training loss: 88.082275390625 = 0.7789788246154785 + 10.0 * 8.730329513549805
Epoch 670, val loss: 0.7917159199714661
Epoch 680, training loss: 88.12660217285156 = 0.7680450081825256 + 10.0 * 8.735856056213379
Epoch 680, val loss: 0.7814696431159973
Epoch 690, training loss: 88.12307739257812 = 0.7570157051086426 + 10.0 * 8.736605644226074
Epoch 690, val loss: 0.7711779475212097
Epoch 700, training loss: 88.14176940917969 = 0.7461370229721069 + 10.0 * 8.73956298828125
Epoch 700, val loss: 0.7610340118408203
Epoch 710, training loss: 88.13522338867188 = 0.7353290319442749 + 10.0 * 8.739989280700684
Epoch 710, val loss: 0.7508766651153564
Epoch 720, training loss: 88.16676330566406 = 0.7247819900512695 + 10.0 * 8.744197845458984
Epoch 720, val loss: 0.7408863306045532
Epoch 730, training loss: 88.17719268798828 = 0.7141911387443542 + 10.0 * 8.74630069732666
Epoch 730, val loss: 0.730987012386322
Epoch 740, training loss: 88.18569946289062 = 0.7038271427154541 + 10.0 * 8.748187065124512
Epoch 740, val loss: 0.7212015390396118
Epoch 750, training loss: 88.14640808105469 = 0.6934800744056702 + 10.0 * 8.745292663574219
Epoch 750, val loss: 0.7116493582725525
Epoch 760, training loss: 88.16639709472656 = 0.6836090683937073 + 10.0 * 8.748278617858887
Epoch 760, val loss: 0.7024917006492615
Epoch 770, training loss: 88.23553466796875 = 0.6738795042037964 + 10.0 * 8.756165504455566
Epoch 770, val loss: 0.6934144496917725
Epoch 780, training loss: 88.23481750488281 = 0.6641330718994141 + 10.0 * 8.757068634033203
Epoch 780, val loss: 0.684357225894928
Epoch 790, training loss: 88.24015808105469 = 0.6546220183372498 + 10.0 * 8.758553504943848
Epoch 790, val loss: 0.6755259037017822
Epoch 800, training loss: 88.25321960449219 = 0.6453109383583069 + 10.0 * 8.760790824890137
Epoch 800, val loss: 0.6670178174972534
Epoch 810, training loss: 88.26097869873047 = 0.6363548040390015 + 10.0 * 8.762462615966797
Epoch 810, val loss: 0.6587219834327698
Epoch 820, training loss: 88.28422546386719 = 0.6276006102561951 + 10.0 * 8.76566219329834
Epoch 820, val loss: 0.6506001353263855
Epoch 830, training loss: 88.3109359741211 = 0.61899334192276 + 10.0 * 8.769193649291992
Epoch 830, val loss: 0.6427793502807617
Epoch 840, training loss: 88.31523895263672 = 0.6107022166252136 + 10.0 * 8.770453453063965
Epoch 840, val loss: 0.6351541876792908
Epoch 850, training loss: 88.37171936035156 = 0.6027376055717468 + 10.0 * 8.776898384094238
Epoch 850, val loss: 0.6276644468307495
Epoch 860, training loss: 88.25811767578125 = 0.5947709083557129 + 10.0 * 8.766334533691406
Epoch 860, val loss: 0.6203760504722595
Epoch 870, training loss: 88.3015365600586 = 0.5873773097991943 + 10.0 * 8.771415710449219
Epoch 870, val loss: 0.6136519312858582
Epoch 880, training loss: 88.26335144042969 = 0.580446183681488 + 10.0 * 8.768290519714355
Epoch 880, val loss: 0.6072844862937927
Epoch 890, training loss: 88.37471008300781 = 0.5732101798057556 + 10.0 * 8.780149459838867
Epoch 890, val loss: 0.6008003354072571
Epoch 900, training loss: 88.29056549072266 = 0.5664507150650024 + 10.0 * 8.772411346435547
Epoch 900, val loss: 0.5948479175567627
Epoch 910, training loss: 88.3405990600586 = 0.5601243376731873 + 10.0 * 8.778047561645508
Epoch 910, val loss: 0.5890143513679504
Epoch 920, training loss: 88.26944732666016 = 0.5539777874946594 + 10.0 * 8.771547317504883
Epoch 920, val loss: 0.5836818814277649
Epoch 930, training loss: 88.34912109375 = 0.5479820966720581 + 10.0 * 8.78011417388916
Epoch 930, val loss: 0.5784011483192444
Epoch 940, training loss: 88.38720703125 = 0.5420671105384827 + 10.0 * 8.784513473510742
Epoch 940, val loss: 0.5730884075164795
Epoch 950, training loss: 88.43232727050781 = 0.5363627076148987 + 10.0 * 8.789596557617188
Epoch 950, val loss: 0.5680686831474304
Epoch 960, training loss: 88.4217758178711 = 0.530911386013031 + 10.0 * 8.78908634185791
Epoch 960, val loss: 0.5634433031082153
Epoch 970, training loss: 88.47351837158203 = 0.5256314873695374 + 10.0 * 8.794788360595703
Epoch 970, val loss: 0.5590264797210693
Epoch 980, training loss: 88.52497863769531 = 0.5204532742500305 + 10.0 * 8.80045223236084
Epoch 980, val loss: 0.554602861404419
Epoch 990, training loss: 88.49103546142578 = 0.5154089331626892 + 10.0 * 8.797562599182129
Epoch 990, val loss: 0.5502916574478149
Epoch 1000, training loss: 88.50293731689453 = 0.5108123421669006 + 10.0 * 8.799212455749512
Epoch 1000, val loss: 0.5465885400772095
Epoch 1010, training loss: 88.48067474365234 = 0.5061641931533813 + 10.0 * 8.79745101928711
Epoch 1010, val loss: 0.5426464080810547
Epoch 1020, training loss: 88.55364227294922 = 0.5016943216323853 + 10.0 * 8.805194854736328
Epoch 1020, val loss: 0.5391468405723572
Epoch 1030, training loss: 88.5931167602539 = 0.4972594082355499 + 10.0 * 8.809585571289062
Epoch 1030, val loss: 0.5354751944541931
Epoch 1040, training loss: 88.54984283447266 = 0.4929618537425995 + 10.0 * 8.805688858032227
Epoch 1040, val loss: 0.5318939685821533
Epoch 1050, training loss: 88.57279205322266 = 0.48892447352409363 + 10.0 * 8.80838680267334
Epoch 1050, val loss: 0.5288751125335693
Epoch 1060, training loss: 88.62309265136719 = 0.4849739968776703 + 10.0 * 8.813811302185059
Epoch 1060, val loss: 0.5257487297058105
Epoch 1070, training loss: 88.63539123535156 = 0.4809735119342804 + 10.0 * 8.815442085266113
Epoch 1070, val loss: 0.5224581360816956
Epoch 1080, training loss: 88.63423919677734 = 0.47712579369544983 + 10.0 * 8.81571102142334
Epoch 1080, val loss: 0.5195749402046204
Epoch 1090, training loss: 88.65473937988281 = 0.473296582698822 + 10.0 * 8.818143844604492
Epoch 1090, val loss: 0.5166499614715576
Epoch 1100, training loss: 88.61600494384766 = 0.4697856605052948 + 10.0 * 8.814621925354004
Epoch 1100, val loss: 0.5137762427330017
Epoch 1110, training loss: 88.66127014160156 = 0.46637216210365295 + 10.0 * 8.819490432739258
Epoch 1110, val loss: 0.511526882648468
Epoch 1120, training loss: 88.6933822631836 = 0.46274659037590027 + 10.0 * 8.823063850402832
Epoch 1120, val loss: 0.5086312294006348
Epoch 1130, training loss: 88.74251556396484 = 0.45921868085861206 + 10.0 * 8.828329086303711
Epoch 1130, val loss: 0.5058841109275818
Epoch 1140, training loss: 88.75068664550781 = 0.4557918906211853 + 10.0 * 8.829488754272461
Epoch 1140, val loss: 0.5034240484237671
Epoch 1150, training loss: 88.75663757324219 = 0.45234984159469604 + 10.0 * 8.830429077148438
Epoch 1150, val loss: 0.5010234117507935
Epoch 1160, training loss: 88.74639129638672 = 0.44899797439575195 + 10.0 * 8.829739570617676
Epoch 1160, val loss: 0.49863386154174805
Epoch 1170, training loss: 88.7634506225586 = 0.44579991698265076 + 10.0 * 8.831765174865723
Epoch 1170, val loss: 0.49630287289619446
Epoch 1180, training loss: 88.79634857177734 = 0.4426148235797882 + 10.0 * 8.835373878479004
Epoch 1180, val loss: 0.49396559596061707
Epoch 1190, training loss: 88.76649475097656 = 0.43945634365081787 + 10.0 * 8.832704544067383
Epoch 1190, val loss: 0.4917965233325958
Epoch 1200, training loss: 88.79293060302734 = 0.43641355633735657 + 10.0 * 8.835651397705078
Epoch 1200, val loss: 0.4895930290222168
Epoch 1210, training loss: 88.81712341308594 = 0.4333314299583435 + 10.0 * 8.83837890625
Epoch 1210, val loss: 0.4874880015850067
Epoch 1220, training loss: 88.85567474365234 = 0.4303654432296753 + 10.0 * 8.842531204223633
Epoch 1220, val loss: 0.48549720644950867
Epoch 1230, training loss: 88.86732482910156 = 0.4273523986339569 + 10.0 * 8.84399700164795
Epoch 1230, val loss: 0.48340633511543274
Epoch 1240, training loss: 88.87515258789062 = 0.4244043231010437 + 10.0 * 8.845074653625488
Epoch 1240, val loss: 0.4812844395637512
Epoch 1250, training loss: 88.89566802978516 = 0.42158442735671997 + 10.0 * 8.847408294677734
Epoch 1250, val loss: 0.4795893430709839
Epoch 1260, training loss: 88.90186309814453 = 0.4187261164188385 + 10.0 * 8.84831428527832
Epoch 1260, val loss: 0.4775449335575104
Epoch 1270, training loss: 88.91803741455078 = 0.4159167408943176 + 10.0 * 8.850212097167969
Epoch 1270, val loss: 0.4757680892944336
Epoch 1280, training loss: 88.88558959960938 = 0.4131768047809601 + 10.0 * 8.847241401672363
Epoch 1280, val loss: 0.47391796112060547
Epoch 1290, training loss: 88.91783142089844 = 0.41039901971817017 + 10.0 * 8.850743293762207
Epoch 1290, val loss: 0.4721769392490387
Epoch 1300, training loss: 88.94055938720703 = 0.40767723321914673 + 10.0 * 8.853288650512695
Epoch 1300, val loss: 0.47031599283218384
Epoch 1310, training loss: 88.96326446533203 = 0.40499240159988403 + 10.0 * 8.855827331542969
Epoch 1310, val loss: 0.4684659242630005
Epoch 1320, training loss: 88.97053527832031 = 0.40230387449264526 + 10.0 * 8.856822967529297
Epoch 1320, val loss: 0.4667156934738159
Epoch 1330, training loss: 89.0023422241211 = 0.3996967673301697 + 10.0 * 8.860264778137207
Epoch 1330, val loss: 0.465119868516922
Epoch 1340, training loss: 88.954345703125 = 0.397045761346817 + 10.0 * 8.855730056762695
Epoch 1340, val loss: 0.4634905755519867
Epoch 1350, training loss: 89.04228210449219 = 0.39462485909461975 + 10.0 * 8.864766120910645
Epoch 1350, val loss: 0.4619305431842804
Epoch 1360, training loss: 88.85420989990234 = 0.39212435483932495 + 10.0 * 8.846208572387695
Epoch 1360, val loss: 0.46039971709251404
Epoch 1370, training loss: 88.69915771484375 = 0.3896787762641907 + 10.0 * 8.830947875976562
Epoch 1370, val loss: 0.4599894881248474
Epoch 1380, training loss: 88.78036499023438 = 0.38749855756759644 + 10.0 * 8.839286804199219
Epoch 1380, val loss: 0.45751142501831055
Epoch 1390, training loss: 88.83309173583984 = 0.38509976863861084 + 10.0 * 8.844799041748047
Epoch 1390, val loss: 0.4562268555164337
Epoch 1400, training loss: 88.83602905273438 = 0.38258635997772217 + 10.0 * 8.845344543457031
Epoch 1400, val loss: 0.4543452560901642
Epoch 1410, training loss: 88.90554809570312 = 0.38012129068374634 + 10.0 * 8.852542877197266
Epoch 1410, val loss: 0.4531308710575104
Epoch 1420, training loss: 88.96805572509766 = 0.3776569366455078 + 10.0 * 8.859040260314941
Epoch 1420, val loss: 0.4515591561794281
Epoch 1430, training loss: 88.9775619506836 = 0.37510958313941956 + 10.0 * 8.860245704650879
Epoch 1430, val loss: 0.44981810450553894
Epoch 1440, training loss: 89.0267562866211 = 0.37270697951316833 + 10.0 * 8.865405082702637
Epoch 1440, val loss: 0.4484598636627197
Epoch 1450, training loss: 89.02023315429688 = 0.3702729046344757 + 10.0 * 8.864995956420898
Epoch 1450, val loss: 0.4468855857849121
Epoch 1460, training loss: 89.05985260009766 = 0.3679206073284149 + 10.0 * 8.869193077087402
Epoch 1460, val loss: 0.44550031423568726
Epoch 1470, training loss: 89.07766723632812 = 0.36557838320732117 + 10.0 * 8.871209144592285
Epoch 1470, val loss: 0.44418904185295105
Epoch 1480, training loss: 89.08731842041016 = 0.36322328448295593 + 10.0 * 8.87240982055664
Epoch 1480, val loss: 0.4427420496940613
Epoch 1490, training loss: 89.08189392089844 = 0.3609224855899811 + 10.0 * 8.87209701538086
Epoch 1490, val loss: 0.44138485193252563
Epoch 1500, training loss: 89.10696411132812 = 0.3586428761482239 + 10.0 * 8.874832153320312
Epoch 1500, val loss: 0.44015681743621826
Epoch 1510, training loss: 89.06087493896484 = 0.356425017118454 + 10.0 * 8.870445251464844
Epoch 1510, val loss: 0.43921488523483276
Epoch 1520, training loss: 89.04087829589844 = 0.3541322648525238 + 10.0 * 8.868674278259277
Epoch 1520, val loss: 0.43771499395370483
Epoch 1530, training loss: 89.10357666015625 = 0.3520221412181854 + 10.0 * 8.875155448913574
Epoch 1530, val loss: 0.43671664595603943
Epoch 1540, training loss: 89.17742156982422 = 0.34990644454956055 + 10.0 * 8.88275146484375
Epoch 1540, val loss: 0.43564561009407043
Epoch 1550, training loss: 89.14989471435547 = 0.3477048873901367 + 10.0 * 8.880219459533691
Epoch 1550, val loss: 0.434345006942749
Epoch 1560, training loss: 89.1413345336914 = 0.3455875515937805 + 10.0 * 8.8795747756958
Epoch 1560, val loss: 0.4334731101989746
Epoch 1570, training loss: 89.1741714477539 = 0.34343448281288147 + 10.0 * 8.883073806762695
Epoch 1570, val loss: 0.4322265386581421
Epoch 1580, training loss: 89.21943664550781 = 0.3413441479206085 + 10.0 * 8.887808799743652
Epoch 1580, val loss: 0.4312342405319214
Epoch 1590, training loss: 89.21209716796875 = 0.33922284841537476 + 10.0 * 8.887287139892578
Epoch 1590, val loss: 0.4303000867366791
Epoch 1600, training loss: 89.17790985107422 = 0.33716753125190735 + 10.0 * 8.884074211120605
Epoch 1600, val loss: 0.42919158935546875
Epoch 1610, training loss: 89.22792053222656 = 0.33518368005752563 + 10.0 * 8.889273643493652
Epoch 1610, val loss: 0.4281838536262512
Epoch 1620, training loss: 89.21947479248047 = 0.33316516876220703 + 10.0 * 8.888630867004395
Epoch 1620, val loss: 0.42713677883148193
Epoch 1630, training loss: 89.212890625 = 0.3312041163444519 + 10.0 * 8.888168334960938
Epoch 1630, val loss: 0.4261833429336548
Epoch 1640, training loss: 89.27025604248047 = 0.32921725511550903 + 10.0 * 8.89410400390625
Epoch 1640, val loss: 0.4253386855125427
Epoch 1650, training loss: 89.24502563476562 = 0.3272224962711334 + 10.0 * 8.891779899597168
Epoch 1650, val loss: 0.42436644434928894
Epoch 1660, training loss: 89.20659637451172 = 0.3253043591976166 + 10.0 * 8.888129234313965
Epoch 1660, val loss: 0.4236338436603546
Epoch 1670, training loss: 89.22322082519531 = 0.3234908878803253 + 10.0 * 8.889972686767578
Epoch 1670, val loss: 0.42283740639686584
Epoch 1680, training loss: 89.212890625 = 0.3215598165988922 + 10.0 * 8.88913345336914
Epoch 1680, val loss: 0.4219857454299927
Epoch 1690, training loss: 89.29595184326172 = 0.31965434551239014 + 10.0 * 8.897629737854004
Epoch 1690, val loss: 0.4210762679576874
Epoch 1700, training loss: 89.29838562011719 = 0.3177558481693268 + 10.0 * 8.898062705993652
Epoch 1700, val loss: 0.42026105523109436
Epoch 1710, training loss: 89.34233093261719 = 0.31585609912872314 + 10.0 * 8.902647018432617
Epoch 1710, val loss: 0.4194541275501251
Epoch 1720, training loss: 89.258544921875 = 0.3141641616821289 + 10.0 * 8.894437789916992
Epoch 1720, val loss: 0.4187021255493164
Epoch 1730, training loss: 89.15850830078125 = 0.31238260865211487 + 10.0 * 8.884612083435059
Epoch 1730, val loss: 0.4179287850856781
Epoch 1740, training loss: 89.21611022949219 = 0.31062766909599304 + 10.0 * 8.890547752380371
Epoch 1740, val loss: 0.4173131585121155
Epoch 1750, training loss: 89.2929458618164 = 0.30884456634521484 + 10.0 * 8.898409843444824
Epoch 1750, val loss: 0.41678598523139954
Epoch 1760, training loss: 89.36145782470703 = 0.3070445954799652 + 10.0 * 8.905441284179688
Epoch 1760, val loss: 0.4161623418331146
Epoch 1770, training loss: 89.31648254394531 = 0.3052884340286255 + 10.0 * 8.901119232177734
Epoch 1770, val loss: 0.4156607687473297
Epoch 1780, training loss: 89.33688354492188 = 0.3035590350627899 + 10.0 * 8.903332710266113
Epoch 1780, val loss: 0.4148791432380676
Epoch 1790, training loss: 89.38494110107422 = 0.3018379807472229 + 10.0 * 8.908309936523438
Epoch 1790, val loss: 0.4145008623600006
Epoch 1800, training loss: 89.42100524902344 = 0.3001062572002411 + 10.0 * 8.912089347839355
Epoch 1800, val loss: 0.41391849517822266
Epoch 1810, training loss: 89.36915588378906 = 0.2983836829662323 + 10.0 * 8.907076835632324
Epoch 1810, val loss: 0.4134003221988678
Epoch 1820, training loss: 89.38445281982422 = 0.2967085838317871 + 10.0 * 8.908774375915527
Epoch 1820, val loss: 0.41286376118659973
Epoch 1830, training loss: 89.4549789428711 = 0.2950460910797119 + 10.0 * 8.915993690490723
Epoch 1830, val loss: 0.41227418184280396
Epoch 1840, training loss: 89.474365234375 = 0.29337257146835327 + 10.0 * 8.918099403381348
Epoch 1840, val loss: 0.4118025600910187
Epoch 1850, training loss: 89.39330291748047 = 0.2917260527610779 + 10.0 * 8.910158157348633
Epoch 1850, val loss: 0.4114372134208679
Epoch 1860, training loss: 89.42980194091797 = 0.29008856415748596 + 10.0 * 8.913971900939941
Epoch 1860, val loss: 0.4112395942211151
Epoch 1870, training loss: 89.3832015991211 = 0.2885441780090332 + 10.0 * 8.909465789794922
Epoch 1870, val loss: 0.41067686676979065
Epoch 1880, training loss: 89.39108276367188 = 0.2869744598865509 + 10.0 * 8.91041088104248
Epoch 1880, val loss: 0.41019999980926514
Epoch 1890, training loss: 89.4616928100586 = 0.28541043400764465 + 10.0 * 8.917628288269043
Epoch 1890, val loss: 0.40983712673187256
Epoch 1900, training loss: 89.49065399169922 = 0.2838364243507385 + 10.0 * 8.920681953430176
Epoch 1900, val loss: 0.409612238407135
Epoch 1910, training loss: 89.47771453857422 = 0.2822657525539398 + 10.0 * 8.91954517364502
Epoch 1910, val loss: 0.4093495011329651
Epoch 1920, training loss: 89.4716796875 = 0.28074774146080017 + 10.0 * 8.919093132019043
Epoch 1920, val loss: 0.4088364839553833
Epoch 1930, training loss: 89.49446868896484 = 0.2791920304298401 + 10.0 * 8.921527862548828
Epoch 1930, val loss: 0.40845030546188354
Epoch 1940, training loss: 89.55364227294922 = 0.2776690423488617 + 10.0 * 8.927597045898438
Epoch 1940, val loss: 0.4082968533039093
Epoch 1950, training loss: 89.51375579833984 = 0.2761295735836029 + 10.0 * 8.923762321472168
Epoch 1950, val loss: 0.40787601470947266
Epoch 1960, training loss: 89.54507446289062 = 0.27460381388664246 + 10.0 * 8.927046775817871
Epoch 1960, val loss: 0.40772587060928345
Epoch 1970, training loss: 89.57189178466797 = 0.2730920612812042 + 10.0 * 8.929880142211914
Epoch 1970, val loss: 0.40720075368881226
Epoch 1980, training loss: 89.53084564208984 = 0.2715771198272705 + 10.0 * 8.92592716217041
Epoch 1980, val loss: 0.40716177225112915
Epoch 1990, training loss: 89.56895446777344 = 0.27008333802223206 + 10.0 * 8.929887771606445
Epoch 1990, val loss: 0.4067889153957367
Epoch 2000, training loss: 89.5881118774414 = 0.2685839533805847 + 10.0 * 8.931952476501465
Epoch 2000, val loss: 0.4065791368484497
Epoch 2010, training loss: 89.56859588623047 = 0.2670792043209076 + 10.0 * 8.930150985717773
Epoch 2010, val loss: 0.40627169609069824
Epoch 2020, training loss: 89.60231018066406 = 0.265604704618454 + 10.0 * 8.933670997619629
Epoch 2020, val loss: 0.4062141180038452
Epoch 2030, training loss: 89.62555694580078 = 0.26413753628730774 + 10.0 * 8.936141967773438
Epoch 2030, val loss: 0.40611329674720764
Epoch 2040, training loss: 89.6443099975586 = 0.2627148926258087 + 10.0 * 8.938158988952637
Epoch 2040, val loss: 0.4062901735305786
Epoch 2050, training loss: 89.59042358398438 = 0.2612910866737366 + 10.0 * 8.932912826538086
Epoch 2050, val loss: 0.4060559868812561
Epoch 2060, training loss: 89.6039047241211 = 0.25984179973602295 + 10.0 * 8.934406280517578
Epoch 2060, val loss: 0.40536028146743774
Epoch 2070, training loss: 89.629150390625 = 0.2583891749382019 + 10.0 * 8.9370756149292
Epoch 2070, val loss: 0.4055618941783905
Epoch 2080, training loss: 89.60488891601562 = 0.2569895386695862 + 10.0 * 8.934789657592773
Epoch 2080, val loss: 0.40511685609817505
Epoch 2090, training loss: 89.55147552490234 = 0.2557280957698822 + 10.0 * 8.929574966430664
Epoch 2090, val loss: 0.40484729409217834
Epoch 2100, training loss: 89.59394836425781 = 0.2543782889842987 + 10.0 * 8.93395709991455
Epoch 2100, val loss: 0.4050326943397522
Epoch 2110, training loss: 89.66239166259766 = 0.25295084714889526 + 10.0 * 8.940943717956543
Epoch 2110, val loss: 0.40502816438674927
Epoch 2120, training loss: 89.69271850585938 = 0.2515334486961365 + 10.0 * 8.94411849975586
Epoch 2120, val loss: 0.4050810635089874
Epoch 2130, training loss: 89.5534439086914 = 0.2501755654811859 + 10.0 * 8.930326461791992
Epoch 2130, val loss: 0.4051688611507416
Epoch 2140, training loss: 89.60771942138672 = 0.24893316626548767 + 10.0 * 8.93587875366211
Epoch 2140, val loss: 0.4049242436885834
Epoch 2150, training loss: 89.65790557861328 = 0.24759125709533691 + 10.0 * 8.941031455993652
Epoch 2150, val loss: 0.40521439909935
Epoch 2160, training loss: 89.69612884521484 = 0.24616163969039917 + 10.0 * 8.94499683380127
Epoch 2160, val loss: 0.4051789343357086
Epoch 2170, training loss: 89.70071411132812 = 0.2447592318058014 + 10.0 * 8.945595741271973
Epoch 2170, val loss: 0.4050793945789337
Epoch 2180, training loss: 89.70885467529297 = 0.2433881163597107 + 10.0 * 8.94654655456543
Epoch 2180, val loss: 0.4051307141780853
Epoch 2190, training loss: 89.72783660888672 = 0.2420264482498169 + 10.0 * 8.948580741882324
Epoch 2190, val loss: 0.4050450325012207
Epoch 2200, training loss: 89.71942901611328 = 0.24065397679805756 + 10.0 * 8.947877883911133
Epoch 2200, val loss: 0.4049959182739258
Epoch 2210, training loss: 89.7068099975586 = 0.23931358754634857 + 10.0 * 8.946749687194824
Epoch 2210, val loss: 0.4054504632949829
Epoch 2220, training loss: 89.73050689697266 = 0.23797491192817688 + 10.0 * 8.94925308227539
Epoch 2220, val loss: 0.40517768263816833
Epoch 2230, training loss: 89.7332992553711 = 0.23664624989032745 + 10.0 * 8.949665069580078
Epoch 2230, val loss: 0.4052296280860901
Epoch 2240, training loss: 89.75553131103516 = 0.23531311750411987 + 10.0 * 8.952021598815918
Epoch 2240, val loss: 0.4053446054458618
Epoch 2250, training loss: 89.74658966064453 = 0.23400495946407318 + 10.0 * 8.951258659362793
Epoch 2250, val loss: 0.4055575430393219
Epoch 2260, training loss: 89.76084899902344 = 0.23268023133277893 + 10.0 * 8.9528169631958
Epoch 2260, val loss: 0.4054264426231384
Epoch 2270, training loss: 89.74720001220703 = 0.2313886135816574 + 10.0 * 8.951581001281738
Epoch 2270, val loss: 0.4055042564868927
Epoch 2280, training loss: 89.77635192871094 = 0.2301604300737381 + 10.0 * 8.954618453979492
Epoch 2280, val loss: 0.40544238686561584
Epoch 2290, training loss: 89.7225112915039 = 0.22894181311130524 + 10.0 * 8.949357032775879
Epoch 2290, val loss: 0.405965119600296
Epoch 2300, training loss: 89.73072814941406 = 0.2276628166437149 + 10.0 * 8.95030689239502
Epoch 2300, val loss: 0.40558961033821106
Epoch 2310, training loss: 89.75186920166016 = 0.2263992577791214 + 10.0 * 8.952547073364258
Epoch 2310, val loss: 0.4057867228984833
Epoch 2320, training loss: 89.81304931640625 = 0.2251136749982834 + 10.0 * 8.958793640136719
Epoch 2320, val loss: 0.4061681628227234
Epoch 2330, training loss: 89.79633331298828 = 0.22380200028419495 + 10.0 * 8.957253456115723
Epoch 2330, val loss: 0.4064050614833832
Epoch 2340, training loss: 89.80339050292969 = 0.22251364588737488 + 10.0 * 8.958087921142578
Epoch 2340, val loss: 0.4064636826515198
Epoch 2350, training loss: 89.82681274414062 = 0.22125087678432465 + 10.0 * 8.960556030273438
Epoch 2350, val loss: 0.40660372376441956
Epoch 2360, training loss: 89.84326934814453 = 0.21996650099754333 + 10.0 * 8.96233081817627
Epoch 2360, val loss: 0.40690872073173523
Epoch 2370, training loss: 89.77819061279297 = 0.21874961256980896 + 10.0 * 8.955944061279297
Epoch 2370, val loss: 0.4070272445678711
Epoch 2380, training loss: 89.79476928710938 = 0.21749569475650787 + 10.0 * 8.957727432250977
Epoch 2380, val loss: 0.40719908475875854
Epoch 2390, training loss: 89.8613052368164 = 0.21623986959457397 + 10.0 * 8.964506149291992
Epoch 2390, val loss: 0.4078320860862732
Epoch 2400, training loss: 89.90234375 = 0.21496576070785522 + 10.0 * 8.968737602233887
Epoch 2400, val loss: 0.40783271193504333
Epoch 2410, training loss: 89.88253784179688 = 0.21366362273693085 + 10.0 * 8.966887474060059
Epoch 2410, val loss: 0.4081951379776001
Epoch 2420, training loss: 89.82727813720703 = 0.21240513026714325 + 10.0 * 8.961487770080566
Epoch 2420, val loss: 0.4084058403968811
Epoch 2430, training loss: 89.8687515258789 = 0.21117156744003296 + 10.0 * 8.965757369995117
Epoch 2430, val loss: 0.408588171005249
Epoch 2440, training loss: 89.92616271972656 = 0.20990736782550812 + 10.0 * 8.971625328063965
Epoch 2440, val loss: 0.4091511368751526
Epoch 2450, training loss: 89.92784118652344 = 0.20864197611808777 + 10.0 * 8.971920013427734
Epoch 2450, val loss: 0.4095361530780792
Epoch 2460, training loss: 89.92732238769531 = 0.2074093520641327 + 10.0 * 8.971990585327148
Epoch 2460, val loss: 0.4100865125656128
Epoch 2470, training loss: 89.92147064208984 = 0.2061699777841568 + 10.0 * 8.971529960632324
Epoch 2470, val loss: 0.4102371633052826
Epoch 2480, training loss: 89.90957641601562 = 0.20494063198566437 + 10.0 * 8.970463752746582
Epoch 2480, val loss: 0.41060590744018555
Epoch 2490, training loss: 89.9400634765625 = 0.20368720591068268 + 10.0 * 8.973637580871582
Epoch 2490, val loss: 0.4111632704734802
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8508695652173913
0.8629283489096574
=== training gcn model ===
Epoch 0, training loss: 103.1640625 = 1.117740511894226 + 10.0 * 10.204632759094238
Epoch 0, val loss: 1.1182808876037598
Epoch 10, training loss: 99.0495834350586 = 1.1132524013519287 + 10.0 * 9.793633460998535
Epoch 10, val loss: 1.1138843297958374
Epoch 20, training loss: 96.8183364868164 = 1.1091046333312988 + 10.0 * 9.5709228515625
Epoch 20, val loss: 1.1097596883773804
Epoch 30, training loss: 95.20622253417969 = 1.1050834655761719 + 10.0 * 9.410114288330078
Epoch 30, val loss: 1.1057713031768799
Epoch 40, training loss: 93.94292449951172 = 1.1012405157089233 + 10.0 * 9.284168243408203
Epoch 40, val loss: 1.1019611358642578
Epoch 50, training loss: 92.91790771484375 = 1.0975793600082397 + 10.0 * 9.182032585144043
Epoch 50, val loss: 1.098326563835144
Epoch 60, training loss: 92.09070587158203 = 1.094059944152832 + 10.0 * 9.099664688110352
Epoch 60, val loss: 1.09483003616333
Epoch 70, training loss: 91.40863037109375 = 1.090672254562378 + 10.0 * 9.031795501708984
Epoch 70, val loss: 1.0914559364318848
Epoch 80, training loss: 90.84487915039062 = 1.0874110460281372 + 10.0 * 8.975747108459473
Epoch 80, val loss: 1.0882091522216797
Epoch 90, training loss: 90.4107894897461 = 1.084229588508606 + 10.0 * 8.932656288146973
Epoch 90, val loss: 1.0850422382354736
Epoch 100, training loss: 90.01060485839844 = 1.0812515020370483 + 10.0 * 8.892934799194336
Epoch 100, val loss: 1.0820672512054443
Epoch 110, training loss: 89.66827392578125 = 1.0782785415649414 + 10.0 * 8.858999252319336
Epoch 110, val loss: 1.0791014432907104
Epoch 120, training loss: 89.38341522216797 = 1.0754616260528564 + 10.0 * 8.830795288085938
Epoch 120, val loss: 1.0762947797775269
Epoch 130, training loss: 89.13011169433594 = 1.0727136135101318 + 10.0 * 8.805739402770996
Epoch 130, val loss: 1.0735572576522827
Epoch 140, training loss: 88.94245910644531 = 1.0700736045837402 + 10.0 * 8.787238121032715
Epoch 140, val loss: 1.0709329843521118
Epoch 150, training loss: 88.76631164550781 = 1.0675263404846191 + 10.0 * 8.769878387451172
Epoch 150, val loss: 1.068395972251892
Epoch 160, training loss: 88.6195297241211 = 1.0650584697723389 + 10.0 * 8.755447387695312
Epoch 160, val loss: 1.0659435987472534
Epoch 170, training loss: 88.48957061767578 = 1.0626709461212158 + 10.0 * 8.742690086364746
Epoch 170, val loss: 1.0635879039764404
Epoch 180, training loss: 88.36222076416016 = 1.060364007949829 + 10.0 * 8.730185508728027
Epoch 180, val loss: 1.061293125152588
Epoch 190, training loss: 88.24535369873047 = 1.05808424949646 + 10.0 * 8.718727111816406
Epoch 190, val loss: 1.059043526649475
Epoch 200, training loss: 88.16539001464844 = 1.0559438467025757 + 10.0 * 8.710944175720215
Epoch 200, val loss: 1.0569261312484741
Epoch 210, training loss: 88.08760833740234 = 1.0537374019622803 + 10.0 * 8.703387260437012
Epoch 210, val loss: 1.0547738075256348
Epoch 220, training loss: 88.00397491455078 = 1.0516220331192017 + 10.0 * 8.695235252380371
Epoch 220, val loss: 1.0526870489120483
Epoch 230, training loss: 87.94542694091797 = 1.0495829582214355 + 10.0 * 8.689584732055664
Epoch 230, val loss: 1.0506720542907715
Epoch 240, training loss: 87.89630126953125 = 1.0474309921264648 + 10.0 * 8.684886932373047
Epoch 240, val loss: 1.0485832691192627
Epoch 250, training loss: 87.88208770751953 = 1.0453993082046509 + 10.0 * 8.68366813659668
Epoch 250, val loss: 1.0466164350509644
Epoch 260, training loss: 87.79680633544922 = 1.0431897640228271 + 10.0 * 8.675361633300781
Epoch 260, val loss: 1.0444179773330688
Epoch 270, training loss: 87.82061004638672 = 1.0412200689315796 + 10.0 * 8.677938461303711
Epoch 270, val loss: 1.0425008535385132
Epoch 280, training loss: 87.7658920288086 = 1.0390433073043823 + 10.0 * 8.672685623168945
Epoch 280, val loss: 1.0403751134872437
Epoch 290, training loss: 87.72438049316406 = 1.0367929935455322 + 10.0 * 8.668758392333984
Epoch 290, val loss: 1.0381932258605957
Epoch 300, training loss: 87.73311614990234 = 1.0344704389572144 + 10.0 * 8.669864654541016
Epoch 300, val loss: 1.035921335220337
Epoch 310, training loss: 87.72471618652344 = 1.0320472717285156 + 10.0 * 8.669266700744629
Epoch 310, val loss: 1.0335965156555176
Epoch 320, training loss: 87.68953704833984 = 1.0294982194900513 + 10.0 * 8.666004180908203
Epoch 320, val loss: 1.0311264991760254
Epoch 330, training loss: 87.69619750976562 = 1.0267747640609741 + 10.0 * 8.666942596435547
Epoch 330, val loss: 1.0285210609436035
Epoch 340, training loss: 87.65873718261719 = 1.0238726139068604 + 10.0 * 8.66348648071289
Epoch 340, val loss: 1.0256866216659546
Epoch 350, training loss: 87.73242950439453 = 1.0209728479385376 + 10.0 * 8.67114543914795
Epoch 350, val loss: 1.0229086875915527
Epoch 360, training loss: 87.68539428710938 = 1.0177127122879028 + 10.0 * 8.666768074035645
Epoch 360, val loss: 1.0197913646697998
Epoch 370, training loss: 87.69713592529297 = 1.0143376588821411 + 10.0 * 8.668279647827148
Epoch 370, val loss: 1.016491413116455
Epoch 380, training loss: 87.71448516845703 = 1.0108006000518799 + 10.0 * 8.670368194580078
Epoch 380, val loss: 1.0130537748336792
Epoch 390, training loss: 87.73271179199219 = 1.007028579711914 + 10.0 * 8.672568321228027
Epoch 390, val loss: 1.009408712387085
Epoch 400, training loss: 87.72023010253906 = 1.0030359029769897 + 10.0 * 8.671719551086426
Epoch 400, val loss: 1.0056339502334595
Epoch 410, training loss: 87.67878723144531 = 0.9984716773033142 + 10.0 * 8.668031692504883
Epoch 410, val loss: 1.00130295753479
Epoch 420, training loss: 87.70205688476562 = 0.9939302802085876 + 10.0 * 8.670812606811523
Epoch 420, val loss: 0.99689781665802
Epoch 430, training loss: 87.71886444091797 = 0.9889936447143555 + 10.0 * 8.67298698425293
Epoch 430, val loss: 0.9921479225158691
Epoch 440, training loss: 87.71598815917969 = 0.9837252497673035 + 10.0 * 8.673226356506348
Epoch 440, val loss: 0.9870818257331848
Epoch 450, training loss: 87.72252655029297 = 0.9781394600868225 + 10.0 * 8.6744384765625
Epoch 450, val loss: 0.9817190766334534
Epoch 460, training loss: 87.74867248535156 = 0.9720853567123413 + 10.0 * 8.677659034729004
Epoch 460, val loss: 0.9758790731430054
Epoch 470, training loss: 87.76398468017578 = 0.9655988812446594 + 10.0 * 8.679838180541992
Epoch 470, val loss: 0.9696078896522522
Epoch 480, training loss: 87.86598205566406 = 0.9588810205459595 + 10.0 * 8.690710067749023
Epoch 480, val loss: 0.9630484580993652
Epoch 490, training loss: 87.94222259521484 = 0.9515494704246521 + 10.0 * 8.699068069458008
Epoch 490, val loss: 0.9561991691589355
Epoch 500, training loss: 87.80094909667969 = 0.9436660408973694 + 10.0 * 8.685728073120117
Epoch 500, val loss: 0.948643147945404
Epoch 510, training loss: 87.84134674072266 = 0.9349532127380371 + 10.0 * 8.69063949584961
Epoch 510, val loss: 0.9400984048843384
Epoch 520, training loss: 87.87834167480469 = 0.926559567451477 + 10.0 * 8.695178031921387
Epoch 520, val loss: 0.9320562481880188
Epoch 530, training loss: 87.88445281982422 = 0.9172537326812744 + 10.0 * 8.696720123291016
Epoch 530, val loss: 0.9231493473052979
Epoch 540, training loss: 87.91612243652344 = 0.9073070287704468 + 10.0 * 8.700881004333496
Epoch 540, val loss: 0.9136649370193481
Epoch 550, training loss: 87.87785339355469 = 0.8970946073532104 + 10.0 * 8.698076248168945
Epoch 550, val loss: 0.903803825378418
Epoch 560, training loss: 87.9388656616211 = 0.8864809274673462 + 10.0 * 8.705238342285156
Epoch 560, val loss: 0.8935308456420898
Epoch 570, training loss: 87.95649719238281 = 0.8752915263175964 + 10.0 * 8.708120346069336
Epoch 570, val loss: 0.8828542828559875
Epoch 580, training loss: 87.98624420166016 = 0.8638127446174622 + 10.0 * 8.71224308013916
Epoch 580, val loss: 0.871815025806427
Epoch 590, training loss: 88.0023422241211 = 0.8520135283470154 + 10.0 * 8.715032577514648
Epoch 590, val loss: 0.8605398535728455
Epoch 600, training loss: 88.06519317626953 = 0.8395426273345947 + 10.0 * 8.722564697265625
Epoch 600, val loss: 0.8486508131027222
Epoch 610, training loss: 87.93106842041016 = 0.8272988796234131 + 10.0 * 8.710376739501953
Epoch 610, val loss: 0.8370632529258728
Epoch 620, training loss: 87.99662017822266 = 0.8145819306373596 + 10.0 * 8.7182035446167
Epoch 620, val loss: 0.824689507484436
Epoch 630, training loss: 88.00920104980469 = 0.8017553687095642 + 10.0 * 8.720744132995605
Epoch 630, val loss: 0.8124709725379944
Epoch 640, training loss: 88.0386734008789 = 0.7884209156036377 + 10.0 * 8.725025177001953
Epoch 640, val loss: 0.7996869087219238
Epoch 650, training loss: 88.10541534423828 = 0.7750551700592041 + 10.0 * 8.733036041259766
Epoch 650, val loss: 0.7869008183479309
Epoch 660, training loss: 88.0450668334961 = 0.7610906958580017 + 10.0 * 8.728397369384766
Epoch 660, val loss: 0.7737105488777161
Epoch 670, training loss: 88.09477996826172 = 0.74759840965271 + 10.0 * 8.734718322753906
Epoch 670, val loss: 0.7608014345169067
Epoch 680, training loss: 88.12200164794922 = 0.7339240312576294 + 10.0 * 8.738807678222656
Epoch 680, val loss: 0.747890055179596
Epoch 690, training loss: 88.09225463867188 = 0.7200568914413452 + 10.0 * 8.73721981048584
Epoch 690, val loss: 0.7346713542938232
Epoch 700, training loss: 88.03267669677734 = 0.7062938213348389 + 10.0 * 8.732638359069824
Epoch 700, val loss: 0.7217863202095032
Epoch 710, training loss: 88.11831665039062 = 0.6931509971618652 + 10.0 * 8.74251651763916
Epoch 710, val loss: 0.7093132734298706
Epoch 720, training loss: 88.09497833251953 = 0.6802782416343689 + 10.0 * 8.741470336914062
Epoch 720, val loss: 0.697279155254364
Epoch 730, training loss: 88.15899658203125 = 0.6673943996429443 + 10.0 * 8.749159812927246
Epoch 730, val loss: 0.685218870639801
Epoch 740, training loss: 88.18486022949219 = 0.6548842787742615 + 10.0 * 8.752997398376465
Epoch 740, val loss: 0.6735502481460571
Epoch 750, training loss: 88.18144989013672 = 0.6424935460090637 + 10.0 * 8.75389575958252
Epoch 750, val loss: 0.6620877981185913
Epoch 760, training loss: 88.17778778076172 = 0.6308589577674866 + 10.0 * 8.754693031311035
Epoch 760, val loss: 0.6511748433113098
Epoch 770, training loss: 88.1951904296875 = 0.619439959526062 + 10.0 * 8.757575035095215
Epoch 770, val loss: 0.6407026648521423
Epoch 780, training loss: 88.20877075195312 = 0.6083762645721436 + 10.0 * 8.760039329528809
Epoch 780, val loss: 0.6304026246070862
Epoch 790, training loss: 88.24815368652344 = 0.5980120897293091 + 10.0 * 8.765013694763184
Epoch 790, val loss: 0.6209959387779236
Epoch 800, training loss: 88.24781799316406 = 0.5878279209136963 + 10.0 * 8.765998840332031
Epoch 800, val loss: 0.6117529273033142
Epoch 810, training loss: 88.26466369628906 = 0.5781837105751038 + 10.0 * 8.768648147583008
Epoch 810, val loss: 0.6031258702278137
Epoch 820, training loss: 88.3034896850586 = 0.5690504312515259 + 10.0 * 8.773443222045898
Epoch 820, val loss: 0.5949440598487854
Epoch 830, training loss: 88.26905822753906 = 0.5602560043334961 + 10.0 * 8.770879745483398
Epoch 830, val loss: 0.5872203707695007
Epoch 840, training loss: 88.33358001708984 = 0.5522019863128662 + 10.0 * 8.778138160705566
Epoch 840, val loss: 0.5800438523292542
Epoch 850, training loss: 88.3429183959961 = 0.5442894697189331 + 10.0 * 8.779863357543945
Epoch 850, val loss: 0.5732758641242981
Epoch 860, training loss: 88.36278533935547 = 0.5370410084724426 + 10.0 * 8.782574653625488
Epoch 860, val loss: 0.5669152736663818
Epoch 870, training loss: 88.21845245361328 = 0.5300129652023315 + 10.0 * 8.768843650817871
Epoch 870, val loss: 0.5608681440353394
Epoch 880, training loss: 88.2042465209961 = 0.5234014391899109 + 10.0 * 8.768084526062012
Epoch 880, val loss: 0.5552821755409241
Epoch 890, training loss: 88.27925872802734 = 0.5172659158706665 + 10.0 * 8.776199340820312
Epoch 890, val loss: 0.5501894354820251
Epoch 900, training loss: 88.37028503417969 = 0.5111624002456665 + 10.0 * 8.78591251373291
Epoch 900, val loss: 0.5452322363853455
Epoch 910, training loss: 88.3923110961914 = 0.505394697189331 + 10.0 * 8.788691520690918
Epoch 910, val loss: 0.5405035614967346
Epoch 920, training loss: 88.41983795166016 = 0.49982303380966187 + 10.0 * 8.792001724243164
Epoch 920, val loss: 0.5359320044517517
Epoch 930, training loss: 88.46646118164062 = 0.4945812225341797 + 10.0 * 8.797187805175781
Epoch 930, val loss: 0.5317193865776062
Epoch 940, training loss: 88.530517578125 = 0.4895576536655426 + 10.0 * 8.804096221923828
Epoch 940, val loss: 0.5277523398399353
Epoch 950, training loss: 88.51777648925781 = 0.48478126525878906 + 10.0 * 8.803299903869629
Epoch 950, val loss: 0.5241038799285889
Epoch 960, training loss: 88.56900787353516 = 0.4801858365535736 + 10.0 * 8.808881759643555
Epoch 960, val loss: 0.5205156803131104
Epoch 970, training loss: 88.56546020507812 = 0.47587552666664124 + 10.0 * 8.808958053588867
Epoch 970, val loss: 0.5173290371894836
Epoch 980, training loss: 88.59334564208984 = 0.4717930257320404 + 10.0 * 8.812154769897461
Epoch 980, val loss: 0.5142810940742493
Epoch 990, training loss: 88.64646911621094 = 0.46775445342063904 + 10.0 * 8.81787109375
Epoch 990, val loss: 0.511297881603241
Epoch 1000, training loss: 88.56379699707031 = 0.4639400243759155 + 10.0 * 8.809985160827637
Epoch 1000, val loss: 0.5084506869316101
Epoch 1010, training loss: 88.57328033447266 = 0.46041935682296753 + 10.0 * 8.811285972595215
Epoch 1010, val loss: 0.5059306621551514
Epoch 1020, training loss: 88.59407043457031 = 0.45673754811286926 + 10.0 * 8.813733100891113
Epoch 1020, val loss: 0.5031514763832092
Epoch 1030, training loss: 88.59454345703125 = 0.4535277485847473 + 10.0 * 8.814101219177246
Epoch 1030, val loss: 0.5009422302246094
Epoch 1040, training loss: 88.63892364501953 = 0.45036882162094116 + 10.0 * 8.818855285644531
Epoch 1040, val loss: 0.498884916305542
Epoch 1050, training loss: 88.70738220214844 = 0.4471043646335602 + 10.0 * 8.826027870178223
Epoch 1050, val loss: 0.4965583086013794
Epoch 1060, training loss: 88.73495483398438 = 0.4440283477306366 + 10.0 * 8.829092025756836
Epoch 1060, val loss: 0.4944503903388977
Epoch 1070, training loss: 88.74297332763672 = 0.44094228744506836 + 10.0 * 8.83020305633545
Epoch 1070, val loss: 0.4924493730068207
Epoch 1080, training loss: 88.77880859375 = 0.43807825446128845 + 10.0 * 8.834073066711426
Epoch 1080, val loss: 0.49058061838150024
Epoch 1090, training loss: 88.78282928466797 = 0.43521395325660706 + 10.0 * 8.834761619567871
Epoch 1090, val loss: 0.4886375963687897
Epoch 1100, training loss: 88.78713989257812 = 0.4324917197227478 + 10.0 * 8.835464477539062
Epoch 1100, val loss: 0.4869784712791443
Epoch 1110, training loss: 88.82571411132812 = 0.42986777424812317 + 10.0 * 8.839584350585938
Epoch 1110, val loss: 0.48539161682128906
Epoch 1120, training loss: 88.85936737060547 = 0.42727500200271606 + 10.0 * 8.843209266662598
Epoch 1120, val loss: 0.48377957940101624
Epoch 1130, training loss: 88.85946655273438 = 0.42478641867637634 + 10.0 * 8.843467712402344
Epoch 1130, val loss: 0.48222556710243225
Epoch 1140, training loss: 88.87393951416016 = 0.42239370942115784 + 10.0 * 8.845154762268066
Epoch 1140, val loss: 0.48076003789901733
Epoch 1150, training loss: 88.87681579589844 = 0.41996872425079346 + 10.0 * 8.845685005187988
Epoch 1150, val loss: 0.47921252250671387
Epoch 1160, training loss: 88.78723907470703 = 0.41767966747283936 + 10.0 * 8.836956024169922
Epoch 1160, val loss: 0.47804713249206543
Epoch 1170, training loss: 88.76879119873047 = 0.4155813753604889 + 10.0 * 8.835321426391602
Epoch 1170, val loss: 0.47701677680015564
Epoch 1180, training loss: 88.8571548461914 = 0.41354018449783325 + 10.0 * 8.844361305236816
Epoch 1180, val loss: 0.47564801573753357
Epoch 1190, training loss: 88.90666198730469 = 0.41142407059669495 + 10.0 * 8.849523544311523
Epoch 1190, val loss: 0.47441163659095764
Epoch 1200, training loss: 88.95932006835938 = 0.4092828035354614 + 10.0 * 8.855003356933594
Epoch 1200, val loss: 0.47316986322402954
Epoch 1210, training loss: 89.01911926269531 = 0.4071619510650635 + 10.0 * 8.86119556427002
Epoch 1210, val loss: 0.4719136655330658
Epoch 1220, training loss: 89.03633880615234 = 0.40508368611335754 + 10.0 * 8.863125801086426
Epoch 1220, val loss: 0.4707180857658386
Epoch 1230, training loss: 89.0392837524414 = 0.4030299186706543 + 10.0 * 8.863625526428223
Epoch 1230, val loss: 0.4695885181427002
Epoch 1240, training loss: 89.04193878173828 = 0.40107041597366333 + 10.0 * 8.864087104797363
Epoch 1240, val loss: 0.4684978127479553
Epoch 1250, training loss: 89.07000732421875 = 0.3991893529891968 + 10.0 * 8.867081642150879
Epoch 1250, val loss: 0.4675247073173523
Epoch 1260, training loss: 89.10897827148438 = 0.3972821831703186 + 10.0 * 8.871169090270996
Epoch 1260, val loss: 0.46652403473854065
Epoch 1270, training loss: 89.13507080078125 = 0.39545297622680664 + 10.0 * 8.873961448669434
Epoch 1270, val loss: 0.465508371591568
Epoch 1280, training loss: 89.12686157226562 = 0.3936440944671631 + 10.0 * 8.873321533203125
Epoch 1280, val loss: 0.4646080732345581
Epoch 1290, training loss: 89.11664581298828 = 0.39188051223754883 + 10.0 * 8.872476577758789
Epoch 1290, val loss: 0.46363866329193115
Epoch 1300, training loss: 89.12519073486328 = 0.3901141881942749 + 10.0 * 8.873507499694824
Epoch 1300, val loss: 0.4626518189907074
Epoch 1310, training loss: 89.1379623413086 = 0.3883775472640991 + 10.0 * 8.874958992004395
Epoch 1310, val loss: 0.46186864376068115
Epoch 1320, training loss: 89.18634796142578 = 0.38669106364250183 + 10.0 * 8.879965782165527
Epoch 1320, val loss: 0.46089887619018555
Epoch 1330, training loss: 89.21477508544922 = 0.38501426577568054 + 10.0 * 8.882975578308105
Epoch 1330, val loss: 0.46008265018463135
Epoch 1340, training loss: 89.23523712158203 = 0.3833451569080353 + 10.0 * 8.885189056396484
Epoch 1340, val loss: 0.4590773582458496
Epoch 1350, training loss: 89.23151397705078 = 0.38167354464530945 + 10.0 * 8.884984016418457
Epoch 1350, val loss: 0.4582616686820984
Epoch 1360, training loss: 89.25936889648438 = 0.3800680339336395 + 10.0 * 8.887929916381836
Epoch 1360, val loss: 0.45741745829582214
Epoch 1370, training loss: 89.27447509765625 = 0.378458708524704 + 10.0 * 8.889601707458496
Epoch 1370, val loss: 0.4566120207309723
Epoch 1380, training loss: 89.3000717163086 = 0.37691593170166016 + 10.0 * 8.892315864562988
Epoch 1380, val loss: 0.4558110237121582
Epoch 1390, training loss: 89.30256652832031 = 0.3753344714641571 + 10.0 * 8.892723083496094
Epoch 1390, val loss: 0.45492681860923767
Epoch 1400, training loss: 89.35601043701172 = 0.37379932403564453 + 10.0 * 8.898221015930176
Epoch 1400, val loss: 0.4543185234069824
Epoch 1410, training loss: 89.38713836669922 = 0.372317373752594 + 10.0 * 8.901482582092285
Epoch 1410, val loss: 0.45352140069007874
Epoch 1420, training loss: 89.35692596435547 = 0.37083953619003296 + 10.0 * 8.898608207702637
Epoch 1420, val loss: 0.4527517259120941
Epoch 1430, training loss: 89.36315155029297 = 0.36934494972229004 + 10.0 * 8.899380683898926
Epoch 1430, val loss: 0.45208725333213806
Epoch 1440, training loss: 89.41246032714844 = 0.36795371770858765 + 10.0 * 8.904451370239258
Epoch 1440, val loss: 0.4513902962207794
Epoch 1450, training loss: 89.4269027709961 = 0.3665071129798889 + 10.0 * 8.906039237976074
Epoch 1450, val loss: 0.4508073031902313
Epoch 1460, training loss: 89.40087127685547 = 0.3650735914707184 + 10.0 * 8.903579711914062
Epoch 1460, val loss: 0.45014941692352295
Epoch 1470, training loss: 89.4004135131836 = 0.3637300133705139 + 10.0 * 8.903668403625488
Epoch 1470, val loss: 0.449452668428421
Epoch 1480, training loss: 89.45143127441406 = 0.3623645007610321 + 10.0 * 8.908906936645508
Epoch 1480, val loss: 0.4488498568534851
Epoch 1490, training loss: 89.50640869140625 = 0.3612825274467468 + 10.0 * 8.914512634277344
Epoch 1490, val loss: 0.44881969690322876
Epoch 1500, training loss: 89.48969268798828 = 0.3600544035434723 + 10.0 * 8.9129638671875
Epoch 1500, val loss: 0.4479319155216217
Epoch 1510, training loss: 89.35541534423828 = 0.3585692346096039 + 10.0 * 8.89968490600586
Epoch 1510, val loss: 0.4472912549972534
Epoch 1520, training loss: 89.41191101074219 = 0.3572598397731781 + 10.0 * 8.905465126037598
Epoch 1520, val loss: 0.4466375708580017
Epoch 1530, training loss: 89.46710968017578 = 0.35593292117118835 + 10.0 * 8.911117553710938
Epoch 1530, val loss: 0.44593900442123413
Epoch 1540, training loss: 89.49813079833984 = 0.35459235310554504 + 10.0 * 8.91435432434082
Epoch 1540, val loss: 0.44543084502220154
Epoch 1550, training loss: 89.52759552001953 = 0.3532499670982361 + 10.0 * 8.917434692382812
Epoch 1550, val loss: 0.44482046365737915
Epoch 1560, training loss: 89.55351257324219 = 0.3519231975078583 + 10.0 * 8.920159339904785
Epoch 1560, val loss: 0.4441099762916565
Epoch 1570, training loss: 89.54975128173828 = 0.35062742233276367 + 10.0 * 8.919912338256836
Epoch 1570, val loss: 0.4436929225921631
Epoch 1580, training loss: 89.56948852539062 = 0.3493344783782959 + 10.0 * 8.922015190124512
Epoch 1580, val loss: 0.4430535137653351
Epoch 1590, training loss: 89.59513092041016 = 0.348036527633667 + 10.0 * 8.92470932006836
Epoch 1590, val loss: 0.44250649213790894
Epoch 1600, training loss: 89.58174896240234 = 0.3467552959918976 + 10.0 * 8.92349910736084
Epoch 1600, val loss: 0.4418709874153137
Epoch 1610, training loss: 89.63526153564453 = 0.3455096185207367 + 10.0 * 8.928975105285645
Epoch 1610, val loss: 0.4414895176887512
Epoch 1620, training loss: 89.65130615234375 = 0.3442150354385376 + 10.0 * 8.930708885192871
Epoch 1620, val loss: 0.4407975375652313
Epoch 1630, training loss: 89.60150909423828 = 0.34294772148132324 + 10.0 * 8.92585563659668
Epoch 1630, val loss: 0.4402840733528137
Epoch 1640, training loss: 89.63603210449219 = 0.3417398929595947 + 10.0 * 8.929429054260254
Epoch 1640, val loss: 0.43983206152915955
Epoch 1650, training loss: 89.67233276367188 = 0.3405202329158783 + 10.0 * 8.933180809020996
Epoch 1650, val loss: 0.43957504630088806
Epoch 1660, training loss: 89.7133560180664 = 0.33945387601852417 + 10.0 * 8.937390327453613
Epoch 1660, val loss: 0.4387996792793274
Epoch 1670, training loss: 89.60411071777344 = 0.3383462727069855 + 10.0 * 8.926576614379883
Epoch 1670, val loss: 0.4385121166706085
Epoch 1680, training loss: 89.62736511230469 = 0.33713144063949585 + 10.0 * 8.929023742675781
Epoch 1680, val loss: 0.43783098459243774
Epoch 1690, training loss: 89.6985092163086 = 0.33590492606163025 + 10.0 * 8.936260223388672
Epoch 1690, val loss: 0.4372591972351074
Epoch 1700, training loss: 89.75208282470703 = 0.33467772603034973 + 10.0 * 8.941740036010742
Epoch 1700, val loss: 0.43686050176620483
Epoch 1710, training loss: 89.77591705322266 = 0.33342477679252625 + 10.0 * 8.944249153137207
Epoch 1710, val loss: 0.43623214960098267
Epoch 1720, training loss: 89.6647720336914 = 0.33229610323905945 + 10.0 * 8.933247566223145
Epoch 1720, val loss: 0.4354602098464966
Epoch 1730, training loss: 89.6715316772461 = 0.33117982745170593 + 10.0 * 8.934035301208496
Epoch 1730, val loss: 0.43543127179145813
Epoch 1740, training loss: 89.72795104980469 = 0.3299952447414398 + 10.0 * 8.93979549407959
Epoch 1740, val loss: 0.4348786473274231
Epoch 1750, training loss: 89.80809020996094 = 0.32873260974884033 + 10.0 * 8.947935104370117
Epoch 1750, val loss: 0.43446287512779236
Epoch 1760, training loss: 89.83650207519531 = 0.32746508717536926 + 10.0 * 8.950902938842773
Epoch 1760, val loss: 0.43391555547714233
Epoch 1770, training loss: 89.8347396850586 = 0.32621946930885315 + 10.0 * 8.950852394104004
Epoch 1770, val loss: 0.4334447383880615
Epoch 1780, training loss: 89.84447479248047 = 0.32498639822006226 + 10.0 * 8.951948165893555
Epoch 1780, val loss: 0.4328945279121399
Epoch 1790, training loss: 89.86490631103516 = 0.3238036632537842 + 10.0 * 8.954110145568848
Epoch 1790, val loss: 0.4324150085449219
Epoch 1800, training loss: 89.9228744506836 = 0.32259228825569153 + 10.0 * 8.960027694702148
Epoch 1800, val loss: 0.43191835284233093
Epoch 1810, training loss: 89.93889617919922 = 0.3213639557361603 + 10.0 * 8.961752891540527
Epoch 1810, val loss: 0.43144071102142334
Epoch 1820, training loss: 89.92560577392578 = 0.3201635181903839 + 10.0 * 8.96054458618164
Epoch 1820, val loss: 0.4310240149497986
Epoch 1830, training loss: 89.91722869873047 = 0.319021075963974 + 10.0 * 8.959820747375488
Epoch 1830, val loss: 0.43069082498550415
Epoch 1840, training loss: 89.95826721191406 = 0.31783822178840637 + 10.0 * 8.964042663574219
Epoch 1840, val loss: 0.4302203059196472
Epoch 1850, training loss: 89.93853759765625 = 0.31664443016052246 + 10.0 * 8.962189674377441
Epoch 1850, val loss: 0.42963165044784546
Epoch 1860, training loss: 89.94837188720703 = 0.3155120015144348 + 10.0 * 8.963285446166992
Epoch 1860, val loss: 0.42932042479515076
Epoch 1870, training loss: 89.98075103759766 = 0.31432461738586426 + 10.0 * 8.966642379760742
Epoch 1870, val loss: 0.42884957790374756
Epoch 1880, training loss: 90.04354858398438 = 0.3131088316440582 + 10.0 * 8.973043441772461
Epoch 1880, val loss: 0.4283097982406616
Epoch 1890, training loss: 90.03286743164062 = 0.3119015097618103 + 10.0 * 8.97209644317627
Epoch 1890, val loss: 0.4279460310935974
Epoch 1900, training loss: 90.02749633789062 = 0.3107045888900757 + 10.0 * 8.971678733825684
Epoch 1900, val loss: 0.4274611473083496
Epoch 1910, training loss: 90.04978942871094 = 0.30955204367637634 + 10.0 * 8.974023818969727
Epoch 1910, val loss: 0.4271451532840729
Epoch 1920, training loss: 90.0407485961914 = 0.30841904878616333 + 10.0 * 8.973233222961426
Epoch 1920, val loss: 0.4266059994697571
Epoch 1930, training loss: 90.0506362915039 = 0.3072703182697296 + 10.0 * 8.974336624145508
Epoch 1930, val loss: 0.42620915174484253
Epoch 1940, training loss: 90.00005340576172 = 0.3060956299304962 + 10.0 * 8.969395637512207
Epoch 1940, val loss: 0.4255048334598541
Epoch 1950, training loss: 89.85107421875 = 0.30520591139793396 + 10.0 * 8.95458698272705
Epoch 1950, val loss: 0.4256404936313629
Epoch 1960, training loss: 89.69396209716797 = 0.30431661009788513 + 10.0 * 8.93896484375
Epoch 1960, val loss: 0.4256848394870758
Epoch 1970, training loss: 89.61180877685547 = 0.30356457829475403 + 10.0 * 8.930824279785156
Epoch 1970, val loss: 0.4242253303527832
Epoch 1980, training loss: 89.7193374633789 = 0.30231189727783203 + 10.0 * 8.941701889038086
Epoch 1980, val loss: 0.4250364303588867
Epoch 1990, training loss: 89.78164672851562 = 0.3011733293533325 + 10.0 * 8.948047637939453
Epoch 1990, val loss: 0.4239841401576996
Epoch 2000, training loss: 89.86109161376953 = 0.2999725639820099 + 10.0 * 8.956111907958984
Epoch 2000, val loss: 0.4238297641277313
Epoch 2010, training loss: 89.94625091552734 = 0.2987329363822937 + 10.0 * 8.964751243591309
Epoch 2010, val loss: 0.4231709837913513
Epoch 2020, training loss: 90.03125762939453 = 0.2974846363067627 + 10.0 * 8.973377227783203
Epoch 2020, val loss: 0.4228263199329376
Epoch 2030, training loss: 90.05351257324219 = 0.29621055722236633 + 10.0 * 8.975729942321777
Epoch 2030, val loss: 0.42222198843955994
Epoch 2040, training loss: 90.08332824707031 = 0.2949587106704712 + 10.0 * 8.978837013244629
Epoch 2040, val loss: 0.42177626490592957
Epoch 2050, training loss: 90.10877990722656 = 0.29371339082717896 + 10.0 * 8.98150634765625
Epoch 2050, val loss: 0.4213140308856964
Epoch 2060, training loss: 90.11383819580078 = 0.2924695611000061 + 10.0 * 8.982136726379395
Epoch 2060, val loss: 0.42081475257873535
Epoch 2070, training loss: 90.1139907836914 = 0.2912307679653168 + 10.0 * 8.98227596282959
Epoch 2070, val loss: 0.4205169379711151
Epoch 2080, training loss: 90.14096069335938 = 0.2900277376174927 + 10.0 * 8.985093116760254
Epoch 2080, val loss: 0.41996216773986816
Epoch 2090, training loss: 90.16667938232422 = 0.28878310322761536 + 10.0 * 8.98779010772705
Epoch 2090, val loss: 0.4195738434791565
Epoch 2100, training loss: 90.1602783203125 = 0.28754809498786926 + 10.0 * 8.987272262573242
Epoch 2100, val loss: 0.4193022549152374
Epoch 2110, training loss: 90.1718978881836 = 0.2863297462463379 + 10.0 * 8.988556861877441
Epoch 2110, val loss: 0.4189281761646271
Epoch 2120, training loss: 90.1561279296875 = 0.28509342670440674 + 10.0 * 8.987103462219238
Epoch 2120, val loss: 0.4183184504508972
Epoch 2130, training loss: 90.08646392822266 = 0.2839427888393402 + 10.0 * 8.980252265930176
Epoch 2130, val loss: 0.41797828674316406
Epoch 2140, training loss: 90.10310363769531 = 0.28285473585128784 + 10.0 * 8.982025146484375
Epoch 2140, val loss: 0.41789838671684265
Epoch 2150, training loss: 90.16583251953125 = 0.28163036704063416 + 10.0 * 8.988420486450195
Epoch 2150, val loss: 0.4174770414829254
Epoch 2160, training loss: 90.20791625976562 = 0.28035926818847656 + 10.0 * 8.992755889892578
Epoch 2160, val loss: 0.41710516810417175
Epoch 2170, training loss: 90.19390106201172 = 0.27908051013946533 + 10.0 * 8.99148178100586
Epoch 2170, val loss: 0.4167855679988861
Epoch 2180, training loss: 90.23247528076172 = 0.2778324782848358 + 10.0 * 8.995464324951172
Epoch 2180, val loss: 0.4163520336151123
Epoch 2190, training loss: 90.24476623535156 = 0.27656474709510803 + 10.0 * 8.996820449829102
Epoch 2190, val loss: 0.4159390926361084
Epoch 2200, training loss: 90.23651123046875 = 0.27532804012298584 + 10.0 * 8.996118545532227
Epoch 2200, val loss: 0.41565749049186707
Epoch 2210, training loss: 90.2647476196289 = 0.27413058280944824 + 10.0 * 8.999061584472656
Epoch 2210, val loss: 0.41523855924606323
Epoch 2220, training loss: 90.31256103515625 = 0.2728995978832245 + 10.0 * 9.003966331481934
Epoch 2220, val loss: 0.4149540960788727
Epoch 2230, training loss: 90.29989624023438 = 0.27162501215934753 + 10.0 * 9.002827644348145
Epoch 2230, val loss: 0.4146082103252411
Epoch 2240, training loss: 90.32855224609375 = 0.27038297057151794 + 10.0 * 9.005816459655762
Epoch 2240, val loss: 0.41431474685668945
Epoch 2250, training loss: 90.35537719726562 = 0.26913172006607056 + 10.0 * 9.008624076843262
Epoch 2250, val loss: 0.41401392221450806
Epoch 2260, training loss: 90.21965789794922 = 0.2679072618484497 + 10.0 * 8.9951753616333
Epoch 2260, val loss: 0.413710355758667
Epoch 2270, training loss: 90.08585357666016 = 0.2667922079563141 + 10.0 * 8.981905937194824
Epoch 2270, val loss: 0.41302114725112915
Epoch 2280, training loss: 90.13449096679688 = 0.2656281590461731 + 10.0 * 8.986886978149414
Epoch 2280, val loss: 0.41321006417274475
Epoch 2290, training loss: 90.243896484375 = 0.2644515931606293 + 10.0 * 8.997944831848145
Epoch 2290, val loss: 0.4133188724517822
Epoch 2300, training loss: 90.32704162597656 = 0.26316818594932556 + 10.0 * 9.006387710571289
Epoch 2300, val loss: 0.41268494725227356
Epoch 2310, training loss: 90.4058837890625 = 0.261886864900589 + 10.0 * 9.014399528503418
Epoch 2310, val loss: 0.4123118817806244
Epoch 2320, training loss: 90.39161682128906 = 0.26060712337493896 + 10.0 * 9.013101577758789
Epoch 2320, val loss: 0.4118514358997345
Epoch 2330, training loss: 90.38960266113281 = 0.25933197140693665 + 10.0 * 9.01302719116211
Epoch 2330, val loss: 0.41175180673599243
Epoch 2340, training loss: 90.41813659667969 = 0.2580626308917999 + 10.0 * 9.016007423400879
Epoch 2340, val loss: 0.41143712401390076
Epoch 2350, training loss: 90.39603424072266 = 0.2567686140537262 + 10.0 * 9.01392650604248
Epoch 2350, val loss: 0.4112382233142853
Epoch 2360, training loss: 90.41510772705078 = 0.2555040121078491 + 10.0 * 9.015960693359375
Epoch 2360, val loss: 0.41102829575538635
Epoch 2370, training loss: 90.4085464477539 = 0.2542387843132019 + 10.0 * 9.015430450439453
Epoch 2370, val loss: 0.41074806451797485
Epoch 2380, training loss: 90.43648529052734 = 0.25297999382019043 + 10.0 * 9.018350601196289
Epoch 2380, val loss: 0.41051581501960754
Epoch 2390, training loss: 90.47830963134766 = 0.2517091929912567 + 10.0 * 9.022660255432129
Epoch 2390, val loss: 0.41032272577285767
Epoch 2400, training loss: 90.1122817993164 = 0.2507670819759369 + 10.0 * 8.986150741577148
Epoch 2400, val loss: 0.411234587430954
Epoch 2410, training loss: 90.20541381835938 = 0.24965178966522217 + 10.0 * 8.995576858520508
Epoch 2410, val loss: 0.41022589802742004
Epoch 2420, training loss: 90.34088897705078 = 0.24845963716506958 + 10.0 * 9.00924301147461
Epoch 2420, val loss: 0.41039568185806274
Epoch 2430, training loss: 90.43360137939453 = 0.24714092910289764 + 10.0 * 9.018646240234375
Epoch 2430, val loss: 0.4097305238246918
Epoch 2440, training loss: 90.4713134765625 = 0.24580521881580353 + 10.0 * 9.022550582885742
Epoch 2440, val loss: 0.40953534841537476
Epoch 2450, training loss: 90.4785385131836 = 0.24446330964565277 + 10.0 * 9.023407936096191
Epoch 2450, val loss: 0.40939202904701233
Epoch 2460, training loss: 90.48062896728516 = 0.2431313842535019 + 10.0 * 9.023749351501465
Epoch 2460, val loss: 0.40931859612464905
Epoch 2470, training loss: 90.48776245117188 = 0.24181954562664032 + 10.0 * 9.0245943069458
Epoch 2470, val loss: 0.40916067361831665
Epoch 2480, training loss: 90.42137145996094 = 0.24059505760669708 + 10.0 * 9.018077850341797
Epoch 2480, val loss: 0.40901002287864685
Epoch 2490, training loss: 90.43846893310547 = 0.23929482698440552 + 10.0 * 9.019917488098145
Epoch 2490, val loss: 0.4087606966495514
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8510144927536232
0.8651017894660582
=== training gcn model ===
Epoch 0, training loss: 102.75907135009766 = 1.1018458604812622 + 10.0 * 10.165722846984863
Epoch 0, val loss: 1.1003683805465698
Epoch 10, training loss: 98.21837615966797 = 1.097925066947937 + 10.0 * 9.712045669555664
Epoch 10, val loss: 1.0965303182601929
Epoch 20, training loss: 96.0994644165039 = 1.0942631959915161 + 10.0 * 9.500520706176758
Epoch 20, val loss: 1.0929111242294312
Epoch 30, training loss: 94.66024017333984 = 1.0907033681869507 + 10.0 * 9.356953620910645
Epoch 30, val loss: 1.089421272277832
Epoch 40, training loss: 93.5542221069336 = 1.087367057800293 + 10.0 * 9.246685981750488
Epoch 40, val loss: 1.086164116859436
Epoch 50, training loss: 92.6440200805664 = 1.0842370986938477 + 10.0 * 9.155978202819824
Epoch 50, val loss: 1.08311128616333
Epoch 60, training loss: 91.90032958984375 = 1.0812711715698242 + 10.0 * 9.08190631866455
Epoch 60, val loss: 1.0802302360534668
Epoch 70, training loss: 91.25906372070312 = 1.078515648841858 + 10.0 * 9.018054962158203
Epoch 70, val loss: 1.0775448083877563
Epoch 80, training loss: 90.72091674804688 = 1.0758733749389648 + 10.0 * 8.96450424194336
Epoch 80, val loss: 1.0749914646148682
Epoch 90, training loss: 90.25218963623047 = 1.0733699798583984 + 10.0 * 8.917881965637207
Epoch 90, val loss: 1.0725727081298828
Epoch 100, training loss: 89.86040496826172 = 1.0710363388061523 + 10.0 * 8.878936767578125
Epoch 100, val loss: 1.070322036743164
Epoch 110, training loss: 89.51513671875 = 1.0687967538833618 + 10.0 * 8.844634056091309
Epoch 110, val loss: 1.068170189857483
Epoch 120, training loss: 89.22920989990234 = 1.0666667222976685 + 10.0 * 8.816254615783691
Epoch 120, val loss: 1.0661269426345825
Epoch 130, training loss: 88.99441528320312 = 1.0646415948867798 + 10.0 * 8.792977333068848
Epoch 130, val loss: 1.0641932487487793
Epoch 140, training loss: 88.77019500732422 = 1.0627096891403198 + 10.0 * 8.77074909210205
Epoch 140, val loss: 1.0623576641082764
Epoch 150, training loss: 88.5810317993164 = 1.0608407258987427 + 10.0 * 8.752018928527832
Epoch 150, val loss: 1.060590386390686
Epoch 160, training loss: 88.41357421875 = 1.0591013431549072 + 10.0 * 8.73544692993164
Epoch 160, val loss: 1.0589370727539062
Epoch 170, training loss: 88.28195190429688 = 1.057435154914856 + 10.0 * 8.722452163696289
Epoch 170, val loss: 1.0573760271072388
Epoch 180, training loss: 88.14555358886719 = 1.055727243423462 + 10.0 * 8.708982467651367
Epoch 180, val loss: 1.0557811260223389
Epoch 190, training loss: 88.02281951904297 = 1.0541001558303833 + 10.0 * 8.696871757507324
Epoch 190, val loss: 1.0542399883270264
Epoch 200, training loss: 87.94583129882812 = 1.0525096654891968 + 10.0 * 8.689332008361816
Epoch 200, val loss: 1.0527480840682983
Epoch 210, training loss: 87.86955261230469 = 1.050895094871521 + 10.0 * 8.681865692138672
Epoch 210, val loss: 1.0512603521347046
Epoch 220, training loss: 87.79131317138672 = 1.0492579936981201 + 10.0 * 8.674205780029297
Epoch 220, val loss: 1.0497184991836548
Epoch 230, training loss: 87.72288513183594 = 1.0475852489471436 + 10.0 * 8.667530059814453
Epoch 230, val loss: 1.048111081123352
Epoch 240, training loss: 87.70570373535156 = 1.0459014177322388 + 10.0 * 8.665980339050293
Epoch 240, val loss: 1.046548843383789
Epoch 250, training loss: 87.60645294189453 = 1.0440641641616821 + 10.0 * 8.656238555908203
Epoch 250, val loss: 1.0448445081710815
Epoch 260, training loss: 87.59651184082031 = 1.0421831607818604 + 10.0 * 8.65543270111084
Epoch 260, val loss: 1.0430595874786377
Epoch 270, training loss: 87.5772705078125 = 1.040254831314087 + 10.0 * 8.653701782226562
Epoch 270, val loss: 1.0412777662277222
Epoch 280, training loss: 87.6238784790039 = 1.038000464439392 + 10.0 * 8.658587455749512
Epoch 280, val loss: 1.039065957069397
Epoch 290, training loss: 87.6419906616211 = 1.0359141826629639 + 10.0 * 8.660608291625977
Epoch 290, val loss: 1.0371599197387695
Epoch 300, training loss: 87.46818542480469 = 1.033233642578125 + 10.0 * 8.643495559692383
Epoch 300, val loss: 1.0346555709838867
Epoch 310, training loss: 87.4706039428711 = 1.030754804611206 + 10.0 * 8.6439847946167
Epoch 310, val loss: 1.0323129892349243
Epoch 320, training loss: 87.44510650634766 = 1.0278648138046265 + 10.0 * 8.641724586486816
Epoch 320, val loss: 1.0295233726501465
Epoch 330, training loss: 87.46807098388672 = 1.0249102115631104 + 10.0 * 8.644315719604492
Epoch 330, val loss: 1.0267351865768433
Epoch 340, training loss: 87.45045471191406 = 1.0215011835098267 + 10.0 * 8.642895698547363
Epoch 340, val loss: 1.0234912633895874
Epoch 350, training loss: 87.41366577148438 = 1.0178852081298828 + 10.0 * 8.639577865600586
Epoch 350, val loss: 1.020089864730835
Epoch 360, training loss: 87.42420196533203 = 1.0141745805740356 + 10.0 * 8.641002655029297
Epoch 360, val loss: 1.0165228843688965
Epoch 370, training loss: 87.43401336669922 = 1.0100139379501343 + 10.0 * 8.642399787902832
Epoch 370, val loss: 1.0125120878219604
Epoch 380, training loss: 87.43408966064453 = 1.0056540966033936 + 10.0 * 8.642843246459961
Epoch 380, val loss: 1.0083824396133423
Epoch 390, training loss: 87.42102813720703 = 1.000910758972168 + 10.0 * 8.642011642456055
Epoch 390, val loss: 1.003885269165039
Epoch 400, training loss: 87.3926010131836 = 0.9958986043930054 + 10.0 * 8.639670372009277
Epoch 400, val loss: 0.9990786910057068
Epoch 410, training loss: 87.38232421875 = 0.9905332922935486 + 10.0 * 8.639179229736328
Epoch 410, val loss: 0.9939254522323608
Epoch 420, training loss: 87.37247467041016 = 0.9846968054771423 + 10.0 * 8.638777732849121
Epoch 420, val loss: 0.9884291291236877
Epoch 430, training loss: 87.3958969116211 = 0.9785042405128479 + 10.0 * 8.641738891601562
Epoch 430, val loss: 0.9825250506401062
Epoch 440, training loss: 87.35287475585938 = 0.9717229008674622 + 10.0 * 8.638114929199219
Epoch 440, val loss: 0.9760936498641968
Epoch 450, training loss: 87.34976959228516 = 0.9648224115371704 + 10.0 * 8.638494491577148
Epoch 450, val loss: 0.9695025086402893
Epoch 460, training loss: 87.35636901855469 = 0.957305371761322 + 10.0 * 8.63990592956543
Epoch 460, val loss: 0.9623268246650696
Epoch 470, training loss: 87.37386322021484 = 0.9492091536521912 + 10.0 * 8.642465591430664
Epoch 470, val loss: 0.9546586275100708
Epoch 480, training loss: 87.38790893554688 = 0.940955638885498 + 10.0 * 8.644695281982422
Epoch 480, val loss: 0.9469101428985596
Epoch 490, training loss: 87.37992095947266 = 0.9322420358657837 + 10.0 * 8.644767761230469
Epoch 490, val loss: 0.9387148022651672
Epoch 500, training loss: 87.34709930419922 = 0.9229516983032227 + 10.0 * 8.642415046691895
Epoch 500, val loss: 0.9298793077468872
Epoch 510, training loss: 87.39441680908203 = 0.9132333993911743 + 10.0 * 8.64811897277832
Epoch 510, val loss: 0.9206935167312622
Epoch 520, training loss: 87.41402435302734 = 0.9031907916069031 + 10.0 * 8.651082992553711
Epoch 520, val loss: 0.9111568927764893
Epoch 530, training loss: 87.39534759521484 = 0.8925482034683228 + 10.0 * 8.650279998779297
Epoch 530, val loss: 0.9011427164077759
Epoch 540, training loss: 87.42354583740234 = 0.881523072719574 + 10.0 * 8.654202461242676
Epoch 540, val loss: 0.8907003402709961
Epoch 550, training loss: 87.4267807006836 = 0.8701580762863159 + 10.0 * 8.655662536621094
Epoch 550, val loss: 0.8800205588340759
Epoch 560, training loss: 87.43343353271484 = 0.8582764267921448 + 10.0 * 8.657515525817871
Epoch 560, val loss: 0.8688420653343201
Epoch 570, training loss: 87.40890502929688 = 0.8461185097694397 + 10.0 * 8.656278610229492
Epoch 570, val loss: 0.85750412940979
Epoch 580, training loss: 87.3660888671875 = 0.8333714604377747 + 10.0 * 8.653271675109863
Epoch 580, val loss: 0.8455310463905334
Epoch 590, training loss: 87.41716766357422 = 0.8209272623062134 + 10.0 * 8.659624099731445
Epoch 590, val loss: 0.8339622020721436
Epoch 600, training loss: 87.42076873779297 = 0.8080728650093079 + 10.0 * 8.661269187927246
Epoch 600, val loss: 0.8220340013504028
Epoch 610, training loss: 87.45663452148438 = 0.7948815822601318 + 10.0 * 8.66617488861084
Epoch 610, val loss: 0.8096776604652405
Epoch 620, training loss: 87.49391174316406 = 0.7816093564033508 + 10.0 * 8.67123031616211
Epoch 620, val loss: 0.7973277568817139
Epoch 630, training loss: 87.4866714477539 = 0.7680795192718506 + 10.0 * 8.671858787536621
Epoch 630, val loss: 0.7846525311470032
Epoch 640, training loss: 87.48699188232422 = 0.7546185851097107 + 10.0 * 8.673237800598145
Epoch 640, val loss: 0.7722824215888977
Epoch 650, training loss: 87.50662994384766 = 0.7411144375801086 + 10.0 * 8.676551818847656
Epoch 650, val loss: 0.7598087787628174
Epoch 660, training loss: 87.54328155517578 = 0.7276959419250488 + 10.0 * 8.681558609008789
Epoch 660, val loss: 0.7475640773773193
Epoch 670, training loss: 87.49442291259766 = 0.7140856385231018 + 10.0 * 8.678033828735352
Epoch 670, val loss: 0.7351164221763611
Epoch 680, training loss: 87.52247619628906 = 0.7009246945381165 + 10.0 * 8.682154655456543
Epoch 680, val loss: 0.7231096625328064
Epoch 690, training loss: 87.56352233886719 = 0.6879186034202576 + 10.0 * 8.687560081481934
Epoch 690, val loss: 0.7111661434173584
Epoch 700, training loss: 87.48835754394531 = 0.6749739050865173 + 10.0 * 8.6813383102417
Epoch 700, val loss: 0.699415385723114
Epoch 710, training loss: 87.48596954345703 = 0.6627838611602783 + 10.0 * 8.682318687438965
Epoch 710, val loss: 0.6884928941726685
Epoch 720, training loss: 87.44051361083984 = 0.6499367356300354 + 10.0 * 8.679057121276855
Epoch 720, val loss: 0.6773512363433838
Epoch 730, training loss: 87.70623779296875 = 0.6400940418243408 + 10.0 * 8.70661449432373
Epoch 730, val loss: 0.667748749256134
Epoch 740, training loss: 87.51823425292969 = 0.6285883784294128 + 10.0 * 8.68896484375
Epoch 740, val loss: 0.6575192809104919
Epoch 750, training loss: 87.42340087890625 = 0.6177955865859985 + 10.0 * 8.680560111999512
Epoch 750, val loss: 0.6478668451309204
Epoch 760, training loss: 87.4559326171875 = 0.6077494025230408 + 10.0 * 8.684818267822266
Epoch 760, val loss: 0.6391618847846985
Epoch 770, training loss: 87.42518615722656 = 0.5978397727012634 + 10.0 * 8.682734489440918
Epoch 770, val loss: 0.6303752660751343
Epoch 780, training loss: 87.46745300292969 = 0.5885204076766968 + 10.0 * 8.68789291381836
Epoch 780, val loss: 0.6219826340675354
Epoch 790, training loss: 87.43968963623047 = 0.5791375041007996 + 10.0 * 8.686055183410645
Epoch 790, val loss: 0.6140238046646118
Epoch 800, training loss: 87.50296020507812 = 0.5702183246612549 + 10.0 * 8.693273544311523
Epoch 800, val loss: 0.6061481833457947
Epoch 810, training loss: 87.5439453125 = 0.561899721622467 + 10.0 * 8.69820499420166
Epoch 810, val loss: 0.5987047553062439
Epoch 820, training loss: 87.58059692382812 = 0.5538893342018127 + 10.0 * 8.70267105102539
Epoch 820, val loss: 0.5916442275047302
Epoch 830, training loss: 87.5955810546875 = 0.546211838722229 + 10.0 * 8.704936981201172
Epoch 830, val loss: 0.5849371552467346
Epoch 840, training loss: 87.63080596923828 = 0.5388538241386414 + 10.0 * 8.709195137023926
Epoch 840, val loss: 0.5785329937934875
Epoch 850, training loss: 87.64823913574219 = 0.5319527983665466 + 10.0 * 8.711628913879395
Epoch 850, val loss: 0.5726000666618347
Epoch 860, training loss: 87.69208526611328 = 0.5254117250442505 + 10.0 * 8.716667175292969
Epoch 860, val loss: 0.5668681859970093
Epoch 870, training loss: 87.70860290527344 = 0.5191522240638733 + 10.0 * 8.718945503234863
Epoch 870, val loss: 0.5614821314811707
Epoch 880, training loss: 87.69590759277344 = 0.5131797194480896 + 10.0 * 8.718273162841797
Epoch 880, val loss: 0.5563920140266418
Epoch 890, training loss: 87.66353607177734 = 0.5075584053993225 + 10.0 * 8.715597152709961
Epoch 890, val loss: 0.5517281889915466
Epoch 900, training loss: 87.69705963134766 = 0.5025067329406738 + 10.0 * 8.719454765319824
Epoch 900, val loss: 0.5473737120628357
Epoch 910, training loss: 87.71302795410156 = 0.4972860813140869 + 10.0 * 8.721574783325195
Epoch 910, val loss: 0.5429558157920837
Epoch 920, training loss: 87.7656021118164 = 0.49238094687461853 + 10.0 * 8.727322578430176
Epoch 920, val loss: 0.5388633012771606
Epoch 930, training loss: 87.79369354248047 = 0.4876418709754944 + 10.0 * 8.730605125427246
Epoch 930, val loss: 0.5350402593612671
Epoch 940, training loss: 87.80370330810547 = 0.4831601679325104 + 10.0 * 8.732053756713867
Epoch 940, val loss: 0.5313857197761536
Epoch 950, training loss: 87.81109619140625 = 0.47883322834968567 + 10.0 * 8.73322582244873
Epoch 950, val loss: 0.5277249813079834
Epoch 960, training loss: 87.82610321044922 = 0.4746563136577606 + 10.0 * 8.73514461517334
Epoch 960, val loss: 0.5244001746177673
Epoch 970, training loss: 87.84856414794922 = 0.47067752480506897 + 10.0 * 8.737788200378418
Epoch 970, val loss: 0.521130383014679
Epoch 980, training loss: 87.8387222290039 = 0.4669008255004883 + 10.0 * 8.737181663513184
Epoch 980, val loss: 0.5181699395179749
Epoch 990, training loss: 87.88445281982422 = 0.4632713496685028 + 10.0 * 8.742117881774902
Epoch 990, val loss: 0.5153228640556335
Epoch 1000, training loss: 87.86939239501953 = 0.45983365178108215 + 10.0 * 8.74095630645752
Epoch 1000, val loss: 0.5125304460525513
Epoch 1010, training loss: 87.94183349609375 = 0.4565414786338806 + 10.0 * 8.748529434204102
Epoch 1010, val loss: 0.509932279586792
Epoch 1020, training loss: 87.787841796875 = 0.45317310094833374 + 10.0 * 8.733467102050781
Epoch 1020, val loss: 0.5075944662094116
Epoch 1030, training loss: 87.79489135742188 = 0.4504185616970062 + 10.0 * 8.734447479248047
Epoch 1030, val loss: 0.5052183270454407
Epoch 1040, training loss: 87.86815643310547 = 0.4474605321884155 + 10.0 * 8.742069244384766
Epoch 1040, val loss: 0.5028550028800964
Epoch 1050, training loss: 87.87625122070312 = 0.44457340240478516 + 10.0 * 8.743167877197266
Epoch 1050, val loss: 0.5008588433265686
Epoch 1060, training loss: 87.9292221069336 = 0.4417007267475128 + 10.0 * 8.748751640319824
Epoch 1060, val loss: 0.4987093508243561
Epoch 1070, training loss: 87.97562408447266 = 0.43888038396835327 + 10.0 * 8.753674507141113
Epoch 1070, val loss: 0.4964580237865448
Epoch 1080, training loss: 87.98359680175781 = 0.43613046407699585 + 10.0 * 8.75474739074707
Epoch 1080, val loss: 0.49455833435058594
Epoch 1090, training loss: 88.00180053710938 = 0.43348515033721924 + 10.0 * 8.756831169128418
Epoch 1090, val loss: 0.4925622344017029
Epoch 1100, training loss: 88.01213073730469 = 0.43091925978660583 + 10.0 * 8.758121490478516
Epoch 1100, val loss: 0.49069324135780334
Epoch 1110, training loss: 87.98970031738281 = 0.428313672542572 + 10.0 * 8.756138801574707
Epoch 1110, val loss: 0.4887780249118805
Epoch 1120, training loss: 87.99703979492188 = 0.4259031116962433 + 10.0 * 8.757113456726074
Epoch 1120, val loss: 0.4872083365917206
Epoch 1130, training loss: 88.06172943115234 = 0.42357152700424194 + 10.0 * 8.763815879821777
Epoch 1130, val loss: 0.4855056703090668
Epoch 1140, training loss: 88.09986114501953 = 0.42119601368904114 + 10.0 * 8.767866134643555
Epoch 1140, val loss: 0.4839933514595032
Epoch 1150, training loss: 88.08236694335938 = 0.4188224971294403 + 10.0 * 8.76635456085205
Epoch 1150, val loss: 0.4824332296848297
Epoch 1160, training loss: 88.1067886352539 = 0.41658133268356323 + 10.0 * 8.769021034240723
Epoch 1160, val loss: 0.4807393550872803
Epoch 1170, training loss: 88.13423919677734 = 0.4143429696559906 + 10.0 * 8.771989822387695
Epoch 1170, val loss: 0.4793440103530884
Epoch 1180, training loss: 88.12390899658203 = 0.4121214747428894 + 10.0 * 8.771178245544434
Epoch 1180, val loss: 0.47784408926963806
Epoch 1190, training loss: 88.15782165527344 = 0.41000571846961975 + 10.0 * 8.774782180786133
Epoch 1190, val loss: 0.4763553738594055
Epoch 1200, training loss: 88.1635971069336 = 0.40788719058036804 + 10.0 * 8.7755708694458
Epoch 1200, val loss: 0.4751722812652588
Epoch 1210, training loss: 88.1751937866211 = 0.4058435261249542 + 10.0 * 8.776934623718262
Epoch 1210, val loss: 0.47371402382850647
Epoch 1220, training loss: 88.1879653930664 = 0.40378016233444214 + 10.0 * 8.77841854095459
Epoch 1220, val loss: 0.47243839502334595
Epoch 1230, training loss: 88.21375274658203 = 0.401784211397171 + 10.0 * 8.781196594238281
Epoch 1230, val loss: 0.4711681306362152
Epoch 1240, training loss: 88.21392822265625 = 0.3997918665409088 + 10.0 * 8.781413078308105
Epoch 1240, val loss: 0.46984681487083435
Epoch 1250, training loss: 88.21891784667969 = 0.3978261351585388 + 10.0 * 8.782109260559082
Epoch 1250, val loss: 0.46873241662979126
Epoch 1260, training loss: 88.27315521240234 = 0.39595919847488403 + 10.0 * 8.7877197265625
Epoch 1260, val loss: 0.46740826964378357
Epoch 1270, training loss: 88.2598876953125 = 0.3940211832523346 + 10.0 * 8.78658676147461
Epoch 1270, val loss: 0.4665006697177887
Epoch 1280, training loss: 88.27262115478516 = 0.39214035868644714 + 10.0 * 8.788047790527344
Epoch 1280, val loss: 0.46536171436309814
Epoch 1290, training loss: 88.28585815429688 = 0.3903012275695801 + 10.0 * 8.789555549621582
Epoch 1290, val loss: 0.46411025524139404
Epoch 1300, training loss: 88.26394653320312 = 0.38848742842674255 + 10.0 * 8.787546157836914
Epoch 1300, val loss: 0.46318212151527405
Epoch 1310, training loss: 88.2839126586914 = 0.3866669833660126 + 10.0 * 8.789724349975586
Epoch 1310, val loss: 0.4619534909725189
Epoch 1320, training loss: 88.32763671875 = 0.3848424553871155 + 10.0 * 8.794279098510742
Epoch 1320, val loss: 0.46105942130088806
Epoch 1330, training loss: 88.30925750732422 = 0.38307517766952515 + 10.0 * 8.792618751525879
Epoch 1330, val loss: 0.4599206745624542
Epoch 1340, training loss: 88.32571411132812 = 0.3813081383705139 + 10.0 * 8.794440269470215
Epoch 1340, val loss: 0.45899873971939087
Epoch 1350, training loss: 88.36734008789062 = 0.3795219957828522 + 10.0 * 8.798781394958496
Epoch 1350, val loss: 0.4578981101512909
Epoch 1360, training loss: 88.36933898925781 = 0.37772953510284424 + 10.0 * 8.799160957336426
Epoch 1360, val loss: 0.45691153407096863
Epoch 1370, training loss: 88.36503601074219 = 0.3759783208370209 + 10.0 * 8.798906326293945
Epoch 1370, val loss: 0.4559437036514282
Epoch 1380, training loss: 88.4222640991211 = 0.3742487132549286 + 10.0 * 8.804800987243652
Epoch 1380, val loss: 0.45504406094551086
Epoch 1390, training loss: 88.38151550292969 = 0.3724697232246399 + 10.0 * 8.800905227661133
Epoch 1390, val loss: 0.45421555638313293
Epoch 1400, training loss: 88.44679260253906 = 0.3708212077617645 + 10.0 * 8.807597160339355
Epoch 1400, val loss: 0.4532780945301056
Epoch 1410, training loss: 88.4626693725586 = 0.36915990710258484 + 10.0 * 8.809350967407227
Epoch 1410, val loss: 0.4522824287414551
Epoch 1420, training loss: 88.41600799560547 = 0.3674427270889282 + 10.0 * 8.80485725402832
Epoch 1420, val loss: 0.4514583647251129
Epoch 1430, training loss: 88.44389343261719 = 0.36567816138267517 + 10.0 * 8.807821273803711
Epoch 1430, val loss: 0.45052608847618103
Epoch 1440, training loss: 88.47085571289062 = 0.3638998568058014 + 10.0 * 8.81069564819336
Epoch 1440, val loss: 0.44945842027664185
Epoch 1450, training loss: 88.48857116699219 = 0.36215534806251526 + 10.0 * 8.812642097473145
Epoch 1450, val loss: 0.4486348330974579
Epoch 1460, training loss: 88.51329040527344 = 0.360431045293808 + 10.0 * 8.815286636352539
Epoch 1460, val loss: 0.4478641748428345
Epoch 1470, training loss: 88.51887512207031 = 0.3586871027946472 + 10.0 * 8.816019058227539
Epoch 1470, val loss: 0.44690191745758057
Epoch 1480, training loss: 88.48506164550781 = 0.3570207357406616 + 10.0 * 8.812804222106934
Epoch 1480, val loss: 0.4460292160511017
Epoch 1490, training loss: 88.49915313720703 = 0.3553696572780609 + 10.0 * 8.81437873840332
Epoch 1490, val loss: 0.44516947865486145
Epoch 1500, training loss: 88.54984283447266 = 0.35370108485221863 + 10.0 * 8.81961441040039
Epoch 1500, val loss: 0.4443731904029846
Epoch 1510, training loss: 88.56822204589844 = 0.3519878685474396 + 10.0 * 8.821622848510742
Epoch 1510, val loss: 0.4435226619243622
Epoch 1520, training loss: 88.51145935058594 = 0.3503279685974121 + 10.0 * 8.816113471984863
Epoch 1520, val loss: 0.44265079498291016
Epoch 1530, training loss: 88.56184387207031 = 0.34868887066841125 + 10.0 * 8.82131576538086
Epoch 1530, val loss: 0.44164401292800903
Epoch 1540, training loss: 88.5413818359375 = 0.34706854820251465 + 10.0 * 8.81943130493164
Epoch 1540, val loss: 0.4410768151283264
Epoch 1550, training loss: 88.5324935913086 = 0.34542572498321533 + 10.0 * 8.818706512451172
Epoch 1550, val loss: 0.4400113821029663
Epoch 1560, training loss: 88.57962036132812 = 0.34371986985206604 + 10.0 * 8.823590278625488
Epoch 1560, val loss: 0.4392368793487549
Epoch 1570, training loss: 88.65117645263672 = 0.3420141041278839 + 10.0 * 8.830916404724121
Epoch 1570, val loss: 0.43843144178390503
Epoch 1580, training loss: 88.65558624267578 = 0.3402741253376007 + 10.0 * 8.831531524658203
Epoch 1580, val loss: 0.43765175342559814
Epoch 1590, training loss: 88.60002899169922 = 0.3385862112045288 + 10.0 * 8.826144218444824
Epoch 1590, val loss: 0.436776340007782
Epoch 1600, training loss: 88.64935302734375 = 0.33697426319122314 + 10.0 * 8.83123779296875
Epoch 1600, val loss: 0.4358444809913635
Epoch 1610, training loss: 88.6675796508789 = 0.33528932929039 + 10.0 * 8.833229064941406
Epoch 1610, val loss: 0.4352268874645233
Epoch 1620, training loss: 88.70381164550781 = 0.33362123370170593 + 10.0 * 8.837018966674805
Epoch 1620, val loss: 0.4344387948513031
Epoch 1630, training loss: 88.70518493652344 = 0.33193907141685486 + 10.0 * 8.837324142456055
Epoch 1630, val loss: 0.43365195393562317
Epoch 1640, training loss: 88.72382354736328 = 0.3302673101425171 + 10.0 * 8.83935546875
Epoch 1640, val loss: 0.4328641891479492
Epoch 1650, training loss: 88.73216247558594 = 0.32859718799591064 + 10.0 * 8.840356826782227
Epoch 1650, val loss: 0.432235985994339
Epoch 1660, training loss: 88.71664428710938 = 0.3269364535808563 + 10.0 * 8.838971138000488
Epoch 1660, val loss: 0.43132472038269043
Epoch 1670, training loss: 88.7371826171875 = 0.3253060579299927 + 10.0 * 8.841187477111816
Epoch 1670, val loss: 0.4305889904499054
Epoch 1680, training loss: 88.68070983886719 = 0.3236362934112549 + 10.0 * 8.83570671081543
Epoch 1680, val loss: 0.42996665835380554
Epoch 1690, training loss: 88.71145629882812 = 0.3220682144165039 + 10.0 * 8.83893871307373
Epoch 1690, val loss: 0.42896148562431335
Epoch 1700, training loss: 88.70767974853516 = 0.3205040693283081 + 10.0 * 8.838717460632324
Epoch 1700, val loss: 0.4284341335296631
Epoch 1710, training loss: 88.7594985961914 = 0.3188675343990326 + 10.0 * 8.844062805175781
Epoch 1710, val loss: 0.4276736080646515
Epoch 1720, training loss: 88.7721176147461 = 0.31718510389328003 + 10.0 * 8.84549331665039
Epoch 1720, val loss: 0.4268737733364105
Epoch 1730, training loss: 88.75523376464844 = 0.31554871797561646 + 10.0 * 8.843968391418457
Epoch 1730, val loss: 0.42612695693969727
Epoch 1740, training loss: 88.80056762695312 = 0.31393396854400635 + 10.0 * 8.848663330078125
Epoch 1740, val loss: 0.42543408274650574
Epoch 1750, training loss: 88.82958984375 = 0.312282919883728 + 10.0 * 8.851730346679688
Epoch 1750, val loss: 0.4246799051761627
Epoch 1760, training loss: 88.82415771484375 = 0.31065863370895386 + 10.0 * 8.851349830627441
Epoch 1760, val loss: 0.4240075647830963
Epoch 1770, training loss: 88.80058288574219 = 0.30904775857925415 + 10.0 * 8.849153518676758
Epoch 1770, val loss: 0.4235531985759735
Epoch 1780, training loss: 88.77182006835938 = 0.3074817657470703 + 10.0 * 8.846433639526367
Epoch 1780, val loss: 0.4223131239414215
Epoch 1790, training loss: 88.78160858154297 = 0.30595633387565613 + 10.0 * 8.847565650939941
Epoch 1790, val loss: 0.4220140874385834
Epoch 1800, training loss: 88.8280029296875 = 0.30435416102409363 + 10.0 * 8.852365493774414
Epoch 1800, val loss: 0.42126157879829407
Epoch 1810, training loss: 88.86042022705078 = 0.302743524312973 + 10.0 * 8.855768203735352
Epoch 1810, val loss: 0.4205905795097351
Epoch 1820, training loss: 88.85806274414062 = 0.3011326789855957 + 10.0 * 8.855692863464355
Epoch 1820, val loss: 0.41996997594833374
Epoch 1830, training loss: 88.86577606201172 = 0.2995198369026184 + 10.0 * 8.8566255569458
Epoch 1830, val loss: 0.41913706064224243
Epoch 1840, training loss: 88.88333892822266 = 0.2979104816913605 + 10.0 * 8.858542442321777
Epoch 1840, val loss: 0.4185800552368164
Epoch 1850, training loss: 88.90951538085938 = 0.29634228348731995 + 10.0 * 8.86131763458252
Epoch 1850, val loss: 0.41797134280204773
Epoch 1860, training loss: 88.90616607666016 = 0.29476049542427063 + 10.0 * 8.861140251159668
Epoch 1860, val loss: 0.41740113496780396
Epoch 1870, training loss: 88.87042236328125 = 0.29334476590156555 + 10.0 * 8.857707977294922
Epoch 1870, val loss: 0.416546493768692
Epoch 1880, training loss: 88.86701202392578 = 0.29181358218193054 + 10.0 * 8.857519149780273
Epoch 1880, val loss: 0.4155550003051758
Epoch 1890, training loss: 88.90160369873047 = 0.29030489921569824 + 10.0 * 8.861129760742188
Epoch 1890, val loss: 0.4154016077518463
Epoch 1900, training loss: 88.94900512695312 = 0.2887095510959625 + 10.0 * 8.866029739379883
Epoch 1900, val loss: 0.414821982383728
Epoch 1910, training loss: 88.91553497314453 = 0.2871069312095642 + 10.0 * 8.862842559814453
Epoch 1910, val loss: 0.41429492831230164
Epoch 1920, training loss: 88.92948150634766 = 0.28554055094718933 + 10.0 * 8.864394187927246
Epoch 1920, val loss: 0.4137347936630249
Epoch 1930, training loss: 88.9305419921875 = 0.28404054045677185 + 10.0 * 8.864649772644043
Epoch 1930, val loss: 0.4130302667617798
Epoch 1940, training loss: 88.99386596679688 = 0.2825445234775543 + 10.0 * 8.871131896972656
Epoch 1940, val loss: 0.4125138819217682
Epoch 1950, training loss: 89.01817321777344 = 0.28099915385246277 + 10.0 * 8.873717308044434
Epoch 1950, val loss: 0.41211646795272827
Epoch 1960, training loss: 88.96080017089844 = 0.27950435876846313 + 10.0 * 8.86812973022461
Epoch 1960, val loss: 0.41153302788734436
Epoch 1970, training loss: 88.98460388183594 = 0.2780590355396271 + 10.0 * 8.870654106140137
Epoch 1970, val loss: 0.4112374782562256
Epoch 1980, training loss: 89.01406860351562 = 0.2765536606311798 + 10.0 * 8.873751640319824
Epoch 1980, val loss: 0.4107109010219574
Epoch 1990, training loss: 89.03504180908203 = 0.27504605054855347 + 10.0 * 8.875999450683594
Epoch 1990, val loss: 0.410070538520813
Epoch 2000, training loss: 89.04833984375 = 0.27351614832878113 + 10.0 * 8.877482414245605
Epoch 2000, val loss: 0.4096749722957611
Epoch 2010, training loss: 89.08634185791016 = 0.27199023962020874 + 10.0 * 8.88143539428711
Epoch 2010, val loss: 0.40913259983062744
Epoch 2020, training loss: 89.07178497314453 = 0.27048081159591675 + 10.0 * 8.880130767822266
Epoch 2020, val loss: 0.4088144600391388
Epoch 2030, training loss: 89.05210876464844 = 0.2689695358276367 + 10.0 * 8.878314018249512
Epoch 2030, val loss: 0.408354789018631
Epoch 2040, training loss: 89.0939712524414 = 0.26747727394104004 + 10.0 * 8.882649421691895
Epoch 2040, val loss: 0.4079098105430603
Epoch 2050, training loss: 89.12940216064453 = 0.26596924662590027 + 10.0 * 8.886343002319336
Epoch 2050, val loss: 0.40738606452941895
Epoch 2060, training loss: 89.10154724121094 = 0.2644614279270172 + 10.0 * 8.883708953857422
Epoch 2060, val loss: 0.407232403755188
Epoch 2070, training loss: 89.13916015625 = 0.26301926374435425 + 10.0 * 8.887614250183105
Epoch 2070, val loss: 0.4066561460494995
Epoch 2080, training loss: 89.15291595458984 = 0.2615439295768738 + 10.0 * 8.889137268066406
Epoch 2080, val loss: 0.40639832615852356
Epoch 2090, training loss: 89.09883117675781 = 0.26010462641716003 + 10.0 * 8.883872985839844
Epoch 2090, val loss: 0.4062325656414032
Epoch 2100, training loss: 89.1270523071289 = 0.25865641236305237 + 10.0 * 8.886838912963867
Epoch 2100, val loss: 0.4056371748447418
Epoch 2110, training loss: 89.15544891357422 = 0.25721266865730286 + 10.0 * 8.889823913574219
Epoch 2110, val loss: 0.40542951226234436
Epoch 2120, training loss: 89.18428802490234 = 0.2557447850704193 + 10.0 * 8.892854690551758
Epoch 2120, val loss: 0.4051968455314636
Epoch 2130, training loss: 89.2083740234375 = 0.25428056716918945 + 10.0 * 8.89540958404541
Epoch 2130, val loss: 0.40496116876602173
Epoch 2140, training loss: 88.98342895507812 = 0.2528679370880127 + 10.0 * 8.873056411743164
Epoch 2140, val loss: 0.40465304255485535
Epoch 2150, training loss: 88.8021469116211 = 0.2517777681350708 + 10.0 * 8.855036735534668
Epoch 2150, val loss: 0.4049079716205597
Epoch 2160, training loss: 88.84095001220703 = 0.2505459487438202 + 10.0 * 8.859040260314941
Epoch 2160, val loss: 0.404721200466156
Epoch 2170, training loss: 88.86936950683594 = 0.24934186041355133 + 10.0 * 8.8620023727417
Epoch 2170, val loss: 0.4029305577278137
Epoch 2180, training loss: 88.87586975097656 = 0.24789148569107056 + 10.0 * 8.862797737121582
Epoch 2180, val loss: 0.40284085273742676
Epoch 2190, training loss: 89.03611755371094 = 0.24655839800834656 + 10.0 * 8.878955841064453
Epoch 2190, val loss: 0.40388891100883484
Epoch 2200, training loss: 89.06563568115234 = 0.2451501339673996 + 10.0 * 8.882048606872559
Epoch 2200, val loss: 0.40506601333618164
Epoch 2210, training loss: 89.10076141357422 = 0.24368686974048615 + 10.0 * 8.885706901550293
Epoch 2210, val loss: 0.4033477306365967
Epoch 2220, training loss: 89.15773010253906 = 0.24220937490463257 + 10.0 * 8.891551971435547
Epoch 2220, val loss: 0.40350496768951416
Epoch 2230, training loss: 89.18467712402344 = 0.2407781034708023 + 10.0 * 8.894390106201172
Epoch 2230, val loss: 0.4036211669445038
Epoch 2240, training loss: 89.20387268066406 = 0.23936259746551514 + 10.0 * 8.896450996398926
Epoch 2240, val loss: 0.4034159183502197
Epoch 2250, training loss: 89.19844818115234 = 0.23795434832572937 + 10.0 * 8.896049499511719
Epoch 2250, val loss: 0.40356069803237915
Epoch 2260, training loss: 89.12606811523438 = 0.23663797974586487 + 10.0 * 8.88894271850586
Epoch 2260, val loss: 0.4032573997974396
Epoch 2270, training loss: 89.0687484741211 = 0.23542658984661102 + 10.0 * 8.883332252502441
Epoch 2270, val loss: 0.402212530374527
Epoch 2280, training loss: 89.1042709350586 = 0.2341017872095108 + 10.0 * 8.887017250061035
Epoch 2280, val loss: 0.40414807200431824
Epoch 2290, training loss: 89.16265869140625 = 0.2326744794845581 + 10.0 * 8.892998695373535
Epoch 2290, val loss: 0.4035990238189697
Epoch 2300, training loss: 89.21328735351562 = 0.23123379051685333 + 10.0 * 8.898205757141113
Epoch 2300, val loss: 0.4034651815891266
Epoch 2310, training loss: 89.24214935302734 = 0.22983233630657196 + 10.0 * 8.90123176574707
Epoch 2310, val loss: 0.4035080075263977
Epoch 2320, training loss: 89.25688934326172 = 0.228432759642601 + 10.0 * 8.90284538269043
Epoch 2320, val loss: 0.40343278646469116
Epoch 2330, training loss: 89.26949310302734 = 0.2270471602678299 + 10.0 * 8.904244422912598
Epoch 2330, val loss: 0.40343615412712097
Epoch 2340, training loss: 89.2801742553711 = 0.22568121552467346 + 10.0 * 8.905449867248535
Epoch 2340, val loss: 0.40362605452537537
Epoch 2350, training loss: 89.30419158935547 = 0.22431086003780365 + 10.0 * 8.907987594604492
Epoch 2350, val loss: 0.4036203920841217
Epoch 2360, training loss: 89.2959976196289 = 0.2229669839143753 + 10.0 * 8.907302856445312
Epoch 2360, val loss: 0.40370380878448486
Epoch 2370, training loss: 89.31739807128906 = 0.2216373085975647 + 10.0 * 8.909576416015625
Epoch 2370, val loss: 0.4036259651184082
Epoch 2380, training loss: 89.29204559326172 = 0.22032631933689117 + 10.0 * 8.907171249389648
Epoch 2380, val loss: 0.40416187047958374
Epoch 2390, training loss: 89.3132553100586 = 0.21900346875190735 + 10.0 * 8.909425735473633
Epoch 2390, val loss: 0.404398113489151
Epoch 2400, training loss: 89.2114028930664 = 0.21773184835910797 + 10.0 * 8.89936637878418
Epoch 2400, val loss: 0.40424466133117676
Epoch 2410, training loss: 89.3076171875 = 0.21648834645748138 + 10.0 * 8.909112930297852
Epoch 2410, val loss: 0.4053402841091156
Epoch 2420, training loss: 89.32624053955078 = 0.21515528857707977 + 10.0 * 8.911108016967773
Epoch 2420, val loss: 0.40496426820755005
Epoch 2430, training loss: 89.34912872314453 = 0.21383965015411377 + 10.0 * 8.913529396057129
Epoch 2430, val loss: 0.4048493802547455
Epoch 2440, training loss: 89.38356018066406 = 0.21249760687351227 + 10.0 * 8.917106628417969
Epoch 2440, val loss: 0.40518081188201904
Epoch 2450, training loss: 89.37727355957031 = 0.21115291118621826 + 10.0 * 8.91661262512207
Epoch 2450, val loss: 0.4059187173843384
Epoch 2460, training loss: 89.36263275146484 = 0.20983213186264038 + 10.0 * 8.91528034210205
Epoch 2460, val loss: 0.40599870681762695
Epoch 2470, training loss: 89.38135528564453 = 0.20851732790470123 + 10.0 * 8.91728401184082
Epoch 2470, val loss: 0.40613722801208496
Epoch 2480, training loss: 89.38988494873047 = 0.20720340311527252 + 10.0 * 8.918268203735352
Epoch 2480, val loss: 0.4063449501991272
Epoch 2490, training loss: 89.3943862915039 = 0.2058945745229721 + 10.0 * 8.918848991394043
Epoch 2490, val loss: 0.40673309564590454
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8568115942028985
0.863507933058031
The final CL Acc:0.85290, 0.00277, The final GNN Acc:0.86385, 0.00092
Begin epxeriment: noisy_level: 0.2 cont_weight: 10 epoch: der1:0.3 der2:0.3 dfr1:0.4 dfr2:0.5
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=2500, cl_num_layers=2, cl_num_proj_hidden=64, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=10.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.3, drop_feat_rate_1=0.4, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=32, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, noisy_level=0.2, num_hidden=64, num_proj_hidden=64, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=40, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 88648])
add edge: torch.Size([2, 106292])
remove edge: torch.Size([2, 71174])
updated graph: torch.Size([2, 88818])
=== Noisy graph ===
/home/project-robust-contrastive/Robust-Contrastive-Learning/model.py:227: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 104.1703109741211 = 1.08842134475708 + 10.0 * 10.308188438415527
Epoch 0, val loss: 1.089163899421692
Epoch 10, training loss: 100.1923599243164 = 1.088035225868225 + 10.0 * 9.910432815551758
Epoch 10, val loss: 1.0887597799301147
Epoch 20, training loss: 98.18832397460938 = 1.0877243280410767 + 10.0 * 9.710060119628906
Epoch 20, val loss: 1.0884538888931274
Epoch 30, training loss: 96.76921081542969 = 1.087417721748352 + 10.0 * 9.5681791305542
Epoch 30, val loss: 1.0881462097167969
Epoch 40, training loss: 95.63817596435547 = 1.0871284008026123 + 10.0 * 9.45510482788086
Epoch 40, val loss: 1.0878591537475586
Epoch 50, training loss: 94.70423126220703 = 1.086835503578186 + 10.0 * 9.361739158630371
Epoch 50, val loss: 1.087565302848816
Epoch 60, training loss: 93.91960144042969 = 1.0865364074707031 + 10.0 * 9.283306121826172
Epoch 60, val loss: 1.0872669219970703
Epoch 70, training loss: 93.25751495361328 = 1.0862332582473755 + 10.0 * 9.217127799987793
Epoch 70, val loss: 1.0869619846343994
Epoch 80, training loss: 92.69718170166016 = 1.0859261751174927 + 10.0 * 9.161125183105469
Epoch 80, val loss: 1.0866533517837524
Epoch 90, training loss: 92.21528625488281 = 1.0856107473373413 + 10.0 * 9.112967491149902
Epoch 90, val loss: 1.086336612701416
Epoch 100, training loss: 91.79126739501953 = 1.0852769613265991 + 10.0 * 9.070599555969238
Epoch 100, val loss: 1.0860008001327515
Epoch 110, training loss: 91.43213653564453 = 1.0849370956420898 + 10.0 * 9.034719467163086
Epoch 110, val loss: 1.0856603384017944
Epoch 120, training loss: 91.10918426513672 = 1.0845798254013062 + 10.0 * 9.002460479736328
Epoch 120, val loss: 1.0853028297424316
Epoch 130, training loss: 90.83771514892578 = 1.0842148065567017 + 10.0 * 8.975350379943848
Epoch 130, val loss: 1.0849411487579346
Epoch 140, training loss: 90.5968246459961 = 1.083825945854187 + 10.0 * 8.951299667358398
Epoch 140, val loss: 1.0845561027526855
Epoch 150, training loss: 90.38599395751953 = 1.0834414958953857 + 10.0 * 8.930254936218262
Epoch 150, val loss: 1.0841712951660156
Epoch 160, training loss: 90.18614196777344 = 1.0830180644989014 + 10.0 * 8.91031265258789
Epoch 160, val loss: 1.0837534666061401
Epoch 170, training loss: 90.01251220703125 = 1.082605242729187 + 10.0 * 8.892991065979004
Epoch 170, val loss: 1.0833410024642944
Epoch 180, training loss: 89.85987091064453 = 1.0821739435195923 + 10.0 * 8.877769470214844
Epoch 180, val loss: 1.0829070806503296
Epoch 190, training loss: 89.76144409179688 = 1.0817521810531616 + 10.0 * 8.867969512939453
Epoch 190, val loss: 1.082484483718872
Epoch 200, training loss: 89.666748046875 = 1.0812995433807373 + 10.0 * 8.858545303344727
Epoch 200, val loss: 1.0820387601852417
Epoch 210, training loss: 89.54998016357422 = 1.0808309316635132 + 10.0 * 8.846914291381836
Epoch 210, val loss: 1.0815714597702026
Epoch 220, training loss: 89.45864868164062 = 1.0803543329238892 + 10.0 * 8.83782958984375
Epoch 220, val loss: 1.0810989141464233
Epoch 230, training loss: 89.36448669433594 = 1.0798767805099487 + 10.0 * 8.828460693359375
Epoch 230, val loss: 1.0806204080581665
Epoch 240, training loss: 89.36097717285156 = 1.0793869495391846 + 10.0 * 8.82815933227539
Epoch 240, val loss: 1.0801386833190918
Epoch 250, training loss: 89.2689208984375 = 1.0788754224777222 + 10.0 * 8.819005012512207
Epoch 250, val loss: 1.079629898071289
Epoch 260, training loss: 89.22711181640625 = 1.0783541202545166 + 10.0 * 8.814875602722168
Epoch 260, val loss: 1.0791078805923462
Epoch 270, training loss: 89.18177795410156 = 1.0778402090072632 + 10.0 * 8.810393333435059
Epoch 270, val loss: 1.0786064863204956
Epoch 280, training loss: 89.16451263427734 = 1.077296257019043 + 10.0 * 8.808721542358398
Epoch 280, val loss: 1.078065276145935
Epoch 290, training loss: 89.15322875976562 = 1.0767549276351929 + 10.0 * 8.807647705078125
Epoch 290, val loss: 1.0775200128555298
Epoch 300, training loss: 89.13186645507812 = 1.0762033462524414 + 10.0 * 8.805566787719727
Epoch 300, val loss: 1.0769798755645752
Epoch 310, training loss: 89.13445281982422 = 1.0756419897079468 + 10.0 * 8.805880546569824
Epoch 310, val loss: 1.0764228105545044
Epoch 320, training loss: 89.11457061767578 = 1.075055480003357 + 10.0 * 8.803951263427734
Epoch 320, val loss: 1.075826644897461
Epoch 330, training loss: 89.0776138305664 = 1.0744543075561523 + 10.0 * 8.800315856933594
Epoch 330, val loss: 1.0752484798431396
Epoch 340, training loss: 89.06195831298828 = 1.0738617181777954 + 10.0 * 8.798810005187988
Epoch 340, val loss: 1.0746536254882812
Epoch 350, training loss: 89.07275390625 = 1.0732462406158447 + 10.0 * 8.79995059967041
Epoch 350, val loss: 1.0740476846694946
Epoch 360, training loss: 89.09024810791016 = 1.0726217031478882 + 10.0 * 8.801762580871582
Epoch 360, val loss: 1.073423147201538
Epoch 370, training loss: 89.06885528564453 = 1.0719703435897827 + 10.0 * 8.799688339233398
Epoch 370, val loss: 1.0727760791778564
Epoch 380, training loss: 89.05156707763672 = 1.0712844133377075 + 10.0 * 8.798028945922852
Epoch 380, val loss: 1.0721067190170288
Epoch 390, training loss: 89.07500457763672 = 1.0706157684326172 + 10.0 * 8.80043888092041
Epoch 390, val loss: 1.071441411972046
Epoch 400, training loss: 89.05799102783203 = 1.069939374923706 + 10.0 * 8.798805236816406
Epoch 400, val loss: 1.0707818269729614
Epoch 410, training loss: 89.07347869873047 = 1.0692353248596191 + 10.0 * 8.800424575805664
Epoch 410, val loss: 1.0700856447219849
Epoch 420, training loss: 89.07780456542969 = 1.0685282945632935 + 10.0 * 8.800928115844727
Epoch 420, val loss: 1.0693796873092651
Epoch 430, training loss: 89.02996063232422 = 1.0677436590194702 + 10.0 * 8.796221733093262
Epoch 430, val loss: 1.0686185359954834
Epoch 440, training loss: 89.12188720703125 = 1.0670422315597534 + 10.0 * 8.805484771728516
Epoch 440, val loss: 1.0679175853729248
Epoch 450, training loss: 89.11672973632812 = 1.0663011074066162 + 10.0 * 8.80504322052002
Epoch 450, val loss: 1.0671991109848022
Epoch 460, training loss: 89.04271697998047 = 1.0655030012130737 + 10.0 * 8.797720909118652
Epoch 460, val loss: 1.0664355754852295
Epoch 470, training loss: 89.07918548583984 = 1.064763069152832 + 10.0 * 8.80144214630127
Epoch 470, val loss: 1.0656875371932983
Epoch 480, training loss: 89.13823699951172 = 1.0640009641647339 + 10.0 * 8.80742359161377
Epoch 480, val loss: 1.0649362802505493
Epoch 490, training loss: 89.18424224853516 = 1.0632015466690063 + 10.0 * 8.812104225158691
Epoch 490, val loss: 1.0641448497772217
Epoch 500, training loss: 89.18670654296875 = 1.0623928308486938 + 10.0 * 8.812431335449219
Epoch 500, val loss: 1.0633528232574463
Epoch 510, training loss: 89.21190643310547 = 1.0615605115890503 + 10.0 * 8.815034866333008
Epoch 510, val loss: 1.0625357627868652
Epoch 520, training loss: 89.25257873535156 = 1.060756802558899 + 10.0 * 8.819181442260742
Epoch 520, val loss: 1.0617518424987793
Epoch 530, training loss: 89.27323150634766 = 1.0599312782287598 + 10.0 * 8.821330070495605
Epoch 530, val loss: 1.060934066772461
Epoch 540, training loss: 89.2776870727539 = 1.0590879917144775 + 10.0 * 8.821859359741211
Epoch 540, val loss: 1.0601133108139038
Epoch 550, training loss: 89.2700424194336 = 1.0582233667373657 + 10.0 * 8.821182250976562
Epoch 550, val loss: 1.059273600578308
Epoch 560, training loss: 89.2538833618164 = 1.057332992553711 + 10.0 * 8.81965446472168
Epoch 560, val loss: 1.0583943128585815
Epoch 570, training loss: 89.22239685058594 = 1.0564554929733276 + 10.0 * 8.816594123840332
Epoch 570, val loss: 1.0575344562530518
Epoch 580, training loss: 89.31929779052734 = 1.0555952787399292 + 10.0 * 8.826370239257812
Epoch 580, val loss: 1.0566946268081665
Epoch 590, training loss: 89.31194305419922 = 1.0547219514846802 + 10.0 * 8.825721740722656
Epoch 590, val loss: 1.0558323860168457
Epoch 600, training loss: 89.34993743896484 = 1.05381441116333 + 10.0 * 8.829611778259277
Epoch 600, val loss: 1.054941177368164
Epoch 610, training loss: 89.33888244628906 = 1.0528675317764282 + 10.0 * 8.828601837158203
Epoch 610, val loss: 1.0540162324905396
Epoch 620, training loss: 89.34957122802734 = 1.0519217252731323 + 10.0 * 8.829765319824219
Epoch 620, val loss: 1.053079605102539
Epoch 630, training loss: 89.36447143554688 = 1.0509577989578247 + 10.0 * 8.831351280212402
Epoch 630, val loss: 1.052150845527649
Epoch 640, training loss: 89.38420867919922 = 1.0499976873397827 + 10.0 * 8.83342170715332
Epoch 640, val loss: 1.0511966943740845
Epoch 650, training loss: 89.45381164550781 = 1.0490074157714844 + 10.0 * 8.840479850769043
Epoch 650, val loss: 1.050221562385559
Epoch 660, training loss: 89.48270416259766 = 1.047987699508667 + 10.0 * 8.84347152709961
Epoch 660, val loss: 1.0492335557937622
Epoch 670, training loss: 89.53316497802734 = 1.04694402217865 + 10.0 * 8.84862232208252
Epoch 670, val loss: 1.04820716381073
Epoch 680, training loss: 89.54890441894531 = 1.0458682775497437 + 10.0 * 8.850303649902344
Epoch 680, val loss: 1.047145962715149
Epoch 690, training loss: 89.5179672241211 = 1.044758677482605 + 10.0 * 8.847320556640625
Epoch 690, val loss: 1.04608952999115
Epoch 700, training loss: 89.54911041259766 = 1.0436915159225464 + 10.0 * 8.850542068481445
Epoch 700, val loss: 1.0450282096862793
Epoch 710, training loss: 89.55902099609375 = 1.0425710678100586 + 10.0 * 8.851644515991211
Epoch 710, val loss: 1.043944239616394
Epoch 720, training loss: 89.62615203857422 = 1.0414814949035645 + 10.0 * 8.858467102050781
Epoch 720, val loss: 1.0428578853607178
Epoch 730, training loss: 89.61272430419922 = 1.040345549583435 + 10.0 * 8.857237815856934
Epoch 730, val loss: 1.0417520999908447
Epoch 740, training loss: 89.64141845703125 = 1.0391989946365356 + 10.0 * 8.860221862792969
Epoch 740, val loss: 1.040629506111145
Epoch 750, training loss: 89.6222915649414 = 1.0380127429962158 + 10.0 * 8.858428001403809
Epoch 750, val loss: 1.0394641160964966
Epoch 760, training loss: 89.6646499633789 = 1.036855936050415 + 10.0 * 8.86277961730957
Epoch 760, val loss: 1.0383412837982178
Epoch 770, training loss: 89.69735717773438 = 1.0356770753860474 + 10.0 * 8.866168022155762
Epoch 770, val loss: 1.0372005701065063
Epoch 780, training loss: 89.75312042236328 = 1.03446626663208 + 10.0 * 8.871865272521973
Epoch 780, val loss: 1.036002516746521
Epoch 790, training loss: 89.78772735595703 = 1.0332348346710205 + 10.0 * 8.875449180603027
Epoch 790, val loss: 1.0347965955734253
Epoch 800, training loss: 89.83830261230469 = 1.0320161581039429 + 10.0 * 8.88062858581543
Epoch 800, val loss: 1.0335923433303833
Epoch 810, training loss: 89.82637786865234 = 1.0307352542877197 + 10.0 * 8.87956428527832
Epoch 810, val loss: 1.032342553138733
Epoch 820, training loss: 89.7881088256836 = 1.0293704271316528 + 10.0 * 8.875873565673828
Epoch 820, val loss: 1.031040906906128
Epoch 830, training loss: 89.70137786865234 = 1.0280972719192505 + 10.0 * 8.867327690124512
Epoch 830, val loss: 1.0297796726226807
Epoch 840, training loss: 89.76246643066406 = 1.0267977714538574 + 10.0 * 8.873567581176758
Epoch 840, val loss: 1.0285087823867798
Epoch 850, training loss: 89.84523010253906 = 1.025481939315796 + 10.0 * 8.881975173950195
Epoch 850, val loss: 1.0272148847579956
Epoch 860, training loss: 89.88837432861328 = 1.0241179466247559 + 10.0 * 8.886425971984863
Epoch 860, val loss: 1.0258902311325073
Epoch 870, training loss: 89.89926147460938 = 1.0227001905441284 + 10.0 * 8.887656211853027
Epoch 870, val loss: 1.0244735479354858
Epoch 880, training loss: 89.94561767578125 = 1.0212006568908691 + 10.0 * 8.892441749572754
Epoch 880, val loss: 1.0230000019073486
Epoch 890, training loss: 89.96359252929688 = 1.0196056365966797 + 10.0 * 8.89439868927002
Epoch 890, val loss: 1.02144193649292
Epoch 900, training loss: 89.98189544677734 = 1.0179575681686401 + 10.0 * 8.896393775939941
Epoch 900, val loss: 1.0198028087615967
Epoch 910, training loss: 89.98807525634766 = 1.0162568092346191 + 10.0 * 8.897181510925293
Epoch 910, val loss: 1.0181413888931274
Epoch 920, training loss: 90.00869750976562 = 1.0145152807235718 + 10.0 * 8.899417877197266
Epoch 920, val loss: 1.016426682472229
Epoch 930, training loss: 90.01206970214844 = 1.0127357244491577 + 10.0 * 8.899933815002441
Epoch 930, val loss: 1.0146732330322266
Epoch 940, training loss: 90.03144073486328 = 1.0109118223190308 + 10.0 * 8.902052879333496
Epoch 940, val loss: 1.0129120349884033
Epoch 950, training loss: 90.07493591308594 = 1.009078025817871 + 10.0 * 8.906585693359375
Epoch 950, val loss: 1.011110544204712
Epoch 960, training loss: 90.05197143554688 = 1.0072129964828491 + 10.0 * 8.904476165771484
Epoch 960, val loss: 1.0092960596084595
Epoch 970, training loss: 90.02387237548828 = 1.005258321762085 + 10.0 * 8.901861190795898
Epoch 970, val loss: 1.0074093341827393
Epoch 980, training loss: 90.08907318115234 = 1.0033777952194214 + 10.0 * 8.9085693359375
Epoch 980, val loss: 1.0055464506149292
Epoch 990, training loss: 90.14905548095703 = 1.0014384984970093 + 10.0 * 8.914761543273926
Epoch 990, val loss: 1.003669261932373
Epoch 1000, training loss: 90.15214538574219 = 0.9994663000106812 + 10.0 * 8.915267944335938
Epoch 1000, val loss: 1.0017356872558594
Epoch 1010, training loss: 90.19022369384766 = 0.9974934458732605 + 10.0 * 8.919273376464844
Epoch 1010, val loss: 0.9998094439506531
Epoch 1020, training loss: 90.1985855102539 = 0.9954603314399719 + 10.0 * 8.920312881469727
Epoch 1020, val loss: 0.9978160858154297
Epoch 1030, training loss: 90.19175720214844 = 0.9934157133102417 + 10.0 * 8.91983413696289
Epoch 1030, val loss: 0.9958567023277283
Epoch 1040, training loss: 90.22701263427734 = 0.9913576245307922 + 10.0 * 8.923565864562988
Epoch 1040, val loss: 0.9938377737998962
Epoch 1050, training loss: 90.26585388183594 = 0.9892884492874146 + 10.0 * 8.927656173706055
Epoch 1050, val loss: 0.9918211698532104
Epoch 1060, training loss: 90.2691650390625 = 0.9871559143066406 + 10.0 * 8.928201675415039
Epoch 1060, val loss: 0.9897634983062744
Epoch 1070, training loss: 90.28498077392578 = 0.9850525259971619 + 10.0 * 8.92999267578125
Epoch 1070, val loss: 0.9876958131790161
Epoch 1080, training loss: 90.30960845947266 = 0.9829339385032654 + 10.0 * 8.93266773223877
Epoch 1080, val loss: 0.9856283664703369
Epoch 1090, training loss: 90.1891860961914 = 0.9807978868484497 + 10.0 * 8.920839309692383
Epoch 1090, val loss: 0.983556866645813
Epoch 1100, training loss: 90.20944213867188 = 0.9786093831062317 + 10.0 * 8.923083305358887
Epoch 1100, val loss: 0.9813956618309021
Epoch 1110, training loss: 90.32002258300781 = 0.9764569997787476 + 10.0 * 8.934356689453125
Epoch 1110, val loss: 0.979337215423584
Epoch 1120, training loss: 90.3042984008789 = 0.9742732048034668 + 10.0 * 8.933002471923828
Epoch 1120, val loss: 0.9772382974624634
Epoch 1130, training loss: 90.3436050415039 = 0.9720339775085449 + 10.0 * 8.93715763092041
Epoch 1130, val loss: 0.9750536680221558
Epoch 1140, training loss: 90.31629180908203 = 0.9698116183280945 + 10.0 * 8.934648513793945
Epoch 1140, val loss: 0.9728790521621704
Epoch 1150, training loss: 90.34758758544922 = 0.9675633907318115 + 10.0 * 8.938002586364746
Epoch 1150, val loss: 0.9707114696502686
Epoch 1160, training loss: 90.38453674316406 = 0.9653457999229431 + 10.0 * 8.941919326782227
Epoch 1160, val loss: 0.9685655236244202
Epoch 1170, training loss: 90.43095397949219 = 0.9631025195121765 + 10.0 * 8.946784973144531
Epoch 1170, val loss: 0.9663774967193604
Epoch 1180, training loss: 90.4415283203125 = 0.9608474373817444 + 10.0 * 8.948068618774414
Epoch 1180, val loss: 0.9641774296760559
Epoch 1190, training loss: 90.5125961303711 = 0.9586201906204224 + 10.0 * 8.955397605895996
Epoch 1190, val loss: 0.9620415568351746
Epoch 1200, training loss: 90.45042419433594 = 0.9563657641410828 + 10.0 * 8.949405670166016
Epoch 1200, val loss: 0.9598166942596436
Epoch 1210, training loss: 90.38318634033203 = 0.9540570378303528 + 10.0 * 8.942913055419922
Epoch 1210, val loss: 0.9576260447502136
Epoch 1220, training loss: 90.34614562988281 = 0.9518474340438843 + 10.0 * 8.93942928314209
Epoch 1220, val loss: 0.9554129838943481
Epoch 1230, training loss: 90.31753540039062 = 0.9497202634811401 + 10.0 * 8.93678092956543
Epoch 1230, val loss: 0.9533228874206543
Epoch 1240, training loss: 90.387939453125 = 0.947462797164917 + 10.0 * 8.944047927856445
Epoch 1240, val loss: 0.9511034488677979
Epoch 1250, training loss: 90.43708801269531 = 0.945203959941864 + 10.0 * 8.949188232421875
Epoch 1250, val loss: 0.9489191174507141
Epoch 1260, training loss: 90.493896484375 = 0.9429752826690674 + 10.0 * 8.95509147644043
Epoch 1260, val loss: 0.9467310309410095
Epoch 1270, training loss: 90.55116271972656 = 0.9407283067703247 + 10.0 * 8.961043357849121
Epoch 1270, val loss: 0.9445359706878662
Epoch 1280, training loss: 90.53358459472656 = 0.9384586215019226 + 10.0 * 8.959512710571289
Epoch 1280, val loss: 0.9423257112503052
Epoch 1290, training loss: 90.57707214355469 = 0.936214804649353 + 10.0 * 8.964085578918457
Epoch 1290, val loss: 0.9401345252990723
Epoch 1300, training loss: 90.60689544677734 = 0.933982789516449 + 10.0 * 8.967290878295898
Epoch 1300, val loss: 0.9379604458808899
Epoch 1310, training loss: 90.61112213134766 = 0.9317503571510315 + 10.0 * 8.967937469482422
Epoch 1310, val loss: 0.9357974529266357
Epoch 1320, training loss: 90.60643768310547 = 0.9295051097869873 + 10.0 * 8.967693328857422
Epoch 1320, val loss: 0.9336212277412415
Epoch 1330, training loss: 90.61780548095703 = 0.9272902011871338 + 10.0 * 8.969051361083984
Epoch 1330, val loss: 0.9314850568771362
Epoch 1340, training loss: 90.69831848144531 = 0.9250949621200562 + 10.0 * 8.977322578430176
Epoch 1340, val loss: 0.9293708801269531
Epoch 1350, training loss: 90.62443542480469 = 0.9228612184524536 + 10.0 * 8.970157623291016
Epoch 1350, val loss: 0.9272357225418091
Epoch 1360, training loss: 90.69152069091797 = 0.9206893444061279 + 10.0 * 8.977083206176758
Epoch 1360, val loss: 0.9251265525817871
Epoch 1370, training loss: 90.73756408691406 = 0.9184930324554443 + 10.0 * 8.98190689086914
Epoch 1370, val loss: 0.9230412840843201
Epoch 1380, training loss: 90.7437744140625 = 0.9163209795951843 + 10.0 * 8.982745170593262
Epoch 1380, val loss: 0.9209509491920471
Epoch 1390, training loss: 90.7291488647461 = 0.9141536355018616 + 10.0 * 8.981499671936035
Epoch 1390, val loss: 0.9188450574874878
Epoch 1400, training loss: 90.74449920654297 = 0.9120083451271057 + 10.0 * 8.983248710632324
Epoch 1400, val loss: 0.9168111085891724
Epoch 1410, training loss: 90.82594299316406 = 0.9098536968231201 + 10.0 * 8.991609573364258
Epoch 1410, val loss: 0.9147545695304871
Epoch 1420, training loss: 90.72230529785156 = 0.9077423810958862 + 10.0 * 8.98145580291748
Epoch 1420, val loss: 0.9127134084701538
Epoch 1430, training loss: 90.80715942382812 = 0.9056850075721741 + 10.0 * 8.990147590637207
Epoch 1430, val loss: 0.9107397198677063
Epoch 1440, training loss: 90.88971710205078 = 0.9035962820053101 + 10.0 * 8.998612403869629
Epoch 1440, val loss: 0.9087327122688293
Epoch 1450, training loss: 90.86144256591797 = 0.901515007019043 + 10.0 * 8.995992660522461
Epoch 1450, val loss: 0.9067296385765076
Epoch 1460, training loss: 90.58560180664062 = 0.899284303188324 + 10.0 * 8.968631744384766
Epoch 1460, val loss: 0.9045602679252625
Epoch 1470, training loss: 90.85957336425781 = 0.8974549174308777 + 10.0 * 8.996212005615234
Epoch 1470, val loss: 0.9028278589248657
Epoch 1480, training loss: 90.92036437988281 = 0.895484209060669 + 10.0 * 9.002488136291504
Epoch 1480, val loss: 0.900958240032196
Epoch 1490, training loss: 90.9498519897461 = 0.8934895992279053 + 10.0 * 9.005636215209961
Epoch 1490, val loss: 0.8990720510482788
Epoch 1500, training loss: 91.00928497314453 = 0.8914939165115356 + 10.0 * 9.011778831481934
Epoch 1500, val loss: 0.8971671462059021
Epoch 1510, training loss: 90.94719696044922 = 0.889488935470581 + 10.0 * 9.005770683288574
Epoch 1510, val loss: 0.895236074924469
Epoch 1520, training loss: 91.00032806396484 = 0.887518584728241 + 10.0 * 9.01128101348877
Epoch 1520, val loss: 0.8933659791946411
Epoch 1530, training loss: 91.02098083496094 = 0.8855484127998352 + 10.0 * 9.013543128967285
Epoch 1530, val loss: 0.8914794921875
Epoch 1540, training loss: 90.99687957763672 = 0.8836172819137573 + 10.0 * 9.011326789855957
Epoch 1540, val loss: 0.8896347284317017
Epoch 1550, training loss: 91.05049133300781 = 0.8816876411437988 + 10.0 * 9.01688003540039
Epoch 1550, val loss: 0.8877919316291809
Epoch 1560, training loss: 91.10173034667969 = 0.8797876238822937 + 10.0 * 9.022193908691406
Epoch 1560, val loss: 0.8859841227531433
Epoch 1570, training loss: 91.07673645019531 = 0.8778854012489319 + 10.0 * 9.019885063171387
Epoch 1570, val loss: 0.8841719627380371
Epoch 1580, training loss: 91.05422973632812 = 0.8760373592376709 + 10.0 * 9.01781940460205
Epoch 1580, val loss: 0.8823991417884827
Epoch 1590, training loss: 90.98316955566406 = 0.8742039799690247 + 10.0 * 9.010896682739258
Epoch 1590, val loss: 0.8806791305541992
Epoch 1600, training loss: 91.11283111572266 = 0.8724145293235779 + 10.0 * 9.024042129516602
Epoch 1600, val loss: 0.8789728879928589
Epoch 1610, training loss: 91.16746520996094 = 0.8706079721450806 + 10.0 * 9.029685974121094
Epoch 1610, val loss: 0.8772550225257874
Epoch 1620, training loss: 91.17218780517578 = 0.8687953352928162 + 10.0 * 9.030339241027832
Epoch 1620, val loss: 0.8755481243133545
Epoch 1630, training loss: 91.22891235351562 = 0.8670118451118469 + 10.0 * 9.036190032958984
Epoch 1630, val loss: 0.873867928981781
Epoch 1640, training loss: 91.15795135498047 = 0.8652466535568237 + 10.0 * 9.02927017211914
Epoch 1640, val loss: 0.8722070455551147
Epoch 1650, training loss: 91.22833251953125 = 0.8635395765304565 + 10.0 * 9.036478996276855
Epoch 1650, val loss: 0.8705795407295227
Epoch 1660, training loss: 91.20350646972656 = 0.861799418926239 + 10.0 * 9.034170150756836
Epoch 1660, val loss: 0.8689396977424622
Epoch 1670, training loss: 91.2649154663086 = 0.8601104617118835 + 10.0 * 9.040480613708496
Epoch 1670, val loss: 0.8673478364944458
Epoch 1680, training loss: 91.25064849853516 = 0.8584113717079163 + 10.0 * 9.039223670959473
Epoch 1680, val loss: 0.8657392263412476
Epoch 1690, training loss: 91.27511596679688 = 0.8567308187484741 + 10.0 * 9.041838645935059
Epoch 1690, val loss: 0.8641693592071533
Epoch 1700, training loss: 91.30467987060547 = 0.855080783367157 + 10.0 * 9.044960021972656
Epoch 1700, val loss: 0.8626182079315186
Epoch 1710, training loss: 91.3270492553711 = 0.8534303307533264 + 10.0 * 9.047361373901367
Epoch 1710, val loss: 0.8610669374465942
Epoch 1720, training loss: 91.365478515625 = 0.8518219590187073 + 10.0 * 9.051365852355957
Epoch 1720, val loss: 0.8595364093780518
Epoch 1730, training loss: 91.29729461669922 = 0.8501791954040527 + 10.0 * 9.044711112976074
Epoch 1730, val loss: 0.8579376339912415
Epoch 1740, training loss: 91.40145874023438 = 0.8487743735313416 + 10.0 * 9.055268287658691
Epoch 1740, val loss: 0.8566184043884277
Epoch 1750, training loss: 91.35848999023438 = 0.8471752405166626 + 10.0 * 9.051131248474121
Epoch 1750, val loss: 0.8551812171936035
Epoch 1760, training loss: 91.39366912841797 = 0.8456124663352966 + 10.0 * 9.054805755615234
Epoch 1760, val loss: 0.853693425655365
Epoch 1770, training loss: 91.38703155517578 = 0.8440653681755066 + 10.0 * 9.054296493530273
Epoch 1770, val loss: 0.8522335886955261
Epoch 1780, training loss: 91.41944885253906 = 0.8425616025924683 + 10.0 * 9.05768871307373
Epoch 1780, val loss: 0.8508332967758179
Epoch 1790, training loss: 91.47552490234375 = 0.8410607576370239 + 10.0 * 9.063446044921875
Epoch 1790, val loss: 0.8494302034378052
Epoch 1800, training loss: 91.51942443847656 = 0.8395807147026062 + 10.0 * 9.067983627319336
Epoch 1800, val loss: 0.848027229309082
Epoch 1810, training loss: 91.52519989013672 = 0.8380966782569885 + 10.0 * 9.068710327148438
Epoch 1810, val loss: 0.8466553688049316
Epoch 1820, training loss: 91.56195831298828 = 0.8366397619247437 + 10.0 * 9.072531700134277
Epoch 1820, val loss: 0.8452900648117065
Epoch 1830, training loss: 91.5799560546875 = 0.8351864218711853 + 10.0 * 9.07447624206543
Epoch 1830, val loss: 0.8439357876777649
Epoch 1840, training loss: 91.57528686523438 = 0.8337430953979492 + 10.0 * 9.0741548538208
Epoch 1840, val loss: 0.8425866365432739
Epoch 1850, training loss: 91.56344604492188 = 0.8323182463645935 + 10.0 * 9.073112487792969
Epoch 1850, val loss: 0.8412305116653442
Epoch 1860, training loss: 91.6122055053711 = 0.8309097290039062 + 10.0 * 9.078129768371582
Epoch 1860, val loss: 0.8399122953414917
Epoch 1870, training loss: 91.62674713134766 = 0.8295077681541443 + 10.0 * 9.079724311828613
Epoch 1870, val loss: 0.8386050462722778
Epoch 1880, training loss: 91.62202453613281 = 0.8281100988388062 + 10.0 * 9.079391479492188
Epoch 1880, val loss: 0.8373090624809265
Epoch 1890, training loss: 91.63980865478516 = 0.8267503976821899 + 10.0 * 9.081305503845215
Epoch 1890, val loss: 0.8360319137573242
Epoch 1900, training loss: 91.6865234375 = 0.8253998756408691 + 10.0 * 9.086112022399902
Epoch 1900, val loss: 0.8347748517990112
Epoch 1910, training loss: 91.70410919189453 = 0.8240402936935425 + 10.0 * 9.088006973266602
Epoch 1910, val loss: 0.8335267901420593
Epoch 1920, training loss: 91.68550109863281 = 0.8227059841156006 + 10.0 * 9.086278915405273
Epoch 1920, val loss: 0.8323124051094055
Epoch 1930, training loss: 91.69469451904297 = 0.8213878870010376 + 10.0 * 9.08733081817627
Epoch 1930, val loss: 0.8310990333557129
Epoch 1940, training loss: 91.7066650390625 = 0.8200839161872864 + 10.0 * 9.088658332824707
Epoch 1940, val loss: 0.8298752903938293
Epoch 1950, training loss: 91.64405059814453 = 0.8187769651412964 + 10.0 * 9.082527160644531
Epoch 1950, val loss: 0.8286873698234558
Epoch 1960, training loss: 91.66107940673828 = 0.8175104260444641 + 10.0 * 9.084356307983398
Epoch 1960, val loss: 0.8275094032287598
Epoch 1970, training loss: 91.71360778808594 = 0.8162247538566589 + 10.0 * 9.089738845825195
Epoch 1970, val loss: 0.8263376355171204
Epoch 1980, training loss: 91.78105163574219 = 0.8149781227111816 + 10.0 * 9.096607208251953
Epoch 1980, val loss: 0.8252010941505432
Epoch 1990, training loss: 91.81683349609375 = 0.8137266039848328 + 10.0 * 9.100310325622559
Epoch 1990, val loss: 0.8240576982498169
Epoch 2000, training loss: 91.80674743652344 = 0.8124739527702332 + 10.0 * 9.099427223205566
Epoch 2000, val loss: 0.8229146599769592
Epoch 2010, training loss: 91.80139923095703 = 0.8112221956253052 + 10.0 * 9.099017143249512
Epoch 2010, val loss: 0.8217824697494507
Epoch 2020, training loss: 91.79460144042969 = 0.8099876642227173 + 10.0 * 9.098461151123047
Epoch 2020, val loss: 0.8206479549407959
Epoch 2030, training loss: 91.80587005615234 = 0.8087763786315918 + 10.0 * 9.099709510803223
Epoch 2030, val loss: 0.8195477724075317
Epoch 2040, training loss: 91.85513305664062 = 0.8075705766677856 + 10.0 * 9.104756355285645
Epoch 2040, val loss: 0.8184620141983032
Epoch 2050, training loss: 91.9045639038086 = 0.8063708543777466 + 10.0 * 9.109819412231445
Epoch 2050, val loss: 0.8173734545707703
Epoch 2060, training loss: 91.88177490234375 = 0.8051560521125793 + 10.0 * 9.107662200927734
Epoch 2060, val loss: 0.8162614107131958
Epoch 2070, training loss: 91.81556701660156 = 0.803995668888092 + 10.0 * 9.101157188415527
Epoch 2070, val loss: 0.8152320384979248
Epoch 2080, training loss: 91.84757232666016 = 0.8028199076652527 + 10.0 * 9.104475021362305
Epoch 2080, val loss: 0.8141884207725525
Epoch 2090, training loss: 91.91091918945312 = 0.8016528487205505 + 10.0 * 9.110926628112793
Epoch 2090, val loss: 0.8131679892539978
Epoch 2100, training loss: 91.88531494140625 = 0.8004581928253174 + 10.0 * 9.108485221862793
Epoch 2100, val loss: 0.8120874762535095
Epoch 2110, training loss: 91.90191650390625 = 0.7992801070213318 + 10.0 * 9.11026382446289
Epoch 2110, val loss: 0.8110297918319702
Epoch 2120, training loss: 91.88841247558594 = 0.7981047034263611 + 10.0 * 9.109030723571777
Epoch 2120, val loss: 0.8099945187568665
Epoch 2130, training loss: 91.84870910644531 = 0.7969686985015869 + 10.0 * 9.10517406463623
Epoch 2130, val loss: 0.8089419603347778
Epoch 2140, training loss: 91.87944793701172 = 0.7958138585090637 + 10.0 * 9.108363151550293
Epoch 2140, val loss: 0.8079328536987305
Epoch 2150, training loss: 91.91524505615234 = 0.7946866750717163 + 10.0 * 9.112055778503418
Epoch 2150, val loss: 0.8069189190864563
Epoch 2160, training loss: 91.97808837890625 = 0.793547511100769 + 10.0 * 9.118453979492188
Epoch 2160, val loss: 0.8059058785438538
Epoch 2170, training loss: 91.92281341552734 = 0.7924000024795532 + 10.0 * 9.113041877746582
Epoch 2170, val loss: 0.8048427700996399
Epoch 2180, training loss: 91.94457244873047 = 0.7912710309028625 + 10.0 * 9.115330696105957
Epoch 2180, val loss: 0.8038554191589355
Epoch 2190, training loss: 91.9964370727539 = 0.790147602558136 + 10.0 * 9.12062931060791
Epoch 2190, val loss: 0.8028515577316284
Epoch 2200, training loss: 91.98079681396484 = 0.7890161275863647 + 10.0 * 9.11917781829834
Epoch 2200, val loss: 0.801855206489563
Epoch 2210, training loss: 91.9420394897461 = 0.787874698638916 + 10.0 * 9.115416526794434
Epoch 2210, val loss: 0.8008354306221008
Epoch 2220, training loss: 91.95597076416016 = 0.7868014574050903 + 10.0 * 9.11691665649414
Epoch 2220, val loss: 0.7998893857002258
Epoch 2230, training loss: 92.01789855957031 = 0.7857041954994202 + 10.0 * 9.12321949005127
Epoch 2230, val loss: 0.7989158034324646
Epoch 2240, training loss: 92.06722259521484 = 0.7846022844314575 + 10.0 * 9.128262519836426
Epoch 2240, val loss: 0.7979388236999512
Epoch 2250, training loss: 92.02253723144531 = 0.7834746837615967 + 10.0 * 9.123906135559082
Epoch 2250, val loss: 0.796923816204071
Epoch 2260, training loss: 92.01162719726562 = 0.7823610901832581 + 10.0 * 9.122926712036133
Epoch 2260, val loss: 0.7959488034248352
Epoch 2270, training loss: 92.06555938720703 = 0.7812581658363342 + 10.0 * 9.128430366516113
Epoch 2270, val loss: 0.7949876189231873
Epoch 2280, training loss: 92.11597442626953 = 0.7801492214202881 + 10.0 * 9.13358211517334
Epoch 2280, val loss: 0.7939997315406799
Epoch 2290, training loss: 92.07112884521484 = 0.7790296673774719 + 10.0 * 9.129209518432617
Epoch 2290, val loss: 0.7930322289466858
Epoch 2300, training loss: 92.08853912353516 = 0.7779310941696167 + 10.0 * 9.131060600280762
Epoch 2300, val loss: 0.7920922636985779
Epoch 2310, training loss: 92.13953399658203 = 0.7768367528915405 + 10.0 * 9.136269569396973
Epoch 2310, val loss: 0.7911376953125
Epoch 2320, training loss: 92.16875457763672 = 0.7757322788238525 + 10.0 * 9.139302253723145
Epoch 2320, val loss: 0.7901752591133118
Epoch 2330, training loss: 92.03105163574219 = 0.7746447324752808 + 10.0 * 9.125640869140625
Epoch 2330, val loss: 0.7891955375671387
Epoch 2340, training loss: 92.02217864990234 = 0.7735515236854553 + 10.0 * 9.124862670898438
Epoch 2340, val loss: 0.7883133888244629
Epoch 2350, training loss: 92.01933288574219 = 0.7724881172180176 + 10.0 * 9.12468433380127
Epoch 2350, val loss: 0.7873178124427795
Epoch 2360, training loss: 91.99683380126953 = 0.7714326977729797 + 10.0 * 9.122540473937988
Epoch 2360, val loss: 0.7864273190498352
Epoch 2370, training loss: 92.02297973632812 = 0.770328164100647 + 10.0 * 9.125265121459961
Epoch 2370, val loss: 0.785452663898468
Epoch 2380, training loss: 92.05547332763672 = 0.7692514657974243 + 10.0 * 9.128622055053711
Epoch 2380, val loss: 0.7845285534858704
Epoch 2390, training loss: 92.1192626953125 = 0.7681716680526733 + 10.0 * 9.135108947753906
Epoch 2390, val loss: 0.7836004495620728
Epoch 2400, training loss: 92.1075439453125 = 0.7670751810073853 + 10.0 * 9.13404655456543
Epoch 2400, val loss: 0.7826360464096069
Epoch 2410, training loss: 92.10528564453125 = 0.7659787535667419 + 10.0 * 9.133931159973145
Epoch 2410, val loss: 0.7816941142082214
Epoch 2420, training loss: 92.13028717041016 = 0.7649179697036743 + 10.0 * 9.136537551879883
Epoch 2420, val loss: 0.7807849645614624
Epoch 2430, training loss: 92.19312286376953 = 0.7638475298881531 + 10.0 * 9.142927169799805
Epoch 2430, val loss: 0.779859185218811
Epoch 2440, training loss: 92.18045806884766 = 0.7627716064453125 + 10.0 * 9.141768455505371
Epoch 2440, val loss: 0.7789182066917419
Epoch 2450, training loss: 92.16658020019531 = 0.7616848349571228 + 10.0 * 9.14048957824707
Epoch 2450, val loss: 0.7779833674430847
Epoch 2460, training loss: 92.16254425048828 = 0.7606139779090881 + 10.0 * 9.140192985534668
Epoch 2460, val loss: 0.7770691514015198
Epoch 2470, training loss: 92.17164611816406 = 0.7595584392547607 + 10.0 * 9.14120864868164
Epoch 2470, val loss: 0.7761603593826294
Epoch 2480, training loss: 92.21890258789062 = 0.7584910988807678 + 10.0 * 9.146040916442871
Epoch 2480, val loss: 0.775232195854187
Epoch 2490, training loss: 92.1269760131836 = 0.7573837041854858 + 10.0 * 9.136959075927734
Epoch 2490, val loss: 0.7742679715156555
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.648695652173913
0.8151851046873869
=== training gcn model ===
Epoch 0, training loss: 103.60845184326172 = 1.1116986274719238 + 10.0 * 10.249674797058105
Epoch 0, val loss: 1.1120929718017578
Epoch 10, training loss: 99.58363342285156 = 1.11134672164917 + 10.0 * 9.84722900390625
Epoch 10, val loss: 1.1117140054702759
Epoch 20, training loss: 97.67829895019531 = 1.110846996307373 + 10.0 * 9.656744956970215
Epoch 20, val loss: 1.1112316846847534
Epoch 30, training loss: 96.29989624023438 = 1.1103154420852661 + 10.0 * 9.51895809173584
Epoch 30, val loss: 1.1107285022735596
Epoch 40, training loss: 95.20355987548828 = 1.1097638607025146 + 10.0 * 9.409379959106445
Epoch 40, val loss: 1.1102046966552734
Epoch 50, training loss: 94.30337524414062 = 1.109204888343811 + 10.0 * 9.319416999816895
Epoch 50, val loss: 1.1096702814102173
Epoch 60, training loss: 93.55010986328125 = 1.1086280345916748 + 10.0 * 9.244148254394531
Epoch 60, val loss: 1.1091200113296509
Epoch 70, training loss: 92.91200256347656 = 1.10804283618927 + 10.0 * 9.18039608001709
Epoch 70, val loss: 1.1085591316223145
Epoch 80, training loss: 92.35279846191406 = 1.1074508428573608 + 10.0 * 9.124534606933594
Epoch 80, val loss: 1.1079950332641602
Epoch 90, training loss: 91.87659454345703 = 1.1068508625030518 + 10.0 * 9.076974868774414
Epoch 90, val loss: 1.107420563697815
Epoch 100, training loss: 91.46586608886719 = 1.1062204837799072 + 10.0 * 9.035964012145996
Epoch 100, val loss: 1.1068193912506104
Epoch 110, training loss: 91.10128021240234 = 1.105592131614685 + 10.0 * 8.999568939208984
Epoch 110, val loss: 1.1062170267105103
Epoch 120, training loss: 90.80136108398438 = 1.1049273014068604 + 10.0 * 8.969643592834473
Epoch 120, val loss: 1.1055854558944702
Epoch 130, training loss: 90.5353775024414 = 1.1042532920837402 + 10.0 * 8.94311237335205
Epoch 130, val loss: 1.1049410104751587
Epoch 140, training loss: 90.28783416748047 = 1.1035590171813965 + 10.0 * 8.918427467346191
Epoch 140, val loss: 1.1042754650115967
Epoch 150, training loss: 90.08631134033203 = 1.1028355360031128 + 10.0 * 8.898347854614258
Epoch 150, val loss: 1.1035922765731812
Epoch 160, training loss: 89.9013671875 = 1.1021140813827515 + 10.0 * 8.879925727844238
Epoch 160, val loss: 1.102901816368103
Epoch 170, training loss: 89.73394775390625 = 1.1013739109039307 + 10.0 * 8.86325740814209
Epoch 170, val loss: 1.1022028923034668
Epoch 180, training loss: 89.60474395751953 = 1.1006125211715698 + 10.0 * 8.85041332244873
Epoch 180, val loss: 1.101470708847046
Epoch 190, training loss: 89.4909439086914 = 1.0998203754425049 + 10.0 * 8.839112281799316
Epoch 190, val loss: 1.1007044315338135
Epoch 200, training loss: 89.3917236328125 = 1.0990021228790283 + 10.0 * 8.829272270202637
Epoch 200, val loss: 1.099933385848999
Epoch 210, training loss: 89.30834197998047 = 1.0982269048690796 + 10.0 * 8.821011543273926
Epoch 210, val loss: 1.0991995334625244
Epoch 220, training loss: 89.2264633178711 = 1.0974074602127075 + 10.0 * 8.812906265258789
Epoch 220, val loss: 1.0984275341033936
Epoch 230, training loss: 89.16809844970703 = 1.0965969562530518 + 10.0 * 8.807149887084961
Epoch 230, val loss: 1.0976686477661133
Epoch 240, training loss: 89.1086654663086 = 1.0957852602005005 + 10.0 * 8.801287651062012
Epoch 240, val loss: 1.0968825817108154
Epoch 250, training loss: 89.0489501953125 = 1.0949605703353882 + 10.0 * 8.795398712158203
Epoch 250, val loss: 1.0961222648620605
Epoch 260, training loss: 89.02061462402344 = 1.0941213369369507 + 10.0 * 8.792649269104004
Epoch 260, val loss: 1.0953596830368042
Epoch 270, training loss: 89.01373291015625 = 1.0932923555374146 + 10.0 * 8.792043685913086
Epoch 270, val loss: 1.0945223569869995
Epoch 280, training loss: 88.95936584472656 = 1.0924967527389526 + 10.0 * 8.786686897277832
Epoch 280, val loss: 1.0938069820404053
Epoch 290, training loss: 88.91767120361328 = 1.0916541814804077 + 10.0 * 8.782602310180664
Epoch 290, val loss: 1.0930179357528687
Epoch 300, training loss: 88.92561340332031 = 1.0908244848251343 + 10.0 * 8.783478736877441
Epoch 300, val loss: 1.0922313928604126
Epoch 310, training loss: 88.91069030761719 = 1.090018391609192 + 10.0 * 8.78206729888916
Epoch 310, val loss: 1.0914602279663086
Epoch 320, training loss: 88.90001678466797 = 1.0892271995544434 + 10.0 * 8.781079292297363
Epoch 320, val loss: 1.0907105207443237
Epoch 330, training loss: 88.87919616699219 = 1.0884284973144531 + 10.0 * 8.779077529907227
Epoch 330, val loss: 1.0899827480316162
Epoch 340, training loss: 88.86984252929688 = 1.0876495838165283 + 10.0 * 8.778219223022461
Epoch 340, val loss: 1.0892547369003296
Epoch 350, training loss: 88.86954498291016 = 1.0869182348251343 + 10.0 * 8.7782621383667
Epoch 350, val loss: 1.0885846614837646
Epoch 360, training loss: 88.85579681396484 = 1.086203694343567 + 10.0 * 8.776959419250488
Epoch 360, val loss: 1.0879409313201904
Epoch 370, training loss: 88.86077880859375 = 1.0855674743652344 + 10.0 * 8.777521133422852
Epoch 370, val loss: 1.0873624086380005
Epoch 380, training loss: 88.86770629882812 = 1.0849528312683105 + 10.0 * 8.778275489807129
Epoch 380, val loss: 1.0868111848831177
Epoch 390, training loss: 88.8688735961914 = 1.0843719244003296 + 10.0 * 8.778450012207031
Epoch 390, val loss: 1.0862714052200317
Epoch 400, training loss: 88.84317016601562 = 1.0838360786437988 + 10.0 * 8.775933265686035
Epoch 400, val loss: 1.0857925415039062
Epoch 410, training loss: 88.835693359375 = 1.083351731300354 + 10.0 * 8.77523422241211
Epoch 410, val loss: 1.0853699445724487
Epoch 420, training loss: 88.8458023071289 = 1.0828827619552612 + 10.0 * 8.776291847229004
Epoch 420, val loss: 1.084964632987976
Epoch 430, training loss: 88.84590148925781 = 1.082421898841858 + 10.0 * 8.776348114013672
Epoch 430, val loss: 1.0845541954040527
Epoch 440, training loss: 88.85694122314453 = 1.0819685459136963 + 10.0 * 8.777497291564941
Epoch 440, val loss: 1.0841660499572754
Epoch 450, training loss: 88.8539047241211 = 1.0815333127975464 + 10.0 * 8.777236938476562
Epoch 450, val loss: 1.0837887525558472
Epoch 460, training loss: 88.86956024169922 = 1.0810920000076294 + 10.0 * 8.778846740722656
Epoch 460, val loss: 1.0834115743637085
Epoch 470, training loss: 88.89342498779297 = 1.080665946006775 + 10.0 * 8.781275749206543
Epoch 470, val loss: 1.0830403566360474
Epoch 480, training loss: 88.99327850341797 = 1.0800526142120361 + 10.0 * 8.791322708129883
Epoch 480, val loss: 1.0825071334838867
Epoch 490, training loss: 89.24431610107422 = 1.0797027349472046 + 10.0 * 8.816461563110352
Epoch 490, val loss: 1.0821669101715088
Epoch 500, training loss: 89.19039916992188 = 1.079344391822815 + 10.0 * 8.811105728149414
Epoch 500, val loss: 1.0819236040115356
Epoch 510, training loss: 89.24066162109375 = 1.0789566040039062 + 10.0 * 8.816170692443848
Epoch 510, val loss: 1.0815536975860596
Epoch 520, training loss: 88.9630126953125 = 1.0785011053085327 + 10.0 * 8.788451194763184
Epoch 520, val loss: 1.0811854600906372
Epoch 530, training loss: 88.9949951171875 = 1.0780622959136963 + 10.0 * 8.791692733764648
Epoch 530, val loss: 1.080807089805603
Epoch 540, training loss: 89.05332946777344 = 1.0776033401489258 + 10.0 * 8.797572135925293
Epoch 540, val loss: 1.0804150104522705
Epoch 550, training loss: 89.08778381347656 = 1.0771644115447998 + 10.0 * 8.801061630249023
Epoch 550, val loss: 1.0800365209579468
Epoch 560, training loss: 89.15283203125 = 1.076733946800232 + 10.0 * 8.807609558105469
Epoch 560, val loss: 1.0796669721603394
Epoch 570, training loss: 89.15620422363281 = 1.0762842893600464 + 10.0 * 8.807991981506348
Epoch 570, val loss: 1.0792781114578247
Epoch 580, training loss: 89.19786834716797 = 1.0758363008499146 + 10.0 * 8.812203407287598
Epoch 580, val loss: 1.078887939453125
Epoch 590, training loss: 89.23957824707031 = 1.0753757953643799 + 10.0 * 8.81641960144043
Epoch 590, val loss: 1.0784945487976074
Epoch 600, training loss: 89.25363159179688 = 1.0749090909957886 + 10.0 * 8.817873001098633
Epoch 600, val loss: 1.0780905485153198
Epoch 610, training loss: 89.2724838256836 = 1.074442982673645 + 10.0 * 8.819804191589355
Epoch 610, val loss: 1.0776900053024292
Epoch 620, training loss: 89.33671569824219 = 1.0739749670028687 + 10.0 * 8.826273918151855
Epoch 620, val loss: 1.0772857666015625
Epoch 630, training loss: 89.34944915771484 = 1.0734951496124268 + 10.0 * 8.827595710754395
Epoch 630, val loss: 1.0768797397613525
Epoch 640, training loss: 89.38433074951172 = 1.073013186454773 + 10.0 * 8.831131935119629
Epoch 640, val loss: 1.0764516592025757
Epoch 650, training loss: 89.40225219726562 = 1.0725243091583252 + 10.0 * 8.832972526550293
Epoch 650, val loss: 1.0760362148284912
Epoch 660, training loss: 89.45668029785156 = 1.0720343589782715 + 10.0 * 8.838464736938477
Epoch 660, val loss: 1.0755894184112549
Epoch 670, training loss: 89.43547058105469 = 1.0715302228927612 + 10.0 * 8.836393356323242
Epoch 670, val loss: 1.0751702785491943
Epoch 680, training loss: 89.49533081054688 = 1.0710251331329346 + 10.0 * 8.84243106842041
Epoch 680, val loss: 1.0747324228286743
Epoch 690, training loss: 89.54476165771484 = 1.0704882144927979 + 10.0 * 8.847427368164062
Epoch 690, val loss: 1.0742700099945068
Epoch 700, training loss: 89.53652954101562 = 1.0699434280395508 + 10.0 * 8.846658706665039
Epoch 700, val loss: 1.0738030672073364
Epoch 710, training loss: 89.5453109741211 = 1.0694079399108887 + 10.0 * 8.847590446472168
Epoch 710, val loss: 1.0733340978622437
Epoch 720, training loss: 89.58699035644531 = 1.0688644647598267 + 10.0 * 8.851812362670898
Epoch 720, val loss: 1.0728695392608643
Epoch 730, training loss: 89.6176986694336 = 1.0683026313781738 + 10.0 * 8.854939460754395
Epoch 730, val loss: 1.0723809003829956
Epoch 740, training loss: 89.6102523803711 = 1.0677144527435303 + 10.0 * 8.854253768920898
Epoch 740, val loss: 1.071869969367981
Epoch 750, training loss: 89.60745239257812 = 1.06710684299469 + 10.0 * 8.854034423828125
Epoch 750, val loss: 1.0713454484939575
Epoch 760, training loss: 89.65508270263672 = 1.0664759874343872 + 10.0 * 8.858860969543457
Epoch 760, val loss: 1.0708039999008179
Epoch 770, training loss: 89.65113067626953 = 1.0658655166625977 + 10.0 * 8.858526229858398
Epoch 770, val loss: 1.0702571868896484
Epoch 780, training loss: 89.63323211669922 = 1.0651441812515259 + 10.0 * 8.85680866241455
Epoch 780, val loss: 1.0696347951889038
Epoch 790, training loss: 89.6565933227539 = 1.0645201206207275 + 10.0 * 8.859207153320312
Epoch 790, val loss: 1.0691027641296387
Epoch 800, training loss: 89.70162200927734 = 1.0639195442199707 + 10.0 * 8.863770484924316
Epoch 800, val loss: 1.06858491897583
Epoch 810, training loss: 89.75636291503906 = 1.063287377357483 + 10.0 * 8.869307518005371
Epoch 810, val loss: 1.068029761314392
Epoch 820, training loss: 89.77044677734375 = 1.0626201629638672 + 10.0 * 8.870782852172852
Epoch 820, val loss: 1.0674539804458618
Epoch 830, training loss: 89.77479553222656 = 1.0619492530822754 + 10.0 * 8.871284484863281
Epoch 830, val loss: 1.066876769065857
Epoch 840, training loss: 89.7666015625 = 1.0612621307373047 + 10.0 * 8.87053394317627
Epoch 840, val loss: 1.066269874572754
Epoch 850, training loss: 89.76402282714844 = 1.060566782951355 + 10.0 * 8.870345115661621
Epoch 850, val loss: 1.065664291381836
Epoch 860, training loss: 89.7937240600586 = 1.0599167346954346 + 10.0 * 8.873380661010742
Epoch 860, val loss: 1.0651001930236816
Epoch 870, training loss: 89.75757598876953 = 1.0590652227401733 + 10.0 * 8.869851112365723
Epoch 870, val loss: 1.0643677711486816
Epoch 880, training loss: 89.82501220703125 = 1.0583903789520264 + 10.0 * 8.876662254333496
Epoch 880, val loss: 1.0637803077697754
Epoch 890, training loss: 89.85137939453125 = 1.057677149772644 + 10.0 * 8.879369735717773
Epoch 890, val loss: 1.063172698020935
Epoch 900, training loss: 89.89632415771484 = 1.0569405555725098 + 10.0 * 8.883938789367676
Epoch 900, val loss: 1.0625457763671875
Epoch 910, training loss: 89.86531066894531 = 1.0561867952346802 + 10.0 * 8.880911827087402
Epoch 910, val loss: 1.0618901252746582
Epoch 920, training loss: 89.9111099243164 = 1.0554306507110596 + 10.0 * 8.885568618774414
Epoch 920, val loss: 1.0612425804138184
Epoch 930, training loss: 89.92183685302734 = 1.0546518564224243 + 10.0 * 8.88671875
Epoch 930, val loss: 1.0605664253234863
Epoch 940, training loss: 89.90805053710938 = 1.0538511276245117 + 10.0 * 8.885419845581055
Epoch 940, val loss: 1.0598840713500977
Epoch 950, training loss: 89.88774871826172 = 1.0530275106430054 + 10.0 * 8.883472442626953
Epoch 950, val loss: 1.059179425239563
Epoch 960, training loss: 89.84342193603516 = 1.0521923303604126 + 10.0 * 8.879122734069824
Epoch 960, val loss: 1.0584619045257568
Epoch 970, training loss: 89.93456268310547 = 1.0514233112335205 + 10.0 * 8.888314247131348
Epoch 970, val loss: 1.057796835899353
Epoch 980, training loss: 90.02030181884766 = 1.0506130456924438 + 10.0 * 8.896968841552734
Epoch 980, val loss: 1.057099461555481
Epoch 990, training loss: 90.02426147460938 = 1.0497760772705078 + 10.0 * 8.897448539733887
Epoch 990, val loss: 1.0563691854476929
Epoch 1000, training loss: 90.02510833740234 = 1.0489115715026855 + 10.0 * 8.897619247436523
Epoch 1000, val loss: 1.05562162399292
Epoch 1010, training loss: 90.0544204711914 = 1.04805588722229 + 10.0 * 8.900636672973633
Epoch 1010, val loss: 1.0549005270004272
Epoch 1020, training loss: 90.08973693847656 = 1.0471800565719604 + 10.0 * 8.904255867004395
Epoch 1020, val loss: 1.0541400909423828
Epoch 1030, training loss: 90.08966827392578 = 1.0462839603424072 + 10.0 * 8.904337882995605
Epoch 1030, val loss: 1.0533586740493774
Epoch 1040, training loss: 90.1176986694336 = 1.0453920364379883 + 10.0 * 8.907230377197266
Epoch 1040, val loss: 1.0526072978973389
Epoch 1050, training loss: 90.17471313476562 = 1.0444804430007935 + 10.0 * 8.913022994995117
Epoch 1050, val loss: 1.0518321990966797
Epoch 1060, training loss: 90.18319702148438 = 1.0435622930526733 + 10.0 * 8.913963317871094
Epoch 1060, val loss: 1.0510361194610596
Epoch 1070, training loss: 90.23823547363281 = 1.042635440826416 + 10.0 * 8.919560432434082
Epoch 1070, val loss: 1.0502293109893799
Epoch 1080, training loss: 90.18540954589844 = 1.0416483879089355 + 10.0 * 8.914376258850098
Epoch 1080, val loss: 1.0493919849395752
Epoch 1090, training loss: 90.18812561035156 = 1.0406999588012695 + 10.0 * 8.914742469787598
Epoch 1090, val loss: 1.04856538772583
Epoch 1100, training loss: 90.21382904052734 = 1.0397204160690308 + 10.0 * 8.917410850524902
Epoch 1100, val loss: 1.0477354526519775
Epoch 1110, training loss: 90.27095031738281 = 1.0387455224990845 + 10.0 * 8.92322063446045
Epoch 1110, val loss: 1.0468988418579102
Epoch 1120, training loss: 90.27098083496094 = 1.0377484560012817 + 10.0 * 8.923322677612305
Epoch 1120, val loss: 1.046049952507019
Epoch 1130, training loss: 90.30482482910156 = 1.0367591381072998 + 10.0 * 8.926806449890137
Epoch 1130, val loss: 1.0452097654342651
Epoch 1140, training loss: 90.33490753173828 = 1.0357580184936523 + 10.0 * 8.929914474487305
Epoch 1140, val loss: 1.0443557500839233
Epoch 1150, training loss: 90.30337524414062 = 1.0347135066986084 + 10.0 * 8.92686653137207
Epoch 1150, val loss: 1.0434600114822388
Epoch 1160, training loss: 90.36270141601562 = 1.0337036848068237 + 10.0 * 8.932899475097656
Epoch 1160, val loss: 1.0425944328308105
Epoch 1170, training loss: 90.39119720458984 = 1.0326709747314453 + 10.0 * 8.935853004455566
Epoch 1170, val loss: 1.0417166948318481
Epoch 1180, training loss: 90.39108276367188 = 1.0316141843795776 + 10.0 * 8.935946464538574
Epoch 1180, val loss: 1.0408021211624146
Epoch 1190, training loss: 90.38900756835938 = 1.0305302143096924 + 10.0 * 8.935847282409668
Epoch 1190, val loss: 1.0399010181427002
Epoch 1200, training loss: 90.4512710571289 = 1.0294548273086548 + 10.0 * 8.942181587219238
Epoch 1200, val loss: 1.038943886756897
Epoch 1210, training loss: 90.45094299316406 = 1.0283676385879517 + 10.0 * 8.94225788116455
Epoch 1210, val loss: 1.0380514860153198
Epoch 1220, training loss: 90.44738006591797 = 1.027299165725708 + 10.0 * 8.942008018493652
Epoch 1220, val loss: 1.0371360778808594
Epoch 1230, training loss: 90.4565200805664 = 1.0262175798416138 + 10.0 * 8.94303035736084
Epoch 1230, val loss: 1.0362213850021362
Epoch 1240, training loss: 90.4141845703125 = 1.025076150894165 + 10.0 * 8.938910484313965
Epoch 1240, val loss: 1.035244107246399
Epoch 1250, training loss: 90.45603942871094 = 1.0239721536636353 + 10.0 * 8.943206787109375
Epoch 1250, val loss: 1.0343209505081177
Epoch 1260, training loss: 90.55288696289062 = 1.0228761434555054 + 10.0 * 8.953001022338867
Epoch 1260, val loss: 1.0333822965621948
Epoch 1270, training loss: 90.5560302734375 = 1.0217323303222656 + 10.0 * 8.95343017578125
Epoch 1270, val loss: 1.0323991775512695
Epoch 1280, training loss: 90.55583953857422 = 1.0205767154693604 + 10.0 * 8.953526496887207
Epoch 1280, val loss: 1.0314297676086426
Epoch 1290, training loss: 90.59605407714844 = 1.0194437503814697 + 10.0 * 8.957660675048828
Epoch 1290, val loss: 1.0304672718048096
Epoch 1300, training loss: 90.65000915527344 = 1.0182790756225586 + 10.0 * 8.963172912597656
Epoch 1300, val loss: 1.029473900794983
Epoch 1310, training loss: 90.61102294921875 = 1.0170776844024658 + 10.0 * 8.959394454956055
Epoch 1310, val loss: 1.0284606218338013
Epoch 1320, training loss: 90.65753173828125 = 1.0158945322036743 + 10.0 * 8.964163780212402
Epoch 1320, val loss: 1.0274429321289062
Epoch 1330, training loss: 90.67266845703125 = 1.0146799087524414 + 10.0 * 8.965799331665039
Epoch 1330, val loss: 1.0264160633087158
Epoch 1340, training loss: 90.71437072753906 = 1.0134408473968506 + 10.0 * 8.9700927734375
Epoch 1340, val loss: 1.025342583656311
Epoch 1350, training loss: 90.71900177001953 = 1.0121546983718872 + 10.0 * 8.970685005187988
Epoch 1350, val loss: 1.0242427587509155
Epoch 1360, training loss: 90.74090576171875 = 1.0107539892196655 + 10.0 * 8.973014831542969
Epoch 1360, val loss: 1.0230531692504883
Epoch 1370, training loss: 90.80799102783203 = 1.0095542669296265 + 10.0 * 8.979844093322754
Epoch 1370, val loss: 1.0220403671264648
Epoch 1380, training loss: 90.58645629882812 = 1.008124828338623 + 10.0 * 8.957833290100098
Epoch 1380, val loss: 1.0208238363265991
Epoch 1390, training loss: 90.63877868652344 = 1.0067100524902344 + 10.0 * 8.96320629119873
Epoch 1390, val loss: 1.019653558731079
Epoch 1400, training loss: 90.63446044921875 = 1.0053436756134033 + 10.0 * 8.962911605834961
Epoch 1400, val loss: 1.0185116529464722
Epoch 1410, training loss: 90.72515869140625 = 1.0040136575698853 + 10.0 * 8.972114562988281
Epoch 1410, val loss: 1.0173728466033936
Epoch 1420, training loss: 90.79228210449219 = 1.0026835203170776 + 10.0 * 8.978960037231445
Epoch 1420, val loss: 1.0162333250045776
Epoch 1430, training loss: 90.7712631225586 = 1.0012880563735962 + 10.0 * 8.976997375488281
Epoch 1430, val loss: 1.0150634050369263
Epoch 1440, training loss: 90.78396606445312 = 0.9999185800552368 + 10.0 * 8.978404998779297
Epoch 1440, val loss: 1.0139055252075195
Epoch 1450, training loss: 90.84115600585938 = 0.99855637550354 + 10.0 * 8.984259605407715
Epoch 1450, val loss: 1.012750506401062
Epoch 1460, training loss: 90.86007690429688 = 0.9971852898597717 + 10.0 * 8.986289024353027
Epoch 1460, val loss: 1.0115845203399658
Epoch 1470, training loss: 90.85353088378906 = 0.9957842826843262 + 10.0 * 8.985774993896484
Epoch 1470, val loss: 1.010407567024231
Epoch 1480, training loss: 90.9007339477539 = 0.9944124221801758 + 10.0 * 8.990632057189941
Epoch 1480, val loss: 1.009243130683899
Epoch 1490, training loss: 90.88925170898438 = 0.9930223226547241 + 10.0 * 8.989623069763184
Epoch 1490, val loss: 1.0080785751342773
Epoch 1500, training loss: 90.94855499267578 = 0.9916492700576782 + 10.0 * 8.995691299438477
Epoch 1500, val loss: 1.0069133043289185
Epoch 1510, training loss: 90.88897705078125 = 0.9902127981185913 + 10.0 * 8.989876747131348
Epoch 1510, val loss: 1.005716323852539
Epoch 1520, training loss: 90.81541442871094 = 0.9887855052947998 + 10.0 * 8.98266315460205
Epoch 1520, val loss: 1.0045256614685059
Epoch 1530, training loss: 90.89358520507812 = 0.9874290823936462 + 10.0 * 8.990615844726562
Epoch 1530, val loss: 1.0033832788467407
Epoch 1540, training loss: 90.94815063476562 = 0.9860329031944275 + 10.0 * 8.996212005615234
Epoch 1540, val loss: 1.00221586227417
Epoch 1550, training loss: 90.96783447265625 = 0.9846251010894775 + 10.0 * 8.998320579528809
Epoch 1550, val loss: 1.0010424852371216
Epoch 1560, training loss: 90.96086120605469 = 0.9832106232643127 + 10.0 * 8.99776554107666
Epoch 1560, val loss: 0.9998621344566345
Epoch 1570, training loss: 91.00147247314453 = 0.9818160533905029 + 10.0 * 9.001965522766113
Epoch 1570, val loss: 0.9986925721168518
Epoch 1580, training loss: 90.99871063232422 = 0.9803950786590576 + 10.0 * 9.001832008361816
Epoch 1580, val loss: 0.9975094199180603
Epoch 1590, training loss: 91.0113754272461 = 0.9789844155311584 + 10.0 * 9.003239631652832
Epoch 1590, val loss: 0.9963364601135254
Epoch 1600, training loss: 91.07500457763672 = 0.9775946736335754 + 10.0 * 9.009740829467773
Epoch 1600, val loss: 0.9951769709587097
Epoch 1610, training loss: 91.04544067382812 = 0.9761642813682556 + 10.0 * 9.006927490234375
Epoch 1610, val loss: 0.9939873218536377
Epoch 1620, training loss: 91.05580139160156 = 0.9747480750083923 + 10.0 * 9.008105278015137
Epoch 1620, val loss: 0.9928190112113953
Epoch 1630, training loss: 91.10252380371094 = 0.9733538627624512 + 10.0 * 9.012917518615723
Epoch 1630, val loss: 0.9916459321975708
Epoch 1640, training loss: 91.12189483642578 = 0.9719655513763428 + 10.0 * 9.014993667602539
Epoch 1640, val loss: 0.9904904961585999
Epoch 1650, training loss: 91.13306427001953 = 0.970564603805542 + 10.0 * 9.016249656677246
Epoch 1650, val loss: 0.9893372654914856
Epoch 1660, training loss: 91.09820556640625 = 0.9691534638404846 + 10.0 * 9.01290512084961
Epoch 1660, val loss: 0.9881709218025208
Epoch 1670, training loss: 91.13298034667969 = 0.9677854776382446 + 10.0 * 9.016519546508789
Epoch 1670, val loss: 0.9870227575302124
Epoch 1680, training loss: 91.1699447631836 = 0.9664066433906555 + 10.0 * 9.020353317260742
Epoch 1680, val loss: 0.985875129699707
Epoch 1690, training loss: 91.21053314208984 = 0.9650301933288574 + 10.0 * 9.024550437927246
Epoch 1690, val loss: 0.9847765564918518
Epoch 1700, training loss: 91.15972137451172 = 0.963687002658844 + 10.0 * 9.019603729248047
Epoch 1700, val loss: 0.9836341142654419
Epoch 1710, training loss: 91.17151641845703 = 0.9623191356658936 + 10.0 * 9.020919799804688
Epoch 1710, val loss: 0.9824864864349365
Epoch 1720, training loss: 91.16960144042969 = 0.9609779119491577 + 10.0 * 9.020862579345703
Epoch 1720, val loss: 0.98136305809021
Epoch 1730, training loss: 91.23979949951172 = 0.9596301913261414 + 10.0 * 9.028017044067383
Epoch 1730, val loss: 0.9802419543266296
Epoch 1740, training loss: 91.25983428955078 = 0.9582763910293579 + 10.0 * 9.030156135559082
Epoch 1740, val loss: 0.9791166186332703
Epoch 1750, training loss: 91.20536041259766 = 0.9569308757781982 + 10.0 * 9.024843215942383
Epoch 1750, val loss: 0.9780220985412598
Epoch 1760, training loss: 91.1885986328125 = 0.9556411504745483 + 10.0 * 9.023295402526855
Epoch 1760, val loss: 0.9769202470779419
Epoch 1770, training loss: 91.2230453491211 = 0.9532220959663391 + 10.0 * 9.026982307434082
Epoch 1770, val loss: 0.9744457602500916
Epoch 1780, training loss: 91.28165435791016 = 0.9496724009513855 + 10.0 * 9.033198356628418
Epoch 1780, val loss: 0.9710660576820374
Epoch 1790, training loss: 91.30615997314453 = 0.9462517499923706 + 10.0 * 9.035990715026855
Epoch 1790, val loss: 0.9678325653076172
Epoch 1800, training loss: 91.30921936035156 = 0.9430221915245056 + 10.0 * 9.036619186401367
Epoch 1800, val loss: 0.9648157358169556
Epoch 1810, training loss: 91.3619384765625 = 0.9399925470352173 + 10.0 * 9.042194366455078
Epoch 1810, val loss: 0.9619965553283691
Epoch 1820, training loss: 91.3369369506836 = 0.9370865821838379 + 10.0 * 9.039984703063965
Epoch 1820, val loss: 0.9592987298965454
Epoch 1830, training loss: 91.35252380371094 = 0.9342713952064514 + 10.0 * 9.041825294494629
Epoch 1830, val loss: 0.9567068219184875
Epoch 1840, training loss: 91.41268920898438 = 0.9315347075462341 + 10.0 * 9.048115730285645
Epoch 1840, val loss: 0.9541847109794617
Epoch 1850, training loss: 91.35147094726562 = 0.9288492798805237 + 10.0 * 9.042262077331543
Epoch 1850, val loss: 0.9517051577568054
Epoch 1860, training loss: 91.32083129882812 = 0.9262118935585022 + 10.0 * 9.039462089538574
Epoch 1860, val loss: 0.9492983222007751
Epoch 1870, training loss: 91.37315368652344 = 0.9236290454864502 + 10.0 * 9.044952392578125
Epoch 1870, val loss: 0.9469225406646729
Epoch 1880, training loss: 91.43775177001953 = 0.9210825562477112 + 10.0 * 9.051667213439941
Epoch 1880, val loss: 0.9445868730545044
Epoch 1890, training loss: 91.47846221923828 = 0.9185500741004944 + 10.0 * 9.055991172790527
Epoch 1890, val loss: 0.9422618746757507
Epoch 1900, training loss: 91.4384536743164 = 0.916028618812561 + 10.0 * 9.052242279052734
Epoch 1900, val loss: 0.9399563670158386
Epoch 1910, training loss: 91.47624206542969 = 0.9135309457778931 + 10.0 * 9.05627155303955
Epoch 1910, val loss: 0.9376895427703857
Epoch 1920, training loss: 91.478759765625 = 0.911041796207428 + 10.0 * 9.056772232055664
Epoch 1920, val loss: 0.9354314804077148
Epoch 1930, training loss: 91.49012756347656 = 0.9085738062858582 + 10.0 * 9.058155059814453
Epoch 1930, val loss: 0.9332044124603271
Epoch 1940, training loss: 91.507568359375 = 0.9060809016227722 + 10.0 * 9.060148239135742
Epoch 1940, val loss: 0.9309283494949341
Epoch 1950, training loss: 91.53499603271484 = 0.9036043882369995 + 10.0 * 9.063138961791992
Epoch 1950, val loss: 0.92868971824646
Epoch 1960, training loss: 91.53848266601562 = 0.9011216759681702 + 10.0 * 9.063735961914062
Epoch 1960, val loss: 0.9264448881149292
Epoch 1970, training loss: 91.52490234375 = 0.8986365795135498 + 10.0 * 9.062626838684082
Epoch 1970, val loss: 0.9241558909416199
Epoch 1980, training loss: 91.48442077636719 = 0.8960838317871094 + 10.0 * 9.058833122253418
Epoch 1980, val loss: 0.9218693375587463
Epoch 1990, training loss: 91.47184753417969 = 0.8936102390289307 + 10.0 * 9.05782413482666
Epoch 1990, val loss: 0.9196681976318359
Epoch 2000, training loss: 91.5043716430664 = 0.891116201877594 + 10.0 * 9.061326026916504
Epoch 2000, val loss: 0.9174361824989319
Epoch 2010, training loss: 91.5572738647461 = 0.8886426091194153 + 10.0 * 9.066863059997559
Epoch 2010, val loss: 0.9151997566223145
Epoch 2020, training loss: 91.58169555664062 = 0.8861544132232666 + 10.0 * 9.069554328918457
Epoch 2020, val loss: 0.9129670262336731
Epoch 2030, training loss: 91.51727294921875 = 0.8836512565612793 + 10.0 * 9.063362121582031
Epoch 2030, val loss: 0.9107387661933899
Epoch 2040, training loss: 91.52994537353516 = 0.8811550140380859 + 10.0 * 9.064878463745117
Epoch 2040, val loss: 0.9084947109222412
Epoch 2050, training loss: 91.61613464355469 = 0.8786488175392151 + 10.0 * 9.073748588562012
Epoch 2050, val loss: 0.9062466025352478
Epoch 2060, training loss: 91.6478042602539 = 0.876120924949646 + 10.0 * 9.077168464660645
Epoch 2060, val loss: 0.9039834141731262
Epoch 2070, training loss: 91.64373779296875 = 0.8735591173171997 + 10.0 * 9.077017784118652
Epoch 2070, val loss: 0.9016864895820618
Epoch 2080, training loss: 91.59425354003906 = 0.8710294365882874 + 10.0 * 9.072321891784668
Epoch 2080, val loss: 0.8994486927986145
Epoch 2090, training loss: 91.6161880493164 = 0.8684351444244385 + 10.0 * 9.074774742126465
Epoch 2090, val loss: 0.8971546292304993
Epoch 2100, training loss: 91.63832092285156 = 0.8658916354179382 + 10.0 * 9.077242851257324
Epoch 2100, val loss: 0.8948492407798767
Epoch 2110, training loss: 91.68274688720703 = 0.8633167147636414 + 10.0 * 9.081942558288574
Epoch 2110, val loss: 0.8925705552101135
Epoch 2120, training loss: 91.67090606689453 = 0.8607056736946106 + 10.0 * 9.08102035522461
Epoch 2120, val loss: 0.8902443051338196
Epoch 2130, training loss: 91.67537689208984 = 0.8581111431121826 + 10.0 * 9.081727027893066
Epoch 2130, val loss: 0.8879343271255493
Epoch 2140, training loss: 91.71731567382812 = 0.8555002808570862 + 10.0 * 9.086181640625
Epoch 2140, val loss: 0.8856085538864136
Epoch 2150, training loss: 91.73703002929688 = 0.8528598546981812 + 10.0 * 9.088417053222656
Epoch 2150, val loss: 0.8832405209541321
Epoch 2160, training loss: 91.73744201660156 = 0.8502284288406372 + 10.0 * 9.08872127532959
Epoch 2160, val loss: 0.8808931708335876
Epoch 2170, training loss: 91.7276382446289 = 0.8475770354270935 + 10.0 * 9.088006019592285
Epoch 2170, val loss: 0.878501296043396
Epoch 2180, training loss: 91.7623519897461 = 0.84492027759552 + 10.0 * 9.091743469238281
Epoch 2180, val loss: 0.8761373162269592
Epoch 2190, training loss: 91.75791931152344 = 0.8422521352767944 + 10.0 * 9.09156608581543
Epoch 2190, val loss: 0.8737249374389648
Epoch 2200, training loss: 91.75154876708984 = 0.8395653367042542 + 10.0 * 9.091197967529297
Epoch 2200, val loss: 0.871322751045227
Epoch 2210, training loss: 91.74603271484375 = 0.836870551109314 + 10.0 * 9.090916633605957
Epoch 2210, val loss: 0.8689060211181641
Epoch 2220, training loss: 91.76596069335938 = 0.8342131972312927 + 10.0 * 9.093174934387207
Epoch 2220, val loss: 0.8664876818656921
Epoch 2230, training loss: 91.70748138427734 = 0.8315427899360657 + 10.0 * 9.087594032287598
Epoch 2230, val loss: 0.8640699982643127
Epoch 2240, training loss: 91.68092346191406 = 0.8288333415985107 + 10.0 * 9.085208892822266
Epoch 2240, val loss: 0.861677885055542
Epoch 2250, training loss: 91.73506927490234 = 0.8260398507118225 + 10.0 * 9.090902328491211
Epoch 2250, val loss: 0.8591890931129456
Epoch 2260, training loss: 91.81748962402344 = 0.8232949376106262 + 10.0 * 9.099419593811035
Epoch 2260, val loss: 0.8567150235176086
Epoch 2270, training loss: 91.82466888427734 = 0.8204901218414307 + 10.0 * 9.100418090820312
Epoch 2270, val loss: 0.8541911840438843
Epoch 2280, training loss: 91.82838439941406 = 0.8176746368408203 + 10.0 * 9.10107135772705
Epoch 2280, val loss: 0.8516908884048462
Epoch 2290, training loss: 91.86681365966797 = 0.8148647546768188 + 10.0 * 9.105195045471191
Epoch 2290, val loss: 0.8491835594177246
Epoch 2300, training loss: 91.84831237792969 = 0.8120189905166626 + 10.0 * 9.103629112243652
Epoch 2300, val loss: 0.8466446399688721
Epoch 2310, training loss: 91.79638671875 = 0.8091335892677307 + 10.0 * 9.098725318908691
Epoch 2310, val loss: 0.8440970182418823
Epoch 2320, training loss: 91.71512603759766 = 0.8063040375709534 + 10.0 * 9.090882301330566
Epoch 2320, val loss: 0.8415557742118835
Epoch 2330, training loss: 91.78244018554688 = 0.8034724593162537 + 10.0 * 9.097896575927734
Epoch 2330, val loss: 0.8390321731567383
Epoch 2340, training loss: 91.81708526611328 = 0.8005611300468445 + 10.0 * 9.101652145385742
Epoch 2340, val loss: 0.8364524245262146
Epoch 2350, training loss: 91.88784790039062 = 0.7976905703544617 + 10.0 * 9.109015464782715
Epoch 2350, val loss: 0.8338981866836548
Epoch 2360, training loss: 91.90118408203125 = 0.7947636842727661 + 10.0 * 9.110642433166504
Epoch 2360, val loss: 0.8313319087028503
Epoch 2370, training loss: 91.89530944824219 = 0.7918304204940796 + 10.0 * 9.110347747802734
Epoch 2370, val loss: 0.828732430934906
Epoch 2380, training loss: 91.9140625 = 0.7888898849487305 + 10.0 * 9.112517356872559
Epoch 2380, val loss: 0.8261443376541138
Epoch 2390, training loss: 91.8514633178711 = 0.7859516143798828 + 10.0 * 9.106551170349121
Epoch 2390, val loss: 0.8235859274864197
Epoch 2400, training loss: 91.88749694824219 = 0.7830499410629272 + 10.0 * 9.110445022583008
Epoch 2400, val loss: 0.8210159540176392
Epoch 2410, training loss: 91.94752502441406 = 0.7800939679145813 + 10.0 * 9.116743087768555
Epoch 2410, val loss: 0.8184252977371216
Epoch 2420, training loss: 91.93939208984375 = 0.7771201729774475 + 10.0 * 9.116227149963379
Epoch 2420, val loss: 0.8158363699913025
Epoch 2430, training loss: 91.9680404663086 = 0.7741478681564331 + 10.0 * 9.119389533996582
Epoch 2430, val loss: 0.8132386207580566
Epoch 2440, training loss: 91.96772766113281 = 0.7711380124092102 + 10.0 * 9.119658470153809
Epoch 2440, val loss: 0.8105894923210144
Epoch 2450, training loss: 91.93863677978516 = 0.7681018114089966 + 10.0 * 9.117053031921387
Epoch 2450, val loss: 0.8079496026039124
Epoch 2460, training loss: 91.92632293701172 = 0.7651699781417847 + 10.0 * 9.11611557006836
Epoch 2460, val loss: 0.8054015040397644
Epoch 2470, training loss: 91.98229217529297 = 0.7622070908546448 + 10.0 * 9.122008323669434
Epoch 2470, val loss: 0.8027945756912231
Epoch 2480, training loss: 92.00701141357422 = 0.759214460849762 + 10.0 * 9.12477970123291
Epoch 2480, val loss: 0.8002023100852966
Epoch 2490, training loss: 91.9987564086914 = 0.7562726140022278 + 10.0 * 9.124248504638672
Epoch 2490, val loss: 0.7976252436637878
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7181159420289855
0.8149677606317468
=== training gcn model ===
Epoch 0, training loss: 104.1259765625 = 1.0924118757247925 + 10.0 * 10.303356170654297
Epoch 0, val loss: 1.09229576587677
Epoch 10, training loss: 99.99981689453125 = 1.0920354127883911 + 10.0 * 9.890778541564941
Epoch 10, val loss: 1.0919939279556274
Epoch 20, training loss: 98.05470275878906 = 1.0918173789978027 + 10.0 * 9.696288108825684
Epoch 20, val loss: 1.091745376586914
Epoch 30, training loss: 96.65596008300781 = 1.0914645195007324 + 10.0 * 9.556449890136719
Epoch 30, val loss: 1.0913927555084229
Epoch 40, training loss: 95.57027435302734 = 1.0911263227462769 + 10.0 * 9.447915077209473
Epoch 40, val loss: 1.0910611152648926
Epoch 50, training loss: 94.6844711303711 = 1.0907591581344604 + 10.0 * 9.359371185302734
Epoch 50, val loss: 1.0907059907913208
Epoch 60, training loss: 93.94599151611328 = 1.090390682220459 + 10.0 * 9.28555965423584
Epoch 60, val loss: 1.0903509855270386
Epoch 70, training loss: 93.30874633789062 = 1.0900026559829712 + 10.0 * 9.221874237060547
Epoch 70, val loss: 1.0899726152420044
Epoch 80, training loss: 92.76839447021484 = 1.089617133140564 + 10.0 * 9.167878150939941
Epoch 80, val loss: 1.0895955562591553
Epoch 90, training loss: 92.30060577392578 = 1.0892139673233032 + 10.0 * 9.121139526367188
Epoch 90, val loss: 1.0892045497894287
Epoch 100, training loss: 91.89253997802734 = 1.0887900590896606 + 10.0 * 9.080374717712402
Epoch 100, val loss: 1.088797926902771
Epoch 110, training loss: 91.52889251708984 = 1.0883407592773438 + 10.0 * 9.044054985046387
Epoch 110, val loss: 1.0883647203445435
Epoch 120, training loss: 91.22931671142578 = 1.087871789932251 + 10.0 * 9.014143943786621
Epoch 120, val loss: 1.0879100561141968
Epoch 130, training loss: 90.97122955322266 = 1.0874079465866089 + 10.0 * 8.988382339477539
Epoch 130, val loss: 1.087458848953247
Epoch 140, training loss: 90.75448608398438 = 1.086928129196167 + 10.0 * 8.966755867004395
Epoch 140, val loss: 1.0869945287704468
Epoch 150, training loss: 90.55162811279297 = 1.086409091949463 + 10.0 * 8.946521759033203
Epoch 150, val loss: 1.0864956378936768
Epoch 160, training loss: 90.3879165649414 = 1.0858827829360962 + 10.0 * 8.930203437805176
Epoch 160, val loss: 1.0859854221343994
Epoch 170, training loss: 90.22817993164062 = 1.0853238105773926 + 10.0 * 8.914285659790039
Epoch 170, val loss: 1.085447907447815
Epoch 180, training loss: 90.11820220947266 = 1.0847750902175903 + 10.0 * 8.903342247009277
Epoch 180, val loss: 1.0849170684814453
Epoch 190, training loss: 89.9976577758789 = 1.0842084884643555 + 10.0 * 8.891345024108887
Epoch 190, val loss: 1.0843716859817505
Epoch 200, training loss: 89.90283203125 = 1.0836145877838135 + 10.0 * 8.881921768188477
Epoch 200, val loss: 1.0838027000427246
Epoch 210, training loss: 89.822998046875 = 1.0830020904541016 + 10.0 * 8.87399959564209
Epoch 210, val loss: 1.0832170248031616
Epoch 220, training loss: 89.73271179199219 = 1.0823822021484375 + 10.0 * 8.865033149719238
Epoch 220, val loss: 1.0826183557510376
Epoch 230, training loss: 89.69355010986328 = 1.0817668437957764 + 10.0 * 8.861178398132324
Epoch 230, val loss: 1.0820324420928955
Epoch 240, training loss: 89.61662292480469 = 1.081122875213623 + 10.0 * 8.85354995727539
Epoch 240, val loss: 1.0814207792282104
Epoch 250, training loss: 89.55831146240234 = 1.0804641246795654 + 10.0 * 8.847784042358398
Epoch 250, val loss: 1.0807896852493286
Epoch 260, training loss: 89.51201629638672 = 1.079796552658081 + 10.0 * 8.843221664428711
Epoch 260, val loss: 1.0801489353179932
Epoch 270, training loss: 89.4829330444336 = 1.0791096687316895 + 10.0 * 8.84038257598877
Epoch 270, val loss: 1.0794905424118042
Epoch 280, training loss: 89.46617889404297 = 1.0784144401550293 + 10.0 * 8.838776588439941
Epoch 280, val loss: 1.078832745552063
Epoch 290, training loss: 89.43234252929688 = 1.0777000188827515 + 10.0 * 8.835464477539062
Epoch 290, val loss: 1.0781537294387817
Epoch 300, training loss: 89.44072723388672 = 1.0769892930984497 + 10.0 * 8.836374282836914
Epoch 300, val loss: 1.0774714946746826
Epoch 310, training loss: 89.39232635498047 = 1.0762511491775513 + 10.0 * 8.831607818603516
Epoch 310, val loss: 1.0767675638198853
Epoch 320, training loss: 89.38954162597656 = 1.0754945278167725 + 10.0 * 8.831404685974121
Epoch 320, val loss: 1.0760403871536255
Epoch 330, training loss: 89.38063049316406 = 1.074730396270752 + 10.0 * 8.83059024810791
Epoch 330, val loss: 1.075318694114685
Epoch 340, training loss: 89.4588623046875 = 1.0739479064941406 + 10.0 * 8.838491439819336
Epoch 340, val loss: 1.0745679140090942
Epoch 350, training loss: 89.43052673339844 = 1.0731977224349976 + 10.0 * 8.835733413696289
Epoch 350, val loss: 1.0738461017608643
Epoch 360, training loss: 89.32896423339844 = 1.0723663568496704 + 10.0 * 8.82565975189209
Epoch 360, val loss: 1.0730726718902588
Epoch 370, training loss: 89.33910369873047 = 1.071555256843567 + 10.0 * 8.826754570007324
Epoch 370, val loss: 1.0722969770431519
Epoch 380, training loss: 89.33136749267578 = 1.0707039833068848 + 10.0 * 8.826066970825195
Epoch 380, val loss: 1.0714892148971558
Epoch 390, training loss: 89.35064697265625 = 1.0698822736740112 + 10.0 * 8.828076362609863
Epoch 390, val loss: 1.0706980228424072
Epoch 400, training loss: 89.35297393798828 = 1.0690184831619263 + 10.0 * 8.82839584350586
Epoch 400, val loss: 1.0698821544647217
Epoch 410, training loss: 89.34358215332031 = 1.0681345462799072 + 10.0 * 8.827544212341309
Epoch 410, val loss: 1.0690439939498901
Epoch 420, training loss: 89.35488891601562 = 1.067233681678772 + 10.0 * 8.828765869140625
Epoch 420, val loss: 1.0681854486465454
Epoch 430, training loss: 89.34317016601562 = 1.0663135051727295 + 10.0 * 8.827685356140137
Epoch 430, val loss: 1.0673141479492188
Epoch 440, training loss: 89.37675476074219 = 1.0654035806655884 + 10.0 * 8.831134796142578
Epoch 440, val loss: 1.066434621810913
Epoch 450, training loss: 89.40665435791016 = 1.0644339323043823 + 10.0 * 8.834222793579102
Epoch 450, val loss: 1.0655162334442139
Epoch 460, training loss: 89.4261245727539 = 1.0635043382644653 + 10.0 * 8.836261749267578
Epoch 460, val loss: 1.0646361112594604
Epoch 470, training loss: 89.40357971191406 = 1.0625206232070923 + 10.0 * 8.834105491638184
Epoch 470, val loss: 1.0637013912200928
Epoch 480, training loss: 89.44552612304688 = 1.0615423917770386 + 10.0 * 8.838398933410645
Epoch 480, val loss: 1.0627565383911133
Epoch 490, training loss: 89.44149017333984 = 1.0605233907699585 + 10.0 * 8.838096618652344
Epoch 490, val loss: 1.061793327331543
Epoch 500, training loss: 89.47029876708984 = 1.059503436088562 + 10.0 * 8.841079711914062
Epoch 500, val loss: 1.060831069946289
Epoch 510, training loss: 89.44672393798828 = 1.0584659576416016 + 10.0 * 8.838826179504395
Epoch 510, val loss: 1.0598337650299072
Epoch 520, training loss: 89.46205139160156 = 1.0574162006378174 + 10.0 * 8.840463638305664
Epoch 520, val loss: 1.0588515996932983
Epoch 530, training loss: 89.4848861694336 = 1.0563393831253052 + 10.0 * 8.842854499816895
Epoch 530, val loss: 1.0578163862228394
Epoch 540, training loss: 89.5035171508789 = 1.0552698373794556 + 10.0 * 8.84482479095459
Epoch 540, val loss: 1.056807279586792
Epoch 550, training loss: 89.5071792602539 = 1.0541846752166748 + 10.0 * 8.84529972076416
Epoch 550, val loss: 1.055777907371521
Epoch 560, training loss: 89.53972625732422 = 1.0530744791030884 + 10.0 * 8.848665237426758
Epoch 560, val loss: 1.0547157526016235
Epoch 570, training loss: 89.59645080566406 = 1.0519413948059082 + 10.0 * 8.854451179504395
Epoch 570, val loss: 1.0536386966705322
Epoch 580, training loss: 89.62080383300781 = 1.0508391857147217 + 10.0 * 8.856996536254883
Epoch 580, val loss: 1.0525928735733032
Epoch 590, training loss: 89.58071899414062 = 1.0496454238891602 + 10.0 * 8.853107452392578
Epoch 590, val loss: 1.0514848232269287
Epoch 600, training loss: 89.62763214111328 = 1.0484896898269653 + 10.0 * 8.857913970947266
Epoch 600, val loss: 1.050366759300232
Epoch 610, training loss: 89.6494369506836 = 1.0472842454910278 + 10.0 * 8.860215187072754
Epoch 610, val loss: 1.0492140054702759
Epoch 620, training loss: 89.60294342041016 = 1.0460467338562012 + 10.0 * 8.855690002441406
Epoch 620, val loss: 1.0480563640594482
Epoch 630, training loss: 89.64427947998047 = 1.0448135137557983 + 10.0 * 8.859946250915527
Epoch 630, val loss: 1.0468887090682983
Epoch 640, training loss: 89.67774963378906 = 1.043529748916626 + 10.0 * 8.863421440124512
Epoch 640, val loss: 1.0456711053848267
Epoch 650, training loss: 89.66057586669922 = 1.042231559753418 + 10.0 * 8.861834526062012
Epoch 650, val loss: 1.0444486141204834
Epoch 660, training loss: 89.7271957397461 = 1.0409066677093506 + 10.0 * 8.86862850189209
Epoch 660, val loss: 1.0431870222091675
Epoch 670, training loss: 89.73086547851562 = 1.0395334959030151 + 10.0 * 8.869132995605469
Epoch 670, val loss: 1.0418564081192017
Epoch 680, training loss: 89.74251556396484 = 1.0381003618240356 + 10.0 * 8.870441436767578
Epoch 680, val loss: 1.0405434370040894
Epoch 690, training loss: 89.78067779541016 = 1.0366170406341553 + 10.0 * 8.874406814575195
Epoch 690, val loss: 1.0391204357147217
Epoch 700, training loss: 89.81672668457031 = 1.0350993871688843 + 10.0 * 8.878162384033203
Epoch 700, val loss: 1.0377025604248047
Epoch 710, training loss: 89.82027435302734 = 1.0335325002670288 + 10.0 * 8.878674507141113
Epoch 710, val loss: 1.0362303256988525
Epoch 720, training loss: 89.79451751708984 = 1.0318927764892578 + 10.0 * 8.876262664794922
Epoch 720, val loss: 1.0346397161483765
Epoch 730, training loss: 89.86234283447266 = 1.030264973640442 + 10.0 * 8.883207321166992
Epoch 730, val loss: 1.033132553100586
Epoch 740, training loss: 89.94499206542969 = 1.0286293029785156 + 10.0 * 8.89163589477539
Epoch 740, val loss: 1.0315876007080078
Epoch 750, training loss: 89.86846160888672 = 1.0269272327423096 + 10.0 * 8.884153366088867
Epoch 750, val loss: 1.029969334602356
Epoch 760, training loss: 89.88002014160156 = 1.025202989578247 + 10.0 * 8.885481834411621
Epoch 760, val loss: 1.0283410549163818
Epoch 770, training loss: 89.90956115722656 = 1.0234413146972656 + 10.0 * 8.888611793518066
Epoch 770, val loss: 1.026678204536438
Epoch 780, training loss: 89.94117736816406 = 1.021683692932129 + 10.0 * 8.891949653625488
Epoch 780, val loss: 1.025008201599121
Epoch 790, training loss: 89.9793701171875 = 1.0198770761489868 + 10.0 * 8.895949363708496
Epoch 790, val loss: 1.0232800245285034
Epoch 800, training loss: 89.96381378173828 = 1.0180108547210693 + 10.0 * 8.894579887390137
Epoch 800, val loss: 1.0215002298355103
Epoch 810, training loss: 89.96662902832031 = 1.0161257982254028 + 10.0 * 8.895050048828125
Epoch 810, val loss: 1.0197312831878662
Epoch 820, training loss: 90.0156021118164 = 1.0142298936843872 + 10.0 * 8.900136947631836
Epoch 820, val loss: 1.0179258584976196
Epoch 830, training loss: 90.06089782714844 = 1.0123162269592285 + 10.0 * 8.904858589172363
Epoch 830, val loss: 1.0161117315292358
Epoch 840, training loss: 90.03257751464844 = 1.010329246520996 + 10.0 * 8.90222454071045
Epoch 840, val loss: 1.0142366886138916
Epoch 850, training loss: 90.10799407958984 = 1.008350133895874 + 10.0 * 8.909964561462402
Epoch 850, val loss: 1.0123337507247925
Epoch 860, training loss: 90.11798858642578 = 1.0063066482543945 + 10.0 * 8.911168098449707
Epoch 860, val loss: 1.0103967189788818
Epoch 870, training loss: 90.1474838256836 = 1.0042611360549927 + 10.0 * 8.914321899414062
Epoch 870, val loss: 1.008453607559204
Epoch 880, training loss: 90.15166473388672 = 1.0021617412567139 + 10.0 * 8.914950370788574
Epoch 880, val loss: 1.00645112991333
Epoch 890, training loss: 90.18102264404297 = 1.0000202655792236 + 10.0 * 8.918100357055664
Epoch 890, val loss: 1.0044223070144653
Epoch 900, training loss: 90.16392517089844 = 0.9978141784667969 + 10.0 * 8.916610717773438
Epoch 900, val loss: 1.002301812171936
Epoch 910, training loss: 90.15658569335938 = 0.9955634474754333 + 10.0 * 8.916102409362793
Epoch 910, val loss: 1.000179409980774
Epoch 920, training loss: 90.15428924560547 = 0.9932565689086914 + 10.0 * 8.91610336303711
Epoch 920, val loss: 0.9980085492134094
Epoch 930, training loss: 90.079833984375 = 0.9909951090812683 + 10.0 * 8.908884048461914
Epoch 930, val loss: 0.9958266615867615
Epoch 940, training loss: 90.15208435058594 = 0.9886486530303955 + 10.0 * 8.916343688964844
Epoch 940, val loss: 0.9936015605926514
Epoch 950, training loss: 90.23159790039062 = 0.9863067865371704 + 10.0 * 8.924529075622559
Epoch 950, val loss: 0.9914038181304932
Epoch 960, training loss: 90.29946899414062 = 0.9839493632316589 + 10.0 * 8.931551933288574
Epoch 960, val loss: 0.9891652464866638
Epoch 970, training loss: 90.22483825683594 = 0.981479823589325 + 10.0 * 8.924335479736328
Epoch 970, val loss: 0.9868117570877075
Epoch 980, training loss: 90.21366119384766 = 0.9790756702423096 + 10.0 * 8.92345905303955
Epoch 980, val loss: 0.9845457077026367
Epoch 990, training loss: 90.2732162475586 = 0.9766628742218018 + 10.0 * 8.929655075073242
Epoch 990, val loss: 0.9822520613670349
Epoch 1000, training loss: 90.38047790527344 = 0.9742029309272766 + 10.0 * 8.940627098083496
Epoch 1000, val loss: 0.979923665523529
Epoch 1010, training loss: 90.3237533569336 = 0.9716747999191284 + 10.0 * 8.935208320617676
Epoch 1010, val loss: 0.9775048494338989
Epoch 1020, training loss: 90.35781860351562 = 0.9691672325134277 + 10.0 * 8.938864707946777
Epoch 1020, val loss: 0.9751220941543579
Epoch 1030, training loss: 90.42816925048828 = 0.9666341543197632 + 10.0 * 8.94615364074707
Epoch 1030, val loss: 0.9727186560630798
Epoch 1040, training loss: 90.46070098876953 = 0.9640737175941467 + 10.0 * 8.949663162231445
Epoch 1040, val loss: 0.970259964466095
Epoch 1050, training loss: 90.47415924072266 = 0.9615115523338318 + 10.0 * 8.951265335083008
Epoch 1050, val loss: 0.9678159356117249
Epoch 1060, training loss: 90.45630645751953 = 0.9589213132858276 + 10.0 * 8.949738502502441
Epoch 1060, val loss: 0.9653584361076355
Epoch 1070, training loss: 90.51036071777344 = 0.9563450217247009 + 10.0 * 8.955401420593262
Epoch 1070, val loss: 0.9628942608833313
Epoch 1080, training loss: 90.51909637451172 = 0.95368492603302 + 10.0 * 8.956541061401367
Epoch 1080, val loss: 0.9603812098503113
Epoch 1090, training loss: 90.5022964477539 = 0.9510281085968018 + 10.0 * 8.955126762390137
Epoch 1090, val loss: 0.9578492641448975
Epoch 1100, training loss: 90.56302642822266 = 0.9483179450035095 + 10.0 * 8.961470603942871
Epoch 1100, val loss: 0.9552717804908752
Epoch 1110, training loss: 90.56631469726562 = 0.9455046653747559 + 10.0 * 8.962080955505371
Epoch 1110, val loss: 0.9525702595710754
Epoch 1120, training loss: 90.56842041015625 = 0.9426019191741943 + 10.0 * 8.962581634521484
Epoch 1120, val loss: 0.9498084783554077
Epoch 1130, training loss: 90.55382537841797 = 0.9396879076957703 + 10.0 * 8.961413383483887
Epoch 1130, val loss: 0.9470488429069519
Epoch 1140, training loss: 90.59152221679688 = 0.9368273615837097 + 10.0 * 8.965469360351562
Epoch 1140, val loss: 0.9443292617797852
Epoch 1150, training loss: 90.5318603515625 = 0.9339147210121155 + 10.0 * 8.959794998168945
Epoch 1150, val loss: 0.9415958523750305
Epoch 1160, training loss: 90.51516723632812 = 0.9310082197189331 + 10.0 * 8.958415985107422
Epoch 1160, val loss: 0.9388587474822998
Epoch 1170, training loss: 90.37229919433594 = 0.9280411601066589 + 10.0 * 8.944425582885742
Epoch 1170, val loss: 0.9360530376434326
Epoch 1180, training loss: 90.5167007446289 = 0.9252623915672302 + 10.0 * 8.95914363861084
Epoch 1180, val loss: 0.9333651065826416
Epoch 1190, training loss: 90.59549713134766 = 0.922399640083313 + 10.0 * 8.967309951782227
Epoch 1190, val loss: 0.93068927526474
Epoch 1200, training loss: 90.65948486328125 = 0.919479250907898 + 10.0 * 8.974000930786133
Epoch 1200, val loss: 0.9279548525810242
Epoch 1210, training loss: 90.64805603027344 = 0.9165470004081726 + 10.0 * 8.973150253295898
Epoch 1210, val loss: 0.9251728057861328
Epoch 1220, training loss: 90.68577575683594 = 0.9136334657669067 + 10.0 * 8.977213859558105
Epoch 1220, val loss: 0.9224164485931396
Epoch 1230, training loss: 90.63275146484375 = 0.9107690453529358 + 10.0 * 8.972198486328125
Epoch 1230, val loss: 0.9196749925613403
Epoch 1240, training loss: 90.67330169677734 = 0.9078720808029175 + 10.0 * 8.976542472839355
Epoch 1240, val loss: 0.9169504046440125
Epoch 1250, training loss: 90.72965240478516 = 0.9049757719039917 + 10.0 * 8.982467651367188
Epoch 1250, val loss: 0.9142225980758667
Epoch 1260, training loss: 90.7310562133789 = 0.9020432233810425 + 10.0 * 8.982900619506836
Epoch 1260, val loss: 0.9114393591880798
Epoch 1270, training loss: 90.70214080810547 = 0.8991320133209229 + 10.0 * 8.980300903320312
Epoch 1270, val loss: 0.9086835980415344
Epoch 1280, training loss: 90.73755645751953 = 0.8962405920028687 + 10.0 * 8.984131813049316
Epoch 1280, val loss: 0.9059906601905823
Epoch 1290, training loss: 90.78014373779297 = 0.8933634161949158 + 10.0 * 8.988677978515625
Epoch 1290, val loss: 0.9032776951789856
Epoch 1300, training loss: 90.76359558105469 = 0.8904895186424255 + 10.0 * 8.987310409545898
Epoch 1300, val loss: 0.9005513191223145
Epoch 1310, training loss: 90.80764770507812 = 0.8875288367271423 + 10.0 * 8.992012023925781
Epoch 1310, val loss: 0.8977491855621338
Epoch 1320, training loss: 90.83162689208984 = 0.8844817280769348 + 10.0 * 8.994714736938477
Epoch 1320, val loss: 0.8948892951011658
Epoch 1330, training loss: 90.78394317626953 = 0.8814177513122559 + 10.0 * 8.990252494812012
Epoch 1330, val loss: 0.8920341730117798
Epoch 1340, training loss: 90.82413482666016 = 0.8783800601959229 + 10.0 * 8.994575500488281
Epoch 1340, val loss: 0.8891677856445312
Epoch 1350, training loss: 90.86813354492188 = 0.8753446936607361 + 10.0 * 8.999279022216797
Epoch 1350, val loss: 0.8863356709480286
Epoch 1360, training loss: 90.8058090209961 = 0.8723005652427673 + 10.0 * 8.993350982666016
Epoch 1360, val loss: 0.8834683299064636
Epoch 1370, training loss: 90.85047149658203 = 0.8692681193351746 + 10.0 * 8.998120307922363
Epoch 1370, val loss: 0.8806620836257935
Epoch 1380, training loss: 90.90159606933594 = 0.8662539124488831 + 10.0 * 9.003534317016602
Epoch 1380, val loss: 0.8778377175331116
Epoch 1390, training loss: 90.91458892822266 = 0.8632280230522156 + 10.0 * 9.005136489868164
Epoch 1390, val loss: 0.8750307559967041
Epoch 1400, training loss: 90.94978332519531 = 0.8602637052536011 + 10.0 * 9.008952140808105
Epoch 1400, val loss: 0.8722556233406067
Epoch 1410, training loss: 90.95213317871094 = 0.8572854995727539 + 10.0 * 9.009485244750977
Epoch 1410, val loss: 0.8695082068443298
Epoch 1420, training loss: 90.97514343261719 = 0.8543674945831299 + 10.0 * 9.012077331542969
Epoch 1420, val loss: 0.866793155670166
Epoch 1430, training loss: 91.00996398925781 = 0.8514235615730286 + 10.0 * 9.015853881835938
Epoch 1430, val loss: 0.8640753626823425
Epoch 1440, training loss: 90.99665832519531 = 0.848554253578186 + 10.0 * 9.014810562133789
Epoch 1440, val loss: 0.8614480495452881
Epoch 1450, training loss: 90.99505615234375 = 0.8457221984863281 + 10.0 * 9.014933586120605
Epoch 1450, val loss: 0.8588602542877197
Epoch 1460, training loss: 91.0057373046875 = 0.8428663611412048 + 10.0 * 9.016286849975586
Epoch 1460, val loss: 0.8562203049659729
Epoch 1470, training loss: 91.00878143310547 = 0.8400467038154602 + 10.0 * 9.016873359680176
Epoch 1470, val loss: 0.8536163568496704
Epoch 1480, training loss: 91.0165786743164 = 0.8372527360916138 + 10.0 * 9.017932891845703
Epoch 1480, val loss: 0.8510458469390869
Epoch 1490, training loss: 91.03985595703125 = 0.8344811797142029 + 10.0 * 9.020537376403809
Epoch 1490, val loss: 0.8485062718391418
Epoch 1500, training loss: 91.03894805908203 = 0.8317214250564575 + 10.0 * 9.020723342895508
Epoch 1500, val loss: 0.8460041880607605
Epoch 1510, training loss: 91.09193420410156 = 0.8290074467658997 + 10.0 * 9.02629280090332
Epoch 1510, val loss: 0.8435199856758118
Epoch 1520, training loss: 90.93157958984375 = 0.8263425827026367 + 10.0 * 9.010523796081543
Epoch 1520, val loss: 0.8410894274711609
Epoch 1530, training loss: 90.95606231689453 = 0.823692262172699 + 10.0 * 9.013236999511719
Epoch 1530, val loss: 0.8386756777763367
Epoch 1540, training loss: 90.99005126953125 = 0.8210791349411011 + 10.0 * 9.016897201538086
Epoch 1540, val loss: 0.8362829685211182
Epoch 1550, training loss: 91.08917236328125 = 0.8184676766395569 + 10.0 * 9.027070999145508
Epoch 1550, val loss: 0.8339263200759888
Epoch 1560, training loss: 91.12834930419922 = 0.8158814311027527 + 10.0 * 9.03124713897705
Epoch 1560, val loss: 0.8315721750259399
Epoch 1570, training loss: 91.13384246826172 = 0.8132923245429993 + 10.0 * 9.032054901123047
Epoch 1570, val loss: 0.8292282819747925
Epoch 1580, training loss: 91.14716339111328 = 0.8107516765594482 + 10.0 * 9.03364086151123
Epoch 1580, val loss: 0.8269306421279907
Epoch 1590, training loss: 91.12393188476562 = 0.8082285523414612 + 10.0 * 9.031570434570312
Epoch 1590, val loss: 0.8246778249740601
Epoch 1600, training loss: 91.1642074584961 = 0.8057687878608704 + 10.0 * 9.035843849182129
Epoch 1600, val loss: 0.8224509358406067
Epoch 1610, training loss: 91.18863677978516 = 0.8033400177955627 + 10.0 * 9.038530349731445
Epoch 1610, val loss: 0.8202522397041321
Epoch 1620, training loss: 91.17635345458984 = 0.8009226322174072 + 10.0 * 9.037542343139648
Epoch 1620, val loss: 0.8180928230285645
Epoch 1630, training loss: 91.15770721435547 = 0.7985449433326721 + 10.0 * 9.035916328430176
Epoch 1630, val loss: 0.8159840106964111
Epoch 1640, training loss: 91.18709564208984 = 0.7962014079093933 + 10.0 * 9.03908920288086
Epoch 1640, val loss: 0.8138945698738098
Epoch 1650, training loss: 91.20824432373047 = 0.7938941717147827 + 10.0 * 9.041435241699219
Epoch 1650, val loss: 0.8118627667427063
Epoch 1660, training loss: 91.21451568603516 = 0.7916354537010193 + 10.0 * 9.042287826538086
Epoch 1660, val loss: 0.8098329305648804
Epoch 1670, training loss: 91.17340087890625 = 0.7894054651260376 + 10.0 * 9.038399696350098
Epoch 1670, val loss: 0.8078442811965942
Epoch 1680, training loss: 91.20417785644531 = 0.7872745990753174 + 10.0 * 9.0416898727417
Epoch 1680, val loss: 0.8059647083282471
Epoch 1690, training loss: 91.26087951660156 = 0.7851462364196777 + 10.0 * 9.04757308959961
Epoch 1690, val loss: 0.8040874004364014
Epoch 1700, training loss: 91.23744201660156 = 0.7830303907394409 + 10.0 * 9.045441627502441
Epoch 1700, val loss: 0.8022074699401855
Epoch 1710, training loss: 91.22223663330078 = 0.7809253931045532 + 10.0 * 9.0441312789917
Epoch 1710, val loss: 0.8003783226013184
Epoch 1720, training loss: 91.22538757324219 = 0.7788751125335693 + 10.0 * 9.04465103149414
Epoch 1720, val loss: 0.7985566258430481
Epoch 1730, training loss: 91.25820922851562 = 0.7768616676330566 + 10.0 * 9.048134803771973
Epoch 1730, val loss: 0.7967635989189148
Epoch 1740, training loss: 91.28547668457031 = 0.7748748064041138 + 10.0 * 9.051060676574707
Epoch 1740, val loss: 0.7950371503829956
Epoch 1750, training loss: 91.31404876708984 = 0.7728918790817261 + 10.0 * 9.054116249084473
Epoch 1750, val loss: 0.7933351993560791
Epoch 1760, training loss: 91.29998016357422 = 0.7709546089172363 + 10.0 * 9.052902221679688
Epoch 1760, val loss: 0.79169762134552
Epoch 1770, training loss: 91.31546020507812 = 0.7690446972846985 + 10.0 * 9.054641723632812
Epoch 1770, val loss: 0.790025532245636
Epoch 1780, training loss: 91.35400390625 = 0.7671681046485901 + 10.0 * 9.058683395385742
Epoch 1780, val loss: 0.7884097695350647
Epoch 1790, training loss: 91.35570526123047 = 0.7653132081031799 + 10.0 * 9.059039115905762
Epoch 1790, val loss: 0.7868276238441467
Epoch 1800, training loss: 91.32838439941406 = 0.7634851336479187 + 10.0 * 9.056489944458008
Epoch 1800, val loss: 0.7852644920349121
Epoch 1810, training loss: 91.26691436767578 = 0.7617055773735046 + 10.0 * 9.050520896911621
Epoch 1810, val loss: 0.7837656736373901
Epoch 1820, training loss: 91.3292465209961 = 0.759914219379425 + 10.0 * 9.056933403015137
Epoch 1820, val loss: 0.7822396755218506
Epoch 1830, training loss: 91.38530731201172 = 0.7581748366355896 + 10.0 * 9.062713623046875
Epoch 1830, val loss: 0.7807586193084717
Epoch 1840, training loss: 91.42542266845703 = 0.7564403414726257 + 10.0 * 9.066898345947266
Epoch 1840, val loss: 0.7793108820915222
Epoch 1850, training loss: 91.31470489501953 = 0.7547838687896729 + 10.0 * 9.055992126464844
Epoch 1850, val loss: 0.7779248356819153
Epoch 1860, training loss: 91.2406997680664 = 0.7531182169914246 + 10.0 * 9.048757553100586
Epoch 1860, val loss: 0.7765232920646667
Epoch 1870, training loss: 91.26138305664062 = 0.7514800429344177 + 10.0 * 9.050990104675293
Epoch 1870, val loss: 0.775185227394104
Epoch 1880, training loss: 91.2212142944336 = 0.7498563528060913 + 10.0 * 9.047136306762695
Epoch 1880, val loss: 0.7737861275672913
Epoch 1890, training loss: 91.27511596679688 = 0.7482894659042358 + 10.0 * 9.052682876586914
Epoch 1890, val loss: 0.7724722027778625
Epoch 1900, training loss: 91.2954330444336 = 0.7467158436775208 + 10.0 * 9.054871559143066
Epoch 1900, val loss: 0.7711057662963867
Epoch 1910, training loss: 91.36349487304688 = 0.7451727390289307 + 10.0 * 9.061832427978516
Epoch 1910, val loss: 0.7698216438293457
Epoch 1920, training loss: 91.40906524658203 = 0.7436280846595764 + 10.0 * 9.066543579101562
Epoch 1920, val loss: 0.7685542702674866
Epoch 1930, training loss: 91.4311752319336 = 0.7420978546142578 + 10.0 * 9.068907737731934
Epoch 1930, val loss: 0.7672920227050781
Epoch 1940, training loss: 91.40332794189453 = 0.7405932545661926 + 10.0 * 9.06627368927002
Epoch 1940, val loss: 0.766061007976532
Epoch 1950, training loss: 91.46453094482422 = 0.7391319870948792 + 10.0 * 9.072539329528809
Epoch 1950, val loss: 0.7648309469223022
Epoch 1960, training loss: 91.39615631103516 = 0.737655758857727 + 10.0 * 9.065850257873535
Epoch 1960, val loss: 0.7636793851852417
Epoch 1970, training loss: 91.40380859375 = 0.7362505793571472 + 10.0 * 9.066755294799805
Epoch 1970, val loss: 0.7625244855880737
Epoch 1980, training loss: 91.43974304199219 = 0.734849214553833 + 10.0 * 9.070489883422852
Epoch 1980, val loss: 0.7613908052444458
Epoch 1990, training loss: 91.48573303222656 = 0.7335126399993896 + 10.0 * 9.07522201538086
Epoch 1990, val loss: 0.7602892518043518
Epoch 2000, training loss: 91.44302368164062 = 0.7321291565895081 + 10.0 * 9.071088790893555
Epoch 2000, val loss: 0.7591525316238403
Epoch 2010, training loss: 91.44670867919922 = 0.7307698130607605 + 10.0 * 9.07159423828125
Epoch 2010, val loss: 0.7580482959747314
Epoch 2020, training loss: 91.50397491455078 = 0.7294234037399292 + 10.0 * 9.077455520629883
Epoch 2020, val loss: 0.7569666504859924
Epoch 2030, training loss: 91.5691146850586 = 0.7281094789505005 + 10.0 * 9.084100723266602
Epoch 2030, val loss: 0.7559075355529785
Epoch 2040, training loss: 91.43578338623047 = 0.7267979383468628 + 10.0 * 9.070898056030273
Epoch 2040, val loss: 0.7548521757125854
Epoch 2050, training loss: 91.33880615234375 = 0.7255488038063049 + 10.0 * 9.061326026916504
Epoch 2050, val loss: 0.7539240717887878
Epoch 2060, training loss: 91.34968566894531 = 0.7243313789367676 + 10.0 * 9.062535285949707
Epoch 2060, val loss: 0.7529517412185669
Epoch 2070, training loss: 91.39956665039062 = 0.723105788230896 + 10.0 * 9.067646026611328
Epoch 2070, val loss: 0.7519763112068176
Epoch 2080, training loss: 91.4695053100586 = 0.7218840718269348 + 10.0 * 9.074762344360352
Epoch 2080, val loss: 0.7510318160057068
Epoch 2090, training loss: 91.53154754638672 = 0.7206649780273438 + 10.0 * 9.081088066101074
Epoch 2090, val loss: 0.7500740885734558
Epoch 2100, training loss: 91.56696319580078 = 0.7194517254829407 + 10.0 * 9.08475112915039
Epoch 2100, val loss: 0.7491084337234497
Epoch 2110, training loss: 91.56838989257812 = 0.7182447910308838 + 10.0 * 9.085014343261719
Epoch 2110, val loss: 0.7481763958930969
Epoch 2120, training loss: 91.54766082763672 = 0.7170535922050476 + 10.0 * 9.083060264587402
Epoch 2120, val loss: 0.7472854852676392
Epoch 2130, training loss: 91.5813217163086 = 0.7158898711204529 + 10.0 * 9.086543083190918
Epoch 2130, val loss: 0.7463868260383606
Epoch 2140, training loss: 91.57205963134766 = 0.7147331833839417 + 10.0 * 9.085732460021973
Epoch 2140, val loss: 0.7455207705497742
Epoch 2150, training loss: 91.56266784667969 = 0.7135924696922302 + 10.0 * 9.084907531738281
Epoch 2150, val loss: 0.7446671724319458
Epoch 2160, training loss: 91.61212921142578 = 0.7124596834182739 + 10.0 * 9.089966773986816
Epoch 2160, val loss: 0.7438125014305115
Epoch 2170, training loss: 91.60839080810547 = 0.7113326191902161 + 10.0 * 9.089705467224121
Epoch 2170, val loss: 0.7429589033126831
Epoch 2180, training loss: 91.54411315917969 = 0.7102699279785156 + 10.0 * 9.08338451385498
Epoch 2180, val loss: 0.742191731929779
Epoch 2190, training loss: 91.50074005126953 = 0.7091794013977051 + 10.0 * 9.079155921936035
Epoch 2190, val loss: 0.7413367629051208
Epoch 2200, training loss: 91.58494567871094 = 0.7081092000007629 + 10.0 * 9.08768367767334
Epoch 2200, val loss: 0.7405132055282593
Epoch 2210, training loss: 91.6366958618164 = 0.7070173025131226 + 10.0 * 9.092967987060547
Epoch 2210, val loss: 0.7396857738494873
Epoch 2220, training loss: 91.66053771972656 = 0.7059427499771118 + 10.0 * 9.095459938049316
Epoch 2220, val loss: 0.7388867139816284
Epoch 2230, training loss: 91.64576721191406 = 0.7048735618591309 + 10.0 * 9.09408950805664
Epoch 2230, val loss: 0.7380828261375427
Epoch 2240, training loss: 91.64510345458984 = 0.7038249373435974 + 10.0 * 9.094127655029297
Epoch 2240, val loss: 0.7373088002204895
Epoch 2250, training loss: 91.6879653930664 = 0.7027746438980103 + 10.0 * 9.098519325256348
Epoch 2250, val loss: 0.7365463972091675
Epoch 2260, training loss: 91.65514373779297 = 0.7017338871955872 + 10.0 * 9.095340728759766
Epoch 2260, val loss: 0.7357637882232666
Epoch 2270, training loss: 91.6905517578125 = 0.7007047533988953 + 10.0 * 9.098984718322754
Epoch 2270, val loss: 0.7350286245346069
Epoch 2280, training loss: 91.702880859375 = 0.6996901631355286 + 10.0 * 9.100318908691406
Epoch 2280, val loss: 0.7342793941497803
Epoch 2290, training loss: 91.74838256835938 = 0.698666512966156 + 10.0 * 9.104970932006836
Epoch 2290, val loss: 0.7335140705108643
Epoch 2300, training loss: 91.71873474121094 = 0.6976590752601624 + 10.0 * 9.102107048034668
Epoch 2300, val loss: 0.732789158821106
Epoch 2310, training loss: 91.73612213134766 = 0.6966625452041626 + 10.0 * 9.1039457321167
Epoch 2310, val loss: 0.7320511341094971
Epoch 2320, training loss: 91.65693664550781 = 0.6956747174263 + 10.0 * 9.096126556396484
Epoch 2320, val loss: 0.7313706278800964
Epoch 2330, training loss: 91.6797866821289 = 0.6947489976882935 + 10.0 * 9.098504066467285
Epoch 2330, val loss: 0.7306681275367737
Epoch 2340, training loss: 91.69629669189453 = 0.693787693977356 + 10.0 * 9.100251197814941
Epoch 2340, val loss: 0.7300026416778564
Epoch 2350, training loss: 91.75419616699219 = 0.6928325295448303 + 10.0 * 9.106136322021484
Epoch 2350, val loss: 0.7293148040771484
Epoch 2360, training loss: 91.80563354492188 = 0.691885232925415 + 10.0 * 9.111374855041504
Epoch 2360, val loss: 0.7286475896835327
Epoch 2370, training loss: 91.7915267944336 = 0.6909407377243042 + 10.0 * 9.110058784484863
Epoch 2370, val loss: 0.7279969453811646
Epoch 2380, training loss: 91.75757598876953 = 0.6900104284286499 + 10.0 * 9.106756210327148
Epoch 2380, val loss: 0.7273406386375427
Epoch 2390, training loss: 91.79047393798828 = 0.6890962719917297 + 10.0 * 9.110137939453125
Epoch 2390, val loss: 0.7267116904258728
Epoch 2400, training loss: 91.81196594238281 = 0.688188910484314 + 10.0 * 9.112378120422363
Epoch 2400, val loss: 0.7260899543762207
Epoch 2410, training loss: 91.78826141357422 = 0.687282145023346 + 10.0 * 9.110097885131836
Epoch 2410, val loss: 0.7254616618156433
Epoch 2420, training loss: 91.76311492919922 = 0.686394214630127 + 10.0 * 9.107671737670898
Epoch 2420, val loss: 0.724789559841156
Epoch 2430, training loss: 91.75057983398438 = 0.6855349540710449 + 10.0 * 9.106504440307617
Epoch 2430, val loss: 0.7242839932441711
Epoch 2440, training loss: 91.78762817382812 = 0.6846747994422913 + 10.0 * 9.110295295715332
Epoch 2440, val loss: 0.7236732840538025
Epoch 2450, training loss: 91.83112335205078 = 0.6838040947914124 + 10.0 * 9.114731788635254
Epoch 2450, val loss: 0.7230543494224548
Epoch 2460, training loss: 91.8731918334961 = 0.6829401254653931 + 10.0 * 9.119025230407715
Epoch 2460, val loss: 0.7224823832511902
Epoch 2470, training loss: 91.85528564453125 = 0.6820833086967468 + 10.0 * 9.11732006072998
Epoch 2470, val loss: 0.7218808531761169
Epoch 2480, training loss: 91.84038543701172 = 0.6812364459037781 + 10.0 * 9.115915298461914
Epoch 2480, val loss: 0.7212832570075989
Epoch 2490, training loss: 91.87782287597656 = 0.6803963780403137 + 10.0 * 9.119742393493652
Epoch 2490, val loss: 0.7207252979278564
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.6565217391304348
0.8158371368543071
The final CL Acc:0.67444, 0.03105, The final GNN Acc:0.81533, 0.00037
raw graph: torch.Size([2, 88648])
add edge: torch.Size([2, 110822])
remove edge: torch.Size([2, 66920])
updated graph: torch.Size([2, 89094])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 103.57946014404297 = 1.0913914442062378 + 10.0 * 10.248806953430176
Epoch 0, val loss: 1.0923569202423096
Epoch 10, training loss: 99.3856201171875 = 1.0911870002746582 + 10.0 * 9.829442977905273
Epoch 10, val loss: 1.0921413898468018
Epoch 20, training loss: 97.20314025878906 = 1.0908061265945435 + 10.0 * 9.611233711242676
Epoch 20, val loss: 1.0917717218399048
Epoch 30, training loss: 95.67925262451172 = 1.0904463529586792 + 10.0 * 9.458880424499512
Epoch 30, val loss: 1.0914183855056763
Epoch 40, training loss: 94.50186157226562 = 1.0901002883911133 + 10.0 * 9.34117603302002
Epoch 40, val loss: 1.0910762548446655
Epoch 50, training loss: 93.51991271972656 = 1.0897523164749146 + 10.0 * 9.243016242980957
Epoch 50, val loss: 1.0907343626022339
Epoch 60, training loss: 92.69844055175781 = 1.0894050598144531 + 10.0 * 9.160903930664062
Epoch 60, val loss: 1.0903924703598022
Epoch 70, training loss: 92.00947570800781 = 1.0890527963638306 + 10.0 * 9.092042922973633
Epoch 70, val loss: 1.0900485515594482
Epoch 80, training loss: 91.43262481689453 = 1.088693618774414 + 10.0 * 9.034393310546875
Epoch 80, val loss: 1.0896977186203003
Epoch 90, training loss: 90.94719696044922 = 1.0883220434188843 + 10.0 * 8.98588752746582
Epoch 90, val loss: 1.0893323421478271
Epoch 100, training loss: 90.5507583618164 = 1.0879426002502441 + 10.0 * 8.946281433105469
Epoch 100, val loss: 1.0889614820480347
Epoch 110, training loss: 90.20463562011719 = 1.0875495672225952 + 10.0 * 8.91170883178711
Epoch 110, val loss: 1.0885732173919678
Epoch 120, training loss: 89.91867065429688 = 1.0871435403823853 + 10.0 * 8.883152961730957
Epoch 120, val loss: 1.088176965713501
Epoch 130, training loss: 89.67805480957031 = 1.0867100954055786 + 10.0 * 8.859134674072266
Epoch 130, val loss: 1.0877532958984375
Epoch 140, training loss: 89.45680236816406 = 1.0862575769424438 + 10.0 * 8.837054252624512
Epoch 140, val loss: 1.0873064994812012
Epoch 150, training loss: 89.2696762084961 = 1.0858142375946045 + 10.0 * 8.81838607788086
Epoch 150, val loss: 1.086869239807129
Epoch 160, training loss: 89.11231231689453 = 1.0853407382965088 + 10.0 * 8.80269718170166
Epoch 160, val loss: 1.0864067077636719
Epoch 170, training loss: 88.96882629394531 = 1.0848534107208252 + 10.0 * 8.788396835327148
Epoch 170, val loss: 1.0859308242797852
Epoch 180, training loss: 88.8882064819336 = 1.0843764543533325 + 10.0 * 8.780383110046387
Epoch 180, val loss: 1.085457682609558
Epoch 190, training loss: 88.77839660644531 = 1.0838643312454224 + 10.0 * 8.769453048706055
Epoch 190, val loss: 1.0849655866622925
Epoch 200, training loss: 88.68096160888672 = 1.083344578742981 + 10.0 * 8.759761810302734
Epoch 200, val loss: 1.0844541788101196
Epoch 210, training loss: 88.60531616210938 = 1.0828248262405396 + 10.0 * 8.752248764038086
Epoch 210, val loss: 1.0839455127716064
Epoch 220, training loss: 88.49446868896484 = 1.0822476148605347 + 10.0 * 8.741222381591797
Epoch 220, val loss: 1.0833823680877686
Epoch 230, training loss: 88.44896697998047 = 1.0816856622695923 + 10.0 * 8.736727714538574
Epoch 230, val loss: 1.0828300714492798
Epoch 240, training loss: 88.39736938476562 = 1.0811030864715576 + 10.0 * 8.731626510620117
Epoch 240, val loss: 1.0822676420211792
Epoch 250, training loss: 88.34147644042969 = 1.0804972648620605 + 10.0 * 8.72609806060791
Epoch 250, val loss: 1.081682562828064
Epoch 260, training loss: 88.32454681396484 = 1.079875111579895 + 10.0 * 8.724467277526855
Epoch 260, val loss: 1.0810703039169312
Epoch 270, training loss: 88.29397583007812 = 1.079257607460022 + 10.0 * 8.721471786499023
Epoch 270, val loss: 1.0804755687713623
Epoch 280, training loss: 88.2422103881836 = 1.0786082744598389 + 10.0 * 8.716360092163086
Epoch 280, val loss: 1.0798470973968506
Epoch 290, training loss: 88.20387268066406 = 1.0779331922531128 + 10.0 * 8.712594032287598
Epoch 290, val loss: 1.079189658164978
Epoch 300, training loss: 88.1962661743164 = 1.0772483348846436 + 10.0 * 8.711901664733887
Epoch 300, val loss: 1.0785326957702637
Epoch 310, training loss: 88.15589141845703 = 1.0765446424484253 + 10.0 * 8.707934379577637
Epoch 310, val loss: 1.0778464078903198
Epoch 320, training loss: 88.154296875 = 1.0758345127105713 + 10.0 * 8.707845687866211
Epoch 320, val loss: 1.0771633386611938
Epoch 330, training loss: 88.14102172851562 = 1.0750908851623535 + 10.0 * 8.70659351348877
Epoch 330, val loss: 1.0764505863189697
Epoch 340, training loss: 88.13323974609375 = 1.0743393898010254 + 10.0 * 8.705889701843262
Epoch 340, val loss: 1.07572340965271
Epoch 350, training loss: 88.13568115234375 = 1.0735588073730469 + 10.0 * 8.706212043762207
Epoch 350, val loss: 1.074975848197937
Epoch 360, training loss: 88.08606719970703 = 1.0727367401123047 + 10.0 * 8.701333045959473
Epoch 360, val loss: 1.0741676092147827
Epoch 370, training loss: 88.17063903808594 = 1.0719739198684692 + 10.0 * 8.709866523742676
Epoch 370, val loss: 1.0734397172927856
Epoch 380, training loss: 88.12952423095703 = 1.0711263418197632 + 10.0 * 8.705839157104492
Epoch 380, val loss: 1.072663426399231
Epoch 390, training loss: 88.0550308227539 = 1.0702829360961914 + 10.0 * 8.698474884033203
Epoch 390, val loss: 1.07181978225708
Epoch 400, training loss: 88.1011962890625 = 1.0694533586502075 + 10.0 * 8.703174591064453
Epoch 400, val loss: 1.071040391921997
Epoch 410, training loss: 88.18009948730469 = 1.068617820739746 + 10.0 * 8.711148262023926
Epoch 410, val loss: 1.0702404975891113
Epoch 420, training loss: 88.1739501953125 = 1.0677425861358643 + 10.0 * 8.710620880126953
Epoch 420, val loss: 1.0694102048873901
Epoch 430, training loss: 88.17459869384766 = 1.066853404045105 + 10.0 * 8.710774421691895
Epoch 430, val loss: 1.068565011024475
Epoch 440, training loss: 88.20062255859375 = 1.065935730934143 + 10.0 * 8.713468551635742
Epoch 440, val loss: 1.0676789283752441
Epoch 450, training loss: 88.22407531738281 = 1.065039038658142 + 10.0 * 8.715903282165527
Epoch 450, val loss: 1.0668412446975708
Epoch 460, training loss: 88.2519760131836 = 1.064126968383789 + 10.0 * 8.718785285949707
Epoch 460, val loss: 1.0659523010253906
Epoch 470, training loss: 88.25654602050781 = 1.0631599426269531 + 10.0 * 8.719338417053223
Epoch 470, val loss: 1.0650407075881958
Epoch 480, training loss: 88.28483581542969 = 1.0621778964996338 + 10.0 * 8.722265243530273
Epoch 480, val loss: 1.0641082525253296
Epoch 490, training loss: 88.26292419433594 = 1.061185359954834 + 10.0 * 8.720173835754395
Epoch 490, val loss: 1.0631635189056396
Epoch 500, training loss: 88.31621551513672 = 1.0601928234100342 + 10.0 * 8.725602149963379
Epoch 500, val loss: 1.0622260570526123
Epoch 510, training loss: 88.33521270751953 = 1.059141755104065 + 10.0 * 8.727606773376465
Epoch 510, val loss: 1.0612205266952515
Epoch 520, training loss: 88.2994384765625 = 1.058074951171875 + 10.0 * 8.724136352539062
Epoch 520, val loss: 1.0602117776870728
Epoch 530, training loss: 88.32492065429688 = 1.0570110082626343 + 10.0 * 8.726790428161621
Epoch 530, val loss: 1.0592069625854492
Epoch 540, training loss: 88.3459243774414 = 1.0559144020080566 + 10.0 * 8.72900104522705
Epoch 540, val loss: 1.058171033859253
Epoch 550, training loss: 88.36156463623047 = 1.0547807216644287 + 10.0 * 8.73067855834961
Epoch 550, val loss: 1.0571081638336182
Epoch 560, training loss: 88.31412506103516 = 1.0536224842071533 + 10.0 * 8.72605037689209
Epoch 560, val loss: 1.0560160875320435
Epoch 570, training loss: 88.34447479248047 = 1.0524345636367798 + 10.0 * 8.729204177856445
Epoch 570, val loss: 1.0548789501190186
Epoch 580, training loss: 88.40583801269531 = 1.0512466430664062 + 10.0 * 8.735459327697754
Epoch 580, val loss: 1.0537456274032593
Epoch 590, training loss: 88.42304992675781 = 1.0499991178512573 + 10.0 * 8.737305641174316
Epoch 590, val loss: 1.0525414943695068
Epoch 600, training loss: 88.507568359375 = 1.0487322807312012 + 10.0 * 8.74588394165039
Epoch 600, val loss: 1.0513604879379272
Epoch 610, training loss: 88.5534439086914 = 1.0474096536636353 + 10.0 * 8.750603675842285
Epoch 610, val loss: 1.0501022338867188
Epoch 620, training loss: 88.51832580566406 = 1.0460124015808105 + 10.0 * 8.747231483459473
Epoch 620, val loss: 1.0487748384475708
Epoch 630, training loss: 88.54882049560547 = 1.044594645500183 + 10.0 * 8.750422477722168
Epoch 630, val loss: 1.0474445819854736
Epoch 640, training loss: 88.59171295166016 = 1.0431499481201172 + 10.0 * 8.75485610961914
Epoch 640, val loss: 1.0460718870162964
Epoch 650, training loss: 88.61258697509766 = 1.041704773902893 + 10.0 * 8.757088661193848
Epoch 650, val loss: 1.044674277305603
Epoch 660, training loss: 88.67276000976562 = 1.0401958227157593 + 10.0 * 8.763256072998047
Epoch 660, val loss: 1.0432575941085815
Epoch 670, training loss: 88.60778045654297 = 1.038705587387085 + 10.0 * 8.75690746307373
Epoch 670, val loss: 1.0418438911437988
Epoch 680, training loss: 88.66084289550781 = 1.0371840000152588 + 10.0 * 8.762365341186523
Epoch 680, val loss: 1.040413737297058
Epoch 690, training loss: 88.62983703613281 = 1.0356229543685913 + 10.0 * 8.759421348571777
Epoch 690, val loss: 1.0389374494552612
Epoch 700, training loss: 88.61507415771484 = 1.0340054035186768 + 10.0 * 8.75810718536377
Epoch 700, val loss: 1.0374276638031006
Epoch 710, training loss: 88.6419448852539 = 1.0324037075042725 + 10.0 * 8.760953903198242
Epoch 710, val loss: 1.0358822345733643
Epoch 720, training loss: 88.63156127929688 = 1.0307765007019043 + 10.0 * 8.760078430175781
Epoch 720, val loss: 1.0343338251113892
Epoch 730, training loss: 88.71769714355469 = 1.0291112661361694 + 10.0 * 8.768857955932617
Epoch 730, val loss: 1.0327637195587158
Epoch 740, training loss: 88.7711410522461 = 1.027381420135498 + 10.0 * 8.774375915527344
Epoch 740, val loss: 1.0311195850372314
Epoch 750, training loss: 88.80610656738281 = 1.025591492652893 + 10.0 * 8.778051376342773
Epoch 750, val loss: 1.0294238328933716
Epoch 760, training loss: 88.8365249633789 = 1.023759365081787 + 10.0 * 8.78127670288086
Epoch 760, val loss: 1.0277042388916016
Epoch 770, training loss: 88.86750030517578 = 1.021918535232544 + 10.0 * 8.784558296203613
Epoch 770, val loss: 1.0259495973587036
Epoch 780, training loss: 88.8899917602539 = 1.0200053453445435 + 10.0 * 8.786998748779297
Epoch 780, val loss: 1.024134635925293
Epoch 790, training loss: 88.94137573242188 = 1.0180671215057373 + 10.0 * 8.792330741882324
Epoch 790, val loss: 1.0223140716552734
Epoch 800, training loss: 89.02165222167969 = 1.0160698890686035 + 10.0 * 8.800558090209961
Epoch 800, val loss: 1.0204344987869263
Epoch 810, training loss: 88.77947235107422 = 1.0140619277954102 + 10.0 * 8.776540756225586
Epoch 810, val loss: 1.0185279846191406
Epoch 820, training loss: 88.78628540039062 = 1.0121023654937744 + 10.0 * 8.77741813659668
Epoch 820, val loss: 1.0166903734207153
Epoch 830, training loss: 88.86698150634766 = 1.0101956129074097 + 10.0 * 8.78567886352539
Epoch 830, val loss: 1.0148839950561523
Epoch 840, training loss: 88.93360900878906 = 1.008195400238037 + 10.0 * 8.79254150390625
Epoch 840, val loss: 1.0129836797714233
Epoch 850, training loss: 88.99175262451172 = 1.0060983896255493 + 10.0 * 8.798565864562988
Epoch 850, val loss: 1.0110028982162476
Epoch 860, training loss: 89.01180267333984 = 1.0039291381835938 + 10.0 * 8.800786972045898
Epoch 860, val loss: 1.0089483261108398
Epoch 870, training loss: 89.0062484741211 = 1.0017333030700684 + 10.0 * 8.800451278686523
Epoch 870, val loss: 1.0068799257278442
Epoch 880, training loss: 89.08849334716797 = 0.9995102882385254 + 10.0 * 8.808897972106934
Epoch 880, val loss: 1.004779577255249
Epoch 890, training loss: 89.05195617675781 = 0.9971947073936462 + 10.0 * 8.805476188659668
Epoch 890, val loss: 1.0026065111160278
Epoch 900, training loss: 89.05657196044922 = 0.9949386119842529 + 10.0 * 8.80616283416748
Epoch 900, val loss: 1.0004786252975464
Epoch 910, training loss: 88.93656158447266 = 0.9925156235694885 + 10.0 * 8.794404983520508
Epoch 910, val loss: 0.9982098937034607
Epoch 920, training loss: 89.06864929199219 = 0.990247905254364 + 10.0 * 8.807840347290039
Epoch 920, val loss: 0.9960530400276184
Epoch 930, training loss: 89.12945556640625 = 0.9878606796264648 + 10.0 * 8.814159393310547
Epoch 930, val loss: 0.9938095808029175
Epoch 940, training loss: 89.18981170654297 = 0.9854032397270203 + 10.0 * 8.820440292358398
Epoch 940, val loss: 0.9914734363555908
Epoch 950, training loss: 89.20084381103516 = 0.9828802943229675 + 10.0 * 8.821796417236328
Epoch 950, val loss: 0.9890906810760498
Epoch 960, training loss: 89.18000030517578 = 0.9803510308265686 + 10.0 * 8.819964408874512
Epoch 960, val loss: 0.9867203831672668
Epoch 970, training loss: 89.21388244628906 = 0.9777868986129761 + 10.0 * 8.823610305786133
Epoch 970, val loss: 0.9842928647994995
Epoch 980, training loss: 89.29293823242188 = 0.9751835465431213 + 10.0 * 8.831775665283203
Epoch 980, val loss: 0.9818499684333801
Epoch 990, training loss: 89.30149841308594 = 0.9725219011306763 + 10.0 * 8.832898139953613
Epoch 990, val loss: 0.9793208837509155
Epoch 1000, training loss: 89.29704284667969 = 0.9697896242141724 + 10.0 * 8.832725524902344
Epoch 1000, val loss: 0.9767751097679138
Epoch 1010, training loss: 89.32622528076172 = 0.9670772552490234 + 10.0 * 8.835914611816406
Epoch 1010, val loss: 0.9742231369018555
Epoch 1020, training loss: 89.3662109375 = 0.9642731547355652 + 10.0 * 8.840193748474121
Epoch 1020, val loss: 0.9715927839279175
Epoch 1030, training loss: 89.4078140258789 = 0.9614506363868713 + 10.0 * 8.844636917114258
Epoch 1030, val loss: 0.9689425826072693
Epoch 1040, training loss: 89.38102722167969 = 0.9585961699485779 + 10.0 * 8.842243194580078
Epoch 1040, val loss: 0.9662579894065857
Epoch 1050, training loss: 89.45205688476562 = 0.9557276368141174 + 10.0 * 8.84963321685791
Epoch 1050, val loss: 0.9635759592056274
Epoch 1060, training loss: 89.34446716308594 = 0.9528528451919556 + 10.0 * 8.83916187286377
Epoch 1060, val loss: 0.9609106779098511
Epoch 1070, training loss: 89.37657928466797 = 0.9499596953392029 + 10.0 * 8.84266185760498
Epoch 1070, val loss: 0.9581635594367981
Epoch 1080, training loss: 89.44549560546875 = 0.9470540881156921 + 10.0 * 8.849843978881836
Epoch 1080, val loss: 0.95542311668396
Epoch 1090, training loss: 89.44792175292969 = 0.9440901875495911 + 10.0 * 8.850382804870605
Epoch 1090, val loss: 0.9526547193527222
Epoch 1100, training loss: 89.4629135131836 = 0.941099226474762 + 10.0 * 8.852181434631348
Epoch 1100, val loss: 0.9498372077941895
Epoch 1110, training loss: 89.50839233398438 = 0.9380995631217957 + 10.0 * 8.85702896118164
Epoch 1110, val loss: 0.9470372796058655
Epoch 1120, training loss: 89.52099609375 = 0.9350734949111938 + 10.0 * 8.85859203338623
Epoch 1120, val loss: 0.9441826343536377
Epoch 1130, training loss: 89.55027770996094 = 0.9320412874221802 + 10.0 * 8.861823081970215
Epoch 1130, val loss: 0.9413453340530396
Epoch 1140, training loss: 89.60298156738281 = 0.9289824962615967 + 10.0 * 8.867399215698242
Epoch 1140, val loss: 0.9384629130363464
Epoch 1150, training loss: 89.47091674804688 = 0.9258393049240112 + 10.0 * 8.854507446289062
Epoch 1150, val loss: 0.935558557510376
Epoch 1160, training loss: 89.49066162109375 = 0.9228199124336243 + 10.0 * 8.856783866882324
Epoch 1160, val loss: 0.932699978351593
Epoch 1170, training loss: 89.60183715820312 = 0.9197254180908203 + 10.0 * 8.86821174621582
Epoch 1170, val loss: 0.9298039674758911
Epoch 1180, training loss: 89.63272857666016 = 0.916566789150238 + 10.0 * 8.87161636352539
Epoch 1180, val loss: 0.9268375635147095
Epoch 1190, training loss: 89.65105438232422 = 0.9134006500244141 + 10.0 * 8.87376594543457
Epoch 1190, val loss: 0.9238418340682983
Epoch 1200, training loss: 89.66685485839844 = 0.9102243185043335 + 10.0 * 8.875662803649902
Epoch 1200, val loss: 0.920863926410675
Epoch 1210, training loss: 89.69205474853516 = 0.9070507287979126 + 10.0 * 8.878499984741211
Epoch 1210, val loss: 0.9178755879402161
Epoch 1220, training loss: 89.7173843383789 = 0.9038762450218201 + 10.0 * 8.88135051727295
Epoch 1220, val loss: 0.9148858785629272
Epoch 1230, training loss: 89.70539855957031 = 0.9006580710411072 + 10.0 * 8.880474090576172
Epoch 1230, val loss: 0.9118608832359314
Epoch 1240, training loss: 89.57723999023438 = 0.8974167108535767 + 10.0 * 8.867982864379883
Epoch 1240, val loss: 0.9088427424430847
Epoch 1250, training loss: 89.63274383544922 = 0.8943107724189758 + 10.0 * 8.8738431930542
Epoch 1250, val loss: 0.9058753848075867
Epoch 1260, training loss: 89.67756652832031 = 0.8911058306694031 + 10.0 * 8.878645896911621
Epoch 1260, val loss: 0.9028491973876953
Epoch 1270, training loss: 89.76946258544922 = 0.8879004120826721 + 10.0 * 8.888155937194824
Epoch 1270, val loss: 0.899820864200592
Epoch 1280, training loss: 89.76588439941406 = 0.8846527338027954 + 10.0 * 8.888123512268066
Epoch 1280, val loss: 0.8967713713645935
Epoch 1290, training loss: 89.74827575683594 = 0.8814235329627991 + 10.0 * 8.886685371398926
Epoch 1290, val loss: 0.8937419652938843
Epoch 1300, training loss: 89.66893005371094 = 0.878194272518158 + 10.0 * 8.879073143005371
Epoch 1300, val loss: 0.8906611204147339
Epoch 1310, training loss: 89.71144104003906 = 0.8749825954437256 + 10.0 * 8.883646011352539
Epoch 1310, val loss: 0.8876385688781738
Epoch 1320, training loss: 89.82200622558594 = 0.8718041181564331 + 10.0 * 8.895020484924316
Epoch 1320, val loss: 0.8846302628517151
Epoch 1330, training loss: 89.8978042602539 = 0.8685800433158875 + 10.0 * 8.902921676635742
Epoch 1330, val loss: 0.8815950155258179
Epoch 1340, training loss: 89.7215347290039 = 0.8653621673583984 + 10.0 * 8.88561725616455
Epoch 1340, val loss: 0.8785570859909058
Epoch 1350, training loss: 89.79828643798828 = 0.8621729612350464 + 10.0 * 8.893610954284668
Epoch 1350, val loss: 0.8755552172660828
Epoch 1360, training loss: 89.705078125 = 0.8588865399360657 + 10.0 * 8.884618759155273
Epoch 1360, val loss: 0.8724492788314819
Epoch 1370, training loss: 89.78211975097656 = 0.8556944131851196 + 10.0 * 8.8926420211792
Epoch 1370, val loss: 0.8694302439689636
Epoch 1380, training loss: 89.8725357055664 = 0.852459728717804 + 10.0 * 8.902007102966309
Epoch 1380, val loss: 0.866348385810852
Epoch 1390, training loss: 89.92093658447266 = 0.8492107391357422 + 10.0 * 8.907172203063965
Epoch 1390, val loss: 0.8632873892784119
Epoch 1400, training loss: 89.91943359375 = 0.84596186876297 + 10.0 * 8.907346725463867
Epoch 1400, val loss: 0.8602228164672852
Epoch 1410, training loss: 89.92701721191406 = 0.8427549600601196 + 10.0 * 8.908426284790039
Epoch 1410, val loss: 0.8571810126304626
Epoch 1420, training loss: 89.9852294921875 = 0.8395575881004333 + 10.0 * 8.914567947387695
Epoch 1420, val loss: 0.8541423082351685
Epoch 1430, training loss: 90.00909423828125 = 0.8363924622535706 + 10.0 * 8.917269706726074
Epoch 1430, val loss: 0.851169764995575
Epoch 1440, training loss: 89.95391082763672 = 0.8332274556159973 + 10.0 * 8.912068367004395
Epoch 1440, val loss: 0.8481624126434326
Epoch 1450, training loss: 90.00623321533203 = 0.8301263451576233 + 10.0 * 8.917611122131348
Epoch 1450, val loss: 0.845233142375946
Epoch 1460, training loss: 90.04301452636719 = 0.8270574808120728 + 10.0 * 8.921595573425293
Epoch 1460, val loss: 0.8423427939414978
Epoch 1470, training loss: 90.04678344726562 = 0.8239783048629761 + 10.0 * 8.922281265258789
Epoch 1470, val loss: 0.839447557926178
Epoch 1480, training loss: 90.0234146118164 = 0.8209255337715149 + 10.0 * 8.920248985290527
Epoch 1480, val loss: 0.8365126252174377
Epoch 1490, training loss: 89.9431381225586 = 0.817957878112793 + 10.0 * 8.912518501281738
Epoch 1490, val loss: 0.8337090015411377
Epoch 1500, training loss: 89.95964050292969 = 0.8150438666343689 + 10.0 * 8.914460182189941
Epoch 1500, val loss: 0.830919086933136
Epoch 1510, training loss: 89.8683853149414 = 0.8121552467346191 + 10.0 * 8.905622482299805
Epoch 1510, val loss: 0.8282285928726196
Epoch 1520, training loss: 90.01717376708984 = 0.8092694282531738 + 10.0 * 8.92078971862793
Epoch 1520, val loss: 0.8254926800727844
Epoch 1530, training loss: 89.97415161132812 = 0.8064256906509399 + 10.0 * 8.916772842407227
Epoch 1530, val loss: 0.8227478861808777
Epoch 1540, training loss: 89.96902465820312 = 0.8034518957138062 + 10.0 * 8.916557312011719
Epoch 1540, val loss: 0.8199467062950134
Epoch 1550, training loss: 90.0270004272461 = 0.8005836606025696 + 10.0 * 8.92264175415039
Epoch 1550, val loss: 0.8172264695167542
Epoch 1560, training loss: 90.09253692626953 = 0.7977297902107239 + 10.0 * 8.92948055267334
Epoch 1560, val loss: 0.8145334124565125
Epoch 1570, training loss: 90.12789916992188 = 0.7948567867279053 + 10.0 * 8.933304786682129
Epoch 1570, val loss: 0.8118160963058472
Epoch 1580, training loss: 90.08088684082031 = 0.7920129895210266 + 10.0 * 8.928887367248535
Epoch 1580, val loss: 0.8091292381286621
Epoch 1590, training loss: 90.1376953125 = 0.7892446517944336 + 10.0 * 8.934844970703125
Epoch 1590, val loss: 0.8064680695533752
Epoch 1600, training loss: 90.1762924194336 = 0.7864753603935242 + 10.0 * 8.938982009887695
Epoch 1600, val loss: 0.8038607835769653
Epoch 1610, training loss: 90.2003402709961 = 0.7837135195732117 + 10.0 * 8.941662788391113
Epoch 1610, val loss: 0.8012453317642212
Epoch 1620, training loss: 89.99778747558594 = 0.7809299230575562 + 10.0 * 8.921686172485352
Epoch 1620, val loss: 0.7985841035842896
Epoch 1630, training loss: 90.02532196044922 = 0.7783389687538147 + 10.0 * 8.924698829650879
Epoch 1630, val loss: 0.7961412668228149
Epoch 1640, training loss: 90.0922622680664 = 0.7756661176681519 + 10.0 * 8.931659698486328
Epoch 1640, val loss: 0.7935569286346436
Epoch 1650, training loss: 90.18058013916016 = 0.773043692111969 + 10.0 * 8.940753936767578
Epoch 1650, val loss: 0.7910747528076172
Epoch 1660, training loss: 90.24640655517578 = 0.7704377770423889 + 10.0 * 8.947596549987793
Epoch 1660, val loss: 0.7886002659797668
Epoch 1670, training loss: 90.22772216796875 = 0.7678349614143372 + 10.0 * 8.945988655090332
Epoch 1670, val loss: 0.7861281633377075
Epoch 1680, training loss: 90.12352752685547 = 0.7652788162231445 + 10.0 * 8.935824394226074
Epoch 1680, val loss: 0.7836745381355286
Epoch 1690, training loss: 90.1673355102539 = 0.7628181576728821 + 10.0 * 8.940451622009277
Epoch 1690, val loss: 0.7813722491264343
Epoch 1700, training loss: 90.18958282470703 = 0.7604526281356812 + 10.0 * 8.942913055419922
Epoch 1700, val loss: 0.7791183590888977
Epoch 1710, training loss: 90.19844055175781 = 0.7579063773155212 + 10.0 * 8.944053649902344
Epoch 1710, val loss: 0.7766078114509583
Epoch 1720, training loss: 90.25070190429688 = 0.7555453181266785 + 10.0 * 8.949515342712402
Epoch 1720, val loss: 0.7743942141532898
Epoch 1730, training loss: 90.34722137451172 = 0.7531229853630066 + 10.0 * 8.959409713745117
Epoch 1730, val loss: 0.7720853686332703
Epoch 1740, training loss: 90.25151062011719 = 0.7507209777832031 + 10.0 * 8.950078964233398
Epoch 1740, val loss: 0.7697962522506714
Epoch 1750, training loss: 90.23988342285156 = 0.7483510971069336 + 10.0 * 8.949152946472168
Epoch 1750, val loss: 0.7675451636314392
Epoch 1760, training loss: 90.32829284667969 = 0.7460441589355469 + 10.0 * 8.95822525024414
Epoch 1760, val loss: 0.7653195261955261
Epoch 1770, training loss: 90.3843994140625 = 0.7437389492988586 + 10.0 * 8.964066505432129
Epoch 1770, val loss: 0.7631198167800903
Epoch 1780, training loss: 90.39007568359375 = 0.7414551973342896 + 10.0 * 8.964861869812012
Epoch 1780, val loss: 0.7609133124351501
Epoch 1790, training loss: 90.3736572265625 = 0.739187479019165 + 10.0 * 8.963446617126465
Epoch 1790, val loss: 0.7587705254554749
Epoch 1800, training loss: 90.3995590209961 = 0.7369710206985474 + 10.0 * 8.966259002685547
Epoch 1800, val loss: 0.7566519975662231
Epoch 1810, training loss: 90.45934295654297 = 0.7347568273544312 + 10.0 * 8.972458839416504
Epoch 1810, val loss: 0.7545408606529236
Epoch 1820, training loss: 90.43807220458984 = 0.7325495481491089 + 10.0 * 8.970552444458008
Epoch 1820, val loss: 0.752435564994812
Epoch 1830, training loss: 90.44422912597656 = 0.7304144501686096 + 10.0 * 8.971381187438965
Epoch 1830, val loss: 0.7503604888916016
Epoch 1840, training loss: 90.25621795654297 = 0.7283517718315125 + 10.0 * 8.952786445617676
Epoch 1840, val loss: 0.7483842968940735
Epoch 1850, training loss: 90.30413818359375 = 0.7262899279594421 + 10.0 * 8.957784652709961
Epoch 1850, val loss: 0.746428906917572
Epoch 1860, training loss: 90.3547134399414 = 0.7242534160614014 + 10.0 * 8.963046073913574
Epoch 1860, val loss: 0.7444711327552795
Epoch 1870, training loss: 90.36065673828125 = 0.7221839427947998 + 10.0 * 8.963847160339355
Epoch 1870, val loss: 0.742517352104187
Epoch 1880, training loss: 90.4430923461914 = 0.7201498746871948 + 10.0 * 8.972294807434082
Epoch 1880, val loss: 0.7405486702919006
Epoch 1890, training loss: 90.48597717285156 = 0.7181261777877808 + 10.0 * 8.976785659790039
Epoch 1890, val loss: 0.7386242747306824
Epoch 1900, training loss: 90.5595474243164 = 0.7161076664924622 + 10.0 * 8.984343528747559
Epoch 1900, val loss: 0.7366775870323181
Epoch 1910, training loss: 90.53569030761719 = 0.7141238451004028 + 10.0 * 8.982156753540039
Epoch 1910, val loss: 0.7347878217697144
Epoch 1920, training loss: 90.54415893554688 = 0.712165117263794 + 10.0 * 8.983199119567871
Epoch 1920, val loss: 0.7329041957855225
Epoch 1930, training loss: 90.56062316894531 = 0.7102181315422058 + 10.0 * 8.985040664672852
Epoch 1930, val loss: 0.7310426831245422
Epoch 1940, training loss: 90.58509063720703 = 0.7082983255386353 + 10.0 * 8.987679481506348
Epoch 1940, val loss: 0.7292114496231079
Epoch 1950, training loss: 90.61087036132812 = 0.706380307674408 + 10.0 * 8.990448951721191
Epoch 1950, val loss: 0.727363109588623
Epoch 1960, training loss: 90.6199951171875 = 0.7044927477836609 + 10.0 * 8.99155044555664
Epoch 1960, val loss: 0.725513756275177
Epoch 1970, training loss: 90.61449432373047 = 0.7026229500770569 + 10.0 * 8.99118709564209
Epoch 1970, val loss: 0.7237457633018494
Epoch 1980, training loss: 90.45368957519531 = 0.7007573843002319 + 10.0 * 8.975293159484863
Epoch 1980, val loss: 0.7219409942626953
Epoch 1990, training loss: 90.5065689086914 = 0.6989431977272034 + 10.0 * 8.980762481689453
Epoch 1990, val loss: 0.720153272151947
Epoch 2000, training loss: 90.3650894165039 = 0.6972828507423401 + 10.0 * 8.966780662536621
Epoch 2000, val loss: 0.7185832262039185
Epoch 2010, training loss: 90.41605377197266 = 0.6956124305725098 + 10.0 * 8.972043991088867
Epoch 2010, val loss: 0.7169235348701477
Epoch 2020, training loss: 90.43659210205078 = 0.693843424320221 + 10.0 * 8.974275588989258
Epoch 2020, val loss: 0.7152332663536072
Epoch 2030, training loss: 90.53626251220703 = 0.6920604705810547 + 10.0 * 8.984419822692871
Epoch 2030, val loss: 0.7135328650474548
Epoch 2040, training loss: 90.59029388427734 = 0.6902782320976257 + 10.0 * 8.990001678466797
Epoch 2040, val loss: 0.7118194699287415
Epoch 2050, training loss: 90.63304138183594 = 0.6885095834732056 + 10.0 * 8.994453430175781
Epoch 2050, val loss: 0.7101127505302429
Epoch 2060, training loss: 90.64915466308594 = 0.6867660880088806 + 10.0 * 8.996238708496094
Epoch 2060, val loss: 0.7084150314331055
Epoch 2070, training loss: 90.66676330566406 = 0.685055673122406 + 10.0 * 8.998170852661133
Epoch 2070, val loss: 0.7067407965660095
Epoch 2080, training loss: 90.69063568115234 = 0.6833574771881104 + 10.0 * 9.000727653503418
Epoch 2080, val loss: 0.7051030993461609
Epoch 2090, training loss: 90.6871109008789 = 0.681685745716095 + 10.0 * 9.000542640686035
Epoch 2090, val loss: 0.7034899592399597
Epoch 2100, training loss: 90.6138916015625 = 0.6800541877746582 + 10.0 * 8.993383407592773
Epoch 2100, val loss: 0.7019747495651245
Epoch 2110, training loss: 90.63375854492188 = 0.6784496307373047 + 10.0 * 8.99553108215332
Epoch 2110, val loss: 0.7004142999649048
Epoch 2120, training loss: 90.6877670288086 = 0.6768397688865662 + 10.0 * 9.001092910766602
Epoch 2120, val loss: 0.6988988518714905
Epoch 2130, training loss: 90.71361541748047 = 0.6752285361289978 + 10.0 * 9.003838539123535
Epoch 2130, val loss: 0.6973957419395447
Epoch 2140, training loss: 90.72425079345703 = 0.6736795902252197 + 10.0 * 9.005056381225586
Epoch 2140, val loss: 0.6959177255630493
Epoch 2150, training loss: 90.56159210205078 = 0.672205924987793 + 10.0 * 8.98893928527832
Epoch 2150, val loss: 0.6945331692695618
Epoch 2160, training loss: 90.41390991210938 = 0.6707343459129333 + 10.0 * 8.97431755065918
Epoch 2160, val loss: 0.6931462287902832
Epoch 2170, training loss: 90.47220611572266 = 0.669259786605835 + 10.0 * 8.980295181274414
Epoch 2170, val loss: 0.6918019652366638
Epoch 2180, training loss: 90.5155029296875 = 0.6678193211555481 + 10.0 * 8.984768867492676
Epoch 2180, val loss: 0.6904577612876892
Epoch 2190, training loss: 90.63493347167969 = 0.6663568019866943 + 10.0 * 8.996857643127441
Epoch 2190, val loss: 0.6891099810600281
Epoch 2200, training loss: 90.71411895751953 = 0.6648502349853516 + 10.0 * 9.004926681518555
Epoch 2200, val loss: 0.6877060532569885
Epoch 2210, training loss: 90.7849349975586 = 0.6633501648902893 + 10.0 * 9.012158393859863
Epoch 2210, val loss: 0.6863114237785339
Epoch 2220, training loss: 90.69908142089844 = 0.6618786454200745 + 10.0 * 9.0037202835083
Epoch 2220, val loss: 0.6849281787872314
Epoch 2230, training loss: 90.73828125 = 0.6604639887809753 + 10.0 * 9.007781982421875
Epoch 2230, val loss: 0.6836282014846802
Epoch 2240, training loss: 90.76704406738281 = 0.6590012907981873 + 10.0 * 9.010804176330566
Epoch 2240, val loss: 0.6822684407234192
Epoch 2250, training loss: 90.81291198730469 = 0.657558798789978 + 10.0 * 9.015535354614258
Epoch 2250, val loss: 0.6809608340263367
Epoch 2260, training loss: 90.81502532958984 = 0.6561152338981628 + 10.0 * 9.015891075134277
Epoch 2260, val loss: 0.6796261072158813
Epoch 2270, training loss: 90.82801818847656 = 0.6547152996063232 + 10.0 * 9.017330169677734
Epoch 2270, val loss: 0.6783291101455688
Epoch 2280, training loss: 90.84213256835938 = 0.6533176898956299 + 10.0 * 9.018880844116211
Epoch 2280, val loss: 0.6770605444908142
Epoch 2290, training loss: 90.8200912475586 = 0.651929497718811 + 10.0 * 9.016816139221191
Epoch 2290, val loss: 0.6757978200912476
Epoch 2300, training loss: 90.82102966308594 = 0.6505704522132874 + 10.0 * 9.017045974731445
Epoch 2300, val loss: 0.6745345592498779
Epoch 2310, training loss: 90.76659393310547 = 0.6492573022842407 + 10.0 * 9.011734008789062
Epoch 2310, val loss: 0.6733230948448181
Epoch 2320, training loss: 90.81391906738281 = 0.6479335427284241 + 10.0 * 9.01659870147705
Epoch 2320, val loss: 0.6721110343933105
Epoch 2330, training loss: 90.846435546875 = 0.6466171145439148 + 10.0 * 9.01998233795166
Epoch 2330, val loss: 0.6709194183349609
Epoch 2340, training loss: 90.89031982421875 = 0.645296573638916 + 10.0 * 9.024502754211426
Epoch 2340, val loss: 0.6697198748588562
Epoch 2350, training loss: 90.92379760742188 = 0.6439884901046753 + 10.0 * 9.02798080444336
Epoch 2350, val loss: 0.6685482859611511
Epoch 2360, training loss: 90.93988800048828 = 0.6426953077316284 + 10.0 * 9.029719352722168
Epoch 2360, val loss: 0.6673859357833862
Epoch 2370, training loss: 90.9227523803711 = 0.6414167284965515 + 10.0 * 9.028133392333984
Epoch 2370, val loss: 0.6662307381629944
Epoch 2380, training loss: 90.908447265625 = 0.6401644349098206 + 10.0 * 9.026827812194824
Epoch 2380, val loss: 0.6650551557540894
Epoch 2390, training loss: 90.87217712402344 = 0.6389548182487488 + 10.0 * 9.023322105407715
Epoch 2390, val loss: 0.6639708876609802
Epoch 2400, training loss: 89.9881362915039 = 0.6379604935646057 + 10.0 * 8.935017585754395
Epoch 2400, val loss: 0.6634111404418945
Epoch 2410, training loss: 90.25302124023438 = 0.6371892690658569 + 10.0 * 8.961583137512207
Epoch 2410, val loss: 0.6622419357299805
Epoch 2420, training loss: 89.93841552734375 = 0.6361171007156372 + 10.0 * 8.930230140686035
Epoch 2420, val loss: 0.6614342927932739
Epoch 2430, training loss: 90.25648498535156 = 0.635056734085083 + 10.0 * 8.962142944335938
Epoch 2430, val loss: 0.6606287360191345
Epoch 2440, training loss: 90.32244873046875 = 0.6338908672332764 + 10.0 * 8.968855857849121
Epoch 2440, val loss: 0.6595272421836853
Epoch 2450, training loss: 90.41870880126953 = 0.6326974034309387 + 10.0 * 8.978601455688477
Epoch 2450, val loss: 0.6584139466285706
Epoch 2460, training loss: 90.5130615234375 = 0.6314883828163147 + 10.0 * 8.988157272338867
Epoch 2460, val loss: 0.6573699712753296
Epoch 2470, training loss: 90.60005950927734 = 0.6302398443222046 + 10.0 * 8.996981620788574
Epoch 2470, val loss: 0.656281590461731
Epoch 2480, training loss: 90.66448974609375 = 0.6290281414985657 + 10.0 * 9.003545761108398
Epoch 2480, val loss: 0.6552149057388306
Epoch 2490, training loss: 90.70294189453125 = 0.6278336048126221 + 10.0 * 9.007511138916016
Epoch 2490, val loss: 0.6541624069213867
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7772463768115941
0.8651017894660582
=== training gcn model ===
Epoch 0, training loss: 103.45892333984375 = 1.0967521667480469 + 10.0 * 10.23621654510498
Epoch 0, val loss: 1.0969687700271606
Epoch 10, training loss: 99.49440002441406 = 1.0965358018875122 + 10.0 * 9.839786529541016
Epoch 10, val loss: 1.0967800617218018
Epoch 20, training loss: 97.31686401367188 = 1.0963770151138306 + 10.0 * 9.622049331665039
Epoch 20, val loss: 1.096643328666687
Epoch 30, training loss: 95.72039031982422 = 1.096198320388794 + 10.0 * 9.462419509887695
Epoch 30, val loss: 1.0964806079864502
Epoch 40, training loss: 94.47663116455078 = 1.0960099697113037 + 10.0 * 9.338062286376953
Epoch 40, val loss: 1.0963115692138672
Epoch 50, training loss: 93.45927429199219 = 1.0958237648010254 + 10.0 * 9.236345291137695
Epoch 50, val loss: 1.0961447954177856
Epoch 60, training loss: 92.61854553222656 = 1.0956250429153442 + 10.0 * 9.152292251586914
Epoch 60, val loss: 1.0959676504135132
Epoch 70, training loss: 91.91741180419922 = 1.0954234600067139 + 10.0 * 9.082199096679688
Epoch 70, val loss: 1.0957872867584229
Epoch 80, training loss: 91.33291625976562 = 1.0952204465866089 + 10.0 * 9.02376937866211
Epoch 80, val loss: 1.0956069231033325
Epoch 90, training loss: 90.84613037109375 = 1.095008373260498 + 10.0 * 8.975111961364746
Epoch 90, val loss: 1.0954153537750244
Epoch 100, training loss: 90.43046569824219 = 1.0947874784469604 + 10.0 * 8.933568000793457
Epoch 100, val loss: 1.095217227935791
Epoch 110, training loss: 90.09970092773438 = 1.0945608615875244 + 10.0 * 8.900514602661133
Epoch 110, val loss: 1.0950121879577637
Epoch 120, training loss: 89.8021469116211 = 1.0943320989608765 + 10.0 * 8.870781898498535
Epoch 120, val loss: 1.0948059558868408
Epoch 130, training loss: 89.5537109375 = 1.094093918800354 + 10.0 * 8.845961570739746
Epoch 130, val loss: 1.094589114189148
Epoch 140, training loss: 89.3355941772461 = 1.0938538312911987 + 10.0 * 8.824173927307129
Epoch 140, val loss: 1.0943756103515625
Epoch 150, training loss: 89.13710021972656 = 1.0936132669448853 + 10.0 * 8.804348945617676
Epoch 150, val loss: 1.094153881072998
Epoch 160, training loss: 88.97146606445312 = 1.0933557748794556 + 10.0 * 8.787811279296875
Epoch 160, val loss: 1.0939207077026367
Epoch 170, training loss: 88.83323669433594 = 1.093097448348999 + 10.0 * 8.774014472961426
Epoch 170, val loss: 1.0936822891235352
Epoch 180, training loss: 88.709716796875 = 1.092820405960083 + 10.0 * 8.761690139770508
Epoch 180, val loss: 1.0934306383132935
Epoch 190, training loss: 88.61582946777344 = 1.0925496816635132 + 10.0 * 8.752327919006348
Epoch 190, val loss: 1.0931860208511353
Epoch 200, training loss: 88.5598373413086 = 1.0922645330429077 + 10.0 * 8.746757507324219
Epoch 200, val loss: 1.092930555343628
Epoch 210, training loss: 88.4523696899414 = 1.0919917821884155 + 10.0 * 8.736037254333496
Epoch 210, val loss: 1.0926711559295654
Epoch 220, training loss: 88.36112213134766 = 1.0916908979415894 + 10.0 * 8.726943016052246
Epoch 220, val loss: 1.092397928237915
Epoch 230, training loss: 88.27704620361328 = 1.0913844108581543 + 10.0 * 8.718565940856934
Epoch 230, val loss: 1.0921176671981812
Epoch 240, training loss: 88.24309539794922 = 1.0910801887512207 + 10.0 * 8.715201377868652
Epoch 240, val loss: 1.091842532157898
Epoch 250, training loss: 88.21998596191406 = 1.0907739400863647 + 10.0 * 8.712921142578125
Epoch 250, val loss: 1.091550350189209
Epoch 260, training loss: 88.1838607788086 = 1.0904487371444702 + 10.0 * 8.709341049194336
Epoch 260, val loss: 1.0912549495697021
Epoch 270, training loss: 88.10885620117188 = 1.0901192426681519 + 10.0 * 8.701873779296875
Epoch 270, val loss: 1.0909475088119507
Epoch 280, training loss: 88.09516906738281 = 1.0897939205169678 + 10.0 * 8.70053768157959
Epoch 280, val loss: 1.0906457901000977
Epoch 290, training loss: 88.0535888671875 = 1.089439868927002 + 10.0 * 8.696414947509766
Epoch 290, val loss: 1.0903165340423584
Epoch 300, training loss: 88.06153106689453 = 1.0886152982711792 + 10.0 * 8.697291374206543
Epoch 300, val loss: 1.0894416570663452
Epoch 310, training loss: 88.05547332763672 = 1.0877048969268799 + 10.0 * 8.696776390075684
Epoch 310, val loss: 1.0884995460510254
Epoch 320, training loss: 88.06159973144531 = 1.0868182182312012 + 10.0 * 8.697478294372559
Epoch 320, val loss: 1.087591290473938
Epoch 330, training loss: 88.07969665527344 = 1.0859410762786865 + 10.0 * 8.699376106262207
Epoch 330, val loss: 1.0867011547088623
Epoch 340, training loss: 88.07183074951172 = 1.0850788354873657 + 10.0 * 8.698675155639648
Epoch 340, val loss: 1.0858150720596313
Epoch 350, training loss: 88.0768814086914 = 1.0842210054397583 + 10.0 * 8.69926643371582
Epoch 350, val loss: 1.0849424600601196
Epoch 360, training loss: 88.0524673461914 = 1.0833208560943604 + 10.0 * 8.696914672851562
Epoch 360, val loss: 1.084030270576477
Epoch 370, training loss: 88.1112289428711 = 1.0825235843658447 + 10.0 * 8.70287036895752
Epoch 370, val loss: 1.0832191705703735
Epoch 380, training loss: 88.09339904785156 = 1.0816514492034912 + 10.0 * 8.70117473602295
Epoch 380, val loss: 1.0823506116867065
Epoch 390, training loss: 88.10836791992188 = 1.0807957649230957 + 10.0 * 8.702756881713867
Epoch 390, val loss: 1.0814846754074097
Epoch 400, training loss: 88.10619354248047 = 1.0799301862716675 + 10.0 * 8.70262622833252
Epoch 400, val loss: 1.0806183815002441
Epoch 410, training loss: 88.13560485839844 = 1.079062581062317 + 10.0 * 8.70565414428711
Epoch 410, val loss: 1.0797430276870728
Epoch 420, training loss: 88.11164093017578 = 1.0781621932983398 + 10.0 * 8.703348159790039
Epoch 420, val loss: 1.0788426399230957
Epoch 430, training loss: 88.14756774902344 = 1.0772709846496582 + 10.0 * 8.707029342651367
Epoch 430, val loss: 1.0779452323913574
Epoch 440, training loss: 88.13291931152344 = 1.0763417482376099 + 10.0 * 8.705657958984375
Epoch 440, val loss: 1.077005386352539
Epoch 450, training loss: 88.11104583740234 = 1.0753778219223022 + 10.0 * 8.703566551208496
Epoch 450, val loss: 1.076059103012085
Epoch 460, training loss: 88.1303939819336 = 1.0744184255599976 + 10.0 * 8.705597877502441
Epoch 460, val loss: 1.0750776529312134
Epoch 470, training loss: 88.158447265625 = 1.0734590291976929 + 10.0 * 8.70849895477295
Epoch 470, val loss: 1.0741262435913086
Epoch 480, training loss: 88.18351745605469 = 1.07248055934906 + 10.0 * 8.711103439331055
Epoch 480, val loss: 1.0731443166732788
Epoch 490, training loss: 88.21385955810547 = 1.0714682340621948 + 10.0 * 8.714239120483398
Epoch 490, val loss: 1.0721368789672852
Epoch 500, training loss: 88.25018310546875 = 1.070465326309204 + 10.0 * 8.717971801757812
Epoch 500, val loss: 1.0711262226104736
Epoch 510, training loss: 88.24650573730469 = 1.0694189071655273 + 10.0 * 8.717708587646484
Epoch 510, val loss: 1.0700877904891968
Epoch 520, training loss: 88.26371002197266 = 1.0683704614639282 + 10.0 * 8.719533920288086
Epoch 520, val loss: 1.0690224170684814
Epoch 530, training loss: 88.2328872680664 = 1.0672434568405151 + 10.0 * 8.716564178466797
Epoch 530, val loss: 1.067915916442871
Epoch 540, training loss: 88.24673461914062 = 1.0661700963974 + 10.0 * 8.718056678771973
Epoch 540, val loss: 1.0668377876281738
Epoch 550, training loss: 88.35014343261719 = 1.0651450157165527 + 10.0 * 8.728499412536621
Epoch 550, val loss: 1.0658293962478638
Epoch 560, training loss: 88.29706573486328 = 1.0639883279800415 + 10.0 * 8.723307609558105
Epoch 560, val loss: 1.0646642446517944
Epoch 570, training loss: 88.33198547363281 = 1.0628465414047241 + 10.0 * 8.726914405822754
Epoch 570, val loss: 1.0635271072387695
Epoch 580, training loss: 88.35871887207031 = 1.0616607666015625 + 10.0 * 8.729705810546875
Epoch 580, val loss: 1.0623424053192139
Epoch 590, training loss: 88.34386444091797 = 1.06046462059021 + 10.0 * 8.728340148925781
Epoch 590, val loss: 1.0611504316329956
Epoch 600, training loss: 88.3458480834961 = 1.0592575073242188 + 10.0 * 8.728658676147461
Epoch 600, val loss: 1.0599578619003296
Epoch 610, training loss: 88.40238952636719 = 1.0580196380615234 + 10.0 * 8.734436988830566
Epoch 610, val loss: 1.0587236881256104
Epoch 620, training loss: 88.44548034667969 = 1.0567518472671509 + 10.0 * 8.738872528076172
Epoch 620, val loss: 1.0574848651885986
Epoch 630, training loss: 88.49114227294922 = 1.0554872751235962 + 10.0 * 8.743565559387207
Epoch 630, val loss: 1.0562212467193604
Epoch 640, training loss: 88.48485565185547 = 1.0541614294052124 + 10.0 * 8.743069648742676
Epoch 640, val loss: 1.0549265146255493
Epoch 650, training loss: 88.49645233154297 = 1.0528247356414795 + 10.0 * 8.744362831115723
Epoch 650, val loss: 1.0536185503005981
Epoch 660, training loss: 88.56226348876953 = 1.0515079498291016 + 10.0 * 8.751075744628906
Epoch 660, val loss: 1.0523138046264648
Epoch 670, training loss: 88.53430938720703 = 1.0501078367233276 + 10.0 * 8.748419761657715
Epoch 670, val loss: 1.0509202480316162
Epoch 680, training loss: 88.56238555908203 = 1.0486904382705688 + 10.0 * 8.75136947631836
Epoch 680, val loss: 1.0495221614837646
Epoch 690, training loss: 88.60051727294922 = 1.0472803115844727 + 10.0 * 8.75532341003418
Epoch 690, val loss: 1.0481382608413696
Epoch 700, training loss: 88.62467956542969 = 1.0458199977874756 + 10.0 * 8.757885932922363
Epoch 700, val loss: 1.0467064380645752
Epoch 710, training loss: 88.66279602050781 = 1.044349193572998 + 10.0 * 8.761844635009766
Epoch 710, val loss: 1.0452522039413452
Epoch 720, training loss: 88.71875 = 1.0428845882415771 + 10.0 * 8.767586708068848
Epoch 720, val loss: 1.0437828302383423
Epoch 730, training loss: 88.51648712158203 = 1.0412530899047852 + 10.0 * 8.747523307800293
Epoch 730, val loss: 1.0421961545944214
Epoch 740, training loss: 88.51252746582031 = 1.0396051406860352 + 10.0 * 8.747292518615723
Epoch 740, val loss: 1.0405683517456055
Epoch 750, training loss: 88.66216278076172 = 1.0380935668945312 + 10.0 * 8.762407302856445
Epoch 750, val loss: 1.0390796661376953
Epoch 760, training loss: 88.63077545166016 = 1.0365047454833984 + 10.0 * 8.759427070617676
Epoch 760, val loss: 1.0375144481658936
Epoch 770, training loss: 88.68931579589844 = 1.0348860025405884 + 10.0 * 8.765442848205566
Epoch 770, val loss: 1.03590989112854
Epoch 780, training loss: 88.73904418945312 = 1.0332430601119995 + 10.0 * 8.770580291748047
Epoch 780, val loss: 1.0342928171157837
Epoch 790, training loss: 88.64417266845703 = 1.0315297842025757 + 10.0 * 8.761263847351074
Epoch 790, val loss: 1.0326063632965088
Epoch 800, training loss: 88.69342803955078 = 1.0298435688018799 + 10.0 * 8.766358375549316
Epoch 800, val loss: 1.0309250354766846
Epoch 810, training loss: 88.75431060791016 = 1.0281169414520264 + 10.0 * 8.772619247436523
Epoch 810, val loss: 1.029227375984192
Epoch 820, training loss: 88.81708526611328 = 1.0263952016830444 + 10.0 * 8.779068946838379
Epoch 820, val loss: 1.0275218486785889
Epoch 830, training loss: 88.84100341796875 = 1.0246282815933228 + 10.0 * 8.781637191772461
Epoch 830, val loss: 1.0257889032363892
Epoch 840, training loss: 88.87762451171875 = 1.0228474140167236 + 10.0 * 8.785477638244629
Epoch 840, val loss: 1.024018406867981
Epoch 850, training loss: 88.83834075927734 = 1.0210298299789429 + 10.0 * 8.781731605529785
Epoch 850, val loss: 1.0222344398498535
Epoch 860, training loss: 88.9006118774414 = 1.0192261934280396 + 10.0 * 8.788138389587402
Epoch 860, val loss: 1.0204626321792603
Epoch 870, training loss: 88.89579772949219 = 1.0173436403274536 + 10.0 * 8.787845611572266
Epoch 870, val loss: 1.0186142921447754
Epoch 880, training loss: 88.90786743164062 = 1.0154902935028076 + 10.0 * 8.789237976074219
Epoch 880, val loss: 1.0167675018310547
Epoch 890, training loss: 88.99517822265625 = 1.0136250257492065 + 10.0 * 8.798154830932617
Epoch 890, val loss: 1.0149098634719849
Epoch 900, training loss: 88.97662353515625 = 1.011673092842102 + 10.0 * 8.79649543762207
Epoch 900, val loss: 1.0129923820495605
Epoch 910, training loss: 89.0038070678711 = 1.0097459554672241 + 10.0 * 8.799406051635742
Epoch 910, val loss: 1.011077642440796
Epoch 920, training loss: 89.02222442626953 = 1.0077753067016602 + 10.0 * 8.801445007324219
Epoch 920, val loss: 1.0090926885604858
Epoch 930, training loss: 89.0238037109375 = 1.0058140754699707 + 10.0 * 8.801798820495605
Epoch 930, val loss: 1.0072097778320312
Epoch 940, training loss: 88.89081573486328 = 1.0038388967514038 + 10.0 * 8.788698196411133
Epoch 940, val loss: 1.0052564144134521
Epoch 950, training loss: 88.96456146240234 = 1.0018552541732788 + 10.0 * 8.796270370483398
Epoch 950, val loss: 1.0032908916473389
Epoch 960, training loss: 89.00272369384766 = 0.999858558177948 + 10.0 * 8.800287246704102
Epoch 960, val loss: 1.0012807846069336
Epoch 970, training loss: 88.98171997070312 = 0.9977717995643616 + 10.0 * 8.798395156860352
Epoch 970, val loss: 0.9992507696151733
Epoch 980, training loss: 89.070556640625 = 0.9957253336906433 + 10.0 * 8.807482719421387
Epoch 980, val loss: 0.9972304105758667
Epoch 990, training loss: 89.10950469970703 = 0.9935933351516724 + 10.0 * 8.811591148376465
Epoch 990, val loss: 0.9951340556144714
Epoch 1000, training loss: 89.09810638427734 = 0.9914511442184448 + 10.0 * 8.81066608428955
Epoch 1000, val loss: 0.9930211901664734
Epoch 1010, training loss: 89.15460968017578 = 0.9893538355827332 + 10.0 * 8.81652545928955
Epoch 1010, val loss: 0.9909467101097107
Epoch 1020, training loss: 89.15959930419922 = 0.9871742129325867 + 10.0 * 8.817242622375488
Epoch 1020, val loss: 0.9887959957122803
Epoch 1030, training loss: 89.17353057861328 = 0.9849868416786194 + 10.0 * 8.818854331970215
Epoch 1030, val loss: 0.9866655468940735
Epoch 1040, training loss: 89.20851135253906 = 0.9827893972396851 + 10.0 * 8.822572708129883
Epoch 1040, val loss: 0.9844816327095032
Epoch 1050, training loss: 89.2198486328125 = 0.9805660843849182 + 10.0 * 8.823927879333496
Epoch 1050, val loss: 0.9822967648506165
Epoch 1060, training loss: 89.28750610351562 = 0.9783753752708435 + 10.0 * 8.830912590026855
Epoch 1060, val loss: 0.98015958070755
Epoch 1070, training loss: 89.22840118408203 = 0.9761101603507996 + 10.0 * 8.825228691101074
Epoch 1070, val loss: 0.9779221415519714
Epoch 1080, training loss: 89.2969741821289 = 0.9738438129425049 + 10.0 * 8.83231258392334
Epoch 1080, val loss: 0.9756836891174316
Epoch 1090, training loss: 89.3094253540039 = 0.9715255498886108 + 10.0 * 8.833789825439453
Epoch 1090, val loss: 0.9733883738517761
Epoch 1100, training loss: 89.35067749023438 = 0.9691867828369141 + 10.0 * 8.838149070739746
Epoch 1100, val loss: 0.9710909128189087
Epoch 1110, training loss: 89.3160400390625 = 0.9668228030204773 + 10.0 * 8.834921836853027
Epoch 1110, val loss: 0.9687635898590088
Epoch 1120, training loss: 89.40707397460938 = 0.9644743800163269 + 10.0 * 8.844259262084961
Epoch 1120, val loss: 0.9664376974105835
Epoch 1130, training loss: 89.40145111083984 = 0.9620572924613953 + 10.0 * 8.843938827514648
Epoch 1130, val loss: 0.9640635251998901
Epoch 1140, training loss: 89.44467163085938 = 0.9596492052078247 + 10.0 * 8.848502159118652
Epoch 1140, val loss: 0.9616910219192505
Epoch 1150, training loss: 89.44097900390625 = 0.9572291374206543 + 10.0 * 8.84837532043457
Epoch 1150, val loss: 0.95930016040802
Epoch 1160, training loss: 89.42433166503906 = 0.9547777771949768 + 10.0 * 8.846955299377441
Epoch 1160, val loss: 0.9568937420845032
Epoch 1170, training loss: 89.49280548095703 = 0.9523114562034607 + 10.0 * 8.854049682617188
Epoch 1170, val loss: 0.9544714093208313
Epoch 1180, training loss: 89.53693389892578 = 0.9498031735420227 + 10.0 * 8.858713150024414
Epoch 1180, val loss: 0.9520025849342346
Epoch 1190, training loss: 89.48815155029297 = 0.9472643733024597 + 10.0 * 8.85408878326416
Epoch 1190, val loss: 0.9494673013687134
Epoch 1200, training loss: 89.49403381347656 = 0.944670557975769 + 10.0 * 8.854936599731445
Epoch 1200, val loss: 0.9469418525695801
Epoch 1210, training loss: 89.54743957519531 = 0.9421214461326599 + 10.0 * 8.8605318069458
Epoch 1210, val loss: 0.9444425702095032
Epoch 1220, training loss: 89.56938934326172 = 0.9395145177841187 + 10.0 * 8.862987518310547
Epoch 1220, val loss: 0.941886305809021
Epoch 1230, training loss: 89.6063461303711 = 0.9368302226066589 + 10.0 * 8.866951942443848
Epoch 1230, val loss: 0.9392679929733276
Epoch 1240, training loss: 89.63714599609375 = 0.9343286752700806 + 10.0 * 8.870282173156738
Epoch 1240, val loss: 0.9367685317993164
Epoch 1250, training loss: 89.49736022949219 = 0.931656539440155 + 10.0 * 8.85657024383545
Epoch 1250, val loss: 0.9341591000556946
Epoch 1260, training loss: 89.56781768798828 = 0.9290197491645813 + 10.0 * 8.863880157470703
Epoch 1260, val loss: 0.9315691590309143
Epoch 1270, training loss: 89.6109619140625 = 0.9263282418251038 + 10.0 * 8.868463516235352
Epoch 1270, val loss: 0.9289388060569763
Epoch 1280, training loss: 89.6330795288086 = 0.9236254692077637 + 10.0 * 8.870945930480957
Epoch 1280, val loss: 0.9262850880622864
Epoch 1290, training loss: 89.66609191894531 = 0.920884370803833 + 10.0 * 8.874521255493164
Epoch 1290, val loss: 0.9236007332801819
Epoch 1300, training loss: 89.67894744873047 = 0.9181191325187683 + 10.0 * 8.876082420349121
Epoch 1300, val loss: 0.920901894569397
Epoch 1310, training loss: 89.70587921142578 = 0.9153618216514587 + 10.0 * 8.87905216217041
Epoch 1310, val loss: 0.9181907773017883
Epoch 1320, training loss: 89.72501373291016 = 0.9125756621360779 + 10.0 * 8.881243705749512
Epoch 1320, val loss: 0.9154513478279114
Epoch 1330, training loss: 89.74722290039062 = 0.9097796082496643 + 10.0 * 8.883744239807129
Epoch 1330, val loss: 0.9127119779586792
Epoch 1340, training loss: 89.75443267822266 = 0.9069635272026062 + 10.0 * 8.884746551513672
Epoch 1340, val loss: 0.9099698662757874
Epoch 1350, training loss: 89.79206085205078 = 0.9041688442230225 + 10.0 * 8.888789176940918
Epoch 1350, val loss: 0.9072296619415283
Epoch 1360, training loss: 89.77830505371094 = 0.901351809501648 + 10.0 * 8.8876953125
Epoch 1360, val loss: 0.9044505953788757
Epoch 1370, training loss: 89.79557037353516 = 0.89853835105896 + 10.0 * 8.889703750610352
Epoch 1370, val loss: 0.9017121195793152
Epoch 1380, training loss: 89.77474212646484 = 0.8957212567329407 + 10.0 * 8.88790225982666
Epoch 1380, val loss: 0.8989535570144653
Epoch 1390, training loss: 89.79583740234375 = 0.8928775787353516 + 10.0 * 8.89029598236084
Epoch 1390, val loss: 0.8961880207061768
Epoch 1400, training loss: 89.82963562011719 = 0.8900765180587769 + 10.0 * 8.893956184387207
Epoch 1400, val loss: 0.8934227824211121
Epoch 1410, training loss: 89.90095520019531 = 0.8872684240341187 + 10.0 * 8.901369094848633
Epoch 1410, val loss: 0.8906596302986145
Epoch 1420, training loss: 89.91030883789062 = 0.8843919634819031 + 10.0 * 8.902591705322266
Epoch 1420, val loss: 0.8878598213195801
Epoch 1430, training loss: 89.88531494140625 = 0.8814697861671448 + 10.0 * 8.900384902954102
Epoch 1430, val loss: 0.8849863409996033
Epoch 1440, training loss: 89.89977264404297 = 0.8785573244094849 + 10.0 * 8.902121543884277
Epoch 1440, val loss: 0.8821181654930115
Epoch 1450, training loss: 89.98316192626953 = 0.8755802512168884 + 10.0 * 8.910758018493652
Epoch 1450, val loss: 0.879228949546814
Epoch 1460, training loss: 89.97434997558594 = 0.8726009130477905 + 10.0 * 8.910174369812012
Epoch 1460, val loss: 0.8762858510017395
Epoch 1470, training loss: 89.94683074951172 = 0.869572639465332 + 10.0 * 8.90772533416748
Epoch 1470, val loss: 0.873327910900116
Epoch 1480, training loss: 89.98413848876953 = 0.8666208386421204 + 10.0 * 8.911751747131348
Epoch 1480, val loss: 0.8703908324241638
Epoch 1490, training loss: 89.97257995605469 = 0.8636301755905151 + 10.0 * 8.910894393920898
Epoch 1490, val loss: 0.8674700856208801
Epoch 1500, training loss: 90.01274108886719 = 0.8606375455856323 + 10.0 * 8.915210723876953
Epoch 1500, val loss: 0.8645731210708618
Epoch 1510, training loss: 90.05294799804688 = 0.8576104044914246 + 10.0 * 8.919533729553223
Epoch 1510, val loss: 0.8616042137145996
Epoch 1520, training loss: 90.06703186035156 = 0.8546218872070312 + 10.0 * 8.92124080657959
Epoch 1520, val loss: 0.8586857318878174
Epoch 1530, training loss: 90.11276245117188 = 0.8516968488693237 + 10.0 * 8.926106452941895
Epoch 1530, val loss: 0.8557486534118652
Epoch 1540, training loss: 90.06493377685547 = 0.8487364053726196 + 10.0 * 8.921619415283203
Epoch 1540, val loss: 0.8528619408607483
Epoch 1550, training loss: 90.09201049804688 = 0.8457561135292053 + 10.0 * 8.924625396728516
Epoch 1550, val loss: 0.850007176399231
Epoch 1560, training loss: 90.1406478881836 = 0.8427960276603699 + 10.0 * 8.929784774780273
Epoch 1560, val loss: 0.8471102714538574
Epoch 1570, training loss: 90.0876235961914 = 0.8397902250289917 + 10.0 * 8.924783706665039
Epoch 1570, val loss: 0.8442052602767944
Epoch 1580, training loss: 90.08262634277344 = 0.8368982672691345 + 10.0 * 8.924572944641113
Epoch 1580, val loss: 0.8413544297218323
Epoch 1590, training loss: 90.06150817871094 = 0.833953857421875 + 10.0 * 8.922755241394043
Epoch 1590, val loss: 0.8384459018707275
Epoch 1600, training loss: 90.11792755126953 = 0.8311594128608704 + 10.0 * 8.92867660522461
Epoch 1600, val loss: 0.8357095122337341
Epoch 1610, training loss: 90.15213775634766 = 0.8282487392425537 + 10.0 * 8.932389259338379
Epoch 1610, val loss: 0.8329073786735535
Epoch 1620, training loss: 90.18097686767578 = 0.8253258466720581 + 10.0 * 8.935564994812012
Epoch 1620, val loss: 0.8300467729568481
Epoch 1630, training loss: 90.20691680908203 = 0.8224177956581116 + 10.0 * 8.93844985961914
Epoch 1630, val loss: 0.827203094959259
Epoch 1640, training loss: 90.18980407714844 = 0.8195395469665527 + 10.0 * 8.937026023864746
Epoch 1640, val loss: 0.8243979811668396
Epoch 1650, training loss: 90.13008117675781 = 0.816712498664856 + 10.0 * 8.931337356567383
Epoch 1650, val loss: 0.8216515183448792
Epoch 1660, training loss: 90.0827865600586 = 0.8139829039573669 + 10.0 * 8.926880836486816
Epoch 1660, val loss: 0.8189886808395386
Epoch 1670, training loss: 89.93592071533203 = 0.8111498951911926 + 10.0 * 8.912477493286133
Epoch 1670, val loss: 0.8162363171577454
Epoch 1680, training loss: 89.97371673583984 = 0.8084484934806824 + 10.0 * 8.916526794433594
Epoch 1680, val loss: 0.8136302828788757
Epoch 1690, training loss: 90.0693359375 = 0.8057330846786499 + 10.0 * 8.926360130310059
Epoch 1690, val loss: 0.8110135793685913
Epoch 1700, training loss: 90.13036346435547 = 0.803015947341919 + 10.0 * 8.932734489440918
Epoch 1700, val loss: 0.8083833456039429
Epoch 1710, training loss: 90.20481872558594 = 0.8002457618713379 + 10.0 * 8.940457344055176
Epoch 1710, val loss: 0.8056785464286804
Epoch 1720, training loss: 90.22868347167969 = 0.7975126504898071 + 10.0 * 8.943117141723633
Epoch 1720, val loss: 0.8029958009719849
Epoch 1730, training loss: 90.10145568847656 = 0.7948224544525146 + 10.0 * 8.930663108825684
Epoch 1730, val loss: 0.8004070520401001
Epoch 1740, training loss: 90.1685562133789 = 0.792178750038147 + 10.0 * 8.937638282775879
Epoch 1740, val loss: 0.7978851199150085
Epoch 1750, training loss: 90.22803497314453 = 0.7895272374153137 + 10.0 * 8.94385051727295
Epoch 1750, val loss: 0.7953060269355774
Epoch 1760, training loss: 90.2503662109375 = 0.7868739366531372 + 10.0 * 8.946349143981934
Epoch 1760, val loss: 0.7927535772323608
Epoch 1770, training loss: 90.21858978271484 = 0.7842262983322144 + 10.0 * 8.943436622619629
Epoch 1770, val loss: 0.7902296185493469
Epoch 1780, training loss: 90.26152801513672 = 0.7816720008850098 + 10.0 * 8.947985649108887
Epoch 1780, val loss: 0.7877339124679565
Epoch 1790, training loss: 90.30152893066406 = 0.7791313529014587 + 10.0 * 8.952239990234375
Epoch 1790, val loss: 0.7852741479873657
Epoch 1800, training loss: 90.32473754882812 = 0.7765676975250244 + 10.0 * 8.954816818237305
Epoch 1800, val loss: 0.7828183770179749
Epoch 1810, training loss: 90.2874984741211 = 0.7740538716316223 + 10.0 * 8.95134449005127
Epoch 1810, val loss: 0.78035569190979
Epoch 1820, training loss: 90.30845642089844 = 0.7715508937835693 + 10.0 * 8.953690528869629
Epoch 1820, val loss: 0.7779881358146667
Epoch 1830, training loss: 90.35689544677734 = 0.7690710425376892 + 10.0 * 8.958782196044922
Epoch 1830, val loss: 0.7756125926971436
Epoch 1840, training loss: 90.34236907958984 = 0.7666033506393433 + 10.0 * 8.957576751708984
Epoch 1840, val loss: 0.7732374668121338
Epoch 1850, training loss: 90.3735580444336 = 0.7641733884811401 + 10.0 * 8.960938453674316
Epoch 1850, val loss: 0.770885705947876
Epoch 1860, training loss: 90.38093566894531 = 0.7617547512054443 + 10.0 * 8.961917877197266
Epoch 1860, val loss: 0.7685319185256958
Epoch 1870, training loss: 90.3371353149414 = 0.7593621015548706 + 10.0 * 8.95777702331543
Epoch 1870, val loss: 0.7662343978881836
Epoch 1880, training loss: 90.2117919921875 = 0.7570626735687256 + 10.0 * 8.945472717285156
Epoch 1880, val loss: 0.7640403509140015
Epoch 1890, training loss: 90.25833129882812 = 0.7547891736030579 + 10.0 * 8.950353622436523
Epoch 1890, val loss: 0.7618489265441895
Epoch 1900, training loss: 90.33622741699219 = 0.7525033354759216 + 10.0 * 8.958372116088867
Epoch 1900, val loss: 0.759650468826294
Epoch 1910, training loss: 90.40252685546875 = 0.7501997351646423 + 10.0 * 8.965232849121094
Epoch 1910, val loss: 0.7574315071105957
Epoch 1920, training loss: 90.41181945800781 = 0.747913122177124 + 10.0 * 8.966390609741211
Epoch 1920, val loss: 0.7552240490913391
Epoch 1930, training loss: 90.4341049194336 = 0.7456504106521606 + 10.0 * 8.96884536743164
Epoch 1930, val loss: 0.7530542016029358
Epoch 1940, training loss: 90.42171478271484 = 0.7434386014938354 + 10.0 * 8.967827796936035
Epoch 1940, val loss: 0.7509354948997498
Epoch 1950, training loss: 90.45690155029297 = 0.7412415742874146 + 10.0 * 8.971566200256348
Epoch 1950, val loss: 0.7488032579421997
Epoch 1960, training loss: 90.46855163574219 = 0.7390629649162292 + 10.0 * 8.972949028015137
Epoch 1960, val loss: 0.7466936111450195
Epoch 1970, training loss: 90.37797546386719 = 0.7369015216827393 + 10.0 * 8.964107513427734
Epoch 1970, val loss: 0.7446320056915283
Epoch 1980, training loss: 90.3671646118164 = 0.734809398651123 + 10.0 * 8.963235855102539
Epoch 1980, val loss: 0.7426471710205078
Epoch 1990, training loss: 90.35552215576172 = 0.7327684760093689 + 10.0 * 8.962275505065918
Epoch 1990, val loss: 0.740688145160675
Epoch 2000, training loss: 90.39702606201172 = 0.7306889295578003 + 10.0 * 8.966633796691895
Epoch 2000, val loss: 0.7386483550071716
Epoch 2010, training loss: 90.4413833618164 = 0.7286611199378967 + 10.0 * 8.971272468566895
Epoch 2010, val loss: 0.7366442680358887
Epoch 2020, training loss: 90.4985122680664 = 0.7266154289245605 + 10.0 * 8.977190017700195
Epoch 2020, val loss: 0.7346847653388977
Epoch 2030, training loss: 90.49151611328125 = 0.7245857119560242 + 10.0 * 8.976693153381348
Epoch 2030, val loss: 0.7327386140823364
Epoch 2040, training loss: 90.47796630859375 = 0.7225770950317383 + 10.0 * 8.97553825378418
Epoch 2040, val loss: 0.7308082580566406
Epoch 2050, training loss: 90.48993682861328 = 0.7206175923347473 + 10.0 * 8.97693157196045
Epoch 2050, val loss: 0.7289325594902039
Epoch 2060, training loss: 90.5181655883789 = 0.7186737656593323 + 10.0 * 8.979948997497559
Epoch 2060, val loss: 0.7270692586898804
Epoch 2070, training loss: 90.53775787353516 = 0.7167285084724426 + 10.0 * 8.98210334777832
Epoch 2070, val loss: 0.7252352833747864
Epoch 2080, training loss: 90.50028228759766 = 0.7148026823997498 + 10.0 * 8.978548049926758
Epoch 2080, val loss: 0.7233755588531494
Epoch 2090, training loss: 90.51970672607422 = 0.7129120826721191 + 10.0 * 8.980679512023926
Epoch 2090, val loss: 0.7215612530708313
Epoch 2100, training loss: 90.49691009521484 = 0.7110232710838318 + 10.0 * 8.978589057922363
Epoch 2100, val loss: 0.7197657823562622
Epoch 2110, training loss: 90.5401382446289 = 0.7091965675354004 + 10.0 * 8.983094215393066
Epoch 2110, val loss: 0.7180089354515076
Epoch 2120, training loss: 90.55818939208984 = 0.7073718905448914 + 10.0 * 8.985081672668457
Epoch 2120, val loss: 0.7162758111953735
Epoch 2130, training loss: 90.49517822265625 = 0.7056357264518738 + 10.0 * 8.978954315185547
Epoch 2130, val loss: 0.7146247625350952
Epoch 2140, training loss: 90.10868072509766 = 0.7040200233459473 + 10.0 * 8.940465927124023
Epoch 2140, val loss: 0.7131924629211426
Epoch 2150, training loss: 90.25312805175781 = 0.7025473117828369 + 10.0 * 8.955058097839355
Epoch 2150, val loss: 0.7117312550544739
Epoch 2160, training loss: 90.13345336914062 = 0.7010660171508789 + 10.0 * 8.943239212036133
Epoch 2160, val loss: 0.7103186249732971
Epoch 2170, training loss: 89.96179962158203 = 0.6993669867515564 + 10.0 * 8.926243782043457
Epoch 2170, val loss: 0.7088399529457092
Epoch 2180, training loss: 90.31975555419922 = 0.6977818608283997 + 10.0 * 8.962197303771973
Epoch 2180, val loss: 0.7072861790657043
Epoch 2190, training loss: 90.2801284790039 = 0.6961956024169922 + 10.0 * 8.958393096923828
Epoch 2190, val loss: 0.7058068513870239
Epoch 2200, training loss: 90.42425537109375 = 0.6944800019264221 + 10.0 * 8.972977638244629
Epoch 2200, val loss: 0.7042002081871033
Epoch 2210, training loss: 90.48362731933594 = 0.6927810311317444 + 10.0 * 8.979084968566895
Epoch 2210, val loss: 0.7026276588439941
Epoch 2220, training loss: 90.53495788574219 = 0.69108647108078 + 10.0 * 8.984387397766113
Epoch 2220, val loss: 0.7010452747344971
Epoch 2230, training loss: 90.55268859863281 = 0.6894179582595825 + 10.0 * 8.986327171325684
Epoch 2230, val loss: 0.6994560956954956
Epoch 2240, training loss: 90.60804748535156 = 0.6878048777580261 + 10.0 * 8.992024421691895
Epoch 2240, val loss: 0.6979546546936035
Epoch 2250, training loss: 90.6430435180664 = 0.6861940622329712 + 10.0 * 8.995684623718262
Epoch 2250, val loss: 0.6964741349220276
Epoch 2260, training loss: 90.6596450805664 = 0.6845983862876892 + 10.0 * 8.997504234313965
Epoch 2260, val loss: 0.6950013041496277
Epoch 2270, training loss: 90.67960357666016 = 0.6830215454101562 + 10.0 * 8.999658584594727
Epoch 2270, val loss: 0.6935330629348755
Epoch 2280, training loss: 90.6974105834961 = 0.6814820766448975 + 10.0 * 9.001592636108398
Epoch 2280, val loss: 0.6920835375785828
Epoch 2290, training loss: 90.71129608154297 = 0.679943323135376 + 10.0 * 9.003134727478027
Epoch 2290, val loss: 0.6906662583351135
Epoch 2300, training loss: 90.67964172363281 = 0.6784197688102722 + 10.0 * 9.0001220703125
Epoch 2300, val loss: 0.6892685294151306
Epoch 2310, training loss: 90.70100402832031 = 0.6769275069236755 + 10.0 * 9.002408027648926
Epoch 2310, val loss: 0.6878722906112671
Epoch 2320, training loss: 90.72679138183594 = 0.6754394769668579 + 10.0 * 9.005135536193848
Epoch 2320, val loss: 0.6865086555480957
Epoch 2330, training loss: 90.74687957763672 = 0.6739726066589355 + 10.0 * 9.007290840148926
Epoch 2330, val loss: 0.685166597366333
Epoch 2340, training loss: 90.76172637939453 = 0.6725293397903442 + 10.0 * 9.008919715881348
Epoch 2340, val loss: 0.6838187575340271
Epoch 2350, training loss: 90.77851104736328 = 0.6710836291313171 + 10.0 * 9.010743141174316
Epoch 2350, val loss: 0.6824849843978882
Epoch 2360, training loss: 90.81828308105469 = 0.6696456670761108 + 10.0 * 9.014863967895508
Epoch 2360, val loss: 0.6811566948890686
Epoch 2370, training loss: 90.78311157226562 = 0.6682170629501343 + 10.0 * 9.011488914489746
Epoch 2370, val loss: 0.6798567175865173
Epoch 2380, training loss: 90.78626251220703 = 0.6668214797973633 + 10.0 * 9.011943817138672
Epoch 2380, val loss: 0.678565263748169
Epoch 2390, training loss: 90.78211212158203 = 0.6654565930366516 + 10.0 * 9.011665344238281
Epoch 2390, val loss: 0.6773377656936646
Epoch 2400, training loss: 90.76395416259766 = 0.664079487323761 + 10.0 * 9.009987831115723
Epoch 2400, val loss: 0.6761404275894165
Epoch 2410, training loss: 90.81978607177734 = 0.6627038717269897 + 10.0 * 9.015707969665527
Epoch 2410, val loss: 0.6749042272567749
Epoch 2420, training loss: 90.853759765625 = 0.6613187789916992 + 10.0 * 9.019244194030762
Epoch 2420, val loss: 0.6736435890197754
Epoch 2430, training loss: 90.80552673339844 = 0.6599517464637756 + 10.0 * 9.014557838439941
Epoch 2430, val loss: 0.6724079251289368
Epoch 2440, training loss: 90.8490219116211 = 0.6585977673530579 + 10.0 * 9.019042015075684
Epoch 2440, val loss: 0.6711764931678772
Epoch 2450, training loss: 90.86322784423828 = 0.6572532057762146 + 10.0 * 9.020597457885742
Epoch 2450, val loss: 0.6699823141098022
Epoch 2460, training loss: 90.8809814453125 = 0.6559244394302368 + 10.0 * 9.022505760192871
Epoch 2460, val loss: 0.6687974333763123
Epoch 2470, training loss: 90.89836120605469 = 0.6546081900596619 + 10.0 * 9.024374961853027
Epoch 2470, val loss: 0.6676151156425476
Epoch 2480, training loss: 90.8954086303711 = 0.6532899141311646 + 10.0 * 9.024211883544922
Epoch 2480, val loss: 0.666472852230072
Epoch 2490, training loss: 90.9152603149414 = 0.652000367641449 + 10.0 * 9.026326179504395
Epoch 2490, val loss: 0.6653187870979309
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7275362318840579
0.8637977251322178
=== training gcn model ===
Epoch 0, training loss: 104.31389617919922 = 1.1019961833953857 + 10.0 * 10.321189880371094
Epoch 0, val loss: 1.1022762060165405
Epoch 10, training loss: 99.86196899414062 = 1.1014097929000854 + 10.0 * 9.876055717468262
Epoch 10, val loss: 1.1017147302627563
Epoch 20, training loss: 97.67981719970703 = 1.1009325981140137 + 10.0 * 9.657888412475586
Epoch 20, val loss: 1.1012587547302246
Epoch 30, training loss: 96.16722106933594 = 1.1004316806793213 + 10.0 * 9.506678581237793
Epoch 30, val loss: 1.1007742881774902
Epoch 40, training loss: 94.96939086914062 = 1.0999085903167725 + 10.0 * 9.386948585510254
Epoch 40, val loss: 1.100276231765747
Epoch 50, training loss: 93.99253845214844 = 1.0993831157684326 + 10.0 * 9.289316177368164
Epoch 50, val loss: 1.099778175354004
Epoch 60, training loss: 93.17822265625 = 1.0988601446151733 + 10.0 * 9.20793628692627
Epoch 60, val loss: 1.0992834568023682
Epoch 70, training loss: 92.48916625976562 = 1.0983302593231201 + 10.0 * 9.139083862304688
Epoch 70, val loss: 1.0987735986709595
Epoch 80, training loss: 91.91136169433594 = 1.0975990295410156 + 10.0 * 9.081376075744629
Epoch 80, val loss: 1.098013162612915
Epoch 90, training loss: 91.40694427490234 = 1.096696138381958 + 10.0 * 9.031024932861328
Epoch 90, val loss: 1.0971072912216187
Epoch 100, training loss: 90.9769058227539 = 1.095772385597229 + 10.0 * 8.988113403320312
Epoch 100, val loss: 1.096184253692627
Epoch 110, training loss: 90.61119079589844 = 1.0948259830474854 + 10.0 * 8.95163631439209
Epoch 110, val loss: 1.0952378511428833
Epoch 120, training loss: 90.27265930175781 = 1.0938842296600342 + 10.0 * 8.917877197265625
Epoch 120, val loss: 1.0943071842193604
Epoch 130, training loss: 89.97891998291016 = 1.0929272174835205 + 10.0 * 8.888599395751953
Epoch 130, val loss: 1.0933551788330078
Epoch 140, training loss: 89.71442413330078 = 1.0919462442398071 + 10.0 * 8.862247467041016
Epoch 140, val loss: 1.0923877954483032
Epoch 150, training loss: 89.48667907714844 = 1.0909740924835205 + 10.0 * 8.839570999145508
Epoch 150, val loss: 1.0914220809936523
Epoch 160, training loss: 89.29103088378906 = 1.089978814125061 + 10.0 * 8.820104598999023
Epoch 160, val loss: 1.0904526710510254
Epoch 170, training loss: 89.14013671875 = 1.089031457901001 + 10.0 * 8.805109977722168
Epoch 170, val loss: 1.0895215272903442
Epoch 180, training loss: 88.99491882324219 = 1.0881129503250122 + 10.0 * 8.790680885314941
Epoch 180, val loss: 1.0886318683624268
Epoch 190, training loss: 88.88309478759766 = 1.0872706174850464 + 10.0 * 8.779582023620605
Epoch 190, val loss: 1.0878115892410278
Epoch 200, training loss: 88.79928588867188 = 1.0864919424057007 + 10.0 * 8.771279335021973
Epoch 200, val loss: 1.087045431137085
Epoch 210, training loss: 88.72251892089844 = 1.0857359170913696 + 10.0 * 8.763677597045898
Epoch 210, val loss: 1.0863016843795776
Epoch 220, training loss: 88.69969940185547 = 1.0850344896316528 + 10.0 * 8.761466026306152
Epoch 220, val loss: 1.085628867149353
Epoch 230, training loss: 88.52970886230469 = 1.0842876434326172 + 10.0 * 8.744542121887207
Epoch 230, val loss: 1.0849062204360962
Epoch 240, training loss: 88.51951599121094 = 1.083573818206787 + 10.0 * 8.7435941696167
Epoch 240, val loss: 1.0842063426971436
Epoch 250, training loss: 88.4621353149414 = 1.0828218460083008 + 10.0 * 8.737931251525879
Epoch 250, val loss: 1.0834825038909912
Epoch 260, training loss: 88.40359497070312 = 1.0820521116256714 + 10.0 * 8.73215389251709
Epoch 260, val loss: 1.0827345848083496
Epoch 270, training loss: 88.38336944580078 = 1.0812714099884033 + 10.0 * 8.730210304260254
Epoch 270, val loss: 1.0819824934005737
Epoch 280, training loss: 88.40615844726562 = 1.0804510116577148 + 10.0 * 8.73257064819336
Epoch 280, val loss: 1.0811876058578491
Epoch 290, training loss: 88.37390899658203 = 1.0796133279800415 + 10.0 * 8.729429244995117
Epoch 290, val loss: 1.080353856086731
Epoch 300, training loss: 88.28453063964844 = 1.0788249969482422 + 10.0 * 8.72057056427002
Epoch 300, val loss: 1.0796160697937012
Epoch 310, training loss: 88.21916961669922 = 1.0779087543487549 + 10.0 * 8.714125633239746
Epoch 310, val loss: 1.0787315368652344
Epoch 320, training loss: 88.25276184082031 = 1.0770291090011597 + 10.0 * 8.717573165893555
Epoch 320, val loss: 1.0778753757476807
Epoch 330, training loss: 88.2398452758789 = 1.076116681098938 + 10.0 * 8.7163724899292
Epoch 330, val loss: 1.0769925117492676
Epoch 340, training loss: 88.27351379394531 = 1.0751718282699585 + 10.0 * 8.719834327697754
Epoch 340, val loss: 1.0760786533355713
Epoch 350, training loss: 88.27240753173828 = 1.0741894245147705 + 10.0 * 8.71982192993164
Epoch 350, val loss: 1.0751352310180664
Epoch 360, training loss: 88.2713394165039 = 1.0731905698776245 + 10.0 * 8.719815254211426
Epoch 360, val loss: 1.0741697549819946
Epoch 370, training loss: 88.3169937133789 = 1.0721633434295654 + 10.0 * 8.724482536315918
Epoch 370, val loss: 1.0731606483459473
Epoch 380, training loss: 88.31536865234375 = 1.0711193084716797 + 10.0 * 8.724424362182617
Epoch 380, val loss: 1.0721458196640015
Epoch 390, training loss: 88.29232025146484 = 1.0700284242630005 + 10.0 * 8.72222900390625
Epoch 390, val loss: 1.0711325407028198
Epoch 400, training loss: 88.2826156616211 = 1.0689046382904053 + 10.0 * 8.7213716506958
Epoch 400, val loss: 1.070043921470642
Epoch 410, training loss: 88.30146789550781 = 1.0677932500839233 + 10.0 * 8.723367691040039
Epoch 410, val loss: 1.0689703226089478
Epoch 420, training loss: 88.2686996459961 = 1.0665901899337769 + 10.0 * 8.720211029052734
Epoch 420, val loss: 1.0678188800811768
Epoch 430, training loss: 88.34332275390625 = 1.0654524564743042 + 10.0 * 8.727787017822266
Epoch 430, val loss: 1.0667054653167725
Epoch 440, training loss: 88.41178131103516 = 1.0642873048782349 + 10.0 * 8.734749794006348
Epoch 440, val loss: 1.065588355064392
Epoch 450, training loss: 88.3769760131836 = 1.063028335571289 + 10.0 * 8.73139476776123
Epoch 450, val loss: 1.0643882751464844
Epoch 460, training loss: 88.37689208984375 = 1.061749815940857 + 10.0 * 8.731513977050781
Epoch 460, val loss: 1.0631649494171143
Epoch 470, training loss: 88.42789459228516 = 1.0604768991470337 + 10.0 * 8.73674201965332
Epoch 470, val loss: 1.061922311782837
Epoch 480, training loss: 88.40138244628906 = 1.0590996742248535 + 10.0 * 8.734228134155273
Epoch 480, val loss: 1.0606142282485962
Epoch 490, training loss: 88.38458251953125 = 1.0577540397644043 + 10.0 * 8.732683181762695
Epoch 490, val loss: 1.0593032836914062
Epoch 500, training loss: 88.4588851928711 = 1.056432843208313 + 10.0 * 8.74024486541748
Epoch 500, val loss: 1.0580353736877441
Epoch 510, training loss: 88.47055053710938 = 1.0550222396850586 + 10.0 * 8.741552352905273
Epoch 510, val loss: 1.0566948652267456
Epoch 520, training loss: 88.54643249511719 = 1.0535917282104492 + 10.0 * 8.749284744262695
Epoch 520, val loss: 1.0553067922592163
Epoch 530, training loss: 88.55379486083984 = 1.0521186590194702 + 10.0 * 8.750167846679688
Epoch 530, val loss: 1.0539047718048096
Epoch 540, training loss: 88.61688232421875 = 1.05056893825531 + 10.0 * 8.756631851196289
Epoch 540, val loss: 1.052427053451538
Epoch 550, training loss: 88.50691223144531 = 1.0490689277648926 + 10.0 * 8.745783805847168
Epoch 550, val loss: 1.0509804487228394
Epoch 560, training loss: 88.55294036865234 = 1.047516942024231 + 10.0 * 8.750542640686035
Epoch 560, val loss: 1.049505591392517
Epoch 570, training loss: 88.58057403564453 = 1.0458958148956299 + 10.0 * 8.753467559814453
Epoch 570, val loss: 1.0479426383972168
Epoch 580, training loss: 88.66447448730469 = 1.0442700386047363 + 10.0 * 8.762020111083984
Epoch 580, val loss: 1.0463907718658447
Epoch 590, training loss: 88.70879364013672 = 1.0425870418548584 + 10.0 * 8.766620635986328
Epoch 590, val loss: 1.0447828769683838
Epoch 600, training loss: 88.59199523925781 = 1.0407390594482422 + 10.0 * 8.755125045776367
Epoch 600, val loss: 1.0429941415786743
Epoch 610, training loss: 88.6987075805664 = 1.039069652557373 + 10.0 * 8.765963554382324
Epoch 610, val loss: 1.0413883924484253
Epoch 620, training loss: 88.7625732421875 = 1.037161111831665 + 10.0 * 8.772541046142578
Epoch 620, val loss: 1.0395814180374146
Epoch 630, training loss: 88.65840911865234 = 1.0352007150650024 + 10.0 * 8.762320518493652
Epoch 630, val loss: 1.0377230644226074
Epoch 640, training loss: 88.634765625 = 1.0332649946212769 + 10.0 * 8.760149955749512
Epoch 640, val loss: 1.0358096361160278
Epoch 650, training loss: 88.72337341308594 = 1.0313236713409424 + 10.0 * 8.769205093383789
Epoch 650, val loss: 1.0339583158493042
Epoch 660, training loss: 88.76945495605469 = 1.0292139053344727 + 10.0 * 8.77402400970459
Epoch 660, val loss: 1.0319522619247437
Epoch 670, training loss: 88.79782104492188 = 1.0270601511001587 + 10.0 * 8.77707576751709
Epoch 670, val loss: 1.0298736095428467
Epoch 680, training loss: 88.86141967773438 = 1.024900197982788 + 10.0 * 8.783651351928711
Epoch 680, val loss: 1.0277811288833618
Epoch 690, training loss: 88.85137176513672 = 1.0226119756698608 + 10.0 * 8.782876014709473
Epoch 690, val loss: 1.0255969762802124
Epoch 700, training loss: 88.86373138427734 = 1.02029287815094 + 10.0 * 8.784343719482422
Epoch 700, val loss: 1.02336847782135
Epoch 710, training loss: 88.94441223144531 = 1.0179102420806885 + 10.0 * 8.79265022277832
Epoch 710, val loss: 1.0210858583450317
Epoch 720, training loss: 88.90180969238281 = 1.015419363975525 + 10.0 * 8.788639068603516
Epoch 720, val loss: 1.0186762809753418
Epoch 730, training loss: 88.91972351074219 = 1.0128434896469116 + 10.0 * 8.790688514709473
Epoch 730, val loss: 1.016207218170166
Epoch 740, training loss: 88.97391510009766 = 1.0102022886276245 + 10.0 * 8.796371459960938
Epoch 740, val loss: 1.0136827230453491
Epoch 750, training loss: 89.0355453491211 = 1.0074790716171265 + 10.0 * 8.802806854248047
Epoch 750, val loss: 1.011046051979065
Epoch 760, training loss: 88.99578094482422 = 1.0046298503875732 + 10.0 * 8.799115180969238
Epoch 760, val loss: 1.0082954168319702
Epoch 770, training loss: 89.009521484375 = 1.00174081325531 + 10.0 * 8.80077838897705
Epoch 770, val loss: 1.0054999589920044
Epoch 780, training loss: 89.0498275756836 = 0.9987733364105225 + 10.0 * 8.805105209350586
Epoch 780, val loss: 1.002657175064087
Epoch 790, training loss: 89.0871810913086 = 0.995709240436554 + 10.0 * 8.809146881103516
Epoch 790, val loss: 0.9996966123580933
Epoch 800, training loss: 89.1104507446289 = 0.9925492405891418 + 10.0 * 8.811790466308594
Epoch 800, val loss: 0.9966782927513123
Epoch 810, training loss: 89.13046264648438 = 0.989370584487915 + 10.0 * 8.814108848571777
Epoch 810, val loss: 0.9935991764068604
Epoch 820, training loss: 89.08770751953125 = 0.9860665798187256 + 10.0 * 8.810163497924805
Epoch 820, val loss: 0.9904502630233765
Epoch 830, training loss: 89.0937728881836 = 0.9827364087104797 + 10.0 * 8.811103820800781
Epoch 830, val loss: 0.9872438907623291
Epoch 840, training loss: 89.1626968383789 = 0.9793925881385803 + 10.0 * 8.818330764770508
Epoch 840, val loss: 0.9840320944786072
Epoch 850, training loss: 89.217041015625 = 0.9759572744369507 + 10.0 * 8.824108123779297
Epoch 850, val loss: 0.9807392954826355
Epoch 860, training loss: 89.02445220947266 = 0.9723803400993347 + 10.0 * 8.805207252502441
Epoch 860, val loss: 0.9772750735282898
Epoch 870, training loss: 89.19300842285156 = 0.9689167141914368 + 10.0 * 8.822408676147461
Epoch 870, val loss: 0.973950982093811
Epoch 880, training loss: 89.18314361572266 = 0.965245246887207 + 10.0 * 8.821789741516113
Epoch 880, val loss: 0.9704863429069519
Epoch 890, training loss: 88.98793029785156 = 0.9615328311920166 + 10.0 * 8.802639961242676
Epoch 890, val loss: 0.9668895602226257
Epoch 900, training loss: 89.05352020263672 = 0.9577681422233582 + 10.0 * 8.809575080871582
Epoch 900, val loss: 0.9632754325866699
Epoch 910, training loss: 89.14173126220703 = 0.9539318084716797 + 10.0 * 8.818779945373535
Epoch 910, val loss: 0.9595978856086731
Epoch 920, training loss: 89.21390533447266 = 0.9500604271888733 + 10.0 * 8.826384544372559
Epoch 920, val loss: 0.9558864235877991
Epoch 930, training loss: 89.26099395751953 = 0.9460669755935669 + 10.0 * 8.83149242401123
Epoch 930, val loss: 0.9520524144172668
Epoch 940, training loss: 89.31645202636719 = 0.9419896602630615 + 10.0 * 8.837446212768555
Epoch 940, val loss: 0.9481298923492432
Epoch 950, training loss: 89.3650131225586 = 0.937798261642456 + 10.0 * 8.842721939086914
Epoch 950, val loss: 0.9441213011741638
Epoch 960, training loss: 89.38227081298828 = 0.9335180521011353 + 10.0 * 8.84487533569336
Epoch 960, val loss: 0.9400299787521362
Epoch 970, training loss: 89.42920684814453 = 0.9291918873786926 + 10.0 * 8.850001335144043
Epoch 970, val loss: 0.9358765482902527
Epoch 980, training loss: 89.43173217773438 = 0.9247295260429382 + 10.0 * 8.850700378417969
Epoch 980, val loss: 0.9316184520721436
Epoch 990, training loss: 89.51982879638672 = 0.9202796816825867 + 10.0 * 8.859954833984375
Epoch 990, val loss: 0.9273996949195862
Epoch 1000, training loss: 89.50137329101562 = 0.9157472252845764 + 10.0 * 8.858562469482422
Epoch 1000, val loss: 0.9230539798736572
Epoch 1010, training loss: 89.56594848632812 = 0.9110927581787109 + 10.0 * 8.865485191345215
Epoch 1010, val loss: 0.9186023473739624
Epoch 1020, training loss: 89.57718658447266 = 0.9063021540641785 + 10.0 * 8.867088317871094
Epoch 1020, val loss: 0.9140145778656006
Epoch 1030, training loss: 89.60443115234375 = 0.9015102386474609 + 10.0 * 8.870291709899902
Epoch 1030, val loss: 0.9094507694244385
Epoch 1040, training loss: 89.45292663574219 = 0.8965422511100769 + 10.0 * 8.85563850402832
Epoch 1040, val loss: 0.9046928286552429
Epoch 1050, training loss: 89.54622650146484 = 0.8917481303215027 + 10.0 * 8.865447998046875
Epoch 1050, val loss: 0.900114119052887
Epoch 1060, training loss: 89.48001861572266 = 0.8868040442466736 + 10.0 * 8.859321594238281
Epoch 1060, val loss: 0.8954522609710693
Epoch 1070, training loss: 89.47997283935547 = 0.8816825747489929 + 10.0 * 8.85982894897461
Epoch 1070, val loss: 0.8905225992202759
Epoch 1080, training loss: 89.57054138183594 = 0.8765861392021179 + 10.0 * 8.86939525604248
Epoch 1080, val loss: 0.8856553435325623
Epoch 1090, training loss: 89.63392639160156 = 0.8713762164115906 + 10.0 * 8.87625503540039
Epoch 1090, val loss: 0.8806626200675964
Epoch 1100, training loss: 89.62721252441406 = 0.8661146759986877 + 10.0 * 8.876110076904297
Epoch 1100, val loss: 0.8756307363510132
Epoch 1110, training loss: 89.70264434814453 = 0.8607807159423828 + 10.0 * 8.884186744689941
Epoch 1110, val loss: 0.8705648183822632
Epoch 1120, training loss: 89.75836944580078 = 0.8553904294967651 + 10.0 * 8.890297889709473
Epoch 1120, val loss: 0.8654003739356995
Epoch 1130, training loss: 89.7605972290039 = 0.8499295115470886 + 10.0 * 8.891066551208496
Epoch 1130, val loss: 0.8601969480514526
Epoch 1140, training loss: 89.79637145996094 = 0.8444240093231201 + 10.0 * 8.895195007324219
Epoch 1140, val loss: 0.8549704551696777
Epoch 1150, training loss: 89.64726257324219 = 0.8383051156997681 + 10.0 * 8.880895614624023
Epoch 1150, val loss: 0.8489492535591125
Epoch 1160, training loss: 88.14373016357422 = 0.8319177031517029 + 10.0 * 8.731181144714355
Epoch 1160, val loss: 0.8431978225708008
Epoch 1170, training loss: 88.1080093383789 = 0.8281532526016235 + 10.0 * 8.727985382080078
Epoch 1170, val loss: 0.8397431373596191
Epoch 1180, training loss: 88.85520935058594 = 0.8233329057693481 + 10.0 * 8.803187370300293
Epoch 1180, val loss: 0.8349026441574097
Epoch 1190, training loss: 89.03941345214844 = 0.8173781633377075 + 10.0 * 8.822203636169434
Epoch 1190, val loss: 0.8292823433876038
Epoch 1200, training loss: 89.35951232910156 = 0.8118758201599121 + 10.0 * 8.854763984680176
Epoch 1200, val loss: 0.8240780830383301
Epoch 1210, training loss: 89.19092559814453 = 0.8059453964233398 + 10.0 * 8.83849811553955
Epoch 1210, val loss: 0.8185021281242371
Epoch 1220, training loss: 89.48294830322266 = 0.8003326058387756 + 10.0 * 8.868261337280273
Epoch 1220, val loss: 0.8131792545318604
Epoch 1230, training loss: 89.54547119140625 = 0.7945932149887085 + 10.0 * 8.87508773803711
Epoch 1230, val loss: 0.8077506422996521
Epoch 1240, training loss: 89.61438751220703 = 0.7888385057449341 + 10.0 * 8.88255500793457
Epoch 1240, val loss: 0.8022904992103577
Epoch 1250, training loss: 89.65982055664062 = 0.7829834222793579 + 10.0 * 8.887683868408203
Epoch 1250, val loss: 0.7967275381088257
Epoch 1260, training loss: 89.75658416748047 = 0.7773858308792114 + 10.0 * 8.897919654846191
Epoch 1260, val loss: 0.7914518117904663
Epoch 1270, training loss: 89.81222534179688 = 0.7716569304466248 + 10.0 * 8.904056549072266
Epoch 1270, val loss: 0.7860163450241089
Epoch 1280, training loss: 89.8558578491211 = 0.7659018039703369 + 10.0 * 8.908995628356934
Epoch 1280, val loss: 0.7805668711662292
Epoch 1290, training loss: 89.90016174316406 = 0.7601386904716492 + 10.0 * 8.914002418518066
Epoch 1290, val loss: 0.7751080393791199
Epoch 1300, training loss: 89.91410827636719 = 0.7543679475784302 + 10.0 * 8.915973663330078
Epoch 1300, val loss: 0.7696317434310913
Epoch 1310, training loss: 89.8946762084961 = 0.7486187219619751 + 10.0 * 8.914606094360352
Epoch 1310, val loss: 0.7642168402671814
Epoch 1320, training loss: 89.96612548828125 = 0.742940366268158 + 10.0 * 8.922318458557129
Epoch 1320, val loss: 0.7588236927986145
Epoch 1330, training loss: 89.97679901123047 = 0.7372100949287415 + 10.0 * 8.923958778381348
Epoch 1330, val loss: 0.7534214854240417
Epoch 1340, training loss: 90.00992584228516 = 0.7315447926521301 + 10.0 * 8.927838325500488
Epoch 1340, val loss: 0.7480722665786743
Epoch 1350, training loss: 90.00222778320312 = 0.725909948348999 + 10.0 * 8.927632331848145
Epoch 1350, val loss: 0.742743968963623
Epoch 1360, training loss: 90.02533721923828 = 0.7203628420829773 + 10.0 * 8.930498123168945
Epoch 1360, val loss: 0.7374831438064575
Epoch 1370, training loss: 90.07522583007812 = 0.7148085832595825 + 10.0 * 8.936041831970215
Epoch 1370, val loss: 0.732254147529602
Epoch 1380, training loss: 90.08235168457031 = 0.7094091773033142 + 10.0 * 8.937294006347656
Epoch 1380, val loss: 0.7271518111228943
Epoch 1390, training loss: 90.12384796142578 = 0.7040762901306152 + 10.0 * 8.941976547241211
Epoch 1390, val loss: 0.7220581769943237
Epoch 1400, training loss: 90.0995864868164 = 0.6986309885978699 + 10.0 * 8.940095901489258
Epoch 1400, val loss: 0.7169502973556519
Epoch 1410, training loss: 90.14127349853516 = 0.6932907104492188 + 10.0 * 8.944798469543457
Epoch 1410, val loss: 0.7119483351707458
Epoch 1420, training loss: 90.16653442382812 = 0.6880011558532715 + 10.0 * 8.947853088378906
Epoch 1420, val loss: 0.7069485783576965
Epoch 1430, training loss: 90.18231964111328 = 0.6827417612075806 + 10.0 * 8.949957847595215
Epoch 1430, val loss: 0.7019633650779724
Epoch 1440, training loss: 89.76102447509766 = 0.6773923635482788 + 10.0 * 8.908363342285156
Epoch 1440, val loss: 0.6969230771064758
Epoch 1450, training loss: 90.02974700927734 = 0.6727396249771118 + 10.0 * 8.935701370239258
Epoch 1450, val loss: 0.6925321817398071
Epoch 1460, training loss: 89.98938751220703 = 0.6677590012550354 + 10.0 * 8.932162284851074
Epoch 1460, val loss: 0.6879008412361145
Epoch 1470, training loss: 90.02008819580078 = 0.6627218127250671 + 10.0 * 8.935736656188965
Epoch 1470, val loss: 0.6831106543540955
Epoch 1480, training loss: 90.17678833007812 = 0.6579160094261169 + 10.0 * 8.951887130737305
Epoch 1480, val loss: 0.6785671710968018
Epoch 1490, training loss: 90.20100402832031 = 0.6530501842498779 + 10.0 * 8.954794883728027
Epoch 1490, val loss: 0.6740021705627441
Epoch 1500, training loss: 90.2618179321289 = 0.64825040102005 + 10.0 * 8.961357116699219
Epoch 1500, val loss: 0.6694821715354919
Epoch 1510, training loss: 90.29206848144531 = 0.6435093879699707 + 10.0 * 8.964856147766113
Epoch 1510, val loss: 0.6650323867797852
Epoch 1520, training loss: 90.3171157836914 = 0.638836145401001 + 10.0 * 8.967827796936035
Epoch 1520, val loss: 0.6606566905975342
Epoch 1530, training loss: 90.14398193359375 = 0.6342594027519226 + 10.0 * 8.950971603393555
Epoch 1530, val loss: 0.6563698649406433
Epoch 1540, training loss: 90.22894287109375 = 0.6297053098678589 + 10.0 * 8.95992374420166
Epoch 1540, val loss: 0.6521031260490417
Epoch 1550, training loss: 90.2822494506836 = 0.6252821087837219 + 10.0 * 8.965696334838867
Epoch 1550, val loss: 0.6479350924491882
Epoch 1560, training loss: 90.36216735839844 = 0.6208397746086121 + 10.0 * 8.974132537841797
Epoch 1560, val loss: 0.6438042521476746
Epoch 1570, training loss: 90.39077758789062 = 0.6164699196815491 + 10.0 * 8.97743034362793
Epoch 1570, val loss: 0.6397305727005005
Epoch 1580, training loss: 90.38294982910156 = 0.61217200756073 + 10.0 * 8.97707748413086
Epoch 1580, val loss: 0.6357275247573853
Epoch 1590, training loss: 90.42134094238281 = 0.6079670190811157 + 10.0 * 8.981337547302246
Epoch 1590, val loss: 0.6318073868751526
Epoch 1600, training loss: 90.34453582763672 = 0.6038400530815125 + 10.0 * 8.974069595336914
Epoch 1600, val loss: 0.6279110908508301
Epoch 1610, training loss: 90.37390899658203 = 0.5999970436096191 + 10.0 * 8.977391242980957
Epoch 1610, val loss: 0.6244254112243652
Epoch 1620, training loss: 90.25775146484375 = 0.5960453152656555 + 10.0 * 8.966170310974121
Epoch 1620, val loss: 0.6206653714179993
Epoch 1630, training loss: 90.29317474365234 = 0.5921124219894409 + 10.0 * 8.97010612487793
Epoch 1630, val loss: 0.6170119643211365
Epoch 1640, training loss: 90.32510375976562 = 0.5883036255836487 + 10.0 * 8.97368049621582
Epoch 1640, val loss: 0.613410234451294
Epoch 1650, training loss: 90.37813568115234 = 0.5845085978507996 + 10.0 * 8.979362487792969
Epoch 1650, val loss: 0.6098445653915405
Epoch 1660, training loss: 90.4202880859375 = 0.580744206905365 + 10.0 * 8.983954429626465
Epoch 1660, val loss: 0.6063394546508789
Epoch 1670, training loss: 90.45063781738281 = 0.5770493149757385 + 10.0 * 8.987359046936035
Epoch 1670, val loss: 0.6029077172279358
Epoch 1680, training loss: 90.4860610961914 = 0.5734331607818604 + 10.0 * 8.991262435913086
Epoch 1680, val loss: 0.599534273147583
Epoch 1690, training loss: 90.49052429199219 = 0.5698795318603516 + 10.0 * 8.992064476013184
Epoch 1690, val loss: 0.5962339043617249
Epoch 1700, training loss: 90.50121307373047 = 0.5664054155349731 + 10.0 * 8.993480682373047
Epoch 1700, val loss: 0.5930067300796509
Epoch 1710, training loss: 90.50953674316406 = 0.5629783272743225 + 10.0 * 8.99465560913086
Epoch 1710, val loss: 0.5898318290710449
Epoch 1720, training loss: 90.54537963867188 = 0.5595851540565491 + 10.0 * 8.998579025268555
Epoch 1720, val loss: 0.5866852402687073
Epoch 1730, training loss: 90.5726089477539 = 0.5562681555747986 + 10.0 * 9.00163459777832
Epoch 1730, val loss: 0.5836079716682434
Epoch 1740, training loss: 90.57169342041016 = 0.5530140995979309 + 10.0 * 9.001867294311523
Epoch 1740, val loss: 0.5806146860122681
Epoch 1750, training loss: 90.58138275146484 = 0.5498244166374207 + 10.0 * 9.003155708312988
Epoch 1750, val loss: 0.5776808261871338
Epoch 1760, training loss: 90.60900115966797 = 0.5467060804367065 + 10.0 * 9.006229400634766
Epoch 1760, val loss: 0.5748238563537598
Epoch 1770, training loss: 90.63610076904297 = 0.5436483025550842 + 10.0 * 9.009244918823242
Epoch 1770, val loss: 0.5720106959342957
Epoch 1780, training loss: 90.6335220336914 = 0.5406583547592163 + 10.0 * 9.009286880493164
Epoch 1780, val loss: 0.5692795515060425
Epoch 1790, training loss: 90.62431335449219 = 0.5377626419067383 + 10.0 * 9.008654594421387
Epoch 1790, val loss: 0.566620409488678
Epoch 1800, training loss: 90.63162231445312 = 0.5348915457725525 + 10.0 * 9.009673118591309
Epoch 1800, val loss: 0.563997209072113
Epoch 1810, training loss: 90.66883087158203 = 0.5320826172828674 + 10.0 * 9.01367473602295
Epoch 1810, val loss: 0.5614441633224487
Epoch 1820, training loss: 90.68048858642578 = 0.5293225049972534 + 10.0 * 9.015116691589355
Epoch 1820, val loss: 0.5589298009872437
Epoch 1830, training loss: 90.69722747802734 = 0.5266244411468506 + 10.0 * 9.017060279846191
Epoch 1830, val loss: 0.5564669966697693
Epoch 1840, training loss: 90.6850814819336 = 0.5239430665969849 + 10.0 * 9.016114234924316
Epoch 1840, val loss: 0.5540635585784912
Epoch 1850, training loss: 90.69012451171875 = 0.5213689208030701 + 10.0 * 9.016875267028809
Epoch 1850, val loss: 0.5517369508743286
Epoch 1860, training loss: 90.72299194335938 = 0.5188884139060974 + 10.0 * 9.020410537719727
Epoch 1860, val loss: 0.5494397282600403
Epoch 1870, training loss: 90.7376480102539 = 0.5164127349853516 + 10.0 * 9.022123336791992
Epoch 1870, val loss: 0.5472354888916016
Epoch 1880, training loss: 90.71503448486328 = 0.5140160322189331 + 10.0 * 9.020101547241211
Epoch 1880, val loss: 0.5450807809829712
Epoch 1890, training loss: 90.74475860595703 = 0.511642336845398 + 10.0 * 9.023311614990234
Epoch 1890, val loss: 0.542987585067749
Epoch 1900, training loss: 90.7922134399414 = 0.509374737739563 + 10.0 * 9.028284072875977
Epoch 1900, val loss: 0.540911853313446
Epoch 1910, training loss: 90.77063751220703 = 0.5071028470993042 + 10.0 * 9.02635383605957
Epoch 1910, val loss: 0.5389188528060913
Epoch 1920, training loss: 90.77308654785156 = 0.5049070715904236 + 10.0 * 9.02681827545166
Epoch 1920, val loss: 0.5369531512260437
Epoch 1930, training loss: 90.79186248779297 = 0.5027062892913818 + 10.0 * 9.028915405273438
Epoch 1930, val loss: 0.5350305438041687
Epoch 1940, training loss: 90.81491088867188 = 0.5005504488945007 + 10.0 * 9.0314359664917
Epoch 1940, val loss: 0.5331434607505798
Epoch 1950, training loss: 90.79757690429688 = 0.49846819043159485 + 10.0 * 9.029911041259766
Epoch 1950, val loss: 0.5313201546669006
Epoch 1960, training loss: 90.78841400146484 = 0.4964098036289215 + 10.0 * 9.029200553894043
Epoch 1960, val loss: 0.5295137166976929
Epoch 1970, training loss: 90.82568359375 = 0.49441292881965637 + 10.0 * 9.033126831054688
Epoch 1970, val loss: 0.5277710556983948
Epoch 1980, training loss: 90.85326385498047 = 0.4924267530441284 + 10.0 * 9.036084175109863
Epoch 1980, val loss: 0.5260631442070007
Epoch 1990, training loss: 90.81173706054688 = 0.4904933273792267 + 10.0 * 9.032124519348145
Epoch 1990, val loss: 0.5243707299232483
Epoch 2000, training loss: 90.84163665771484 = 0.4886104464530945 + 10.0 * 9.035303115844727
Epoch 2000, val loss: 0.5227634310722351
Epoch 2010, training loss: 90.87017059326172 = 0.4867473542690277 + 10.0 * 9.038342475891113
Epoch 2010, val loss: 0.5211607813835144
Epoch 2020, training loss: 90.88443756103516 = 0.4849247634410858 + 10.0 * 9.03995132446289
Epoch 2020, val loss: 0.5195948481559753
Epoch 2030, training loss: 90.84551239013672 = 0.4831470847129822 + 10.0 * 9.036236763000488
Epoch 2030, val loss: 0.5180731415748596
Epoch 2040, training loss: 90.88784790039062 = 0.48141732811927795 + 10.0 * 9.040643692016602
Epoch 2040, val loss: 0.5166016817092896
Epoch 2050, training loss: 90.91130065917969 = 0.4797084331512451 + 10.0 * 9.043159484863281
Epoch 2050, val loss: 0.5151649713516235
Epoch 2060, training loss: 90.87715911865234 = 0.478017121553421 + 10.0 * 9.03991413116455
Epoch 2060, val loss: 0.5137554407119751
Epoch 2070, training loss: 90.76744079589844 = 0.47647204995155334 + 10.0 * 9.029096603393555
Epoch 2070, val loss: 0.512473464012146
Epoch 2080, training loss: 90.75663757324219 = 0.47496703267097473 + 10.0 * 9.028166770935059
Epoch 2080, val loss: 0.5112226009368896
Epoch 2090, training loss: 90.80767059326172 = 0.4734308421611786 + 10.0 * 9.03342342376709
Epoch 2090, val loss: 0.5099350214004517
Epoch 2100, training loss: 90.82207489013672 = 0.47194722294807434 + 10.0 * 9.035013198852539
Epoch 2100, val loss: 0.5087143778800964
Epoch 2110, training loss: 90.80755615234375 = 0.4704318046569824 + 10.0 * 9.033712387084961
Epoch 2110, val loss: 0.507480800151825
Epoch 2120, training loss: 90.85526275634766 = 0.4689161777496338 + 10.0 * 9.038634300231934
Epoch 2120, val loss: 0.5062598586082458
Epoch 2130, training loss: 90.82342529296875 = 0.4674341678619385 + 10.0 * 9.035598754882812
Epoch 2130, val loss: 0.5050769448280334
Epoch 2140, training loss: 90.87439727783203 = 0.46599280834198 + 10.0 * 9.040840148925781
Epoch 2140, val loss: 0.5039099454879761
Epoch 2150, training loss: 90.91333770751953 = 0.4645785987377167 + 10.0 * 9.044876098632812
Epoch 2150, val loss: 0.5027732849121094
Epoch 2160, training loss: 90.93550109863281 = 0.4631524085998535 + 10.0 * 9.047235488891602
Epoch 2160, val loss: 0.501630961894989
Epoch 2170, training loss: 90.87925720214844 = 0.46185123920440674 + 10.0 * 9.041740417480469
Epoch 2170, val loss: 0.5005937218666077
Epoch 2180, training loss: 90.89562225341797 = 0.4605497717857361 + 10.0 * 9.04350757598877
Epoch 2180, val loss: 0.49959343671798706
Epoch 2190, training loss: 90.90714263916016 = 0.45926761627197266 + 10.0 * 9.044787406921387
Epoch 2190, val loss: 0.4986041188240051
Epoch 2200, training loss: 90.95890808105469 = 0.4580268859863281 + 10.0 * 9.050088882446289
Epoch 2200, val loss: 0.4976387917995453
Epoch 2210, training loss: 91.00743103027344 = 0.4567282497882843 + 10.0 * 9.055070877075195
Epoch 2210, val loss: 0.4966335892677307
Epoch 2220, training loss: 91.01472473144531 = 0.4554476737976074 + 10.0 * 9.055928230285645
Epoch 2220, val loss: 0.49562638998031616
Epoch 2230, training loss: 91.03173065185547 = 0.4542028605937958 + 10.0 * 9.05775260925293
Epoch 2230, val loss: 0.49465832114219666
Epoch 2240, training loss: 90.9987564086914 = 0.45299017429351807 + 10.0 * 9.054576873779297
Epoch 2240, val loss: 0.49372395873069763
Epoch 2250, training loss: 90.99044036865234 = 0.4517936110496521 + 10.0 * 9.053865432739258
Epoch 2250, val loss: 0.492841899394989
Epoch 2260, training loss: 90.65914916992188 = 0.45074617862701416 + 10.0 * 9.020840644836426
Epoch 2260, val loss: 0.49210551381111145
Epoch 2270, training loss: 90.79374694824219 = 0.4497036039829254 + 10.0 * 9.034403800964355
Epoch 2270, val loss: 0.49131396412849426
Epoch 2280, training loss: 90.79335021972656 = 0.44862625002861023 + 10.0 * 9.034472465515137
Epoch 2280, val loss: 0.49051007628440857
Epoch 2290, training loss: 90.84842681884766 = 0.447531133890152 + 10.0 * 9.04008960723877
Epoch 2290, val loss: 0.4896913468837738
Epoch 2300, training loss: 90.88658905029297 = 0.4464854598045349 + 10.0 * 9.044010162353516
Epoch 2300, val loss: 0.48893409967422485
Epoch 2310, training loss: 90.97525787353516 = 0.44543156027793884 + 10.0 * 9.052982330322266
Epoch 2310, val loss: 0.4881415069103241
Epoch 2320, training loss: 91.0359878540039 = 0.44436588883399963 + 10.0 * 9.059162139892578
Epoch 2320, val loss: 0.48736461997032166
Epoch 2330, training loss: 91.03372955322266 = 0.4433180093765259 + 10.0 * 9.059041023254395
Epoch 2330, val loss: 0.4865880012512207
Epoch 2340, training loss: 91.08234405517578 = 0.44229769706726074 + 10.0 * 9.064004898071289
Epoch 2340, val loss: 0.48585012555122375
Epoch 2350, training loss: 91.09764862060547 = 0.4412935674190521 + 10.0 * 9.065635681152344
Epoch 2350, val loss: 0.4851158559322357
Epoch 2360, training loss: 91.10733032226562 = 0.440294474363327 + 10.0 * 9.066703796386719
Epoch 2360, val loss: 0.48441067337989807
Epoch 2370, training loss: 91.12857818603516 = 0.43932396173477173 + 10.0 * 9.068925857543945
Epoch 2370, val loss: 0.4837155044078827
Epoch 2380, training loss: 91.15786743164062 = 0.4383677840232849 + 10.0 * 9.07194995880127
Epoch 2380, val loss: 0.48304280638694763
Epoch 2390, training loss: 91.12287139892578 = 0.4374353587627411 + 10.0 * 9.068543434143066
Epoch 2390, val loss: 0.4823912978172302
Epoch 2400, training loss: 91.14285278320312 = 0.4365083873271942 + 10.0 * 9.070634841918945
Epoch 2400, val loss: 0.4817667603492737
Epoch 2410, training loss: 91.20902252197266 = 0.43559345602989197 + 10.0 * 9.077342987060547
Epoch 2410, val loss: 0.48111310601234436
Epoch 2420, training loss: 91.20646667480469 = 0.43468335270881653 + 10.0 * 9.077178001403809
Epoch 2420, val loss: 0.4804639220237732
Epoch 2430, training loss: 91.20809173583984 = 0.43379101157188416 + 10.0 * 9.07742977142334
Epoch 2430, val loss: 0.4798380136489868
Epoch 2440, training loss: 91.21914672851562 = 0.43292272090911865 + 10.0 * 9.078622817993164
Epoch 2440, val loss: 0.47922414541244507
Epoch 2450, training loss: 91.2500228881836 = 0.4320630431175232 + 10.0 * 9.081796646118164
Epoch 2450, val loss: 0.47861143946647644
Epoch 2460, training loss: 91.23738098144531 = 0.4312128722667694 + 10.0 * 9.08061695098877
Epoch 2460, val loss: 0.4780190885066986
Epoch 2470, training loss: 91.18634033203125 = 0.43037766218185425 + 10.0 * 9.075596809387207
Epoch 2470, val loss: 0.4774509370326996
Epoch 2480, training loss: 91.19481658935547 = 0.4295789897441864 + 10.0 * 9.076523780822754
Epoch 2480, val loss: 0.4769198000431061
Epoch 2490, training loss: 91.26219940185547 = 0.42876189947128296 + 10.0 * 9.083343505859375
Epoch 2490, val loss: 0.47637200355529785
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8263768115942028
0.8647395493733248
The final CL Acc:0.77705, 0.04035, The final GNN Acc:0.86455, 0.00055
Begin epxeriment: noisy_level: 0.2 cont_weight: 10 epoch: der1:0.3 der2:0.4 dfr1:0.4 dfr2:0
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=2500, cl_num_layers=2, cl_num_proj_hidden=64, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=10.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.4, drop_feat_rate_1=0.4, drop_feat_rate_2=0.0, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=32, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, noisy_level=0.2, num_hidden=64, num_proj_hidden=64, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=40, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 88648])
add edge: torch.Size([2, 106308])
remove edge: torch.Size([2, 70622])
updated graph: torch.Size([2, 88282])
=== Noisy graph ===
/home/project-robust-contrastive/Robust-Contrastive-Learning/model.py:227: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 102.35076141357422 = 1.0949910879135132 + 10.0 * 10.125576972961426
Epoch 0, val loss: 1.0941334962844849
Epoch 10, training loss: 97.80029296875 = 1.0948927402496338 + 10.0 * 9.670539855957031
Epoch 10, val loss: 1.094048261642456
Epoch 20, training loss: 96.17733764648438 = 1.0948580503463745 + 10.0 * 9.508248329162598
Epoch 20, val loss: 1.094000220298767
Epoch 30, training loss: 95.06787872314453 = 1.0947978496551514 + 10.0 * 9.397308349609375
Epoch 30, val loss: 1.0939186811447144
Epoch 40, training loss: 94.18632507324219 = 1.094723105430603 + 10.0 * 9.309160232543945
Epoch 40, val loss: 1.0938316583633423
Epoch 50, training loss: 93.47476959228516 = 1.0946595668792725 + 10.0 * 9.238011360168457
Epoch 50, val loss: 1.0937508344650269
Epoch 60, training loss: 92.86065673828125 = 1.0945936441421509 + 10.0 * 9.176606178283691
Epoch 60, val loss: 1.0936692953109741
Epoch 70, training loss: 92.32280731201172 = 1.0945321321487427 + 10.0 * 9.122827529907227
Epoch 70, val loss: 1.093589425086975
Epoch 80, training loss: 91.8454360961914 = 1.0944699048995972 + 10.0 * 9.07509708404541
Epoch 80, val loss: 1.0935075283050537
Epoch 90, training loss: 91.4231948852539 = 1.0944132804870605 + 10.0 * 9.032877922058105
Epoch 90, val loss: 1.0934327840805054
Epoch 100, training loss: 91.04869079589844 = 1.0943623781204224 + 10.0 * 8.99543285369873
Epoch 100, val loss: 1.0933631658554077
Epoch 110, training loss: 90.71495056152344 = 1.094311237335205 + 10.0 * 8.962063789367676
Epoch 110, val loss: 1.093293309211731
Epoch 120, training loss: 90.4156265258789 = 1.0942591428756714 + 10.0 * 8.932136535644531
Epoch 120, val loss: 1.0932233333587646
Epoch 130, training loss: 90.15966033935547 = 1.0942118167877197 + 10.0 * 8.90654468536377
Epoch 130, val loss: 1.09315824508667
Epoch 140, training loss: 89.9201431274414 = 1.0941641330718994 + 10.0 * 8.882597923278809
Epoch 140, val loss: 1.0930933952331543
Epoch 150, training loss: 89.69686126708984 = 1.0941178798675537 + 10.0 * 8.860274314880371
Epoch 150, val loss: 1.0930291414260864
Epoch 160, training loss: 89.5019760131836 = 1.0940723419189453 + 10.0 * 8.840790748596191
Epoch 160, val loss: 1.0929661989212036
Epoch 170, training loss: 89.35142517089844 = 1.0940276384353638 + 10.0 * 8.825739860534668
Epoch 170, val loss: 1.0929025411605835
Epoch 180, training loss: 89.19840240478516 = 1.093989372253418 + 10.0 * 8.810441970825195
Epoch 180, val loss: 1.092848777770996
Epoch 190, training loss: 89.05339813232422 = 1.093948245048523 + 10.0 * 8.795945167541504
Epoch 190, val loss: 1.0927919149398804
Epoch 200, training loss: 88.93898010253906 = 1.093909740447998 + 10.0 * 8.784506797790527
Epoch 200, val loss: 1.0927356481552124
Epoch 210, training loss: 88.83565521240234 = 1.0938694477081299 + 10.0 * 8.774178504943848
Epoch 210, val loss: 1.0926823616027832
Epoch 220, training loss: 88.7527084350586 = 1.0938329696655273 + 10.0 * 8.765887260437012
Epoch 220, val loss: 1.0926265716552734
Epoch 230, training loss: 88.65975952148438 = 1.0937966108322144 + 10.0 * 8.756596565246582
Epoch 230, val loss: 1.0925747156143188
Epoch 240, training loss: 88.57865905761719 = 1.0937631130218506 + 10.0 * 8.748489379882812
Epoch 240, val loss: 1.0925253629684448
Epoch 250, training loss: 88.52423095703125 = 1.093729019165039 + 10.0 * 8.743050575256348
Epoch 250, val loss: 1.092477560043335
Epoch 260, training loss: 88.44598388671875 = 1.0936943292617798 + 10.0 * 8.735228538513184
Epoch 260, val loss: 1.0924253463745117
Epoch 270, training loss: 88.41629791259766 = 1.0936640501022339 + 10.0 * 8.732263565063477
Epoch 270, val loss: 1.0923800468444824
Epoch 280, training loss: 88.3692398071289 = 1.0936346054077148 + 10.0 * 8.727560043334961
Epoch 280, val loss: 1.092334508895874
Epoch 290, training loss: 88.33557891845703 = 1.0936063528060913 + 10.0 * 8.724197387695312
Epoch 290, val loss: 1.0922931432724
Epoch 300, training loss: 88.29098510742188 = 1.0935782194137573 + 10.0 * 8.719740867614746
Epoch 300, val loss: 1.0922493934631348
Epoch 310, training loss: 88.26860046386719 = 1.0935500860214233 + 10.0 * 8.717504501342773
Epoch 310, val loss: 1.0922075510025024
Epoch 320, training loss: 88.23371124267578 = 1.0935231447219849 + 10.0 * 8.714018821716309
Epoch 320, val loss: 1.0921672582626343
Epoch 330, training loss: 88.2077865600586 = 1.0934983491897583 + 10.0 * 8.71142864227295
Epoch 330, val loss: 1.092125415802002
Epoch 340, training loss: 88.17843627929688 = 1.0934720039367676 + 10.0 * 8.70849609375
Epoch 340, val loss: 1.0920883417129517
Epoch 350, training loss: 88.16940307617188 = 1.093446969985962 + 10.0 * 8.707595825195312
Epoch 350, val loss: 1.0920512676239014
Epoch 360, training loss: 88.13610076904297 = 1.0934232473373413 + 10.0 * 8.704267501831055
Epoch 360, val loss: 1.0920119285583496
Epoch 370, training loss: 88.13968658447266 = 1.093400001525879 + 10.0 * 8.704628944396973
Epoch 370, val loss: 1.0919755697250366
Epoch 380, training loss: 88.12128448486328 = 1.0933780670166016 + 10.0 * 8.702791213989258
Epoch 380, val loss: 1.0919417142868042
Epoch 390, training loss: 88.11756134033203 = 1.0933586359024048 + 10.0 * 8.702420234680176
Epoch 390, val loss: 1.0919082164764404
Epoch 400, training loss: 88.11087036132812 = 1.0933351516723633 + 10.0 * 8.701753616333008
Epoch 400, val loss: 1.0918728113174438
Epoch 410, training loss: 88.1296615600586 = 1.0933163166046143 + 10.0 * 8.703634262084961
Epoch 410, val loss: 1.091841697692871
Epoch 420, training loss: 88.12379455566406 = 1.0932894945144653 + 10.0 * 8.70305061340332
Epoch 420, val loss: 1.0918028354644775
Epoch 430, training loss: 88.11732482910156 = 1.0932763814926147 + 10.0 * 8.702404975891113
Epoch 430, val loss: 1.0917761325836182
Epoch 440, training loss: 88.0042724609375 = 1.0932501554489136 + 10.0 * 8.691102981567383
Epoch 440, val loss: 1.0917398929595947
Epoch 450, training loss: 88.08230590820312 = 1.0932403802871704 + 10.0 * 8.698906898498535
Epoch 450, val loss: 1.0917161703109741
Epoch 460, training loss: 88.05458068847656 = 1.0932252407073975 + 10.0 * 8.696135520935059
Epoch 460, val loss: 1.0916898250579834
Epoch 470, training loss: 88.05965423583984 = 1.0932097434997559 + 10.0 * 8.69664478302002
Epoch 470, val loss: 1.091662883758545
Epoch 480, training loss: 88.07538604736328 = 1.0931951999664307 + 10.0 * 8.698219299316406
Epoch 480, val loss: 1.091635823249817
Epoch 490, training loss: 88.03314208984375 = 1.0931757688522339 + 10.0 * 8.69399642944336
Epoch 490, val loss: 1.0916063785552979
Epoch 500, training loss: 88.0777816772461 = 1.0931650400161743 + 10.0 * 8.698461532592773
Epoch 500, val loss: 1.0915837287902832
Epoch 510, training loss: 88.07418060302734 = 1.0931499004364014 + 10.0 * 8.698102951049805
Epoch 510, val loss: 1.0915592908859253
Epoch 520, training loss: 88.06973266601562 = 1.0931358337402344 + 10.0 * 8.697659492492676
Epoch 520, val loss: 1.0915346145629883
Epoch 530, training loss: 88.0660400390625 = 1.0931226015090942 + 10.0 * 8.697291374206543
Epoch 530, val loss: 1.0915098190307617
Epoch 540, training loss: 88.10063171386719 = 1.093112826347351 + 10.0 * 8.700752258300781
Epoch 540, val loss: 1.0914912223815918
Epoch 550, training loss: 88.10444641113281 = 1.09309983253479 + 10.0 * 8.70113468170166
Epoch 550, val loss: 1.0914667844772339
Epoch 560, training loss: 88.1113052368164 = 1.0930887460708618 + 10.0 * 8.701822280883789
Epoch 560, val loss: 1.0914454460144043
Epoch 570, training loss: 88.09883880615234 = 1.0930768251419067 + 10.0 * 8.700575828552246
Epoch 570, val loss: 1.0914247035980225
Epoch 580, training loss: 88.13565826416016 = 1.0930678844451904 + 10.0 * 8.704258918762207
Epoch 580, val loss: 1.0914063453674316
Epoch 590, training loss: 88.19219970703125 = 1.0930589437484741 + 10.0 * 8.709914207458496
Epoch 590, val loss: 1.0913842916488647
Epoch 600, training loss: 88.20126342773438 = 1.0930484533309937 + 10.0 * 8.710821151733398
Epoch 600, val loss: 1.091368556022644
Epoch 610, training loss: 88.11447143554688 = 1.0930354595184326 + 10.0 * 8.702143669128418
Epoch 610, val loss: 1.0913456678390503
Epoch 620, training loss: 88.15071868896484 = 1.0930280685424805 + 10.0 * 8.705769538879395
Epoch 620, val loss: 1.0913290977478027
Epoch 630, training loss: 88.19359588623047 = 1.0930218696594238 + 10.0 * 8.710057258605957
Epoch 630, val loss: 1.0913134813308716
Epoch 640, training loss: 88.17606353759766 = 1.0930122137069702 + 10.0 * 8.708305358886719
Epoch 640, val loss: 1.0912957191467285
Epoch 650, training loss: 88.18714904785156 = 1.0930026769638062 + 10.0 * 8.7094144821167
Epoch 650, val loss: 1.0912790298461914
Epoch 660, training loss: 88.19940948486328 = 1.0929956436157227 + 10.0 * 8.710641860961914
Epoch 660, val loss: 1.0912623405456543
Epoch 670, training loss: 88.20057678222656 = 1.0929882526397705 + 10.0 * 8.710759162902832
Epoch 670, val loss: 1.0912470817565918
Epoch 680, training loss: 88.21478271484375 = 1.0929820537567139 + 10.0 * 8.712180137634277
Epoch 680, val loss: 1.091233253479004
Epoch 690, training loss: 88.22462463378906 = 1.0929759740829468 + 10.0 * 8.713164329528809
Epoch 690, val loss: 1.091219186782837
Epoch 700, training loss: 88.25032043457031 = 1.092969536781311 + 10.0 * 8.715734481811523
Epoch 700, val loss: 1.0912046432495117
Epoch 710, training loss: 88.23990631103516 = 1.0929614305496216 + 10.0 * 8.714694023132324
Epoch 710, val loss: 1.0911914110183716
Epoch 720, training loss: 88.23967742919922 = 1.0929547548294067 + 10.0 * 8.714672088623047
Epoch 720, val loss: 1.091173529624939
Epoch 730, training loss: 88.26200103759766 = 1.0929511785507202 + 10.0 * 8.71690559387207
Epoch 730, val loss: 1.0911636352539062
Epoch 740, training loss: 88.27220916748047 = 1.0929462909698486 + 10.0 * 8.717926025390625
Epoch 740, val loss: 1.0911519527435303
Epoch 750, training loss: 88.2886962890625 = 1.0929415225982666 + 10.0 * 8.719575881958008
Epoch 750, val loss: 1.0911407470703125
Epoch 760, training loss: 88.28433227539062 = 1.0929361581802368 + 10.0 * 8.71914005279541
Epoch 760, val loss: 1.0911296606063843
Epoch 770, training loss: 88.32147979736328 = 1.0929328203201294 + 10.0 * 8.722854614257812
Epoch 770, val loss: 1.0911190509796143
Epoch 780, training loss: 88.34776306152344 = 1.0929288864135742 + 10.0 * 8.725483894348145
Epoch 780, val loss: 1.0911086797714233
Epoch 790, training loss: 88.31502532958984 = 1.0929232835769653 + 10.0 * 8.722209930419922
Epoch 790, val loss: 1.091097354888916
Epoch 800, training loss: 88.3585433959961 = 1.0929183959960938 + 10.0 * 8.7265625
Epoch 800, val loss: 1.0910857915878296
Epoch 810, training loss: 88.32424926757812 = 1.09291410446167 + 10.0 * 8.72313404083252
Epoch 810, val loss: 1.0910758972167969
Epoch 820, training loss: 88.35346984863281 = 1.092912197113037 + 10.0 * 8.726056098937988
Epoch 820, val loss: 1.091066837310791
Epoch 830, training loss: 88.37943267822266 = 1.0929089784622192 + 10.0 * 8.728652000427246
Epoch 830, val loss: 1.0910564661026
Epoch 840, training loss: 88.37677001953125 = 1.09290611743927 + 10.0 * 8.728386878967285
Epoch 840, val loss: 1.0910496711730957
Epoch 850, training loss: 88.36678314208984 = 1.0929017066955566 + 10.0 * 8.727388381958008
Epoch 850, val loss: 1.091039776802063
Epoch 860, training loss: 88.41412353515625 = 1.0928987264633179 + 10.0 * 8.732122421264648
Epoch 860, val loss: 1.091031551361084
Epoch 870, training loss: 88.44749450683594 = 1.0928981304168701 + 10.0 * 8.73546028137207
Epoch 870, val loss: 1.091024398803711
Epoch 880, training loss: 88.3900146484375 = 1.0928928852081299 + 10.0 * 8.729711532592773
Epoch 880, val loss: 1.0910171270370483
Epoch 890, training loss: 88.3702621459961 = 1.092890977859497 + 10.0 * 8.727737426757812
Epoch 890, val loss: 1.0910087823867798
Epoch 900, training loss: 88.43568420410156 = 1.0928890705108643 + 10.0 * 8.73427963256836
Epoch 900, val loss: 1.0910001993179321
Epoch 910, training loss: 88.5144271850586 = 1.0928895473480225 + 10.0 * 8.742154121398926
Epoch 910, val loss: 1.0909966230392456
Epoch 920, training loss: 88.51973724365234 = 1.092888593673706 + 10.0 * 8.742685317993164
Epoch 920, val loss: 1.0909888744354248
Epoch 930, training loss: 88.52916717529297 = 1.0928863286972046 + 10.0 * 8.743627548217773
Epoch 930, val loss: 1.090982437133789
Epoch 940, training loss: 88.53436279296875 = 1.0928845405578613 + 10.0 * 8.744147300720215
Epoch 940, val loss: 1.090976595878601
Epoch 950, training loss: 88.5379409790039 = 1.0928820371627808 + 10.0 * 8.744505882263184
Epoch 950, val loss: 1.0909689664840698
Epoch 960, training loss: 88.56244659423828 = 1.0928800106048584 + 10.0 * 8.746956825256348
Epoch 960, val loss: 1.0909643173217773
Epoch 970, training loss: 88.56964111328125 = 1.0928784608840942 + 10.0 * 8.747675895690918
Epoch 970, val loss: 1.0909584760665894
Epoch 980, training loss: 88.60272216796875 = 1.092877984046936 + 10.0 * 8.750984191894531
Epoch 980, val loss: 1.09095299243927
Epoch 990, training loss: 88.53876495361328 = 1.0928704738616943 + 10.0 * 8.744588851928711
Epoch 990, val loss: 1.0909432172775269
Epoch 1000, training loss: 88.61865997314453 = 1.0928702354431152 + 10.0 * 8.752578735351562
Epoch 1000, val loss: 1.090936303138733
Epoch 1010, training loss: 88.59102630615234 = 1.0928715467453003 + 10.0 * 8.749814987182617
Epoch 1010, val loss: 1.0909392833709717
Epoch 1020, training loss: 88.51435852050781 = 1.0928653478622437 + 10.0 * 8.742149353027344
Epoch 1020, val loss: 1.0909281969070435
Epoch 1030, training loss: 88.59191131591797 = 1.0928682088851929 + 10.0 * 8.74990463256836
Epoch 1030, val loss: 1.0909268856048584
Epoch 1040, training loss: 88.6635513305664 = 1.0928703546524048 + 10.0 * 8.757067680358887
Epoch 1040, val loss: 1.0909247398376465
Epoch 1050, training loss: 88.69145202636719 = 1.0928685665130615 + 10.0 * 8.759859085083008
Epoch 1050, val loss: 1.0909206867218018
Epoch 1060, training loss: 88.70523071289062 = 1.0928688049316406 + 10.0 * 8.761236190795898
Epoch 1060, val loss: 1.0909082889556885
Epoch 1070, training loss: 88.4030990600586 = 1.092843770980835 + 10.0 * 8.731025695800781
Epoch 1070, val loss: 1.0908972024917603
Epoch 1080, training loss: 88.58014678955078 = 1.0928564071655273 + 10.0 * 8.74872875213623
Epoch 1080, val loss: 1.0908972024917603
Epoch 1090, training loss: 88.59990692138672 = 1.0928555727005005 + 10.0 * 8.750704765319824
Epoch 1090, val loss: 1.0908962488174438
Epoch 1100, training loss: 88.67235565185547 = 1.0928593873977661 + 10.0 * 8.757949829101562
Epoch 1100, val loss: 1.0908961296081543
Epoch 1110, training loss: 88.67955780029297 = 1.092860460281372 + 10.0 * 8.75866985321045
Epoch 1110, val loss: 1.0908955335617065
Epoch 1120, training loss: 88.73542022705078 = 1.0928618907928467 + 10.0 * 8.76425552368164
Epoch 1120, val loss: 1.0908942222595215
Epoch 1130, training loss: 88.74687957763672 = 1.0928608179092407 + 10.0 * 8.765401840209961
Epoch 1130, val loss: 1.090890645980835
Epoch 1140, training loss: 88.7891616821289 = 1.0928618907928467 + 10.0 * 8.76962947845459
Epoch 1140, val loss: 1.0908887386322021
Epoch 1150, training loss: 88.74881744384766 = 1.0928583145141602 + 10.0 * 8.765596389770508
Epoch 1150, val loss: 1.0908836126327515
Epoch 1160, training loss: 88.78462219238281 = 1.0928566455841064 + 10.0 * 8.769176483154297
Epoch 1160, val loss: 1.0908805131912231
Epoch 1170, training loss: 88.74102783203125 = 1.0928544998168945 + 10.0 * 8.764817237854004
Epoch 1170, val loss: 1.0908758640289307
Epoch 1180, training loss: 88.8289566040039 = 1.0928564071655273 + 10.0 * 8.77361011505127
Epoch 1180, val loss: 1.0908739566802979
Epoch 1190, training loss: 88.89472198486328 = 1.0928595066070557 + 10.0 * 8.780186653137207
Epoch 1190, val loss: 1.0908745527267456
Epoch 1200, training loss: 88.93862915039062 = 1.0928592681884766 + 10.0 * 8.784577369689941
Epoch 1200, val loss: 1.090873122215271
Epoch 1210, training loss: 88.9302749633789 = 1.0928575992584229 + 10.0 * 8.78374195098877
Epoch 1210, val loss: 1.0908702611923218
Epoch 1220, training loss: 88.97542572021484 = 1.0928587913513184 + 10.0 * 8.788256645202637
Epoch 1220, val loss: 1.0908693075180054
Epoch 1230, training loss: 88.97992706298828 = 1.092857837677002 + 10.0 * 8.78870677947998
Epoch 1230, val loss: 1.090867042541504
Epoch 1240, training loss: 88.98894500732422 = 1.0928571224212646 + 10.0 * 8.7896089553833
Epoch 1240, val loss: 1.0908641815185547
Epoch 1250, training loss: 89.05562591552734 = 1.0928575992584229 + 10.0 * 8.796277046203613
Epoch 1250, val loss: 1.09086275100708
Epoch 1260, training loss: 89.07701110839844 = 1.0928562879562378 + 10.0 * 8.798415184020996
Epoch 1260, val loss: 1.0908607244491577
Epoch 1270, training loss: 89.07042694091797 = 1.0928562879562378 + 10.0 * 8.797757148742676
Epoch 1270, val loss: 1.090858817100525
Epoch 1280, training loss: 89.10723876953125 = 1.0928562879562378 + 10.0 * 8.801438331604004
Epoch 1280, val loss: 1.090856671333313
Epoch 1290, training loss: 89.12325286865234 = 1.0928558111190796 + 10.0 * 8.80303955078125
Epoch 1290, val loss: 1.0908554792404175
Epoch 1300, training loss: 89.13941192626953 = 1.0928558111190796 + 10.0 * 8.804655075073242
Epoch 1300, val loss: 1.0908539295196533
Epoch 1310, training loss: 89.15804290771484 = 1.0928555727005005 + 10.0 * 8.8065185546875
Epoch 1310, val loss: 1.0908522605895996
Epoch 1320, training loss: 89.23178100585938 = 1.0928535461425781 + 10.0 * 8.81389331817627
Epoch 1320, val loss: 1.0908464193344116
Epoch 1330, training loss: 89.3050765991211 = 1.0928536653518677 + 10.0 * 8.821222305297852
Epoch 1330, val loss: 1.0908489227294922
Epoch 1340, training loss: 89.23674011230469 = 1.0928524732589722 + 10.0 * 8.8143892288208
Epoch 1340, val loss: 1.0908468961715698
Epoch 1350, training loss: 89.3091049194336 = 1.0928542613983154 + 10.0 * 8.821624755859375
Epoch 1350, val loss: 1.090846300125122
Epoch 1360, training loss: 89.38040161132812 = 1.0928561687469482 + 10.0 * 8.828754425048828
Epoch 1360, val loss: 1.0908454656600952
Epoch 1370, training loss: 89.33690643310547 = 1.0928536653518677 + 10.0 * 8.8244047164917
Epoch 1370, val loss: 1.0908424854278564
Epoch 1380, training loss: 89.3679428100586 = 1.0928539037704468 + 10.0 * 8.827508926391602
Epoch 1380, val loss: 1.090842843055725
Epoch 1390, training loss: 89.45024108886719 = 1.0928555727005005 + 10.0 * 8.835738182067871
Epoch 1390, val loss: 1.090843677520752
Epoch 1400, training loss: 89.44283294677734 = 1.092854619026184 + 10.0 * 8.834997177124023
Epoch 1400, val loss: 1.0908420085906982
Epoch 1410, training loss: 89.40231323242188 = 1.0928512811660767 + 10.0 * 8.83094596862793
Epoch 1410, val loss: 1.0908379554748535
Epoch 1420, training loss: 89.37554931640625 = 1.092850685119629 + 10.0 * 8.828269958496094
Epoch 1420, val loss: 1.0908371210098267
Epoch 1430, training loss: 89.47274017333984 = 1.0928528308868408 + 10.0 * 8.83798885345459
Epoch 1430, val loss: 1.0908379554748535
Epoch 1440, training loss: 89.54290008544922 = 1.0928549766540527 + 10.0 * 8.845004081726074
Epoch 1440, val loss: 1.0908390283584595
Epoch 1450, training loss: 89.50389099121094 = 1.092850685119629 + 10.0 * 8.841104507446289
Epoch 1450, val loss: 1.090834140777588
Epoch 1460, training loss: 89.5330810546875 = 1.0928515195846558 + 10.0 * 8.844022750854492
Epoch 1460, val loss: 1.0908347368240356
Epoch 1470, training loss: 89.59626007080078 = 1.0928536653518677 + 10.0 * 8.850339889526367
Epoch 1470, val loss: 1.0908360481262207
Epoch 1480, training loss: 89.55846405029297 = 1.0928503274917603 + 10.0 * 8.846561431884766
Epoch 1480, val loss: 1.0908323526382446
Epoch 1490, training loss: 89.54739379882812 = 1.0928518772125244 + 10.0 * 8.845454216003418
Epoch 1490, val loss: 1.0908325910568237
Epoch 1500, training loss: 89.64326477050781 = 1.09285306930542 + 10.0 * 8.85504150390625
Epoch 1500, val loss: 1.0908334255218506
Epoch 1510, training loss: 89.6714859008789 = 1.0928521156311035 + 10.0 * 8.857863426208496
Epoch 1510, val loss: 1.0908305644989014
Epoch 1520, training loss: 89.54826354980469 = 1.0928477048873901 + 10.0 * 8.845541000366211
Epoch 1520, val loss: 1.0908267498016357
Epoch 1530, training loss: 89.56515502929688 = 1.0928480625152588 + 10.0 * 8.847230911254883
Epoch 1530, val loss: 1.0908271074295044
Epoch 1540, training loss: 89.62423706054688 = 1.0928494930267334 + 10.0 * 8.85313892364502
Epoch 1540, val loss: 1.0908278226852417
Epoch 1550, training loss: 89.69538879394531 = 1.0928517580032349 + 10.0 * 8.860254287719727
Epoch 1550, val loss: 1.0908293724060059
Epoch 1560, training loss: 89.66999816894531 = 1.0928499698638916 + 10.0 * 8.857714653015137
Epoch 1560, val loss: 1.0908273458480835
Epoch 1570, training loss: 89.6986083984375 = 1.0928508043289185 + 10.0 * 8.860575675964355
Epoch 1570, val loss: 1.090827465057373
Epoch 1580, training loss: 89.73609924316406 = 1.0928515195846558 + 10.0 * 8.864324569702148
Epoch 1580, val loss: 1.0908278226852417
Epoch 1590, training loss: 89.71485900878906 = 1.092851161956787 + 10.0 * 8.862200736999512
Epoch 1590, val loss: 1.0908260345458984
Epoch 1600, training loss: 89.74967956542969 = 1.092850923538208 + 10.0 * 8.865682601928711
Epoch 1600, val loss: 1.0908257961273193
Epoch 1610, training loss: 89.77518463134766 = 1.0928508043289185 + 10.0 * 8.868233680725098
Epoch 1610, val loss: 1.0908260345458984
Epoch 1620, training loss: 89.72089385986328 = 1.0928471088409424 + 10.0 * 8.862804412841797
Epoch 1620, val loss: 1.0908219814300537
Epoch 1630, training loss: 89.69115447998047 = 1.09284508228302 + 10.0 * 8.859830856323242
Epoch 1630, val loss: 1.0908199548721313
Epoch 1640, training loss: 89.71432495117188 = 1.0928471088409424 + 10.0 * 8.862147331237793
Epoch 1640, val loss: 1.0908206701278687
Epoch 1650, training loss: 89.80227661132812 = 1.0928486585617065 + 10.0 * 8.870943069458008
Epoch 1650, val loss: 1.0908236503601074
Epoch 1660, training loss: 89.84710693359375 = 1.0928504467010498 + 10.0 * 8.875425338745117
Epoch 1660, val loss: 1.0908238887786865
Epoch 1670, training loss: 89.74137878417969 = 1.0928447246551514 + 10.0 * 8.864853858947754
Epoch 1670, val loss: 1.0908193588256836
Epoch 1680, training loss: 89.78265380859375 = 1.0928459167480469 + 10.0 * 8.868980407714844
Epoch 1680, val loss: 1.0908201932907104
Epoch 1690, training loss: 89.84033203125 = 1.0928462743759155 + 10.0 * 8.874748229980469
Epoch 1690, val loss: 1.0908209085464478
Epoch 1700, training loss: 89.88493347167969 = 1.092848777770996 + 10.0 * 8.8792085647583
Epoch 1700, val loss: 1.0908223390579224
Epoch 1710, training loss: 89.89392852783203 = 1.0928479433059692 + 10.0 * 8.880107879638672
Epoch 1710, val loss: 1.0908215045928955
Epoch 1720, training loss: 89.91209411621094 = 1.0928490161895752 + 10.0 * 8.881924629211426
Epoch 1720, val loss: 1.0908215045928955
Epoch 1730, training loss: 89.9096450805664 = 1.092847466468811 + 10.0 * 8.88167953491211
Epoch 1730, val loss: 1.0908210277557373
Epoch 1740, training loss: 89.96163177490234 = 1.0928490161895752 + 10.0 * 8.88687801361084
Epoch 1740, val loss: 1.090821623802185
Epoch 1750, training loss: 89.97156524658203 = 1.0928481817245483 + 10.0 * 8.887871742248535
Epoch 1750, val loss: 1.0908206701278687
Epoch 1760, training loss: 89.9309310913086 = 1.0928449630737305 + 10.0 * 8.883809089660645
Epoch 1760, val loss: 1.0908184051513672
Epoch 1770, training loss: 89.9900131225586 = 1.0928471088409424 + 10.0 * 8.889716148376465
Epoch 1770, val loss: 1.0908207893371582
Epoch 1780, training loss: 89.97550964355469 = 1.0928460359573364 + 10.0 * 8.888265609741211
Epoch 1780, val loss: 1.090819239616394
Epoch 1790, training loss: 89.9737548828125 = 1.0928453207015991 + 10.0 * 8.888091087341309
Epoch 1790, val loss: 1.0908185243606567
Epoch 1800, training loss: 90.0271987915039 = 1.0928455591201782 + 10.0 * 8.89343547821045
Epoch 1800, val loss: 1.090819001197815
Epoch 1810, training loss: 90.08102416992188 = 1.0928466320037842 + 10.0 * 8.89881706237793
Epoch 1810, val loss: 1.090819001197815
Epoch 1820, training loss: 90.01797485351562 = 1.0928441286087036 + 10.0 * 8.892513275146484
Epoch 1820, val loss: 1.0908164978027344
Epoch 1830, training loss: 90.02379608154297 = 1.0928441286087036 + 10.0 * 8.893095016479492
Epoch 1830, val loss: 1.090816617012024
Epoch 1840, training loss: 90.0777816772461 = 1.0928447246551514 + 10.0 * 8.898493766784668
Epoch 1840, val loss: 1.0908172130584717
Epoch 1850, training loss: 90.11583709716797 = 1.092846155166626 + 10.0 * 8.902298927307129
Epoch 1850, val loss: 1.0908186435699463
Epoch 1860, training loss: 90.08983612060547 = 1.0928438901901245 + 10.0 * 8.899699211120605
Epoch 1860, val loss: 1.090816855430603
Epoch 1870, training loss: 90.11486053466797 = 1.0928444862365723 + 10.0 * 8.902201652526855
Epoch 1870, val loss: 1.0908178091049194
Epoch 1880, training loss: 90.15789794921875 = 1.0928459167480469 + 10.0 * 8.906505584716797
Epoch 1880, val loss: 1.0908178091049194
Epoch 1890, training loss: 90.11174011230469 = 1.0928438901901245 + 10.0 * 8.90188980102539
Epoch 1890, val loss: 1.0908163785934448
Epoch 1900, training loss: 90.15013885498047 = 1.0928446054458618 + 10.0 * 8.905729293823242
Epoch 1900, val loss: 1.090816855430603
Epoch 1910, training loss: 90.19518280029297 = 1.092844843864441 + 10.0 * 8.910234451293945
Epoch 1910, val loss: 1.090816617012024
Epoch 1920, training loss: 90.07862854003906 = 1.0928395986557007 + 10.0 * 8.898578643798828
Epoch 1920, val loss: 1.0908130407333374
Epoch 1930, training loss: 90.12134552001953 = 1.092840313911438 + 10.0 * 8.902850151062012
Epoch 1930, val loss: 1.0908129215240479
Epoch 1940, training loss: 90.18035888671875 = 1.0928415060043335 + 10.0 * 8.908751487731934
Epoch 1940, val loss: 1.0908150672912598
Epoch 1950, training loss: 90.22428131103516 = 1.0928428173065186 + 10.0 * 8.9131441116333
Epoch 1950, val loss: 1.0908161401748657
Epoch 1960, training loss: 90.2170639038086 = 1.092842698097229 + 10.0 * 8.912422180175781
Epoch 1960, val loss: 1.090814471244812
Epoch 1970, training loss: 90.2197036743164 = 1.092841625213623 + 10.0 * 8.912686347961426
Epoch 1970, val loss: 1.0908145904541016
Epoch 1980, training loss: 90.23591613769531 = 1.0928412675857544 + 10.0 * 8.914307594299316
Epoch 1980, val loss: 1.0908143520355225
Epoch 1990, training loss: 90.24779510498047 = 1.0928407907485962 + 10.0 * 8.915494918823242
Epoch 1990, val loss: 1.0908148288726807
Epoch 2000, training loss: 90.24654388427734 = 1.0928395986557007 + 10.0 * 8.915369987487793
Epoch 2000, val loss: 1.0908129215240479
Epoch 2010, training loss: 90.23896789550781 = 1.0928393602371216 + 10.0 * 8.914612770080566
Epoch 2010, val loss: 1.0908116102218628
Epoch 2020, training loss: 90.2761459350586 = 1.0928399562835693 + 10.0 * 8.918330192565918
Epoch 2020, val loss: 1.0908130407333374
Epoch 2030, training loss: 90.29705047607422 = 1.092840552330017 + 10.0 * 8.92042064666748
Epoch 2030, val loss: 1.0908132791519165
Epoch 2040, training loss: 90.2911605834961 = 1.0928391218185425 + 10.0 * 8.919832229614258
Epoch 2040, val loss: 1.0908128023147583
Epoch 2050, training loss: 90.29495239257812 = 1.0928385257720947 + 10.0 * 8.920210838317871
Epoch 2050, val loss: 1.0908119678497314
Epoch 2060, training loss: 90.33472442626953 = 1.092839002609253 + 10.0 * 8.924188613891602
Epoch 2060, val loss: 1.0908123254776
Epoch 2070, training loss: 90.3212661743164 = 1.092838168144226 + 10.0 * 8.922842979431152
Epoch 2070, val loss: 1.0908112525939941
Epoch 2080, training loss: 90.34229278564453 = 1.092838168144226 + 10.0 * 8.924945831298828
Epoch 2080, val loss: 1.0908113718032837
Epoch 2090, training loss: 90.38813018798828 = 1.092839002609253 + 10.0 * 8.929529190063477
Epoch 2090, val loss: 1.0908117294311523
Epoch 2100, training loss: 90.42687225341797 = 1.0928386449813843 + 10.0 * 8.933403015136719
Epoch 2100, val loss: 1.090812087059021
Epoch 2110, training loss: 90.01188659667969 = 1.092808485031128 + 10.0 * 8.891907691955566
Epoch 2110, val loss: 1.0907809734344482
Epoch 2120, training loss: 90.16860961914062 = 1.0928161144256592 + 10.0 * 8.90757942199707
Epoch 2120, val loss: 1.090789794921875
Epoch 2130, training loss: 90.19645690917969 = 1.0928199291229248 + 10.0 * 8.910364151000977
Epoch 2130, val loss: 1.0907931327819824
Epoch 2140, training loss: 90.24894714355469 = 1.0928220748901367 + 10.0 * 8.915613174438477
Epoch 2140, val loss: 1.090796709060669
Epoch 2150, training loss: 90.3062744140625 = 1.0928258895874023 + 10.0 * 8.921344757080078
Epoch 2150, val loss: 1.090800404548645
Epoch 2160, training loss: 90.3958740234375 = 1.0928300619125366 + 10.0 * 8.930304527282715
Epoch 2160, val loss: 1.0908048152923584
Epoch 2170, training loss: 90.45372772216797 = 1.0928313732147217 + 10.0 * 8.936089515686035
Epoch 2170, val loss: 1.0908066034317017
Epoch 2180, training loss: 90.49808502197266 = 1.0928330421447754 + 10.0 * 8.94052505493164
Epoch 2180, val loss: 1.0908074378967285
Epoch 2190, training loss: 90.48243713378906 = 1.0928318500518799 + 10.0 * 8.938960075378418
Epoch 2190, val loss: 1.0908066034317017
Epoch 2200, training loss: 90.51268005371094 = 1.0928328037261963 + 10.0 * 8.941984176635742
Epoch 2200, val loss: 1.0908076763153076
Epoch 2210, training loss: 90.55862426757812 = 1.092833399772644 + 10.0 * 8.946578979492188
Epoch 2210, val loss: 1.0908082723617554
Epoch 2220, training loss: 90.547607421875 = 1.0928330421447754 + 10.0 * 8.945477485656738
Epoch 2220, val loss: 1.0908079147338867
Epoch 2230, training loss: 90.51188659667969 = 1.0928312540054321 + 10.0 * 8.94190502166748
Epoch 2230, val loss: 1.090806245803833
Epoch 2240, training loss: 90.5549545288086 = 1.092832326889038 + 10.0 * 8.946211814880371
Epoch 2240, val loss: 1.0908082723617554
Epoch 2250, training loss: 90.60054016113281 = 1.0928337574005127 + 10.0 * 8.950770378112793
Epoch 2250, val loss: 1.090808391571045
Epoch 2260, training loss: 90.60462951660156 = 1.0928326845169067 + 10.0 * 8.951179504394531
Epoch 2260, val loss: 1.0908077955245972
Epoch 2270, training loss: 90.58740234375 = 1.0928317308425903 + 10.0 * 8.949457168579102
Epoch 2270, val loss: 1.0908066034317017
Epoch 2280, training loss: 90.64208221435547 = 1.0928322076797485 + 10.0 * 8.954924583435059
Epoch 2280, val loss: 1.090807557106018
Epoch 2290, training loss: 90.63795471191406 = 1.0928325653076172 + 10.0 * 8.954511642456055
Epoch 2290, val loss: 1.0908077955245972
Epoch 2300, training loss: 90.64399719238281 = 1.0928308963775635 + 10.0 * 8.955116271972656
Epoch 2300, val loss: 1.0908061265945435
Epoch 2310, training loss: 90.63658142089844 = 1.0928308963775635 + 10.0 * 8.954374313354492
Epoch 2310, val loss: 1.0908069610595703
Epoch 2320, training loss: 90.67742156982422 = 1.0928316116333008 + 10.0 * 8.95845890045166
Epoch 2320, val loss: 1.0908074378967285
Epoch 2330, training loss: 90.67919921875 = 1.092832326889038 + 10.0 * 8.958636283874512
Epoch 2330, val loss: 1.0908069610595703
Epoch 2340, training loss: 90.51940155029297 = 1.0928179025650024 + 10.0 * 8.942658424377441
Epoch 2340, val loss: 1.0907959938049316
Epoch 2350, training loss: 90.36886596679688 = 1.0928125381469727 + 10.0 * 8.927605628967285
Epoch 2350, val loss: 1.0907882452011108
Epoch 2360, training loss: 90.50315856933594 = 1.0928173065185547 + 10.0 * 8.941034317016602
Epoch 2360, val loss: 1.0907936096191406
Epoch 2370, training loss: 90.57611083984375 = 1.0928213596343994 + 10.0 * 8.948328971862793
Epoch 2370, val loss: 1.0907979011535645
Epoch 2380, training loss: 90.65971374511719 = 1.0928248167037964 + 10.0 * 8.95668888092041
Epoch 2380, val loss: 1.0908019542694092
Epoch 2390, training loss: 90.71014404296875 = 1.092827320098877 + 10.0 * 8.961731910705566
Epoch 2390, val loss: 1.0908044576644897
Epoch 2400, training loss: 90.71951293945312 = 1.092827320098877 + 10.0 * 8.962668418884277
Epoch 2400, val loss: 1.090804934501648
Epoch 2410, training loss: 90.7549057006836 = 1.0928282737731934 + 10.0 * 8.966207504272461
Epoch 2410, val loss: 1.0908056497573853
Epoch 2420, training loss: 90.76683807373047 = 1.0928281545639038 + 10.0 * 8.967401504516602
Epoch 2420, val loss: 1.0908044576644897
Epoch 2430, training loss: 90.77425384521484 = 1.0928276777267456 + 10.0 * 8.96814250946045
Epoch 2430, val loss: 1.0908048152923584
Epoch 2440, training loss: 90.81188201904297 = 1.0928285121917725 + 10.0 * 8.971905708312988
Epoch 2440, val loss: 1.0908055305480957
Epoch 2450, training loss: 90.81118774414062 = 1.0928277969360352 + 10.0 * 8.97183609008789
Epoch 2450, val loss: 1.0908048152923584
Epoch 2460, training loss: 90.84603118896484 = 1.0928279161453247 + 10.0 * 8.975320816040039
Epoch 2460, val loss: 1.0908052921295166
Epoch 2470, training loss: 90.8650894165039 = 1.0928276777267456 + 10.0 * 8.977226257324219
Epoch 2470, val loss: 1.0908057689666748
Epoch 2480, training loss: 90.87271118164062 = 1.0928272008895874 + 10.0 * 8.977988243103027
Epoch 2480, val loss: 1.0908046960830688
Epoch 2490, training loss: 90.82840728759766 = 1.0928245782852173 + 10.0 * 8.97355842590332
Epoch 2490, val loss: 1.0908024311065674
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.3973913043478261
0.8167789610954141
=== training gcn model ===
Epoch 0, training loss: 101.88905334472656 = 1.0936172008514404 + 10.0 * 10.079543113708496
Epoch 0, val loss: 1.093532919883728
Epoch 10, training loss: 97.1095199584961 = 1.0934497117996216 + 10.0 * 9.601606369018555
Epoch 10, val loss: 1.093359112739563
Epoch 20, training loss: 95.54020690917969 = 1.0931816101074219 + 10.0 * 9.4447021484375
Epoch 20, val loss: 1.093104600906372
Epoch 30, training loss: 94.42911529541016 = 1.0929510593414307 + 10.0 * 9.333616256713867
Epoch 30, val loss: 1.0928750038146973
Epoch 40, training loss: 93.5451431274414 = 1.0926932096481323 + 10.0 * 9.245244979858398
Epoch 40, val loss: 1.092621922492981
Epoch 50, training loss: 92.8284912109375 = 1.0924363136291504 + 10.0 * 9.173604965209961
Epoch 50, val loss: 1.0923676490783691
Epoch 60, training loss: 92.23272705078125 = 1.0921770334243774 + 10.0 * 9.114054679870605
Epoch 60, val loss: 1.092110514640808
Epoch 70, training loss: 91.71281433105469 = 1.0919184684753418 + 10.0 * 9.062089920043945
Epoch 70, val loss: 1.091854453086853
Epoch 80, training loss: 91.25193786621094 = 1.091660499572754 + 10.0 * 9.016027450561523
Epoch 80, val loss: 1.0916005373001099
Epoch 90, training loss: 90.84602355957031 = 1.0914124250411987 + 10.0 * 8.97546100616455
Epoch 90, val loss: 1.0913538932800293
Epoch 100, training loss: 90.50401306152344 = 1.0911685228347778 + 10.0 * 8.9412841796875
Epoch 100, val loss: 1.091111183166504
Epoch 110, training loss: 90.20306396484375 = 1.090924859046936 + 10.0 * 8.911213874816895
Epoch 110, val loss: 1.0908674001693726
Epoch 120, training loss: 89.94237518310547 = 1.0906851291656494 + 10.0 * 8.88516902923584
Epoch 120, val loss: 1.090631365776062
Epoch 130, training loss: 89.6893081665039 = 1.0904444456100464 + 10.0 * 8.859886169433594
Epoch 130, val loss: 1.0903884172439575
Epoch 140, training loss: 89.4791030883789 = 1.0902067422866821 + 10.0 * 8.838889122009277
Epoch 140, val loss: 1.090152382850647
Epoch 150, training loss: 89.29730224609375 = 1.089971899986267 + 10.0 * 8.820733070373535
Epoch 150, val loss: 1.0899181365966797
Epoch 160, training loss: 89.14070129394531 = 1.089744210243225 + 10.0 * 8.805095672607422
Epoch 160, val loss: 1.089690089225769
Epoch 170, training loss: 88.98601531982422 = 1.0895103216171265 + 10.0 * 8.789650917053223
Epoch 170, val loss: 1.0894596576690674
Epoch 180, training loss: 88.86833953857422 = 1.0892832279205322 + 10.0 * 8.777905464172363
Epoch 180, val loss: 1.0892313718795776
Epoch 190, training loss: 88.75697326660156 = 1.089052438735962 + 10.0 * 8.766792297363281
Epoch 190, val loss: 1.089003562927246
Epoch 200, training loss: 88.65782928466797 = 1.0888302326202393 + 10.0 * 8.7568998336792
Epoch 200, val loss: 1.088783860206604
Epoch 210, training loss: 88.56532287597656 = 1.0886015892028809 + 10.0 * 8.747672080993652
Epoch 210, val loss: 1.0885556936264038
Epoch 220, training loss: 88.53131866455078 = 1.0883760452270508 + 10.0 * 8.744294166564941
Epoch 220, val loss: 1.0883299112319946
Epoch 230, training loss: 88.43717956542969 = 1.0881519317626953 + 10.0 * 8.734903335571289
Epoch 230, val loss: 1.0881136655807495
Epoch 240, training loss: 88.37650299072266 = 1.0879321098327637 + 10.0 * 8.728857040405273
Epoch 240, val loss: 1.0878920555114746
Epoch 250, training loss: 88.31962585449219 = 1.0877180099487305 + 10.0 * 8.723191261291504
Epoch 250, val loss: 1.0876786708831787
Epoch 260, training loss: 88.26893615722656 = 1.0875014066696167 + 10.0 * 8.718143463134766
Epoch 260, val loss: 1.0874632596969604
Epoch 270, training loss: 88.23876953125 = 1.0872832536697388 + 10.0 * 8.71514892578125
Epoch 270, val loss: 1.0872465372085571
Epoch 280, training loss: 88.20291900634766 = 1.087070345878601 + 10.0 * 8.71158504486084
Epoch 280, val loss: 1.0870333909988403
Epoch 290, training loss: 88.18627166748047 = 1.0868583917617798 + 10.0 * 8.709940910339355
Epoch 290, val loss: 1.086824893951416
Epoch 300, training loss: 88.14764404296875 = 1.0866457223892212 + 10.0 * 8.706099510192871
Epoch 300, val loss: 1.086613416671753
Epoch 310, training loss: 88.1214599609375 = 1.0864346027374268 + 10.0 * 8.703502655029297
Epoch 310, val loss: 1.0864063501358032
Epoch 320, training loss: 88.1001968383789 = 1.0862245559692383 + 10.0 * 8.701396942138672
Epoch 320, val loss: 1.0861997604370117
Epoch 330, training loss: 88.05934143066406 = 1.0860159397125244 + 10.0 * 8.697332382202148
Epoch 330, val loss: 1.08599054813385
Epoch 340, training loss: 88.0313720703125 = 1.0858112573623657 + 10.0 * 8.69455623626709
Epoch 340, val loss: 1.0857863426208496
Epoch 350, training loss: 88.05708312988281 = 1.085608959197998 + 10.0 * 8.697147369384766
Epoch 350, val loss: 1.085586667060852
Epoch 360, training loss: 88.05914306640625 = 1.0854065418243408 + 10.0 * 8.69737434387207
Epoch 360, val loss: 1.0853825807571411
Epoch 370, training loss: 88.02781677246094 = 1.085201621055603 + 10.0 * 8.69426155090332
Epoch 370, val loss: 1.085180640220642
Epoch 380, training loss: 88.02286529541016 = 1.0850017070770264 + 10.0 * 8.69378662109375
Epoch 380, val loss: 1.0849803686141968
Epoch 390, training loss: 88.04147338867188 = 1.0848021507263184 + 10.0 * 8.695667266845703
Epoch 390, val loss: 1.0847833156585693
Epoch 400, training loss: 88.1020278930664 = 1.08460533618927 + 10.0 * 8.701742172241211
Epoch 400, val loss: 1.0845866203308105
Epoch 410, training loss: 88.13911437988281 = 1.084409236907959 + 10.0 * 8.705470085144043
Epoch 410, val loss: 1.0843894481658936
Epoch 420, training loss: 88.1181869506836 = 1.0842088460922241 + 10.0 * 8.703397750854492
Epoch 420, val loss: 1.0841941833496094
Epoch 430, training loss: 88.02725219726562 = 1.0840071439743042 + 10.0 * 8.694324493408203
Epoch 430, val loss: 1.0839946269989014
Epoch 440, training loss: 88.01099395751953 = 1.0838154554367065 + 10.0 * 8.692717552185059
Epoch 440, val loss: 1.0838031768798828
Epoch 450, training loss: 88.04857635498047 = 1.083627462387085 + 10.0 * 8.696495056152344
Epoch 450, val loss: 1.0836162567138672
Epoch 460, training loss: 88.05473327636719 = 1.083437204360962 + 10.0 * 8.69713020324707
Epoch 460, val loss: 1.0834275484085083
Epoch 470, training loss: 88.08454895019531 = 1.083247184753418 + 10.0 * 8.700130462646484
Epoch 470, val loss: 1.0832401514053345
Epoch 480, training loss: 88.08626556396484 = 1.0830587148666382 + 10.0 * 8.70032024383545
Epoch 480, val loss: 1.0830528736114502
Epoch 490, training loss: 88.07494354248047 = 1.0828664302825928 + 10.0 * 8.69920825958252
Epoch 490, val loss: 1.082863211631775
Epoch 500, training loss: 88.09514617919922 = 1.0826798677444458 + 10.0 * 8.70124626159668
Epoch 500, val loss: 1.0826774835586548
Epoch 510, training loss: 88.07220458984375 = 1.0824912786483765 + 10.0 * 8.69897174835205
Epoch 510, val loss: 1.0824919939041138
Epoch 520, training loss: 88.09256744384766 = 1.0823074579238892 + 10.0 * 8.70102596282959
Epoch 520, val loss: 1.0823099613189697
Epoch 530, training loss: 88.11756896972656 = 1.082123875617981 + 10.0 * 8.703544616699219
Epoch 530, val loss: 1.082127332687378
Epoch 540, training loss: 88.14299011230469 = 1.0819419622421265 + 10.0 * 8.70610523223877
Epoch 540, val loss: 1.0819469690322876
Epoch 550, training loss: 88.15261840820312 = 1.0817549228668213 + 10.0 * 8.707086563110352
Epoch 550, val loss: 1.0817639827728271
Epoch 560, training loss: 88.15645599365234 = 1.0815774202346802 + 10.0 * 8.707487106323242
Epoch 560, val loss: 1.081586480140686
Epoch 570, training loss: 88.1709213256836 = 1.081398367881775 + 10.0 * 8.708951950073242
Epoch 570, val loss: 1.0814087390899658
Epoch 580, training loss: 88.18816375732422 = 1.0812175273895264 + 10.0 * 8.710695266723633
Epoch 580, val loss: 1.0812280178070068
Epoch 590, training loss: 88.18331146240234 = 1.0810346603393555 + 10.0 * 8.710227966308594
Epoch 590, val loss: 1.0810444355010986
Epoch 600, training loss: 88.20069122314453 = 1.080863356590271 + 10.0 * 8.711982727050781
Epoch 600, val loss: 1.0808756351470947
Epoch 610, training loss: 88.29643249511719 = 1.0806913375854492 + 10.0 * 8.721574783325195
Epoch 610, val loss: 1.0807077884674072
Epoch 620, training loss: 88.20235443115234 = 1.0805118083953857 + 10.0 * 8.712183952331543
Epoch 620, val loss: 1.0805288553237915
Epoch 630, training loss: 88.23167419433594 = 1.080336093902588 + 10.0 * 8.715133666992188
Epoch 630, val loss: 1.0803568363189697
Epoch 640, training loss: 88.25223541259766 = 1.0801644325256348 + 10.0 * 8.717206954956055
Epoch 640, val loss: 1.080188274383545
Epoch 650, training loss: 88.26931762695312 = 1.079994559288025 + 10.0 * 8.718932151794434
Epoch 650, val loss: 1.0800163745880127
Epoch 660, training loss: 88.28450775146484 = 1.0798234939575195 + 10.0 * 8.720468521118164
Epoch 660, val loss: 1.0798492431640625
Epoch 670, training loss: 88.27547454833984 = 1.07965087890625 + 10.0 * 8.719582557678223
Epoch 670, val loss: 1.0796774625778198
Epoch 680, training loss: 88.29962158203125 = 1.0794841051101685 + 10.0 * 8.722013473510742
Epoch 680, val loss: 1.0795117616653442
Epoch 690, training loss: 88.3426513671875 = 1.0793187618255615 + 10.0 * 8.726333618164062
Epoch 690, val loss: 1.0793449878692627
Epoch 700, training loss: 88.36795806884766 = 1.079149603843689 + 10.0 * 8.728880882263184
Epoch 700, val loss: 1.0791770219802856
Epoch 710, training loss: 88.34251403808594 = 1.0789833068847656 + 10.0 * 8.72635269165039
Epoch 710, val loss: 1.079013466835022
Epoch 720, training loss: 88.34559631347656 = 1.0788177251815796 + 10.0 * 8.726677894592285
Epoch 720, val loss: 1.0788521766662598
Epoch 730, training loss: 88.36133575439453 = 1.07865571975708 + 10.0 * 8.728267669677734
Epoch 730, val loss: 1.078690767288208
Epoch 740, training loss: 88.37229919433594 = 1.0784926414489746 + 10.0 * 8.72938060760498
Epoch 740, val loss: 1.0785300731658936
Epoch 750, training loss: 88.38127136230469 = 1.0783295631408691 + 10.0 * 8.730294227600098
Epoch 750, val loss: 1.0783662796020508
Epoch 760, training loss: 88.39395904541016 = 1.0781692266464233 + 10.0 * 8.731578826904297
Epoch 760, val loss: 1.078210711479187
Epoch 770, training loss: 88.41954803466797 = 1.0780086517333984 + 10.0 * 8.734153747558594
Epoch 770, val loss: 1.0780508518218994
Epoch 780, training loss: 88.44314575195312 = 1.077850580215454 + 10.0 * 8.736529350280762
Epoch 780, val loss: 1.077895998954773
Epoch 790, training loss: 88.43119812011719 = 1.077688455581665 + 10.0 * 8.735350608825684
Epoch 790, val loss: 1.0777359008789062
Epoch 800, training loss: 88.4138412475586 = 1.077531337738037 + 10.0 * 8.733631134033203
Epoch 800, val loss: 1.0775773525238037
Epoch 810, training loss: 88.4224624633789 = 1.0773746967315674 + 10.0 * 8.734508514404297
Epoch 810, val loss: 1.0774245262145996
Epoch 820, training loss: 88.48856353759766 = 1.0772223472595215 + 10.0 * 8.741133689880371
Epoch 820, val loss: 1.0772732496261597
Epoch 830, training loss: 88.44151306152344 = 1.0770598649978638 + 10.0 * 8.736445426940918
Epoch 830, val loss: 1.0771127939224243
Epoch 840, training loss: 88.46785736083984 = 1.0769089460372925 + 10.0 * 8.739094734191895
Epoch 840, val loss: 1.0769646167755127
Epoch 850, training loss: 88.51190948486328 = 1.0767563581466675 + 10.0 * 8.743515014648438
Epoch 850, val loss: 1.0768145322799683
Epoch 860, training loss: 88.48612976074219 = 1.076602578163147 + 10.0 * 8.74095344543457
Epoch 860, val loss: 1.0766624212265015
Epoch 870, training loss: 88.53160858154297 = 1.076454758644104 + 10.0 * 8.745515823364258
Epoch 870, val loss: 1.0765169858932495
Epoch 880, training loss: 88.5396499633789 = 1.0763062238693237 + 10.0 * 8.746334075927734
Epoch 880, val loss: 1.0763698816299438
Epoch 890, training loss: 88.52737426757812 = 1.0761559009552002 + 10.0 * 8.745121955871582
Epoch 890, val loss: 1.0762195587158203
Epoch 900, training loss: 88.61046600341797 = 1.0760101079940796 + 10.0 * 8.753445625305176
Epoch 900, val loss: 1.0760749578475952
Epoch 910, training loss: 88.61724090576172 = 1.0758618116378784 + 10.0 * 8.754137992858887
Epoch 910, val loss: 1.0759283304214478
Epoch 920, training loss: 88.55868530273438 = 1.075705647468567 + 10.0 * 8.748297691345215
Epoch 920, val loss: 1.0757732391357422
Epoch 930, training loss: 88.5854721069336 = 1.0755643844604492 + 10.0 * 8.750990867614746
Epoch 930, val loss: 1.0756347179412842
Epoch 940, training loss: 88.617919921875 = 1.0754249095916748 + 10.0 * 8.754249572753906
Epoch 940, val loss: 1.0754961967468262
Epoch 950, training loss: 88.66707611083984 = 1.0752835273742676 + 10.0 * 8.75917911529541
Epoch 950, val loss: 1.0753556489944458
Epoch 960, training loss: 88.63506317138672 = 1.075133204460144 + 10.0 * 8.755992889404297
Epoch 960, val loss: 1.0752100944519043
Epoch 970, training loss: 88.66734313964844 = 1.074994444847107 + 10.0 * 8.759234428405762
Epoch 970, val loss: 1.0750725269317627
Epoch 980, training loss: 88.6904067993164 = 1.0748538970947266 + 10.0 * 8.761555671691895
Epoch 980, val loss: 1.074934720993042
Epoch 990, training loss: 88.71440887451172 = 1.0747120380401611 + 10.0 * 8.763969421386719
Epoch 990, val loss: 1.0747942924499512
Epoch 1000, training loss: 88.7168960571289 = 1.0745710134506226 + 10.0 * 8.764232635498047
Epoch 1000, val loss: 1.0746550559997559
Epoch 1010, training loss: 88.67877197265625 = 1.0744184255599976 + 10.0 * 8.760435104370117
Epoch 1010, val loss: 1.0745091438293457
Epoch 1020, training loss: 88.67334747314453 = 1.0742857456207275 + 10.0 * 8.759905815124512
Epoch 1020, val loss: 1.074373722076416
Epoch 1030, training loss: 88.73999786376953 = 1.0741528272628784 + 10.0 * 8.766584396362305
Epoch 1030, val loss: 1.074241280555725
Epoch 1040, training loss: 88.79171752929688 = 1.0740219354629517 + 10.0 * 8.771769523620605
Epoch 1040, val loss: 1.0741097927093506
Epoch 1050, training loss: 88.8157958984375 = 1.073886513710022 + 10.0 * 8.774190902709961
Epoch 1050, val loss: 1.073978066444397
Epoch 1060, training loss: 88.82299041748047 = 1.0737487077713013 + 10.0 * 8.774924278259277
Epoch 1060, val loss: 1.07384192943573
Epoch 1070, training loss: 88.82527923583984 = 1.0736156702041626 + 10.0 * 8.775166511535645
Epoch 1070, val loss: 1.0737100839614868
Epoch 1080, training loss: 88.85708618164062 = 1.0734835863113403 + 10.0 * 8.778360366821289
Epoch 1080, val loss: 1.0735795497894287
Epoch 1090, training loss: 88.87784576416016 = 1.0733503103256226 + 10.0 * 8.780449867248535
Epoch 1090, val loss: 1.0734468698501587
Epoch 1100, training loss: 88.89917755126953 = 1.0732203722000122 + 10.0 * 8.78259563446045
Epoch 1100, val loss: 1.0733180046081543
Epoch 1110, training loss: 88.90316772460938 = 1.0730857849121094 + 10.0 * 8.783008575439453
Epoch 1110, val loss: 1.073183298110962
Epoch 1120, training loss: 88.9088134765625 = 1.07295560836792 + 10.0 * 8.783586502075195
Epoch 1120, val loss: 1.0730572938919067
Epoch 1130, training loss: 88.94530487060547 = 1.0728261470794678 + 10.0 * 8.787248611450195
Epoch 1130, val loss: 1.072930932044983
Epoch 1140, training loss: 88.96395111083984 = 1.0726977586746216 + 10.0 * 8.789125442504883
Epoch 1140, val loss: 1.0728014707565308
Epoch 1150, training loss: 88.92635345458984 = 1.072567105293274 + 10.0 * 8.785378456115723
Epoch 1150, val loss: 1.0726752281188965
Epoch 1160, training loss: 88.98576354980469 = 1.0724411010742188 + 10.0 * 8.791332244873047
Epoch 1160, val loss: 1.0725517272949219
Epoch 1170, training loss: 89.02474212646484 = 1.0723167657852173 + 10.0 * 8.795242309570312
Epoch 1170, val loss: 1.0724270343780518
Epoch 1180, training loss: 88.9647445678711 = 1.0721813440322876 + 10.0 * 8.78925609588623
Epoch 1180, val loss: 1.0722975730895996
Epoch 1190, training loss: 88.96613311767578 = 1.0720568895339966 + 10.0 * 8.789407730102539
Epoch 1190, val loss: 1.072173833847046
Epoch 1200, training loss: 89.05229187011719 = 1.0719398260116577 + 10.0 * 8.798035621643066
Epoch 1200, val loss: 1.072056531906128
Epoch 1210, training loss: 89.09369659423828 = 1.0718169212341309 + 10.0 * 8.8021879196167
Epoch 1210, val loss: 1.0719352960586548
Epoch 1220, training loss: 89.1015625 = 1.071694016456604 + 10.0 * 8.802987098693848
Epoch 1220, val loss: 1.0718128681182861
Epoch 1230, training loss: 89.05687713623047 = 1.0715681314468384 + 10.0 * 8.798530578613281
Epoch 1230, val loss: 1.0716910362243652
Epoch 1240, training loss: 89.10616302490234 = 1.0714472532272339 + 10.0 * 8.803471565246582
Epoch 1240, val loss: 1.0715734958648682
Epoch 1250, training loss: 89.14546966552734 = 1.0713304281234741 + 10.0 * 8.807414054870605
Epoch 1250, val loss: 1.071457028388977
Epoch 1260, training loss: 89.14459991455078 = 1.0712087154388428 + 10.0 * 8.807339668273926
Epoch 1260, val loss: 1.0713379383087158
Epoch 1270, training loss: 89.1281509399414 = 1.0710861682891846 + 10.0 * 8.805706977844238
Epoch 1270, val loss: 1.0712180137634277
Epoch 1280, training loss: 89.15052795410156 = 1.0709683895111084 + 10.0 * 8.807955741882324
Epoch 1280, val loss: 1.0711016654968262
Epoch 1290, training loss: 89.19355773925781 = 1.0708531141281128 + 10.0 * 8.812270164489746
Epoch 1290, val loss: 1.0709881782531738
Epoch 1300, training loss: 89.22773742675781 = 1.0707380771636963 + 10.0 * 8.815699577331543
Epoch 1300, val loss: 1.0708732604980469
Epoch 1310, training loss: 89.19789123535156 = 1.0706180334091187 + 10.0 * 8.812726974487305
Epoch 1310, val loss: 1.0707533359527588
Epoch 1320, training loss: 89.1590576171875 = 1.0704963207244873 + 10.0 * 8.808856010437012
Epoch 1320, val loss: 1.0706367492675781
Epoch 1330, training loss: 89.18079376220703 = 1.0703833103179932 + 10.0 * 8.811040878295898
Epoch 1330, val loss: 1.0705252885818481
Epoch 1340, training loss: 89.24668884277344 = 1.0702733993530273 + 10.0 * 8.817641258239746
Epoch 1340, val loss: 1.0704175233840942
Epoch 1350, training loss: 89.2405014038086 = 1.0701558589935303 + 10.0 * 8.817034721374512
Epoch 1350, val loss: 1.0703026056289673
Epoch 1360, training loss: 89.34970092773438 = 1.0700465440750122 + 10.0 * 8.82796573638916
Epoch 1360, val loss: 1.070192575454712
Epoch 1370, training loss: 89.25961303710938 = 1.0699305534362793 + 10.0 * 8.818967819213867
Epoch 1370, val loss: 1.0700808763504028
Epoch 1380, training loss: 89.29991149902344 = 1.069822072982788 + 10.0 * 8.82300853729248
Epoch 1380, val loss: 1.0699740648269653
Epoch 1390, training loss: 89.34781646728516 = 1.0697121620178223 + 10.0 * 8.827810287475586
Epoch 1390, val loss: 1.0698639154434204
Epoch 1400, training loss: 89.25912475585938 = 1.069589376449585 + 10.0 * 8.818953514099121
Epoch 1400, val loss: 1.0697507858276367
Epoch 1410, training loss: 89.30400085449219 = 1.0694862604141235 + 10.0 * 8.823451042175293
Epoch 1410, val loss: 1.0696463584899902
Epoch 1420, training loss: 89.34263610839844 = 1.0693823099136353 + 10.0 * 8.827325820922852
Epoch 1420, val loss: 1.0695416927337646
Epoch 1430, training loss: 89.41107177734375 = 1.0692780017852783 + 10.0 * 8.834179878234863
Epoch 1430, val loss: 1.069440245628357
Epoch 1440, training loss: 89.41255187988281 = 1.069169282913208 + 10.0 * 8.834338188171387
Epoch 1440, val loss: 1.0693331956863403
Epoch 1450, training loss: 89.42179107666016 = 1.0690603256225586 + 10.0 * 8.835272789001465
Epoch 1450, val loss: 1.069225788116455
Epoch 1460, training loss: 89.46190643310547 = 1.0689570903778076 + 10.0 * 8.839295387268066
Epoch 1460, val loss: 1.0691245794296265
Epoch 1470, training loss: 89.51676177978516 = 1.0688543319702148 + 10.0 * 8.8447904586792
Epoch 1470, val loss: 1.0690231323242188
Epoch 1480, training loss: 89.4787368774414 = 1.0687451362609863 + 10.0 * 8.840998649597168
Epoch 1480, val loss: 1.0689141750335693
Epoch 1490, training loss: 89.4915771484375 = 1.0686421394348145 + 10.0 * 8.842293739318848
Epoch 1490, val loss: 1.0688141584396362
Epoch 1500, training loss: 89.54667663574219 = 1.0685421228408813 + 10.0 * 8.847813606262207
Epoch 1500, val loss: 1.0687164068222046
Epoch 1510, training loss: 89.58231353759766 = 1.0684363842010498 + 10.0 * 8.851387977600098
Epoch 1510, val loss: 1.0686134099960327
Epoch 1520, training loss: 89.54991912841797 = 1.0683308839797974 + 10.0 * 8.848158836364746
Epoch 1520, val loss: 1.0685079097747803
Epoch 1530, training loss: 89.54949188232422 = 1.0682291984558105 + 10.0 * 8.848126411437988
Epoch 1530, val loss: 1.0684096813201904
Epoch 1540, training loss: 89.59618377685547 = 1.068131685256958 + 10.0 * 8.852805137634277
Epoch 1540, val loss: 1.068313717842102
Epoch 1550, training loss: 89.62213897705078 = 1.0680314302444458 + 10.0 * 8.8554105758667
Epoch 1550, val loss: 1.0682116746902466
Epoch 1560, training loss: 89.56897735595703 = 1.067926287651062 + 10.0 * 8.850105285644531
Epoch 1560, val loss: 1.0681135654449463
Epoch 1570, training loss: 89.60150909423828 = 1.0678293704986572 + 10.0 * 8.853367805480957
Epoch 1570, val loss: 1.0680184364318848
Epoch 1580, training loss: 89.65638732910156 = 1.0677337646484375 + 10.0 * 8.858865737915039
Epoch 1580, val loss: 1.0679244995117188
Epoch 1590, training loss: 89.67144775390625 = 1.0676348209381104 + 10.0 * 8.860381126403809
Epoch 1590, val loss: 1.067826509475708
Epoch 1600, training loss: 89.6724853515625 = 1.0675350427627563 + 10.0 * 8.860494613647461
Epoch 1600, val loss: 1.0677311420440674
Epoch 1610, training loss: 89.70996856689453 = 1.0674396753311157 + 10.0 * 8.864253044128418
Epoch 1610, val loss: 1.0676372051239014
Epoch 1620, training loss: 89.72984313964844 = 1.0673447847366333 + 10.0 * 8.866250038146973
Epoch 1620, val loss: 1.0675439834594727
Epoch 1630, training loss: 89.7242431640625 = 1.0672491788864136 + 10.0 * 8.865699768066406
Epoch 1630, val loss: 1.0674502849578857
Epoch 1640, training loss: 89.72003936767578 = 1.0671497583389282 + 10.0 * 8.865289688110352
Epoch 1640, val loss: 1.0673551559448242
Epoch 1650, training loss: 89.75419616699219 = 1.0670585632324219 + 10.0 * 8.86871337890625
Epoch 1650, val loss: 1.0672645568847656
Epoch 1660, training loss: 89.81428527832031 = 1.0669673681259155 + 10.0 * 8.874731063842773
Epoch 1660, val loss: 1.0671747922897339
Epoch 1670, training loss: 89.73388671875 = 1.0668652057647705 + 10.0 * 8.86670207977295
Epoch 1670, val loss: 1.0670758485794067
Epoch 1680, training loss: 89.72126007080078 = 1.0667729377746582 + 10.0 * 8.865448951721191
Epoch 1680, val loss: 1.0669841766357422
Epoch 1690, training loss: 89.80097198486328 = 1.0666831731796265 + 10.0 * 8.873429298400879
Epoch 1690, val loss: 1.0668964385986328
Epoch 1700, training loss: 89.83586883544922 = 1.0665947198867798 + 10.0 * 8.876927375793457
Epoch 1700, val loss: 1.0668107271194458
Epoch 1710, training loss: 89.87776184082031 = 1.0665051937103271 + 10.0 * 8.881125450134277
Epoch 1710, val loss: 1.0667228698730469
Epoch 1720, training loss: 89.89086151123047 = 1.0664143562316895 + 10.0 * 8.882444381713867
Epoch 1720, val loss: 1.0666357278823853
Epoch 1730, training loss: 89.90111541748047 = 1.0663244724273682 + 10.0 * 8.883479118347168
Epoch 1730, val loss: 1.066546082496643
Epoch 1740, training loss: 89.91372680664062 = 1.0662356615066528 + 10.0 * 8.884748458862305
Epoch 1740, val loss: 1.0664584636688232
Epoch 1750, training loss: 89.96923065185547 = 1.0661488771438599 + 10.0 * 8.890308380126953
Epoch 1750, val loss: 1.0663756132125854
Epoch 1760, training loss: 89.94535064697266 = 1.0660597085952759 + 10.0 * 8.88792896270752
Epoch 1760, val loss: 1.0662866830825806
Epoch 1770, training loss: 89.94182586669922 = 1.0659700632095337 + 10.0 * 8.887585639953613
Epoch 1770, val loss: 1.0662002563476562
Epoch 1780, training loss: 89.98434448242188 = 1.0658849477767944 + 10.0 * 8.891845703125
Epoch 1780, val loss: 1.0661169290542603
Epoch 1790, training loss: 90.03401184082031 = 1.0658010244369507 + 10.0 * 8.896821022033691
Epoch 1790, val loss: 1.0660350322723389
Epoch 1800, training loss: 90.02039337158203 = 1.065712332725525 + 10.0 * 8.895467758178711
Epoch 1800, val loss: 1.0659494400024414
Epoch 1810, training loss: 90.01231384277344 = 1.0656238794326782 + 10.0 * 8.894669532775879
Epoch 1810, val loss: 1.0658615827560425
Epoch 1820, training loss: 90.04483795166016 = 1.0655410289764404 + 10.0 * 8.897929191589355
Epoch 1820, val loss: 1.0657823085784912
Epoch 1830, training loss: 90.0767593383789 = 1.0654600858688354 + 10.0 * 8.901129722595215
Epoch 1830, val loss: 1.0657031536102295
Epoch 1840, training loss: 90.02220916748047 = 1.0653716325759888 + 10.0 * 8.895684242248535
Epoch 1840, val loss: 1.0656166076660156
Epoch 1850, training loss: 89.99686431884766 = 1.0652852058410645 + 10.0 * 8.893157958984375
Epoch 1850, val loss: 1.0655328035354614
Epoch 1860, training loss: 89.99285125732422 = 1.0651999711990356 + 10.0 * 8.892765045166016
Epoch 1860, val loss: 1.0654497146606445
Epoch 1870, training loss: 90.0312271118164 = 1.0651226043701172 + 10.0 * 8.896610260009766
Epoch 1870, val loss: 1.0653749704360962
Epoch 1880, training loss: 90.09503173828125 = 1.0650466680526733 + 10.0 * 8.902997970581055
Epoch 1880, val loss: 1.0652992725372314
Epoch 1890, training loss: 90.11385345458984 = 1.0649642944335938 + 10.0 * 8.904889106750488
Epoch 1890, val loss: 1.065221905708313
Epoch 1900, training loss: 90.13298797607422 = 1.0648869276046753 + 10.0 * 8.90680980682373
Epoch 1900, val loss: 1.0651440620422363
Epoch 1910, training loss: 90.15310668945312 = 1.064803123474121 + 10.0 * 8.908830642700195
Epoch 1910, val loss: 1.0650618076324463
Epoch 1920, training loss: 90.0868911743164 = 1.0647222995758057 + 10.0 * 8.902216911315918
Epoch 1920, val loss: 1.0649818181991577
Epoch 1930, training loss: 90.08274841308594 = 1.0646432638168335 + 10.0 * 8.901810646057129
Epoch 1930, val loss: 1.0649070739746094
Epoch 1940, training loss: 90.13652801513672 = 1.064568281173706 + 10.0 * 8.907196044921875
Epoch 1940, val loss: 1.0648350715637207
Epoch 1950, training loss: 90.14752197265625 = 1.0644893646240234 + 10.0 * 8.908303260803223
Epoch 1950, val loss: 1.064757227897644
Epoch 1960, training loss: 90.1364974975586 = 1.0644135475158691 + 10.0 * 8.907208442687988
Epoch 1960, val loss: 1.0646823644638062
Epoch 1970, training loss: 90.1849594116211 = 1.0643401145935059 + 10.0 * 8.91206169128418
Epoch 1970, val loss: 1.064611792564392
Epoch 1980, training loss: 90.23162078857422 = 1.0642658472061157 + 10.0 * 8.916735649108887
Epoch 1980, val loss: 1.0645406246185303
Epoch 1990, training loss: 90.22509765625 = 1.0641902685165405 + 10.0 * 8.91609001159668
Epoch 1990, val loss: 1.0644665956497192
Epoch 2000, training loss: 90.23002624511719 = 1.0641130208969116 + 10.0 * 8.91659164428711
Epoch 2000, val loss: 1.0643914937973022
Epoch 2010, training loss: 90.2347412109375 = 1.0640382766723633 + 10.0 * 8.917070388793945
Epoch 2010, val loss: 1.0643209218978882
Epoch 2020, training loss: 90.24796295166016 = 1.063965916633606 + 10.0 * 8.918399810791016
Epoch 2020, val loss: 1.0642492771148682
Epoch 2030, training loss: 90.2403335571289 = 1.063887357711792 + 10.0 * 8.917644500732422
Epoch 2030, val loss: 1.0641748905181885
Epoch 2040, training loss: 90.24494934082031 = 1.063816785812378 + 10.0 * 8.918112754821777
Epoch 2040, val loss: 1.0641030073165894
Epoch 2050, training loss: 90.2715072631836 = 1.0637447834014893 + 10.0 * 8.9207763671875
Epoch 2050, val loss: 1.0640368461608887
Epoch 2060, training loss: 90.30598449707031 = 1.0636754035949707 + 10.0 * 8.924230575561523
Epoch 2060, val loss: 1.0639674663543701
Epoch 2070, training loss: 90.29524230957031 = 1.0636019706726074 + 10.0 * 8.923164367675781
Epoch 2070, val loss: 1.0638952255249023
Epoch 2080, training loss: 90.304443359375 = 1.0635312795639038 + 10.0 * 8.924091339111328
Epoch 2080, val loss: 1.063828468322754
Epoch 2090, training loss: 90.32992553710938 = 1.0634626150131226 + 10.0 * 8.92664623260498
Epoch 2090, val loss: 1.0637614727020264
Epoch 2100, training loss: 90.35149383544922 = 1.0633927583694458 + 10.0 * 8.928810119628906
Epoch 2100, val loss: 1.063693642616272
Epoch 2110, training loss: 90.35111999511719 = 1.0633227825164795 + 10.0 * 8.928779602050781
Epoch 2110, val loss: 1.0636262893676758
Epoch 2120, training loss: 90.37569427490234 = 1.063254952430725 + 10.0 * 8.931243896484375
Epoch 2120, val loss: 1.0635591745376587
Epoch 2130, training loss: 90.38809204101562 = 1.0631859302520752 + 10.0 * 8.932490348815918
Epoch 2130, val loss: 1.0634936094284058
Epoch 2140, training loss: 90.39207458496094 = 1.0631170272827148 + 10.0 * 8.93289566040039
Epoch 2140, val loss: 1.0634254217147827
Epoch 2150, training loss: 90.38414001464844 = 1.0630460977554321 + 10.0 * 8.932108879089355
Epoch 2150, val loss: 1.063357949256897
Epoch 2160, training loss: 90.39158630371094 = 1.0629807710647583 + 10.0 * 8.932860374450684
Epoch 2160, val loss: 1.0632939338684082
Epoch 2170, training loss: 90.41789245605469 = 1.0629148483276367 + 10.0 * 8.935498237609863
Epoch 2170, val loss: 1.0632325410842896
Epoch 2180, training loss: 90.4522705078125 = 1.062849521636963 + 10.0 * 8.938941955566406
Epoch 2180, val loss: 1.0631698369979858
Epoch 2190, training loss: 90.33898162841797 = 1.0627710819244385 + 10.0 * 8.927620887756348
Epoch 2190, val loss: 1.0630861520767212
Epoch 2200, training loss: 90.35758972167969 = 1.0626927614212036 + 10.0 * 8.929490089416504
Epoch 2200, val loss: 1.0630186796188354
Epoch 2210, training loss: 90.1959457397461 = 1.0626124143600464 + 10.0 * 8.91333293914795
Epoch 2210, val loss: 1.0629405975341797
Epoch 2220, training loss: 90.15725708007812 = 1.0625512599945068 + 10.0 * 8.909470558166504
Epoch 2220, val loss: 1.0628856420516968
Epoch 2230, training loss: 90.24038696289062 = 1.0625007152557373 + 10.0 * 8.9177885055542
Epoch 2230, val loss: 1.062833547592163
Epoch 2240, training loss: 90.31053924560547 = 1.06244695186615 + 10.0 * 8.924809455871582
Epoch 2240, val loss: 1.0627814531326294
Epoch 2250, training loss: 90.38894653320312 = 1.062391757965088 + 10.0 * 8.932655334472656
Epoch 2250, val loss: 1.0627264976501465
Epoch 2260, training loss: 90.42691040039062 = 1.0623345375061035 + 10.0 * 8.936457633972168
Epoch 2260, val loss: 1.0626715421676636
Epoch 2270, training loss: 90.46345520019531 = 1.0622740983963013 + 10.0 * 8.940118789672852
Epoch 2270, val loss: 1.062611699104309
Epoch 2280, training loss: 90.47535705566406 = 1.062212586402893 + 10.0 * 8.941314697265625
Epoch 2280, val loss: 1.0625536441802979
Epoch 2290, training loss: 90.48385620117188 = 1.0621521472930908 + 10.0 * 8.942171096801758
Epoch 2290, val loss: 1.0624960660934448
Epoch 2300, training loss: 90.51902770996094 = 1.06209397315979 + 10.0 * 8.945693016052246
Epoch 2300, val loss: 1.0624396800994873
Epoch 2310, training loss: 90.54334259033203 = 1.062035322189331 + 10.0 * 8.94813060760498
Epoch 2310, val loss: 1.062382459640503
Epoch 2320, training loss: 90.57032012939453 = 1.0619747638702393 + 10.0 * 8.950834274291992
Epoch 2320, val loss: 1.062325119972229
Epoch 2330, training loss: 90.56379699707031 = 1.0619157552719116 + 10.0 * 8.950188636779785
Epoch 2330, val loss: 1.0622671842575073
Epoch 2340, training loss: 90.56965637207031 = 1.0618562698364258 + 10.0 * 8.950779914855957
Epoch 2340, val loss: 1.0622105598449707
Epoch 2350, training loss: 90.59407043457031 = 1.0617990493774414 + 10.0 * 8.953227043151855
Epoch 2350, val loss: 1.0621564388275146
Epoch 2360, training loss: 90.59443664550781 = 1.061740517616272 + 10.0 * 8.953269958496094
Epoch 2360, val loss: 1.0621014833450317
Epoch 2370, training loss: 90.5557861328125 = 1.0616768598556519 + 10.0 * 8.949411392211914
Epoch 2370, val loss: 1.0620423555374146
Epoch 2380, training loss: 90.51348114013672 = 1.0616132020950317 + 10.0 * 8.945186614990234
Epoch 2380, val loss: 1.0619817972183228
Epoch 2390, training loss: 90.57479095458984 = 1.0615622997283936 + 10.0 * 8.951322555541992
Epoch 2390, val loss: 1.0619302988052368
Epoch 2400, training loss: 90.64530944824219 = 1.061509609222412 + 10.0 * 8.958379745483398
Epoch 2400, val loss: 1.0618798732757568
Epoch 2410, training loss: 90.69635009765625 = 1.0614572763442993 + 10.0 * 8.963489532470703
Epoch 2410, val loss: 1.0618302822113037
Epoch 2420, training loss: 90.6502914428711 = 1.0613970756530762 + 10.0 * 8.958889961242676
Epoch 2420, val loss: 1.0617711544036865
Epoch 2430, training loss: 90.66083526611328 = 1.0613436698913574 + 10.0 * 8.959949493408203
Epoch 2430, val loss: 1.0617194175720215
Epoch 2440, training loss: 90.7071762084961 = 1.0612925291061401 + 10.0 * 8.964588165283203
Epoch 2440, val loss: 1.061671495437622
Epoch 2450, training loss: 90.7594223022461 = 1.0612417459487915 + 10.0 * 8.969818115234375
Epoch 2450, val loss: 1.0616228580474854
Epoch 2460, training loss: 90.71475219726562 = 1.0611741542816162 + 10.0 * 8.965357780456543
Epoch 2460, val loss: 1.0615431070327759
Epoch 2470, training loss: 90.61212158203125 = 1.061110258102417 + 10.0 * 8.955101013183594
Epoch 2470, val loss: 1.061492919921875
Epoch 2480, training loss: 90.62144470214844 = 1.0610620975494385 + 10.0 * 8.956037521362305
Epoch 2480, val loss: 1.061448097229004
Epoch 2490, training loss: 90.68387603759766 = 1.0610153675079346 + 10.0 * 8.962285995483398
Epoch 2490, val loss: 1.0614060163497925
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.39666666666666667
0.8133739042237196
=== training gcn model ===
Epoch 0, training loss: 101.60163879394531 = 1.0998083353042603 + 10.0 * 10.050183296203613
Epoch 0, val loss: 1.099726915359497
Epoch 10, training loss: 97.91881561279297 = 1.099242091178894 + 10.0 * 9.681957244873047
Epoch 10, val loss: 1.0992013216018677
Epoch 20, training loss: 96.32698059082031 = 1.098952054977417 + 10.0 * 9.522802352905273
Epoch 20, val loss: 1.0989142656326294
Epoch 30, training loss: 95.09329223632812 = 1.0986926555633545 + 10.0 * 9.399459838867188
Epoch 30, val loss: 1.0986627340316772
Epoch 40, training loss: 94.13136291503906 = 1.0986207723617554 + 10.0 * 9.303274154663086
Epoch 40, val loss: 1.0986151695251465
Epoch 50, training loss: 93.3496322631836 = 1.098613977432251 + 10.0 * 9.225101470947266
Epoch 50, val loss: 1.098612904548645
Epoch 60, training loss: 92.69166564941406 = 1.0986131429672241 + 10.0 * 9.159305572509766
Epoch 60, val loss: 1.098612666130066
Epoch 70, training loss: 92.12601470947266 = 1.0986127853393555 + 10.0 * 9.102740287780762
Epoch 70, val loss: 1.0986127853393555
Epoch 80, training loss: 91.62993621826172 = 1.0986124277114868 + 10.0 * 9.053133010864258
Epoch 80, val loss: 1.0986123085021973
Epoch 90, training loss: 91.2059555053711 = 1.0986121892929077 + 10.0 * 9.010734558105469
Epoch 90, val loss: 1.0986120700836182
Epoch 100, training loss: 90.8316650390625 = 1.0986119508743286 + 10.0 * 8.973305702209473
Epoch 100, val loss: 1.0986119508743286
Epoch 110, training loss: 90.50405883789062 = 1.0986117124557495 + 10.0 * 8.940545082092285
Epoch 110, val loss: 1.098611831665039
Epoch 120, training loss: 90.21725463867188 = 1.09861159324646 + 10.0 * 8.911864280700684
Epoch 120, val loss: 1.0986117124557495
Epoch 130, training loss: 89.96499633789062 = 1.0986113548278809 + 10.0 * 8.886638641357422
Epoch 130, val loss: 1.0986114740371704
Epoch 140, training loss: 89.74921417236328 = 1.0986112356185913 + 10.0 * 8.865060806274414
Epoch 140, val loss: 1.0986113548278809
Epoch 150, training loss: 89.5627212524414 = 1.0986111164093018 + 10.0 * 8.846410751342773
Epoch 150, val loss: 1.0986113548278809
Epoch 160, training loss: 89.38619995117188 = 1.0986109972000122 + 10.0 * 8.82875919342041
Epoch 160, val loss: 1.0986113548278809
Epoch 170, training loss: 89.26031494140625 = 1.0986109972000122 + 10.0 * 8.816170692443848
Epoch 170, val loss: 1.09861159324646
Epoch 180, training loss: 89.11772918701172 = 1.0986111164093018 + 10.0 * 8.801912307739258
Epoch 180, val loss: 1.09861159324646
Epoch 190, training loss: 89.0144271850586 = 1.0986109972000122 + 10.0 * 8.791582107543945
Epoch 190, val loss: 1.0986117124557495
Epoch 200, training loss: 88.90274810791016 = 1.0986109972000122 + 10.0 * 8.780413627624512
Epoch 200, val loss: 1.0986117124557495
Epoch 210, training loss: 88.81372833251953 = 1.0986108779907227 + 10.0 * 8.771512031555176
Epoch 210, val loss: 1.09861159324646
Epoch 220, training loss: 88.72838592529297 = 1.098610758781433 + 10.0 * 8.762977600097656
Epoch 220, val loss: 1.09861159324646
Epoch 230, training loss: 88.65189361572266 = 1.098610758781433 + 10.0 * 8.755328178405762
Epoch 230, val loss: 1.0986117124557495
Epoch 240, training loss: 88.59415435791016 = 1.098610758781433 + 10.0 * 8.749554634094238
Epoch 240, val loss: 1.09861159324646
Epoch 250, training loss: 88.56573486328125 = 1.0986106395721436 + 10.0 * 8.746712684631348
Epoch 250, val loss: 1.098611831665039
Epoch 260, training loss: 88.51473236083984 = 1.098610520362854 + 10.0 * 8.741612434387207
Epoch 260, val loss: 1.0986117124557495
Epoch 270, training loss: 88.4781723022461 = 1.098610520362854 + 10.0 * 8.737956047058105
Epoch 270, val loss: 1.0986117124557495
Epoch 280, training loss: 88.4489517211914 = 1.0986104011535645 + 10.0 * 8.735033988952637
Epoch 280, val loss: 1.098611831665039
Epoch 290, training loss: 88.38963317871094 = 1.0986104011535645 + 10.0 * 8.72910213470459
Epoch 290, val loss: 1.0986119508743286
Epoch 300, training loss: 88.36481475830078 = 1.0986104011535645 + 10.0 * 8.7266206741333
Epoch 300, val loss: 1.0986120700836182
Epoch 310, training loss: 88.34627532958984 = 1.0986104011535645 + 10.0 * 8.724766731262207
Epoch 310, val loss: 1.0986120700836182
Epoch 320, training loss: 88.34375762939453 = 1.098610281944275 + 10.0 * 8.724514961242676
Epoch 320, val loss: 1.0986120700836182
Epoch 330, training loss: 88.3101577758789 = 1.0986101627349854 + 10.0 * 8.721155166625977
Epoch 330, val loss: 1.0986120700836182
Epoch 340, training loss: 88.27516174316406 = 1.0986100435256958 + 10.0 * 8.717655181884766
Epoch 340, val loss: 1.0986120700836182
Epoch 350, training loss: 88.29483795166016 = 1.0986100435256958 + 10.0 * 8.719622611999512
Epoch 350, val loss: 1.0986123085021973
Epoch 360, training loss: 88.28096008300781 = 1.0986099243164062 + 10.0 * 8.71823501586914
Epoch 360, val loss: 1.0986123085021973
Epoch 370, training loss: 88.2786636352539 = 1.0986099243164062 + 10.0 * 8.718005180358887
Epoch 370, val loss: 1.0986124277114868
Epoch 380, training loss: 88.27698516845703 = 1.0986099243164062 + 10.0 * 8.7178373336792
Epoch 380, val loss: 1.0986125469207764
Epoch 390, training loss: 88.25150299072266 = 1.0986099243164062 + 10.0 * 8.715289115905762
Epoch 390, val loss: 1.0986125469207764
Epoch 400, training loss: 88.23789978027344 = 1.0986098051071167 + 10.0 * 8.713929176330566
Epoch 400, val loss: 1.0986125469207764
Epoch 410, training loss: 88.23973083496094 = 1.0986099243164062 + 10.0 * 8.714112281799316
Epoch 410, val loss: 1.0986125469207764
Epoch 420, training loss: 88.25797271728516 = 1.0986099243164062 + 10.0 * 8.715936660766602
Epoch 420, val loss: 1.098612666130066
Epoch 430, training loss: 88.28412628173828 = 1.0986099243164062 + 10.0 * 8.718551635742188
Epoch 430, val loss: 1.0986125469207764
Epoch 440, training loss: 88.2609634399414 = 1.0986098051071167 + 10.0 * 8.716235160827637
Epoch 440, val loss: 1.098612666130066
Epoch 450, training loss: 88.29000854492188 = 1.0986098051071167 + 10.0 * 8.71914005279541
Epoch 450, val loss: 1.098612666130066
Epoch 460, training loss: 88.28357696533203 = 1.0986096858978271 + 10.0 * 8.718496322631836
Epoch 460, val loss: 1.0986127853393555
Epoch 470, training loss: 88.29248046875 = 1.0986096858978271 + 10.0 * 8.71938705444336
Epoch 470, val loss: 1.098612904548645
Epoch 480, training loss: 88.28439331054688 = 1.0986095666885376 + 10.0 * 8.718578338623047
Epoch 480, val loss: 1.098612904548645
Epoch 490, training loss: 88.28050231933594 = 1.0986095666885376 + 10.0 * 8.718189239501953
Epoch 490, val loss: 1.0986127853393555
Epoch 500, training loss: 88.3319320678711 = 1.0986095666885376 + 10.0 * 8.723332405090332
Epoch 500, val loss: 1.0986127853393555
Epoch 510, training loss: 88.3505859375 = 1.0986093282699585 + 10.0 * 8.725197792053223
Epoch 510, val loss: 1.0986127853393555
Epoch 520, training loss: 88.36904907226562 = 1.0986093282699585 + 10.0 * 8.727044105529785
Epoch 520, val loss: 1.098612666130066
Epoch 530, training loss: 88.34672546386719 = 1.0986089706420898 + 10.0 * 8.724811553955078
Epoch 530, val loss: 1.0986123085021973
Epoch 540, training loss: 88.37999725341797 = 1.0986087322235107 + 10.0 * 8.72813892364502
Epoch 540, val loss: 1.0986120700836182
Epoch 550, training loss: 88.38201141357422 = 1.0986087322235107 + 10.0 * 8.728340148925781
Epoch 550, val loss: 1.098611831665039
Epoch 560, training loss: 88.3982925415039 = 1.098608374595642 + 10.0 * 8.729968070983887
Epoch 560, val loss: 1.0986117124557495
Epoch 570, training loss: 88.4307632446289 = 1.0986078977584839 + 10.0 * 8.73321533203125
Epoch 570, val loss: 1.0986113548278809
Epoch 580, training loss: 88.39288330078125 = 1.0986075401306152 + 10.0 * 8.729427337646484
Epoch 580, val loss: 1.0986108779907227
Epoch 590, training loss: 88.43485260009766 = 1.0986071825027466 + 10.0 * 8.733624458312988
Epoch 590, val loss: 1.0986104011535645
Epoch 600, training loss: 88.45838165283203 = 1.098606824874878 + 10.0 * 8.735977172851562
Epoch 600, val loss: 1.0986096858978271
Epoch 610, training loss: 88.44116973876953 = 1.098605990409851 + 10.0 * 8.734256744384766
Epoch 610, val loss: 1.0986089706420898
Epoch 620, training loss: 88.39241027832031 = 1.0986052751541138 + 10.0 * 8.72938060760498
Epoch 620, val loss: 1.0986080169677734
Epoch 630, training loss: 88.42134094238281 = 1.0986047983169556 + 10.0 * 8.732274055480957
Epoch 630, val loss: 1.0986065864562988
Epoch 640, training loss: 88.41053771972656 = 1.0986042022705078 + 10.0 * 8.731193542480469
Epoch 640, val loss: 1.098605990409851
Epoch 650, training loss: 88.4574966430664 = 1.098603367805481 + 10.0 * 8.735889434814453
Epoch 650, val loss: 1.098604440689087
Epoch 660, training loss: 88.47769165039062 = 1.0986024141311646 + 10.0 * 8.737909317016602
Epoch 660, val loss: 1.0986034870147705
Epoch 670, training loss: 88.49144744873047 = 1.09860098361969 + 10.0 * 8.73928451538086
Epoch 670, val loss: 1.0986013412475586
Epoch 680, training loss: 88.51776885986328 = 1.0986003875732422 + 10.0 * 8.74191665649414
Epoch 680, val loss: 1.098599910736084
Epoch 690, training loss: 88.5744857788086 = 1.0985989570617676 + 10.0 * 8.747588157653809
Epoch 690, val loss: 1.0985981225967407
Epoch 700, training loss: 88.55662536621094 = 1.098597526550293 + 10.0 * 8.745802879333496
Epoch 700, val loss: 1.098596453666687
Epoch 710, training loss: 88.58946990966797 = 1.0985958576202393 + 10.0 * 8.7490873336792
Epoch 710, val loss: 1.098594307899475
Epoch 720, training loss: 88.60012817382812 = 1.0985949039459229 + 10.0 * 8.750153541564941
Epoch 720, val loss: 1.0985926389694214
Epoch 730, training loss: 88.58551025390625 = 1.09859299659729 + 10.0 * 8.74869155883789
Epoch 730, val loss: 1.098590612411499
Epoch 740, training loss: 88.59107971191406 = 1.0985908508300781 + 10.0 * 8.749248504638672
Epoch 740, val loss: 1.098588228225708
Epoch 750, training loss: 88.62590789794922 = 1.0985890626907349 + 10.0 * 8.752732276916504
Epoch 750, val loss: 1.0985865592956543
Epoch 760, training loss: 88.68228149414062 = 1.0985873937606812 + 10.0 * 8.758369445800781
Epoch 760, val loss: 1.098584771156311
Epoch 770, training loss: 88.65965270996094 = 1.0985848903656006 + 10.0 * 8.75610637664795
Epoch 770, val loss: 1.0985827445983887
Epoch 780, training loss: 88.64404296875 = 1.098583698272705 + 10.0 * 8.754545211791992
Epoch 780, val loss: 1.098581075668335
Epoch 790, training loss: 88.68769836425781 = 1.0985817909240723 + 10.0 * 8.758912086486816
Epoch 790, val loss: 1.0985791683197021
Epoch 800, training loss: 88.68052673339844 = 1.0985798835754395 + 10.0 * 8.758194923400879
Epoch 800, val loss: 1.0985774993896484
Epoch 810, training loss: 88.71051788330078 = 1.0985780954360962 + 10.0 * 8.761194229125977
Epoch 810, val loss: 1.0985759496688843
Epoch 820, training loss: 88.7366714477539 = 1.0985769033432007 + 10.0 * 8.763809204101562
Epoch 820, val loss: 1.0985736846923828
Epoch 830, training loss: 88.75634002685547 = 1.098575234413147 + 10.0 * 8.765776634216309
Epoch 830, val loss: 1.0985724925994873
Epoch 840, training loss: 88.7720947265625 = 1.0985733270645142 + 10.0 * 8.767352104187012
Epoch 840, val loss: 1.0985714197158813
Epoch 850, training loss: 88.79362487792969 = 1.09857177734375 + 10.0 * 8.769505500793457
Epoch 850, val loss: 1.098569393157959
Epoch 860, training loss: 88.81966400146484 = 1.0985685586929321 + 10.0 * 8.772109031677246
Epoch 860, val loss: 1.0985661745071411
Epoch 870, training loss: 88.86164093017578 = 1.0985678434371948 + 10.0 * 8.776308059692383
Epoch 870, val loss: 1.0985651016235352
Epoch 880, training loss: 88.85728454589844 = 1.0985664129257202 + 10.0 * 8.775872230529785
Epoch 880, val loss: 1.0985630750656128
Epoch 890, training loss: 88.84286499023438 = 1.098563551902771 + 10.0 * 8.774430274963379
Epoch 890, val loss: 1.09856116771698
Epoch 900, training loss: 88.86819458007812 = 1.098562479019165 + 10.0 * 8.776963233947754
Epoch 900, val loss: 1.0985599756240845
Epoch 910, training loss: 88.92277526855469 = 1.0985615253448486 + 10.0 * 8.782421112060547
Epoch 910, val loss: 1.0985585451126099
Epoch 920, training loss: 88.94806671142578 = 1.0985602140426636 + 10.0 * 8.784951210021973
Epoch 920, val loss: 1.0985567569732666
Epoch 930, training loss: 88.92697143554688 = 1.0985586643218994 + 10.0 * 8.782841682434082
Epoch 930, val loss: 1.0985543727874756
Epoch 940, training loss: 88.9259033203125 = 1.0985567569732666 + 10.0 * 8.782734870910645
Epoch 940, val loss: 1.09855318069458
Epoch 950, training loss: 88.94985961914062 = 1.0985554456710815 + 10.0 * 8.785130500793457
Epoch 950, val loss: 1.098552942276001
Epoch 960, training loss: 89.00798034667969 = 1.0985552072525024 + 10.0 * 8.790942192077637
Epoch 960, val loss: 1.098551630973816
Epoch 970, training loss: 89.00955200195312 = 1.0985534191131592 + 10.0 * 8.791099548339844
Epoch 970, val loss: 1.0985502004623413
Epoch 980, training loss: 89.024658203125 = 1.0985530614852905 + 10.0 * 8.792610168457031
Epoch 980, val loss: 1.0985486507415771
Epoch 990, training loss: 89.02715301513672 = 1.0985500812530518 + 10.0 * 8.79286003112793
Epoch 990, val loss: 1.0985463857650757
Epoch 1000, training loss: 89.0719985961914 = 1.098550796508789 + 10.0 * 8.797345161437988
Epoch 1000, val loss: 1.0985467433929443
Epoch 1010, training loss: 89.09752655029297 = 1.0985496044158936 + 10.0 * 8.799898147583008
Epoch 1010, val loss: 1.0985465049743652
Epoch 1020, training loss: 89.08704376220703 = 1.098547339439392 + 10.0 * 8.798849105834961
Epoch 1020, val loss: 1.098544955253601
Epoch 1030, training loss: 89.04440307617188 = 1.0985474586486816 + 10.0 * 8.794585227966309
Epoch 1030, val loss: 1.0985432863235474
Epoch 1040, training loss: 89.1019058227539 = 1.098548412322998 + 10.0 * 8.800335884094238
Epoch 1040, val loss: 1.0985438823699951
Epoch 1050, training loss: 89.17052459716797 = 1.098545789718628 + 10.0 * 8.807197570800781
Epoch 1050, val loss: 1.0985431671142578
Epoch 1060, training loss: 89.1889419555664 = 1.0985445976257324 + 10.0 * 8.809040069580078
Epoch 1060, val loss: 1.0985416173934937
Epoch 1070, training loss: 89.16946411132812 = 1.0985437631607056 + 10.0 * 8.807092666625977
Epoch 1070, val loss: 1.0985406637191772
Epoch 1080, training loss: 89.18656921386719 = 1.098543643951416 + 10.0 * 8.808802604675293
Epoch 1080, val loss: 1.0985397100448608
Epoch 1090, training loss: 89.21492767333984 = 1.0985440015792847 + 10.0 * 8.811638832092285
Epoch 1090, val loss: 1.0985394716262817
Epoch 1100, training loss: 89.24231719970703 = 1.0985426902770996 + 10.0 * 8.814377784729004
Epoch 1100, val loss: 1.098538875579834
Epoch 1110, training loss: 89.29609680175781 = 1.0985422134399414 + 10.0 * 8.819755554199219
Epoch 1110, val loss: 1.0985387563705444
Epoch 1120, training loss: 89.28199005126953 = 1.0985411405563354 + 10.0 * 8.818345069885254
Epoch 1120, val loss: 1.098536729812622
Epoch 1130, training loss: 89.29463958740234 = 1.0985406637191772 + 10.0 * 8.819609642028809
Epoch 1130, val loss: 1.0985362529754639
Epoch 1140, training loss: 89.30509185791016 = 1.098540186882019 + 10.0 * 8.82065486907959
Epoch 1140, val loss: 1.0985360145568848
Epoch 1150, training loss: 89.31790924072266 = 1.0985392332077026 + 10.0 * 8.82193660736084
Epoch 1150, val loss: 1.0985352993011475
Epoch 1160, training loss: 89.34431457519531 = 1.0985389947891235 + 10.0 * 8.824577331542969
Epoch 1160, val loss: 1.0985360145568848
Epoch 1170, training loss: 89.35183715820312 = 1.098539113998413 + 10.0 * 8.825329780578613
Epoch 1170, val loss: 1.0985347032546997
Epoch 1180, training loss: 89.39031219482422 = 1.0985383987426758 + 10.0 * 8.829176902770996
Epoch 1180, val loss: 1.0985337495803833
Epoch 1190, training loss: 89.38501739501953 = 1.0985381603240967 + 10.0 * 8.82864761352539
Epoch 1190, val loss: 1.0985331535339355
Epoch 1200, training loss: 89.41981506347656 = 1.0985379219055176 + 10.0 * 8.832127571105957
Epoch 1200, val loss: 1.0985333919525146
Epoch 1210, training loss: 89.41905975341797 = 1.0985368490219116 + 10.0 * 8.832052230834961
Epoch 1210, val loss: 1.0985326766967773
Epoch 1220, training loss: 89.42755889892578 = 1.098536491394043 + 10.0 * 8.832902908325195
Epoch 1220, val loss: 1.0985331535339355
Epoch 1230, training loss: 89.40313720703125 = 1.098533034324646 + 10.0 * 8.830460548400879
Epoch 1230, val loss: 1.098524570465088
Epoch 1240, training loss: 89.09070587158203 = 1.098526954650879 + 10.0 * 8.79921817779541
Epoch 1240, val loss: 1.0985227823257446
Epoch 1250, training loss: 88.88441467285156 = 1.098526120185852 + 10.0 * 8.778589248657227
Epoch 1250, val loss: 1.0985203981399536
Epoch 1260, training loss: 89.11455535888672 = 1.0985331535339355 + 10.0 * 8.801602363586426
Epoch 1260, val loss: 1.0985281467437744
Epoch 1270, training loss: 89.13771057128906 = 1.098530650138855 + 10.0 * 8.80391788482666
Epoch 1270, val loss: 1.0985255241394043
Epoch 1280, training loss: 89.21797943115234 = 1.0985329151153564 + 10.0 * 8.811944961547852
Epoch 1280, val loss: 1.0985277891159058
Epoch 1290, training loss: 89.27789306640625 = 1.0985324382781982 + 10.0 * 8.817935943603516
Epoch 1290, val loss: 1.098528265953064
Epoch 1300, training loss: 89.29434967041016 = 1.0985320806503296 + 10.0 * 8.819581985473633
Epoch 1300, val loss: 1.0985268354415894
Epoch 1310, training loss: 89.34349822998047 = 1.098532795906067 + 10.0 * 8.824496269226074
Epoch 1310, val loss: 1.098528265953064
Epoch 1320, training loss: 89.4016342163086 = 1.098533272743225 + 10.0 * 8.830309867858887
Epoch 1320, val loss: 1.0985281467437744
Epoch 1330, training loss: 89.39086151123047 = 1.0985324382781982 + 10.0 * 8.829233169555664
Epoch 1330, val loss: 1.0985279083251953
Epoch 1340, training loss: 89.40510559082031 = 1.09853196144104 + 10.0 * 8.830657005310059
Epoch 1340, val loss: 1.098527431488037
Epoch 1350, training loss: 89.46003723144531 = 1.098532795906067 + 10.0 * 8.836150169372559
Epoch 1350, val loss: 1.0985277891159058
Epoch 1360, training loss: 89.49654388427734 = 1.0985331535339355 + 10.0 * 8.839800834655762
Epoch 1360, val loss: 1.0985279083251953
Epoch 1370, training loss: 89.49288940429688 = 1.0985324382781982 + 10.0 * 8.839435577392578
Epoch 1370, val loss: 1.098527431488037
Epoch 1380, training loss: 89.53998565673828 = 1.0985324382781982 + 10.0 * 8.844144821166992
Epoch 1380, val loss: 1.0985273122787476
Epoch 1390, training loss: 89.53091430664062 = 1.0985329151153564 + 10.0 * 8.84323787689209
Epoch 1390, val loss: 1.0985273122787476
Epoch 1400, training loss: 89.49420928955078 = 1.0985326766967773 + 10.0 * 8.839567184448242
Epoch 1400, val loss: 1.098526954650879
Epoch 1410, training loss: 89.55513763427734 = 1.0985316038131714 + 10.0 * 8.845660209655762
Epoch 1410, val loss: 1.0985262393951416
Epoch 1420, training loss: 89.60348510742188 = 1.0985311269760132 + 10.0 * 8.850495338439941
Epoch 1420, val loss: 1.0985262393951416
Epoch 1430, training loss: 89.60694885253906 = 1.098530888557434 + 10.0 * 8.850841522216797
Epoch 1430, val loss: 1.0985268354415894
Epoch 1440, training loss: 89.62635803222656 = 1.0985320806503296 + 10.0 * 8.852782249450684
Epoch 1440, val loss: 1.0985270738601685
Epoch 1450, training loss: 89.71855926513672 = 1.0985321998596191 + 10.0 * 8.8620023727417
Epoch 1450, val loss: 1.0985268354415894
Epoch 1460, training loss: 89.72189331054688 = 1.0985304117202759 + 10.0 * 8.862336158752441
Epoch 1460, val loss: 1.0985240936279297
Epoch 1470, training loss: 89.6424789428711 = 1.0985307693481445 + 10.0 * 8.854394912719727
Epoch 1470, val loss: 1.098525047302246
Epoch 1480, training loss: 89.59390258789062 = 1.0985277891159058 + 10.0 * 8.84953784942627
Epoch 1480, val loss: 1.0985223054885864
Epoch 1490, training loss: 89.63595581054688 = 1.098529577255249 + 10.0 * 8.853742599487305
Epoch 1490, val loss: 1.0985243320465088
Epoch 1500, training loss: 89.72010040283203 = 1.0985307693481445 + 10.0 * 8.862156867980957
Epoch 1500, val loss: 1.0985257625579834
Epoch 1510, training loss: 89.7628402709961 = 1.0985304117202759 + 10.0 * 8.866430282592773
Epoch 1510, val loss: 1.0985256433486938
Epoch 1520, training loss: 89.7178726196289 = 1.0985294580459595 + 10.0 * 8.861934661865234
Epoch 1520, val loss: 1.0985254049301147
Epoch 1530, training loss: 89.73920440673828 = 1.098530650138855 + 10.0 * 8.864067077636719
Epoch 1530, val loss: 1.0985249280929565
Epoch 1540, training loss: 89.81635284423828 = 1.0985304117202759 + 10.0 * 8.871782302856445
Epoch 1540, val loss: 1.0985254049301147
Epoch 1550, training loss: 89.82654571533203 = 1.098528504371643 + 10.0 * 8.872801780700684
Epoch 1550, val loss: 1.0985239744186401
Epoch 1560, training loss: 89.79170227050781 = 1.0985299348831177 + 10.0 * 8.869317054748535
Epoch 1560, val loss: 1.0985236167907715
Epoch 1570, training loss: 89.86141967773438 = 1.0985298156738281 + 10.0 * 8.876288414001465
Epoch 1570, val loss: 1.098524808883667
Epoch 1580, training loss: 89.90388488769531 = 1.0985304117202759 + 10.0 * 8.880535125732422
Epoch 1580, val loss: 1.0985242128372192
Epoch 1590, training loss: 89.87931823730469 = 1.0985298156738281 + 10.0 * 8.87807846069336
Epoch 1590, val loss: 1.098524570465088
Epoch 1600, training loss: 89.8991470336914 = 1.0985301733016968 + 10.0 * 8.880061149597168
Epoch 1600, val loss: 1.0985242128372192
Epoch 1610, training loss: 89.92108917236328 = 1.0985302925109863 + 10.0 * 8.882255554199219
Epoch 1610, val loss: 1.098525047302246
Epoch 1620, training loss: 89.96415710449219 = 1.0985300540924072 + 10.0 * 8.88656234741211
Epoch 1620, val loss: 1.098523736000061
Epoch 1630, training loss: 89.93716430664062 = 1.0985242128372192 + 10.0 * 8.88386344909668
Epoch 1630, val loss: 1.0985212326049805
Epoch 1640, training loss: 89.96636962890625 = 1.098534345626831 + 10.0 * 8.886783599853516
Epoch 1640, val loss: 1.0985246896743774
Epoch 1650, training loss: 89.8785171508789 = 1.0985302925109863 + 10.0 * 8.877998352050781
Epoch 1650, val loss: 1.09852135181427
Epoch 1660, training loss: 89.90446472167969 = 1.0985299348831177 + 10.0 * 8.880593299865723
Epoch 1660, val loss: 1.0985220670700073
Epoch 1670, training loss: 89.96833038330078 = 1.098531723022461 + 10.0 * 8.886980056762695
Epoch 1670, val loss: 1.0985240936279297
Epoch 1680, training loss: 90.02896118164062 = 1.0985324382781982 + 10.0 * 8.89304256439209
Epoch 1680, val loss: 1.098524808883667
Epoch 1690, training loss: 90.05665588378906 = 1.098533034324646 + 10.0 * 8.895812034606934
Epoch 1690, val loss: 1.0985252857208252
Epoch 1700, training loss: 90.01455688476562 = 1.0985329151153564 + 10.0 * 8.891602516174316
Epoch 1700, val loss: 1.098524808883667
Epoch 1710, training loss: 90.037353515625 = 1.0985326766967773 + 10.0 * 8.893881797790527
Epoch 1710, val loss: 1.0985239744186401
Epoch 1720, training loss: 90.09331512451172 = 1.0985339879989624 + 10.0 * 8.8994779586792
Epoch 1720, val loss: 1.0985249280929565
Epoch 1730, training loss: 90.14676666259766 = 1.0985337495803833 + 10.0 * 8.904823303222656
Epoch 1730, val loss: 1.0985256433486938
Epoch 1740, training loss: 90.07931518554688 = 1.0985339879989624 + 10.0 * 8.898077964782715
Epoch 1740, val loss: 1.0985249280929565
Epoch 1750, training loss: 90.03624725341797 = 1.0985318422317505 + 10.0 * 8.893771171569824
Epoch 1750, val loss: 1.0985225439071655
Epoch 1760, training loss: 90.08385467529297 = 1.0985318422317505 + 10.0 * 8.898531913757324
Epoch 1760, val loss: 1.0985232591629028
Epoch 1770, training loss: 90.10869598388672 = 1.0985326766967773 + 10.0 * 8.901016235351562
Epoch 1770, val loss: 1.098523497581482
Epoch 1780, training loss: 90.15520477294922 = 1.098533272743225 + 10.0 * 8.905667304992676
Epoch 1780, val loss: 1.098524808883667
Epoch 1790, training loss: 90.14315032958984 = 1.0985331535339355 + 10.0 * 8.904461860656738
Epoch 1790, val loss: 1.0985238552093506
Epoch 1800, training loss: 90.16412353515625 = 1.098532795906067 + 10.0 * 8.906558990478516
Epoch 1800, val loss: 1.0985242128372192
Epoch 1810, training loss: 90.20842742919922 = 1.098533034324646 + 10.0 * 8.910989761352539
Epoch 1810, val loss: 1.0985239744186401
Epoch 1820, training loss: 90.20923614501953 = 1.0985335111618042 + 10.0 * 8.911069869995117
Epoch 1820, val loss: 1.0985244512557983
Epoch 1830, training loss: 90.04269409179688 = 1.0985280275344849 + 10.0 * 8.894416809082031
Epoch 1830, val loss: 1.0985188484191895
Epoch 1840, training loss: 90.06654357910156 = 1.09852933883667 + 10.0 * 8.896801948547363
Epoch 1840, val loss: 1.0985209941864014
Epoch 1850, training loss: 90.13239288330078 = 1.0985301733016968 + 10.0 * 8.903386116027832
Epoch 1850, val loss: 1.0985209941864014
Epoch 1860, training loss: 90.20990753173828 = 1.0985314846038818 + 10.0 * 8.911137580871582
Epoch 1860, val loss: 1.098522424697876
Epoch 1870, training loss: 90.24850463867188 = 1.0985321998596191 + 10.0 * 8.914997100830078
Epoch 1870, val loss: 1.0985236167907715
Epoch 1880, training loss: 90.25125885009766 = 1.0985320806503296 + 10.0 * 8.91527271270752
Epoch 1880, val loss: 1.0985232591629028
Epoch 1890, training loss: 90.29722595214844 = 1.098533034324646 + 10.0 * 8.919869422912598
Epoch 1890, val loss: 1.0985240936279297
Epoch 1900, training loss: 90.2066879272461 = 1.0985296964645386 + 10.0 * 8.910816192626953
Epoch 1900, val loss: 1.0985208749771118
Epoch 1910, training loss: 90.26380157470703 = 1.0985312461853027 + 10.0 * 8.916526794433594
Epoch 1910, val loss: 1.0985223054885864
Epoch 1920, training loss: 90.34314727783203 = 1.0985325574874878 + 10.0 * 8.924461364746094
Epoch 1920, val loss: 1.098523497581482
Epoch 1930, training loss: 90.36991119384766 = 1.098532795906067 + 10.0 * 8.92713737487793
Epoch 1930, val loss: 1.0985239744186401
Epoch 1940, training loss: 90.32408905029297 = 1.098532795906067 + 10.0 * 8.922555923461914
Epoch 1940, val loss: 1.098523497581482
Epoch 1950, training loss: 90.3739242553711 = 1.098533034324646 + 10.0 * 8.927538871765137
Epoch 1950, val loss: 1.0985239744186401
Epoch 1960, training loss: 90.39765930175781 = 1.0985325574874878 + 10.0 * 8.929912567138672
Epoch 1960, val loss: 1.098523497581482
Epoch 1970, training loss: 90.38042449951172 = 1.0985325574874878 + 10.0 * 8.928189277648926
Epoch 1970, val loss: 1.098522663116455
Epoch 1980, training loss: 90.44137573242188 = 1.0985307693481445 + 10.0 * 8.934284210205078
Epoch 1980, val loss: 1.0985208749771118
Epoch 1990, training loss: 90.10054016113281 = 1.0985231399536133 + 10.0 * 8.900201797485352
Epoch 1990, val loss: 1.098513126373291
Epoch 2000, training loss: 90.0767822265625 = 1.0985256433486938 + 10.0 * 8.897825241088867
Epoch 2000, val loss: 1.0985132455825806
Epoch 2010, training loss: 89.98589324951172 = 1.0985249280929565 + 10.0 * 8.888736724853516
Epoch 2010, val loss: 1.0985137224197388
Epoch 2020, training loss: 90.04270935058594 = 1.098527431488037 + 10.0 * 8.894418716430664
Epoch 2020, val loss: 1.0985157489776611
Epoch 2030, training loss: 90.12063598632812 = 1.0985275506973267 + 10.0 * 8.90221118927002
Epoch 2030, val loss: 1.0985181331634521
Epoch 2040, training loss: 90.22434997558594 = 1.0985301733016968 + 10.0 * 8.912581443786621
Epoch 2040, val loss: 1.0985190868377686
Epoch 2050, training loss: 90.29379272460938 = 1.0985307693481445 + 10.0 * 8.919526100158691
Epoch 2050, val loss: 1.0985188484191895
Epoch 2060, training loss: 90.34925842285156 = 1.0985302925109863 + 10.0 * 8.92507266998291
Epoch 2060, val loss: 1.098520040512085
Epoch 2070, training loss: 90.36402130126953 = 1.0985313653945923 + 10.0 * 8.926548957824707
Epoch 2070, val loss: 1.0985203981399536
Epoch 2080, training loss: 90.38549041748047 = 1.0985313653945923 + 10.0 * 8.928695678710938
Epoch 2080, val loss: 1.0985196828842163
Epoch 2090, training loss: 90.3917465209961 = 1.0985305309295654 + 10.0 * 8.9293212890625
Epoch 2090, val loss: 1.0985199213027954
Epoch 2100, training loss: 90.43499755859375 = 1.0985318422317505 + 10.0 * 8.933646202087402
Epoch 2100, val loss: 1.0985207557678223
Epoch 2110, training loss: 90.44898223876953 = 1.0985316038131714 + 10.0 * 8.93504524230957
Epoch 2110, val loss: 1.0985194444656372
Epoch 2120, training loss: 90.43846893310547 = 1.0985292196273804 + 10.0 * 8.93399429321289
Epoch 2120, val loss: 1.0985174179077148
Epoch 2130, training loss: 90.45101928710938 = 1.098531723022461 + 10.0 * 8.935248374938965
Epoch 2130, val loss: 1.0985195636749268
Epoch 2140, training loss: 90.48863983154297 = 1.0985311269760132 + 10.0 * 8.939010620117188
Epoch 2140, val loss: 1.0985188484191895
Epoch 2150, training loss: 90.48809814453125 = 1.0985301733016968 + 10.0 * 8.938956260681152
Epoch 2150, val loss: 1.0985186100006104
Epoch 2160, training loss: 90.50947570800781 = 1.0985316038131714 + 10.0 * 8.941094398498535
Epoch 2160, val loss: 1.0985195636749268
Epoch 2170, training loss: 90.52320861816406 = 1.0985316038131714 + 10.0 * 8.94246768951416
Epoch 2170, val loss: 1.0985184907913208
Epoch 2180, training loss: 90.54055786132812 = 1.0985313653945923 + 10.0 * 8.944202423095703
Epoch 2180, val loss: 1.0985190868377686
Epoch 2190, training loss: 90.59111785888672 = 1.098531723022461 + 10.0 * 8.949258804321289
Epoch 2190, val loss: 1.0985198020935059
Epoch 2200, training loss: 90.56038665771484 = 1.0985312461853027 + 10.0 * 8.946185111999512
Epoch 2200, val loss: 1.0985183715820312
Epoch 2210, training loss: 90.59912109375 = 1.0985313653945923 + 10.0 * 8.950058937072754
Epoch 2210, val loss: 1.0985196828842163
Epoch 2220, training loss: 90.62654113769531 = 1.0985318422317505 + 10.0 * 8.952800750732422
Epoch 2220, val loss: 1.0985196828842163
Epoch 2230, training loss: 90.65077209472656 = 1.0985314846038818 + 10.0 * 8.95522403717041
Epoch 2230, val loss: 1.0985181331634521
Epoch 2240, training loss: 90.661376953125 = 1.0985312461853027 + 10.0 * 8.956284523010254
Epoch 2240, val loss: 1.0985188484191895
Epoch 2250, training loss: 90.6653060913086 = 1.0985313653945923 + 10.0 * 8.956677436828613
Epoch 2250, val loss: 1.098519206047058
Epoch 2260, training loss: 90.66478729248047 = 1.0985312461853027 + 10.0 * 8.956624984741211
Epoch 2260, val loss: 1.0985182523727417
Epoch 2270, training loss: 90.68157196044922 = 1.0985310077667236 + 10.0 * 8.958303451538086
Epoch 2270, val loss: 1.0985181331634521
Epoch 2280, training loss: 90.70036315917969 = 1.09853196144104 + 10.0 * 8.960183143615723
Epoch 2280, val loss: 1.098519206047058
Epoch 2290, training loss: 90.72286224365234 = 1.09853196144104 + 10.0 * 8.962432861328125
Epoch 2290, val loss: 1.0985177755355835
Epoch 2300, training loss: 90.64488220214844 = 1.098528265953064 + 10.0 * 8.954635620117188
Epoch 2300, val loss: 1.0985130071640015
Epoch 2310, training loss: 90.50933074951172 = 1.0985301733016968 + 10.0 * 8.941080093383789
Epoch 2310, val loss: 1.0985174179077148
Epoch 2320, training loss: 90.42606353759766 = 1.0985242128372192 + 10.0 * 8.932753562927246
Epoch 2320, val loss: 1.0985113382339478
Epoch 2330, training loss: 90.46300506591797 = 1.098526120185852 + 10.0 * 8.936448097229004
Epoch 2330, val loss: 1.0985134840011597
Epoch 2340, training loss: 90.56886291503906 = 1.0985276699066162 + 10.0 * 8.947033882141113
Epoch 2340, val loss: 1.0985145568847656
Epoch 2350, training loss: 90.66397094726562 = 1.0985292196273804 + 10.0 * 8.956544876098633
Epoch 2350, val loss: 1.0985150337219238
Epoch 2360, training loss: 90.72042846679688 = 1.098529577255249 + 10.0 * 8.962190628051758
Epoch 2360, val loss: 1.0985158681869507
Epoch 2370, training loss: 90.73155212402344 = 1.0985294580459595 + 10.0 * 8.963302612304688
Epoch 2370, val loss: 1.0985153913497925
Epoch 2380, training loss: 90.74581909179688 = 1.0985298156738281 + 10.0 * 8.964729309082031
Epoch 2380, val loss: 1.0985159873962402
Epoch 2390, training loss: 90.33000183105469 = 1.0985065698623657 + 10.0 * 8.923150062561035
Epoch 2390, val loss: 1.098495364189148
Epoch 2400, training loss: 90.41722106933594 = 1.098516821861267 + 10.0 * 8.931870460510254
Epoch 2400, val loss: 1.0984998941421509
Epoch 2410, training loss: 90.55020904541016 = 1.0985236167907715 + 10.0 * 8.945168495178223
Epoch 2410, val loss: 1.0985089540481567
Epoch 2420, training loss: 90.625732421875 = 1.0985244512557983 + 10.0 * 8.952720642089844
Epoch 2420, val loss: 1.098509669303894
Epoch 2430, training loss: 90.77143859863281 = 1.0985265970230103 + 10.0 * 8.967290878295898
Epoch 2430, val loss: 1.0985121726989746
Epoch 2440, training loss: 90.88356018066406 = 1.0985283851623535 + 10.0 * 8.978503227233887
Epoch 2440, val loss: 1.0985132455825806
Epoch 2450, training loss: 90.92121124267578 = 1.0985280275344849 + 10.0 * 8.982268333435059
Epoch 2450, val loss: 1.098513126373291
Epoch 2460, training loss: 90.96218872070312 = 1.098529577255249 + 10.0 * 8.986366271972656
Epoch 2460, val loss: 1.0985145568847656
Epoch 2470, training loss: 90.99659729003906 = 1.098528504371643 + 10.0 * 8.98980712890625
Epoch 2470, val loss: 1.0985143184661865
Epoch 2480, training loss: 90.99868774414062 = 1.0985292196273804 + 10.0 * 8.990015983581543
Epoch 2480, val loss: 1.0985143184661865
Epoch 2490, training loss: 91.05499267578125 = 1.0985289812088013 + 10.0 * 8.995646476745605
Epoch 2490, val loss: 1.0985145568847656
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.20594202898550723
0.8155473447801204
The final CL Acc:0.33333, 0.09008, The final GNN Acc:0.81523, 0.00141
raw graph: torch.Size([2, 88648])
add edge: torch.Size([2, 111098])
remove edge: torch.Size([2, 66704])
updated graph: torch.Size([2, 89154])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 100.53624725341797 = 1.1100670099258423 + 10.0 * 9.942617416381836
Epoch 0, val loss: 1.1093394756317139
Epoch 10, training loss: 96.6158447265625 = 1.1095894575119019 + 10.0 * 9.550625801086426
Epoch 10, val loss: 1.1088082790374756
Epoch 20, training loss: 94.8757095336914 = 1.109193205833435 + 10.0 * 9.376651763916016
Epoch 20, val loss: 1.1083852052688599
Epoch 30, training loss: 93.54914093017578 = 1.1088043451309204 + 10.0 * 9.244033813476562
Epoch 30, val loss: 1.1079792976379395
Epoch 40, training loss: 92.52592468261719 = 1.1084164381027222 + 10.0 * 9.141751289367676
Epoch 40, val loss: 1.1075751781463623
Epoch 50, training loss: 91.71977996826172 = 1.1080268621444702 + 10.0 * 9.061175346374512
Epoch 50, val loss: 1.1071679592132568
Epoch 60, training loss: 91.06753540039062 = 1.10763680934906 + 10.0 * 8.995989799499512
Epoch 60, val loss: 1.1067591905593872
Epoch 70, training loss: 90.52717590332031 = 1.1072514057159424 + 10.0 * 8.941991806030273
Epoch 70, val loss: 1.106360912322998
Epoch 80, training loss: 90.06224060058594 = 1.106871485710144 + 10.0 * 8.895536422729492
Epoch 80, val loss: 1.1059658527374268
Epoch 90, training loss: 89.6709976196289 = 1.1065001487731934 + 10.0 * 8.856450080871582
Epoch 90, val loss: 1.1055790185928345
Epoch 100, training loss: 89.33245849609375 = 1.1061348915100098 + 10.0 * 8.822632789611816
Epoch 100, val loss: 1.1051967144012451
Epoch 110, training loss: 89.03508758544922 = 1.1057692766189575 + 10.0 * 8.792932510375977
Epoch 110, val loss: 1.104814887046814
Epoch 120, training loss: 88.7647705078125 = 1.105411410331726 + 10.0 * 8.765935897827148
Epoch 120, val loss: 1.1044400930404663
Epoch 130, training loss: 88.52433013916016 = 1.105061411857605 + 10.0 * 8.741926193237305
Epoch 130, val loss: 1.104073166847229
Epoch 140, training loss: 88.32027435302734 = 1.1047159433364868 + 10.0 * 8.721555709838867
Epoch 140, val loss: 1.1037120819091797
Epoch 150, training loss: 88.13964080810547 = 1.1043797731399536 + 10.0 * 8.703526496887207
Epoch 150, val loss: 1.1033565998077393
Epoch 160, training loss: 87.98806762695312 = 1.1040523052215576 + 10.0 * 8.68840217590332
Epoch 160, val loss: 1.1030128002166748
Epoch 170, training loss: 87.83525848388672 = 1.1037213802337646 + 10.0 * 8.6731538772583
Epoch 170, val loss: 1.1026686429977417
Epoch 180, training loss: 87.70478057861328 = 1.1033979654312134 + 10.0 * 8.660138130187988
Epoch 180, val loss: 1.1023284196853638
Epoch 190, training loss: 87.59293365478516 = 1.1030783653259277 + 10.0 * 8.648984909057617
Epoch 190, val loss: 1.1019937992095947
Epoch 200, training loss: 87.49933624267578 = 1.1027634143829346 + 10.0 * 8.639657974243164
Epoch 200, val loss: 1.1016634702682495
Epoch 210, training loss: 87.42191314697266 = 1.102451205253601 + 10.0 * 8.631946563720703
Epoch 210, val loss: 1.1013349294662476
Epoch 220, training loss: 87.35755157470703 = 1.1021424531936646 + 10.0 * 8.625540733337402
Epoch 220, val loss: 1.101009488105774
Epoch 230, training loss: 87.30658721923828 = 1.101839303970337 + 10.0 * 8.620474815368652
Epoch 230, val loss: 1.1006919145584106
Epoch 240, training loss: 87.2241439819336 = 1.1015377044677734 + 10.0 * 8.612260818481445
Epoch 240, val loss: 1.1003777980804443
Epoch 250, training loss: 87.16812133789062 = 1.1012412309646606 + 10.0 * 8.606687545776367
Epoch 250, val loss: 1.1000659465789795
Epoch 260, training loss: 87.13301086425781 = 1.1009507179260254 + 10.0 * 8.603205680847168
Epoch 260, val loss: 1.0997604131698608
Epoch 270, training loss: 87.1001968383789 = 1.100659728050232 + 10.0 * 8.599953651428223
Epoch 270, val loss: 1.0994560718536377
Epoch 280, training loss: 87.0682373046875 = 1.1003690958023071 + 10.0 * 8.596786499023438
Epoch 280, val loss: 1.099149465560913
Epoch 290, training loss: 87.01725769042969 = 1.1000852584838867 + 10.0 * 8.591717720031738
Epoch 290, val loss: 1.0988503694534302
Epoch 300, training loss: 87.01179504394531 = 1.0998146533966064 + 10.0 * 8.591197967529297
Epoch 300, val loss: 1.0985673666000366
Epoch 310, training loss: 86.98326110839844 = 1.0995368957519531 + 10.0 * 8.588372230529785
Epoch 310, val loss: 1.0982764959335327
Epoch 320, training loss: 86.99507141113281 = 1.0992653369903564 + 10.0 * 8.589580535888672
Epoch 320, val loss: 1.0979918241500854
Epoch 330, training loss: 86.98201751708984 = 1.098994493484497 + 10.0 * 8.588302612304688
Epoch 330, val loss: 1.097704529762268
Epoch 340, training loss: 86.97248840332031 = 1.0987284183502197 + 10.0 * 8.58737564086914
Epoch 340, val loss: 1.0974265336990356
Epoch 350, training loss: 86.95343780517578 = 1.0984594821929932 + 10.0 * 8.585497856140137
Epoch 350, val loss: 1.0971462726593018
Epoch 360, training loss: 86.97161102294922 = 1.098201870918274 + 10.0 * 8.587340354919434
Epoch 360, val loss: 1.0968750715255737
Epoch 370, training loss: 86.96854400634766 = 1.0979435443878174 + 10.0 * 8.58705997467041
Epoch 370, val loss: 1.0966055393218994
Epoch 380, training loss: 86.95628356933594 = 1.0976864099502563 + 10.0 * 8.585859298706055
Epoch 380, val loss: 1.09633469581604
Epoch 390, training loss: 86.94681549072266 = 1.0974315404891968 + 10.0 * 8.584938049316406
Epoch 390, val loss: 1.096068024635315
Epoch 400, training loss: 86.98139953613281 = 1.0971858501434326 + 10.0 * 8.588421821594238
Epoch 400, val loss: 1.0958082675933838
Epoch 410, training loss: 86.97931671142578 = 1.096935749053955 + 10.0 * 8.588237762451172
Epoch 410, val loss: 1.0955487489700317
Epoch 420, training loss: 86.9853744506836 = 1.0966906547546387 + 10.0 * 8.588869094848633
Epoch 420, val loss: 1.0952918529510498
Epoch 430, training loss: 86.97679901123047 = 1.0964452028274536 + 10.0 * 8.588035583496094
Epoch 430, val loss: 1.0950342416763306
Epoch 440, training loss: 86.94852447509766 = 1.096199870109558 + 10.0 * 8.585232734680176
Epoch 440, val loss: 1.0947818756103516
Epoch 450, training loss: 86.96678924560547 = 1.0959656238555908 + 10.0 * 8.587082862854004
Epoch 450, val loss: 1.0945351123809814
Epoch 460, training loss: 86.98544311523438 = 1.0957345962524414 + 10.0 * 8.588971138000488
Epoch 460, val loss: 1.0942920446395874
Epoch 470, training loss: 87.00762939453125 = 1.0955026149749756 + 10.0 * 8.591212272644043
Epoch 470, val loss: 1.0940512418746948
Epoch 480, training loss: 87.0021743774414 = 1.095273494720459 + 10.0 * 8.590689659118652
Epoch 480, val loss: 1.0938109159469604
Epoch 490, training loss: 87.02178192138672 = 1.0950461626052856 + 10.0 * 8.592673301696777
Epoch 490, val loss: 1.0935733318328857
Epoch 500, training loss: 87.07059478759766 = 1.0948240756988525 + 10.0 * 8.597577095031738
Epoch 500, val loss: 1.0933390855789185
Epoch 510, training loss: 87.06781768798828 = 1.0945990085601807 + 10.0 * 8.597322463989258
Epoch 510, val loss: 1.0931059122085571
Epoch 520, training loss: 87.09027099609375 = 1.0943773984909058 + 10.0 * 8.599589347839355
Epoch 520, val loss: 1.0928744077682495
Epoch 530, training loss: 87.10139465332031 = 1.094153881072998 + 10.0 * 8.600724220275879
Epoch 530, val loss: 1.092643141746521
Epoch 540, training loss: 87.10628509521484 = 1.0939396619796753 + 10.0 * 8.601234436035156
Epoch 540, val loss: 1.0924171209335327
Epoch 550, training loss: 87.12291717529297 = 1.0937248468399048 + 10.0 * 8.60291862487793
Epoch 550, val loss: 1.092193841934204
Epoch 560, training loss: 87.14208221435547 = 1.0935114622116089 + 10.0 * 8.604856491088867
Epoch 560, val loss: 1.09197199344635
Epoch 570, training loss: 87.14044189453125 = 1.093300223350525 + 10.0 * 8.604714393615723
Epoch 570, val loss: 1.0917524099349976
Epoch 580, training loss: 87.1468505859375 = 1.093109369277954 + 10.0 * 8.605374336242676
Epoch 580, val loss: 1.0915641784667969
Epoch 590, training loss: 87.17674255371094 = 1.0930354595184326 + 10.0 * 8.608370780944824
Epoch 590, val loss: 1.0914971828460693
Epoch 600, training loss: 87.18479919433594 = 1.0930349826812744 + 10.0 * 8.609176635742188
Epoch 600, val loss: 1.0914844274520874
Epoch 610, training loss: 87.21688842773438 = 1.0930129289627075 + 10.0 * 8.612387657165527
Epoch 610, val loss: 1.0914584398269653
Epoch 620, training loss: 87.2325668334961 = 1.0930041074752808 + 10.0 * 8.613956451416016
Epoch 620, val loss: 1.0914409160614014
Epoch 630, training loss: 87.24700927734375 = 1.0929940938949585 + 10.0 * 8.615401268005371
Epoch 630, val loss: 1.0914225578308105
Epoch 640, training loss: 87.25708770751953 = 1.0929818153381348 + 10.0 * 8.616411209106445
Epoch 640, val loss: 1.091402530670166
Epoch 650, training loss: 87.29483795166016 = 1.0929678678512573 + 10.0 * 8.620187759399414
Epoch 650, val loss: 1.091385006904602
Epoch 660, training loss: 87.19096374511719 = 1.0929526090621948 + 10.0 * 8.609801292419434
Epoch 660, val loss: 1.0913578271865845
Epoch 670, training loss: 87.25269317626953 = 1.0929521322250366 + 10.0 * 8.615974426269531
Epoch 670, val loss: 1.091349482536316
Epoch 680, training loss: 87.28987884521484 = 1.092947244644165 + 10.0 * 8.6196928024292
Epoch 680, val loss: 1.0913379192352295
Epoch 690, training loss: 87.33089447021484 = 1.0929397344589233 + 10.0 * 8.623795509338379
Epoch 690, val loss: 1.0913231372833252
Epoch 700, training loss: 87.3465805053711 = 1.0929309129714966 + 10.0 * 8.625364303588867
Epoch 700, val loss: 1.0913053750991821
Epoch 710, training loss: 87.33072662353516 = 1.0929210186004639 + 10.0 * 8.623781204223633
Epoch 710, val loss: 1.0912904739379883
Epoch 720, training loss: 87.37537384033203 = 1.0929144620895386 + 10.0 * 8.628246307373047
Epoch 720, val loss: 1.0912773609161377
Epoch 730, training loss: 87.42930603027344 = 1.0929096937179565 + 10.0 * 8.633639335632324
Epoch 730, val loss: 1.0912638902664185
Epoch 740, training loss: 87.41886901855469 = 1.0928983688354492 + 10.0 * 8.632596969604492
Epoch 740, val loss: 1.0912466049194336
Epoch 750, training loss: 87.47596740722656 = 1.0928924083709717 + 10.0 * 8.638307571411133
Epoch 750, val loss: 1.0912333726882935
Epoch 760, training loss: 87.49452209472656 = 1.0928845405578613 + 10.0 * 8.64016342163086
Epoch 760, val loss: 1.0912195444107056
Epoch 770, training loss: 87.50161743164062 = 1.0928773880004883 + 10.0 * 8.640873908996582
Epoch 770, val loss: 1.0912057161331177
Epoch 780, training loss: 87.53136444091797 = 1.0928714275360107 + 10.0 * 8.64384937286377
Epoch 780, val loss: 1.0911935567855835
Epoch 790, training loss: 87.55551147460938 = 1.0928654670715332 + 10.0 * 8.646265029907227
Epoch 790, val loss: 1.0911786556243896
Epoch 800, training loss: 87.60020446777344 = 1.0928609371185303 + 10.0 * 8.650734901428223
Epoch 800, val loss: 1.0911678075790405
Epoch 810, training loss: 87.58684539794922 = 1.092850923538208 + 10.0 * 8.649399757385254
Epoch 810, val loss: 1.0911539793014526
Epoch 820, training loss: 87.61573791503906 = 1.0928491353988647 + 10.0 * 8.652288436889648
Epoch 820, val loss: 1.0911445617675781
Epoch 830, training loss: 87.6725845336914 = 1.0928438901901245 + 10.0 * 8.657974243164062
Epoch 830, val loss: 1.0911319255828857
Epoch 840, training loss: 87.6472396850586 = 1.0928362607955933 + 10.0 * 8.655440330505371
Epoch 840, val loss: 1.0911166667938232
Epoch 850, training loss: 87.67955017089844 = 1.0928288698196411 + 10.0 * 8.658672332763672
Epoch 850, val loss: 1.0911054611206055
Epoch 860, training loss: 87.6538314819336 = 1.0928188562393188 + 10.0 * 8.65610122680664
Epoch 860, val loss: 1.091088891029358
Epoch 870, training loss: 87.6735610961914 = 1.092818260192871 + 10.0 * 8.658074378967285
Epoch 870, val loss: 1.0910849571228027
Epoch 880, training loss: 87.71466064453125 = 1.0928162336349487 + 10.0 * 8.66218376159668
Epoch 880, val loss: 1.0910743474960327
Epoch 890, training loss: 87.73686981201172 = 1.092809796333313 + 10.0 * 8.664405822753906
Epoch 890, val loss: 1.0910612344741821
Epoch 900, training loss: 87.74098205566406 = 1.0928057432174683 + 10.0 * 8.664817810058594
Epoch 900, val loss: 1.0910536050796509
Epoch 910, training loss: 87.78892517089844 = 1.0928031206130981 + 10.0 * 8.669611930847168
Epoch 910, val loss: 1.0910457372665405
Epoch 920, training loss: 87.81218719482422 = 1.0927969217300415 + 10.0 * 8.6719388961792
Epoch 920, val loss: 1.0910359621047974
Epoch 930, training loss: 87.78617095947266 = 1.0927914381027222 + 10.0 * 8.66933822631836
Epoch 930, val loss: 1.0910247564315796
Epoch 940, training loss: 87.84657287597656 = 1.092791199684143 + 10.0 * 8.675378799438477
Epoch 940, val loss: 1.091017484664917
Epoch 950, training loss: 87.87565612792969 = 1.0927873849868774 + 10.0 * 8.6782865524292
Epoch 950, val loss: 1.0910078287124634
Epoch 960, training loss: 87.88618469238281 = 1.092782735824585 + 10.0 * 8.679340362548828
Epoch 960, val loss: 1.090998649597168
Epoch 970, training loss: 87.90289306640625 = 1.0927789211273193 + 10.0 * 8.681011199951172
Epoch 970, val loss: 1.0909888744354248
Epoch 980, training loss: 87.92627716064453 = 1.092777132987976 + 10.0 * 8.683350563049316
Epoch 980, val loss: 1.090982437133789
Epoch 990, training loss: 87.87813568115234 = 1.0927681922912598 + 10.0 * 8.678537368774414
Epoch 990, val loss: 1.0909711122512817
Epoch 1000, training loss: 87.94075012207031 = 1.0927687883377075 + 10.0 * 8.684798240661621
Epoch 1000, val loss: 1.0909675359725952
Epoch 1010, training loss: 88.00244140625 = 1.0927681922912598 + 10.0 * 8.690967559814453
Epoch 1010, val loss: 1.0909603834152222
Epoch 1020, training loss: 88.01244354248047 = 1.092765212059021 + 10.0 * 8.691967964172363
Epoch 1020, val loss: 1.0909528732299805
Epoch 1030, training loss: 87.8975830078125 = 1.0927438735961914 + 10.0 * 8.6804838180542
Epoch 1030, val loss: 1.0909258127212524
Epoch 1040, training loss: 87.88968658447266 = 1.0927412509918213 + 10.0 * 8.679694175720215
Epoch 1040, val loss: 1.0909178256988525
Epoch 1050, training loss: 87.94652557373047 = 1.0927457809448242 + 10.0 * 8.685378074645996
Epoch 1050, val loss: 1.0909186601638794
Epoch 1060, training loss: 87.98625183105469 = 1.0927469730377197 + 10.0 * 8.689350128173828
Epoch 1060, val loss: 1.0909156799316406
Epoch 1070, training loss: 88.06257629394531 = 1.0927493572235107 + 10.0 * 8.696982383728027
Epoch 1070, val loss: 1.090914011001587
Epoch 1080, training loss: 88.07337951660156 = 1.0927464962005615 + 10.0 * 8.698063850402832
Epoch 1080, val loss: 1.0909068584442139
Epoch 1090, training loss: 88.11257934570312 = 1.0927454233169556 + 10.0 * 8.701983451843262
Epoch 1090, val loss: 1.0909026861190796
Epoch 1100, training loss: 88.12419128417969 = 1.0927425622940063 + 10.0 * 8.703145027160645
Epoch 1100, val loss: 1.0908955335617065
Epoch 1110, training loss: 88.13544464111328 = 1.0927393436431885 + 10.0 * 8.704270362854004
Epoch 1110, val loss: 1.0908880233764648
Epoch 1120, training loss: 88.17767333984375 = 1.0927401781082153 + 10.0 * 8.70849323272705
Epoch 1120, val loss: 1.0908843278884888
Epoch 1130, training loss: 88.18846893310547 = 1.092737078666687 + 10.0 * 8.709573745727539
Epoch 1130, val loss: 1.0908777713775635
Epoch 1140, training loss: 88.22110748291016 = 1.0927364826202393 + 10.0 * 8.712837219238281
Epoch 1140, val loss: 1.0908747911453247
Epoch 1150, training loss: 88.23796844482422 = 1.0927321910858154 + 10.0 * 8.714523315429688
Epoch 1150, val loss: 1.0908654928207397
Epoch 1160, training loss: 88.25856018066406 = 1.09273099899292 + 10.0 * 8.716583251953125
Epoch 1160, val loss: 1.090861201286316
Epoch 1170, training loss: 88.25402069091797 = 1.0927281379699707 + 10.0 * 8.716129302978516
Epoch 1170, val loss: 1.090855360031128
Epoch 1180, training loss: 88.28527069091797 = 1.0927265882492065 + 10.0 * 8.719254493713379
Epoch 1180, val loss: 1.0908511877059937
Epoch 1190, training loss: 88.3011474609375 = 1.0927268266677856 + 10.0 * 8.720842361450195
Epoch 1190, val loss: 1.0908466577529907
Epoch 1200, training loss: 88.314453125 = 1.0927244424819946 + 10.0 * 8.722172737121582
Epoch 1200, val loss: 1.090842366218567
Epoch 1210, training loss: 88.32003784179688 = 1.0927220582962036 + 10.0 * 8.722731590270996
Epoch 1210, val loss: 1.0908371210098267
Epoch 1220, training loss: 88.3367691040039 = 1.0927202701568604 + 10.0 * 8.724405288696289
Epoch 1220, val loss: 1.0908329486846924
Epoch 1230, training loss: 88.38309478759766 = 1.0927225351333618 + 10.0 * 8.729037284851074
Epoch 1230, val loss: 1.0908316373825073
Epoch 1240, training loss: 88.40950012207031 = 1.0927215814590454 + 10.0 * 8.731678009033203
Epoch 1240, val loss: 1.0908268690109253
Epoch 1250, training loss: 88.1694107055664 = 1.0926965475082397 + 10.0 * 8.707671165466309
Epoch 1250, val loss: 1.0907899141311646
Epoch 1260, training loss: 88.27528381347656 = 1.0927070379257202 + 10.0 * 8.718257904052734
Epoch 1260, val loss: 1.0908021926879883
Epoch 1270, training loss: 88.32598876953125 = 1.0927091836929321 + 10.0 * 8.72332763671875
Epoch 1270, val loss: 1.0908045768737793
Epoch 1280, training loss: 88.2811508178711 = 1.0927077531814575 + 10.0 * 8.718844413757324
Epoch 1280, val loss: 1.0908019542694092
Epoch 1290, training loss: 88.35400390625 = 1.092711091041565 + 10.0 * 8.726129531860352
Epoch 1290, val loss: 1.0908030271530151
Epoch 1300, training loss: 88.42056274414062 = 1.0927152633666992 + 10.0 * 8.73278522491455
Epoch 1300, val loss: 1.0908026695251465
Epoch 1310, training loss: 88.47248077392578 = 1.0927153825759888 + 10.0 * 8.737977027893066
Epoch 1310, val loss: 1.0908007621765137
Epoch 1320, training loss: 88.48612976074219 = 1.092713713645935 + 10.0 * 8.739341735839844
Epoch 1320, val loss: 1.0907961130142212
Epoch 1330, training loss: 88.49650573730469 = 1.0927127599716187 + 10.0 * 8.740379333496094
Epoch 1330, val loss: 1.0907931327819824
Epoch 1340, training loss: 88.53311157226562 = 1.0927129983901978 + 10.0 * 8.744039535522461
Epoch 1340, val loss: 1.0907918214797974
Epoch 1350, training loss: 88.58163452148438 = 1.0927119255065918 + 10.0 * 8.748891830444336
Epoch 1350, val loss: 1.0907871723175049
Epoch 1360, training loss: 88.59687805175781 = 1.092712163925171 + 10.0 * 8.75041675567627
Epoch 1360, val loss: 1.0907856225967407
Epoch 1370, training loss: 88.60672760009766 = 1.092120885848999 + 10.0 * 8.751461029052734
Epoch 1370, val loss: 1.0901435613632202
Epoch 1380, training loss: 88.54340362548828 = 1.0909650325775146 + 10.0 * 8.745244026184082
Epoch 1380, val loss: 1.0890880823135376
Epoch 1390, training loss: 88.49319458007812 = 1.0898946523666382 + 10.0 * 8.74032974243164
Epoch 1390, val loss: 1.0881296396255493
Epoch 1400, training loss: 88.55046844482422 = 1.0889779329299927 + 10.0 * 8.746149063110352
Epoch 1400, val loss: 1.087307333946228
Epoch 1410, training loss: 88.62641143798828 = 1.0881763696670532 + 10.0 * 8.753824234008789
Epoch 1410, val loss: 1.0865843296051025
Epoch 1420, training loss: 88.6019058227539 = 1.0874552726745605 + 10.0 * 8.751444816589355
Epoch 1420, val loss: 1.0859334468841553
Epoch 1430, training loss: 88.65192413330078 = 1.0868122577667236 + 10.0 * 8.756510734558105
Epoch 1430, val loss: 1.0853495597839355
Epoch 1440, training loss: 88.66646575927734 = 1.0862184762954712 + 10.0 * 8.758024215698242
Epoch 1440, val loss: 1.0848113298416138
Epoch 1450, training loss: 88.56282806396484 = 1.0856544971466064 + 10.0 * 8.747716903686523
Epoch 1450, val loss: 1.0842972993850708
Epoch 1460, training loss: 88.6009750366211 = 1.085143804550171 + 10.0 * 8.751583099365234
Epoch 1460, val loss: 1.0838309526443481
Epoch 1470, training loss: 88.60111236572266 = 1.0846604108810425 + 10.0 * 8.7516450881958
Epoch 1470, val loss: 1.083385944366455
Epoch 1480, training loss: 88.65843963623047 = 1.08420729637146 + 10.0 * 8.757423400878906
Epoch 1480, val loss: 1.082970380783081
Epoch 1490, training loss: 88.7174301147461 = 1.083778977394104 + 10.0 * 8.763364791870117
Epoch 1490, val loss: 1.0825752019882202
Epoch 1500, training loss: 88.73809814453125 = 1.0833662748336792 + 10.0 * 8.765473365783691
Epoch 1500, val loss: 1.0821949243545532
Epoch 1510, training loss: 88.77570343017578 = 1.0829704999923706 + 10.0 * 8.76927375793457
Epoch 1510, val loss: 1.0818291902542114
Epoch 1520, training loss: 88.7834243774414 = 1.0825891494750977 + 10.0 * 8.7700834274292
Epoch 1520, val loss: 1.0814745426177979
Epoch 1530, training loss: 88.80809020996094 = 1.0822205543518066 + 10.0 * 8.772586822509766
Epoch 1530, val loss: 1.0811340808868408
Epoch 1540, training loss: 88.8324203491211 = 1.0818673372268677 + 10.0 * 8.775054931640625
Epoch 1540, val loss: 1.0808053016662598
Epoch 1550, training loss: 88.74009704589844 = 1.081512451171875 + 10.0 * 8.76585865020752
Epoch 1550, val loss: 1.08047354221344
Epoch 1560, training loss: 88.76923370361328 = 1.0811783075332642 + 10.0 * 8.768805503845215
Epoch 1560, val loss: 1.080164909362793
Epoch 1570, training loss: 88.82276916503906 = 1.080859899520874 + 10.0 * 8.774190902709961
Epoch 1570, val loss: 1.0798678398132324
Epoch 1580, training loss: 88.85176849365234 = 1.0805480480194092 + 10.0 * 8.777121543884277
Epoch 1580, val loss: 1.0795741081237793
Epoch 1590, training loss: 88.90892791748047 = 1.0802444219589233 + 10.0 * 8.782868385314941
Epoch 1590, val loss: 1.079288125038147
Epoch 1600, training loss: 88.94264221191406 = 1.079941749572754 + 10.0 * 8.786270141601562
Epoch 1600, val loss: 1.0790032148361206
Epoch 1610, training loss: 88.85189819335938 = 1.079638957977295 + 10.0 * 8.777226448059082
Epoch 1610, val loss: 1.078721046447754
Epoch 1620, training loss: 88.89527893066406 = 1.0793635845184326 + 10.0 * 8.781591415405273
Epoch 1620, val loss: 1.07846200466156
Epoch 1630, training loss: 88.96243286132812 = 1.0790899991989136 + 10.0 * 8.788334846496582
Epoch 1630, val loss: 1.0782039165496826
Epoch 1640, training loss: 89.01618957519531 = 1.0788240432739258 + 10.0 * 8.793736457824707
Epoch 1640, val loss: 1.0779533386230469
Epoch 1650, training loss: 89.0635986328125 = 1.0785627365112305 + 10.0 * 8.798503875732422
Epoch 1650, val loss: 1.0777065753936768
Epoch 1660, training loss: 89.05906677246094 = 1.078302025794983 + 10.0 * 8.798076629638672
Epoch 1660, val loss: 1.0774595737457275
Epoch 1670, training loss: 89.08991241455078 = 1.078047752380371 + 10.0 * 8.801186561584473
Epoch 1670, val loss: 1.0772193670272827
Epoch 1680, training loss: 89.10095977783203 = 1.0777976512908936 + 10.0 * 8.802316665649414
Epoch 1680, val loss: 1.076981544494629
Epoch 1690, training loss: 89.10218048095703 = 1.0775542259216309 + 10.0 * 8.802462577819824
Epoch 1690, val loss: 1.0767518281936646
Epoch 1700, training loss: 89.12158966064453 = 1.077310562133789 + 10.0 * 8.804428100585938
Epoch 1700, val loss: 1.0765221118927002
Epoch 1710, training loss: 89.14332580566406 = 1.0770772695541382 + 10.0 * 8.806624412536621
Epoch 1710, val loss: 1.076300024986267
Epoch 1720, training loss: 89.17887115478516 = 1.0768475532531738 + 10.0 * 8.810201644897461
Epoch 1720, val loss: 1.0760796070098877
Epoch 1730, training loss: 89.18482971191406 = 1.0766165256500244 + 10.0 * 8.810821533203125
Epoch 1730, val loss: 1.0758615732192993
Epoch 1740, training loss: 89.2076644897461 = 1.076393961906433 + 10.0 * 8.813127517700195
Epoch 1740, val loss: 1.0756491422653198
Epoch 1750, training loss: 89.19427490234375 = 1.076169729232788 + 10.0 * 8.811810493469238
Epoch 1750, val loss: 1.0754361152648926
Epoch 1760, training loss: 89.23213195800781 = 1.0759549140930176 + 10.0 * 8.815617561340332
Epoch 1760, val loss: 1.0752328634262085
Epoch 1770, training loss: 89.2510986328125 = 1.0757417678833008 + 10.0 * 8.817535400390625
Epoch 1770, val loss: 1.0750296115875244
Epoch 1780, training loss: 89.30313873291016 = 1.075534701347351 + 10.0 * 8.822760581970215
Epoch 1780, val loss: 1.0748310089111328
Epoch 1790, training loss: 89.28136444091797 = 1.0753247737884521 + 10.0 * 8.82060432434082
Epoch 1790, val loss: 1.0746320486068726
Epoch 1800, training loss: 89.35662078857422 = 1.0751113891601562 + 10.0 * 8.828150749206543
Epoch 1800, val loss: 1.074411153793335
Epoch 1810, training loss: 89.09915924072266 = 1.0748968124389648 + 10.0 * 8.8024263381958
Epoch 1810, val loss: 1.0742090940475464
Epoch 1820, training loss: 89.00060272216797 = 1.0746883153915405 + 10.0 * 8.792591094970703
Epoch 1820, val loss: 1.074026107788086
Epoch 1830, training loss: 88.97103118896484 = 1.0744823217391968 + 10.0 * 8.789654731750488
Epoch 1830, val loss: 1.0738333463668823
Epoch 1840, training loss: 89.01620483398438 = 1.0742985010147095 + 10.0 * 8.794191360473633
Epoch 1840, val loss: 1.0736539363861084
Epoch 1850, training loss: 89.09845733642578 = 1.0741204023361206 + 10.0 * 8.802433967590332
Epoch 1850, val loss: 1.0734800100326538
Epoch 1860, training loss: 89.20646667480469 = 1.0739424228668213 + 10.0 * 8.813252449035645
Epoch 1860, val loss: 1.0733085870742798
Epoch 1870, training loss: 89.26956176757812 = 1.0737643241882324 + 10.0 * 8.819580078125
Epoch 1870, val loss: 1.0731385946273804
Epoch 1880, training loss: 89.31443786621094 = 1.073586106300354 + 10.0 * 8.824085235595703
Epoch 1880, val loss: 1.0729676485061646
Epoch 1890, training loss: 89.28671264648438 = 1.0734014511108398 + 10.0 * 8.821331024169922
Epoch 1890, val loss: 1.072786569595337
Epoch 1900, training loss: 89.31244659423828 = 1.0732232332229614 + 10.0 * 8.823922157287598
Epoch 1900, val loss: 1.0726200342178345
Epoch 1910, training loss: 89.34937286376953 = 1.0730512142181396 + 10.0 * 8.827631950378418
Epoch 1910, val loss: 1.072456955909729
Epoch 1920, training loss: 89.38677978515625 = 1.0728824138641357 + 10.0 * 8.831389427185059
Epoch 1920, val loss: 1.0722944736480713
Epoch 1930, training loss: 89.38059997558594 = 1.0727099180221558 + 10.0 * 8.830789566040039
Epoch 1930, val loss: 1.0721291303634644
Epoch 1940, training loss: 89.40928649902344 = 1.0725423097610474 + 10.0 * 8.833674430847168
Epoch 1940, val loss: 1.071969747543335
Epoch 1950, training loss: 89.41482543945312 = 1.072373867034912 + 10.0 * 8.834245681762695
Epoch 1950, val loss: 1.0718085765838623
Epoch 1960, training loss: 89.44225311279297 = 1.0722110271453857 + 10.0 * 8.837003707885742
Epoch 1960, val loss: 1.0716519355773926
Epoch 1970, training loss: 89.44202423095703 = 1.0720438957214355 + 10.0 * 8.836997985839844
Epoch 1970, val loss: 1.0714930295944214
Epoch 1980, training loss: 89.46562957763672 = 1.0718841552734375 + 10.0 * 8.839374542236328
Epoch 1980, val loss: 1.0713393688201904
Epoch 1990, training loss: 89.3827896118164 = 1.0717133283615112 + 10.0 * 8.831107139587402
Epoch 1990, val loss: 1.0711803436279297
Epoch 2000, training loss: 89.47309112548828 = 1.0715653896331787 + 10.0 * 8.840152740478516
Epoch 2000, val loss: 1.0710352659225464
Epoch 2010, training loss: 89.52893829345703 = 1.0714144706726074 + 10.0 * 8.845752716064453
Epoch 2010, val loss: 1.0708892345428467
Epoch 2020, training loss: 89.55503845214844 = 1.0712640285491943 + 10.0 * 8.848377227783203
Epoch 2020, val loss: 1.0707454681396484
Epoch 2030, training loss: 89.50287628173828 = 1.0711050033569336 + 10.0 * 8.84317684173584
Epoch 2030, val loss: 1.0705944299697876
Epoch 2040, training loss: 89.52974700927734 = 1.0709565877914429 + 10.0 * 8.845879554748535
Epoch 2040, val loss: 1.0704506635665894
Epoch 2050, training loss: 89.58683013916016 = 1.0708093643188477 + 10.0 * 8.851602554321289
Epoch 2050, val loss: 1.0703099966049194
Epoch 2060, training loss: 89.58409118652344 = 1.070663571357727 + 10.0 * 8.851343154907227
Epoch 2060, val loss: 1.0701700448989868
Epoch 2070, training loss: 89.52422332763672 = 1.070508599281311 + 10.0 * 8.84537124633789
Epoch 2070, val loss: 1.0700242519378662
Epoch 2080, training loss: 89.46260070800781 = 1.0703487396240234 + 10.0 * 8.839224815368652
Epoch 2080, val loss: 1.0698678493499756
Epoch 2090, training loss: 89.34904479980469 = 1.0701953172683716 + 10.0 * 8.827884674072266
Epoch 2090, val loss: 1.069724678993225
Epoch 2100, training loss: 89.41142272949219 = 1.0700571537017822 + 10.0 * 8.834136009216309
Epoch 2100, val loss: 1.0695914030075073
Epoch 2110, training loss: 89.45317077636719 = 1.0699275732040405 + 10.0 * 8.838323593139648
Epoch 2110, val loss: 1.0694677829742432
Epoch 2120, training loss: 89.52571868896484 = 1.069797396659851 + 10.0 * 8.845592498779297
Epoch 2120, val loss: 1.069342017173767
Epoch 2130, training loss: 89.59506225585938 = 1.0696666240692139 + 10.0 * 8.852540016174316
Epoch 2130, val loss: 1.0692169666290283
Epoch 2140, training loss: 89.62028503417969 = 1.0695338249206543 + 10.0 * 8.855074882507324
Epoch 2140, val loss: 1.069089412689209
Epoch 2150, training loss: 89.61558532714844 = 1.0693999528884888 + 10.0 * 8.854619026184082
Epoch 2150, val loss: 1.0689631700515747
Epoch 2160, training loss: 89.63675689697266 = 1.069266438484192 + 10.0 * 8.856748580932617
Epoch 2160, val loss: 1.0688347816467285
Epoch 2170, training loss: 89.67281341552734 = 1.069139003753662 + 10.0 * 8.860367774963379
Epoch 2170, val loss: 1.0687133073806763
Epoch 2180, training loss: 89.68576049804688 = 1.069010615348816 + 10.0 * 8.861675262451172
Epoch 2180, val loss: 1.0685889720916748
Epoch 2190, training loss: 89.67149353027344 = 1.0688787698745728 + 10.0 * 8.860261917114258
Epoch 2190, val loss: 1.068463921546936
Epoch 2200, training loss: 89.70904541015625 = 1.0687527656555176 + 10.0 * 8.864028930664062
Epoch 2200, val loss: 1.0683444738388062
Epoch 2210, training loss: 89.71543884277344 = 1.0686285495758057 + 10.0 * 8.864681243896484
Epoch 2210, val loss: 1.0682262182235718
Epoch 2220, training loss: 89.75348663330078 = 1.0685057640075684 + 10.0 * 8.868497848510742
Epoch 2220, val loss: 1.0681078433990479
Epoch 2230, training loss: 89.7392807006836 = 1.0683785676956177 + 10.0 * 8.867090225219727
Epoch 2230, val loss: 1.0679880380630493
Epoch 2240, training loss: 89.71723175048828 = 1.0682531595230103 + 10.0 * 8.864897727966309
Epoch 2240, val loss: 1.067868947982788
Epoch 2250, training loss: 89.74776458740234 = 1.0681347846984863 + 10.0 * 8.867962837219238
Epoch 2250, val loss: 1.0677562952041626
Epoch 2260, training loss: 89.74420166015625 = 1.0680111646652222 + 10.0 * 8.867619514465332
Epoch 2260, val loss: 1.0676385164260864
Epoch 2270, training loss: 89.5424575805664 = 1.0678496360778809 + 10.0 * 8.847460746765137
Epoch 2270, val loss: 1.0674896240234375
Epoch 2280, training loss: 89.57643127441406 = 1.067745566368103 + 10.0 * 8.850868225097656
Epoch 2280, val loss: 1.0673835277557373
Epoch 2290, training loss: 89.60720825195312 = 1.0676337480545044 + 10.0 * 8.853957176208496
Epoch 2290, val loss: 1.0672779083251953
Epoch 2300, training loss: 89.665771484375 = 1.067527174949646 + 10.0 * 8.859824180603027
Epoch 2300, val loss: 1.067175030708313
Epoch 2310, training loss: 89.72594451904297 = 1.0674221515655518 + 10.0 * 8.865852355957031
Epoch 2310, val loss: 1.0670748949050903
Epoch 2320, training loss: 89.76940155029297 = 1.0673131942749023 + 10.0 * 8.870208740234375
Epoch 2320, val loss: 1.0669721364974976
Epoch 2330, training loss: 89.79045104980469 = 1.067203402519226 + 10.0 * 8.87232494354248
Epoch 2330, val loss: 1.0668672323226929
Epoch 2340, training loss: 89.82589721679688 = 1.0670932531356812 + 10.0 * 8.875880241394043
Epoch 2340, val loss: 1.0667613744735718
Epoch 2350, training loss: 89.80812072753906 = 1.0669820308685303 + 10.0 * 8.874114036560059
Epoch 2350, val loss: 1.0666556358337402
Epoch 2360, training loss: 89.82776641845703 = 1.0668716430664062 + 10.0 * 8.876089096069336
Epoch 2360, val loss: 1.0665525197982788
Epoch 2370, training loss: 89.84126281738281 = 1.0667635202407837 + 10.0 * 8.877449989318848
Epoch 2370, val loss: 1.066449522972107
Epoch 2380, training loss: 89.62214660644531 = 1.0666301250457764 + 10.0 * 8.855551719665527
Epoch 2380, val loss: 1.0663275718688965
Epoch 2390, training loss: 89.652099609375 = 1.0665251016616821 + 10.0 * 8.858556747436523
Epoch 2390, val loss: 1.0662283897399902
Epoch 2400, training loss: 89.75084686279297 = 1.0664305686950684 + 10.0 * 8.868441581726074
Epoch 2400, val loss: 1.0661348104476929
Epoch 2410, training loss: 89.83104705810547 = 1.066335678100586 + 10.0 * 8.876470565795898
Epoch 2410, val loss: 1.0660429000854492
Epoch 2420, training loss: 89.88138580322266 = 1.0662363767623901 + 10.0 * 8.881514549255371
Epoch 2420, val loss: 1.06594717502594
Epoch 2430, training loss: 89.91127014160156 = 1.066136121749878 + 10.0 * 8.884512901306152
Epoch 2430, val loss: 1.0658516883850098
Epoch 2440, training loss: 89.89251708984375 = 1.0660322904586792 + 10.0 * 8.882648468017578
Epoch 2440, val loss: 1.0657531023025513
Epoch 2450, training loss: 89.90362548828125 = 1.0659313201904297 + 10.0 * 8.883769035339355
Epoch 2450, val loss: 1.0656589269638062
Epoch 2460, training loss: 89.93217468261719 = 1.0658339262008667 + 10.0 * 8.88663387298584
Epoch 2460, val loss: 1.0655652284622192
Epoch 2470, training loss: 89.95542907714844 = 1.0657355785369873 + 10.0 * 8.888969421386719
Epoch 2470, val loss: 1.0654728412628174
Epoch 2480, training loss: 89.9181137084961 = 1.0656336545944214 + 10.0 * 8.885248184204102
Epoch 2480, val loss: 1.065372347831726
Epoch 2490, training loss: 89.94296264648438 = 1.0655385255813599 + 10.0 * 8.88774299621582
Epoch 2490, val loss: 1.0652847290039062
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.3973913043478261
0.8645222053176846
=== training gcn model ===
Epoch 0, training loss: 102.44914245605469 = 1.0930914878845215 + 10.0 * 10.135604858398438
Epoch 0, val loss: 1.092380404472351
Epoch 10, training loss: 97.47095489501953 = 1.092888593673706 + 10.0 * 9.63780689239502
Epoch 10, val loss: 1.0921502113342285
Epoch 20, training loss: 95.34125518798828 = 1.0926105976104736 + 10.0 * 9.424863815307617
Epoch 20, val loss: 1.091869831085205
Epoch 30, training loss: 94.05620574951172 = 1.092329502105713 + 10.0 * 9.296387672424316
Epoch 30, val loss: 1.0915967226028442
Epoch 40, training loss: 93.04390716552734 = 1.0920716524124146 + 10.0 * 9.195183753967285
Epoch 40, val loss: 1.0913372039794922
Epoch 50, training loss: 92.23107147216797 = 1.091819405555725 + 10.0 * 9.113924980163574
Epoch 50, val loss: 1.0910871028900146
Epoch 60, training loss: 91.55960083007812 = 1.0915517807006836 + 10.0 * 9.046804428100586
Epoch 60, val loss: 1.0908201932907104
Epoch 70, training loss: 91.0014419555664 = 1.0912854671478271 + 10.0 * 8.991015434265137
Epoch 70, val loss: 1.0905576944351196
Epoch 80, training loss: 90.51836395263672 = 1.0910283327102661 + 10.0 * 8.942733764648438
Epoch 80, val loss: 1.090303659439087
Epoch 90, training loss: 90.09356689453125 = 1.0907737016677856 + 10.0 * 8.90027904510498
Epoch 90, val loss: 1.0900522470474243
Epoch 100, training loss: 89.72875213623047 = 1.0905206203460693 + 10.0 * 8.863822937011719
Epoch 100, val loss: 1.0898010730743408
Epoch 110, training loss: 89.4123306274414 = 1.0902767181396484 + 10.0 * 8.832204818725586
Epoch 110, val loss: 1.0895613431930542
Epoch 120, training loss: 89.13394165039062 = 1.09003746509552 + 10.0 * 8.804390907287598
Epoch 120, val loss: 1.0893242359161377
Epoch 130, training loss: 88.89261627197266 = 1.0897938013076782 + 10.0 * 8.780282974243164
Epoch 130, val loss: 1.0890859365463257
Epoch 140, training loss: 88.6752700805664 = 1.0895566940307617 + 10.0 * 8.75857162475586
Epoch 140, val loss: 1.0888499021530151
Epoch 150, training loss: 88.49906921386719 = 1.089324951171875 + 10.0 * 8.740974426269531
Epoch 150, val loss: 1.0886229276657104
Epoch 160, training loss: 88.3393783569336 = 1.0890977382659912 + 10.0 * 8.725028038024902
Epoch 160, val loss: 1.0883944034576416
Epoch 170, training loss: 88.21198272705078 = 1.088869333267212 + 10.0 * 8.712311744689941
Epoch 170, val loss: 1.0881701707839966
Epoch 180, training loss: 88.06571960449219 = 1.0886428356170654 + 10.0 * 8.697707176208496
Epoch 180, val loss: 1.0879433155059814
Epoch 190, training loss: 87.96138763427734 = 1.088417887687683 + 10.0 * 8.687296867370605
Epoch 190, val loss: 1.0877221822738647
Epoch 200, training loss: 87.83943176269531 = 1.0881927013397217 + 10.0 * 8.67512321472168
Epoch 200, val loss: 1.0875014066696167
Epoch 210, training loss: 87.76624298095703 = 1.0879744291305542 + 10.0 * 8.667826652526855
Epoch 210, val loss: 1.0872832536697388
Epoch 220, training loss: 87.69755554199219 = 1.0877617597579956 + 10.0 * 8.660979270935059
Epoch 220, val loss: 1.087073564529419
Epoch 230, training loss: 87.6298828125 = 1.087540626525879 + 10.0 * 8.654233932495117
Epoch 230, val loss: 1.0868537425994873
Epoch 240, training loss: 87.57511138916016 = 1.0873303413391113 + 10.0 * 8.648777961730957
Epoch 240, val loss: 1.0866433382034302
Epoch 250, training loss: 87.5044937133789 = 1.087118148803711 + 10.0 * 8.64173698425293
Epoch 250, val loss: 1.0864332914352417
Epoch 260, training loss: 87.46351623535156 = 1.0869075059890747 + 10.0 * 8.63766098022461
Epoch 260, val loss: 1.0862241983413696
Epoch 270, training loss: 87.41249084472656 = 1.0866976976394653 + 10.0 * 8.63257884979248
Epoch 270, val loss: 1.0860167741775513
Epoch 280, training loss: 87.37350463867188 = 1.086491346359253 + 10.0 * 8.628701210021973
Epoch 280, val loss: 1.0858123302459717
Epoch 290, training loss: 87.32792663574219 = 1.0862882137298584 + 10.0 * 8.624163627624512
Epoch 290, val loss: 1.0856112241744995
Epoch 300, training loss: 87.2917251586914 = 1.086084246635437 + 10.0 * 8.620564460754395
Epoch 300, val loss: 1.0854088068008423
Epoch 310, training loss: 87.27232360839844 = 1.0858782529830933 + 10.0 * 8.618644714355469
Epoch 310, val loss: 1.0852046012878418
Epoch 320, training loss: 87.25163269042969 = 1.0856728553771973 + 10.0 * 8.616596221923828
Epoch 320, val loss: 1.0850074291229248
Epoch 330, training loss: 87.24144744873047 = 1.085482120513916 + 10.0 * 8.615596771240234
Epoch 330, val loss: 1.0848139524459839
Epoch 340, training loss: 87.22396087646484 = 1.0852832794189453 + 10.0 * 8.61386775970459
Epoch 340, val loss: 1.0846189260482788
Epoch 350, training loss: 87.20441436767578 = 1.0850900411605835 + 10.0 * 8.611932754516602
Epoch 350, val loss: 1.0844265222549438
Epoch 360, training loss: 87.18013000488281 = 1.0848959684371948 + 10.0 * 8.60952377319336
Epoch 360, val loss: 1.0842342376708984
Epoch 370, training loss: 87.1915283203125 = 1.0846993923187256 + 10.0 * 8.610682487487793
Epoch 370, val loss: 1.0840407609939575
Epoch 380, training loss: 87.14851379394531 = 1.0845057964324951 + 10.0 * 8.606401443481445
Epoch 380, val loss: 1.0838490724563599
Epoch 390, training loss: 87.14949798583984 = 1.0843130350112915 + 10.0 * 8.606518745422363
Epoch 390, val loss: 1.0836598873138428
Epoch 400, training loss: 87.13203430175781 = 1.0841238498687744 + 10.0 * 8.604791641235352
Epoch 400, val loss: 1.0834733247756958
Epoch 410, training loss: 87.15216064453125 = 1.0839369297027588 + 10.0 * 8.60682201385498
Epoch 410, val loss: 1.0832881927490234
Epoch 420, training loss: 87.1321792602539 = 1.0837496519088745 + 10.0 * 8.604843139648438
Epoch 420, val loss: 1.0831019878387451
Epoch 430, training loss: 87.13020324707031 = 1.0835633277893066 + 10.0 * 8.604663848876953
Epoch 430, val loss: 1.082917332649231
Epoch 440, training loss: 87.1168212890625 = 1.0833765268325806 + 10.0 * 8.603344917297363
Epoch 440, val loss: 1.0827332735061646
Epoch 450, training loss: 87.11160278320312 = 1.0831903219223022 + 10.0 * 8.6028413772583
Epoch 450, val loss: 1.0825495719909668
Epoch 460, training loss: 87.10332489013672 = 1.0830105543136597 + 10.0 * 8.602031707763672
Epoch 460, val loss: 1.0823718309402466
Epoch 470, training loss: 87.12730407714844 = 1.0828304290771484 + 10.0 * 8.604447364807129
Epoch 470, val loss: 1.0821926593780518
Epoch 480, training loss: 87.11907196044922 = 1.0826481580734253 + 10.0 * 8.603642463684082
Epoch 480, val loss: 1.0820122957229614
Epoch 490, training loss: 87.11936950683594 = 1.0824681520462036 + 10.0 * 8.603690147399902
Epoch 490, val loss: 1.0818361043930054
Epoch 500, training loss: 87.129150390625 = 1.0822862386703491 + 10.0 * 8.604686737060547
Epoch 500, val loss: 1.0816556215286255
Epoch 510, training loss: 87.10128784179688 = 1.0821037292480469 + 10.0 * 8.60191822052002
Epoch 510, val loss: 1.081472635269165
Epoch 520, training loss: 87.07813262939453 = 1.0819246768951416 + 10.0 * 8.599620819091797
Epoch 520, val loss: 1.0812983512878418
Epoch 530, training loss: 87.11325073242188 = 1.0817514657974243 + 10.0 * 8.603150367736816
Epoch 530, val loss: 1.0811283588409424
Epoch 540, training loss: 87.13267517089844 = 1.0815790891647339 + 10.0 * 8.605109214782715
Epoch 540, val loss: 1.0809587240219116
Epoch 550, training loss: 87.14527893066406 = 1.081407070159912 + 10.0 * 8.6063871383667
Epoch 550, val loss: 1.0807881355285645
Epoch 560, training loss: 87.1445541381836 = 1.0812265872955322 + 10.0 * 8.606332778930664
Epoch 560, val loss: 1.0806118249893188
Epoch 570, training loss: 87.1463851928711 = 1.0810537338256836 + 10.0 * 8.60653305053711
Epoch 570, val loss: 1.0804436206817627
Epoch 580, training loss: 87.14727783203125 = 1.0808809995651245 + 10.0 * 8.606639862060547
Epoch 580, val loss: 1.0802725553512573
Epoch 590, training loss: 87.18114471435547 = 1.080713152885437 + 10.0 * 8.6100435256958
Epoch 590, val loss: 1.0801067352294922
Epoch 600, training loss: 87.21670532226562 = 1.0805474519729614 + 10.0 * 8.613615036010742
Epoch 600, val loss: 1.0799424648284912
Epoch 610, training loss: 87.21878814697266 = 1.0803762674331665 + 10.0 * 8.61384105682373
Epoch 610, val loss: 1.0797719955444336
Epoch 620, training loss: 87.2302474975586 = 1.0802104473114014 + 10.0 * 8.61500358581543
Epoch 620, val loss: 1.0796098709106445
Epoch 630, training loss: 87.25519561767578 = 1.0800445079803467 + 10.0 * 8.617514610290527
Epoch 630, val loss: 1.079446792602539
Epoch 640, training loss: 87.26349639892578 = 1.079875111579895 + 10.0 * 8.618362426757812
Epoch 640, val loss: 1.0792791843414307
Epoch 650, training loss: 87.22232055664062 = 1.0797003507614136 + 10.0 * 8.614262580871582
Epoch 650, val loss: 1.0791116952896118
Epoch 660, training loss: 87.2634048461914 = 1.0795440673828125 + 10.0 * 8.618386268615723
Epoch 660, val loss: 1.0789568424224854
Epoch 670, training loss: 87.33114624023438 = 1.0793836116790771 + 10.0 * 8.625176429748535
Epoch 670, val loss: 1.078797698020935
Epoch 680, training loss: 87.307861328125 = 1.0792229175567627 + 10.0 * 8.62286376953125
Epoch 680, val loss: 1.0786387920379639
Epoch 690, training loss: 87.37096405029297 = 1.0790655612945557 + 10.0 * 8.629190444946289
Epoch 690, val loss: 1.0784822702407837
Epoch 700, training loss: 87.37277221679688 = 1.0789040327072144 + 10.0 * 8.629386901855469
Epoch 700, val loss: 1.0783249139785767
Epoch 710, training loss: 87.3877944946289 = 1.0787397623062134 + 10.0 * 8.630905151367188
Epoch 710, val loss: 1.0781607627868652
Epoch 720, training loss: 87.39924621582031 = 1.0785837173461914 + 10.0 * 8.63206672668457
Epoch 720, val loss: 1.0780103206634521
Epoch 730, training loss: 87.3514633178711 = 1.0784167051315308 + 10.0 * 8.627305030822754
Epoch 730, val loss: 1.0778486728668213
Epoch 740, training loss: 87.37234497070312 = 1.0782654285430908 + 10.0 * 8.62940788269043
Epoch 740, val loss: 1.077699899673462
Epoch 750, training loss: 87.37786102294922 = 1.0781102180480957 + 10.0 * 8.629975318908691
Epoch 750, val loss: 1.0775483846664429
Epoch 760, training loss: 87.408203125 = 1.077960729598999 + 10.0 * 8.633024215698242
Epoch 760, val loss: 1.0773996114730835
Epoch 770, training loss: 87.45807647705078 = 1.0778110027313232 + 10.0 * 8.638026237487793
Epoch 770, val loss: 1.0772522687911987
Epoch 780, training loss: 87.48478698730469 = 1.0776584148406982 + 10.0 * 8.64071273803711
Epoch 780, val loss: 1.077101707458496
Epoch 790, training loss: 87.50688934326172 = 1.0775046348571777 + 10.0 * 8.642938613891602
Epoch 790, val loss: 1.0769495964050293
Epoch 800, training loss: 87.5208511352539 = 1.0773545503616333 + 10.0 * 8.644350051879883
Epoch 800, val loss: 1.0768022537231445
Epoch 810, training loss: 87.52801513671875 = 1.0772016048431396 + 10.0 * 8.645081520080566
Epoch 810, val loss: 1.0766521692276
Epoch 820, training loss: 87.58241271972656 = 1.0770546197891235 + 10.0 * 8.650535583496094
Epoch 820, val loss: 1.076509952545166
Epoch 830, training loss: 87.59915161132812 = 1.076904058456421 + 10.0 * 8.65222454071045
Epoch 830, val loss: 1.076360821723938
Epoch 840, training loss: 87.59207153320312 = 1.0767335891723633 + 10.0 * 8.651533126831055
Epoch 840, val loss: 1.0761981010437012
Epoch 850, training loss: 87.55340576171875 = 1.0765964984893799 + 10.0 * 8.647680282592773
Epoch 850, val loss: 1.076056718826294
Epoch 860, training loss: 87.55184173583984 = 1.0764511823654175 + 10.0 * 8.647539138793945
Epoch 860, val loss: 1.0759168863296509
Epoch 870, training loss: 87.58106994628906 = 1.0763083696365356 + 10.0 * 8.650476455688477
Epoch 870, val loss: 1.075778841972351
Epoch 880, training loss: 87.64308166503906 = 1.0761692523956299 + 10.0 * 8.65669059753418
Epoch 880, val loss: 1.0756429433822632
Epoch 890, training loss: 87.68170166015625 = 1.0760263204574585 + 10.0 * 8.660567283630371
Epoch 890, val loss: 1.0755031108856201
Epoch 900, training loss: 87.6493911743164 = 1.0758740901947021 + 10.0 * 8.65735149383545
Epoch 900, val loss: 1.0753493309020996
Epoch 910, training loss: 87.73066711425781 = 1.0757360458374023 + 10.0 * 8.66549301147461
Epoch 910, val loss: 1.0752193927764893
Epoch 920, training loss: 87.72032165527344 = 1.0755940675735474 + 10.0 * 8.664472579956055
Epoch 920, val loss: 1.0750807523727417
Epoch 930, training loss: 87.7556381225586 = 1.0754549503326416 + 10.0 * 8.668018341064453
Epoch 930, val loss: 1.0749452114105225
Epoch 940, training loss: 87.77561950683594 = 1.0753180980682373 + 10.0 * 8.67003059387207
Epoch 940, val loss: 1.0748103857040405
Epoch 950, training loss: 87.81139373779297 = 1.0751805305480957 + 10.0 * 8.67362117767334
Epoch 950, val loss: 1.074675440788269
Epoch 960, training loss: 87.82794189453125 = 1.075042486190796 + 10.0 * 8.67529010772705
Epoch 960, val loss: 1.0745402574539185
Epoch 970, training loss: 87.84247589111328 = 1.074904441833496 + 10.0 * 8.676756858825684
Epoch 970, val loss: 1.074406385421753
Epoch 980, training loss: 87.86445617675781 = 1.0747663974761963 + 10.0 * 8.67896842956543
Epoch 980, val loss: 1.0742712020874023
Epoch 990, training loss: 87.84505462646484 = 1.0746235847473145 + 10.0 * 8.677042961120605
Epoch 990, val loss: 1.0741322040557861
Epoch 1000, training loss: 87.83291625976562 = 1.0744807720184326 + 10.0 * 8.675844192504883
Epoch 1000, val loss: 1.0739946365356445
Epoch 1010, training loss: 87.86711883544922 = 1.0743519067764282 + 10.0 * 8.679277420043945
Epoch 1010, val loss: 1.0738681554794312
Epoch 1020, training loss: 87.89302062988281 = 1.0742220878601074 + 10.0 * 8.681879997253418
Epoch 1020, val loss: 1.0737414360046387
Epoch 1030, training loss: 87.93408966064453 = 1.0740915536880493 + 10.0 * 8.685999870300293
Epoch 1030, val loss: 1.0736128091812134
Epoch 1040, training loss: 87.96133422851562 = 1.0739595890045166 + 10.0 * 8.688737869262695
Epoch 1040, val loss: 1.0734838247299194
Epoch 1050, training loss: 87.98653411865234 = 1.0738285779953003 + 10.0 * 8.69127082824707
Epoch 1050, val loss: 1.073357343673706
Epoch 1060, training loss: 88.00395202636719 = 1.0736984014511108 + 10.0 * 8.693025588989258
Epoch 1060, val loss: 1.0732301473617554
Epoch 1070, training loss: 88.01643371582031 = 1.073569893836975 + 10.0 * 8.694286346435547
Epoch 1070, val loss: 1.0731054544448853
Epoch 1080, training loss: 88.05313110351562 = 1.073440670967102 + 10.0 * 8.697969436645508
Epoch 1080, val loss: 1.0729767084121704
Epoch 1090, training loss: 88.0250473022461 = 1.073305606842041 + 10.0 * 8.695174217224121
Epoch 1090, val loss: 1.0728472471237183
Epoch 1100, training loss: 87.98330688476562 = 1.0731686353683472 + 10.0 * 8.691014289855957
Epoch 1100, val loss: 1.0727158784866333
Epoch 1110, training loss: 87.96644592285156 = 1.073043704032898 + 10.0 * 8.689340591430664
Epoch 1110, val loss: 1.0725913047790527
Epoch 1120, training loss: 88.01219177246094 = 1.072918176651001 + 10.0 * 8.693926811218262
Epoch 1120, val loss: 1.0724684000015259
Epoch 1130, training loss: 88.08340454101562 = 1.0727986097335815 + 10.0 * 8.70106029510498
Epoch 1130, val loss: 1.072352647781372
Epoch 1140, training loss: 88.12388610839844 = 1.0726749897003174 + 10.0 * 8.705121040344238
Epoch 1140, val loss: 1.0722318887710571
Epoch 1150, training loss: 88.1461181640625 = 1.0725518465042114 + 10.0 * 8.707356452941895
Epoch 1150, val loss: 1.072112798690796
Epoch 1160, training loss: 88.13825225830078 = 1.0724278688430786 + 10.0 * 8.706583023071289
Epoch 1160, val loss: 1.0719908475875854
Epoch 1170, training loss: 88.19381713867188 = 1.072306752204895 + 10.0 * 8.712151527404785
Epoch 1170, val loss: 1.0718742609024048
Epoch 1180, training loss: 88.1867446899414 = 1.072180151939392 + 10.0 * 8.711456298828125
Epoch 1180, val loss: 1.0717508792877197
Epoch 1190, training loss: 88.20870971679688 = 1.072054147720337 + 10.0 * 8.713665962219238
Epoch 1190, val loss: 1.071630597114563
Epoch 1200, training loss: 88.22628021240234 = 1.0719366073608398 + 10.0 * 8.715434074401855
Epoch 1200, val loss: 1.0715175867080688
Epoch 1210, training loss: 88.27017211914062 = 1.0718207359313965 + 10.0 * 8.71983528137207
Epoch 1210, val loss: 1.0714035034179688
Epoch 1220, training loss: 88.27520751953125 = 1.0716983079910278 + 10.0 * 8.72035026550293
Epoch 1220, val loss: 1.0712854862213135
Epoch 1230, training loss: 88.27325439453125 = 1.0715807676315308 + 10.0 * 8.72016716003418
Epoch 1230, val loss: 1.0711710453033447
Epoch 1240, training loss: 88.3072280883789 = 1.0714620351791382 + 10.0 * 8.723576545715332
Epoch 1240, val loss: 1.0710561275482178
Epoch 1250, training loss: 88.35345458984375 = 1.071348786354065 + 10.0 * 8.72821044921875
Epoch 1250, val loss: 1.070946455001831
Epoch 1260, training loss: 88.35417175292969 = 1.0712318420410156 + 10.0 * 8.728294372558594
Epoch 1260, val loss: 1.0708321332931519
Epoch 1270, training loss: 88.36028289794922 = 1.0711153745651245 + 10.0 * 8.728917121887207
Epoch 1270, val loss: 1.0707197189331055
Epoch 1280, training loss: 88.38363647460938 = 1.071000337600708 + 10.0 * 8.731264114379883
Epoch 1280, val loss: 1.0706058740615845
Epoch 1290, training loss: 88.4062728881836 = 1.0708823204040527 + 10.0 * 8.733538627624512
Epoch 1290, val loss: 1.0704931020736694
Epoch 1300, training loss: 88.43165588378906 = 1.07076895236969 + 10.0 * 8.736088752746582
Epoch 1300, val loss: 1.0703845024108887
Epoch 1310, training loss: 88.43148803710938 = 1.0706558227539062 + 10.0 * 8.736083030700684
Epoch 1310, val loss: 1.0702741146087646
Epoch 1320, training loss: 88.45923614501953 = 1.0705454349517822 + 10.0 * 8.738868713378906
Epoch 1320, val loss: 1.0701675415039062
Epoch 1330, training loss: 88.47332763671875 = 1.0704313516616821 + 10.0 * 8.740289688110352
Epoch 1330, val loss: 1.0700584650039673
Epoch 1340, training loss: 88.51421356201172 = 1.0703226327896118 + 10.0 * 8.744389533996582
Epoch 1340, val loss: 1.0699505805969238
Epoch 1350, training loss: 88.45999908447266 = 1.0702037811279297 + 10.0 * 8.73897933959961
Epoch 1350, val loss: 1.069837212562561
Epoch 1360, training loss: 88.49593353271484 = 1.0700985193252563 + 10.0 * 8.742583274841309
Epoch 1360, val loss: 1.069736123085022
Epoch 1370, training loss: 88.54618835449219 = 1.069989800453186 + 10.0 * 8.74761962890625
Epoch 1370, val loss: 1.069632649421692
Epoch 1380, training loss: 88.55634307861328 = 1.0698819160461426 + 10.0 * 8.748645782470703
Epoch 1380, val loss: 1.0695250034332275
Epoch 1390, training loss: 88.57425689697266 = 1.0697726011276245 + 10.0 * 8.750448226928711
Epoch 1390, val loss: 1.0694204568862915
Epoch 1400, training loss: 88.57447814941406 = 1.0696645975112915 + 10.0 * 8.750481605529785
Epoch 1400, val loss: 1.0693161487579346
Epoch 1410, training loss: 88.61974334716797 = 1.0695595741271973 + 10.0 * 8.75501823425293
Epoch 1410, val loss: 1.069215178489685
Epoch 1420, training loss: 88.61754608154297 = 1.0694528818130493 + 10.0 * 8.754809379577637
Epoch 1420, val loss: 1.0691120624542236
Epoch 1430, training loss: 88.69132995605469 = 1.069337248802185 + 10.0 * 8.762199401855469
Epoch 1430, val loss: 1.0689959526062012
Epoch 1440, training loss: 88.57317352294922 = 1.0692192316055298 + 10.0 * 8.750394821166992
Epoch 1440, val loss: 1.068881869316101
Epoch 1450, training loss: 88.61740112304688 = 1.0691215991973877 + 10.0 * 8.754827499389648
Epoch 1450, val loss: 1.068790078163147
Epoch 1460, training loss: 88.62376403808594 = 1.069024682044983 + 10.0 * 8.755474090576172
Epoch 1460, val loss: 1.0687005519866943
Epoch 1470, training loss: 88.66355895996094 = 1.0689243078231812 + 10.0 * 8.7594633102417
Epoch 1470, val loss: 1.0686029195785522
Epoch 1480, training loss: 88.67930603027344 = 1.0688244104385376 + 10.0 * 8.761048316955566
Epoch 1480, val loss: 1.0685049295425415
Epoch 1490, training loss: 88.71177673339844 = 1.0687240362167358 + 10.0 * 8.764305114746094
Epoch 1490, val loss: 1.0684081315994263
Epoch 1500, training loss: 88.71611022949219 = 1.068621039390564 + 10.0 * 8.764749526977539
Epoch 1500, val loss: 1.0683090686798096
Epoch 1510, training loss: 88.75428009033203 = 1.0685210227966309 + 10.0 * 8.768575668334961
Epoch 1510, val loss: 1.068214774131775
Epoch 1520, training loss: 88.75420379638672 = 1.0684207677841187 + 10.0 * 8.76857852935791
Epoch 1520, val loss: 1.0681164264678955
Epoch 1530, training loss: 88.76325225830078 = 1.0683189630508423 + 10.0 * 8.769493103027344
Epoch 1530, val loss: 1.0680211782455444
Epoch 1540, training loss: 88.80918884277344 = 1.0682239532470703 + 10.0 * 8.774096488952637
Epoch 1540, val loss: 1.0679278373718262
Epoch 1550, training loss: 88.79501342773438 = 1.0681235790252686 + 10.0 * 8.772688865661621
Epoch 1550, val loss: 1.067830204963684
Epoch 1560, training loss: 88.75157165527344 = 1.0680177211761475 + 10.0 * 8.768355369567871
Epoch 1560, val loss: 1.0677298307418823
Epoch 1570, training loss: 88.77593994140625 = 1.0679206848144531 + 10.0 * 8.77080249786377
Epoch 1570, val loss: 1.0676378011703491
Epoch 1580, training loss: 88.82764434814453 = 1.0678285360336304 + 10.0 * 8.775981903076172
Epoch 1580, val loss: 1.0675476789474487
Epoch 1590, training loss: 88.86344909667969 = 1.0677366256713867 + 10.0 * 8.779571533203125
Epoch 1590, val loss: 1.0674594640731812
Epoch 1600, training loss: 88.84539031982422 = 1.0676356554031372 + 10.0 * 8.777775764465332
Epoch 1600, val loss: 1.067362666130066
Epoch 1610, training loss: 88.8771743774414 = 1.0675432682037354 + 10.0 * 8.780962944030762
Epoch 1610, val loss: 1.067274570465088
Epoch 1620, training loss: 88.90589141845703 = 1.067449688911438 + 10.0 * 8.783843994140625
Epoch 1620, val loss: 1.0671838521957397
Epoch 1630, training loss: 88.85549926757812 = 1.0673481225967407 + 10.0 * 8.778815269470215
Epoch 1630, val loss: 1.0670886039733887
Epoch 1640, training loss: 88.81473541259766 = 1.0672425031661987 + 10.0 * 8.774748802185059
Epoch 1640, val loss: 1.0669879913330078
Epoch 1650, training loss: 88.87393188476562 = 1.06715989112854 + 10.0 * 8.78067684173584
Epoch 1650, val loss: 1.0669066905975342
Epoch 1660, training loss: 88.90834045410156 = 1.0670695304870605 + 10.0 * 8.784127235412598
Epoch 1660, val loss: 1.0668201446533203
Epoch 1670, training loss: 88.9361343383789 = 1.0669796466827393 + 10.0 * 8.78691577911377
Epoch 1670, val loss: 1.0667351484298706
Epoch 1680, training loss: 88.9251937866211 = 1.0668885707855225 + 10.0 * 8.7858304977417
Epoch 1680, val loss: 1.0666462182998657
Epoch 1690, training loss: 88.77568817138672 = 1.066776990890503 + 10.0 * 8.770891189575195
Epoch 1690, val loss: 1.0665433406829834
Epoch 1700, training loss: 88.82533264160156 = 1.066689133644104 + 10.0 * 8.775864601135254
Epoch 1700, val loss: 1.0664557218551636
Epoch 1710, training loss: 88.84251403808594 = 1.0665996074676514 + 10.0 * 8.777591705322266
Epoch 1710, val loss: 1.066371202468872
Epoch 1720, training loss: 88.88797760009766 = 1.0665197372436523 + 10.0 * 8.782145500183105
Epoch 1720, val loss: 1.0662953853607178
Epoch 1730, training loss: 88.95954132080078 = 1.0664387941360474 + 10.0 * 8.789310455322266
Epoch 1730, val loss: 1.066217064857483
Epoch 1740, training loss: 88.98330688476562 = 1.066352128982544 + 10.0 * 8.791695594787598
Epoch 1740, val loss: 1.0661342144012451
Epoch 1750, training loss: 88.99465942382812 = 1.0662633180618286 + 10.0 * 8.792840003967285
Epoch 1750, val loss: 1.0660479068756104
Epoch 1760, training loss: 89.00098419189453 = 1.0661773681640625 + 10.0 * 8.79348087310791
Epoch 1760, val loss: 1.0659677982330322
Epoch 1770, training loss: 89.02091217041016 = 1.0660927295684814 + 10.0 * 8.79548168182373
Epoch 1770, val loss: 1.0658848285675049
Epoch 1780, training loss: 89.03768920898438 = 1.0660059452056885 + 10.0 * 8.797167778015137
Epoch 1780, val loss: 1.0658031702041626
Epoch 1790, training loss: 89.0482406616211 = 1.0659205913543701 + 10.0 * 8.798232078552246
Epoch 1790, val loss: 1.0657204389572144
Epoch 1800, training loss: 89.08930206298828 = 1.0658375024795532 + 10.0 * 8.802347183227539
Epoch 1800, val loss: 1.065641164779663
Epoch 1810, training loss: 89.10802459716797 = 1.0657544136047363 + 10.0 * 8.804226875305176
Epoch 1810, val loss: 1.065561056137085
Epoch 1820, training loss: 89.11565399169922 = 1.0656658411026 + 10.0 * 8.804998397827148
Epoch 1820, val loss: 1.0654767751693726
Epoch 1830, training loss: 89.08686828613281 = 1.0655817985534668 + 10.0 * 8.802128791809082
Epoch 1830, val loss: 1.06539785861969
Epoch 1840, training loss: 89.1377182006836 = 1.0655007362365723 + 10.0 * 8.807222366333008
Epoch 1840, val loss: 1.0653201341629028
Epoch 1850, training loss: 89.15477752685547 = 1.0654207468032837 + 10.0 * 8.808935165405273
Epoch 1850, val loss: 1.0652446746826172
Epoch 1860, training loss: 89.15912628173828 = 1.0653382539749146 + 10.0 * 8.809378623962402
Epoch 1860, val loss: 1.0651651620864868
Epoch 1870, training loss: 89.16786193847656 = 1.0652570724487305 + 10.0 * 8.810260772705078
Epoch 1870, val loss: 1.0650893449783325
Epoch 1880, training loss: 89.20341491699219 = 1.0651779174804688 + 10.0 * 8.813823699951172
Epoch 1880, val loss: 1.0650135278701782
Epoch 1890, training loss: 89.1666030883789 = 1.065092921257019 + 10.0 * 8.810151100158691
Epoch 1890, val loss: 1.0649304389953613
Epoch 1900, training loss: 89.21125030517578 = 1.0650137662887573 + 10.0 * 8.814623832702637
Epoch 1900, val loss: 1.0648577213287354
Epoch 1910, training loss: 89.25609588623047 = 1.064937949180603 + 10.0 * 8.81911563873291
Epoch 1910, val loss: 1.064784049987793
Epoch 1920, training loss: 89.24979400634766 = 1.06485915184021 + 10.0 * 8.818493843078613
Epoch 1920, val loss: 1.0647094249725342
Epoch 1930, training loss: 89.24806213378906 = 1.0647814273834229 + 10.0 * 8.818327903747559
Epoch 1930, val loss: 1.0646336078643799
Epoch 1940, training loss: 89.16714477539062 = 1.064691424369812 + 10.0 * 8.810245513916016
Epoch 1940, val loss: 1.0645493268966675
Epoch 1950, training loss: 89.05113220214844 = 1.0645979642868042 + 10.0 * 8.798653602600098
Epoch 1950, val loss: 1.0644646883010864
Epoch 1960, training loss: 89.06476593017578 = 1.0645183324813843 + 10.0 * 8.800024032592773
Epoch 1960, val loss: 1.064386010169983
Epoch 1970, training loss: 89.13385009765625 = 1.064453363418579 + 10.0 * 8.806940078735352
Epoch 1970, val loss: 1.0643243789672852
Epoch 1980, training loss: 89.20201873779297 = 1.0643861293792725 + 10.0 * 8.813763618469238
Epoch 1980, val loss: 1.0642588138580322
Epoch 1990, training loss: 89.25377655029297 = 1.0643153190612793 + 10.0 * 8.81894588470459
Epoch 1990, val loss: 1.0641920566558838
Epoch 2000, training loss: 89.29407501220703 = 1.0642443895339966 + 10.0 * 8.822982788085938
Epoch 2000, val loss: 1.0641251802444458
Epoch 2010, training loss: 89.29552459716797 = 1.0641703605651855 + 10.0 * 8.823135375976562
Epoch 2010, val loss: 1.0640525817871094
Epoch 2020, training loss: 89.30249786376953 = 1.0640945434570312 + 10.0 * 8.823840141296387
Epoch 2020, val loss: 1.0639840364456177
Epoch 2030, training loss: 89.3509521484375 = 1.0640254020690918 + 10.0 * 8.828692436218262
Epoch 2030, val loss: 1.0639166831970215
Epoch 2040, training loss: 89.37040710449219 = 1.0639532804489136 + 10.0 * 8.830645561218262
Epoch 2040, val loss: 1.0638477802276611
Epoch 2050, training loss: 89.35294342041016 = 1.0638781785964966 + 10.0 * 8.828906059265137
Epoch 2050, val loss: 1.0637778043746948
Epoch 2060, training loss: 89.37617492675781 = 1.0638071298599243 + 10.0 * 8.831236839294434
Epoch 2060, val loss: 1.0637098550796509
Epoch 2070, training loss: 89.39705657958984 = 1.063736915588379 + 10.0 * 8.833332061767578
Epoch 2070, val loss: 1.0636446475982666
Epoch 2080, training loss: 89.40973663330078 = 1.0636671781539917 + 10.0 * 8.834607124328613
Epoch 2080, val loss: 1.063578724861145
Epoch 2090, training loss: 89.41678619384766 = 1.0635960102081299 + 10.0 * 8.835318565368652
Epoch 2090, val loss: 1.0635100603103638
Epoch 2100, training loss: 89.45170593261719 = 1.0635281801223755 + 10.0 * 8.838817596435547
Epoch 2100, val loss: 1.0634450912475586
Epoch 2110, training loss: 89.46484375 = 1.0634583234786987 + 10.0 * 8.84013843536377
Epoch 2110, val loss: 1.0633797645568848
Epoch 2120, training loss: 89.44686126708984 = 1.063387393951416 + 10.0 * 8.838347434997559
Epoch 2120, val loss: 1.0633137226104736
Epoch 2130, training loss: 89.44332122802734 = 1.0633149147033691 + 10.0 * 8.838000297546387
Epoch 2130, val loss: 1.0632442235946655
Epoch 2140, training loss: 89.46965026855469 = 1.063249111175537 + 10.0 * 8.8406400680542
Epoch 2140, val loss: 1.06318199634552
Epoch 2150, training loss: 89.49348449707031 = 1.0631839036941528 + 10.0 * 8.843029975891113
Epoch 2150, val loss: 1.0631213188171387
Epoch 2160, training loss: 89.50951385498047 = 1.0631167888641357 + 10.0 * 8.844639778137207
Epoch 2160, val loss: 1.063056230545044
Epoch 2170, training loss: 89.48654174804688 = 1.0630477666854858 + 10.0 * 8.8423490524292
Epoch 2170, val loss: 1.0629920959472656
Epoch 2180, training loss: 89.49723052978516 = 1.0629810094833374 + 10.0 * 8.843424797058105
Epoch 2180, val loss: 1.0629301071166992
Epoch 2190, training loss: 89.53787994384766 = 1.0629180669784546 + 10.0 * 8.847496032714844
Epoch 2190, val loss: 1.0628713369369507
Epoch 2200, training loss: 89.52790069580078 = 1.0628517866134644 + 10.0 * 8.846505165100098
Epoch 2200, val loss: 1.0628077983856201
Epoch 2210, training loss: 89.51297760009766 = 1.0627834796905518 + 10.0 * 8.845019340515137
Epoch 2210, val loss: 1.0627460479736328
Epoch 2220, training loss: 89.54786682128906 = 1.0627179145812988 + 10.0 * 8.848514556884766
Epoch 2220, val loss: 1.0626773834228516
Epoch 2230, training loss: 89.42178344726562 = 1.0626343488693237 + 10.0 * 8.835914611816406
Epoch 2230, val loss: 1.0626072883605957
Epoch 2240, training loss: 89.41260528564453 = 1.0625708103179932 + 10.0 * 8.835003852844238
Epoch 2240, val loss: 1.0625468492507935
Epoch 2250, training loss: 89.40850067138672 = 1.0625059604644775 + 10.0 * 8.834599494934082
Epoch 2250, val loss: 1.062484860420227
Epoch 2260, training loss: 89.31620788574219 = 1.0624055862426758 + 10.0 * 8.825380325317383
Epoch 2260, val loss: 1.0623927116394043
Epoch 2270, training loss: 89.244384765625 = 1.0623472929000854 + 10.0 * 8.818203926086426
Epoch 2270, val loss: 1.0623332262039185
Epoch 2280, training loss: 89.33012390136719 = 1.0622955560684204 + 10.0 * 8.826783180236816
Epoch 2280, val loss: 1.0622899532318115
Epoch 2290, training loss: 89.27497100830078 = 1.0622326135635376 + 10.0 * 8.821273803710938
Epoch 2290, val loss: 1.062231183052063
Epoch 2300, training loss: 89.31539916992188 = 1.0621806383132935 + 10.0 * 8.825322151184082
Epoch 2300, val loss: 1.0621833801269531
Epoch 2310, training loss: 89.3545913696289 = 1.062126874923706 + 10.0 * 8.829246520996094
Epoch 2310, val loss: 1.0621304512023926
Epoch 2320, training loss: 89.41111755371094 = 1.0620737075805664 + 10.0 * 8.834904670715332
Epoch 2320, val loss: 1.0620801448822021
Epoch 2330, training loss: 89.46525573730469 = 1.062021255493164 + 10.0 * 8.840323448181152
Epoch 2330, val loss: 1.0620315074920654
Epoch 2340, training loss: 89.52030944824219 = 1.0619666576385498 + 10.0 * 8.845834732055664
Epoch 2340, val loss: 1.0619796514511108
Epoch 2350, training loss: 89.55928802490234 = 1.0619102716445923 + 10.0 * 8.849737167358398
Epoch 2350, val loss: 1.061927080154419
Epoch 2360, training loss: 89.57542419433594 = 1.0618520975112915 + 10.0 * 8.851357460021973
Epoch 2360, val loss: 1.061872959136963
Epoch 2370, training loss: 89.58262634277344 = 1.0617954730987549 + 10.0 * 8.852083206176758
Epoch 2370, val loss: 1.0618187189102173
Epoch 2380, training loss: 89.61833953857422 = 1.0617386102676392 + 10.0 * 8.855660438537598
Epoch 2380, val loss: 1.0617661476135254
Epoch 2390, training loss: 89.65070343017578 = 1.0616825819015503 + 10.0 * 8.858901977539062
Epoch 2390, val loss: 1.0617139339447021
Epoch 2400, training loss: 89.634765625 = 1.0616233348846436 + 10.0 * 8.857314109802246
Epoch 2400, val loss: 1.0616577863693237
Epoch 2410, training loss: 89.66708374023438 = 1.0615681409835815 + 10.0 * 8.860551834106445
Epoch 2410, val loss: 1.0616060495376587
Epoch 2420, training loss: 89.68266296386719 = 1.0615153312683105 + 10.0 * 8.862114906311035
Epoch 2420, val loss: 1.0615568161010742
Epoch 2430, training loss: 89.69815063476562 = 1.0614593029022217 + 10.0 * 8.863668441772461
Epoch 2430, val loss: 1.0615034103393555
Epoch 2440, training loss: 89.67631530761719 = 1.0613999366760254 + 10.0 * 8.861491203308105
Epoch 2440, val loss: 1.0614511966705322
Epoch 2450, training loss: 89.6776351928711 = 1.0613466501235962 + 10.0 * 8.861628532409668
Epoch 2450, val loss: 1.061397910118103
Epoch 2460, training loss: 89.70285034179688 = 1.0612921714782715 + 10.0 * 8.864155769348145
Epoch 2460, val loss: 1.0613486766815186
Epoch 2470, training loss: 89.72472381591797 = 1.061240315437317 + 10.0 * 8.866348266601562
Epoch 2470, val loss: 1.0613001585006714
Epoch 2480, training loss: 89.76602935791016 = 1.0611886978149414 + 10.0 * 8.870484352111816
Epoch 2480, val loss: 1.061251163482666
Epoch 2490, training loss: 89.72311401367188 = 1.0611295700073242 + 10.0 * 8.866198539733887
Epoch 2490, val loss: 1.0611988306045532
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.5078260869565218
0.8621314207056437
=== training gcn model ===
Epoch 0, training loss: 101.23675537109375 = 1.1102932691574097 + 10.0 * 10.012646675109863
Epoch 0, val loss: 1.1101956367492676
Epoch 10, training loss: 96.73052978515625 = 1.1096328496932983 + 10.0 * 9.562089920043945
Epoch 10, val loss: 1.1095969676971436
Epoch 20, training loss: 94.89635467529297 = 1.109313726425171 + 10.0 * 9.378704071044922
Epoch 20, val loss: 1.1092774868011475
Epoch 30, training loss: 93.63011169433594 = 1.1090272665023804 + 10.0 * 9.252108573913574
Epoch 30, val loss: 1.1089973449707031
Epoch 40, training loss: 92.65049743652344 = 1.1087353229522705 + 10.0 * 9.154176712036133
Epoch 40, val loss: 1.10870361328125
Epoch 50, training loss: 91.86550903320312 = 1.108433485031128 + 10.0 * 9.07570743560791
Epoch 50, val loss: 1.1083991527557373
Epoch 60, training loss: 91.20854949951172 = 1.1081289052963257 + 10.0 * 9.010042190551758
Epoch 60, val loss: 1.1080958843231201
Epoch 70, training loss: 90.65991973876953 = 1.107830286026001 + 10.0 * 8.955208778381348
Epoch 70, val loss: 1.1077979803085327
Epoch 80, training loss: 90.18730926513672 = 1.107527256011963 + 10.0 * 8.907978057861328
Epoch 80, val loss: 1.107494592666626
Epoch 90, training loss: 89.77709197998047 = 1.107231616973877 + 10.0 * 8.866986274719238
Epoch 90, val loss: 1.1072003841400146
Epoch 100, training loss: 89.431640625 = 1.106938123703003 + 10.0 * 8.832469940185547
Epoch 100, val loss: 1.1069083213806152
Epoch 110, training loss: 89.12767028808594 = 1.1066452264785767 + 10.0 * 8.802103042602539
Epoch 110, val loss: 1.106615662574768
Epoch 120, training loss: 88.87628936767578 = 1.1063593626022339 + 10.0 * 8.776992797851562
Epoch 120, val loss: 1.1063312292099
Epoch 130, training loss: 88.66374969482422 = 1.106074333190918 + 10.0 * 8.755767822265625
Epoch 130, val loss: 1.1060479879379272
Epoch 140, training loss: 88.46473693847656 = 1.1057900190353394 + 10.0 * 8.735895156860352
Epoch 140, val loss: 1.1057636737823486
Epoch 150, training loss: 88.31546783447266 = 1.105508804321289 + 10.0 * 8.720995903015137
Epoch 150, val loss: 1.1054829359054565
Epoch 160, training loss: 88.17164611816406 = 1.1052255630493164 + 10.0 * 8.706642150878906
Epoch 160, val loss: 1.105201005935669
Epoch 170, training loss: 88.0467758178711 = 1.1049456596374512 + 10.0 * 8.694183349609375
Epoch 170, val loss: 1.1049214601516724
Epoch 180, training loss: 87.93258666992188 = 1.1046682596206665 + 10.0 * 8.682791709899902
Epoch 180, val loss: 1.1046441793441772
Epoch 190, training loss: 87.83172607421875 = 1.1043894290924072 + 10.0 * 8.672733306884766
Epoch 190, val loss: 1.1043660640716553
Epoch 200, training loss: 87.75370025634766 = 1.1041138172149658 + 10.0 * 8.664958953857422
Epoch 200, val loss: 1.1040905714035034
Epoch 210, training loss: 87.6785888671875 = 1.103838562965393 + 10.0 * 8.657475471496582
Epoch 210, val loss: 1.103814959526062
Epoch 220, training loss: 87.6162338256836 = 1.1035643815994263 + 10.0 * 8.651267051696777
Epoch 220, val loss: 1.1035419702529907
Epoch 230, training loss: 87.5434799194336 = 1.10329270362854 + 10.0 * 8.644018173217773
Epoch 230, val loss: 1.1032696962356567
Epoch 240, training loss: 87.49729919433594 = 1.1030218601226807 + 10.0 * 8.63942813873291
Epoch 240, val loss: 1.1029994487762451
Epoch 250, training loss: 87.47447967529297 = 1.1027534008026123 + 10.0 * 8.63717269897461
Epoch 250, val loss: 1.1027313470840454
Epoch 260, training loss: 87.43492126464844 = 1.102488398551941 + 10.0 * 8.633243560791016
Epoch 260, val loss: 1.1024672985076904
Epoch 270, training loss: 87.38819885253906 = 1.1022181510925293 + 10.0 * 8.6285982131958
Epoch 270, val loss: 1.1021980047225952
Epoch 280, training loss: 87.35026550292969 = 1.1019542217254639 + 10.0 * 8.624831199645996
Epoch 280, val loss: 1.1019349098205566
Epoch 290, training loss: 87.32535552978516 = 1.101690649986267 + 10.0 * 8.622365951538086
Epoch 290, val loss: 1.1016710996627808
Epoch 300, training loss: 87.31389617919922 = 1.1014281511306763 + 10.0 * 8.621247291564941
Epoch 300, val loss: 1.1014095544815063
Epoch 310, training loss: 87.2790298461914 = 1.1011677980422974 + 10.0 * 8.617786407470703
Epoch 310, val loss: 1.1011489629745483
Epoch 320, training loss: 87.27804565429688 = 1.1009080410003662 + 10.0 * 8.617713928222656
Epoch 320, val loss: 1.1008899211883545
Epoch 330, training loss: 87.25434875488281 = 1.1006487607955933 + 10.0 * 8.61536979675293
Epoch 330, val loss: 1.1006309986114502
Epoch 340, training loss: 87.23486328125 = 1.1003893613815308 + 10.0 * 8.613447189331055
Epoch 340, val loss: 1.1003735065460205
Epoch 350, training loss: 87.21932220458984 = 1.1001360416412354 + 10.0 * 8.611918449401855
Epoch 350, val loss: 1.1001192331314087
Epoch 360, training loss: 87.23487854003906 = 1.0998821258544922 + 10.0 * 8.613499641418457
Epoch 360, val loss: 1.0998659133911133
Epoch 370, training loss: 87.23487091064453 = 1.0996285676956177 + 10.0 * 8.613523483276367
Epoch 370, val loss: 1.0996133089065552
Epoch 380, training loss: 87.2004165649414 = 1.0993728637695312 + 10.0 * 8.61010456085205
Epoch 380, val loss: 1.0993595123291016
Epoch 390, training loss: 87.20646667480469 = 1.0991255044937134 + 10.0 * 8.610733985900879
Epoch 390, val loss: 1.0991110801696777
Epoch 400, training loss: 87.20753479003906 = 1.0988764762878418 + 10.0 * 8.610865592956543
Epoch 400, val loss: 1.0988632440567017
Epoch 410, training loss: 87.22634887695312 = 1.0986419916152954 + 10.0 * 8.612771034240723
Epoch 410, val loss: 1.0986348390579224
Epoch 420, training loss: 87.21958923339844 = 1.098557710647583 + 10.0 * 8.612103462219238
Epoch 420, val loss: 1.0985686779022217
Epoch 430, training loss: 87.23428344726562 = 1.0985711812973022 + 10.0 * 8.613571166992188
Epoch 430, val loss: 1.0985774993896484
Epoch 440, training loss: 87.22370147705078 = 1.098559021949768 + 10.0 * 8.61251449584961
Epoch 440, val loss: 1.0985655784606934
Epoch 450, training loss: 87.19890594482422 = 1.0985466241836548 + 10.0 * 8.61003589630127
Epoch 450, val loss: 1.098559021949768
Epoch 460, training loss: 87.25432586669922 = 1.0985475778579712 + 10.0 * 8.615577697753906
Epoch 460, val loss: 1.0985585451126099
Epoch 470, training loss: 87.26191711425781 = 1.0985455513000488 + 10.0 * 8.616336822509766
Epoch 470, val loss: 1.0985559225082397
Epoch 480, training loss: 87.28521728515625 = 1.0985430479049683 + 10.0 * 8.618667602539062
Epoch 480, val loss: 1.0985552072525024
Epoch 490, training loss: 87.28865814208984 = 1.098542332649231 + 10.0 * 8.619011878967285
Epoch 490, val loss: 1.0985538959503174
Epoch 500, training loss: 87.273681640625 = 1.0985406637191772 + 10.0 * 8.617513656616211
Epoch 500, val loss: 1.0985534191131592
Epoch 510, training loss: 87.3251953125 = 1.0985406637191772 + 10.0 * 8.622665405273438
Epoch 510, val loss: 1.0985515117645264
Epoch 520, training loss: 87.33540344238281 = 1.0985397100448608 + 10.0 * 8.623685836791992
Epoch 520, val loss: 1.098550796508789
Epoch 530, training loss: 87.33145904541016 = 1.0985383987426758 + 10.0 * 8.623291969299316
Epoch 530, val loss: 1.0985503196716309
Epoch 540, training loss: 87.3389663696289 = 1.098537564277649 + 10.0 * 8.624042510986328
Epoch 540, val loss: 1.098549485206604
Epoch 550, training loss: 87.35399627685547 = 1.0985360145568848 + 10.0 * 8.6255464553833
Epoch 550, val loss: 1.0985478162765503
Epoch 560, training loss: 87.37873077392578 = 1.0985357761383057 + 10.0 * 8.628019332885742
Epoch 560, val loss: 1.0985476970672607
Epoch 570, training loss: 87.4228744506836 = 1.0985349416732788 + 10.0 * 8.632433891296387
Epoch 570, val loss: 1.0985469818115234
Epoch 580, training loss: 87.41227722167969 = 1.098534107208252 + 10.0 * 8.63137435913086
Epoch 580, val loss: 1.0985453128814697
Epoch 590, training loss: 87.41878509521484 = 1.09853196144104 + 10.0 * 8.632024765014648
Epoch 590, val loss: 1.0985440015792847
Epoch 600, training loss: 87.42253112792969 = 1.0985318422317505 + 10.0 * 8.632399559020996
Epoch 600, val loss: 1.0985445976257324
Epoch 610, training loss: 87.45873260498047 = 1.0985318422317505 + 10.0 * 8.636019706726074
Epoch 610, val loss: 1.0985441207885742
Epoch 620, training loss: 87.47347259521484 = 1.0985310077667236 + 10.0 * 8.637494087219238
Epoch 620, val loss: 1.098543643951416
Epoch 630, training loss: 87.4962387084961 = 1.0985301733016968 + 10.0 * 8.6397705078125
Epoch 630, val loss: 1.09854257106781
Epoch 640, training loss: 87.51468658447266 = 1.0985300540924072 + 10.0 * 8.64161491394043
Epoch 640, val loss: 1.0985428094863892
Epoch 650, training loss: 87.50247192382812 = 1.098528504371643 + 10.0 * 8.64039421081543
Epoch 650, val loss: 1.0985413789749146
Epoch 660, training loss: 87.53433227539062 = 1.0985283851623535 + 10.0 * 8.643580436706543
Epoch 660, val loss: 1.098541021347046
Epoch 670, training loss: 87.55198669433594 = 1.0985273122787476 + 10.0 * 8.645345687866211
Epoch 670, val loss: 1.0985389947891235
Epoch 680, training loss: 87.55387115478516 = 1.0985249280929565 + 10.0 * 8.64553451538086
Epoch 680, val loss: 1.0985382795333862
Epoch 690, training loss: 87.53567504882812 = 1.0985252857208252 + 10.0 * 8.643714904785156
Epoch 690, val loss: 1.0985385179519653
Epoch 700, training loss: 87.57728576660156 = 1.0985257625579834 + 10.0 * 8.647875785827637
Epoch 700, val loss: 1.0985385179519653
Epoch 710, training loss: 87.60394287109375 = 1.0985254049301147 + 10.0 * 8.650541305541992
Epoch 710, val loss: 1.0985383987426758
Epoch 720, training loss: 87.62519836425781 = 1.0985252857208252 + 10.0 * 8.652667045593262
Epoch 720, val loss: 1.0985383987426758
Epoch 730, training loss: 87.63306427001953 = 1.0985249280929565 + 10.0 * 8.653453826904297
Epoch 730, val loss: 1.0985373258590698
Epoch 740, training loss: 87.64778900146484 = 1.0984119176864624 + 10.0 * 8.654937744140625
Epoch 740, val loss: 1.0983562469482422
Epoch 750, training loss: 87.63827514648438 = 1.0979487895965576 + 10.0 * 8.654032707214355
Epoch 750, val loss: 1.097801685333252
Epoch 760, training loss: 87.65726470947266 = 1.0975067615509033 + 10.0 * 8.655976295471191
Epoch 760, val loss: 1.0972832441329956
Epoch 770, training loss: 87.6781234741211 = 1.0971295833587646 + 10.0 * 8.658099174499512
Epoch 770, val loss: 1.0968396663665771
Epoch 780, training loss: 87.72069549560547 = 1.096810221672058 + 10.0 * 8.662388801574707
Epoch 780, val loss: 1.0964601039886475
Epoch 790, training loss: 87.62859344482422 = 1.0965301990509033 + 10.0 * 8.653206825256348
Epoch 790, val loss: 1.096125841140747
Epoch 800, training loss: 87.65471649169922 = 1.0962895154953003 + 10.0 * 8.655842781066895
Epoch 800, val loss: 1.0958343744277954
Epoch 810, training loss: 87.71228790283203 = 1.096073865890503 + 10.0 * 8.66162109375
Epoch 810, val loss: 1.0955737829208374
Epoch 820, training loss: 87.74092102050781 = 1.0958821773529053 + 10.0 * 8.664504051208496
Epoch 820, val loss: 1.0953397750854492
Epoch 830, training loss: 87.77352905273438 = 1.0957069396972656 + 10.0 * 8.6677827835083
Epoch 830, val loss: 1.0951251983642578
Epoch 840, training loss: 87.79969024658203 = 1.095545768737793 + 10.0 * 8.670414924621582
Epoch 840, val loss: 1.0949267148971558
Epoch 850, training loss: 87.86705780029297 = 1.0953950881958008 + 10.0 * 8.677165985107422
Epoch 850, val loss: 1.0947374105453491
Epoch 860, training loss: 87.77898406982422 = 1.0952532291412354 + 10.0 * 8.668373107910156
Epoch 860, val loss: 1.0945611000061035
Epoch 870, training loss: 87.80701446533203 = 1.0951265096664429 + 10.0 * 8.671189308166504
Epoch 870, val loss: 1.0944026708602905
Epoch 880, training loss: 87.8193359375 = 1.0950099229812622 + 10.0 * 8.672432899475098
Epoch 880, val loss: 1.094254493713379
Epoch 890, training loss: 87.85460662841797 = 1.0948975086212158 + 10.0 * 8.675971031188965
Epoch 890, val loss: 1.0941126346588135
Epoch 900, training loss: 87.89110565185547 = 1.094794750213623 + 10.0 * 8.679631233215332
Epoch 900, val loss: 1.0939801931381226
Epoch 910, training loss: 87.908203125 = 1.0946954488754272 + 10.0 * 8.681350708007812
Epoch 910, val loss: 1.0938531160354614
Epoch 920, training loss: 87.939208984375 = 1.0946029424667358 + 10.0 * 8.684460639953613
Epoch 920, val loss: 1.0937329530715942
Epoch 930, training loss: 87.95623779296875 = 1.0945132970809937 + 10.0 * 8.686172485351562
Epoch 930, val loss: 1.0936156511306763
Epoch 940, training loss: 87.97648620605469 = 1.0944277048110962 + 10.0 * 8.68820571899414
Epoch 940, val loss: 1.0935052633285522
Epoch 950, training loss: 88.00768280029297 = 1.0943505764007568 + 10.0 * 8.691332817077637
Epoch 950, val loss: 1.0934031009674072
Epoch 960, training loss: 87.97962188720703 = 1.0942736864089966 + 10.0 * 8.6885347366333
Epoch 960, val loss: 1.0933016538619995
Epoch 970, training loss: 88.041748046875 = 1.0941872596740723 + 10.0 * 8.694756507873535
Epoch 970, val loss: 1.0931875705718994
Epoch 980, training loss: 88.11359405517578 = 1.09413480758667 + 10.0 * 8.701946258544922
Epoch 980, val loss: 1.0931156873703003
Epoch 990, training loss: 88.09732818603516 = 1.0940704345703125 + 10.0 * 8.700325965881348
Epoch 990, val loss: 1.0930304527282715
Epoch 1000, training loss: 88.11572265625 = 1.0940089225769043 + 10.0 * 8.702171325683594
Epoch 1000, val loss: 1.0929481983184814
Epoch 1010, training loss: 88.16914367675781 = 1.0939521789550781 + 10.0 * 8.70751953125
Epoch 1010, val loss: 1.092869758605957
Epoch 1020, training loss: 88.21430206298828 = 1.0938962697982788 + 10.0 * 8.712040901184082
Epoch 1020, val loss: 1.0927927494049072
Epoch 1030, training loss: 88.20452880859375 = 1.093841314315796 + 10.0 * 8.711069107055664
Epoch 1030, val loss: 1.0927177667617798
Epoch 1040, training loss: 88.25355529785156 = 1.0937907695770264 + 10.0 * 8.71597671508789
Epoch 1040, val loss: 1.0926470756530762
Epoch 1050, training loss: 88.30276489257812 = 1.093743085861206 + 10.0 * 8.720902442932129
Epoch 1050, val loss: 1.0925792455673218
Epoch 1060, training loss: 88.13582611083984 = 1.0936710834503174 + 10.0 * 8.704215049743652
Epoch 1060, val loss: 1.092496633529663
Epoch 1070, training loss: 88.16777801513672 = 1.0936373472213745 + 10.0 * 8.707414627075195
Epoch 1070, val loss: 1.092438817024231
Epoch 1080, training loss: 88.2032699584961 = 1.093593955039978 + 10.0 * 8.710967063903809
Epoch 1080, val loss: 1.0923781394958496
Epoch 1090, training loss: 88.2907485961914 = 1.0935591459274292 + 10.0 * 8.719718933105469
Epoch 1090, val loss: 1.092324137687683
Epoch 1100, training loss: 88.3455810546875 = 1.0935235023498535 + 10.0 * 8.72520637512207
Epoch 1100, val loss: 1.092271327972412
Epoch 1110, training loss: 88.34368896484375 = 1.0934834480285645 + 10.0 * 8.725020408630371
Epoch 1110, val loss: 1.09221351146698
Epoch 1120, training loss: 88.3763198852539 = 1.093449354171753 + 10.0 * 8.728286743164062
Epoch 1120, val loss: 1.0921627283096313
Epoch 1130, training loss: 88.41557312011719 = 1.09341561794281 + 10.0 * 8.732215881347656
Epoch 1130, val loss: 1.0921125411987305
Epoch 1140, training loss: 88.43519592285156 = 1.0933833122253418 + 10.0 * 8.73418140411377
Epoch 1140, val loss: 1.0920637845993042
Epoch 1150, training loss: 88.44737243652344 = 1.093351125717163 + 10.0 * 8.73540210723877
Epoch 1150, val loss: 1.0920158624649048
Epoch 1160, training loss: 88.46481323242188 = 1.0933198928833008 + 10.0 * 8.737149238586426
Epoch 1160, val loss: 1.0919688940048218
Epoch 1170, training loss: 88.4811782836914 = 1.093290090560913 + 10.0 * 8.738788604736328
Epoch 1170, val loss: 1.091924786567688
Epoch 1180, training loss: 88.49127197265625 = 1.0932598114013672 + 10.0 * 8.739801406860352
Epoch 1180, val loss: 1.091879963874817
Epoch 1190, training loss: 88.49851989746094 = 1.093232274055481 + 10.0 * 8.74052906036377
Epoch 1190, val loss: 1.0918384790420532
Epoch 1200, training loss: 88.55242919921875 = 1.0932108163833618 + 10.0 * 8.745922088623047
Epoch 1200, val loss: 1.0918006896972656
Epoch 1210, training loss: 88.5439682006836 = 1.0931830406188965 + 10.0 * 8.745078086853027
Epoch 1210, val loss: 1.091760277748108
Epoch 1220, training loss: 88.57234954833984 = 1.0931602716445923 + 10.0 * 8.747919082641602
Epoch 1220, val loss: 1.0917235612869263
Epoch 1230, training loss: 88.61023712158203 = 1.0931377410888672 + 10.0 * 8.751709938049316
Epoch 1230, val loss: 1.0916866064071655
Epoch 1240, training loss: 88.32569885253906 = 1.0930765867233276 + 10.0 * 8.723261833190918
Epoch 1240, val loss: 1.0916041135787964
Epoch 1250, training loss: 88.42584991455078 = 1.0930384397506714 + 10.0 * 8.733281135559082
Epoch 1250, val loss: 1.091551423072815
Epoch 1260, training loss: 88.39676666259766 = 1.0930372476577759 + 10.0 * 8.730372428894043
Epoch 1260, val loss: 1.091559886932373
Epoch 1270, training loss: 88.17855834960938 = 1.093018889427185 + 10.0 * 8.7085542678833
Epoch 1270, val loss: 1.0915164947509766
Epoch 1280, training loss: 88.24978637695312 = 1.0930007696151733 + 10.0 * 8.715678215026855
Epoch 1280, val loss: 1.091493844985962
Epoch 1290, training loss: 88.24034118652344 = 1.0929876565933228 + 10.0 * 8.71473503112793
Epoch 1290, val loss: 1.0914664268493652
Epoch 1300, training loss: 88.30043029785156 = 1.0929807424545288 + 10.0 * 8.720745086669922
Epoch 1300, val loss: 1.0914461612701416
Epoch 1310, training loss: 88.40140533447266 = 1.092969298362732 + 10.0 * 8.730843544006348
Epoch 1310, val loss: 1.0914239883422852
Epoch 1320, training loss: 88.48530578613281 = 1.0929573774337769 + 10.0 * 8.739234924316406
Epoch 1320, val loss: 1.0914002656936646
Epoch 1330, training loss: 88.53935241699219 = 1.0929453372955322 + 10.0 * 8.744640350341797
Epoch 1330, val loss: 1.0913764238357544
Epoch 1340, training loss: 88.60973358154297 = 1.0929327011108398 + 10.0 * 8.751680374145508
Epoch 1340, val loss: 1.0913532972335815
Epoch 1350, training loss: 88.6458740234375 = 1.0929200649261475 + 10.0 * 8.755295753479004
Epoch 1350, val loss: 1.0913301706314087
Epoch 1360, training loss: 88.65409851074219 = 1.09290611743927 + 10.0 * 8.756119728088379
Epoch 1360, val loss: 1.0913060903549194
Epoch 1370, training loss: 88.6683578491211 = 1.0928937196731567 + 10.0 * 8.757546424865723
Epoch 1370, val loss: 1.0912835597991943
Epoch 1380, training loss: 88.69450378417969 = 1.0928820371627808 + 10.0 * 8.760162353515625
Epoch 1380, val loss: 1.091261625289917
Epoch 1390, training loss: 88.72212219238281 = 1.0928709506988525 + 10.0 * 8.762925148010254
Epoch 1390, val loss: 1.0912402868270874
Epoch 1400, training loss: 88.73464965820312 = 1.0928581953048706 + 10.0 * 8.764179229736328
Epoch 1400, val loss: 1.0912193059921265
Epoch 1410, training loss: 88.7760009765625 = 1.0928473472595215 + 10.0 * 8.768315315246582
Epoch 1410, val loss: 1.0911989212036133
Epoch 1420, training loss: 88.82121276855469 = 1.092837929725647 + 10.0 * 8.77283763885498
Epoch 1420, val loss: 1.0911798477172852
Epoch 1430, training loss: 88.84400177001953 = 1.092828392982483 + 10.0 * 8.775117874145508
Epoch 1430, val loss: 1.091160774230957
Epoch 1440, training loss: 88.84283447265625 = 1.0928175449371338 + 10.0 * 8.775001525878906
Epoch 1440, val loss: 1.0911415815353394
Epoch 1450, training loss: 88.81719970703125 = 1.0928049087524414 + 10.0 * 8.772439956665039
Epoch 1450, val loss: 1.0911213159561157
Epoch 1460, training loss: 88.86528015136719 = 1.0927989482879639 + 10.0 * 8.77724838256836
Epoch 1460, val loss: 1.0911054611206055
Epoch 1470, training loss: 88.90314483642578 = 1.0927916765213013 + 10.0 * 8.781035423278809
Epoch 1470, val loss: 1.0910900831222534
Epoch 1480, training loss: 88.94354248046875 = 1.0927836894989014 + 10.0 * 8.785076141357422
Epoch 1480, val loss: 1.0910743474960327
Epoch 1490, training loss: 88.96138000488281 = 1.0927757024765015 + 10.0 * 8.786860466003418
Epoch 1490, val loss: 1.0910580158233643
Epoch 1500, training loss: 88.90843963623047 = 1.0927646160125732 + 10.0 * 8.781567573547363
Epoch 1500, val loss: 1.091040849685669
Epoch 1510, training loss: 88.93865966796875 = 1.0927590131759644 + 10.0 * 8.784589767456055
Epoch 1510, val loss: 1.0910273790359497
Epoch 1520, training loss: 88.94758605957031 = 1.092751145362854 + 10.0 * 8.785483360290527
Epoch 1520, val loss: 1.0910122394561768
Epoch 1530, training loss: 88.92396545410156 = 1.0927410125732422 + 10.0 * 8.783122062683105
Epoch 1530, val loss: 1.0909943580627441
Epoch 1540, training loss: 88.97628021240234 = 1.0927369594573975 + 10.0 * 8.788354873657227
Epoch 1540, val loss: 1.0909842252731323
Epoch 1550, training loss: 89.02264404296875 = 1.0927326679229736 + 10.0 * 8.792990684509277
Epoch 1550, val loss: 1.0909732580184937
Epoch 1560, training loss: 89.0506362915039 = 1.0927274227142334 + 10.0 * 8.795790672302246
Epoch 1560, val loss: 1.0909597873687744
Epoch 1570, training loss: 89.04541778564453 = 1.0927202701568604 + 10.0 * 8.795269966125488
Epoch 1570, val loss: 1.0909475088119507
Epoch 1580, training loss: 89.0543212890625 = 1.0927125215530396 + 10.0 * 8.796160697937012
Epoch 1580, val loss: 1.0909345149993896
Epoch 1590, training loss: 89.06424713134766 = 1.0927090644836426 + 10.0 * 8.79715347290039
Epoch 1590, val loss: 1.0909227132797241
Epoch 1600, training loss: 89.10419464111328 = 1.0927045345306396 + 10.0 * 8.801149368286133
Epoch 1600, val loss: 1.09091317653656
Epoch 1610, training loss: 89.1039047241211 = 1.0926995277404785 + 10.0 * 8.80112075805664
Epoch 1610, val loss: 1.090901494026184
Epoch 1620, training loss: 89.13775634765625 = 1.092695951461792 + 10.0 * 8.804506301879883
Epoch 1620, val loss: 1.0908923149108887
Epoch 1630, training loss: 89.11737060546875 = 1.0926904678344727 + 10.0 * 8.802468299865723
Epoch 1630, val loss: 1.0908797979354858
Epoch 1640, training loss: 89.14566040039062 = 1.0926862955093384 + 10.0 * 8.805296897888184
Epoch 1640, val loss: 1.0908706188201904
Epoch 1650, training loss: 89.18476104736328 = 1.092684030532837 + 10.0 * 8.809207916259766
Epoch 1650, val loss: 1.0908626317977905
Epoch 1660, training loss: 89.19136810302734 = 1.0926796197891235 + 10.0 * 8.809868812561035
Epoch 1660, val loss: 1.090852975845337
Epoch 1670, training loss: 89.19296264648438 = 1.0926753282546997 + 10.0 * 8.810029029846191
Epoch 1670, val loss: 1.0908417701721191
Epoch 1680, training loss: 89.19815063476562 = 1.0926684141159058 + 10.0 * 8.810548782348633
Epoch 1680, val loss: 1.0908300876617432
Epoch 1690, training loss: 89.1471939086914 = 1.0926625728607178 + 10.0 * 8.805453300476074
Epoch 1690, val loss: 1.0908206701278687
Epoch 1700, training loss: 89.18611907958984 = 1.0926607847213745 + 10.0 * 8.809346199035645
Epoch 1700, val loss: 1.0908138751983643
Epoch 1710, training loss: 89.23757934570312 = 1.0926604270935059 + 10.0 * 8.814492225646973
Epoch 1710, val loss: 1.0908087491989136
Epoch 1720, training loss: 89.2627944946289 = 1.0926575660705566 + 10.0 * 8.81701374053955
Epoch 1720, val loss: 1.090800166130066
Epoch 1730, training loss: 89.26020812988281 = 1.0926522016525269 + 10.0 * 8.816755294799805
Epoch 1730, val loss: 1.0907906293869019
Epoch 1740, training loss: 89.2905502319336 = 1.0926510095596313 + 10.0 * 8.81978988647461
Epoch 1740, val loss: 1.0907865762710571
Epoch 1750, training loss: 89.32086944580078 = 1.0926504135131836 + 10.0 * 8.822821617126465
Epoch 1750, val loss: 1.0907783508300781
Epoch 1760, training loss: 89.2842788696289 = 1.0926439762115479 + 10.0 * 8.81916332244873
Epoch 1760, val loss: 1.09076988697052
Epoch 1770, training loss: 89.2929458618164 = 1.0926417112350464 + 10.0 * 8.820030212402344
Epoch 1770, val loss: 1.090764045715332
Epoch 1780, training loss: 89.34453582763672 = 1.09264075756073 + 10.0 * 8.825189590454102
Epoch 1780, val loss: 1.09075927734375
Epoch 1790, training loss: 89.35322570800781 = 1.092638611793518 + 10.0 * 8.826059341430664
Epoch 1790, val loss: 1.090751051902771
Epoch 1800, training loss: 89.35557556152344 = 1.0926363468170166 + 10.0 * 8.8262939453125
Epoch 1800, val loss: 1.090746283531189
Epoch 1810, training loss: 89.3935775756836 = 1.0926363468170166 + 10.0 * 8.830094337463379
Epoch 1810, val loss: 1.0907429456710815
Epoch 1820, training loss: 89.35861206054688 = 1.092630386352539 + 10.0 * 8.826598167419434
Epoch 1820, val loss: 1.0907336473464966
Epoch 1830, training loss: 89.37570190429688 = 1.092626690864563 + 10.0 * 8.828307151794434
Epoch 1830, val loss: 1.0907268524169922
Epoch 1840, training loss: 89.37581634521484 = 1.0926254987716675 + 10.0 * 8.82831859588623
Epoch 1840, val loss: 1.0907212495803833
Epoch 1850, training loss: 89.42682647705078 = 1.0926251411437988 + 10.0 * 8.833419799804688
Epoch 1850, val loss: 1.0907175540924072
Epoch 1860, training loss: 89.45721435546875 = 1.0926246643066406 + 10.0 * 8.836459159851074
Epoch 1860, val loss: 1.0907143354415894
Epoch 1870, training loss: 89.47351837158203 = 1.0926228761672974 + 10.0 * 8.838089942932129
Epoch 1870, val loss: 1.090707540512085
Epoch 1880, training loss: 89.44184112548828 = 1.092618465423584 + 10.0 * 8.834921836853027
Epoch 1880, val loss: 1.090700387954712
Epoch 1890, training loss: 89.45978546142578 = 1.092616081237793 + 10.0 * 8.83671760559082
Epoch 1890, val loss: 1.0906963348388672
Epoch 1900, training loss: 89.48612213134766 = 1.0926165580749512 + 10.0 * 8.839350700378418
Epoch 1900, val loss: 1.0906929969787598
Epoch 1910, training loss: 89.49764251708984 = 1.0926152467727661 + 10.0 * 8.840502738952637
Epoch 1910, val loss: 1.0906884670257568
Epoch 1920, training loss: 89.42943572998047 = 1.0926074981689453 + 10.0 * 8.833683013916016
Epoch 1920, val loss: 1.0906777381896973
Epoch 1930, training loss: 89.3943099975586 = 1.092599630355835 + 10.0 * 8.830171585083008
Epoch 1930, val loss: 1.0906695127487183
Epoch 1940, training loss: 89.4140853881836 = 1.0926012992858887 + 10.0 * 8.832148551940918
Epoch 1940, val loss: 1.0906671285629272
Epoch 1950, training loss: 89.47660827636719 = 1.092603325843811 + 10.0 * 8.838399887084961
Epoch 1950, val loss: 1.0906661748886108
Epoch 1960, training loss: 89.52701568603516 = 1.0926053524017334 + 10.0 * 8.843441009521484
Epoch 1960, val loss: 1.0906656980514526
Epoch 1970, training loss: 89.54705810546875 = 1.0926028490066528 + 10.0 * 8.84544563293457
Epoch 1970, val loss: 1.0906603336334229
Epoch 1980, training loss: 89.5357437133789 = 1.0926012992858887 + 10.0 * 8.844314575195312
Epoch 1980, val loss: 1.0906561613082886
Epoch 1990, training loss: 89.56695556640625 = 1.0926024913787842 + 10.0 * 8.847434997558594
Epoch 1990, val loss: 1.09065580368042
Epoch 2000, training loss: 89.58708190917969 = 1.0926015377044678 + 10.0 * 8.849448204040527
Epoch 2000, val loss: 1.090652585029602
Epoch 2010, training loss: 89.58174896240234 = 1.0925980806350708 + 10.0 * 8.848915100097656
Epoch 2010, val loss: 1.0906466245651245
Epoch 2020, training loss: 89.5836181640625 = 1.0925976037979126 + 10.0 * 8.849102020263672
Epoch 2020, val loss: 1.0906431674957275
Epoch 2030, training loss: 89.6056900024414 = 1.0925976037979126 + 10.0 * 8.851308822631836
Epoch 2030, val loss: 1.0906412601470947
Epoch 2040, training loss: 89.60556030273438 = 1.0925958156585693 + 10.0 * 8.851296424865723
Epoch 2040, val loss: 1.0906381607055664
Epoch 2050, training loss: 89.6129150390625 = 1.092594861984253 + 10.0 * 8.852031707763672
Epoch 2050, val loss: 1.090635895729065
Epoch 2060, training loss: 89.63492584228516 = 1.0925946235656738 + 10.0 * 8.854232788085938
Epoch 2060, val loss: 1.090633511543274
Epoch 2070, training loss: 89.61558532714844 = 1.0925869941711426 + 10.0 * 8.852299690246582
Epoch 2070, val loss: 1.0906227827072144
Epoch 2080, training loss: 89.63644409179688 = 1.0925886631011963 + 10.0 * 8.854385375976562
Epoch 2080, val loss: 1.090621829032898
Epoch 2090, training loss: 89.64917755126953 = 1.0925898551940918 + 10.0 * 8.855658531188965
Epoch 2090, val loss: 1.0906208753585815
Epoch 2100, training loss: 89.65380859375 = 1.0925875902175903 + 10.0 * 8.856122016906738
Epoch 2100, val loss: 1.090619444847107
Epoch 2110, training loss: 89.67552185058594 = 1.0925910472869873 + 10.0 * 8.858293533325195
Epoch 2110, val loss: 1.090619683265686
Epoch 2120, training loss: 89.69236755371094 = 1.092590570449829 + 10.0 * 8.859977722167969
Epoch 2120, val loss: 1.0906175374984741
Epoch 2130, training loss: 89.67586517333984 = 1.0925862789154053 + 10.0 * 8.858327865600586
Epoch 2130, val loss: 1.090612769126892
Epoch 2140, training loss: 89.69168853759766 = 1.0925862789154053 + 10.0 * 8.85991096496582
Epoch 2140, val loss: 1.0906107425689697
Epoch 2150, training loss: 89.73558044433594 = 1.0925873517990112 + 10.0 * 8.864298820495605
Epoch 2150, val loss: 1.0906107425689697
Epoch 2160, training loss: 89.7447509765625 = 1.0925873517990112 + 10.0 * 8.865216255187988
Epoch 2160, val loss: 1.0906074047088623
Epoch 2170, training loss: 89.73664855957031 = 1.0925847291946411 + 10.0 * 8.86440658569336
Epoch 2170, val loss: 1.0906020402908325
Epoch 2180, training loss: 89.69047546386719 = 1.0925735235214233 + 10.0 * 8.859789848327637
Epoch 2180, val loss: 1.0905945301055908
Epoch 2190, training loss: 89.71368408203125 = 1.092576026916504 + 10.0 * 8.86211109161377
Epoch 2190, val loss: 1.0905953645706177
Epoch 2200, training loss: 89.75469207763672 = 1.0925787687301636 + 10.0 * 8.866211891174316
Epoch 2200, val loss: 1.09059476852417
Epoch 2210, training loss: 89.64362335205078 = 1.092553973197937 + 10.0 * 8.855107307434082
Epoch 2210, val loss: 1.0905745029449463
Epoch 2220, training loss: 89.52581024169922 = 1.09254789352417 + 10.0 * 8.843326568603516
Epoch 2220, val loss: 1.0905629396438599
Epoch 2230, training loss: 89.5392837524414 = 1.0925441980361938 + 10.0 * 8.844674110412598
Epoch 2230, val loss: 1.0905593633651733
Epoch 2240, training loss: 89.60074615478516 = 1.09255051612854 + 10.0 * 8.85081958770752
Epoch 2240, val loss: 1.0905638933181763
Epoch 2250, training loss: 89.64961242675781 = 1.0925575494766235 + 10.0 * 8.855705261230469
Epoch 2250, val loss: 1.0905711650848389
Epoch 2260, training loss: 89.6993179321289 = 1.0925605297088623 + 10.0 * 8.860675811767578
Epoch 2260, val loss: 1.0905730724334717
Epoch 2270, training loss: 89.74813079833984 = 1.0925648212432861 + 10.0 * 8.865556716918945
Epoch 2270, val loss: 1.0905760526657104
Epoch 2280, training loss: 89.783203125 = 1.0925662517547607 + 10.0 * 8.869063377380371
Epoch 2280, val loss: 1.0905752182006836
Epoch 2290, training loss: 89.80270385742188 = 1.0925664901733398 + 10.0 * 8.871013641357422
Epoch 2290, val loss: 1.0905749797821045
Epoch 2300, training loss: 89.82903289794922 = 1.0925676822662354 + 10.0 * 8.87364673614502
Epoch 2300, val loss: 1.090575098991394
Epoch 2310, training loss: 89.83561706542969 = 1.0925663709640503 + 10.0 * 8.87430477142334
Epoch 2310, val loss: 1.0905728340148926
Epoch 2320, training loss: 89.8654556274414 = 1.0925666093826294 + 10.0 * 8.877288818359375
Epoch 2320, val loss: 1.0905719995498657
Epoch 2330, training loss: 89.86750793457031 = 1.0925647020339966 + 10.0 * 8.877493858337402
Epoch 2330, val loss: 1.0905699729919434
Epoch 2340, training loss: 89.88375091552734 = 1.0925648212432861 + 10.0 * 8.879117965698242
Epoch 2340, val loss: 1.0905689001083374
Epoch 2350, training loss: 89.90105438232422 = 1.0925648212432861 + 10.0 * 8.88084888458252
Epoch 2350, val loss: 1.0905681848526
Epoch 2360, training loss: 89.91716766357422 = 1.092564582824707 + 10.0 * 8.88245964050293
Epoch 2360, val loss: 1.0905667543411255
Epoch 2370, training loss: 89.91529846191406 = 1.0925624370574951 + 10.0 * 8.88227367401123
Epoch 2370, val loss: 1.0905647277832031
Epoch 2380, training loss: 89.92899322509766 = 1.0925624370574951 + 10.0 * 8.88364315032959
Epoch 2380, val loss: 1.0905643701553345
Epoch 2390, training loss: 89.93254852294922 = 1.0925605297088623 + 10.0 * 8.88399887084961
Epoch 2390, val loss: 1.090562105178833
Epoch 2400, training loss: 89.9544906616211 = 1.0925599336624146 + 10.0 * 8.88619327545166
Epoch 2400, val loss: 1.0905601978302002
Epoch 2410, training loss: 89.98526000976562 = 1.0925605297088623 + 10.0 * 8.889269828796387
Epoch 2410, val loss: 1.090559959411621
Epoch 2420, training loss: 89.94210815429688 = 1.0925554037094116 + 10.0 * 8.884955406188965
Epoch 2420, val loss: 1.0905553102493286
Epoch 2430, training loss: 89.96977996826172 = 1.092556357383728 + 10.0 * 8.88772201538086
Epoch 2430, val loss: 1.0905556678771973
Epoch 2440, training loss: 89.99938201904297 = 1.09255850315094 + 10.0 * 8.890682220458984
Epoch 2440, val loss: 1.0905566215515137
Epoch 2450, training loss: 90.03519439697266 = 1.0925589799880981 + 10.0 * 8.89426326751709
Epoch 2450, val loss: 1.090556263923645
Epoch 2460, training loss: 89.97689819335938 = 1.0925534963607788 + 10.0 * 8.888434410095215
Epoch 2460, val loss: 1.090550422668457
Epoch 2470, training loss: 89.98759460449219 = 1.0925519466400146 + 10.0 * 8.889504432678223
Epoch 2470, val loss: 1.0905489921569824
Epoch 2480, training loss: 90.0205307006836 = 1.0925538539886475 + 10.0 * 8.892797470092773
Epoch 2480, val loss: 1.0905503034591675
Epoch 2490, training loss: 90.04478454589844 = 1.0925559997558594 + 10.0 * 8.895222663879395
Epoch 2490, val loss: 1.0905510187149048
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.3973913043478261
0.8649568934289648
The final CL Acc:0.43420, 0.05206, The final GNN Acc:0.86387, 0.00124
Begin epxeriment: noisy_level: 0.2 cont_weight: 10 epoch: der1:0.3 der2:0.4 dfr1:0.4 dfr2:0.4
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=2500, cl_num_layers=2, cl_num_proj_hidden=64, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=10.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.4, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=32, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, noisy_level=0.2, num_hidden=64, num_proj_hidden=64, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=40, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 88648])
add edge: torch.Size([2, 106334])
remove edge: torch.Size([2, 70866])
updated graph: torch.Size([2, 88552])
=== Noisy graph ===
/home/project-robust-contrastive/Robust-Contrastive-Learning/model.py:227: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 104.20223999023438 = 1.1065964698791504 + 10.0 * 10.309564590454102
Epoch 0, val loss: 1.1064566373825073
Epoch 10, training loss: 100.19013977050781 = 1.1057034730911255 + 10.0 * 9.908443450927734
Epoch 10, val loss: 1.10567045211792
Epoch 20, training loss: 98.36585998535156 = 1.1053558588027954 + 10.0 * 9.72605037689209
Epoch 20, val loss: 1.1053355932235718
Epoch 30, training loss: 97.06967163085938 = 1.1050225496292114 + 10.0 * 9.596464157104492
Epoch 30, val loss: 1.1050083637237549
Epoch 40, training loss: 96.04798889160156 = 1.1046793460845947 + 10.0 * 9.494330406188965
Epoch 40, val loss: 1.1046638488769531
Epoch 50, training loss: 95.19737243652344 = 1.104319453239441 + 10.0 * 9.409305572509766
Epoch 50, val loss: 1.1043076515197754
Epoch 60, training loss: 94.46797180175781 = 1.1039543151855469 + 10.0 * 9.336400985717773
Epoch 60, val loss: 1.1039419174194336
Epoch 70, training loss: 93.830322265625 = 1.1035921573638916 + 10.0 * 9.272672653198242
Epoch 70, val loss: 1.1035829782485962
Epoch 80, training loss: 93.2816390991211 = 1.1032155752182007 + 10.0 * 9.217842102050781
Epoch 80, val loss: 1.1032114028930664
Epoch 90, training loss: 92.80359649658203 = 1.1028354167938232 + 10.0 * 9.170076370239258
Epoch 90, val loss: 1.1028339862823486
Epoch 100, training loss: 92.37923431396484 = 1.1024404764175415 + 10.0 * 9.127679824829102
Epoch 100, val loss: 1.1024473905563354
Epoch 110, training loss: 92.02259063720703 = 1.1020359992980957 + 10.0 * 9.092055320739746
Epoch 110, val loss: 1.1020489931106567
Epoch 120, training loss: 91.71522521972656 = 1.1016334295272827 + 10.0 * 9.061359405517578
Epoch 120, val loss: 1.1016536951065063
Epoch 130, training loss: 91.44465637207031 = 1.1012145280838013 + 10.0 * 9.034344673156738
Epoch 130, val loss: 1.1012459993362427
Epoch 140, training loss: 91.22427368164062 = 1.1007912158966064 + 10.0 * 9.012348175048828
Epoch 140, val loss: 1.1008226871490479
Epoch 150, training loss: 91.01547241210938 = 1.1003565788269043 + 10.0 * 8.991511344909668
Epoch 150, val loss: 1.1004081964492798
Epoch 160, training loss: 90.84381866455078 = 1.0999292135238647 + 10.0 * 8.97438907623291
Epoch 160, val loss: 1.0999945402145386
Epoch 170, training loss: 90.70720672607422 = 1.0994974374771118 + 10.0 * 8.960771560668945
Epoch 170, val loss: 1.0995670557022095
Epoch 180, training loss: 90.58119201660156 = 1.0990650653839111 + 10.0 * 8.948212623596191
Epoch 180, val loss: 1.0991389751434326
Epoch 190, training loss: 90.47130584716797 = 1.0986578464508057 + 10.0 * 8.937265396118164
Epoch 190, val loss: 1.0987415313720703
Epoch 200, training loss: 90.40250396728516 = 1.0982979536056519 + 10.0 * 8.930420875549316
Epoch 200, val loss: 1.0983998775482178
Epoch 210, training loss: 90.30577087402344 = 1.0979971885681152 + 10.0 * 8.920777320861816
Epoch 210, val loss: 1.098103404045105
Epoch 220, training loss: 90.24021911621094 = 1.0977801084518433 + 10.0 * 8.914243698120117
Epoch 220, val loss: 1.0978952646255493
Epoch 230, training loss: 90.17832946777344 = 1.0976308584213257 + 10.0 * 8.908069610595703
Epoch 230, val loss: 1.0977602005004883
Epoch 240, training loss: 90.08326721191406 = 1.0975255966186523 + 10.0 * 8.898573875427246
Epoch 240, val loss: 1.097658634185791
Epoch 250, training loss: 90.07479095458984 = 1.0974339246749878 + 10.0 * 8.897735595703125
Epoch 250, val loss: 1.0975719690322876
Epoch 260, training loss: 89.99932861328125 = 1.0973501205444336 + 10.0 * 8.89019775390625
Epoch 260, val loss: 1.0974957942962646
Epoch 270, training loss: 89.98536682128906 = 1.097277045249939 + 10.0 * 8.888809204101562
Epoch 270, val loss: 1.0974284410476685
Epoch 280, training loss: 89.93318176269531 = 1.09719717502594 + 10.0 * 8.883598327636719
Epoch 280, val loss: 1.0973544120788574
Epoch 290, training loss: 89.87860870361328 = 1.0971181392669678 + 10.0 * 8.878149032592773
Epoch 290, val loss: 1.0972846746444702
Epoch 300, training loss: 89.83070373535156 = 1.097030758857727 + 10.0 * 8.873367309570312
Epoch 300, val loss: 1.0972059965133667
Epoch 310, training loss: 89.87303924560547 = 1.096961259841919 + 10.0 * 8.877607345581055
Epoch 310, val loss: 1.0971392393112183
Epoch 320, training loss: 89.83245849609375 = 1.0968812704086304 + 10.0 * 8.873558044433594
Epoch 320, val loss: 1.097068190574646
Epoch 330, training loss: 89.83390045166016 = 1.0968070030212402 + 10.0 * 8.873708724975586
Epoch 330, val loss: 1.097000241279602
Epoch 340, training loss: 89.79667663574219 = 1.0967220067977905 + 10.0 * 8.8699951171875
Epoch 340, val loss: 1.0969204902648926
Epoch 350, training loss: 89.79998016357422 = 1.0966392755508423 + 10.0 * 8.870333671569824
Epoch 350, val loss: 1.0968443155288696
Epoch 360, training loss: 89.8349609375 = 1.0965579748153687 + 10.0 * 8.87384033203125
Epoch 360, val loss: 1.0967730283737183
Epoch 370, training loss: 89.79873657226562 = 1.0964598655700684 + 10.0 * 8.870227813720703
Epoch 370, val loss: 1.0966815948486328
Epoch 380, training loss: 89.8123550415039 = 1.0963760614395142 + 10.0 * 8.871598243713379
Epoch 380, val loss: 1.0966050624847412
Epoch 390, training loss: 89.8321762084961 = 1.0962836742401123 + 10.0 * 8.873589515686035
Epoch 390, val loss: 1.096522569656372
Epoch 400, training loss: 89.8153305053711 = 1.0961893796920776 + 10.0 * 8.87191390991211
Epoch 400, val loss: 1.0964380502700806
Epoch 410, training loss: 89.93216705322266 = 1.0960897207260132 + 10.0 * 8.883607864379883
Epoch 410, val loss: 1.0963327884674072
Epoch 420, training loss: 90.0303955078125 = 1.0960006713867188 + 10.0 * 8.893439292907715
Epoch 420, val loss: 1.0962637662887573
Epoch 430, training loss: 89.72879791259766 = 1.0958713293075562 + 10.0 * 8.863292694091797
Epoch 430, val loss: 1.0961456298828125
Epoch 440, training loss: 89.77460479736328 = 1.0957897901535034 + 10.0 * 8.867881774902344
Epoch 440, val loss: 1.0960705280303955
Epoch 450, training loss: 89.76647186279297 = 1.0956666469573975 + 10.0 * 8.867080688476562
Epoch 450, val loss: 1.0959656238555908
Epoch 460, training loss: 89.91326904296875 = 1.0955777168273926 + 10.0 * 8.881769180297852
Epoch 460, val loss: 1.0958752632141113
Epoch 470, training loss: 89.78805541992188 = 1.0954465866088867 + 10.0 * 8.869260787963867
Epoch 470, val loss: 1.0957592725753784
Epoch 480, training loss: 90.15077209472656 = 1.095357060432434 + 10.0 * 8.90554141998291
Epoch 480, val loss: 1.0956714153289795
Epoch 490, training loss: 89.80667877197266 = 1.0951955318450928 + 10.0 * 8.871149063110352
Epoch 490, val loss: 1.0955332517623901
Epoch 500, training loss: 89.96867370605469 = 1.095092535018921 + 10.0 * 8.887357711791992
Epoch 500, val loss: 1.0954313278198242
Epoch 510, training loss: 89.9454116821289 = 1.0949647426605225 + 10.0 * 8.885045051574707
Epoch 510, val loss: 1.095325231552124
Epoch 520, training loss: 89.97459411621094 = 1.094834804534912 + 10.0 * 8.887975692749023
Epoch 520, val loss: 1.0952000617980957
Epoch 530, training loss: 90.09846496582031 = 1.0947070121765137 + 10.0 * 8.900376319885254
Epoch 530, val loss: 1.0950829982757568
Epoch 540, training loss: 90.14160919189453 = 1.094565987586975 + 10.0 * 8.904704093933105
Epoch 540, val loss: 1.0949546098709106
Epoch 550, training loss: 90.16339874267578 = 1.0944240093231201 + 10.0 * 8.90689754486084
Epoch 550, val loss: 1.0948247909545898
Epoch 560, training loss: 90.16703796386719 = 1.0942718982696533 + 10.0 * 8.90727710723877
Epoch 560, val loss: 1.09468674659729
Epoch 570, training loss: 90.21543884277344 = 1.094114065170288 + 10.0 * 8.912132263183594
Epoch 570, val loss: 1.0945439338684082
Epoch 580, training loss: 90.21452331542969 = 1.0939549207687378 + 10.0 * 8.912056922912598
Epoch 580, val loss: 1.094399333000183
Epoch 590, training loss: 90.22013854980469 = 1.0937927961349487 + 10.0 * 8.912633895874023
Epoch 590, val loss: 1.0942480564117432
Epoch 600, training loss: 90.2919692993164 = 1.093626618385315 + 10.0 * 8.91983413696289
Epoch 600, val loss: 1.0940974950790405
Epoch 610, training loss: 90.25460052490234 = 1.093459963798523 + 10.0 * 8.91611385345459
Epoch 610, val loss: 1.093947172164917
Epoch 620, training loss: 90.29112243652344 = 1.0932862758636475 + 10.0 * 8.919783592224121
Epoch 620, val loss: 1.0937902927398682
Epoch 630, training loss: 90.3927230834961 = 1.093115210533142 + 10.0 * 8.929960250854492
Epoch 630, val loss: 1.0936323404312134
Epoch 640, training loss: 90.36707305908203 = 1.0929358005523682 + 10.0 * 8.927413940429688
Epoch 640, val loss: 1.0934721231460571
Epoch 650, training loss: 90.37987518310547 = 1.0927553176879883 + 10.0 * 8.928711891174316
Epoch 650, val loss: 1.0933088064193726
Epoch 660, training loss: 90.39105987548828 = 1.092568278312683 + 10.0 * 8.929849624633789
Epoch 660, val loss: 1.0931414365768433
Epoch 670, training loss: 90.4072494506836 = 1.0923810005187988 + 10.0 * 8.931486129760742
Epoch 670, val loss: 1.092970609664917
Epoch 680, training loss: 90.4236068725586 = 1.092188835144043 + 10.0 * 8.933141708374023
Epoch 680, val loss: 1.092795968055725
Epoch 690, training loss: 90.4517822265625 = 1.0919969081878662 + 10.0 * 8.935978889465332
Epoch 690, val loss: 1.0926207304000854
Epoch 700, training loss: 90.41085815429688 = 1.0917959213256836 + 10.0 * 8.931905746459961
Epoch 700, val loss: 1.0924416780471802
Epoch 710, training loss: 90.50410461425781 = 1.0915861129760742 + 10.0 * 8.941251754760742
Epoch 710, val loss: 1.09225332736969
Epoch 720, training loss: 90.44537353515625 = 1.0913687944412231 + 10.0 * 8.935400009155273
Epoch 720, val loss: 1.092058539390564
Epoch 730, training loss: 90.53076934814453 = 1.0911887884140015 + 10.0 * 8.943958282470703
Epoch 730, val loss: 1.091886281967163
Epoch 740, training loss: 90.5524673461914 = 1.0909799337387085 + 10.0 * 8.946148872375488
Epoch 740, val loss: 1.091697335243225
Epoch 750, training loss: 90.5512924194336 = 1.0907623767852783 + 10.0 * 8.946053504943848
Epoch 750, val loss: 1.0915032625198364
Epoch 760, training loss: 90.56258392333984 = 1.090546727180481 + 10.0 * 8.947203636169434
Epoch 760, val loss: 1.091309666633606
Epoch 770, training loss: 90.58560180664062 = 1.0903282165527344 + 10.0 * 8.949527740478516
Epoch 770, val loss: 1.0911117792129517
Epoch 780, training loss: 90.62554168701172 = 1.090110421180725 + 10.0 * 8.953542709350586
Epoch 780, val loss: 1.0909154415130615
Epoch 790, training loss: 90.67416381835938 = 1.089883804321289 + 10.0 * 8.958428382873535
Epoch 790, val loss: 1.0907100439071655
Epoch 800, training loss: 90.62825775146484 = 1.0896490812301636 + 10.0 * 8.953861236572266
Epoch 800, val loss: 1.0904983282089233
Epoch 810, training loss: 90.60057830810547 = 1.089408278465271 + 10.0 * 8.951116561889648
Epoch 810, val loss: 1.0902822017669678
Epoch 820, training loss: 90.66898345947266 = 1.0891788005828857 + 10.0 * 8.957980155944824
Epoch 820, val loss: 1.0900744199752808
Epoch 830, training loss: 90.76043701171875 = 1.088950276374817 + 10.0 * 8.967148780822754
Epoch 830, val loss: 1.0898696184158325
Epoch 840, training loss: 90.75340270996094 = 1.0887064933776855 + 10.0 * 8.966469764709473
Epoch 840, val loss: 1.0896484851837158
Epoch 850, training loss: 90.73502349853516 = 1.0884560346603394 + 10.0 * 8.964656829833984
Epoch 850, val loss: 1.089426040649414
Epoch 860, training loss: 90.81423950195312 = 1.0882121324539185 + 10.0 * 8.972602844238281
Epoch 860, val loss: 1.0892096757888794
Epoch 870, training loss: 90.8557357788086 = 1.087971806526184 + 10.0 * 8.976776123046875
Epoch 870, val loss: 1.0889842510223389
Epoch 880, training loss: 90.79851531982422 = 1.087666392326355 + 10.0 * 8.971084594726562
Epoch 880, val loss: 1.0887163877487183
Epoch 890, training loss: 90.82306671142578 = 1.0874402523040771 + 10.0 * 8.973562240600586
Epoch 890, val loss: 1.0885109901428223
Epoch 900, training loss: 90.79212951660156 = 1.0871477127075195 + 10.0 * 8.970498085021973
Epoch 900, val loss: 1.0882478952407837
Epoch 910, training loss: 90.86042022705078 = 1.0868971347808838 + 10.0 * 8.977352142333984
Epoch 910, val loss: 1.08802330493927
Epoch 920, training loss: 90.88850402832031 = 1.0866392850875854 + 10.0 * 8.980186462402344
Epoch 920, val loss: 1.0877885818481445
Epoch 930, training loss: 90.90699005126953 = 1.0863683223724365 + 10.0 * 8.982062339782715
Epoch 930, val loss: 1.087547779083252
Epoch 940, training loss: 90.95777130126953 = 1.0860947370529175 + 10.0 * 8.987167358398438
Epoch 940, val loss: 1.0873048305511475
Epoch 950, training loss: 90.97003936767578 = 1.085819125175476 + 10.0 * 8.988422393798828
Epoch 950, val loss: 1.0870574712753296
Epoch 960, training loss: 90.94576263427734 = 1.085524082183838 + 10.0 * 8.986023902893066
Epoch 960, val loss: 1.086785912513733
Epoch 970, training loss: 90.99024200439453 = 1.0852335691452026 + 10.0 * 8.990500450134277
Epoch 970, val loss: 1.0865318775177002
Epoch 980, training loss: 91.02517700195312 = 1.0849488973617554 + 10.0 * 8.994023323059082
Epoch 980, val loss: 1.0862748622894287
Epoch 990, training loss: 91.0850830078125 = 1.0846706628799438 + 10.0 * 9.000041007995605
Epoch 990, val loss: 1.086022973060608
Epoch 1000, training loss: 91.07453918457031 = 1.0843530893325806 + 10.0 * 8.999018669128418
Epoch 1000, val loss: 1.0857399702072144
Epoch 1010, training loss: 91.08253479003906 = 1.0840507745742798 + 10.0 * 8.999848365783691
Epoch 1010, val loss: 1.0854697227478027
Epoch 1020, training loss: 91.15052032470703 = 1.0837513208389282 + 10.0 * 9.006677627563477
Epoch 1020, val loss: 1.0851997137069702
Epoch 1030, training loss: 91.14436340332031 = 1.0834380388259888 + 10.0 * 9.00609302520752
Epoch 1030, val loss: 1.0849183797836304
Epoch 1040, training loss: 91.1439208984375 = 1.0831220149993896 + 10.0 * 9.00607967376709
Epoch 1040, val loss: 1.0846322774887085
Epoch 1050, training loss: 91.16725158691406 = 1.0827937126159668 + 10.0 * 9.008445739746094
Epoch 1050, val loss: 1.0843344926834106
Epoch 1060, training loss: 91.06165313720703 = 1.0824639797210693 + 10.0 * 8.997919082641602
Epoch 1060, val loss: 1.0840506553649902
Epoch 1070, training loss: 91.14823150634766 = 1.0821585655212402 + 10.0 * 9.006607055664062
Epoch 1070, val loss: 1.0837739706039429
Epoch 1080, training loss: 91.1948013305664 = 1.0818544626235962 + 10.0 * 9.0112943649292
Epoch 1080, val loss: 1.0834980010986328
Epoch 1090, training loss: 91.25584411621094 = 1.081528902053833 + 10.0 * 9.017431259155273
Epoch 1090, val loss: 1.083211064338684
Epoch 1100, training loss: 91.25424194335938 = 1.0811960697174072 + 10.0 * 9.017304420471191
Epoch 1100, val loss: 1.082914113998413
Epoch 1110, training loss: 91.1706314086914 = 1.0808444023132324 + 10.0 * 9.008978843688965
Epoch 1110, val loss: 1.082595705986023
Epoch 1120, training loss: 91.23954772949219 = 1.0805033445358276 + 10.0 * 9.015904426574707
Epoch 1120, val loss: 1.0822981595993042
Epoch 1130, training loss: 91.27481842041016 = 1.0801600217819214 + 10.0 * 9.019465446472168
Epoch 1130, val loss: 1.0819950103759766
Epoch 1140, training loss: 91.35276794433594 = 1.0798232555389404 + 10.0 * 9.027294158935547
Epoch 1140, val loss: 1.0816956758499146
Epoch 1150, training loss: 91.33926391601562 = 1.0794719457626343 + 10.0 * 9.025979042053223
Epoch 1150, val loss: 1.0813772678375244
Epoch 1160, training loss: 91.3842544555664 = 1.0791118144989014 + 10.0 * 9.03051471710205
Epoch 1160, val loss: 1.0810636281967163
Epoch 1170, training loss: 91.39311981201172 = 1.0787488222122192 + 10.0 * 9.031436920166016
Epoch 1170, val loss: 1.0807397365570068
Epoch 1180, training loss: 91.41056060791016 = 1.078374981880188 + 10.0 * 9.033218383789062
Epoch 1180, val loss: 1.0804105997085571
Epoch 1190, training loss: 91.42981719970703 = 1.0779956579208374 + 10.0 * 9.035181999206543
Epoch 1190, val loss: 1.0800749063491821
Epoch 1200, training loss: 91.41368103027344 = 1.0776114463806152 + 10.0 * 9.03360652923584
Epoch 1200, val loss: 1.0797297954559326
Epoch 1210, training loss: 91.25651550292969 = 1.077181100845337 + 10.0 * 9.01793384552002
Epoch 1210, val loss: 1.079339861869812
Epoch 1220, training loss: 91.32723999023438 = 1.0767738819122314 + 10.0 * 9.025046348571777
Epoch 1220, val loss: 1.0789889097213745
Epoch 1230, training loss: 91.34967041015625 = 1.0763511657714844 + 10.0 * 9.027332305908203
Epoch 1230, val loss: 1.078616738319397
Epoch 1240, training loss: 91.31682586669922 = 1.0758533477783203 + 10.0 * 9.024097442626953
Epoch 1240, val loss: 1.078187108039856
Epoch 1250, training loss: 91.39730834960938 = 1.0754246711730957 + 10.0 * 9.032188415527344
Epoch 1250, val loss: 1.077789068222046
Epoch 1260, training loss: 91.42394256591797 = 1.0748902559280396 + 10.0 * 9.034905433654785
Epoch 1260, val loss: 1.0773251056671143
Epoch 1270, training loss: 91.15869140625 = 1.0743738412857056 + 10.0 * 9.008432388305664
Epoch 1270, val loss: 1.0768649578094482
Epoch 1280, training loss: 91.35762023925781 = 1.0738943815231323 + 10.0 * 9.028372764587402
Epoch 1280, val loss: 1.0764381885528564
Epoch 1290, training loss: 91.4183349609375 = 1.0733815431594849 + 10.0 * 9.03449535369873
Epoch 1290, val loss: 1.0759918689727783
Epoch 1300, training loss: 91.48161315917969 = 1.0728764533996582 + 10.0 * 9.040873527526855
Epoch 1300, val loss: 1.0755425691604614
Epoch 1310, training loss: 91.53299713134766 = 1.0723659992218018 + 10.0 * 9.046063423156738
Epoch 1310, val loss: 1.0750917196273804
Epoch 1320, training loss: 91.563232421875 = 1.0718507766723633 + 10.0 * 9.049138069152832
Epoch 1320, val loss: 1.0746352672576904
Epoch 1330, training loss: 91.57501983642578 = 1.071331262588501 + 10.0 * 9.050368309020996
Epoch 1330, val loss: 1.074174404144287
Epoch 1340, training loss: 91.62652587890625 = 1.070798635482788 + 10.0 * 9.055572509765625
Epoch 1340, val loss: 1.073708176612854
Epoch 1350, training loss: 91.65057373046875 = 1.0702661275863647 + 10.0 * 9.05803108215332
Epoch 1350, val loss: 1.0732430219650269
Epoch 1360, training loss: 91.59550476074219 = 1.0697325468063354 + 10.0 * 9.052577018737793
Epoch 1360, val loss: 1.072769045829773
Epoch 1370, training loss: 91.62649536132812 = 1.0691962242126465 + 10.0 * 9.055729866027832
Epoch 1370, val loss: 1.0723044872283936
Epoch 1380, training loss: 91.71517944335938 = 1.0686625242233276 + 10.0 * 9.064651489257812
Epoch 1380, val loss: 1.0718319416046143
Epoch 1390, training loss: 91.75275421142578 = 1.0681074857711792 + 10.0 * 9.068464279174805
Epoch 1390, val loss: 1.0713469982147217
Epoch 1400, training loss: 91.74169921875 = 1.0675512552261353 + 10.0 * 9.067415237426758
Epoch 1400, val loss: 1.070863962173462
Epoch 1410, training loss: 91.78588104248047 = 1.0669934749603271 + 10.0 * 9.07188892364502
Epoch 1410, val loss: 1.0703773498535156
Epoch 1420, training loss: 91.8287582397461 = 1.0664349794387817 + 10.0 * 9.076231956481934
Epoch 1420, val loss: 1.069887399673462
Epoch 1430, training loss: 91.8048095703125 = 1.065866470336914 + 10.0 * 9.073894500732422
Epoch 1430, val loss: 1.0693928003311157
Epoch 1440, training loss: 91.83975219726562 = 1.0652906894683838 + 10.0 * 9.077445983886719
Epoch 1440, val loss: 1.0688974857330322
Epoch 1450, training loss: 91.85354614257812 = 1.064713478088379 + 10.0 * 9.078883171081543
Epoch 1450, val loss: 1.0683906078338623
Epoch 1460, training loss: 91.87533569335938 = 1.0641345977783203 + 10.0 * 9.081120491027832
Epoch 1460, val loss: 1.06789231300354
Epoch 1470, training loss: 91.91751098632812 = 1.0635608434677124 + 10.0 * 9.085394859313965
Epoch 1470, val loss: 1.067400574684143
Epoch 1480, training loss: 91.95040893554688 = 1.062976360321045 + 10.0 * 9.088743209838867
Epoch 1480, val loss: 1.066894769668579
Epoch 1490, training loss: 91.99705505371094 = 1.0623893737792969 + 10.0 * 9.093466758728027
Epoch 1490, val loss: 1.0663822889328003
Epoch 1500, training loss: 92.01630401611328 = 1.0618011951446533 + 10.0 * 9.095450401306152
Epoch 1500, val loss: 1.0658704042434692
Epoch 1510, training loss: 92.0121841430664 = 1.0611966848373413 + 10.0 * 9.095098495483398
Epoch 1510, val loss: 1.0653560161590576
Epoch 1520, training loss: 92.05623626708984 = 1.0605990886688232 + 10.0 * 9.099563598632812
Epoch 1520, val loss: 1.064841866493225
Epoch 1530, training loss: 92.06818389892578 = 1.0599980354309082 + 10.0 * 9.100818634033203
Epoch 1530, val loss: 1.0643268823623657
Epoch 1540, training loss: 91.80590057373047 = 1.0593599081039429 + 10.0 * 9.074654579162598
Epoch 1540, val loss: 1.0637880563735962
Epoch 1550, training loss: 91.88813781738281 = 1.0587748289108276 + 10.0 * 9.08293628692627
Epoch 1550, val loss: 1.0632904767990112
Epoch 1560, training loss: 92.03773498535156 = 1.058193564414978 + 10.0 * 9.097953796386719
Epoch 1560, val loss: 1.0627832412719727
Epoch 1570, training loss: 92.09381103515625 = 1.0575895309448242 + 10.0 * 9.103622436523438
Epoch 1570, val loss: 1.0622750520706177
Epoch 1580, training loss: 92.1407699584961 = 1.0569769144058228 + 10.0 * 9.108379364013672
Epoch 1580, val loss: 1.0617574453353882
Epoch 1590, training loss: 92.13317108154297 = 1.0563600063323975 + 10.0 * 9.107681274414062
Epoch 1590, val loss: 1.061236023902893
Epoch 1600, training loss: 92.14202117919922 = 1.0557570457458496 + 10.0 * 9.108626365661621
Epoch 1600, val loss: 1.0607270002365112
Epoch 1610, training loss: 92.2201156616211 = 1.0551481246948242 + 10.0 * 9.116497039794922
Epoch 1610, val loss: 1.0602214336395264
Epoch 1620, training loss: 92.18775939941406 = 1.0545337200164795 + 10.0 * 9.113322257995605
Epoch 1620, val loss: 1.0597021579742432
Epoch 1630, training loss: 92.2221450805664 = 1.0539230108261108 + 10.0 * 9.116822242736816
Epoch 1630, val loss: 1.0591996908187866
Epoch 1640, training loss: 92.27953338623047 = 1.0533223152160645 + 10.0 * 9.122621536254883
Epoch 1640, val loss: 1.0586944818496704
Epoch 1650, training loss: 92.26009368896484 = 1.052712082862854 + 10.0 * 9.12073802947998
Epoch 1650, val loss: 1.0581883192062378
Epoch 1660, training loss: 92.34184265136719 = 1.0521032810211182 + 10.0 * 9.128973960876465
Epoch 1660, val loss: 1.0576850175857544
Epoch 1670, training loss: 92.36604309082031 = 1.0514960289001465 + 10.0 * 9.131454467773438
Epoch 1670, val loss: 1.05717933177948
Epoch 1680, training loss: 92.32061767578125 = 1.0508766174316406 + 10.0 * 9.126974105834961
Epoch 1680, val loss: 1.056665301322937
Epoch 1690, training loss: 92.363525390625 = 1.0502718687057495 + 10.0 * 9.131325721740723
Epoch 1690, val loss: 1.056168794631958
Epoch 1700, training loss: 92.43241119384766 = 1.0496630668640137 + 10.0 * 9.138275146484375
Epoch 1700, val loss: 1.055668592453003
Epoch 1710, training loss: 92.40083312988281 = 1.0490471124649048 + 10.0 * 9.135178565979004
Epoch 1710, val loss: 1.0551652908325195
Epoch 1720, training loss: 92.38539123535156 = 1.0484527349472046 + 10.0 * 9.13369369506836
Epoch 1720, val loss: 1.0546714067459106
Epoch 1730, training loss: 92.43073272705078 = 1.0478445291519165 + 10.0 * 9.138288497924805
Epoch 1730, val loss: 1.0541681051254272
Epoch 1740, training loss: 92.33960723876953 = 1.0473389625549316 + 10.0 * 9.129226684570312
Epoch 1740, val loss: 1.0537309646606445
Epoch 1750, training loss: 92.16595458984375 = 1.0467214584350586 + 10.0 * 9.111923217773438
Epoch 1750, val loss: 1.0532170534133911
Epoch 1760, training loss: 92.1832046508789 = 1.0461277961730957 + 10.0 * 9.113707542419434
Epoch 1760, val loss: 1.0527623891830444
Epoch 1770, training loss: 92.26980590820312 = 1.0455596446990967 + 10.0 * 9.122424125671387
Epoch 1770, val loss: 1.0523157119750977
Epoch 1780, training loss: 92.36084747314453 = 1.04500412940979 + 10.0 * 9.131584167480469
Epoch 1780, val loss: 1.0518707036972046
Epoch 1790, training loss: 92.36300659179688 = 1.0444170236587524 + 10.0 * 9.131858825683594
Epoch 1790, val loss: 1.051405668258667
Epoch 1800, training loss: 92.40123748779297 = 1.0438296794891357 + 10.0 * 9.135740280151367
Epoch 1800, val loss: 1.0509341955184937
Epoch 1810, training loss: 92.44892120361328 = 1.0432535409927368 + 10.0 * 9.1405668258667
Epoch 1810, val loss: 1.0504727363586426
Epoch 1820, training loss: 92.49565887451172 = 1.042678713798523 + 10.0 * 9.14529800415039
Epoch 1820, val loss: 1.050012469291687
Epoch 1830, training loss: 92.5063705444336 = 1.0420969724655151 + 10.0 * 9.146427154541016
Epoch 1830, val loss: 1.0495538711547852
Epoch 1840, training loss: 92.53209686279297 = 1.0415254831314087 + 10.0 * 9.149057388305664
Epoch 1840, val loss: 1.0491043329238892
Epoch 1850, training loss: 92.56439208984375 = 1.0409584045410156 + 10.0 * 9.15234375
Epoch 1850, val loss: 1.0486572980880737
Epoch 1860, training loss: 92.60957336425781 = 1.0404009819030762 + 10.0 * 9.156917572021484
Epoch 1860, val loss: 1.0482102632522583
Epoch 1870, training loss: 92.63003540039062 = 1.039842963218689 + 10.0 * 9.159019470214844
Epoch 1870, val loss: 1.0477674007415771
Epoch 1880, training loss: 92.57752990722656 = 1.0392829179763794 + 10.0 * 9.153824806213379
Epoch 1880, val loss: 1.0473206043243408
Epoch 1890, training loss: 92.64057159423828 = 1.0387362241744995 + 10.0 * 9.160183906555176
Epoch 1890, val loss: 1.0468891859054565
Epoch 1900, training loss: 92.66539764404297 = 1.0382016897201538 + 10.0 * 9.1627197265625
Epoch 1900, val loss: 1.0464650392532349
Epoch 1910, training loss: 92.65738677978516 = 1.037656545639038 + 10.0 * 9.161972999572754
Epoch 1910, val loss: 1.0460388660430908
Epoch 1920, training loss: 92.71573638916016 = 1.0371276140213013 + 10.0 * 9.167860984802246
Epoch 1920, val loss: 1.045615315437317
Epoch 1930, training loss: 92.7508316040039 = 1.0365941524505615 + 10.0 * 9.17142391204834
Epoch 1930, val loss: 1.04519522190094
Epoch 1940, training loss: 92.2724838256836 = 1.0360314846038818 + 10.0 * 9.123644828796387
Epoch 1940, val loss: 1.044754147529602
Epoch 1950, training loss: 92.40595245361328 = 1.0355546474456787 + 10.0 * 9.137040138244629
Epoch 1950, val loss: 1.0443605184555054
Epoch 1960, training loss: 92.41490173339844 = 1.0350679159164429 + 10.0 * 9.137983322143555
Epoch 1960, val loss: 1.0440013408660889
Epoch 1970, training loss: 92.40879821777344 = 1.0345499515533447 + 10.0 * 9.13742446899414
Epoch 1970, val loss: 1.0436345338821411
Epoch 1980, training loss: 92.49578094482422 = 1.0340496301651 + 10.0 * 9.146173477172852
Epoch 1980, val loss: 1.0432699918746948
Epoch 1990, training loss: 92.6326675415039 = 1.033552885055542 + 10.0 * 9.159911155700684
Epoch 1990, val loss: 1.0428956747055054
Epoch 2000, training loss: 92.71408081054688 = 1.033057689666748 + 10.0 * 9.168102264404297
Epoch 2000, val loss: 1.0425286293029785
Epoch 2010, training loss: 92.75878143310547 = 1.0325621366500854 + 10.0 * 9.172621726989746
Epoch 2010, val loss: 1.0421556234359741
Epoch 2020, training loss: 92.74526977539062 = 1.0320719480514526 + 10.0 * 9.171319961547852
Epoch 2020, val loss: 1.0417801141738892
Epoch 2030, training loss: 92.73438262939453 = 1.031593918800354 + 10.0 * 9.170278549194336
Epoch 2030, val loss: 1.0414128303527832
Epoch 2040, training loss: 92.77678680419922 = 1.0311154127120972 + 10.0 * 9.174567222595215
Epoch 2040, val loss: 1.041061282157898
Epoch 2050, training loss: 92.75104522705078 = 1.030627965927124 + 10.0 * 9.172041893005371
Epoch 2050, val loss: 1.0406935214996338
Epoch 2060, training loss: 92.81505584716797 = 1.0301560163497925 + 10.0 * 9.178489685058594
Epoch 2060, val loss: 1.04033362865448
Epoch 2070, training loss: 92.85037231445312 = 1.029685139656067 + 10.0 * 9.182068824768066
Epoch 2070, val loss: 1.0399688482284546
Epoch 2080, training loss: 92.775146484375 = 1.0292179584503174 + 10.0 * 9.174592971801758
Epoch 2080, val loss: 1.0396164655685425
Epoch 2090, training loss: 92.76811981201172 = 1.0287609100341797 + 10.0 * 9.173935890197754
Epoch 2090, val loss: 1.0392605066299438
Epoch 2100, training loss: 92.85762023925781 = 1.0283050537109375 + 10.0 * 9.182931900024414
Epoch 2100, val loss: 1.0389174222946167
Epoch 2110, training loss: 92.93154907226562 = 1.02785325050354 + 10.0 * 9.190369606018066
Epoch 2110, val loss: 1.0385762453079224
Epoch 2120, training loss: 92.92788696289062 = 1.027410626411438 + 10.0 * 9.190047264099121
Epoch 2120, val loss: 1.0382391214370728
Epoch 2130, training loss: 92.93810272216797 = 1.0269745588302612 + 10.0 * 9.191112518310547
Epoch 2130, val loss: 1.0379071235656738
Epoch 2140, training loss: 92.94979095458984 = 1.0265374183654785 + 10.0 * 9.192325592041016
Epoch 2140, val loss: 1.0375728607177734
Epoch 2150, training loss: 92.9535140991211 = 1.0261108875274658 + 10.0 * 9.192740440368652
Epoch 2150, val loss: 1.0372531414031982
Epoch 2160, training loss: 92.95919799804688 = 1.0256874561309814 + 10.0 * 9.193350791931152
Epoch 2160, val loss: 1.0369421243667603
Epoch 2170, training loss: 93.00928497314453 = 1.0252652168273926 + 10.0 * 9.19840145111084
Epoch 2170, val loss: 1.0366324186325073
Epoch 2180, training loss: 93.03606414794922 = 1.0248503684997559 + 10.0 * 9.20112133026123
Epoch 2180, val loss: 1.0363428592681885
Epoch 2190, training loss: 92.1846923828125 = 1.0243873596191406 + 10.0 * 9.1160306930542
Epoch 2190, val loss: 1.0360883474349976
Epoch 2200, training loss: 92.57886505126953 = 1.02411949634552 + 10.0 * 9.155474662780762
Epoch 2200, val loss: 1.035799503326416
Epoch 2210, training loss: 92.13082885742188 = 1.0236883163452148 + 10.0 * 9.110713958740234
Epoch 2210, val loss: 1.035515308380127
Epoch 2220, training loss: 92.46095275878906 = 1.023342490196228 + 10.0 * 9.143760681152344
Epoch 2220, val loss: 1.0352531671524048
Epoch 2230, training loss: 92.39956665039062 = 1.0229603052139282 + 10.0 * 9.13766098022461
Epoch 2230, val loss: 1.035008430480957
Epoch 2240, training loss: 92.63446044921875 = 1.022616982460022 + 10.0 * 9.161184310913086
Epoch 2240, val loss: 1.0347715616226196
Epoch 2250, training loss: 92.5316390991211 = 1.022250771522522 + 10.0 * 9.150938987731934
Epoch 2250, val loss: 1.0345600843429565
Epoch 2260, training loss: 92.58409881591797 = 1.021872639656067 + 10.0 * 9.156222343444824
Epoch 2260, val loss: 1.034256935119629
Epoch 2270, training loss: 92.61809539794922 = 1.0215095281600952 + 10.0 * 9.159658432006836
Epoch 2270, val loss: 1.033968210220337
Epoch 2280, training loss: 92.7167739868164 = 1.021139144897461 + 10.0 * 9.169563293457031
Epoch 2280, val loss: 1.0337218046188354
Epoch 2290, training loss: 92.78282928466797 = 1.0207741260528564 + 10.0 * 9.1762056350708
Epoch 2290, val loss: 1.0334593057632446
Epoch 2300, training loss: 92.78728485107422 = 1.0204074382781982 + 10.0 * 9.176687240600586
Epoch 2300, val loss: 1.0331913232803345
Epoch 2310, training loss: 92.85697174072266 = 1.0200525522232056 + 10.0 * 9.18369197845459
Epoch 2310, val loss: 1.032935380935669
Epoch 2320, training loss: 92.91114807128906 = 1.0197007656097412 + 10.0 * 9.1891450881958
Epoch 2320, val loss: 1.0326803922653198
Epoch 2330, training loss: 92.92891693115234 = 1.0193514823913574 + 10.0 * 9.190957069396973
Epoch 2330, val loss: 1.0324194431304932
Epoch 2340, training loss: 92.91814422607422 = 1.0190058946609497 + 10.0 * 9.189913749694824
Epoch 2340, val loss: 1.0321524143218994
Epoch 2350, training loss: 92.96705627441406 = 1.0186651945114136 + 10.0 * 9.194839477539062
Epoch 2350, val loss: 1.0318987369537354
Epoch 2360, training loss: 92.9699935913086 = 1.0183284282684326 + 10.0 * 9.19516658782959
Epoch 2360, val loss: 1.0316548347473145
Epoch 2370, training loss: 92.96533203125 = 1.017995834350586 + 10.0 * 9.194733619689941
Epoch 2370, val loss: 1.031415343284607
Epoch 2380, training loss: 93.01299285888672 = 1.0176697969436646 + 10.0 * 9.199532508850098
Epoch 2380, val loss: 1.0311928987503052
Epoch 2390, training loss: 92.99540710449219 = 1.017345905303955 + 10.0 * 9.197805404663086
Epoch 2390, val loss: 1.0309783220291138
Epoch 2400, training loss: 93.00739288330078 = 1.017027735710144 + 10.0 * 9.199036598205566
Epoch 2400, val loss: 1.0307539701461792
Epoch 2410, training loss: 93.03899383544922 = 1.01671302318573 + 10.0 * 9.202227592468262
Epoch 2410, val loss: 1.0305267572402954
Epoch 2420, training loss: 93.0829849243164 = 1.016402006149292 + 10.0 * 9.206658363342285
Epoch 2420, val loss: 1.0303038358688354
Epoch 2430, training loss: 93.0565414428711 = 1.0160903930664062 + 10.0 * 9.204045295715332
Epoch 2430, val loss: 1.0300815105438232
Epoch 2440, training loss: 93.07649230957031 = 1.0157873630523682 + 10.0 * 9.206070899963379
Epoch 2440, val loss: 1.0298662185668945
Epoch 2450, training loss: 93.10372924804688 = 1.0154858827590942 + 10.0 * 9.208824157714844
Epoch 2450, val loss: 1.0296556949615479
Epoch 2460, training loss: 93.10997772216797 = 1.0151817798614502 + 10.0 * 9.209479331970215
Epoch 2460, val loss: 1.0294454097747803
Epoch 2470, training loss: 93.1078109741211 = 1.0148978233337402 + 10.0 * 9.209291458129883
Epoch 2470, val loss: 1.0292418003082275
Epoch 2480, training loss: 93.11801147460938 = 1.014600396156311 + 10.0 * 9.21034049987793
Epoch 2480, val loss: 1.0290472507476807
Epoch 2490, training loss: 93.15019226074219 = 1.014303207397461 + 10.0 * 9.21358871459961
Epoch 2490, val loss: 1.0288492441177368
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.20594202898550723
0.8132290081866261
=== training gcn model ===
Epoch 0, training loss: 103.353271484375 = 1.1163737773895264 + 10.0 * 10.223690032958984
Epoch 0, val loss: 1.1163305044174194
Epoch 10, training loss: 99.60671997070312 = 1.115964412689209 + 10.0 * 9.849075317382812
Epoch 10, val loss: 1.1158959865570068
Epoch 20, training loss: 97.93769836425781 = 1.115601897239685 + 10.0 * 9.682209968566895
Epoch 20, val loss: 1.1155507564544678
Epoch 30, training loss: 96.66104888916016 = 1.1151926517486572 + 10.0 * 9.554585456848145
Epoch 30, val loss: 1.115142822265625
Epoch 40, training loss: 95.61741638183594 = 1.1148204803466797 + 10.0 * 9.4502592086792
Epoch 40, val loss: 1.1147749423980713
Epoch 50, training loss: 94.72160339355469 = 1.1144206523895264 + 10.0 * 9.360718727111816
Epoch 50, val loss: 1.1143786907196045
Epoch 60, training loss: 93.9567642211914 = 1.114009976387024 + 10.0 * 9.28427505493164
Epoch 60, val loss: 1.113972783088684
Epoch 70, training loss: 93.3015365600586 = 1.1135895252227783 + 10.0 * 9.218794822692871
Epoch 70, val loss: 1.1135557889938354
Epoch 80, training loss: 92.7266616821289 = 1.1131654977798462 + 10.0 * 9.161349296569824
Epoch 80, val loss: 1.1131325960159302
Epoch 90, training loss: 92.22484588623047 = 1.1127437353134155 + 10.0 * 9.111209869384766
Epoch 90, val loss: 1.112714409828186
Epoch 100, training loss: 91.77859497070312 = 1.112291693687439 + 10.0 * 9.066630363464355
Epoch 100, val loss: 1.1122668981552124
Epoch 110, training loss: 91.39519500732422 = 1.1118417978286743 + 10.0 * 9.028335571289062
Epoch 110, val loss: 1.1118193864822388
Epoch 120, training loss: 91.06788635253906 = 1.1113911867141724 + 10.0 * 8.995649337768555
Epoch 120, val loss: 1.1113725900650024
Epoch 130, training loss: 90.78073120117188 = 1.1109474897384644 + 10.0 * 8.966978073120117
Epoch 130, val loss: 1.1109325885772705
Epoch 140, training loss: 90.51930236816406 = 1.1104873418807983 + 10.0 * 8.940881729125977
Epoch 140, val loss: 1.110473394393921
Epoch 150, training loss: 90.32316589355469 = 1.1100012063980103 + 10.0 * 8.921316146850586
Epoch 150, val loss: 1.1099989414215088
Epoch 160, training loss: 90.1281509399414 = 1.1095517873764038 + 10.0 * 8.901860237121582
Epoch 160, val loss: 1.1095397472381592
Epoch 170, training loss: 89.96292877197266 = 1.1090842485427856 + 10.0 * 8.885384559631348
Epoch 170, val loss: 1.1090821027755737
Epoch 180, training loss: 89.8137435913086 = 1.1085655689239502 + 10.0 * 8.87051773071289
Epoch 180, val loss: 1.1085742712020874
Epoch 190, training loss: 89.72250366210938 = 1.1080927848815918 + 10.0 * 8.861440658569336
Epoch 190, val loss: 1.1080890893936157
Epoch 200, training loss: 89.57899475097656 = 1.10757315158844 + 10.0 * 8.847142219543457
Epoch 200, val loss: 1.1075806617736816
Epoch 210, training loss: 89.46414184570312 = 1.1070470809936523 + 10.0 * 8.835709571838379
Epoch 210, val loss: 1.1070631742477417
Epoch 220, training loss: 89.38292694091797 = 1.1065183877944946 + 10.0 * 8.827640533447266
Epoch 220, val loss: 1.106534719467163
Epoch 230, training loss: 89.38220977783203 = 1.1060221195220947 + 10.0 * 8.827618598937988
Epoch 230, val loss: 1.1060398817062378
Epoch 240, training loss: 89.28157806396484 = 1.1054284572601318 + 10.0 * 8.817614555358887
Epoch 240, val loss: 1.105437994003296
Epoch 250, training loss: 89.27120208740234 = 1.1048977375030518 + 10.0 * 8.816630363464355
Epoch 250, val loss: 1.1049176454544067
Epoch 260, training loss: 89.16993713378906 = 1.1043334007263184 + 10.0 * 8.806560516357422
Epoch 260, val loss: 1.1043683290481567
Epoch 270, training loss: 89.09424591064453 = 1.1037616729736328 + 10.0 * 8.79904842376709
Epoch 270, val loss: 1.1037964820861816
Epoch 280, training loss: 89.11140441894531 = 1.1032243967056274 + 10.0 * 8.800817489624023
Epoch 280, val loss: 1.1032538414001465
Epoch 290, training loss: 89.0834732055664 = 1.102661371231079 + 10.0 * 8.798081398010254
Epoch 290, val loss: 1.102698564529419
Epoch 300, training loss: 89.053955078125 = 1.1021084785461426 + 10.0 * 8.795184135437012
Epoch 300, val loss: 1.1021462678909302
Epoch 310, training loss: 89.02497863769531 = 1.1015663146972656 + 10.0 * 8.792341232299805
Epoch 310, val loss: 1.1016144752502441
Epoch 320, training loss: 89.04235076904297 = 1.101038932800293 + 10.0 * 8.7941312789917
Epoch 320, val loss: 1.1010922193527222
Epoch 330, training loss: 88.99842071533203 = 1.1004903316497803 + 10.0 * 8.789793014526367
Epoch 330, val loss: 1.1005522012710571
Epoch 340, training loss: 89.08125305175781 = 1.1000521183013916 + 10.0 * 8.798120498657227
Epoch 340, val loss: 1.1001179218292236
Epoch 350, training loss: 89.02613830566406 = 1.0996066331863403 + 10.0 * 8.79265308380127
Epoch 350, val loss: 1.099679946899414
Epoch 360, training loss: 89.04689025878906 = 1.0992076396942139 + 10.0 * 8.794768333435059
Epoch 360, val loss: 1.0992767810821533
Epoch 370, training loss: 89.00634002685547 = 1.0988117456436157 + 10.0 * 8.790753364562988
Epoch 370, val loss: 1.0988843441009521
Epoch 380, training loss: 89.03114318847656 = 1.0984680652618408 + 10.0 * 8.793268203735352
Epoch 380, val loss: 1.0985532999038696
Epoch 390, training loss: 89.01111602783203 = 1.098168134689331 + 10.0 * 8.791295051574707
Epoch 390, val loss: 1.0982567071914673
Epoch 400, training loss: 89.04080963134766 = 1.0979090929031372 + 10.0 * 8.794290542602539
Epoch 400, val loss: 1.0980031490325928
Epoch 410, training loss: 89.03135681152344 = 1.0976909399032593 + 10.0 * 8.793366432189941
Epoch 410, val loss: 1.0977972745895386
Epoch 420, training loss: 89.0386734008789 = 1.0975042581558228 + 10.0 * 8.794116973876953
Epoch 420, val loss: 1.097621202468872
Epoch 430, training loss: 89.03511047363281 = 1.0973520278930664 + 10.0 * 8.79377555847168
Epoch 430, val loss: 1.0974751710891724
Epoch 440, training loss: 89.0244369506836 = 1.0972203016281128 + 10.0 * 8.79272174835205
Epoch 440, val loss: 1.0973509550094604
Epoch 450, training loss: 89.0937728881836 = 1.0971111059188843 + 10.0 * 8.799665451049805
Epoch 450, val loss: 1.0972473621368408
Epoch 460, training loss: 89.03372955322266 = 1.097004771232605 + 10.0 * 8.793672561645508
Epoch 460, val loss: 1.097147822380066
Epoch 470, training loss: 89.03488159179688 = 1.0969154834747314 + 10.0 * 8.79379653930664
Epoch 470, val loss: 1.0970598459243774
Epoch 480, training loss: 89.05216217041016 = 1.0968314409255981 + 10.0 * 8.795533180236816
Epoch 480, val loss: 1.0969789028167725
Epoch 490, training loss: 89.04020690917969 = 1.0967570543289185 + 10.0 * 8.794344902038574
Epoch 490, val loss: 1.096911907196045
Epoch 500, training loss: 89.07173919677734 = 1.0966860055923462 + 10.0 * 8.797505378723145
Epoch 500, val loss: 1.0968413352966309
Epoch 510, training loss: 89.05956268310547 = 1.0966150760650635 + 10.0 * 8.796294212341309
Epoch 510, val loss: 1.09677255153656
Epoch 520, training loss: 89.05435943603516 = 1.096549391746521 + 10.0 * 8.795781135559082
Epoch 520, val loss: 1.0967072248458862
Epoch 530, training loss: 89.10343170166016 = 1.0964901447296143 + 10.0 * 8.800694465637207
Epoch 530, val loss: 1.0966534614562988
Epoch 540, training loss: 89.0365219116211 = 1.096419095993042 + 10.0 * 8.794010162353516
Epoch 540, val loss: 1.0965849161148071
Epoch 550, training loss: 89.12461853027344 = 1.0963581800460815 + 10.0 * 8.802825927734375
Epoch 550, val loss: 1.0965274572372437
Epoch 560, training loss: 89.09321594238281 = 1.0963022708892822 + 10.0 * 8.799691200256348
Epoch 560, val loss: 1.0964725017547607
Epoch 570, training loss: 89.10398864746094 = 1.0962541103363037 + 10.0 * 8.800773620605469
Epoch 570, val loss: 1.0964264869689941
Epoch 580, training loss: 89.09579467773438 = 1.0961949825286865 + 10.0 * 8.799960136413574
Epoch 580, val loss: 1.0963727235794067
Epoch 590, training loss: 89.10924530029297 = 1.0961391925811768 + 10.0 * 8.801310539245605
Epoch 590, val loss: 1.0963212251663208
Epoch 600, training loss: 89.11731719970703 = 1.0960843563079834 + 10.0 * 8.802123069763184
Epoch 600, val loss: 1.0962653160095215
Epoch 610, training loss: 89.13418579101562 = 1.0960261821746826 + 10.0 * 8.803815841674805
Epoch 610, val loss: 1.0962138175964355
Epoch 620, training loss: 89.1451416015625 = 1.0959666967391968 + 10.0 * 8.804917335510254
Epoch 620, val loss: 1.0961564779281616
Epoch 630, training loss: 89.1473388671875 = 1.0959044694900513 + 10.0 * 8.805143356323242
Epoch 630, val loss: 1.096098780632019
Epoch 640, training loss: 89.16666412353516 = 1.0958471298217773 + 10.0 * 8.80708122253418
Epoch 640, val loss: 1.0960460901260376
Epoch 650, training loss: 89.19884490966797 = 1.095787525177002 + 10.0 * 8.81030559539795
Epoch 650, val loss: 1.0959893465042114
Epoch 660, training loss: 89.2269058227539 = 1.095727562904358 + 10.0 * 8.813117980957031
Epoch 660, val loss: 1.0959312915802002
Epoch 670, training loss: 89.22612762451172 = 1.0956621170043945 + 10.0 * 8.8130464553833
Epoch 670, val loss: 1.0958689451217651
Epoch 680, training loss: 89.23589324951172 = 1.095594048500061 + 10.0 * 8.814029693603516
Epoch 680, val loss: 1.09580659866333
Epoch 690, training loss: 89.21965789794922 = 1.0955302715301514 + 10.0 * 8.812413215637207
Epoch 690, val loss: 1.095745325088501
Epoch 700, training loss: 89.24485778808594 = 1.0954623222351074 + 10.0 * 8.814939498901367
Epoch 700, val loss: 1.0956826210021973
Epoch 710, training loss: 89.2777099609375 = 1.0954011678695679 + 10.0 * 8.818231582641602
Epoch 710, val loss: 1.0956262350082397
Epoch 720, training loss: 89.31535339355469 = 1.0953372716903687 + 10.0 * 8.822001457214355
Epoch 720, val loss: 1.095564603805542
Epoch 730, training loss: 89.32765197753906 = 1.0952647924423218 + 10.0 * 8.823238372802734
Epoch 730, val loss: 1.095495343208313
Epoch 740, training loss: 89.32096099853516 = 1.0951875448226929 + 10.0 * 8.822577476501465
Epoch 740, val loss: 1.0954241752624512
Epoch 750, training loss: 89.38248443603516 = 1.0951133966445923 + 10.0 * 8.828737258911133
Epoch 750, val loss: 1.0953580141067505
Epoch 760, training loss: 89.36925506591797 = 1.095034122467041 + 10.0 * 8.827422142028809
Epoch 760, val loss: 1.0952800512313843
Epoch 770, training loss: 89.402099609375 = 1.094956398010254 + 10.0 * 8.830714225769043
Epoch 770, val loss: 1.0952112674713135
Epoch 780, training loss: 89.43144989013672 = 1.0948691368103027 + 10.0 * 8.833658218383789
Epoch 780, val loss: 1.0951313972473145
Epoch 790, training loss: 89.43730163574219 = 1.094786524772644 + 10.0 * 8.834251403808594
Epoch 790, val loss: 1.0950478315353394
Epoch 800, training loss: 89.40260314941406 = 1.0946953296661377 + 10.0 * 8.830790519714355
Epoch 800, val loss: 1.0949653387069702
Epoch 810, training loss: 89.45423889160156 = 1.0946071147918701 + 10.0 * 8.835963249206543
Epoch 810, val loss: 1.0948750972747803
Epoch 820, training loss: 89.4063720703125 = 1.0945278406143188 + 10.0 * 8.831184387207031
Epoch 820, val loss: 1.0948125123977661
Epoch 830, training loss: 89.40776062011719 = 1.0944195985794067 + 10.0 * 8.831334114074707
Epoch 830, val loss: 1.094710350036621
Epoch 840, training loss: 89.42670440673828 = 1.0943326950073242 + 10.0 * 8.833237648010254
Epoch 840, val loss: 1.094626784324646
Epoch 850, training loss: 89.41761016845703 = 1.0942389965057373 + 10.0 * 8.832337379455566
Epoch 850, val loss: 1.094539999961853
Epoch 860, training loss: 89.46551513671875 = 1.0941542387008667 + 10.0 * 8.837136268615723
Epoch 860, val loss: 1.0944609642028809
Epoch 870, training loss: 89.52313232421875 = 1.094061017036438 + 10.0 * 8.842906951904297
Epoch 870, val loss: 1.094374656677246
Epoch 880, training loss: 89.51402282714844 = 1.0939574241638184 + 10.0 * 8.84200668334961
Epoch 880, val loss: 1.094273328781128
Epoch 890, training loss: 89.50791931152344 = 1.093858003616333 + 10.0 * 8.841405868530273
Epoch 890, val loss: 1.0941803455352783
Epoch 900, training loss: 89.52983093261719 = 1.0937532186508179 + 10.0 * 8.843607902526855
Epoch 900, val loss: 1.094085693359375
Epoch 910, training loss: 89.59329986572266 = 1.0936540365219116 + 10.0 * 8.84996509552002
Epoch 910, val loss: 1.0939931869506836
Epoch 920, training loss: 89.6237564086914 = 1.093553900718689 + 10.0 * 8.853020668029785
Epoch 920, val loss: 1.0938996076583862
Epoch 930, training loss: 89.6290512084961 = 1.0934534072875977 + 10.0 * 8.853559494018555
Epoch 930, val loss: 1.093802809715271
Epoch 940, training loss: 89.64295959472656 = 1.0933542251586914 + 10.0 * 8.854960441589355
Epoch 940, val loss: 1.093711495399475
Epoch 950, training loss: 89.68090057373047 = 1.093248963356018 + 10.0 * 8.858765602111816
Epoch 950, val loss: 1.0936119556427002
Epoch 960, training loss: 89.71359252929688 = 1.093146562576294 + 10.0 * 8.862044334411621
Epoch 960, val loss: 1.0935171842575073
Epoch 970, training loss: 89.71814727783203 = 1.0930429697036743 + 10.0 * 8.862510681152344
Epoch 970, val loss: 1.0934231281280518
Epoch 980, training loss: 89.73585510253906 = 1.092941164970398 + 10.0 * 8.864291191101074
Epoch 980, val loss: 1.0933284759521484
Epoch 990, training loss: 89.76644897460938 = 1.0928378105163574 + 10.0 * 8.867361068725586
Epoch 990, val loss: 1.0932295322418213
Epoch 1000, training loss: 89.75823211669922 = 1.0927265882492065 + 10.0 * 8.86655044555664
Epoch 1000, val loss: 1.0931215286254883
Epoch 1010, training loss: 89.75263977050781 = 1.0926204919815063 + 10.0 * 8.866002082824707
Epoch 1010, val loss: 1.0930248498916626
Epoch 1020, training loss: 89.81734466552734 = 1.0925259590148926 + 10.0 * 8.872481346130371
Epoch 1020, val loss: 1.0929406881332397
Epoch 1030, training loss: 89.85860443115234 = 1.0924261808395386 + 10.0 * 8.876618385314941
Epoch 1030, val loss: 1.0928514003753662
Epoch 1040, training loss: 89.84729766845703 = 1.0923258066177368 + 10.0 * 8.875497817993164
Epoch 1040, val loss: 1.0927594900131226
Epoch 1050, training loss: 89.85452270507812 = 1.0922263860702515 + 10.0 * 8.876230239868164
Epoch 1050, val loss: 1.092665195465088
Epoch 1060, training loss: 89.9200210571289 = 1.0921274423599243 + 10.0 * 8.882789611816406
Epoch 1060, val loss: 1.092573881149292
Epoch 1070, training loss: 89.93759155273438 = 1.0920237302780151 + 10.0 * 8.884556770324707
Epoch 1070, val loss: 1.0924791097640991
Epoch 1080, training loss: 89.95101928710938 = 1.0919212102890015 + 10.0 * 8.885910034179688
Epoch 1080, val loss: 1.0923879146575928
Epoch 1090, training loss: 89.97814178466797 = 1.09181809425354 + 10.0 * 8.888631820678711
Epoch 1090, val loss: 1.0922895669937134
Epoch 1100, training loss: 90.01931762695312 = 1.0917134284973145 + 10.0 * 8.892760276794434
Epoch 1100, val loss: 1.0921941995620728
Epoch 1110, training loss: 90.01155853271484 = 1.091593861579895 + 10.0 * 8.891996383666992
Epoch 1110, val loss: 1.0920871496200562
Epoch 1120, training loss: 90.01551818847656 = 1.091493844985962 + 10.0 * 8.892402648925781
Epoch 1120, val loss: 1.091986060142517
Epoch 1130, training loss: 90.03594207763672 = 1.0913751125335693 + 10.0 * 8.89445686340332
Epoch 1130, val loss: 1.091883659362793
Epoch 1140, training loss: 90.05780792236328 = 1.0912797451019287 + 10.0 * 8.896653175354004
Epoch 1140, val loss: 1.0917918682098389
Epoch 1150, training loss: 90.09780883789062 = 1.0911693572998047 + 10.0 * 8.900663375854492
Epoch 1150, val loss: 1.091693639755249
Epoch 1160, training loss: 90.10014343261719 = 1.091058611869812 + 10.0 * 8.900908470153809
Epoch 1160, val loss: 1.0915920734405518
Epoch 1170, training loss: 90.07039642333984 = 1.0909408330917358 + 10.0 * 8.897945404052734
Epoch 1170, val loss: 1.0914791822433472
Epoch 1180, training loss: 90.08238983154297 = 1.0908321142196655 + 10.0 * 8.899155616760254
Epoch 1180, val loss: 1.0913777351379395
Epoch 1190, training loss: 90.13907623291016 = 1.090722680091858 + 10.0 * 8.90483570098877
Epoch 1190, val loss: 1.0912829637527466
Epoch 1200, training loss: 90.19096374511719 = 1.0906139612197876 + 10.0 * 8.910035133361816
Epoch 1200, val loss: 1.091184139251709
Epoch 1210, training loss: 90.16089630126953 = 1.0904982089996338 + 10.0 * 8.907039642333984
Epoch 1210, val loss: 1.0910725593566895
Epoch 1220, training loss: 90.17045593261719 = 1.0903788805007935 + 10.0 * 8.908007621765137
Epoch 1220, val loss: 1.0909713506698608
Epoch 1230, training loss: 90.18978881835938 = 1.090267539024353 + 10.0 * 8.909952163696289
Epoch 1230, val loss: 1.090868353843689
Epoch 1240, training loss: 90.19998168945312 = 1.090151309967041 + 10.0 * 8.910983085632324
Epoch 1240, val loss: 1.0907621383666992
Epoch 1250, training loss: 90.2435302734375 = 1.0900394916534424 + 10.0 * 8.915349006652832
Epoch 1250, val loss: 1.0906580686569214
Epoch 1260, training loss: 90.24906921386719 = 1.089919924736023 + 10.0 * 8.915914535522461
Epoch 1260, val loss: 1.0905479192733765
Epoch 1270, training loss: 90.2540054321289 = 1.0898009538650513 + 10.0 * 8.916420936584473
Epoch 1270, val loss: 1.0904409885406494
Epoch 1280, training loss: 90.29718780517578 = 1.0896812677383423 + 10.0 * 8.920750617980957
Epoch 1280, val loss: 1.0903294086456299
Epoch 1290, training loss: 90.26688385009766 = 1.0895551443099976 + 10.0 * 8.917733192443848
Epoch 1290, val loss: 1.0902169942855835
Epoch 1300, training loss: 90.28937530517578 = 1.0894283056259155 + 10.0 * 8.919994354248047
Epoch 1300, val loss: 1.0901029109954834
Epoch 1310, training loss: 90.25409698486328 = 1.0893043279647827 + 10.0 * 8.916479110717773
Epoch 1310, val loss: 1.0899876356124878
Epoch 1320, training loss: 90.28588104248047 = 1.0891814231872559 + 10.0 * 8.919670104980469
Epoch 1320, val loss: 1.0898789167404175
Epoch 1330, training loss: 90.34857177734375 = 1.0890676975250244 + 10.0 * 8.92595100402832
Epoch 1330, val loss: 1.0897746086120605
Epoch 1340, training loss: 90.3817367553711 = 1.0889480113983154 + 10.0 * 8.929278373718262
Epoch 1340, val loss: 1.0896649360656738
Epoch 1350, training loss: 90.32308959960938 = 1.0888164043426514 + 10.0 * 8.92342758178711
Epoch 1350, val loss: 1.0895448923110962
Epoch 1360, training loss: 90.35946655273438 = 1.0886882543563843 + 10.0 * 8.927077293395996
Epoch 1360, val loss: 1.0894275903701782
Epoch 1370, training loss: 90.41527557373047 = 1.0885672569274902 + 10.0 * 8.932670593261719
Epoch 1370, val loss: 1.089317798614502
Epoch 1380, training loss: 90.38680267333984 = 1.0884355306625366 + 10.0 * 8.929837226867676
Epoch 1380, val loss: 1.0891995429992676
Epoch 1390, training loss: 90.40592956542969 = 1.088301181793213 + 10.0 * 8.9317626953125
Epoch 1390, val loss: 1.089078664779663
Epoch 1400, training loss: 90.4360122680664 = 1.0881742238998413 + 10.0 * 8.934783935546875
Epoch 1400, val loss: 1.0889631509780884
Epoch 1410, training loss: 90.50231170654297 = 1.0880520343780518 + 10.0 * 8.941426277160645
Epoch 1410, val loss: 1.0888524055480957
Epoch 1420, training loss: 90.5324478149414 = 1.0879255533218384 + 10.0 * 8.944452285766602
Epoch 1420, val loss: 1.0887223482131958
Epoch 1430, training loss: 90.4879379272461 = 1.0877749919891357 + 10.0 * 8.94001579284668
Epoch 1430, val loss: 1.0886013507843018
Epoch 1440, training loss: 90.48583984375 = 1.0876376628875732 + 10.0 * 8.939820289611816
Epoch 1440, val loss: 1.0884705781936646
Epoch 1450, training loss: 90.47776794433594 = 1.0875043869018555 + 10.0 * 8.939026832580566
Epoch 1450, val loss: 1.08834969997406
Epoch 1460, training loss: 90.50537109375 = 1.0873695611953735 + 10.0 * 8.941800117492676
Epoch 1460, val loss: 1.0882277488708496
Epoch 1470, training loss: 90.5634765625 = 1.087240219116211 + 10.0 * 8.947623252868652
Epoch 1470, val loss: 1.0881093740463257
Epoch 1480, training loss: 90.59501647949219 = 1.0870987176895142 + 10.0 * 8.95079231262207
Epoch 1480, val loss: 1.0879818201065063
Epoch 1490, training loss: 90.58067321777344 = 1.0869604349136353 + 10.0 * 8.949371337890625
Epoch 1490, val loss: 1.0878547430038452
Epoch 1500, training loss: 90.59358978271484 = 1.0868158340454102 + 10.0 * 8.950677871704102
Epoch 1500, val loss: 1.0877279043197632
Epoch 1510, training loss: 90.59138488769531 = 1.0866739749908447 + 10.0 * 8.950470924377441
Epoch 1510, val loss: 1.0875974893569946
Epoch 1520, training loss: 90.5853271484375 = 1.086534857749939 + 10.0 * 8.94987964630127
Epoch 1520, val loss: 1.0874675512313843
Epoch 1530, training loss: 90.64444732666016 = 1.086390495300293 + 10.0 * 8.955805778503418
Epoch 1530, val loss: 1.0873360633850098
Epoch 1540, training loss: 90.70304870605469 = 1.0862576961517334 + 10.0 * 8.961679458618164
Epoch 1540, val loss: 1.0872188806533813
Epoch 1550, training loss: 90.7178726196289 = 1.0861157178878784 + 10.0 * 8.963175773620605
Epoch 1550, val loss: 1.0870858430862427
Epoch 1560, training loss: 90.69476318359375 = 1.0859650373458862 + 10.0 * 8.9608793258667
Epoch 1560, val loss: 1.0869499444961548
Epoch 1570, training loss: 90.77176666259766 = 1.0858216285705566 + 10.0 * 8.968594551086426
Epoch 1570, val loss: 1.0868192911148071
Epoch 1580, training loss: 90.7696533203125 = 1.0856776237487793 + 10.0 * 8.96839714050293
Epoch 1580, val loss: 1.0866917371749878
Epoch 1590, training loss: 90.78191375732422 = 1.0855270624160767 + 10.0 * 8.96963882446289
Epoch 1590, val loss: 1.0865501165390015
Epoch 1600, training loss: 90.80793762207031 = 1.0853817462921143 + 10.0 * 8.97225570678711
Epoch 1600, val loss: 1.0864198207855225
Epoch 1610, training loss: 90.7696762084961 = 1.0852251052856445 + 10.0 * 8.96844482421875
Epoch 1610, val loss: 1.0862828493118286
Epoch 1620, training loss: 90.80156707763672 = 1.0850672721862793 + 10.0 * 8.971650123596191
Epoch 1620, val loss: 1.0861371755599976
Epoch 1630, training loss: 90.822265625 = 1.0849159955978394 + 10.0 * 8.973734855651855
Epoch 1630, val loss: 1.0859988927841187
Epoch 1640, training loss: 90.85757446289062 = 1.0847609043121338 + 10.0 * 8.97728157043457
Epoch 1640, val loss: 1.085862398147583
Epoch 1650, training loss: 90.88536071777344 = 1.084611415863037 + 10.0 * 8.980074882507324
Epoch 1650, val loss: 1.0857216119766235
Epoch 1660, training loss: 90.88687896728516 = 1.0844521522521973 + 10.0 * 8.980242729187012
Epoch 1660, val loss: 1.0855785608291626
Epoch 1670, training loss: 90.88148498535156 = 1.0842838287353516 + 10.0 * 8.979720115661621
Epoch 1670, val loss: 1.0854278802871704
Epoch 1680, training loss: 90.88739776611328 = 1.084120750427246 + 10.0 * 8.980327606201172
Epoch 1680, val loss: 1.085279941558838
Epoch 1690, training loss: 90.94674682617188 = 1.0839622020721436 + 10.0 * 8.986278533935547
Epoch 1690, val loss: 1.0851372480392456
Epoch 1700, training loss: 90.96112823486328 = 1.0838032960891724 + 10.0 * 8.987732887268066
Epoch 1700, val loss: 1.0849913358688354
Epoch 1710, training loss: 90.9226303100586 = 1.0836257934570312 + 10.0 * 8.98390007019043
Epoch 1710, val loss: 1.0848281383514404
Epoch 1720, training loss: 90.93767547607422 = 1.0834660530090332 + 10.0 * 8.985421180725098
Epoch 1720, val loss: 1.0846848487854004
Epoch 1730, training loss: 91.01031494140625 = 1.0832993984222412 + 10.0 * 8.992701530456543
Epoch 1730, val loss: 1.0845340490341187
Epoch 1740, training loss: 91.00056457519531 = 1.0831290483474731 + 10.0 * 8.991743087768555
Epoch 1740, val loss: 1.0843852758407593
Epoch 1750, training loss: 90.95317077636719 = 1.082964539527893 + 10.0 * 8.987020492553711
Epoch 1750, val loss: 1.084226131439209
Epoch 1760, training loss: 91.00701904296875 = 1.0827763080596924 + 10.0 * 8.992424011230469
Epoch 1760, val loss: 1.0840678215026855
Epoch 1770, training loss: 91.05107879638672 = 1.0826196670532227 + 10.0 * 8.996846199035645
Epoch 1770, val loss: 1.0839210748672485
Epoch 1780, training loss: 91.05692291259766 = 1.0824447870254517 + 10.0 * 8.997447967529297
Epoch 1780, val loss: 1.0837643146514893
Epoch 1790, training loss: 91.04268646240234 = 1.0822700262069702 + 10.0 * 8.996042251586914
Epoch 1790, val loss: 1.0836095809936523
Epoch 1800, training loss: 91.01814270019531 = 1.0820914506912231 + 10.0 * 8.99360466003418
Epoch 1800, val loss: 1.0834425687789917
Epoch 1810, training loss: 91.0330810546875 = 1.0819098949432373 + 10.0 * 8.9951171875
Epoch 1810, val loss: 1.0832830667495728
Epoch 1820, training loss: 91.09031677246094 = 1.081739902496338 + 10.0 * 9.00085735321045
Epoch 1820, val loss: 1.0831290483474731
Epoch 1830, training loss: 91.14143371582031 = 1.0815712213516235 + 10.0 * 9.005986213684082
Epoch 1830, val loss: 1.0829740762710571
Epoch 1840, training loss: 91.11573028564453 = 1.0813809633255005 + 10.0 * 9.003435134887695
Epoch 1840, val loss: 1.0828100442886353
Epoch 1850, training loss: 91.1351547241211 = 1.0811810493469238 + 10.0 * 9.005396842956543
Epoch 1850, val loss: 1.082629680633545
Epoch 1860, training loss: 91.12704467773438 = 1.0809917449951172 + 10.0 * 9.004605293273926
Epoch 1860, val loss: 1.082457423210144
Epoch 1870, training loss: 91.14744567871094 = 1.080806016921997 + 10.0 * 9.006664276123047
Epoch 1870, val loss: 1.0822877883911133
Epoch 1880, training loss: 91.0509033203125 = 1.0806169509887695 + 10.0 * 8.997028350830078
Epoch 1880, val loss: 1.0820988416671753
Epoch 1890, training loss: 91.30738067626953 = 1.0803937911987305 + 10.0 * 9.022699356079102
Epoch 1890, val loss: 1.0819132328033447
Epoch 1900, training loss: 90.98139190673828 = 1.080161690711975 + 10.0 * 8.99012279510498
Epoch 1900, val loss: 1.081714391708374
Epoch 1910, training loss: 91.1220932006836 = 1.0800122022628784 + 10.0 * 9.0042085647583
Epoch 1910, val loss: 1.0815671682357788
Epoch 1920, training loss: 91.2149658203125 = 1.079848289489746 + 10.0 * 9.013511657714844
Epoch 1920, val loss: 1.0814297199249268
Epoch 1930, training loss: 91.26190948486328 = 1.079686164855957 + 10.0 * 9.018221855163574
Epoch 1930, val loss: 1.0812879800796509
Epoch 1940, training loss: 91.28065490722656 = 1.079511046409607 + 10.0 * 9.020113945007324
Epoch 1940, val loss: 1.0811315774917603
Epoch 1950, training loss: 91.32744598388672 = 1.0793287754058838 + 10.0 * 9.024811744689941
Epoch 1950, val loss: 1.0809682607650757
Epoch 1960, training loss: 91.36445617675781 = 1.079143762588501 + 10.0 * 9.028531074523926
Epoch 1960, val loss: 1.0808058977127075
Epoch 1970, training loss: 91.35171508789062 = 1.0789490938186646 + 10.0 * 9.027276992797852
Epoch 1970, val loss: 1.0806344747543335
Epoch 1980, training loss: 91.37908935546875 = 1.078752875328064 + 10.0 * 9.030034065246582
Epoch 1980, val loss: 1.0804630517959595
Epoch 1990, training loss: 91.42417907714844 = 1.0785585641860962 + 10.0 * 9.034562110900879
Epoch 1990, val loss: 1.080291509628296
Epoch 2000, training loss: 91.37596130371094 = 1.0783545970916748 + 10.0 * 9.029760360717773
Epoch 2000, val loss: 1.0801090002059937
Epoch 2010, training loss: 91.4335708618164 = 1.0781587362289429 + 10.0 * 9.035541534423828
Epoch 2010, val loss: 1.0799413919448853
Epoch 2020, training loss: 91.50171661376953 = 1.0779709815979004 + 10.0 * 9.042374610900879
Epoch 2020, val loss: 1.0797699689865112
Epoch 2030, training loss: 91.49805450439453 = 1.0777652263641357 + 10.0 * 9.042028427124023
Epoch 2030, val loss: 1.0795879364013672
Epoch 2040, training loss: 91.43523406982422 = 1.077533483505249 + 10.0 * 9.035770416259766
Epoch 2040, val loss: 1.0793869495391846
Epoch 2050, training loss: 91.50640869140625 = 1.0773364305496216 + 10.0 * 9.042906761169434
Epoch 2050, val loss: 1.0792138576507568
Epoch 2060, training loss: 91.56570434570312 = 1.077135443687439 + 10.0 * 9.048856735229492
Epoch 2060, val loss: 1.0790369510650635
Epoch 2070, training loss: 91.59523010253906 = 1.0769323110580444 + 10.0 * 9.05182933807373
Epoch 2070, val loss: 1.078855276107788
Epoch 2080, training loss: 91.5859146118164 = 1.076723575592041 + 10.0 * 9.050919532775879
Epoch 2080, val loss: 1.0786747932434082
Epoch 2090, training loss: 91.60309600830078 = 1.076517939567566 + 10.0 * 9.052658081054688
Epoch 2090, val loss: 1.0784896612167358
Epoch 2100, training loss: 91.56698608398438 = 1.0763100385665894 + 10.0 * 9.049067497253418
Epoch 2100, val loss: 1.0782967805862427
Epoch 2110, training loss: 91.59619140625 = 1.0760931968688965 + 10.0 * 9.052009582519531
Epoch 2110, val loss: 1.0781090259552002
Epoch 2120, training loss: 91.6666030883789 = 1.0758858919143677 + 10.0 * 9.05907154083252
Epoch 2120, val loss: 1.077925443649292
Epoch 2130, training loss: 91.6961669921875 = 1.0756758451461792 + 10.0 * 9.06204891204834
Epoch 2130, val loss: 1.0777342319488525
Epoch 2140, training loss: 91.5887680053711 = 1.0754525661468506 + 10.0 * 9.051331520080566
Epoch 2140, val loss: 1.0775420665740967
Epoch 2150, training loss: 91.58845520019531 = 1.0752289295196533 + 10.0 * 9.051322937011719
Epoch 2150, val loss: 1.07735013961792
Epoch 2160, training loss: 91.64601135253906 = 1.0750186443328857 + 10.0 * 9.057099342346191
Epoch 2160, val loss: 1.0771640539169312
Epoch 2170, training loss: 91.71967315673828 = 1.074806809425354 + 10.0 * 9.064486503601074
Epoch 2170, val loss: 1.0769764184951782
Epoch 2180, training loss: 91.78304290771484 = 1.0745922327041626 + 10.0 * 9.070844650268555
Epoch 2180, val loss: 1.0767875909805298
Epoch 2190, training loss: 91.7725830078125 = 1.074367642402649 + 10.0 * 9.06982135772705
Epoch 2190, val loss: 1.0765836238861084
Epoch 2200, training loss: 91.78669738769531 = 1.0741462707519531 + 10.0 * 9.071255683898926
Epoch 2200, val loss: 1.076388955116272
Epoch 2210, training loss: 91.82794952392578 = 1.0739257335662842 + 10.0 * 9.07540225982666
Epoch 2210, val loss: 1.0761970281600952
Epoch 2220, training loss: 91.80374908447266 = 1.0736963748931885 + 10.0 * 9.073004722595215
Epoch 2220, val loss: 1.075989842414856
Epoch 2230, training loss: 91.818115234375 = 1.0734745264053345 + 10.0 * 9.074464797973633
Epoch 2230, val loss: 1.075787901878357
Epoch 2240, training loss: 91.83226776123047 = 1.0732507705688477 + 10.0 * 9.075901985168457
Epoch 2240, val loss: 1.0755894184112549
Epoch 2250, training loss: 91.83668518066406 = 1.0730199813842773 + 10.0 * 9.076366424560547
Epoch 2250, val loss: 1.075386643409729
Epoch 2260, training loss: 91.88848876953125 = 1.0727945566177368 + 10.0 * 9.08156967163086
Epoch 2260, val loss: 1.0751903057098389
Epoch 2270, training loss: 91.9233169555664 = 1.072574257850647 + 10.0 * 9.085074424743652
Epoch 2270, val loss: 1.0749967098236084
Epoch 2280, training loss: 91.91654968261719 = 1.0723445415496826 + 10.0 * 9.084421157836914
Epoch 2280, val loss: 1.074790358543396
Epoch 2290, training loss: 91.87564849853516 = 1.0721063613891602 + 10.0 * 9.080354690551758
Epoch 2290, val loss: 1.0745816230773926
Epoch 2300, training loss: 91.82463836669922 = 1.0718568563461304 + 10.0 * 9.075278282165527
Epoch 2300, val loss: 1.0743564367294312
Epoch 2310, training loss: 91.62116241455078 = 1.0716100931167603 + 10.0 * 9.05495548248291
Epoch 2310, val loss: 1.0741558074951172
Epoch 2320, training loss: 91.73452758789062 = 1.0713882446289062 + 10.0 * 9.066313743591309
Epoch 2320, val loss: 1.0739574432373047
Epoch 2330, training loss: 91.86152648925781 = 1.0711898803710938 + 10.0 * 9.079033851623535
Epoch 2330, val loss: 1.073777437210083
Epoch 2340, training loss: 91.91265869140625 = 1.070939540863037 + 10.0 * 9.084172248840332
Epoch 2340, val loss: 1.0735527276992798
Epoch 2350, training loss: 91.89695739746094 = 1.0707234144210815 + 10.0 * 9.082623481750488
Epoch 2350, val loss: 1.0733641386032104
Epoch 2360, training loss: 91.9329605102539 = 1.0705006122589111 + 10.0 * 9.0862455368042
Epoch 2360, val loss: 1.0731712579727173
Epoch 2370, training loss: 92.00279235839844 = 1.0702804327011108 + 10.0 * 9.09325122833252
Epoch 2370, val loss: 1.0729811191558838
Epoch 2380, training loss: 92.09734344482422 = 1.070053219795227 + 10.0 * 9.102728843688965
Epoch 2380, val loss: 1.0727834701538086
Epoch 2390, training loss: 92.15351104736328 = 1.069830060005188 + 10.0 * 9.108367919921875
Epoch 2390, val loss: 1.0725845098495483
Epoch 2400, training loss: 92.191650390625 = 1.0695925951004028 + 10.0 * 9.112205505371094
Epoch 2400, val loss: 1.072379231452942
Epoch 2410, training loss: 92.2083511352539 = 1.0693576335906982 + 10.0 * 9.113899230957031
Epoch 2410, val loss: 1.0721744298934937
Epoch 2420, training loss: 92.24930572509766 = 1.0691229104995728 + 10.0 * 9.11801815032959
Epoch 2420, val loss: 1.071960687637329
Epoch 2430, training loss: 92.23368835449219 = 1.068880319595337 + 10.0 * 9.116480827331543
Epoch 2430, val loss: 1.0717577934265137
Epoch 2440, training loss: 92.27042388916016 = 1.0686439275741577 + 10.0 * 9.12017822265625
Epoch 2440, val loss: 1.0715491771697998
Epoch 2450, training loss: 92.30732727050781 = 1.0684071779251099 + 10.0 * 9.123891830444336
Epoch 2450, val loss: 1.0713427066802979
Epoch 2460, training loss: 92.27122497558594 = 1.0681581497192383 + 10.0 * 9.120306015014648
Epoch 2460, val loss: 1.0711270570755005
Epoch 2470, training loss: 92.29054260253906 = 1.0679137706756592 + 10.0 * 9.122262954711914
Epoch 2470, val loss: 1.0709189176559448
Epoch 2480, training loss: 92.32154846191406 = 1.067674994468689 + 10.0 * 9.125387191772461
Epoch 2480, val loss: 1.070712685585022
Epoch 2490, training loss: 92.31262969970703 = 1.067431092262268 + 10.0 * 9.124520301818848
Epoch 2490, val loss: 1.0705015659332275
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.20594202898550723
0.8122871839455191
=== training gcn model ===
Epoch 0, training loss: 103.76795959472656 = 1.0945924520492554 + 10.0 * 10.26733684539795
Epoch 0, val loss: 1.0936305522918701
Epoch 10, training loss: 100.19062805175781 = 1.0944924354553223 + 10.0 * 9.909613609313965
Epoch 10, val loss: 1.093511700630188
Epoch 20, training loss: 98.24049377441406 = 1.094387173652649 + 10.0 * 9.71461009979248
Epoch 20, val loss: 1.0933938026428223
Epoch 30, training loss: 96.76358032226562 = 1.094293475151062 + 10.0 * 9.56692886352539
Epoch 30, val loss: 1.0932819843292236
Epoch 40, training loss: 95.57951354980469 = 1.0941922664642334 + 10.0 * 9.448532104492188
Epoch 40, val loss: 1.0931655168533325
Epoch 50, training loss: 94.63939666748047 = 1.0940881967544556 + 10.0 * 9.354531288146973
Epoch 50, val loss: 1.0930430889129639
Epoch 60, training loss: 93.87454223632812 = 1.0939799547195435 + 10.0 * 9.278056144714355
Epoch 60, val loss: 1.092916488647461
Epoch 70, training loss: 93.2264404296875 = 1.0938694477081299 + 10.0 * 9.2132568359375
Epoch 70, val loss: 1.0927892923355103
Epoch 80, training loss: 92.67662048339844 = 1.0937567949295044 + 10.0 * 9.158286094665527
Epoch 80, val loss: 1.0926612615585327
Epoch 90, training loss: 92.1826400756836 = 1.0936479568481445 + 10.0 * 9.108899116516113
Epoch 90, val loss: 1.0925374031066895
Epoch 100, training loss: 91.75470733642578 = 1.0935311317443848 + 10.0 * 9.066118240356445
Epoch 100, val loss: 1.0924065113067627
Epoch 110, training loss: 91.38275146484375 = 1.0934157371520996 + 10.0 * 9.02893352508545
Epoch 110, val loss: 1.092276930809021
Epoch 120, training loss: 91.07372283935547 = 1.093291163444519 + 10.0 * 8.998043060302734
Epoch 120, val loss: 1.092140555381775
Epoch 130, training loss: 90.78731536865234 = 1.0931669473648071 + 10.0 * 8.969414710998535
Epoch 130, val loss: 1.0920065641403198
Epoch 140, training loss: 90.55258178710938 = 1.0930370092391968 + 10.0 * 8.945954322814941
Epoch 140, val loss: 1.0918670892715454
Epoch 150, training loss: 90.33780670166016 = 1.0928928852081299 + 10.0 * 8.924490928649902
Epoch 150, val loss: 1.0917108058929443
Epoch 160, training loss: 90.18446350097656 = 1.0927507877349854 + 10.0 * 8.909171104431152
Epoch 160, val loss: 1.0915582180023193
Epoch 170, training loss: 90.01585388183594 = 1.0925958156585693 + 10.0 * 8.892325401306152
Epoch 170, val loss: 1.0913958549499512
Epoch 180, training loss: 89.89060974121094 = 1.092432975769043 + 10.0 * 8.879817962646484
Epoch 180, val loss: 1.0912275314331055
Epoch 190, training loss: 89.75568389892578 = 1.0922702550888062 + 10.0 * 8.866341590881348
Epoch 190, val loss: 1.0910557508468628
Epoch 200, training loss: 89.65898132324219 = 1.0920988321304321 + 10.0 * 8.856687545776367
Epoch 200, val loss: 1.0908780097961426
Epoch 210, training loss: 89.57379150390625 = 1.091927409172058 + 10.0 * 8.848186492919922
Epoch 210, val loss: 1.0907036066055298
Epoch 220, training loss: 89.49726867675781 = 1.0917515754699707 + 10.0 * 8.840551376342773
Epoch 220, val loss: 1.0905264616012573
Epoch 230, training loss: 89.45955657958984 = 1.0915673971176147 + 10.0 * 8.836798667907715
Epoch 230, val loss: 1.0903289318084717
Epoch 240, training loss: 89.36497497558594 = 1.091381549835205 + 10.0 * 8.827359199523926
Epoch 240, val loss: 1.0901446342468262
Epoch 250, training loss: 89.34889221191406 = 1.0912010669708252 + 10.0 * 8.825769424438477
Epoch 250, val loss: 1.0899512767791748
Epoch 260, training loss: 89.33624267578125 = 1.0910152196884155 + 10.0 * 8.824522018432617
Epoch 260, val loss: 1.0897623300552368
Epoch 270, training loss: 89.2947006225586 = 1.0908170938491821 + 10.0 * 8.820387840270996
Epoch 270, val loss: 1.0895720720291138
Epoch 280, training loss: 89.27155303955078 = 1.0906224250793457 + 10.0 * 8.818093299865723
Epoch 280, val loss: 1.0893765687942505
Epoch 290, training loss: 89.24848937988281 = 1.090417742729187 + 10.0 * 8.815807342529297
Epoch 290, val loss: 1.0891661643981934
Epoch 300, training loss: 89.24395751953125 = 1.0902113914489746 + 10.0 * 8.815374374389648
Epoch 300, val loss: 1.0889616012573242
Epoch 310, training loss: 89.22543334960938 = 1.0899962186813354 + 10.0 * 8.813543319702148
Epoch 310, val loss: 1.088746190071106
Epoch 320, training loss: 89.20197296142578 = 1.089773178100586 + 10.0 * 8.811220169067383
Epoch 320, val loss: 1.0885274410247803
Epoch 330, training loss: 89.2304916381836 = 1.0895541906356812 + 10.0 * 8.814093589782715
Epoch 330, val loss: 1.08830988407135
Epoch 340, training loss: 89.20823669433594 = 1.0893232822418213 + 10.0 * 8.811891555786133
Epoch 340, val loss: 1.0880861282348633
Epoch 350, training loss: 89.20596313476562 = 1.089086651802063 + 10.0 * 8.811687469482422
Epoch 350, val loss: 1.087849736213684
Epoch 360, training loss: 89.24867248535156 = 1.0888458490371704 + 10.0 * 8.815982818603516
Epoch 360, val loss: 1.0876121520996094
Epoch 370, training loss: 89.21246337890625 = 1.0885941982269287 + 10.0 * 8.812387466430664
Epoch 370, val loss: 1.0873724222183228
Epoch 380, training loss: 89.23726654052734 = 1.0883466005325317 + 10.0 * 8.814891815185547
Epoch 380, val loss: 1.0871199369430542
Epoch 390, training loss: 89.26756286621094 = 1.0880903005599976 + 10.0 * 8.817947387695312
Epoch 390, val loss: 1.0868746042251587
Epoch 400, training loss: 89.26244354248047 = 1.0878254175186157 + 10.0 * 8.817461967468262
Epoch 400, val loss: 1.0866163969039917
Epoch 410, training loss: 89.3612060546875 = 1.0875365734100342 + 10.0 * 8.827366828918457
Epoch 410, val loss: 1.0863269567489624
Epoch 420, training loss: 89.42769622802734 = 1.087303638458252 + 10.0 * 8.834039688110352
Epoch 420, val loss: 1.0861265659332275
Epoch 430, training loss: 89.13506317138672 = 1.0869826078414917 + 10.0 * 8.804807662963867
Epoch 430, val loss: 1.0858193635940552
Epoch 440, training loss: 89.28260803222656 = 1.0867303609848022 + 10.0 * 8.819587707519531
Epoch 440, val loss: 1.0855602025985718
Epoch 450, training loss: 89.36439514160156 = 1.0864397287368774 + 10.0 * 8.827795028686523
Epoch 450, val loss: 1.08528470993042
Epoch 460, training loss: 89.43400573730469 = 1.086151361465454 + 10.0 * 8.834785461425781
Epoch 460, val loss: 1.0850062370300293
Epoch 470, training loss: 89.41680908203125 = 1.0858503580093384 + 10.0 * 8.83309555053711
Epoch 470, val loss: 1.0847169160842896
Epoch 480, training loss: 89.42516326904297 = 1.0855499505996704 + 10.0 * 8.833961486816406
Epoch 480, val loss: 1.0844310522079468
Epoch 490, training loss: 89.42949676513672 = 1.0852291584014893 + 10.0 * 8.834426879882812
Epoch 490, val loss: 1.0841283798217773
Epoch 500, training loss: 89.44041442871094 = 1.084920048713684 + 10.0 * 8.835549354553223
Epoch 500, val loss: 1.0838292837142944
Epoch 510, training loss: 89.45083618164062 = 1.084607720375061 + 10.0 * 8.83662223815918
Epoch 510, val loss: 1.08353590965271
Epoch 520, training loss: 89.48639678955078 = 1.0842936038970947 + 10.0 * 8.8402099609375
Epoch 520, val loss: 1.0832360982894897
Epoch 530, training loss: 89.51107788085938 = 1.0839722156524658 + 10.0 * 8.842710494995117
Epoch 530, val loss: 1.0829260349273682
Epoch 540, training loss: 89.53926086425781 = 1.0836396217346191 + 10.0 * 8.845561981201172
Epoch 540, val loss: 1.082617998123169
Epoch 550, training loss: 89.47676086425781 = 1.0832873582839966 + 10.0 * 8.839346885681152
Epoch 550, val loss: 1.0822848081588745
Epoch 560, training loss: 89.57439422607422 = 1.0829687118530273 + 10.0 * 8.849142074584961
Epoch 560, val loss: 1.0819897651672363
Epoch 570, training loss: 89.58021545410156 = 1.0826325416564941 + 10.0 * 8.84975814819336
Epoch 570, val loss: 1.081668496131897
Epoch 580, training loss: 89.5555419921875 = 1.0822688341140747 + 10.0 * 8.84732723236084
Epoch 580, val loss: 1.0813385248184204
Epoch 590, training loss: 89.60130310058594 = 1.0819218158721924 + 10.0 * 8.851938247680664
Epoch 590, val loss: 1.0809999704360962
Epoch 600, training loss: 89.60456848144531 = 1.0815571546554565 + 10.0 * 8.852300643920898
Epoch 600, val loss: 1.0806645154953003
Epoch 610, training loss: 89.59815979003906 = 1.081193208694458 + 10.0 * 8.851696968078613
Epoch 610, val loss: 1.0803163051605225
Epoch 620, training loss: 89.69564819335938 = 1.0808264017105103 + 10.0 * 8.861482620239258
Epoch 620, val loss: 1.079979658126831
Epoch 630, training loss: 89.72528839111328 = 1.080458641052246 + 10.0 * 8.864482879638672
Epoch 630, val loss: 1.0796387195587158
Epoch 640, training loss: 89.74028015136719 = 1.080065369606018 + 10.0 * 8.866022109985352
Epoch 640, val loss: 1.079271912574768
Epoch 650, training loss: 89.72496032714844 = 1.0797051191329956 + 10.0 * 8.86452579498291
Epoch 650, val loss: 1.078930377960205
Epoch 660, training loss: 89.75076293945312 = 1.079339861869812 + 10.0 * 8.867142677307129
Epoch 660, val loss: 1.0785930156707764
Epoch 670, training loss: 89.79998779296875 = 1.0789474248886108 + 10.0 * 8.872103691101074
Epoch 670, val loss: 1.0782251358032227
Epoch 680, training loss: 89.8167724609375 = 1.0785592794418335 + 10.0 * 8.873821258544922
Epoch 680, val loss: 1.0778732299804688
Epoch 690, training loss: 89.82767486572266 = 1.0781598091125488 + 10.0 * 8.874951362609863
Epoch 690, val loss: 1.0775021314620972
Epoch 700, training loss: 89.83145141601562 = 1.077762484550476 + 10.0 * 8.87536907196045
Epoch 700, val loss: 1.0771414041519165
Epoch 710, training loss: 89.86814880371094 = 1.0773727893829346 + 10.0 * 8.879077911376953
Epoch 710, val loss: 1.0767748355865479
Epoch 720, training loss: 89.9129867553711 = 1.0769721269607544 + 10.0 * 8.883601188659668
Epoch 720, val loss: 1.076407790184021
Epoch 730, training loss: 89.95957946777344 = 1.0765657424926758 + 10.0 * 8.888300895690918
Epoch 730, val loss: 1.076019525527954
Epoch 740, training loss: 89.62716674804688 = 1.0759997367858887 + 10.0 * 8.855116844177246
Epoch 740, val loss: 1.0754996538162231
Epoch 750, training loss: 90.01876068115234 = 1.075730562210083 + 10.0 * 8.894303321838379
Epoch 750, val loss: 1.0752424001693726
Epoch 760, training loss: 89.70722198486328 = 1.0752325057983398 + 10.0 * 8.863199234008789
Epoch 760, val loss: 1.0748107433319092
Epoch 770, training loss: 89.72830963134766 = 1.074852705001831 + 10.0 * 8.86534595489502
Epoch 770, val loss: 1.0744500160217285
Epoch 780, training loss: 89.82656860351562 = 1.0744495391845703 + 10.0 * 8.875211715698242
Epoch 780, val loss: 1.07406747341156
Epoch 790, training loss: 89.8379898071289 = 1.0740299224853516 + 10.0 * 8.876396179199219
Epoch 790, val loss: 1.0736937522888184
Epoch 800, training loss: 89.92581176757812 = 1.0736186504364014 + 10.0 * 8.88521957397461
Epoch 800, val loss: 1.0733145475387573
Epoch 810, training loss: 89.9947280883789 = 1.0731830596923828 + 10.0 * 8.892154693603516
Epoch 810, val loss: 1.0729116201400757
Epoch 820, training loss: 90.05821990966797 = 1.072761058807373 + 10.0 * 8.89854621887207
Epoch 820, val loss: 1.0725229978561401
Epoch 830, training loss: 90.06302642822266 = 1.0723145008087158 + 10.0 * 8.89907169342041
Epoch 830, val loss: 1.0721139907836914
Epoch 840, training loss: 90.12605285644531 = 1.0718668699264526 + 10.0 * 8.905418395996094
Epoch 840, val loss: 1.0717072486877441
Epoch 850, training loss: 90.1332778930664 = 1.0714017152786255 + 10.0 * 8.906187057495117
Epoch 850, val loss: 1.071277379989624
Epoch 860, training loss: 90.15072631835938 = 1.0709421634674072 + 10.0 * 8.907978057861328
Epoch 860, val loss: 1.0708577632904053
Epoch 870, training loss: 90.20834350585938 = 1.070483684539795 + 10.0 * 8.913785934448242
Epoch 870, val loss: 1.070428490638733
Epoch 880, training loss: 90.23153686523438 = 1.070007562637329 + 10.0 * 8.916152954101562
Epoch 880, val loss: 1.0699870586395264
Epoch 890, training loss: 90.23686981201172 = 1.069542646408081 + 10.0 * 8.916732788085938
Epoch 890, val loss: 1.0695559978485107
Epoch 900, training loss: 90.23454284667969 = 1.0690621137619019 + 10.0 * 8.916547775268555
Epoch 900, val loss: 1.0691134929656982
Epoch 910, training loss: 90.24710083007812 = 1.0685566663742065 + 10.0 * 8.917854309082031
Epoch 910, val loss: 1.0686609745025635
Epoch 920, training loss: 90.28965759277344 = 1.0680745840072632 + 10.0 * 8.922158241271973
Epoch 920, val loss: 1.0682132244110107
Epoch 930, training loss: 90.29051208496094 = 1.0675806999206543 + 10.0 * 8.922292709350586
Epoch 930, val loss: 1.067760705947876
Epoch 940, training loss: 90.36990356445312 = 1.0670814514160156 + 10.0 * 8.930282592773438
Epoch 940, val loss: 1.06730318069458
Epoch 950, training loss: 90.4017562866211 = 1.0665793418884277 + 10.0 * 8.933517456054688
Epoch 950, val loss: 1.066831111907959
Epoch 960, training loss: 90.34517669677734 = 1.0660501718521118 + 10.0 * 8.927912712097168
Epoch 960, val loss: 1.06634521484375
Epoch 970, training loss: 90.4039535522461 = 1.0655345916748047 + 10.0 * 8.933841705322266
Epoch 970, val loss: 1.0658648014068604
Epoch 980, training loss: 90.45195007324219 = 1.0650125741958618 + 10.0 * 8.93869400024414
Epoch 980, val loss: 1.065403699874878
Epoch 990, training loss: 90.45072174072266 = 1.0645021200180054 + 10.0 * 8.93862247467041
Epoch 990, val loss: 1.0649206638336182
Epoch 1000, training loss: 90.51541900634766 = 1.0639721155166626 + 10.0 * 8.945144653320312
Epoch 1000, val loss: 1.0644400119781494
Epoch 1010, training loss: 90.52980041503906 = 1.0634346008300781 + 10.0 * 8.946637153625488
Epoch 1010, val loss: 1.063930869102478
Epoch 1020, training loss: 90.52465057373047 = 1.0628947019577026 + 10.0 * 8.946175575256348
Epoch 1020, val loss: 1.0634353160858154
Epoch 1030, training loss: 90.56803131103516 = 1.062351107597351 + 10.0 * 8.950568199157715
Epoch 1030, val loss: 1.0629353523254395
Epoch 1040, training loss: 90.5912094116211 = 1.0617908239364624 + 10.0 * 8.95294189453125
Epoch 1040, val loss: 1.0624198913574219
Epoch 1050, training loss: 90.64013671875 = 1.061249017715454 + 10.0 * 8.95788860321045
Epoch 1050, val loss: 1.0619208812713623
Epoch 1060, training loss: 90.69151306152344 = 1.0606951713562012 + 10.0 * 8.963082313537598
Epoch 1060, val loss: 1.0613973140716553
Epoch 1070, training loss: 90.7406234741211 = 1.0601284503936768 + 10.0 * 8.968050003051758
Epoch 1070, val loss: 1.06085205078125
Epoch 1080, training loss: 90.76307678222656 = 1.05955171585083 + 10.0 * 8.970352172851562
Epoch 1080, val loss: 1.0603262186050415
Epoch 1090, training loss: 90.67120361328125 = 1.0589594841003418 + 10.0 * 8.961224555969238
Epoch 1090, val loss: 1.059790849685669
Epoch 1100, training loss: 90.6769790649414 = 1.0583864450454712 + 10.0 * 8.961858749389648
Epoch 1100, val loss: 1.059253454208374
Epoch 1110, training loss: 90.7634048461914 = 1.0578265190124512 + 10.0 * 8.970558166503906
Epoch 1110, val loss: 1.0587342977523804
Epoch 1120, training loss: 90.781494140625 = 1.0572625398635864 + 10.0 * 8.97242259979248
Epoch 1120, val loss: 1.058190107345581
Epoch 1130, training loss: 90.81710052490234 = 1.0566768646240234 + 10.0 * 8.976041793823242
Epoch 1130, val loss: 1.0576461553573608
Epoch 1140, training loss: 90.84140014648438 = 1.0561037063598633 + 10.0 * 8.97852897644043
Epoch 1140, val loss: 1.0571064949035645
Epoch 1150, training loss: 90.8602066040039 = 1.0555057525634766 + 10.0 * 8.980470657348633
Epoch 1150, val loss: 1.0565404891967773
Epoch 1160, training loss: 90.85540771484375 = 1.0549014806747437 + 10.0 * 8.980051040649414
Epoch 1160, val loss: 1.0559769868850708
Epoch 1170, training loss: 90.88923645019531 = 1.0543080568313599 + 10.0 * 8.983492851257324
Epoch 1170, val loss: 1.0554149150848389
Epoch 1180, training loss: 90.92935180664062 = 1.053719162940979 + 10.0 * 8.987563133239746
Epoch 1180, val loss: 1.0548585653305054
Epoch 1190, training loss: 90.92032623291016 = 1.0531045198440552 + 10.0 * 8.986721992492676
Epoch 1190, val loss: 1.0542964935302734
Epoch 1200, training loss: 90.98282623291016 = 1.0525155067443848 + 10.0 * 8.99303150177002
Epoch 1200, val loss: 1.053714632987976
Epoch 1210, training loss: 91.05382537841797 = 1.0519109964370728 + 10.0 * 9.000191688537598
Epoch 1210, val loss: 1.0531448125839233
Epoch 1220, training loss: 91.04954528808594 = 1.0512887239456177 + 10.0 * 8.999825477600098
Epoch 1220, val loss: 1.0525450706481934
Epoch 1230, training loss: 91.06675720214844 = 1.050667405128479 + 10.0 * 9.001608848571777
Epoch 1230, val loss: 1.051971673965454
Epoch 1240, training loss: 91.09229278564453 = 1.0500569343566895 + 10.0 * 9.004223823547363
Epoch 1240, val loss: 1.0513830184936523
Epoch 1250, training loss: 91.126953125 = 1.0494245290756226 + 10.0 * 9.007753372192383
Epoch 1250, val loss: 1.0508067607879639
Epoch 1260, training loss: 91.1583480834961 = 1.0488035678863525 + 10.0 * 9.010953903198242
Epoch 1260, val loss: 1.050209403038025
Epoch 1270, training loss: 91.07550811767578 = 1.0481274127960205 + 10.0 * 9.002737998962402
Epoch 1270, val loss: 1.049590826034546
Epoch 1280, training loss: 91.07728576660156 = 1.0474839210510254 + 10.0 * 9.00298023223877
Epoch 1280, val loss: 1.0489823818206787
Epoch 1290, training loss: 91.10259246826172 = 1.0468655824661255 + 10.0 * 9.005572319030762
Epoch 1290, val loss: 1.0484024286270142
Epoch 1300, training loss: 91.18006134033203 = 1.0462393760681152 + 10.0 * 9.013381958007812
Epoch 1300, val loss: 1.0478116273880005
Epoch 1310, training loss: 91.21916198730469 = 1.0455944538116455 + 10.0 * 9.017356872558594
Epoch 1310, val loss: 1.0472065210342407
Epoch 1320, training loss: 91.20514678955078 = 1.0449399948120117 + 10.0 * 9.016020774841309
Epoch 1320, val loss: 1.046590805053711
Epoch 1330, training loss: 91.30516815185547 = 1.0443143844604492 + 10.0 * 9.02608585357666
Epoch 1330, val loss: 1.0459898710250854
Epoch 1340, training loss: 91.29698944091797 = 1.0436497926712036 + 10.0 * 9.025334358215332
Epoch 1340, val loss: 1.0453640222549438
Epoch 1350, training loss: 91.31995391845703 = 1.0429913997650146 + 10.0 * 9.02769660949707
Epoch 1350, val loss: 1.0447481870651245
Epoch 1360, training loss: 91.3783187866211 = 1.0423369407653809 + 10.0 * 9.033597946166992
Epoch 1360, val loss: 1.0441375970840454
Epoch 1370, training loss: 91.36737060546875 = 1.0416741371154785 + 10.0 * 9.032569885253906
Epoch 1370, val loss: 1.043512225151062
Epoch 1380, training loss: 91.40560150146484 = 1.0409996509552002 + 10.0 * 9.036459922790527
Epoch 1380, val loss: 1.0428738594055176
Epoch 1390, training loss: 91.43312072753906 = 1.0403296947479248 + 10.0 * 9.039278984069824
Epoch 1390, val loss: 1.0422617197036743
Epoch 1400, training loss: 91.43364715576172 = 1.0396615266799927 + 10.0 * 9.039398193359375
Epoch 1400, val loss: 1.041609764099121
Epoch 1410, training loss: 91.47518920898438 = 1.0390084981918335 + 10.0 * 9.043618202209473
Epoch 1410, val loss: 1.0410079956054688
Epoch 1420, training loss: 91.45374298095703 = 1.0383284091949463 + 10.0 * 9.04154109954834
Epoch 1420, val loss: 1.040368914604187
Epoch 1430, training loss: 91.41987609863281 = 1.037688136100769 + 10.0 * 9.03821849822998
Epoch 1430, val loss: 1.0397685766220093
Epoch 1440, training loss: 91.4147720336914 = 1.036901593208313 + 10.0 * 9.037786483764648
Epoch 1440, val loss: 1.0390135049819946
Epoch 1450, training loss: 91.56929779052734 = 1.0362893342971802 + 10.0 * 9.053300857543945
Epoch 1450, val loss: 1.0384498834609985
Epoch 1460, training loss: 91.47115325927734 = 1.0355620384216309 + 10.0 * 9.043559074401855
Epoch 1460, val loss: 1.0377711057662964
Epoch 1470, training loss: 91.34619140625 = 1.0348800420761108 + 10.0 * 9.03113079071045
Epoch 1470, val loss: 1.0371206998825073
Epoch 1480, training loss: 91.46023559570312 = 1.0342093706130981 + 10.0 * 9.0426025390625
Epoch 1480, val loss: 1.0364933013916016
Epoch 1490, training loss: 91.3714828491211 = 1.033502221107483 + 10.0 * 9.033798217773438
Epoch 1490, val loss: 1.0358248949050903
Epoch 1500, training loss: 91.53190612792969 = 1.0328086614608765 + 10.0 * 9.049909591674805
Epoch 1500, val loss: 1.035175085067749
Epoch 1510, training loss: 91.46417999267578 = 1.032128930091858 + 10.0 * 9.043205261230469
Epoch 1510, val loss: 1.0345369577407837
Epoch 1520, training loss: 91.47152709960938 = 1.0314571857452393 + 10.0 * 9.044007301330566
Epoch 1520, val loss: 1.0338960886001587
Epoch 1530, training loss: 91.5533218383789 = 1.0307868719100952 + 10.0 * 9.052253723144531
Epoch 1530, val loss: 1.033264398574829
Epoch 1540, training loss: 91.57711791992188 = 1.0300822257995605 + 10.0 * 9.054703712463379
Epoch 1540, val loss: 1.0325835943222046
Epoch 1550, training loss: 91.59288024902344 = 1.0293761491775513 + 10.0 * 9.056350708007812
Epoch 1550, val loss: 1.0319247245788574
Epoch 1560, training loss: 91.6639175415039 = 1.0286946296691895 + 10.0 * 9.063522338867188
Epoch 1560, val loss: 1.0312811136245728
Epoch 1570, training loss: 91.676025390625 = 1.0279768705368042 + 10.0 * 9.064805030822754
Epoch 1570, val loss: 1.030601143836975
Epoch 1580, training loss: 91.66209411621094 = 1.0272448062896729 + 10.0 * 9.063485145568848
Epoch 1580, val loss: 1.0299227237701416
Epoch 1590, training loss: 91.6258544921875 = 1.0265159606933594 + 10.0 * 9.05993366241455
Epoch 1590, val loss: 1.0292385816574097
Epoch 1600, training loss: 91.58812713623047 = 1.0257729291915894 + 10.0 * 9.056235313415527
Epoch 1600, val loss: 1.0285253524780273
Epoch 1610, training loss: 91.64160919189453 = 1.0250500440597534 + 10.0 * 9.06165599822998
Epoch 1610, val loss: 1.0278621912002563
Epoch 1620, training loss: 91.48478698730469 = 1.0243085622787476 + 10.0 * 9.046048164367676
Epoch 1620, val loss: 1.027144193649292
Epoch 1630, training loss: 91.45925903320312 = 1.0235710144042969 + 10.0 * 9.04356861114502
Epoch 1630, val loss: 1.026475191116333
Epoch 1640, training loss: 91.61994934082031 = 1.0229034423828125 + 10.0 * 9.059704780578613
Epoch 1640, val loss: 1.0258337259292603
Epoch 1650, training loss: 91.58824157714844 = 1.022145390510559 + 10.0 * 9.056609153747559
Epoch 1650, val loss: 1.0251141786575317
Epoch 1660, training loss: 91.66056060791016 = 1.0214005708694458 + 10.0 * 9.063916206359863
Epoch 1660, val loss: 1.0244194269180298
Epoch 1670, training loss: 91.72850799560547 = 1.0206599235534668 + 10.0 * 9.070784568786621
Epoch 1670, val loss: 1.023726463317871
Epoch 1680, training loss: 91.72611236572266 = 1.019921898841858 + 10.0 * 9.070619583129883
Epoch 1680, val loss: 1.0230255126953125
Epoch 1690, training loss: 91.78253173828125 = 1.0191665887832642 + 10.0 * 9.076336860656738
Epoch 1690, val loss: 1.0223478078842163
Epoch 1700, training loss: 91.84115600585938 = 1.0184319019317627 + 10.0 * 9.08227252960205
Epoch 1700, val loss: 1.0216572284698486
Epoch 1710, training loss: 91.86809539794922 = 1.0176805257797241 + 10.0 * 9.085041999816895
Epoch 1710, val loss: 1.0209537744522095
Epoch 1720, training loss: 91.84171295166016 = 1.0169129371643066 + 10.0 * 9.082479476928711
Epoch 1720, val loss: 1.020249843597412
Epoch 1730, training loss: 91.84056091308594 = 1.016162395477295 + 10.0 * 9.082440376281738
Epoch 1730, val loss: 1.0195541381835938
Epoch 1740, training loss: 91.90193176269531 = 1.0154097080230713 + 10.0 * 9.088651657104492
Epoch 1740, val loss: 1.018862009048462
Epoch 1750, training loss: 91.92408752441406 = 1.014670968055725 + 10.0 * 9.090941429138184
Epoch 1750, val loss: 1.0181859731674194
Epoch 1760, training loss: 91.94581604003906 = 1.0139199495315552 + 10.0 * 9.093189239501953
Epoch 1760, val loss: 1.0174955129623413
Epoch 1770, training loss: 91.94437408447266 = 1.0131694078445435 + 10.0 * 9.093120574951172
Epoch 1770, val loss: 1.016808032989502
Epoch 1780, training loss: 92.00836181640625 = 1.0124248266220093 + 10.0 * 9.099593162536621
Epoch 1780, val loss: 1.0161187648773193
Epoch 1790, training loss: 91.83248901367188 = 1.0116735696792603 + 10.0 * 9.08208179473877
Epoch 1790, val loss: 1.0154228210449219
Epoch 1800, training loss: 91.93230438232422 = 1.0109394788742065 + 10.0 * 9.09213638305664
Epoch 1800, val loss: 1.014757752418518
Epoch 1810, training loss: 91.93738555908203 = 1.0101721286773682 + 10.0 * 9.092721939086914
Epoch 1810, val loss: 1.0140591859817505
Epoch 1820, training loss: 92.0069580078125 = 1.0094434022903442 + 10.0 * 9.099751472473145
Epoch 1820, val loss: 1.0133854150772095
Epoch 1830, training loss: 92.05411529541016 = 1.0087000131607056 + 10.0 * 9.104541778564453
Epoch 1830, val loss: 1.0126956701278687
Epoch 1840, training loss: 92.00823974609375 = 1.0079421997070312 + 10.0 * 9.100029945373535
Epoch 1840, val loss: 1.0120166540145874
Epoch 1850, training loss: 92.01957702636719 = 1.0072218179702759 + 10.0 * 9.101235389709473
Epoch 1850, val loss: 1.0113528966903687
Epoch 1860, training loss: 92.07662963867188 = 1.0064918994903564 + 10.0 * 9.107013702392578
Epoch 1860, val loss: 1.0106875896453857
Epoch 1870, training loss: 92.04815673828125 = 1.0057488679885864 + 10.0 * 9.104240417480469
Epoch 1870, val loss: 1.0100021362304688
Epoch 1880, training loss: 92.10224151611328 = 1.0050266981124878 + 10.0 * 9.109721183776855
Epoch 1880, val loss: 1.0093410015106201
Epoch 1890, training loss: 92.1305923461914 = 1.004294991493225 + 10.0 * 9.112629890441895
Epoch 1890, val loss: 1.008668065071106
Epoch 1900, training loss: 92.1509017944336 = 1.0035583972930908 + 10.0 * 9.114734649658203
Epoch 1900, val loss: 1.007991075515747
Epoch 1910, training loss: 92.14791870117188 = 1.0028287172317505 + 10.0 * 9.114508628845215
Epoch 1910, val loss: 1.0073118209838867
Epoch 1920, training loss: 92.16683197021484 = 1.002098798751831 + 10.0 * 9.116473197937012
Epoch 1920, val loss: 1.0066417455673218
Epoch 1930, training loss: 92.1649398803711 = 1.0013773441314697 + 10.0 * 9.116355895996094
Epoch 1930, val loss: 1.0059665441513062
Epoch 1940, training loss: 92.20774841308594 = 1.0006616115570068 + 10.0 * 9.120708465576172
Epoch 1940, val loss: 1.005306363105774
Epoch 1950, training loss: 92.20439147949219 = 0.999934732913971 + 10.0 * 9.12044620513916
Epoch 1950, val loss: 1.0046215057373047
Epoch 1960, training loss: 92.15037536621094 = 0.9991921782493591 + 10.0 * 9.115118026733398
Epoch 1960, val loss: 1.0039541721343994
Epoch 1970, training loss: 92.19795227050781 = 0.9984952211380005 + 10.0 * 9.119945526123047
Epoch 1970, val loss: 1.0033038854599
Epoch 1980, training loss: 92.26184844970703 = 0.9977783560752869 + 10.0 * 9.1264066696167
Epoch 1980, val loss: 1.0026391744613647
Epoch 1990, training loss: 92.26652526855469 = 0.9970593452453613 + 10.0 * 9.126946449279785
Epoch 1990, val loss: 1.0019670724868774
Epoch 2000, training loss: 92.26314544677734 = 0.9963456988334656 + 10.0 * 9.126680374145508
Epoch 2000, val loss: 1.0013127326965332
Epoch 2010, training loss: 92.28799438476562 = 0.9956387877464294 + 10.0 * 9.129236221313477
Epoch 2010, val loss: 1.0006685256958008
Epoch 2020, training loss: 92.24728393554688 = 0.9949190616607666 + 10.0 * 9.125236511230469
Epoch 2020, val loss: 1.0000067949295044
Epoch 2030, training loss: 92.25077056884766 = 0.994215190410614 + 10.0 * 9.125655174255371
Epoch 2030, val loss: 0.999367892742157
Epoch 2040, training loss: 92.28827667236328 = 0.9935218691825867 + 10.0 * 9.129475593566895
Epoch 2040, val loss: 0.9987214803695679
Epoch 2050, training loss: 92.33392333984375 = 0.9928324818611145 + 10.0 * 9.134108543395996
Epoch 2050, val loss: 0.9980726838111877
Epoch 2060, training loss: 92.3190689086914 = 0.9921258687973022 + 10.0 * 9.132694244384766
Epoch 2060, val loss: 0.9974208474159241
Epoch 2070, training loss: 92.12097930908203 = 0.9913804531097412 + 10.0 * 9.112959861755371
Epoch 2070, val loss: 0.9967377781867981
Epoch 2080, training loss: 92.0962905883789 = 0.9907452464103699 + 10.0 * 9.110554695129395
Epoch 2080, val loss: 0.9961407780647278
Epoch 2090, training loss: 92.058837890625 = 0.9900758862495422 + 10.0 * 9.106876373291016
Epoch 2090, val loss: 0.9954952001571655
Epoch 2100, training loss: 92.14907836914062 = 0.9894716143608093 + 10.0 * 9.115961074829102
Epoch 2100, val loss: 0.9949319958686829
Epoch 2110, training loss: 92.23157501220703 = 0.9888302683830261 + 10.0 * 9.124274253845215
Epoch 2110, val loss: 0.9943321347236633
Epoch 2120, training loss: 92.31391143798828 = 0.9881691336631775 + 10.0 * 9.132574081420898
Epoch 2120, val loss: 0.9937163591384888
Epoch 2130, training loss: 92.31838989257812 = 0.9874914884567261 + 10.0 * 9.133090019226074
Epoch 2130, val loss: 0.9930887222290039
Epoch 2140, training loss: 92.37989807128906 = 0.9868320226669312 + 10.0 * 9.139307022094727
Epoch 2140, val loss: 0.9924718141555786
Epoch 2150, training loss: 92.1416015625 = 0.9860832691192627 + 10.0 * 9.115551948547363
Epoch 2150, val loss: 0.9917656183242798
Epoch 2160, training loss: 92.16166687011719 = 0.9855006337165833 + 10.0 * 9.117616653442383
Epoch 2160, val loss: 0.9912317991256714
Epoch 2170, training loss: 92.2363052368164 = 0.9848732948303223 + 10.0 * 9.125143051147461
Epoch 2170, val loss: 0.9906811118125916
Epoch 2180, training loss: 92.29139709472656 = 0.9842482209205627 + 10.0 * 9.130715370178223
Epoch 2180, val loss: 0.9901079535484314
Epoch 2190, training loss: 92.36516571044922 = 0.9836196899414062 + 10.0 * 9.138154983520508
Epoch 2190, val loss: 0.9895299673080444
Epoch 2200, training loss: 92.4160385131836 = 0.9829809665679932 + 10.0 * 9.143305778503418
Epoch 2200, val loss: 0.9889354109764099
Epoch 2210, training loss: 92.42452239990234 = 0.9823260307312012 + 10.0 * 9.144220352172852
Epoch 2210, val loss: 0.988327145576477
Epoch 2220, training loss: 92.40472412109375 = 0.9816713929176331 + 10.0 * 9.142305374145508
Epoch 2220, val loss: 0.9877252578735352
Epoch 2230, training loss: 92.42919921875 = 0.9810448884963989 + 10.0 * 9.144815444946289
Epoch 2230, val loss: 0.9871467351913452
Epoch 2240, training loss: 92.48462677001953 = 0.980414628982544 + 10.0 * 9.150421142578125
Epoch 2240, val loss: 0.9865668416023254
Epoch 2250, training loss: 92.44043731689453 = 0.9797738790512085 + 10.0 * 9.146066665649414
Epoch 2250, val loss: 0.9859784841537476
Epoch 2260, training loss: 92.43888854980469 = 0.9791396260261536 + 10.0 * 9.145975112915039
Epoch 2260, val loss: 0.9854040741920471
Epoch 2270, training loss: 92.50245666503906 = 0.9785324931144714 + 10.0 * 9.152392387390137
Epoch 2270, val loss: 0.9848478436470032
Epoch 2280, training loss: 92.53289794921875 = 0.9779103994369507 + 10.0 * 9.155498504638672
Epoch 2280, val loss: 0.9842813611030579
Epoch 2290, training loss: 92.51736450195312 = 0.977294385433197 + 10.0 * 9.154006958007812
Epoch 2290, val loss: 0.9837256073951721
Epoch 2300, training loss: 92.55609893798828 = 0.9766897559165955 + 10.0 * 9.157940864562988
Epoch 2300, val loss: 0.9831604361534119
Epoch 2310, training loss: 92.5435791015625 = 0.9760750532150269 + 10.0 * 9.156750679016113
Epoch 2310, val loss: 0.9826022982597351
Epoch 2320, training loss: 92.50851440429688 = 0.9754817485809326 + 10.0 * 9.153303146362305
Epoch 2320, val loss: 0.9820423722267151
Epoch 2330, training loss: 92.5641098022461 = 0.97491055727005 + 10.0 * 9.158920288085938
Epoch 2330, val loss: 0.9815155267715454
Epoch 2340, training loss: 92.60332489013672 = 0.9743233919143677 + 10.0 * 9.1628999710083
Epoch 2340, val loss: 0.9809736013412476
Epoch 2350, training loss: 92.6150131225586 = 0.9737128019332886 + 10.0 * 9.164130210876465
Epoch 2350, val loss: 0.9804321527481079
Epoch 2360, training loss: 92.59101104736328 = 0.9731029868125916 + 10.0 * 9.16179084777832
Epoch 2360, val loss: 0.9798800349235535
Epoch 2370, training loss: 92.6099624633789 = 0.9725075364112854 + 10.0 * 9.163744926452637
Epoch 2370, val loss: 0.9793354868888855
Epoch 2380, training loss: 92.64897918701172 = 0.9719223380088806 + 10.0 * 9.167705535888672
Epoch 2380, val loss: 0.9787909984588623
Epoch 2390, training loss: 92.68901824951172 = 0.971337080001831 + 10.0 * 9.171768188476562
Epoch 2390, val loss: 0.9782470464706421
Epoch 2400, training loss: 92.66619110107422 = 0.9707472920417786 + 10.0 * 9.169544219970703
Epoch 2400, val loss: 0.977721095085144
Epoch 2410, training loss: 92.64643096923828 = 0.9701666831970215 + 10.0 * 9.16762638092041
Epoch 2410, val loss: 0.9771909117698669
Epoch 2420, training loss: 92.68964385986328 = 0.9696002006530762 + 10.0 * 9.172004699707031
Epoch 2420, val loss: 0.9766756892204285
Epoch 2430, training loss: 92.71772003173828 = 0.9690280556678772 + 10.0 * 9.174869537353516
Epoch 2430, val loss: 0.9761480689048767
Epoch 2440, training loss: 92.74327087402344 = 0.9684613943099976 + 10.0 * 9.177480697631836
Epoch 2440, val loss: 0.9756212830543518
Epoch 2450, training loss: 92.68241119384766 = 0.9678858518600464 + 10.0 * 9.171452522277832
Epoch 2450, val loss: 0.9751004576683044
Epoch 2460, training loss: 92.67647552490234 = 0.9673528671264648 + 10.0 * 9.17091178894043
Epoch 2460, val loss: 0.9746095538139343
Epoch 2470, training loss: 92.72784423828125 = 0.9668149352073669 + 10.0 * 9.176103591918945
Epoch 2470, val loss: 0.9741125106811523
Epoch 2480, training loss: 92.7768783569336 = 0.9662653803825378 + 10.0 * 9.181061744689941
Epoch 2480, val loss: 0.9736030101776123
Epoch 2490, training loss: 92.79768371582031 = 0.9657183885574341 + 10.0 * 9.183196067810059
Epoch 2490, val loss: 0.9730968475341797
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.4711594202898551
0.8155473447801204
The final CL Acc:0.29435, 0.12502, The final GNN Acc:0.81369, 0.00137
raw graph: torch.Size([2, 88648])
add edge: torch.Size([2, 110976])
remove edge: torch.Size([2, 66630])
updated graph: torch.Size([2, 88958])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 103.27436828613281 = 1.091111660003662 + 10.0 * 10.2183256149292
Epoch 0, val loss: 1.0903698205947876
Epoch 10, training loss: 99.0027084350586 = 1.0908137559890747 + 10.0 * 9.791189193725586
Epoch 10, val loss: 1.0900788307189941
Epoch 20, training loss: 96.99996948242188 = 1.0905579328536987 + 10.0 * 9.590940475463867
Epoch 20, val loss: 1.0898035764694214
Epoch 30, training loss: 95.56183624267578 = 1.0902413129806519 + 10.0 * 9.447159767150879
Epoch 30, val loss: 1.0894888639450073
Epoch 40, training loss: 94.46141815185547 = 1.0899531841278076 + 10.0 * 9.337146759033203
Epoch 40, val loss: 1.0892060995101929
Epoch 50, training loss: 93.5410385131836 = 1.089643120765686 + 10.0 * 9.245139122009277
Epoch 50, val loss: 1.0888993740081787
Epoch 60, training loss: 92.76122283935547 = 1.0893210172653198 + 10.0 * 9.167190551757812
Epoch 60, val loss: 1.088579535484314
Epoch 70, training loss: 92.11437225341797 = 1.0889809131622314 + 10.0 * 9.1025390625
Epoch 70, val loss: 1.0882463455200195
Epoch 80, training loss: 91.5524673461914 = 1.0886391401290894 + 10.0 * 9.046382904052734
Epoch 80, val loss: 1.0879065990447998
Epoch 90, training loss: 91.07062530517578 = 1.0882614850997925 + 10.0 * 8.998235702514648
Epoch 90, val loss: 1.087536096572876
Epoch 100, training loss: 90.6667251586914 = 1.087868571281433 + 10.0 * 8.9578857421875
Epoch 100, val loss: 1.0871542692184448
Epoch 110, training loss: 90.3245849609375 = 1.0874640941619873 + 10.0 * 8.923711776733398
Epoch 110, val loss: 1.0867586135864258
Epoch 120, training loss: 90.13822937011719 = 1.0870155096054077 + 10.0 * 8.905121803283691
Epoch 120, val loss: 1.086317539215088
Epoch 130, training loss: 89.90398406982422 = 1.0865850448608398 + 10.0 * 8.881739616394043
Epoch 130, val loss: 1.0859066247940063
Epoch 140, training loss: 89.64081573486328 = 1.0861283540725708 + 10.0 * 8.85546875
Epoch 140, val loss: 1.0854564905166626
Epoch 150, training loss: 89.44378662109375 = 1.0856672525405884 + 10.0 * 8.835811614990234
Epoch 150, val loss: 1.0850114822387695
Epoch 160, training loss: 89.28634643554688 = 1.085059404373169 + 10.0 * 8.820128440856934
Epoch 160, val loss: 1.0844155550003052
Epoch 170, training loss: 89.16426086425781 = 1.0845187902450562 + 10.0 * 8.807973861694336
Epoch 170, val loss: 1.083897352218628
Epoch 180, training loss: 89.00196075439453 = 1.083906888961792 + 10.0 * 8.791805267333984
Epoch 180, val loss: 1.0833011865615845
Epoch 190, training loss: 88.92240905761719 = 1.0832881927490234 + 10.0 * 8.78391170501709
Epoch 190, val loss: 1.0826977491378784
Epoch 200, training loss: 88.80844116210938 = 1.0826163291931152 + 10.0 * 8.772582054138184
Epoch 200, val loss: 1.082046389579773
Epoch 210, training loss: 88.74256134033203 = 1.0819624662399292 + 10.0 * 8.766059875488281
Epoch 210, val loss: 1.0814119577407837
Epoch 220, training loss: 88.65630340576172 = 1.0812660455703735 + 10.0 * 8.757503509521484
Epoch 220, val loss: 1.0807318687438965
Epoch 230, training loss: 88.56841278076172 = 1.0805574655532837 + 10.0 * 8.748785018920898
Epoch 230, val loss: 1.080038070678711
Epoch 240, training loss: 88.52259063720703 = 1.079839825630188 + 10.0 * 8.744275093078613
Epoch 240, val loss: 1.0793490409851074
Epoch 250, training loss: 88.47251892089844 = 1.0790965557098389 + 10.0 * 8.73934268951416
Epoch 250, val loss: 1.0786162614822388
Epoch 260, training loss: 88.42227172851562 = 1.078324317932129 + 10.0 * 8.734395027160645
Epoch 260, val loss: 1.077872395515442
Epoch 270, training loss: 88.37602996826172 = 1.077549934387207 + 10.0 * 8.72984790802002
Epoch 270, val loss: 1.0771093368530273
Epoch 280, training loss: 88.34838104248047 = 1.0767637491226196 + 10.0 * 8.727161407470703
Epoch 280, val loss: 1.0763499736785889
Epoch 290, training loss: 88.32051086425781 = 1.0759541988372803 + 10.0 * 8.724455833435059
Epoch 290, val loss: 1.0755650997161865
Epoch 300, training loss: 88.2582778930664 = 1.0751163959503174 + 10.0 * 8.718316078186035
Epoch 300, val loss: 1.074758529663086
Epoch 310, training loss: 88.25557708740234 = 1.0742840766906738 + 10.0 * 8.71812915802002
Epoch 310, val loss: 1.073946237564087
Epoch 320, training loss: 88.2315673828125 = 1.0734325647354126 + 10.0 * 8.715813636779785
Epoch 320, val loss: 1.0731130838394165
Epoch 330, training loss: 88.22071838378906 = 1.0725375413894653 + 10.0 * 8.714818000793457
Epoch 330, val loss: 1.072243571281433
Epoch 340, training loss: 88.25603485107422 = 1.0716454982757568 + 10.0 * 8.718439102172852
Epoch 340, val loss: 1.0713508129119873
Epoch 350, training loss: 88.20533752441406 = 1.0707181692123413 + 10.0 * 8.713461875915527
Epoch 350, val loss: 1.070459246635437
Epoch 360, training loss: 88.19327545166016 = 1.0697425603866577 + 10.0 * 8.712353706359863
Epoch 360, val loss: 1.069524884223938
Epoch 370, training loss: 88.20450592041016 = 1.0687369108200073 + 10.0 * 8.713577270507812
Epoch 370, val loss: 1.0685391426086426
Epoch 380, training loss: 88.2112808227539 = 1.0677275657653809 + 10.0 * 8.71435546875
Epoch 380, val loss: 1.0675632953643799
Epoch 390, training loss: 88.2382583618164 = 1.0667052268981934 + 10.0 * 8.717155456542969
Epoch 390, val loss: 1.06656813621521
Epoch 400, training loss: 88.26724243164062 = 1.0656464099884033 + 10.0 * 8.720159530639648
Epoch 400, val loss: 1.0655509233474731
Epoch 410, training loss: 88.20095825195312 = 1.0645055770874023 + 10.0 * 8.713644981384277
Epoch 410, val loss: 1.0644733905792236
Epoch 420, training loss: 88.24589538574219 = 1.063372015953064 + 10.0 * 8.718252182006836
Epoch 420, val loss: 1.0633488893508911
Epoch 430, training loss: 88.21403503417969 = 1.062205195426941 + 10.0 * 8.71518325805664
Epoch 430, val loss: 1.0622245073318481
Epoch 440, training loss: 88.20667266845703 = 1.0610525608062744 + 10.0 * 8.71456241607666
Epoch 440, val loss: 1.0610980987548828
Epoch 450, training loss: 88.25149536132812 = 1.0598673820495605 + 10.0 * 8.719162940979004
Epoch 450, val loss: 1.0599452257156372
Epoch 460, training loss: 88.27066040039062 = 1.0586780309677124 + 10.0 * 8.721198081970215
Epoch 460, val loss: 1.058772325515747
Epoch 470, training loss: 88.27452850341797 = 1.0574170351028442 + 10.0 * 8.721711158752441
Epoch 470, val loss: 1.0575581789016724
Epoch 480, training loss: 88.29853820800781 = 1.0561835765838623 + 10.0 * 8.724235534667969
Epoch 480, val loss: 1.0563619136810303
Epoch 490, training loss: 88.33718872070312 = 1.0548591613769531 + 10.0 * 8.728232383728027
Epoch 490, val loss: 1.0550845861434937
Epoch 500, training loss: 88.2389144897461 = 1.053473949432373 + 10.0 * 8.718544006347656
Epoch 500, val loss: 1.0537184476852417
Epoch 510, training loss: 88.28229522705078 = 1.0521537065505981 + 10.0 * 8.723013877868652
Epoch 510, val loss: 1.0524214506149292
Epoch 520, training loss: 88.29501342773438 = 1.0507546663284302 + 10.0 * 8.724425315856934
Epoch 520, val loss: 1.0510677099227905
Epoch 530, training loss: 88.27950286865234 = 1.0493568181991577 + 10.0 * 8.723014831542969
Epoch 530, val loss: 1.0496867895126343
Epoch 540, training loss: 88.3124771118164 = 1.0479212999343872 + 10.0 * 8.726455688476562
Epoch 540, val loss: 1.0482888221740723
Epoch 550, training loss: 88.3340072631836 = 1.0464015007019043 + 10.0 * 8.728760719299316
Epoch 550, val loss: 1.046810269355774
Epoch 560, training loss: 88.36138916015625 = 1.0448338985443115 + 10.0 * 8.731656074523926
Epoch 560, val loss: 1.0452609062194824
Epoch 570, training loss: 88.40505981445312 = 1.0431519746780396 + 10.0 * 8.736190795898438
Epoch 570, val loss: 1.0436190366744995
Epoch 580, training loss: 88.3147964477539 = 1.0413799285888672 + 10.0 * 8.727341651916504
Epoch 580, val loss: 1.0418970584869385
Epoch 590, training loss: 88.32801818847656 = 1.0395504236221313 + 10.0 * 8.728846549987793
Epoch 590, val loss: 1.040097951889038
Epoch 600, training loss: 88.38749694824219 = 1.0377517938613892 + 10.0 * 8.73497486114502
Epoch 600, val loss: 1.0383286476135254
Epoch 610, training loss: 88.39010620117188 = 1.0358260869979858 + 10.0 * 8.735427856445312
Epoch 610, val loss: 1.036457896232605
Epoch 620, training loss: 88.43317413330078 = 1.0338823795318604 + 10.0 * 8.73992919921875
Epoch 620, val loss: 1.034542441368103
Epoch 630, training loss: 88.44425201416016 = 1.031931757926941 + 10.0 * 8.741231918334961
Epoch 630, val loss: 1.0326334238052368
Epoch 640, training loss: 88.46858215332031 = 1.0299186706542969 + 10.0 * 8.743866920471191
Epoch 640, val loss: 1.030649185180664
Epoch 650, training loss: 88.49369049072266 = 1.027841329574585 + 10.0 * 8.74658489227295
Epoch 650, val loss: 1.0285805463790894
Epoch 660, training loss: 88.51634216308594 = 1.025684118270874 + 10.0 * 8.749066352844238
Epoch 660, val loss: 1.0264619588851929
Epoch 670, training loss: 88.53627014160156 = 1.0234570503234863 + 10.0 * 8.751280784606934
Epoch 670, val loss: 1.0242338180541992
Epoch 680, training loss: 88.60189819335938 = 1.021085262298584 + 10.0 * 8.758081436157227
Epoch 680, val loss: 1.0218863487243652
Epoch 690, training loss: 88.58318328857422 = 1.0186210870742798 + 10.0 * 8.75645637512207
Epoch 690, val loss: 1.0194172859191895
Epoch 700, training loss: 88.67442321777344 = 1.0161247253417969 + 10.0 * 8.765829086303711
Epoch 700, val loss: 1.01691734790802
Epoch 710, training loss: 88.52313232421875 = 1.0134563446044922 + 10.0 * 8.750967025756836
Epoch 710, val loss: 1.0143414735794067
Epoch 720, training loss: 88.5738525390625 = 1.0108951330184937 + 10.0 * 8.756296157836914
Epoch 720, val loss: 1.011770486831665
Epoch 730, training loss: 88.65065002441406 = 1.0083205699920654 + 10.0 * 8.764232635498047
Epoch 730, val loss: 1.0092302560806274
Epoch 740, training loss: 88.5379638671875 = 1.0055251121520996 + 10.0 * 8.753244400024414
Epoch 740, val loss: 1.0065007209777832
Epoch 750, training loss: 88.61661529541016 = 1.00287663936615 + 10.0 * 8.761373519897461
Epoch 750, val loss: 1.0039256811141968
Epoch 760, training loss: 88.54603576660156 = 1.0003764629364014 + 10.0 * 8.754566192626953
Epoch 760, val loss: 1.0013437271118164
Epoch 770, training loss: 88.62689208984375 = 0.9975720047950745 + 10.0 * 8.762931823730469
Epoch 770, val loss: 0.9986236095428467
Epoch 780, training loss: 88.57238006591797 = 0.9947128891944885 + 10.0 * 8.757766723632812
Epoch 780, val loss: 0.9958257675170898
Epoch 790, training loss: 88.66568756103516 = 0.9918571710586548 + 10.0 * 8.767382621765137
Epoch 790, val loss: 0.9929903149604797
Epoch 800, training loss: 88.7210464477539 = 0.9889512062072754 + 10.0 * 8.773209571838379
Epoch 800, val loss: 0.9901108741760254
Epoch 810, training loss: 88.76842498779297 = 0.9859546422958374 + 10.0 * 8.778246879577637
Epoch 810, val loss: 0.987159788608551
Epoch 820, training loss: 88.79578399658203 = 0.9828866720199585 + 10.0 * 8.781290054321289
Epoch 820, val loss: 0.9841271638870239
Epoch 830, training loss: 88.73938751220703 = 0.9796960353851318 + 10.0 * 8.775968551635742
Epoch 830, val loss: 0.9809865355491638
Epoch 840, training loss: 88.77362060546875 = 0.9765613675117493 + 10.0 * 8.779706001281738
Epoch 840, val loss: 0.9779086709022522
Epoch 850, training loss: 88.89403533935547 = 0.9734373092651367 + 10.0 * 8.792059898376465
Epoch 850, val loss: 0.9748502373695374
Epoch 860, training loss: 88.9544677734375 = 0.9701636433601379 + 10.0 * 8.798430442810059
Epoch 860, val loss: 0.9716352820396423
Epoch 870, training loss: 88.96112823486328 = 0.9668340086936951 + 10.0 * 8.799428939819336
Epoch 870, val loss: 0.9683682918548584
Epoch 880, training loss: 88.95182800292969 = 0.9635505080223083 + 10.0 * 8.798828125
Epoch 880, val loss: 0.9651157855987549
Epoch 890, training loss: 89.01022338867188 = 0.9601260423660278 + 10.0 * 8.805009841918945
Epoch 890, val loss: 0.9617417454719543
Epoch 900, training loss: 88.90532684326172 = 0.956580638885498 + 10.0 * 8.79487419128418
Epoch 900, val loss: 0.9583384394645691
Epoch 910, training loss: 88.97835540771484 = 0.9531459212303162 + 10.0 * 8.802520751953125
Epoch 910, val loss: 0.954929769039154
Epoch 920, training loss: 89.01651000976562 = 0.9496526122093201 + 10.0 * 8.806685447692871
Epoch 920, val loss: 0.9514876008033752
Epoch 930, training loss: 89.08616638183594 = 0.9460872411727905 + 10.0 * 8.814007759094238
Epoch 930, val loss: 0.9480090141296387
Epoch 940, training loss: 89.12753295898438 = 0.9424947500228882 + 10.0 * 8.818503379821777
Epoch 940, val loss: 0.9444717764854431
Epoch 950, training loss: 89.07625579833984 = 0.9388298988342285 + 10.0 * 8.813742637634277
Epoch 950, val loss: 0.9408609867095947
Epoch 960, training loss: 89.14675903320312 = 0.9351342916488647 + 10.0 * 8.821162223815918
Epoch 960, val loss: 0.9372426271438599
Epoch 970, training loss: 89.1701431274414 = 0.9313912391662598 + 10.0 * 8.823875427246094
Epoch 970, val loss: 0.933598518371582
Epoch 980, training loss: 89.23629760742188 = 0.9278066158294678 + 10.0 * 8.830849647521973
Epoch 980, val loss: 0.9300693869590759
Epoch 990, training loss: 89.2281723022461 = 0.9240768551826477 + 10.0 * 8.830409049987793
Epoch 990, val loss: 0.9264001846313477
Epoch 1000, training loss: 89.24932098388672 = 0.9202302098274231 + 10.0 * 8.83290958404541
Epoch 1000, val loss: 0.9226415753364563
Epoch 1010, training loss: 89.2812271118164 = 0.9163715243339539 + 10.0 * 8.836484909057617
Epoch 1010, val loss: 0.9188909530639648
Epoch 1020, training loss: 89.27781677246094 = 0.9125003218650818 + 10.0 * 8.836531639099121
Epoch 1020, val loss: 0.9151029586791992
Epoch 1030, training loss: 89.32351684570312 = 0.9085858464241028 + 10.0 * 8.841493606567383
Epoch 1030, val loss: 0.9112761616706848
Epoch 1040, training loss: 89.31793212890625 = 0.904690146446228 + 10.0 * 8.841323852539062
Epoch 1040, val loss: 0.9074181318283081
Epoch 1050, training loss: 89.26262664794922 = 0.900706946849823 + 10.0 * 8.83619213104248
Epoch 1050, val loss: 0.9035592675209045
Epoch 1060, training loss: 89.31085205078125 = 0.896815836429596 + 10.0 * 8.84140396118164
Epoch 1060, val loss: 0.8997460603713989
Epoch 1070, training loss: 89.35785675048828 = 0.8928766846656799 + 10.0 * 8.846498489379883
Epoch 1070, val loss: 0.8959236145019531
Epoch 1080, training loss: 89.40113830566406 = 0.8889304399490356 + 10.0 * 8.851221084594727
Epoch 1080, val loss: 0.8920803070068359
Epoch 1090, training loss: 89.40636444091797 = 0.8849234580993652 + 10.0 * 8.852144241333008
Epoch 1090, val loss: 0.8881592154502869
Epoch 1100, training loss: 89.40880584716797 = 0.8808991312980652 + 10.0 * 8.852790832519531
Epoch 1100, val loss: 0.884270429611206
Epoch 1110, training loss: 89.42069244384766 = 0.8768739104270935 + 10.0 * 8.854381561279297
Epoch 1110, val loss: 0.8803339004516602
Epoch 1120, training loss: 89.39482116699219 = 0.8728252053260803 + 10.0 * 8.85219955444336
Epoch 1120, val loss: 0.8764175176620483
Epoch 1130, training loss: 89.38079071044922 = 0.8688979148864746 + 10.0 * 8.851189613342285
Epoch 1130, val loss: 0.8726288080215454
Epoch 1140, training loss: 89.42833709716797 = 0.864916205406189 + 10.0 * 8.856342315673828
Epoch 1140, val loss: 0.8687486052513123
Epoch 1150, training loss: 89.4906234741211 = 0.8609330058097839 + 10.0 * 8.862969398498535
Epoch 1150, val loss: 0.8648271560668945
Epoch 1160, training loss: 89.52790069580078 = 0.8568629622459412 + 10.0 * 8.867103576660156
Epoch 1160, val loss: 0.8608896732330322
Epoch 1170, training loss: 89.53223419189453 = 0.8528103828430176 + 10.0 * 8.867941856384277
Epoch 1170, val loss: 0.8569529056549072
Epoch 1180, training loss: 89.4361572265625 = 0.848843514919281 + 10.0 * 8.858731269836426
Epoch 1180, val loss: 0.8531673550605774
Epoch 1190, training loss: 89.49854278564453 = 0.8448912501335144 + 10.0 * 8.865365028381348
Epoch 1190, val loss: 0.8492457866668701
Epoch 1200, training loss: 89.50826263427734 = 0.8409929275512695 + 10.0 * 8.866726875305176
Epoch 1200, val loss: 0.8455320596694946
Epoch 1210, training loss: 89.58245849609375 = 0.836946427822113 + 10.0 * 8.874551773071289
Epoch 1210, val loss: 0.8415237665176392
Epoch 1220, training loss: 89.51190185546875 = 0.8327190279960632 + 10.0 * 8.867918014526367
Epoch 1220, val loss: 0.8374494910240173
Epoch 1230, training loss: 89.61456298828125 = 0.828597366809845 + 10.0 * 8.878596305847168
Epoch 1230, val loss: 0.8334574699401855
Epoch 1240, training loss: 89.69194030761719 = 0.8244664669036865 + 10.0 * 8.886747360229492
Epoch 1240, val loss: 0.8294731974601746
Epoch 1250, training loss: 89.73655700683594 = 0.8202826976776123 + 10.0 * 8.891627311706543
Epoch 1250, val loss: 0.8254143595695496
Epoch 1260, training loss: 89.7619400024414 = 0.8160861134529114 + 10.0 * 8.894585609436035
Epoch 1260, val loss: 0.8213754296302795
Epoch 1270, training loss: 89.81465911865234 = 0.811897873878479 + 10.0 * 8.900276184082031
Epoch 1270, val loss: 0.8173176050186157
Epoch 1280, training loss: 89.75542449951172 = 0.8076707124710083 + 10.0 * 8.894775390625
Epoch 1280, val loss: 0.8132466673851013
Epoch 1290, training loss: 89.79833984375 = 0.8034700751304626 + 10.0 * 8.899487495422363
Epoch 1290, val loss: 0.8091749548912048
Epoch 1300, training loss: 89.8276596069336 = 0.7992664575576782 + 10.0 * 8.902839660644531
Epoch 1300, val loss: 0.8051018714904785
Epoch 1310, training loss: 89.845947265625 = 0.7950379252433777 + 10.0 * 8.905091285705566
Epoch 1310, val loss: 0.8009844422340393
Epoch 1320, training loss: 89.80573272705078 = 0.7907764911651611 + 10.0 * 8.901494979858398
Epoch 1320, val loss: 0.7968961596488953
Epoch 1330, training loss: 89.78182220458984 = 0.7864941954612732 + 10.0 * 8.89953327178955
Epoch 1330, val loss: 0.7927038073539734
Epoch 1340, training loss: 89.85182189941406 = 0.7822889685630798 + 10.0 * 8.906953811645508
Epoch 1340, val loss: 0.7886664867401123
Epoch 1350, training loss: 89.89446258544922 = 0.7780409455299377 + 10.0 * 8.911642074584961
Epoch 1350, val loss: 0.7845438718795776
Epoch 1360, training loss: 89.91828155517578 = 0.7737616896629333 + 10.0 * 8.91445255279541
Epoch 1360, val loss: 0.7803831696510315
Epoch 1370, training loss: 89.92738342285156 = 0.769489049911499 + 10.0 * 8.915789604187012
Epoch 1370, val loss: 0.7762739658355713
Epoch 1380, training loss: 89.93812561035156 = 0.765214741230011 + 10.0 * 8.917291641235352
Epoch 1380, val loss: 0.7721653580665588
Epoch 1390, training loss: 89.95625305175781 = 0.7610111236572266 + 10.0 * 8.919524192810059
Epoch 1390, val loss: 0.7681065797805786
Epoch 1400, training loss: 89.9143295288086 = 0.7567973136901855 + 10.0 * 8.915753364562988
Epoch 1400, val loss: 0.7639887928962708
Epoch 1410, training loss: 89.93313598632812 = 0.7526415586471558 + 10.0 * 8.918049812316895
Epoch 1410, val loss: 0.7599701285362244
Epoch 1420, training loss: 89.96063995361328 = 0.7485234141349792 + 10.0 * 8.921212196350098
Epoch 1420, val loss: 0.7560425996780396
Epoch 1430, training loss: 89.97265625 = 0.7443404197692871 + 10.0 * 8.922831535339355
Epoch 1430, val loss: 0.7519899606704712
Epoch 1440, training loss: 89.98896026611328 = 0.7401530742645264 + 10.0 * 8.924880981445312
Epoch 1440, val loss: 0.7479569911956787
Epoch 1450, training loss: 89.98664855957031 = 0.7360391616821289 + 10.0 * 8.925061225891113
Epoch 1450, val loss: 0.7440069913864136
Epoch 1460, training loss: 90.01435089111328 = 0.7319716215133667 + 10.0 * 8.928237915039062
Epoch 1460, val loss: 0.7400720119476318
Epoch 1470, training loss: 90.01860046386719 = 0.7279038429260254 + 10.0 * 8.929069519042969
Epoch 1470, val loss: 0.7361510992050171
Epoch 1480, training loss: 90.0408935546875 = 0.7238706946372986 + 10.0 * 8.931702613830566
Epoch 1480, val loss: 0.7322927117347717
Epoch 1490, training loss: 90.0899887084961 = 0.7198415994644165 + 10.0 * 8.93701457977295
Epoch 1490, val loss: 0.7284173369407654
Epoch 1500, training loss: 90.08985900878906 = 0.7158631682395935 + 10.0 * 8.937398910522461
Epoch 1500, val loss: 0.7245966792106628
Epoch 1510, training loss: 90.041259765625 = 0.7119224667549133 + 10.0 * 8.932933807373047
Epoch 1510, val loss: 0.7207909822463989
Epoch 1520, training loss: 89.94315338134766 = 0.7079574465751648 + 10.0 * 8.9235200881958
Epoch 1520, val loss: 0.7170095443725586
Epoch 1530, training loss: 90.01922607421875 = 0.7040519714355469 + 10.0 * 8.931517601013184
Epoch 1530, val loss: 0.7132479548454285
Epoch 1540, training loss: 90.07807159423828 = 0.7001768946647644 + 10.0 * 8.937788963317871
Epoch 1540, val loss: 0.7095162868499756
Epoch 1550, training loss: 90.13842010498047 = 0.6962779760360718 + 10.0 * 8.9442138671875
Epoch 1550, val loss: 0.7057502269744873
Epoch 1560, training loss: 90.1148910522461 = 0.6923983097076416 + 10.0 * 8.942249298095703
Epoch 1560, val loss: 0.7020421028137207
Epoch 1570, training loss: 90.14572143554688 = 0.6884713172912598 + 10.0 * 8.945725440979004
Epoch 1570, val loss: 0.6982661485671997
Epoch 1580, training loss: 90.16232299804688 = 0.6845244765281677 + 10.0 * 8.947779655456543
Epoch 1580, val loss: 0.694530189037323
Epoch 1590, training loss: 90.153076171875 = 0.6806521415710449 + 10.0 * 8.947242736816406
Epoch 1590, val loss: 0.6907941699028015
Epoch 1600, training loss: 90.13639068603516 = 0.6767780184745789 + 10.0 * 8.945960998535156
Epoch 1600, val loss: 0.6870997548103333
Epoch 1610, training loss: 90.17754364013672 = 0.6730020046234131 + 10.0 * 8.950453758239746
Epoch 1610, val loss: 0.6835383176803589
Epoch 1620, training loss: 90.17749786376953 = 0.6692525744438171 + 10.0 * 8.950824737548828
Epoch 1620, val loss: 0.6800152063369751
Epoch 1630, training loss: 90.21028900146484 = 0.6655533313751221 + 10.0 * 8.954473495483398
Epoch 1630, val loss: 0.6765235066413879
Epoch 1640, training loss: 90.22514343261719 = 0.6618923544883728 + 10.0 * 8.956324577331543
Epoch 1640, val loss: 0.6730571985244751
Epoch 1650, training loss: 90.25292205810547 = 0.6583329439163208 + 10.0 * 8.95945930480957
Epoch 1650, val loss: 0.6696428060531616
Epoch 1660, training loss: 90.2486572265625 = 0.6547338962554932 + 10.0 * 8.959392547607422
Epoch 1660, val loss: 0.6662091612815857
Epoch 1670, training loss: 90.23822784423828 = 0.6512069702148438 + 10.0 * 8.958702087402344
Epoch 1670, val loss: 0.662879467010498
Epoch 1680, training loss: 90.25810241699219 = 0.6478033065795898 + 10.0 * 8.961030006408691
Epoch 1680, val loss: 0.6596325039863586
Epoch 1690, training loss: 90.3038330078125 = 0.6443748474121094 + 10.0 * 8.965946197509766
Epoch 1690, val loss: 0.6564067602157593
Epoch 1700, training loss: 90.27487182617188 = 0.6409642696380615 + 10.0 * 8.963391304016113
Epoch 1700, val loss: 0.6531744599342346
Epoch 1710, training loss: 90.21057891845703 = 0.6376574039459229 + 10.0 * 8.957292556762695
Epoch 1710, val loss: 0.6500716209411621
Epoch 1720, training loss: 90.21259307861328 = 0.6344375014305115 + 10.0 * 8.957815170288086
Epoch 1720, val loss: 0.64701247215271
Epoch 1730, training loss: 90.25214385986328 = 0.6311867833137512 + 10.0 * 8.962095260620117
Epoch 1730, val loss: 0.6439294815063477
Epoch 1740, training loss: 90.33023834228516 = 0.6279991865158081 + 10.0 * 8.970224380493164
Epoch 1740, val loss: 0.6409046649932861
Epoch 1750, training loss: 90.3346939086914 = 0.6247790455818176 + 10.0 * 8.970991134643555
Epoch 1750, val loss: 0.6378408074378967
Epoch 1760, training loss: 90.2177734375 = 0.6215786933898926 + 10.0 * 8.959619522094727
Epoch 1760, val loss: 0.6348242163658142
Epoch 1770, training loss: 90.11151123046875 = 0.6187852025032043 + 10.0 * 8.949273109436035
Epoch 1770, val loss: 0.6322253942489624
Epoch 1780, training loss: 89.93362426757812 = 0.6156911849975586 + 10.0 * 8.931793212890625
Epoch 1780, val loss: 0.6293542385101318
Epoch 1790, training loss: 90.11265563964844 = 0.6128547787666321 + 10.0 * 8.949979782104492
Epoch 1790, val loss: 0.6265959739685059
Epoch 1800, training loss: 90.10746765136719 = 0.6099067330360413 + 10.0 * 8.949755668640137
Epoch 1800, val loss: 0.6238619685173035
Epoch 1810, training loss: 90.22528839111328 = 0.6069852709770203 + 10.0 * 8.961830139160156
Epoch 1810, val loss: 0.6210892796516418
Epoch 1820, training loss: 90.28619384765625 = 0.6041094064712524 + 10.0 * 8.968208312988281
Epoch 1820, val loss: 0.6183578968048096
Epoch 1830, training loss: 90.31437683105469 = 0.601203203201294 + 10.0 * 8.971317291259766
Epoch 1830, val loss: 0.6156681180000305
Epoch 1840, training loss: 90.3016586303711 = 0.5982841849327087 + 10.0 * 8.970337867736816
Epoch 1840, val loss: 0.6130048036575317
Epoch 1850, training loss: 90.3177490234375 = 0.5954275131225586 + 10.0 * 8.9722318649292
Epoch 1850, val loss: 0.6103464961051941
Epoch 1860, training loss: 90.34846496582031 = 0.5926235318183899 + 10.0 * 8.975584030151367
Epoch 1860, val loss: 0.6076847314834595
Epoch 1870, training loss: 90.38285827636719 = 0.5898440480232239 + 10.0 * 8.979301452636719
Epoch 1870, val loss: 0.6051012873649597
Epoch 1880, training loss: 90.38060760498047 = 0.5871298909187317 + 10.0 * 8.979348182678223
Epoch 1880, val loss: 0.6025570034980774
Epoch 1890, training loss: 90.35848999023438 = 0.5844311714172363 + 10.0 * 8.977405548095703
Epoch 1890, val loss: 0.6000580191612244
Epoch 1900, training loss: 90.3776626586914 = 0.5818199515342712 + 10.0 * 8.979584693908691
Epoch 1900, val loss: 0.5976274013519287
Epoch 1910, training loss: 90.39248657226562 = 0.5792387127876282 + 10.0 * 8.981325149536133
Epoch 1910, val loss: 0.5952019691467285
Epoch 1920, training loss: 90.38203430175781 = 0.5766975283622742 + 10.0 * 8.980533599853516
Epoch 1920, val loss: 0.5928599238395691
Epoch 1930, training loss: 90.37727355957031 = 0.5741212964057922 + 10.0 * 8.980315208435059
Epoch 1930, val loss: 0.5905028581619263
Epoch 1940, training loss: 90.44035339355469 = 0.5716570019721985 + 10.0 * 8.986869812011719
Epoch 1940, val loss: 0.5882138609886169
Epoch 1950, training loss: 90.44944763183594 = 0.5691572427749634 + 10.0 * 8.988028526306152
Epoch 1950, val loss: 0.5859047174453735
Epoch 1960, training loss: 90.45484161376953 = 0.5666933655738831 + 10.0 * 8.988814353942871
Epoch 1960, val loss: 0.5836526155471802
Epoch 1970, training loss: 90.23836517333984 = 0.5643313527107239 + 10.0 * 8.967403411865234
Epoch 1970, val loss: 0.5814818739891052
Epoch 1980, training loss: 90.3411865234375 = 0.5620273351669312 + 10.0 * 8.97791576385498
Epoch 1980, val loss: 0.5794297456741333
Epoch 1990, training loss: 90.40052032470703 = 0.5596975088119507 + 10.0 * 8.984082221984863
Epoch 1990, val loss: 0.5773130655288696
Epoch 2000, training loss: 90.50159454345703 = 0.5573580861091614 + 10.0 * 8.994423866271973
Epoch 2000, val loss: 0.5752083659172058
Epoch 2010, training loss: 90.51520538330078 = 0.5550665855407715 + 10.0 * 8.996013641357422
Epoch 2010, val loss: 0.573114275932312
Epoch 2020, training loss: 90.5001220703125 = 0.5527781844139099 + 10.0 * 8.994733810424805
Epoch 2020, val loss: 0.5710254311561584
Epoch 2030, training loss: 90.49020385742188 = 0.5505548119544983 + 10.0 * 8.993965148925781
Epoch 2030, val loss: 0.5690011978149414
Epoch 2040, training loss: 90.52754211425781 = 0.5483691692352295 + 10.0 * 8.997917175292969
Epoch 2040, val loss: 0.5670461654663086
Epoch 2050, training loss: 90.37236022949219 = 0.5461931228637695 + 10.0 * 8.982616424560547
Epoch 2050, val loss: 0.5651295185089111
Epoch 2060, training loss: 90.40080261230469 = 0.5441146492958069 + 10.0 * 8.985669136047363
Epoch 2060, val loss: 0.5632573366165161
Epoch 2070, training loss: 90.39006805419922 = 0.5420156121253967 + 10.0 * 8.9848051071167
Epoch 2070, val loss: 0.561381995677948
Epoch 2080, training loss: 90.47864532470703 = 0.5400214195251465 + 10.0 * 8.99386215209961
Epoch 2080, val loss: 0.5595665574073792
Epoch 2090, training loss: 90.53568267822266 = 0.5380063652992249 + 10.0 * 8.999767303466797
Epoch 2090, val loss: 0.5577688813209534
Epoch 2100, training loss: 90.59379577636719 = 0.535970151424408 + 10.0 * 9.005782127380371
Epoch 2100, val loss: 0.5559651851654053
Epoch 2110, training loss: 90.5732650756836 = 0.533946692943573 + 10.0 * 9.003931999206543
Epoch 2110, val loss: 0.554169237613678
Epoch 2120, training loss: 90.58943939208984 = 0.5319510698318481 + 10.0 * 9.005748748779297
Epoch 2120, val loss: 0.552442729473114
Epoch 2130, training loss: 90.62248229980469 = 0.5299843549728394 + 10.0 * 9.009249687194824
Epoch 2130, val loss: 0.5507254600524902
Epoch 2140, training loss: 90.65692901611328 = 0.5280584096908569 + 10.0 * 9.012887001037598
Epoch 2140, val loss: 0.5490438342094421
Epoch 2150, training loss: 90.65211486816406 = 0.5261694192886353 + 10.0 * 9.012594223022461
Epoch 2150, val loss: 0.5474287271499634
Epoch 2160, training loss: 90.66767883300781 = 0.5243136882781982 + 10.0 * 9.014336585998535
Epoch 2160, val loss: 0.5458065271377563
Epoch 2170, training loss: 90.694580078125 = 0.5224751234054565 + 10.0 * 9.017210006713867
Epoch 2170, val loss: 0.5442174077033997
Epoch 2180, training loss: 90.61664581298828 = 0.5206490755081177 + 10.0 * 9.009599685668945
Epoch 2180, val loss: 0.5426769256591797
Epoch 2190, training loss: 90.58069610595703 = 0.5189132690429688 + 10.0 * 9.00617790222168
Epoch 2190, val loss: 0.5412033796310425
Epoch 2200, training loss: 90.65705108642578 = 0.5171970129013062 + 10.0 * 9.013985633850098
Epoch 2200, val loss: 0.5397512316703796
Epoch 2210, training loss: 90.69705963134766 = 0.5154702067375183 + 10.0 * 9.018158912658691
Epoch 2210, val loss: 0.5382691025733948
Epoch 2220, training loss: 90.73371887207031 = 0.5137574672698975 + 10.0 * 9.02199649810791
Epoch 2220, val loss: 0.5367990136146545
Epoch 2230, training loss: 90.76338195800781 = 0.5120669007301331 + 10.0 * 9.025131225585938
Epoch 2230, val loss: 0.5353777408599854
Epoch 2240, training loss: 90.69467163085938 = 0.5103926062583923 + 10.0 * 9.018427848815918
Epoch 2240, val loss: 0.5339402556419373
Epoch 2250, training loss: 90.37113189697266 = 0.5088905096054077 + 10.0 * 8.986224174499512
Epoch 2250, val loss: 0.5326625108718872
Epoch 2260, training loss: 90.45362091064453 = 0.5073490142822266 + 10.0 * 8.994626998901367
Epoch 2260, val loss: 0.5314883589744568
Epoch 2270, training loss: 90.46417236328125 = 0.5059788227081299 + 10.0 * 8.995819091796875
Epoch 2270, val loss: 0.5303003787994385
Epoch 2280, training loss: 90.4942626953125 = 0.5044139623641968 + 10.0 * 8.998984336853027
Epoch 2280, val loss: 0.5290051698684692
Epoch 2290, training loss: 90.58765411376953 = 0.5029268860816956 + 10.0 * 9.008472442626953
Epoch 2290, val loss: 0.5277591943740845
Epoch 2300, training loss: 90.6494369506836 = 0.5014344453811646 + 10.0 * 9.014800071716309
Epoch 2300, val loss: 0.5265001654624939
Epoch 2310, training loss: 90.72502136230469 = 0.49991491436958313 + 10.0 * 9.022510528564453
Epoch 2310, val loss: 0.5252554416656494
Epoch 2320, training loss: 90.76982116699219 = 0.49841010570526123 + 10.0 * 9.027140617370605
Epoch 2320, val loss: 0.5240219235420227
Epoch 2330, training loss: 90.78245544433594 = 0.4969201982021332 + 10.0 * 9.02855396270752
Epoch 2330, val loss: 0.5227981209754944
Epoch 2340, training loss: 90.59991455078125 = 0.49547579884529114 + 10.0 * 9.010443687438965
Epoch 2340, val loss: 0.5215476751327515
Epoch 2350, training loss: 90.57096099853516 = 0.49405941367149353 + 10.0 * 9.0076904296875
Epoch 2350, val loss: 0.5204576849937439
Epoch 2360, training loss: 90.68602752685547 = 0.49270161986351013 + 10.0 * 9.019332885742188
Epoch 2360, val loss: 0.5193393230438232
Epoch 2370, training loss: 90.65449523925781 = 0.4913669526576996 + 10.0 * 9.016313552856445
Epoch 2370, val loss: 0.5182172656059265
Epoch 2380, training loss: 90.67475128173828 = 0.49002644419670105 + 10.0 * 9.018472671508789
Epoch 2380, val loss: 0.5172240734100342
Epoch 2390, training loss: 90.75662994384766 = 0.4886481761932373 + 10.0 * 9.026798248291016
Epoch 2390, val loss: 0.5161078572273254
Epoch 2400, training loss: 90.82958221435547 = 0.48728668689727783 + 10.0 * 9.034229278564453
Epoch 2400, val loss: 0.514976978302002
Epoch 2410, training loss: 90.86676025390625 = 0.485948771238327 + 10.0 * 9.038081169128418
Epoch 2410, val loss: 0.5138927102088928
Epoch 2420, training loss: 90.88007354736328 = 0.4846273958683014 + 10.0 * 9.039545059204102
Epoch 2420, val loss: 0.5128520727157593
Epoch 2430, training loss: 90.90764617919922 = 0.4833300709724426 + 10.0 * 9.042431831359863
Epoch 2430, val loss: 0.5118367671966553
Epoch 2440, training loss: 90.88014221191406 = 0.48204028606414795 + 10.0 * 9.039810180664062
Epoch 2440, val loss: 0.5108006000518799
Epoch 2450, training loss: 90.9270248413086 = 0.48078975081443787 + 10.0 * 9.044623374938965
Epoch 2450, val loss: 0.5098312497138977
Epoch 2460, training loss: 90.96915435791016 = 0.4795371890068054 + 10.0 * 9.048961639404297
Epoch 2460, val loss: 0.5088360905647278
Epoch 2470, training loss: 90.96273803710938 = 0.47830817103385925 + 10.0 * 9.048442840576172
Epoch 2470, val loss: 0.5078722834587097
Epoch 2480, training loss: 90.98555755615234 = 0.4770985543727875 + 10.0 * 9.050846099853516
Epoch 2480, val loss: 0.5069326162338257
Epoch 2490, training loss: 91.01045227050781 = 0.47589871287345886 + 10.0 * 9.053455352783203
Epoch 2490, val loss: 0.5059894323348999
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8231884057971014
0.8651742374846049
=== training gcn model ===
Epoch 0, training loss: 102.6934814453125 = 1.0925419330596924 + 10.0 * 10.160093307495117
Epoch 0, val loss: 1.0935128927230835
Epoch 10, training loss: 98.7846908569336 = 1.0922012329101562 + 10.0 * 9.769248962402344
Epoch 10, val loss: 1.0931777954101562
Epoch 20, training loss: 96.80158996582031 = 1.0917502641677856 + 10.0 * 9.57098388671875
Epoch 20, val loss: 1.0927568674087524
Epoch 30, training loss: 95.3198471069336 = 1.0912984609603882 + 10.0 * 9.42285442352295
Epoch 30, val loss: 1.0923261642456055
Epoch 40, training loss: 94.16488647460938 = 1.0908628702163696 + 10.0 * 9.307401657104492
Epoch 40, val loss: 1.0919055938720703
Epoch 50, training loss: 93.21773529052734 = 1.090477705001831 + 10.0 * 9.212725639343262
Epoch 50, val loss: 1.0915580987930298
Epoch 60, training loss: 92.43787384033203 = 1.0902621746063232 + 10.0 * 9.134760856628418
Epoch 60, val loss: 1.09138822555542
Epoch 70, training loss: 91.78948974609375 = 1.0901062488555908 + 10.0 * 9.069938659667969
Epoch 70, val loss: 1.0912566184997559
Epoch 80, training loss: 91.251708984375 = 1.0899356603622437 + 10.0 * 9.0161771774292
Epoch 80, val loss: 1.0911058187484741
Epoch 90, training loss: 90.79806518554688 = 1.08976411819458 + 10.0 * 8.970829963684082
Epoch 90, val loss: 1.0909532308578491
Epoch 100, training loss: 90.40934753417969 = 1.0895912647247314 + 10.0 * 8.931975364685059
Epoch 100, val loss: 1.0908039808273315
Epoch 110, training loss: 90.06928253173828 = 1.0894036293029785 + 10.0 * 8.897988319396973
Epoch 110, val loss: 1.0906362533569336
Epoch 120, training loss: 89.79165649414062 = 1.0892317295074463 + 10.0 * 8.87024211883545
Epoch 120, val loss: 1.0904887914657593
Epoch 130, training loss: 89.54691314697266 = 1.089030146598816 + 10.0 * 8.845788955688477
Epoch 130, val loss: 1.0903112888336182
Epoch 140, training loss: 89.3367919921875 = 1.0888489484786987 + 10.0 * 8.824793815612793
Epoch 140, val loss: 1.0901525020599365
Epoch 150, training loss: 89.13884735107422 = 1.0886321067810059 + 10.0 * 8.805021286010742
Epoch 150, val loss: 1.0899602174758911
Epoch 160, training loss: 88.98771667480469 = 1.0884320735931396 + 10.0 * 8.789928436279297
Epoch 160, val loss: 1.0897886753082275
Epoch 170, training loss: 88.8695068359375 = 1.0882185697555542 + 10.0 * 8.778128623962402
Epoch 170, val loss: 1.0896036624908447
Epoch 180, training loss: 88.74337768554688 = 1.087984323501587 + 10.0 * 8.765539169311523
Epoch 180, val loss: 1.0894020795822144
Epoch 190, training loss: 88.62749481201172 = 1.087754249572754 + 10.0 * 8.753973960876465
Epoch 190, val loss: 1.0892009735107422
Epoch 200, training loss: 88.54883575439453 = 1.0875284671783447 + 10.0 * 8.746129989624023
Epoch 200, val loss: 1.0890028476715088
Epoch 210, training loss: 88.5356674194336 = 1.087288498878479 + 10.0 * 8.744837760925293
Epoch 210, val loss: 1.0887891054153442
Epoch 220, training loss: 88.43449401855469 = 1.087052583694458 + 10.0 * 8.73474407196045
Epoch 220, val loss: 1.0885841846466064
Epoch 230, training loss: 88.35726165771484 = 1.0867996215820312 + 10.0 * 8.727046012878418
Epoch 230, val loss: 1.0883700847625732
Epoch 240, training loss: 88.30927276611328 = 1.0865343809127808 + 10.0 * 8.722273826599121
Epoch 240, val loss: 1.0881400108337402
Epoch 250, training loss: 88.2699203491211 = 1.0862702131271362 + 10.0 * 8.718364715576172
Epoch 250, val loss: 1.0879104137420654
Epoch 260, training loss: 88.2367172241211 = 1.0859977006912231 + 10.0 * 8.715071678161621
Epoch 260, val loss: 1.0876715183258057
Epoch 270, training loss: 88.21295928955078 = 1.085722804069519 + 10.0 * 8.712723731994629
Epoch 270, val loss: 1.087429165840149
Epoch 280, training loss: 88.18629455566406 = 1.0854268074035645 + 10.0 * 8.710086822509766
Epoch 280, val loss: 1.0871673822402954
Epoch 290, training loss: 88.18097686767578 = 1.08513343334198 + 10.0 * 8.70958423614502
Epoch 290, val loss: 1.0869097709655762
Epoch 300, training loss: 88.16267395019531 = 1.0848217010498047 + 10.0 * 8.707784652709961
Epoch 300, val loss: 1.086631417274475
Epoch 310, training loss: 88.1375732421875 = 1.0844975709915161 + 10.0 * 8.705307960510254
Epoch 310, val loss: 1.086348533630371
Epoch 320, training loss: 88.12139129638672 = 1.0841730833053589 + 10.0 * 8.70372200012207
Epoch 320, val loss: 1.0860670804977417
Epoch 330, training loss: 88.13562774658203 = 1.083843469619751 + 10.0 * 8.705178260803223
Epoch 330, val loss: 1.0857751369476318
Epoch 340, training loss: 88.1264419555664 = 1.0834892988204956 + 10.0 * 8.70429515838623
Epoch 340, val loss: 1.0854605436325073
Epoch 350, training loss: 88.09031677246094 = 1.0831319093704224 + 10.0 * 8.700718879699707
Epoch 350, val loss: 1.0851397514343262
Epoch 360, training loss: 88.15313720703125 = 1.0827577114105225 + 10.0 * 8.707037925720215
Epoch 360, val loss: 1.0848088264465332
Epoch 370, training loss: 88.19743347167969 = 1.082397222518921 + 10.0 * 8.711503982543945
Epoch 370, val loss: 1.0845102071762085
Epoch 380, training loss: 88.13549041748047 = 1.082001805305481 + 10.0 * 8.70534896850586
Epoch 380, val loss: 1.0841606855392456
Epoch 390, training loss: 88.13313293457031 = 1.0816245079040527 + 10.0 * 8.705150604248047
Epoch 390, val loss: 1.0838311910629272
Epoch 400, training loss: 88.17098999023438 = 1.081222653388977 + 10.0 * 8.708976745605469
Epoch 400, val loss: 1.083475947380066
Epoch 410, training loss: 88.1790542602539 = 1.080802321434021 + 10.0 * 8.70982551574707
Epoch 410, val loss: 1.0831061601638794
Epoch 420, training loss: 88.19706726074219 = 1.0803768634796143 + 10.0 * 8.711668968200684
Epoch 420, val loss: 1.082730770111084
Epoch 430, training loss: 88.23483276367188 = 1.0799459218978882 + 10.0 * 8.71548843383789
Epoch 430, val loss: 1.0823514461517334
Epoch 440, training loss: 88.21491241455078 = 1.079491376876831 + 10.0 * 8.713541984558105
Epoch 440, val loss: 1.081945776939392
Epoch 450, training loss: 88.25981903076172 = 1.0790448188781738 + 10.0 * 8.718076705932617
Epoch 450, val loss: 1.0815622806549072
Epoch 460, training loss: 88.27312469482422 = 1.078590750694275 + 10.0 * 8.719453811645508
Epoch 460, val loss: 1.0811569690704346
Epoch 470, training loss: 88.29032135009766 = 1.0781075954437256 + 10.0 * 8.721220970153809
Epoch 470, val loss: 1.0807334184646606
Epoch 480, training loss: 88.30440521240234 = 1.0776015520095825 + 10.0 * 8.722681045532227
Epoch 480, val loss: 1.08029043674469
Epoch 490, training loss: 88.40274810791016 = 1.0771559476852417 + 10.0 * 8.732559204101562
Epoch 490, val loss: 1.0798918008804321
Epoch 500, training loss: 88.3076171875 = 1.0766409635543823 + 10.0 * 8.723097801208496
Epoch 500, val loss: 1.079448938369751
Epoch 510, training loss: 88.29558563232422 = 1.0761196613311768 + 10.0 * 8.721946716308594
Epoch 510, val loss: 1.0789811611175537
Epoch 520, training loss: 88.30077362060547 = 1.0756264925003052 + 10.0 * 8.722514152526855
Epoch 520, val loss: 1.0785489082336426
Epoch 530, training loss: 88.36827850341797 = 1.0751287937164307 + 10.0 * 8.729314804077148
Epoch 530, val loss: 1.0781129598617554
Epoch 540, training loss: 88.3914794921875 = 1.0745813846588135 + 10.0 * 8.731689453125
Epoch 540, val loss: 1.0776143074035645
Epoch 550, training loss: 88.42633819580078 = 1.0740300416946411 + 10.0 * 8.735231399536133
Epoch 550, val loss: 1.0771338939666748
Epoch 560, training loss: 88.4575424194336 = 1.073470950126648 + 10.0 * 8.738407135009766
Epoch 560, val loss: 1.0766452550888062
Epoch 570, training loss: 88.49028015136719 = 1.0729001760482788 + 10.0 * 8.741738319396973
Epoch 570, val loss: 1.0761395692825317
Epoch 580, training loss: 88.42955017089844 = 1.072309970855713 + 10.0 * 8.735723495483398
Epoch 580, val loss: 1.0755927562713623
Epoch 590, training loss: 88.4679946899414 = 1.0717138051986694 + 10.0 * 8.739627838134766
Epoch 590, val loss: 1.0750856399536133
Epoch 600, training loss: 88.50106811523438 = 1.0711199045181274 + 10.0 * 8.74299430847168
Epoch 600, val loss: 1.0745527744293213
Epoch 610, training loss: 88.53269958496094 = 1.0705244541168213 + 10.0 * 8.746217727661133
Epoch 610, val loss: 1.0740188360214233
Epoch 620, training loss: 88.56689453125 = 1.069911241531372 + 10.0 * 8.749698638916016
Epoch 620, val loss: 1.0734728574752808
Epoch 630, training loss: 88.6129379272461 = 1.06928551197052 + 10.0 * 8.754365921020508
Epoch 630, val loss: 1.0729082822799683
Epoch 640, training loss: 88.60735321044922 = 1.0686143636703491 + 10.0 * 8.753873825073242
Epoch 640, val loss: 1.072286605834961
Epoch 650, training loss: 88.59537506103516 = 1.0679043531417847 + 10.0 * 8.752747535705566
Epoch 650, val loss: 1.071679949760437
Epoch 660, training loss: 88.56100463867188 = 1.067229151725769 + 10.0 * 8.749377250671387
Epoch 660, val loss: 1.0710583925247192
Epoch 670, training loss: 88.57398223876953 = 1.0665463209152222 + 10.0 * 8.750743865966797
Epoch 670, val loss: 1.0704563856124878
Epoch 680, training loss: 88.63249969482422 = 1.0658597946166992 + 10.0 * 8.756664276123047
Epoch 680, val loss: 1.0698432922363281
Epoch 690, training loss: 88.64762115478516 = 1.0651453733444214 + 10.0 * 8.758247375488281
Epoch 690, val loss: 1.0691967010498047
Epoch 700, training loss: 88.6900863647461 = 1.0644351243972778 + 10.0 * 8.762564659118652
Epoch 700, val loss: 1.068563461303711
Epoch 710, training loss: 88.73004913330078 = 1.0636849403381348 + 10.0 * 8.766636848449707
Epoch 710, val loss: 1.0678805112838745
Epoch 720, training loss: 88.77950286865234 = 1.0629162788391113 + 10.0 * 8.771657943725586
Epoch 720, val loss: 1.0672041177749634
Epoch 730, training loss: 88.81640625 = 1.0621267557144165 + 10.0 * 8.77542781829834
Epoch 730, val loss: 1.0664901733398438
Epoch 740, training loss: 88.8475112915039 = 1.0613316297531128 + 10.0 * 8.778617858886719
Epoch 740, val loss: 1.0657854080200195
Epoch 750, training loss: 88.86328887939453 = 1.0604946613311768 + 10.0 * 8.780279159545898
Epoch 750, val loss: 1.0650163888931274
Epoch 760, training loss: 88.90339660644531 = 1.0596089363098145 + 10.0 * 8.784379005432129
Epoch 760, val loss: 1.0642300844192505
Epoch 770, training loss: 88.87532806396484 = 1.0586901903152466 + 10.0 * 8.78166389465332
Epoch 770, val loss: 1.0634007453918457
Epoch 780, training loss: 88.8863296508789 = 1.0577946901321411 + 10.0 * 8.782854080200195
Epoch 780, val loss: 1.0625842809677124
Epoch 790, training loss: 88.82007598876953 = 1.0568037033081055 + 10.0 * 8.776327133178711
Epoch 790, val loss: 1.0617070198059082
Epoch 800, training loss: 88.924560546875 = 1.055840253829956 + 10.0 * 8.786871910095215
Epoch 800, val loss: 1.0608198642730713
Epoch 810, training loss: 88.93965911865234 = 1.0548005104064941 + 10.0 * 8.788485527038574
Epoch 810, val loss: 1.0598808526992798
Epoch 820, training loss: 88.98174285888672 = 1.0536980628967285 + 10.0 * 8.792804718017578
Epoch 820, val loss: 1.0588778257369995
Epoch 830, training loss: 89.04637145996094 = 1.052587866783142 + 10.0 * 8.799378395080566
Epoch 830, val loss: 1.0578635931015015
Epoch 840, training loss: 89.06432342529297 = 1.0514165163040161 + 10.0 * 8.801290512084961
Epoch 840, val loss: 1.056796908378601
Epoch 850, training loss: 89.09294891357422 = 1.0502026081085205 + 10.0 * 8.804274559020996
Epoch 850, val loss: 1.0556913614273071
Epoch 860, training loss: 89.10253143310547 = 1.0489895343780518 + 10.0 * 8.805354118347168
Epoch 860, val loss: 1.0545923709869385
Epoch 870, training loss: 89.1684341430664 = 1.0477707386016846 + 10.0 * 8.812067031860352
Epoch 870, val loss: 1.0534741878509521
Epoch 880, training loss: 89.15243530273438 = 1.0465037822723389 + 10.0 * 8.810593605041504
Epoch 880, val loss: 1.0523202419281006
Epoch 890, training loss: 89.20892333984375 = 1.0452078580856323 + 10.0 * 8.81637191772461
Epoch 890, val loss: 1.0511484146118164
Epoch 900, training loss: 89.23707580566406 = 1.043896198272705 + 10.0 * 8.819317817687988
Epoch 900, val loss: 1.0499507188796997
Epoch 910, training loss: 89.27495574951172 = 1.042547583580017 + 10.0 * 8.823240280151367
Epoch 910, val loss: 1.0487173795700073
Epoch 920, training loss: 89.17852020263672 = 1.0410919189453125 + 10.0 * 8.813742637634277
Epoch 920, val loss: 1.0473674535751343
Epoch 930, training loss: 89.20706939697266 = 1.0398603677749634 + 10.0 * 8.816720962524414
Epoch 930, val loss: 1.046286940574646
Epoch 940, training loss: 89.25044250488281 = 1.0384674072265625 + 10.0 * 8.821197509765625
Epoch 940, val loss: 1.045029878616333
Epoch 950, training loss: 89.309326171875 = 1.037010908126831 + 10.0 * 8.827231407165527
Epoch 950, val loss: 1.0436972379684448
Epoch 960, training loss: 89.34282684326172 = 1.0355149507522583 + 10.0 * 8.830731391906738
Epoch 960, val loss: 1.0423473119735718
Epoch 970, training loss: 89.36077880859375 = 1.0339994430541992 + 10.0 * 8.832677841186523
Epoch 970, val loss: 1.0409570932388306
Epoch 980, training loss: 89.3798599243164 = 1.0324722528457642 + 10.0 * 8.834738731384277
Epoch 980, val loss: 1.0395790338516235
Epoch 990, training loss: 89.4353256225586 = 1.0309405326843262 + 10.0 * 8.840438842773438
Epoch 990, val loss: 1.0381746292114258
Epoch 1000, training loss: 89.41024780273438 = 1.0293586254119873 + 10.0 * 8.838088989257812
Epoch 1000, val loss: 1.0367122888565063
Epoch 1010, training loss: 89.41959381103516 = 1.0277003049850464 + 10.0 * 8.839189529418945
Epoch 1010, val loss: 1.0352016687393188
Epoch 1020, training loss: 89.44517517089844 = 1.026031494140625 + 10.0 * 8.841914176940918
Epoch 1020, val loss: 1.033674716949463
Epoch 1030, training loss: 89.49313354492188 = 1.0243587493896484 + 10.0 * 8.846877098083496
Epoch 1030, val loss: 1.0321166515350342
Epoch 1040, training loss: 89.4712142944336 = 1.0225839614868164 + 10.0 * 8.844862937927246
Epoch 1040, val loss: 1.0305135250091553
Epoch 1050, training loss: 89.54278564453125 = 1.0208312273025513 + 10.0 * 8.852195739746094
Epoch 1050, val loss: 1.028913140296936
Epoch 1060, training loss: 89.52997589111328 = 1.0190349817276 + 10.0 * 8.851094245910645
Epoch 1060, val loss: 1.027279019355774
Epoch 1070, training loss: 89.54845428466797 = 1.017221212387085 + 10.0 * 8.853123664855957
Epoch 1070, val loss: 1.0256277322769165
Epoch 1080, training loss: 89.61504364013672 = 1.0153855085372925 + 10.0 * 8.859965324401855
Epoch 1080, val loss: 1.0239605903625488
Epoch 1090, training loss: 89.60659790039062 = 1.013504981994629 + 10.0 * 8.859309196472168
Epoch 1090, val loss: 1.0222538709640503
Epoch 1100, training loss: 89.57738494873047 = 1.0116225481033325 + 10.0 * 8.856576919555664
Epoch 1100, val loss: 1.0205471515655518
Epoch 1110, training loss: 89.66902923583984 = 1.0097347497940063 + 10.0 * 8.86592960357666
Epoch 1110, val loss: 1.0188254117965698
Epoch 1120, training loss: 89.63492584228516 = 1.0078306198120117 + 10.0 * 8.862709999084473
Epoch 1120, val loss: 1.0170897245407104
Epoch 1130, training loss: 89.64433288574219 = 1.005813717842102 + 10.0 * 8.863851547241211
Epoch 1130, val loss: 1.015255093574524
Epoch 1140, training loss: 89.5932388305664 = 1.0039286613464355 + 10.0 * 8.858930587768555
Epoch 1140, val loss: 1.0135356187820435
Epoch 1150, training loss: 89.64464569091797 = 1.0019644498825073 + 10.0 * 8.86426830291748
Epoch 1150, val loss: 1.0117779970169067
Epoch 1160, training loss: 89.67979431152344 = 0.9999734163284302 + 10.0 * 8.867981910705566
Epoch 1160, val loss: 1.0099929571151733
Epoch 1170, training loss: 89.71886444091797 = 0.997994065284729 + 10.0 * 8.872087478637695
Epoch 1170, val loss: 1.0082145929336548
Epoch 1180, training loss: 89.72866821289062 = 0.9959964156150818 + 10.0 * 8.87326717376709
Epoch 1180, val loss: 1.006434679031372
Epoch 1190, training loss: 89.74822235107422 = 0.9939877390861511 + 10.0 * 8.875423431396484
Epoch 1190, val loss: 1.0046091079711914
Epoch 1200, training loss: 89.6588134765625 = 0.9920390844345093 + 10.0 * 8.866677284240723
Epoch 1200, val loss: 1.002889633178711
Epoch 1210, training loss: 89.3707504272461 = 0.9899292588233948 + 10.0 * 8.838082313537598
Epoch 1210, val loss: 1.000964879989624
Epoch 1220, training loss: 89.44499206542969 = 0.9879635572433472 + 10.0 * 8.845703125
Epoch 1220, val loss: 0.9992000460624695
Epoch 1230, training loss: 89.49488830566406 = 0.9860546588897705 + 10.0 * 8.850883483886719
Epoch 1230, val loss: 0.997440755367279
Epoch 1240, training loss: 89.70983123779297 = 0.9840611219406128 + 10.0 * 8.872576713562012
Epoch 1240, val loss: 0.9956716895103455
Epoch 1250, training loss: 89.6048355102539 = 0.982030987739563 + 10.0 * 8.862279891967773
Epoch 1250, val loss: 0.9938525557518005
Epoch 1260, training loss: 89.6773910522461 = 0.9800029397010803 + 10.0 * 8.869738578796387
Epoch 1260, val loss: 0.9920092225074768
Epoch 1270, training loss: 89.74658966064453 = 0.9779832363128662 + 10.0 * 8.876860618591309
Epoch 1270, val loss: 0.9901914000511169
Epoch 1280, training loss: 89.8318099975586 = 0.9760010838508606 + 10.0 * 8.885581016540527
Epoch 1280, val loss: 0.9883840680122375
Epoch 1290, training loss: 89.70710754394531 = 0.9739201664924622 + 10.0 * 8.873318672180176
Epoch 1290, val loss: 0.9864415526390076
Epoch 1300, training loss: 89.75366973876953 = 0.9718548059463501 + 10.0 * 8.878181457519531
Epoch 1300, val loss: 0.9846257567405701
Epoch 1310, training loss: 89.50007629394531 = 0.9698439836502075 + 10.0 * 8.853023529052734
Epoch 1310, val loss: 0.9827486276626587
Epoch 1320, training loss: 89.65128326416016 = 0.9678947925567627 + 10.0 * 8.868338584899902
Epoch 1320, val loss: 0.9809868931770325
Epoch 1330, training loss: 89.7033462524414 = 0.9658592939376831 + 10.0 * 8.873748779296875
Epoch 1330, val loss: 0.9791507124900818
Epoch 1340, training loss: 89.73976135253906 = 0.9638563394546509 + 10.0 * 8.87759017944336
Epoch 1340, val loss: 0.9773501753807068
Epoch 1350, training loss: 89.80182647705078 = 0.9617969989776611 + 10.0 * 8.884002685546875
Epoch 1350, val loss: 0.975480854511261
Epoch 1360, training loss: 89.86656951904297 = 0.959710419178009 + 10.0 * 8.89068603515625
Epoch 1360, val loss: 0.9735842943191528
Epoch 1370, training loss: 89.91835021972656 = 0.9575927257537842 + 10.0 * 8.896075248718262
Epoch 1370, val loss: 0.9716576933860779
Epoch 1380, training loss: 89.8757095336914 = 0.9554319381713867 + 10.0 * 8.892027854919434
Epoch 1380, val loss: 0.9697077870368958
Epoch 1390, training loss: 89.94420623779297 = 0.9532976150512695 + 10.0 * 8.899090766906738
Epoch 1390, val loss: 0.9677592515945435
Epoch 1400, training loss: 89.99884796142578 = 0.9511617422103882 + 10.0 * 8.904767990112305
Epoch 1400, val loss: 0.9658247232437134
Epoch 1410, training loss: 89.99504089355469 = 0.9490541219711304 + 10.0 * 8.9045991897583
Epoch 1410, val loss: 0.9639231562614441
Epoch 1420, training loss: 90.04049682617188 = 0.9470304846763611 + 10.0 * 8.909346580505371
Epoch 1420, val loss: 0.9620645046234131
Epoch 1430, training loss: 90.05589294433594 = 0.9449282288551331 + 10.0 * 8.911096572875977
Epoch 1430, val loss: 0.9601660966873169
Epoch 1440, training loss: 90.10124969482422 = 0.9428189396858215 + 10.0 * 8.91584300994873
Epoch 1440, val loss: 0.958225667476654
Epoch 1450, training loss: 90.00821685791016 = 0.9407458901405334 + 10.0 * 8.906747817993164
Epoch 1450, val loss: 0.956353485584259
Epoch 1460, training loss: 90.05127716064453 = 0.9386376738548279 + 10.0 * 8.911264419555664
Epoch 1460, val loss: 0.9544482827186584
Epoch 1470, training loss: 90.12771606445312 = 0.9365515112876892 + 10.0 * 8.919116020202637
Epoch 1470, val loss: 0.9525357484817505
Epoch 1480, training loss: 90.15186309814453 = 0.934478223323822 + 10.0 * 8.921738624572754
Epoch 1480, val loss: 0.9506516456604004
Epoch 1490, training loss: 90.1416015625 = 0.93242347240448 + 10.0 * 8.920917510986328
Epoch 1490, val loss: 0.9487866163253784
Epoch 1500, training loss: 90.19163513183594 = 0.9303852915763855 + 10.0 * 8.926125526428223
Epoch 1500, val loss: 0.9469369649887085
Epoch 1510, training loss: 90.1991195678711 = 0.928345263004303 + 10.0 * 8.927077293395996
Epoch 1510, val loss: 0.9450622797012329
Epoch 1520, training loss: 90.17899322509766 = 0.9262800216674805 + 10.0 * 8.925271987915039
Epoch 1520, val loss: 0.9431885480880737
Epoch 1530, training loss: 90.19232940673828 = 0.9243268966674805 + 10.0 * 8.926800727844238
Epoch 1530, val loss: 0.9414032101631165
Epoch 1540, training loss: 90.23826599121094 = 0.9223312735557556 + 10.0 * 8.93159294128418
Epoch 1540, val loss: 0.9396021366119385
Epoch 1550, training loss: 90.2066650390625 = 0.9202907681465149 + 10.0 * 8.928637504577637
Epoch 1550, val loss: 0.9377278685569763
Epoch 1560, training loss: 90.28128051757812 = 0.9182954430580139 + 10.0 * 8.936298370361328
Epoch 1560, val loss: 0.9359286427497864
Epoch 1570, training loss: 90.27753448486328 = 0.9163298010826111 + 10.0 * 8.936120986938477
Epoch 1570, val loss: 0.9341394305229187
Epoch 1580, training loss: 90.28446960449219 = 0.914361298084259 + 10.0 * 8.937010765075684
Epoch 1580, val loss: 0.9323503971099854
Epoch 1590, training loss: 90.31609344482422 = 0.9123921394348145 + 10.0 * 8.940370559692383
Epoch 1590, val loss: 0.9305604100227356
Epoch 1600, training loss: 90.3438949584961 = 0.9104418158531189 + 10.0 * 8.94334602355957
Epoch 1600, val loss: 0.9287942051887512
Epoch 1610, training loss: 90.3521728515625 = 0.9085006713867188 + 10.0 * 8.944367408752441
Epoch 1610, val loss: 0.9270144104957581
Epoch 1620, training loss: 90.33881378173828 = 0.9065618515014648 + 10.0 * 8.943224906921387
Epoch 1620, val loss: 0.9252488613128662
Epoch 1630, training loss: 90.35061645507812 = 0.9046281576156616 + 10.0 * 8.944599151611328
Epoch 1630, val loss: 0.9235057234764099
Epoch 1640, training loss: 90.35442352294922 = 0.9027044177055359 + 10.0 * 8.945172309875488
Epoch 1640, val loss: 0.9217628240585327
Epoch 1650, training loss: 90.41201782226562 = 0.9008238911628723 + 10.0 * 8.951119422912598
Epoch 1650, val loss: 0.9200490117073059
Epoch 1660, training loss: 90.4078369140625 = 0.8989306092262268 + 10.0 * 8.95089054107666
Epoch 1660, val loss: 0.9183385372161865
Epoch 1670, training loss: 90.4493637084961 = 0.8970407247543335 + 10.0 * 8.955232620239258
Epoch 1670, val loss: 0.9166216850280762
Epoch 1680, training loss: 90.3866195678711 = 0.8951539993286133 + 10.0 * 8.949146270751953
Epoch 1680, val loss: 0.9149113893508911
Epoch 1690, training loss: 90.39279174804688 = 0.8933578133583069 + 10.0 * 8.949943542480469
Epoch 1690, val loss: 0.9132989048957825
Epoch 1700, training loss: 90.45620727539062 = 0.8915441036224365 + 10.0 * 8.956466674804688
Epoch 1700, val loss: 0.9116671085357666
Epoch 1710, training loss: 90.48332977294922 = 0.88970547914505 + 10.0 * 8.959362983703613
Epoch 1710, val loss: 0.9100209474563599
Epoch 1720, training loss: 90.50450897216797 = 0.8878843188285828 + 10.0 * 8.961662292480469
Epoch 1720, val loss: 0.9083749651908875
Epoch 1730, training loss: 90.49392700195312 = 0.8860851526260376 + 10.0 * 8.960783958435059
Epoch 1730, val loss: 0.9067608714103699
Epoch 1740, training loss: 90.47472381591797 = 0.8842759132385254 + 10.0 * 8.959044456481934
Epoch 1740, val loss: 0.9051380753517151
Epoch 1750, training loss: 90.5120620727539 = 0.8826358914375305 + 10.0 * 8.962942123413086
Epoch 1750, val loss: 0.9036328196525574
Epoch 1760, training loss: 90.46131896972656 = 0.8810231685638428 + 10.0 * 8.958029747009277
Epoch 1760, val loss: 0.9021393656730652
Epoch 1770, training loss: 90.3831558227539 = 0.8793636560440063 + 10.0 * 8.950379371643066
Epoch 1770, val loss: 0.9007177352905273
Epoch 1780, training loss: 90.31562042236328 = 0.8777243494987488 + 10.0 * 8.9437894821167
Epoch 1780, val loss: 0.8992855548858643
Epoch 1790, training loss: 90.33293914794922 = 0.8760080933570862 + 10.0 * 8.945693016052246
Epoch 1790, val loss: 0.897739589214325
Epoch 1800, training loss: 90.36622619628906 = 0.8742750883102417 + 10.0 * 8.94919490814209
Epoch 1800, val loss: 0.8961954712867737
Epoch 1810, training loss: 90.4452896118164 = 0.8726091384887695 + 10.0 * 8.957267761230469
Epoch 1810, val loss: 0.8947229981422424
Epoch 1820, training loss: 90.51168823242188 = 0.8709686994552612 + 10.0 * 8.964071273803711
Epoch 1820, val loss: 0.8932600021362305
Epoch 1830, training loss: 90.49662780761719 = 0.8693333864212036 + 10.0 * 8.962729454040527
Epoch 1830, val loss: 0.8917964100837708
Epoch 1840, training loss: 90.5140609741211 = 0.8677470684051514 + 10.0 * 8.964632034301758
Epoch 1840, val loss: 0.8903665542602539
Epoch 1850, training loss: 90.50807189941406 = 0.8661572933197021 + 10.0 * 8.964191436767578
Epoch 1850, val loss: 0.8889510035514832
Epoch 1860, training loss: 90.58448028564453 = 0.8645994067192078 + 10.0 * 8.9719877243042
Epoch 1860, val loss: 0.8875598907470703
Epoch 1870, training loss: 90.58292388916016 = 0.8630359172821045 + 10.0 * 8.971988677978516
Epoch 1870, val loss: 0.886186420917511
Epoch 1880, training loss: 90.59161376953125 = 0.8615143895149231 + 10.0 * 8.973010063171387
Epoch 1880, val loss: 0.8848251104354858
Epoch 1890, training loss: 90.55695343017578 = 0.8600005507469177 + 10.0 * 8.969695091247559
Epoch 1890, val loss: 0.8834803700447083
Epoch 1900, training loss: 90.51638793945312 = 0.858535647392273 + 10.0 * 8.965785026550293
Epoch 1900, val loss: 0.8821461200714111
Epoch 1910, training loss: 90.54851531982422 = 0.857048511505127 + 10.0 * 8.969146728515625
Epoch 1910, val loss: 0.8808215856552124
Epoch 1920, training loss: 90.61189270019531 = 0.8555594086647034 + 10.0 * 8.97563362121582
Epoch 1920, val loss: 0.8795266151428223
Epoch 1930, training loss: 90.66154479980469 = 0.8540931940078735 + 10.0 * 8.980745315551758
Epoch 1930, val loss: 0.878254234790802
Epoch 1940, training loss: 90.65182495117188 = 0.8526254296302795 + 10.0 * 8.979920387268066
Epoch 1940, val loss: 0.8769689798355103
Epoch 1950, training loss: 90.63671875 = 0.8511900901794434 + 10.0 * 8.97855281829834
Epoch 1950, val loss: 0.8757180571556091
Epoch 1960, training loss: 90.69055938720703 = 0.8497715592384338 + 10.0 * 8.984079360961914
Epoch 1960, val loss: 0.874464750289917
Epoch 1970, training loss: 90.70606994628906 = 0.8483502864837646 + 10.0 * 8.985772132873535
Epoch 1970, val loss: 0.8732099533081055
Epoch 1980, training loss: 90.58417510986328 = 0.8469971418380737 + 10.0 * 8.97371768951416
Epoch 1980, val loss: 0.8720042705535889
Epoch 1990, training loss: 90.4847412109375 = 0.8456581234931946 + 10.0 * 8.963908195495605
Epoch 1990, val loss: 0.87079918384552
Epoch 2000, training loss: 90.57817077636719 = 0.8443418145179749 + 10.0 * 8.973382949829102
Epoch 2000, val loss: 0.8696506023406982
Epoch 2010, training loss: 90.56720733642578 = 0.8429797291755676 + 10.0 * 8.97242259979248
Epoch 2010, val loss: 0.8685003519058228
Epoch 2020, training loss: 90.64532470703125 = 0.8416323661804199 + 10.0 * 8.980369567871094
Epoch 2020, val loss: 0.8673129081726074
Epoch 2030, training loss: 90.71302795410156 = 0.8402866125106812 + 10.0 * 8.987274169921875
Epoch 2030, val loss: 0.8661261796951294
Epoch 2040, training loss: 90.74632263183594 = 0.8389429450035095 + 10.0 * 8.990737915039062
Epoch 2040, val loss: 0.8649680614471436
Epoch 2050, training loss: 90.71566772460938 = 0.8376193642616272 + 10.0 * 8.987805366516113
Epoch 2050, val loss: 0.863837718963623
Epoch 2060, training loss: 90.72702026367188 = 0.8363443613052368 + 10.0 * 8.989068031311035
Epoch 2060, val loss: 0.8627370595932007
Epoch 2070, training loss: 90.77235412597656 = 0.8350524306297302 + 10.0 * 8.993730545043945
Epoch 2070, val loss: 0.8616354465484619
Epoch 2080, training loss: 90.77483367919922 = 0.8337756395339966 + 10.0 * 8.994105339050293
Epoch 2080, val loss: 0.8605506420135498
Epoch 2090, training loss: 90.79674530029297 = 0.8325273394584656 + 10.0 * 8.996421813964844
Epoch 2090, val loss: 0.8594687581062317
Epoch 2100, training loss: 90.81197357177734 = 0.8312768936157227 + 10.0 * 8.998069763183594
Epoch 2100, val loss: 0.8584073781967163
Epoch 2110, training loss: 90.81268310546875 = 0.8300485610961914 + 10.0 * 8.998263359069824
Epoch 2110, val loss: 0.8573517799377441
Epoch 2120, training loss: 90.21771240234375 = 0.8284589648246765 + 10.0 * 8.938924789428711
Epoch 2120, val loss: 0.8557559847831726
Epoch 2130, training loss: 88.43109130859375 = 0.8281127214431763 + 10.0 * 8.760297775268555
Epoch 2130, val loss: 0.8559789061546326
Epoch 2140, training loss: 88.37483978271484 = 0.8278833031654358 + 10.0 * 8.754695892333984
Epoch 2140, val loss: 0.8554123640060425
Epoch 2150, training loss: 88.83412170410156 = 0.8268496990203857 + 10.0 * 8.800726890563965
Epoch 2150, val loss: 0.8538219928741455
Epoch 2160, training loss: 88.2276382446289 = 0.8257910013198853 + 10.0 * 8.740184783935547
Epoch 2160, val loss: 0.8529346585273743
Epoch 2170, training loss: 89.055419921875 = 0.8248415589332581 + 10.0 * 8.823057174682617
Epoch 2170, val loss: 0.8524072170257568
Epoch 2180, training loss: 89.0936279296875 = 0.8236365914344788 + 10.0 * 8.826998710632324
Epoch 2180, val loss: 0.851636528968811
Epoch 2190, training loss: 89.53822326660156 = 0.8224855065345764 + 10.0 * 8.871573448181152
Epoch 2190, val loss: 0.8506871461868286
Epoch 2200, training loss: 89.64779663085938 = 0.8212588429450989 + 10.0 * 8.882654190063477
Epoch 2200, val loss: 0.8496325016021729
Epoch 2210, training loss: 89.69607543945312 = 0.8201091289520264 + 10.0 * 8.88759708404541
Epoch 2210, val loss: 0.8486137986183167
Epoch 2220, training loss: 89.89092254638672 = 0.8189934492111206 + 10.0 * 8.907193183898926
Epoch 2220, val loss: 0.84764164686203
Epoch 2230, training loss: 89.95459747314453 = 0.8178504705429077 + 10.0 * 8.913675308227539
Epoch 2230, val loss: 0.8467525243759155
Epoch 2240, training loss: 90.09994506835938 = 0.8168333768844604 + 10.0 * 8.928311347961426
Epoch 2240, val loss: 0.8459205627441406
Epoch 2250, training loss: 90.17387390136719 = 0.815777599811554 + 10.0 * 8.935809135437012
Epoch 2250, val loss: 0.8450274467468262
Epoch 2260, training loss: 90.1773681640625 = 0.8147333264350891 + 10.0 * 8.936263084411621
Epoch 2260, val loss: 0.8441690802574158
Epoch 2270, training loss: 90.25552368164062 = 0.8137015104293823 + 10.0 * 8.944182395935059
Epoch 2270, val loss: 0.8433063626289368
Epoch 2280, training loss: 90.34423828125 = 0.8126799464225769 + 10.0 * 8.953155517578125
Epoch 2280, val loss: 0.8424260020256042
Epoch 2290, training loss: 90.40238952636719 = 0.8116780519485474 + 10.0 * 8.959071159362793
Epoch 2290, val loss: 0.8415897488594055
Epoch 2300, training loss: 90.41902923583984 = 0.8106792569160461 + 10.0 * 8.960835456848145
Epoch 2300, val loss: 0.8407702445983887
Epoch 2310, training loss: 90.48584747314453 = 0.8096970915794373 + 10.0 * 8.967615127563477
Epoch 2310, val loss: 0.8399525880813599
Epoch 2320, training loss: 90.53841400146484 = 0.808738112449646 + 10.0 * 8.972967147827148
Epoch 2320, val loss: 0.8391583561897278
Epoch 2330, training loss: 90.55416870117188 = 0.807775616645813 + 10.0 * 8.974638938903809
Epoch 2330, val loss: 0.8383610248565674
Epoch 2340, training loss: 90.56661224365234 = 0.8068142533302307 + 10.0 * 8.975979804992676
Epoch 2340, val loss: 0.8375547528266907
Epoch 2350, training loss: 90.627685546875 = 0.8058772683143616 + 10.0 * 8.98218059539795
Epoch 2350, val loss: 0.8367542028427124
Epoch 2360, training loss: 90.64635467529297 = 0.8049417734146118 + 10.0 * 8.98414134979248
Epoch 2360, val loss: 0.835979700088501
Epoch 2370, training loss: 90.6552963256836 = 0.8040058612823486 + 10.0 * 8.985128402709961
Epoch 2370, val loss: 0.8352174758911133
Epoch 2380, training loss: 90.68762969970703 = 0.8030912280082703 + 10.0 * 8.98845386505127
Epoch 2380, val loss: 0.8344712257385254
Epoch 2390, training loss: 90.70413970947266 = 0.802192747592926 + 10.0 * 8.990194320678711
Epoch 2390, val loss: 0.8337447047233582
Epoch 2400, training loss: 90.71904754638672 = 0.8012912273406982 + 10.0 * 8.991775512695312
Epoch 2400, val loss: 0.8329453468322754
Epoch 2410, training loss: 90.73410034179688 = 0.80062335729599 + 10.0 * 8.993348121643066
Epoch 2410, val loss: 0.8324604034423828
Epoch 2420, training loss: 90.6201400756836 = 0.799748420715332 + 10.0 * 8.982038497924805
Epoch 2420, val loss: 0.8317485451698303
Epoch 2430, training loss: 90.8896484375 = 0.7989275455474854 + 10.0 * 9.009072303771973
Epoch 2430, val loss: 0.831098735332489
Epoch 2440, training loss: 90.93203735351562 = 0.798054575920105 + 10.0 * 9.013398170471191
Epoch 2440, val loss: 0.8304100036621094
Epoch 2450, training loss: 90.96875762939453 = 0.797175407409668 + 10.0 * 9.017158508300781
Epoch 2450, val loss: 0.8297262191772461
Epoch 2460, training loss: 91.00302124023438 = 0.7963084578514099 + 10.0 * 9.020670890808105
Epoch 2460, val loss: 0.8290237784385681
Epoch 2470, training loss: 91.0468978881836 = 0.7954692840576172 + 10.0 * 9.025142669677734
Epoch 2470, val loss: 0.8283514976501465
Epoch 2480, training loss: 91.08686065673828 = 0.7946462035179138 + 10.0 * 9.029221534729004
Epoch 2480, val loss: 0.8276699185371399
Epoch 2490, training loss: 91.12086486816406 = 0.7938302159309387 + 10.0 * 9.032703399658203
Epoch 2490, val loss: 0.8270099759101868
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.5207246376811594
0.8652466855031515
=== training gcn model ===
Epoch 0, training loss: 103.15411376953125 = 1.0944092273712158 + 10.0 * 10.205970764160156
Epoch 0, val loss: 1.0933955907821655
Epoch 10, training loss: 99.02164459228516 = 1.0941609144210815 + 10.0 * 9.79274845123291
Epoch 10, val loss: 1.0931309461593628
Epoch 20, training loss: 96.96416473388672 = 1.0938489437103271 + 10.0 * 9.587031364440918
Epoch 20, val loss: 1.0928107500076294
Epoch 30, training loss: 95.38819885253906 = 1.0935707092285156 + 10.0 * 9.429463386535645
Epoch 30, val loss: 1.092540979385376
Epoch 40, training loss: 94.18389129638672 = 1.0932849645614624 + 10.0 * 9.309061050415039
Epoch 40, val loss: 1.0922602415084839
Epoch 50, training loss: 93.23007202148438 = 1.092989206314087 + 10.0 * 9.213708877563477
Epoch 50, val loss: 1.0919716358184814
Epoch 60, training loss: 92.44318389892578 = 1.092697024345398 + 10.0 * 9.135048866271973
Epoch 60, val loss: 1.0916850566864014
Epoch 70, training loss: 91.78163146972656 = 1.0923912525177002 + 10.0 * 9.068923950195312
Epoch 70, val loss: 1.0913857221603394
Epoch 80, training loss: 91.22965240478516 = 1.0920768976211548 + 10.0 * 9.013757705688477
Epoch 80, val loss: 1.0910775661468506
Epoch 90, training loss: 90.75516510009766 = 1.091762900352478 + 10.0 * 8.966340065002441
Epoch 90, val loss: 1.0907716751098633
Epoch 100, training loss: 90.35104370117188 = 1.0914411544799805 + 10.0 * 8.925960540771484
Epoch 100, val loss: 1.0904568433761597
Epoch 110, training loss: 90.00587463378906 = 1.091107964515686 + 10.0 * 8.89147663116455
Epoch 110, val loss: 1.0901293754577637
Epoch 120, training loss: 89.70462036132812 = 1.0907782316207886 + 10.0 * 8.861384391784668
Epoch 120, val loss: 1.0898056030273438
Epoch 130, training loss: 89.4316635131836 = 1.0904293060302734 + 10.0 * 8.834123611450195
Epoch 130, val loss: 1.0894662141799927
Epoch 140, training loss: 89.20279693603516 = 1.0900766849517822 + 10.0 * 8.811271667480469
Epoch 140, val loss: 1.0891169309616089
Epoch 150, training loss: 88.99824523925781 = 1.0897046327590942 + 10.0 * 8.790853500366211
Epoch 150, val loss: 1.0887500047683716
Epoch 160, training loss: 88.80408477783203 = 1.0893245935440063 + 10.0 * 8.771475791931152
Epoch 160, val loss: 1.0883833169937134
Epoch 170, training loss: 88.6455078125 = 1.0889304876327515 + 10.0 * 8.755658149719238
Epoch 170, val loss: 1.0879942178726196
Epoch 180, training loss: 88.52811431884766 = 1.0885294675827026 + 10.0 * 8.743958473205566
Epoch 180, val loss: 1.0876009464263916
Epoch 190, training loss: 88.42040252685547 = 1.0881139039993286 + 10.0 * 8.73322868347168
Epoch 190, val loss: 1.0871858596801758
Epoch 200, training loss: 88.28951263427734 = 1.08768630027771 + 10.0 * 8.720182418823242
Epoch 200, val loss: 1.0867669582366943
Epoch 210, training loss: 88.17652893066406 = 1.0872559547424316 + 10.0 * 8.708927154541016
Epoch 210, val loss: 1.0863440036773682
Epoch 220, training loss: 88.10098266601562 = 1.0868061780929565 + 10.0 * 8.701417922973633
Epoch 220, val loss: 1.085902214050293
Epoch 230, training loss: 88.01811218261719 = 1.0863419771194458 + 10.0 * 8.693177223205566
Epoch 230, val loss: 1.0854506492614746
Epoch 240, training loss: 87.95594787597656 = 1.0858590602874756 + 10.0 * 8.68700885772705
Epoch 240, val loss: 1.0849709510803223
Epoch 250, training loss: 87.89701843261719 = 1.0853723287582397 + 10.0 * 8.681164741516113
Epoch 250, val loss: 1.0845019817352295
Epoch 260, training loss: 87.86297607421875 = 1.0848822593688965 + 10.0 * 8.67780876159668
Epoch 260, val loss: 1.0840272903442383
Epoch 270, training loss: 87.81291961669922 = 1.084343671798706 + 10.0 * 8.672857284545898
Epoch 270, val loss: 1.0834932327270508
Epoch 280, training loss: 87.8728256225586 = 1.0838209390640259 + 10.0 * 8.678899765014648
Epoch 280, val loss: 1.0830312967300415
Epoch 290, training loss: 87.85631561279297 = 1.0832794904708862 + 10.0 * 8.677303314208984
Epoch 290, val loss: 1.0824546813964844
Epoch 300, training loss: 87.80777740478516 = 1.0827090740203857 + 10.0 * 8.672506332397461
Epoch 300, val loss: 1.0819215774536133
Epoch 310, training loss: 87.74687957763672 = 1.082139015197754 + 10.0 * 8.666474342346191
Epoch 310, val loss: 1.0813757181167603
Epoch 320, training loss: 87.70700073242188 = 1.0815720558166504 + 10.0 * 8.662542343139648
Epoch 320, val loss: 1.0808202028274536
Epoch 330, training loss: 87.72342681884766 = 1.0809708833694458 + 10.0 * 8.66424560546875
Epoch 330, val loss: 1.0802421569824219
Epoch 340, training loss: 87.71368408203125 = 1.0803718566894531 + 10.0 * 8.663331985473633
Epoch 340, val loss: 1.0796717405319214
Epoch 350, training loss: 87.71676635742188 = 1.079760193824768 + 10.0 * 8.663701057434082
Epoch 350, val loss: 1.0790812969207764
Epoch 360, training loss: 87.71417999267578 = 1.0791709423065186 + 10.0 * 8.663500785827637
Epoch 360, val loss: 1.0785118341445923
Epoch 370, training loss: 87.69242858886719 = 1.0785443782806396 + 10.0 * 8.661388397216797
Epoch 370, val loss: 1.077910304069519
Epoch 380, training loss: 87.7115249633789 = 1.0779329538345337 + 10.0 * 8.663358688354492
Epoch 380, val loss: 1.0773264169692993
Epoch 390, training loss: 87.71751403808594 = 1.0773060321807861 + 10.0 * 8.664020538330078
Epoch 390, val loss: 1.0767267942428589
Epoch 400, training loss: 87.72576141357422 = 1.0766794681549072 + 10.0 * 8.664907455444336
Epoch 400, val loss: 1.0761241912841797
Epoch 410, training loss: 87.70382690429688 = 1.076032280921936 + 10.0 * 8.662778854370117
Epoch 410, val loss: 1.0755038261413574
Epoch 420, training loss: 87.71636962890625 = 1.0754027366638184 + 10.0 * 8.66409683227539
Epoch 420, val loss: 1.074904203414917
Epoch 430, training loss: 87.77641296386719 = 1.074739933013916 + 10.0 * 8.670167922973633
Epoch 430, val loss: 1.0742627382278442
Epoch 440, training loss: 87.82122039794922 = 1.0740925073623657 + 10.0 * 8.674713134765625
Epoch 440, val loss: 1.0736517906188965
Epoch 450, training loss: 87.73429870605469 = 1.0734182596206665 + 10.0 * 8.666088104248047
Epoch 450, val loss: 1.0730055570602417
Epoch 460, training loss: 87.74895477294922 = 1.072764277458191 + 10.0 * 8.667619705200195
Epoch 460, val loss: 1.0723799467086792
Epoch 470, training loss: 87.79659271240234 = 1.0721042156219482 + 10.0 * 8.672449111938477
Epoch 470, val loss: 1.0717499256134033
Epoch 480, training loss: 87.79006958007812 = 1.071428894996643 + 10.0 * 8.67186450958252
Epoch 480, val loss: 1.0711032152175903
Epoch 490, training loss: 87.82479095458984 = 1.0707590579986572 + 10.0 * 8.675402641296387
Epoch 490, val loss: 1.070464015007019
Epoch 500, training loss: 87.78849792480469 = 1.0700546503067017 + 10.0 * 8.671844482421875
Epoch 500, val loss: 1.06979501247406
Epoch 510, training loss: 87.84896850585938 = 1.0693682432174683 + 10.0 * 8.677960395812988
Epoch 510, val loss: 1.0691132545471191
Epoch 520, training loss: 87.82618713378906 = 1.0686379671096802 + 10.0 * 8.67575454711914
Epoch 520, val loss: 1.0684269666671753
Epoch 530, training loss: 87.84640502929688 = 1.0679895877838135 + 10.0 * 8.677841186523438
Epoch 530, val loss: 1.0678402185440063
Epoch 540, training loss: 87.8453598022461 = 1.0672844648361206 + 10.0 * 8.677807807922363
Epoch 540, val loss: 1.0671660900115967
Epoch 550, training loss: 87.81214141845703 = 1.0665279626846313 + 10.0 * 8.674561500549316
Epoch 550, val loss: 1.0664557218551636
Epoch 560, training loss: 87.87775421142578 = 1.0658372640609741 + 10.0 * 8.681192398071289
Epoch 560, val loss: 1.0658013820648193
Epoch 570, training loss: 87.92655944824219 = 1.0651123523712158 + 10.0 * 8.686144828796387
Epoch 570, val loss: 1.065111517906189
Epoch 580, training loss: 87.95172882080078 = 1.064375400543213 + 10.0 * 8.688735008239746
Epoch 580, val loss: 1.0644185543060303
Epoch 590, training loss: 87.9219970703125 = 1.0636100769042969 + 10.0 * 8.68583869934082
Epoch 590, val loss: 1.063692331314087
Epoch 600, training loss: 87.98158264160156 = 1.062856912612915 + 10.0 * 8.691872596740723
Epoch 600, val loss: 1.0629851818084717
Epoch 610, training loss: 88.01040649414062 = 1.0620900392532349 + 10.0 * 8.694831848144531
Epoch 610, val loss: 1.0622544288635254
Epoch 620, training loss: 88.0015869140625 = 1.0612822771072388 + 10.0 * 8.69403076171875
Epoch 620, val loss: 1.0615154504776
Epoch 630, training loss: 88.16638946533203 = 1.0604180097579956 + 10.0 * 8.710597038269043
Epoch 630, val loss: 1.0606452226638794
Epoch 640, training loss: 88.21247100830078 = 1.059647798538208 + 10.0 * 8.715282440185547
Epoch 640, val loss: 1.0600101947784424
Epoch 650, training loss: 88.04547882080078 = 1.058899164199829 + 10.0 * 8.698657989501953
Epoch 650, val loss: 1.0592321157455444
Epoch 660, training loss: 88.01850128173828 = 1.0580922365188599 + 10.0 * 8.696041107177734
Epoch 660, val loss: 1.0585113763809204
Epoch 670, training loss: 88.0322265625 = 1.057324767112732 + 10.0 * 8.697489738464355
Epoch 670, val loss: 1.0577867031097412
Epoch 680, training loss: 88.04859161376953 = 1.0564688444137573 + 10.0 * 8.699213027954102
Epoch 680, val loss: 1.0569807291030884
Epoch 690, training loss: 88.11011505126953 = 1.0556734800338745 + 10.0 * 8.7054443359375
Epoch 690, val loss: 1.0562318563461304
Epoch 700, training loss: 88.16513061523438 = 1.0548242330551147 + 10.0 * 8.711030960083008
Epoch 700, val loss: 1.0554367303848267
Epoch 710, training loss: 88.19499206542969 = 1.053961157798767 + 10.0 * 8.714102745056152
Epoch 710, val loss: 1.0546268224716187
Epoch 720, training loss: 88.22259521484375 = 1.0531044006347656 + 10.0 * 8.716948509216309
Epoch 720, val loss: 1.0538264513015747
Epoch 730, training loss: 88.28573608398438 = 1.0522148609161377 + 10.0 * 8.723352432250977
Epoch 730, val loss: 1.0529903173446655
Epoch 740, training loss: 88.2900390625 = 1.0513185262680054 + 10.0 * 8.723872184753418
Epoch 740, val loss: 1.0521411895751953
Epoch 750, training loss: 88.31838989257812 = 1.0503923892974854 + 10.0 * 8.726799964904785
Epoch 750, val loss: 1.0512858629226685
Epoch 760, training loss: 88.3306655883789 = 1.0494626760482788 + 10.0 * 8.728120803833008
Epoch 760, val loss: 1.0504144430160522
Epoch 770, training loss: 88.35303497314453 = 1.0485081672668457 + 10.0 * 8.730452537536621
Epoch 770, val loss: 1.0495233535766602
Epoch 780, training loss: 88.29300689697266 = 1.0475373268127441 + 10.0 * 8.724546432495117
Epoch 780, val loss: 1.0486111640930176
Epoch 790, training loss: 88.35088348388672 = 1.046635389328003 + 10.0 * 8.730424880981445
Epoch 790, val loss: 1.0477683544158936
Epoch 800, training loss: 88.42634582519531 = 1.0456866025924683 + 10.0 * 8.738065719604492
Epoch 800, val loss: 1.0468945503234863
Epoch 810, training loss: 88.46983337402344 = 1.0447213649749756 + 10.0 * 8.742510795593262
Epoch 810, val loss: 1.0459932088851929
Epoch 820, training loss: 88.48950958251953 = 1.0437195301055908 + 10.0 * 8.744579315185547
Epoch 820, val loss: 1.0450736284255981
Epoch 830, training loss: 88.51813507080078 = 1.0427298545837402 + 10.0 * 8.747540473937988
Epoch 830, val loss: 1.04413902759552
Epoch 840, training loss: 88.50144958496094 = 1.0417265892028809 + 10.0 * 8.745972633361816
Epoch 840, val loss: 1.04319167137146
Epoch 850, training loss: 88.53679656982422 = 1.040716290473938 + 10.0 * 8.749608039855957
Epoch 850, val loss: 1.042244791984558
Epoch 860, training loss: 88.54960632324219 = 1.0396746397018433 + 10.0 * 8.750993728637695
Epoch 860, val loss: 1.041273593902588
Epoch 870, training loss: 88.57355499267578 = 1.0386301279067993 + 10.0 * 8.75349235534668
Epoch 870, val loss: 1.0402976274490356
Epoch 880, training loss: 88.57288360595703 = 1.0375694036483765 + 10.0 * 8.753531455993652
Epoch 880, val loss: 1.0392959117889404
Epoch 890, training loss: 88.6479263305664 = 1.0364941358566284 + 10.0 * 8.761143684387207
Epoch 890, val loss: 1.038306713104248
Epoch 900, training loss: 88.55632781982422 = 1.0354236364364624 + 10.0 * 8.752090454101562
Epoch 900, val loss: 1.0373096466064453
Epoch 910, training loss: 88.60433959960938 = 1.0343345403671265 + 10.0 * 8.757000923156738
Epoch 910, val loss: 1.036301612854004
Epoch 920, training loss: 88.65692138671875 = 1.0332506895065308 + 10.0 * 8.762367248535156
Epoch 920, val loss: 1.0352928638458252
Epoch 930, training loss: 88.67375183105469 = 1.032118797302246 + 10.0 * 8.76416301727295
Epoch 930, val loss: 1.03424072265625
Epoch 940, training loss: 88.69475555419922 = 1.0309700965881348 + 10.0 * 8.766378402709961
Epoch 940, val loss: 1.0331640243530273
Epoch 950, training loss: 88.69978332519531 = 1.0298372507095337 + 10.0 * 8.76699447631836
Epoch 950, val loss: 1.0321074724197388
Epoch 960, training loss: 88.71492004394531 = 1.0286563634872437 + 10.0 * 8.76862621307373
Epoch 960, val loss: 1.0310089588165283
Epoch 970, training loss: 88.7226333618164 = 1.0274791717529297 + 10.0 * 8.769515037536621
Epoch 970, val loss: 1.0299180746078491
Epoch 980, training loss: 88.74176788330078 = 1.026265025138855 + 10.0 * 8.771550178527832
Epoch 980, val loss: 1.028780221939087
Epoch 990, training loss: 88.75184631347656 = 1.0250205993652344 + 10.0 * 8.772682189941406
Epoch 990, val loss: 1.027624487876892
Epoch 1000, training loss: 88.72551727294922 = 1.023715853691101 + 10.0 * 8.770180702209473
Epoch 1000, val loss: 1.0263968706130981
Epoch 1010, training loss: 88.80326843261719 = 1.0224956274032593 + 10.0 * 8.778077125549316
Epoch 1010, val loss: 1.0252593755722046
Epoch 1020, training loss: 88.80071258544922 = 1.0212132930755615 + 10.0 * 8.777950286865234
Epoch 1020, val loss: 1.0240954160690308
Epoch 1030, training loss: 88.82710266113281 = 1.0199124813079834 + 10.0 * 8.780718803405762
Epoch 1030, val loss: 1.0228792428970337
Epoch 1040, training loss: 88.83110809326172 = 1.0185976028442383 + 10.0 * 8.781250953674316
Epoch 1040, val loss: 1.021641731262207
Epoch 1050, training loss: 88.85530853271484 = 1.0172499418258667 + 10.0 * 8.783805847167969
Epoch 1050, val loss: 1.020398736000061
Epoch 1060, training loss: 88.83562469482422 = 1.0158686637878418 + 10.0 * 8.781975746154785
Epoch 1060, val loss: 1.019112467765808
Epoch 1070, training loss: 88.84844207763672 = 1.0144453048706055 + 10.0 * 8.78339958190918
Epoch 1070, val loss: 1.0177940130233765
Epoch 1080, training loss: 88.90357208251953 = 1.0129951238632202 + 10.0 * 8.789057731628418
Epoch 1080, val loss: 1.0164451599121094
Epoch 1090, training loss: 88.93936157226562 = 1.0114766359329224 + 10.0 * 8.7927885055542
Epoch 1090, val loss: 1.0150121450424194
Epoch 1100, training loss: 88.90171813964844 = 1.0098659992218018 + 10.0 * 8.789185523986816
Epoch 1100, val loss: 1.0135189294815063
Epoch 1110, training loss: 88.93812561035156 = 1.008254051208496 + 10.0 * 8.792986869812012
Epoch 1110, val loss: 1.0120054483413696
Epoch 1120, training loss: 88.9715347290039 = 1.0066298246383667 + 10.0 * 8.796490669250488
Epoch 1120, val loss: 1.0104843378067017
Epoch 1130, training loss: 88.98435974121094 = 1.0049883127212524 + 10.0 * 8.797937393188477
Epoch 1130, val loss: 1.0089552402496338
Epoch 1140, training loss: 89.00985717773438 = 1.0033073425292969 + 10.0 * 8.800655364990234
Epoch 1140, val loss: 1.0074037313461304
Epoch 1150, training loss: 89.04808807373047 = 1.001624345779419 + 10.0 * 8.804646492004395
Epoch 1150, val loss: 1.0058138370513916
Epoch 1160, training loss: 89.05438995361328 = 0.9998718500137329 + 10.0 * 8.805452346801758
Epoch 1160, val loss: 1.0042061805725098
Epoch 1170, training loss: 89.00579071044922 = 0.9981151223182678 + 10.0 * 8.80076789855957
Epoch 1170, val loss: 1.0025646686553955
Epoch 1180, training loss: 88.93712615966797 = 0.9962312579154968 + 10.0 * 8.794089317321777
Epoch 1180, val loss: 1.0008563995361328
Epoch 1190, training loss: 88.89555358886719 = 0.9944949150085449 + 10.0 * 8.790105819702148
Epoch 1190, val loss: 0.9992024302482605
Epoch 1200, training loss: 88.92166137695312 = 0.9927456974983215 + 10.0 * 8.792891502380371
Epoch 1200, val loss: 0.9975565671920776
Epoch 1210, training loss: 88.87290954589844 = 0.9909511804580688 + 10.0 * 8.788195610046387
Epoch 1210, val loss: 0.9959033727645874
Epoch 1220, training loss: 88.94268798828125 = 0.9891384243965149 + 10.0 * 8.795354843139648
Epoch 1220, val loss: 0.9941948056221008
Epoch 1230, training loss: 89.00296783447266 = 0.9872554540634155 + 10.0 * 8.801570892333984
Epoch 1230, val loss: 0.9924425482749939
Epoch 1240, training loss: 89.0562515258789 = 0.9853472113609314 + 10.0 * 8.807090759277344
Epoch 1240, val loss: 0.9906669855117798
Epoch 1250, training loss: 89.08908081054688 = 0.9834012985229492 + 10.0 * 8.810567855834961
Epoch 1250, val loss: 0.9888501763343811
Epoch 1260, training loss: 89.12001037597656 = 0.9814160466194153 + 10.0 * 8.813859939575195
Epoch 1260, val loss: 0.986992597579956
Epoch 1270, training loss: 89.13005828857422 = 0.97939133644104 + 10.0 * 8.81506633758545
Epoch 1270, val loss: 0.9851139783859253
Epoch 1280, training loss: 89.17140197753906 = 0.977360725402832 + 10.0 * 8.819403648376465
Epoch 1280, val loss: 0.9832155108451843
Epoch 1290, training loss: 89.18057250976562 = 0.9752827882766724 + 10.0 * 8.820528984069824
Epoch 1290, val loss: 0.9812889099121094
Epoch 1300, training loss: 89.20655822753906 = 0.973185122013092 + 10.0 * 8.82333755493164
Epoch 1300, val loss: 0.9793245792388916
Epoch 1310, training loss: 89.22994232177734 = 0.9710886478424072 + 10.0 * 8.825884819030762
Epoch 1310, val loss: 0.9773447513580322
Epoch 1320, training loss: 89.24320220947266 = 0.9689028859138489 + 10.0 * 8.82742977142334
Epoch 1320, val loss: 0.9753300547599792
Epoch 1330, training loss: 89.27539825439453 = 0.9667150378227234 + 10.0 * 8.8308687210083
Epoch 1330, val loss: 0.9732921719551086
Epoch 1340, training loss: 89.2928695678711 = 0.9644977450370789 + 10.0 * 8.832837104797363
Epoch 1340, val loss: 0.9712159037590027
Epoch 1350, training loss: 89.30026245117188 = 0.962253987789154 + 10.0 * 8.833800315856934
Epoch 1350, val loss: 0.9691004157066345
Epoch 1360, training loss: 89.25019836425781 = 0.9599186182022095 + 10.0 * 8.829028129577637
Epoch 1360, val loss: 0.9669148921966553
Epoch 1370, training loss: 89.28736877441406 = 0.9575823545455933 + 10.0 * 8.832979202270508
Epoch 1370, val loss: 0.9647473692893982
Epoch 1380, training loss: 89.27922058105469 = 0.9551506042480469 + 10.0 * 8.832406997680664
Epoch 1380, val loss: 0.9624608755111694
Epoch 1390, training loss: 89.27986907958984 = 0.9526579976081848 + 10.0 * 8.832720756530762
Epoch 1390, val loss: 0.9601389765739441
Epoch 1400, training loss: 89.33843231201172 = 0.9501697421073914 + 10.0 * 8.838826179504395
Epoch 1400, val loss: 0.9577963352203369
Epoch 1410, training loss: 89.3531494140625 = 0.9476511478424072 + 10.0 * 8.84054946899414
Epoch 1410, val loss: 0.9554389119148254
Epoch 1420, training loss: 89.37214660644531 = 0.94509357213974 + 10.0 * 8.842705726623535
Epoch 1420, val loss: 0.9530419707298279
Epoch 1430, training loss: 89.42276763916016 = 0.9425407648086548 + 10.0 * 8.8480224609375
Epoch 1430, val loss: 0.950633704662323
Epoch 1440, training loss: 89.36288452148438 = 0.9399200677871704 + 10.0 * 8.842296600341797
Epoch 1440, val loss: 0.9481902718544006
Epoch 1450, training loss: 89.3924331665039 = 0.9373412728309631 + 10.0 * 8.84550952911377
Epoch 1450, val loss: 0.9457534551620483
Epoch 1460, training loss: 89.4618911743164 = 0.9347319602966309 + 10.0 * 8.852716445922852
Epoch 1460, val loss: 0.9433050155639648
Epoch 1470, training loss: 89.43866729736328 = 0.9320762753486633 + 10.0 * 8.850659370422363
Epoch 1470, val loss: 0.9407870769500732
Epoch 1480, training loss: 89.39482879638672 = 0.9293894171714783 + 10.0 * 8.84654426574707
Epoch 1480, val loss: 0.9382880330085754
Epoch 1490, training loss: 89.45549011230469 = 0.926730215549469 + 10.0 * 8.852876663208008
Epoch 1490, val loss: 0.9357991218566895
Epoch 1500, training loss: 89.4766616821289 = 0.9240317344665527 + 10.0 * 8.855262756347656
Epoch 1500, val loss: 0.933277428150177
Epoch 1510, training loss: 89.54138946533203 = 0.9213336110115051 + 10.0 * 8.862005233764648
Epoch 1510, val loss: 0.9307573437690735
Epoch 1520, training loss: 89.55319213867188 = 0.9185971617698669 + 10.0 * 8.863459587097168
Epoch 1520, val loss: 0.9282141327857971
Epoch 1530, training loss: 89.55667877197266 = 0.9158718585968018 + 10.0 * 8.864080429077148
Epoch 1530, val loss: 0.9256528615951538
Epoch 1540, training loss: 89.56583404541016 = 0.9131064414978027 + 10.0 * 8.865272521972656
Epoch 1540, val loss: 0.9230773448944092
Epoch 1550, training loss: 89.58769226074219 = 0.910323977470398 + 10.0 * 8.86773681640625
Epoch 1550, val loss: 0.9204895496368408
Epoch 1560, training loss: 89.58636474609375 = 0.9075427055358887 + 10.0 * 8.86788272857666
Epoch 1560, val loss: 0.9178977012634277
Epoch 1570, training loss: 89.63662719726562 = 0.9047513604164124 + 10.0 * 8.873187065124512
Epoch 1570, val loss: 0.9152746796607971
Epoch 1580, training loss: 89.62396240234375 = 0.9019142389297485 + 10.0 * 8.872204780578613
Epoch 1580, val loss: 0.9126499891281128
Epoch 1590, training loss: 89.5885238647461 = 0.899035632610321 + 10.0 * 8.868948936462402
Epoch 1590, val loss: 0.9099546670913696
Epoch 1600, training loss: 89.58182525634766 = 0.8962104320526123 + 10.0 * 8.868561744689941
Epoch 1600, val loss: 0.9073187112808228
Epoch 1610, training loss: 89.65247344970703 = 0.8935007452964783 + 10.0 * 8.875897407531738
Epoch 1610, val loss: 0.9047811627388
Epoch 1620, training loss: 89.66413116455078 = 0.8906198740005493 + 10.0 * 8.877351760864258
Epoch 1620, val loss: 0.9021120071411133
Epoch 1630, training loss: 89.73211669921875 = 0.8877695798873901 + 10.0 * 8.884434700012207
Epoch 1630, val loss: 0.8994433283805847
Epoch 1640, training loss: 89.73613739013672 = 0.8848832845687866 + 10.0 * 8.885126113891602
Epoch 1640, val loss: 0.896779477596283
Epoch 1650, training loss: 89.75392150878906 = 0.8820003271102905 + 10.0 * 8.887191772460938
Epoch 1650, val loss: 0.8940895199775696
Epoch 1660, training loss: 89.77581787109375 = 0.879132091999054 + 10.0 * 8.889668464660645
Epoch 1660, val loss: 0.8914228081703186
Epoch 1670, training loss: 89.7943115234375 = 0.876249372959137 + 10.0 * 8.891805648803711
Epoch 1670, val loss: 0.8887482285499573
Epoch 1680, training loss: 89.80305480957031 = 0.8733713626861572 + 10.0 * 8.89296817779541
Epoch 1680, val loss: 0.8860820531845093
Epoch 1690, training loss: 89.83838653564453 = 0.8704829216003418 + 10.0 * 8.896790504455566
Epoch 1690, val loss: 0.8834050893783569
Epoch 1700, training loss: 89.81636810302734 = 0.8676037192344666 + 10.0 * 8.894876480102539
Epoch 1700, val loss: 0.8807160258293152
Epoch 1710, training loss: 89.81002807617188 = 0.8647034764289856 + 10.0 * 8.894533157348633
Epoch 1710, val loss: 0.8780290484428406
Epoch 1720, training loss: 89.75347137451172 = 0.8617771863937378 + 10.0 * 8.889169692993164
Epoch 1720, val loss: 0.8753353953361511
Epoch 1730, training loss: 89.82162475585938 = 0.8589498400688171 + 10.0 * 8.896267890930176
Epoch 1730, val loss: 0.8727138638496399
Epoch 1740, training loss: 89.87339782714844 = 0.8560596108436584 + 10.0 * 8.901734352111816
Epoch 1740, val loss: 0.8700279593467712
Epoch 1750, training loss: 89.89352416992188 = 0.8531363606452942 + 10.0 * 8.90403938293457
Epoch 1750, val loss: 0.8673253655433655
Epoch 1760, training loss: 89.90935516357422 = 0.8502548933029175 + 10.0 * 8.905909538269043
Epoch 1760, val loss: 0.8646349906921387
Epoch 1770, training loss: 89.91578674316406 = 0.847344696521759 + 10.0 * 8.906844139099121
Epoch 1770, val loss: 0.8619470000267029
Epoch 1780, training loss: 89.94048309326172 = 0.8444295525550842 + 10.0 * 8.909605026245117
Epoch 1780, val loss: 0.8592475652694702
Epoch 1790, training loss: 89.91441345214844 = 0.8415253162384033 + 10.0 * 8.907289505004883
Epoch 1790, val loss: 0.8565885424613953
Epoch 1800, training loss: 89.96048736572266 = 0.8386679291725159 + 10.0 * 8.912181854248047
Epoch 1800, val loss: 0.8539102673530579
Epoch 1810, training loss: 89.97852325439453 = 0.8357518911361694 + 10.0 * 8.914277076721191
Epoch 1810, val loss: 0.8512380123138428
Epoch 1820, training loss: 89.9708251953125 = 0.8328994512557983 + 10.0 * 8.913792610168457
Epoch 1820, val loss: 0.8485745191574097
Epoch 1830, training loss: 90.01673126220703 = 0.8300161361694336 + 10.0 * 8.918671607971191
Epoch 1830, val loss: 0.8459380865097046
Epoch 1840, training loss: 90.03933715820312 = 0.8271435499191284 + 10.0 * 8.921219825744629
Epoch 1840, val loss: 0.8433036208152771
Epoch 1850, training loss: 90.00979614257812 = 0.824274480342865 + 10.0 * 8.91855239868164
Epoch 1850, val loss: 0.8406744003295898
Epoch 1860, training loss: 89.96569061279297 = 0.8214529156684875 + 10.0 * 8.914423942565918
Epoch 1860, val loss: 0.8380566239356995
Epoch 1870, training loss: 89.97967529296875 = 0.8186876177787781 + 10.0 * 8.916098594665527
Epoch 1870, val loss: 0.83555668592453
Epoch 1880, training loss: 90.01969146728516 = 0.8158262968063354 + 10.0 * 8.92038631439209
Epoch 1880, val loss: 0.8329175710678101
Epoch 1890, training loss: 90.06855010986328 = 0.8130150437355042 + 10.0 * 8.925553321838379
Epoch 1890, val loss: 0.8303418755531311
Epoch 1900, training loss: 90.07640075683594 = 0.810188889503479 + 10.0 * 8.926621437072754
Epoch 1900, val loss: 0.8277388215065002
Epoch 1910, training loss: 90.02766418457031 = 0.8074712753295898 + 10.0 * 8.922019004821777
Epoch 1910, val loss: 0.825282096862793
Epoch 1920, training loss: 90.06356048583984 = 0.8047604560852051 + 10.0 * 8.92587947845459
Epoch 1920, val loss: 0.8228005766868591
Epoch 1930, training loss: 90.109130859375 = 0.8019673824310303 + 10.0 * 8.930716514587402
Epoch 1930, val loss: 0.8202504515647888
Epoch 1940, training loss: 90.10550689697266 = 0.7991631627082825 + 10.0 * 8.930634498596191
Epoch 1940, val loss: 0.8176529407501221
Epoch 1950, training loss: 90.09794616699219 = 0.7964227199554443 + 10.0 * 8.93015193939209
Epoch 1950, val loss: 0.8151881098747253
Epoch 1960, training loss: 90.15946197509766 = 0.7937267422676086 + 10.0 * 8.93657398223877
Epoch 1960, val loss: 0.8127679228782654
Epoch 1970, training loss: 90.12225341796875 = 0.7910566926002502 + 10.0 * 8.933119773864746
Epoch 1970, val loss: 0.8102939128875732
Epoch 1980, training loss: 90.17063903808594 = 0.7883934378623962 + 10.0 * 8.938224792480469
Epoch 1980, val loss: 0.8078781962394714
Epoch 1990, training loss: 90.20734405517578 = 0.7857400178909302 + 10.0 * 8.942159652709961
Epoch 1990, val loss: 0.8054687976837158
Epoch 2000, training loss: 90.1700439453125 = 0.7830957770347595 + 10.0 * 8.938694953918457
Epoch 2000, val loss: 0.8030773401260376
Epoch 2010, training loss: 90.01321411132812 = 0.7805110812187195 + 10.0 * 8.923270225524902
Epoch 2010, val loss: 0.8006925582885742
Epoch 2020, training loss: 89.95722198486328 = 0.7779109477996826 + 10.0 * 8.91793155670166
Epoch 2020, val loss: 0.7983564138412476
Epoch 2030, training loss: 90.00865936279297 = 0.7754273414611816 + 10.0 * 8.923322677612305
Epoch 2030, val loss: 0.7960954904556274
Epoch 2040, training loss: 90.11991119384766 = 0.772878885269165 + 10.0 * 8.93470287322998
Epoch 2040, val loss: 0.7937648892402649
Epoch 2050, training loss: 90.20673370361328 = 0.7703490853309631 + 10.0 * 8.943638801574707
Epoch 2050, val loss: 0.7914577126502991
Epoch 2060, training loss: 90.23963928222656 = 0.7678079009056091 + 10.0 * 8.947183609008789
Epoch 2060, val loss: 0.7891527414321899
Epoch 2070, training loss: 90.23517608642578 = 0.765266478061676 + 10.0 * 8.946990966796875
Epoch 2070, val loss: 0.786846935749054
Epoch 2080, training loss: 90.26504516601562 = 0.7627823948860168 + 10.0 * 8.950226783752441
Epoch 2080, val loss: 0.7846041917800903
Epoch 2090, training loss: 90.27798461914062 = 0.7603003978729248 + 10.0 * 8.95176887512207
Epoch 2090, val loss: 0.7823367118835449
Epoch 2100, training loss: 90.28766632080078 = 0.7578532695770264 + 10.0 * 8.952981948852539
Epoch 2100, val loss: 0.7800895571708679
Epoch 2110, training loss: 90.27364349365234 = 0.7554063200950623 + 10.0 * 8.951823234558105
Epoch 2110, val loss: 0.7778781056404114
Epoch 2120, training loss: 90.29951477050781 = 0.7529956698417664 + 10.0 * 8.954651832580566
Epoch 2120, val loss: 0.7756835222244263
Epoch 2130, training loss: 90.30897521972656 = 0.750592052936554 + 10.0 * 8.955838203430176
Epoch 2130, val loss: 0.7735002040863037
Epoch 2140, training loss: 90.28478240966797 = 0.7482283711433411 + 10.0 * 8.953655242919922
Epoch 2140, val loss: 0.7713479399681091
Epoch 2150, training loss: 90.32009887695312 = 0.7458648681640625 + 10.0 * 8.957423210144043
Epoch 2150, val loss: 0.7691970467567444
Epoch 2160, training loss: 90.3319320678711 = 0.7435858249664307 + 10.0 * 8.958834648132324
Epoch 2160, val loss: 0.7671226859092712
Epoch 2170, training loss: 90.32811737060547 = 0.7412849068641663 + 10.0 * 8.958683013916016
Epoch 2170, val loss: 0.7650415897369385
Epoch 2180, training loss: 90.34344482421875 = 0.7389808893203735 + 10.0 * 8.96044635772705
Epoch 2180, val loss: 0.7629525065422058
Epoch 2190, training loss: 90.25543975830078 = 0.736690104007721 + 10.0 * 8.951875686645508
Epoch 2190, val loss: 0.7608645558357239
Epoch 2200, training loss: 90.21868133544922 = 0.7347334623336792 + 10.0 * 8.948394775390625
Epoch 2200, val loss: 0.7589788436889648
Epoch 2210, training loss: 90.17561340332031 = 0.7325490713119507 + 10.0 * 8.944306373596191
Epoch 2210, val loss: 0.7570874094963074
Epoch 2220, training loss: 90.0722427368164 = 0.7304777503013611 + 10.0 * 8.934176445007324
Epoch 2220, val loss: 0.7552898526191711
Epoch 2230, training loss: 90.12322235107422 = 0.7283117175102234 + 10.0 * 8.939491271972656
Epoch 2230, val loss: 0.7532436847686768
Epoch 2240, training loss: 90.18345642089844 = 0.7261378765106201 + 10.0 * 8.945732116699219
Epoch 2240, val loss: 0.7512460947036743
Epoch 2250, training loss: 90.25651550292969 = 0.7239813208580017 + 10.0 * 8.953252792358398
Epoch 2250, val loss: 0.7492877244949341
Epoch 2260, training loss: 90.33867645263672 = 0.7218323349952698 + 10.0 * 8.961684226989746
Epoch 2260, val loss: 0.747347891330719
Epoch 2270, training loss: 90.37571716308594 = 0.7196560502052307 + 10.0 * 8.965605735778809
Epoch 2270, val loss: 0.745373547077179
Epoch 2280, training loss: 90.34550476074219 = 0.7174869775772095 + 10.0 * 8.962801933288574
Epoch 2280, val loss: 0.7433900833129883
Epoch 2290, training loss: 90.39059448242188 = 0.7153121829032898 + 10.0 * 8.967528343200684
Epoch 2290, val loss: 0.7413991689682007
Epoch 2300, training loss: 90.40057373046875 = 0.713165819644928 + 10.0 * 8.968740463256836
Epoch 2300, val loss: 0.7394316792488098
Epoch 2310, training loss: 90.40158081054688 = 0.711028516292572 + 10.0 * 8.96905517578125
Epoch 2310, val loss: 0.7375022172927856
Epoch 2320, training loss: 90.42430877685547 = 0.7089310884475708 + 10.0 * 8.971537590026855
Epoch 2320, val loss: 0.7356133460998535
Epoch 2330, training loss: 90.45735931396484 = 0.7068232297897339 + 10.0 * 8.975053787231445
Epoch 2330, val loss: 0.7337010502815247
Epoch 2340, training loss: 90.37661743164062 = 0.7051832675933838 + 10.0 * 8.967143058776855
Epoch 2340, val loss: 0.7323156595230103
Epoch 2350, training loss: 90.07942199707031 = 0.7032661437988281 + 10.0 * 8.937615394592285
Epoch 2350, val loss: 0.7305328249931335
Epoch 2360, training loss: 90.35418701171875 = 0.7015807032585144 + 10.0 * 8.96526050567627
Epoch 2360, val loss: 0.7290653586387634
Epoch 2370, training loss: 90.13500213623047 = 0.6997649073600769 + 10.0 * 8.943523406982422
Epoch 2370, val loss: 0.7274363040924072
Epoch 2380, training loss: 90.05957794189453 = 0.6978603005409241 + 10.0 * 8.936171531677246
Epoch 2380, val loss: 0.725669801235199
Epoch 2390, training loss: 90.22750854492188 = 0.6960607171058655 + 10.0 * 8.953145027160645
Epoch 2390, val loss: 0.7240943312644958
Epoch 2400, training loss: 90.35015869140625 = 0.6943213939666748 + 10.0 * 8.965583801269531
Epoch 2400, val loss: 0.722562313079834
Epoch 2410, training loss: 90.35101318359375 = 0.6924644112586975 + 10.0 * 8.96585464477539
Epoch 2410, val loss: 0.720937192440033
Epoch 2420, training loss: 90.41151428222656 = 0.690594494342804 + 10.0 * 8.972091674804688
Epoch 2420, val loss: 0.719279408454895
Epoch 2430, training loss: 90.48435974121094 = 0.688719630241394 + 10.0 * 8.97956371307373
Epoch 2430, val loss: 0.7176164388656616
Epoch 2440, training loss: 90.53412628173828 = 0.686860203742981 + 10.0 * 8.984726905822754
Epoch 2440, val loss: 0.7159813642501831
Epoch 2450, training loss: 90.5179443359375 = 0.6850365400314331 + 10.0 * 8.983290672302246
Epoch 2450, val loss: 0.7143668532371521
Epoch 2460, training loss: 90.56050872802734 = 0.6832625865936279 + 10.0 * 8.987724304199219
Epoch 2460, val loss: 0.7127933502197266
Epoch 2470, training loss: 90.59841918945312 = 0.6814994812011719 + 10.0 * 8.991691589355469
Epoch 2470, val loss: 0.7112517952919006
Epoch 2480, training loss: 90.59748077392578 = 0.6797564029693604 + 10.0 * 8.991772651672363
Epoch 2480, val loss: 0.7097157835960388
Epoch 2490, training loss: 90.58328247070312 = 0.6780327558517456 + 10.0 * 8.990525245666504
Epoch 2490, val loss: 0.7082073092460632
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7207246376811594
0.8641599652249512
The final CL Acc:0.68821, 0.12560, The final GNN Acc:0.86486, 0.00050
Begin epxeriment: noisy_level: 0.2 cont_weight: 10 epoch: der1:0.3 der2:0.4 dfr1:0.4 dfr2:0.3
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=2500, cl_num_layers=2, cl_num_proj_hidden=64, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=10.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.4, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=32, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, noisy_level=0.2, num_hidden=64, num_proj_hidden=64, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=40, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 88648])
add edge: torch.Size([2, 106292])
remove edge: torch.Size([2, 70568])
updated graph: torch.Size([2, 88212])
=== Noisy graph ===
/home/project-robust-contrastive/Robust-Contrastive-Learning/model.py:227: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 103.45513916015625 = 1.0965113639831543 + 10.0 * 10.235862731933594
Epoch 0, val loss: 1.0960352420806885
Epoch 10, training loss: 99.54930877685547 = 1.0963958501815796 + 10.0 * 9.845291137695312
Epoch 10, val loss: 1.0958976745605469
Epoch 20, training loss: 97.81854248046875 = 1.096245288848877 + 10.0 * 9.672229766845703
Epoch 20, val loss: 1.095740556716919
Epoch 30, training loss: 96.47618103027344 = 1.0961283445358276 + 10.0 * 9.538004875183105
Epoch 30, val loss: 1.0956082344055176
Epoch 40, training loss: 95.4314193725586 = 1.0960114002227783 + 10.0 * 9.433541297912598
Epoch 40, val loss: 1.0954759120941162
Epoch 50, training loss: 94.57927703857422 = 1.0958921909332275 + 10.0 * 9.34833812713623
Epoch 50, val loss: 1.0953422784805298
Epoch 60, training loss: 93.8495101928711 = 1.095772624015808 + 10.0 * 9.275373458862305
Epoch 60, val loss: 1.095206618309021
Epoch 70, training loss: 93.21558380126953 = 1.0956507921218872 + 10.0 * 9.211993217468262
Epoch 70, val loss: 1.0950698852539062
Epoch 80, training loss: 92.6610107421875 = 1.0955313444137573 + 10.0 * 9.156548500061035
Epoch 80, val loss: 1.094934105873108
Epoch 90, training loss: 92.17694854736328 = 1.0954132080078125 + 10.0 * 9.108153343200684
Epoch 90, val loss: 1.0947984457015991
Epoch 100, training loss: 91.74945068359375 = 1.0952914953231812 + 10.0 * 9.06541633605957
Epoch 100, val loss: 1.094656229019165
Epoch 110, training loss: 91.38160705566406 = 1.0951545238494873 + 10.0 * 9.028645515441895
Epoch 110, val loss: 1.09450364112854
Epoch 120, training loss: 91.04558563232422 = 1.0950173139572144 + 10.0 * 8.995057106018066
Epoch 120, val loss: 1.0943498611450195
Epoch 130, training loss: 90.77008819580078 = 1.0948874950408936 + 10.0 * 8.967519760131836
Epoch 130, val loss: 1.0941951274871826
Epoch 140, training loss: 90.50704193115234 = 1.094742774963379 + 10.0 * 8.941229820251465
Epoch 140, val loss: 1.0940405130386353
Epoch 150, training loss: 90.27210998535156 = 1.0945900678634644 + 10.0 * 8.917752265930176
Epoch 150, val loss: 1.0938702821731567
Epoch 160, training loss: 90.07061004638672 = 1.0944461822509766 + 10.0 * 8.897616386413574
Epoch 160, val loss: 1.0937129259109497
Epoch 170, training loss: 89.89209747314453 = 1.0942916870117188 + 10.0 * 8.879780769348145
Epoch 170, val loss: 1.0935362577438354
Epoch 180, training loss: 89.88455963134766 = 1.094164490699768 + 10.0 * 8.879039764404297
Epoch 180, val loss: 1.0934072732925415
Epoch 190, training loss: 89.7660140991211 = 1.0940560102462769 + 10.0 * 8.867196083068848
Epoch 190, val loss: 1.093266248703003
Epoch 200, training loss: 89.51927947998047 = 1.093798041343689 + 10.0 * 8.842548370361328
Epoch 200, val loss: 1.0929958820343018
Epoch 210, training loss: 89.41133880615234 = 1.0936321020126343 + 10.0 * 8.831769943237305
Epoch 210, val loss: 1.0928115844726562
Epoch 220, training loss: 89.29798126220703 = 1.0934709310531616 + 10.0 * 8.820451736450195
Epoch 220, val loss: 1.0926353931427002
Epoch 230, training loss: 89.22557830810547 = 1.0932868719100952 + 10.0 * 8.81322956085205
Epoch 230, val loss: 1.092437982559204
Epoch 240, training loss: 89.137451171875 = 1.093101143836975 + 10.0 * 8.804434776306152
Epoch 240, val loss: 1.0922318696975708
Epoch 250, training loss: 89.07806396484375 = 1.0929173231124878 + 10.0 * 8.798514366149902
Epoch 250, val loss: 1.0920345783233643
Epoch 260, training loss: 89.03008270263672 = 1.0927226543426514 + 10.0 * 8.793736457824707
Epoch 260, val loss: 1.0918190479278564
Epoch 270, training loss: 88.99702453613281 = 1.0925168991088867 + 10.0 * 8.790451049804688
Epoch 270, val loss: 1.0915961265563965
Epoch 280, training loss: 88.94498443603516 = 1.0923075675964355 + 10.0 * 8.78526782989502
Epoch 280, val loss: 1.0913770198822021
Epoch 290, training loss: 88.89614868164062 = 1.0921013355255127 + 10.0 * 8.780405044555664
Epoch 290, val loss: 1.091154932975769
Epoch 300, training loss: 88.85340881347656 = 1.0918771028518677 + 10.0 * 8.776152610778809
Epoch 300, val loss: 1.0909229516983032
Epoch 310, training loss: 88.8419189453125 = 1.091625690460205 + 10.0 * 8.775029182434082
Epoch 310, val loss: 1.0906492471694946
Epoch 320, training loss: 88.77864074707031 = 1.0914278030395508 + 10.0 * 8.768720626831055
Epoch 320, val loss: 1.090444803237915
Epoch 330, training loss: 88.76172637939453 = 1.0911970138549805 + 10.0 * 8.767053604125977
Epoch 330, val loss: 1.090199589729309
Epoch 340, training loss: 88.72157287597656 = 1.0909554958343506 + 10.0 * 8.7630615234375
Epoch 340, val loss: 1.0899443626403809
Epoch 350, training loss: 88.70203399658203 = 1.0907196998596191 + 10.0 * 8.761131286621094
Epoch 350, val loss: 1.089699387550354
Epoch 360, training loss: 88.68450927734375 = 1.0904560089111328 + 10.0 * 8.759405136108398
Epoch 360, val loss: 1.0894155502319336
Epoch 370, training loss: 88.67781829833984 = 1.0902043581008911 + 10.0 * 8.758761405944824
Epoch 370, val loss: 1.0891568660736084
Epoch 380, training loss: 88.6451644897461 = 1.0899349451065063 + 10.0 * 8.755522727966309
Epoch 380, val loss: 1.0888781547546387
Epoch 390, training loss: 88.62691497802734 = 1.089666485786438 + 10.0 * 8.753725051879883
Epoch 390, val loss: 1.088598608970642
Epoch 400, training loss: 88.6142578125 = 1.0893911123275757 + 10.0 * 8.752486228942871
Epoch 400, val loss: 1.08830988407135
Epoch 410, training loss: 88.60247802734375 = 1.089112401008606 + 10.0 * 8.751337051391602
Epoch 410, val loss: 1.0880212783813477
Epoch 420, training loss: 88.60760498046875 = 1.0888341665267944 + 10.0 * 8.751876831054688
Epoch 420, val loss: 1.0877286195755005
Epoch 430, training loss: 88.61054992675781 = 1.08854079246521 + 10.0 * 8.752201080322266
Epoch 430, val loss: 1.0874340534210205
Epoch 440, training loss: 88.59708404541016 = 1.0882494449615479 + 10.0 * 8.750883102416992
Epoch 440, val loss: 1.0871340036392212
Epoch 450, training loss: 88.58002471923828 = 1.0879535675048828 + 10.0 * 8.749207496643066
Epoch 450, val loss: 1.0868128538131714
Epoch 460, training loss: 88.55730438232422 = 1.0876504182815552 + 10.0 * 8.746965408325195
Epoch 460, val loss: 1.0865018367767334
Epoch 470, training loss: 88.54647827148438 = 1.0873562097549438 + 10.0 * 8.745912551879883
Epoch 470, val loss: 1.0862125158309937
Epoch 480, training loss: 88.85465240478516 = 1.0870782136917114 + 10.0 * 8.77675724029541
Epoch 480, val loss: 1.0858896970748901
Epoch 490, training loss: 88.68367004394531 = 1.086713433265686 + 10.0 * 8.759695053100586
Epoch 490, val loss: 1.0855565071105957
Epoch 500, training loss: 88.69168853759766 = 1.086410641670227 + 10.0 * 8.760527610778809
Epoch 500, val loss: 1.0852229595184326
Epoch 510, training loss: 88.67957305908203 = 1.0861111879348755 + 10.0 * 8.759346008300781
Epoch 510, val loss: 1.0849227905273438
Epoch 520, training loss: 88.62200164794922 = 1.085777759552002 + 10.0 * 8.753622055053711
Epoch 520, val loss: 1.0845848321914673
Epoch 530, training loss: 88.63216400146484 = 1.0854555368423462 + 10.0 * 8.754671096801758
Epoch 530, val loss: 1.0842585563659668
Epoch 540, training loss: 88.6316909790039 = 1.0851233005523682 + 10.0 * 8.754656791687012
Epoch 540, val loss: 1.083914041519165
Epoch 550, training loss: 88.64669036865234 = 1.0847761631011963 + 10.0 * 8.75619125366211
Epoch 550, val loss: 1.0835721492767334
Epoch 560, training loss: 88.68621826171875 = 1.0844553709030151 + 10.0 * 8.760175704956055
Epoch 560, val loss: 1.0832359790802002
Epoch 570, training loss: 88.65087127685547 = 1.08409583568573 + 10.0 * 8.756677627563477
Epoch 570, val loss: 1.0828756093978882
Epoch 580, training loss: 88.658203125 = 1.0837385654449463 + 10.0 * 8.7574462890625
Epoch 580, val loss: 1.0825183391571045
Epoch 590, training loss: 88.75843811035156 = 1.0834016799926758 + 10.0 * 8.76750373840332
Epoch 590, val loss: 1.082181692123413
Epoch 600, training loss: 88.77363586425781 = 1.0828999280929565 + 10.0 * 8.769073486328125
Epoch 600, val loss: 1.0817047357559204
Epoch 610, training loss: 88.86775207519531 = 1.082702398300171 + 10.0 * 8.778505325317383
Epoch 610, val loss: 1.0814679861068726
Epoch 620, training loss: 88.7148208618164 = 1.0822381973266602 + 10.0 * 8.76325798034668
Epoch 620, val loss: 1.0810219049453735
Epoch 630, training loss: 88.67797088623047 = 1.0818809270858765 + 10.0 * 8.75960922241211
Epoch 630, val loss: 1.0806466341018677
Epoch 640, training loss: 88.7626724243164 = 1.0814810991287231 + 10.0 * 8.768118858337402
Epoch 640, val loss: 1.0802606344223022
Epoch 650, training loss: 88.80196380615234 = 1.0810836553573608 + 10.0 * 8.772088050842285
Epoch 650, val loss: 1.07986581325531
Epoch 660, training loss: 88.82958984375 = 1.080675482749939 + 10.0 * 8.77489185333252
Epoch 660, val loss: 1.0794662237167358
Epoch 670, training loss: 88.8554916381836 = 1.0802686214447021 + 10.0 * 8.777522087097168
Epoch 670, val loss: 1.0790592432022095
Epoch 680, training loss: 88.87306213378906 = 1.079831838607788 + 10.0 * 8.779322624206543
Epoch 680, val loss: 1.078631043434143
Epoch 690, training loss: 88.90625762939453 = 1.0794204473495483 + 10.0 * 8.782683372497559
Epoch 690, val loss: 1.0782077312469482
Epoch 700, training loss: 88.92830657958984 = 1.0789897441864014 + 10.0 * 8.784932136535645
Epoch 700, val loss: 1.0777899026870728
Epoch 710, training loss: 88.9676513671875 = 1.078565239906311 + 10.0 * 8.788908004760742
Epoch 710, val loss: 1.0773892402648926
Epoch 720, training loss: 88.98757934570312 = 1.0781487226486206 + 10.0 * 8.790943145751953
Epoch 720, val loss: 1.0769648551940918
Epoch 730, training loss: 88.96968078613281 = 1.077701449394226 + 10.0 * 8.78919792175293
Epoch 730, val loss: 1.0765295028686523
Epoch 740, training loss: 88.9697265625 = 1.0772572755813599 + 10.0 * 8.789247512817383
Epoch 740, val loss: 1.076080083847046
Epoch 750, training loss: 89.00152587890625 = 1.076826572418213 + 10.0 * 8.79246997833252
Epoch 750, val loss: 1.0756587982177734
Epoch 760, training loss: 89.05439758300781 = 1.0764186382293701 + 10.0 * 8.797798156738281
Epoch 760, val loss: 1.0752543210983276
Epoch 770, training loss: 89.07147979736328 = 1.0759828090667725 + 10.0 * 8.79955005645752
Epoch 770, val loss: 1.0748282670974731
Epoch 780, training loss: 89.06487274169922 = 1.0755298137664795 + 10.0 * 8.798933982849121
Epoch 780, val loss: 1.0743870735168457
Epoch 790, training loss: 89.07471466064453 = 1.075095295906067 + 10.0 * 8.799962043762207
Epoch 790, val loss: 1.0739562511444092
Epoch 800, training loss: 89.08505249023438 = 1.074644684791565 + 10.0 * 8.801040649414062
Epoch 800, val loss: 1.0735114812850952
Epoch 810, training loss: 89.10444641113281 = 1.07420015335083 + 10.0 * 8.803024291992188
Epoch 810, val loss: 1.0730700492858887
Epoch 820, training loss: 89.11248779296875 = 1.0737173557281494 + 10.0 * 8.803876876831055
Epoch 820, val loss: 1.0726101398468018
Epoch 830, training loss: 89.13201141357422 = 1.073269009590149 + 10.0 * 8.80587387084961
Epoch 830, val loss: 1.0721526145935059
Epoch 840, training loss: 89.14862060546875 = 1.0727882385253906 + 10.0 * 8.807583808898926
Epoch 840, val loss: 1.071679949760437
Epoch 850, training loss: 89.1345443725586 = 1.0722919702529907 + 10.0 * 8.806225776672363
Epoch 850, val loss: 1.0712063312530518
Epoch 860, training loss: 89.11085510253906 = 1.0718246698379517 + 10.0 * 8.803903579711914
Epoch 860, val loss: 1.0707464218139648
Epoch 870, training loss: 89.13548278808594 = 1.0713578462600708 + 10.0 * 8.806412696838379
Epoch 870, val loss: 1.0702694654464722
Epoch 880, training loss: 89.15634155273438 = 1.0708746910095215 + 10.0 * 8.80854606628418
Epoch 880, val loss: 1.0697906017303467
Epoch 890, training loss: 89.19973754882812 = 1.070382833480835 + 10.0 * 8.812935829162598
Epoch 890, val loss: 1.0693116188049316
Epoch 900, training loss: 89.18756866455078 = 1.0698962211608887 + 10.0 * 8.811767578125
Epoch 900, val loss: 1.0688157081604004
Epoch 910, training loss: 89.21949005126953 = 1.0693899393081665 + 10.0 * 8.815010070800781
Epoch 910, val loss: 1.068328619003296
Epoch 920, training loss: 89.23517608642578 = 1.0688996315002441 + 10.0 * 8.816627502441406
Epoch 920, val loss: 1.0678271055221558
Epoch 930, training loss: 89.28561401367188 = 1.0684058666229248 + 10.0 * 8.821721076965332
Epoch 930, val loss: 1.0673314332962036
Epoch 940, training loss: 89.31726837158203 = 1.0678772926330566 + 10.0 * 8.824938774108887
Epoch 940, val loss: 1.066819190979004
Epoch 950, training loss: 89.2625503540039 = 1.0673693418502808 + 10.0 * 8.819518089294434
Epoch 950, val loss: 1.0663036108016968
Epoch 960, training loss: 89.33828735351562 = 1.0668938159942627 + 10.0 * 8.827138900756836
Epoch 960, val loss: 1.0658230781555176
Epoch 970, training loss: 89.34304809570312 = 1.0663912296295166 + 10.0 * 8.827665328979492
Epoch 970, val loss: 1.065324068069458
Epoch 980, training loss: 89.33490753173828 = 1.0658620595932007 + 10.0 * 8.826904296875
Epoch 980, val loss: 1.0648229122161865
Epoch 990, training loss: 89.37113189697266 = 1.0653525590896606 + 10.0 * 8.830577850341797
Epoch 990, val loss: 1.064307689666748
Epoch 1000, training loss: 89.35935974121094 = 1.0648161172866821 + 10.0 * 8.82945442199707
Epoch 1000, val loss: 1.0637706518173218
Epoch 1010, training loss: 89.40995788574219 = 1.064286470413208 + 10.0 * 8.834567070007324
Epoch 1010, val loss: 1.0632483959197998
Epoch 1020, training loss: 89.46808624267578 = 1.0637706518173218 + 10.0 * 8.840431213378906
Epoch 1020, val loss: 1.062714695930481
Epoch 1030, training loss: 89.44499206542969 = 1.0632150173187256 + 10.0 * 8.838177680969238
Epoch 1030, val loss: 1.0621790885925293
Epoch 1040, training loss: 89.4437255859375 = 1.0626721382141113 + 10.0 * 8.838105201721191
Epoch 1040, val loss: 1.0616428852081299
Epoch 1050, training loss: 89.4754867553711 = 1.0621418952941895 + 10.0 * 8.841334342956543
Epoch 1050, val loss: 1.0611135959625244
Epoch 1060, training loss: 89.52983093261719 = 1.0616015195846558 + 10.0 * 8.846822738647461
Epoch 1060, val loss: 1.0605725049972534
Epoch 1070, training loss: 89.5150146484375 = 1.0610334873199463 + 10.0 * 8.84539794921875
Epoch 1070, val loss: 1.0600053071975708
Epoch 1080, training loss: 89.50849914550781 = 1.0604628324508667 + 10.0 * 8.844803810119629
Epoch 1080, val loss: 1.0594402551651
Epoch 1090, training loss: 89.50608825683594 = 1.0599092245101929 + 10.0 * 8.84461784362793
Epoch 1090, val loss: 1.0588845014572144
Epoch 1100, training loss: 89.55036926269531 = 1.0593515634536743 + 10.0 * 8.849102020263672
Epoch 1100, val loss: 1.0583256483078003
Epoch 1110, training loss: 89.5232162475586 = 1.058767318725586 + 10.0 * 8.846445083618164
Epoch 1110, val loss: 1.0577287673950195
Epoch 1120, training loss: 89.510498046875 = 1.0581917762756348 + 10.0 * 8.845231056213379
Epoch 1120, val loss: 1.0571670532226562
Epoch 1130, training loss: 89.58826446533203 = 1.0576225519180298 + 10.0 * 8.853063583374023
Epoch 1130, val loss: 1.0565983057022095
Epoch 1140, training loss: 89.5521240234375 = 1.057021141052246 + 10.0 * 8.849510192871094
Epoch 1140, val loss: 1.056013584136963
Epoch 1150, training loss: 89.59822845458984 = 1.0564472675323486 + 10.0 * 8.854177474975586
Epoch 1150, val loss: 1.0554195642471313
Epoch 1160, training loss: 89.64117431640625 = 1.0558497905731201 + 10.0 * 8.858532905578613
Epoch 1160, val loss: 1.0548373460769653
Epoch 1170, training loss: 89.66612243652344 = 1.0552798509597778 + 10.0 * 8.861083984375
Epoch 1170, val loss: 1.0542559623718262
Epoch 1180, training loss: 89.64810180664062 = 1.054641604423523 + 10.0 * 8.859346389770508
Epoch 1180, val loss: 1.0536119937896729
Epoch 1190, training loss: 89.65756225585938 = 1.0540847778320312 + 10.0 * 8.860347747802734
Epoch 1190, val loss: 1.053065299987793
Epoch 1200, training loss: 89.67762756347656 = 1.0534703731536865 + 10.0 * 8.86241626739502
Epoch 1200, val loss: 1.0524605512619019
Epoch 1210, training loss: 89.70681762695312 = 1.0528749227523804 + 10.0 * 8.865394592285156
Epoch 1210, val loss: 1.051859974861145
Epoch 1220, training loss: 89.6788330078125 = 1.052241563796997 + 10.0 * 8.862659454345703
Epoch 1220, val loss: 1.0512263774871826
Epoch 1230, training loss: 89.71607208251953 = 1.0516407489776611 + 10.0 * 8.866442680358887
Epoch 1230, val loss: 1.0506178140640259
Epoch 1240, training loss: 89.74890899658203 = 1.0510156154632568 + 10.0 * 8.869789123535156
Epoch 1240, val loss: 1.0500061511993408
Epoch 1250, training loss: 89.75017547607422 = 1.0503888130187988 + 10.0 * 8.869977951049805
Epoch 1250, val loss: 1.0493723154067993
Epoch 1260, training loss: 89.74957275390625 = 1.0497374534606934 + 10.0 * 8.869983673095703
Epoch 1260, val loss: 1.0487455129623413
Epoch 1270, training loss: 89.82952880859375 = 1.0491317510604858 + 10.0 * 8.878039360046387
Epoch 1270, val loss: 1.0481294393539429
Epoch 1280, training loss: 89.69699096679688 = 1.0484668016433716 + 10.0 * 8.864851951599121
Epoch 1280, val loss: 1.0474563837051392
Epoch 1290, training loss: 89.7850341796875 = 1.0477572679519653 + 10.0 * 8.873727798461914
Epoch 1290, val loss: 1.0467486381530762
Epoch 1300, training loss: 89.8381118774414 = 1.0472147464752197 + 10.0 * 8.87908935546875
Epoch 1300, val loss: 1.0462260246276855
Epoch 1310, training loss: 89.81654357910156 = 1.046546459197998 + 10.0 * 8.876999855041504
Epoch 1310, val loss: 1.0455673933029175
Epoch 1320, training loss: 89.75135803222656 = 1.0458643436431885 + 10.0 * 8.870549201965332
Epoch 1320, val loss: 1.0449070930480957
Epoch 1330, training loss: 89.81098937988281 = 1.0452110767364502 + 10.0 * 8.876577377319336
Epoch 1330, val loss: 1.0442469120025635
Epoch 1340, training loss: 89.83534240722656 = 1.0445371866226196 + 10.0 * 8.879079818725586
Epoch 1340, val loss: 1.0435794591903687
Epoch 1350, training loss: 89.87169647216797 = 1.0438488721847534 + 10.0 * 8.882784843444824
Epoch 1350, val loss: 1.0428967475891113
Epoch 1360, training loss: 89.8258056640625 = 1.043170690536499 + 10.0 * 8.878263473510742
Epoch 1360, val loss: 1.0422457456588745
Epoch 1370, training loss: 89.90242767333984 = 1.0424998998641968 + 10.0 * 8.885992050170898
Epoch 1370, val loss: 1.0415993928909302
Epoch 1380, training loss: 89.95472717285156 = 1.0418124198913574 + 10.0 * 8.891291618347168
Epoch 1380, val loss: 1.0409108400344849
Epoch 1390, training loss: 89.98974609375 = 1.0411183834075928 + 10.0 * 8.89486312866211
Epoch 1390, val loss: 1.0402498245239258
Epoch 1400, training loss: 89.98300170898438 = 1.0404324531555176 + 10.0 * 8.894256591796875
Epoch 1400, val loss: 1.0395588874816895
Epoch 1410, training loss: 90.0535659790039 = 1.0397355556488037 + 10.0 * 8.901383399963379
Epoch 1410, val loss: 1.0388672351837158
Epoch 1420, training loss: 90.08260345458984 = 1.039036512374878 + 10.0 * 8.904356002807617
Epoch 1420, val loss: 1.038191795349121
Epoch 1430, training loss: 90.099853515625 = 1.0383304357528687 + 10.0 * 8.906152725219727
Epoch 1430, val loss: 1.0375142097473145
Epoch 1440, training loss: 90.12430572509766 = 1.037625789642334 + 10.0 * 8.90866756439209
Epoch 1440, val loss: 1.0368221998214722
Epoch 1450, training loss: 90.16539764404297 = 1.0369123220443726 + 10.0 * 8.912848472595215
Epoch 1450, val loss: 1.0361207723617554
Epoch 1460, training loss: 90.03951263427734 = 1.0361831188201904 + 10.0 * 8.9003324508667
Epoch 1460, val loss: 1.0354079008102417
Epoch 1470, training loss: 90.0972671508789 = 1.0354641675949097 + 10.0 * 8.906180381774902
Epoch 1470, val loss: 1.0347319841384888
Epoch 1480, training loss: 90.1093978881836 = 1.034759759902954 + 10.0 * 8.907464027404785
Epoch 1480, val loss: 1.0340421199798584
Epoch 1490, training loss: 90.15270233154297 = 1.034061312675476 + 10.0 * 8.911864280700684
Epoch 1490, val loss: 1.0333614349365234
Epoch 1500, training loss: 90.16698455810547 = 1.033345341682434 + 10.0 * 8.913363456726074
Epoch 1500, val loss: 1.0326493978500366
Epoch 1510, training loss: 90.19208526611328 = 1.032607913017273 + 10.0 * 8.915947914123535
Epoch 1510, val loss: 1.0319666862487793
Epoch 1520, training loss: 90.21190643310547 = 1.0318840742111206 + 10.0 * 8.918002128601074
Epoch 1520, val loss: 1.0312427282333374
Epoch 1530, training loss: 90.25296783447266 = 1.0311833620071411 + 10.0 * 8.922178268432617
Epoch 1530, val loss: 1.0305428504943848
Epoch 1540, training loss: 90.262451171875 = 1.0304535627365112 + 10.0 * 8.923199653625488
Epoch 1540, val loss: 1.0298473834991455
Epoch 1550, training loss: 90.28207397460938 = 1.029727578163147 + 10.0 * 8.9252347946167
Epoch 1550, val loss: 1.029151201248169
Epoch 1560, training loss: 90.28036499023438 = 1.0290007591247559 + 10.0 * 8.92513656616211
Epoch 1560, val loss: 1.0284318923950195
Epoch 1570, training loss: 90.30184936523438 = 1.0282528400421143 + 10.0 * 8.927359580993652
Epoch 1570, val loss: 1.0277330875396729
Epoch 1580, training loss: 90.3354263305664 = 1.0275297164916992 + 10.0 * 8.930789947509766
Epoch 1580, val loss: 1.027014970779419
Epoch 1590, training loss: 90.33769989013672 = 1.0267821550369263 + 10.0 * 8.931092262268066
Epoch 1590, val loss: 1.0263060331344604
Epoch 1600, training loss: 90.35952758789062 = 1.0260483026504517 + 10.0 * 8.933347702026367
Epoch 1600, val loss: 1.025601863861084
Epoch 1610, training loss: 90.40633392333984 = 1.0253221988677979 + 10.0 * 8.938100814819336
Epoch 1610, val loss: 1.0248966217041016
Epoch 1620, training loss: 90.24851989746094 = 1.0245195627212524 + 10.0 * 8.922399520874023
Epoch 1620, val loss: 1.0241668224334717
Epoch 1630, training loss: 90.19900512695312 = 1.0238230228424072 + 10.0 * 8.91751766204834
Epoch 1630, val loss: 1.0234630107879639
Epoch 1640, training loss: 90.27433013916016 = 1.0230969190597534 + 10.0 * 8.92512321472168
Epoch 1640, val loss: 1.0227454900741577
Epoch 1650, training loss: 90.34996795654297 = 1.0223751068115234 + 10.0 * 8.932759284973145
Epoch 1650, val loss: 1.0220643281936646
Epoch 1660, training loss: 90.42022705078125 = 1.0216577053070068 + 10.0 * 8.93985652923584
Epoch 1660, val loss: 1.021360158920288
Epoch 1670, training loss: 90.46109008789062 = 1.0209184885025024 + 10.0 * 8.94401741027832
Epoch 1670, val loss: 1.0206657648086548
Epoch 1680, training loss: 90.45652770996094 = 1.0201876163482666 + 10.0 * 8.943634033203125
Epoch 1680, val loss: 1.0199517011642456
Epoch 1690, training loss: 90.46405792236328 = 1.0194380283355713 + 10.0 * 8.944461822509766
Epoch 1690, val loss: 1.0192465782165527
Epoch 1700, training loss: 90.5171890258789 = 1.0187090635299683 + 10.0 * 8.949848175048828
Epoch 1700, val loss: 1.0185736417770386
Epoch 1710, training loss: 90.53302001953125 = 1.01798415184021 + 10.0 * 8.95150375366211
Epoch 1710, val loss: 1.0178630352020264
Epoch 1720, training loss: 90.56317138671875 = 1.017256259918213 + 10.0 * 8.954591751098633
Epoch 1720, val loss: 1.0171781778335571
Epoch 1730, training loss: 90.54114532470703 = 1.0165369510650635 + 10.0 * 8.952460289001465
Epoch 1730, val loss: 1.0165036916732788
Epoch 1740, training loss: 90.5899429321289 = 1.0158199071884155 + 10.0 * 8.957411766052246
Epoch 1740, val loss: 1.0158288478851318
Epoch 1750, training loss: 90.56926727294922 = 1.0150892734527588 + 10.0 * 8.95541763305664
Epoch 1750, val loss: 1.0151314735412598
Epoch 1760, training loss: 90.60838317871094 = 1.0143606662750244 + 10.0 * 8.959402084350586
Epoch 1760, val loss: 1.0144603252410889
Epoch 1770, training loss: 90.666015625 = 1.0136444568634033 + 10.0 * 8.965237617492676
Epoch 1770, val loss: 1.0137789249420166
Epoch 1780, training loss: 90.5721435546875 = 1.0129094123840332 + 10.0 * 8.955923080444336
Epoch 1780, val loss: 1.0131192207336426
Epoch 1790, training loss: 90.61410522460938 = 1.0122103691101074 + 10.0 * 8.960189819335938
Epoch 1790, val loss: 1.0124496221542358
Epoch 1800, training loss: 90.64608001708984 = 1.0114951133728027 + 10.0 * 8.963458061218262
Epoch 1800, val loss: 1.0117777585983276
Epoch 1810, training loss: 90.67992401123047 = 1.0107812881469727 + 10.0 * 8.966914176940918
Epoch 1810, val loss: 1.0110994577407837
Epoch 1820, training loss: 90.63770294189453 = 1.0100513696670532 + 10.0 * 8.96276569366455
Epoch 1820, val loss: 1.010440707206726
Epoch 1830, training loss: 90.66773986816406 = 1.0093737840652466 + 10.0 * 8.965836524963379
Epoch 1830, val loss: 1.009807825088501
Epoch 1840, training loss: 90.71566772460938 = 1.0086522102355957 + 10.0 * 8.970701217651367
Epoch 1840, val loss: 1.0091336965560913
Epoch 1850, training loss: 90.73052215576172 = 1.0079255104064941 + 10.0 * 8.972259521484375
Epoch 1850, val loss: 1.008459448814392
Epoch 1860, training loss: 90.74193572998047 = 1.007226586341858 + 10.0 * 8.973470687866211
Epoch 1860, val loss: 1.0077950954437256
Epoch 1870, training loss: 90.71662902832031 = 1.0065046548843384 + 10.0 * 8.971012115478516
Epoch 1870, val loss: 1.0071208477020264
Epoch 1880, training loss: 90.6652603149414 = 1.0058485269546509 + 10.0 * 8.965940475463867
Epoch 1880, val loss: 1.0065333843231201
Epoch 1890, training loss: 90.66708374023438 = 1.0051497220993042 + 10.0 * 8.966193199157715
Epoch 1890, val loss: 1.0058754682540894
Epoch 1900, training loss: 90.56978607177734 = 1.0045177936553955 + 10.0 * 8.956526756286621
Epoch 1900, val loss: 1.0052812099456787
Epoch 1910, training loss: 90.5615234375 = 1.0038864612579346 + 10.0 * 8.955763816833496
Epoch 1910, val loss: 1.0046861171722412
Epoch 1920, training loss: 90.5855712890625 = 1.0032230615615845 + 10.0 * 8.958234786987305
Epoch 1920, val loss: 1.0040650367736816
Epoch 1930, training loss: 90.65414428710938 = 1.0025614500045776 + 10.0 * 8.965158462524414
Epoch 1930, val loss: 1.003459095954895
Epoch 1940, training loss: 90.7275161743164 = 1.0019124746322632 + 10.0 * 8.972559928894043
Epoch 1940, val loss: 1.0028504133224487
Epoch 1950, training loss: 90.75398254394531 = 1.0012584924697876 + 10.0 * 8.975272178649902
Epoch 1950, val loss: 1.0022426843643188
Epoch 1960, training loss: 90.72933197021484 = 1.0005967617034912 + 10.0 * 8.97287368774414
Epoch 1960, val loss: 1.0016260147094727
Epoch 1970, training loss: 90.78802490234375 = 0.9999567270278931 + 10.0 * 8.97880744934082
Epoch 1970, val loss: 1.0010392665863037
Epoch 1980, training loss: 90.83131408691406 = 0.999304473400116 + 10.0 * 8.983201026916504
Epoch 1980, val loss: 1.0004383325576782
Epoch 1990, training loss: 90.83915710449219 = 0.9986544847488403 + 10.0 * 8.984049797058105
Epoch 1990, val loss: 0.9998378157615662
Epoch 2000, training loss: 90.81605529785156 = 0.9980121850967407 + 10.0 * 8.981804847717285
Epoch 2000, val loss: 0.999239981174469
Epoch 2010, training loss: 90.85380554199219 = 0.9973758459091187 + 10.0 * 8.98564338684082
Epoch 2010, val loss: 0.9986532330513
Epoch 2020, training loss: 90.89363861083984 = 0.9967467784881592 + 10.0 * 8.989688873291016
Epoch 2020, val loss: 0.9980679750442505
Epoch 2030, training loss: 90.90801239013672 = 0.9961147904396057 + 10.0 * 8.991189956665039
Epoch 2030, val loss: 0.9974769949913025
Epoch 2040, training loss: 90.86621856689453 = 0.9954715967178345 + 10.0 * 8.987074851989746
Epoch 2040, val loss: 0.9968690276145935
Epoch 2050, training loss: 90.86024475097656 = 0.994857132434845 + 10.0 * 8.986538887023926
Epoch 2050, val loss: 0.9963114857673645
Epoch 2060, training loss: 90.86846923828125 = 0.9942655563354492 + 10.0 * 8.987421035766602
Epoch 2060, val loss: 0.9957616925239563
Epoch 2070, training loss: 90.93981170654297 = 0.9936909079551697 + 10.0 * 8.994611740112305
Epoch 2070, val loss: 0.9952190518379211
Epoch 2080, training loss: 90.97025299072266 = 0.9931021928787231 + 10.0 * 8.99771499633789
Epoch 2080, val loss: 0.9946755170822144
Epoch 2090, training loss: 90.97309112548828 = 0.9924852848052979 + 10.0 * 8.99806022644043
Epoch 2090, val loss: 0.9940995573997498
Epoch 2100, training loss: 90.9728775024414 = 0.9918899536132812 + 10.0 * 8.998098373413086
Epoch 2100, val loss: 0.9935409426689148
Epoch 2110, training loss: 91.02838134765625 = 0.991295576095581 + 10.0 * 9.003708839416504
Epoch 2110, val loss: 0.9929879903793335
Epoch 2120, training loss: 91.04232025146484 = 0.990713357925415 + 10.0 * 9.005160331726074
Epoch 2120, val loss: 0.9924511313438416
Epoch 2130, training loss: 91.04811096191406 = 0.990124523639679 + 10.0 * 9.00579833984375
Epoch 2130, val loss: 0.9918991327285767
Epoch 2140, training loss: 91.0661849975586 = 0.989547073841095 + 10.0 * 9.00766372680664
Epoch 2140, val loss: 0.9913713932037354
Epoch 2150, training loss: 91.08064270019531 = 0.9889402985572815 + 10.0 * 9.009170532226562
Epoch 2150, val loss: 0.990820586681366
Epoch 2160, training loss: 91.05812072753906 = 0.9883802533149719 + 10.0 * 9.006974220275879
Epoch 2160, val loss: 0.9903214573860168
Epoch 2170, training loss: 91.14261627197266 = 0.9878331422805786 + 10.0 * 9.015478134155273
Epoch 2170, val loss: 0.9898187518119812
Epoch 2180, training loss: 91.1694564819336 = 0.9872642159461975 + 10.0 * 9.018218994140625
Epoch 2180, val loss: 0.989306628704071
Epoch 2190, training loss: 91.23954010009766 = 0.9866073727607727 + 10.0 * 9.025293350219727
Epoch 2190, val loss: 0.9886959791183472
Epoch 2200, training loss: 91.21693420410156 = 0.9859262704849243 + 10.0 * 9.023100852966309
Epoch 2200, val loss: 0.9880579113960266
Epoch 2210, training loss: 91.2657470703125 = 0.9852546453475952 + 10.0 * 9.02804946899414
Epoch 2210, val loss: 0.9874352812767029
Epoch 2220, training loss: 91.2500228881836 = 0.9845818877220154 + 10.0 * 9.026544570922852
Epoch 2220, val loss: 0.9868271946907043
Epoch 2230, training loss: 91.18353271484375 = 0.9839342832565308 + 10.0 * 9.019960403442383
Epoch 2230, val loss: 0.9862472414970398
Epoch 2240, training loss: 91.16069030761719 = 0.9833176732063293 + 10.0 * 9.01773738861084
Epoch 2240, val loss: 0.9856954216957092
Epoch 2250, training loss: 91.1690444946289 = 0.982667088508606 + 10.0 * 9.018637657165527
Epoch 2250, val loss: 0.9851168394088745
Epoch 2260, training loss: 91.24102020263672 = 0.9820617437362671 + 10.0 * 9.025896072387695
Epoch 2260, val loss: 0.9845739006996155
Epoch 2270, training loss: 91.35128784179688 = 0.9814468622207642 + 10.0 * 9.03698444366455
Epoch 2270, val loss: 0.9840390086174011
Epoch 2280, training loss: 91.37883758544922 = 0.9808182120323181 + 10.0 * 9.039801597595215
Epoch 2280, val loss: 0.9834821820259094
Epoch 2290, training loss: 91.38619232177734 = 0.980188250541687 + 10.0 * 9.040600776672363
Epoch 2290, val loss: 0.9829253554344177
Epoch 2300, training loss: 91.35767364501953 = 0.9795647263526917 + 10.0 * 9.037810325622559
Epoch 2300, val loss: 0.9823750257492065
Epoch 2310, training loss: 91.40434265136719 = 0.9789629578590393 + 10.0 * 9.042537689208984
Epoch 2310, val loss: 0.9818465709686279
Epoch 2320, training loss: 91.445556640625 = 0.9783710837364197 + 10.0 * 9.04671859741211
Epoch 2320, val loss: 0.9813272953033447
Epoch 2330, training loss: 91.34935760498047 = 0.9777635931968689 + 10.0 * 9.03715991973877
Epoch 2330, val loss: 0.9807845950126648
Epoch 2340, training loss: 91.39314270019531 = 0.9771828055381775 + 10.0 * 9.041596412658691
Epoch 2340, val loss: 0.9802929759025574
Epoch 2350, training loss: 91.46391296386719 = 0.9766120910644531 + 10.0 * 9.04872989654541
Epoch 2350, val loss: 0.9798004627227783
Epoch 2360, training loss: 91.48099517822266 = 0.9760355353355408 + 10.0 * 9.050496101379395
Epoch 2360, val loss: 0.9793069958686829
Epoch 2370, training loss: 91.4695816040039 = 0.9754656553268433 + 10.0 * 9.04941177368164
Epoch 2370, val loss: 0.9787951111793518
Epoch 2380, training loss: 91.45750427246094 = 0.9749001860618591 + 10.0 * 9.048260688781738
Epoch 2380, val loss: 0.9783271551132202
Epoch 2390, training loss: 91.50089263916016 = 0.9743472337722778 + 10.0 * 9.052654266357422
Epoch 2390, val loss: 0.9778622388839722
Epoch 2400, training loss: 91.5325927734375 = 0.9738047122955322 + 10.0 * 9.055878639221191
Epoch 2400, val loss: 0.9773919582366943
Epoch 2410, training loss: 91.4766845703125 = 0.9732614159584045 + 10.0 * 9.050342559814453
Epoch 2410, val loss: 0.9769257307052612
Epoch 2420, training loss: 91.41255187988281 = 0.9727344512939453 + 10.0 * 9.043981552124023
Epoch 2420, val loss: 0.976485550403595
Epoch 2430, training loss: 91.46605682373047 = 0.9722031950950623 + 10.0 * 9.049385070800781
Epoch 2430, val loss: 0.9760545492172241
Epoch 2440, training loss: 91.49620056152344 = 0.9716805815696716 + 10.0 * 9.052452087402344
Epoch 2440, val loss: 0.9756251573562622
Epoch 2450, training loss: 91.52493286132812 = 0.9711577296257019 + 10.0 * 9.055377006530762
Epoch 2450, val loss: 0.9751733541488647
Epoch 2460, training loss: 91.55247497558594 = 0.9706401824951172 + 10.0 * 9.058183670043945
Epoch 2460, val loss: 0.9747451543807983
Epoch 2470, training loss: 91.5712661743164 = 0.9701188206672668 + 10.0 * 9.060114860534668
Epoch 2470, val loss: 0.9743196368217468
Epoch 2480, training loss: 91.49864959716797 = 0.96961909532547 + 10.0 * 9.052903175354004
Epoch 2480, val loss: 0.9739052057266235
Epoch 2490, training loss: 91.4488754272461 = 0.9691323041915894 + 10.0 * 9.047974586486816
Epoch 2490, val loss: 0.9735098481178284
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.5228985507246376
0.8151126566688401
=== training gcn model ===
Epoch 0, training loss: 103.71002960205078 = 1.096916913986206 + 10.0 * 10.261311531066895
Epoch 0, val loss: 1.0968115329742432
Epoch 10, training loss: 99.89116668701172 = 1.0964101552963257 + 10.0 * 9.879475593566895
Epoch 10, val loss: 1.0963014364242554
Epoch 20, training loss: 97.97801208496094 = 1.095833659172058 + 10.0 * 9.688218116760254
Epoch 20, val loss: 1.0957260131835938
Epoch 30, training loss: 96.57392883300781 = 1.0953067541122437 + 10.0 * 9.54786205291748
Epoch 30, val loss: 1.0952008962631226
Epoch 40, training loss: 95.4922866821289 = 1.0947680473327637 + 10.0 * 9.439752578735352
Epoch 40, val loss: 1.0946664810180664
Epoch 50, training loss: 94.6273193359375 = 1.094224452972412 + 10.0 * 9.353309631347656
Epoch 50, val loss: 1.0941270589828491
Epoch 60, training loss: 93.90497589111328 = 1.0936824083328247 + 10.0 * 9.281129837036133
Epoch 60, val loss: 1.0935876369476318
Epoch 70, training loss: 93.29161071777344 = 1.0931358337402344 + 10.0 * 9.219847679138184
Epoch 70, val loss: 1.0930479764938354
Epoch 80, training loss: 92.76815032958984 = 1.0925861597061157 + 10.0 * 9.167556762695312
Epoch 80, val loss: 1.0924971103668213
Epoch 90, training loss: 92.32688903808594 = 1.0920300483703613 + 10.0 * 9.123485565185547
Epoch 90, val loss: 1.0919435024261475
Epoch 100, training loss: 91.91951751708984 = 1.0914684534072876 + 10.0 * 9.082804679870605
Epoch 100, val loss: 1.0913808345794678
Epoch 110, training loss: 91.56753540039062 = 1.090897798538208 + 10.0 * 9.047663688659668
Epoch 110, val loss: 1.0908143520355225
Epoch 120, training loss: 91.26236724853516 = 1.0903148651123047 + 10.0 * 9.017205238342285
Epoch 120, val loss: 1.090226411819458
Epoch 130, training loss: 91.00223541259766 = 1.0897388458251953 + 10.0 * 8.991250038146973
Epoch 130, val loss: 1.0896507501602173
Epoch 140, training loss: 90.77107238769531 = 1.0891482830047607 + 10.0 * 8.968192100524902
Epoch 140, val loss: 1.0890710353851318
Epoch 150, training loss: 90.56475067138672 = 1.088544249534607 + 10.0 * 8.947620391845703
Epoch 150, val loss: 1.088470697402954
Epoch 160, training loss: 90.38612365722656 = 1.0879682302474976 + 10.0 * 8.929815292358398
Epoch 160, val loss: 1.0878864526748657
Epoch 170, training loss: 90.2354736328125 = 1.0873692035675049 + 10.0 * 8.914810180664062
Epoch 170, val loss: 1.0872949361801147
Epoch 180, training loss: 90.11366271972656 = 1.08676016330719 + 10.0 * 8.902689933776855
Epoch 180, val loss: 1.086690902709961
Epoch 190, training loss: 89.98599243164062 = 1.0861176252365112 + 10.0 * 8.889986991882324
Epoch 190, val loss: 1.0860493183135986
Epoch 200, training loss: 89.86593627929688 = 1.085495948791504 + 10.0 * 8.878044128417969
Epoch 200, val loss: 1.0854454040527344
Epoch 210, training loss: 89.77159118652344 = 1.0848749876022339 + 10.0 * 8.868671417236328
Epoch 210, val loss: 1.0848256349563599
Epoch 220, training loss: 89.694091796875 = 1.0842417478561401 + 10.0 * 8.860984802246094
Epoch 220, val loss: 1.0842018127441406
Epoch 230, training loss: 89.60340881347656 = 1.083572268486023 + 10.0 * 8.851984024047852
Epoch 230, val loss: 1.0835449695587158
Epoch 240, training loss: 89.55397033691406 = 1.0829445123672485 + 10.0 * 8.847102165222168
Epoch 240, val loss: 1.0829066038131714
Epoch 250, training loss: 89.49600982666016 = 1.0823016166687012 + 10.0 * 8.841371536254883
Epoch 250, val loss: 1.082265853881836
Epoch 260, training loss: 89.48368835449219 = 1.0816078186035156 + 10.0 * 8.840208053588867
Epoch 260, val loss: 1.0815629959106445
Epoch 270, training loss: 89.44915008544922 = 1.0809742212295532 + 10.0 * 8.836817741394043
Epoch 270, val loss: 1.080957055091858
Epoch 280, training loss: 89.37556457519531 = 1.080320119857788 + 10.0 * 8.829524040222168
Epoch 280, val loss: 1.080310344696045
Epoch 290, training loss: 89.35497283935547 = 1.0796761512756348 + 10.0 * 8.827529907226562
Epoch 290, val loss: 1.0796613693237305
Epoch 300, training loss: 89.35466003417969 = 1.079057216644287 + 10.0 * 8.827560424804688
Epoch 300, val loss: 1.0790424346923828
Epoch 310, training loss: 89.34642791748047 = 1.0784627199172974 + 10.0 * 8.826796531677246
Epoch 310, val loss: 1.078445315361023
Epoch 320, training loss: 89.33299255371094 = 1.0779083967208862 + 10.0 * 8.825508117675781
Epoch 320, val loss: 1.077911138534546
Epoch 330, training loss: 89.28893280029297 = 1.0773752927780151 + 10.0 * 8.821155548095703
Epoch 330, val loss: 1.0773963928222656
Epoch 340, training loss: 89.26333618164062 = 1.076907992362976 + 10.0 * 8.818643569946289
Epoch 340, val loss: 1.0769307613372803
Epoch 350, training loss: 89.26560974121094 = 1.0764594078063965 + 10.0 * 8.818914413452148
Epoch 350, val loss: 1.0764974355697632
Epoch 360, training loss: 89.28960418701172 = 1.0760356187820435 + 10.0 * 8.821356773376465
Epoch 360, val loss: 1.076097846031189
Epoch 370, training loss: 89.25550842285156 = 1.075610637664795 + 10.0 * 8.81799030303955
Epoch 370, val loss: 1.0756745338439941
Epoch 380, training loss: 89.2540512084961 = 1.0751930475234985 + 10.0 * 8.817885398864746
Epoch 380, val loss: 1.075270414352417
Epoch 390, training loss: 89.29075622558594 = 1.074797511100769 + 10.0 * 8.821596145629883
Epoch 390, val loss: 1.074881672859192
Epoch 400, training loss: 89.21228790283203 = 1.0743353366851807 + 10.0 * 8.81379508972168
Epoch 400, val loss: 1.0744452476501465
Epoch 410, training loss: 89.26498413085938 = 1.0739527940750122 + 10.0 * 8.819103240966797
Epoch 410, val loss: 1.0740535259246826
Epoch 420, training loss: 89.41761779785156 = 1.0735646486282349 + 10.0 * 8.834405899047852
Epoch 420, val loss: 1.0736868381500244
Epoch 430, training loss: 89.21623992919922 = 1.0730825662612915 + 10.0 * 8.814315795898438
Epoch 430, val loss: 1.0732195377349854
Epoch 440, training loss: 89.29961395263672 = 1.0726877450942993 + 10.0 * 8.82269287109375
Epoch 440, val loss: 1.0728201866149902
Epoch 450, training loss: 89.34234619140625 = 1.0722429752349854 + 10.0 * 8.827010154724121
Epoch 450, val loss: 1.0723912715911865
Epoch 460, training loss: 89.33797454833984 = 1.0717883110046387 + 10.0 * 8.826619148254395
Epoch 460, val loss: 1.0719447135925293
Epoch 470, training loss: 89.36084747314453 = 1.0713304281234741 + 10.0 * 8.828951835632324
Epoch 470, val loss: 1.0714977979660034
Epoch 480, training loss: 89.35132598876953 = 1.070852279663086 + 10.0 * 8.828046798706055
Epoch 480, val loss: 1.071027159690857
Epoch 490, training loss: 89.39552307128906 = 1.070375919342041 + 10.0 * 8.832514762878418
Epoch 490, val loss: 1.0705615282058716
Epoch 500, training loss: 89.3747787475586 = 1.0698920488357544 + 10.0 * 8.830488204956055
Epoch 500, val loss: 1.070099115371704
Epoch 510, training loss: 89.41448211669922 = 1.0694063901901245 + 10.0 * 8.834507942199707
Epoch 510, val loss: 1.0696289539337158
Epoch 520, training loss: 89.45349884033203 = 1.068913459777832 + 10.0 * 8.838458061218262
Epoch 520, val loss: 1.0691367387771606
Epoch 530, training loss: 89.45271301269531 = 1.0684021711349487 + 10.0 * 8.838430404663086
Epoch 530, val loss: 1.0686476230621338
Epoch 540, training loss: 89.47421264648438 = 1.0678789615631104 + 10.0 * 8.840633392333984
Epoch 540, val loss: 1.068143606185913
Epoch 550, training loss: 89.48435974121094 = 1.0673704147338867 + 10.0 * 8.841699600219727
Epoch 550, val loss: 1.0676451921463013
Epoch 560, training loss: 89.52082061767578 = 1.0668381452560425 + 10.0 * 8.84539794921875
Epoch 560, val loss: 1.0671204328536987
Epoch 570, training loss: 89.56238555908203 = 1.0663198232650757 + 10.0 * 8.84960651397705
Epoch 570, val loss: 1.066601037979126
Epoch 580, training loss: 89.54483032226562 = 1.0657556056976318 + 10.0 * 8.847907066345215
Epoch 580, val loss: 1.0660570859909058
Epoch 590, training loss: 89.55426025390625 = 1.065197229385376 + 10.0 * 8.848905563354492
Epoch 590, val loss: 1.065529227256775
Epoch 600, training loss: 89.53895568847656 = 1.0646182298660278 + 10.0 * 8.847433090209961
Epoch 600, val loss: 1.0649566650390625
Epoch 610, training loss: 89.76348876953125 = 1.0640819072723389 + 10.0 * 8.869940757751465
Epoch 610, val loss: 1.0644307136535645
Epoch 620, training loss: 89.75544738769531 = 1.063697099685669 + 10.0 * 8.86917495727539
Epoch 620, val loss: 1.064098596572876
Epoch 630, training loss: 89.58159637451172 = 1.0630147457122803 + 10.0 * 8.851858139038086
Epoch 630, val loss: 1.063397765159607
Epoch 640, training loss: 89.71991729736328 = 1.0624443292617798 + 10.0 * 8.865747451782227
Epoch 640, val loss: 1.0628459453582764
Epoch 650, training loss: 89.76481628417969 = 1.061851143836975 + 10.0 * 8.870296478271484
Epoch 650, val loss: 1.0622859001159668
Epoch 660, training loss: 89.78868103027344 = 1.0612516403198242 + 10.0 * 8.872743606567383
Epoch 660, val loss: 1.0617115497589111
Epoch 670, training loss: 89.82958221435547 = 1.0606220960617065 + 10.0 * 8.876895904541016
Epoch 670, val loss: 1.0610926151275635
Epoch 680, training loss: 89.8635482788086 = 1.05997896194458 + 10.0 * 8.880356788635254
Epoch 680, val loss: 1.060484528541565
Epoch 690, training loss: 89.93360900878906 = 1.0593353509902954 + 10.0 * 8.88742733001709
Epoch 690, val loss: 1.0598353147506714
Epoch 700, training loss: 89.94856262207031 = 1.058677315711975 + 10.0 * 8.888988494873047
Epoch 700, val loss: 1.0592132806777954
Epoch 710, training loss: 90.00839233398438 = 1.0580083131790161 + 10.0 * 8.895038604736328
Epoch 710, val loss: 1.0585579872131348
Epoch 720, training loss: 89.99063110351562 = 1.057315707206726 + 10.0 * 8.893331527709961
Epoch 720, val loss: 1.0578805208206177
Epoch 730, training loss: 90.01679992675781 = 1.0566067695617676 + 10.0 * 8.896018981933594
Epoch 730, val loss: 1.0572049617767334
Epoch 740, training loss: 90.0361557006836 = 1.0558900833129883 + 10.0 * 8.898026466369629
Epoch 740, val loss: 1.0565125942230225
Epoch 750, training loss: 90.0860824584961 = 1.0551841259002686 + 10.0 * 8.90308952331543
Epoch 750, val loss: 1.055835485458374
Epoch 760, training loss: 90.14932250976562 = 1.0544553995132446 + 10.0 * 8.909486770629883
Epoch 760, val loss: 1.0551272630691528
Epoch 770, training loss: 90.1978759765625 = 1.0537227392196655 + 10.0 * 8.91441535949707
Epoch 770, val loss: 1.0544238090515137
Epoch 780, training loss: 90.22152709960938 = 1.052973985671997 + 10.0 * 8.916855812072754
Epoch 780, val loss: 1.0536757707595825
Epoch 790, training loss: 90.16635131835938 = 1.0521910190582275 + 10.0 * 8.911416053771973
Epoch 790, val loss: 1.0529320240020752
Epoch 800, training loss: 90.22383117675781 = 1.0514042377471924 + 10.0 * 8.917242050170898
Epoch 800, val loss: 1.0521711111068726
Epoch 810, training loss: 90.25163269042969 = 1.050611138343811 + 10.0 * 8.9201021194458
Epoch 810, val loss: 1.0514031648635864
Epoch 820, training loss: 90.27964782714844 = 1.0498080253601074 + 10.0 * 8.92298412322998
Epoch 820, val loss: 1.050630807876587
Epoch 830, training loss: 90.30854797363281 = 1.04899001121521 + 10.0 * 8.925955772399902
Epoch 830, val loss: 1.0498391389846802
Epoch 840, training loss: 90.3120346069336 = 1.0481600761413574 + 10.0 * 8.926387786865234
Epoch 840, val loss: 1.0490309000015259
Epoch 850, training loss: 90.26555633544922 = 1.0472989082336426 + 10.0 * 8.921825408935547
Epoch 850, val loss: 1.0482301712036133
Epoch 860, training loss: 90.30378723144531 = 1.0464732646942139 + 10.0 * 8.925731658935547
Epoch 860, val loss: 1.0474183559417725
Epoch 870, training loss: 90.38848876953125 = 1.0456252098083496 + 10.0 * 8.934286117553711
Epoch 870, val loss: 1.0466033220291138
Epoch 880, training loss: 90.43233489990234 = 1.0447463989257812 + 10.0 * 8.938758850097656
Epoch 880, val loss: 1.0457499027252197
Epoch 890, training loss: 90.38241577148438 = 1.0438088178634644 + 10.0 * 8.933860778808594
Epoch 890, val loss: 1.0448496341705322
Epoch 900, training loss: 90.42073822021484 = 1.0429332256317139 + 10.0 * 8.937780380249023
Epoch 900, val loss: 1.0439903736114502
Epoch 910, training loss: 90.43023681640625 = 1.042020559310913 + 10.0 * 8.938821792602539
Epoch 910, val loss: 1.043121099472046
Epoch 920, training loss: 90.4795913696289 = 1.0410685539245605 + 10.0 * 8.943852424621582
Epoch 920, val loss: 1.0422052145004272
Epoch 930, training loss: 90.44269561767578 = 1.0400490760803223 + 10.0 * 8.940264701843262
Epoch 930, val loss: 1.0412073135375977
Epoch 940, training loss: 90.61511993408203 = 1.0391606092453003 + 10.0 * 8.957595825195312
Epoch 940, val loss: 1.0403751134872437
Epoch 950, training loss: 90.55917358398438 = 1.0380666255950928 + 10.0 * 8.95211124420166
Epoch 950, val loss: 1.0393136739730835
Epoch 960, training loss: 90.6217041015625 = 1.0370588302612305 + 10.0 * 8.958464622497559
Epoch 960, val loss: 1.038343071937561
Epoch 970, training loss: 90.63385009765625 = 1.0359972715377808 + 10.0 * 8.959785461425781
Epoch 970, val loss: 1.0373117923736572
Epoch 980, training loss: 90.63469696044922 = 1.0349410772323608 + 10.0 * 8.959975242614746
Epoch 980, val loss: 1.036301612854004
Epoch 990, training loss: 90.67543029785156 = 1.0338778495788574 + 10.0 * 8.964155197143555
Epoch 990, val loss: 1.0352879762649536
Epoch 1000, training loss: 90.74238586425781 = 1.0327904224395752 + 10.0 * 8.970959663391113
Epoch 1000, val loss: 1.0342447757720947
Epoch 1010, training loss: 90.7287368774414 = 1.0316904783248901 + 10.0 * 8.969704627990723
Epoch 1010, val loss: 1.033163070678711
Epoch 1020, training loss: 90.75984954833984 = 1.0305455923080444 + 10.0 * 8.972929954528809
Epoch 1020, val loss: 1.032097339630127
Epoch 1030, training loss: 90.80116271972656 = 1.029384732246399 + 10.0 * 8.977177619934082
Epoch 1030, val loss: 1.0309840440750122
Epoch 1040, training loss: 90.84983825683594 = 1.028228998184204 + 10.0 * 8.982160568237305
Epoch 1040, val loss: 1.0298714637756348
Epoch 1050, training loss: 90.88922119140625 = 1.0270471572875977 + 10.0 * 8.986217498779297
Epoch 1050, val loss: 1.0287129878997803
Epoch 1060, training loss: 90.85147857666016 = 1.0258387327194214 + 10.0 * 8.982563972473145
Epoch 1060, val loss: 1.0275537967681885
Epoch 1070, training loss: 90.88326263427734 = 1.024607539176941 + 10.0 * 8.985865592956543
Epoch 1070, val loss: 1.0263690948486328
Epoch 1080, training loss: 90.92255401611328 = 1.023344874382019 + 10.0 * 8.989920616149902
Epoch 1080, val loss: 1.025177001953125
Epoch 1090, training loss: 90.90684509277344 = 1.0220491886138916 + 10.0 * 8.988479614257812
Epoch 1090, val loss: 1.0239289999008179
Epoch 1100, training loss: 90.92520904541016 = 1.020799160003662 + 10.0 * 8.99044132232666
Epoch 1100, val loss: 1.022722840309143
Epoch 1110, training loss: 90.98743438720703 = 1.019482135772705 + 10.0 * 8.996794700622559
Epoch 1110, val loss: 1.0214500427246094
Epoch 1120, training loss: 90.98497009277344 = 1.0181456804275513 + 10.0 * 8.996683120727539
Epoch 1120, val loss: 1.020155429840088
Epoch 1130, training loss: 90.93330383300781 = 1.0167827606201172 + 10.0 * 8.99165153503418
Epoch 1130, val loss: 1.018826961517334
Epoch 1140, training loss: 90.98544311523438 = 1.0153831243515015 + 10.0 * 8.9970064163208
Epoch 1140, val loss: 1.0175132751464844
Epoch 1150, training loss: 91.04029083251953 = 1.0139756202697754 + 10.0 * 9.002631187438965
Epoch 1150, val loss: 1.0161561965942383
Epoch 1160, training loss: 91.0417709350586 = 1.0125434398651123 + 10.0 * 9.002923011779785
Epoch 1160, val loss: 1.0147597789764404
Epoch 1170, training loss: 91.0620346069336 = 1.011062502861023 + 10.0 * 9.005097389221191
Epoch 1170, val loss: 1.013336420059204
Epoch 1180, training loss: 91.0741195678711 = 1.0095679759979248 + 10.0 * 9.006455421447754
Epoch 1180, val loss: 1.0119009017944336
Epoch 1190, training loss: 91.088623046875 = 1.008026123046875 + 10.0 * 9.00805950164795
Epoch 1190, val loss: 1.0104299783706665
Epoch 1200, training loss: 91.12416076660156 = 1.0064513683319092 + 10.0 * 9.011770248413086
Epoch 1200, val loss: 1.0089045763015747
Epoch 1210, training loss: 91.1122055053711 = 1.0048601627349854 + 10.0 * 9.010734558105469
Epoch 1210, val loss: 1.0073697566986084
Epoch 1220, training loss: 91.12794494628906 = 1.0032402276992798 + 10.0 * 9.012470245361328
Epoch 1220, val loss: 1.0057851076126099
Epoch 1230, training loss: 91.1781005859375 = 1.0015989542007446 + 10.0 * 9.01764965057373
Epoch 1230, val loss: 1.0042017698287964
Epoch 1240, training loss: 91.23335266113281 = 1.0000003576278687 + 10.0 * 9.023335456848145
Epoch 1240, val loss: 1.0026596784591675
Epoch 1250, training loss: 91.12100219726562 = 0.9981241226196289 + 10.0 * 9.012288093566895
Epoch 1250, val loss: 1.0008546113967896
Epoch 1260, training loss: 91.15164947509766 = 0.9964603185653687 + 10.0 * 9.015519142150879
Epoch 1260, val loss: 0.9992460608482361
Epoch 1270, training loss: 91.21460723876953 = 0.99472576379776 + 10.0 * 9.021987915039062
Epoch 1270, val loss: 0.9975559115409851
Epoch 1280, training loss: 91.26820373535156 = 0.9929408431053162 + 10.0 * 9.027525901794434
Epoch 1280, val loss: 0.99582839012146
Epoch 1290, training loss: 91.26341247558594 = 0.9911202192306519 + 10.0 * 9.027229309082031
Epoch 1290, val loss: 0.9940820932388306
Epoch 1300, training loss: 91.24955749511719 = 0.989255964756012 + 10.0 * 9.026029586791992
Epoch 1300, val loss: 0.9922670722007751
Epoch 1310, training loss: 91.30420684814453 = 0.9874185919761658 + 10.0 * 9.031679153442383
Epoch 1310, val loss: 0.9905002117156982
Epoch 1320, training loss: 91.3112564086914 = 0.9854972958564758 + 10.0 * 9.032575607299805
Epoch 1320, val loss: 0.9886529445648193
Epoch 1330, training loss: 91.2940902709961 = 0.983545184135437 + 10.0 * 9.031054496765137
Epoch 1330, val loss: 0.986733078956604
Epoch 1340, training loss: 91.31526184082031 = 0.9816011786460876 + 10.0 * 9.033366203308105
Epoch 1340, val loss: 0.9848572611808777
Epoch 1350, training loss: 91.36564636230469 = 0.9796120524406433 + 10.0 * 9.038602828979492
Epoch 1350, val loss: 0.982944905757904
Epoch 1360, training loss: 91.35069274902344 = 0.977584719657898 + 10.0 * 9.037310600280762
Epoch 1360, val loss: 0.9809859991073608
Epoch 1370, training loss: 91.35820007324219 = 0.9755196571350098 + 10.0 * 9.038268089294434
Epoch 1370, val loss: 0.9789903163909912
Epoch 1380, training loss: 91.36813354492188 = 0.9734283685684204 + 10.0 * 9.039470672607422
Epoch 1380, val loss: 0.9769681692123413
Epoch 1390, training loss: 91.35716247558594 = 0.9713147878646851 + 10.0 * 9.03858470916748
Epoch 1390, val loss: 0.9749274849891663
Epoch 1400, training loss: 91.41160583496094 = 0.9691585302352905 + 10.0 * 9.044244766235352
Epoch 1400, val loss: 0.9728748798370361
Epoch 1410, training loss: 91.45911407470703 = 0.9669777750968933 + 10.0 * 9.049213409423828
Epoch 1410, val loss: 0.9707677364349365
Epoch 1420, training loss: 91.38169860839844 = 0.9647613167762756 + 10.0 * 9.041693687438965
Epoch 1420, val loss: 0.9686058759689331
Epoch 1430, training loss: 91.42972564697266 = 0.9624966382980347 + 10.0 * 9.046723365783691
Epoch 1430, val loss: 0.9664583206176758
Epoch 1440, training loss: 91.43360137939453 = 0.9602460265159607 + 10.0 * 9.047335624694824
Epoch 1440, val loss: 0.9642724990844727
Epoch 1450, training loss: 91.4661636352539 = 0.9579600095748901 + 10.0 * 9.050820350646973
Epoch 1450, val loss: 0.9620748162269592
Epoch 1460, training loss: 91.44711303710938 = 0.9556387662887573 + 10.0 * 9.049147605895996
Epoch 1460, val loss: 0.9598385095596313
Epoch 1470, training loss: 91.48546600341797 = 0.953308641910553 + 10.0 * 9.053215980529785
Epoch 1470, val loss: 0.9575998187065125
Epoch 1480, training loss: 91.5046157836914 = 0.950937807559967 + 10.0 * 9.055368423461914
Epoch 1480, val loss: 0.9553226828575134
Epoch 1490, training loss: 91.55519104003906 = 0.9485657215118408 + 10.0 * 9.060663223266602
Epoch 1490, val loss: 0.9530110359191895
Epoch 1500, training loss: 91.423828125 = 0.9461104273796082 + 10.0 * 9.047771453857422
Epoch 1500, val loss: 0.9506452679634094
Epoch 1510, training loss: 91.33436584472656 = 0.9436919093132019 + 10.0 * 9.039067268371582
Epoch 1510, val loss: 0.9483842849731445
Epoch 1520, training loss: 91.4634017944336 = 0.9412941336631775 + 10.0 * 9.052210807800293
Epoch 1520, val loss: 0.9460380673408508
Epoch 1530, training loss: 91.54019165039062 = 0.938834547996521 + 10.0 * 9.060135841369629
Epoch 1530, val loss: 0.9436671137809753
Epoch 1540, training loss: 91.57672119140625 = 0.9363711476325989 + 10.0 * 9.064035415649414
Epoch 1540, val loss: 0.9413028359413147
Epoch 1550, training loss: 91.58949279785156 = 0.9338548183441162 + 10.0 * 9.065564155578613
Epoch 1550, val loss: 0.9388864040374756
Epoch 1560, training loss: 91.59967803955078 = 0.9313395023345947 + 10.0 * 9.06683349609375
Epoch 1560, val loss: 0.9364414215087891
Epoch 1570, training loss: 91.59864044189453 = 0.9288232326507568 + 10.0 * 9.066981315612793
Epoch 1570, val loss: 0.9340280294418335
Epoch 1580, training loss: 91.65633392333984 = 0.9262900352478027 + 10.0 * 9.073003768920898
Epoch 1580, val loss: 0.9316033124923706
Epoch 1590, training loss: 91.65895080566406 = 0.9237371683120728 + 10.0 * 9.073521614074707
Epoch 1590, val loss: 0.9291414022445679
Epoch 1600, training loss: 91.61664581298828 = 0.9211665987968445 + 10.0 * 9.069547653198242
Epoch 1600, val loss: 0.9266851544380188
Epoch 1610, training loss: 91.65908813476562 = 0.9185835123062134 + 10.0 * 9.074049949645996
Epoch 1610, val loss: 0.9241884350776672
Epoch 1620, training loss: 91.69691467285156 = 0.9159848690032959 + 10.0 * 9.078092575073242
Epoch 1620, val loss: 0.9216879606246948
Epoch 1630, training loss: 91.69042205810547 = 0.9133780002593994 + 10.0 * 9.077704429626465
Epoch 1630, val loss: 0.9191774725914001
Epoch 1640, training loss: 91.73674774169922 = 0.9107598662376404 + 10.0 * 9.082598686218262
Epoch 1640, val loss: 0.9166640043258667
Epoch 1650, training loss: 91.77357482910156 = 0.908141016960144 + 10.0 * 9.086543083190918
Epoch 1650, val loss: 0.9141215682029724
Epoch 1660, training loss: 91.721435546875 = 0.9055022597312927 + 10.0 * 9.08159351348877
Epoch 1660, val loss: 0.9115990996360779
Epoch 1670, training loss: 91.3109130859375 = 0.9027777910232544 + 10.0 * 9.040813446044922
Epoch 1670, val loss: 0.9089608192443848
Epoch 1680, training loss: 91.4413833618164 = 0.9002344012260437 + 10.0 * 9.05411434173584
Epoch 1680, val loss: 0.9065667390823364
Epoch 1690, training loss: 91.41242218017578 = 0.897699236869812 + 10.0 * 9.051472663879395
Epoch 1690, val loss: 0.9041418433189392
Epoch 1700, training loss: 91.51850891113281 = 0.8950936794281006 + 10.0 * 9.062341690063477
Epoch 1700, val loss: 0.9016477465629578
Epoch 1710, training loss: 91.67333984375 = 0.8924873471260071 + 10.0 * 9.078084945678711
Epoch 1710, val loss: 0.8991590738296509
Epoch 1720, training loss: 91.7208023071289 = 0.8898374438285828 + 10.0 * 9.083096504211426
Epoch 1720, val loss: 0.8966205716133118
Epoch 1730, training loss: 91.81689453125 = 0.8871936202049255 + 10.0 * 9.09296989440918
Epoch 1730, val loss: 0.8940946459770203
Epoch 1740, training loss: 91.90846252441406 = 0.8845322728157043 + 10.0 * 9.10239315032959
Epoch 1740, val loss: 0.8915606141090393
Epoch 1750, training loss: 91.88673400878906 = 0.8818364143371582 + 10.0 * 9.100489616394043
Epoch 1750, val loss: 0.8889973163604736
Epoch 1760, training loss: 91.93672180175781 = 0.879174530506134 + 10.0 * 9.105754852294922
Epoch 1760, val loss: 0.886446475982666
Epoch 1770, training loss: 91.90321350097656 = 0.8764098882675171 + 10.0 * 9.102680206298828
Epoch 1770, val loss: 0.8838208317756653
Epoch 1780, training loss: 91.90686798095703 = 0.8743342161178589 + 10.0 * 9.103253364562988
Epoch 1780, val loss: 0.8818970322608948
Epoch 1790, training loss: 91.95441436767578 = 0.8717964887619019 + 10.0 * 9.108262062072754
Epoch 1790, val loss: 0.8794360160827637
Epoch 1800, training loss: 91.8011245727539 = 0.8691501021385193 + 10.0 * 9.0931978225708
Epoch 1800, val loss: 0.8768998384475708
Epoch 1810, training loss: 91.92684936523438 = 0.8663749694824219 + 10.0 * 9.106047630310059
Epoch 1810, val loss: 0.8742740750312805
Epoch 1820, training loss: 92.06309509277344 = 0.8637890219688416 + 10.0 * 9.119930267333984
Epoch 1820, val loss: 0.8718296885490417
Epoch 1830, training loss: 92.12818908691406 = 0.8611131906509399 + 10.0 * 9.126707077026367
Epoch 1830, val loss: 0.8692833185195923
Epoch 1840, training loss: 92.22488403320312 = 0.8584496974945068 + 10.0 * 9.136643409729004
Epoch 1840, val loss: 0.8667451739311218
Epoch 1850, training loss: 92.19196319580078 = 0.8557449579238892 + 10.0 * 9.133622169494629
Epoch 1850, val loss: 0.864159107208252
Epoch 1860, training loss: 92.22854614257812 = 0.8530368804931641 + 10.0 * 9.137551307678223
Epoch 1860, val loss: 0.8615987300872803
Epoch 1870, training loss: 92.2968978881836 = 0.8503657579421997 + 10.0 * 9.1446533203125
Epoch 1870, val loss: 0.8590461611747742
Epoch 1880, training loss: 92.3510513305664 = 0.8476927280426025 + 10.0 * 9.150335311889648
Epoch 1880, val loss: 0.8564985990524292
Epoch 1890, training loss: 92.34269714355469 = 0.8450141549110413 + 10.0 * 9.149767875671387
Epoch 1890, val loss: 0.853943407535553
Epoch 1900, training loss: 92.36634063720703 = 0.8423404693603516 + 10.0 * 9.152400016784668
Epoch 1900, val loss: 0.8514065146446228
Epoch 1910, training loss: 92.34910583496094 = 0.8396931290626526 + 10.0 * 9.150941848754883
Epoch 1910, val loss: 0.8488805890083313
Epoch 1920, training loss: 92.39895629882812 = 0.8370320200920105 + 10.0 * 9.156192779541016
Epoch 1920, val loss: 0.8463760018348694
Epoch 1930, training loss: 92.44509887695312 = 0.8343985080718994 + 10.0 * 9.161069869995117
Epoch 1930, val loss: 0.8438606262207031
Epoch 1940, training loss: 92.38105010986328 = 0.8317629098892212 + 10.0 * 9.154928207397461
Epoch 1940, val loss: 0.8413498401641846
Epoch 1950, training loss: 92.42860412597656 = 0.8291776776313782 + 10.0 * 9.159942626953125
Epoch 1950, val loss: 0.8388954401016235
Epoch 1960, training loss: 92.45718383789062 = 0.8265257477760315 + 10.0 * 9.163065910339355
Epoch 1960, val loss: 0.8363921046257019
Epoch 1970, training loss: 92.49604034423828 = 0.8239092826843262 + 10.0 * 9.167213439941406
Epoch 1970, val loss: 0.8338800072669983
Epoch 1980, training loss: 92.52721405029297 = 0.8212876319885254 + 10.0 * 9.170592308044434
Epoch 1980, val loss: 0.8314115405082703
Epoch 1990, training loss: 91.9923324584961 = 0.8185654878616333 + 10.0 * 9.117376327514648
Epoch 1990, val loss: 0.8288769721984863
Epoch 2000, training loss: 91.97608184814453 = 0.8160784840583801 + 10.0 * 9.116000175476074
Epoch 2000, val loss: 0.8265936374664307
Epoch 2010, training loss: 92.16146850585938 = 0.8141975402832031 + 10.0 * 9.134726524353027
Epoch 2010, val loss: 0.8247146010398865
Epoch 2020, training loss: 92.14070892333984 = 0.8117142915725708 + 10.0 * 9.132899284362793
Epoch 2020, val loss: 0.8223749399185181
Epoch 2030, training loss: 92.21836853027344 = 0.809223473072052 + 10.0 * 9.140913963317871
Epoch 2030, val loss: 0.8200601935386658
Epoch 2040, training loss: 92.33165740966797 = 0.8067371249198914 + 10.0 * 9.152491569519043
Epoch 2040, val loss: 0.8177195191383362
Epoch 2050, training loss: 92.42977905273438 = 0.8041930794715881 + 10.0 * 9.162558555603027
Epoch 2050, val loss: 0.8153414130210876
Epoch 2060, training loss: 92.4971923828125 = 0.8016204833984375 + 10.0 * 9.169557571411133
Epoch 2060, val loss: 0.812935471534729
Epoch 2070, training loss: 92.5129623413086 = 0.7990911602973938 + 10.0 * 9.171387672424316
Epoch 2070, val loss: 0.8105571269989014
Epoch 2080, training loss: 92.5380630493164 = 0.7966161370277405 + 10.0 * 9.174144744873047
Epoch 2080, val loss: 0.8082019686698914
Epoch 2090, training loss: 92.58525085449219 = 0.7941649556159973 + 10.0 * 9.179108619689941
Epoch 2090, val loss: 0.8059027194976807
Epoch 2100, training loss: 92.5788345336914 = 0.7917295098304749 + 10.0 * 9.178709983825684
Epoch 2100, val loss: 0.8035984039306641
Epoch 2110, training loss: 92.60655975341797 = 0.7893033623695374 + 10.0 * 9.18172550201416
Epoch 2110, val loss: 0.8013161420822144
Epoch 2120, training loss: 92.61901092529297 = 0.7868946194648743 + 10.0 * 9.183211326599121
Epoch 2120, val loss: 0.7990419864654541
Epoch 2130, training loss: 92.66631317138672 = 0.7844853401184082 + 10.0 * 9.188182830810547
Epoch 2130, val loss: 0.7967810034751892
Epoch 2140, training loss: 92.63220977783203 = 0.7820895910263062 + 10.0 * 9.185011863708496
Epoch 2140, val loss: 0.7945623397827148
Epoch 2150, training loss: 92.46725463867188 = 0.779703676700592 + 10.0 * 9.168755531311035
Epoch 2150, val loss: 0.7923485040664673
Epoch 2160, training loss: 92.47836303710938 = 0.7774625420570374 + 10.0 * 9.170089721679688
Epoch 2160, val loss: 0.7902623414993286
Epoch 2170, training loss: 92.53913879394531 = 0.7752182483673096 + 10.0 * 9.176392555236816
Epoch 2170, val loss: 0.7881571650505066
Epoch 2180, training loss: 92.63239288330078 = 0.7728844285011292 + 10.0 * 9.18595027923584
Epoch 2180, val loss: 0.7859918475151062
Epoch 2190, training loss: 92.70178985595703 = 0.7705252170562744 + 10.0 * 9.193126678466797
Epoch 2190, val loss: 0.7837756872177124
Epoch 2200, training loss: 92.75776672363281 = 0.7681688666343689 + 10.0 * 9.198960304260254
Epoch 2200, val loss: 0.7816001772880554
Epoch 2210, training loss: 92.7526626586914 = 0.765845000743866 + 10.0 * 9.198681831359863
Epoch 2210, val loss: 0.779426634311676
Epoch 2220, training loss: 92.78328704833984 = 0.7635590434074402 + 10.0 * 9.201972961425781
Epoch 2220, val loss: 0.7773091793060303
Epoch 2230, training loss: 92.72101593017578 = 0.7612403035163879 + 10.0 * 9.195978164672852
Epoch 2230, val loss: 0.7751794457435608
Epoch 2240, training loss: 92.7541275024414 = 0.7589954137802124 + 10.0 * 9.19951343536377
Epoch 2240, val loss: 0.7730848789215088
Epoch 2250, training loss: 92.81254577636719 = 0.7567477822303772 + 10.0 * 9.20557975769043
Epoch 2250, val loss: 0.7710012197494507
Epoch 2260, training loss: 92.80562591552734 = 0.7544959783554077 + 10.0 * 9.205113410949707
Epoch 2260, val loss: 0.7689179182052612
Epoch 2270, training loss: 92.80946350097656 = 0.7522708177566528 + 10.0 * 9.205718994140625
Epoch 2270, val loss: 0.7668628692626953
Epoch 2280, training loss: 92.84771728515625 = 0.7500585317611694 + 10.0 * 9.209765434265137
Epoch 2280, val loss: 0.7648370862007141
Epoch 2290, training loss: 92.63709259033203 = 0.7478545308113098 + 10.0 * 9.188923835754395
Epoch 2290, val loss: 0.7627919316291809
Epoch 2300, training loss: 92.52806854248047 = 0.7460096478462219 + 10.0 * 9.178205490112305
Epoch 2300, val loss: 0.7611697912216187
Epoch 2310, training loss: 92.62107849121094 = 0.7444583177566528 + 10.0 * 9.187662124633789
Epoch 2310, val loss: 0.7598291635513306
Epoch 2320, training loss: 92.42086029052734 = 0.7426674365997314 + 10.0 * 9.167819023132324
Epoch 2320, val loss: 0.7581349015235901
Epoch 2330, training loss: 92.5849380493164 = 0.7405239343643188 + 10.0 * 9.184441566467285
Epoch 2330, val loss: 0.7561938166618347
Epoch 2340, training loss: 92.71026611328125 = 0.7384313344955444 + 10.0 * 9.197183609008789
Epoch 2340, val loss: 0.7542761564254761
Epoch 2350, training loss: 92.80824279785156 = 0.7363632321357727 + 10.0 * 9.20718765258789
Epoch 2350, val loss: 0.7523745894432068
Epoch 2360, training loss: 92.91389465332031 = 0.7342519760131836 + 10.0 * 9.217964172363281
Epoch 2360, val loss: 0.7504214644432068
Epoch 2370, training loss: 92.93843841552734 = 0.7321703433990479 + 10.0 * 9.220626831054688
Epoch 2370, val loss: 0.7484927177429199
Epoch 2380, training loss: 93.0178451538086 = 0.7301028370857239 + 10.0 * 9.228774070739746
Epoch 2380, val loss: 0.74659663438797
Epoch 2390, training loss: 93.04593658447266 = 0.7280349731445312 + 10.0 * 9.231790542602539
Epoch 2390, val loss: 0.744686484336853
Epoch 2400, training loss: 93.09526062011719 = 0.7259826064109802 + 10.0 * 9.23692798614502
Epoch 2400, val loss: 0.7427917122840881
Epoch 2410, training loss: 93.03598022460938 = 0.7239500284194946 + 10.0 * 9.231203079223633
Epoch 2410, val loss: 0.7409433722496033
Epoch 2420, training loss: 93.09510803222656 = 0.7219482064247131 + 10.0 * 9.237316131591797
Epoch 2420, val loss: 0.739089846611023
Epoch 2430, training loss: 93.10617065429688 = 0.7199728488922119 + 10.0 * 9.238619804382324
Epoch 2430, val loss: 0.737278401851654
Epoch 2440, training loss: 93.0911865234375 = 0.7179962396621704 + 10.0 * 9.237318992614746
Epoch 2440, val loss: 0.7354734539985657
Epoch 2450, training loss: 93.1277084350586 = 0.7160537838935852 + 10.0 * 9.241165161132812
Epoch 2450, val loss: 0.7336927652359009
Epoch 2460, training loss: 92.98503112792969 = 0.7141079306602478 + 10.0 * 9.227091789245605
Epoch 2460, val loss: 0.7319397926330566
Epoch 2470, training loss: 93.05387115478516 = 0.7122921943664551 + 10.0 * 9.23415756225586
Epoch 2470, val loss: 0.7302965521812439
Epoch 2480, training loss: 93.08061981201172 = 0.7103608250617981 + 10.0 * 9.23702621459961
Epoch 2480, val loss: 0.7285047769546509
Epoch 2490, training loss: 93.0948486328125 = 0.7084619402885437 + 10.0 * 9.238637924194336
Epoch 2490, val loss: 0.7268087267875671
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7240579710144928
0.8126494240382526
=== training gcn model ===
Epoch 0, training loss: 102.28124237060547 = 1.0986114740371704 + 10.0 * 10.118263244628906
Epoch 0, val loss: 1.098612904548645
Epoch 10, training loss: 98.58213806152344 = 1.0986114740371704 + 10.0 * 9.748353004455566
Epoch 10, val loss: 1.098612904548645
Epoch 20, training loss: 96.94022369384766 = 1.0986114740371704 + 10.0 * 9.584161758422852
Epoch 20, val loss: 1.098612904548645
Epoch 30, training loss: 95.74690246582031 = 1.0986114740371704 + 10.0 * 9.464829444885254
Epoch 30, val loss: 1.098612904548645
Epoch 40, training loss: 94.77835083007812 = 1.0986114740371704 + 10.0 * 9.367974281311035
Epoch 40, val loss: 1.098612904548645
Epoch 50, training loss: 93.96649932861328 = 1.0986114740371704 + 10.0 * 9.286788940429688
Epoch 50, val loss: 1.098612904548645
Epoch 60, training loss: 93.26178741455078 = 1.0986114740371704 + 10.0 * 9.216318130493164
Epoch 60, val loss: 1.098612904548645
Epoch 70, training loss: 92.6576156616211 = 1.0986114740371704 + 10.0 * 9.155900955200195
Epoch 70, val loss: 1.098612904548645
Epoch 80, training loss: 92.14341735839844 = 1.0986114740371704 + 10.0 * 9.104480743408203
Epoch 80, val loss: 1.098612904548645
Epoch 90, training loss: 91.68914031982422 = 1.0986114740371704 + 10.0 * 9.059053421020508
Epoch 90, val loss: 1.098612904548645
Epoch 100, training loss: 91.29949951171875 = 1.0986114740371704 + 10.0 * 9.020089149475098
Epoch 100, val loss: 1.098612904548645
Epoch 110, training loss: 90.94725799560547 = 1.0986114740371704 + 10.0 * 8.984865188598633
Epoch 110, val loss: 1.098612904548645
Epoch 120, training loss: 90.64573669433594 = 1.0986114740371704 + 10.0 * 8.954712867736816
Epoch 120, val loss: 1.098612904548645
Epoch 130, training loss: 90.38446807861328 = 1.0986114740371704 + 10.0 * 8.92858600616455
Epoch 130, val loss: 1.098612904548645
Epoch 140, training loss: 90.15928649902344 = 1.0986114740371704 + 10.0 * 8.906067848205566
Epoch 140, val loss: 1.098612904548645
Epoch 150, training loss: 89.98188018798828 = 1.0986114740371704 + 10.0 * 8.888326644897461
Epoch 150, val loss: 1.098612904548645
Epoch 160, training loss: 89.80572509765625 = 1.0986114740371704 + 10.0 * 8.870711326599121
Epoch 160, val loss: 1.098612904548645
Epoch 170, training loss: 89.64865112304688 = 1.0986114740371704 + 10.0 * 8.85500431060791
Epoch 170, val loss: 1.098612904548645
Epoch 180, training loss: 89.50749206542969 = 1.0986114740371704 + 10.0 * 8.840888023376465
Epoch 180, val loss: 1.098612904548645
Epoch 190, training loss: 89.41464233398438 = 1.0986114740371704 + 10.0 * 8.831603050231934
Epoch 190, val loss: 1.098612904548645
Epoch 200, training loss: 89.29742431640625 = 1.0986114740371704 + 10.0 * 8.819881439208984
Epoch 200, val loss: 1.098612904548645
Epoch 210, training loss: 89.2194595336914 = 1.0986114740371704 + 10.0 * 8.812085151672363
Epoch 210, val loss: 1.098612904548645
Epoch 220, training loss: 89.14327239990234 = 1.0986114740371704 + 10.0 * 8.804466247558594
Epoch 220, val loss: 1.098612904548645
Epoch 230, training loss: 89.07705688476562 = 1.0986114740371704 + 10.0 * 8.797844886779785
Epoch 230, val loss: 1.098612904548645
Epoch 240, training loss: 88.97935485839844 = 1.0986114740371704 + 10.0 * 8.788074493408203
Epoch 240, val loss: 1.098612904548645
Epoch 250, training loss: 88.9425048828125 = 1.0986114740371704 + 10.0 * 8.78438949584961
Epoch 250, val loss: 1.098612904548645
Epoch 260, training loss: 88.92827606201172 = 1.0986114740371704 + 10.0 * 8.782966613769531
Epoch 260, val loss: 1.098612904548645
Epoch 270, training loss: 88.87349700927734 = 1.0986114740371704 + 10.0 * 8.777488708496094
Epoch 270, val loss: 1.098612904548645
Epoch 280, training loss: 88.83983612060547 = 1.0986114740371704 + 10.0 * 8.77412223815918
Epoch 280, val loss: 1.098612904548645
Epoch 290, training loss: 88.82009887695312 = 1.0986114740371704 + 10.0 * 8.772149085998535
Epoch 290, val loss: 1.098612904548645
Epoch 300, training loss: 88.77637481689453 = 1.0986114740371704 + 10.0 * 8.767776489257812
Epoch 300, val loss: 1.098612904548645
Epoch 310, training loss: 88.77018737792969 = 1.0986114740371704 + 10.0 * 8.767157554626465
Epoch 310, val loss: 1.098612904548645
Epoch 320, training loss: 88.7497787475586 = 1.0986114740371704 + 10.0 * 8.765116691589355
Epoch 320, val loss: 1.098612904548645
Epoch 330, training loss: 88.7668228149414 = 1.0986114740371704 + 10.0 * 8.766820907592773
Epoch 330, val loss: 1.098612904548645
Epoch 340, training loss: 88.7327651977539 = 1.0986114740371704 + 10.0 * 8.763415336608887
Epoch 340, val loss: 1.098612904548645
Epoch 350, training loss: 88.71064758300781 = 1.0986114740371704 + 10.0 * 8.76120376586914
Epoch 350, val loss: 1.098612904548645
Epoch 360, training loss: 88.83338165283203 = 1.0986114740371704 + 10.0 * 8.773477554321289
Epoch 360, val loss: 1.098612904548645
Epoch 370, training loss: 88.95497131347656 = 1.0986114740371704 + 10.0 * 8.785635948181152
Epoch 370, val loss: 1.098612904548645
Epoch 380, training loss: 88.66210174560547 = 1.0986114740371704 + 10.0 * 8.756349563598633
Epoch 380, val loss: 1.098612904548645
Epoch 390, training loss: 88.73037719726562 = 1.0986114740371704 + 10.0 * 8.763176918029785
Epoch 390, val loss: 1.098612904548645
Epoch 400, training loss: 88.65381622314453 = 1.0986114740371704 + 10.0 * 8.755520820617676
Epoch 400, val loss: 1.098612904548645
Epoch 410, training loss: 88.7192611694336 = 1.0986114740371704 + 10.0 * 8.762064933776855
Epoch 410, val loss: 1.098612904548645
Epoch 420, training loss: 88.76988220214844 = 1.0986114740371704 + 10.0 * 8.76712703704834
Epoch 420, val loss: 1.098612904548645
Epoch 430, training loss: 88.76570892333984 = 1.0986114740371704 + 10.0 * 8.76671028137207
Epoch 430, val loss: 1.098612904548645
Epoch 440, training loss: 88.76718139648438 = 1.0986114740371704 + 10.0 * 8.766857147216797
Epoch 440, val loss: 1.098612904548645
Epoch 450, training loss: 88.75643157958984 = 1.0986114740371704 + 10.0 * 8.765782356262207
Epoch 450, val loss: 1.098612904548645
Epoch 460, training loss: 88.76844024658203 = 1.0986114740371704 + 10.0 * 8.766983032226562
Epoch 460, val loss: 1.098612904548645
Epoch 470, training loss: 88.76229858398438 = 1.0986114740371704 + 10.0 * 8.766368865966797
Epoch 470, val loss: 1.098612904548645
Epoch 480, training loss: 88.78849029541016 = 1.0986114740371704 + 10.0 * 8.768987655639648
Epoch 480, val loss: 1.098612904548645
Epoch 490, training loss: 88.77678680419922 = 1.0986114740371704 + 10.0 * 8.767817497253418
Epoch 490, val loss: 1.098612904548645
Epoch 500, training loss: 88.80675506591797 = 1.0986114740371704 + 10.0 * 8.770814895629883
Epoch 500, val loss: 1.098612904548645
Epoch 510, training loss: 88.7992172241211 = 1.0986114740371704 + 10.0 * 8.770060539245605
Epoch 510, val loss: 1.098612904548645
Epoch 520, training loss: 88.80192565917969 = 1.0986114740371704 + 10.0 * 8.770331382751465
Epoch 520, val loss: 1.098612904548645
Epoch 530, training loss: 88.82688903808594 = 1.0986114740371704 + 10.0 * 8.772828102111816
Epoch 530, val loss: 1.098612904548645
Epoch 540, training loss: 88.85343170166016 = 1.0986114740371704 + 10.0 * 8.775482177734375
Epoch 540, val loss: 1.098612904548645
Epoch 550, training loss: 88.85440826416016 = 1.0986114740371704 + 10.0 * 8.775579452514648
Epoch 550, val loss: 1.098612904548645
Epoch 560, training loss: 88.87448120117188 = 1.0986114740371704 + 10.0 * 8.777586936950684
Epoch 560, val loss: 1.098612904548645
Epoch 570, training loss: 88.87120056152344 = 1.0986114740371704 + 10.0 * 8.77725887298584
Epoch 570, val loss: 1.098612904548645
Epoch 580, training loss: 88.9201889038086 = 1.0986114740371704 + 10.0 * 8.782157897949219
Epoch 580, val loss: 1.098612904548645
Epoch 590, training loss: 88.95024108886719 = 1.0986114740371704 + 10.0 * 8.785162925720215
Epoch 590, val loss: 1.098612904548645
Epoch 600, training loss: 88.96326446533203 = 1.0986114740371704 + 10.0 * 8.786465644836426
Epoch 600, val loss: 1.098612904548645
Epoch 610, training loss: 88.91390228271484 = 1.0986114740371704 + 10.0 * 8.781529426574707
Epoch 610, val loss: 1.098612904548645
Epoch 620, training loss: 88.93218231201172 = 1.0986114740371704 + 10.0 * 8.783357620239258
Epoch 620, val loss: 1.098612904548645
Epoch 630, training loss: 88.9638671875 = 1.0986114740371704 + 10.0 * 8.78652572631836
Epoch 630, val loss: 1.098612904548645
Epoch 640, training loss: 88.97534942626953 = 1.0986114740371704 + 10.0 * 8.787673950195312
Epoch 640, val loss: 1.098612904548645
Epoch 650, training loss: 89.00300598144531 = 1.0986114740371704 + 10.0 * 8.79043960571289
Epoch 650, val loss: 1.098612904548645
Epoch 660, training loss: 89.0060806274414 = 1.0986114740371704 + 10.0 * 8.790746688842773
Epoch 660, val loss: 1.098612904548645
Epoch 670, training loss: 89.0141830444336 = 1.0986114740371704 + 10.0 * 8.791557312011719
Epoch 670, val loss: 1.098612904548645
Epoch 680, training loss: 89.03987884521484 = 1.0986114740371704 + 10.0 * 8.794126510620117
Epoch 680, val loss: 1.098612904548645
Epoch 690, training loss: 89.02812957763672 = 1.0986114740371704 + 10.0 * 8.792951583862305
Epoch 690, val loss: 1.098612904548645
Epoch 700, training loss: 89.02812957763672 = 1.0986114740371704 + 10.0 * 8.792951583862305
Epoch 700, val loss: 1.098612904548645
Epoch 710, training loss: 89.05663299560547 = 1.0986114740371704 + 10.0 * 8.795802116394043
Epoch 710, val loss: 1.098612904548645
Epoch 720, training loss: 89.06305694580078 = 1.0986114740371704 + 10.0 * 8.7964448928833
Epoch 720, val loss: 1.098612904548645
Epoch 730, training loss: 89.09306335449219 = 1.0986114740371704 + 10.0 * 8.799445152282715
Epoch 730, val loss: 1.098612904548645
Epoch 740, training loss: 89.09339141845703 = 1.0986114740371704 + 10.0 * 8.799478530883789
Epoch 740, val loss: 1.098612904548645
Epoch 750, training loss: 89.14791870117188 = 1.0986114740371704 + 10.0 * 8.804930686950684
Epoch 750, val loss: 1.098612904548645
Epoch 760, training loss: 89.15599822998047 = 1.0986114740371704 + 10.0 * 8.80573844909668
Epoch 760, val loss: 1.098612904548645
Epoch 770, training loss: 89.15731048583984 = 1.0986114740371704 + 10.0 * 8.805870056152344
Epoch 770, val loss: 1.098612904548645
Epoch 780, training loss: 89.17305755615234 = 1.0986114740371704 + 10.0 * 8.80744457244873
Epoch 780, val loss: 1.0986127853393555
Epoch 790, training loss: 89.20789337158203 = 1.0986114740371704 + 10.0 * 8.810928344726562
Epoch 790, val loss: 1.0986127853393555
Epoch 800, training loss: 89.20960235595703 = 1.0986114740371704 + 10.0 * 8.8110990524292
Epoch 800, val loss: 1.0986127853393555
Epoch 810, training loss: 89.27674865722656 = 1.0986114740371704 + 10.0 * 8.817813873291016
Epoch 810, val loss: 1.0986127853393555
Epoch 820, training loss: 89.2389144897461 = 1.0986114740371704 + 10.0 * 8.814030647277832
Epoch 820, val loss: 1.0986127853393555
Epoch 830, training loss: 89.11312103271484 = 1.0986114740371704 + 10.0 * 8.801450729370117
Epoch 830, val loss: 1.0986127853393555
Epoch 840, training loss: 89.27275848388672 = 1.0986114740371704 + 10.0 * 8.817415237426758
Epoch 840, val loss: 1.0986127853393555
Epoch 850, training loss: 89.2287368774414 = 1.0986114740371704 + 10.0 * 8.813013076782227
Epoch 850, val loss: 1.0986127853393555
Epoch 860, training loss: 89.30825805664062 = 1.0986114740371704 + 10.0 * 8.820964813232422
Epoch 860, val loss: 1.0986127853393555
Epoch 870, training loss: 89.30313873291016 = 1.0986114740371704 + 10.0 * 8.820452690124512
Epoch 870, val loss: 1.0986127853393555
Epoch 880, training loss: 89.33798217773438 = 1.0986114740371704 + 10.0 * 8.82393741607666
Epoch 880, val loss: 1.0986127853393555
Epoch 890, training loss: 89.36348724365234 = 1.0986114740371704 + 10.0 * 8.82648754119873
Epoch 890, val loss: 1.0986127853393555
Epoch 900, training loss: 89.40778350830078 = 1.0986114740371704 + 10.0 * 8.830917358398438
Epoch 900, val loss: 1.0986127853393555
Epoch 910, training loss: 89.35540008544922 = 1.0986114740371704 + 10.0 * 8.825678825378418
Epoch 910, val loss: 1.0986127853393555
Epoch 920, training loss: 89.40044403076172 = 1.0986114740371704 + 10.0 * 8.830183029174805
Epoch 920, val loss: 1.0986127853393555
Epoch 930, training loss: 89.46009063720703 = 1.0986114740371704 + 10.0 * 8.836148262023926
Epoch 930, val loss: 1.0986462831497192
Epoch 940, training loss: 89.40357208251953 = 1.0986114740371704 + 10.0 * 8.830495834350586
Epoch 940, val loss: 1.098612904548645
Epoch 950, training loss: 89.43173217773438 = 1.0986114740371704 + 10.0 * 8.833312034606934
Epoch 950, val loss: 1.098612904548645
Epoch 960, training loss: 89.45215606689453 = 1.0986114740371704 + 10.0 * 8.835354804992676
Epoch 960, val loss: 1.098612904548645
Epoch 970, training loss: 89.47915649414062 = 1.0986114740371704 + 10.0 * 8.838054656982422
Epoch 970, val loss: 1.098612904548645
Epoch 980, training loss: 89.55391693115234 = 1.0986114740371704 + 10.0 * 8.84553050994873
Epoch 980, val loss: 1.098612904548645
Epoch 990, training loss: 89.55372619628906 = 1.0986114740371704 + 10.0 * 8.845511436462402
Epoch 990, val loss: 1.098612904548645
Epoch 1000, training loss: 89.55461120605469 = 1.0986114740371704 + 10.0 * 8.845600128173828
Epoch 1000, val loss: 1.098612904548645
Epoch 1010, training loss: 89.51814270019531 = 1.0986114740371704 + 10.0 * 8.84195327758789
Epoch 1010, val loss: 1.098612904548645
Epoch 1020, training loss: 89.54607391357422 = 1.0986114740371704 + 10.0 * 8.844746589660645
Epoch 1020, val loss: 1.098612904548645
Epoch 1030, training loss: 89.58515930175781 = 1.0986114740371704 + 10.0 * 8.848654747009277
Epoch 1030, val loss: 1.098612904548645
Epoch 1040, training loss: 89.56629180908203 = 1.0986114740371704 + 10.0 * 8.846768379211426
Epoch 1040, val loss: 1.098612904548645
Epoch 1050, training loss: 89.61299896240234 = 1.0986114740371704 + 10.0 * 8.851438522338867
Epoch 1050, val loss: 1.098612904548645
Epoch 1060, training loss: 89.64268493652344 = 1.0986114740371704 + 10.0 * 8.85440731048584
Epoch 1060, val loss: 1.098612904548645
Epoch 1070, training loss: 89.63355255126953 = 1.0986114740371704 + 10.0 * 8.853494644165039
Epoch 1070, val loss: 1.098612904548645
Epoch 1080, training loss: 89.65211486816406 = 1.0986114740371704 + 10.0 * 8.855350494384766
Epoch 1080, val loss: 1.098612904548645
Epoch 1090, training loss: 89.64237213134766 = 1.0986114740371704 + 10.0 * 8.854375839233398
Epoch 1090, val loss: 1.098612904548645
Epoch 1100, training loss: 89.65550994873047 = 1.0986114740371704 + 10.0 * 8.855690002441406
Epoch 1100, val loss: 1.098612904548645
Epoch 1110, training loss: 89.69596099853516 = 1.0986114740371704 + 10.0 * 8.859735488891602
Epoch 1110, val loss: 1.098612904548645
Epoch 1120, training loss: 89.71492767333984 = 1.0986114740371704 + 10.0 * 8.861631393432617
Epoch 1120, val loss: 1.098612904548645
Epoch 1130, training loss: 89.7461929321289 = 1.0986114740371704 + 10.0 * 8.864758491516113
Epoch 1130, val loss: 1.098612904548645
Epoch 1140, training loss: 89.73583221435547 = 1.0986114740371704 + 10.0 * 8.86372184753418
Epoch 1140, val loss: 1.098612904548645
Epoch 1150, training loss: 89.77141571044922 = 1.0986114740371704 + 10.0 * 8.867280960083008
Epoch 1150, val loss: 1.098612904548645
Epoch 1160, training loss: 89.78975677490234 = 1.0986114740371704 + 10.0 * 8.869114875793457
Epoch 1160, val loss: 1.098612904548645
Epoch 1170, training loss: 89.8092269897461 = 1.0986114740371704 + 10.0 * 8.871061325073242
Epoch 1170, val loss: 1.098612904548645
Epoch 1180, training loss: 89.82672119140625 = 1.0986114740371704 + 10.0 * 8.872811317443848
Epoch 1180, val loss: 1.098612904548645
Epoch 1190, training loss: 89.86005401611328 = 1.0986114740371704 + 10.0 * 8.876144409179688
Epoch 1190, val loss: 1.098612904548645
Epoch 1200, training loss: 89.79544830322266 = 1.0986114740371704 + 10.0 * 8.869684219360352
Epoch 1200, val loss: 1.098612904548645
Epoch 1210, training loss: 89.89444732666016 = 1.0986114740371704 + 10.0 * 8.879583358764648
Epoch 1210, val loss: 1.098612904548645
Epoch 1220, training loss: 89.92017364501953 = 1.0986114740371704 + 10.0 * 8.882156372070312
Epoch 1220, val loss: 1.098612904548645
Epoch 1230, training loss: 89.868408203125 = 1.0986114740371704 + 10.0 * 8.87697982788086
Epoch 1230, val loss: 1.098612904548645
Epoch 1240, training loss: 89.88758087158203 = 1.0986114740371704 + 10.0 * 8.878896713256836
Epoch 1240, val loss: 1.098612904548645
Epoch 1250, training loss: 89.85248565673828 = 1.0986114740371704 + 10.0 * 8.875387191772461
Epoch 1250, val loss: 1.098612904548645
Epoch 1260, training loss: 89.88331604003906 = 1.0986114740371704 + 10.0 * 8.878470420837402
Epoch 1260, val loss: 1.098612904548645
Epoch 1270, training loss: 89.90709686279297 = 1.0986114740371704 + 10.0 * 8.88084888458252
Epoch 1270, val loss: 1.098612904548645
Epoch 1280, training loss: 89.91910552978516 = 1.0986114740371704 + 10.0 * 8.882049560546875
Epoch 1280, val loss: 1.098612904548645
Epoch 1290, training loss: 89.95870971679688 = 1.0986114740371704 + 10.0 * 8.88601016998291
Epoch 1290, val loss: 1.098612904548645
Epoch 1300, training loss: 89.93183898925781 = 1.0986114740371704 + 10.0 * 8.883322715759277
Epoch 1300, val loss: 1.098612904548645
Epoch 1310, training loss: 89.76429748535156 = 1.0986114740371704 + 10.0 * 8.866568565368652
Epoch 1310, val loss: 1.098612904548645
Epoch 1320, training loss: 89.7945785522461 = 1.0986114740371704 + 10.0 * 8.869596481323242
Epoch 1320, val loss: 1.098612904548645
Epoch 1330, training loss: 89.90131378173828 = 1.0986114740371704 + 10.0 * 8.880270004272461
Epoch 1330, val loss: 1.098612904548645
Epoch 1340, training loss: 89.96981048583984 = 1.0986114740371704 + 10.0 * 8.887120246887207
Epoch 1340, val loss: 1.098612904548645
Epoch 1350, training loss: 90.0373306274414 = 1.0986114740371704 + 10.0 * 8.893872261047363
Epoch 1350, val loss: 1.098612904548645
Epoch 1360, training loss: 90.0306625366211 = 1.0986114740371704 + 10.0 * 8.893205642700195
Epoch 1360, val loss: 1.098612904548645
Epoch 1370, training loss: 90.05594635009766 = 1.0986114740371704 + 10.0 * 8.895733833312988
Epoch 1370, val loss: 1.098612904548645
Epoch 1380, training loss: 90.05290985107422 = 1.0986114740371704 + 10.0 * 8.895429611206055
Epoch 1380, val loss: 1.098612904548645
Epoch 1390, training loss: 90.0811538696289 = 1.0986114740371704 + 10.0 * 8.89825439453125
Epoch 1390, val loss: 1.098612904548645
Epoch 1400, training loss: 90.12785339355469 = 1.0986114740371704 + 10.0 * 8.902924537658691
Epoch 1400, val loss: 1.098612904548645
Epoch 1410, training loss: 90.13861083984375 = 1.0986114740371704 + 10.0 * 8.904000282287598
Epoch 1410, val loss: 1.098612904548645
Epoch 1420, training loss: 90.10588073730469 = 1.0986114740371704 + 10.0 * 8.900727272033691
Epoch 1420, val loss: 1.098612904548645
Epoch 1430, training loss: 90.1448745727539 = 1.0986114740371704 + 10.0 * 8.904626846313477
Epoch 1430, val loss: 1.098612904548645
Epoch 1440, training loss: 90.14177703857422 = 1.0986114740371704 + 10.0 * 8.904316902160645
Epoch 1440, val loss: 1.098612904548645
Epoch 1450, training loss: 90.18352508544922 = 1.0986114740371704 + 10.0 * 8.908491134643555
Epoch 1450, val loss: 1.098612904548645
Epoch 1460, training loss: 90.20366668701172 = 1.0986114740371704 + 10.0 * 8.910505294799805
Epoch 1460, val loss: 1.098612904548645
Epoch 1470, training loss: 90.13973999023438 = 1.0986114740371704 + 10.0 * 8.904112815856934
Epoch 1470, val loss: 1.098612904548645
Epoch 1480, training loss: 90.16264343261719 = 1.0986114740371704 + 10.0 * 8.906403541564941
Epoch 1480, val loss: 1.098612904548645
Epoch 1490, training loss: 90.22382354736328 = 1.0986114740371704 + 10.0 * 8.912521362304688
Epoch 1490, val loss: 1.098612904548645
Epoch 1500, training loss: 90.26685333251953 = 1.0986114740371704 + 10.0 * 8.916824340820312
Epoch 1500, val loss: 1.098612904548645
Epoch 1510, training loss: 90.26085662841797 = 1.0986114740371704 + 10.0 * 8.916224479675293
Epoch 1510, val loss: 1.098612904548645
Epoch 1520, training loss: 90.28840637207031 = 1.0986114740371704 + 10.0 * 8.91897964477539
Epoch 1520, val loss: 1.098612904548645
Epoch 1530, training loss: 90.30876922607422 = 1.0986114740371704 + 10.0 * 8.921015739440918
Epoch 1530, val loss: 1.098612904548645
Epoch 1540, training loss: 90.27777099609375 = 1.0986114740371704 + 10.0 * 8.917916297912598
Epoch 1540, val loss: 1.098612904548645
Epoch 1550, training loss: 90.15338897705078 = 1.0986114740371704 + 10.0 * 8.905477523803711
Epoch 1550, val loss: 1.098612904548645
Epoch 1560, training loss: 90.13528442382812 = 1.0986114740371704 + 10.0 * 8.903667449951172
Epoch 1560, val loss: 1.098612904548645
Epoch 1570, training loss: 90.032470703125 = 1.0986114740371704 + 10.0 * 8.893385887145996
Epoch 1570, val loss: 1.098612904548645
Epoch 1580, training loss: 90.08672332763672 = 1.0986114740371704 + 10.0 * 8.898811340332031
Epoch 1580, val loss: 1.098612904548645
Epoch 1590, training loss: 90.19975280761719 = 1.0986114740371704 + 10.0 * 8.910114288330078
Epoch 1590, val loss: 1.098612904548645
Epoch 1600, training loss: 90.26619720458984 = 1.0986114740371704 + 10.0 * 8.91675853729248
Epoch 1600, val loss: 1.098612904548645
Epoch 1610, training loss: 90.31887817382812 = 1.0986114740371704 + 10.0 * 8.922026634216309
Epoch 1610, val loss: 1.098612904548645
Epoch 1620, training loss: 90.36454772949219 = 1.0986114740371704 + 10.0 * 8.926593780517578
Epoch 1620, val loss: 1.098612904548645
Epoch 1630, training loss: 90.32817840576172 = 1.0986114740371704 + 10.0 * 8.922956466674805
Epoch 1630, val loss: 1.098612904548645
Epoch 1640, training loss: 90.35266876220703 = 1.0986114740371704 + 10.0 * 8.925405502319336
Epoch 1640, val loss: 1.098612904548645
Epoch 1650, training loss: 90.39710235595703 = 1.0986114740371704 + 10.0 * 8.929849624633789
Epoch 1650, val loss: 1.098612904548645
Epoch 1660, training loss: 90.35771179199219 = 1.0986114740371704 + 10.0 * 8.925909996032715
Epoch 1660, val loss: 1.098612904548645
Epoch 1670, training loss: 90.36072540283203 = 1.0986114740371704 + 10.0 * 8.9262113571167
Epoch 1670, val loss: 1.098612904548645
Epoch 1680, training loss: 90.41963195800781 = 1.0986114740371704 + 10.0 * 8.93210220336914
Epoch 1680, val loss: 1.098612904548645
Epoch 1690, training loss: 90.3856430053711 = 1.0986114740371704 + 10.0 * 8.928703308105469
Epoch 1690, val loss: 1.098612904548645
Epoch 1700, training loss: 90.38457489013672 = 1.0986114740371704 + 10.0 * 8.928596496582031
Epoch 1700, val loss: 1.098612904548645
Epoch 1710, training loss: 90.44895935058594 = 1.0986114740371704 + 10.0 * 8.93503475189209
Epoch 1710, val loss: 1.098612904548645
Epoch 1720, training loss: 90.44939422607422 = 1.0986114740371704 + 10.0 * 8.935078620910645
Epoch 1720, val loss: 1.098612904548645
Epoch 1730, training loss: 90.48011016845703 = 1.0986114740371704 + 10.0 * 8.938150405883789
Epoch 1730, val loss: 1.098612904548645
Epoch 1740, training loss: 90.4627685546875 = 1.0986114740371704 + 10.0 * 8.936415672302246
Epoch 1740, val loss: 1.098612904548645
Epoch 1750, training loss: 90.35447692871094 = 1.0986114740371704 + 10.0 * 8.925586700439453
Epoch 1750, val loss: 1.098612904548645
Epoch 1760, training loss: 90.44095611572266 = 1.0986114740371704 + 10.0 * 8.934234619140625
Epoch 1760, val loss: 1.098612904548645
Epoch 1770, training loss: 90.48101806640625 = 1.0986114740371704 + 10.0 * 8.938241004943848
Epoch 1770, val loss: 1.098612904548645
Epoch 1780, training loss: 90.5428237915039 = 1.0986114740371704 + 10.0 * 8.944421768188477
Epoch 1780, val loss: 1.098612904548645
Epoch 1790, training loss: 90.53707122802734 = 1.0986114740371704 + 10.0 * 8.943845748901367
Epoch 1790, val loss: 1.098612904548645
Epoch 1800, training loss: 90.54381561279297 = 1.0986114740371704 + 10.0 * 8.944520950317383
Epoch 1800, val loss: 1.098612904548645
Epoch 1810, training loss: 90.59552764892578 = 1.0986114740371704 + 10.0 * 8.949691772460938
Epoch 1810, val loss: 1.098612904548645
Epoch 1820, training loss: 90.60591888427734 = 1.0986114740371704 + 10.0 * 8.95073127746582
Epoch 1820, val loss: 1.098612904548645
Epoch 1830, training loss: 90.62015533447266 = 1.0986114740371704 + 10.0 * 8.952154159545898
Epoch 1830, val loss: 1.098612904548645
Epoch 1840, training loss: 90.63343048095703 = 1.0986114740371704 + 10.0 * 8.953481674194336
Epoch 1840, val loss: 1.098612904548645
Epoch 1850, training loss: 90.67675018310547 = 1.0986114740371704 + 10.0 * 8.95781421661377
Epoch 1850, val loss: 1.098612904548645
Epoch 1860, training loss: 90.62029266357422 = 1.0986114740371704 + 10.0 * 8.952168464660645
Epoch 1860, val loss: 1.098612904548645
Epoch 1870, training loss: 90.64810943603516 = 1.0986114740371704 + 10.0 * 8.954950332641602
Epoch 1870, val loss: 1.098612904548645
Epoch 1880, training loss: 90.67696380615234 = 1.0986114740371704 + 10.0 * 8.95783519744873
Epoch 1880, val loss: 1.098612904548645
Epoch 1890, training loss: 90.75369262695312 = 1.0986114740371704 + 10.0 * 8.965508460998535
Epoch 1890, val loss: 1.098612904548645
Epoch 1900, training loss: 90.67701721191406 = 1.0986114740371704 + 10.0 * 8.957840919494629
Epoch 1900, val loss: 1.098612904548645
Epoch 1910, training loss: 90.57654571533203 = 1.0986114740371704 + 10.0 * 8.947793960571289
Epoch 1910, val loss: 1.098612904548645
Epoch 1920, training loss: 90.63992309570312 = 1.0986114740371704 + 10.0 * 8.954131126403809
Epoch 1920, val loss: 1.098612904548645
Epoch 1930, training loss: 90.74076080322266 = 1.0986114740371704 + 10.0 * 8.964215278625488
Epoch 1930, val loss: 1.098612904548645
Epoch 1940, training loss: 90.71477508544922 = 1.0986114740371704 + 10.0 * 8.961616516113281
Epoch 1940, val loss: 1.098612904548645
Epoch 1950, training loss: 90.75048065185547 = 1.0986114740371704 + 10.0 * 8.965187072753906
Epoch 1950, val loss: 1.098612904548645
Epoch 1960, training loss: 90.74852752685547 = 1.0986114740371704 + 10.0 * 8.964991569519043
Epoch 1960, val loss: 1.098612904548645
Epoch 1970, training loss: 90.71231842041016 = 1.0986114740371704 + 10.0 * 8.961370468139648
Epoch 1970, val loss: 1.098612904548645
Epoch 1980, training loss: 90.7791748046875 = 1.0986114740371704 + 10.0 * 8.968056678771973
Epoch 1980, val loss: 1.098612904548645
Epoch 1990, training loss: 90.8293685913086 = 1.0986114740371704 + 10.0 * 8.973075866699219
Epoch 1990, val loss: 1.098612904548645
Epoch 2000, training loss: 90.79961395263672 = 1.0986114740371704 + 10.0 * 8.970100402832031
Epoch 2000, val loss: 1.098612904548645
Epoch 2010, training loss: 90.81441497802734 = 1.0986114740371704 + 10.0 * 8.971580505371094
Epoch 2010, val loss: 1.098612904548645
Epoch 2020, training loss: 90.85549926757812 = 1.0986114740371704 + 10.0 * 8.975688934326172
Epoch 2020, val loss: 1.098612904548645
Epoch 2030, training loss: 90.86762237548828 = 1.0986114740371704 + 10.0 * 8.976901054382324
Epoch 2030, val loss: 1.098612904548645
Epoch 2040, training loss: 90.88842010498047 = 1.0986114740371704 + 10.0 * 8.978981018066406
Epoch 2040, val loss: 1.098612904548645
Epoch 2050, training loss: 90.89415740966797 = 1.0986114740371704 + 10.0 * 8.979555130004883
Epoch 2050, val loss: 1.098612904548645
Epoch 2060, training loss: 90.74235534667969 = 1.0986114740371704 + 10.0 * 8.964374542236328
Epoch 2060, val loss: 1.098612904548645
Epoch 2070, training loss: 90.75457000732422 = 1.0986114740371704 + 10.0 * 8.965596199035645
Epoch 2070, val loss: 1.098612904548645
Epoch 2080, training loss: 90.79915618896484 = 1.0986114740371704 + 10.0 * 8.970054626464844
Epoch 2080, val loss: 1.098612904548645
Epoch 2090, training loss: 90.85567474365234 = 1.0986114740371704 + 10.0 * 8.975706100463867
Epoch 2090, val loss: 1.098612904548645
Epoch 2100, training loss: 90.92337799072266 = 1.0986114740371704 + 10.0 * 8.982477188110352
Epoch 2100, val loss: 1.098612904548645
Epoch 2110, training loss: 90.89657592773438 = 1.0986114740371704 + 10.0 * 8.979796409606934
Epoch 2110, val loss: 1.098612904548645
Epoch 2120, training loss: 90.92565155029297 = 1.0986114740371704 + 10.0 * 8.982704162597656
Epoch 2120, val loss: 1.098612904548645
Epoch 2130, training loss: 90.94725799560547 = 1.0986114740371704 + 10.0 * 8.984865188598633
Epoch 2130, val loss: 1.098612904548645
Epoch 2140, training loss: 90.99100494384766 = 1.0986114740371704 + 10.0 * 8.989239692687988
Epoch 2140, val loss: 1.098612904548645
Epoch 2150, training loss: 90.9737319946289 = 1.0986114740371704 + 10.0 * 8.987512588500977
Epoch 2150, val loss: 1.098612904548645
Epoch 2160, training loss: 91.02545928955078 = 1.0986114740371704 + 10.0 * 8.992685317993164
Epoch 2160, val loss: 1.098612904548645
Epoch 2170, training loss: 91.01052856445312 = 1.0986114740371704 + 10.0 * 8.991191864013672
Epoch 2170, val loss: 1.098612904548645
Epoch 2180, training loss: 91.02376556396484 = 1.0986114740371704 + 10.0 * 8.992515563964844
Epoch 2180, val loss: 1.098612904548645
Epoch 2190, training loss: 91.04974365234375 = 1.0986114740371704 + 10.0 * 8.995113372802734
Epoch 2190, val loss: 1.098612904548645
Epoch 2200, training loss: 91.01716613769531 = 1.0986114740371704 + 10.0 * 8.99185562133789
Epoch 2200, val loss: 1.098612904548645
Epoch 2210, training loss: 91.00140380859375 = 1.0986114740371704 + 10.0 * 8.990279197692871
Epoch 2210, val loss: 1.098612904548645
Epoch 2220, training loss: 91.04607391357422 = 1.0986114740371704 + 10.0 * 8.994746208190918
Epoch 2220, val loss: 1.098612904548645
Epoch 2230, training loss: 91.0851821899414 = 1.0986114740371704 + 10.0 * 8.9986572265625
Epoch 2230, val loss: 1.098612904548645
Epoch 2240, training loss: 91.07393646240234 = 1.0986114740371704 + 10.0 * 8.997532844543457
Epoch 2240, val loss: 1.098612904548645
Epoch 2250, training loss: 91.08768463134766 = 1.0986114740371704 + 10.0 * 8.998907089233398
Epoch 2250, val loss: 1.098612904548645
Epoch 2260, training loss: 91.13365936279297 = 1.0983768701553345 + 10.0 * 9.003528594970703
Epoch 2260, val loss: 1.098331332206726
Epoch 2270, training loss: 91.09812927246094 = 1.0974446535110474 + 10.0 * 9.000068664550781
Epoch 2270, val loss: 1.0974985361099243
Epoch 2280, training loss: 91.08334350585938 = 1.0965359210968018 + 10.0 * 8.99868106842041
Epoch 2280, val loss: 1.096712350845337
Epoch 2290, training loss: 91.16143798828125 = 1.0957427024841309 + 10.0 * 9.006569862365723
Epoch 2290, val loss: 1.0960291624069214
Epoch 2300, training loss: 91.1600570678711 = 1.0950403213500977 + 10.0 * 9.006502151489258
Epoch 2300, val loss: 1.0954210758209229
Epoch 2310, training loss: 90.96198272705078 = 1.094382882118225 + 10.0 * 8.986760139465332
Epoch 2310, val loss: 1.094845175743103
Epoch 2320, training loss: 90.90975952148438 = 1.0937312841415405 + 10.0 * 8.981602668762207
Epoch 2320, val loss: 1.0942723751068115
Epoch 2330, training loss: 90.9703598022461 = 1.093099594116211 + 10.0 * 8.987726211547852
Epoch 2330, val loss: 1.0937130451202393
Epoch 2340, training loss: 91.0384292602539 = 1.09247624874115 + 10.0 * 8.994595527648926
Epoch 2340, val loss: 1.0931622982025146
Epoch 2350, training loss: 91.1127700805664 = 1.0918669700622559 + 10.0 * 9.002090454101562
Epoch 2350, val loss: 1.0926254987716675
Epoch 2360, training loss: 91.1504135131836 = 1.0912669897079468 + 10.0 * 9.005914688110352
Epoch 2360, val loss: 1.092096209526062
Epoch 2370, training loss: 91.16755676269531 = 1.090666651725769 + 10.0 * 9.007688522338867
Epoch 2370, val loss: 1.0915652513504028
Epoch 2380, training loss: 91.16352844238281 = 1.0900628566741943 + 10.0 * 9.007346153259277
Epoch 2380, val loss: 1.0910276174545288
Epoch 2390, training loss: 91.1111068725586 = 1.089454174041748 + 10.0 * 9.002164840698242
Epoch 2390, val loss: 1.0904901027679443
Epoch 2400, training loss: 91.1542739868164 = 1.0888408422470093 + 10.0 * 9.006543159484863
Epoch 2400, val loss: 1.0899461507797241
Epoch 2410, training loss: 91.20922088623047 = 1.0882307291030884 + 10.0 * 9.01209831237793
Epoch 2410, val loss: 1.0894018411636353
Epoch 2420, training loss: 91.18885040283203 = 1.0876103639602661 + 10.0 * 9.010124206542969
Epoch 2420, val loss: 1.0888508558273315
Epoch 2430, training loss: 91.21499633789062 = 1.0869935750961304 + 10.0 * 9.012800216674805
Epoch 2430, val loss: 1.0882972478866577
Epoch 2440, training loss: 91.25331115722656 = 1.0863759517669678 + 10.0 * 9.016694068908691
Epoch 2440, val loss: 1.0877470970153809
Epoch 2450, training loss: 91.25244903564453 = 1.0857489109039307 + 10.0 * 9.016670227050781
Epoch 2450, val loss: 1.0871772766113281
Epoch 2460, training loss: 91.23538208007812 = 1.085097312927246 + 10.0 * 9.01502799987793
Epoch 2460, val loss: 1.086595058441162
Epoch 2470, training loss: 91.23993682861328 = 1.084433913230896 + 10.0 * 9.01555061340332
Epoch 2470, val loss: 1.0859931707382202
Epoch 2480, training loss: 91.29638671875 = 1.08375883102417 + 10.0 * 9.021263122558594
Epoch 2480, val loss: 1.0853838920593262
Epoch 2490, training loss: 91.29539489746094 = 1.083082914352417 + 10.0 * 9.021230697631836
Epoch 2490, val loss: 1.0847712755203247
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.39666666666666667
0.8123596319640659
The final CL Acc:0.54787, 0.13482, The final GNN Acc:0.81337, 0.00124
raw graph: torch.Size([2, 88648])
add edge: torch.Size([2, 110758])
remove edge: torch.Size([2, 66442])
updated graph: torch.Size([2, 88552])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 103.14275360107422 = 1.115040898323059 + 10.0 * 10.202771186828613
Epoch 0, val loss: 1.114938735961914
Epoch 10, training loss: 98.98406982421875 = 1.1143977642059326 + 10.0 * 9.786967277526855
Epoch 10, val loss: 1.1143184900283813
Epoch 20, training loss: 96.86873626708984 = 1.1139163970947266 + 10.0 * 9.575482368469238
Epoch 20, val loss: 1.11382257938385
Epoch 30, training loss: 95.42911529541016 = 1.1133888959884644 + 10.0 * 9.431572914123535
Epoch 30, val loss: 1.113282322883606
Epoch 40, training loss: 94.29810333251953 = 1.1128778457641602 + 10.0 * 9.318522453308105
Epoch 40, val loss: 1.1127636432647705
Epoch 50, training loss: 93.3757095336914 = 1.1123735904693604 + 10.0 * 9.226333618164062
Epoch 50, val loss: 1.1122467517852783
Epoch 60, training loss: 92.61097717285156 = 1.111883282661438 + 10.0 * 9.149909019470215
Epoch 60, val loss: 1.1117382049560547
Epoch 70, training loss: 91.97012329101562 = 1.1113773584365845 + 10.0 * 9.085874557495117
Epoch 70, val loss: 1.1112196445465088
Epoch 80, training loss: 91.4234619140625 = 1.1108781099319458 + 10.0 * 9.031258583068848
Epoch 80, val loss: 1.110707402229309
Epoch 90, training loss: 90.96409606933594 = 1.110364317893982 + 10.0 * 8.985372543334961
Epoch 90, val loss: 1.1101750135421753
Epoch 100, training loss: 90.56236267089844 = 1.1098439693450928 + 10.0 * 8.945252418518066
Epoch 100, val loss: 1.10963773727417
Epoch 110, training loss: 90.21094512939453 = 1.1093170642852783 + 10.0 * 8.910162925720215
Epoch 110, val loss: 1.1090995073318481
Epoch 120, training loss: 89.9052505493164 = 1.1087942123413086 + 10.0 * 8.879645347595215
Epoch 120, val loss: 1.1085574626922607
Epoch 130, training loss: 89.64471435546875 = 1.1082602739334106 + 10.0 * 8.853645324707031
Epoch 130, val loss: 1.1080045700073242
Epoch 140, training loss: 89.42129516601562 = 1.1077239513397217 + 10.0 * 8.8313570022583
Epoch 140, val loss: 1.1074589490890503
Epoch 150, training loss: 89.22138977050781 = 1.1071876287460327 + 10.0 * 8.811420440673828
Epoch 150, val loss: 1.1069086790084839
Epoch 160, training loss: 89.03697967529297 = 1.1066443920135498 + 10.0 * 8.793033599853516
Epoch 160, val loss: 1.1063404083251953
Epoch 170, training loss: 88.89634704589844 = 1.106095552444458 + 10.0 * 8.779025077819824
Epoch 170, val loss: 1.1057792901992798
Epoch 180, training loss: 88.760986328125 = 1.1055347919464111 + 10.0 * 8.765544891357422
Epoch 180, val loss: 1.105201005935669
Epoch 190, training loss: 88.6718978881836 = 1.104993224143982 + 10.0 * 8.75669002532959
Epoch 190, val loss: 1.1046299934387207
Epoch 200, training loss: 88.59442901611328 = 1.104427695274353 + 10.0 * 8.74899959564209
Epoch 200, val loss: 1.1040527820587158
Epoch 210, training loss: 88.48265838623047 = 1.1038563251495361 + 10.0 * 8.737879753112793
Epoch 210, val loss: 1.1034759283065796
Epoch 220, training loss: 88.48117065429688 = 1.103312611579895 + 10.0 * 8.737786293029785
Epoch 220, val loss: 1.1029367446899414
Epoch 230, training loss: 88.35447692871094 = 1.1026445627212524 + 10.0 * 8.725183486938477
Epoch 230, val loss: 1.1022405624389648
Epoch 240, training loss: 88.3571548461914 = 1.1021161079406738 + 10.0 * 8.725503921508789
Epoch 240, val loss: 1.1016895771026611
Epoch 250, training loss: 88.3067855834961 = 1.1015180349349976 + 10.0 * 8.720526695251465
Epoch 250, val loss: 1.1010804176330566
Epoch 260, training loss: 88.27555847167969 = 1.100915551185608 + 10.0 * 8.717464447021484
Epoch 260, val loss: 1.100475549697876
Epoch 270, training loss: 88.28068542480469 = 1.1002702713012695 + 10.0 * 8.71804141998291
Epoch 270, val loss: 1.099813461303711
Epoch 280, training loss: 88.30294799804688 = 1.0996952056884766 + 10.0 * 8.720325469970703
Epoch 280, val loss: 1.099217414855957
Epoch 290, training loss: 88.17115020751953 = 1.0990861654281616 + 10.0 * 8.707206726074219
Epoch 290, val loss: 1.0986028909683228
Epoch 300, training loss: 88.17166900634766 = 1.0984688997268677 + 10.0 * 8.707319259643555
Epoch 300, val loss: 1.0979686975479126
Epoch 310, training loss: 88.15016174316406 = 1.0978217124938965 + 10.0 * 8.705233573913574
Epoch 310, val loss: 1.0973167419433594
Epoch 320, training loss: 88.14005279541016 = 1.0971781015396118 + 10.0 * 8.7042875289917
Epoch 320, val loss: 1.0966691970825195
Epoch 330, training loss: 88.1451416015625 = 1.0965125560760498 + 10.0 * 8.704862594604492
Epoch 330, val loss: 1.0959985256195068
Epoch 340, training loss: 88.1622085571289 = 1.0958715677261353 + 10.0 * 8.706633567810059
Epoch 340, val loss: 1.095329999923706
Epoch 350, training loss: 88.17079162597656 = 1.095210313796997 + 10.0 * 8.707558631896973
Epoch 350, val loss: 1.0946663618087769
Epoch 360, training loss: 88.15548706054688 = 1.0945472717285156 + 10.0 * 8.706093788146973
Epoch 360, val loss: 1.0940032005310059
Epoch 370, training loss: 88.15668487548828 = 1.0939154624938965 + 10.0 * 8.706276893615723
Epoch 370, val loss: 1.093356728553772
Epoch 380, training loss: 88.15686798095703 = 1.0932836532592773 + 10.0 * 8.706357955932617
Epoch 380, val loss: 1.0927060842514038
Epoch 390, training loss: 88.16072082519531 = 1.0926600694656372 + 10.0 * 8.706806182861328
Epoch 390, val loss: 1.0920830965042114
Epoch 400, training loss: 88.19755554199219 = 1.0920614004135132 + 10.0 * 8.710549354553223
Epoch 400, val loss: 1.0914820432662964
Epoch 410, training loss: 88.1793212890625 = 1.0914596319198608 + 10.0 * 8.708786010742188
Epoch 410, val loss: 1.0908817052841187
Epoch 420, training loss: 88.25382232666016 = 1.0908933877944946 + 10.0 * 8.716292381286621
Epoch 420, val loss: 1.0903035402297974
Epoch 430, training loss: 88.16120147705078 = 1.0903420448303223 + 10.0 * 8.707086563110352
Epoch 430, val loss: 1.089760661125183
Epoch 440, training loss: 88.20175170898438 = 1.0898706912994385 + 10.0 * 8.711187362670898
Epoch 440, val loss: 1.0892645120620728
Epoch 450, training loss: 88.2271728515625 = 1.0894100666046143 + 10.0 * 8.713776588439941
Epoch 450, val loss: 1.0887997150421143
Epoch 460, training loss: 88.24606323242188 = 1.0889776945114136 + 10.0 * 8.71570873260498
Epoch 460, val loss: 1.088360071182251
Epoch 470, training loss: 88.24773406982422 = 1.088557481765747 + 10.0 * 8.715917587280273
Epoch 470, val loss: 1.0879384279251099
Epoch 480, training loss: 88.25568389892578 = 1.0881712436676025 + 10.0 * 8.716751098632812
Epoch 480, val loss: 1.0875325202941895
Epoch 490, training loss: 88.29698181152344 = 1.087803602218628 + 10.0 * 8.720917701721191
Epoch 490, val loss: 1.0871508121490479
Epoch 500, training loss: 88.29051971435547 = 1.0873796939849854 + 10.0 * 8.720314025878906
Epoch 500, val loss: 1.0867490768432617
Epoch 510, training loss: 88.30260467529297 = 1.087035894393921 + 10.0 * 8.721556663513184
Epoch 510, val loss: 1.08637535572052
Epoch 520, training loss: 88.34078979492188 = 1.0866881608963013 + 10.0 * 8.725410461425781
Epoch 520, val loss: 1.0860275030136108
Epoch 530, training loss: 88.3484878540039 = 1.0863415002822876 + 10.0 * 8.726214408874512
Epoch 530, val loss: 1.0856785774230957
Epoch 540, training loss: 88.37596893310547 = 1.0859942436218262 + 10.0 * 8.728998184204102
Epoch 540, val loss: 1.0853257179260254
Epoch 550, training loss: 88.42687225341797 = 1.08563232421875 + 10.0 * 8.734124183654785
Epoch 550, val loss: 1.084965467453003
Epoch 560, training loss: 88.44004821777344 = 1.085265874862671 + 10.0 * 8.735478401184082
Epoch 560, val loss: 1.0845707654953003
Epoch 570, training loss: 88.44775390625 = 1.0848993062973022 + 10.0 * 8.736285209655762
Epoch 570, val loss: 1.0842081308364868
Epoch 580, training loss: 88.5089340209961 = 1.084547758102417 + 10.0 * 8.742438316345215
Epoch 580, val loss: 1.0838541984558105
Epoch 590, training loss: 88.51066589355469 = 1.084168553352356 + 10.0 * 8.742650032043457
Epoch 590, val loss: 1.0834729671478271
Epoch 600, training loss: 88.58177185058594 = 1.0838038921356201 + 10.0 * 8.749796867370605
Epoch 600, val loss: 1.0830836296081543
Epoch 610, training loss: 88.55476379394531 = 1.0834059715270996 + 10.0 * 8.747136116027832
Epoch 610, val loss: 1.0826926231384277
Epoch 620, training loss: 88.54316711425781 = 1.0829894542694092 + 10.0 * 8.746017456054688
Epoch 620, val loss: 1.0822678804397583
Epoch 630, training loss: 88.56456756591797 = 1.0826135873794556 + 10.0 * 8.74819564819336
Epoch 630, val loss: 1.0818983316421509
Epoch 640, training loss: 88.64253234863281 = 1.0822200775146484 + 10.0 * 8.756031036376953
Epoch 640, val loss: 1.0814894437789917
Epoch 650, training loss: 88.6981201171875 = 1.0818206071853638 + 10.0 * 8.761630058288574
Epoch 650, val loss: 1.0810725688934326
Epoch 660, training loss: 88.73744201660156 = 1.0814056396484375 + 10.0 * 8.765604019165039
Epoch 660, val loss: 1.080653429031372
Epoch 670, training loss: 88.74340057373047 = 1.0809749364852905 + 10.0 * 8.766242027282715
Epoch 670, val loss: 1.080209493637085
Epoch 680, training loss: 88.75611114501953 = 1.0805426836013794 + 10.0 * 8.767557144165039
Epoch 680, val loss: 1.079767107963562
Epoch 690, training loss: 88.76937103271484 = 1.0801143646240234 + 10.0 * 8.768925666809082
Epoch 690, val loss: 1.0793342590332031
Epoch 700, training loss: 88.76969909667969 = 1.079660177230835 + 10.0 * 8.769003868103027
Epoch 700, val loss: 1.0788813829421997
Epoch 710, training loss: 88.76802062988281 = 1.0791840553283691 + 10.0 * 8.76888370513916
Epoch 710, val loss: 1.0783954858779907
Epoch 720, training loss: 88.81099700927734 = 1.078721523284912 + 10.0 * 8.77322769165039
Epoch 720, val loss: 1.07793128490448
Epoch 730, training loss: 88.85050201416016 = 1.0782570838928223 + 10.0 * 8.77722454071045
Epoch 730, val loss: 1.0774643421173096
Epoch 740, training loss: 88.9285659790039 = 1.0777839422225952 + 10.0 * 8.785078048706055
Epoch 740, val loss: 1.076987624168396
Epoch 750, training loss: 88.87544250488281 = 1.077300786972046 + 10.0 * 8.779813766479492
Epoch 750, val loss: 1.0764867067337036
Epoch 760, training loss: 88.87429809570312 = 1.0767802000045776 + 10.0 * 8.779751777648926
Epoch 760, val loss: 1.07595694065094
Epoch 770, training loss: 88.91802215576172 = 1.0762916803359985 + 10.0 * 8.784173011779785
Epoch 770, val loss: 1.0754754543304443
Epoch 780, training loss: 88.97847747802734 = 1.0758020877838135 + 10.0 * 8.790266990661621
Epoch 780, val loss: 1.074952244758606
Epoch 790, training loss: 88.9912338256836 = 1.075265645980835 + 10.0 * 8.791597366333008
Epoch 790, val loss: 1.0744365453720093
Epoch 800, training loss: 88.98514556884766 = 1.0747321844100952 + 10.0 * 8.791041374206543
Epoch 800, val loss: 1.0738722085952759
Epoch 810, training loss: 89.01934051513672 = 1.0741677284240723 + 10.0 * 8.794517517089844
Epoch 810, val loss: 1.0733217000961304
Epoch 820, training loss: 89.04805755615234 = 1.0736123323440552 + 10.0 * 8.797444343566895
Epoch 820, val loss: 1.0727499723434448
Epoch 830, training loss: 89.09951782226562 = 1.073055624961853 + 10.0 * 8.802645683288574
Epoch 830, val loss: 1.0721917152404785
Epoch 840, training loss: 89.12372589111328 = 1.0724875926971436 + 10.0 * 8.805124282836914
Epoch 840, val loss: 1.0716192722320557
Epoch 850, training loss: 89.12928009033203 = 1.0719034671783447 + 10.0 * 8.805737495422363
Epoch 850, val loss: 1.0710283517837524
Epoch 860, training loss: 89.18928527832031 = 1.071317434310913 + 10.0 * 8.811796188354492
Epoch 860, val loss: 1.0704385042190552
Epoch 870, training loss: 89.1380844116211 = 1.0707141160964966 + 10.0 * 8.806736946105957
Epoch 870, val loss: 1.0698356628417969
Epoch 880, training loss: 89.19249725341797 = 1.070131540298462 + 10.0 * 8.812236785888672
Epoch 880, val loss: 1.0692249536514282
Epoch 890, training loss: 89.19341278076172 = 1.0695109367370605 + 10.0 * 8.812390327453613
Epoch 890, val loss: 1.068620204925537
Epoch 900, training loss: 89.2109146118164 = 1.0688929557800293 + 10.0 * 8.814202308654785
Epoch 900, val loss: 1.0679889917373657
Epoch 910, training loss: 89.24372863769531 = 1.0682610273361206 + 10.0 * 8.817546844482422
Epoch 910, val loss: 1.0673532485961914
Epoch 920, training loss: 89.27255249023438 = 1.0676279067993164 + 10.0 * 8.8204927444458
Epoch 920, val loss: 1.066694974899292
Epoch 930, training loss: 89.27081298828125 = 1.0669726133346558 + 10.0 * 8.82038402557373
Epoch 930, val loss: 1.0660549402236938
Epoch 940, training loss: 89.26892852783203 = 1.0663024187088013 + 10.0 * 8.820262908935547
Epoch 940, val loss: 1.0653653144836426
Epoch 950, training loss: 89.25470733642578 = 1.0656286478042603 + 10.0 * 8.818907737731934
Epoch 950, val loss: 1.0646960735321045
Epoch 960, training loss: 89.3045425415039 = 1.064076542854309 + 10.0 * 8.82404613494873
Epoch 960, val loss: 1.0630933046340942
Epoch 970, training loss: 89.33174896240234 = 1.0621018409729004 + 10.0 * 8.826964378356934
Epoch 970, val loss: 1.061178207397461
Epoch 980, training loss: 89.30445098876953 = 1.060158371925354 + 10.0 * 8.824429512023926
Epoch 980, val loss: 1.0593469142913818
Epoch 990, training loss: 89.36495208740234 = 1.058356523513794 + 10.0 * 8.830659866333008
Epoch 990, val loss: 1.057644009590149
Epoch 1000, training loss: 89.41043853759766 = 1.0566350221633911 + 10.0 * 8.835380554199219
Epoch 1000, val loss: 1.0559964179992676
Epoch 1010, training loss: 89.40990447998047 = 1.0549650192260742 + 10.0 * 8.835494041442871
Epoch 1010, val loss: 1.054396390914917
Epoch 1020, training loss: 89.44489288330078 = 1.0533380508422852 + 10.0 * 8.839155197143555
Epoch 1020, val loss: 1.0528391599655151
Epoch 1030, training loss: 89.44036102294922 = 1.0517257452011108 + 10.0 * 8.838863372802734
Epoch 1030, val loss: 1.0512702465057373
Epoch 1040, training loss: 89.46280670166016 = 1.0501207113265991 + 10.0 * 8.841268539428711
Epoch 1040, val loss: 1.0497299432754517
Epoch 1050, training loss: 89.4546890258789 = 1.0484304428100586 + 10.0 * 8.840625762939453
Epoch 1050, val loss: 1.0480661392211914
Epoch 1060, training loss: 89.49333953857422 = 1.0466160774230957 + 10.0 * 8.844672203063965
Epoch 1060, val loss: 1.046308159828186
Epoch 1070, training loss: 89.5433349609375 = 1.044803500175476 + 10.0 * 8.849853515625
Epoch 1070, val loss: 1.0445505380630493
Epoch 1080, training loss: 89.5634765625 = 1.0430541038513184 + 10.0 * 8.852042198181152
Epoch 1080, val loss: 1.042863130569458
Epoch 1090, training loss: 89.5658187866211 = 1.0412847995758057 + 10.0 * 8.852453231811523
Epoch 1090, val loss: 1.0411254167556763
Epoch 1100, training loss: 89.5421142578125 = 1.0394665002822876 + 10.0 * 8.850264549255371
Epoch 1100, val loss: 1.0393401384353638
Epoch 1110, training loss: 89.54108428955078 = 1.0376815795898438 + 10.0 * 8.850339889526367
Epoch 1110, val loss: 1.0376523733139038
Epoch 1120, training loss: 89.47479248046875 = 1.035871982574463 + 10.0 * 8.843892097473145
Epoch 1120, val loss: 1.0358721017837524
Epoch 1130, training loss: 89.59698486328125 = 1.0340988636016846 + 10.0 * 8.85628890991211
Epoch 1130, val loss: 1.0341274738311768
Epoch 1140, training loss: 89.63135528564453 = 1.032330870628357 + 10.0 * 8.859902381896973
Epoch 1140, val loss: 1.0324115753173828
Epoch 1150, training loss: 89.60498046875 = 1.0304679870605469 + 10.0 * 8.857450485229492
Epoch 1150, val loss: 1.0306061506271362
Epoch 1160, training loss: 89.64922332763672 = 1.0286270380020142 + 10.0 * 8.862059593200684
Epoch 1160, val loss: 1.0288060903549194
Epoch 1170, training loss: 89.67085266113281 = 1.026756763458252 + 10.0 * 8.864409446716309
Epoch 1170, val loss: 1.026977777481079
Epoch 1180, training loss: 89.7141342163086 = 1.0248597860336304 + 10.0 * 8.868927955627441
Epoch 1180, val loss: 1.025123953819275
Epoch 1190, training loss: 89.72190856933594 = 1.0229450464248657 + 10.0 * 8.86989688873291
Epoch 1190, val loss: 1.023242473602295
Epoch 1200, training loss: 89.77102661132812 = 1.0209964513778687 + 10.0 * 8.87500286102295
Epoch 1200, val loss: 1.021346926689148
Epoch 1210, training loss: 89.71892547607422 = 1.0190221071243286 + 10.0 * 8.869990348815918
Epoch 1210, val loss: 1.0193798542022705
Epoch 1220, training loss: 89.73572540283203 = 1.0170167684555054 + 10.0 * 8.871870994567871
Epoch 1220, val loss: 1.0174357891082764
Epoch 1230, training loss: 89.77333068847656 = 1.0149856805801392 + 10.0 * 8.875834465026855
Epoch 1230, val loss: 1.015468955039978
Epoch 1240, training loss: 89.82172393798828 = 1.0129667520523071 + 10.0 * 8.880875587463379
Epoch 1240, val loss: 1.013494610786438
Epoch 1250, training loss: 89.79996490478516 = 1.0108659267425537 + 10.0 * 8.878910064697266
Epoch 1250, val loss: 1.0114338397979736
Epoch 1260, training loss: 89.8084945678711 = 1.0087560415267944 + 10.0 * 8.879973411560059
Epoch 1260, val loss: 1.0093705654144287
Epoch 1270, training loss: 89.83814239501953 = 1.0066274404525757 + 10.0 * 8.883151054382324
Epoch 1270, val loss: 1.0073086023330688
Epoch 1280, training loss: 89.85696411132812 = 1.004488229751587 + 10.0 * 8.885248184204102
Epoch 1280, val loss: 1.0051686763763428
Epoch 1290, training loss: 89.87434387207031 = 1.0022755861282349 + 10.0 * 8.88720703125
Epoch 1290, val loss: 1.0030181407928467
Epoch 1300, training loss: 89.89203643798828 = 1.0000373125076294 + 10.0 * 8.889200210571289
Epoch 1300, val loss: 1.0008355379104614
Epoch 1310, training loss: 89.86366271972656 = 0.9977414011955261 + 10.0 * 8.886591911315918
Epoch 1310, val loss: 0.9985800981521606
Epoch 1320, training loss: 89.88477325439453 = 0.9954630732536316 + 10.0 * 8.888931274414062
Epoch 1320, val loss: 0.9963687658309937
Epoch 1330, training loss: 89.89825439453125 = 0.9931420087814331 + 10.0 * 8.890511512756348
Epoch 1330, val loss: 0.9940981268882751
Epoch 1340, training loss: 89.94551849365234 = 0.9908152222633362 + 10.0 * 8.89547061920166
Epoch 1340, val loss: 0.9917946457862854
Epoch 1350, training loss: 89.9392318725586 = 0.9884202480316162 + 10.0 * 8.895081520080566
Epoch 1350, val loss: 0.9894739985466003
Epoch 1360, training loss: 89.9449462890625 = 0.9860166311264038 + 10.0 * 8.895893096923828
Epoch 1360, val loss: 0.987120509147644
Epoch 1370, training loss: 89.96685028076172 = 0.9835814237594604 + 10.0 * 8.898326873779297
Epoch 1370, val loss: 0.9847226738929749
Epoch 1380, training loss: 89.94222259521484 = 0.9811025261878967 + 10.0 * 8.896112442016602
Epoch 1380, val loss: 0.9823422431945801
Epoch 1390, training loss: 89.97699737548828 = 0.9786547422409058 + 10.0 * 8.899834632873535
Epoch 1390, val loss: 0.9799407124519348
Epoch 1400, training loss: 90.01914978027344 = 0.9761816263198853 + 10.0 * 8.904296875
Epoch 1400, val loss: 0.9775353074073792
Epoch 1410, training loss: 90.01666259765625 = 0.9735819697380066 + 10.0 * 8.904308319091797
Epoch 1410, val loss: 0.9750127792358398
Epoch 1420, training loss: 89.94331359863281 = 0.9710710644721985 + 10.0 * 8.897224426269531
Epoch 1420, val loss: 0.9725372195243835
Epoch 1430, training loss: 89.86831665039062 = 0.9684897661209106 + 10.0 * 8.889982223510742
Epoch 1430, val loss: 0.9700093865394592
Epoch 1440, training loss: 89.96531677246094 = 0.965914249420166 + 10.0 * 8.899940490722656
Epoch 1440, val loss: 0.9675208926200867
Epoch 1450, training loss: 89.86600494384766 = 0.9632949829101562 + 10.0 * 8.890271186828613
Epoch 1450, val loss: 0.964918315410614
Epoch 1460, training loss: 89.9250259399414 = 0.9607097506523132 + 10.0 * 8.896431922912598
Epoch 1460, val loss: 0.9624267220497131
Epoch 1470, training loss: 90.0165023803711 = 0.9580742120742798 + 10.0 * 8.905842781066895
Epoch 1470, val loss: 0.9598392844200134
Epoch 1480, training loss: 90.05644989013672 = 0.9553850293159485 + 10.0 * 8.910106658935547
Epoch 1480, val loss: 0.9571952223777771
Epoch 1490, training loss: 90.06265258789062 = 0.9527584910392761 + 10.0 * 8.910989761352539
Epoch 1490, val loss: 0.9546458721160889
Epoch 1500, training loss: 90.09680938720703 = 0.9500385522842407 + 10.0 * 8.914677619934082
Epoch 1500, val loss: 0.9519879817962646
Epoch 1510, training loss: 90.07074737548828 = 0.9472752213478088 + 10.0 * 8.912347793579102
Epoch 1510, val loss: 0.9493076205253601
Epoch 1520, training loss: 90.10669708251953 = 0.9444918632507324 + 10.0 * 8.916220664978027
Epoch 1520, val loss: 0.9466443657875061
Epoch 1530, training loss: 90.10640716552734 = 0.9416682124137878 + 10.0 * 8.916474342346191
Epoch 1530, val loss: 0.9438605308532715
Epoch 1540, training loss: 90.13993072509766 = 0.9388459324836731 + 10.0 * 8.920108795166016
Epoch 1540, val loss: 0.941127598285675
Epoch 1550, training loss: 90.11473846435547 = 0.9359827041625977 + 10.0 * 8.917875289916992
Epoch 1550, val loss: 0.9383456707000732
Epoch 1560, training loss: 90.13917541503906 = 0.9331303238868713 + 10.0 * 8.920604705810547
Epoch 1560, val loss: 0.9355514049530029
Epoch 1570, training loss: 90.17648315429688 = 0.9302555918693542 + 10.0 * 8.924623489379883
Epoch 1570, val loss: 0.9327884912490845
Epoch 1580, training loss: 90.18675231933594 = 0.9273254871368408 + 10.0 * 8.925943374633789
Epoch 1580, val loss: 0.9299347400665283
Epoch 1590, training loss: 90.19824981689453 = 0.9243865609169006 + 10.0 * 8.927386283874512
Epoch 1590, val loss: 0.9270553588867188
Epoch 1600, training loss: 90.2337417602539 = 0.9214510917663574 + 10.0 * 8.931229591369629
Epoch 1600, val loss: 0.9242311120033264
Epoch 1610, training loss: 90.22510528564453 = 0.9184688329696655 + 10.0 * 8.930663108825684
Epoch 1610, val loss: 0.9213132262229919
Epoch 1620, training loss: 90.17402648925781 = 0.9154534339904785 + 10.0 * 8.925857543945312
Epoch 1620, val loss: 0.9183775186538696
Epoch 1630, training loss: 90.05094146728516 = 0.9123379588127136 + 10.0 * 8.913860321044922
Epoch 1630, val loss: 0.9153897166252136
Epoch 1640, training loss: 90.1535873413086 = 0.9094284176826477 + 10.0 * 8.924415588378906
Epoch 1640, val loss: 0.9125735759735107
Epoch 1650, training loss: 90.21450805664062 = 0.9064368009567261 + 10.0 * 8.930807113647461
Epoch 1650, val loss: 0.909649670124054
Epoch 1660, training loss: 90.26750183105469 = 0.903393030166626 + 10.0 * 8.936410903930664
Epoch 1660, val loss: 0.9066948294639587
Epoch 1670, training loss: 90.30179595947266 = 0.9002800583839417 + 10.0 * 8.94015121459961
Epoch 1670, val loss: 0.9036688208580017
Epoch 1680, training loss: 90.25394439697266 = 0.8971301317214966 + 10.0 * 8.935681343078613
Epoch 1680, val loss: 0.9006153345108032
Epoch 1690, training loss: 90.3022689819336 = 0.8939914703369141 + 10.0 * 8.940828323364258
Epoch 1690, val loss: 0.8975802063941956
Epoch 1700, training loss: 90.33833312988281 = 0.8908205032348633 + 10.0 * 8.944750785827637
Epoch 1700, val loss: 0.8945086598396301
Epoch 1710, training loss: 90.34813690185547 = 0.8876047134399414 + 10.0 * 8.946053504943848
Epoch 1710, val loss: 0.8914059996604919
Epoch 1720, training loss: 90.33208465576172 = 0.8843854665756226 + 10.0 * 8.944769859313965
Epoch 1720, val loss: 0.8882978558540344
Epoch 1730, training loss: 90.3674087524414 = 0.8811448812484741 + 10.0 * 8.948626518249512
Epoch 1730, val loss: 0.8851727247238159
Epoch 1740, training loss: 90.4151840209961 = 0.8778629302978516 + 10.0 * 8.95373249053955
Epoch 1740, val loss: 0.8820022940635681
Epoch 1750, training loss: 90.41876220703125 = 0.8745395541191101 + 10.0 * 8.954421997070312
Epoch 1750, val loss: 0.8787969350814819
Epoch 1760, training loss: 90.38126373291016 = 0.8711923956871033 + 10.0 * 8.951006889343262
Epoch 1760, val loss: 0.875566303730011
Epoch 1770, training loss: 90.4189682006836 = 0.867836058139801 + 10.0 * 8.955113410949707
Epoch 1770, val loss: 0.8723404407501221
Epoch 1780, training loss: 90.41285705566406 = 0.8644464612007141 + 10.0 * 8.954840660095215
Epoch 1780, val loss: 0.8690710663795471
Epoch 1790, training loss: 90.35684967041016 = 0.86098712682724 + 10.0 * 8.949586868286133
Epoch 1790, val loss: 0.8657323122024536
Epoch 1800, training loss: 90.39405822753906 = 0.8575789928436279 + 10.0 * 8.95364761352539
Epoch 1800, val loss: 0.8624683618545532
Epoch 1810, training loss: 90.4124755859375 = 0.8542090654373169 + 10.0 * 8.955826759338379
Epoch 1810, val loss: 0.8592259287834167
Epoch 1820, training loss: 90.46217346191406 = 0.8507742285728455 + 10.0 * 8.961139678955078
Epoch 1820, val loss: 0.8559223413467407
Epoch 1830, training loss: 90.47119903564453 = 0.8472999930381775 + 10.0 * 8.962389945983887
Epoch 1830, val loss: 0.8525797128677368
Epoch 1840, training loss: 90.50068664550781 = 0.8438155055046082 + 10.0 * 8.965686798095703
Epoch 1840, val loss: 0.8492425680160522
Epoch 1850, training loss: 90.52225494384766 = 0.8403108716011047 + 10.0 * 8.968194961547852
Epoch 1850, val loss: 0.8458806276321411
Epoch 1860, training loss: 90.50318908691406 = 0.8367627263069153 + 10.0 * 8.966642379760742
Epoch 1860, val loss: 0.8425061106681824
Epoch 1870, training loss: 90.50606536865234 = 0.8332769870758057 + 10.0 * 8.967279434204102
Epoch 1870, val loss: 0.8391405940055847
Epoch 1880, training loss: 90.53313446044922 = 0.8297588229179382 + 10.0 * 8.970337867736816
Epoch 1880, val loss: 0.8357733488082886
Epoch 1890, training loss: 90.52222442626953 = 0.8262506127357483 + 10.0 * 8.969597816467285
Epoch 1890, val loss: 0.8323737382888794
Epoch 1900, training loss: 90.52076721191406 = 0.8226610422134399 + 10.0 * 8.969810485839844
Epoch 1900, val loss: 0.8289861083030701
Epoch 1910, training loss: 90.50159454345703 = 0.8191269636154175 + 10.0 * 8.968246459960938
Epoch 1910, val loss: 0.8256047368049622
Epoch 1920, training loss: 90.50064849853516 = 0.8155714869499207 + 10.0 * 8.968507766723633
Epoch 1920, val loss: 0.8222038149833679
Epoch 1930, training loss: 90.52934265136719 = 0.8119786381721497 + 10.0 * 8.971735954284668
Epoch 1930, val loss: 0.8187914490699768
Epoch 1940, training loss: 90.59417724609375 = 0.8083654046058655 + 10.0 * 8.978581428527832
Epoch 1940, val loss: 0.8153622150421143
Epoch 1950, training loss: 90.60353088378906 = 0.8047322034835815 + 10.0 * 8.979879379272461
Epoch 1950, val loss: 0.8119248151779175
Epoch 1960, training loss: 90.5931396484375 = 0.8011130094528198 + 10.0 * 8.979203224182129
Epoch 1960, val loss: 0.8084858059883118
Epoch 1970, training loss: 90.59717559814453 = 0.7975269556045532 + 10.0 * 8.979965209960938
Epoch 1970, val loss: 0.8050854206085205
Epoch 1980, training loss: 90.63288879394531 = 0.7938957810401917 + 10.0 * 8.983899116516113
Epoch 1980, val loss: 0.8016465902328491
Epoch 1990, training loss: 90.63079833984375 = 0.7902474403381348 + 10.0 * 8.984055519104004
Epoch 1990, val loss: 0.7982109189033508
Epoch 2000, training loss: 90.63323974609375 = 0.7866490483283997 + 10.0 * 8.984659194946289
Epoch 2000, val loss: 0.7947970032691956
Epoch 2010, training loss: 90.52562713623047 = 0.783061146736145 + 10.0 * 8.97425651550293
Epoch 2010, val loss: 0.7914339303970337
Epoch 2020, training loss: 90.44818878173828 = 0.7795776724815369 + 10.0 * 8.9668607711792
Epoch 2020, val loss: 0.7880269289016724
Epoch 2030, training loss: 90.32618713378906 = 0.7760325074195862 + 10.0 * 8.955015182495117
Epoch 2030, val loss: 0.7847883701324463
Epoch 2040, training loss: 90.3613052368164 = 0.7726485729217529 + 10.0 * 8.95886516571045
Epoch 2040, val loss: 0.781552255153656
Epoch 2050, training loss: 90.36549377441406 = 0.7690171599388123 + 10.0 * 8.959647178649902
Epoch 2050, val loss: 0.7781634330749512
Epoch 2060, training loss: 90.30760192871094 = 0.7653987407684326 + 10.0 * 8.95422077178955
Epoch 2060, val loss: 0.774677038192749
Epoch 2070, training loss: 90.40076446533203 = 0.7618011236190796 + 10.0 * 8.963895797729492
Epoch 2070, val loss: 0.7713180184364319
Epoch 2080, training loss: 90.47723388671875 = 0.7581490278244019 + 10.0 * 8.971908569335938
Epoch 2080, val loss: 0.7678768634796143
Epoch 2090, training loss: 90.55663299560547 = 0.7545068860054016 + 10.0 * 8.980212211608887
Epoch 2090, val loss: 0.764423668384552
Epoch 2100, training loss: 90.57609558105469 = 0.7508182525634766 + 10.0 * 8.982527732849121
Epoch 2100, val loss: 0.7609432935714722
Epoch 2110, training loss: 90.61488342285156 = 0.7471602559089661 + 10.0 * 8.986772537231445
Epoch 2110, val loss: 0.7575219869613647
Epoch 2120, training loss: 90.6332015991211 = 0.743513286113739 + 10.0 * 8.988968849182129
Epoch 2120, val loss: 0.754088282585144
Epoch 2130, training loss: 90.60159301757812 = 0.7399033308029175 + 10.0 * 8.98616886138916
Epoch 2130, val loss: 0.7506906986236572
Epoch 2140, training loss: 90.61843872070312 = 0.7362669110298157 + 10.0 * 8.9882173538208
Epoch 2140, val loss: 0.7472978830337524
Epoch 2150, training loss: 90.62455749511719 = 0.7327025532722473 + 10.0 * 8.989185333251953
Epoch 2150, val loss: 0.7439531683921814
Epoch 2160, training loss: 90.65093231201172 = 0.7291348576545715 + 10.0 * 8.992179870605469
Epoch 2160, val loss: 0.7406064867973328
Epoch 2170, training loss: 90.6780014038086 = 0.7255747318267822 + 10.0 * 8.99524211883545
Epoch 2170, val loss: 0.737241804599762
Epoch 2180, training loss: 90.67747497558594 = 0.7220178246498108 + 10.0 * 8.995546340942383
Epoch 2180, val loss: 0.733914852142334
Epoch 2190, training loss: 90.64363861083984 = 0.7185015082359314 + 10.0 * 8.992513656616211
Epoch 2190, val loss: 0.7305956482887268
Epoch 2200, training loss: 90.65643310546875 = 0.715045154094696 + 10.0 * 8.994138717651367
Epoch 2200, val loss: 0.7273615002632141
Epoch 2210, training loss: 90.69779205322266 = 0.7115335464477539 + 10.0 * 8.998625755310059
Epoch 2210, val loss: 0.7240668535232544
Epoch 2220, training loss: 90.7176513671875 = 0.7080528140068054 + 10.0 * 9.000959396362305
Epoch 2220, val loss: 0.7207750082015991
Epoch 2230, training loss: 90.74685668945312 = 0.7045460343360901 + 10.0 * 9.004231452941895
Epoch 2230, val loss: 0.7175145149230957
Epoch 2240, training loss: 90.7160873413086 = 0.7010725140571594 + 10.0 * 9.001501083374023
Epoch 2240, val loss: 0.7142022848129272
Epoch 2250, training loss: 90.61767578125 = 0.6977230310440063 + 10.0 * 8.991994857788086
Epoch 2250, val loss: 0.7111372947692871
Epoch 2260, training loss: 90.61563873291016 = 0.6943593621253967 + 10.0 * 8.992128372192383
Epoch 2260, val loss: 0.7079334259033203
Epoch 2270, training loss: 90.69210815429688 = 0.6909631490707397 + 10.0 * 9.000114440917969
Epoch 2270, val loss: 0.7047246694564819
Epoch 2280, training loss: 90.67481231689453 = 0.6875858902931213 + 10.0 * 8.998723030090332
Epoch 2280, val loss: 0.7015282511711121
Epoch 2290, training loss: 90.6974105834961 = 0.6842764616012573 + 10.0 * 9.001314163208008
Epoch 2290, val loss: 0.6983929872512817
Epoch 2300, training loss: 90.75397491455078 = 0.6809551119804382 + 10.0 * 9.007302284240723
Epoch 2300, val loss: 0.6952854990959167
Epoch 2310, training loss: 90.76252746582031 = 0.6775749325752258 + 10.0 * 9.008495330810547
Epoch 2310, val loss: 0.6921387314796448
Epoch 2320, training loss: 90.79560089111328 = 0.6742275953292847 + 10.0 * 9.012137413024902
Epoch 2320, val loss: 0.6889975070953369
Epoch 2330, training loss: 90.78475952148438 = 0.6708961129188538 + 10.0 * 9.011385917663574
Epoch 2330, val loss: 0.6858697533607483
Epoch 2340, training loss: 90.76439666748047 = 0.6675914525985718 + 10.0 * 9.009679794311523
Epoch 2340, val loss: 0.6827524900436401
Epoch 2350, training loss: 90.80220794677734 = 0.6643082499504089 + 10.0 * 9.013790130615234
Epoch 2350, val loss: 0.6796508431434631
Epoch 2360, training loss: 90.79029083251953 = 0.6609306931495667 + 10.0 * 9.012935638427734
Epoch 2360, val loss: 0.6765180230140686
Epoch 2370, training loss: 90.8397216796875 = 0.6575168371200562 + 10.0 * 9.018220901489258
Epoch 2370, val loss: 0.6732879281044006
Epoch 2380, training loss: 90.84898376464844 = 0.6539320349693298 + 10.0 * 9.019505500793457
Epoch 2380, val loss: 0.6699724197387695
Epoch 2390, training loss: 90.7986068725586 = 0.6503835916519165 + 10.0 * 9.014822006225586
Epoch 2390, val loss: 0.6667180061340332
Epoch 2400, training loss: 90.75164794921875 = 0.6469128727912903 + 10.0 * 9.010473251342773
Epoch 2400, val loss: 0.6634501218795776
Epoch 2410, training loss: 90.70570373535156 = 0.643646240234375 + 10.0 * 9.006205558776855
Epoch 2410, val loss: 0.6604596376419067
Epoch 2420, training loss: 90.71049499511719 = 0.6403788328170776 + 10.0 * 9.007011413574219
Epoch 2420, val loss: 0.6573960185050964
Epoch 2430, training loss: 90.62589263916016 = 0.6373209357261658 + 10.0 * 8.998857498168945
Epoch 2430, val loss: 0.6545190811157227
Epoch 2440, training loss: 90.60228729248047 = 0.6343623399734497 + 10.0 * 8.996792793273926
Epoch 2440, val loss: 0.6516890525817871
Epoch 2450, training loss: 90.63602447509766 = 0.6312993168830872 + 10.0 * 9.000472068786621
Epoch 2450, val loss: 0.6488991975784302
Epoch 2460, training loss: 90.6937255859375 = 0.6282058358192444 + 10.0 * 9.006551742553711
Epoch 2460, val loss: 0.6460233926773071
Epoch 2470, training loss: 90.72805786132812 = 0.6250797510147095 + 10.0 * 9.010297775268555
Epoch 2470, val loss: 0.6431137323379517
Epoch 2480, training loss: 90.7765884399414 = 0.6219056844711304 + 10.0 * 9.01546859741211
Epoch 2480, val loss: 0.6401400566101074
Epoch 2490, training loss: 90.81802368164062 = 0.6188217997550964 + 10.0 * 9.019920349121094
Epoch 2490, val loss: 0.6372244358062744
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7998550724637681
0.8633630370209375
=== training gcn model ===
Epoch 0, training loss: 102.85741424560547 = 1.0943466424942017 + 10.0 * 10.17630672454834
Epoch 0, val loss: 1.09369695186615
Epoch 10, training loss: 98.85762023925781 = 1.0939483642578125 + 10.0 * 9.7763671875
Epoch 10, val loss: 1.0933234691619873
Epoch 20, training loss: 96.87247467041016 = 1.0936356782913208 + 10.0 * 9.57788372039795
Epoch 20, val loss: 1.093018651008606
Epoch 30, training loss: 95.38475799560547 = 1.0932986736297607 + 10.0 * 9.429145812988281
Epoch 30, val loss: 1.0926891565322876
Epoch 40, training loss: 94.1962661743164 = 1.0929770469665527 + 10.0 * 9.310328483581543
Epoch 40, val loss: 1.0923751592636108
Epoch 50, training loss: 93.22843170166016 = 1.092660903930664 + 10.0 * 9.213577270507812
Epoch 50, val loss: 1.09206223487854
Epoch 60, training loss: 92.41854858398438 = 1.0923261642456055 + 10.0 * 9.132622718811035
Epoch 60, val loss: 1.091732144355774
Epoch 70, training loss: 91.73973846435547 = 1.0919901132583618 + 10.0 * 9.064775466918945
Epoch 70, val loss: 1.0914024114608765
Epoch 80, training loss: 91.17665100097656 = 1.0916544198989868 + 10.0 * 9.008500099182129
Epoch 80, val loss: 1.0910733938217163
Epoch 90, training loss: 90.6984634399414 = 1.0913057327270508 + 10.0 * 8.960715293884277
Epoch 90, val loss: 1.090731143951416
Epoch 100, training loss: 90.30146789550781 = 1.0909539461135864 + 10.0 * 8.921051025390625
Epoch 100, val loss: 1.0903847217559814
Epoch 110, training loss: 89.96195983886719 = 1.0905717611312866 + 10.0 * 8.887139320373535
Epoch 110, val loss: 1.0900139808654785
Epoch 120, training loss: 89.66674041748047 = 1.0902296304702759 + 10.0 * 8.857650756835938
Epoch 120, val loss: 1.08968186378479
Epoch 130, training loss: 89.4098129272461 = 1.089834213256836 + 10.0 * 8.831997871398926
Epoch 130, val loss: 1.089294672012329
Epoch 140, training loss: 89.18193817138672 = 1.089442253112793 + 10.0 * 8.809249877929688
Epoch 140, val loss: 1.0889118909835815
Epoch 150, training loss: 89.00796508789062 = 1.0890487432479858 + 10.0 * 8.791891098022461
Epoch 150, val loss: 1.0885330438613892
Epoch 160, training loss: 88.84362030029297 = 1.088639259338379 + 10.0 * 8.775498390197754
Epoch 160, val loss: 1.0881301164627075
Epoch 170, training loss: 88.71672058105469 = 1.0882489681243896 + 10.0 * 8.762846946716309
Epoch 170, val loss: 1.0877572298049927
Epoch 180, training loss: 88.58068084716797 = 1.087814211845398 + 10.0 * 8.749286651611328
Epoch 180, val loss: 1.087332844734192
Epoch 190, training loss: 88.4797134399414 = 1.0873956680297852 + 10.0 * 8.739232063293457
Epoch 190, val loss: 1.0869277715682983
Epoch 200, training loss: 88.38086700439453 = 1.0869642496109009 + 10.0 * 8.729390144348145
Epoch 200, val loss: 1.0865110158920288
Epoch 210, training loss: 88.3294906616211 = 1.0865111351013184 + 10.0 * 8.724298477172852
Epoch 210, val loss: 1.0860726833343506
Epoch 220, training loss: 88.2380142211914 = 1.086068868637085 + 10.0 * 8.715194702148438
Epoch 220, val loss: 1.0856496095657349
Epoch 230, training loss: 88.15026092529297 = 1.0855953693389893 + 10.0 * 8.706466674804688
Epoch 230, val loss: 1.0851949453353882
Epoch 240, training loss: 88.10969543457031 = 1.0851178169250488 + 10.0 * 8.702457427978516
Epoch 240, val loss: 1.0847294330596924
Epoch 250, training loss: 88.07670593261719 = 1.0846368074417114 + 10.0 * 8.699206352233887
Epoch 250, val loss: 1.0842740535736084
Epoch 260, training loss: 88.01463317871094 = 1.0841494798660278 + 10.0 * 8.693048477172852
Epoch 260, val loss: 1.0838063955307007
Epoch 270, training loss: 87.98987579345703 = 1.0836362838745117 + 10.0 * 8.690624237060547
Epoch 270, val loss: 1.0833135843276978
Epoch 280, training loss: 87.96713256835938 = 1.083115577697754 + 10.0 * 8.68840217590332
Epoch 280, val loss: 1.082815408706665
Epoch 290, training loss: 87.94725799560547 = 1.0825862884521484 + 10.0 * 8.686467170715332
Epoch 290, val loss: 1.08231520652771
Epoch 300, training loss: 87.91170501708984 = 1.0820536613464355 + 10.0 * 8.682965278625488
Epoch 300, val loss: 1.0818090438842773
Epoch 310, training loss: 87.92058563232422 = 1.0815221071243286 + 10.0 * 8.683906555175781
Epoch 310, val loss: 1.081299901008606
Epoch 320, training loss: 87.89659118652344 = 1.0809547901153564 + 10.0 * 8.681563377380371
Epoch 320, val loss: 1.0807619094848633
Epoch 330, training loss: 87.8896255493164 = 1.0803951025009155 + 10.0 * 8.680922508239746
Epoch 330, val loss: 1.0802257061004639
Epoch 340, training loss: 87.90445709228516 = 1.079813838005066 + 10.0 * 8.682464599609375
Epoch 340, val loss: 1.0796698331832886
Epoch 350, training loss: 87.90214538574219 = 1.0792126655578613 + 10.0 * 8.682292938232422
Epoch 350, val loss: 1.0790908336639404
Epoch 360, training loss: 87.90347290039062 = 1.0786415338516235 + 10.0 * 8.682482719421387
Epoch 360, val loss: 1.078568935394287
Epoch 370, training loss: 87.88630676269531 = 1.0780338048934937 + 10.0 * 8.680827140808105
Epoch 370, val loss: 1.0779982805252075
Epoch 380, training loss: 87.8968734741211 = 1.0774223804473877 + 10.0 * 8.681944847106934
Epoch 380, val loss: 1.0774121284484863
Epoch 390, training loss: 87.91265106201172 = 1.0767945051193237 + 10.0 * 8.683585166931152
Epoch 390, val loss: 1.0768061876296997
Epoch 400, training loss: 87.92975616455078 = 1.0761626958847046 + 10.0 * 8.685359001159668
Epoch 400, val loss: 1.0762240886688232
Epoch 410, training loss: 88.06930541992188 = 1.0755265951156616 + 10.0 * 8.69937801361084
Epoch 410, val loss: 1.0756182670593262
Epoch 420, training loss: 87.94044494628906 = 1.0748599767684937 + 10.0 * 8.686558723449707
Epoch 420, val loss: 1.075005292892456
Epoch 430, training loss: 87.94768524169922 = 1.074220895767212 + 10.0 * 8.687346458435059
Epoch 430, val loss: 1.0744022130966187
Epoch 440, training loss: 87.9683837890625 = 1.0735447406768799 + 10.0 * 8.689483642578125
Epoch 440, val loss: 1.0737680196762085
Epoch 450, training loss: 88.03671264648438 = 1.0728914737701416 + 10.0 * 8.696382522583008
Epoch 450, val loss: 1.0731595754623413
Epoch 460, training loss: 88.02108764648438 = 1.0721932649612427 + 10.0 * 8.694889068603516
Epoch 460, val loss: 1.0725101232528687
Epoch 470, training loss: 88.03189086914062 = 1.0714836120605469 + 10.0 * 8.696041107177734
Epoch 470, val loss: 1.0718464851379395
Epoch 480, training loss: 87.97511291503906 = 1.0706950426101685 + 10.0 * 8.690442085266113
Epoch 480, val loss: 1.0711275339126587
Epoch 490, training loss: 88.06890106201172 = 1.0700820684432983 + 10.0 * 8.699881553649902
Epoch 490, val loss: 1.0704998970031738
Epoch 500, training loss: 87.99080657958984 = 1.0693049430847168 + 10.0 * 8.692150115966797
Epoch 500, val loss: 1.069826364517212
Epoch 510, training loss: 87.99761199951172 = 1.0685276985168457 + 10.0 * 8.69290828704834
Epoch 510, val loss: 1.0690830945968628
Epoch 520, training loss: 88.07431030273438 = 1.067785620689392 + 10.0 * 8.700652122497559
Epoch 520, val loss: 1.0683883428573608
Epoch 530, training loss: 88.09549713134766 = 1.0670017004013062 + 10.0 * 8.702849388122559
Epoch 530, val loss: 1.0676604509353638
Epoch 540, training loss: 88.09127807617188 = 1.0662009716033936 + 10.0 * 8.702507972717285
Epoch 540, val loss: 1.066919207572937
Epoch 550, training loss: 88.12895202636719 = 1.0653893947601318 + 10.0 * 8.706356048583984
Epoch 550, val loss: 1.0661426782608032
Epoch 560, training loss: 88.15132141113281 = 1.0645298957824707 + 10.0 * 8.70867919921875
Epoch 560, val loss: 1.0653630495071411
Epoch 570, training loss: 88.1654281616211 = 1.063666820526123 + 10.0 * 8.710176467895508
Epoch 570, val loss: 1.0645698308944702
Epoch 580, training loss: 88.18842315673828 = 1.062749981880188 + 10.0 * 8.712567329406738
Epoch 580, val loss: 1.0637073516845703
Epoch 590, training loss: 88.2134017944336 = 1.0618005990982056 + 10.0 * 8.715160369873047
Epoch 590, val loss: 1.0628243684768677
Epoch 600, training loss: 88.2272720336914 = 1.06083083152771 + 10.0 * 8.716644287109375
Epoch 600, val loss: 1.0619388818740845
Epoch 610, training loss: 88.2105712890625 = 1.0598323345184326 + 10.0 * 8.71507453918457
Epoch 610, val loss: 1.0610289573669434
Epoch 620, training loss: 88.38282775878906 = 1.0588301420211792 + 10.0 * 8.732399940490723
Epoch 620, val loss: 1.0601205825805664
Epoch 630, training loss: 88.25749969482422 = 1.0578105449676514 + 10.0 * 8.719968795776367
Epoch 630, val loss: 1.0591630935668945
Epoch 640, training loss: 88.28038787841797 = 1.0567525625228882 + 10.0 * 8.722363471984863
Epoch 640, val loss: 1.0581727027893066
Epoch 650, training loss: 88.32205200195312 = 1.0556691884994507 + 10.0 * 8.726637840270996
Epoch 650, val loss: 1.0571837425231934
Epoch 660, training loss: 88.38243103027344 = 1.0545928478240967 + 10.0 * 8.732783317565918
Epoch 660, val loss: 1.056193232536316
Epoch 670, training loss: 88.34822845458984 = 1.0533924102783203 + 10.0 * 8.729483604431152
Epoch 670, val loss: 1.0551046133041382
Epoch 680, training loss: 88.38768005371094 = 1.0522956848144531 + 10.0 * 8.733538627624512
Epoch 680, val loss: 1.0540716648101807
Epoch 690, training loss: 88.44107818603516 = 1.0511258840560913 + 10.0 * 8.738995552062988
Epoch 690, val loss: 1.0529961585998535
Epoch 700, training loss: 88.48600006103516 = 1.0499073266983032 + 10.0 * 8.743609428405762
Epoch 700, val loss: 1.0518834590911865
Epoch 710, training loss: 88.52397155761719 = 1.0486760139465332 + 10.0 * 8.747529983520508
Epoch 710, val loss: 1.0507339239120483
Epoch 720, training loss: 88.53520202636719 = 1.047486662864685 + 10.0 * 8.748771667480469
Epoch 720, val loss: 1.0496110916137695
Epoch 730, training loss: 88.57239532470703 = 1.0462409257888794 + 10.0 * 8.752614974975586
Epoch 730, val loss: 1.0484709739685059
Epoch 740, training loss: 88.4103775024414 = 1.0448410511016846 + 10.0 * 8.736554145812988
Epoch 740, val loss: 1.0471997261047363
Epoch 750, training loss: 88.52216339111328 = 1.0436005592346191 + 10.0 * 8.747856140136719
Epoch 750, val loss: 1.0460357666015625
Epoch 760, training loss: 88.60313415527344 = 1.042263150215149 + 10.0 * 8.756086349487305
Epoch 760, val loss: 1.0448009967803955
Epoch 770, training loss: 88.633544921875 = 1.0409077405929565 + 10.0 * 8.75926399230957
Epoch 770, val loss: 1.0435383319854736
Epoch 780, training loss: 88.66567993164062 = 1.039535641670227 + 10.0 * 8.762614250183105
Epoch 780, val loss: 1.042264461517334
Epoch 790, training loss: 88.6949462890625 = 1.0381323099136353 + 10.0 * 8.765681266784668
Epoch 790, val loss: 1.0409618616104126
Epoch 800, training loss: 88.6808090209961 = 1.0366864204406738 + 10.0 * 8.764411926269531
Epoch 800, val loss: 1.0396212339401245
Epoch 810, training loss: 88.72837829589844 = 1.0352849960327148 + 10.0 * 8.769309043884277
Epoch 810, val loss: 1.0383045673370361
Epoch 820, training loss: 88.73604583740234 = 1.0338011980056763 + 10.0 * 8.770224571228027
Epoch 820, val loss: 1.036928415298462
Epoch 830, training loss: 88.64804077148438 = 1.0322638750076294 + 10.0 * 8.761577606201172
Epoch 830, val loss: 1.0355167388916016
Epoch 840, training loss: 88.70341491699219 = 1.0308071374893188 + 10.0 * 8.767260551452637
Epoch 840, val loss: 1.0341577529907227
Epoch 850, training loss: 88.76461029052734 = 1.0293318033218384 + 10.0 * 8.773527145385742
Epoch 850, val loss: 1.0327683687210083
Epoch 860, training loss: 88.78598022460938 = 1.027808427810669 + 10.0 * 8.775816917419434
Epoch 860, val loss: 1.0313622951507568
Epoch 870, training loss: 88.82002258300781 = 1.0262532234191895 + 10.0 * 8.779376983642578
Epoch 870, val loss: 1.0299166440963745
Epoch 880, training loss: 88.86653137207031 = 1.024696707725525 + 10.0 * 8.784183502197266
Epoch 880, val loss: 1.0284548997879028
Epoch 890, training loss: 88.86878967285156 = 1.0230895280838013 + 10.0 * 8.784570693969727
Epoch 890, val loss: 1.0269654989242554
Epoch 900, training loss: 88.87184143066406 = 1.021481990814209 + 10.0 * 8.785036087036133
Epoch 900, val loss: 1.0254803895950317
Epoch 910, training loss: 88.90757751464844 = 1.0198644399642944 + 10.0 * 8.78877067565918
Epoch 910, val loss: 1.023980975151062
Epoch 920, training loss: 88.92291259765625 = 1.0182793140411377 + 10.0 * 8.7904634475708
Epoch 920, val loss: 1.0225292444229126
Epoch 930, training loss: 88.87110900878906 = 1.0168004035949707 + 10.0 * 8.785430908203125
Epoch 930, val loss: 1.0211399793624878
Epoch 940, training loss: 88.89207458496094 = 1.0151046514511108 + 10.0 * 8.787696838378906
Epoch 940, val loss: 1.0195342302322388
Epoch 950, training loss: 88.94815826416016 = 1.0134702920913696 + 10.0 * 8.793468475341797
Epoch 950, val loss: 1.0180318355560303
Epoch 960, training loss: 88.96432495117188 = 1.0117007493972778 + 10.0 * 8.795262336730957
Epoch 960, val loss: 1.0163953304290771
Epoch 970, training loss: 88.98585510253906 = 1.009950876235962 + 10.0 * 8.797590255737305
Epoch 970, val loss: 1.0147852897644043
Epoch 980, training loss: 89.03496551513672 = 1.008196234703064 + 10.0 * 8.802677154541016
Epoch 980, val loss: 1.0131648778915405
Epoch 990, training loss: 89.00143432617188 = 1.0063810348510742 + 10.0 * 8.799505233764648
Epoch 990, val loss: 1.0114829540252686
Epoch 1000, training loss: 89.03033447265625 = 1.0045788288116455 + 10.0 * 8.802576065063477
Epoch 1000, val loss: 1.009836196899414
Epoch 1010, training loss: 89.0950927734375 = 1.002743124961853 + 10.0 * 8.809234619140625
Epoch 1010, val loss: 1.0081379413604736
Epoch 1020, training loss: 89.09080505371094 = 1.0007916688919067 + 10.0 * 8.809000968933105
Epoch 1020, val loss: 1.006319284439087
Epoch 1030, training loss: 89.10196685791016 = 0.9987254738807678 + 10.0 * 8.810323715209961
Epoch 1030, val loss: 1.004420518875122
Epoch 1040, training loss: 89.15662384033203 = 0.9965761303901672 + 10.0 * 8.816004753112793
Epoch 1040, val loss: 1.0024616718292236
Epoch 1050, training loss: 89.17303466796875 = 0.994442880153656 + 10.0 * 8.817858695983887
Epoch 1050, val loss: 1.0004926919937134
Epoch 1060, training loss: 89.18742370605469 = 0.9923048615455627 + 10.0 * 8.819512367248535
Epoch 1060, val loss: 0.9985504746437073
Epoch 1070, training loss: 89.1753158569336 = 0.9901363253593445 + 10.0 * 8.818517684936523
Epoch 1070, val loss: 0.9964842796325684
Epoch 1080, training loss: 89.15548706054688 = 0.9880242943763733 + 10.0 * 8.816746711730957
Epoch 1080, val loss: 0.9946534037590027
Epoch 1090, training loss: 88.96873474121094 = 0.9858869910240173 + 10.0 * 8.798284530639648
Epoch 1090, val loss: 0.9926978349685669
Epoch 1100, training loss: 88.98773193359375 = 0.9837648272514343 + 10.0 * 8.800396919250488
Epoch 1100, val loss: 0.9907457232475281
Epoch 1110, training loss: 89.11486053466797 = 0.9816398024559021 + 10.0 * 8.813322067260742
Epoch 1110, val loss: 0.9887880682945251
Epoch 1120, training loss: 89.15074920654297 = 0.9794450402259827 + 10.0 * 8.817130088806152
Epoch 1120, val loss: 0.9867729544639587
Epoch 1130, training loss: 89.20086669921875 = 0.9772554636001587 + 10.0 * 8.82236099243164
Epoch 1130, val loss: 0.984772264957428
Epoch 1140, training loss: 89.21255493164062 = 0.9750545620918274 + 10.0 * 8.823750495910645
Epoch 1140, val loss: 0.9827352166175842
Epoch 1150, training loss: 88.99320983886719 = 0.9727476239204407 + 10.0 * 8.802045822143555
Epoch 1150, val loss: 0.980646014213562
Epoch 1160, training loss: 89.1312255859375 = 0.9705528020858765 + 10.0 * 8.816067695617676
Epoch 1160, val loss: 0.9786340594291687
Epoch 1170, training loss: 89.20529174804688 = 0.9683732390403748 + 10.0 * 8.823691368103027
Epoch 1170, val loss: 0.9766401052474976
Epoch 1180, training loss: 89.3095703125 = 0.9661479592323303 + 10.0 * 8.834342002868652
Epoch 1180, val loss: 0.9745875000953674
Epoch 1190, training loss: 89.32583618164062 = 0.9638734459877014 + 10.0 * 8.836195945739746
Epoch 1190, val loss: 0.9725086092948914
Epoch 1200, training loss: 89.35099792480469 = 0.9616079330444336 + 10.0 * 8.83893871307373
Epoch 1200, val loss: 0.9704288244247437
Epoch 1210, training loss: 89.28458404541016 = 0.9593263864517212 + 10.0 * 8.832525253295898
Epoch 1210, val loss: 0.9683342576026917
Epoch 1220, training loss: 89.3424301147461 = 0.957038938999176 + 10.0 * 8.838539123535156
Epoch 1220, val loss: 0.9662423133850098
Epoch 1230, training loss: 89.00341796875 = 0.9546198844909668 + 10.0 * 8.804880142211914
Epoch 1230, val loss: 0.9640514850616455
Epoch 1240, training loss: 89.1875 = 0.952481746673584 + 10.0 * 8.823501586914062
Epoch 1240, val loss: 0.9620611071586609
Epoch 1250, training loss: 89.2886734008789 = 0.9502037167549133 + 10.0 * 8.833847045898438
Epoch 1250, val loss: 0.9599683880805969
Epoch 1260, training loss: 89.3074951171875 = 0.9479058980941772 + 10.0 * 8.835958480834961
Epoch 1260, val loss: 0.9578628540039062
Epoch 1270, training loss: 89.35347747802734 = 0.9456099271774292 + 10.0 * 8.840786933898926
Epoch 1270, val loss: 0.9557523727416992
Epoch 1280, training loss: 89.3929214477539 = 0.9432960152626038 + 10.0 * 8.844962120056152
Epoch 1280, val loss: 0.9536384344100952
Epoch 1290, training loss: 89.43412017822266 = 0.9409933686256409 + 10.0 * 8.849312782287598
Epoch 1290, val loss: 0.951521635055542
Epoch 1300, training loss: 89.46363067626953 = 0.938668429851532 + 10.0 * 8.852496147155762
Epoch 1300, val loss: 0.949392557144165
Epoch 1310, training loss: 89.44268035888672 = 0.936313807964325 + 10.0 * 8.85063648223877
Epoch 1310, val loss: 0.9472190737724304
Epoch 1320, training loss: 89.4496078491211 = 0.933983564376831 + 10.0 * 8.8515625
Epoch 1320, val loss: 0.9450995326042175
Epoch 1330, training loss: 89.50935363769531 = 0.9316349029541016 + 10.0 * 8.857771873474121
Epoch 1330, val loss: 0.9429451823234558
Epoch 1340, training loss: 89.56411743164062 = 0.9292796850204468 + 10.0 * 8.863483428955078
Epoch 1340, val loss: 0.9408044815063477
Epoch 1350, training loss: 89.5645751953125 = 0.9268863201141357 + 10.0 * 8.863768577575684
Epoch 1350, val loss: 0.9386043548583984
Epoch 1360, training loss: 89.54545593261719 = 0.9244867563247681 + 10.0 * 8.862096786499023
Epoch 1360, val loss: 0.9364174008369446
Epoch 1370, training loss: 89.61531066894531 = 0.9221150875091553 + 10.0 * 8.869319915771484
Epoch 1370, val loss: 0.9342479705810547
Epoch 1380, training loss: 89.63333129882812 = 0.919723391532898 + 10.0 * 8.871360778808594
Epoch 1380, val loss: 0.9320396184921265
Epoch 1390, training loss: 89.59697723388672 = 0.9173112511634827 + 10.0 * 8.867966651916504
Epoch 1390, val loss: 0.9298378229141235
Epoch 1400, training loss: 89.6434097290039 = 0.9149379134178162 + 10.0 * 8.872846603393555
Epoch 1400, val loss: 0.927649199962616
Epoch 1410, training loss: 89.6701889038086 = 0.9125760197639465 + 10.0 * 8.875761032104492
Epoch 1410, val loss: 0.9254620671272278
Epoch 1420, training loss: 89.67089080810547 = 0.9101815223693848 + 10.0 * 8.876070976257324
Epoch 1420, val loss: 0.9232783317565918
Epoch 1430, training loss: 89.6484603881836 = 0.9077961444854736 + 10.0 * 8.874066352844238
Epoch 1430, val loss: 0.9210846424102783
Epoch 1440, training loss: 89.65556335449219 = 0.9053979516029358 + 10.0 * 8.875017166137695
Epoch 1440, val loss: 0.918893039226532
Epoch 1450, training loss: 89.63057708740234 = 0.9030181765556335 + 10.0 * 8.872756004333496
Epoch 1450, val loss: 0.9167085886001587
Epoch 1460, training loss: 89.70767974853516 = 0.9006932973861694 + 10.0 * 8.880698204040527
Epoch 1460, val loss: 0.9145674705505371
Epoch 1470, training loss: 89.7453842163086 = 0.8983086943626404 + 10.0 * 8.8847074508667
Epoch 1470, val loss: 0.9123754501342773
Epoch 1480, training loss: 89.73627471923828 = 0.8959366679191589 + 10.0 * 8.884034156799316
Epoch 1480, val loss: 0.9101758599281311
Epoch 1490, training loss: 89.73760986328125 = 0.8935551047325134 + 10.0 * 8.884405136108398
Epoch 1490, val loss: 0.9080022573471069
Epoch 1500, training loss: 89.76881408691406 = 0.8912025690078735 + 10.0 * 8.887761116027832
Epoch 1500, val loss: 0.9058312177658081
Epoch 1510, training loss: 89.7803955078125 = 0.8887979388237 + 10.0 * 8.889159202575684
Epoch 1510, val loss: 0.9036490321159363
Epoch 1520, training loss: 89.78900909423828 = 0.8864253759384155 + 10.0 * 8.890257835388184
Epoch 1520, val loss: 0.9014768600463867
Epoch 1530, training loss: 89.79987335205078 = 0.8840480446815491 + 10.0 * 8.891582489013672
Epoch 1530, val loss: 0.8993067741394043
Epoch 1540, training loss: 89.79389953613281 = 0.8816718459129333 + 10.0 * 8.891222953796387
Epoch 1540, val loss: 0.8971152305603027
Epoch 1550, training loss: 89.8141860961914 = 0.8793068528175354 + 10.0 * 8.893487930297852
Epoch 1550, val loss: 0.8949515223503113
Epoch 1560, training loss: 89.85125732421875 = 0.8769528865814209 + 10.0 * 8.897430419921875
Epoch 1560, val loss: 0.892781138420105
Epoch 1570, training loss: 89.84141540527344 = 0.8745690584182739 + 10.0 * 8.896684646606445
Epoch 1570, val loss: 0.8906179666519165
Epoch 1580, training loss: 89.86304473876953 = 0.8722341060638428 + 10.0 * 8.899081230163574
Epoch 1580, val loss: 0.8884482383728027
Epoch 1590, training loss: 89.88235473632812 = 0.8698758482933044 + 10.0 * 8.90124797821045
Epoch 1590, val loss: 0.8863078355789185
Epoch 1600, training loss: 89.87525177001953 = 0.867516279220581 + 10.0 * 8.900774002075195
Epoch 1600, val loss: 0.8841447830200195
Epoch 1610, training loss: 89.86604309082031 = 0.865146815776825 + 10.0 * 8.900089263916016
Epoch 1610, val loss: 0.8820029497146606
Epoch 1620, training loss: 89.90972137451172 = 0.8628129959106445 + 10.0 * 8.904690742492676
Epoch 1620, val loss: 0.8798408508300781
Epoch 1630, training loss: 89.90714263916016 = 0.8604692220687866 + 10.0 * 8.904667854309082
Epoch 1630, val loss: 0.8776798844337463
Epoch 1640, training loss: 89.8908462524414 = 0.8581069707870483 + 10.0 * 8.903273582458496
Epoch 1640, val loss: 0.8755412697792053
Epoch 1650, training loss: 89.92596435546875 = 0.8557927012443542 + 10.0 * 8.907017707824707
Epoch 1650, val loss: 0.8733812570571899
Epoch 1660, training loss: 89.93682861328125 = 0.8534830808639526 + 10.0 * 8.908334732055664
Epoch 1660, val loss: 0.8713124394416809
Epoch 1670, training loss: 89.68345642089844 = 0.8511844873428345 + 10.0 * 8.883227348327637
Epoch 1670, val loss: 0.8691902756690979
Epoch 1680, training loss: 89.59727478027344 = 0.8489237427711487 + 10.0 * 8.874835014343262
Epoch 1680, val loss: 0.867114782333374
Epoch 1690, training loss: 89.68806457519531 = 0.8466930985450745 + 10.0 * 8.884137153625488
Epoch 1690, val loss: 0.8650721907615662
Epoch 1700, training loss: 89.66035461425781 = 0.8444992899894714 + 10.0 * 8.881586074829102
Epoch 1700, val loss: 0.8630797863006592
Epoch 1710, training loss: 89.7718505859375 = 0.8422213196754456 + 10.0 * 8.892962455749512
Epoch 1710, val loss: 0.8610011339187622
Epoch 1720, training loss: 89.82601165771484 = 0.8399529457092285 + 10.0 * 8.898606300354004
Epoch 1720, val loss: 0.8589147925376892
Epoch 1730, training loss: 89.86672973632812 = 0.8376789093017578 + 10.0 * 8.902905464172363
Epoch 1730, val loss: 0.8568224906921387
Epoch 1740, training loss: 89.9297103881836 = 0.8353680372238159 + 10.0 * 8.90943431854248
Epoch 1740, val loss: 0.8547085523605347
Epoch 1750, training loss: 89.95459747314453 = 0.8330796957015991 + 10.0 * 8.912152290344238
Epoch 1750, val loss: 0.8525913953781128
Epoch 1760, training loss: 89.97515106201172 = 0.830784022808075 + 10.0 * 8.914436340332031
Epoch 1760, val loss: 0.8504959940910339
Epoch 1770, training loss: 89.99098205566406 = 0.8284940123558044 + 10.0 * 8.91624927520752
Epoch 1770, val loss: 0.8483844995498657
Epoch 1780, training loss: 89.98653411865234 = 0.8261969685554504 + 10.0 * 8.916033744812012
Epoch 1780, val loss: 0.8462847471237183
Epoch 1790, training loss: 90.02835083007812 = 0.8239172101020813 + 10.0 * 8.920443534851074
Epoch 1790, val loss: 0.8441779017448425
Epoch 1800, training loss: 90.00968933105469 = 0.821649968624115 + 10.0 * 8.918804168701172
Epoch 1800, val loss: 0.8420683145523071
Epoch 1810, training loss: 89.98860168457031 = 0.8193598985671997 + 10.0 * 8.916924476623535
Epoch 1810, val loss: 0.8399707674980164
Epoch 1820, training loss: 90.03854370117188 = 0.8171292543411255 + 10.0 * 8.922141075134277
Epoch 1820, val loss: 0.8379065990447998
Epoch 1830, training loss: 90.05818176269531 = 0.8148658275604248 + 10.0 * 8.924331665039062
Epoch 1830, val loss: 0.8358342051506042
Epoch 1840, training loss: 90.07951354980469 = 0.8126058578491211 + 10.0 * 8.926691055297852
Epoch 1840, val loss: 0.8337555527687073
Epoch 1850, training loss: 90.02427673339844 = 0.8103743195533752 + 10.0 * 8.921390533447266
Epoch 1850, val loss: 0.8317121267318726
Epoch 1860, training loss: 90.06729888916016 = 0.8084103465080261 + 10.0 * 8.925889015197754
Epoch 1860, val loss: 0.8300022482872009
Epoch 1870, training loss: 89.91378784179688 = 0.8060506582260132 + 10.0 * 8.910773277282715
Epoch 1870, val loss: 0.8277499079704285
Epoch 1880, training loss: 89.93842315673828 = 0.8041771650314331 + 10.0 * 8.913424491882324
Epoch 1880, val loss: 0.8260601758956909
Epoch 1890, training loss: 90.03262329101562 = 0.8019868731498718 + 10.0 * 8.923063278198242
Epoch 1890, val loss: 0.8240878582000732
Epoch 1900, training loss: 90.04570007324219 = 0.7998216152191162 + 10.0 * 8.924588203430176
Epoch 1900, val loss: 0.8220873475074768
Epoch 1910, training loss: 90.10980224609375 = 0.7976248860359192 + 10.0 * 8.931218147277832
Epoch 1910, val loss: 0.820076048374176
Epoch 1920, training loss: 90.2204360961914 = 0.7954553961753845 + 10.0 * 8.942498207092285
Epoch 1920, val loss: 0.8180727362632751
Epoch 1930, training loss: 90.27647399902344 = 0.7932462692260742 + 10.0 * 8.948323249816895
Epoch 1930, val loss: 0.8160560131072998
Epoch 1940, training loss: 90.27445983886719 = 0.7910315990447998 + 10.0 * 8.948343276977539
Epoch 1940, val loss: 0.8140255212783813
Epoch 1950, training loss: 90.33389282226562 = 0.7887917160987854 + 10.0 * 8.954509735107422
Epoch 1950, val loss: 0.8119760155677795
Epoch 1960, training loss: 90.32138061523438 = 0.7865620255470276 + 10.0 * 8.953481674194336
Epoch 1960, val loss: 0.8099331259727478
Epoch 1970, training loss: 90.38459777832031 = 0.7843285202980042 + 10.0 * 8.960026741027832
Epoch 1970, val loss: 0.807883083820343
Epoch 1980, training loss: 90.3979263305664 = 0.7820994853973389 + 10.0 * 8.961583137512207
Epoch 1980, val loss: 0.8058380484580994
Epoch 1990, training loss: 90.36854553222656 = 0.7798735499382019 + 10.0 * 8.958867073059082
Epoch 1990, val loss: 0.803792417049408
Epoch 2000, training loss: 90.40829467773438 = 0.7776656150817871 + 10.0 * 8.96306324005127
Epoch 2000, val loss: 0.8017706871032715
Epoch 2010, training loss: 90.45094299316406 = 0.7754589915275574 + 10.0 * 8.967548370361328
Epoch 2010, val loss: 0.7997524738311768
Epoch 2020, training loss: 90.45458221435547 = 0.7732642292976379 + 10.0 * 8.968132019042969
Epoch 2020, val loss: 0.7977370023727417
Epoch 2030, training loss: 90.46973419189453 = 0.7710485458374023 + 10.0 * 8.969868659973145
Epoch 2030, val loss: 0.7957148551940918
Epoch 2040, training loss: 90.49883270263672 = 0.7688507437705994 + 10.0 * 8.972997665405273
Epoch 2040, val loss: 0.7936887145042419
Epoch 2050, training loss: 90.51947784423828 = 0.7666375637054443 + 10.0 * 8.9752836227417
Epoch 2050, val loss: 0.7916579842567444
Epoch 2060, training loss: 90.50514221191406 = 0.7644318342208862 + 10.0 * 8.97407054901123
Epoch 2060, val loss: 0.7896147966384888
Epoch 2070, training loss: 90.37351989746094 = 0.7621936798095703 + 10.0 * 8.961133003234863
Epoch 2070, val loss: 0.7875570058822632
Epoch 2080, training loss: 90.4313735961914 = 0.7600877285003662 + 10.0 * 8.96712875366211
Epoch 2080, val loss: 0.7856260538101196
Epoch 2090, training loss: 90.50814819335938 = 0.7579195499420166 + 10.0 * 8.97502326965332
Epoch 2090, val loss: 0.7836219668388367
Epoch 2100, training loss: 90.54791259765625 = 0.7557302117347717 + 10.0 * 8.979218482971191
Epoch 2100, val loss: 0.7816000580787659
Epoch 2110, training loss: 90.58174133300781 = 0.7535486817359924 + 10.0 * 8.982819557189941
Epoch 2110, val loss: 0.7795722484588623
Epoch 2120, training loss: 90.5453109741211 = 0.7513620853424072 + 10.0 * 8.979394912719727
Epoch 2120, val loss: 0.777559220790863
Epoch 2130, training loss: 90.56922912597656 = 0.749219536781311 + 10.0 * 8.982000350952148
Epoch 2130, val loss: 0.7755792140960693
Epoch 2140, training loss: 90.63716888427734 = 0.7470477819442749 + 10.0 * 8.989011764526367
Epoch 2140, val loss: 0.7735898494720459
Epoch 2150, training loss: 90.60845184326172 = 0.7448965311050415 + 10.0 * 8.986355781555176
Epoch 2150, val loss: 0.7716109156608582
Epoch 2160, training loss: 90.59999084472656 = 0.7427788972854614 + 10.0 * 8.98572063446045
Epoch 2160, val loss: 0.769712507724762
Epoch 2170, training loss: 90.6276626586914 = 0.7406604290008545 + 10.0 * 8.988699913024902
Epoch 2170, val loss: 0.7677698135375977
Epoch 2180, training loss: 90.67054748535156 = 0.7385365962982178 + 10.0 * 8.99320125579834
Epoch 2180, val loss: 0.7658384442329407
Epoch 2190, training loss: 90.70185852050781 = 0.7364279627799988 + 10.0 * 8.996542930603027
Epoch 2190, val loss: 0.7639164924621582
Epoch 2200, training loss: 90.65772247314453 = 0.7343193292617798 + 10.0 * 8.992340087890625
Epoch 2200, val loss: 0.7620081901550293
Epoch 2210, training loss: 90.55415344238281 = 0.7323039770126343 + 10.0 * 8.982184410095215
Epoch 2210, val loss: 0.7601547837257385
Epoch 2220, training loss: 90.52122497558594 = 0.730379581451416 + 10.0 * 8.979084968566895
Epoch 2220, val loss: 0.7584238648414612
Epoch 2230, training loss: 90.5898208618164 = 0.7283833622932434 + 10.0 * 8.986143112182617
Epoch 2230, val loss: 0.756661593914032
Epoch 2240, training loss: 90.6473617553711 = 0.7263367772102356 + 10.0 * 8.99210262298584
Epoch 2240, val loss: 0.7548261284828186
Epoch 2250, training loss: 90.72038269042969 = 0.724277913570404 + 10.0 * 8.99960994720459
Epoch 2250, val loss: 0.7529712915420532
Epoch 2260, training loss: 90.76068115234375 = 0.7222259640693665 + 10.0 * 9.00384521484375
Epoch 2260, val loss: 0.7511284947395325
Epoch 2270, training loss: 90.70059967041016 = 0.7202093601226807 + 10.0 * 8.998039245605469
Epoch 2270, val loss: 0.7492727041244507
Epoch 2280, training loss: 90.7264633178711 = 0.7182046175003052 + 10.0 * 9.000825881958008
Epoch 2280, val loss: 0.7475061416625977
Epoch 2290, training loss: 90.76508331298828 = 0.716221809387207 + 10.0 * 9.00488567352295
Epoch 2290, val loss: 0.7457186579704285
Epoch 2300, training loss: 90.79117584228516 = 0.7142381072044373 + 10.0 * 9.00769329071045
Epoch 2300, val loss: 0.7439283132553101
Epoch 2310, training loss: 90.78933715820312 = 0.7122687697410583 + 10.0 * 9.007707595825195
Epoch 2310, val loss: 0.7421608567237854
Epoch 2320, training loss: 90.66498565673828 = 0.7104543447494507 + 10.0 * 8.995452880859375
Epoch 2320, val loss: 0.7404601573944092
Epoch 2330, training loss: 90.75589752197266 = 0.7087971568107605 + 10.0 * 9.00471019744873
Epoch 2330, val loss: 0.739035427570343
Epoch 2340, training loss: 90.68458557128906 = 0.7069513201713562 + 10.0 * 8.997762680053711
Epoch 2340, val loss: 0.7375268936157227
Epoch 2350, training loss: 90.76577758789062 = 0.7051283121109009 + 10.0 * 9.006064414978027
Epoch 2350, val loss: 0.735893726348877
Epoch 2360, training loss: 90.87474822998047 = 0.7032516002655029 + 10.0 * 9.017148971557617
Epoch 2360, val loss: 0.7341582775115967
Epoch 2370, training loss: 90.9110336303711 = 0.7013475894927979 + 10.0 * 9.020968437194824
Epoch 2370, val loss: 0.7324661016464233
Epoch 2380, training loss: 90.97148895263672 = 0.6994182467460632 + 10.0 * 9.027207374572754
Epoch 2380, val loss: 0.7307446002960205
Epoch 2390, training loss: 91.01750183105469 = 0.6975138187408447 + 10.0 * 9.031998634338379
Epoch 2390, val loss: 0.7290212512016296
Epoch 2400, training loss: 91.03191375732422 = 0.6956153512001038 + 10.0 * 9.033629417419434
Epoch 2400, val loss: 0.7273202538490295
Epoch 2410, training loss: 91.02693939208984 = 0.6937369704246521 + 10.0 * 9.033320426940918
Epoch 2410, val loss: 0.725649356842041
Epoch 2420, training loss: 91.01051330566406 = 0.6918786764144897 + 10.0 * 9.03186321258545
Epoch 2420, val loss: 0.7240120768547058
Epoch 2430, training loss: 91.04446411132812 = 0.6900306344032288 + 10.0 * 9.035443305969238
Epoch 2430, val loss: 0.7223743796348572
Epoch 2440, training loss: 91.06883239746094 = 0.6881980299949646 + 10.0 * 9.038064002990723
Epoch 2440, val loss: 0.720746636390686
Epoch 2450, training loss: 91.09233856201172 = 0.6864062547683716 + 10.0 * 9.040593147277832
Epoch 2450, val loss: 0.7191413044929504
Epoch 2460, training loss: 91.11875915527344 = 0.6846165060997009 + 10.0 * 9.043414115905762
Epoch 2460, val loss: 0.7175694108009338
Epoch 2470, training loss: 91.11839294433594 = 0.6828430891036987 + 10.0 * 9.043554306030273
Epoch 2470, val loss: 0.7160218954086304
Epoch 2480, training loss: 91.07864379882812 = 0.6810705661773682 + 10.0 * 9.03975772857666
Epoch 2480, val loss: 0.7144990563392639
Epoch 2490, training loss: 91.12296295166016 = 0.6793537735939026 + 10.0 * 9.044361114501953
Epoch 2490, val loss: 0.7129788398742676
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7014492753623188
0.8640150691878578
=== training gcn model ===
Epoch 0, training loss: 102.9293441772461 = 1.1069432497024536 + 10.0 * 10.18224048614502
Epoch 0, val loss: 1.107171893119812
Epoch 10, training loss: 98.84068298339844 = 1.1062358617782593 + 10.0 * 9.773444175720215
Epoch 10, val loss: 1.1065000295639038
Epoch 20, training loss: 96.8415756225586 = 1.1056290864944458 + 10.0 * 9.57359504699707
Epoch 20, val loss: 1.105870246887207
Epoch 30, training loss: 95.37033081054688 = 1.1049712896347046 + 10.0 * 9.426535606384277
Epoch 30, val loss: 1.1052254438400269
Epoch 40, training loss: 94.22517395019531 = 1.104336142539978 + 10.0 * 9.31208324432373
Epoch 40, val loss: 1.104591965675354
Epoch 50, training loss: 93.32926940917969 = 1.1036953926086426 + 10.0 * 9.222557067871094
Epoch 50, val loss: 1.10395348072052
Epoch 60, training loss: 92.5784912109375 = 1.103053092956543 + 10.0 * 9.147543907165527
Epoch 60, val loss: 1.1033201217651367
Epoch 70, training loss: 91.95420837402344 = 1.1024054288864136 + 10.0 * 9.085180282592773
Epoch 70, val loss: 1.1026792526245117
Epoch 80, training loss: 91.42788696289062 = 1.1017688512802124 + 10.0 * 9.032611846923828
Epoch 80, val loss: 1.102053165435791
Epoch 90, training loss: 90.97701263427734 = 1.1011292934417725 + 10.0 * 8.987588882446289
Epoch 90, val loss: 1.1014196872711182
Epoch 100, training loss: 90.57279205322266 = 1.100487470626831 + 10.0 * 8.947230339050293
Epoch 100, val loss: 1.1007791757583618
Epoch 110, training loss: 90.23826599121094 = 1.0998388528823853 + 10.0 * 8.913843154907227
Epoch 110, val loss: 1.1001430749893188
Epoch 120, training loss: 89.92820739746094 = 1.0991787910461426 + 10.0 * 8.882902145385742
Epoch 120, val loss: 1.0994937419891357
Epoch 130, training loss: 89.68827056884766 = 1.098523736000061 + 10.0 * 8.85897445678711
Epoch 130, val loss: 1.0988495349884033
Epoch 140, training loss: 89.45169067382812 = 1.0978474617004395 + 10.0 * 8.835384368896484
Epoch 140, val loss: 1.0981942415237427
Epoch 150, training loss: 89.24211883544922 = 1.0971709489822388 + 10.0 * 8.814495086669922
Epoch 150, val loss: 1.0975162982940674
Epoch 160, training loss: 89.06531524658203 = 1.0964972972869873 + 10.0 * 8.796881675720215
Epoch 160, val loss: 1.0968589782714844
Epoch 170, training loss: 88.89675903320312 = 1.095808506011963 + 10.0 * 8.780095100402832
Epoch 170, val loss: 1.0961815118789673
Epoch 180, training loss: 88.75608825683594 = 1.0951101779937744 + 10.0 * 8.766098022460938
Epoch 180, val loss: 1.0954971313476562
Epoch 190, training loss: 88.65769958496094 = 1.094417691230774 + 10.0 * 8.756327629089355
Epoch 190, val loss: 1.094807744026184
Epoch 200, training loss: 88.57540130615234 = 1.0937070846557617 + 10.0 * 8.748169898986816
Epoch 200, val loss: 1.0941256284713745
Epoch 210, training loss: 88.45829772949219 = 1.0929837226867676 + 10.0 * 8.736531257629395
Epoch 210, val loss: 1.093397855758667
Epoch 220, training loss: 88.38663482666016 = 1.0922784805297852 + 10.0 * 8.729435920715332
Epoch 220, val loss: 1.0927315950393677
Epoch 230, training loss: 88.27864074707031 = 1.0914928913116455 + 10.0 * 8.718714714050293
Epoch 230, val loss: 1.091948390007019
Epoch 240, training loss: 88.26399993896484 = 1.090779185295105 + 10.0 * 8.717321395874023
Epoch 240, val loss: 1.0912326574325562
Epoch 250, training loss: 88.20708465576172 = 1.0900108814239502 + 10.0 * 8.71170711517334
Epoch 250, val loss: 1.0904860496520996
Epoch 260, training loss: 88.14457702636719 = 1.0892127752304077 + 10.0 * 8.705536842346191
Epoch 260, val loss: 1.0897079706192017
Epoch 270, training loss: 88.11045837402344 = 1.0884112119674683 + 10.0 * 8.702204704284668
Epoch 270, val loss: 1.088926076889038
Epoch 280, training loss: 88.08551025390625 = 1.0875978469848633 + 10.0 * 8.699790954589844
Epoch 280, val loss: 1.0881234407424927
Epoch 290, training loss: 88.01240539550781 = 1.0867540836334229 + 10.0 * 8.692564964294434
Epoch 290, val loss: 1.0873066186904907
Epoch 300, training loss: 87.9818344116211 = 1.0859273672103882 + 10.0 * 8.689590454101562
Epoch 300, val loss: 1.0864818096160889
Epoch 310, training loss: 87.96664428710938 = 1.0850924253463745 + 10.0 * 8.688155174255371
Epoch 310, val loss: 1.085671067237854
Epoch 320, training loss: 87.95491790771484 = 1.0842729806900024 + 10.0 * 8.687064170837402
Epoch 320, val loss: 1.0848649740219116
Epoch 330, training loss: 87.95442199707031 = 1.0834345817565918 + 10.0 * 8.687098503112793
Epoch 330, val loss: 1.0840356349945068
Epoch 340, training loss: 87.92530059814453 = 1.0825773477554321 + 10.0 * 8.684271812438965
Epoch 340, val loss: 1.0832014083862305
Epoch 350, training loss: 87.91543579101562 = 1.081751823425293 + 10.0 * 8.683368682861328
Epoch 350, val loss: 1.0824084281921387
Epoch 360, training loss: 87.89881134033203 = 1.080927848815918 + 10.0 * 8.681788444519043
Epoch 360, val loss: 1.0815871953964233
Epoch 370, training loss: 87.91006469726562 = 1.0800756216049194 + 10.0 * 8.682998657226562
Epoch 370, val loss: 1.080772042274475
Epoch 380, training loss: 87.8972396850586 = 1.0792680978775024 + 10.0 * 8.68179702758789
Epoch 380, val loss: 1.0799734592437744
Epoch 390, training loss: 87.90193939208984 = 1.0784653425216675 + 10.0 * 8.682347297668457
Epoch 390, val loss: 1.079199194908142
Epoch 400, training loss: 87.88560485839844 = 1.0776604413986206 + 10.0 * 8.680794715881348
Epoch 400, val loss: 1.0784037113189697
Epoch 410, training loss: 87.87788391113281 = 1.0768983364105225 + 10.0 * 8.680098533630371
Epoch 410, val loss: 1.0776581764221191
Epoch 420, training loss: 87.89910888671875 = 1.0761879682540894 + 10.0 * 8.682291984558105
Epoch 420, val loss: 1.0769600868225098
Epoch 430, training loss: 87.89661407470703 = 1.0754696130752563 + 10.0 * 8.682114601135254
Epoch 430, val loss: 1.0762690305709839
Epoch 440, training loss: 87.91534423828125 = 1.074779748916626 + 10.0 * 8.684056282043457
Epoch 440, val loss: 1.0755953788757324
Epoch 450, training loss: 87.91930389404297 = 1.0740773677825928 + 10.0 * 8.68452262878418
Epoch 450, val loss: 1.074915885925293
Epoch 460, training loss: 87.9203109741211 = 1.073405146598816 + 10.0 * 8.684690475463867
Epoch 460, val loss: 1.0742794275283813
Epoch 470, training loss: 87.9532470703125 = 1.0727466344833374 + 10.0 * 8.688050270080566
Epoch 470, val loss: 1.0736160278320312
Epoch 480, training loss: 87.97111511230469 = 1.0720783472061157 + 10.0 * 8.68990421295166
Epoch 480, val loss: 1.0729777812957764
Epoch 490, training loss: 87.9621353149414 = 1.0714069604873657 + 10.0 * 8.68907356262207
Epoch 490, val loss: 1.0723259449005127
Epoch 500, training loss: 87.94498443603516 = 1.0707101821899414 + 10.0 * 8.687427520751953
Epoch 500, val loss: 1.0716897249221802
Epoch 510, training loss: 87.97362518310547 = 1.0700639486312866 + 10.0 * 8.690356254577637
Epoch 510, val loss: 1.0710502862930298
Epoch 520, training loss: 88.00965118408203 = 1.0694137811660767 + 10.0 * 8.694024085998535
Epoch 520, val loss: 1.0704158544540405
Epoch 530, training loss: 88.0040283203125 = 1.0687114000320435 + 10.0 * 8.69353199005127
Epoch 530, val loss: 1.069749355316162
Epoch 540, training loss: 88.01431274414062 = 1.0680187940597534 + 10.0 * 8.694629669189453
Epoch 540, val loss: 1.0690765380859375
Epoch 550, training loss: 88.01828002929688 = 1.0673304796218872 + 10.0 * 8.69509506225586
Epoch 550, val loss: 1.0684233903884888
Epoch 560, training loss: 88.02173614501953 = 1.0666183233261108 + 10.0 * 8.695511817932129
Epoch 560, val loss: 1.0677536725997925
Epoch 570, training loss: 88.0619888305664 = 1.0659006834030151 + 10.0 * 8.69960880279541
Epoch 570, val loss: 1.067046046257019
Epoch 580, training loss: 88.08866119384766 = 1.0652059316635132 + 10.0 * 8.70234489440918
Epoch 580, val loss: 1.0663838386535645
Epoch 590, training loss: 88.04678344726562 = 1.0644376277923584 + 10.0 * 8.698234558105469
Epoch 590, val loss: 1.0656440258026123
Epoch 600, training loss: 88.09796905517578 = 1.0637292861938477 + 10.0 * 8.703424453735352
Epoch 600, val loss: 1.0649688243865967
Epoch 610, training loss: 88.12274169921875 = 1.062990427017212 + 10.0 * 8.705975532531738
Epoch 610, val loss: 1.0642666816711426
Epoch 620, training loss: 88.16602325439453 = 1.0622154474258423 + 10.0 * 8.710380554199219
Epoch 620, val loss: 1.0635238885879517
Epoch 630, training loss: 88.15019226074219 = 1.0614337921142578 + 10.0 * 8.70887565612793
Epoch 630, val loss: 1.062782645225525
Epoch 640, training loss: 88.16730499267578 = 1.060665249824524 + 10.0 * 8.710663795471191
Epoch 640, val loss: 1.0620568990707397
Epoch 650, training loss: 88.19035339355469 = 1.0598511695861816 + 10.0 * 8.71304988861084
Epoch 650, val loss: 1.0612773895263672
Epoch 660, training loss: 88.16715240478516 = 1.0589709281921387 + 10.0 * 8.71081829071045
Epoch 660, val loss: 1.0604314804077148
Epoch 670, training loss: 88.19180297851562 = 1.0581246614456177 + 10.0 * 8.713367462158203
Epoch 670, val loss: 1.059661626815796
Epoch 680, training loss: 88.1445541381836 = 1.0573819875717163 + 10.0 * 8.708717346191406
Epoch 680, val loss: 1.0588891506195068
Epoch 690, training loss: 88.14327239990234 = 1.0565210580825806 + 10.0 * 8.708675384521484
Epoch 690, val loss: 1.0581247806549072
Epoch 700, training loss: 88.1132583618164 = 1.0556117296218872 + 10.0 * 8.705764770507812
Epoch 700, val loss: 1.0572630167007446
Epoch 710, training loss: 88.11280059814453 = 1.0547270774841309 + 10.0 * 8.70580768585205
Epoch 710, val loss: 1.0564160346984863
Epoch 720, training loss: 88.14207458496094 = 1.0538235902786255 + 10.0 * 8.70882511138916
Epoch 720, val loss: 1.0555450916290283
Epoch 730, training loss: 88.1845474243164 = 1.0528885126113892 + 10.0 * 8.713166236877441
Epoch 730, val loss: 1.0546514987945557
Epoch 740, training loss: 88.21025085449219 = 1.0519362688064575 + 10.0 * 8.715831756591797
Epoch 740, val loss: 1.0537384748458862
Epoch 750, training loss: 88.22602844238281 = 1.050963282585144 + 10.0 * 8.717506408691406
Epoch 750, val loss: 1.052817940711975
Epoch 760, training loss: 88.22599029541016 = 1.0499672889709473 + 10.0 * 8.717602729797363
Epoch 760, val loss: 1.051866888999939
Epoch 770, training loss: 88.28337097167969 = 1.048959732055664 + 10.0 * 8.723441123962402
Epoch 770, val loss: 1.0509079694747925
Epoch 780, training loss: 88.28179931640625 = 1.0479397773742676 + 10.0 * 8.72338581085205
Epoch 780, val loss: 1.0499217510223389
Epoch 790, training loss: 88.34894561767578 = 1.046920895576477 + 10.0 * 8.730202674865723
Epoch 790, val loss: 1.048943281173706
Epoch 800, training loss: 88.33094024658203 = 1.0458548069000244 + 10.0 * 8.728508949279785
Epoch 800, val loss: 1.0479457378387451
Epoch 810, training loss: 88.36058044433594 = 1.044782280921936 + 10.0 * 8.731579780578613
Epoch 810, val loss: 1.0469125509262085
Epoch 820, training loss: 88.37126922607422 = 1.0436968803405762 + 10.0 * 8.732757568359375
Epoch 820, val loss: 1.0458701848983765
Epoch 830, training loss: 88.3988265991211 = 1.0426123142242432 + 10.0 * 8.735621452331543
Epoch 830, val loss: 1.044839859008789
Epoch 840, training loss: 88.42461395263672 = 1.041503667831421 + 10.0 * 8.738310813903809
Epoch 840, val loss: 1.0437896251678467
Epoch 850, training loss: 88.47493743896484 = 1.0403972864151 + 10.0 * 8.743453979492188
Epoch 850, val loss: 1.0427333116531372
Epoch 860, training loss: 88.48335266113281 = 1.0392481088638306 + 10.0 * 8.744410514831543
Epoch 860, val loss: 1.0416253805160522
Epoch 870, training loss: 88.4842758178711 = 1.0380654335021973 + 10.0 * 8.744621276855469
Epoch 870, val loss: 1.0404962301254272
Epoch 880, training loss: 88.52216339111328 = 1.0368969440460205 + 10.0 * 8.748526573181152
Epoch 880, val loss: 1.0393885374069214
Epoch 890, training loss: 88.4902572631836 = 1.0356594324111938 + 10.0 * 8.74545955657959
Epoch 890, val loss: 1.0382119417190552
Epoch 900, training loss: 88.5505142211914 = 1.0344481468200684 + 10.0 * 8.751606941223145
Epoch 900, val loss: 1.0370666980743408
Epoch 910, training loss: 88.60221099853516 = 1.0332245826721191 + 10.0 * 8.756898880004883
Epoch 910, val loss: 1.035887360572815
Epoch 920, training loss: 88.60169219970703 = 1.0319459438323975 + 10.0 * 8.756975173950195
Epoch 920, val loss: 1.0346747636795044
Epoch 930, training loss: 88.6263427734375 = 1.030678153038025 + 10.0 * 8.759566307067871
Epoch 930, val loss: 1.033474087715149
Epoch 940, training loss: 88.64117431640625 = 1.0293852090835571 + 10.0 * 8.761178970336914
Epoch 940, val loss: 1.0322401523590088
Epoch 950, training loss: 88.6752700805664 = 1.0280673503875732 + 10.0 * 8.76471996307373
Epoch 950, val loss: 1.030990719795227
Epoch 960, training loss: 88.71897888183594 = 1.0267066955566406 + 10.0 * 8.769227981567383
Epoch 960, val loss: 1.0297274589538574
Epoch 970, training loss: 88.73063659667969 = 1.0253714323043823 + 10.0 * 8.770526885986328
Epoch 970, val loss: 1.0284132957458496
Epoch 980, training loss: 88.76949310302734 = 1.023983120918274 + 10.0 * 8.774550437927246
Epoch 980, val loss: 1.02712082862854
Epoch 990, training loss: 88.82315826416016 = 1.0226205587387085 + 10.0 * 8.780054092407227
Epoch 990, val loss: 1.0257993936538696
Epoch 1000, training loss: 88.82337951660156 = 1.0211775302886963 + 10.0 * 8.780220031738281
Epoch 1000, val loss: 1.024424433708191
Epoch 1010, training loss: 88.82467651367188 = 1.0197420120239258 + 10.0 * 8.780492782592773
Epoch 1010, val loss: 1.0230677127838135
Epoch 1020, training loss: 88.83563232421875 = 1.018263816833496 + 10.0 * 8.781736373901367
Epoch 1020, val loss: 1.0216797590255737
Epoch 1030, training loss: 88.88957214355469 = 1.0167969465255737 + 10.0 * 8.787277221679688
Epoch 1030, val loss: 1.0202549695968628
Epoch 1040, training loss: 88.89451599121094 = 1.0152990818023682 + 10.0 * 8.787921905517578
Epoch 1040, val loss: 1.0188430547714233
Epoch 1050, training loss: 88.92798614501953 = 1.0137782096862793 + 10.0 * 8.791420936584473
Epoch 1050, val loss: 1.0173804759979248
Epoch 1060, training loss: 88.9397964477539 = 1.0122588872909546 + 10.0 * 8.792753219604492
Epoch 1060, val loss: 1.0159250497817993
Epoch 1070, training loss: 88.95504760742188 = 1.0106980800628662 + 10.0 * 8.794435501098633
Epoch 1070, val loss: 1.0144646167755127
Epoch 1080, training loss: 88.97029113769531 = 1.009079933166504 + 10.0 * 8.796121597290039
Epoch 1080, val loss: 1.0128906965255737
Epoch 1090, training loss: 88.98023223876953 = 1.0074489116668701 + 10.0 * 8.79727840423584
Epoch 1090, val loss: 1.0113444328308105
Epoch 1100, training loss: 88.97715759277344 = 1.0058459043502808 + 10.0 * 8.797131538391113
Epoch 1100, val loss: 1.009832739830017
Epoch 1110, training loss: 88.99083709716797 = 1.0041953325271606 + 10.0 * 8.798664093017578
Epoch 1110, val loss: 1.0082635879516602
Epoch 1120, training loss: 89.034912109375 = 1.0025140047073364 + 10.0 * 8.803239822387695
Epoch 1120, val loss: 1.0066601037979126
Epoch 1130, training loss: 89.07146453857422 = 1.0008258819580078 + 10.0 * 8.807064056396484
Epoch 1130, val loss: 1.005047082901001
Epoch 1140, training loss: 89.0610122680664 = 0.9990700483322144 + 10.0 * 8.806194305419922
Epoch 1140, val loss: 1.0033966302871704
Epoch 1150, training loss: 89.10430145263672 = 0.9973563551902771 + 10.0 * 8.810694694519043
Epoch 1150, val loss: 1.0017368793487549
Epoch 1160, training loss: 89.11846160888672 = 0.995570182800293 + 10.0 * 8.812289237976074
Epoch 1160, val loss: 1.000028133392334
Epoch 1170, training loss: 88.93907165527344 = 0.9936859011650085 + 10.0 * 8.794538497924805
Epoch 1170, val loss: 0.9983005523681641
Epoch 1180, training loss: 88.95468139648438 = 0.9919277429580688 + 10.0 * 8.79627513885498
Epoch 1180, val loss: 0.9965487122535706
Epoch 1190, training loss: 89.00292205810547 = 0.990147590637207 + 10.0 * 8.801277160644531
Epoch 1190, val loss: 0.9949064254760742
Epoch 1200, training loss: 88.94583892822266 = 0.9882237315177917 + 10.0 * 8.795762062072754
Epoch 1200, val loss: 0.9931076765060425
Epoch 1210, training loss: 89.01932525634766 = 0.9865360260009766 + 10.0 * 8.803278923034668
Epoch 1210, val loss: 0.9914147257804871
Epoch 1220, training loss: 89.09391021728516 = 0.9846287965774536 + 10.0 * 8.810928344726562
Epoch 1220, val loss: 0.9896300435066223
Epoch 1230, training loss: 89.154541015625 = 0.9827263355255127 + 10.0 * 8.817181587219238
Epoch 1230, val loss: 0.9878133535385132
Epoch 1240, training loss: 89.17636108398438 = 0.9808030128479004 + 10.0 * 8.819555282592773
Epoch 1240, val loss: 0.9859940409660339
Epoch 1250, training loss: 89.2148666381836 = 0.9788137078285217 + 10.0 * 8.82360553741455
Epoch 1250, val loss: 0.9841194152832031
Epoch 1260, training loss: 89.24726867675781 = 0.9768339991569519 + 10.0 * 8.827043533325195
Epoch 1260, val loss: 0.9822291731834412
Epoch 1270, training loss: 89.27140045166016 = 0.9748438596725464 + 10.0 * 8.829655647277832
Epoch 1270, val loss: 0.980327844619751
Epoch 1280, training loss: 89.2607421875 = 0.9727960228919983 + 10.0 * 8.828794479370117
Epoch 1280, val loss: 0.9784058332443237
Epoch 1290, training loss: 89.29867553710938 = 0.9707725048065186 + 10.0 * 8.83279037475586
Epoch 1290, val loss: 0.9764849543571472
Epoch 1300, training loss: 89.35210418701172 = 0.9687265157699585 + 10.0 * 8.838337898254395
Epoch 1300, val loss: 0.9745407700538635
Epoch 1310, training loss: 89.35601806640625 = 0.9666164517402649 + 10.0 * 8.838940620422363
Epoch 1310, val loss: 0.9725201725959778
Epoch 1320, training loss: 89.38848876953125 = 0.9645386934280396 + 10.0 * 8.842394828796387
Epoch 1320, val loss: 0.9705603718757629
Epoch 1330, training loss: 89.37989044189453 = 0.9624255895614624 + 10.0 * 8.84174633026123
Epoch 1330, val loss: 0.968569815158844
Epoch 1340, training loss: 89.33438873291016 = 0.9602578282356262 + 10.0 * 8.83741283416748
Epoch 1340, val loss: 0.966511607170105
Epoch 1350, training loss: 89.33657836914062 = 0.9581325650215149 + 10.0 * 8.837844848632812
Epoch 1350, val loss: 0.9644826054573059
Epoch 1360, training loss: 89.38099670410156 = 0.956041157245636 + 10.0 * 8.842495918273926
Epoch 1360, val loss: 0.9625067710876465
Epoch 1370, training loss: 89.37380981445312 = 0.9539033770561218 + 10.0 * 8.84199047088623
Epoch 1370, val loss: 0.9604552388191223
Epoch 1380, training loss: 89.37445831298828 = 0.9516968727111816 + 10.0 * 8.842275619506836
Epoch 1380, val loss: 0.9584053158760071
Epoch 1390, training loss: 89.41852569580078 = 0.9495294094085693 + 10.0 * 8.846899032592773
Epoch 1390, val loss: 0.9563351273536682
Epoch 1400, training loss: 89.45417785644531 = 0.9473154544830322 + 10.0 * 8.850686073303223
Epoch 1400, val loss: 0.9542319178581238
Epoch 1410, training loss: 89.45721435546875 = 0.9450617432594299 + 10.0 * 8.851215362548828
Epoch 1410, val loss: 0.9521146416664124
Epoch 1420, training loss: 89.49090576171875 = 0.9428132176399231 + 10.0 * 8.854809761047363
Epoch 1420, val loss: 0.9499887824058533
Epoch 1430, training loss: 89.50040435791016 = 0.9405290484428406 + 10.0 * 8.855987548828125
Epoch 1430, val loss: 0.947843074798584
Epoch 1440, training loss: 89.52469635009766 = 0.938260555267334 + 10.0 * 8.858643531799316
Epoch 1440, val loss: 0.9457044005393982
Epoch 1450, training loss: 89.51283264160156 = 0.9359508156776428 + 10.0 * 8.857687950134277
Epoch 1450, val loss: 0.9434993267059326
Epoch 1460, training loss: 89.49243927001953 = 0.9336601495742798 + 10.0 * 8.855877876281738
Epoch 1460, val loss: 0.9413526058197021
Epoch 1470, training loss: 89.53253173828125 = 0.9313411116600037 + 10.0 * 8.860118865966797
Epoch 1470, val loss: 0.9391637444496155
Epoch 1480, training loss: 89.5569076538086 = 0.9290156364440918 + 10.0 * 8.862789154052734
Epoch 1480, val loss: 0.9369761943817139
Epoch 1490, training loss: 89.60994720458984 = 0.9266924262046814 + 10.0 * 8.868326187133789
Epoch 1490, val loss: 0.9347780346870422
Epoch 1500, training loss: 89.62303161621094 = 0.9243192076683044 + 10.0 * 8.869871139526367
Epoch 1500, val loss: 0.9325241446495056
Epoch 1510, training loss: 89.49883270263672 = 0.9218694567680359 + 10.0 * 8.857696533203125
Epoch 1510, val loss: 0.9302502870559692
Epoch 1520, training loss: 89.47618103027344 = 0.9194762110710144 + 10.0 * 8.855669975280762
Epoch 1520, val loss: 0.9279963374137878
Epoch 1530, training loss: 89.49539947509766 = 0.9171309471130371 + 10.0 * 8.857827186584473
Epoch 1530, val loss: 0.9257856607437134
Epoch 1540, training loss: 89.53499603271484 = 0.9148045778274536 + 10.0 * 8.862019538879395
Epoch 1540, val loss: 0.9235087633132935
Epoch 1550, training loss: 89.28050994873047 = 0.9122686982154846 + 10.0 * 8.836824417114258
Epoch 1550, val loss: 0.9212075471878052
Epoch 1560, training loss: 89.48841094970703 = 0.9100555777549744 + 10.0 * 8.85783576965332
Epoch 1560, val loss: 0.9190839529037476
Epoch 1570, training loss: 89.3560562133789 = 0.9078032970428467 + 10.0 * 8.84482479095459
Epoch 1570, val loss: 0.9169641137123108
Epoch 1580, training loss: 89.28751373291016 = 0.9053436517715454 + 10.0 * 8.838216781616211
Epoch 1580, val loss: 0.914613664150238
Epoch 1590, training loss: 89.43042755126953 = 0.9030264616012573 + 10.0 * 8.852740287780762
Epoch 1590, val loss: 0.9124221205711365
Epoch 1600, training loss: 89.44688415527344 = 0.9005917310714722 + 10.0 * 8.854629516601562
Epoch 1600, val loss: 0.9101583957672119
Epoch 1610, training loss: 89.49736022949219 = 0.8981902599334717 + 10.0 * 8.859916687011719
Epoch 1610, val loss: 0.9078915119171143
Epoch 1620, training loss: 89.55191802978516 = 0.8957409262657166 + 10.0 * 8.865617752075195
Epoch 1620, val loss: 0.9055760502815247
Epoch 1630, training loss: 89.57376098632812 = 0.8932690620422363 + 10.0 * 8.868048667907715
Epoch 1630, val loss: 0.9032530188560486
Epoch 1640, training loss: 89.62919616699219 = 0.8907940983772278 + 10.0 * 8.87384033203125
Epoch 1640, val loss: 0.9009279012680054
Epoch 1650, training loss: 89.58232879638672 = 0.8882673978805542 + 10.0 * 8.869405746459961
Epoch 1650, val loss: 0.898545503616333
Epoch 1660, training loss: 89.61324310302734 = 0.885811448097229 + 10.0 * 8.872743606567383
Epoch 1660, val loss: 0.896223247051239
Epoch 1670, training loss: 89.66458892822266 = 0.8832902908325195 + 10.0 * 8.878129959106445
Epoch 1670, val loss: 0.8938658833503723
Epoch 1680, training loss: 89.74798583984375 = 0.8808028697967529 + 10.0 * 8.886717796325684
Epoch 1680, val loss: 0.8915172219276428
Epoch 1690, training loss: 89.69819641113281 = 0.8782830834388733 + 10.0 * 8.881991386413574
Epoch 1690, val loss: 0.8891496062278748
Epoch 1700, training loss: 89.71759033203125 = 0.8757737278938293 + 10.0 * 8.88418197631836
Epoch 1700, val loss: 0.8868063688278198
Epoch 1710, training loss: 89.7675552368164 = 0.873272180557251 + 10.0 * 8.88942813873291
Epoch 1710, val loss: 0.8844433426856995
Epoch 1720, training loss: 89.753173828125 = 0.8707278966903687 + 10.0 * 8.88824462890625
Epoch 1720, val loss: 0.8820481896400452
Epoch 1730, training loss: 89.73798370361328 = 0.8681945204734802 + 10.0 * 8.886979103088379
Epoch 1730, val loss: 0.8797024488449097
Epoch 1740, training loss: 89.79654693603516 = 0.8656920194625854 + 10.0 * 8.893085479736328
Epoch 1740, val loss: 0.8773524165153503
Epoch 1750, training loss: 89.81659698486328 = 0.8631597757339478 + 10.0 * 8.895343780517578
Epoch 1750, val loss: 0.8749805092811584
Epoch 1760, training loss: 89.77415466308594 = 0.8606132864952087 + 10.0 * 8.89135456085205
Epoch 1760, val loss: 0.8726219534873962
Epoch 1770, training loss: 89.78660583496094 = 0.8582621812820435 + 10.0 * 8.892834663391113
Epoch 1770, val loss: 0.8704243898391724
Epoch 1780, training loss: 89.63201141357422 = 0.8555206656455994 + 10.0 * 8.877649307250977
Epoch 1780, val loss: 0.8678582310676575
Epoch 1790, training loss: 89.7007064819336 = 0.8532119393348694 + 10.0 * 8.884749412536621
Epoch 1790, val loss: 0.8657049536705017
Epoch 1800, training loss: 89.7677001953125 = 0.850685715675354 + 10.0 * 8.891701698303223
Epoch 1800, val loss: 0.8633291721343994
Epoch 1810, training loss: 89.83007049560547 = 0.8481833934783936 + 10.0 * 8.898188591003418
Epoch 1810, val loss: 0.8610059022903442
Epoch 1820, training loss: 89.8619384765625 = 0.8456577658653259 + 10.0 * 8.901628494262695
Epoch 1820, val loss: 0.8586506247520447
Epoch 1830, training loss: 89.81033325195312 = 0.8431841731071472 + 10.0 * 8.89671516418457
Epoch 1830, val loss: 0.8563388586044312
Epoch 1840, training loss: 89.84664916992188 = 0.8406907320022583 + 10.0 * 8.900595664978027
Epoch 1840, val loss: 0.8540421724319458
Epoch 1850, training loss: 89.9050064086914 = 0.8381660580635071 + 10.0 * 8.906683921813965
Epoch 1850, val loss: 0.8516868352890015
Epoch 1860, training loss: 89.93782806396484 = 0.8355758786201477 + 10.0 * 8.910224914550781
Epoch 1860, val loss: 0.8492643237113953
Epoch 1870, training loss: 89.92584991455078 = 0.832954466342926 + 10.0 * 8.909289360046387
Epoch 1870, val loss: 0.8468433022499084
Epoch 1880, training loss: 89.93832397460938 = 0.8303042054176331 + 10.0 * 8.910801887512207
Epoch 1880, val loss: 0.8443744778633118
Epoch 1890, training loss: 89.95175170898438 = 0.8275953531265259 + 10.0 * 8.912415504455566
Epoch 1890, val loss: 0.8418946862220764
Epoch 1900, training loss: 89.9437255859375 = 0.8249282240867615 + 10.0 * 8.911879539489746
Epoch 1900, val loss: 0.8394169807434082
Epoch 1910, training loss: 89.9879150390625 = 0.8222479224205017 + 10.0 * 8.916566848754883
Epoch 1910, val loss: 0.8369318842887878
Epoch 1920, training loss: 89.99114990234375 = 0.8195812702178955 + 10.0 * 8.917157173156738
Epoch 1920, val loss: 0.8344188332557678
Epoch 1930, training loss: 89.91397094726562 = 0.8169050216674805 + 10.0 * 8.909707069396973
Epoch 1930, val loss: 0.8319976925849915
Epoch 1940, training loss: 89.92792510986328 = 0.8142956495285034 + 10.0 * 8.91136360168457
Epoch 1940, val loss: 0.8295774459838867
Epoch 1950, training loss: 89.98165130615234 = 0.811580240726471 + 10.0 * 8.917007446289062
Epoch 1950, val loss: 0.8270649313926697
Epoch 1960, training loss: 90.01789855957031 = 0.8090320229530334 + 10.0 * 8.920886993408203
Epoch 1960, val loss: 0.8246971964836121
Epoch 1970, training loss: 90.02494812011719 = 0.8063551783561707 + 10.0 * 8.921858787536621
Epoch 1970, val loss: 0.8222435116767883
Epoch 1980, training loss: 89.93899536132812 = 0.8037759065628052 + 10.0 * 8.913521766662598
Epoch 1980, val loss: 0.819883406162262
Epoch 1990, training loss: 89.90258026123047 = 0.8012388348579407 + 10.0 * 8.910134315490723
Epoch 1990, val loss: 0.8174998760223389
Epoch 2000, training loss: 89.86395263671875 = 0.7986972332000732 + 10.0 * 8.906525611877441
Epoch 2000, val loss: 0.8151602149009705
Epoch 2010, training loss: 89.93286895751953 = 0.7961273789405823 + 10.0 * 8.913674354553223
Epoch 2010, val loss: 0.8128117918968201
Epoch 2020, training loss: 89.98680877685547 = 0.7935369610786438 + 10.0 * 8.919327735900879
Epoch 2020, val loss: 0.8104056715965271
Epoch 2030, training loss: 90.03224182128906 = 0.7909122109413147 + 10.0 * 8.92413330078125
Epoch 2030, val loss: 0.8080177307128906
Epoch 2040, training loss: 90.06087493896484 = 0.7883189916610718 + 10.0 * 8.927255630493164
Epoch 2040, val loss: 0.8056310415267944
Epoch 2050, training loss: 90.0523681640625 = 0.7857236862182617 + 10.0 * 8.926664352416992
Epoch 2050, val loss: 0.8032679557800293
Epoch 2060, training loss: 90.0887680053711 = 0.7831538319587708 + 10.0 * 8.930561065673828
Epoch 2060, val loss: 0.8009194731712341
Epoch 2070, training loss: 90.13590240478516 = 0.7805829048156738 + 10.0 * 8.935531616210938
Epoch 2070, val loss: 0.7985455989837646
Epoch 2080, training loss: 90.10564422607422 = 0.7779876589775085 + 10.0 * 8.93276596069336
Epoch 2080, val loss: 0.7962075471878052
Epoch 2090, training loss: 90.10459899902344 = 0.7754506468772888 + 10.0 * 8.932914733886719
Epoch 2090, val loss: 0.7939230799674988
Epoch 2100, training loss: 90.03343963623047 = 0.7729384899139404 + 10.0 * 8.926050186157227
Epoch 2100, val loss: 0.7916256189346313
Epoch 2110, training loss: 90.09231567382812 = 0.7704548239707947 + 10.0 * 8.932186126708984
Epoch 2110, val loss: 0.7893474102020264
Epoch 2120, training loss: 90.15409088134766 = 0.7679333686828613 + 10.0 * 8.938615798950195
Epoch 2120, val loss: 0.7870425581932068
Epoch 2130, training loss: 90.15423583984375 = 0.7653799057006836 + 10.0 * 8.938885688781738
Epoch 2130, val loss: 0.7847558856010437
Epoch 2140, training loss: 90.1531982421875 = 0.7628793716430664 + 10.0 * 8.939031600952148
Epoch 2140, val loss: 0.7824831604957581
Epoch 2150, training loss: 90.18729400634766 = 0.7604153752326965 + 10.0 * 8.94268798828125
Epoch 2150, val loss: 0.7802034020423889
Epoch 2160, training loss: 90.1854476928711 = 0.75791335105896 + 10.0 * 8.942753791809082
Epoch 2160, val loss: 0.7779538035392761
Epoch 2170, training loss: 90.22207641601562 = 0.7554289698600769 + 10.0 * 8.946664810180664
Epoch 2170, val loss: 0.7756966948509216
Epoch 2180, training loss: 90.24663543701172 = 0.7529183626174927 + 10.0 * 8.949371337890625
Epoch 2180, val loss: 0.7734259963035583
Epoch 2190, training loss: 90.22010040283203 = 0.750465989112854 + 10.0 * 8.9469633102417
Epoch 2190, val loss: 0.7712055444717407
Epoch 2200, training loss: 90.21517181396484 = 0.7480191588401794 + 10.0 * 8.946715354919434
Epoch 2200, val loss: 0.7689793109893799
Epoch 2210, training loss: 90.17957305908203 = 0.7455267310142517 + 10.0 * 8.943404197692871
Epoch 2210, val loss: 0.7667679786682129
Epoch 2220, training loss: 90.26058959960938 = 0.7431915998458862 + 10.0 * 8.951739311218262
Epoch 2220, val loss: 0.764708399772644
Epoch 2230, training loss: 90.283447265625 = 0.7408877015113831 + 10.0 * 8.954256057739258
Epoch 2230, val loss: 0.762641429901123
Epoch 2240, training loss: 90.3044662475586 = 0.7385343909263611 + 10.0 * 8.95659351348877
Epoch 2240, val loss: 0.7605410814285278
Epoch 2250, training loss: 90.37409210205078 = 0.7361522316932678 + 10.0 * 8.963793754577637
Epoch 2250, val loss: 0.7584184408187866
Epoch 2260, training loss: 90.4038314819336 = 0.7337535619735718 + 10.0 * 8.967007637023926
Epoch 2260, val loss: 0.7562682032585144
Epoch 2270, training loss: 90.34369659423828 = 0.7313156723976135 + 10.0 * 8.961237907409668
Epoch 2270, val loss: 0.7541322112083435
Epoch 2280, training loss: 90.36259460449219 = 0.7290416359901428 + 10.0 * 8.96335506439209
Epoch 2280, val loss: 0.7520645260810852
Epoch 2290, training loss: 90.4185562133789 = 0.7267019152641296 + 10.0 * 8.969185829162598
Epoch 2290, val loss: 0.7500003576278687
Epoch 2300, training loss: 90.46324920654297 = 0.7243409156799316 + 10.0 * 8.97389030456543
Epoch 2300, val loss: 0.7478956580162048
Epoch 2310, training loss: 90.44384765625 = 0.7220069766044617 + 10.0 * 8.972184181213379
Epoch 2310, val loss: 0.7458229064941406
Epoch 2320, training loss: 90.45372772216797 = 0.7196534276008606 + 10.0 * 8.973407745361328
Epoch 2320, val loss: 0.7437435984611511
Epoch 2330, training loss: 90.44371795654297 = 0.7174294590950012 + 10.0 * 8.972628593444824
Epoch 2330, val loss: 0.7418076992034912
Epoch 2340, training loss: 90.11421966552734 = 0.7154622673988342 + 10.0 * 8.939875602722168
Epoch 2340, val loss: 0.7401160597801208
Epoch 2350, training loss: 90.00021362304688 = 0.7133650183677673 + 10.0 * 8.928685188293457
Epoch 2350, val loss: 0.7382264733314514
Epoch 2360, training loss: 89.8171157836914 = 0.711401104927063 + 10.0 * 8.910571098327637
Epoch 2360, val loss: 0.736564576625824
Epoch 2370, training loss: 89.95042419433594 = 0.7093269228935242 + 10.0 * 8.92410945892334
Epoch 2370, val loss: 0.7347252368927002
Epoch 2380, training loss: 90.08226013183594 = 0.707339882850647 + 10.0 * 8.937492370605469
Epoch 2380, val loss: 0.7329970598220825
Epoch 2390, training loss: 90.1333236694336 = 0.7052251696586609 + 10.0 * 8.94281005859375
Epoch 2390, val loss: 0.7311506271362305
Epoch 2400, training loss: 90.1889419555664 = 0.7030760645866394 + 10.0 * 8.948586463928223
Epoch 2400, val loss: 0.7292620539665222
Epoch 2410, training loss: 90.24565124511719 = 0.700913667678833 + 10.0 * 8.954473495483398
Epoch 2410, val loss: 0.72737056016922
Epoch 2420, training loss: 90.28072357177734 = 0.6987290978431702 + 10.0 * 8.958199501037598
Epoch 2420, val loss: 0.7254682779312134
Epoch 2430, training loss: 90.28736877441406 = 0.6965724229812622 + 10.0 * 8.95907974243164
Epoch 2430, val loss: 0.7235694527626038
Epoch 2440, training loss: 90.27490234375 = 0.6944148540496826 + 10.0 * 8.958048820495605
Epoch 2440, val loss: 0.7217261791229248
Epoch 2450, training loss: 90.32479858398438 = 0.6923129558563232 + 10.0 * 8.963248252868652
Epoch 2450, val loss: 0.7198764085769653
Epoch 2460, training loss: 90.3390884399414 = 0.6901999711990356 + 10.0 * 8.964888572692871
Epoch 2460, val loss: 0.7180543541908264
Epoch 2470, training loss: 90.35334777832031 = 0.6881091594696045 + 10.0 * 8.966524124145508
Epoch 2470, val loss: 0.7162341475486755
Epoch 2480, training loss: 90.33922576904297 = 0.6860188841819763 + 10.0 * 8.965320587158203
Epoch 2480, val loss: 0.7144356369972229
Epoch 2490, training loss: 90.344970703125 = 0.6839728951454163 + 10.0 * 8.966099739074707
Epoch 2490, val loss: 0.7126681804656982
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7665217391304348
0.8642324132434979
The final CL Acc:0.75594, 0.04086, The final GNN Acc:0.86387, 0.00037
Begin epxeriment: noisy_level: 0.2 cont_weight: 10 epoch: der1:0.3 der2:0.4 dfr1:0.4 dfr2:0.5
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=2500, cl_num_layers=2, cl_num_proj_hidden=64, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=10.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.4, drop_feat_rate_1=0.4, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=32, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, noisy_level=0.2, num_hidden=64, num_proj_hidden=64, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=40, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 88648])
add edge: torch.Size([2, 106322])
remove edge: torch.Size([2, 70924])
updated graph: torch.Size([2, 88598])
=== Noisy graph ===
/home/project-robust-contrastive/Robust-Contrastive-Learning/model.py:227: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 104.70270538330078 = 1.0978769063949585 + 10.0 * 10.360483169555664
Epoch 0, val loss: 1.0979524850845337
Epoch 10, training loss: 101.04403686523438 = 1.0977561473846436 + 10.0 * 9.994627952575684
Epoch 10, val loss: 1.0978507995605469
Epoch 20, training loss: 99.16922760009766 = 1.0976285934448242 + 10.0 * 9.807160377502441
Epoch 20, val loss: 1.097733497619629
Epoch 30, training loss: 97.82337188720703 = 1.097460150718689 + 10.0 * 9.672591209411621
Epoch 30, val loss: 1.0975866317749023
Epoch 40, training loss: 96.78105926513672 = 1.0972787141799927 + 10.0 * 9.568377494812012
Epoch 40, val loss: 1.0974225997924805
Epoch 50, training loss: 95.91808319091797 = 1.0970873832702637 + 10.0 * 9.482099533081055
Epoch 50, val loss: 1.0972543954849243
Epoch 60, training loss: 95.17687225341797 = 1.0968968868255615 + 10.0 * 9.407998085021973
Epoch 60, val loss: 1.0970873832702637
Epoch 70, training loss: 94.53133392333984 = 1.0967060327529907 + 10.0 * 9.343462944030762
Epoch 70, val loss: 1.0969198942184448
Epoch 80, training loss: 93.96783447265625 = 1.0965135097503662 + 10.0 * 9.287132263183594
Epoch 80, val loss: 1.09674870967865
Epoch 90, training loss: 93.47427368164062 = 1.0963104963302612 + 10.0 * 9.23779582977295
Epoch 90, val loss: 1.0965722799301147
Epoch 100, training loss: 93.05145263671875 = 1.0961041450500488 + 10.0 * 9.195534706115723
Epoch 100, val loss: 1.096386194229126
Epoch 110, training loss: 92.67940521240234 = 1.0958828926086426 + 10.0 * 9.15835189819336
Epoch 110, val loss: 1.0961873531341553
Epoch 120, training loss: 92.34046936035156 = 1.0956470966339111 + 10.0 * 9.124482154846191
Epoch 120, val loss: 1.0959768295288086
Epoch 130, training loss: 92.05989837646484 = 1.0954071283340454 + 10.0 * 9.09644889831543
Epoch 130, val loss: 1.0957664251327515
Epoch 140, training loss: 91.79229736328125 = 1.095160722732544 + 10.0 * 9.069713592529297
Epoch 140, val loss: 1.0955411195755005
Epoch 150, training loss: 91.57254791259766 = 1.0949006080627441 + 10.0 * 9.047764778137207
Epoch 150, val loss: 1.0953048467636108
Epoch 160, training loss: 91.38008117675781 = 1.0946251153945923 + 10.0 * 9.028545379638672
Epoch 160, val loss: 1.0950565338134766
Epoch 170, training loss: 91.19318389892578 = 1.0943430662155151 + 10.0 * 9.009883880615234
Epoch 170, val loss: 1.094801425933838
Epoch 180, training loss: 91.02925872802734 = 1.0940500497817993 + 10.0 * 8.993520736694336
Epoch 180, val loss: 1.0945401191711426
Epoch 190, training loss: 90.89086151123047 = 1.0937401056289673 + 10.0 * 8.979711532592773
Epoch 190, val loss: 1.094258427619934
Epoch 200, training loss: 90.79309844970703 = 1.0934327840805054 + 10.0 * 8.969966888427734
Epoch 200, val loss: 1.093971848487854
Epoch 210, training loss: 90.69165802001953 = 1.0931068658828735 + 10.0 * 8.959855079650879
Epoch 210, val loss: 1.093674898147583
Epoch 220, training loss: 90.5799789428711 = 1.0927540063858032 + 10.0 * 8.948722839355469
Epoch 220, val loss: 1.0933533906936646
Epoch 230, training loss: 90.48750305175781 = 1.092393398284912 + 10.0 * 8.9395112991333
Epoch 230, val loss: 1.0930290222167969
Epoch 240, training loss: 90.41842651367188 = 1.0920175313949585 + 10.0 * 8.93264102935791
Epoch 240, val loss: 1.092682123184204
Epoch 250, training loss: 90.44635772705078 = 1.091638445854187 + 10.0 * 8.93547248840332
Epoch 250, val loss: 1.0923389196395874
Epoch 260, training loss: 90.40541076660156 = 1.0912437438964844 + 10.0 * 8.931416511535645
Epoch 260, val loss: 1.0919758081436157
Epoch 270, training loss: 90.24388885498047 = 1.0908098220825195 + 10.0 * 8.915307998657227
Epoch 270, val loss: 1.0915870666503906
Epoch 280, training loss: 90.20587921142578 = 1.0904008150100708 + 10.0 * 8.911547660827637
Epoch 280, val loss: 1.0912128686904907
Epoch 290, training loss: 90.20703125 = 1.0899773836135864 + 10.0 * 8.911705017089844
Epoch 290, val loss: 1.0908279418945312
Epoch 300, training loss: 90.1773681640625 = 1.0895406007766724 + 10.0 * 8.908782958984375
Epoch 300, val loss: 1.0904340744018555
Epoch 310, training loss: 90.20256042480469 = 1.0890997648239136 + 10.0 * 8.911346435546875
Epoch 310, val loss: 1.0900304317474365
Epoch 320, training loss: 90.26187133789062 = 1.0886504650115967 + 10.0 * 8.917322158813477
Epoch 320, val loss: 1.089626431465149
Epoch 330, training loss: 90.2204360961914 = 1.088163137435913 + 10.0 * 8.913227081298828
Epoch 330, val loss: 1.089186668395996
Epoch 340, training loss: 90.04056549072266 = 1.0876387357711792 + 10.0 * 8.895292282104492
Epoch 340, val loss: 1.0886914730072021
Epoch 350, training loss: 90.1361312866211 = 1.0871840715408325 + 10.0 * 8.904894828796387
Epoch 350, val loss: 1.0882854461669922
Epoch 360, training loss: 90.10649871826172 = 1.0866740942001343 + 10.0 * 8.901982307434082
Epoch 360, val loss: 1.087819218635559
Epoch 370, training loss: 90.08564758300781 = 1.0861527919769287 + 10.0 * 8.89995002746582
Epoch 370, val loss: 1.0873401165008545
Epoch 380, training loss: 90.09561920166016 = 1.085637092590332 + 10.0 * 8.90099811553955
Epoch 380, val loss: 1.0868686437606812
Epoch 390, training loss: 90.0910873413086 = 1.085103988647461 + 10.0 * 8.900598526000977
Epoch 390, val loss: 1.0863887071609497
Epoch 400, training loss: 90.11660766601562 = 1.08456552028656 + 10.0 * 8.903203964233398
Epoch 400, val loss: 1.085890769958496
Epoch 410, training loss: 90.08860778808594 = 1.0840140581130981 + 10.0 * 8.900459289550781
Epoch 410, val loss: 1.0853846073150635
Epoch 420, training loss: 90.14735412597656 = 1.083472728729248 + 10.0 * 8.906388282775879
Epoch 420, val loss: 1.0848826169967651
Epoch 430, training loss: 90.11672973632812 = 1.0828830003738403 + 10.0 * 8.9033842086792
Epoch 430, val loss: 1.0843526124954224
Epoch 440, training loss: 90.11058044433594 = 1.0822978019714355 + 10.0 * 8.902828216552734
Epoch 440, val loss: 1.0838217735290527
Epoch 450, training loss: 90.12560272216797 = 1.0817067623138428 + 10.0 * 8.904390335083008
Epoch 450, val loss: 1.0832669734954834
Epoch 460, training loss: 90.14849090576172 = 1.0811357498168945 + 10.0 * 8.90673542022705
Epoch 460, val loss: 1.0827564001083374
Epoch 470, training loss: 90.13441467285156 = 1.0805227756500244 + 10.0 * 8.905389785766602
Epoch 470, val loss: 1.0821841955184937
Epoch 480, training loss: 90.15596771240234 = 1.0798839330673218 + 10.0 * 8.907608032226562
Epoch 480, val loss: 1.0816090106964111
Epoch 490, training loss: 90.16226959228516 = 1.079250454902649 + 10.0 * 8.90830135345459
Epoch 490, val loss: 1.0810171365737915
Epoch 500, training loss: 90.12332916259766 = 1.0786175727844238 + 10.0 * 8.904470443725586
Epoch 500, val loss: 1.0804654359817505
Epoch 510, training loss: 90.08897399902344 = 1.07793390750885 + 10.0 * 8.901103973388672
Epoch 510, val loss: 1.0797783136367798
Epoch 520, training loss: 90.28022003173828 = 1.0772759914398193 + 10.0 * 8.920293807983398
Epoch 520, val loss: 1.0792075395584106
Epoch 530, training loss: 90.25183868408203 = 1.0765620470046997 + 10.0 * 8.91752815246582
Epoch 530, val loss: 1.0785553455352783
Epoch 540, training loss: 90.22734832763672 = 1.0758733749389648 + 10.0 * 8.91514778137207
Epoch 540, val loss: 1.0779132843017578
Epoch 550, training loss: 90.19912719726562 = 1.0751488208770752 + 10.0 * 8.912397384643555
Epoch 550, val loss: 1.077237606048584
Epoch 560, training loss: 90.27886962890625 = 1.0743961334228516 + 10.0 * 8.92044734954834
Epoch 560, val loss: 1.076545238494873
Epoch 570, training loss: 90.35356903076172 = 1.0737112760543823 + 10.0 * 8.927986145019531
Epoch 570, val loss: 1.0759135484695435
Epoch 580, training loss: 90.3851547241211 = 1.0729460716247559 + 10.0 * 8.931221008300781
Epoch 580, val loss: 1.0751988887786865
Epoch 590, training loss: 90.4441146850586 = 1.0721713304519653 + 10.0 * 8.937193870544434
Epoch 590, val loss: 1.0744789838790894
Epoch 600, training loss: 90.47237396240234 = 1.0713565349578857 + 10.0 * 8.940101623535156
Epoch 600, val loss: 1.0737224817276
Epoch 610, training loss: 90.48585510253906 = 1.0705304145812988 + 10.0 * 8.941532135009766
Epoch 610, val loss: 1.0729600191116333
Epoch 620, training loss: 90.52515411376953 = 1.0696766376495361 + 10.0 * 8.945547103881836
Epoch 620, val loss: 1.0721614360809326
Epoch 630, training loss: 90.55728912353516 = 1.0687772035598755 + 10.0 * 8.948850631713867
Epoch 630, val loss: 1.0713307857513428
Epoch 640, training loss: 90.56321716308594 = 1.0678412914276123 + 10.0 * 8.94953727722168
Epoch 640, val loss: 1.0704560279846191
Epoch 650, training loss: 90.63016510009766 = 1.0669387578964233 + 10.0 * 8.95632266998291
Epoch 650, val loss: 1.0696167945861816
Epoch 660, training loss: 90.45530700683594 = 1.0659347772598267 + 10.0 * 8.938937187194824
Epoch 660, val loss: 1.068673014640808
Epoch 670, training loss: 90.5714340209961 = 1.0648995637893677 + 10.0 * 8.950653076171875
Epoch 670, val loss: 1.0677096843719482
Epoch 680, training loss: 90.65176391601562 = 1.0638445615768433 + 10.0 * 8.958791732788086
Epoch 680, val loss: 1.066725492477417
Epoch 690, training loss: 90.65879821777344 = 1.0627286434173584 + 10.0 * 8.959607124328613
Epoch 690, val loss: 1.0657031536102295
Epoch 700, training loss: 90.6994400024414 = 1.0615977048873901 + 10.0 * 8.963784217834473
Epoch 700, val loss: 1.0646635293960571
Epoch 710, training loss: 90.73101043701172 = 1.0604643821716309 + 10.0 * 8.96705436706543
Epoch 710, val loss: 1.0636107921600342
Epoch 720, training loss: 90.62467193603516 = 1.0592402219772339 + 10.0 * 8.95654296875
Epoch 720, val loss: 1.062477469444275
Epoch 730, training loss: 90.71369171142578 = 1.0580554008483887 + 10.0 * 8.965563774108887
Epoch 730, val loss: 1.0613595247268677
Epoch 740, training loss: 90.78366088867188 = 1.056937575340271 + 10.0 * 8.972672462463379
Epoch 740, val loss: 1.0603235960006714
Epoch 750, training loss: 90.72035217285156 = 1.0556282997131348 + 10.0 * 8.966472625732422
Epoch 750, val loss: 1.0591264963150024
Epoch 760, training loss: 90.68719482421875 = 1.0543653964996338 + 10.0 * 8.963282585144043
Epoch 760, val loss: 1.0579495429992676
Epoch 770, training loss: 90.74855041503906 = 1.053114652633667 + 10.0 * 8.96954345703125
Epoch 770, val loss: 1.056780457496643
Epoch 780, training loss: 90.77236938476562 = 1.0518161058425903 + 10.0 * 8.972055435180664
Epoch 780, val loss: 1.055576205253601
Epoch 790, training loss: 90.8113784790039 = 1.050484299659729 + 10.0 * 8.976089477539062
Epoch 790, val loss: 1.0543386936187744
Epoch 800, training loss: 90.81916809082031 = 1.0491244792938232 + 10.0 * 8.977004051208496
Epoch 800, val loss: 1.053067684173584
Epoch 810, training loss: 90.85189819335938 = 1.0477169752120972 + 10.0 * 8.98041820526123
Epoch 810, val loss: 1.051771640777588
Epoch 820, training loss: 90.88272094726562 = 1.0463435649871826 + 10.0 * 8.983637809753418
Epoch 820, val loss: 1.0504926443099976
Epoch 830, training loss: 90.91547393798828 = 1.0449286699295044 + 10.0 * 8.987054824829102
Epoch 830, val loss: 1.0491777658462524
Epoch 840, training loss: 90.92543029785156 = 1.0434889793395996 + 10.0 * 8.988194465637207
Epoch 840, val loss: 1.0478317737579346
Epoch 850, training loss: 90.92937469482422 = 1.0420167446136475 + 10.0 * 8.988736152648926
Epoch 850, val loss: 1.0464749336242676
Epoch 860, training loss: 90.95195007324219 = 1.0405174493789673 + 10.0 * 8.991143226623535
Epoch 860, val loss: 1.0450735092163086
Epoch 870, training loss: 91.15653228759766 = 1.0390268564224243 + 10.0 * 9.011751174926758
Epoch 870, val loss: 1.0436969995498657
Epoch 880, training loss: 91.1579818725586 = 1.0375558137893677 + 10.0 * 9.012042045593262
Epoch 880, val loss: 1.0423035621643066
Epoch 890, training loss: 90.92665100097656 = 1.0358362197875977 + 10.0 * 8.989081382751465
Epoch 890, val loss: 1.0407443046569824
Epoch 900, training loss: 90.87225341796875 = 1.0343080759048462 + 10.0 * 8.983794212341309
Epoch 900, val loss: 1.0392950773239136
Epoch 910, training loss: 90.93568420410156 = 1.0327153205871582 + 10.0 * 8.990297317504883
Epoch 910, val loss: 1.0378507375717163
Epoch 920, training loss: 90.99223327636719 = 1.031083583831787 + 10.0 * 8.996114730834961
Epoch 920, val loss: 1.0363432168960571
Epoch 930, training loss: 91.05365753173828 = 1.029477834701538 + 10.0 * 9.00241756439209
Epoch 930, val loss: 1.0348447561264038
Epoch 940, training loss: 91.10419464111328 = 1.027840495109558 + 10.0 * 9.007635116577148
Epoch 940, val loss: 1.0333324670791626
Epoch 950, training loss: 91.11995697021484 = 1.0261633396148682 + 10.0 * 9.009379386901855
Epoch 950, val loss: 1.03177011013031
Epoch 960, training loss: 91.15687561035156 = 1.0245107412338257 + 10.0 * 9.013236045837402
Epoch 960, val loss: 1.030242681503296
Epoch 970, training loss: 91.17268371582031 = 1.022778868675232 + 10.0 * 9.014989852905273
Epoch 970, val loss: 1.028641939163208
Epoch 980, training loss: 91.19284057617188 = 1.0210734605789185 + 10.0 * 9.017176628112793
Epoch 980, val loss: 1.0270657539367676
Epoch 990, training loss: 91.17724609375 = 1.0193111896514893 + 10.0 * 9.015793800354004
Epoch 990, val loss: 1.0254442691802979
Epoch 1000, training loss: 91.16813659667969 = 1.017571210861206 + 10.0 * 9.015056610107422
Epoch 1000, val loss: 1.0238239765167236
Epoch 1010, training loss: 91.22331237792969 = 1.0158295631408691 + 10.0 * 9.020748138427734
Epoch 1010, val loss: 1.0222179889678955
Epoch 1020, training loss: 91.29118347167969 = 1.0140694379806519 + 10.0 * 9.027711868286133
Epoch 1020, val loss: 1.0205968618392944
Epoch 1030, training loss: 91.26841735839844 = 1.012305736541748 + 10.0 * 9.02561092376709
Epoch 1030, val loss: 1.018950343132019
Epoch 1040, training loss: 91.33370971679688 = 1.0105669498443604 + 10.0 * 9.03231430053711
Epoch 1040, val loss: 1.0173618793487549
Epoch 1050, training loss: 91.221923828125 = 1.0079556703567505 + 10.0 * 9.02139663696289
Epoch 1050, val loss: 1.014732003211975
Epoch 1060, training loss: 91.28555297851562 = 1.004550576210022 + 10.0 * 9.028100967407227
Epoch 1060, val loss: 1.0114411115646362
Epoch 1070, training loss: 91.31829071044922 = 1.0011838674545288 + 10.0 * 9.031710624694824
Epoch 1070, val loss: 1.0082279443740845
Epoch 1080, training loss: 91.39672088623047 = 0.9979398846626282 + 10.0 * 9.039877891540527
Epoch 1080, val loss: 1.0051230192184448
Epoch 1090, training loss: 91.43030548095703 = 0.9948241114616394 + 10.0 * 9.043547630310059
Epoch 1090, val loss: 1.0021226406097412
Epoch 1100, training loss: 91.31816864013672 = 0.991848349571228 + 10.0 * 9.032631874084473
Epoch 1100, val loss: 0.9993191361427307
Epoch 1110, training loss: 91.20479583740234 = 0.9888750910758972 + 10.0 * 9.021592140197754
Epoch 1110, val loss: 0.9964722394943237
Epoch 1120, training loss: 91.4225845336914 = 0.9861173629760742 + 10.0 * 9.043646812438965
Epoch 1120, val loss: 0.9938793778419495
Epoch 1130, training loss: 91.38935089111328 = 0.9832479357719421 + 10.0 * 9.040610313415527
Epoch 1130, val loss: 0.9911691546440125
Epoch 1140, training loss: 91.47628784179688 = 0.9804439544677734 + 10.0 * 9.04958438873291
Epoch 1140, val loss: 0.9885036945343018
Epoch 1150, training loss: 91.51666259765625 = 0.9776486754417419 + 10.0 * 9.053901672363281
Epoch 1150, val loss: 0.9858629107475281
Epoch 1160, training loss: 91.43378448486328 = 0.974845290184021 + 10.0 * 9.045893669128418
Epoch 1160, val loss: 0.983203649520874
Epoch 1170, training loss: 91.46427917480469 = 0.9721031188964844 + 10.0 * 9.049217224121094
Epoch 1170, val loss: 0.9806147217750549
Epoch 1180, training loss: 91.5350112915039 = 0.9693735837936401 + 10.0 * 9.056563377380371
Epoch 1180, val loss: 0.9780532717704773
Epoch 1190, training loss: 91.61288452148438 = 0.9666885137557983 + 10.0 * 9.064619064331055
Epoch 1190, val loss: 0.9755246639251709
Epoch 1200, training loss: 91.57740783691406 = 0.9639942646026611 + 10.0 * 9.061341285705566
Epoch 1200, val loss: 0.9729956388473511
Epoch 1210, training loss: 91.57794952392578 = 0.9613267779350281 + 10.0 * 9.061662673950195
Epoch 1210, val loss: 0.9704900979995728
Epoch 1220, training loss: 91.66036224365234 = 0.9586902260780334 + 10.0 * 9.070167541503906
Epoch 1220, val loss: 0.9680145978927612
Epoch 1230, training loss: 91.73595428466797 = 0.9560181498527527 + 10.0 * 9.077993392944336
Epoch 1230, val loss: 0.965503454208374
Epoch 1240, training loss: 91.6479263305664 = 0.953536331653595 + 10.0 * 9.069438934326172
Epoch 1240, val loss: 0.9631896018981934
Epoch 1250, training loss: 91.59907531738281 = 0.9509744048118591 + 10.0 * 9.064809799194336
Epoch 1250, val loss: 0.960811197757721
Epoch 1260, training loss: 91.65243530273438 = 0.9484289288520813 + 10.0 * 9.070401191711426
Epoch 1260, val loss: 0.9584425091743469
Epoch 1270, training loss: 91.8010025024414 = 0.9458959698677063 + 10.0 * 9.085511207580566
Epoch 1270, val loss: 0.9560766816139221
Epoch 1280, training loss: 91.82071685791016 = 0.9433705806732178 + 10.0 * 9.087735176086426
Epoch 1280, val loss: 0.9537186622619629
Epoch 1290, training loss: 91.87397766113281 = 0.9408543109893799 + 10.0 * 9.09331226348877
Epoch 1290, val loss: 0.9513731002807617
Epoch 1300, training loss: 91.89967346191406 = 0.9383580684661865 + 10.0 * 9.096132278442383
Epoch 1300, val loss: 0.9490407705307007
Epoch 1310, training loss: 91.91605377197266 = 0.9358561038970947 + 10.0 * 9.09801959991455
Epoch 1310, val loss: 0.9467208385467529
Epoch 1320, training loss: 91.88187408447266 = 0.9333744049072266 + 10.0 * 9.094850540161133
Epoch 1320, val loss: 0.9443926811218262
Epoch 1330, training loss: 91.93048095703125 = 0.9309176802635193 + 10.0 * 9.099956512451172
Epoch 1330, val loss: 0.9421321153640747
Epoch 1340, training loss: 91.99776458740234 = 0.9284957647323608 + 10.0 * 9.106926918029785
Epoch 1340, val loss: 0.9398801922798157
Epoch 1350, training loss: 91.96293640136719 = 0.9260330200195312 + 10.0 * 9.103690147399902
Epoch 1350, val loss: 0.9376046657562256
Epoch 1360, training loss: 91.94930267333984 = 0.9236433506011963 + 10.0 * 9.10256576538086
Epoch 1360, val loss: 0.9353799819946289
Epoch 1370, training loss: 92.04817962646484 = 0.9213475584983826 + 10.0 * 9.112683296203613
Epoch 1370, val loss: 0.9332363605499268
Epoch 1380, training loss: 92.01850891113281 = 0.9190412759780884 + 10.0 * 9.109946250915527
Epoch 1380, val loss: 0.9310874938964844
Epoch 1390, training loss: 92.04981231689453 = 0.9167374968528748 + 10.0 * 9.113306999206543
Epoch 1390, val loss: 0.9289880394935608
Epoch 1400, training loss: 92.10442352294922 = 0.9144271612167358 + 10.0 * 9.118999481201172
Epoch 1400, val loss: 0.9268696904182434
Epoch 1410, training loss: 92.19644927978516 = 0.9121183753013611 + 10.0 * 9.128433227539062
Epoch 1410, val loss: 0.9247521758079529
Epoch 1420, training loss: 92.15713500976562 = 0.9097954630851746 + 10.0 * 9.124733924865723
Epoch 1420, val loss: 0.9226178526878357
Epoch 1430, training loss: 92.17411804199219 = 0.9074777364730835 + 10.0 * 9.126664161682129
Epoch 1430, val loss: 0.9205077886581421
Epoch 1440, training loss: 91.95084381103516 = 0.9052078127861023 + 10.0 * 9.10456371307373
Epoch 1440, val loss: 0.9183900952339172
Epoch 1450, training loss: 92.09514617919922 = 0.9029694199562073 + 10.0 * 9.119217872619629
Epoch 1450, val loss: 0.9163517951965332
Epoch 1460, training loss: 92.22062683105469 = 0.9007383584976196 + 10.0 * 9.131988525390625
Epoch 1460, val loss: 0.914304792881012
Epoch 1470, training loss: 92.30401611328125 = 0.8985005617141724 + 10.0 * 9.140551567077637
Epoch 1470, val loss: 0.9122676253318787
Epoch 1480, training loss: 92.29112243652344 = 0.8962546586990356 + 10.0 * 9.139486312866211
Epoch 1480, val loss: 0.9102086424827576
Epoch 1490, training loss: 92.3080825805664 = 0.8940432667732239 + 10.0 * 9.141404151916504
Epoch 1490, val loss: 0.9081809520721436
Epoch 1500, training loss: 92.34989929199219 = 0.8918282985687256 + 10.0 * 9.145807266235352
Epoch 1500, val loss: 0.9061607718467712
Epoch 1510, training loss: 92.39007568359375 = 0.8896169066429138 + 10.0 * 9.150045394897461
Epoch 1510, val loss: 0.9041446447372437
Epoch 1520, training loss: 92.35192108154297 = 0.8874151110649109 + 10.0 * 9.146450996398926
Epoch 1520, val loss: 0.9021474123001099
Epoch 1530, training loss: 92.38861083984375 = 0.8852424025535583 + 10.0 * 9.150337219238281
Epoch 1530, val loss: 0.9001587629318237
Epoch 1540, training loss: 92.46802520751953 = 0.8830884099006653 + 10.0 * 9.158493995666504
Epoch 1540, val loss: 0.898181140422821
Epoch 1550, training loss: 92.47309875488281 = 0.8808722496032715 + 10.0 * 9.159222602844238
Epoch 1550, val loss: 0.8961781859397888
Epoch 1560, training loss: 92.456298828125 = 0.8786925673484802 + 10.0 * 9.157760620117188
Epoch 1560, val loss: 0.8942027688026428
Epoch 1570, training loss: 92.47763061523438 = 0.8765165209770203 + 10.0 * 9.160111427307129
Epoch 1570, val loss: 0.8922244906425476
Epoch 1580, training loss: 92.48367309570312 = 0.8743346929550171 + 10.0 * 9.160933494567871
Epoch 1580, val loss: 0.8902658224105835
Epoch 1590, training loss: 92.48451232910156 = 0.8721902966499329 + 10.0 * 9.161231994628906
Epoch 1590, val loss: 0.8883234858512878
Epoch 1600, training loss: 92.31432342529297 = 0.8700295090675354 + 10.0 * 9.144429206848145
Epoch 1600, val loss: 0.8863912224769592
Epoch 1610, training loss: 92.34600830078125 = 0.8679564595222473 + 10.0 * 9.147805213928223
Epoch 1610, val loss: 0.8845210075378418
Epoch 1620, training loss: 92.38489532470703 = 0.8658193349838257 + 10.0 * 9.151906967163086
Epoch 1620, val loss: 0.8825870156288147
Epoch 1630, training loss: 92.44429779052734 = 0.8636778593063354 + 10.0 * 9.158061981201172
Epoch 1630, val loss: 0.8806715607643127
Epoch 1640, training loss: 92.50337219238281 = 0.8614907264709473 + 10.0 * 9.164188385009766
Epoch 1640, val loss: 0.8787024617195129
Epoch 1650, training loss: 92.55474853515625 = 0.8591805100440979 + 10.0 * 9.169556617736816
Epoch 1650, val loss: 0.8765919208526611
Epoch 1660, training loss: 92.4298324584961 = 0.8567988872528076 + 10.0 * 9.157303810119629
Epoch 1660, val loss: 0.8744850754737854
Epoch 1670, training loss: 92.48426055908203 = 0.8543865084648132 + 10.0 * 9.16298770904541
Epoch 1670, val loss: 0.8723093867301941
Epoch 1680, training loss: 92.48139953613281 = 0.8519580364227295 + 10.0 * 9.162943840026855
Epoch 1680, val loss: 0.8700895309448242
Epoch 1690, training loss: 92.54415130615234 = 0.8495491743087769 + 10.0 * 9.16946029663086
Epoch 1690, val loss: 0.8679053783416748
Epoch 1700, training loss: 92.61835479736328 = 0.8471307158470154 + 10.0 * 9.177122116088867
Epoch 1700, val loss: 0.8657335638999939
Epoch 1710, training loss: 92.59734344482422 = 0.8446832895278931 + 10.0 * 9.17526626586914
Epoch 1710, val loss: 0.8635537624359131
Epoch 1720, training loss: 92.60753631591797 = 0.8422567248344421 + 10.0 * 9.176527976989746
Epoch 1720, val loss: 0.8613867163658142
Epoch 1730, training loss: 92.6279525756836 = 0.8398615121841431 + 10.0 * 9.17880916595459
Epoch 1730, val loss: 0.8592299818992615
Epoch 1740, training loss: 92.67097473144531 = 0.8374717235565186 + 10.0 * 9.183350563049316
Epoch 1740, val loss: 0.8570926189422607
Epoch 1750, training loss: 92.72483825683594 = 0.8350953459739685 + 10.0 * 9.188974380493164
Epoch 1750, val loss: 0.8549680113792419
Epoch 1760, training loss: 92.60851287841797 = 0.832713782787323 + 10.0 * 9.177579879760742
Epoch 1760, val loss: 0.852846622467041
Epoch 1770, training loss: 92.66409301757812 = 0.8303523659706116 + 10.0 * 9.183374404907227
Epoch 1770, val loss: 0.8507458567619324
Epoch 1780, training loss: 92.7354507446289 = 0.828018844127655 + 10.0 * 9.190743446350098
Epoch 1780, val loss: 0.8486598134040833
Epoch 1790, training loss: 92.76575469970703 = 0.8256888389587402 + 10.0 * 9.194005966186523
Epoch 1790, val loss: 0.8465738892555237
Epoch 1800, training loss: 92.70030975341797 = 0.8233144283294678 + 10.0 * 9.187700271606445
Epoch 1800, val loss: 0.8444464206695557
Epoch 1810, training loss: 92.61188507080078 = 0.8206470608711243 + 10.0 * 9.179123878479004
Epoch 1810, val loss: 0.8420240879058838
Epoch 1820, training loss: 92.70510864257812 = 0.8179470300674438 + 10.0 * 9.188715934753418
Epoch 1820, val loss: 0.8395901322364807
Epoch 1830, training loss: 92.72431945800781 = 0.815322995185852 + 10.0 * 9.190899848937988
Epoch 1830, val loss: 0.8371973633766174
Epoch 1840, training loss: 92.79534149169922 = 0.8127618432044983 + 10.0 * 9.198258399963379
Epoch 1840, val loss: 0.8349002003669739
Epoch 1850, training loss: 92.82520294189453 = 0.8102331757545471 + 10.0 * 9.201497077941895
Epoch 1850, val loss: 0.8326125741004944
Epoch 1860, training loss: 92.7715072631836 = 0.807716429233551 + 10.0 * 9.196378707885742
Epoch 1860, val loss: 0.8303657174110413
Epoch 1870, training loss: 92.829345703125 = 0.8052552938461304 + 10.0 * 9.202409744262695
Epoch 1870, val loss: 0.828150749206543
Epoch 1880, training loss: 92.88072967529297 = 0.8027973175048828 + 10.0 * 9.207793235778809
Epoch 1880, val loss: 0.8259522318840027
Epoch 1890, training loss: 92.86085510253906 = 0.8003560304641724 + 10.0 * 9.206049919128418
Epoch 1890, val loss: 0.8237529993057251
Epoch 1900, training loss: 92.8554458618164 = 0.7979403138160706 + 10.0 * 9.205750465393066
Epoch 1900, val loss: 0.8215830326080322
Epoch 1910, training loss: 92.77162170410156 = 0.7955762147903442 + 10.0 * 9.197604179382324
Epoch 1910, val loss: 0.8194582462310791
Epoch 1920, training loss: 92.73694610595703 = 0.7932345867156982 + 10.0 * 9.194371223449707
Epoch 1920, val loss: 0.8173704147338867
Epoch 1930, training loss: 92.83622741699219 = 0.7909286022186279 + 10.0 * 9.204529762268066
Epoch 1930, val loss: 0.8152788281440735
Epoch 1940, training loss: 92.89730834960938 = 0.7886306643486023 + 10.0 * 9.210867881774902
Epoch 1940, val loss: 0.8132217526435852
Epoch 1950, training loss: 92.91151428222656 = 0.7863386273384094 + 10.0 * 9.212517738342285
Epoch 1950, val loss: 0.8111672401428223
Epoch 1960, training loss: 92.92794036865234 = 0.784062922000885 + 10.0 * 9.214387893676758
Epoch 1960, val loss: 0.8091328144073486
Epoch 1970, training loss: 92.93077087402344 = 0.7818180918693542 + 10.0 * 9.214895248413086
Epoch 1970, val loss: 0.8071378469467163
Epoch 1980, training loss: 92.89006042480469 = 0.7795993685722351 + 10.0 * 9.21104621887207
Epoch 1980, val loss: 0.8051711916923523
Epoch 1990, training loss: 92.91258239746094 = 0.7774283289909363 + 10.0 * 9.213515281677246
Epoch 1990, val loss: 0.8032388687133789
Epoch 2000, training loss: 92.96099090576172 = 0.7752484083175659 + 10.0 * 9.218574523925781
Epoch 2000, val loss: 0.8012989163398743
Epoch 2010, training loss: 92.99769592285156 = 0.7731072902679443 + 10.0 * 9.222458839416504
Epoch 2010, val loss: 0.7993884682655334
Epoch 2020, training loss: 92.72006225585938 = 0.7709667086601257 + 10.0 * 9.194910049438477
Epoch 2020, val loss: 0.7975000739097595
Epoch 2030, training loss: 92.82852172851562 = 0.7689300179481506 + 10.0 * 9.20595932006836
Epoch 2030, val loss: 0.7957108616828918
Epoch 2040, training loss: 92.882080078125 = 0.7668881416320801 + 10.0 * 9.211519241333008
Epoch 2040, val loss: 0.7939103245735168
Epoch 2050, training loss: 92.9510726928711 = 0.7648546099662781 + 10.0 * 9.218622207641602
Epoch 2050, val loss: 0.7921296954154968
Epoch 2060, training loss: 93.00316619873047 = 0.7628032565116882 + 10.0 * 9.22403621673584
Epoch 2060, val loss: 0.7903133034706116
Epoch 2070, training loss: 92.96146392822266 = 0.7607608437538147 + 10.0 * 9.220070838928223
Epoch 2070, val loss: 0.7885177135467529
Epoch 2080, training loss: 93.02214813232422 = 0.758740246295929 + 10.0 * 9.226340293884277
Epoch 2080, val loss: 0.7867377400398254
Epoch 2090, training loss: 93.03520965576172 = 0.7567283511161804 + 10.0 * 9.227848052978516
Epoch 2090, val loss: 0.7849593758583069
Epoch 2100, training loss: 92.9717788696289 = 0.7547434568405151 + 10.0 * 9.22170352935791
Epoch 2100, val loss: 0.7832193970680237
Epoch 2110, training loss: 93.0014419555664 = 0.7527939081192017 + 10.0 * 9.224864959716797
Epoch 2110, val loss: 0.7815011143684387
Epoch 2120, training loss: 93.05409240722656 = 0.7508511543273926 + 10.0 * 9.230323791503906
Epoch 2120, val loss: 0.779779851436615
Epoch 2130, training loss: 93.09141540527344 = 0.7489175200462341 + 10.0 * 9.23425006866455
Epoch 2130, val loss: 0.7780820727348328
Epoch 2140, training loss: 93.05536651611328 = 0.7470027804374695 + 10.0 * 9.230836868286133
Epoch 2140, val loss: 0.7763945460319519
Epoch 2150, training loss: 93.09249114990234 = 0.7451136112213135 + 10.0 * 9.234737396240234
Epoch 2150, val loss: 0.7747430801391602
Epoch 2160, training loss: 93.11144256591797 = 0.7432431578636169 + 10.0 * 9.236820220947266
Epoch 2160, val loss: 0.7730897068977356
Epoch 2170, training loss: 93.10320281982422 = 0.7413860559463501 + 10.0 * 9.236181259155273
Epoch 2170, val loss: 0.771459698677063
Epoch 2180, training loss: 93.14159393310547 = 0.7395467162132263 + 10.0 * 9.240204811096191
Epoch 2180, val loss: 0.7698423266410828
Epoch 2190, training loss: 93.16964721679688 = 0.7377350330352783 + 10.0 * 9.243191719055176
Epoch 2190, val loss: 0.7682413458824158
Epoch 2200, training loss: 93.16682434082031 = 0.7359390258789062 + 10.0 * 9.243088722229004
Epoch 2200, val loss: 0.7666552066802979
Epoch 2210, training loss: 93.15718841552734 = 0.7341499924659729 + 10.0 * 9.242303848266602
Epoch 2210, val loss: 0.7650843262672424
Epoch 2220, training loss: 93.14122009277344 = 0.7323743104934692 + 10.0 * 9.240884780883789
Epoch 2220, val loss: 0.763524055480957
Epoch 2230, training loss: 93.17543029785156 = 0.7306000590324402 + 10.0 * 9.24448299407959
Epoch 2230, val loss: 0.7619391083717346
Epoch 2240, training loss: 93.17786407470703 = 0.7288678884506226 + 10.0 * 9.24489974975586
Epoch 2240, val loss: 0.760417103767395
Epoch 2250, training loss: 93.21650695800781 = 0.7271414995193481 + 10.0 * 9.248936653137207
Epoch 2250, val loss: 0.7589048147201538
Epoch 2260, training loss: 93.22496795654297 = 0.7254230380058289 + 10.0 * 9.249954223632812
Epoch 2260, val loss: 0.7573956251144409
Epoch 2270, training loss: 93.26527404785156 = 0.7237051725387573 + 10.0 * 9.254157066345215
Epoch 2270, val loss: 0.7558830380439758
Epoch 2280, training loss: 93.16419219970703 = 0.7219796776771545 + 10.0 * 9.244221687316895
Epoch 2280, val loss: 0.7543582916259766
Epoch 2290, training loss: 93.16668701171875 = 0.7203219532966614 + 10.0 * 9.244636535644531
Epoch 2290, val loss: 0.7529350519180298
Epoch 2300, training loss: 93.21391296386719 = 0.7186825275421143 + 10.0 * 9.249523162841797
Epoch 2300, val loss: 0.75147545337677
Epoch 2310, training loss: 93.2956771850586 = 0.7170469164848328 + 10.0 * 9.25786304473877
Epoch 2310, val loss: 0.7500123381614685
Epoch 2320, training loss: 93.30176544189453 = 0.7153983116149902 + 10.0 * 9.258636474609375
Epoch 2320, val loss: 0.7485721707344055
Epoch 2330, training loss: 93.2888412475586 = 0.7137775421142578 + 10.0 * 9.257506370544434
Epoch 2330, val loss: 0.7471540570259094
Epoch 2340, training loss: 93.30562591552734 = 0.7121728658676147 + 10.0 * 9.259345054626465
Epoch 2340, val loss: 0.745768129825592
Epoch 2350, training loss: 93.32575988769531 = 0.7105975151062012 + 10.0 * 9.261516571044922
Epoch 2350, val loss: 0.7443960905075073
Epoch 2360, training loss: 93.31389617919922 = 0.7090281248092651 + 10.0 * 9.260486602783203
Epoch 2360, val loss: 0.7430551648139954
Epoch 2370, training loss: 93.34700775146484 = 0.7074828147888184 + 10.0 * 9.263952255249023
Epoch 2370, val loss: 0.741729736328125
Epoch 2380, training loss: 93.35890197753906 = 0.705967903137207 + 10.0 * 9.26529312133789
Epoch 2380, val loss: 0.7404083013534546
Epoch 2390, training loss: 93.36032104492188 = 0.7044592499732971 + 10.0 * 9.265585899353027
Epoch 2390, val loss: 0.7391216158866882
Epoch 2400, training loss: 93.34113311767578 = 0.7029714584350586 + 10.0 * 9.263815879821777
Epoch 2400, val loss: 0.7378599643707275
Epoch 2410, training loss: 93.36759948730469 = 0.701518714427948 + 10.0 * 9.266608238220215
Epoch 2410, val loss: 0.7366130948066711
Epoch 2420, training loss: 93.38238525390625 = 0.7000782489776611 + 10.0 * 9.268230438232422
Epoch 2420, val loss: 0.7353852987289429
Epoch 2430, training loss: 93.3970718383789 = 0.6986657381057739 + 10.0 * 9.269840240478516
Epoch 2430, val loss: 0.7341779470443726
Epoch 2440, training loss: 93.17176055908203 = 0.6972357630729675 + 10.0 * 9.247452735900879
Epoch 2440, val loss: 0.7329901456832886
Epoch 2450, training loss: 92.90348815917969 = 0.6959911584854126 + 10.0 * 9.220749855041504
Epoch 2450, val loss: 0.7317538261413574
Epoch 2460, training loss: 92.79348754882812 = 0.6946330070495605 + 10.0 * 9.209885597229004
Epoch 2460, val loss: 0.730654239654541
Epoch 2470, training loss: 92.80284881591797 = 0.693365216255188 + 10.0 * 9.21094799041748
Epoch 2470, val loss: 0.7297868728637695
Epoch 2480, training loss: 92.92340087890625 = 0.6921230554580688 + 10.0 * 9.223127365112305
Epoch 2480, val loss: 0.7286704778671265
Epoch 2490, training loss: 92.90941619873047 = 0.6908373236656189 + 10.0 * 9.221858024597168
Epoch 2490, val loss: 0.7275881767272949
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.6602898550724637
0.8198217778743752
=== training gcn model ===
Epoch 0, training loss: 103.8898696899414 = 1.0972754955291748 + 10.0 * 10.27925968170166
Epoch 0, val loss: 1.0976365804672241
Epoch 10, training loss: 100.33878326416016 = 1.0968860387802124 + 10.0 * 9.924189567565918
Epoch 10, val loss: 1.0972820520401
Epoch 20, training loss: 98.56642150878906 = 1.0964410305023193 + 10.0 * 9.746997833251953
Epoch 20, val loss: 1.0968526601791382
Epoch 30, training loss: 97.21460723876953 = 1.095914602279663 + 10.0 * 9.611868858337402
Epoch 30, val loss: 1.0963512659072876
Epoch 40, training loss: 96.1222152709961 = 1.09539794921875 + 10.0 * 9.502681732177734
Epoch 40, val loss: 1.0958589315414429
Epoch 50, training loss: 95.20769500732422 = 1.0948596000671387 + 10.0 * 9.411283493041992
Epoch 50, val loss: 1.0953431129455566
Epoch 60, training loss: 94.43710327148438 = 1.09431791305542 + 10.0 * 9.33427906036377
Epoch 60, val loss: 1.0948266983032227
Epoch 70, training loss: 93.7704086303711 = 1.093819499015808 + 10.0 * 9.267659187316895
Epoch 70, val loss: 1.0943623781204224
Epoch 80, training loss: 93.19005584716797 = 1.0934512615203857 + 10.0 * 9.209660530090332
Epoch 80, val loss: 1.0940420627593994
Epoch 90, training loss: 92.6860580444336 = 1.0932236909866333 + 10.0 * 9.159283638000488
Epoch 90, val loss: 1.0938366651535034
Epoch 100, training loss: 92.25013732910156 = 1.093001365661621 + 10.0 * 9.115713119506836
Epoch 100, val loss: 1.0936400890350342
Epoch 110, training loss: 91.8638687133789 = 1.0927770137786865 + 10.0 * 9.077109336853027
Epoch 110, val loss: 1.093442678451538
Epoch 120, training loss: 91.5789566040039 = 1.0925523042678833 + 10.0 * 9.048640251159668
Epoch 120, val loss: 1.0932453870773315
Epoch 130, training loss: 91.28108215332031 = 1.092328429222107 + 10.0 * 9.018875122070312
Epoch 130, val loss: 1.0930439233779907
Epoch 140, training loss: 91.01708984375 = 1.0921015739440918 + 10.0 * 8.992498397827148
Epoch 140, val loss: 1.0928401947021484
Epoch 150, training loss: 90.7914810180664 = 1.0918635129928589 + 10.0 * 8.969961166381836
Epoch 150, val loss: 1.092628002166748
Epoch 160, training loss: 90.5821304321289 = 1.0916199684143066 + 10.0 * 8.949050903320312
Epoch 160, val loss: 1.0924088954925537
Epoch 170, training loss: 90.41195678710938 = 1.091381311416626 + 10.0 * 8.93205738067627
Epoch 170, val loss: 1.0921945571899414
Epoch 180, training loss: 90.28804779052734 = 1.0911345481872559 + 10.0 * 8.91969108581543
Epoch 180, val loss: 1.091969609260559
Epoch 190, training loss: 90.15128326416016 = 1.0908725261688232 + 10.0 * 8.906041145324707
Epoch 190, val loss: 1.0917359590530396
Epoch 200, training loss: 90.05435943603516 = 1.0906280279159546 + 10.0 * 8.89637279510498
Epoch 200, val loss: 1.0915182828903198
Epoch 210, training loss: 89.93328094482422 = 1.0903544425964355 + 10.0 * 8.884292602539062
Epoch 210, val loss: 1.091274619102478
Epoch 220, training loss: 89.85515594482422 = 1.0900770425796509 + 10.0 * 8.876507759094238
Epoch 220, val loss: 1.091029405593872
Epoch 230, training loss: 89.99581909179688 = 1.0897856950759888 + 10.0 * 8.890604019165039
Epoch 230, val loss: 1.0907474756240845
Epoch 240, training loss: 89.81031036376953 = 1.0895144939422607 + 10.0 * 8.872079849243164
Epoch 240, val loss: 1.090528964996338
Epoch 250, training loss: 89.6343765258789 = 1.0892086029052734 + 10.0 * 8.854516983032227
Epoch 250, val loss: 1.090230941772461
Epoch 260, training loss: 89.61506652832031 = 1.088923454284668 + 10.0 * 8.852614402770996
Epoch 260, val loss: 1.089986801147461
Epoch 270, training loss: 89.54450988769531 = 1.0886057615280151 + 10.0 * 8.845590591430664
Epoch 270, val loss: 1.089697003364563
Epoch 280, training loss: 89.52742004394531 = 1.0882877111434937 + 10.0 * 8.843913078308105
Epoch 280, val loss: 1.0894118547439575
Epoch 290, training loss: 89.4836654663086 = 1.0879428386688232 + 10.0 * 8.839571952819824
Epoch 290, val loss: 1.089106798171997
Epoch 300, training loss: 89.46559143066406 = 1.0875909328460693 + 10.0 * 8.837800025939941
Epoch 300, val loss: 1.088783860206604
Epoch 310, training loss: 89.44739532470703 = 1.0872271060943604 + 10.0 * 8.836016654968262
Epoch 310, val loss: 1.0884560346603394
Epoch 320, training loss: 89.42357635498047 = 1.0868604183197021 + 10.0 * 8.833671569824219
Epoch 320, val loss: 1.0881232023239136
Epoch 330, training loss: 89.40829467773438 = 1.086470603942871 + 10.0 * 8.832181930541992
Epoch 330, val loss: 1.0877708196640015
Epoch 340, training loss: 89.38162231445312 = 1.0859050750732422 + 10.0 * 8.829571723937988
Epoch 340, val loss: 1.0871763229370117
Epoch 350, training loss: 89.35426330566406 = 1.0848302841186523 + 10.0 * 8.826943397521973
Epoch 350, val loss: 1.08609139919281
Epoch 360, training loss: 89.3788070678711 = 1.083756923675537 + 10.0 * 8.82950496673584
Epoch 360, val loss: 1.084996223449707
Epoch 370, training loss: 89.36138153076172 = 1.0827243328094482 + 10.0 * 8.827865600585938
Epoch 370, val loss: 1.08396315574646
Epoch 380, training loss: 89.36239624023438 = 1.0817465782165527 + 10.0 * 8.828064918518066
Epoch 380, val loss: 1.082978367805481
Epoch 390, training loss: 89.3901596069336 = 1.0807876586914062 + 10.0 * 8.830937385559082
Epoch 390, val loss: 1.082027554512024
Epoch 400, training loss: 89.39777374267578 = 1.0798604488372803 + 10.0 * 8.831791877746582
Epoch 400, val loss: 1.081100583076477
Epoch 410, training loss: 89.37248992919922 = 1.0788780450820923 + 10.0 * 8.829360961914062
Epoch 410, val loss: 1.0801355838775635
Epoch 420, training loss: 89.37152099609375 = 1.0779533386230469 + 10.0 * 8.82935619354248
Epoch 420, val loss: 1.0792208909988403
Epoch 430, training loss: 89.38510131835938 = 1.0769836902618408 + 10.0 * 8.830812454223633
Epoch 430, val loss: 1.0782685279846191
Epoch 440, training loss: 89.3446044921875 = 1.0759872198104858 + 10.0 * 8.826861381530762
Epoch 440, val loss: 1.0772984027862549
Epoch 450, training loss: 89.38526916503906 = 1.0749841928482056 + 10.0 * 8.831028938293457
Epoch 450, val loss: 1.076323390007019
Epoch 460, training loss: 89.41484832763672 = 1.073978304862976 + 10.0 * 8.834087371826172
Epoch 460, val loss: 1.0753326416015625
Epoch 470, training loss: 89.43377685546875 = 1.072941541671753 + 10.0 * 8.83608341217041
Epoch 470, val loss: 1.0743156671524048
Epoch 480, training loss: 89.33830261230469 = 1.071845293045044 + 10.0 * 8.826645851135254
Epoch 480, val loss: 1.0732730627059937
Epoch 490, training loss: 89.38167572021484 = 1.070813775062561 + 10.0 * 8.831086158752441
Epoch 490, val loss: 1.0722347497940063
Epoch 500, training loss: 89.48438262939453 = 1.0697520971298218 + 10.0 * 8.841463088989258
Epoch 500, val loss: 1.0712324380874634
Epoch 510, training loss: 89.43649291992188 = 1.0686652660369873 + 10.0 * 8.836782455444336
Epoch 510, val loss: 1.0701857805252075
Epoch 520, training loss: 89.47169494628906 = 1.067572832107544 + 10.0 * 8.840412139892578
Epoch 520, val loss: 1.0691269636154175
Epoch 530, training loss: 89.50990295410156 = 1.0664691925048828 + 10.0 * 8.844343185424805
Epoch 530, val loss: 1.0680720806121826
Epoch 540, training loss: 89.49456787109375 = 1.0653480291366577 + 10.0 * 8.84292221069336
Epoch 540, val loss: 1.0669866800308228
Epoch 550, training loss: 89.49555206298828 = 1.0641999244689941 + 10.0 * 8.843134880065918
Epoch 550, val loss: 1.0658881664276123
Epoch 560, training loss: 89.52324676513672 = 1.0630264282226562 + 10.0 * 8.84602165222168
Epoch 560, val loss: 1.0647629499435425
Epoch 570, training loss: 89.53968048095703 = 1.061835527420044 + 10.0 * 8.847784042358398
Epoch 570, val loss: 1.0636191368103027
Epoch 580, training loss: 89.60147094726562 = 1.060631275177002 + 10.0 * 8.854084014892578
Epoch 580, val loss: 1.0624685287475586
Epoch 590, training loss: 89.53656768798828 = 1.0593661069869995 + 10.0 * 8.8477201461792
Epoch 590, val loss: 1.0612598657608032
Epoch 600, training loss: 89.5722427368164 = 1.0581265687942505 + 10.0 * 8.851411819458008
Epoch 600, val loss: 1.0600730180740356
Epoch 610, training loss: 89.6048355102539 = 1.0568288564682007 + 10.0 * 8.8548002243042
Epoch 610, val loss: 1.0588470697402954
Epoch 620, training loss: 89.62395477294922 = 1.0555018186569214 + 10.0 * 8.856844902038574
Epoch 620, val loss: 1.0575703382492065
Epoch 630, training loss: 89.58464050292969 = 1.0540975332260132 + 10.0 * 8.85305404663086
Epoch 630, val loss: 1.056228756904602
Epoch 640, training loss: 89.62369537353516 = 1.0526776313781738 + 10.0 * 8.857101440429688
Epoch 640, val loss: 1.0548834800720215
Epoch 650, training loss: 89.72518157958984 = 1.0513060092926025 + 10.0 * 8.867387771606445
Epoch 650, val loss: 1.0535598993301392
Epoch 660, training loss: 89.6696548461914 = 1.049817442893982 + 10.0 * 8.861983299255371
Epoch 660, val loss: 1.0521477460861206
Epoch 670, training loss: 89.74183654785156 = 1.0483371019363403 + 10.0 * 8.869349479675293
Epoch 670, val loss: 1.0507326126098633
Epoch 680, training loss: 89.7693862915039 = 1.046783208847046 + 10.0 * 8.872260093688965
Epoch 680, val loss: 1.0492647886276245
Epoch 690, training loss: 89.76985168457031 = 1.0452277660369873 + 10.0 * 8.872462272644043
Epoch 690, val loss: 1.0477863550186157
Epoch 700, training loss: 89.76930236816406 = 1.0436334609985352 + 10.0 * 8.872567176818848
Epoch 700, val loss: 1.0462642908096313
Epoch 710, training loss: 89.79144287109375 = 1.0420643091201782 + 10.0 * 8.874938011169434
Epoch 710, val loss: 1.0447804927825928
Epoch 720, training loss: 89.85720825195312 = 1.0404515266418457 + 10.0 * 8.881675720214844
Epoch 720, val loss: 1.0432437658309937
Epoch 730, training loss: 89.86856842041016 = 1.0388425588607788 + 10.0 * 8.882972717285156
Epoch 730, val loss: 1.0417091846466064
Epoch 740, training loss: 89.90972900390625 = 1.0372036695480347 + 10.0 * 8.887252807617188
Epoch 740, val loss: 1.0401581525802612
Epoch 750, training loss: 89.9127426147461 = 1.0355247259140015 + 10.0 * 8.88772201538086
Epoch 750, val loss: 1.0385682582855225
Epoch 760, training loss: 89.94866180419922 = 1.033853530883789 + 10.0 * 8.891481399536133
Epoch 760, val loss: 1.0369904041290283
Epoch 770, training loss: 89.96334075927734 = 1.0321333408355713 + 10.0 * 8.893120765686035
Epoch 770, val loss: 1.0353503227233887
Epoch 780, training loss: 89.95491790771484 = 1.0304032564163208 + 10.0 * 8.892451286315918
Epoch 780, val loss: 1.03373122215271
Epoch 790, training loss: 89.98080444335938 = 1.0286651849746704 + 10.0 * 8.895214080810547
Epoch 790, val loss: 1.032073736190796
Epoch 800, training loss: 89.9852066040039 = 1.0268886089324951 + 10.0 * 8.895832061767578
Epoch 800, val loss: 1.030389428138733
Epoch 810, training loss: 89.9888916015625 = 1.0250871181488037 + 10.0 * 8.896380424499512
Epoch 810, val loss: 1.0286980867385864
Epoch 820, training loss: 90.0120849609375 = 1.0232692956924438 + 10.0 * 8.898881912231445
Epoch 820, val loss: 1.0269933938980103
Epoch 830, training loss: 90.05648040771484 = 1.021460771560669 + 10.0 * 8.903501510620117
Epoch 830, val loss: 1.0252577066421509
Epoch 840, training loss: 90.0499267578125 = 1.019559621810913 + 10.0 * 8.903036117553711
Epoch 840, val loss: 1.0234720706939697
Epoch 850, training loss: 90.09683227539062 = 1.017688512802124 + 10.0 * 8.907915115356445
Epoch 850, val loss: 1.021715760231018
Epoch 860, training loss: 90.0993881225586 = 1.0157498121261597 + 10.0 * 8.908364295959473
Epoch 860, val loss: 1.0198770761489868
Epoch 870, training loss: 90.06635284423828 = 1.0137697458267212 + 10.0 * 8.905258178710938
Epoch 870, val loss: 1.0180230140686035
Epoch 880, training loss: 90.05606079101562 = 1.0118621587753296 + 10.0 * 8.904419898986816
Epoch 880, val loss: 1.0162148475646973
Epoch 890, training loss: 90.10813903808594 = 1.0099372863769531 + 10.0 * 8.909820556640625
Epoch 890, val loss: 1.0144116878509521
Epoch 900, training loss: 90.14405822753906 = 1.0079699754714966 + 10.0 * 8.91360855102539
Epoch 900, val loss: 1.0125212669372559
Epoch 910, training loss: 90.1602554321289 = 1.0059384107589722 + 10.0 * 8.91543197631836
Epoch 910, val loss: 1.0106260776519775
Epoch 920, training loss: 90.21399688720703 = 1.0038771629333496 + 10.0 * 8.921011924743652
Epoch 920, val loss: 1.0086781978607178
Epoch 930, training loss: 90.23387145996094 = 1.0018333196640015 + 10.0 * 8.92320442199707
Epoch 930, val loss: 1.0067354440689087
Epoch 940, training loss: 90.11775970458984 = 0.9997079372406006 + 10.0 * 8.911805152893066
Epoch 940, val loss: 1.0047484636306763
Epoch 950, training loss: 90.10772705078125 = 0.9975368976593018 + 10.0 * 8.911019325256348
Epoch 950, val loss: 1.0026817321777344
Epoch 960, training loss: 90.24031829833984 = 0.9954749345779419 + 10.0 * 8.924484252929688
Epoch 960, val loss: 1.0007507801055908
Epoch 970, training loss: 90.12446594238281 = 0.993330717086792 + 10.0 * 8.913113594055176
Epoch 970, val loss: 0.9987510442733765
Epoch 980, training loss: 90.23162078857422 = 0.9912145137786865 + 10.0 * 8.924040794372559
Epoch 980, val loss: 0.9967396259307861
Epoch 990, training loss: 90.3076400756836 = 0.9890950322151184 + 10.0 * 8.931854248046875
Epoch 990, val loss: 0.9947299361228943
Epoch 1000, training loss: 90.36180114746094 = 0.9868888854980469 + 10.0 * 8.937491416931152
Epoch 1000, val loss: 0.9926482439041138
Epoch 1010, training loss: 90.37367248535156 = 0.9846166372299194 + 10.0 * 8.938905715942383
Epoch 1010, val loss: 0.9905091524124146
Epoch 1020, training loss: 90.39158630371094 = 0.9823290705680847 + 10.0 * 8.940925598144531
Epoch 1020, val loss: 0.9883346557617188
Epoch 1030, training loss: 90.2285385131836 = 0.9798932075500488 + 10.0 * 8.924863815307617
Epoch 1030, val loss: 0.9860219359397888
Epoch 1040, training loss: 90.49068450927734 = 0.9775577783584595 + 10.0 * 8.951313018798828
Epoch 1040, val loss: 0.9838250279426575
Epoch 1050, training loss: 90.37533569335938 = 0.9753215909004211 + 10.0 * 8.940001487731934
Epoch 1050, val loss: 0.9817745089530945
Epoch 1060, training loss: 90.38235473632812 = 0.9729102253913879 + 10.0 * 8.94094467163086
Epoch 1060, val loss: 0.979523777961731
Epoch 1070, training loss: 90.37718200683594 = 0.9705029726028442 + 10.0 * 8.940668106079102
Epoch 1070, val loss: 0.9772169589996338
Epoch 1080, training loss: 90.48792266845703 = 0.968103289604187 + 10.0 * 8.951982498168945
Epoch 1080, val loss: 0.9749829173088074
Epoch 1090, training loss: 90.45997619628906 = 0.9656380414962769 + 10.0 * 8.949434280395508
Epoch 1090, val loss: 0.9726653695106506
Epoch 1100, training loss: 90.50509643554688 = 0.9631612300872803 + 10.0 * 8.954194068908691
Epoch 1100, val loss: 0.9703401923179626
Epoch 1110, training loss: 90.49530029296875 = 0.9606285095214844 + 10.0 * 8.953466415405273
Epoch 1110, val loss: 0.9679630398750305
Epoch 1120, training loss: 90.57235717773438 = 0.9581315517425537 + 10.0 * 8.96142292022705
Epoch 1120, val loss: 0.965621292591095
Epoch 1130, training loss: 90.56401062011719 = 0.9555695652961731 + 10.0 * 8.960844039916992
Epoch 1130, val loss: 0.9632019996643066
Epoch 1140, training loss: 90.60900115966797 = 0.952974259853363 + 10.0 * 8.96560287475586
Epoch 1140, val loss: 0.9607618451118469
Epoch 1150, training loss: 90.61732482910156 = 0.9503759741783142 + 10.0 * 8.966694831848145
Epoch 1150, val loss: 0.958310604095459
Epoch 1160, training loss: 90.63117980957031 = 0.9477524757385254 + 10.0 * 8.968342781066895
Epoch 1160, val loss: 0.955861508846283
Epoch 1170, training loss: 90.66691589355469 = 0.9451119899749756 + 10.0 * 8.972180366516113
Epoch 1170, val loss: 0.9533765912055969
Epoch 1180, training loss: 90.69721221923828 = 0.9424830079078674 + 10.0 * 8.975473403930664
Epoch 1180, val loss: 0.9509186148643494
Epoch 1190, training loss: 90.69876098632812 = 0.9397904872894287 + 10.0 * 8.975896835327148
Epoch 1190, val loss: 0.9483753442764282
Epoch 1200, training loss: 90.71154022216797 = 0.9371358752250671 + 10.0 * 8.97744083404541
Epoch 1200, val loss: 0.9458580017089844
Epoch 1210, training loss: 90.72514343261719 = 0.9343834519386292 + 10.0 * 8.97907543182373
Epoch 1210, val loss: 0.9432942867279053
Epoch 1220, training loss: 90.73873138427734 = 0.9316757321357727 + 10.0 * 8.980705261230469
Epoch 1220, val loss: 0.9407540559768677
Epoch 1230, training loss: 90.73365783691406 = 0.9289216995239258 + 10.0 * 8.980473518371582
Epoch 1230, val loss: 0.9381659626960754
Epoch 1240, training loss: 90.7544937133789 = 0.9261937737464905 + 10.0 * 8.982830047607422
Epoch 1240, val loss: 0.935584545135498
Epoch 1250, training loss: 90.7814712524414 = 0.9234387874603271 + 10.0 * 8.985803604125977
Epoch 1250, val loss: 0.9329975843429565
Epoch 1260, training loss: 90.8216323852539 = 0.9206770658493042 + 10.0 * 8.990095138549805
Epoch 1260, val loss: 0.9303820729255676
Epoch 1270, training loss: 90.78749084472656 = 0.9178796410560608 + 10.0 * 8.986961364746094
Epoch 1270, val loss: 0.9277234673500061
Epoch 1280, training loss: 90.81642150878906 = 0.9151172041893005 + 10.0 * 8.990130424499512
Epoch 1280, val loss: 0.9251678586006165
Epoch 1290, training loss: 90.87437438964844 = 0.9123534560203552 + 10.0 * 8.99620246887207
Epoch 1290, val loss: 0.9225665330886841
Epoch 1300, training loss: 90.80425262451172 = 0.9095212817192078 + 10.0 * 8.989473342895508
Epoch 1300, val loss: 0.9198728799819946
Epoch 1310, training loss: 90.84204864501953 = 0.9067107439041138 + 10.0 * 8.993534088134766
Epoch 1310, val loss: 0.9172638654708862
Epoch 1320, training loss: 90.89540100097656 = 0.9039005041122437 + 10.0 * 8.999150276184082
Epoch 1320, val loss: 0.9146191477775574
Epoch 1330, training loss: 90.87142181396484 = 0.901019275188446 + 10.0 * 8.997040748596191
Epoch 1330, val loss: 0.9118837714195251
Epoch 1340, training loss: 90.88716888427734 = 0.898185670375824 + 10.0 * 8.99889850616455
Epoch 1340, val loss: 0.9091700911521912
Epoch 1350, training loss: 90.81300354003906 = 0.8953637480735779 + 10.0 * 8.991764068603516
Epoch 1350, val loss: 0.9065411686897278
Epoch 1360, training loss: 90.88697052001953 = 0.8924696445465088 + 10.0 * 8.999449729919434
Epoch 1360, val loss: 0.9038349986076355
Epoch 1370, training loss: 90.89671325683594 = 0.8896035552024841 + 10.0 * 9.000711441040039
Epoch 1370, val loss: 0.9011462330818176
Epoch 1380, training loss: 90.98677825927734 = 0.8866874575614929 + 10.0 * 9.010008811950684
Epoch 1380, val loss: 0.898421585559845
Epoch 1390, training loss: 90.96570587158203 = 0.8837340474128723 + 10.0 * 9.008196830749512
Epoch 1390, val loss: 0.8956400156021118
Epoch 1400, training loss: 90.9795913696289 = 0.8808022737503052 + 10.0 * 9.009878158569336
Epoch 1400, val loss: 0.8928887248039246
Epoch 1410, training loss: 91.05540466308594 = 0.8778648376464844 + 10.0 * 9.017753601074219
Epoch 1410, val loss: 0.890146017074585
Epoch 1420, training loss: 91.06432342529297 = 0.8748965859413147 + 10.0 * 9.018942832946777
Epoch 1420, val loss: 0.8873706459999084
Epoch 1430, training loss: 91.08418273925781 = 0.8719370365142822 + 10.0 * 9.021224021911621
Epoch 1430, val loss: 0.884566068649292
Epoch 1440, training loss: 91.08013153076172 = 0.868942379951477 + 10.0 * 9.021119117736816
Epoch 1440, val loss: 0.8817774057388306
Epoch 1450, training loss: 91.1231918334961 = 0.8659666180610657 + 10.0 * 9.02572250366211
Epoch 1450, val loss: 0.8789806962013245
Epoch 1460, training loss: 91.13059997558594 = 0.8629759550094604 + 10.0 * 9.026762008666992
Epoch 1460, val loss: 0.8761937618255615
Epoch 1470, training loss: 91.15493774414062 = 0.8599663376808167 + 10.0 * 9.029497146606445
Epoch 1470, val loss: 0.8733887076377869
Epoch 1480, training loss: 91.19791412353516 = 0.8569744825363159 + 10.0 * 9.034093856811523
Epoch 1480, val loss: 0.8706170916557312
Epoch 1490, training loss: 91.2547607421875 = 0.8539595007896423 + 10.0 * 9.040080070495605
Epoch 1490, val loss: 0.8677808046340942
Epoch 1500, training loss: 91.2148208618164 = 0.850896954536438 + 10.0 * 9.036392211914062
Epoch 1500, val loss: 0.8649259209632874
Epoch 1510, training loss: 91.15394592285156 = 0.8478837609291077 + 10.0 * 9.030606269836426
Epoch 1510, val loss: 0.8621232509613037
Epoch 1520, training loss: 91.15638732910156 = 0.8448902368545532 + 10.0 * 9.031149864196777
Epoch 1520, val loss: 0.8593374490737915
Epoch 1530, training loss: 91.22847747802734 = 0.8418514728546143 + 10.0 * 9.038662910461426
Epoch 1530, val loss: 0.8564925789833069
Epoch 1540, training loss: 91.24037170410156 = 0.8388429284095764 + 10.0 * 9.040152549743652
Epoch 1540, val loss: 0.8536716103553772
Epoch 1550, training loss: 91.24158477783203 = 0.8358047008514404 + 10.0 * 9.04057788848877
Epoch 1550, val loss: 0.8508449792861938
Epoch 1560, training loss: 91.21930694580078 = 0.8327584862709045 + 10.0 * 9.038655281066895
Epoch 1560, val loss: 0.8480291962623596
Epoch 1570, training loss: 91.2337417602539 = 0.8296950459480286 + 10.0 * 9.040404319763184
Epoch 1570, val loss: 0.8452032804489136
Epoch 1580, training loss: 91.29509735107422 = 0.8266650438308716 + 10.0 * 9.046842575073242
Epoch 1580, val loss: 0.8423752784729004
Epoch 1590, training loss: 91.31950378417969 = 0.8235930800437927 + 10.0 * 9.049591064453125
Epoch 1590, val loss: 0.8394953012466431
Epoch 1600, training loss: 91.23192596435547 = 0.8204330205917358 + 10.0 * 9.041149139404297
Epoch 1600, val loss: 0.8365238904953003
Epoch 1610, training loss: 91.02184295654297 = 0.8176224231719971 + 10.0 * 9.020421981811523
Epoch 1610, val loss: 0.8340058326721191
Epoch 1620, training loss: 90.97505950927734 = 0.8146533370018005 + 10.0 * 9.016040802001953
Epoch 1620, val loss: 0.8313128352165222
Epoch 1630, training loss: 90.90363311767578 = 0.8117577433586121 + 10.0 * 9.009187698364258
Epoch 1630, val loss: 0.828650951385498
Epoch 1640, training loss: 90.98795318603516 = 0.8088744878768921 + 10.0 * 9.017908096313477
Epoch 1640, val loss: 0.8258692026138306
Epoch 1650, training loss: 91.08173370361328 = 0.8059684634208679 + 10.0 * 9.027576446533203
Epoch 1650, val loss: 0.823177695274353
Epoch 1660, training loss: 91.15327453613281 = 0.8029175400733948 + 10.0 * 9.035036087036133
Epoch 1660, val loss: 0.8203557133674622
Epoch 1670, training loss: 91.2069091796875 = 0.7999139428138733 + 10.0 * 9.04069995880127
Epoch 1670, val loss: 0.8175711631774902
Epoch 1680, training loss: 91.2320556640625 = 0.79688960313797 + 10.0 * 9.043516159057617
Epoch 1680, val loss: 0.814764142036438
Epoch 1690, training loss: 91.16570281982422 = 0.7938456535339355 + 10.0 * 9.037185668945312
Epoch 1690, val loss: 0.8119430541992188
Epoch 1700, training loss: 91.23270416259766 = 0.7909029126167297 + 10.0 * 9.044179916381836
Epoch 1700, val loss: 0.8092164993286133
Epoch 1710, training loss: 91.2831039428711 = 0.7879743576049805 + 10.0 * 9.04951286315918
Epoch 1710, val loss: 0.806502103805542
Epoch 1720, training loss: 91.2966079711914 = 0.7849853038787842 + 10.0 * 9.051161766052246
Epoch 1720, val loss: 0.8037444353103638
Epoch 1730, training loss: 91.27474212646484 = 0.7820643782615662 + 10.0 * 9.049267768859863
Epoch 1730, val loss: 0.8010416626930237
Epoch 1740, training loss: 91.25648498535156 = 0.7790959477424622 + 10.0 * 9.047739028930664
Epoch 1740, val loss: 0.7983306646347046
Epoch 1750, training loss: 91.3376235961914 = 0.7762564420700073 + 10.0 * 9.056137084960938
Epoch 1750, val loss: 0.7957433462142944
Epoch 1760, training loss: 91.38310241699219 = 0.7733962535858154 + 10.0 * 9.060970306396484
Epoch 1760, val loss: 0.7931095957756042
Epoch 1770, training loss: 91.33828735351562 = 0.7705011367797852 + 10.0 * 9.056778907775879
Epoch 1770, val loss: 0.7904170155525208
Epoch 1780, training loss: 91.33705139160156 = 0.7676534652709961 + 10.0 * 9.056940078735352
Epoch 1780, val loss: 0.787808358669281
Epoch 1790, training loss: 91.28449249267578 = 0.7648231983184814 + 10.0 * 9.051966667175293
Epoch 1790, val loss: 0.7852106094360352
Epoch 1800, training loss: 91.33379364013672 = 0.7620409727096558 + 10.0 * 9.057175636291504
Epoch 1800, val loss: 0.7826669216156006
Epoch 1810, training loss: 91.32161712646484 = 0.7592638731002808 + 10.0 * 9.056235313415527
Epoch 1810, val loss: 0.7801192998886108
Epoch 1820, training loss: 91.39252471923828 = 0.7564981579780579 + 10.0 * 9.063602447509766
Epoch 1820, val loss: 0.7776194214820862
Epoch 1830, training loss: 91.4884033203125 = 0.7537700533866882 + 10.0 * 9.073463439941406
Epoch 1830, val loss: 0.7750844359397888
Epoch 1840, training loss: 91.4721450805664 = 0.7510033249855042 + 10.0 * 9.072113990783691
Epoch 1840, val loss: 0.7725515365600586
Epoch 1850, training loss: 91.49626922607422 = 0.7482684254646301 + 10.0 * 9.074800491333008
Epoch 1850, val loss: 0.7700396180152893
Epoch 1860, training loss: 91.47411346435547 = 0.7455178499221802 + 10.0 * 9.072858810424805
Epoch 1860, val loss: 0.7675287127494812
Epoch 1870, training loss: 91.47557067871094 = 0.7428269386291504 + 10.0 * 9.073274612426758
Epoch 1870, val loss: 0.7650497555732727
Epoch 1880, training loss: 91.52354431152344 = 0.7401493191719055 + 10.0 * 9.078339576721191
Epoch 1880, val loss: 0.7626487016677856
Epoch 1890, training loss: 91.58930206298828 = 0.7375185489654541 + 10.0 * 9.08517837524414
Epoch 1890, val loss: 0.7602441906929016
Epoch 1900, training loss: 91.49405670166016 = 0.7348445653915405 + 10.0 * 9.075921058654785
Epoch 1900, val loss: 0.7578220963478088
Epoch 1910, training loss: 91.54729461669922 = 0.7322580218315125 + 10.0 * 9.081502914428711
Epoch 1910, val loss: 0.7554734349250793
Epoch 1920, training loss: 91.60917663574219 = 0.729677140712738 + 10.0 * 9.087949752807617
Epoch 1920, val loss: 0.7531346678733826
Epoch 1930, training loss: 91.62686157226562 = 0.7271244525909424 + 10.0 * 9.089973449707031
Epoch 1930, val loss: 0.750832736492157
Epoch 1940, training loss: 91.58358001708984 = 0.7245774865150452 + 10.0 * 9.08590030670166
Epoch 1940, val loss: 0.7485370635986328
Epoch 1950, training loss: 91.5905532836914 = 0.7220879793167114 + 10.0 * 9.086846351623535
Epoch 1950, val loss: 0.7462971806526184
Epoch 1960, training loss: 91.6159896850586 = 0.7196178436279297 + 10.0 * 9.08963680267334
Epoch 1960, val loss: 0.7440410256385803
Epoch 1970, training loss: 91.43057250976562 = 0.7171871662139893 + 10.0 * 9.071338653564453
Epoch 1970, val loss: 0.7419165968894958
Epoch 1980, training loss: 91.36125183105469 = 0.7148858308792114 + 10.0 * 9.06463623046875
Epoch 1980, val loss: 0.7397744059562683
Epoch 1990, training loss: 91.4288558959961 = 0.7126254439353943 + 10.0 * 9.071622848510742
Epoch 1990, val loss: 0.7377505302429199
Epoch 2000, training loss: 91.544189453125 = 0.7102147936820984 + 10.0 * 9.08339786529541
Epoch 2000, val loss: 0.7355933785438538
Epoch 2010, training loss: 91.6430892944336 = 0.7078328728675842 + 10.0 * 9.093525886535645
Epoch 2010, val loss: 0.733498752117157
Epoch 2020, training loss: 91.68999481201172 = 0.705437183380127 + 10.0 * 9.098455429077148
Epoch 2020, val loss: 0.7313855290412903
Epoch 2030, training loss: 91.67200469970703 = 0.7030647993087769 + 10.0 * 9.096894264221191
Epoch 2030, val loss: 0.7292609810829163
Epoch 2040, training loss: 91.71179962158203 = 0.7007409930229187 + 10.0 * 9.101105690002441
Epoch 2040, val loss: 0.727205216884613
Epoch 2050, training loss: 91.70123291015625 = 0.698431134223938 + 10.0 * 9.100279808044434
Epoch 2050, val loss: 0.725159764289856
Epoch 2060, training loss: 91.72235107421875 = 0.6961705088615417 + 10.0 * 9.102618217468262
Epoch 2060, val loss: 0.7231342792510986
Epoch 2070, training loss: 91.72643280029297 = 0.6939193606376648 + 10.0 * 9.103251457214355
Epoch 2070, val loss: 0.7211388349533081
Epoch 2080, training loss: 91.71022033691406 = 0.6917245388031006 + 10.0 * 9.101849555969238
Epoch 2080, val loss: 0.7192317843437195
Epoch 2090, training loss: 91.77444458007812 = 0.6895521879196167 + 10.0 * 9.108489036560059
Epoch 2090, val loss: 0.7173072695732117
Epoch 2100, training loss: 91.7895736694336 = 0.6873914003372192 + 10.0 * 9.110218048095703
Epoch 2100, val loss: 0.7154029607772827
Epoch 2110, training loss: 91.73026275634766 = 0.6852813363075256 + 10.0 * 9.104497909545898
Epoch 2110, val loss: 0.7135239243507385
Epoch 2120, training loss: 91.73978424072266 = 0.6831806302070618 + 10.0 * 9.105660438537598
Epoch 2120, val loss: 0.7117266058921814
Epoch 2130, training loss: 91.76970672607422 = 0.6810970306396484 + 10.0 * 9.108860969543457
Epoch 2130, val loss: 0.7099241614341736
Epoch 2140, training loss: 91.82971954345703 = 0.6790609359741211 + 10.0 * 9.115065574645996
Epoch 2140, val loss: 0.7081219553947449
Epoch 2150, training loss: 91.82621002197266 = 0.6770126819610596 + 10.0 * 9.114919662475586
Epoch 2150, val loss: 0.7063379287719727
Epoch 2160, training loss: 91.85779571533203 = 0.6750052571296692 + 10.0 * 9.118279457092285
Epoch 2160, val loss: 0.7045674920082092
Epoch 2170, training loss: 91.89119720458984 = 0.6729943156242371 + 10.0 * 9.121820449829102
Epoch 2170, val loss: 0.7027876973152161
Epoch 2180, training loss: 91.85104370117188 = 0.671000599861145 + 10.0 * 9.11800479888916
Epoch 2180, val loss: 0.7010460495948792
Epoch 2190, training loss: 91.87237548828125 = 0.6690629720687866 + 10.0 * 9.120331764221191
Epoch 2190, val loss: 0.6993480324745178
Epoch 2200, training loss: 91.90069580078125 = 0.6671269536018372 + 10.0 * 9.123356819152832
Epoch 2200, val loss: 0.6976699829101562
Epoch 2210, training loss: 91.95100402832031 = 0.6652333736419678 + 10.0 * 9.12857723236084
Epoch 2210, val loss: 0.6959747076034546
Epoch 2220, training loss: 91.89159393310547 = 0.6633252501487732 + 10.0 * 9.122827529907227
Epoch 2220, val loss: 0.6943461298942566
Epoch 2230, training loss: 91.90377044677734 = 0.6614856123924255 + 10.0 * 9.124228477478027
Epoch 2230, val loss: 0.6927313208580017
Epoch 2240, training loss: 91.95663452148438 = 0.659649133682251 + 10.0 * 9.129697799682617
Epoch 2240, val loss: 0.6911475658416748
Epoch 2250, training loss: 91.95993041992188 = 0.6578139662742615 + 10.0 * 9.13021183013916
Epoch 2250, val loss: 0.6895448565483093
Epoch 2260, training loss: 91.92388153076172 = 0.6560367941856384 + 10.0 * 9.126784324645996
Epoch 2260, val loss: 0.6880260705947876
Epoch 2270, training loss: 91.92029571533203 = 0.6542738676071167 + 10.0 * 9.126602172851562
Epoch 2270, val loss: 0.6865299940109253
Epoch 2280, training loss: 91.9881591796875 = 0.6525231599807739 + 10.0 * 9.133563041687012
Epoch 2280, val loss: 0.6849977374076843
Epoch 2290, training loss: 91.97640228271484 = 0.6507788896560669 + 10.0 * 9.132562637329102
Epoch 2290, val loss: 0.6835039854049683
Epoch 2300, training loss: 91.98353576660156 = 0.6490640044212341 + 10.0 * 9.133447647094727
Epoch 2300, val loss: 0.6819979548454285
Epoch 2310, training loss: 91.97310638427734 = 0.6473625302314758 + 10.0 * 9.132574081420898
Epoch 2310, val loss: 0.6805223226547241
Epoch 2320, training loss: 91.93836975097656 = 0.6457282304763794 + 10.0 * 9.129263877868652
Epoch 2320, val loss: 0.6791363954544067
Epoch 2330, training loss: 91.90118408203125 = 0.6441653370857239 + 10.0 * 9.125701904296875
Epoch 2330, val loss: 0.6778198480606079
Epoch 2340, training loss: 91.94390106201172 = 0.6425831317901611 + 10.0 * 9.130131721496582
Epoch 2340, val loss: 0.6765177249908447
Epoch 2350, training loss: 91.99301147460938 = 0.6410031914710999 + 10.0 * 9.135200500488281
Epoch 2350, val loss: 0.675225555896759
Epoch 2360, training loss: 92.03477478027344 = 0.6394076943397522 + 10.0 * 9.13953685760498
Epoch 2360, val loss: 0.673897385597229
Epoch 2370, training loss: 92.01187133789062 = 0.6377851963043213 + 10.0 * 9.137408256530762
Epoch 2370, val loss: 0.6724902987480164
Epoch 2380, training loss: 92.03141021728516 = 0.63618004322052 + 10.0 * 9.13952350616455
Epoch 2380, val loss: 0.6711156964302063
Epoch 2390, training loss: 92.07164764404297 = 0.6345825791358948 + 10.0 * 9.143706321716309
Epoch 2390, val loss: 0.6697602868080139
Epoch 2400, training loss: 92.03681945800781 = 0.632967472076416 + 10.0 * 9.140385627746582
Epoch 2400, val loss: 0.6684061288833618
Epoch 2410, training loss: 92.02397155761719 = 0.6314231157302856 + 10.0 * 9.139254570007324
Epoch 2410, val loss: 0.6670824289321899
Epoch 2420, training loss: 92.09313201904297 = 0.6298818588256836 + 10.0 * 9.14632511138916
Epoch 2420, val loss: 0.665736973285675
Epoch 2430, training loss: 92.13392639160156 = 0.6283425092697144 + 10.0 * 9.150558471679688
Epoch 2430, val loss: 0.6644290685653687
Epoch 2440, training loss: 92.14347839355469 = 0.6268158555030823 + 10.0 * 9.151666641235352
Epoch 2440, val loss: 0.6631308197975159
Epoch 2450, training loss: 92.04405975341797 = 0.6253038644790649 + 10.0 * 9.141875267028809
Epoch 2450, val loss: 0.6618554592132568
Epoch 2460, training loss: 92.05057525634766 = 0.6238193511962891 + 10.0 * 9.142675399780273
Epoch 2460, val loss: 0.6606125831604004
Epoch 2470, training loss: 92.12535858154297 = 0.6223651766777039 + 10.0 * 9.150299072265625
Epoch 2470, val loss: 0.6593939065933228
Epoch 2480, training loss: 92.19075012207031 = 0.6209073662757874 + 10.0 * 9.156984329223633
Epoch 2480, val loss: 0.6581533551216125
Epoch 2490, training loss: 92.18370819091797 = 0.6194576621055603 + 10.0 * 9.156424522399902
Epoch 2490, val loss: 0.6569159030914307
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7472463768115942
0.8169963051510541
=== training gcn model ===
Epoch 0, training loss: 103.94093322753906 = 1.1082569360733032 + 10.0 * 10.283267974853516
Epoch 0, val loss: 1.1076048612594604
Epoch 10, training loss: 100.32711029052734 = 1.1075104475021362 + 10.0 * 9.92195987701416
Epoch 10, val loss: 1.1068942546844482
Epoch 20, training loss: 98.57106018066406 = 1.1069786548614502 + 10.0 * 9.746408462524414
Epoch 20, val loss: 1.1063480377197266
Epoch 30, training loss: 97.2796401977539 = 1.1064729690551758 + 10.0 * 9.617316246032715
Epoch 30, val loss: 1.1058249473571777
Epoch 40, training loss: 96.3083724975586 = 1.10596764087677 + 10.0 * 9.520240783691406
Epoch 40, val loss: 1.1053142547607422
Epoch 50, training loss: 95.5021743774414 = 1.105439305305481 + 10.0 * 9.43967342376709
Epoch 50, val loss: 1.1047767400741577
Epoch 60, training loss: 94.80657196044922 = 1.1049238443374634 + 10.0 * 9.37016487121582
Epoch 60, val loss: 1.104251503944397
Epoch 70, training loss: 94.1829605102539 = 1.104387879371643 + 10.0 * 9.307857513427734
Epoch 70, val loss: 1.1037023067474365
Epoch 80, training loss: 93.62796020507812 = 1.1038533449172974 + 10.0 * 9.252410888671875
Epoch 80, val loss: 1.1031509637832642
Epoch 90, training loss: 93.12757110595703 = 1.1033005714416504 + 10.0 * 9.20242691040039
Epoch 90, val loss: 1.102587103843689
Epoch 100, training loss: 92.68667602539062 = 1.1027379035949707 + 10.0 * 9.158393859863281
Epoch 100, val loss: 1.102012276649475
Epoch 110, training loss: 92.31304168701172 = 1.1021479368209839 + 10.0 * 9.121088981628418
Epoch 110, val loss: 1.1014068126678467
Epoch 120, training loss: 91.97730255126953 = 1.101568579673767 + 10.0 * 9.087573051452637
Epoch 120, val loss: 1.1008179187774658
Epoch 130, training loss: 91.69517517089844 = 1.1009632349014282 + 10.0 * 9.05942153930664
Epoch 130, val loss: 1.1002018451690674
Epoch 140, training loss: 91.43221282958984 = 1.1003307104110718 + 10.0 * 9.033187866210938
Epoch 140, val loss: 1.0995674133300781
Epoch 150, training loss: 91.23564147949219 = 1.0997213125228882 + 10.0 * 9.013591766357422
Epoch 150, val loss: 1.0989508628845215
Epoch 160, training loss: 91.01703643798828 = 1.0990965366363525 + 10.0 * 8.991793632507324
Epoch 160, val loss: 1.098299264907837
Epoch 170, training loss: 90.83295440673828 = 1.0984693765640259 + 10.0 * 8.973447799682617
Epoch 170, val loss: 1.0976593494415283
Epoch 180, training loss: 90.69923400878906 = 1.0978106260299683 + 10.0 * 8.960142135620117
Epoch 180, val loss: 1.0969899892807007
Epoch 190, training loss: 90.57539367675781 = 1.0971654653549194 + 10.0 * 8.947822570800781
Epoch 190, val loss: 1.0963352918624878
Epoch 200, training loss: 90.45270538330078 = 1.096487045288086 + 10.0 * 8.93562126159668
Epoch 200, val loss: 1.0956460237503052
Epoch 210, training loss: 90.38047790527344 = 1.0958240032196045 + 10.0 * 8.928464889526367
Epoch 210, val loss: 1.094972848892212
Epoch 220, training loss: 90.25196075439453 = 1.095123529434204 + 10.0 * 8.91568374633789
Epoch 220, val loss: 1.0942625999450684
Epoch 230, training loss: 90.18403625488281 = 1.0944236516952515 + 10.0 * 8.908961296081543
Epoch 230, val loss: 1.093562364578247
Epoch 240, training loss: 90.12210845947266 = 1.09370756149292 + 10.0 * 8.902840614318848
Epoch 240, val loss: 1.0928254127502441
Epoch 250, training loss: 90.07823944091797 = 1.0929869413375854 + 10.0 * 8.89852523803711
Epoch 250, val loss: 1.0920989513397217
Epoch 260, training loss: 90.02342987060547 = 1.092293620109558 + 10.0 * 8.89311408996582
Epoch 260, val loss: 1.0913701057434082
Epoch 270, training loss: 90.02368927001953 = 1.0916240215301514 + 10.0 * 8.893206596374512
Epoch 270, val loss: 1.090688943862915
Epoch 280, training loss: 90.00041961669922 = 1.090950846672058 + 10.0 * 8.890947341918945
Epoch 280, val loss: 1.0900216102600098
Epoch 290, training loss: 89.93572235107422 = 1.0902868509292603 + 10.0 * 8.884543418884277
Epoch 290, val loss: 1.0893522500991821
Epoch 300, training loss: 89.87838745117188 = 1.0896880626678467 + 10.0 * 8.878870010375977
Epoch 300, val loss: 1.0887490510940552
Epoch 310, training loss: 89.83425903320312 = 1.0890955924987793 + 10.0 * 8.874516487121582
Epoch 310, val loss: 1.0881479978561401
Epoch 320, training loss: 89.91653442382812 = 1.0886024236679077 + 10.0 * 8.882793426513672
Epoch 320, val loss: 1.0876294374465942
Epoch 330, training loss: 89.87747955322266 = 1.088110089302063 + 10.0 * 8.878936767578125
Epoch 330, val loss: 1.0871316194534302
Epoch 340, training loss: 89.81304168701172 = 1.0876543521881104 + 10.0 * 8.872538566589355
Epoch 340, val loss: 1.0866808891296387
Epoch 350, training loss: 89.84934997558594 = 1.0872344970703125 + 10.0 * 8.876211166381836
Epoch 350, val loss: 1.0862690210342407
Epoch 360, training loss: 89.82514190673828 = 1.0868275165557861 + 10.0 * 8.873830795288086
Epoch 360, val loss: 1.0858556032180786
Epoch 370, training loss: 89.81240844726562 = 1.0864245891571045 + 10.0 * 8.872598648071289
Epoch 370, val loss: 1.0854521989822388
Epoch 380, training loss: 89.82653045654297 = 1.0860259532928467 + 10.0 * 8.87405014038086
Epoch 380, val loss: 1.085050344467163
Epoch 390, training loss: 89.84754180908203 = 1.0856388807296753 + 10.0 * 8.876190185546875
Epoch 390, val loss: 1.084663987159729
Epoch 400, training loss: 89.86170959472656 = 1.0852384567260742 + 10.0 * 8.877647399902344
Epoch 400, val loss: 1.0842572450637817
Epoch 410, training loss: 89.85436248779297 = 1.0842946767807007 + 10.0 * 8.877006530761719
Epoch 410, val loss: 1.083229422569275
Epoch 420, training loss: 89.86103820800781 = 1.0829561948776245 + 10.0 * 8.877808570861816
Epoch 420, val loss: 1.0819741487503052
Epoch 430, training loss: 89.8616714477539 = 1.0816704034805298 + 10.0 * 8.878000259399414
Epoch 430, val loss: 1.0807732343673706
Epoch 440, training loss: 89.84884643554688 = 1.0804466009140015 + 10.0 * 8.876840591430664
Epoch 440, val loss: 1.0796290636062622
Epoch 450, training loss: 89.82516479492188 = 1.0792995691299438 + 10.0 * 8.87458610534668
Epoch 450, val loss: 1.0785231590270996
Epoch 460, training loss: 89.91383361816406 = 1.0782040357589722 + 10.0 * 8.883563041687012
Epoch 460, val loss: 1.0774688720703125
Epoch 470, training loss: 89.86795806884766 = 1.077131986618042 + 10.0 * 8.879082679748535
Epoch 470, val loss: 1.0764516592025757
Epoch 480, training loss: 89.8008041381836 = 1.076043963432312 + 10.0 * 8.872476577758789
Epoch 480, val loss: 1.0753989219665527
Epoch 490, training loss: 89.8418960571289 = 1.0750430822372437 + 10.0 * 8.87668514251709
Epoch 490, val loss: 1.0744229555130005
Epoch 500, training loss: 89.85308837890625 = 1.074022889137268 + 10.0 * 8.877906799316406
Epoch 500, val loss: 1.0734397172927856
Epoch 510, training loss: 89.8951416015625 = 1.0729975700378418 + 10.0 * 8.882214546203613
Epoch 510, val loss: 1.0724502801895142
Epoch 520, training loss: 89.91887664794922 = 1.0719562768936157 + 10.0 * 8.884692192077637
Epoch 520, val loss: 1.0714372396469116
Epoch 530, training loss: 89.95856475830078 = 1.0709149837493896 + 10.0 * 8.888765335083008
Epoch 530, val loss: 1.070425271987915
Epoch 540, training loss: 89.99754333496094 = 1.0698593854904175 + 10.0 * 8.892767906188965
Epoch 540, val loss: 1.0693740844726562
Epoch 550, training loss: 89.96114349365234 = 1.0687834024429321 + 10.0 * 8.889235496520996
Epoch 550, val loss: 1.0683518648147583
Epoch 560, training loss: 89.96439361572266 = 1.0676835775375366 + 10.0 * 8.889671325683594
Epoch 560, val loss: 1.0672872066497803
Epoch 570, training loss: 90.00800323486328 = 1.0665653944015503 + 10.0 * 8.894144058227539
Epoch 570, val loss: 1.0661898851394653
Epoch 580, training loss: 90.1051254272461 = 1.0654748678207397 + 10.0 * 8.90396499633789
Epoch 580, val loss: 1.0651352405548096
Epoch 590, training loss: 89.94500732421875 = 1.0643203258514404 + 10.0 * 8.888068199157715
Epoch 590, val loss: 1.0640137195587158
Epoch 600, training loss: 89.95541381835938 = 1.0630909204483032 + 10.0 * 8.889232635498047
Epoch 600, val loss: 1.0628124475479126
Epoch 610, training loss: 90.11266326904297 = 1.0620182752609253 + 10.0 * 8.905064582824707
Epoch 610, val loss: 1.06175696849823
Epoch 620, training loss: 90.08900451660156 = 1.0607324838638306 + 10.0 * 8.902827262878418
Epoch 620, val loss: 1.0605155229568481
Epoch 630, training loss: 90.1053466796875 = 1.059464931488037 + 10.0 * 8.90458869934082
Epoch 630, val loss: 1.0592800378799438
Epoch 640, training loss: 90.07910919189453 = 1.0581153631210327 + 10.0 * 8.902099609375
Epoch 640, val loss: 1.0579804182052612
Epoch 650, training loss: 90.07720947265625 = 1.0567772388458252 + 10.0 * 8.902043342590332
Epoch 650, val loss: 1.0566625595092773
Epoch 660, training loss: 90.18646240234375 = 1.0553863048553467 + 10.0 * 8.913106918334961
Epoch 660, val loss: 1.0553159713745117
Epoch 670, training loss: 90.17577362060547 = 1.0539911985397339 + 10.0 * 8.912178039550781
Epoch 670, val loss: 1.053968071937561
Epoch 680, training loss: 90.1843490600586 = 1.0525845289230347 + 10.0 * 8.913176536560059
Epoch 680, val loss: 1.0526117086410522
Epoch 690, training loss: 90.17454528808594 = 1.051070213317871 + 10.0 * 8.912347793579102
Epoch 690, val loss: 1.0511648654937744
Epoch 700, training loss: 90.11450958251953 = 1.0495684146881104 + 10.0 * 8.906494140625
Epoch 700, val loss: 1.0496785640716553
Epoch 710, training loss: 90.18172454833984 = 1.0480785369873047 + 10.0 * 8.91336441040039
Epoch 710, val loss: 1.0482323169708252
Epoch 720, training loss: 90.24376678466797 = 1.046531319618225 + 10.0 * 8.919723510742188
Epoch 720, val loss: 1.0467256307601929
Epoch 730, training loss: 90.31884002685547 = 1.0449391603469849 + 10.0 * 8.927390098571777
Epoch 730, val loss: 1.0451908111572266
Epoch 740, training loss: 90.30818176269531 = 1.0433199405670166 + 10.0 * 8.926486015319824
Epoch 740, val loss: 1.0436218976974487
Epoch 750, training loss: 90.3216323852539 = 1.0417025089263916 + 10.0 * 8.927992820739746
Epoch 750, val loss: 1.0420434474945068
Epoch 760, training loss: 90.37196350097656 = 1.0400111675262451 + 10.0 * 8.933195114135742
Epoch 760, val loss: 1.0404049158096313
Epoch 770, training loss: 90.36945343017578 = 1.0383092164993286 + 10.0 * 8.933115005493164
Epoch 770, val loss: 1.0387345552444458
Epoch 780, training loss: 90.37889099121094 = 1.0365575551986694 + 10.0 * 8.934232711791992
Epoch 780, val loss: 1.0370306968688965
Epoch 790, training loss: 90.3968276977539 = 1.0347788333892822 + 10.0 * 8.93620491027832
Epoch 790, val loss: 1.0352931022644043
Epoch 800, training loss: 90.40824127197266 = 1.0329490900039673 + 10.0 * 8.937528610229492
Epoch 800, val loss: 1.0335239171981812
Epoch 810, training loss: 90.45361328125 = 1.0311269760131836 + 10.0 * 8.942248344421387
Epoch 810, val loss: 1.0317590236663818
Epoch 820, training loss: 90.5040054321289 = 1.029284954071045 + 10.0 * 8.94747257232666
Epoch 820, val loss: 1.0299713611602783
Epoch 830, training loss: 90.44941711425781 = 1.0273568630218506 + 10.0 * 8.942205429077148
Epoch 830, val loss: 1.028087854385376
Epoch 840, training loss: 90.4493408203125 = 1.0254188776016235 + 10.0 * 8.942392349243164
Epoch 840, val loss: 1.0262025594711304
Epoch 850, training loss: 90.52732849121094 = 1.0234276056289673 + 10.0 * 8.950389862060547
Epoch 850, val loss: 1.0242748260498047
Epoch 860, training loss: 90.52031707763672 = 1.0214166641235352 + 10.0 * 8.94989013671875
Epoch 860, val loss: 1.022314190864563
Epoch 870, training loss: 90.56666564941406 = 1.019359827041626 + 10.0 * 8.954730033874512
Epoch 870, val loss: 1.0202999114990234
Epoch 880, training loss: 90.53856658935547 = 1.0171940326690674 + 10.0 * 8.952136993408203
Epoch 880, val loss: 1.0181964635849
Epoch 890, training loss: 90.53768920898438 = 1.0149790048599243 + 10.0 * 8.952271461486816
Epoch 890, val loss: 1.0160415172576904
Epoch 900, training loss: 90.61589813232422 = 1.0127745866775513 + 10.0 * 8.960312843322754
Epoch 900, val loss: 1.0138897895812988
Epoch 910, training loss: 90.57404327392578 = 1.0103700160980225 + 10.0 * 8.956367492675781
Epoch 910, val loss: 1.011541724205017
Epoch 920, training loss: 90.61593627929688 = 1.0080299377441406 + 10.0 * 8.960790634155273
Epoch 920, val loss: 1.0092675685882568
Epoch 930, training loss: 90.6036148071289 = 1.0056202411651611 + 10.0 * 8.959798812866211
Epoch 930, val loss: 1.0069503784179688
Epoch 940, training loss: 90.65666961669922 = 1.0031476020812988 + 10.0 * 8.965352058410645
Epoch 940, val loss: 1.0045490264892578
Epoch 950, training loss: 90.68527221679688 = 1.0005990266799927 + 10.0 * 8.968466758728027
Epoch 950, val loss: 1.0020662546157837
Epoch 960, training loss: 90.68218231201172 = 0.997973620891571 + 10.0 * 8.96842098236084
Epoch 960, val loss: 0.999489963054657
Epoch 970, training loss: 90.86315155029297 = 0.9954348802566528 + 10.0 * 8.986771583557129
Epoch 970, val loss: 0.997054934501648
Epoch 980, training loss: 90.62278747558594 = 0.9925938248634338 + 10.0 * 8.963019371032715
Epoch 980, val loss: 0.9942946434020996
Epoch 990, training loss: 90.70875549316406 = 0.9895821809768677 + 10.0 * 8.971917152404785
Epoch 990, val loss: 0.9913743138313293
Epoch 1000, training loss: 90.63752746582031 = 0.9864073991775513 + 10.0 * 8.965112686157227
Epoch 1000, val loss: 0.9882420897483826
Epoch 1010, training loss: 90.74669647216797 = 0.9832087755203247 + 10.0 * 8.976348876953125
Epoch 1010, val loss: 0.9851351976394653
Epoch 1020, training loss: 90.75320434570312 = 0.979839563369751 + 10.0 * 8.977335929870605
Epoch 1020, val loss: 0.9819082617759705
Epoch 1030, training loss: 90.79084014892578 = 0.9764028191566467 + 10.0 * 8.981443405151367
Epoch 1030, val loss: 0.9785505533218384
Epoch 1040, training loss: 90.83505249023438 = 0.9728758931159973 + 10.0 * 8.986217498779297
Epoch 1040, val loss: 0.975095808506012
Epoch 1050, training loss: 90.91387939453125 = 0.9691607356071472 + 10.0 * 8.994471549987793
Epoch 1050, val loss: 0.9713965058326721
Epoch 1060, training loss: 91.0702896118164 = 0.9656904935836792 + 10.0 * 9.010459899902344
Epoch 1060, val loss: 0.9681316614151001
Epoch 1070, training loss: 90.58468627929688 = 0.9617587327957153 + 10.0 * 8.962292671203613
Epoch 1070, val loss: 0.9642646312713623
Epoch 1080, training loss: 90.82432556152344 = 0.9580525159835815 + 10.0 * 8.986627578735352
Epoch 1080, val loss: 0.9606322646141052
Epoch 1090, training loss: 90.78498077392578 = 0.9542337656021118 + 10.0 * 8.983075141906738
Epoch 1090, val loss: 0.9569243788719177
Epoch 1100, training loss: 90.84793090820312 = 0.9503329396247864 + 10.0 * 8.98975944519043
Epoch 1100, val loss: 0.9531306624412537
Epoch 1110, training loss: 90.9422607421875 = 0.9463903307914734 + 10.0 * 8.999587059020996
Epoch 1110, val loss: 0.9492825269699097
Epoch 1120, training loss: 90.96456146240234 = 0.9423267841339111 + 10.0 * 9.002223014831543
Epoch 1120, val loss: 0.9453091025352478
Epoch 1130, training loss: 91.00334930419922 = 0.9382407069206238 + 10.0 * 9.006510734558105
Epoch 1130, val loss: 0.9413301944732666
Epoch 1140, training loss: 91.03970336914062 = 0.9340676665306091 + 10.0 * 9.010563850402832
Epoch 1140, val loss: 0.9372755885124207
Epoch 1150, training loss: 91.01599884033203 = 0.9298573136329651 + 10.0 * 9.008614540100098
Epoch 1150, val loss: 0.9331793785095215
Epoch 1160, training loss: 91.02552795410156 = 0.9255480170249939 + 10.0 * 9.009998321533203
Epoch 1160, val loss: 0.9289939999580383
Epoch 1170, training loss: 91.07616424560547 = 0.9212467670440674 + 10.0 * 9.015491485595703
Epoch 1170, val loss: 0.9248048663139343
Epoch 1180, training loss: 91.1456069946289 = 0.9168727397918701 + 10.0 * 9.022873878479004
Epoch 1180, val loss: 0.9205467700958252
Epoch 1190, training loss: 91.15576934814453 = 0.9124294519424438 + 10.0 * 9.024333953857422
Epoch 1190, val loss: 0.9162319898605347
Epoch 1200, training loss: 91.16606903076172 = 0.9079481959342957 + 10.0 * 9.025812149047852
Epoch 1200, val loss: 0.9118788242340088
Epoch 1210, training loss: 91.17716217041016 = 0.9034060835838318 + 10.0 * 9.027376174926758
Epoch 1210, val loss: 0.9074912667274475
Epoch 1220, training loss: 91.2088851928711 = 0.8988602757453918 + 10.0 * 9.03100299835205
Epoch 1220, val loss: 0.9030560255050659
Epoch 1230, training loss: 91.20637512207031 = 0.8942597508430481 + 10.0 * 9.031211853027344
Epoch 1230, val loss: 0.8985965847969055
Epoch 1240, training loss: 91.18292999267578 = 0.8896102905273438 + 10.0 * 9.029332160949707
Epoch 1240, val loss: 0.8940991163253784
Epoch 1250, training loss: 91.231201171875 = 0.8849698305130005 + 10.0 * 9.034623146057129
Epoch 1250, val loss: 0.8895891904830933
Epoch 1260, training loss: 91.265869140625 = 0.8802787065505981 + 10.0 * 9.038558959960938
Epoch 1260, val loss: 0.8850538730621338
Epoch 1270, training loss: 91.33087158203125 = 0.8755674362182617 + 10.0 * 9.045530319213867
Epoch 1270, val loss: 0.8804769515991211
Epoch 1280, training loss: 91.19344329833984 = 0.8707677125930786 + 10.0 * 9.032267570495605
Epoch 1280, val loss: 0.8758630156517029
Epoch 1290, training loss: 91.22338104248047 = 0.8659793138504028 + 10.0 * 9.03573989868164
Epoch 1290, val loss: 0.8712403178215027
Epoch 1300, training loss: 91.2803955078125 = 0.8611932396888733 + 10.0 * 9.04192066192627
Epoch 1300, val loss: 0.8665975332260132
Epoch 1310, training loss: 91.28761291503906 = 0.8564444780349731 + 10.0 * 9.043116569519043
Epoch 1310, val loss: 0.862026572227478
Epoch 1320, training loss: 91.33811950683594 = 0.8516520261764526 + 10.0 * 9.048646926879883
Epoch 1320, val loss: 0.8574195504188538
Epoch 1330, training loss: 91.36804962158203 = 0.8468583226203918 + 10.0 * 9.052119255065918
Epoch 1330, val loss: 0.8527606129646301
Epoch 1340, training loss: 91.39096069335938 = 0.8420960307121277 + 10.0 * 9.054886817932129
Epoch 1340, val loss: 0.8481793403625488
Epoch 1350, training loss: 91.39791870117188 = 0.8373026847839355 + 10.0 * 9.056061744689941
Epoch 1350, val loss: 0.8435729146003723
Epoch 1360, training loss: 91.39634704589844 = 0.8324925899505615 + 10.0 * 9.05638599395752
Epoch 1360, val loss: 0.8389484882354736
Epoch 1370, training loss: 91.38497161865234 = 0.8276892304420471 + 10.0 * 9.0557279586792
Epoch 1370, val loss: 0.8343406915664673
Epoch 1380, training loss: 91.38447570800781 = 0.8229188323020935 + 10.0 * 9.05615520477295
Epoch 1380, val loss: 0.8297873139381409
Epoch 1390, training loss: 91.39186096191406 = 0.818171501159668 + 10.0 * 9.057369232177734
Epoch 1390, val loss: 0.825255274772644
Epoch 1400, training loss: 91.45701599121094 = 0.813410758972168 + 10.0 * 9.064360618591309
Epoch 1400, val loss: 0.8206880688667297
Epoch 1410, training loss: 91.43551635742188 = 0.8086970448493958 + 10.0 * 9.062681198120117
Epoch 1410, val loss: 0.8162125945091248
Epoch 1420, training loss: 91.4461898803711 = 0.8040199875831604 + 10.0 * 9.064216613769531
Epoch 1420, val loss: 0.8117392063140869
Epoch 1430, training loss: 91.47067260742188 = 0.799318253993988 + 10.0 * 9.06713581085205
Epoch 1430, val loss: 0.8073010444641113
Epoch 1440, training loss: 91.54251098632812 = 0.7946504354476929 + 10.0 * 9.074786186218262
Epoch 1440, val loss: 0.8028589487075806
Epoch 1450, training loss: 91.56317901611328 = 0.7899584174156189 + 10.0 * 9.077322006225586
Epoch 1450, val loss: 0.7983863949775696
Epoch 1460, training loss: 91.53172302246094 = 0.7852919101715088 + 10.0 * 9.0746431350708
Epoch 1460, val loss: 0.7939731478691101
Epoch 1470, training loss: 91.53569030761719 = 0.7806715965270996 + 10.0 * 9.075502395629883
Epoch 1470, val loss: 0.7895955443382263
Epoch 1480, training loss: 91.56732177734375 = 0.7760754823684692 + 10.0 * 9.079124450683594
Epoch 1480, val loss: 0.7852734327316284
Epoch 1490, training loss: 91.58100891113281 = 0.7715245485305786 + 10.0 * 9.080948829650879
Epoch 1490, val loss: 0.7809608578681946
Epoch 1500, training loss: 91.5897445678711 = 0.766944169998169 + 10.0 * 9.082280158996582
Epoch 1500, val loss: 0.7766528725624084
Epoch 1510, training loss: 91.60783386230469 = 0.7623746991157532 + 10.0 * 9.084546089172363
Epoch 1510, val loss: 0.7723804116249084
Epoch 1520, training loss: 91.61073303222656 = 0.7578391432762146 + 10.0 * 9.08528995513916
Epoch 1520, val loss: 0.7681180834770203
Epoch 1530, training loss: 91.58694458007812 = 0.7533202767372131 + 10.0 * 9.083362579345703
Epoch 1530, val loss: 0.7638604044914246
Epoch 1540, training loss: 91.62316131591797 = 0.749033510684967 + 10.0 * 9.08741283416748
Epoch 1540, val loss: 0.7598637938499451
Epoch 1550, training loss: 91.63764190673828 = 0.7447368502616882 + 10.0 * 9.089290618896484
Epoch 1550, val loss: 0.7558643221855164
Epoch 1560, training loss: 91.64459991455078 = 0.740399181842804 + 10.0 * 9.09041976928711
Epoch 1560, val loss: 0.7518017888069153
Epoch 1570, training loss: 91.62828826904297 = 0.7361014485359192 + 10.0 * 9.089219093322754
Epoch 1570, val loss: 0.747808575630188
Epoch 1580, training loss: 91.67274475097656 = 0.73189377784729 + 10.0 * 9.094084739685059
Epoch 1580, val loss: 0.7439219355583191
Epoch 1590, training loss: 91.69573211669922 = 0.727752149105072 + 10.0 * 9.096797943115234
Epoch 1590, val loss: 0.74007648229599
Epoch 1600, training loss: 91.71956634521484 = 0.7236724495887756 + 10.0 * 9.099589347839355
Epoch 1600, val loss: 0.7363138794898987
Epoch 1610, training loss: 91.69781494140625 = 0.719655454158783 + 10.0 * 9.09781551361084
Epoch 1610, val loss: 0.73260897397995
Epoch 1620, training loss: 91.65218353271484 = 0.715745210647583 + 10.0 * 9.093644142150879
Epoch 1620, val loss: 0.729076623916626
Epoch 1630, training loss: 91.61605834960938 = 0.7119073867797852 + 10.0 * 9.090415000915527
Epoch 1630, val loss: 0.7255193591117859
Epoch 1640, training loss: 91.6510009765625 = 0.7081686854362488 + 10.0 * 9.094283103942871
Epoch 1640, val loss: 0.7221553325653076
Epoch 1650, training loss: 91.71736907958984 = 0.7045008540153503 + 10.0 * 9.101286888122559
Epoch 1650, val loss: 0.7188360691070557
Epoch 1660, training loss: 91.79048156738281 = 0.7008177638053894 + 10.0 * 9.108965873718262
Epoch 1660, val loss: 0.71550452709198
Epoch 1670, training loss: 91.80076599121094 = 0.6971927285194397 + 10.0 * 9.110357284545898
Epoch 1670, val loss: 0.712245523929596
Epoch 1680, training loss: 91.83858489990234 = 0.6936116218566895 + 10.0 * 9.114497184753418
Epoch 1680, val loss: 0.7090499997138977
Epoch 1690, training loss: 91.72185516357422 = 0.6901503801345825 + 10.0 * 9.103170394897461
Epoch 1690, val loss: 0.7059882283210754
Epoch 1700, training loss: 91.78108215332031 = 0.6867901682853699 + 10.0 * 9.109429359436035
Epoch 1700, val loss: 0.7029346227645874
Epoch 1710, training loss: 91.8023910522461 = 0.6833699941635132 + 10.0 * 9.111902236938477
Epoch 1710, val loss: 0.6999387145042419
Epoch 1720, training loss: 91.89431762695312 = 0.6800901889801025 + 10.0 * 9.12142276763916
Epoch 1720, val loss: 0.6970577239990234
Epoch 1730, training loss: 91.87068176269531 = 0.6768036484718323 + 10.0 * 9.11938762664795
Epoch 1730, val loss: 0.6941530704498291
Epoch 1740, training loss: 91.9010009765625 = 0.6735531091690063 + 10.0 * 9.1227445602417
Epoch 1740, val loss: 0.6913450360298157
Epoch 1750, training loss: 91.94342041015625 = 0.6703840494155884 + 10.0 * 9.127303123474121
Epoch 1750, val loss: 0.6886184811592102
Epoch 1760, training loss: 91.90857696533203 = 0.6672707200050354 + 10.0 * 9.124130249023438
Epoch 1760, val loss: 0.685935378074646
Epoch 1770, training loss: 91.91889953613281 = 0.6641658544540405 + 10.0 * 9.125473022460938
Epoch 1770, val loss: 0.6832666993141174
Epoch 1780, training loss: 91.98333740234375 = 0.661121666431427 + 10.0 * 9.132221221923828
Epoch 1780, val loss: 0.6806396245956421
Epoch 1790, training loss: 91.99732208251953 = 0.658133327960968 + 10.0 * 9.133918762207031
Epoch 1790, val loss: 0.67805415391922
Epoch 1800, training loss: 92.00871276855469 = 0.6551731824874878 + 10.0 * 9.135354042053223
Epoch 1800, val loss: 0.6755434274673462
Epoch 1810, training loss: 92.00570678710938 = 0.6522841453552246 + 10.0 * 9.135342597961426
Epoch 1810, val loss: 0.6730681657791138
Epoch 1820, training loss: 92.00355529785156 = 0.6494078636169434 + 10.0 * 9.135415077209473
Epoch 1820, val loss: 0.6706064939498901
Epoch 1830, training loss: 91.99576568603516 = 0.6466132402420044 + 10.0 * 9.134915351867676
Epoch 1830, val loss: 0.6682875752449036
Epoch 1840, training loss: 92.05252075195312 = 0.6438015699386597 + 10.0 * 9.14087200164795
Epoch 1840, val loss: 0.6659205555915833
Epoch 1850, training loss: 92.05470275878906 = 0.6410254836082458 + 10.0 * 9.14136791229248
Epoch 1850, val loss: 0.663578987121582
Epoch 1860, training loss: 92.05233001708984 = 0.6382679343223572 + 10.0 * 9.141406059265137
Epoch 1860, val loss: 0.6612447500228882
Epoch 1870, training loss: 92.06006622314453 = 0.6355876922607422 + 10.0 * 9.142447471618652
Epoch 1870, val loss: 0.659028172492981
Epoch 1880, training loss: 92.1004867553711 = 0.6329822540283203 + 10.0 * 9.146750450134277
Epoch 1880, val loss: 0.6568387746810913
Epoch 1890, training loss: 92.10040283203125 = 0.6303814053535461 + 10.0 * 9.147002220153809
Epoch 1890, val loss: 0.6547198295593262
Epoch 1900, training loss: 92.14867401123047 = 0.6278917789459229 + 10.0 * 9.152078628540039
Epoch 1900, val loss: 0.652658998966217
Epoch 1910, training loss: 92.00955963134766 = 0.6254227161407471 + 10.0 * 9.13841438293457
Epoch 1910, val loss: 0.6506627798080444
Epoch 1920, training loss: 92.05697631835938 = 0.6229727268218994 + 10.0 * 9.143400192260742
Epoch 1920, val loss: 0.6487150192260742
Epoch 1930, training loss: 92.08709716796875 = 0.6206215620040894 + 10.0 * 9.146647453308105
Epoch 1930, val loss: 0.646808922290802
Epoch 1940, training loss: 92.1598892211914 = 0.6184358596801758 + 10.0 * 9.154145240783691
Epoch 1940, val loss: 0.6450166702270508
Epoch 1950, training loss: 91.86556243896484 = 0.6161857843399048 + 10.0 * 9.124937057495117
Epoch 1950, val loss: 0.6432991623878479
Epoch 1960, training loss: 91.8799819946289 = 0.6139674782752991 + 10.0 * 9.126601219177246
Epoch 1960, val loss: 0.6414839029312134
Epoch 1970, training loss: 91.85311126708984 = 0.6117488741874695 + 10.0 * 9.124135971069336
Epoch 1970, val loss: 0.6397773623466492
Epoch 1980, training loss: 91.95446014404297 = 0.6095678806304932 + 10.0 * 9.134489059448242
Epoch 1980, val loss: 0.6379774212837219
Epoch 1990, training loss: 92.0594482421875 = 0.6074130535125732 + 10.0 * 9.145203590393066
Epoch 1990, val loss: 0.6363110542297363
Epoch 2000, training loss: 92.1097412109375 = 0.605291485786438 + 10.0 * 9.150444984436035
Epoch 2000, val loss: 0.6346626877784729
Epoch 2010, training loss: 92.19376373291016 = 0.6031829714775085 + 10.0 * 9.159058570861816
Epoch 2010, val loss: 0.6330230832099915
Epoch 2020, training loss: 92.1758041381836 = 0.601090133190155 + 10.0 * 9.157471656799316
Epoch 2020, val loss: 0.6313815116882324
Epoch 2030, training loss: 92.13343811035156 = 0.5990493893623352 + 10.0 * 9.153438568115234
Epoch 2030, val loss: 0.6298385858535767
Epoch 2040, training loss: 92.117431640625 = 0.5970390439033508 + 10.0 * 9.152039527893066
Epoch 2040, val loss: 0.6282970905303955
Epoch 2050, training loss: 92.17444610595703 = 0.5950781106948853 + 10.0 * 9.157937049865723
Epoch 2050, val loss: 0.6268273591995239
Epoch 2060, training loss: 92.22566223144531 = 0.593127429485321 + 10.0 * 9.163253784179688
Epoch 2060, val loss: 0.6253390312194824
Epoch 2070, training loss: 92.2212142944336 = 0.5912415981292725 + 10.0 * 9.162997245788574
Epoch 2070, val loss: 0.6239304542541504
Epoch 2080, training loss: 92.25067901611328 = 0.589388906955719 + 10.0 * 9.166129112243652
Epoch 2080, val loss: 0.6225589513778687
Epoch 2090, training loss: 92.28502655029297 = 0.5875711441040039 + 10.0 * 9.169745445251465
Epoch 2090, val loss: 0.621166467666626
Epoch 2100, training loss: 92.22687530517578 = 0.5857982039451599 + 10.0 * 9.164107322692871
Epoch 2100, val loss: 0.6198854446411133
Epoch 2110, training loss: 92.27494812011719 = 0.5840834379196167 + 10.0 * 9.169086456298828
Epoch 2110, val loss: 0.6186062693595886
Epoch 2120, training loss: 92.2905044555664 = 0.5823742151260376 + 10.0 * 9.170812606811523
Epoch 2120, val loss: 0.6173352003097534
Epoch 2130, training loss: 92.18051147460938 = 0.5807151794433594 + 10.0 * 9.159979820251465
Epoch 2130, val loss: 0.6161463260650635
Epoch 2140, training loss: 92.24895477294922 = 0.5790902972221375 + 10.0 * 9.166986465454102
Epoch 2140, val loss: 0.6150028109550476
Epoch 2150, training loss: 92.30047607421875 = 0.5774814486503601 + 10.0 * 9.1722993850708
Epoch 2150, val loss: 0.6138445138931274
Epoch 2160, training loss: 92.37390899658203 = 0.5759005546569824 + 10.0 * 9.179800987243652
Epoch 2160, val loss: 0.6126821041107178
Epoch 2170, training loss: 92.38851928710938 = 0.5743319988250732 + 10.0 * 9.181418418884277
Epoch 2170, val loss: 0.6115941405296326
Epoch 2180, training loss: 92.3807601928711 = 0.5728054642677307 + 10.0 * 9.180795669555664
Epoch 2180, val loss: 0.6105531454086304
Epoch 2190, training loss: 92.20401763916016 = 0.571333646774292 + 10.0 * 9.163268089294434
Epoch 2190, val loss: 0.6095940470695496
Epoch 2200, training loss: 92.19174194335938 = 0.5698428750038147 + 10.0 * 9.162190437316895
Epoch 2200, val loss: 0.6085091233253479
Epoch 2210, training loss: 92.24151611328125 = 0.5684381127357483 + 10.0 * 9.16730785369873
Epoch 2210, val loss: 0.6075384616851807
Epoch 2220, training loss: 92.34911346435547 = 0.5670416951179504 + 10.0 * 9.178207397460938
Epoch 2220, val loss: 0.6065933704376221
Epoch 2230, training loss: 92.42422485351562 = 0.5656613707542419 + 10.0 * 9.185856819152832
Epoch 2230, val loss: 0.6056810021400452
Epoch 2240, training loss: 92.41004180908203 = 0.5643044710159302 + 10.0 * 9.18457317352295
Epoch 2240, val loss: 0.6047771573066711
Epoch 2250, training loss: 92.42926788330078 = 0.5629802942276001 + 10.0 * 9.186628341674805
Epoch 2250, val loss: 0.603907585144043
Epoch 2260, training loss: 92.49726867675781 = 0.5616629123687744 + 10.0 * 9.193560600280762
Epoch 2260, val loss: 0.6030460596084595
Epoch 2270, training loss: 92.49018859863281 = 0.5603666305541992 + 10.0 * 9.19298267364502
Epoch 2270, val loss: 0.602223813533783
Epoch 2280, training loss: 92.43063354492188 = 0.5590957403182983 + 10.0 * 9.187153816223145
Epoch 2280, val loss: 0.6014434695243835
Epoch 2290, training loss: 92.4638671875 = 0.5578919053077698 + 10.0 * 9.190597534179688
Epoch 2290, val loss: 0.6006766557693481
Epoch 2300, training loss: 92.51880645751953 = 0.5566678047180176 + 10.0 * 9.196213722229004
Epoch 2300, val loss: 0.5998886823654175
Epoch 2310, training loss: 92.50505828857422 = 0.5554762482643127 + 10.0 * 9.194958686828613
Epoch 2310, val loss: 0.5991187691688538
Epoch 2320, training loss: 92.4977035522461 = 0.5543075799942017 + 10.0 * 9.194339752197266
Epoch 2320, val loss: 0.5984274744987488
Epoch 2330, training loss: 92.55158996582031 = 0.5531539916992188 + 10.0 * 9.199843406677246
Epoch 2330, val loss: 0.5977193713188171
Epoch 2340, training loss: 92.51622772216797 = 0.552030086517334 + 10.0 * 9.196419715881348
Epoch 2340, val loss: 0.5970417261123657
Epoch 2350, training loss: 92.53962707519531 = 0.5509329438209534 + 10.0 * 9.198869705200195
Epoch 2350, val loss: 0.5964018702507019
Epoch 2360, training loss: 92.5563735961914 = 0.5498392581939697 + 10.0 * 9.200653076171875
Epoch 2360, val loss: 0.59573894739151
Epoch 2370, training loss: 92.5990982055664 = 0.5487855076789856 + 10.0 * 9.205031394958496
Epoch 2370, val loss: 0.5951620936393738
Epoch 2380, training loss: 92.58543395996094 = 0.5477283596992493 + 10.0 * 9.203770637512207
Epoch 2380, val loss: 0.5945643186569214
Epoch 2390, training loss: 92.60559844970703 = 0.5466971397399902 + 10.0 * 9.205889701843262
Epoch 2390, val loss: 0.593946099281311
Epoch 2400, training loss: 92.6172103881836 = 0.5456750392913818 + 10.0 * 9.2071533203125
Epoch 2400, val loss: 0.593392014503479
Epoch 2410, training loss: 92.65267944335938 = 0.5446738600730896 + 10.0 * 9.210801124572754
Epoch 2410, val loss: 0.5928465127944946
Epoch 2420, training loss: 92.54515075683594 = 0.543687641620636 + 10.0 * 9.200146675109863
Epoch 2420, val loss: 0.5922905802726746
Epoch 2430, training loss: 92.56596374511719 = 0.542753279209137 + 10.0 * 9.20232105255127
Epoch 2430, val loss: 0.5917782187461853
Epoch 2440, training loss: 92.61058807373047 = 0.5418033599853516 + 10.0 * 9.206878662109375
Epoch 2440, val loss: 0.5912131667137146
Epoch 2450, training loss: 92.64761352539062 = 0.5408614277839661 + 10.0 * 9.210675239562988
Epoch 2450, val loss: 0.590690016746521
Epoch 2460, training loss: 92.63692474365234 = 0.5399399399757385 + 10.0 * 9.209698677062988
Epoch 2460, val loss: 0.5901413559913635
Epoch 2470, training loss: 92.66470336914062 = 0.5390410423278809 + 10.0 * 9.212566375732422
Epoch 2470, val loss: 0.5896614193916321
Epoch 2480, training loss: 92.70368957519531 = 0.5381536483764648 + 10.0 * 9.216553688049316
Epoch 2480, val loss: 0.5891960263252258
Epoch 2490, training loss: 92.69438934326172 = 0.5372818112373352 + 10.0 * 9.215710639953613
Epoch 2490, val loss: 0.5887549519538879
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7663768115942029
0.8177932333550678
The final CL Acc:0.72464, 0.04617, The final GNN Acc:0.81820, 0.00119
raw graph: torch.Size([2, 88648])
add edge: torch.Size([2, 110942])
remove edge: torch.Size([2, 66346])
updated graph: torch.Size([2, 88640])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 104.08866119384766 = 1.0986127853393555 + 10.0 * 10.299005508422852
Epoch 0, val loss: 1.0986131429672241
Epoch 10, training loss: 100.12730407714844 = 1.0986114740371704 + 10.0 * 9.90286922454834
Epoch 10, val loss: 1.098612904548645
Epoch 20, training loss: 97.85311126708984 = 1.0986114740371704 + 10.0 * 9.675450325012207
Epoch 20, val loss: 1.098612904548645
Epoch 30, training loss: 96.17326354980469 = 1.0986114740371704 + 10.0 * 9.507465362548828
Epoch 30, val loss: 1.098612904548645
Epoch 40, training loss: 94.86111450195312 = 1.0986114740371704 + 10.0 * 9.376250267028809
Epoch 40, val loss: 1.098612904548645
Epoch 50, training loss: 93.81363677978516 = 1.0986114740371704 + 10.0 * 9.271502494812012
Epoch 50, val loss: 1.098612904548645
Epoch 60, training loss: 92.95652770996094 = 1.0986114740371704 + 10.0 * 9.185791969299316
Epoch 60, val loss: 1.098612904548645
Epoch 70, training loss: 92.24222564697266 = 1.0986114740371704 + 10.0 * 9.114361763000488
Epoch 70, val loss: 1.098612904548645
Epoch 80, training loss: 91.64804077148438 = 1.0986114740371704 + 10.0 * 9.054943084716797
Epoch 80, val loss: 1.098612904548645
Epoch 90, training loss: 91.15790557861328 = 1.0986114740371704 + 10.0 * 9.005929946899414
Epoch 90, val loss: 1.098612904548645
Epoch 100, training loss: 90.74254608154297 = 1.0986114740371704 + 10.0 * 8.964393615722656
Epoch 100, val loss: 1.098612904548645
Epoch 110, training loss: 90.39612579345703 = 1.0986114740371704 + 10.0 * 8.9297513961792
Epoch 110, val loss: 1.098612904548645
Epoch 120, training loss: 90.0853042602539 = 1.0986114740371704 + 10.0 * 8.898669242858887
Epoch 120, val loss: 1.098612904548645
Epoch 130, training loss: 89.82865905761719 = 1.0986114740371704 + 10.0 * 8.873004913330078
Epoch 130, val loss: 1.098612904548645
Epoch 140, training loss: 89.60218048095703 = 1.0986114740371704 + 10.0 * 8.850357055664062
Epoch 140, val loss: 1.098612904548645
Epoch 150, training loss: 89.41730499267578 = 1.0986114740371704 + 10.0 * 8.831869125366211
Epoch 150, val loss: 1.098612904548645
Epoch 160, training loss: 89.25068664550781 = 1.0986114740371704 + 10.0 * 8.815207481384277
Epoch 160, val loss: 1.098612904548645
Epoch 170, training loss: 89.1335220336914 = 1.0986114740371704 + 10.0 * 8.803491592407227
Epoch 170, val loss: 1.098612904548645
Epoch 180, training loss: 88.99784851074219 = 1.0986114740371704 + 10.0 * 8.789923667907715
Epoch 180, val loss: 1.098612904548645
Epoch 190, training loss: 88.9128189086914 = 1.0986114740371704 + 10.0 * 8.781420707702637
Epoch 190, val loss: 1.098612904548645
Epoch 200, training loss: 88.79701232910156 = 1.0986114740371704 + 10.0 * 8.769840240478516
Epoch 200, val loss: 1.098612904548645
Epoch 210, training loss: 88.73318481445312 = 1.0986114740371704 + 10.0 * 8.763457298278809
Epoch 210, val loss: 1.098612904548645
Epoch 220, training loss: 88.66948699951172 = 1.0986114740371704 + 10.0 * 8.757087707519531
Epoch 220, val loss: 1.098612904548645
Epoch 230, training loss: 88.60791778564453 = 1.0986114740371704 + 10.0 * 8.750930786132812
Epoch 230, val loss: 1.098612904548645
Epoch 240, training loss: 88.55223083496094 = 1.0986114740371704 + 10.0 * 8.745362281799316
Epoch 240, val loss: 1.098612904548645
Epoch 250, training loss: 88.50653839111328 = 1.0986114740371704 + 10.0 * 8.740793228149414
Epoch 250, val loss: 1.098612904548645
Epoch 260, training loss: 88.48320770263672 = 1.0986114740371704 + 10.0 * 8.738459587097168
Epoch 260, val loss: 1.098612904548645
Epoch 270, training loss: 88.42989349365234 = 1.0986114740371704 + 10.0 * 8.733128547668457
Epoch 270, val loss: 1.098612904548645
Epoch 280, training loss: 88.41919708251953 = 1.0986114740371704 + 10.0 * 8.73205852508545
Epoch 280, val loss: 1.098612904548645
Epoch 290, training loss: 88.48977661132812 = 1.0986114740371704 + 10.0 * 8.739116668701172
Epoch 290, val loss: 1.098612904548645
Epoch 300, training loss: 88.42635345458984 = 1.0986114740371704 + 10.0 * 8.73277473449707
Epoch 300, val loss: 1.098612904548645
Epoch 310, training loss: 88.35494232177734 = 1.0986114740371704 + 10.0 * 8.72563362121582
Epoch 310, val loss: 1.098612904548645
Epoch 320, training loss: 88.37926483154297 = 1.0986114740371704 + 10.0 * 8.728065490722656
Epoch 320, val loss: 1.098612904548645
Epoch 330, training loss: 88.39112091064453 = 1.0986114740371704 + 10.0 * 8.72925090789795
Epoch 330, val loss: 1.098612904548645
Epoch 340, training loss: 88.37362670898438 = 1.0986114740371704 + 10.0 * 8.72750186920166
Epoch 340, val loss: 1.098612904548645
Epoch 350, training loss: 88.3992691040039 = 1.0986114740371704 + 10.0 * 8.730066299438477
Epoch 350, val loss: 1.098612904548645
Epoch 360, training loss: 88.42101287841797 = 1.0986114740371704 + 10.0 * 8.732240676879883
Epoch 360, val loss: 1.098612904548645
Epoch 370, training loss: 88.40087127685547 = 1.0986114740371704 + 10.0 * 8.730226516723633
Epoch 370, val loss: 1.098612904548645
Epoch 380, training loss: 88.39349365234375 = 1.0986114740371704 + 10.0 * 8.729488372802734
Epoch 380, val loss: 1.098612904548645
Epoch 390, training loss: 88.43499755859375 = 1.0986114740371704 + 10.0 * 8.733638763427734
Epoch 390, val loss: 1.098612904548645
Epoch 400, training loss: 88.4092025756836 = 1.0986114740371704 + 10.0 * 8.731059074401855
Epoch 400, val loss: 1.098612904548645
Epoch 410, training loss: 88.42472076416016 = 1.0986114740371704 + 10.0 * 8.732610702514648
Epoch 410, val loss: 1.098612904548645
Epoch 420, training loss: 88.4964599609375 = 1.0986114740371704 + 10.0 * 8.739785194396973
Epoch 420, val loss: 1.098612904548645
Epoch 430, training loss: 88.5007095336914 = 1.0986114740371704 + 10.0 * 8.740209579467773
Epoch 430, val loss: 1.098612904548645
Epoch 440, training loss: 88.47620391845703 = 1.0986114740371704 + 10.0 * 8.737759590148926
Epoch 440, val loss: 1.098612904548645
Epoch 450, training loss: 88.47014617919922 = 1.0986114740371704 + 10.0 * 8.737154006958008
Epoch 450, val loss: 1.098612904548645
Epoch 460, training loss: 88.52666473388672 = 1.0986114740371704 + 10.0 * 8.742805480957031
Epoch 460, val loss: 1.098612904548645
Epoch 470, training loss: 88.60489654541016 = 1.0986114740371704 + 10.0 * 8.750628471374512
Epoch 470, val loss: 1.098612904548645
Epoch 480, training loss: 88.54450225830078 = 1.0986114740371704 + 10.0 * 8.744588851928711
Epoch 480, val loss: 1.098612904548645
Epoch 490, training loss: 88.59583282470703 = 1.0986114740371704 + 10.0 * 8.749722480773926
Epoch 490, val loss: 1.098612904548645
Epoch 500, training loss: 88.6397705078125 = 1.0986114740371704 + 10.0 * 8.75411605834961
Epoch 500, val loss: 1.098612904548645
Epoch 510, training loss: 88.5856704711914 = 1.0986114740371704 + 10.0 * 8.748705863952637
Epoch 510, val loss: 1.098612904548645
Epoch 520, training loss: 88.6206283569336 = 1.0986114740371704 + 10.0 * 8.752202033996582
Epoch 520, val loss: 1.098612904548645
Epoch 530, training loss: 88.65705108642578 = 1.0986114740371704 + 10.0 * 8.755844116210938
Epoch 530, val loss: 1.098612904548645
Epoch 540, training loss: 88.71483612060547 = 1.0986114740371704 + 10.0 * 8.761622428894043
Epoch 540, val loss: 1.098612904548645
Epoch 550, training loss: 88.73284149169922 = 1.0986114740371704 + 10.0 * 8.763422966003418
Epoch 550, val loss: 1.098612904548645
Epoch 560, training loss: 88.71526336669922 = 1.0986114740371704 + 10.0 * 8.761665344238281
Epoch 560, val loss: 1.098612904548645
Epoch 570, training loss: 88.73517608642578 = 1.0986114740371704 + 10.0 * 8.763656616210938
Epoch 570, val loss: 1.098612904548645
Epoch 580, training loss: 88.80204010009766 = 1.0986114740371704 + 10.0 * 8.770342826843262
Epoch 580, val loss: 1.098612904548645
Epoch 590, training loss: 88.81285858154297 = 1.0986114740371704 + 10.0 * 8.771425247192383
Epoch 590, val loss: 1.098612904548645
Epoch 600, training loss: 88.79800415039062 = 1.0986114740371704 + 10.0 * 8.769939422607422
Epoch 600, val loss: 1.098612904548645
Epoch 610, training loss: 88.7999038696289 = 1.0986114740371704 + 10.0 * 8.770129203796387
Epoch 610, val loss: 1.098612904548645
Epoch 620, training loss: 88.85350799560547 = 1.0986114740371704 + 10.0 * 8.775489807128906
Epoch 620, val loss: 1.098612904548645
Epoch 630, training loss: 88.88639831542969 = 1.0986114740371704 + 10.0 * 8.778779029846191
Epoch 630, val loss: 1.098612904548645
Epoch 640, training loss: 88.93231964111328 = 1.0986114740371704 + 10.0 * 8.783370971679688
Epoch 640, val loss: 1.098612904548645
Epoch 650, training loss: 88.93598937988281 = 1.0986114740371704 + 10.0 * 8.783738136291504
Epoch 650, val loss: 1.098612904548645
Epoch 660, training loss: 88.9318618774414 = 1.0986114740371704 + 10.0 * 8.7833251953125
Epoch 660, val loss: 1.098612904548645
Epoch 670, training loss: 88.89566040039062 = 1.0986114740371704 + 10.0 * 8.779705047607422
Epoch 670, val loss: 1.098612904548645
Epoch 680, training loss: 88.91570281982422 = 1.0986114740371704 + 10.0 * 8.781709671020508
Epoch 680, val loss: 1.098612904548645
Epoch 690, training loss: 88.91751861572266 = 1.0986114740371704 + 10.0 * 8.781890869140625
Epoch 690, val loss: 1.098612904548645
Epoch 700, training loss: 88.97628021240234 = 1.0986114740371704 + 10.0 * 8.78776741027832
Epoch 700, val loss: 1.098612904548645
Epoch 710, training loss: 89.0260009765625 = 1.0986114740371704 + 10.0 * 8.792738914489746
Epoch 710, val loss: 1.098612904548645
Epoch 720, training loss: 89.07555389404297 = 1.0986114740371704 + 10.0 * 8.797694206237793
Epoch 720, val loss: 1.098612904548645
Epoch 730, training loss: 89.0945816040039 = 1.0986114740371704 + 10.0 * 8.799596786499023
Epoch 730, val loss: 1.098612904548645
Epoch 740, training loss: 89.16504669189453 = 1.0986114740371704 + 10.0 * 8.80664348602295
Epoch 740, val loss: 1.098612904548645
Epoch 750, training loss: 89.14551544189453 = 1.0986114740371704 + 10.0 * 8.80469036102295
Epoch 750, val loss: 1.098612904548645
Epoch 760, training loss: 89.1141128540039 = 1.0986114740371704 + 10.0 * 8.801549911499023
Epoch 760, val loss: 1.098612904548645
Epoch 770, training loss: 89.17430877685547 = 1.0986114740371704 + 10.0 * 8.80756950378418
Epoch 770, val loss: 1.098612904548645
Epoch 780, training loss: 89.22909545898438 = 1.0986114740371704 + 10.0 * 8.813048362731934
Epoch 780, val loss: 1.098612904548645
Epoch 790, training loss: 89.27796936035156 = 1.0986114740371704 + 10.0 * 8.817935943603516
Epoch 790, val loss: 1.098612904548645
Epoch 800, training loss: 89.30521392822266 = 1.0986114740371704 + 10.0 * 8.820660591125488
Epoch 800, val loss: 1.098612904548645
Epoch 810, training loss: 89.33493041992188 = 1.0986114740371704 + 10.0 * 8.82363224029541
Epoch 810, val loss: 1.098612904548645
Epoch 820, training loss: 89.38398742675781 = 1.0986114740371704 + 10.0 * 8.828537940979004
Epoch 820, val loss: 1.098612904548645
Epoch 830, training loss: 89.44550323486328 = 1.0986114740371704 + 10.0 * 8.834689140319824
Epoch 830, val loss: 1.098612904548645
Epoch 840, training loss: 89.47071838378906 = 1.0986114740371704 + 10.0 * 8.837210655212402
Epoch 840, val loss: 1.098612904548645
Epoch 850, training loss: 89.41040802001953 = 1.0986114740371704 + 10.0 * 8.83117961883545
Epoch 850, val loss: 1.098612904548645
Epoch 860, training loss: 89.4765625 = 1.0986114740371704 + 10.0 * 8.83779525756836
Epoch 860, val loss: 1.098612904548645
Epoch 870, training loss: 89.5087890625 = 1.0986114740371704 + 10.0 * 8.841017723083496
Epoch 870, val loss: 1.098612904548645
Epoch 880, training loss: 89.5254135131836 = 1.0986114740371704 + 10.0 * 8.842679977416992
Epoch 880, val loss: 1.098612904548645
Epoch 890, training loss: 89.5680923461914 = 1.0986114740371704 + 10.0 * 8.846948623657227
Epoch 890, val loss: 1.098612904548645
Epoch 900, training loss: 89.63604736328125 = 1.0986114740371704 + 10.0 * 8.853743553161621
Epoch 900, val loss: 1.098612904548645
Epoch 910, training loss: 89.59843444824219 = 1.0986114740371704 + 10.0 * 8.849982261657715
Epoch 910, val loss: 1.098612904548645
Epoch 920, training loss: 89.63396453857422 = 1.0986114740371704 + 10.0 * 8.853535652160645
Epoch 920, val loss: 1.098612904548645
Epoch 930, training loss: 89.64330291748047 = 1.0986114740371704 + 10.0 * 8.854469299316406
Epoch 930, val loss: 1.098612904548645
Epoch 940, training loss: 89.69285583496094 = 1.0986114740371704 + 10.0 * 8.859424591064453
Epoch 940, val loss: 1.098612904548645
Epoch 950, training loss: 89.61968231201172 = 1.0986114740371704 + 10.0 * 8.852107048034668
Epoch 950, val loss: 1.098612904548645
Epoch 960, training loss: 89.64766693115234 = 1.0986114740371704 + 10.0 * 8.85490608215332
Epoch 960, val loss: 1.098612904548645
Epoch 970, training loss: 89.6880111694336 = 1.0986114740371704 + 10.0 * 8.858940124511719
Epoch 970, val loss: 1.098612904548645
Epoch 980, training loss: 89.73369598388672 = 1.0986114740371704 + 10.0 * 8.863508224487305
Epoch 980, val loss: 1.098612904548645
Epoch 990, training loss: 89.72708129882812 = 1.0986114740371704 + 10.0 * 8.862847328186035
Epoch 990, val loss: 1.098612904548645
Epoch 1000, training loss: 89.7921371459961 = 1.0986114740371704 + 10.0 * 8.869352340698242
Epoch 1000, val loss: 1.098612904548645
Epoch 1010, training loss: 89.75017547607422 = 1.0986114740371704 + 10.0 * 8.865156173706055
Epoch 1010, val loss: 1.098612904548645
Epoch 1020, training loss: 89.64734649658203 = 1.0986114740371704 + 10.0 * 8.854873657226562
Epoch 1020, val loss: 1.098612904548645
Epoch 1030, training loss: 89.7568359375 = 1.0986114740371704 + 10.0 * 8.865822792053223
Epoch 1030, val loss: 1.098612904548645
Epoch 1040, training loss: 89.82001495361328 = 1.0986114740371704 + 10.0 * 8.872140884399414
Epoch 1040, val loss: 1.098612904548645
Epoch 1050, training loss: 89.88245391845703 = 1.0986114740371704 + 10.0 * 8.878384590148926
Epoch 1050, val loss: 1.098612904548645
Epoch 1060, training loss: 89.93347930908203 = 1.0986114740371704 + 10.0 * 8.8834867477417
Epoch 1060, val loss: 1.098612904548645
Epoch 1070, training loss: 89.92880249023438 = 1.0986114740371704 + 10.0 * 8.88301944732666
Epoch 1070, val loss: 1.098612904548645
Epoch 1080, training loss: 89.96725463867188 = 1.0986114740371704 + 10.0 * 8.88686466217041
Epoch 1080, val loss: 1.098612904548645
Epoch 1090, training loss: 90.00204467773438 = 1.0986114740371704 + 10.0 * 8.89034366607666
Epoch 1090, val loss: 1.098612904548645
Epoch 1100, training loss: 89.98221588134766 = 1.0986114740371704 + 10.0 * 8.888360977172852
Epoch 1100, val loss: 1.098612904548645
Epoch 1110, training loss: 89.98234558105469 = 1.0986114740371704 + 10.0 * 8.888373374938965
Epoch 1110, val loss: 1.098612904548645
Epoch 1120, training loss: 90.0174560546875 = 1.0986114740371704 + 10.0 * 8.891884803771973
Epoch 1120, val loss: 1.098612904548645
Epoch 1130, training loss: 90.08983612060547 = 1.0986114740371704 + 10.0 * 8.89912223815918
Epoch 1130, val loss: 1.098612904548645
Epoch 1140, training loss: 90.1302261352539 = 1.0986114740371704 + 10.0 * 8.903162002563477
Epoch 1140, val loss: 1.098612904548645
Epoch 1150, training loss: 90.10830688476562 = 1.0986114740371704 + 10.0 * 8.900969505310059
Epoch 1150, val loss: 1.098612904548645
Epoch 1160, training loss: 90.05664825439453 = 1.0986114740371704 + 10.0 * 8.895803451538086
Epoch 1160, val loss: 1.098612904548645
Epoch 1170, training loss: 90.10316467285156 = 1.0986114740371704 + 10.0 * 8.900455474853516
Epoch 1170, val loss: 1.098612904548645
Epoch 1180, training loss: 90.11174011230469 = 1.0986114740371704 + 10.0 * 8.901312828063965
Epoch 1180, val loss: 1.098612904548645
Epoch 1190, training loss: 90.15125274658203 = 1.0986114740371704 + 10.0 * 8.905263900756836
Epoch 1190, val loss: 1.098612904548645
Epoch 1200, training loss: 90.20014190673828 = 1.0986114740371704 + 10.0 * 8.91015338897705
Epoch 1200, val loss: 1.098612904548645
Epoch 1210, training loss: 90.22511291503906 = 1.0986114740371704 + 10.0 * 8.912650108337402
Epoch 1210, val loss: 1.098612904548645
Epoch 1220, training loss: 90.2574691772461 = 1.0986114740371704 + 10.0 * 8.915885925292969
Epoch 1220, val loss: 1.098612904548645
Epoch 1230, training loss: 90.26422882080078 = 1.0986114740371704 + 10.0 * 8.9165620803833
Epoch 1230, val loss: 1.098612904548645
Epoch 1240, training loss: 90.2751693725586 = 1.0986114740371704 + 10.0 * 8.917655944824219
Epoch 1240, val loss: 1.098612904548645
Epoch 1250, training loss: 90.27388000488281 = 1.0986114740371704 + 10.0 * 8.917527198791504
Epoch 1250, val loss: 1.098612904548645
Epoch 1260, training loss: 90.29265594482422 = 1.0986114740371704 + 10.0 * 8.919404983520508
Epoch 1260, val loss: 1.098612904548645
Epoch 1270, training loss: 90.26203918457031 = 1.0986114740371704 + 10.0 * 8.916342735290527
Epoch 1270, val loss: 1.098612904548645
Epoch 1280, training loss: 90.2931137084961 = 1.0986114740371704 + 10.0 * 8.919450759887695
Epoch 1280, val loss: 1.098612904548645
Epoch 1290, training loss: 90.33444213867188 = 1.0986114740371704 + 10.0 * 8.923583030700684
Epoch 1290, val loss: 1.098612904548645
Epoch 1300, training loss: 90.38290405273438 = 1.0986114740371704 + 10.0 * 8.92842960357666
Epoch 1300, val loss: 1.098612904548645
Epoch 1310, training loss: 90.41454315185547 = 1.0986114740371704 + 10.0 * 8.93159294128418
Epoch 1310, val loss: 1.098612904548645
Epoch 1320, training loss: 90.36956024169922 = 1.0986114740371704 + 10.0 * 8.927095413208008
Epoch 1320, val loss: 1.098612904548645
Epoch 1330, training loss: 90.4756088256836 = 1.0986114740371704 + 10.0 * 8.937700271606445
Epoch 1330, val loss: 1.098612904548645
Epoch 1340, training loss: 90.35771942138672 = 1.0986114740371704 + 10.0 * 8.925910949707031
Epoch 1340, val loss: 1.098612904548645
Epoch 1350, training loss: 90.3963851928711 = 1.0986114740371704 + 10.0 * 8.929777145385742
Epoch 1350, val loss: 1.098612904548645
Epoch 1360, training loss: 90.48066711425781 = 1.0986114740371704 + 10.0 * 8.93820571899414
Epoch 1360, val loss: 1.098612904548645
Epoch 1370, training loss: 90.51805114746094 = 1.0986114740371704 + 10.0 * 8.941944122314453
Epoch 1370, val loss: 1.098612904548645
Epoch 1380, training loss: 90.53705596923828 = 1.0986114740371704 + 10.0 * 8.94384479522705
Epoch 1380, val loss: 1.098612904548645
Epoch 1390, training loss: 90.3360824584961 = 1.0986114740371704 + 10.0 * 8.923747062683105
Epoch 1390, val loss: 1.098612904548645
Epoch 1400, training loss: 90.38113403320312 = 1.0986114740371704 + 10.0 * 8.928252220153809
Epoch 1400, val loss: 1.098612904548645
Epoch 1410, training loss: 90.46871948242188 = 1.0986114740371704 + 10.0 * 8.937010765075684
Epoch 1410, val loss: 1.098612904548645
Epoch 1420, training loss: 90.61186218261719 = 1.0986114740371704 + 10.0 * 8.951325416564941
Epoch 1420, val loss: 1.098612904548645
Epoch 1430, training loss: 90.39053344726562 = 1.0986114740371704 + 10.0 * 8.929192543029785
Epoch 1430, val loss: 1.098612904548645
Epoch 1440, training loss: 90.478759765625 = 1.0986114740371704 + 10.0 * 8.93801498413086
Epoch 1440, val loss: 1.098612904548645
Epoch 1450, training loss: 90.49073028564453 = 1.0986114740371704 + 10.0 * 8.93921184539795
Epoch 1450, val loss: 1.098612904548645
Epoch 1460, training loss: 90.38436126708984 = 1.0986114740371704 + 10.0 * 8.92857551574707
Epoch 1460, val loss: 1.098612904548645
Epoch 1470, training loss: 90.51434326171875 = 1.0986114740371704 + 10.0 * 8.941573143005371
Epoch 1470, val loss: 1.098612904548645
Epoch 1480, training loss: 90.57771301269531 = 1.0986114740371704 + 10.0 * 8.94791030883789
Epoch 1480, val loss: 1.098612904548645
Epoch 1490, training loss: 90.61433410644531 = 1.0986114740371704 + 10.0 * 8.95157241821289
Epoch 1490, val loss: 1.098612904548645
Epoch 1500, training loss: 90.63786315917969 = 1.0986114740371704 + 10.0 * 8.953925132751465
Epoch 1500, val loss: 1.098612904548645
Epoch 1510, training loss: 90.66429901123047 = 1.0986114740371704 + 10.0 * 8.956568717956543
Epoch 1510, val loss: 1.098612904548645
Epoch 1520, training loss: 90.69258117675781 = 1.0986114740371704 + 10.0 * 8.959397315979004
Epoch 1520, val loss: 1.098612904548645
Epoch 1530, training loss: 90.71500396728516 = 1.0986114740371704 + 10.0 * 8.961639404296875
Epoch 1530, val loss: 1.098612904548645
Epoch 1540, training loss: 90.6550064086914 = 1.0986114740371704 + 10.0 * 8.955639839172363
Epoch 1540, val loss: 1.098612904548645
Epoch 1550, training loss: 90.64183807373047 = 1.0986114740371704 + 10.0 * 8.954322814941406
Epoch 1550, val loss: 1.098612904548645
Epoch 1560, training loss: 90.70927429199219 = 1.0986114740371704 + 10.0 * 8.961066246032715
Epoch 1560, val loss: 1.098612904548645
Epoch 1570, training loss: 90.8058853149414 = 1.0986114740371704 + 10.0 * 8.970727920532227
Epoch 1570, val loss: 1.098612904548645
Epoch 1580, training loss: 90.81050872802734 = 1.0986114740371704 + 10.0 * 8.971189498901367
Epoch 1580, val loss: 1.098612904548645
Epoch 1590, training loss: 90.81688690185547 = 1.0986114740371704 + 10.0 * 8.971827507019043
Epoch 1590, val loss: 1.098612904548645
Epoch 1600, training loss: 90.85784149169922 = 1.0986114740371704 + 10.0 * 8.975923538208008
Epoch 1600, val loss: 1.098612904548645
Epoch 1610, training loss: 90.88475799560547 = 1.0986114740371704 + 10.0 * 8.978614807128906
Epoch 1610, val loss: 1.098612904548645
Epoch 1620, training loss: 90.94942474365234 = 1.0986114740371704 + 10.0 * 8.985081672668457
Epoch 1620, val loss: 1.098612904548645
Epoch 1630, training loss: 90.26746368408203 = 1.0986114740371704 + 10.0 * 8.916885375976562
Epoch 1630, val loss: 1.098612904548645
Epoch 1640, training loss: 90.12957000732422 = 1.0986114740371704 + 10.0 * 8.903096199035645
Epoch 1640, val loss: 1.098612904548645
Epoch 1650, training loss: 90.31421661376953 = 1.0986114740371704 + 10.0 * 8.921560287475586
Epoch 1650, val loss: 1.098612904548645
Epoch 1660, training loss: 90.2978744506836 = 1.0986114740371704 + 10.0 * 8.919926643371582
Epoch 1660, val loss: 1.098612904548645
Epoch 1670, training loss: 90.57550811767578 = 1.0986114740371704 + 10.0 * 8.9476900100708
Epoch 1670, val loss: 1.098612904548645
Epoch 1680, training loss: 90.3991470336914 = 1.0986114740371704 + 10.0 * 8.9300537109375
Epoch 1680, val loss: 1.098612904548645
Epoch 1690, training loss: 90.47257232666016 = 1.0986114740371704 + 10.0 * 8.937396049499512
Epoch 1690, val loss: 1.098612904548645
Epoch 1700, training loss: 90.59320068359375 = 1.0986114740371704 + 10.0 * 8.949459075927734
Epoch 1700, val loss: 1.098612904548645
Epoch 1710, training loss: 90.69330596923828 = 1.0986114740371704 + 10.0 * 8.95946979522705
Epoch 1710, val loss: 1.098612904548645
Epoch 1720, training loss: 90.80259704589844 = 1.0986114740371704 + 10.0 * 8.970398902893066
Epoch 1720, val loss: 1.098612904548645
Epoch 1730, training loss: 90.86522674560547 = 1.0986114740371704 + 10.0 * 8.976661682128906
Epoch 1730, val loss: 1.098612904548645
Epoch 1740, training loss: 90.93492126464844 = 1.0986114740371704 + 10.0 * 8.983631134033203
Epoch 1740, val loss: 1.098612904548645
Epoch 1750, training loss: 90.9244613647461 = 1.0986114740371704 + 10.0 * 8.982584953308105
Epoch 1750, val loss: 1.098612904548645
Epoch 1760, training loss: 90.97471618652344 = 1.0986114740371704 + 10.0 * 8.987610816955566
Epoch 1760, val loss: 1.098612904548645
Epoch 1770, training loss: 91.03697204589844 = 1.0986114740371704 + 10.0 * 8.993836402893066
Epoch 1770, val loss: 1.098612904548645
Epoch 1780, training loss: 91.0174331665039 = 1.0986114740371704 + 10.0 * 8.99188232421875
Epoch 1780, val loss: 1.098612904548645
Epoch 1790, training loss: 91.05119323730469 = 1.0986114740371704 + 10.0 * 8.995258331298828
Epoch 1790, val loss: 1.098612904548645
Epoch 1800, training loss: 91.09190368652344 = 1.0986114740371704 + 10.0 * 8.999329566955566
Epoch 1800, val loss: 1.098612904548645
Epoch 1810, training loss: 91.09172821044922 = 1.0986114740371704 + 10.0 * 8.999311447143555
Epoch 1810, val loss: 1.098612904548645
Epoch 1820, training loss: 91.09464263916016 = 1.0986114740371704 + 10.0 * 8.999603271484375
Epoch 1820, val loss: 1.098612904548645
Epoch 1830, training loss: 91.15015411376953 = 1.0986114740371704 + 10.0 * 9.005154609680176
Epoch 1830, val loss: 1.098612904548645
Epoch 1840, training loss: 91.18180847167969 = 1.0986114740371704 + 10.0 * 9.008319854736328
Epoch 1840, val loss: 1.098612904548645
Epoch 1850, training loss: 91.1157455444336 = 1.0986114740371704 + 10.0 * 9.001713752746582
Epoch 1850, val loss: 1.098612904548645
Epoch 1860, training loss: 91.0228500366211 = 1.0986114740371704 + 10.0 * 8.992424011230469
Epoch 1860, val loss: 1.098612904548645
Epoch 1870, training loss: 91.10633087158203 = 1.0986114740371704 + 10.0 * 9.000772476196289
Epoch 1870, val loss: 1.098612904548645
Epoch 1880, training loss: 91.17818450927734 = 1.0986114740371704 + 10.0 * 9.007957458496094
Epoch 1880, val loss: 1.098612904548645
Epoch 1890, training loss: 91.25206756591797 = 1.0986114740371704 + 10.0 * 9.015345573425293
Epoch 1890, val loss: 1.098612904548645
Epoch 1900, training loss: 91.2966079711914 = 1.0986114740371704 + 10.0 * 9.019800186157227
Epoch 1900, val loss: 1.098612904548645
Epoch 1910, training loss: 90.03795623779297 = 1.0986114740371704 + 10.0 * 8.89393424987793
Epoch 1910, val loss: 1.098612904548645
Epoch 1920, training loss: 90.77366638183594 = 1.0986114740371704 + 10.0 * 8.96750545501709
Epoch 1920, val loss: 1.098612904548645
Epoch 1930, training loss: 90.34467315673828 = 1.0986114740371704 + 10.0 * 8.924606323242188
Epoch 1930, val loss: 1.098612904548645
Epoch 1940, training loss: 90.57199096679688 = 1.0986114740371704 + 10.0 * 8.947338104248047
Epoch 1940, val loss: 1.098612904548645
Epoch 1950, training loss: 90.71839141845703 = 1.0986114740371704 + 10.0 * 8.9619779586792
Epoch 1950, val loss: 1.098612904548645
Epoch 1960, training loss: 90.83991241455078 = 1.0986114740371704 + 10.0 * 8.974130630493164
Epoch 1960, val loss: 1.098612904548645
Epoch 1970, training loss: 90.83363342285156 = 1.0986114740371704 + 10.0 * 8.973502159118652
Epoch 1970, val loss: 1.098612904548645
Epoch 1980, training loss: 90.99054718017578 = 1.0986114740371704 + 10.0 * 8.9891939163208
Epoch 1980, val loss: 1.098612904548645
Epoch 1990, training loss: 91.08589935302734 = 1.0986114740371704 + 10.0 * 8.99872875213623
Epoch 1990, val loss: 1.098612904548645
Epoch 2000, training loss: 91.15261840820312 = 1.0986114740371704 + 10.0 * 9.005400657653809
Epoch 2000, val loss: 1.098612904548645
Epoch 2010, training loss: 91.1866455078125 = 1.0986114740371704 + 10.0 * 9.008803367614746
Epoch 2010, val loss: 1.098612904548645
Epoch 2020, training loss: 91.22410583496094 = 1.0986114740371704 + 10.0 * 9.01254940032959
Epoch 2020, val loss: 1.098612904548645
Epoch 2030, training loss: 91.30204772949219 = 1.0986114740371704 + 10.0 * 9.020343780517578
Epoch 2030, val loss: 1.098612904548645
Epoch 2040, training loss: 91.29849243164062 = 1.0986114740371704 + 10.0 * 9.019988059997559
Epoch 2040, val loss: 1.098612904548645
Epoch 2050, training loss: 91.33153533935547 = 1.0986114740371704 + 10.0 * 9.023292541503906
Epoch 2050, val loss: 1.098612904548645
Epoch 2060, training loss: 91.38584899902344 = 1.0986114740371704 + 10.0 * 9.02872371673584
Epoch 2060, val loss: 1.098612904548645
Epoch 2070, training loss: 91.37129211425781 = 1.0986114740371704 + 10.0 * 9.027268409729004
Epoch 2070, val loss: 1.098612904548645
Epoch 2080, training loss: 91.3861312866211 = 1.0986114740371704 + 10.0 * 9.028752326965332
Epoch 2080, val loss: 1.098612904548645
Epoch 2090, training loss: 91.40050506591797 = 1.0986114740371704 + 10.0 * 9.030189514160156
Epoch 2090, val loss: 1.098612904548645
Epoch 2100, training loss: 91.43582153320312 = 1.0986114740371704 + 10.0 * 9.033720970153809
Epoch 2100, val loss: 1.098612904548645
Epoch 2110, training loss: 91.43182373046875 = 1.0986114740371704 + 10.0 * 9.033321380615234
Epoch 2110, val loss: 1.098612904548645
Epoch 2120, training loss: 91.4941635131836 = 1.0986114740371704 + 10.0 * 9.039555549621582
Epoch 2120, val loss: 1.098612904548645
Epoch 2130, training loss: 91.42633819580078 = 1.0986114740371704 + 10.0 * 9.0327730178833
Epoch 2130, val loss: 1.098612904548645
Epoch 2140, training loss: 91.37989044189453 = 1.0986114740371704 + 10.0 * 9.028127670288086
Epoch 2140, val loss: 1.098612904548645
Epoch 2150, training loss: 91.4432373046875 = 1.0986114740371704 + 10.0 * 9.034462928771973
Epoch 2150, val loss: 1.098612904548645
Epoch 2160, training loss: 91.51188659667969 = 1.0986114740371704 + 10.0 * 9.041327476501465
Epoch 2160, val loss: 1.098612904548645
Epoch 2170, training loss: 91.51517486572266 = 1.0985643863677979 + 10.0 * 9.041661262512207
Epoch 2170, val loss: 1.0985279083251953
Epoch 2180, training loss: 91.52885437011719 = 1.097733974456787 + 10.0 * 9.043111801147461
Epoch 2180, val loss: 1.0977516174316406
Epoch 2190, training loss: 91.57093811035156 = 1.0967844724655151 + 10.0 * 9.047414779663086
Epoch 2190, val loss: 1.0969219207763672
Epoch 2200, training loss: 91.56365966796875 = 1.095901370048523 + 10.0 * 9.046775817871094
Epoch 2200, val loss: 1.0961472988128662
Epoch 2210, training loss: 91.57420349121094 = 1.0950415134429932 + 10.0 * 9.047916412353516
Epoch 2210, val loss: 1.0953832864761353
Epoch 2220, training loss: 91.58280181884766 = 1.0941684246063232 + 10.0 * 9.048863410949707
Epoch 2220, val loss: 1.0946019887924194
Epoch 2230, training loss: 91.59952545166016 = 1.0932669639587402 + 10.0 * 9.050625801086426
Epoch 2230, val loss: 1.0937882661819458
Epoch 2240, training loss: 91.60679626464844 = 1.0923272371292114 + 10.0 * 9.051446914672852
Epoch 2240, val loss: 1.0929341316223145
Epoch 2250, training loss: 91.60214233398438 = 1.0913642644882202 + 10.0 * 9.051077842712402
Epoch 2250, val loss: 1.092053771018982
Epoch 2260, training loss: 91.63780975341797 = 1.0903902053833008 + 10.0 * 9.054741859436035
Epoch 2260, val loss: 1.0911606550216675
Epoch 2270, training loss: 91.63880157470703 = 1.0894019603729248 + 10.0 * 9.054940223693848
Epoch 2270, val loss: 1.0902546644210815
Epoch 2280, training loss: 91.6597671508789 = 1.0884063243865967 + 10.0 * 9.057135581970215
Epoch 2280, val loss: 1.0893431901931763
Epoch 2290, training loss: 91.62525939941406 = 1.0873966217041016 + 10.0 * 9.053786277770996
Epoch 2290, val loss: 1.0884127616882324
Epoch 2300, training loss: 91.66395568847656 = 1.0863733291625977 + 10.0 * 9.057758331298828
Epoch 2300, val loss: 1.0874639749526978
Epoch 2310, training loss: 91.66686248779297 = 1.0853418111801147 + 10.0 * 9.058152198791504
Epoch 2310, val loss: 1.086512804031372
Epoch 2320, training loss: 91.73257446289062 = 1.0842974185943604 + 10.0 * 9.064827919006348
Epoch 2320, val loss: 1.085545301437378
Epoch 2330, training loss: 91.70039367675781 = 1.083236575126648 + 10.0 * 9.061716079711914
Epoch 2330, val loss: 1.0845619440078735
Epoch 2340, training loss: 91.67950439453125 = 1.0821638107299805 + 10.0 * 9.059734344482422
Epoch 2340, val loss: 1.083571434020996
Epoch 2350, training loss: 91.73477172851562 = 1.0810482501983643 + 10.0 * 9.065372467041016
Epoch 2350, val loss: 1.0825321674346924
Epoch 2360, training loss: 91.76560974121094 = 1.07988440990448 + 10.0 * 9.068572044372559
Epoch 2360, val loss: 1.0814424753189087
Epoch 2370, training loss: 91.7514877319336 = 1.0786678791046143 + 10.0 * 9.067281723022461
Epoch 2370, val loss: 1.0803031921386719
Epoch 2380, training loss: 91.78419494628906 = 1.0774046182632446 + 10.0 * 9.0706787109375
Epoch 2380, val loss: 1.0791281461715698
Epoch 2390, training loss: 91.79178619384766 = 1.0761091709136963 + 10.0 * 9.07156753540039
Epoch 2390, val loss: 1.0779316425323486
Epoch 2400, training loss: 91.7822494506836 = 1.0747824907302856 + 10.0 * 9.070746421813965
Epoch 2400, val loss: 1.0767005681991577
Epoch 2410, training loss: 91.8053970336914 = 1.0734328031539917 + 10.0 * 9.073196411132812
Epoch 2410, val loss: 1.0754594802856445
Epoch 2420, training loss: 91.80633544921875 = 1.072049617767334 + 10.0 * 9.0734281539917
Epoch 2420, val loss: 1.0741926431655884
Epoch 2430, training loss: 91.7363052368164 = 1.0706250667572021 + 10.0 * 9.066568374633789
Epoch 2430, val loss: 1.072872519493103
Epoch 2440, training loss: 91.59937286376953 = 1.0691667795181274 + 10.0 * 9.053020477294922
Epoch 2440, val loss: 1.0715365409851074
Epoch 2450, training loss: 91.47240447998047 = 1.0677145719528198 + 10.0 * 9.0404691696167
Epoch 2450, val loss: 1.0701662302017212
Epoch 2460, training loss: 91.50566101074219 = 1.0662370920181274 + 10.0 * 9.04394245147705
Epoch 2460, val loss: 1.0688202381134033
Epoch 2470, training loss: 91.57207489013672 = 1.0647608041763306 + 10.0 * 9.050731658935547
Epoch 2470, val loss: 1.0674504041671753
Epoch 2480, training loss: 91.64559936523438 = 1.063251256942749 + 10.0 * 9.058235168457031
Epoch 2480, val loss: 1.0660475492477417
Epoch 2490, training loss: 91.73709106445312 = 1.0617254972457886 + 10.0 * 9.067537307739258
Epoch 2490, val loss: 1.0646318197250366
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.4181159420289855
0.863507933058031
=== training gcn model ===
Epoch 0, training loss: 104.06484985351562 = 1.1092174053192139 + 10.0 * 10.295563697814941
Epoch 0, val loss: 1.1092016696929932
Epoch 10, training loss: 99.83718872070312 = 1.1089991331100464 + 10.0 * 9.872818946838379
Epoch 10, val loss: 1.1089603900909424
Epoch 20, training loss: 97.45342254638672 = 1.108711838722229 + 10.0 * 9.63447093963623
Epoch 20, val loss: 1.1086740493774414
Epoch 30, training loss: 95.94065856933594 = 1.1083558797836304 + 10.0 * 9.483230590820312
Epoch 30, val loss: 1.1083084344863892
Epoch 40, training loss: 94.75315856933594 = 1.1079899072647095 + 10.0 * 9.364517211914062
Epoch 40, val loss: 1.1079485416412354
Epoch 50, training loss: 93.81038665771484 = 1.1076252460479736 + 10.0 * 9.270276069641113
Epoch 50, val loss: 1.10758376121521
Epoch 60, training loss: 93.03499603271484 = 1.1072496175765991 + 10.0 * 9.192774772644043
Epoch 60, val loss: 1.1072090864181519
Epoch 70, training loss: 92.37261199951172 = 1.10688054561615 + 10.0 * 9.12657356262207
Epoch 70, val loss: 1.1068447828292847
Epoch 80, training loss: 91.80451202392578 = 1.1064918041229248 + 10.0 * 9.069802284240723
Epoch 80, val loss: 1.1064566373825073
Epoch 90, training loss: 91.31253051757812 = 1.106109380722046 + 10.0 * 9.020642280578613
Epoch 90, val loss: 1.1060794591903687
Epoch 100, training loss: 90.88638305664062 = 1.1057220697402954 + 10.0 * 8.978066444396973
Epoch 100, val loss: 1.1056913137435913
Epoch 110, training loss: 90.51952362060547 = 1.1053268909454346 + 10.0 * 8.94141960144043
Epoch 110, val loss: 1.1052976846694946
Epoch 120, training loss: 90.20486450195312 = 1.1049166917800903 + 10.0 * 8.909994125366211
Epoch 120, val loss: 1.1048951148986816
Epoch 130, training loss: 89.93940734863281 = 1.1045031547546387 + 10.0 * 8.883490562438965
Epoch 130, val loss: 1.1044816970825195
Epoch 140, training loss: 89.69010925292969 = 1.1040678024291992 + 10.0 * 8.858604431152344
Epoch 140, val loss: 1.1040507555007935
Epoch 150, training loss: 89.4834976196289 = 1.1036055088043213 + 10.0 * 8.83798885345459
Epoch 150, val loss: 1.1035921573638916
Epoch 160, training loss: 89.27783203125 = 1.1031363010406494 + 10.0 * 8.817469596862793
Epoch 160, val loss: 1.1031250953674316
Epoch 170, training loss: 89.10975646972656 = 1.1026438474655151 + 10.0 * 8.800710678100586
Epoch 170, val loss: 1.1026413440704346
Epoch 180, training loss: 88.94557189941406 = 1.1021347045898438 + 10.0 * 8.784343719482422
Epoch 180, val loss: 1.10213303565979
Epoch 190, training loss: 88.79356384277344 = 1.101606011390686 + 10.0 * 8.769195556640625
Epoch 190, val loss: 1.101610541343689
Epoch 200, training loss: 88.67996215820312 = 1.1010839939117432 + 10.0 * 8.757887840270996
Epoch 200, val loss: 1.1010876893997192
Epoch 210, training loss: 88.55671691894531 = 1.100540280342102 + 10.0 * 8.745617866516113
Epoch 210, val loss: 1.1005537509918213
Epoch 220, training loss: 88.48995208740234 = 1.099976658821106 + 10.0 * 8.738997459411621
Epoch 220, val loss: 1.0999971628189087
Epoch 230, training loss: 88.39556884765625 = 1.0994582176208496 + 10.0 * 8.72961139678955
Epoch 230, val loss: 1.0994819402694702
Epoch 240, training loss: 88.26924896240234 = 1.098915696144104 + 10.0 * 8.717033386230469
Epoch 240, val loss: 1.0989384651184082
Epoch 250, training loss: 88.20193481445312 = 1.0984280109405518 + 10.0 * 8.71035099029541
Epoch 250, val loss: 1.0984470844268799
Epoch 260, training loss: 88.13591003417969 = 1.0979549884796143 + 10.0 * 8.703795433044434
Epoch 260, val loss: 1.0979820489883423
Epoch 270, training loss: 88.09503173828125 = 1.0975514650344849 + 10.0 * 8.699748039245605
Epoch 270, val loss: 1.0975905656814575
Epoch 280, training loss: 88.07049560546875 = 1.09721839427948 + 10.0 * 8.697327613830566
Epoch 280, val loss: 1.097262978553772
Epoch 290, training loss: 88.10760498046875 = 1.0969427824020386 + 10.0 * 8.701066970825195
Epoch 290, val loss: 1.097006916999817
Epoch 300, training loss: 88.00460815429688 = 1.09671151638031 + 10.0 * 8.690790176391602
Epoch 300, val loss: 1.096778392791748
Epoch 310, training loss: 88.02931213378906 = 1.0965389013290405 + 10.0 * 8.693277359008789
Epoch 310, val loss: 1.0966140031814575
Epoch 320, training loss: 87.9852294921875 = 1.096373438835144 + 10.0 * 8.688885688781738
Epoch 320, val loss: 1.096456527709961
Epoch 330, training loss: 87.98451232910156 = 1.0962287187576294 + 10.0 * 8.688828468322754
Epoch 330, val loss: 1.0963221788406372
Epoch 340, training loss: 87.99551391601562 = 1.0961084365844727 + 10.0 * 8.689940452575684
Epoch 340, val loss: 1.0962015390396118
Epoch 350, training loss: 87.97038269042969 = 1.0959765911102295 + 10.0 * 8.687440872192383
Epoch 350, val loss: 1.0960769653320312
Epoch 360, training loss: 87.97785949707031 = 1.0958517789840698 + 10.0 * 8.688200950622559
Epoch 360, val loss: 1.0959571599960327
Epoch 370, training loss: 87.97212982177734 = 1.095729112625122 + 10.0 * 8.687640190124512
Epoch 370, val loss: 1.0958408117294312
Epoch 380, training loss: 87.97906494140625 = 1.0956133604049683 + 10.0 * 8.688344955444336
Epoch 380, val loss: 1.0957298278808594
Epoch 390, training loss: 87.98059844970703 = 1.0954912900924683 + 10.0 * 8.68851089477539
Epoch 390, val loss: 1.0956097841262817
Epoch 400, training loss: 87.99044036865234 = 1.0953677892684937 + 10.0 * 8.689507484436035
Epoch 400, val loss: 1.0954890251159668
Epoch 410, training loss: 87.97977447509766 = 1.095232605934143 + 10.0 * 8.688454627990723
Epoch 410, val loss: 1.0953824520111084
Epoch 420, training loss: 88.07404327392578 = 1.0951650142669678 + 10.0 * 8.697888374328613
Epoch 420, val loss: 1.0953128337860107
Epoch 430, training loss: 88.04413604736328 = 1.0950132608413696 + 10.0 * 8.69491195678711
Epoch 430, val loss: 1.0951507091522217
Epoch 440, training loss: 88.0681381225586 = 1.094880223274231 + 10.0 * 8.697325706481934
Epoch 440, val loss: 1.0950242280960083
Epoch 450, training loss: 88.11491394042969 = 1.0947352647781372 + 10.0 * 8.702017784118652
Epoch 450, val loss: 1.0948857069015503
Epoch 460, training loss: 88.03572845458984 = 1.0945863723754883 + 10.0 * 8.694113731384277
Epoch 460, val loss: 1.0947381258010864
Epoch 470, training loss: 88.0432357788086 = 1.0944552421569824 + 10.0 * 8.694878578186035
Epoch 470, val loss: 1.094612717628479
Epoch 480, training loss: 88.05254364013672 = 1.094312071800232 + 10.0 * 8.695822715759277
Epoch 480, val loss: 1.0944818258285522
Epoch 490, training loss: 88.11276245117188 = 1.0941720008850098 + 10.0 * 8.701859474182129
Epoch 490, val loss: 1.0943481922149658
Epoch 500, training loss: 88.162353515625 = 1.0940330028533936 + 10.0 * 8.706831932067871
Epoch 500, val loss: 1.0942143201828003
Epoch 510, training loss: 88.08829498291016 = 1.0938633680343628 + 10.0 * 8.699442863464355
Epoch 510, val loss: 1.0940579175949097
Epoch 520, training loss: 88.08065795898438 = 1.0937232971191406 + 10.0 * 8.698694229125977
Epoch 520, val loss: 1.0939208269119263
Epoch 530, training loss: 88.12046813964844 = 1.093577265739441 + 10.0 * 8.702689170837402
Epoch 530, val loss: 1.0937800407409668
Epoch 540, training loss: 88.18714904785156 = 1.093420147895813 + 10.0 * 8.709372520446777
Epoch 540, val loss: 1.0936367511749268
Epoch 550, training loss: 88.22605895996094 = 1.0932643413543701 + 10.0 * 8.713279724121094
Epoch 550, val loss: 1.093483805656433
Epoch 560, training loss: 88.15159606933594 = 1.0930801630020142 + 10.0 * 8.705851554870605
Epoch 560, val loss: 1.0933074951171875
Epoch 570, training loss: 88.30103302001953 = 1.0929243564605713 + 10.0 * 8.720810890197754
Epoch 570, val loss: 1.0931586027145386
Epoch 580, training loss: 88.20464324951172 = 1.0927456617355347 + 10.0 * 8.711190223693848
Epoch 580, val loss: 1.0930002927780151
Epoch 590, training loss: 88.23197174072266 = 1.092583179473877 + 10.0 * 8.71393871307373
Epoch 590, val loss: 1.0928412675857544
Epoch 600, training loss: 88.28356170654297 = 1.0924140214920044 + 10.0 * 8.719114303588867
Epoch 600, val loss: 1.0926786661148071
Epoch 610, training loss: 88.32769775390625 = 1.0922247171401978 + 10.0 * 8.723546981811523
Epoch 610, val loss: 1.0924968719482422
Epoch 620, training loss: 88.28223419189453 = 1.0920265913009644 + 10.0 * 8.71902084350586
Epoch 620, val loss: 1.092315435409546
Epoch 630, training loss: 88.31779479980469 = 1.0918406248092651 + 10.0 * 8.72259521484375
Epoch 630, val loss: 1.0921400785446167
Epoch 640, training loss: 88.41674041748047 = 1.0916547775268555 + 10.0 * 8.732508659362793
Epoch 640, val loss: 1.0919625759124756
Epoch 650, training loss: 88.44076538085938 = 1.091455101966858 + 10.0 * 8.734930992126465
Epoch 650, val loss: 1.0917719602584839
Epoch 660, training loss: 88.44225311279297 = 1.0912562608718872 + 10.0 * 8.735099792480469
Epoch 660, val loss: 1.091580867767334
Epoch 670, training loss: 88.45921325683594 = 1.091049313545227 + 10.0 * 8.73681640625
Epoch 670, val loss: 1.0913825035095215
Epoch 680, training loss: 88.43189239501953 = 1.0908310413360596 + 10.0 * 8.734106063842773
Epoch 680, val loss: 1.0911790132522583
Epoch 690, training loss: 88.54613494873047 = 1.0906152725219727 + 10.0 * 8.745552062988281
Epoch 690, val loss: 1.0909699201583862
Epoch 700, training loss: 88.52582550048828 = 1.0903977155685425 + 10.0 * 8.743542671203613
Epoch 700, val loss: 1.0907613039016724
Epoch 710, training loss: 88.5754165649414 = 1.09018874168396 + 10.0 * 8.748522758483887
Epoch 710, val loss: 1.0905673503875732
Epoch 720, training loss: 88.62804412841797 = 1.0899642705917358 + 10.0 * 8.75380802154541
Epoch 720, val loss: 1.090354561805725
Epoch 730, training loss: 88.65606689453125 = 1.0897362232208252 + 10.0 * 8.756632804870605
Epoch 730, val loss: 1.0901376008987427
Epoch 740, training loss: 88.68739318847656 = 1.0895076990127563 + 10.0 * 8.759788513183594
Epoch 740, val loss: 1.089919090270996
Epoch 750, training loss: 88.6936264038086 = 1.0892727375030518 + 10.0 * 8.760435104370117
Epoch 750, val loss: 1.0896954536437988
Epoch 760, training loss: 88.73866271972656 = 1.0890345573425293 + 10.0 * 8.764963150024414
Epoch 760, val loss: 1.0894678831100464
Epoch 770, training loss: 88.70704650878906 = 1.0887932777404785 + 10.0 * 8.761825561523438
Epoch 770, val loss: 1.0892384052276611
Epoch 780, training loss: 88.77903747558594 = 1.088533878326416 + 10.0 * 8.769050598144531
Epoch 780, val loss: 1.0889906883239746
Epoch 790, training loss: 88.79875946044922 = 1.0882941484451294 + 10.0 * 8.77104663848877
Epoch 790, val loss: 1.088768482208252
Epoch 800, training loss: 88.78038787841797 = 1.0880348682403564 + 10.0 * 8.769235610961914
Epoch 800, val loss: 1.0885142087936401
Epoch 810, training loss: 88.8295669555664 = 1.0877732038497925 + 10.0 * 8.774179458618164
Epoch 810, val loss: 1.0882712602615356
Epoch 820, training loss: 88.9111557006836 = 1.087515115737915 + 10.0 * 8.782363891601562
Epoch 820, val loss: 1.0880229473114014
Epoch 830, training loss: 88.87722778320312 = 1.087230920791626 + 10.0 * 8.778999328613281
Epoch 830, val loss: 1.0877487659454346
Epoch 840, training loss: 88.94276428222656 = 1.0869618654251099 + 10.0 * 8.7855806350708
Epoch 840, val loss: 1.0874929428100586
Epoch 850, training loss: 88.97599792480469 = 1.0866880416870117 + 10.0 * 8.788930892944336
Epoch 850, val loss: 1.0872347354888916
Epoch 860, training loss: 89.02362823486328 = 1.0864038467407227 + 10.0 * 8.793722152709961
Epoch 860, val loss: 1.0869654417037964
Epoch 870, training loss: 89.04523468017578 = 1.0861254930496216 + 10.0 * 8.795910835266113
Epoch 870, val loss: 1.0866972208023071
Epoch 880, training loss: 88.8063735961914 = 1.0857738256454468 + 10.0 * 8.772059440612793
Epoch 880, val loss: 1.0863791704177856
Epoch 890, training loss: 89.16826629638672 = 1.0855679512023926 + 10.0 * 8.808269500732422
Epoch 890, val loss: 1.086155652999878
Epoch 900, training loss: 89.10823822021484 = 1.0852617025375366 + 10.0 * 8.802297592163086
Epoch 900, val loss: 1.0858713388442993
Epoch 910, training loss: 89.17597961425781 = 1.0849456787109375 + 10.0 * 8.809103012084961
Epoch 910, val loss: 1.085565447807312
Epoch 920, training loss: 89.25869750976562 = 1.0846447944641113 + 10.0 * 8.817404747009277
Epoch 920, val loss: 1.0852882862091064
Epoch 930, training loss: 89.31578826904297 = 1.0843459367752075 + 10.0 * 8.823144912719727
Epoch 930, val loss: 1.085003137588501
Epoch 940, training loss: 89.36891174316406 = 1.0840380191802979 + 10.0 * 8.828487396240234
Epoch 940, val loss: 1.0847097635269165
Epoch 950, training loss: 89.4354476928711 = 1.0837302207946777 + 10.0 * 8.835171699523926
Epoch 950, val loss: 1.0844157934188843
Epoch 960, training loss: 89.46469116210938 = 1.083412528038025 + 10.0 * 8.838128089904785
Epoch 960, val loss: 1.0841158628463745
Epoch 970, training loss: 89.52796173095703 = 1.0830940008163452 + 10.0 * 8.844487190246582
Epoch 970, val loss: 1.083807349205017
Epoch 980, training loss: 89.57538604736328 = 1.082763433456421 + 10.0 * 8.849262237548828
Epoch 980, val loss: 1.083496332168579
Epoch 990, training loss: 89.45884704589844 = 1.0824159383773804 + 10.0 * 8.83764362335205
Epoch 990, val loss: 1.083172082901001
Epoch 1000, training loss: 89.56289672851562 = 1.0820894241333008 + 10.0 * 8.8480806350708
Epoch 1000, val loss: 1.082862138748169
Epoch 1010, training loss: 89.62432098388672 = 1.081765055656433 + 10.0 * 8.854255676269531
Epoch 1010, val loss: 1.082552194595337
Epoch 1020, training loss: 89.67300415039062 = 1.0814235210418701 + 10.0 * 8.859158515930176
Epoch 1020, val loss: 1.082222819328308
Epoch 1030, training loss: 89.68903350830078 = 1.0810753107070923 + 10.0 * 8.860795974731445
Epoch 1030, val loss: 1.0818867683410645
Epoch 1040, training loss: 89.71974182128906 = 1.0807205438613892 + 10.0 * 8.86390209197998
Epoch 1040, val loss: 1.081558108329773
Epoch 1050, training loss: 89.75048828125 = 1.080359935760498 + 10.0 * 8.867012977600098
Epoch 1050, val loss: 1.0812174081802368
Epoch 1060, training loss: 89.81878662109375 = 1.080011010169983 + 10.0 * 8.87387752532959
Epoch 1060, val loss: 1.0808790922164917
Epoch 1070, training loss: 89.76983642578125 = 1.0796352624893188 + 10.0 * 8.869020462036133
Epoch 1070, val loss: 1.0805234909057617
Epoch 1080, training loss: 89.88420104980469 = 1.07926607131958 + 10.0 * 8.8804931640625
Epoch 1080, val loss: 1.080175757408142
Epoch 1090, training loss: 89.8785629272461 = 1.0788887739181519 + 10.0 * 8.87996768951416
Epoch 1090, val loss: 1.0798234939575195
Epoch 1100, training loss: 89.9146499633789 = 1.078504204750061 + 10.0 * 8.883614540100098
Epoch 1100, val loss: 1.0794517993927002
Epoch 1110, training loss: 89.95036315917969 = 1.0781240463256836 + 10.0 * 8.887224197387695
Epoch 1110, val loss: 1.079089641571045
Epoch 1120, training loss: 89.96521759033203 = 1.0777326822280884 + 10.0 * 8.888748168945312
Epoch 1120, val loss: 1.078727126121521
Epoch 1130, training loss: 90.01889038085938 = 1.0773431062698364 + 10.0 * 8.89415454864502
Epoch 1130, val loss: 1.0783560276031494
Epoch 1140, training loss: 89.96070098876953 = 1.0769215822219849 + 10.0 * 8.888378143310547
Epoch 1140, val loss: 1.077958345413208
Epoch 1150, training loss: 89.9177017211914 = 1.0765000581741333 + 10.0 * 8.884119987487793
Epoch 1150, val loss: 1.0775734186172485
Epoch 1160, training loss: 90.01998138427734 = 1.076107144355774 + 10.0 * 8.894387245178223
Epoch 1160, val loss: 1.0771942138671875
Epoch 1170, training loss: 90.0938720703125 = 1.0756926536560059 + 10.0 * 8.90181827545166
Epoch 1170, val loss: 1.0768117904663086
Epoch 1180, training loss: 90.07921600341797 = 1.0752774477005005 + 10.0 * 8.90039348602295
Epoch 1180, val loss: 1.0764214992523193
Epoch 1190, training loss: 90.13605499267578 = 1.0748640298843384 + 10.0 * 8.906118392944336
Epoch 1190, val loss: 1.0760308504104614
Epoch 1200, training loss: 90.16490936279297 = 1.0744422674179077 + 10.0 * 8.90904712677002
Epoch 1200, val loss: 1.0756341218948364
Epoch 1210, training loss: 90.16635131835938 = 1.0740078687667847 + 10.0 * 8.909235000610352
Epoch 1210, val loss: 1.0752278566360474
Epoch 1220, training loss: 90.18762969970703 = 1.0735703706741333 + 10.0 * 8.911405563354492
Epoch 1220, val loss: 1.0748175382614136
Epoch 1230, training loss: 90.20780181884766 = 1.0731338262557983 + 10.0 * 8.913466453552246
Epoch 1230, val loss: 1.0744094848632812
Epoch 1240, training loss: 90.27494812011719 = 1.0726889371871948 + 10.0 * 8.920226097106934
Epoch 1240, val loss: 1.0739924907684326
Epoch 1250, training loss: 89.90995788574219 = 1.0721840858459473 + 10.0 * 8.883777618408203
Epoch 1250, val loss: 1.0734940767288208
Epoch 1260, training loss: 90.0667953491211 = 1.0717321634292603 + 10.0 * 8.899506568908691
Epoch 1260, val loss: 1.0730866193771362
Epoch 1270, training loss: 89.9114761352539 = 1.0712581872940063 + 10.0 * 8.884021759033203
Epoch 1270, val loss: 1.0726594924926758
Epoch 1280, training loss: 89.98983001708984 = 1.0708112716674805 + 10.0 * 8.891901969909668
Epoch 1280, val loss: 1.0722390413284302
Epoch 1290, training loss: 90.10089874267578 = 1.0703647136688232 + 10.0 * 8.903053283691406
Epoch 1290, val loss: 1.0718175172805786
Epoch 1300, training loss: 90.2012939453125 = 1.069905400276184 + 10.0 * 8.913138389587402
Epoch 1300, val loss: 1.0713882446289062
Epoch 1310, training loss: 90.25626373291016 = 1.0694187879562378 + 10.0 * 8.918684005737305
Epoch 1310, val loss: 1.0709377527236938
Epoch 1320, training loss: 90.26346588134766 = 1.0689313411712646 + 10.0 * 8.919453620910645
Epoch 1320, val loss: 1.070482611656189
Epoch 1330, training loss: 90.27730560302734 = 1.0684354305267334 + 10.0 * 8.920886993408203
Epoch 1330, val loss: 1.0700198411941528
Epoch 1340, training loss: 90.34574890136719 = 1.0679374933242798 + 10.0 * 8.927781105041504
Epoch 1340, val loss: 1.0695583820343018
Epoch 1350, training loss: 90.36473083496094 = 1.067433476448059 + 10.0 * 8.929729461669922
Epoch 1350, val loss: 1.0690879821777344
Epoch 1360, training loss: 90.361328125 = 1.0669230222702026 + 10.0 * 8.92944049835205
Epoch 1360, val loss: 1.0686140060424805
Epoch 1370, training loss: 90.36198425292969 = 1.066412329673767 + 10.0 * 8.929556846618652
Epoch 1370, val loss: 1.0681450366973877
Epoch 1380, training loss: 90.4017562866211 = 1.0659042596817017 + 10.0 * 8.933585166931152
Epoch 1380, val loss: 1.067665696144104
Epoch 1390, training loss: 90.43663787841797 = 1.0653855800628662 + 10.0 * 8.937125205993652
Epoch 1390, val loss: 1.0671828985214233
Epoch 1400, training loss: 90.44049835205078 = 1.0648564100265503 + 10.0 * 8.9375638961792
Epoch 1400, val loss: 1.0666905641555786
Epoch 1410, training loss: 90.44367980957031 = 1.0643283128738403 + 10.0 * 8.937934875488281
Epoch 1410, val loss: 1.0661993026733398
Epoch 1420, training loss: 90.49667358398438 = 1.0637967586517334 + 10.0 * 8.94328784942627
Epoch 1420, val loss: 1.0657075643539429
Epoch 1430, training loss: 90.54126739501953 = 1.0632619857788086 + 10.0 * 8.947800636291504
Epoch 1430, val loss: 1.0652135610580444
Epoch 1440, training loss: 90.5745849609375 = 1.0627155303955078 + 10.0 * 8.951187133789062
Epoch 1440, val loss: 1.064710021018982
Epoch 1450, training loss: 90.52445220947266 = 1.0621702671051025 + 10.0 * 8.94622802734375
Epoch 1450, val loss: 1.0642037391662598
Epoch 1460, training loss: 90.55701446533203 = 1.0616247653961182 + 10.0 * 8.949539184570312
Epoch 1460, val loss: 1.0637047290802002
Epoch 1470, training loss: 90.62132263183594 = 1.0610719919204712 + 10.0 * 8.956025123596191
Epoch 1470, val loss: 1.0631886720657349
Epoch 1480, training loss: 90.60763549804688 = 1.0605096817016602 + 10.0 * 8.954712867736816
Epoch 1480, val loss: 1.0626741647720337
Epoch 1490, training loss: 90.66493225097656 = 1.059954285621643 + 10.0 * 8.960497856140137
Epoch 1490, val loss: 1.06215500831604
Epoch 1500, training loss: 90.68122863769531 = 1.0593820810317993 + 10.0 * 8.96218490600586
Epoch 1500, val loss: 1.06163489818573
Epoch 1510, training loss: 90.63477325439453 = 1.0588163137435913 + 10.0 * 8.957595825195312
Epoch 1510, val loss: 1.0611094236373901
Epoch 1520, training loss: 90.56830596923828 = 1.05825936794281 + 10.0 * 8.951004981994629
Epoch 1520, val loss: 1.060598373413086
Epoch 1530, training loss: 90.61219024658203 = 1.0576989650726318 + 10.0 * 8.955449104309082
Epoch 1530, val loss: 1.060073971748352
Epoch 1540, training loss: 90.65982055664062 = 1.0571320056915283 + 10.0 * 8.9602689743042
Epoch 1540, val loss: 1.0595552921295166
Epoch 1550, training loss: 90.6861572265625 = 1.056555986404419 + 10.0 * 8.962960243225098
Epoch 1550, val loss: 1.0590276718139648
Epoch 1560, training loss: 90.72630310058594 = 1.0559746026992798 + 10.0 * 8.967032432556152
Epoch 1560, val loss: 1.0584903955459595
Epoch 1570, training loss: 90.7125473022461 = 1.055382251739502 + 10.0 * 8.965716361999512
Epoch 1570, val loss: 1.0579473972320557
Epoch 1580, training loss: 90.7577896118164 = 1.0547987222671509 + 10.0 * 8.970298767089844
Epoch 1580, val loss: 1.0574147701263428
Epoch 1590, training loss: 90.7919921875 = 1.0542080402374268 + 10.0 * 8.97377872467041
Epoch 1590, val loss: 1.0568745136260986
Epoch 1600, training loss: 90.70927429199219 = 1.053598403930664 + 10.0 * 8.965567588806152
Epoch 1600, val loss: 1.0563273429870605
Epoch 1610, training loss: 90.67656707763672 = 1.0530345439910889 + 10.0 * 8.962353706359863
Epoch 1610, val loss: 1.0557845830917358
Epoch 1620, training loss: 90.73131561279297 = 1.0523911714553833 + 10.0 * 8.96789264678955
Epoch 1620, val loss: 1.0552006959915161
Epoch 1630, training loss: 90.61090850830078 = 1.0517432689666748 + 10.0 * 8.955916404724121
Epoch 1630, val loss: 1.0546314716339111
Epoch 1640, training loss: 90.71009063720703 = 1.0511890649795532 + 10.0 * 8.965890884399414
Epoch 1640, val loss: 1.0541162490844727
Epoch 1650, training loss: 90.6594467163086 = 1.0505601167678833 + 10.0 * 8.960888862609863
Epoch 1650, val loss: 1.05354642868042
Epoch 1660, training loss: 90.71208953857422 = 1.0499764680862427 + 10.0 * 8.966211318969727
Epoch 1660, val loss: 1.0530173778533936
Epoch 1670, training loss: 90.80705261230469 = 1.0494093894958496 + 10.0 * 8.975764274597168
Epoch 1670, val loss: 1.0525034666061401
Epoch 1680, training loss: 90.86370849609375 = 1.048831820487976 + 10.0 * 8.981488227844238
Epoch 1680, val loss: 1.051972508430481
Epoch 1690, training loss: 90.8968505859375 = 1.0482345819473267 + 10.0 * 8.984861373901367
Epoch 1690, val loss: 1.0514166355133057
Epoch 1700, training loss: 90.94671630859375 = 1.047662615776062 + 10.0 * 8.98990535736084
Epoch 1700, val loss: 1.0509039163589478
Epoch 1710, training loss: 90.74117279052734 = 1.0470441579818726 + 10.0 * 8.969412803649902
Epoch 1710, val loss: 1.0503448247909546
Epoch 1720, training loss: 90.79618072509766 = 1.0464653968811035 + 10.0 * 8.974971771240234
Epoch 1720, val loss: 1.049825668334961
Epoch 1730, training loss: 90.8598403930664 = 1.045724868774414 + 10.0 * 8.981411933898926
Epoch 1730, val loss: 1.0490193367004395
Epoch 1740, training loss: 90.92208099365234 = 1.0436584949493408 + 10.0 * 8.987842559814453
Epoch 1740, val loss: 1.0468753576278687
Epoch 1750, training loss: 90.95718383789062 = 1.0414741039276123 + 10.0 * 8.991571426391602
Epoch 1750, val loss: 1.0446789264678955
Epoch 1760, training loss: 90.98373413085938 = 1.0394474267959595 + 10.0 * 8.994428634643555
Epoch 1760, val loss: 1.0426526069641113
Epoch 1770, training loss: 90.96134948730469 = 1.0375592708587646 + 10.0 * 8.992379188537598
Epoch 1770, val loss: 1.0407726764678955
Epoch 1780, training loss: 90.96159362792969 = 1.035783290863037 + 10.0 * 8.992581367492676
Epoch 1780, val loss: 1.0390132665634155
Epoch 1790, training loss: 91.0000228881836 = 1.0340946912765503 + 10.0 * 8.99659252166748
Epoch 1790, val loss: 1.0373421907424927
Epoch 1800, training loss: 91.00664520263672 = 1.032462477684021 + 10.0 * 8.997418403625488
Epoch 1800, val loss: 1.0357400178909302
Epoch 1810, training loss: 90.99271392822266 = 1.030862808227539 + 10.0 * 8.996185302734375
Epoch 1810, val loss: 1.0341756343841553
Epoch 1820, training loss: 91.04436492919922 = 1.029304027557373 + 10.0 * 9.001505851745605
Epoch 1820, val loss: 1.0326547622680664
Epoch 1830, training loss: 91.052490234375 = 1.0277706384658813 + 10.0 * 9.002471923828125
Epoch 1830, val loss: 1.031165599822998
Epoch 1840, training loss: 91.06410217285156 = 1.0262525081634521 + 10.0 * 9.003785133361816
Epoch 1840, val loss: 1.0296761989593506
Epoch 1850, training loss: 91.05498504638672 = 1.024741530418396 + 10.0 * 9.003024101257324
Epoch 1850, val loss: 1.0282162427902222
Epoch 1860, training loss: 91.10885620117188 = 1.023253083229065 + 10.0 * 9.008560180664062
Epoch 1860, val loss: 1.0267648696899414
Epoch 1870, training loss: 91.09315490722656 = 1.0217564105987549 + 10.0 * 9.007139205932617
Epoch 1870, val loss: 1.0253266096115112
Epoch 1880, training loss: 91.04557037353516 = 1.0202536582946777 + 10.0 * 9.002531051635742
Epoch 1880, val loss: 1.0238771438598633
Epoch 1890, training loss: 91.08040618896484 = 1.0187560319900513 + 10.0 * 9.006165504455566
Epoch 1890, val loss: 1.022426962852478
Epoch 1900, training loss: 91.13392639160156 = 1.0172425508499146 + 10.0 * 9.01166820526123
Epoch 1900, val loss: 1.0209758281707764
Epoch 1910, training loss: 91.14546966552734 = 1.0157123804092407 + 10.0 * 9.012975692749023
Epoch 1910, val loss: 1.0195029973983765
Epoch 1920, training loss: 91.00762176513672 = 1.0141240358352661 + 10.0 * 8.999349594116211
Epoch 1920, val loss: 1.0179773569107056
Epoch 1930, training loss: 91.02848052978516 = 1.0125483274459839 + 10.0 * 9.001592636108398
Epoch 1930, val loss: 1.0164715051651
Epoch 1940, training loss: 91.0472412109375 = 1.0109682083129883 + 10.0 * 9.003626823425293
Epoch 1940, val loss: 1.0149788856506348
Epoch 1950, training loss: 91.04497528076172 = 1.009360671043396 + 10.0 * 9.003561019897461
Epoch 1950, val loss: 1.0134438276290894
Epoch 1960, training loss: 91.10884857177734 = 1.0077286958694458 + 10.0 * 9.010111808776855
Epoch 1960, val loss: 1.011884093284607
Epoch 1970, training loss: 91.12848663330078 = 1.0060786008834839 + 10.0 * 9.012240409851074
Epoch 1970, val loss: 1.010319471359253
Epoch 1980, training loss: 91.14179992675781 = 1.004408359527588 + 10.0 * 9.013738632202148
Epoch 1980, val loss: 1.0087385177612305
Epoch 1990, training loss: 91.08480834960938 = 1.002709150314331 + 10.0 * 9.008210182189941
Epoch 1990, val loss: 1.0071219205856323
Epoch 2000, training loss: 91.1178970336914 = 1.0010191202163696 + 10.0 * 9.011687278747559
Epoch 2000, val loss: 1.0055345296859741
Epoch 2010, training loss: 91.17676544189453 = 0.999313473701477 + 10.0 * 9.017745018005371
Epoch 2010, val loss: 1.0039206743240356
Epoch 2020, training loss: 91.22776794433594 = 0.9975888133049011 + 10.0 * 9.023017883300781
Epoch 2020, val loss: 1.002292275428772
Epoch 2030, training loss: 91.22586822509766 = 0.9958509802818298 + 10.0 * 9.023001670837402
Epoch 2030, val loss: 1.000659465789795
Epoch 2040, training loss: 91.2529296875 = 0.9941125512123108 + 10.0 * 9.02588176727295
Epoch 2040, val loss: 0.9990208745002747
Epoch 2050, training loss: 91.2703857421875 = 0.9923713207244873 + 10.0 * 9.027801513671875
Epoch 2050, val loss: 0.9973840713500977
Epoch 2060, training loss: 91.24250030517578 = 0.9906095862388611 + 10.0 * 9.025189399719238
Epoch 2060, val loss: 0.995731770992279
Epoch 2070, training loss: 91.22480010986328 = 0.9888561367988586 + 10.0 * 9.023594856262207
Epoch 2070, val loss: 0.9940782189369202
Epoch 2080, training loss: 91.26535034179688 = 0.9871100783348083 + 10.0 * 9.027824401855469
Epoch 2080, val loss: 0.9924361109733582
Epoch 2090, training loss: 91.31555938720703 = 0.9853562712669373 + 10.0 * 9.03302001953125
Epoch 2090, val loss: 0.9907879829406738
Epoch 2100, training loss: 91.28968811035156 = 0.9835851192474365 + 10.0 * 9.030611038208008
Epoch 2100, val loss: 0.9891300201416016
Epoch 2110, training loss: 91.30436706542969 = 0.9818148612976074 + 10.0 * 9.032255172729492
Epoch 2110, val loss: 0.9874591827392578
Epoch 2120, training loss: 91.33139038085938 = 0.9800501465797424 + 10.0 * 9.035134315490723
Epoch 2120, val loss: 0.9858071208000183
Epoch 2130, training loss: 91.34764099121094 = 0.9782737493515015 + 10.0 * 9.03693675994873
Epoch 2130, val loss: 0.9841552972793579
Epoch 2140, training loss: 91.35539245605469 = 0.9764896631240845 + 10.0 * 9.037890434265137
Epoch 2140, val loss: 0.9824679493904114
Epoch 2150, training loss: 91.363037109375 = 0.9747005701065063 + 10.0 * 9.038833618164062
Epoch 2150, val loss: 0.9807933568954468
Epoch 2160, training loss: 91.3082275390625 = 0.9728855490684509 + 10.0 * 9.033534049987793
Epoch 2160, val loss: 0.979106068611145
Epoch 2170, training loss: 91.2569351196289 = 0.9710781574249268 + 10.0 * 9.028585433959961
Epoch 2170, val loss: 0.9773885607719421
Epoch 2180, training loss: 91.26289367675781 = 0.9692867994308472 + 10.0 * 9.0293607711792
Epoch 2180, val loss: 0.9757388830184937
Epoch 2190, training loss: 91.32234954833984 = 0.9674888253211975 + 10.0 * 9.035486221313477
Epoch 2190, val loss: 0.9740568399429321
Epoch 2200, training loss: 91.36551666259766 = 0.9656495451927185 + 10.0 * 9.039986610412598
Epoch 2200, val loss: 0.9723379611968994
Epoch 2210, training loss: 91.34223937988281 = 0.9637507200241089 + 10.0 * 9.037848472595215
Epoch 2210, val loss: 0.970552384853363
Epoch 2220, training loss: 91.36729431152344 = 0.9617990851402283 + 10.0 * 9.040549278259277
Epoch 2220, val loss: 0.9687192440032959
Epoch 2230, training loss: 91.38553619384766 = 0.9597403407096863 + 10.0 * 9.042579650878906
Epoch 2230, val loss: 0.9667782783508301
Epoch 2240, training loss: 91.3857650756836 = 0.9575670957565308 + 10.0 * 9.04281997680664
Epoch 2240, val loss: 0.9647366404533386
Epoch 2250, training loss: 91.39180755615234 = 0.9553336501121521 + 10.0 * 9.043647766113281
Epoch 2250, val loss: 0.9626503586769104
Epoch 2260, training loss: 91.38330078125 = 0.9530474543571472 + 10.0 * 9.043025016784668
Epoch 2260, val loss: 0.960503876209259
Epoch 2270, training loss: 91.22874450683594 = 0.9507574439048767 + 10.0 * 9.027798652648926
Epoch 2270, val loss: 0.9583672285079956
Epoch 2280, training loss: 91.34278106689453 = 0.9486395120620728 + 10.0 * 9.039414405822754
Epoch 2280, val loss: 0.9564130306243896
Epoch 2290, training loss: 91.2070541381836 = 0.9463988542556763 + 10.0 * 9.026065826416016
Epoch 2290, val loss: 0.9543173909187317
Epoch 2300, training loss: 91.17439270019531 = 0.944146454334259 + 10.0 * 9.023024559020996
Epoch 2300, val loss: 0.9521939158439636
Epoch 2310, training loss: 91.20193481445312 = 0.9419366121292114 + 10.0 * 9.025999069213867
Epoch 2310, val loss: 0.9501402974128723
Epoch 2320, training loss: 91.28861999511719 = 0.939653217792511 + 10.0 * 9.034896850585938
Epoch 2320, val loss: 0.9479962587356567
Epoch 2330, training loss: 91.33425903320312 = 0.9373752474784851 + 10.0 * 9.039688110351562
Epoch 2330, val loss: 0.9458649158477783
Epoch 2340, training loss: 91.3993148803711 = 0.9350797533988953 + 10.0 * 9.046422958374023
Epoch 2340, val loss: 0.9437198638916016
Epoch 2350, training loss: 91.42130279541016 = 0.9327614307403564 + 10.0 * 9.048853874206543
Epoch 2350, val loss: 0.9415419101715088
Epoch 2360, training loss: 91.44046783447266 = 0.930438220500946 + 10.0 * 9.051003456115723
Epoch 2360, val loss: 0.939360499382019
Epoch 2370, training loss: 91.42948913574219 = 0.9281220436096191 + 10.0 * 9.05013656616211
Epoch 2370, val loss: 0.9371811747550964
Epoch 2380, training loss: 91.46713256835938 = 0.9258001446723938 + 10.0 * 9.054133415222168
Epoch 2380, val loss: 0.9349891543388367
Epoch 2390, training loss: 91.50392150878906 = 0.9234744310379028 + 10.0 * 9.05804443359375
Epoch 2390, val loss: 0.9328057169914246
Epoch 2400, training loss: 91.49761962890625 = 0.9211500287055969 + 10.0 * 9.057646751403809
Epoch 2400, val loss: 0.930614173412323
Epoch 2410, training loss: 91.25680541992188 = 0.9188538789749146 + 10.0 * 9.033795356750488
Epoch 2410, val loss: 0.9284109473228455
Epoch 2420, training loss: 91.42899322509766 = 0.9167547225952148 + 10.0 * 9.051223754882812
Epoch 2420, val loss: 0.9264845252037048
Epoch 2430, training loss: 91.57389068603516 = 0.9144350290298462 + 10.0 * 9.065945625305176
Epoch 2430, val loss: 0.9243006110191345
Epoch 2440, training loss: 91.62212371826172 = 0.9121373891830444 + 10.0 * 9.070998191833496
Epoch 2440, val loss: 0.9221569895744324
Epoch 2450, training loss: 91.67446899414062 = 0.909860372543335 + 10.0 * 9.076460838317871
Epoch 2450, val loss: 0.920012891292572
Epoch 2460, training loss: 91.66739654541016 = 0.907579779624939 + 10.0 * 9.075982093811035
Epoch 2460, val loss: 0.9178818464279175
Epoch 2470, training loss: 91.68917846679688 = 0.905348539352417 + 10.0 * 9.07838249206543
Epoch 2470, val loss: 0.9157814979553223
Epoch 2480, training loss: 91.71408081054688 = 0.9031144976615906 + 10.0 * 9.081096649169922
Epoch 2480, val loss: 0.9136673212051392
Epoch 2490, training loss: 91.7234878540039 = 0.9008846879005432 + 10.0 * 9.082260131835938
Epoch 2490, val loss: 0.9115912914276123
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.5544927536231884
0.8650293414475115
=== training gcn model ===
Epoch 0, training loss: 103.9508285522461 = 1.0943434238433838 + 10.0 * 10.285648345947266
Epoch 0, val loss: 1.0948455333709717
Epoch 10, training loss: 100.3597412109375 = 1.0941486358642578 + 10.0 * 9.926559448242188
Epoch 10, val loss: 1.09468412399292
Epoch 20, training loss: 98.18875885009766 = 1.093971610069275 + 10.0 * 9.709478378295898
Epoch 20, val loss: 1.0945271253585815
Epoch 30, training loss: 96.57664489746094 = 1.0937989950180054 + 10.0 * 9.548284530639648
Epoch 30, val loss: 1.0943713188171387
Epoch 40, training loss: 95.29875946044922 = 1.0936181545257568 + 10.0 * 9.420514106750488
Epoch 40, val loss: 1.0942158699035645
Epoch 50, training loss: 94.2849349975586 = 1.0934346914291382 + 10.0 * 9.3191499710083
Epoch 50, val loss: 1.0940535068511963
Epoch 60, training loss: 93.44944763183594 = 1.093254566192627 + 10.0 * 9.23561954498291
Epoch 60, val loss: 1.0938955545425415
Epoch 70, training loss: 92.7643051147461 = 1.0930520296096802 + 10.0 * 9.16712474822998
Epoch 70, val loss: 1.0937179327011108
Epoch 80, training loss: 92.18167877197266 = 1.0928478240966797 + 10.0 * 9.108882904052734
Epoch 80, val loss: 1.093537449836731
Epoch 90, training loss: 91.68059539794922 = 1.0926355123519897 + 10.0 * 9.058795928955078
Epoch 90, val loss: 1.0933468341827393
Epoch 100, training loss: 91.2432861328125 = 1.0924121141433716 + 10.0 * 9.015087127685547
Epoch 100, val loss: 1.093148946762085
Epoch 110, training loss: 90.8760986328125 = 1.0921705961227417 + 10.0 * 8.978392601013184
Epoch 110, val loss: 1.092928171157837
Epoch 120, training loss: 90.55888366699219 = 1.0919387340545654 + 10.0 * 8.946694374084473
Epoch 120, val loss: 1.0927214622497559
Epoch 130, training loss: 90.24164581298828 = 1.0916824340820312 + 10.0 * 8.914996147155762
Epoch 130, val loss: 1.0924936532974243
Epoch 140, training loss: 90.01941680908203 = 1.091436743736267 + 10.0 * 8.892797470092773
Epoch 140, val loss: 1.0922702550888062
Epoch 150, training loss: 89.81607818603516 = 1.0911715030670166 + 10.0 * 8.872490882873535
Epoch 150, val loss: 1.0920310020446777
Epoch 160, training loss: 89.6340103149414 = 1.0909053087234497 + 10.0 * 8.854310989379883
Epoch 160, val loss: 1.0917888879776
Epoch 170, training loss: 89.47271728515625 = 1.0906202793121338 + 10.0 * 8.83820915222168
Epoch 170, val loss: 1.0915330648422241
Epoch 180, training loss: 89.34223937988281 = 1.0903358459472656 + 10.0 * 8.825190544128418
Epoch 180, val loss: 1.0912781953811646
Epoch 190, training loss: 89.21942138671875 = 1.0900324583053589 + 10.0 * 8.812938690185547
Epoch 190, val loss: 1.091004729270935
Epoch 200, training loss: 89.14013671875 = 1.0897200107574463 + 10.0 * 8.805041313171387
Epoch 200, val loss: 1.0907227993011475
Epoch 210, training loss: 89.04942321777344 = 1.0894019603729248 + 10.0 * 8.796002388000488
Epoch 210, val loss: 1.090436339378357
Epoch 220, training loss: 88.93241119384766 = 1.0890783071517944 + 10.0 * 8.784333229064941
Epoch 220, val loss: 1.090142846107483
Epoch 230, training loss: 88.86689758300781 = 1.0887380838394165 + 10.0 * 8.777815818786621
Epoch 230, val loss: 1.0898334980010986
Epoch 240, training loss: 88.80511474609375 = 1.0883941650390625 + 10.0 * 8.771672248840332
Epoch 240, val loss: 1.0895224809646606
Epoch 250, training loss: 88.76913452148438 = 1.0880467891693115 + 10.0 * 8.768109321594238
Epoch 250, val loss: 1.089208722114563
Epoch 260, training loss: 88.70759582519531 = 1.0876939296722412 + 10.0 * 8.761990547180176
Epoch 260, val loss: 1.08888578414917
Epoch 270, training loss: 88.67939758300781 = 1.0873055458068848 + 10.0 * 8.759209632873535
Epoch 270, val loss: 1.0885300636291504
Epoch 280, training loss: 88.6288833618164 = 1.086926817893982 + 10.0 * 8.754195213317871
Epoch 280, val loss: 1.0881905555725098
Epoch 290, training loss: 88.6449203491211 = 1.0865446329116821 + 10.0 * 8.755837440490723
Epoch 290, val loss: 1.087846279144287
Epoch 300, training loss: 88.62109375 = 1.0861620903015137 + 10.0 * 8.753493309020996
Epoch 300, val loss: 1.0874992609024048
Epoch 310, training loss: 88.58747100830078 = 1.085754632949829 + 10.0 * 8.750171661376953
Epoch 310, val loss: 1.0871318578720093
Epoch 320, training loss: 88.59029388427734 = 1.0853350162506104 + 10.0 * 8.750495910644531
Epoch 320, val loss: 1.0867512226104736
Epoch 330, training loss: 88.61311340332031 = 1.084926962852478 + 10.0 * 8.75281810760498
Epoch 330, val loss: 1.0863854885101318
Epoch 340, training loss: 88.57551574707031 = 1.0844995975494385 + 10.0 * 8.749101638793945
Epoch 340, val loss: 1.085992693901062
Epoch 350, training loss: 88.57288360595703 = 1.0840613842010498 + 10.0 * 8.748882293701172
Epoch 350, val loss: 1.0855967998504639
Epoch 360, training loss: 88.60846710205078 = 1.0836329460144043 + 10.0 * 8.752483367919922
Epoch 360, val loss: 1.085204005241394
Epoch 370, training loss: 88.6278076171875 = 1.0831931829452515 + 10.0 * 8.754461288452148
Epoch 370, val loss: 1.0848113298416138
Epoch 380, training loss: 88.65340423583984 = 1.082748532295227 + 10.0 * 8.757065773010254
Epoch 380, val loss: 1.0844008922576904
Epoch 390, training loss: 88.59305572509766 = 1.0822854042053223 + 10.0 * 8.751077651977539
Epoch 390, val loss: 1.0839893817901611
Epoch 400, training loss: 88.60636138916016 = 1.0818108320236206 + 10.0 * 8.75245475769043
Epoch 400, val loss: 1.083553671836853
Epoch 410, training loss: 88.6091079711914 = 1.0813642740249634 + 10.0 * 8.752774238586426
Epoch 410, val loss: 1.0831406116485596
Epoch 420, training loss: 88.63923645019531 = 1.0809153318405151 + 10.0 * 8.755831718444824
Epoch 420, val loss: 1.082740306854248
Epoch 430, training loss: 88.65581512451172 = 1.0804444551467896 + 10.0 * 8.757536888122559
Epoch 430, val loss: 1.0823091268539429
Epoch 440, training loss: 88.68866729736328 = 1.0799604654312134 + 10.0 * 8.760869979858398
Epoch 440, val loss: 1.0818707942962646
Epoch 450, training loss: 88.69425964355469 = 1.0794743299484253 + 10.0 * 8.761478424072266
Epoch 450, val loss: 1.0814305543899536
Epoch 460, training loss: 88.73231506347656 = 1.0789817571640015 + 10.0 * 8.76533317565918
Epoch 460, val loss: 1.0809861421585083
Epoch 470, training loss: 88.71729278564453 = 1.0784672498703003 + 10.0 * 8.763882637023926
Epoch 470, val loss: 1.0805213451385498
Epoch 480, training loss: 88.74401092529297 = 1.0779540538787842 + 10.0 * 8.766605377197266
Epoch 480, val loss: 1.080049753189087
Epoch 490, training loss: 88.7762451171875 = 1.0774263143539429 + 10.0 * 8.769882202148438
Epoch 490, val loss: 1.0795806646347046
Epoch 500, training loss: 88.79981231689453 = 1.0768941640853882 + 10.0 * 8.77229118347168
Epoch 500, val loss: 1.07907235622406
Epoch 510, training loss: 88.74417877197266 = 1.0763249397277832 + 10.0 * 8.766785621643066
Epoch 510, val loss: 1.0785646438598633
Epoch 520, training loss: 88.77387237548828 = 1.075771450996399 + 10.0 * 8.76980972290039
Epoch 520, val loss: 1.0780575275421143
Epoch 530, training loss: 88.84513854980469 = 1.0752108097076416 + 10.0 * 8.776992797851562
Epoch 530, val loss: 1.0775436162948608
Epoch 540, training loss: 88.8525161743164 = 1.074622631072998 + 10.0 * 8.777789115905762
Epoch 540, val loss: 1.0770047903060913
Epoch 550, training loss: 88.84881591796875 = 1.074048638343811 + 10.0 * 8.77747631072998
Epoch 550, val loss: 1.0764824151992798
Epoch 560, training loss: 88.90010070800781 = 1.0734565258026123 + 10.0 * 8.78266429901123
Epoch 560, val loss: 1.0759316682815552
Epoch 570, training loss: 88.8860092163086 = 1.0728389024734497 + 10.0 * 8.781316757202148
Epoch 570, val loss: 1.0753698348999023
Epoch 580, training loss: 88.8868637084961 = 1.0721898078918457 + 10.0 * 8.78146743774414
Epoch 580, val loss: 1.0747857093811035
Epoch 590, training loss: 88.82635498046875 = 1.0715017318725586 + 10.0 * 8.775485038757324
Epoch 590, val loss: 1.0741461515426636
Epoch 600, training loss: 88.90836334228516 = 1.070838212966919 + 10.0 * 8.78375244140625
Epoch 600, val loss: 1.0735267400741577
Epoch 610, training loss: 88.90006256103516 = 1.070216178894043 + 10.0 * 8.782984733581543
Epoch 610, val loss: 1.0729504823684692
Epoch 620, training loss: 88.95438385009766 = 1.069520354270935 + 10.0 * 8.78848648071289
Epoch 620, val loss: 1.0723049640655518
Epoch 630, training loss: 89.00212860107422 = 1.0688271522521973 + 10.0 * 8.793330192565918
Epoch 630, val loss: 1.071675181388855
Epoch 640, training loss: 89.03022766113281 = 1.0681025981903076 + 10.0 * 8.796213150024414
Epoch 640, val loss: 1.071003794670105
Epoch 650, training loss: 89.04063415527344 = 1.0673749446868896 + 10.0 * 8.79732608795166
Epoch 650, val loss: 1.0703413486480713
Epoch 660, training loss: 89.08145904541016 = 1.0666253566741943 + 10.0 * 8.801483154296875
Epoch 660, val loss: 1.0696583986282349
Epoch 670, training loss: 89.06761169433594 = 1.0658615827560425 + 10.0 * 8.800174713134766
Epoch 670, val loss: 1.0689600706100464
Epoch 680, training loss: 89.08394622802734 = 1.0650882720947266 + 10.0 * 8.801885604858398
Epoch 680, val loss: 1.0682551860809326
Epoch 690, training loss: 89.13421630859375 = 1.0643013715744019 + 10.0 * 8.806991577148438
Epoch 690, val loss: 1.0675139427185059
Epoch 700, training loss: 89.07518005371094 = 1.0635101795196533 + 10.0 * 8.801167488098145
Epoch 700, val loss: 1.0667921304702759
Epoch 710, training loss: 89.06388854980469 = 1.0626558065414429 + 10.0 * 8.80012321472168
Epoch 710, val loss: 1.066017746925354
Epoch 720, training loss: 89.23365020751953 = 1.061874508857727 + 10.0 * 8.817177772521973
Epoch 720, val loss: 1.0652998685836792
Epoch 730, training loss: 89.17155456542969 = 1.0609899759292603 + 10.0 * 8.811056137084961
Epoch 730, val loss: 1.0645047426223755
Epoch 740, training loss: 89.16693115234375 = 1.0601266622543335 + 10.0 * 8.810680389404297
Epoch 740, val loss: 1.0637134313583374
Epoch 750, training loss: 89.1843032836914 = 1.0592334270477295 + 10.0 * 8.812506675720215
Epoch 750, val loss: 1.0629017353057861
Epoch 760, training loss: 89.23944091796875 = 1.058335304260254 + 10.0 * 8.818110466003418
Epoch 760, val loss: 1.0620805025100708
Epoch 770, training loss: 89.28023529052734 = 1.0574166774749756 + 10.0 * 8.822281837463379
Epoch 770, val loss: 1.061236023902893
Epoch 780, training loss: 89.30264282226562 = 1.0564550161361694 + 10.0 * 8.824618339538574
Epoch 780, val loss: 1.060369610786438
Epoch 790, training loss: 89.18247985839844 = 1.0554533004760742 + 10.0 * 8.812703132629395
Epoch 790, val loss: 1.059463620185852
Epoch 800, training loss: 89.21243286132812 = 1.0545161962509155 + 10.0 * 8.815791130065918
Epoch 800, val loss: 1.0585912466049194
Epoch 810, training loss: 89.3387680053711 = 1.0535539388656616 + 10.0 * 8.828521728515625
Epoch 810, val loss: 1.0577021837234497
Epoch 820, training loss: 89.37970733642578 = 1.0525217056274414 + 10.0 * 8.832718849182129
Epoch 820, val loss: 1.056764006614685
Epoch 830, training loss: 89.39122009277344 = 1.0514827966690063 + 10.0 * 8.83397388458252
Epoch 830, val loss: 1.055809497833252
Epoch 840, training loss: 89.41744995117188 = 1.050429344177246 + 10.0 * 8.836702346801758
Epoch 840, val loss: 1.0548449754714966
Epoch 850, training loss: 89.37783813476562 = 1.0493329763412476 + 10.0 * 8.832850456237793
Epoch 850, val loss: 1.053849458694458
Epoch 860, training loss: 89.45362091064453 = 1.04824960231781 + 10.0 * 8.840537071228027
Epoch 860, val loss: 1.0528393983840942
Epoch 870, training loss: 89.51737213134766 = 1.047139048576355 + 10.0 * 8.847023010253906
Epoch 870, val loss: 1.0518229007720947
Epoch 880, training loss: 89.55204010009766 = 1.0460175275802612 + 10.0 * 8.850602149963379
Epoch 880, val loss: 1.0507960319519043
Epoch 890, training loss: 89.55686950683594 = 1.0448604822158813 + 10.0 * 8.851201057434082
Epoch 890, val loss: 1.0497384071350098
Epoch 900, training loss: 89.59416198730469 = 1.0436835289001465 + 10.0 * 8.855047225952148
Epoch 900, val loss: 1.0486607551574707
Epoch 910, training loss: 89.60093688964844 = 1.042492151260376 + 10.0 * 8.855844497680664
Epoch 910, val loss: 1.0475687980651855
Epoch 920, training loss: 89.5981216430664 = 1.041284203529358 + 10.0 * 8.855684280395508
Epoch 920, val loss: 1.0464539527893066
Epoch 930, training loss: 89.55255126953125 = 1.0400370359420776 + 10.0 * 8.851251602172852
Epoch 930, val loss: 1.045291781425476
Epoch 940, training loss: 89.61164855957031 = 1.038822054862976 + 10.0 * 8.857282638549805
Epoch 940, val loss: 1.044197916984558
Epoch 950, training loss: 89.58236694335938 = 1.0374529361724854 + 10.0 * 8.854491233825684
Epoch 950, val loss: 1.0429348945617676
Epoch 960, training loss: 89.66629791259766 = 1.036230206489563 + 10.0 * 8.863006591796875
Epoch 960, val loss: 1.0417956113815308
Epoch 970, training loss: 89.69036865234375 = 1.0349081754684448 + 10.0 * 8.865546226501465
Epoch 970, val loss: 1.0405936241149902
Epoch 980, training loss: 89.74801635742188 = 1.0335520505905151 + 10.0 * 8.87144660949707
Epoch 980, val loss: 1.0393446683883667
Epoch 990, training loss: 89.76081085205078 = 1.032166600227356 + 10.0 * 8.872864723205566
Epoch 990, val loss: 1.038075566291809
Epoch 1000, training loss: 89.75753784179688 = 1.0308232307434082 + 10.0 * 8.872671127319336
Epoch 1000, val loss: 1.0367791652679443
Epoch 1010, training loss: 89.60059356689453 = 1.029379963874817 + 10.0 * 8.857121467590332
Epoch 1010, val loss: 1.0355619192123413
Epoch 1020, training loss: 89.56063842773438 = 1.0280061960220337 + 10.0 * 8.853262901306152
Epoch 1020, val loss: 1.034227728843689
Epoch 1030, training loss: 89.62621307373047 = 1.0266778469085693 + 10.0 * 8.859952926635742
Epoch 1030, val loss: 1.0330411195755005
Epoch 1040, training loss: 89.53115844726562 = 1.0252318382263184 + 10.0 * 8.850592613220215
Epoch 1040, val loss: 1.0316821336746216
Epoch 1050, training loss: 89.7072982788086 = 1.0237623453140259 + 10.0 * 8.868352890014648
Epoch 1050, val loss: 1.030311107635498
Epoch 1060, training loss: 89.75444030761719 = 1.0222314596176147 + 10.0 * 8.873220443725586
Epoch 1060, val loss: 1.0289071798324585
Epoch 1070, training loss: 89.81105041503906 = 1.020699143409729 + 10.0 * 8.879034996032715
Epoch 1070, val loss: 1.0274910926818848
Epoch 1080, training loss: 89.87596893310547 = 1.0191508531570435 + 10.0 * 8.885682106018066
Epoch 1080, val loss: 1.0260543823242188
Epoch 1090, training loss: 89.90105438232422 = 1.0175870656967163 + 10.0 * 8.888346672058105
Epoch 1090, val loss: 1.0246083736419678
Epoch 1100, training loss: 89.93830871582031 = 1.016004204750061 + 10.0 * 8.892230033874512
Epoch 1100, val loss: 1.023136854171753
Epoch 1110, training loss: 89.99243927001953 = 1.0144000053405762 + 10.0 * 8.897804260253906
Epoch 1110, val loss: 1.0216608047485352
Epoch 1120, training loss: 89.95335388183594 = 1.0127562284469604 + 10.0 * 8.894060134887695
Epoch 1120, val loss: 1.0201332569122314
Epoch 1130, training loss: 89.97955322265625 = 1.0111069679260254 + 10.0 * 8.896844863891602
Epoch 1130, val loss: 1.018601655960083
Epoch 1140, training loss: 90.02449035644531 = 1.0094422101974487 + 10.0 * 8.901504516601562
Epoch 1140, val loss: 1.0170609951019287
Epoch 1150, training loss: 90.07740783691406 = 1.007753849029541 + 10.0 * 8.906965255737305
Epoch 1150, val loss: 1.0154881477355957
Epoch 1160, training loss: 90.00579833984375 = 1.0060094594955444 + 10.0 * 8.899978637695312
Epoch 1160, val loss: 1.013872504234314
Epoch 1170, training loss: 89.47895812988281 = 1.0040383338928223 + 10.0 * 8.847492218017578
Epoch 1170, val loss: 1.0120489597320557
Epoch 1180, training loss: 89.9286117553711 = 1.0025174617767334 + 10.0 * 8.892609596252441
Epoch 1180, val loss: 1.0106253623962402
Epoch 1190, training loss: 90.0049057006836 = 1.0008279085159302 + 10.0 * 8.900407791137695
Epoch 1190, val loss: 1.0090630054473877
Epoch 1200, training loss: 90.002685546875 = 0.9990835785865784 + 10.0 * 8.900360107421875
Epoch 1200, val loss: 1.0074224472045898
Epoch 1210, training loss: 90.10582733154297 = 0.9973103404045105 + 10.0 * 8.910852432250977
Epoch 1210, val loss: 1.0057934522628784
Epoch 1220, training loss: 90.15882110595703 = 0.9955213069915771 + 10.0 * 8.916330337524414
Epoch 1220, val loss: 1.004119634628296
Epoch 1230, training loss: 90.19862365722656 = 0.9937402009963989 + 10.0 * 8.920488357543945
Epoch 1230, val loss: 1.0024694204330444
Epoch 1240, training loss: 90.18013763427734 = 0.9919390678405762 + 10.0 * 8.91882038116455
Epoch 1240, val loss: 1.000798225402832
Epoch 1250, training loss: 90.23922729492188 = 0.9901615381240845 + 10.0 * 8.924906730651855
Epoch 1250, val loss: 0.9991490840911865
Epoch 1260, training loss: 90.1971664428711 = 0.9883716702461243 + 10.0 * 8.920879364013672
Epoch 1260, val loss: 0.9974855780601501
Epoch 1270, training loss: 90.25896453857422 = 0.9865946769714355 + 10.0 * 8.927236557006836
Epoch 1270, val loss: 0.9958290457725525
Epoch 1280, training loss: 90.31376647949219 = 0.9848186373710632 + 10.0 * 8.932894706726074
Epoch 1280, val loss: 0.9941760301589966
Epoch 1290, training loss: 90.32655334472656 = 0.9830310344696045 + 10.0 * 8.934351921081543
Epoch 1290, val loss: 0.9925171136856079
Epoch 1300, training loss: 90.37733459472656 = 0.9812383651733398 + 10.0 * 8.93960952758789
Epoch 1300, val loss: 0.990863025188446
Epoch 1310, training loss: 90.34529113769531 = 0.9795290231704712 + 10.0 * 8.936575889587402
Epoch 1310, val loss: 0.9892638325691223
Epoch 1320, training loss: 90.23457336425781 = 0.9778102040290833 + 10.0 * 8.925676345825195
Epoch 1320, val loss: 0.9876569509506226
Epoch 1330, training loss: 90.2977066040039 = 0.9760720133781433 + 10.0 * 8.93216323852539
Epoch 1330, val loss: 0.986049234867096
Epoch 1340, training loss: 90.3566665649414 = 0.974358856678009 + 10.0 * 8.938230514526367
Epoch 1340, val loss: 0.9844633936882019
Epoch 1350, training loss: 90.4215316772461 = 0.9726640582084656 + 10.0 * 8.944887161254883
Epoch 1350, val loss: 0.9828947186470032
Epoch 1360, training loss: 90.33952331542969 = 0.9709421992301941 + 10.0 * 8.936858177185059
Epoch 1360, val loss: 0.9812937378883362
Epoch 1370, training loss: 90.34983825683594 = 0.9692832231521606 + 10.0 * 8.938055038452148
Epoch 1370, val loss: 0.9797733426094055
Epoch 1380, training loss: 90.41081237792969 = 0.9676539897918701 + 10.0 * 8.944315910339355
Epoch 1380, val loss: 0.9782601594924927
Epoch 1390, training loss: 90.48815155029297 = 0.9660128355026245 + 10.0 * 8.952214241027832
Epoch 1390, val loss: 0.9767283797264099
Epoch 1400, training loss: 90.50984191894531 = 0.9644011855125427 + 10.0 * 8.954544067382812
Epoch 1400, val loss: 0.9752286672592163
Epoch 1410, training loss: 90.50010681152344 = 0.9627799987792969 + 10.0 * 8.95373249053955
Epoch 1410, val loss: 0.9737340211868286
Epoch 1420, training loss: 90.49539184570312 = 0.9612332582473755 + 10.0 * 8.953415870666504
Epoch 1420, val loss: 0.972278356552124
Epoch 1430, training loss: 90.55532836914062 = 0.9596883058547974 + 10.0 * 8.959564208984375
Epoch 1430, val loss: 0.9708437919616699
Epoch 1440, training loss: 90.61201477050781 = 0.9581464529037476 + 10.0 * 8.965387344360352
Epoch 1440, val loss: 0.9693980813026428
Epoch 1450, training loss: 90.60982513427734 = 0.9566196203231812 + 10.0 * 8.965320587158203
Epoch 1450, val loss: 0.9679572582244873
Epoch 1460, training loss: 90.6047134399414 = 0.9551170468330383 + 10.0 * 8.964960098266602
Epoch 1460, val loss: 0.966533362865448
Epoch 1470, training loss: 90.60990142822266 = 0.9536445140838623 + 10.0 * 8.965625762939453
Epoch 1470, val loss: 0.9651473760604858
Epoch 1480, training loss: 90.41523742675781 = 0.9521596431732178 + 10.0 * 8.946308135986328
Epoch 1480, val loss: 0.9637778997421265
Epoch 1490, training loss: 90.2010269165039 = 0.9506933093070984 + 10.0 * 8.925033569335938
Epoch 1490, val loss: 0.9623529314994812
Epoch 1500, training loss: 90.45235443115234 = 0.9493798613548279 + 10.0 * 8.950297355651855
Epoch 1500, val loss: 0.961105465888977
Epoch 1510, training loss: 90.48234558105469 = 0.947977602481842 + 10.0 * 8.953436851501465
Epoch 1510, val loss: 0.9598058462142944
Epoch 1520, training loss: 90.52181243896484 = 0.9466096758842468 + 10.0 * 8.957520484924316
Epoch 1520, val loss: 0.9585314989089966
Epoch 1530, training loss: 90.57233428955078 = 0.9453047513961792 + 10.0 * 8.962702751159668
Epoch 1530, val loss: 0.9573042392730713
Epoch 1540, training loss: 90.64498901367188 = 0.9439958930015564 + 10.0 * 8.970099449157715
Epoch 1540, val loss: 0.9560796022415161
Epoch 1550, training loss: 90.69416046142578 = 0.9427077770233154 + 10.0 * 8.97514533996582
Epoch 1550, val loss: 0.9548828601837158
Epoch 1560, training loss: 90.64694213867188 = 0.9414510130882263 + 10.0 * 8.970548629760742
Epoch 1560, val loss: 0.9537169933319092
Epoch 1570, training loss: 90.67892456054688 = 0.9402029514312744 + 10.0 * 8.973872184753418
Epoch 1570, val loss: 0.9525401592254639
Epoch 1580, training loss: 90.75336456298828 = 0.9389858245849609 + 10.0 * 8.981437683105469
Epoch 1580, val loss: 0.9513913989067078
Epoch 1590, training loss: 90.7936782836914 = 0.937788188457489 + 10.0 * 8.985589027404785
Epoch 1590, val loss: 0.9502502083778381
Epoch 1600, training loss: 90.79083251953125 = 0.9366015791893005 + 10.0 * 8.98542308807373
Epoch 1600, val loss: 0.9491172432899475
Epoch 1610, training loss: 90.77738952636719 = 0.9354305267333984 + 10.0 * 8.984195709228516
Epoch 1610, val loss: 0.9480307698249817
Epoch 1620, training loss: 90.8177719116211 = 0.9342851042747498 + 10.0 * 8.988348007202148
Epoch 1620, val loss: 0.9469590187072754
Epoch 1630, training loss: 90.82907104492188 = 0.9331655502319336 + 10.0 * 8.989590644836426
Epoch 1630, val loss: 0.9458911418914795
Epoch 1640, training loss: 90.86690521240234 = 0.9320571422576904 + 10.0 * 8.993484497070312
Epoch 1640, val loss: 0.9448498487472534
Epoch 1650, training loss: 90.87324523925781 = 0.9309550523757935 + 10.0 * 8.994229316711426
Epoch 1650, val loss: 0.9438095092773438
Epoch 1660, training loss: 90.6526870727539 = 0.9298678040504456 + 10.0 * 8.972281455993652
Epoch 1660, val loss: 0.9427509903907776
Epoch 1670, training loss: 90.7033462524414 = 0.9288116097450256 + 10.0 * 8.977453231811523
Epoch 1670, val loss: 0.9417499899864197
Epoch 1680, training loss: 90.72206115722656 = 0.9278349280357361 + 10.0 * 8.979422569274902
Epoch 1680, val loss: 0.94080650806427
Epoch 1690, training loss: 90.77340698242188 = 0.9267935156822205 + 10.0 * 8.984661102294922
Epoch 1690, val loss: 0.9398365616798401
Epoch 1700, training loss: 90.89055633544922 = 0.9257988333702087 + 10.0 * 8.996476173400879
Epoch 1700, val loss: 0.9388915300369263
Epoch 1710, training loss: 90.90715026855469 = 0.9248082041740417 + 10.0 * 8.998234748840332
Epoch 1710, val loss: 0.9379511475563049
Epoch 1720, training loss: 90.89007568359375 = 0.9238335490226746 + 10.0 * 8.996623992919922
Epoch 1720, val loss: 0.9370254874229431
Epoch 1730, training loss: 90.94657897949219 = 0.922888994216919 + 10.0 * 9.002368927001953
Epoch 1730, val loss: 0.9361225366592407
Epoch 1740, training loss: 90.93468475341797 = 0.9219449162483215 + 10.0 * 9.001274108886719
Epoch 1740, val loss: 0.9352019429206848
Epoch 1750, training loss: 90.97224426269531 = 0.9210327863693237 + 10.0 * 9.005121231079102
Epoch 1750, val loss: 0.9343147277832031
Epoch 1760, training loss: 91.0217514038086 = 0.920124888420105 + 10.0 * 9.010162353515625
Epoch 1760, val loss: 0.9334486126899719
Epoch 1770, training loss: 91.01089477539062 = 0.9192320704460144 + 10.0 * 9.00916576385498
Epoch 1770, val loss: 0.9325960278511047
Epoch 1780, training loss: 91.00747680664062 = 0.9183632135391235 + 10.0 * 9.0089111328125
Epoch 1780, val loss: 0.9317413568496704
Epoch 1790, training loss: 91.03279876708984 = 0.917485237121582 + 10.0 * 9.011530876159668
Epoch 1790, val loss: 0.9308756589889526
Epoch 1800, training loss: 90.95114135742188 = 0.9166476726531982 + 10.0 * 9.003449440002441
Epoch 1800, val loss: 0.9301000833511353
Epoch 1810, training loss: 91.0038070678711 = 0.9158374667167664 + 10.0 * 9.008796691894531
Epoch 1810, val loss: 0.9293178915977478
Epoch 1820, training loss: 91.07308959960938 = 0.9150263667106628 + 10.0 * 9.015806198120117
Epoch 1820, val loss: 0.9285584092140198
Epoch 1830, training loss: 91.08645629882812 = 0.9142166376113892 + 10.0 * 9.017224311828613
Epoch 1830, val loss: 0.9277927279472351
Epoch 1840, training loss: 91.09403228759766 = 0.9134293794631958 + 10.0 * 9.018060684204102
Epoch 1840, val loss: 0.9270553588867188
Epoch 1850, training loss: 91.00189971923828 = 0.9126533269882202 + 10.0 * 9.00892448425293
Epoch 1850, val loss: 0.9263305068016052
Epoch 1860, training loss: 90.96881866455078 = 0.9119341373443604 + 10.0 * 9.005688667297363
Epoch 1860, val loss: 0.9256343841552734
Epoch 1870, training loss: 91.11360168457031 = 0.9112207889556885 + 10.0 * 9.020237922668457
Epoch 1870, val loss: 0.9249784350395203
Epoch 1880, training loss: 90.93463134765625 = 0.9105326533317566 + 10.0 * 9.002409934997559
Epoch 1880, val loss: 0.924320638179779
Epoch 1890, training loss: 90.96063995361328 = 0.9098373055458069 + 10.0 * 9.005080223083496
Epoch 1890, val loss: 0.9236541986465454
Epoch 1900, training loss: 90.89665222167969 = 0.9091594815254211 + 10.0 * 8.998749732971191
Epoch 1900, val loss: 0.9230204820632935
Epoch 1910, training loss: 91.00186157226562 = 0.9084775447845459 + 10.0 * 9.00933837890625
Epoch 1910, val loss: 0.922428548336029
Epoch 1920, training loss: 91.07422637939453 = 0.907797634601593 + 10.0 * 9.016642570495605
Epoch 1920, val loss: 0.9218021035194397
Epoch 1930, training loss: 91.15508270263672 = 0.90711510181427 + 10.0 * 9.024797439575195
Epoch 1930, val loss: 0.9211881756782532
Epoch 1940, training loss: 91.20301055908203 = 0.9064411520957947 + 10.0 * 9.029657363891602
Epoch 1940, val loss: 0.9206016063690186
Epoch 1950, training loss: 91.19646453857422 = 0.9057751297950745 + 10.0 * 9.029068946838379
Epoch 1950, val loss: 0.9199978113174438
Epoch 1960, training loss: 91.24017333984375 = 0.9051260948181152 + 10.0 * 9.033504486083984
Epoch 1960, val loss: 0.919395923614502
Epoch 1970, training loss: 91.18730926513672 = 0.9044890403747559 + 10.0 * 9.028282165527344
Epoch 1970, val loss: 0.9188248515129089
Epoch 1980, training loss: 91.23841857910156 = 0.9038669466972351 + 10.0 * 9.033454895019531
Epoch 1980, val loss: 0.9182493686676025
Epoch 1990, training loss: 91.28255462646484 = 0.9032637476921082 + 10.0 * 9.037928581237793
Epoch 1990, val loss: 0.917702853679657
Epoch 2000, training loss: 91.29631042480469 = 0.9026622176170349 + 10.0 * 9.0393648147583
Epoch 2000, val loss: 0.9171575307846069
Epoch 2010, training loss: 91.28219604492188 = 0.9020554423332214 + 10.0 * 9.03801441192627
Epoch 2010, val loss: 0.9166264533996582
Epoch 2020, training loss: 91.29842376708984 = 0.9014622569084167 + 10.0 * 9.03969669342041
Epoch 2020, val loss: 0.9160947203636169
Epoch 2030, training loss: 91.31755828857422 = 0.9008740782737732 + 10.0 * 9.041668891906738
Epoch 2030, val loss: 0.9155808091163635
Epoch 2040, training loss: 91.31046295166016 = 0.9002969264984131 + 10.0 * 9.041016578674316
Epoch 2040, val loss: 0.9150905013084412
Epoch 2050, training loss: 91.3295669555664 = 0.8997318744659424 + 10.0 * 9.042983055114746
Epoch 2050, val loss: 0.9145877361297607
Epoch 2060, training loss: 91.37110137939453 = 0.8991738557815552 + 10.0 * 9.047192573547363
Epoch 2060, val loss: 0.914121150970459
Epoch 2070, training loss: 91.3823013305664 = 0.8986148238182068 + 10.0 * 9.048368453979492
Epoch 2070, val loss: 0.913633406162262
Epoch 2080, training loss: 91.35889434814453 = 0.8980898857116699 + 10.0 * 9.046080589294434
Epoch 2080, val loss: 0.9131974577903748
Epoch 2090, training loss: 91.34205627441406 = 0.8975589871406555 + 10.0 * 9.044449806213379
Epoch 2090, val loss: 0.9127371311187744
Epoch 2100, training loss: 91.38877868652344 = 0.8970281481742859 + 10.0 * 9.049175262451172
Epoch 2100, val loss: 0.9122928977012634
Epoch 2110, training loss: 91.43096923828125 = 0.8965125679969788 + 10.0 * 9.053445816040039
Epoch 2110, val loss: 0.9118323922157288
Epoch 2120, training loss: 91.42447662353516 = 0.8959965705871582 + 10.0 * 9.052847862243652
Epoch 2120, val loss: 0.9114151000976562
Epoch 2130, training loss: 91.42924499511719 = 0.8954916596412659 + 10.0 * 9.053375244140625
Epoch 2130, val loss: 0.9109814763069153
Epoch 2140, training loss: 91.45008087158203 = 0.8949940204620361 + 10.0 * 9.055508613586426
Epoch 2140, val loss: 0.9105787873268127
Epoch 2150, training loss: 91.38636016845703 = 0.8944933414459229 + 10.0 * 9.049186706542969
Epoch 2150, val loss: 0.9100936651229858
Epoch 2160, training loss: 91.51957702636719 = 0.8940845727920532 + 10.0 * 9.062549591064453
Epoch 2160, val loss: 0.9098160862922668
Epoch 2170, training loss: 90.96531677246094 = 0.8936072587966919 + 10.0 * 9.007170677185059
Epoch 2170, val loss: 0.9093984365463257
Epoch 2180, training loss: 91.12210845947266 = 0.8931571245193481 + 10.0 * 9.022894859313965
Epoch 2180, val loss: 0.9090142250061035
Epoch 2190, training loss: 91.0828857421875 = 0.8926917910575867 + 10.0 * 9.01901912689209
Epoch 2190, val loss: 0.9085763692855835
Epoch 2200, training loss: 91.19869232177734 = 0.8922035098075867 + 10.0 * 9.030649185180664
Epoch 2200, val loss: 0.9081263542175293
Epoch 2210, training loss: 91.28120422363281 = 0.8917387127876282 + 10.0 * 9.038946151733398
Epoch 2210, val loss: 0.9077786207199097
Epoch 2220, training loss: 91.33905029296875 = 0.8912758827209473 + 10.0 * 9.044777870178223
Epoch 2220, val loss: 0.9073378443717957
Epoch 2230, training loss: 91.4609603881836 = 0.8908224105834961 + 10.0 * 9.057013511657715
Epoch 2230, val loss: 0.9069682359695435
Epoch 2240, training loss: 91.50313568115234 = 0.8903700709342957 + 10.0 * 9.06127643585205
Epoch 2240, val loss: 0.9065911173820496
Epoch 2250, training loss: 91.4627456665039 = 0.889921247959137 + 10.0 * 9.057282447814941
Epoch 2250, val loss: 0.9062005877494812
Epoch 2260, training loss: 91.5028305053711 = 0.8894743919372559 + 10.0 * 9.061335563659668
Epoch 2260, val loss: 0.9058213233947754
Epoch 2270, training loss: 91.54680633544922 = 0.8890323638916016 + 10.0 * 9.065777778625488
Epoch 2270, val loss: 0.9054215550422668
Epoch 2280, training loss: 91.54484558105469 = 0.8885926008224487 + 10.0 * 9.065625190734863
Epoch 2280, val loss: 0.9050485491752625
Epoch 2290, training loss: 91.53288269042969 = 0.8881554007530212 + 10.0 * 9.064473152160645
Epoch 2290, val loss: 0.904718816280365
Epoch 2300, training loss: 91.57796478271484 = 0.8877173066139221 + 10.0 * 9.069025039672852
Epoch 2300, val loss: 0.9042806029319763
Epoch 2310, training loss: 91.55901336669922 = 0.8872897028923035 + 10.0 * 9.067172050476074
Epoch 2310, val loss: 0.903983473777771
Epoch 2320, training loss: 91.5589599609375 = 0.8868783712387085 + 10.0 * 9.067208290100098
Epoch 2320, val loss: 0.9036831259727478
Epoch 2330, training loss: 91.59127807617188 = 0.8864597082138062 + 10.0 * 9.07048225402832
Epoch 2330, val loss: 0.9033154845237732
Epoch 2340, training loss: 91.63388061523438 = 0.8860445022583008 + 10.0 * 9.074783325195312
Epoch 2340, val loss: 0.9029747247695923
Epoch 2350, training loss: 91.5941390991211 = 0.8856270909309387 + 10.0 * 9.07085132598877
Epoch 2350, val loss: 0.9026542901992798
Epoch 2360, training loss: 91.59201049804688 = 0.8852149248123169 + 10.0 * 9.070679664611816
Epoch 2360, val loss: 0.9023484587669373
Epoch 2370, training loss: 91.64434814453125 = 0.8848034143447876 + 10.0 * 9.07595443725586
Epoch 2370, val loss: 0.9020180106163025
Epoch 2380, training loss: 91.65077209472656 = 0.8843927979469299 + 10.0 * 9.076638221740723
Epoch 2380, val loss: 0.9016729593276978
Epoch 2390, training loss: 91.6956558227539 = 0.8839811086654663 + 10.0 * 9.081167221069336
Epoch 2390, val loss: 0.9013400673866272
Epoch 2400, training loss: 91.69038391113281 = 0.8835793137550354 + 10.0 * 9.080679893493652
Epoch 2400, val loss: 0.9010152220726013
Epoch 2410, training loss: 91.69310760498047 = 0.8831796050071716 + 10.0 * 9.080992698669434
Epoch 2410, val loss: 0.9007147550582886
Epoch 2420, training loss: 91.71796417236328 = 0.8827767372131348 + 10.0 * 9.083518981933594
Epoch 2420, val loss: 0.9004119038581848
Epoch 2430, training loss: 91.70161437988281 = 0.8823784589767456 + 10.0 * 9.081923484802246
Epoch 2430, val loss: 0.9001127481460571
Epoch 2440, training loss: 91.67987823486328 = 0.8819805979728699 + 10.0 * 9.079790115356445
Epoch 2440, val loss: 0.8998113870620728
Epoch 2450, training loss: 91.68869018554688 = 0.8815930485725403 + 10.0 * 9.080709457397461
Epoch 2450, val loss: 0.8995360136032104
Epoch 2460, training loss: 91.68387603759766 = 0.8812194466590881 + 10.0 * 9.080265998840332
Epoch 2460, val loss: 0.8992785811424255
Epoch 2470, training loss: 91.6773681640625 = 0.8808510303497314 + 10.0 * 9.079651832580566
Epoch 2470, val loss: 0.8989850282669067
Epoch 2480, training loss: 91.74815368652344 = 0.8804768919944763 + 10.0 * 9.086767196655273
Epoch 2480, val loss: 0.8987272381782532
Epoch 2490, training loss: 91.72557830810547 = 0.8800972104072571 + 10.0 * 9.084547996520996
Epoch 2490, val loss: 0.8984760642051697
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.5034782608695652
0.8650293414475115
The final CL Acc:0.49203, 0.05626, The final GNN Acc:0.86452, 0.00072
Begin epxeriment: noisy_level: 0.2 cont_weight: 10 epoch: der1:0.4 der2:0 dfr1:0.4 dfr2:0
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=2500, cl_num_layers=2, cl_num_proj_hidden=64, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=10.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.0, drop_feat_rate_1=0.4, drop_feat_rate_2=0.0, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=32, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, noisy_level=0.2, num_hidden=64, num_proj_hidden=64, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=40, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 88648])
add edge: torch.Size([2, 106338])
remove edge: torch.Size([2, 71072])
updated graph: torch.Size([2, 88762])
=== Noisy graph ===
/home/project-robust-contrastive/Robust-Contrastive-Learning/model.py:227: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 108.0295181274414 = 2.206596851348877 + 10.0 * 10.582292556762695
Epoch 0, val loss: 2.2037413120269775
Epoch 10, training loss: 108.01170349121094 = 2.1901780366897583 + 10.0 * 10.582152366638184
Epoch 10, val loss: 2.1883602142333984
Epoch 20, training loss: 108.0057601928711 = 2.188336730003357 + 10.0 * 10.581742286682129
Epoch 20, val loss: 2.185981512069702
Epoch 30, training loss: 107.99105834960938 = 2.1868834495544434 + 10.0 * 10.58041763305664
Epoch 30, val loss: 2.1840667724609375
Epoch 40, training loss: 107.9421615600586 = 2.1861525774002075 + 10.0 * 10.575601577758789
Epoch 40, val loss: 2.183156728744507
Epoch 50, training loss: 107.77073669433594 = 2.186091899871826 + 10.0 * 10.558465003967285
Epoch 50, val loss: 2.1832900047302246
Epoch 60, training loss: 107.27206420898438 = 2.186755895614624 + 10.0 * 10.50853157043457
Epoch 60, val loss: 2.1844451427459717
Epoch 70, training loss: 106.0782470703125 = 2.1884716749191284 + 10.0 * 10.388978004455566
Epoch 70, val loss: 2.1868162155151367
Epoch 80, training loss: 103.87242126464844 = 2.1883755922317505 + 10.0 * 10.168404579162598
Epoch 80, val loss: 2.1873080730438232
Epoch 90, training loss: 101.90998840332031 = 2.188367009162903 + 10.0 * 9.972162246704102
Epoch 90, val loss: 2.1878936290740967
Epoch 100, training loss: 101.00526428222656 = 2.1869852542877197 + 10.0 * 9.881827354431152
Epoch 100, val loss: 2.1867947578430176
Epoch 110, training loss: 100.05158233642578 = 2.1852304935455322 + 10.0 * 9.78663444519043
Epoch 110, val loss: 2.1851792335510254
Epoch 120, training loss: 98.6388168334961 = 2.1841347217559814 + 10.0 * 9.645467758178711
Epoch 120, val loss: 2.184129238128662
Epoch 130, training loss: 96.7650146484375 = 2.1844388246536255 + 10.0 * 9.458057403564453
Epoch 130, val loss: 2.1844308376312256
Epoch 140, training loss: 95.59818267822266 = 2.184916853904724 + 10.0 * 9.341326713562012
Epoch 140, val loss: 2.1847782135009766
Epoch 150, training loss: 95.27752685546875 = 2.184085488319397 + 10.0 * 9.309344291687012
Epoch 150, val loss: 2.1838560104370117
Epoch 160, training loss: 95.0776138305664 = 2.182045817375183 + 10.0 * 9.289556503295898
Epoch 160, val loss: 2.181835174560547
Epoch 170, training loss: 94.91041564941406 = 2.181113362312317 + 10.0 * 9.272930145263672
Epoch 170, val loss: 2.1809182167053223
Epoch 180, training loss: 94.69648742675781 = 2.1808760166168213 + 10.0 * 9.251561164855957
Epoch 180, val loss: 2.180607318878174
Epoch 190, training loss: 94.35870361328125 = 2.1808857917785645 + 10.0 * 9.217782020568848
Epoch 190, val loss: 2.180514097213745
Epoch 200, training loss: 93.74884796142578 = 2.1817827224731445 + 10.0 * 9.156705856323242
Epoch 200, val loss: 2.181288242340088
Epoch 210, training loss: 92.88519287109375 = 2.1836212873458862 + 10.0 * 9.070157051086426
Epoch 210, val loss: 2.182981014251709
Epoch 220, training loss: 92.29743194580078 = 2.1847403049468994 + 10.0 * 9.011269569396973
Epoch 220, val loss: 2.1839089393615723
Epoch 230, training loss: 91.78252410888672 = 2.184577465057373 + 10.0 * 8.959794998168945
Epoch 230, val loss: 2.1836600303649902
Epoch 240, training loss: 91.3385238647461 = 2.1842278242111206 + 10.0 * 8.915430068969727
Epoch 240, val loss: 2.183323860168457
Epoch 250, training loss: 91.13787841796875 = 2.1840574741363525 + 10.0 * 8.895381927490234
Epoch 250, val loss: 2.1830976009368896
Epoch 260, training loss: 90.9249038696289 = 2.183567523956299 + 10.0 * 8.874133110046387
Epoch 260, val loss: 2.1826438903808594
Epoch 270, training loss: 90.6448974609375 = 2.1837470531463623 + 10.0 * 8.846115112304688
Epoch 270, val loss: 2.1828222274780273
Epoch 280, training loss: 90.35836029052734 = 2.184004068374634 + 10.0 * 8.817435264587402
Epoch 280, val loss: 2.1831064224243164
Epoch 290, training loss: 90.1636734008789 = 2.184000253677368 + 10.0 * 8.797967910766602
Epoch 290, val loss: 2.1830766201019287
Epoch 300, training loss: 90.04015350341797 = 2.183624744415283 + 10.0 * 8.785653114318848
Epoch 300, val loss: 2.1826953887939453
Epoch 310, training loss: 89.89453887939453 = 2.183278799057007 + 10.0 * 8.771125793457031
Epoch 310, val loss: 2.1823291778564453
Epoch 320, training loss: 89.7265853881836 = 2.183173418045044 + 10.0 * 8.754341125488281
Epoch 320, val loss: 2.182204484939575
Epoch 330, training loss: 89.57958984375 = 2.1829835176467896 + 10.0 * 8.739660263061523
Epoch 330, val loss: 2.1820120811462402
Epoch 340, training loss: 89.48774719238281 = 2.1827269792556763 + 10.0 * 8.730502128601074
Epoch 340, val loss: 2.1817288398742676
Epoch 350, training loss: 89.43121337890625 = 2.182440400123596 + 10.0 * 8.72487735748291
Epoch 350, val loss: 2.1814231872558594
Epoch 360, training loss: 89.380615234375 = 2.182142496109009 + 10.0 * 8.719846725463867
Epoch 360, val loss: 2.181098461151123
Epoch 370, training loss: 89.32012939453125 = 2.1818859577178955 + 10.0 * 8.713824272155762
Epoch 370, val loss: 2.180837631225586
Epoch 380, training loss: 89.24153900146484 = 2.181681752204895 + 10.0 * 8.705986022949219
Epoch 380, val loss: 2.1806163787841797
Epoch 390, training loss: 89.1572265625 = 2.181529998779297 + 10.0 * 8.697568893432617
Epoch 390, val loss: 2.180452346801758
Epoch 400, training loss: 89.0372085571289 = 2.1814417839050293 + 10.0 * 8.685576438903809
Epoch 400, val loss: 2.180349826812744
Epoch 410, training loss: 88.92750549316406 = 2.181423544883728 + 10.0 * 8.67460823059082
Epoch 410, val loss: 2.1803126335144043
Epoch 420, training loss: 88.84551239013672 = 2.1813868284225464 + 10.0 * 8.666412353515625
Epoch 420, val loss: 2.1802234649658203
Epoch 430, training loss: 88.7764663696289 = 2.1813037395477295 + 10.0 * 8.659516334533691
Epoch 430, val loss: 2.180128812789917
Epoch 440, training loss: 88.71131896972656 = 2.1812326908111572 + 10.0 * 8.653008460998535
Epoch 440, val loss: 2.1800060272216797
Epoch 450, training loss: 88.6395263671875 = 2.1811635494232178 + 10.0 * 8.64583683013916
Epoch 450, val loss: 2.1799259185791016
Epoch 460, training loss: 88.56255340576172 = 2.1811670064926147 + 10.0 * 8.638138771057129
Epoch 460, val loss: 2.1799044609069824
Epoch 470, training loss: 88.47156524658203 = 2.181192636489868 + 10.0 * 8.629037857055664
Epoch 470, val loss: 2.1798813343048096
Epoch 480, training loss: 88.39048767089844 = 2.181236147880554 + 10.0 * 8.620924949645996
Epoch 480, val loss: 2.17987322807312
Epoch 490, training loss: 88.31575775146484 = 2.1812796592712402 + 10.0 * 8.613447189331055
Epoch 490, val loss: 2.1798739433288574
Epoch 500, training loss: 88.25910949707031 = 2.1812607049942017 + 10.0 * 8.60778522491455
Epoch 500, val loss: 2.179821014404297
Epoch 510, training loss: 88.1767578125 = 2.181198000907898 + 10.0 * 8.599555969238281
Epoch 510, val loss: 2.179739475250244
Epoch 520, training loss: 88.10877227783203 = 2.181158661842346 + 10.0 * 8.592761039733887
Epoch 520, val loss: 2.179680109024048
Epoch 530, training loss: 88.02571868896484 = 2.1811718940734863 + 10.0 * 8.584454536437988
Epoch 530, val loss: 2.1796321868896484
Epoch 540, training loss: 87.95425415039062 = 2.1811798810958862 + 10.0 * 8.577306747436523
Epoch 540, val loss: 2.179593801498413
Epoch 550, training loss: 87.90393829345703 = 2.181170344352722 + 10.0 * 8.572277069091797
Epoch 550, val loss: 2.1795449256896973
Epoch 560, training loss: 87.84957885742188 = 2.181119441986084 + 10.0 * 8.566845893859863
Epoch 560, val loss: 2.179469108581543
Epoch 570, training loss: 87.80703735351562 = 2.1810622215270996 + 10.0 * 8.562597274780273
Epoch 570, val loss: 2.1793742179870605
Epoch 580, training loss: 87.77116394042969 = 2.1809916496276855 + 10.0 * 8.559017181396484
Epoch 580, val loss: 2.1792654991149902
Epoch 590, training loss: 87.75888061523438 = 2.180909276008606 + 10.0 * 8.5577974319458
Epoch 590, val loss: 2.179159164428711
Epoch 600, training loss: 87.7279281616211 = 2.1808258295059204 + 10.0 * 8.554710388183594
Epoch 600, val loss: 2.1790218353271484
Epoch 610, training loss: 87.67932891845703 = 2.1807297468185425 + 10.0 * 8.549860000610352
Epoch 610, val loss: 2.1789212226867676
Epoch 620, training loss: 87.64708709716797 = 2.1806702613830566 + 10.0 * 8.54664134979248
Epoch 620, val loss: 2.178838014602661
Epoch 630, training loss: 87.61827850341797 = 2.1806191205978394 + 10.0 * 8.543766021728516
Epoch 630, val loss: 2.1787590980529785
Epoch 640, training loss: 87.59132385253906 = 2.1805672645568848 + 10.0 * 8.541075706481934
Epoch 640, val loss: 2.178678035736084
Epoch 650, training loss: 87.60535430908203 = 2.180519461631775 + 10.0 * 8.54248332977295
Epoch 650, val loss: 2.1785905361175537
Epoch 660, training loss: 87.54794311523438 = 2.180427670478821 + 10.0 * 8.536751747131348
Epoch 660, val loss: 2.178490400314331
Epoch 670, training loss: 87.52762603759766 = 2.180365204811096 + 10.0 * 8.5347261428833
Epoch 670, val loss: 2.17840576171875
Epoch 680, training loss: 87.50696563720703 = 2.1803152561187744 + 10.0 * 8.532665252685547
Epoch 680, val loss: 2.1783287525177
Epoch 690, training loss: 87.4844970703125 = 2.1802706718444824 + 10.0 * 8.530423164367676
Epoch 690, val loss: 2.1782588958740234
Epoch 700, training loss: 87.46214294433594 = 2.180241107940674 + 10.0 * 8.528189659118652
Epoch 700, val loss: 2.1782007217407227
Epoch 710, training loss: 87.45467376708984 = 2.180204391479492 + 10.0 * 8.527446746826172
Epoch 710, val loss: 2.178157091140747
Epoch 720, training loss: 87.42451477050781 = 2.180189371109009 + 10.0 * 8.524432182312012
Epoch 720, val loss: 2.178091049194336
Epoch 730, training loss: 87.39019012451172 = 2.180163621902466 + 10.0 * 8.521002769470215
Epoch 730, val loss: 2.178053379058838
Epoch 740, training loss: 87.36392974853516 = 2.1801748275756836 + 10.0 * 8.518375396728516
Epoch 740, val loss: 2.1780381202697754
Epoch 750, training loss: 87.33318328857422 = 2.1802079677581787 + 10.0 * 8.515297889709473
Epoch 750, val loss: 2.1780333518981934
Epoch 760, training loss: 87.31146240234375 = 2.1802403926849365 + 10.0 * 8.51312255859375
Epoch 760, val loss: 2.17803955078125
Epoch 770, training loss: 87.32121276855469 = 2.180251717567444 + 10.0 * 8.5140962600708
Epoch 770, val loss: 2.1780238151550293
Epoch 780, training loss: 87.25760650634766 = 2.180262565612793 + 10.0 * 8.507734298706055
Epoch 780, val loss: 2.1779956817626953
Epoch 790, training loss: 87.22599792480469 = 2.1802879571914673 + 10.0 * 8.504570960998535
Epoch 790, val loss: 2.1779940128326416
Epoch 800, training loss: 87.2008285522461 = 2.180316686630249 + 10.0 * 8.50205135345459
Epoch 800, val loss: 2.1779944896698
Epoch 810, training loss: 87.21534729003906 = 2.1803213357925415 + 10.0 * 8.50350284576416
Epoch 810, val loss: 2.177989959716797
Epoch 820, training loss: 87.16400909423828 = 2.180332660675049 + 10.0 * 8.498367309570312
Epoch 820, val loss: 2.177950382232666
Epoch 830, training loss: 87.13675689697266 = 2.1803210973739624 + 10.0 * 8.495643615722656
Epoch 830, val loss: 2.1779136657714844
Epoch 840, training loss: 87.11681365966797 = 2.1802968978881836 + 10.0 * 8.493651390075684
Epoch 840, val loss: 2.177877187728882
Epoch 850, training loss: 87.09856414794922 = 2.1802903413772583 + 10.0 * 8.491827011108398
Epoch 850, val loss: 2.177837371826172
Epoch 860, training loss: 87.14368438720703 = 2.1802631616592407 + 10.0 * 8.496342658996582
Epoch 860, val loss: 2.1777806282043457
Epoch 870, training loss: 87.0802230834961 = 2.1802022457122803 + 10.0 * 8.490002632141113
Epoch 870, val loss: 2.17771053314209
Epoch 880, training loss: 87.05751037597656 = 2.180163025856018 + 10.0 * 8.4877347946167
Epoch 880, val loss: 2.1776480674743652
Epoch 890, training loss: 87.04449462890625 = 2.180129885673523 + 10.0 * 8.48643684387207
Epoch 890, val loss: 2.1775870323181152
Epoch 900, training loss: 87.031982421875 = 2.180086851119995 + 10.0 * 8.485189437866211
Epoch 900, val loss: 2.177525043487549
Epoch 910, training loss: 87.01968383789062 = 2.1800448894500732 + 10.0 * 8.483963966369629
Epoch 910, val loss: 2.1774630546569824
Epoch 920, training loss: 87.01744079589844 = 2.179986000061035 + 10.0 * 8.483745574951172
Epoch 920, val loss: 2.1774072647094727
Epoch 930, training loss: 87.00796508789062 = 2.1799691915512085 + 10.0 * 8.482799530029297
Epoch 930, val loss: 2.177330493927002
Epoch 940, training loss: 87.00634002685547 = 2.179917097091675 + 10.0 * 8.48264217376709
Epoch 940, val loss: 2.177262306213379
Epoch 950, training loss: 86.97805786132812 = 2.1798555850982666 + 10.0 * 8.479820251464844
Epoch 950, val loss: 2.177203893661499
Epoch 960, training loss: 86.96199798583984 = 2.1798224449157715 + 10.0 * 8.478217124938965
Epoch 960, val loss: 2.1771466732025146
Epoch 970, training loss: 86.95356750488281 = 2.1797869205474854 + 10.0 * 8.477377891540527
Epoch 970, val loss: 2.1770949363708496
Epoch 980, training loss: 86.9517593383789 = 2.179758310317993 + 10.0 * 8.477200508117676
Epoch 980, val loss: 2.1770410537719727
Epoch 990, training loss: 86.93102264404297 = 2.1797107458114624 + 10.0 * 8.475131034851074
Epoch 990, val loss: 2.1769919395446777
Epoch 1000, training loss: 86.91962432861328 = 2.179676651954651 + 10.0 * 8.473994255065918
Epoch 1000, val loss: 2.1769375801086426
Epoch 1010, training loss: 86.9178466796875 = 2.17965829372406 + 10.0 * 8.4738187789917
Epoch 1010, val loss: 2.176886558532715
Epoch 1020, training loss: 86.90161895751953 = 2.1795992851257324 + 10.0 * 8.47220230102539
Epoch 1020, val loss: 2.1768360137939453
Epoch 1030, training loss: 86.93943786621094 = 2.179576873779297 + 10.0 * 8.475985527038574
Epoch 1030, val loss: 2.1767759323120117
Epoch 1040, training loss: 86.88823699951172 = 2.1795225143432617 + 10.0 * 8.470871925354004
Epoch 1040, val loss: 2.1767163276672363
Epoch 1050, training loss: 86.8664321899414 = 2.179482102394104 + 10.0 * 8.468694686889648
Epoch 1050, val loss: 2.176666259765625
Epoch 1060, training loss: 86.85570526123047 = 2.1794593334198 + 10.0 * 8.46762466430664
Epoch 1060, val loss: 2.176619052886963
Epoch 1070, training loss: 86.84346771240234 = 2.179428219795227 + 10.0 * 8.46640396118164
Epoch 1070, val loss: 2.176574230194092
Epoch 1080, training loss: 86.8358154296875 = 2.1794068813323975 + 10.0 * 8.465641021728516
Epoch 1080, val loss: 2.176530361175537
Epoch 1090, training loss: 86.82893371582031 = 2.1793692111968994 + 10.0 * 8.464956283569336
Epoch 1090, val loss: 2.176480770111084
Epoch 1100, training loss: 86.82135772705078 = 2.179331421852112 + 10.0 * 8.464202880859375
Epoch 1100, val loss: 2.1764261722564697
Epoch 1110, training loss: 86.81952667236328 = 2.179295301437378 + 10.0 * 8.464022636413574
Epoch 1110, val loss: 2.176384449005127
Epoch 1120, training loss: 86.80818939208984 = 2.1792702674865723 + 10.0 * 8.462892532348633
Epoch 1120, val loss: 2.1763243675231934
Epoch 1130, training loss: 86.7851791381836 = 2.179223418235779 + 10.0 * 8.460596084594727
Epoch 1130, val loss: 2.1762733459472656
Epoch 1140, training loss: 86.77655792236328 = 2.179194211959839 + 10.0 * 8.459736824035645
Epoch 1140, val loss: 2.1762351989746094
Epoch 1150, training loss: 86.7733383178711 = 2.1791735887527466 + 10.0 * 8.459416389465332
Epoch 1150, val loss: 2.1761913299560547
Epoch 1160, training loss: 86.76559448242188 = 2.1791363954544067 + 10.0 * 8.458645820617676
Epoch 1160, val loss: 2.176138401031494
Epoch 1170, training loss: 86.75101470947266 = 2.1790990829467773 + 10.0 * 8.457191467285156
Epoch 1170, val loss: 2.176093339920044
Epoch 1180, training loss: 86.74006652832031 = 2.1790696382522583 + 10.0 * 8.456099510192871
Epoch 1180, val loss: 2.1760482788085938
Epoch 1190, training loss: 86.73020935058594 = 2.1790478229522705 + 10.0 * 8.455116271972656
Epoch 1190, val loss: 2.176006317138672
Epoch 1200, training loss: 86.73360443115234 = 2.1790261268615723 + 10.0 * 8.45545768737793
Epoch 1200, val loss: 2.1759591102600098
Epoch 1210, training loss: 86.73535919189453 = 2.17896568775177 + 10.0 * 8.455639839172363
Epoch 1210, val loss: 2.1759090423583984
Epoch 1220, training loss: 86.71488189697266 = 2.178939700126648 + 10.0 * 8.453594207763672
Epoch 1220, val loss: 2.175856113433838
Epoch 1230, training loss: 86.70020294189453 = 2.1788992881774902 + 10.0 * 8.452130317687988
Epoch 1230, val loss: 2.1758055686950684
Epoch 1240, training loss: 86.69489288330078 = 2.1788625717163086 + 10.0 * 8.451602935791016
Epoch 1240, val loss: 2.175764560699463
Epoch 1250, training loss: 86.70221710205078 = 2.178836464881897 + 10.0 * 8.452338218688965
Epoch 1250, val loss: 2.1757209300994873
Epoch 1260, training loss: 86.68830871582031 = 2.1787859201431274 + 10.0 * 8.450952529907227
Epoch 1260, val loss: 2.175663471221924
Epoch 1270, training loss: 86.67390441894531 = 2.1787495613098145 + 10.0 * 8.449515342712402
Epoch 1270, val loss: 2.1756105422973633
Epoch 1280, training loss: 86.66555786132812 = 2.178715705871582 + 10.0 * 8.448683738708496
Epoch 1280, val loss: 2.1755661964416504
Epoch 1290, training loss: 86.65882110595703 = 2.1786863803863525 + 10.0 * 8.448013305664062
Epoch 1290, val loss: 2.175525665283203
Epoch 1300, training loss: 86.6520767211914 = 2.1786534786224365 + 10.0 * 8.447342872619629
Epoch 1300, val loss: 2.1754839420318604
Epoch 1310, training loss: 86.66062927246094 = 2.1786181926727295 + 10.0 * 8.448201179504395
Epoch 1310, val loss: 2.175443649291992
Epoch 1320, training loss: 86.69464874267578 = 2.1785762310028076 + 10.0 * 8.451607704162598
Epoch 1320, val loss: 2.1753745079040527
Epoch 1330, training loss: 86.64997863769531 = 2.178506374359131 + 10.0 * 8.447147369384766
Epoch 1330, val loss: 2.175307512283325
Epoch 1340, training loss: 86.6348876953125 = 2.178475260734558 + 10.0 * 8.44564151763916
Epoch 1340, val loss: 2.1752676963806152
Epoch 1350, training loss: 86.62313842773438 = 2.1784504652023315 + 10.0 * 8.44446849822998
Epoch 1350, val loss: 2.1752309799194336
Epoch 1360, training loss: 86.61467742919922 = 2.1784250736236572 + 10.0 * 8.443624496459961
Epoch 1360, val loss: 2.1751956939697266
Epoch 1370, training loss: 86.60803985595703 = 2.1784013509750366 + 10.0 * 8.442964553833008
Epoch 1370, val loss: 2.175161361694336
Epoch 1380, training loss: 86.60144805908203 = 2.1783746480941772 + 10.0 * 8.442307472229004
Epoch 1380, val loss: 2.1751255989074707
Epoch 1390, training loss: 86.59525299072266 = 2.1783493757247925 + 10.0 * 8.441690444946289
Epoch 1390, val loss: 2.175090789794922
Epoch 1400, training loss: 86.61864471435547 = 2.1783275604248047 + 10.0 * 8.444031715393066
Epoch 1400, val loss: 2.175056219100952
Epoch 1410, training loss: 86.61453247070312 = 2.178270936012268 + 10.0 * 8.443626403808594
Epoch 1410, val loss: 2.1749954223632812
Epoch 1420, training loss: 86.58841705322266 = 2.1782290935516357 + 10.0 * 8.441019058227539
Epoch 1420, val loss: 2.17495059967041
Epoch 1430, training loss: 86.57539367675781 = 2.178205132484436 + 10.0 * 8.439718246459961
Epoch 1430, val loss: 2.174915075302124
Epoch 1440, training loss: 86.56781768798828 = 2.1781809329986572 + 10.0 * 8.438962936401367
Epoch 1440, val loss: 2.1748814582824707
Epoch 1450, training loss: 86.56095886230469 = 2.1781580448150635 + 10.0 * 8.43828010559082
Epoch 1450, val loss: 2.1748509407043457
Epoch 1460, training loss: 86.55613708496094 = 2.1781333684921265 + 10.0 * 8.437800407409668
Epoch 1460, val loss: 2.174818515777588
Epoch 1470, training loss: 86.58859252929688 = 2.178109049797058 + 10.0 * 8.441048622131348
Epoch 1470, val loss: 2.1747853755950928
Epoch 1480, training loss: 86.58920288085938 = 2.178072452545166 + 10.0 * 8.441113471984863
Epoch 1480, val loss: 2.1747217178344727
Epoch 1490, training loss: 86.54931640625 = 2.1780093908309937 + 10.0 * 8.43713092803955
Epoch 1490, val loss: 2.174675941467285
Epoch 1500, training loss: 86.53715515136719 = 2.1779874563217163 + 10.0 * 8.435916900634766
Epoch 1500, val loss: 2.1746416091918945
Epoch 1510, training loss: 86.53135681152344 = 2.1779611110687256 + 10.0 * 8.435338973999023
Epoch 1510, val loss: 2.174607276916504
Epoch 1520, training loss: 86.52645874023438 = 2.177938938140869 + 10.0 * 8.43485164642334
Epoch 1520, val loss: 2.174575090408325
Epoch 1530, training loss: 86.52139282226562 = 2.1779098510742188 + 10.0 * 8.434348106384277
Epoch 1530, val loss: 2.17454195022583
Epoch 1540, training loss: 86.51679992675781 = 2.177883267402649 + 10.0 * 8.433892250061035
Epoch 1540, val loss: 2.1745071411132812
Epoch 1550, training loss: 86.51325225830078 = 2.1778523921966553 + 10.0 * 8.433540344238281
Epoch 1550, val loss: 2.174470901489258
Epoch 1560, training loss: 86.56778717041016 = 2.177803158760071 + 10.0 * 8.438998222351074
Epoch 1560, val loss: 2.1744260787963867
Epoch 1570, training loss: 86.52287292480469 = 2.177780270576477 + 10.0 * 8.43450927734375
Epoch 1570, val loss: 2.174375057220459
Epoch 1580, training loss: 86.50788879394531 = 2.177738308906555 + 10.0 * 8.433014869689941
Epoch 1580, val loss: 2.1743292808532715
Epoch 1590, training loss: 86.49860382080078 = 2.177703380584717 + 10.0 * 8.432089805603027
Epoch 1590, val loss: 2.1742920875549316
Epoch 1600, training loss: 86.49396514892578 = 2.177671194076538 + 10.0 * 8.431629180908203
Epoch 1600, val loss: 2.174254894256592
Epoch 1610, training loss: 86.49964904785156 = 2.177639365196228 + 10.0 * 8.43220043182373
Epoch 1610, val loss: 2.1742215156555176
Epoch 1620, training loss: 86.49894714355469 = 2.1775983572006226 + 10.0 * 8.432134628295898
Epoch 1620, val loss: 2.174170970916748
Epoch 1630, training loss: 86.48770904541016 = 2.1775612831115723 + 10.0 * 8.431015014648438
Epoch 1630, val loss: 2.1741223335266113
Epoch 1640, training loss: 86.48088073730469 = 2.1775283813476562 + 10.0 * 8.43033504486084
Epoch 1640, val loss: 2.174086093902588
Epoch 1650, training loss: 86.47541046142578 = 2.177498459815979 + 10.0 * 8.429791450500488
Epoch 1650, val loss: 2.1740479469299316
Epoch 1660, training loss: 86.47084045410156 = 2.1774654388427734 + 10.0 * 8.429337501525879
Epoch 1660, val loss: 2.1740121841430664
Epoch 1670, training loss: 86.47212219238281 = 2.1774264574050903 + 10.0 * 8.429469108581543
Epoch 1670, val loss: 2.1739730834960938
Epoch 1680, training loss: 86.49116516113281 = 2.1773940324783325 + 10.0 * 8.431377410888672
Epoch 1680, val loss: 2.173923969268799
Epoch 1690, training loss: 86.46575927734375 = 2.177345633506775 + 10.0 * 8.428841590881348
Epoch 1690, val loss: 2.173877716064453
Epoch 1700, training loss: 86.45844268798828 = 2.1773178577423096 + 10.0 * 8.428112983703613
Epoch 1700, val loss: 2.173835277557373
Epoch 1710, training loss: 86.45761108398438 = 2.1772797107696533 + 10.0 * 8.428033828735352
Epoch 1710, val loss: 2.173797607421875
Epoch 1720, training loss: 86.50614929199219 = 2.1772435903549194 + 10.0 * 8.432889938354492
Epoch 1720, val loss: 2.1737570762634277
Epoch 1730, training loss: 86.46479797363281 = 2.1771918535232544 + 10.0 * 8.428760528564453
Epoch 1730, val loss: 2.1736931800842285
Epoch 1740, training loss: 86.4490966796875 = 2.1771563291549683 + 10.0 * 8.427194595336914
Epoch 1740, val loss: 2.1736574172973633
Epoch 1750, training loss: 86.44195556640625 = 2.177124857902527 + 10.0 * 8.426483154296875
Epoch 1750, val loss: 2.173616886138916
Epoch 1760, training loss: 86.4377670288086 = 2.1770914793014526 + 10.0 * 8.426067352294922
Epoch 1760, val loss: 2.173582077026367
Epoch 1770, training loss: 86.43417358398438 = 2.1770650148391724 + 10.0 * 8.425710678100586
Epoch 1770, val loss: 2.173546314239502
Epoch 1780, training loss: 86.43038940429688 = 2.1770317554473877 + 10.0 * 8.425335884094238
Epoch 1780, val loss: 2.1735095977783203
Epoch 1790, training loss: 86.42977142333984 = 2.177004098892212 + 10.0 * 8.425276756286621
Epoch 1790, val loss: 2.1734743118286133
Epoch 1800, training loss: 86.4482421875 = 2.1769641637802124 + 10.0 * 8.427127838134766
Epoch 1800, val loss: 2.173430919647217
Epoch 1810, training loss: 86.4347152709961 = 2.1769087314605713 + 10.0 * 8.425780296325684
Epoch 1810, val loss: 2.173368453979492
Epoch 1820, training loss: 86.41934204101562 = 2.1768715381622314 + 10.0 * 8.424246788024902
Epoch 1820, val loss: 2.173327684402466
Epoch 1830, training loss: 86.41422271728516 = 2.1768434047698975 + 10.0 * 8.423738479614258
Epoch 1830, val loss: 2.1732959747314453
Epoch 1840, training loss: 86.42676544189453 = 2.176813006401062 + 10.0 * 8.424995422363281
Epoch 1840, val loss: 2.17326021194458
Epoch 1850, training loss: 86.41030883789062 = 2.1767574548721313 + 10.0 * 8.423355102539062
Epoch 1850, val loss: 2.1732051372528076
Epoch 1860, training loss: 86.41214752197266 = 2.176718235015869 + 10.0 * 8.423542976379395
Epoch 1860, val loss: 2.173159122467041
Epoch 1870, training loss: 86.40614318847656 = 2.1766929626464844 + 10.0 * 8.422945022583008
Epoch 1870, val loss: 2.173129081726074
Epoch 1880, training loss: 86.39823913574219 = 2.1766611337661743 + 10.0 * 8.422158241271973
Epoch 1880, val loss: 2.1730949878692627
Epoch 1890, training loss: 86.39447784423828 = 2.176637887954712 + 10.0 * 8.421784400939941
Epoch 1890, val loss: 2.173065423965454
Epoch 1900, training loss: 86.39878845214844 = 2.1766140460968018 + 10.0 * 8.42221736907959
Epoch 1900, val loss: 2.173031806945801
Epoch 1910, training loss: 86.39191436767578 = 2.176570415496826 + 10.0 * 8.421534538269043
Epoch 1910, val loss: 2.1729893684387207
Epoch 1920, training loss: 86.39024353027344 = 2.1765313148498535 + 10.0 * 8.421371459960938
Epoch 1920, val loss: 2.1729464530944824
Epoch 1930, training loss: 86.38313293457031 = 2.176498293876648 + 10.0 * 8.420663833618164
Epoch 1930, val loss: 2.172912120819092
Epoch 1940, training loss: 86.3786849975586 = 2.176473617553711 + 10.0 * 8.420221328735352
Epoch 1940, val loss: 2.172884225845337
Epoch 1950, training loss: 86.3753890991211 = 2.1764473915100098 + 10.0 * 8.419894218444824
Epoch 1950, val loss: 2.1728525161743164
Epoch 1960, training loss: 86.3912124633789 = 2.1764169931411743 + 10.0 * 8.421480178833008
Epoch 1960, val loss: 2.1728227138519287
Epoch 1970, training loss: 86.38291931152344 = 2.176369547843933 + 10.0 * 8.420655250549316
Epoch 1970, val loss: 2.172766923904419
Epoch 1980, training loss: 86.37329864501953 = 2.176332473754883 + 10.0 * 8.419696807861328
Epoch 1980, val loss: 2.1727261543273926
Epoch 1990, training loss: 86.36549377441406 = 2.176297664642334 + 10.0 * 8.418919563293457
Epoch 1990, val loss: 2.17269229888916
Epoch 2000, training loss: 86.36133575439453 = 2.1762741804122925 + 10.0 * 8.418505668640137
Epoch 2000, val loss: 2.1726646423339844
Epoch 2010, training loss: 86.37614440917969 = 2.1762447357177734 + 10.0 * 8.419989585876465
Epoch 2010, val loss: 2.1726298332214355
Epoch 2020, training loss: 86.35748291015625 = 2.1762006282806396 + 10.0 * 8.41812801361084
Epoch 2020, val loss: 2.1725831031799316
Epoch 2030, training loss: 86.35282897949219 = 2.176165223121643 + 10.0 * 8.4176664352417
Epoch 2030, val loss: 2.1725478172302246
Epoch 2040, training loss: 86.34929656982422 = 2.1761356592178345 + 10.0 * 8.417316436767578
Epoch 2040, val loss: 2.1725144386291504
Epoch 2050, training loss: 86.34667205810547 = 2.176110863685608 + 10.0 * 8.4170560836792
Epoch 2050, val loss: 2.172487497329712
Epoch 2060, training loss: 86.34257507324219 = 2.1760846376419067 + 10.0 * 8.416648864746094
Epoch 2060, val loss: 2.1724581718444824
Epoch 2070, training loss: 86.34049224853516 = 2.1760568618774414 + 10.0 * 8.416443824768066
Epoch 2070, val loss: 2.172428607940674
Epoch 2080, training loss: 86.34634399414062 = 2.1760250329971313 + 10.0 * 8.417032241821289
Epoch 2080, val loss: 2.172398805618286
Epoch 2090, training loss: 86.3407974243164 = 2.1759811639785767 + 10.0 * 8.416481971740723
Epoch 2090, val loss: 2.1723482608795166
Epoch 2100, training loss: 86.35440063476562 = 2.175935745239258 + 10.0 * 8.4178466796875
Epoch 2100, val loss: 2.172311305999756
Epoch 2110, training loss: 86.32964324951172 = 2.1759122610092163 + 10.0 * 8.415372848510742
Epoch 2110, val loss: 2.172274589538574
Epoch 2120, training loss: 86.32876586914062 = 2.175886869430542 + 10.0 * 8.415287971496582
Epoch 2120, val loss: 2.172243595123291
Epoch 2130, training loss: 86.32244873046875 = 2.1758575439453125 + 10.0 * 8.41465950012207
Epoch 2130, val loss: 2.172215700149536
Epoch 2140, training loss: 86.32110595703125 = 2.175831437110901 + 10.0 * 8.41452693939209
Epoch 2140, val loss: 2.1721901893615723
Epoch 2150, training loss: 86.35357666015625 = 2.1758060455322266 + 10.0 * 8.417777061462402
Epoch 2150, val loss: 2.172163486480713
Epoch 2160, training loss: 86.32279205322266 = 2.1757543087005615 + 10.0 * 8.414704322814941
Epoch 2160, val loss: 2.1721062660217285
Epoch 2170, training loss: 86.31908416748047 = 2.1757251024246216 + 10.0 * 8.414335250854492
Epoch 2170, val loss: 2.17207407951355
Epoch 2180, training loss: 86.31925201416016 = 2.1756943464279175 + 10.0 * 8.414355278015137
Epoch 2180, val loss: 2.172041416168213
Epoch 2190, training loss: 86.3231201171875 = 2.175667405128479 + 10.0 * 8.414745330810547
Epoch 2190, val loss: 2.1720120906829834
Epoch 2200, training loss: 86.3081283569336 = 2.175622344017029 + 10.0 * 8.413250923156738
Epoch 2200, val loss: 2.171971559524536
Epoch 2210, training loss: 86.3022232055664 = 2.1756012439727783 + 10.0 * 8.412662506103516
Epoch 2210, val loss: 2.171948194503784
Epoch 2220, training loss: 86.29788970947266 = 2.1755754947662354 + 10.0 * 8.4122314453125
Epoch 2220, val loss: 2.1719210147857666
Epoch 2230, training loss: 86.29589080810547 = 2.175552487373352 + 10.0 * 8.412034034729004
Epoch 2230, val loss: 2.1718969345092773
Epoch 2240, training loss: 86.30982971191406 = 2.17552649974823 + 10.0 * 8.413430213928223
Epoch 2240, val loss: 2.1718673706054688
Epoch 2250, training loss: 86.29431915283203 = 2.1754871606826782 + 10.0 * 8.411883354187012
Epoch 2250, val loss: 2.171825647354126
Epoch 2260, training loss: 86.29307556152344 = 2.17545223236084 + 10.0 * 8.411762237548828
Epoch 2260, val loss: 2.1717910766601562
Epoch 2270, training loss: 86.30342864990234 = 2.1754270792007446 + 10.0 * 8.412799835205078
Epoch 2270, val loss: 2.1717615127563477
Epoch 2280, training loss: 86.29387664794922 = 2.175384283065796 + 10.0 * 8.411849021911621
Epoch 2280, val loss: 2.1717183589935303
Epoch 2290, training loss: 86.28382110595703 = 2.1753536462783813 + 10.0 * 8.410846710205078
Epoch 2290, val loss: 2.171689987182617
Epoch 2300, training loss: 86.27788543701172 = 2.175323486328125 + 10.0 * 8.410256385803223
Epoch 2300, val loss: 2.171661376953125
Epoch 2310, training loss: 86.27404022216797 = 2.1753013134002686 + 10.0 * 8.409873962402344
Epoch 2310, val loss: 2.1716341972351074
Epoch 2320, training loss: 86.27469635009766 = 2.1752755641937256 + 10.0 * 8.409941673278809
Epoch 2320, val loss: 2.1716084480285645
Epoch 2330, training loss: 86.3100814819336 = 2.175243616104126 + 10.0 * 8.413483619689941
Epoch 2330, val loss: 2.1715707778930664
Epoch 2340, training loss: 86.27720642089844 = 2.17520010471344 + 10.0 * 8.410200119018555
Epoch 2340, val loss: 2.1715312004089355
Epoch 2350, training loss: 86.26870727539062 = 2.1751681566238403 + 10.0 * 8.409353256225586
Epoch 2350, val loss: 2.1714959144592285
Epoch 2360, training loss: 86.26441955566406 = 2.1751413345336914 + 10.0 * 8.408927917480469
Epoch 2360, val loss: 2.171468734741211
Epoch 2370, training loss: 86.28408813476562 = 2.1751190423965454 + 10.0 * 8.410897254943848
Epoch 2370, val loss: 2.1714391708374023
Epoch 2380, training loss: 86.26258087158203 = 2.1750658750534058 + 10.0 * 8.408751487731934
Epoch 2380, val loss: 2.171396493911743
Epoch 2390, training loss: 86.25718688964844 = 2.1750411987304688 + 10.0 * 8.408214569091797
Epoch 2390, val loss: 2.171367883682251
Epoch 2400, training loss: 86.25296020507812 = 2.1750104427337646 + 10.0 * 8.407794952392578
Epoch 2400, val loss: 2.171337604522705
Epoch 2410, training loss: 86.25102233886719 = 2.174984097480774 + 10.0 * 8.40760326385498
Epoch 2410, val loss: 2.1713099479675293
Epoch 2420, training loss: 86.31182098388672 = 2.1749502420425415 + 10.0 * 8.413686752319336
Epoch 2420, val loss: 2.171276807785034
Epoch 2430, training loss: 86.2682876586914 = 2.1748937368392944 + 10.0 * 8.40933895111084
Epoch 2430, val loss: 2.1712164878845215
Epoch 2440, training loss: 86.24539947509766 = 2.174848198890686 + 10.0 * 8.407054901123047
Epoch 2440, val loss: 2.1711740493774414
Epoch 2450, training loss: 86.24429321289062 = 2.1748191118240356 + 10.0 * 8.406947135925293
Epoch 2450, val loss: 2.1711440086364746
Epoch 2460, training loss: 86.23900604248047 = 2.1747900247573853 + 10.0 * 8.406421661376953
Epoch 2460, val loss: 2.1711137294769287
Epoch 2470, training loss: 86.23675537109375 = 2.17476224899292 + 10.0 * 8.40619945526123
Epoch 2470, val loss: 2.1710867881774902
Epoch 2480, training loss: 86.23487854003906 = 2.1747320890426636 + 10.0 * 8.406014442443848
Epoch 2480, val loss: 2.171055793762207
Epoch 2490, training loss: 86.2536392211914 = 2.174691081047058 + 10.0 * 8.4078950881958
Epoch 2490, val loss: 2.1710205078125
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.3982608695652174
0.8140983844091865
=== training gcn model ===
Epoch 0, training loss: 108.0197525024414 = 2.197016954421997 + 10.0 * 10.582273483276367
Epoch 0, val loss: 2.193807601928711
Epoch 10, training loss: 108.0082778930664 = 2.1871933937072754 + 10.0 * 10.582108497619629
Epoch 10, val loss: 2.1848626136779785
Epoch 20, training loss: 107.99456787109375 = 2.178407669067383 + 10.0 * 10.581616401672363
Epoch 20, val loss: 2.176518440246582
Epoch 30, training loss: 107.96974182128906 = 2.169488549232483 + 10.0 * 10.580025672912598
Epoch 30, val loss: 2.1682841777801514
Epoch 40, training loss: 107.90694427490234 = 2.1622484922409058 + 10.0 * 10.574469566345215
Epoch 40, val loss: 2.1617417335510254
Epoch 50, training loss: 107.71330261230469 = 2.1579290628433228 + 10.0 * 10.555537223815918
Epoch 50, val loss: 2.157884120941162
Epoch 60, training loss: 107.20244598388672 = 2.1566338539123535 + 10.0 * 10.504581451416016
Epoch 60, val loss: 2.1570470333099365
Epoch 70, training loss: 105.99890899658203 = 2.161035180091858 + 10.0 * 10.383787155151367
Epoch 70, val loss: 2.161850929260254
Epoch 80, training loss: 103.33058166503906 = 2.1704195737838745 + 10.0 * 10.116016387939453
Epoch 80, val loss: 2.171478509902954
Epoch 90, training loss: 100.05188751220703 = 2.176515579223633 + 10.0 * 9.787537574768066
Epoch 90, val loss: 2.176677942276001
Epoch 100, training loss: 98.14152526855469 = 2.172776222229004 + 10.0 * 9.596875190734863
Epoch 100, val loss: 2.172294855117798
Epoch 110, training loss: 96.62853240966797 = 2.165440320968628 + 10.0 * 9.446309089660645
Epoch 110, val loss: 2.164459466934204
Epoch 120, training loss: 95.58875274658203 = 2.157670259475708 + 10.0 * 9.343108177185059
Epoch 120, val loss: 2.1568455696105957
Epoch 130, training loss: 95.15071105957031 = 2.152586340904236 + 10.0 * 9.299812316894531
Epoch 130, val loss: 2.1520185470581055
Epoch 140, training loss: 94.95135498046875 = 2.1494288444519043 + 10.0 * 9.280192375183105
Epoch 140, val loss: 2.1491026878356934
Epoch 150, training loss: 94.76741027832031 = 2.147706627845764 + 10.0 * 9.261970520019531
Epoch 150, val loss: 2.1475071907043457
Epoch 160, training loss: 94.54473114013672 = 2.146010637283325 + 10.0 * 9.239871978759766
Epoch 160, val loss: 2.1458945274353027
Epoch 170, training loss: 94.25934600830078 = 2.14500892162323 + 10.0 * 9.211433410644531
Epoch 170, val loss: 2.1448678970336914
Epoch 180, training loss: 93.84058380126953 = 2.145194411277771 + 10.0 * 9.169538497924805
Epoch 180, val loss: 2.145096778869629
Epoch 190, training loss: 93.2527847290039 = 2.1462987661361694 + 10.0 * 9.110648155212402
Epoch 190, val loss: 2.146232843399048
Epoch 200, training loss: 92.67990112304688 = 2.148221254348755 + 10.0 * 9.053167343139648
Epoch 200, val loss: 2.1481287479400635
Epoch 210, training loss: 92.4065170288086 = 2.14949893951416 + 10.0 * 9.025701522827148
Epoch 210, val loss: 2.1494007110595703
Epoch 220, training loss: 92.16860961914062 = 2.1491565704345703 + 10.0 * 9.001945495605469
Epoch 220, val loss: 2.149061679840088
Epoch 230, training loss: 91.75946044921875 = 2.149417757987976 + 10.0 * 8.961004257202148
Epoch 230, val loss: 2.1493287086486816
Epoch 240, training loss: 91.14383697509766 = 2.1512826681137085 + 10.0 * 8.899255752563477
Epoch 240, val loss: 2.151245594024658
Epoch 250, training loss: 90.60137176513672 = 2.152917981147766 + 10.0 * 8.84484577178955
Epoch 250, val loss: 2.1528944969177246
Epoch 260, training loss: 90.27375030517578 = 2.1531096696853638 + 10.0 * 8.812064170837402
Epoch 260, val loss: 2.1530065536499023
Epoch 270, training loss: 90.05886840820312 = 2.152453064918518 + 10.0 * 8.790641784667969
Epoch 270, val loss: 2.1522879600524902
Epoch 280, training loss: 89.8996810913086 = 2.151642322540283 + 10.0 * 8.77480411529541
Epoch 280, val loss: 2.1514477729797363
Epoch 290, training loss: 89.72364044189453 = 2.1511428356170654 + 10.0 * 8.75724983215332
Epoch 290, val loss: 2.1509337425231934
Epoch 300, training loss: 89.52901458740234 = 2.1509010791778564 + 10.0 * 8.737811088562012
Epoch 300, val loss: 2.1507325172424316
Epoch 310, training loss: 89.37747192382812 = 2.1507703065872192 + 10.0 * 8.72266960144043
Epoch 310, val loss: 2.1506187915802
Epoch 320, training loss: 89.27190399169922 = 2.1504790782928467 + 10.0 * 8.712141990661621
Epoch 320, val loss: 2.1503307819366455
Epoch 330, training loss: 89.1630630493164 = 2.1500813961029053 + 10.0 * 8.701298713684082
Epoch 330, val loss: 2.1499295234680176
Epoch 340, training loss: 89.03311920166016 = 2.149796724319458 + 10.0 * 8.688332557678223
Epoch 340, val loss: 2.14970064163208
Epoch 350, training loss: 88.88817596435547 = 2.149779796600342 + 10.0 * 8.673839569091797
Epoch 350, val loss: 2.149695873260498
Epoch 360, training loss: 88.76647186279297 = 2.1498788595199585 + 10.0 * 8.661659240722656
Epoch 360, val loss: 2.149775505065918
Epoch 370, training loss: 88.63920593261719 = 2.1497925519943237 + 10.0 * 8.648941040039062
Epoch 370, val loss: 2.1497137546539307
Epoch 380, training loss: 88.53207397460938 = 2.14963698387146 + 10.0 * 8.638243675231934
Epoch 380, val loss: 2.149563789367676
Epoch 390, training loss: 88.44450378417969 = 2.1494868993759155 + 10.0 * 8.629501342773438
Epoch 390, val loss: 2.1494076251983643
Epoch 400, training loss: 88.37025451660156 = 2.1493356227874756 + 10.0 * 8.622091293334961
Epoch 400, val loss: 2.149244785308838
Epoch 410, training loss: 88.31440734863281 = 2.1491562128067017 + 10.0 * 8.616525650024414
Epoch 410, val loss: 2.1490349769592285
Epoch 420, training loss: 88.24044799804688 = 2.1489189863204956 + 10.0 * 8.609152793884277
Epoch 420, val loss: 2.1487975120544434
Epoch 430, training loss: 88.16944122314453 = 2.148698091506958 + 10.0 * 8.60207462310791
Epoch 430, val loss: 2.1485934257507324
Epoch 440, training loss: 88.08726501464844 = 2.1485482454299927 + 10.0 * 8.5938720703125
Epoch 440, val loss: 2.1484503746032715
Epoch 450, training loss: 88.06206512451172 = 2.1484854221343994 + 10.0 * 8.591358184814453
Epoch 450, val loss: 2.1483664512634277
Epoch 460, training loss: 87.9251708984375 = 2.148297667503357 + 10.0 * 8.57768726348877
Epoch 460, val loss: 2.148235321044922
Epoch 470, training loss: 87.8510513305664 = 2.1482017040252686 + 10.0 * 8.570284843444824
Epoch 470, val loss: 2.148145914077759
Epoch 480, training loss: 87.7750244140625 = 2.1481202840805054 + 10.0 * 8.562690734863281
Epoch 480, val loss: 2.1480660438537598
Epoch 490, training loss: 87.7155532836914 = 2.148039698600769 + 10.0 * 8.556751251220703
Epoch 490, val loss: 2.14797043800354
Epoch 500, training loss: 87.66742706298828 = 2.1478383541107178 + 10.0 * 8.551959037780762
Epoch 500, val loss: 2.147813558578491
Epoch 510, training loss: 87.61080169677734 = 2.1476601362228394 + 10.0 * 8.546314239501953
Epoch 510, val loss: 2.147629499435425
Epoch 520, training loss: 87.56986999511719 = 2.147461771965027 + 10.0 * 8.542241096496582
Epoch 520, val loss: 2.1474201679229736
Epoch 530, training loss: 87.53527069091797 = 2.1472229957580566 + 10.0 * 8.53880500793457
Epoch 530, val loss: 2.147184371948242
Epoch 540, training loss: 87.50463104248047 = 2.1469646692276 + 10.0 * 8.5357666015625
Epoch 540, val loss: 2.146929979324341
Epoch 550, training loss: 87.47545623779297 = 2.146713137626648 + 10.0 * 8.53287410736084
Epoch 550, val loss: 2.1466708183288574
Epoch 560, training loss: 87.50946044921875 = 2.146458148956299 + 10.0 * 8.536299705505371
Epoch 560, val loss: 2.1464061737060547
Epoch 570, training loss: 87.42952728271484 = 2.1461137533187866 + 10.0 * 8.528341293334961
Epoch 570, val loss: 2.146091938018799
Epoch 580, training loss: 87.3978271484375 = 2.1458739042282104 + 10.0 * 8.525195121765137
Epoch 580, val loss: 2.145822525024414
Epoch 590, training loss: 87.36396789550781 = 2.145604372024536 + 10.0 * 8.521836280822754
Epoch 590, val loss: 2.1455740928649902
Epoch 600, training loss: 87.33518981933594 = 2.1453635692596436 + 10.0 * 8.518982887268066
Epoch 600, val loss: 2.1453328132629395
Epoch 610, training loss: 87.30770874023438 = 2.1451358795166016 + 10.0 * 8.516257286071777
Epoch 610, val loss: 2.145101547241211
Epoch 620, training loss: 87.2832260131836 = 2.1449081897735596 + 10.0 * 8.513832092285156
Epoch 620, val loss: 2.144873857498169
Epoch 630, training loss: 87.26087951660156 = 2.144651174545288 + 10.0 * 8.511622428894043
Epoch 630, val loss: 2.1446216106414795
Epoch 640, training loss: 87.23822784423828 = 2.144393801689148 + 10.0 * 8.509383201599121
Epoch 640, val loss: 2.1443722248077393
Epoch 650, training loss: 87.2109603881836 = 2.144173264503479 + 10.0 * 8.506678581237793
Epoch 650, val loss: 2.1441445350646973
Epoch 660, training loss: 87.18257141113281 = 2.1439703702926636 + 10.0 * 8.503859519958496
Epoch 660, val loss: 2.1439356803894043
Epoch 670, training loss: 87.15654754638672 = 2.143770933151245 + 10.0 * 8.501277923583984
Epoch 670, val loss: 2.143735647201538
Epoch 680, training loss: 87.13272857666016 = 2.143579125404358 + 10.0 * 8.49891471862793
Epoch 680, val loss: 2.1435389518737793
Epoch 690, training loss: 87.10932922363281 = 2.1433721780776978 + 10.0 * 8.49659538269043
Epoch 690, val loss: 2.1433327198028564
Epoch 700, training loss: 87.08325958251953 = 2.143139362335205 + 10.0 * 8.494011878967285
Epoch 700, val loss: 2.1431100368499756
Epoch 710, training loss: 87.06185913085938 = 2.142929196357727 + 10.0 * 8.49189281463623
Epoch 710, val loss: 2.1429123878479004
Epoch 720, training loss: 87.03883361816406 = 2.142735242843628 + 10.0 * 8.489609718322754
Epoch 720, val loss: 2.142712116241455
Epoch 730, training loss: 87.01847839355469 = 2.1425331830978394 + 10.0 * 8.487594604492188
Epoch 730, val loss: 2.1425037384033203
Epoch 740, training loss: 87.04670715332031 = 2.1423338651657104 + 10.0 * 8.490437507629395
Epoch 740, val loss: 2.142275333404541
Epoch 750, training loss: 86.98739624023438 = 2.1420302391052246 + 10.0 * 8.484537124633789
Epoch 750, val loss: 2.142021417617798
Epoch 760, training loss: 86.96851348876953 = 2.141770839691162 + 10.0 * 8.482674598693848
Epoch 760, val loss: 2.141777515411377
Epoch 770, training loss: 86.95140838623047 = 2.141526699066162 + 10.0 * 8.480988502502441
Epoch 770, val loss: 2.141521453857422
Epoch 780, training loss: 86.93501281738281 = 2.1412540674209595 + 10.0 * 8.479375839233398
Epoch 780, val loss: 2.1412594318389893
Epoch 790, training loss: 86.91992950439453 = 2.1409857273101807 + 10.0 * 8.47789478302002
Epoch 790, val loss: 2.1409947872161865
Epoch 800, training loss: 86.91482543945312 = 2.1406949758529663 + 10.0 * 8.477413177490234
Epoch 800, val loss: 2.1407196521759033
Epoch 810, training loss: 86.9019775390625 = 2.140423536300659 + 10.0 * 8.476155281066895
Epoch 810, val loss: 2.1404168605804443
Epoch 820, training loss: 86.88016510009766 = 2.1400864124298096 + 10.0 * 8.474008560180664
Epoch 820, val loss: 2.140120506286621
Epoch 830, training loss: 86.86856842041016 = 2.1397922039031982 + 10.0 * 8.472877502441406
Epoch 830, val loss: 2.139838695526123
Epoch 840, training loss: 86.87444305419922 = 2.139475464820862 + 10.0 * 8.47349739074707
Epoch 840, val loss: 2.139557361602783
Epoch 850, training loss: 86.8513412475586 = 2.1391955614089966 + 10.0 * 8.471214294433594
Epoch 850, val loss: 2.1392345428466797
Epoch 860, training loss: 86.8388671875 = 2.1388691663742065 + 10.0 * 8.469999313354492
Epoch 860, val loss: 2.138941764831543
Epoch 870, training loss: 86.82392120361328 = 2.1385895013809204 + 10.0 * 8.468533515930176
Epoch 870, val loss: 2.1386477947235107
Epoch 880, training loss: 86.81405639648438 = 2.1382877826690674 + 10.0 * 8.46757698059082
Epoch 880, val loss: 2.138355016708374
Epoch 890, training loss: 86.80587005615234 = 2.137987732887268 + 10.0 * 8.466788291931152
Epoch 890, val loss: 2.138061285018921
Epoch 900, training loss: 86.82344818115234 = 2.137694478034973 + 10.0 * 8.468575477600098
Epoch 900, val loss: 2.137756109237671
Epoch 910, training loss: 86.79833221435547 = 2.137373924255371 + 10.0 * 8.466095924377441
Epoch 910, val loss: 2.1374518871307373
Epoch 920, training loss: 86.78130340576172 = 2.137053966522217 + 10.0 * 8.464425086975098
Epoch 920, val loss: 2.1371407508850098
Epoch 930, training loss: 86.77378845214844 = 2.136752724647522 + 10.0 * 8.463704109191895
Epoch 930, val loss: 2.136848211288452
Epoch 940, training loss: 86.76463317871094 = 2.1364517211914062 + 10.0 * 8.462818145751953
Epoch 940, val loss: 2.136556625366211
Epoch 950, training loss: 86.76170349121094 = 2.136149048805237 + 10.0 * 8.462555885314941
Epoch 950, val loss: 2.136265277862549
Epoch 960, training loss: 86.7791519165039 = 2.1358247995376587 + 10.0 * 8.464332580566406
Epoch 960, val loss: 2.135962963104248
Epoch 970, training loss: 86.74816131591797 = 2.1355481147766113 + 10.0 * 8.461260795593262
Epoch 970, val loss: 2.135655403137207
Epoch 980, training loss: 86.7362289428711 = 2.1352375745773315 + 10.0 * 8.460099220275879
Epoch 980, val loss: 2.135359525680542
Epoch 990, training loss: 86.72918701171875 = 2.134947180747986 + 10.0 * 8.459424018859863
Epoch 990, val loss: 2.1350746154785156
Epoch 1000, training loss: 86.72339630126953 = 2.1346523761749268 + 10.0 * 8.458874702453613
Epoch 1000, val loss: 2.1347904205322266
Epoch 1010, training loss: 86.7495346069336 = 2.1343588829040527 + 10.0 * 8.461517333984375
Epoch 1010, val loss: 2.134503126144409
Epoch 1020, training loss: 86.71436309814453 = 2.134066104888916 + 10.0 * 8.458029747009277
Epoch 1020, val loss: 2.134204387664795
Epoch 1030, training loss: 86.70452880859375 = 2.133775472640991 + 10.0 * 8.457075119018555
Epoch 1030, val loss: 2.133924961090088
Epoch 1040, training loss: 86.69499206542969 = 2.133493661880493 + 10.0 * 8.45615005493164
Epoch 1040, val loss: 2.1336469650268555
Epoch 1050, training loss: 86.72021484375 = 2.1332250833511353 + 10.0 * 8.458699226379395
Epoch 1050, val loss: 2.133366107940674
Epoch 1060, training loss: 86.70684814453125 = 2.132908582687378 + 10.0 * 8.457393646240234
Epoch 1060, val loss: 2.1330766677856445
Epoch 1070, training loss: 86.67501831054688 = 2.1326268911361694 + 10.0 * 8.454238891601562
Epoch 1070, val loss: 2.132793426513672
Epoch 1080, training loss: 86.669677734375 = 2.1323537826538086 + 10.0 * 8.45373249053955
Epoch 1080, val loss: 2.1325271129608154
Epoch 1090, training loss: 86.65872955322266 = 2.1320912837982178 + 10.0 * 8.452664375305176
Epoch 1090, val loss: 2.1322708129882812
Epoch 1100, training loss: 86.65538024902344 = 2.1318368911743164 + 10.0 * 8.452354431152344
Epoch 1100, val loss: 2.1320114135742188
Epoch 1110, training loss: 86.65006256103516 = 2.1315624713897705 + 10.0 * 8.451849937438965
Epoch 1110, val loss: 2.131747245788574
Epoch 1120, training loss: 86.64363098144531 = 2.131290555000305 + 10.0 * 8.451234817504883
Epoch 1120, val loss: 2.131479263305664
Epoch 1130, training loss: 86.62997436523438 = 2.1310240030288696 + 10.0 * 8.449894905090332
Epoch 1130, val loss: 2.131223201751709
Epoch 1140, training loss: 86.6220703125 = 2.130768656730652 + 10.0 * 8.449130058288574
Epoch 1140, val loss: 2.1309757232666016
Epoch 1150, training loss: 86.61446380615234 = 2.130520820617676 + 10.0 * 8.448393821716309
Epoch 1150, val loss: 2.1307291984558105
Epoch 1160, training loss: 86.61405181884766 = 2.1302928924560547 + 10.0 * 8.448375701904297
Epoch 1160, val loss: 2.1304783821105957
Epoch 1170, training loss: 86.63333129882812 = 2.1299391984939575 + 10.0 * 8.450339317321777
Epoch 1170, val loss: 2.130218505859375
Epoch 1180, training loss: 86.61376953125 = 2.1296801567077637 + 10.0 * 8.448409080505371
Epoch 1180, val loss: 2.129927635192871
Epoch 1190, training loss: 86.5882339477539 = 2.1294589042663574 + 10.0 * 8.445878028869629
Epoch 1190, val loss: 2.1296803951263428
Epoch 1200, training loss: 86.58181762695312 = 2.1292299032211304 + 10.0 * 8.445259094238281
Epoch 1200, val loss: 2.1294546127319336
Epoch 1210, training loss: 86.57321166992188 = 2.1289809942245483 + 10.0 * 8.444422721862793
Epoch 1210, val loss: 2.1292219161987305
Epoch 1220, training loss: 86.5662612915039 = 2.128738760948181 + 10.0 * 8.44375228881836
Epoch 1220, val loss: 2.128988742828369
Epoch 1230, training loss: 86.55948638916016 = 2.128500819206238 + 10.0 * 8.443098068237305
Epoch 1230, val loss: 2.128755807876587
Epoch 1240, training loss: 86.55307006835938 = 2.1282628774642944 + 10.0 * 8.442480087280273
Epoch 1240, val loss: 2.1285243034362793
Epoch 1250, training loss: 86.58914184570312 = 2.128013491630554 + 10.0 * 8.446112632751465
Epoch 1250, val loss: 2.1282973289489746
Epoch 1260, training loss: 86.58159637451172 = 2.127768635749817 + 10.0 * 8.445383071899414
Epoch 1260, val loss: 2.1280148029327393
Epoch 1270, training loss: 86.53730773925781 = 2.1275030374526978 + 10.0 * 8.440980911254883
Epoch 1270, val loss: 2.1277692317962646
Epoch 1280, training loss: 86.52859497070312 = 2.1272565126419067 + 10.0 * 8.440134048461914
Epoch 1280, val loss: 2.127540349960327
Epoch 1290, training loss: 86.52307891845703 = 2.1270288228988647 + 10.0 * 8.439604759216309
Epoch 1290, val loss: 2.127321720123291
Epoch 1300, training loss: 86.5150375366211 = 2.1268062591552734 + 10.0 * 8.438822746276855
Epoch 1300, val loss: 2.1270992755889893
Epoch 1310, training loss: 86.50899505615234 = 2.126576542854309 + 10.0 * 8.438241958618164
Epoch 1310, val loss: 2.1268763542175293
Epoch 1320, training loss: 86.53886413574219 = 2.126345992088318 + 10.0 * 8.441251754760742
Epoch 1320, val loss: 2.126652956008911
Epoch 1330, training loss: 86.52012634277344 = 2.126085877418518 + 10.0 * 8.439404487609863
Epoch 1330, val loss: 2.126405715942383
Epoch 1340, training loss: 86.50173950195312 = 2.1258569955825806 + 10.0 * 8.437588691711426
Epoch 1340, val loss: 2.126163959503174
Epoch 1350, training loss: 86.48635864257812 = 2.125614047050476 + 10.0 * 8.436075210571289
Epoch 1350, val loss: 2.1259422302246094
Epoch 1360, training loss: 86.47955322265625 = 2.125393509864807 + 10.0 * 8.435415267944336
Epoch 1360, val loss: 2.1257271766662598
Epoch 1370, training loss: 86.47392272949219 = 2.1251680850982666 + 10.0 * 8.43487548828125
Epoch 1370, val loss: 2.1255149841308594
Epoch 1380, training loss: 86.46753692626953 = 2.124951720237732 + 10.0 * 8.434258460998535
Epoch 1380, val loss: 2.125300407409668
Epoch 1390, training loss: 86.46321868896484 = 2.1247317790985107 + 10.0 * 8.43384838104248
Epoch 1390, val loss: 2.1250855922698975
Epoch 1400, training loss: 86.4917984008789 = 2.1245107650756836 + 10.0 * 8.436728477478027
Epoch 1400, val loss: 2.1248669624328613
Epoch 1410, training loss: 86.46245574951172 = 2.1242746114730835 + 10.0 * 8.433817863464355
Epoch 1410, val loss: 2.124636173248291
Epoch 1420, training loss: 86.47208404541016 = 2.124052047729492 + 10.0 * 8.434803009033203
Epoch 1420, val loss: 2.124403476715088
Epoch 1430, training loss: 86.4469223022461 = 2.1237865686416626 + 10.0 * 8.432313919067383
Epoch 1430, val loss: 2.124175786972046
Epoch 1440, training loss: 86.43675231933594 = 2.1235777139663696 + 10.0 * 8.431317329406738
Epoch 1440, val loss: 2.1239659786224365
Epoch 1450, training loss: 86.43110656738281 = 2.1233564615249634 + 10.0 * 8.430774688720703
Epoch 1450, val loss: 2.123762369155884
Epoch 1460, training loss: 86.42369079589844 = 2.1231539249420166 + 10.0 * 8.4300537109375
Epoch 1460, val loss: 2.1235575675964355
Epoch 1470, training loss: 86.4203109741211 = 2.1229459047317505 + 10.0 * 8.429736137390137
Epoch 1470, val loss: 2.123352527618408
Epoch 1480, training loss: 86.44316101074219 = 2.1227437257766724 + 10.0 * 8.432042121887207
Epoch 1480, val loss: 2.1231400966644287
Epoch 1490, training loss: 86.41009521484375 = 2.1224955320358276 + 10.0 * 8.428759574890137
Epoch 1490, val loss: 2.122922897338867
Epoch 1500, training loss: 86.40312957763672 = 2.1222764253616333 + 10.0 * 8.428085327148438
Epoch 1500, val loss: 2.1227054595947266
Epoch 1510, training loss: 86.39986419677734 = 2.1220699548721313 + 10.0 * 8.427779197692871
Epoch 1510, val loss: 2.122504472732544
Epoch 1520, training loss: 86.41598510742188 = 2.121872663497925 + 10.0 * 8.429410934448242
Epoch 1520, val loss: 2.1222991943359375
Epoch 1530, training loss: 86.3949203491211 = 2.121621608734131 + 10.0 * 8.427330017089844
Epoch 1530, val loss: 2.1220896244049072
Epoch 1540, training loss: 86.38616943359375 = 2.121428370475769 + 10.0 * 8.426473617553711
Epoch 1540, val loss: 2.121890068054199
Epoch 1550, training loss: 86.3819580078125 = 2.1212044954299927 + 10.0 * 8.426074981689453
Epoch 1550, val loss: 2.121692180633545
Epoch 1560, training loss: 86.41466522216797 = 2.1209763288497925 + 10.0 * 8.42936897277832
Epoch 1560, val loss: 2.1214890480041504
Epoch 1570, training loss: 86.3802490234375 = 2.1208051443099976 + 10.0 * 8.425944328308105
Epoch 1570, val loss: 2.121267795562744
Epoch 1580, training loss: 86.36616516113281 = 2.1205708980560303 + 10.0 * 8.424559593200684
Epoch 1580, val loss: 2.121063709259033
Epoch 1590, training loss: 86.37347412109375 = 2.120377779006958 + 10.0 * 8.425310134887695
Epoch 1590, val loss: 2.1208653450012207
Epoch 1600, training loss: 86.36399841308594 = 2.1201651096343994 + 10.0 * 8.424383163452148
Epoch 1600, val loss: 2.1206531524658203
Epoch 1610, training loss: 86.35320281982422 = 2.119927167892456 + 10.0 * 8.423327445983887
Epoch 1610, val loss: 2.120441436767578
Epoch 1620, training loss: 86.34810638427734 = 2.119732975959778 + 10.0 * 8.422837257385254
Epoch 1620, val loss: 2.120248317718506
Epoch 1630, training loss: 86.34415435791016 = 2.119529128074646 + 10.0 * 8.422462463378906
Epoch 1630, val loss: 2.120053768157959
Epoch 1640, training loss: 86.33934783935547 = 2.1193372011184692 + 10.0 * 8.422000885009766
Epoch 1640, val loss: 2.1198601722717285
Epoch 1650, training loss: 86.3363037109375 = 2.119138479232788 + 10.0 * 8.421716690063477
Epoch 1650, val loss: 2.1196651458740234
Epoch 1660, training loss: 86.34864807128906 = 2.118943214416504 + 10.0 * 8.42297077178955
Epoch 1660, val loss: 2.119467258453369
Epoch 1670, training loss: 86.35004425048828 = 2.118692398071289 + 10.0 * 8.423135757446289
Epoch 1670, val loss: 2.1192638874053955
Epoch 1680, training loss: 86.33356475830078 = 2.118495464324951 + 10.0 * 8.421506881713867
Epoch 1680, val loss: 2.1190295219421387
Epoch 1690, training loss: 86.32321166992188 = 2.1182669401168823 + 10.0 * 8.42049503326416
Epoch 1690, val loss: 2.118826150894165
Epoch 1700, training loss: 86.31775665283203 = 2.118064284324646 + 10.0 * 8.41996955871582
Epoch 1700, val loss: 2.11863112449646
Epoch 1710, training loss: 86.31481170654297 = 2.1178789138793945 + 10.0 * 8.419692993164062
Epoch 1710, val loss: 2.1184370517730713
Epoch 1720, training loss: 86.310546875 = 2.1176741123199463 + 10.0 * 8.419286727905273
Epoch 1720, val loss: 2.11824369430542
Epoch 1730, training loss: 86.31748962402344 = 2.117475390434265 + 10.0 * 8.420001029968262
Epoch 1730, val loss: 2.1180481910705566
Epoch 1740, training loss: 86.32310485839844 = 2.1172348260879517 + 10.0 * 8.420587539672852
Epoch 1740, val loss: 2.1178388595581055
Epoch 1750, training loss: 86.3052749633789 = 2.117040991783142 + 10.0 * 8.4188232421875
Epoch 1750, val loss: 2.117616891860962
Epoch 1760, training loss: 86.30036163330078 = 2.116825580596924 + 10.0 * 8.418353080749512
Epoch 1760, val loss: 2.117417812347412
Epoch 1770, training loss: 86.29447937011719 = 2.116622805595398 + 10.0 * 8.41778564453125
Epoch 1770, val loss: 2.1172242164611816
Epoch 1780, training loss: 86.2938003540039 = 2.11641788482666 + 10.0 * 8.41773796081543
Epoch 1780, val loss: 2.1170296669006348
Epoch 1790, training loss: 86.31572723388672 = 2.116194725036621 + 10.0 * 8.419953346252441
Epoch 1790, val loss: 2.116833448410034
Epoch 1800, training loss: 86.28881072998047 = 2.116003632545471 + 10.0 * 8.417280197143555
Epoch 1800, val loss: 2.1166069507598877
Epoch 1810, training loss: 86.28406524658203 = 2.115782618522644 + 10.0 * 8.416828155517578
Epoch 1810, val loss: 2.1164045333862305
Epoch 1820, training loss: 86.28113555908203 = 2.1155775785446167 + 10.0 * 8.416555404663086
Epoch 1820, val loss: 2.1162071228027344
Epoch 1830, training loss: 86.2789077758789 = 2.115390419960022 + 10.0 * 8.416352272033691
Epoch 1830, val loss: 2.1160120964050293
Epoch 1840, training loss: 86.290771484375 = 2.1151912212371826 + 10.0 * 8.417558670043945
Epoch 1840, val loss: 2.115813732147217
Epoch 1850, training loss: 86.27610778808594 = 2.114945411682129 + 10.0 * 8.416116714477539
Epoch 1850, val loss: 2.1156005859375
Epoch 1860, training loss: 86.27985382080078 = 2.1147559881210327 + 10.0 * 8.416509628295898
Epoch 1860, val loss: 2.115387439727783
Epoch 1870, training loss: 86.26931762695312 = 2.1145379543304443 + 10.0 * 8.415477752685547
Epoch 1870, val loss: 2.115184783935547
Epoch 1880, training loss: 86.2879638671875 = 2.11433744430542 + 10.0 * 8.417363166809082
Epoch 1880, val loss: 2.11498761177063
Epoch 1890, training loss: 86.26367950439453 = 2.1141233444213867 + 10.0 * 8.414956092834473
Epoch 1890, val loss: 2.1147820949554443
Epoch 1900, training loss: 86.264404296875 = 2.1139031648635864 + 10.0 * 8.41504955291748
Epoch 1900, val loss: 2.114579200744629
Epoch 1910, training loss: 86.25948333740234 = 2.113714575767517 + 10.0 * 8.414576530456543
Epoch 1910, val loss: 2.1143851280212402
Epoch 1920, training loss: 86.2558364868164 = 2.113517999649048 + 10.0 * 8.41423225402832
Epoch 1920, val loss: 2.1141932010650635
Epoch 1930, training loss: 86.25301361083984 = 2.1133168935775757 + 10.0 * 8.413969039916992
Epoch 1930, val loss: 2.113999366760254
Epoch 1940, training loss: 86.25579071044922 = 2.1131242513656616 + 10.0 * 8.414266586303711
Epoch 1940, val loss: 2.1138052940368652
Epoch 1950, training loss: 86.26949310302734 = 2.112930655479431 + 10.0 * 8.415656089782715
Epoch 1950, val loss: 2.113603115081787
Epoch 1960, training loss: 86.2690658569336 = 2.112682580947876 + 10.0 * 8.415637969970703
Epoch 1960, val loss: 2.1133828163146973
Epoch 1970, training loss: 86.25141143798828 = 2.112463593482971 + 10.0 * 8.413894653320312
Epoch 1970, val loss: 2.113180637359619
Epoch 1980, training loss: 86.24263763427734 = 2.1122748851776123 + 10.0 * 8.413036346435547
Epoch 1980, val loss: 2.1129844188690186
Epoch 1990, training loss: 86.23979187011719 = 2.112072706222534 + 10.0 * 8.412771224975586
Epoch 1990, val loss: 2.1127963066101074
Epoch 2000, training loss: 86.23737335205078 = 2.111885190010071 + 10.0 * 8.412549018859863
Epoch 2000, val loss: 2.112612247467041
Epoch 2010, training loss: 86.23572540283203 = 2.1116878986358643 + 10.0 * 8.41240406036377
Epoch 2010, val loss: 2.112426280975342
Epoch 2020, training loss: 86.2491455078125 = 2.11148464679718 + 10.0 * 8.413766860961914
Epoch 2020, val loss: 2.112239360809326
Epoch 2030, training loss: 86.24064636230469 = 2.1112669706344604 + 10.0 * 8.412938117980957
Epoch 2030, val loss: 2.1120243072509766
Epoch 2040, training loss: 86.23279571533203 = 2.1110777854919434 + 10.0 * 8.412172317504883
Epoch 2040, val loss: 2.1118228435516357
Epoch 2050, training loss: 86.22864532470703 = 2.110878586769104 + 10.0 * 8.411776542663574
Epoch 2050, val loss: 2.1116366386413574
Epoch 2060, training loss: 86.2250747680664 = 2.1106964349746704 + 10.0 * 8.41143798828125
Epoch 2060, val loss: 2.1114535331726074
Epoch 2070, training loss: 86.2275161743164 = 2.1105153560638428 + 10.0 * 8.411700248718262
Epoch 2070, val loss: 2.111269950866699
Epoch 2080, training loss: 86.2328872680664 = 2.1103087663650513 + 10.0 * 8.41225814819336
Epoch 2080, val loss: 2.111079216003418
Epoch 2090, training loss: 86.2173080444336 = 2.1100962162017822 + 10.0 * 8.410720825195312
Epoch 2090, val loss: 2.110872745513916
Epoch 2100, training loss: 86.2176742553711 = 2.1098928451538086 + 10.0 * 8.410778045654297
Epoch 2100, val loss: 2.110694408416748
Epoch 2110, training loss: 86.21275329589844 = 2.109727144241333 + 10.0 * 8.410303115844727
Epoch 2110, val loss: 2.1105170249938965
Epoch 2120, training loss: 86.21159362792969 = 2.1095404624938965 + 10.0 * 8.410204887390137
Epoch 2120, val loss: 2.1103386878967285
Epoch 2130, training loss: 86.2717514038086 = 2.109374165534973 + 10.0 * 8.416237831115723
Epoch 2130, val loss: 2.1101508140563965
Epoch 2140, training loss: 86.23118591308594 = 2.1091158390045166 + 10.0 * 8.412206649780273
Epoch 2140, val loss: 2.1099441051483154
Epoch 2150, training loss: 86.21046447753906 = 2.108932375907898 + 10.0 * 8.41015338897705
Epoch 2150, val loss: 2.1097500324249268
Epoch 2160, training loss: 86.20095825195312 = 2.108746647834778 + 10.0 * 8.409220695495605
Epoch 2160, val loss: 2.1095762252807617
Epoch 2170, training loss: 86.19843292236328 = 2.1085715293884277 + 10.0 * 8.40898609161377
Epoch 2170, val loss: 2.109408378601074
Epoch 2180, training loss: 86.1961669921875 = 2.108397126197815 + 10.0 * 8.408777236938477
Epoch 2180, val loss: 2.1092395782470703
Epoch 2190, training loss: 86.1957015991211 = 2.1082199811935425 + 10.0 * 8.408747673034668
Epoch 2190, val loss: 2.1090707778930664
Epoch 2200, training loss: 86.24170684814453 = 2.10803759098053 + 10.0 * 8.413366317749023
Epoch 2200, val loss: 2.1088945865631104
Epoch 2210, training loss: 86.21077728271484 = 2.10784113407135 + 10.0 * 8.410293579101562
Epoch 2210, val loss: 2.1086935997009277
Epoch 2220, training loss: 86.1886215209961 = 2.1076433658599854 + 10.0 * 8.408098220825195
Epoch 2220, val loss: 2.1085147857666016
Epoch 2230, training loss: 86.18783569335938 = 2.1074692010879517 + 10.0 * 8.408037185668945
Epoch 2230, val loss: 2.1083462238311768
Epoch 2240, training loss: 86.18440246582031 = 2.1072932481765747 + 10.0 * 8.407711029052734
Epoch 2240, val loss: 2.108181953430176
Epoch 2250, training loss: 86.18289947509766 = 2.1071207523345947 + 10.0 * 8.407577514648438
Epoch 2250, val loss: 2.1080195903778076
Epoch 2260, training loss: 86.19684600830078 = 2.1069384813308716 + 10.0 * 8.408990859985352
Epoch 2260, val loss: 2.107853889465332
Epoch 2270, training loss: 86.18029022216797 = 2.1067628860473633 + 10.0 * 8.407352447509766
Epoch 2270, val loss: 2.1076722145080566
Epoch 2280, training loss: 86.17704772949219 = 2.1065828800201416 + 10.0 * 8.4070463180542
Epoch 2280, val loss: 2.107504367828369
Epoch 2290, training loss: 86.17414093017578 = 2.106406569480896 + 10.0 * 8.406773567199707
Epoch 2290, val loss: 2.107339382171631
Epoch 2300, training loss: 86.18465423583984 = 2.1062381267547607 + 10.0 * 8.407841682434082
Epoch 2300, val loss: 2.107177972793579
Epoch 2310, training loss: 86.18289184570312 = 2.106045126914978 + 10.0 * 8.407684326171875
Epoch 2310, val loss: 2.106996536254883
Epoch 2320, training loss: 86.17361450195312 = 2.105893135070801 + 10.0 * 8.406771659851074
Epoch 2320, val loss: 2.1068227291107178
Epoch 2330, training loss: 86.16342163085938 = 2.1057121753692627 + 10.0 * 8.405771255493164
Epoch 2330, val loss: 2.1066670417785645
Epoch 2340, training loss: 86.16136169433594 = 2.105547547340393 + 10.0 * 8.4055814743042
Epoch 2340, val loss: 2.1065115928649902
Epoch 2350, training loss: 86.15855407714844 = 2.105393648147583 + 10.0 * 8.405316352844238
Epoch 2350, val loss: 2.1063570976257324
Epoch 2360, training loss: 86.2233657836914 = 2.105250597000122 + 10.0 * 8.411811828613281
Epoch 2360, val loss: 2.1061930656433105
Epoch 2370, training loss: 86.1717529296875 = 2.104999303817749 + 10.0 * 8.406675338745117
Epoch 2370, val loss: 2.106001615524292
Epoch 2380, training loss: 86.15658569335938 = 2.1048364639282227 + 10.0 * 8.40517520904541
Epoch 2380, val loss: 2.105839729309082
Epoch 2390, training loss: 86.15037536621094 = 2.104692816734314 + 10.0 * 8.404568672180176
Epoch 2390, val loss: 2.105685234069824
Epoch 2400, training loss: 86.15300750732422 = 2.104541540145874 + 10.0 * 8.404847145080566
Epoch 2400, val loss: 2.105532646179199
Epoch 2410, training loss: 86.18883514404297 = 2.1043777465820312 + 10.0 * 8.408445358276367
Epoch 2410, val loss: 2.10537052154541
Epoch 2420, training loss: 86.15129089355469 = 2.1041640043258667 + 10.0 * 8.404712677001953
Epoch 2420, val loss: 2.1052021980285645
Epoch 2430, training loss: 86.14142608642578 = 2.104016661643982 + 10.0 * 8.403740882873535
Epoch 2430, val loss: 2.1050429344177246
Epoch 2440, training loss: 86.13899993896484 = 2.103860378265381 + 10.0 * 8.40351390838623
Epoch 2440, val loss: 2.104896068572998
Epoch 2450, training loss: 86.13780212402344 = 2.1036990880966187 + 10.0 * 8.403409957885742
Epoch 2450, val loss: 2.104750633239746
Epoch 2460, training loss: 86.1363754272461 = 2.1035492420196533 + 10.0 * 8.40328311920166
Epoch 2460, val loss: 2.1046042442321777
Epoch 2470, training loss: 86.17159271240234 = 2.1034029722213745 + 10.0 * 8.406819343566895
Epoch 2470, val loss: 2.104450225830078
Epoch 2480, training loss: 86.14400482177734 = 2.103205442428589 + 10.0 * 8.404080390930176
Epoch 2480, val loss: 2.104279041290283
Epoch 2490, training loss: 86.13274383544922 = 2.103048324584961 + 10.0 * 8.402969360351562
Epoch 2490, val loss: 2.1041274070739746
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.39666666666666667
0.8135912482793596
=== training gcn model ===
Epoch 0, training loss: 108.0347900390625 = 2.2123308181762695 + 10.0 * 10.582245826721191
Epoch 0, val loss: 2.2091758251190186
Epoch 10, training loss: 108.0120849609375 = 2.1923748254776 + 10.0 * 10.581971168518066
Epoch 10, val loss: 2.18973970413208
Epoch 20, training loss: 107.98722839355469 = 2.174343466758728 + 10.0 * 10.58128833770752
Epoch 20, val loss: 2.1722664833068848
Epoch 30, training loss: 107.95297241210938 = 2.1585943698883057 + 10.0 * 10.579438209533691
Epoch 30, val loss: 2.157215118408203
Epoch 40, training loss: 107.88980865478516 = 2.152593970298767 + 10.0 * 10.573720932006836
Epoch 40, val loss: 2.1524393558502197
Epoch 50, training loss: 107.69585418701172 = 2.150657534599304 + 10.0 * 10.554519653320312
Epoch 50, val loss: 2.1507811546325684
Epoch 60, training loss: 107.11524963378906 = 2.15084445476532 + 10.0 * 10.496439933776855
Epoch 60, val loss: 2.151493549346924
Epoch 70, training loss: 105.55847930908203 = 2.155375838279724 + 10.0 * 10.340311050415039
Epoch 70, val loss: 2.156076669692993
Epoch 80, training loss: 102.60220336914062 = 2.160099148750305 + 10.0 * 10.044210433959961
Epoch 80, val loss: 2.1606998443603516
Epoch 90, training loss: 99.89486694335938 = 2.1616967916488647 + 10.0 * 9.773317337036133
Epoch 90, val loss: 2.16204833984375
Epoch 100, training loss: 98.70555877685547 = 2.163488745689392 + 10.0 * 9.654207229614258
Epoch 100, val loss: 2.163670778274536
Epoch 110, training loss: 97.33567810058594 = 2.1631728410720825 + 10.0 * 9.517251014709473
Epoch 110, val loss: 2.1627373695373535
Epoch 120, training loss: 95.919921875 = 2.160173177719116 + 10.0 * 9.375974655151367
Epoch 120, val loss: 2.1600332260131836
Epoch 130, training loss: 95.2710189819336 = 2.155596137046814 + 10.0 * 9.311542510986328
Epoch 130, val loss: 2.1556077003479004
Epoch 140, training loss: 95.08264923095703 = 2.150076985359192 + 10.0 * 9.293256759643555
Epoch 140, val loss: 2.150212049484253
Epoch 150, training loss: 94.90593719482422 = 2.1464550495147705 + 10.0 * 9.275948524475098
Epoch 150, val loss: 2.14664888381958
Epoch 160, training loss: 94.72174072265625 = 2.144903779029846 + 10.0 * 9.257683753967285
Epoch 160, val loss: 2.145203113555908
Epoch 170, training loss: 94.5101547241211 = 2.143915295600891 + 10.0 * 9.236623764038086
Epoch 170, val loss: 2.144300937652588
Epoch 180, training loss: 94.22529602050781 = 2.143176555633545 + 10.0 * 9.208211898803711
Epoch 180, val loss: 2.143634557723999
Epoch 190, training loss: 93.80391693115234 = 2.143411159515381 + 10.0 * 9.166050910949707
Epoch 190, val loss: 2.143895149230957
Epoch 200, training loss: 93.14134216308594 = 2.1448731422424316 + 10.0 * 9.09964656829834
Epoch 200, val loss: 2.1453418731689453
Epoch 210, training loss: 92.20875549316406 = 2.147954225540161 + 10.0 * 9.00607967376709
Epoch 210, val loss: 2.148319721221924
Epoch 220, training loss: 91.48424530029297 = 2.1510818004608154 + 10.0 * 8.933316230773926
Epoch 220, val loss: 2.151304006576538
Epoch 230, training loss: 91.1336669921875 = 2.1523295640945435 + 10.0 * 8.898134231567383
Epoch 230, val loss: 2.1523733139038086
Epoch 240, training loss: 90.79806518554688 = 2.152220368385315 + 10.0 * 8.864583969116211
Epoch 240, val loss: 2.152282953262329
Epoch 250, training loss: 90.41014862060547 = 2.1520289182662964 + 10.0 * 8.825811386108398
Epoch 250, val loss: 2.152069091796875
Epoch 260, training loss: 90.02761840820312 = 2.152405858039856 + 10.0 * 8.787521362304688
Epoch 260, val loss: 2.1525373458862305
Epoch 270, training loss: 89.73994445800781 = 2.1527217626571655 + 10.0 * 8.758722305297852
Epoch 270, val loss: 2.1528851985931396
Epoch 280, training loss: 89.55998992919922 = 2.1525652408599854 + 10.0 * 8.740742683410645
Epoch 280, val loss: 2.1526100635528564
Epoch 290, training loss: 89.42048645019531 = 2.151889204978943 + 10.0 * 8.726860046386719
Epoch 290, val loss: 2.151907444000244
Epoch 300, training loss: 89.28375244140625 = 2.1511107683181763 + 10.0 * 8.713264465332031
Epoch 300, val loss: 2.1511549949645996
Epoch 310, training loss: 89.14762878417969 = 2.15042245388031 + 10.0 * 8.69972038269043
Epoch 310, val loss: 2.1504833698272705
Epoch 320, training loss: 89.02558135986328 = 2.149778723716736 + 10.0 * 8.687580108642578
Epoch 320, val loss: 2.149857759475708
Epoch 330, training loss: 88.92723846435547 = 2.149198293685913 + 10.0 * 8.677803993225098
Epoch 330, val loss: 2.149275302886963
Epoch 340, training loss: 88.85030364990234 = 2.1486263275146484 + 10.0 * 8.670167922973633
Epoch 340, val loss: 2.148709297180176
Epoch 350, training loss: 88.75630187988281 = 2.1480950117111206 + 10.0 * 8.660820960998535
Epoch 350, val loss: 2.1481752395629883
Epoch 360, training loss: 88.6629409790039 = 2.1476398706436157 + 10.0 * 8.651530265808105
Epoch 360, val loss: 2.147705078125
Epoch 370, training loss: 88.56892395019531 = 2.147205114364624 + 10.0 * 8.642171859741211
Epoch 370, val loss: 2.147271156311035
Epoch 380, training loss: 88.50081634521484 = 2.1468089818954468 + 10.0 * 8.635400772094727
Epoch 380, val loss: 2.1468377113342285
Epoch 390, training loss: 88.4157485961914 = 2.14625883102417 + 10.0 * 8.626949310302734
Epoch 390, val loss: 2.1463091373443604
Epoch 400, training loss: 88.35404968261719 = 2.1456947326660156 + 10.0 * 8.62083625793457
Epoch 400, val loss: 2.1457602977752686
Epoch 410, training loss: 88.29849243164062 = 2.1451269388198853 + 10.0 * 8.615336418151855
Epoch 410, val loss: 2.14522647857666
Epoch 420, training loss: 88.25038146972656 = 2.144561767578125 + 10.0 * 8.61058235168457
Epoch 420, val loss: 2.144667625427246
Epoch 430, training loss: 88.2081069946289 = 2.144003391265869 + 10.0 * 8.606410026550293
Epoch 430, val loss: 2.1441128253936768
Epoch 440, training loss: 88.17341613769531 = 2.14345920085907 + 10.0 * 8.602995872497559
Epoch 440, val loss: 2.1435537338256836
Epoch 450, training loss: 88.15628051757812 = 2.1429336071014404 + 10.0 * 8.601334571838379
Epoch 450, val loss: 2.14302396774292
Epoch 460, training loss: 88.1111831665039 = 2.1424665451049805 + 10.0 * 8.596872329711914
Epoch 460, val loss: 2.142566680908203
Epoch 470, training loss: 88.0848617553711 = 2.1420832872390747 + 10.0 * 8.594278335571289
Epoch 470, val loss: 2.142150640487671
Epoch 480, training loss: 88.0468521118164 = 2.1416707038879395 + 10.0 * 8.5905179977417
Epoch 480, val loss: 2.141803741455078
Epoch 490, training loss: 88.00630950927734 = 2.1413947343826294 + 10.0 * 8.586491584777832
Epoch 490, val loss: 2.141490936279297
Epoch 500, training loss: 87.96965789794922 = 2.141087293624878 + 10.0 * 8.582857131958008
Epoch 500, val loss: 2.141246795654297
Epoch 510, training loss: 87.92405700683594 = 2.1409168243408203 + 10.0 * 8.578313827514648
Epoch 510, val loss: 2.1410279273986816
Epoch 520, training loss: 87.86766815185547 = 2.140742301940918 + 10.0 * 8.57269287109375
Epoch 520, val loss: 2.1408469676971436
Epoch 530, training loss: 87.81906127929688 = 2.1405832767486572 + 10.0 * 8.56784725189209
Epoch 530, val loss: 2.140704393386841
Epoch 540, training loss: 87.7804946899414 = 2.1404563188552856 + 10.0 * 8.564003944396973
Epoch 540, val loss: 2.1405763626098633
Epoch 550, training loss: 87.7257308959961 = 2.14030921459198 + 10.0 * 8.558542251586914
Epoch 550, val loss: 2.1404294967651367
Epoch 560, training loss: 87.67304992675781 = 2.140196204185486 + 10.0 * 8.553285598754883
Epoch 560, val loss: 2.1403069496154785
Epoch 570, training loss: 87.6275863647461 = 2.1400781869888306 + 10.0 * 8.548750877380371
Epoch 570, val loss: 2.1401896476745605
Epoch 580, training loss: 87.61286163330078 = 2.139917254447937 + 10.0 * 8.547294616699219
Epoch 580, val loss: 2.1400651931762695
Epoch 590, training loss: 87.5373764038086 = 2.1398009061813354 + 10.0 * 8.53975772857666
Epoch 590, val loss: 2.1399130821228027
Epoch 600, training loss: 87.48969268798828 = 2.1396998167037964 + 10.0 * 8.534998893737793
Epoch 600, val loss: 2.1397809982299805
Epoch 610, training loss: 87.4631576538086 = 2.139612555503845 + 10.0 * 8.532354354858398
Epoch 610, val loss: 2.1396448612213135
Epoch 620, training loss: 87.4148178100586 = 2.1394110918045044 + 10.0 * 8.52754020690918
Epoch 620, val loss: 2.1394734382629395
Epoch 630, training loss: 87.37146759033203 = 2.1392627954483032 + 10.0 * 8.523221015930176
Epoch 630, val loss: 2.1393256187438965
Epoch 640, training loss: 87.33723449707031 = 2.139094591140747 + 10.0 * 8.519814491271973
Epoch 640, val loss: 2.139163017272949
Epoch 650, training loss: 87.30768585205078 = 2.138927698135376 + 10.0 * 8.516875267028809
Epoch 650, val loss: 2.1389918327331543
Epoch 660, training loss: 87.3221206665039 = 2.138743758201599 + 10.0 * 8.518338203430176
Epoch 660, val loss: 2.1387863159179688
Epoch 670, training loss: 87.26705169677734 = 2.1384655237197876 + 10.0 * 8.512858390808105
Epoch 670, val loss: 2.1385340690612793
Epoch 680, training loss: 87.23965454101562 = 2.138230800628662 + 10.0 * 8.51014232635498
Epoch 680, val loss: 2.138298988342285
Epoch 690, training loss: 87.21422576904297 = 2.1380151510238647 + 10.0 * 8.507620811462402
Epoch 690, val loss: 2.1380796432495117
Epoch 700, training loss: 87.1910629272461 = 2.1377803087234497 + 10.0 * 8.505328178405762
Epoch 700, val loss: 2.137845516204834
Epoch 710, training loss: 87.1705093383789 = 2.1375412940979004 + 10.0 * 8.503296852111816
Epoch 710, val loss: 2.1376116275787354
Epoch 720, training loss: 87.15064239501953 = 2.1372965574264526 + 10.0 * 8.501334190368652
Epoch 720, val loss: 2.137375593185425
Epoch 730, training loss: 87.13129425048828 = 2.137048840522766 + 10.0 * 8.499424934387207
Epoch 730, val loss: 2.137132167816162
Epoch 740, training loss: 87.15866088867188 = 2.136834502220154 + 10.0 * 8.502182960510254
Epoch 740, val loss: 2.1368696689605713
Epoch 750, training loss: 87.10325622558594 = 2.13652503490448 + 10.0 * 8.496672630310059
Epoch 750, val loss: 2.1366000175476074
Epoch 760, training loss: 87.08317565917969 = 2.136240839958191 + 10.0 * 8.494693756103516
Epoch 760, val loss: 2.136345148086548
Epoch 770, training loss: 87.06309509277344 = 2.1359623670578003 + 10.0 * 8.49271297454834
Epoch 770, val loss: 2.13608980178833
Epoch 780, training loss: 87.05789184570312 = 2.1356676816940308 + 10.0 * 8.492222785949707
Epoch 780, val loss: 2.135833740234375
Epoch 790, training loss: 87.03724670410156 = 2.1354302167892456 + 10.0 * 8.490181922912598
Epoch 790, val loss: 2.1355502605438232
Epoch 800, training loss: 87.01807403564453 = 2.1351200342178345 + 10.0 * 8.488295555114746
Epoch 800, val loss: 2.1352834701538086
Epoch 810, training loss: 86.99982452392578 = 2.134860396385193 + 10.0 * 8.486496925354004
Epoch 810, val loss: 2.1350173950195312
Epoch 820, training loss: 86.98648071289062 = 2.1345807313919067 + 10.0 * 8.485189437866211
Epoch 820, val loss: 2.134749174118042
Epoch 830, training loss: 86.9842758178711 = 2.1343092918395996 + 10.0 * 8.484996795654297
Epoch 830, val loss: 2.134474992752075
Epoch 840, training loss: 86.9635238647461 = 2.134008765220642 + 10.0 * 8.482951164245605
Epoch 840, val loss: 2.1341934204101562
Epoch 850, training loss: 86.9462890625 = 2.1337286233901978 + 10.0 * 8.481256484985352
Epoch 850, val loss: 2.133920669555664
Epoch 860, training loss: 86.93416595458984 = 2.133448839187622 + 10.0 * 8.480072021484375
Epoch 860, val loss: 2.1336517333984375
Epoch 870, training loss: 86.9297866821289 = 2.1331855058670044 + 10.0 * 8.479660034179688
Epoch 870, val loss: 2.1333770751953125
Epoch 880, training loss: 86.91252136230469 = 2.1328606605529785 + 10.0 * 8.47796630859375
Epoch 880, val loss: 2.13310170173645
Epoch 890, training loss: 86.9052734375 = 2.132596254348755 + 10.0 * 8.477267265319824
Epoch 890, val loss: 2.132812976837158
Epoch 900, training loss: 86.8904037475586 = 2.1323165893554688 + 10.0 * 8.475809097290039
Epoch 900, val loss: 2.132544994354248
Epoch 910, training loss: 86.89310455322266 = 2.1320470571517944 + 10.0 * 8.476105690002441
Epoch 910, val loss: 2.132283926010132
Epoch 920, training loss: 86.87948608398438 = 2.1317230463027954 + 10.0 * 8.474776268005371
Epoch 920, val loss: 2.1320061683654785
Epoch 930, training loss: 86.8566665649414 = 2.131472945213318 + 10.0 * 8.472519874572754
Epoch 930, val loss: 2.1317458152770996
Epoch 940, training loss: 86.84751892089844 = 2.1312129497528076 + 10.0 * 8.471631050109863
Epoch 940, val loss: 2.1314878463745117
Epoch 950, training loss: 86.83576965332031 = 2.130936861038208 + 10.0 * 8.470483779907227
Epoch 950, val loss: 2.1312332153320312
Epoch 960, training loss: 86.8254623413086 = 2.13068163394928 + 10.0 * 8.469477653503418
Epoch 960, val loss: 2.1309828758239746
Epoch 970, training loss: 86.81551361083984 = 2.130419611930847 + 10.0 * 8.468509674072266
Epoch 970, val loss: 2.1307342052459717
Epoch 980, training loss: 86.80850219726562 = 2.130163550376892 + 10.0 * 8.467833518981934
Epoch 980, val loss: 2.1304845809936523
Epoch 990, training loss: 86.79832458496094 = 2.129868984222412 + 10.0 * 8.466845512390137
Epoch 990, val loss: 2.1302270889282227
Epoch 1000, training loss: 86.79405975341797 = 2.1295970678329468 + 10.0 * 8.466445922851562
Epoch 1000, val loss: 2.1299641132354736
Epoch 1010, training loss: 86.77986907958984 = 2.1293379068374634 + 10.0 * 8.465052604675293
Epoch 1010, val loss: 2.1297125816345215
Epoch 1020, training loss: 86.76932525634766 = 2.1290979385375977 + 10.0 * 8.464022636413574
Epoch 1020, val loss: 2.129471778869629
Epoch 1030, training loss: 86.76327514648438 = 2.1288623809814453 + 10.0 * 8.463441848754883
Epoch 1030, val loss: 2.12923264503479
Epoch 1040, training loss: 86.77973937988281 = 2.128616452217102 + 10.0 * 8.465112686157227
Epoch 1040, val loss: 2.1289825439453125
Epoch 1050, training loss: 86.74520874023438 = 2.128324866294861 + 10.0 * 8.461688041687012
Epoch 1050, val loss: 2.128721237182617
Epoch 1060, training loss: 86.73941802978516 = 2.1280596256256104 + 10.0 * 8.461135864257812
Epoch 1060, val loss: 2.128488063812256
Epoch 1070, training loss: 86.72761535644531 = 2.127822160720825 + 10.0 * 8.459979057312012
Epoch 1070, val loss: 2.12825345993042
Epoch 1080, training loss: 86.7254867553711 = 2.1275882720947266 + 10.0 * 8.459790229797363
Epoch 1080, val loss: 2.1280157566070557
Epoch 1090, training loss: 86.71356201171875 = 2.127315640449524 + 10.0 * 8.458624839782715
Epoch 1090, val loss: 2.1277685165405273
Epoch 1100, training loss: 86.70606231689453 = 2.12706196308136 + 10.0 * 8.457900047302246
Epoch 1100, val loss: 2.127519369125366
Epoch 1110, training loss: 86.69902038574219 = 2.126792788505554 + 10.0 * 8.457222938537598
Epoch 1110, val loss: 2.1272783279418945
Epoch 1120, training loss: 86.69075012207031 = 2.1265534162521362 + 10.0 * 8.456418991088867
Epoch 1120, val loss: 2.1270384788513184
Epoch 1130, training loss: 86.70346069335938 = 2.1262768507003784 + 10.0 * 8.457718849182129
Epoch 1130, val loss: 2.126795768737793
Epoch 1140, training loss: 86.68465423583984 = 2.1260321140289307 + 10.0 * 8.455862045288086
Epoch 1140, val loss: 2.1265358924865723
Epoch 1150, training loss: 86.66886138916016 = 2.125770926475525 + 10.0 * 8.454309463500977
Epoch 1150, val loss: 2.1262874603271484
Epoch 1160, training loss: 86.66288757324219 = 2.1255074739456177 + 10.0 * 8.45373821258545
Epoch 1160, val loss: 2.1260454654693604
Epoch 1170, training loss: 86.67060089111328 = 2.1252399682998657 + 10.0 * 8.454536437988281
Epoch 1170, val loss: 2.125808000564575
Epoch 1180, training loss: 86.66197204589844 = 2.125002384185791 + 10.0 * 8.453697204589844
Epoch 1180, val loss: 2.125516891479492
Epoch 1190, training loss: 86.64430236816406 = 2.124696373939514 + 10.0 * 8.451960563659668
Epoch 1190, val loss: 2.125260829925537
Epoch 1200, training loss: 86.63957977294922 = 2.1244359016418457 + 10.0 * 8.45151424407959
Epoch 1200, val loss: 2.125020980834961
Epoch 1210, training loss: 86.62916564941406 = 2.1241984367370605 + 10.0 * 8.450496673583984
Epoch 1210, val loss: 2.124779224395752
Epoch 1220, training loss: 86.62445068359375 = 2.1239606142044067 + 10.0 * 8.450048446655273
Epoch 1220, val loss: 2.1245365142822266
Epoch 1230, training loss: 86.65533447265625 = 2.1237123012542725 + 10.0 * 8.45316219329834
Epoch 1230, val loss: 2.124288558959961
Epoch 1240, training loss: 86.6123046875 = 2.123415470123291 + 10.0 * 8.448888778686523
Epoch 1240, val loss: 2.1240234375
Epoch 1250, training loss: 86.60689544677734 = 2.1231666803359985 + 10.0 * 8.448372840881348
Epoch 1250, val loss: 2.1237807273864746
Epoch 1260, training loss: 86.59730529785156 = 2.122931122779846 + 10.0 * 8.447437286376953
Epoch 1260, val loss: 2.123542308807373
Epoch 1270, training loss: 86.59091186523438 = 2.122681736946106 + 10.0 * 8.446823120117188
Epoch 1270, val loss: 2.1233067512512207
Epoch 1280, training loss: 86.58465576171875 = 2.122441291809082 + 10.0 * 8.446221351623535
Epoch 1280, val loss: 2.123070240020752
Epoch 1290, training loss: 86.58088684082031 = 2.1222054958343506 + 10.0 * 8.445867538452148
Epoch 1290, val loss: 2.122833490371704
Epoch 1300, training loss: 86.58416748046875 = 2.1219452619552612 + 10.0 * 8.446222305297852
Epoch 1300, val loss: 2.122588872909546
Epoch 1310, training loss: 86.57830047607422 = 2.1216992139816284 + 10.0 * 8.445660591125488
Epoch 1310, val loss: 2.12233304977417
Epoch 1320, training loss: 86.5685043334961 = 2.1214340925216675 + 10.0 * 8.444706916809082
Epoch 1320, val loss: 2.122093915939331
Epoch 1330, training loss: 86.56043243408203 = 2.1211793422698975 + 10.0 * 8.443925857543945
Epoch 1330, val loss: 2.121854305267334
Epoch 1340, training loss: 86.54967498779297 = 2.1209486722946167 + 10.0 * 8.442873001098633
Epoch 1340, val loss: 2.1216177940368652
Epoch 1350, training loss: 86.54894256591797 = 2.120719790458679 + 10.0 * 8.442822456359863
Epoch 1350, val loss: 2.121387004852295
Epoch 1360, training loss: 86.55595397949219 = 2.1204885244369507 + 10.0 * 8.443546295166016
Epoch 1360, val loss: 2.1211462020874023
Epoch 1370, training loss: 86.53972625732422 = 2.1202057600021362 + 10.0 * 8.441951751708984
Epoch 1370, val loss: 2.120910167694092
Epoch 1380, training loss: 86.53146362304688 = 2.1199790239334106 + 10.0 * 8.44114875793457
Epoch 1380, val loss: 2.1206722259521484
Epoch 1390, training loss: 86.54766845703125 = 2.1197152137756348 + 10.0 * 8.442795753479004
Epoch 1390, val loss: 2.1204323768615723
Epoch 1400, training loss: 86.52366638183594 = 2.119486451148987 + 10.0 * 8.440418243408203
Epoch 1400, val loss: 2.1201882362365723
Epoch 1410, training loss: 86.51587677001953 = 2.1192424297332764 + 10.0 * 8.439663887023926
Epoch 1410, val loss: 2.119952440261841
Epoch 1420, training loss: 86.51560974121094 = 2.119006395339966 + 10.0 * 8.439661026000977
Epoch 1420, val loss: 2.119722366333008
Epoch 1430, training loss: 86.51165008544922 = 2.1187671422958374 + 10.0 * 8.439288139343262
Epoch 1430, val loss: 2.119488477706909
Epoch 1440, training loss: 86.50299072265625 = 2.1185179948806763 + 10.0 * 8.438447952270508
Epoch 1440, val loss: 2.1192574501037598
Epoch 1450, training loss: 86.51409149169922 = 2.1182610988616943 + 10.0 * 8.439582824707031
Epoch 1450, val loss: 2.1190290451049805
Epoch 1460, training loss: 86.4941177368164 = 2.1180328130722046 + 10.0 * 8.43760871887207
Epoch 1460, val loss: 2.118779182434082
Epoch 1470, training loss: 86.48890686035156 = 2.117799997329712 + 10.0 * 8.437110900878906
Epoch 1470, val loss: 2.1185505390167236
Epoch 1480, training loss: 86.48481750488281 = 2.117555022239685 + 10.0 * 8.436726570129395
Epoch 1480, val loss: 2.1183271408081055
Epoch 1490, training loss: 86.48043060302734 = 2.11733341217041 + 10.0 * 8.436309814453125
Epoch 1490, val loss: 2.1181020736694336
Epoch 1500, training loss: 86.50439453125 = 2.1170960664749146 + 10.0 * 8.438730239868164
Epoch 1500, val loss: 2.117875814437866
Epoch 1510, training loss: 86.49191284179688 = 2.1168389320373535 + 10.0 * 8.437507629394531
Epoch 1510, val loss: 2.1176295280456543
Epoch 1520, training loss: 86.46832275390625 = 2.116599202156067 + 10.0 * 8.435172080993652
Epoch 1520, val loss: 2.1173949241638184
Epoch 1530, training loss: 86.46622467041016 = 2.1163653135299683 + 10.0 * 8.434986114501953
Epoch 1530, val loss: 2.117175579071045
Epoch 1540, training loss: 86.46028900146484 = 2.1161493062973022 + 10.0 * 8.43441390991211
Epoch 1540, val loss: 2.116959810256958
Epoch 1550, training loss: 86.48261260986328 = 2.1159424781799316 + 10.0 * 8.436666488647461
Epoch 1550, val loss: 2.11673641204834
Epoch 1560, training loss: 86.46028900146484 = 2.115653157234192 + 10.0 * 8.434463500976562
Epoch 1560, val loss: 2.116504669189453
Epoch 1570, training loss: 86.4543685913086 = 2.115443229675293 + 10.0 * 8.433893203735352
Epoch 1570, val loss: 2.116276979446411
Epoch 1580, training loss: 86.44554901123047 = 2.115209698677063 + 10.0 * 8.43303394317627
Epoch 1580, val loss: 2.1160683631896973
Epoch 1590, training loss: 86.44010925292969 = 2.1149991750717163 + 10.0 * 8.432511329650879
Epoch 1590, val loss: 2.115861415863037
Epoch 1600, training loss: 86.43595123291016 = 2.114780902862549 + 10.0 * 8.432116508483887
Epoch 1600, val loss: 2.1156561374664307
Epoch 1610, training loss: 86.44041442871094 = 2.1145564317703247 + 10.0 * 8.432585716247559
Epoch 1610, val loss: 2.1154494285583496
Epoch 1620, training loss: 86.44070434570312 = 2.1143347024917603 + 10.0 * 8.432637214660645
Epoch 1620, val loss: 2.1152095794677734
Epoch 1630, training loss: 86.42489624023438 = 2.114091396331787 + 10.0 * 8.43108081817627
Epoch 1630, val loss: 2.1149845123291016
Epoch 1640, training loss: 86.42035675048828 = 2.113871455192566 + 10.0 * 8.430648803710938
Epoch 1640, val loss: 2.114783763885498
Epoch 1650, training loss: 86.415771484375 = 2.1136659383773804 + 10.0 * 8.430211067199707
Epoch 1650, val loss: 2.1145877838134766
Epoch 1660, training loss: 86.41178131103516 = 2.1134597063064575 + 10.0 * 8.429832458496094
Epoch 1660, val loss: 2.1143946647644043
Epoch 1670, training loss: 86.40982055664062 = 2.113255262374878 + 10.0 * 8.429656028747559
Epoch 1670, val loss: 2.114199638366699
Epoch 1680, training loss: 86.4289321899414 = 2.11302649974823 + 10.0 * 8.43159008026123
Epoch 1680, val loss: 2.1139931678771973
Epoch 1690, training loss: 86.41053771972656 = 2.1128408908843994 + 10.0 * 8.429769515991211
Epoch 1690, val loss: 2.1137747764587402
Epoch 1700, training loss: 86.4072036743164 = 2.112608790397644 + 10.0 * 8.429459571838379
Epoch 1700, val loss: 2.1135621070861816
Epoch 1710, training loss: 86.39741516113281 = 2.112369656562805 + 10.0 * 8.428504943847656
Epoch 1710, val loss: 2.113368034362793
Epoch 1720, training loss: 86.39007568359375 = 2.1121984720230103 + 10.0 * 8.427787780761719
Epoch 1720, val loss: 2.1131768226623535
Epoch 1730, training loss: 86.38399505615234 = 2.111989378929138 + 10.0 * 8.427200317382812
Epoch 1730, val loss: 2.112992286682129
Epoch 1740, training loss: 86.3808364868164 = 2.1117907762527466 + 10.0 * 8.426904678344727
Epoch 1740, val loss: 2.1128063201904297
Epoch 1750, training loss: 86.38157653808594 = 2.1115870475769043 + 10.0 * 8.42699909210205
Epoch 1750, val loss: 2.112621307373047
Epoch 1760, training loss: 86.39485931396484 = 2.111373543739319 + 10.0 * 8.428348541259766
Epoch 1760, val loss: 2.1124229431152344
Epoch 1770, training loss: 86.3817367553711 = 2.1111961603164673 + 10.0 * 8.427053451538086
Epoch 1770, val loss: 2.112212657928467
Epoch 1780, training loss: 86.37447357177734 = 2.110968232154846 + 10.0 * 8.426350593566895
Epoch 1780, val loss: 2.1120238304138184
Epoch 1790, training loss: 86.36332702636719 = 2.1107842922210693 + 10.0 * 8.425253868103027
Epoch 1790, val loss: 2.1118323802948
Epoch 1800, training loss: 86.35899353027344 = 2.1105772256851196 + 10.0 * 8.424840927124023
Epoch 1800, val loss: 2.111654281616211
Epoch 1810, training loss: 86.358642578125 = 2.1103814840316772 + 10.0 * 8.424825668334961
Epoch 1810, val loss: 2.1114730834960938
Epoch 1820, training loss: 86.36038970947266 = 2.1101889610290527 + 10.0 * 8.425020217895508
Epoch 1820, val loss: 2.11128306388855
Epoch 1830, training loss: 86.35437774658203 = 2.110002279281616 + 10.0 * 8.424437522888184
Epoch 1830, val loss: 2.111093759536743
Epoch 1840, training loss: 86.35688018798828 = 2.1097874641418457 + 10.0 * 8.42470932006836
Epoch 1840, val loss: 2.1109085083007812
Epoch 1850, training loss: 86.34149169921875 = 2.1095885038375854 + 10.0 * 8.423190116882324
Epoch 1850, val loss: 2.1107101440429688
Epoch 1860, training loss: 86.33922576904297 = 2.109405040740967 + 10.0 * 8.422982215881348
Epoch 1860, val loss: 2.1105268001556396
Epoch 1870, training loss: 86.33432006835938 = 2.1092188358306885 + 10.0 * 8.422510147094727
Epoch 1870, val loss: 2.1103475093841553
Epoch 1880, training loss: 86.3333740234375 = 2.109038472175598 + 10.0 * 8.422433853149414
Epoch 1880, val loss: 2.11016845703125
Epoch 1890, training loss: 86.3501968383789 = 2.1088603734970093 + 10.0 * 8.42413330078125
Epoch 1890, val loss: 2.109980583190918
Epoch 1900, training loss: 86.34288787841797 = 2.1086106300354004 + 10.0 * 8.42342758178711
Epoch 1900, val loss: 2.109783887863159
Epoch 1910, training loss: 86.32756805419922 = 2.1084214448928833 + 10.0 * 8.421915054321289
Epoch 1910, val loss: 2.1095919609069824
Epoch 1920, training loss: 86.32695770263672 = 2.1082119941711426 + 10.0 * 8.421874046325684
Epoch 1920, val loss: 2.109407901763916
Epoch 1930, training loss: 86.31969451904297 = 2.1080223321914673 + 10.0 * 8.421167373657227
Epoch 1930, val loss: 2.1092214584350586
Epoch 1940, training loss: 86.31355285644531 = 2.1078449487686157 + 10.0 * 8.420571327209473
Epoch 1940, val loss: 2.1090407371520996
Epoch 1950, training loss: 86.31049346923828 = 2.107654333114624 + 10.0 * 8.420284271240234
Epoch 1950, val loss: 2.1088619232177734
Epoch 1960, training loss: 86.32341766357422 = 2.1074804067611694 + 10.0 * 8.42159366607666
Epoch 1960, val loss: 2.1086788177490234
Epoch 1970, training loss: 86.3083267211914 = 2.107245683670044 + 10.0 * 8.4201078414917
Epoch 1970, val loss: 2.10847806930542
Epoch 1980, training loss: 86.30411529541016 = 2.1070436239242554 + 10.0 * 8.419707298278809
Epoch 1980, val loss: 2.1082816123962402
Epoch 1990, training loss: 86.29792022705078 = 2.1068496704101562 + 10.0 * 8.419107437133789
Epoch 1990, val loss: 2.108102321624756
Epoch 2000, training loss: 86.29617309570312 = 2.106673240661621 + 10.0 * 8.418950080871582
Epoch 2000, val loss: 2.1079282760620117
Epoch 2010, training loss: 86.30014038085938 = 2.106490135192871 + 10.0 * 8.419364929199219
Epoch 2010, val loss: 2.1077535152435303
Epoch 2020, training loss: 86.29830932617188 = 2.10629141330719 + 10.0 * 8.419201850891113
Epoch 2020, val loss: 2.1075665950775146
Epoch 2030, training loss: 86.29275512695312 = 2.1060961484909058 + 10.0 * 8.418665885925293
Epoch 2030, val loss: 2.1073832511901855
Epoch 2040, training loss: 86.29468536376953 = 2.105908989906311 + 10.0 * 8.418877601623535
Epoch 2040, val loss: 2.1072049140930176
Epoch 2050, training loss: 86.28217315673828 = 2.1057208776474 + 10.0 * 8.417645454406738
Epoch 2050, val loss: 2.1070199012756348
Epoch 2060, training loss: 86.28101348876953 = 2.1055251359939575 + 10.0 * 8.417549133300781
Epoch 2060, val loss: 2.1068453788757324
Epoch 2070, training loss: 86.27892303466797 = 2.105345606803894 + 10.0 * 8.417357444763184
Epoch 2070, val loss: 2.106673240661621
Epoch 2080, training loss: 86.30208587646484 = 2.1051502227783203 + 10.0 * 8.419693946838379
Epoch 2080, val loss: 2.1064977645874023
Epoch 2090, training loss: 86.27539825439453 = 2.1049511432647705 + 10.0 * 8.417044639587402
Epoch 2090, val loss: 2.106290817260742
Epoch 2100, training loss: 86.27143859863281 = 2.1047645807266235 + 10.0 * 8.416666984558105
Epoch 2100, val loss: 2.106114625930786
Epoch 2110, training loss: 86.2679443359375 = 2.104588031768799 + 10.0 * 8.416335105895996
Epoch 2110, val loss: 2.105942726135254
Epoch 2120, training loss: 86.27590942382812 = 2.104421854019165 + 10.0 * 8.41714859008789
Epoch 2120, val loss: 2.1057732105255127
Epoch 2130, training loss: 86.26323699951172 = 2.1042139530181885 + 10.0 * 8.415902137756348
Epoch 2130, val loss: 2.1055960655212402
Epoch 2140, training loss: 86.26173400878906 = 2.104036569595337 + 10.0 * 8.415769577026367
Epoch 2140, val loss: 2.105424642562866
Epoch 2150, training loss: 86.27970886230469 = 2.1038472652435303 + 10.0 * 8.417586326599121
Epoch 2150, val loss: 2.1052489280700684
Epoch 2160, training loss: 86.259033203125 = 2.103668689727783 + 10.0 * 8.415536880493164
Epoch 2160, val loss: 2.1050679683685303
Epoch 2170, training loss: 86.25212097167969 = 2.1034772396087646 + 10.0 * 8.414864540100098
Epoch 2170, val loss: 2.10490345954895
Epoch 2180, training loss: 86.24806213378906 = 2.1033118963241577 + 10.0 * 8.414475440979004
Epoch 2180, val loss: 2.104741334915161
Epoch 2190, training loss: 86.24766540527344 = 2.1031359434127808 + 10.0 * 8.414453506469727
Epoch 2190, val loss: 2.104581832885742
Epoch 2200, training loss: 86.26627349853516 = 2.102945566177368 + 10.0 * 8.416333198547363
Epoch 2200, val loss: 2.104417324066162
Epoch 2210, training loss: 86.25164794921875 = 2.1027708053588867 + 10.0 * 8.414888381958008
Epoch 2210, val loss: 2.104226589202881
Epoch 2220, training loss: 86.24162292480469 = 2.102586269378662 + 10.0 * 8.413904190063477
Epoch 2220, val loss: 2.1040573120117188
Epoch 2230, training loss: 86.236328125 = 2.102433681488037 + 10.0 * 8.413389205932617
Epoch 2230, val loss: 2.1038997173309326
Epoch 2240, training loss: 86.23348236083984 = 2.1022597551345825 + 10.0 * 8.413122177124023
Epoch 2240, val loss: 2.103745460510254
Epoch 2250, training loss: 86.2386474609375 = 2.102110266685486 + 10.0 * 8.413653373718262
Epoch 2250, val loss: 2.1035890579223633
Epoch 2260, training loss: 86.2471923828125 = 2.1019163131713867 + 10.0 * 8.414527893066406
Epoch 2260, val loss: 2.1034114360809326
Epoch 2270, training loss: 86.2271957397461 = 2.1017208099365234 + 10.0 * 8.41254711151123
Epoch 2270, val loss: 2.1032309532165527
Epoch 2280, training loss: 86.22295379638672 = 2.1015514135360718 + 10.0 * 8.412139892578125
Epoch 2280, val loss: 2.1030731201171875
Epoch 2290, training loss: 86.22001647949219 = 2.101394534111023 + 10.0 * 8.41186237335205
Epoch 2290, val loss: 2.102924346923828
Epoch 2300, training loss: 86.217041015625 = 2.101240038871765 + 10.0 * 8.411580085754395
Epoch 2300, val loss: 2.1027774810791016
Epoch 2310, training loss: 86.22578430175781 = 2.101093292236328 + 10.0 * 8.412468910217285
Epoch 2310, val loss: 2.102626323699951
Epoch 2320, training loss: 86.21414184570312 = 2.100877523422241 + 10.0 * 8.41132640838623
Epoch 2320, val loss: 2.102440357208252
Epoch 2330, training loss: 86.22225189208984 = 2.1007091999053955 + 10.0 * 8.412154197692871
Epoch 2330, val loss: 2.1022605895996094
Epoch 2340, training loss: 86.20752716064453 = 2.100537896156311 + 10.0 * 8.410698890686035
Epoch 2340, val loss: 2.1021077632904053
Epoch 2350, training loss: 86.20427703857422 = 2.1003772020339966 + 10.0 * 8.41038990020752
Epoch 2350, val loss: 2.101968288421631
Epoch 2360, training loss: 86.20118713378906 = 2.1002310514450073 + 10.0 * 8.41009521484375
Epoch 2360, val loss: 2.101830005645752
Epoch 2370, training loss: 86.19827270507812 = 2.100086212158203 + 10.0 * 8.409818649291992
Epoch 2370, val loss: 2.1016883850097656
Epoch 2380, training loss: 86.19609069824219 = 2.0999319553375244 + 10.0 * 8.409616470336914
Epoch 2380, val loss: 2.1015448570251465
Epoch 2390, training loss: 86.24705505371094 = 2.0997759103775024 + 10.0 * 8.414728164672852
Epoch 2390, val loss: 2.1013927459716797
Epoch 2400, training loss: 86.21282958984375 = 2.09956693649292 + 10.0 * 8.41132640838623
Epoch 2400, val loss: 2.1012001037597656
Epoch 2410, training loss: 86.1937026977539 = 2.099396586418152 + 10.0 * 8.409430503845215
Epoch 2410, val loss: 2.1010384559631348
Epoch 2420, training loss: 86.18589782714844 = 2.099246382713318 + 10.0 * 8.408665657043457
Epoch 2420, val loss: 2.1008992195129395
Epoch 2430, training loss: 86.18363952636719 = 2.099100708961487 + 10.0 * 8.408453941345215
Epoch 2430, val loss: 2.100764274597168
Epoch 2440, training loss: 86.18003845214844 = 2.098958373069763 + 10.0 * 8.40810775756836
Epoch 2440, val loss: 2.1006293296813965
Epoch 2450, training loss: 86.17991638183594 = 2.098816752433777 + 10.0 * 8.408109664916992
Epoch 2450, val loss: 2.1004903316497803
Epoch 2460, training loss: 86.21662139892578 = 2.0986708402633667 + 10.0 * 8.411794662475586
Epoch 2460, val loss: 2.100339412689209
Epoch 2470, training loss: 86.18619537353516 = 2.0984551906585693 + 10.0 * 8.408773422241211
Epoch 2470, val loss: 2.1001667976379395
Epoch 2480, training loss: 86.17372131347656 = 2.0983165502548218 + 10.0 * 8.407540321350098
Epoch 2480, val loss: 2.1000189781188965
Epoch 2490, training loss: 86.1680679321289 = 2.0981603860855103 + 10.0 * 8.406991004943848
Epoch 2490, val loss: 2.099882125854492
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.39565217391304347
0.8145330725204666
The final CL Acc:0.39686, 0.00107, The final GNN Acc:0.81407, 0.00038
raw graph: torch.Size([2, 88648])
add edge: torch.Size([2, 110752])
remove edge: torch.Size([2, 66372])
updated graph: torch.Size([2, 88476])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 107.99311065673828 = 2.1706475019454956 + 10.0 * 10.582246780395508
Epoch 0, val loss: 2.170694351196289
Epoch 10, training loss: 107.98085021972656 = 2.161777138710022 + 10.0 * 10.581907272338867
Epoch 10, val loss: 2.1621501445770264
Epoch 20, training loss: 107.96186065673828 = 2.1538774967193604 + 10.0 * 10.580798149108887
Epoch 20, val loss: 2.15450382232666
Epoch 30, training loss: 107.9151840209961 = 2.1482101678848267 + 10.0 * 10.57669734954834
Epoch 30, val loss: 2.1491189002990723
Epoch 40, training loss: 107.76042175292969 = 2.1466513872146606 + 10.0 * 10.561376571655273
Epoch 40, val loss: 2.1478519439697266
Epoch 50, training loss: 107.28765869140625 = 2.148714542388916 + 10.0 * 10.513895034790039
Epoch 50, val loss: 2.149940013885498
Epoch 60, training loss: 106.18903350830078 = 2.1528923511505127 + 10.0 * 10.403614044189453
Epoch 60, val loss: 2.1540117263793945
Epoch 70, training loss: 104.22020721435547 = 2.159245252609253 + 10.0 * 10.206095695495605
Epoch 70, val loss: 2.160621166229248
Epoch 80, training loss: 102.04314422607422 = 2.166996955871582 + 10.0 * 9.987614631652832
Epoch 80, val loss: 2.1681876182556152
Epoch 90, training loss: 100.71854400634766 = 2.1698793172836304 + 10.0 * 9.854866981506348
Epoch 90, val loss: 2.1700940132141113
Epoch 100, training loss: 99.50419616699219 = 2.1654752492904663 + 10.0 * 9.733872413635254
Epoch 100, val loss: 2.1650137901306152
Epoch 110, training loss: 98.14878845214844 = 2.1619335412979126 + 10.0 * 9.598685264587402
Epoch 110, val loss: 2.161482334136963
Epoch 120, training loss: 96.90060424804688 = 2.1603060960769653 + 10.0 * 9.474029541015625
Epoch 120, val loss: 2.15993595123291
Epoch 130, training loss: 95.94493103027344 = 2.1602132320404053 + 10.0 * 9.378472328186035
Epoch 130, val loss: 2.15997314453125
Epoch 140, training loss: 95.14087677001953 = 2.160576581954956 + 10.0 * 9.298029899597168
Epoch 140, val loss: 2.160460948944092
Epoch 150, training loss: 94.68400573730469 = 2.1595910787582397 + 10.0 * 9.25244140625
Epoch 150, val loss: 2.1594200134277344
Epoch 160, training loss: 94.49024963378906 = 2.1565001010894775 + 10.0 * 9.23337459564209
Epoch 160, val loss: 2.1562328338623047
Epoch 170, training loss: 94.3003158569336 = 2.1524407863616943 + 10.0 * 9.214787483215332
Epoch 170, val loss: 2.152217388153076
Epoch 180, training loss: 93.99282836914062 = 2.1489237546920776 + 10.0 * 9.1843900680542
Epoch 180, val loss: 2.148787498474121
Epoch 190, training loss: 93.4634780883789 = 2.1460249423980713 + 10.0 * 9.131745338439941
Epoch 190, val loss: 2.1459789276123047
Epoch 200, training loss: 92.65581512451172 = 2.144157886505127 + 10.0 * 9.051165580749512
Epoch 200, val loss: 2.1440560817718506
Epoch 210, training loss: 92.05509948730469 = 2.1429132223129272 + 10.0 * 8.991218566894531
Epoch 210, val loss: 2.1426594257354736
Epoch 220, training loss: 91.79661560058594 = 2.142314910888672 + 10.0 * 8.965429306030273
Epoch 220, val loss: 2.1419005393981934
Epoch 230, training loss: 91.4508285522461 = 2.1424944400787354 + 10.0 * 8.93083381652832
Epoch 230, val loss: 2.142052412033081
Epoch 240, training loss: 91.02594757080078 = 2.1442750692367554 + 10.0 * 8.888167381286621
Epoch 240, val loss: 2.143893241882324
Epoch 250, training loss: 90.68582916259766 = 2.1464332342147827 + 10.0 * 8.8539400100708
Epoch 250, val loss: 2.1460089683532715
Epoch 260, training loss: 90.55342864990234 = 2.146790623664856 + 10.0 * 8.84066390991211
Epoch 260, val loss: 2.1462392807006836
Epoch 270, training loss: 90.40955352783203 = 2.1458911895751953 + 10.0 * 8.826366424560547
Epoch 270, val loss: 2.145311117172241
Epoch 280, training loss: 90.2510757446289 = 2.145408511161804 + 10.0 * 8.810566902160645
Epoch 280, val loss: 2.144944667816162
Epoch 290, training loss: 90.03146362304688 = 2.1457184553146362 + 10.0 * 8.78857421875
Epoch 290, val loss: 2.1453542709350586
Epoch 300, training loss: 89.71768188476562 = 2.1466230154037476 + 10.0 * 8.757105827331543
Epoch 300, val loss: 2.1464481353759766
Epoch 310, training loss: 89.36018371582031 = 2.1480751037597656 + 10.0 * 8.721210479736328
Epoch 310, val loss: 2.1480062007904053
Epoch 320, training loss: 89.03060150146484 = 2.1491748094558716 + 10.0 * 8.688142776489258
Epoch 320, val loss: 2.1492137908935547
Epoch 330, training loss: 88.80260467529297 = 2.1498055458068848 + 10.0 * 8.66528034210205
Epoch 330, val loss: 2.1498966217041016
Epoch 340, training loss: 88.64958190917969 = 2.149765133857727 + 10.0 * 8.649981498718262
Epoch 340, val loss: 2.149872064590454
Epoch 350, training loss: 88.54904174804688 = 2.149411201477051 + 10.0 * 8.639963150024414
Epoch 350, val loss: 2.149496555328369
Epoch 360, training loss: 88.47138214111328 = 2.1491174697875977 + 10.0 * 8.632226943969727
Epoch 360, val loss: 2.149143695831299
Epoch 370, training loss: 88.4061050415039 = 2.148811101913452 + 10.0 * 8.62572956085205
Epoch 370, val loss: 2.1488866806030273
Epoch 380, training loss: 88.3449935913086 = 2.148628830909729 + 10.0 * 8.619636535644531
Epoch 380, val loss: 2.1487255096435547
Epoch 390, training loss: 88.27996826171875 = 2.1485103368759155 + 10.0 * 8.61314582824707
Epoch 390, val loss: 2.148616313934326
Epoch 400, training loss: 88.20691680908203 = 2.148422956466675 + 10.0 * 8.605849266052246
Epoch 400, val loss: 2.1485469341278076
Epoch 410, training loss: 88.13668060302734 = 2.1483705043792725 + 10.0 * 8.598831176757812
Epoch 410, val loss: 2.1484873294830322
Epoch 420, training loss: 88.0469970703125 = 2.1483906507492065 + 10.0 * 8.589860916137695
Epoch 420, val loss: 2.1485257148742676
Epoch 430, training loss: 87.97168731689453 = 2.1483867168426514 + 10.0 * 8.582330703735352
Epoch 430, val loss: 2.148560047149658
Epoch 440, training loss: 87.90724182128906 = 2.148332357406616 + 10.0 * 8.575891494750977
Epoch 440, val loss: 2.1485257148742676
Epoch 450, training loss: 87.89518737792969 = 2.1482216119766235 + 10.0 * 8.57469654083252
Epoch 450, val loss: 2.1483750343322754
Epoch 460, training loss: 87.79544067382812 = 2.1479313373565674 + 10.0 * 8.564750671386719
Epoch 460, val loss: 2.148207187652588
Epoch 470, training loss: 87.7324447631836 = 2.147786855697632 + 10.0 * 8.558465957641602
Epoch 470, val loss: 2.148007392883301
Epoch 480, training loss: 87.6668701171875 = 2.1476287841796875 + 10.0 * 8.551923751831055
Epoch 480, val loss: 2.147876739501953
Epoch 490, training loss: 87.6030044555664 = 2.147484302520752 + 10.0 * 8.545552253723145
Epoch 490, val loss: 2.1477530002593994
Epoch 500, training loss: 87.55079650878906 = 2.1473082304000854 + 10.0 * 8.540349006652832
Epoch 500, val loss: 2.1476235389709473
Epoch 510, training loss: 87.47545623779297 = 2.147203207015991 + 10.0 * 8.532825469970703
Epoch 510, val loss: 2.1474990844726562
Epoch 520, training loss: 87.41943359375 = 2.14707088470459 + 10.0 * 8.527235984802246
Epoch 520, val loss: 2.1473841667175293
Epoch 530, training loss: 87.36621856689453 = 2.146937608718872 + 10.0 * 8.521928787231445
Epoch 530, val loss: 2.147271156311035
Epoch 540, training loss: 87.32088470458984 = 2.1467885971069336 + 10.0 * 8.517409324645996
Epoch 540, val loss: 2.1471691131591797
Epoch 550, training loss: 87.25942993164062 = 2.146660327911377 + 10.0 * 8.511277198791504
Epoch 550, val loss: 2.147057056427002
Epoch 560, training loss: 87.2088851928711 = 2.146560311317444 + 10.0 * 8.506232261657715
Epoch 560, val loss: 2.14695405960083
Epoch 570, training loss: 87.18107604980469 = 2.1464120149612427 + 10.0 * 8.503466606140137
Epoch 570, val loss: 2.146836757659912
Epoch 580, training loss: 87.12168884277344 = 2.1462396383285522 + 10.0 * 8.49754524230957
Epoch 580, val loss: 2.1466598510742188
Epoch 590, training loss: 87.08479309082031 = 2.146028995513916 + 10.0 * 8.493876457214355
Epoch 590, val loss: 2.146465539932251
Epoch 600, training loss: 87.06053924560547 = 2.1458311080932617 + 10.0 * 8.491471290588379
Epoch 600, val loss: 2.146188259124756
Epoch 610, training loss: 87.0250244140625 = 2.1455070972442627 + 10.0 * 8.487951278686523
Epoch 610, val loss: 2.14591908454895
Epoch 620, training loss: 87.001220703125 = 2.1451903581619263 + 10.0 * 8.485603332519531
Epoch 620, val loss: 2.1456284523010254
Epoch 630, training loss: 86.97527313232422 = 2.144859552383423 + 10.0 * 8.483041763305664
Epoch 630, val loss: 2.1452865600585938
Epoch 640, training loss: 86.95360565185547 = 2.14453661441803 + 10.0 * 8.48090648651123
Epoch 640, val loss: 2.1449689865112305
Epoch 650, training loss: 86.92906188964844 = 2.1442134380340576 + 10.0 * 8.478485107421875
Epoch 650, val loss: 2.1446614265441895
Epoch 660, training loss: 86.90448760986328 = 2.1439119577407837 + 10.0 * 8.476057052612305
Epoch 660, val loss: 2.1443471908569336
Epoch 670, training loss: 86.89836883544922 = 2.143606424331665 + 10.0 * 8.475476264953613
Epoch 670, val loss: 2.144022226333618
Epoch 680, training loss: 86.86425018310547 = 2.1432552337646484 + 10.0 * 8.472099304199219
Epoch 680, val loss: 2.143718719482422
Epoch 690, training loss: 86.83374786376953 = 2.1429730653762817 + 10.0 * 8.469077110290527
Epoch 690, val loss: 2.143430233001709
Epoch 700, training loss: 86.80171203613281 = 2.14268159866333 + 10.0 * 8.465902328491211
Epoch 700, val loss: 2.1431355476379395
Epoch 710, training loss: 86.78885650634766 = 2.1423985958099365 + 10.0 * 8.464646339416504
Epoch 710, val loss: 2.1428580284118652
Epoch 720, training loss: 86.77327728271484 = 2.142118453979492 + 10.0 * 8.463115692138672
Epoch 720, val loss: 2.1425204277038574
Epoch 730, training loss: 86.72893524169922 = 2.1418256759643555 + 10.0 * 8.458711624145508
Epoch 730, val loss: 2.1422438621520996
Epoch 740, training loss: 86.70184326171875 = 2.1415317058563232 + 10.0 * 8.45603084564209
Epoch 740, val loss: 2.1419668197631836
Epoch 750, training loss: 86.68013763427734 = 2.141247510910034 + 10.0 * 8.453888893127441
Epoch 750, val loss: 2.141690969467163
Epoch 760, training loss: 86.6594467163086 = 2.140978693962097 + 10.0 * 8.451847076416016
Epoch 760, val loss: 2.1414132118225098
Epoch 770, training loss: 86.67092895507812 = 2.1406580209732056 + 10.0 * 8.453027725219727
Epoch 770, val loss: 2.1411120891571045
Epoch 780, training loss: 86.62899017333984 = 2.1403802633285522 + 10.0 * 8.448861122131348
Epoch 780, val loss: 2.1408133506774902
Epoch 790, training loss: 86.60111236572266 = 2.140128493309021 + 10.0 * 8.446098327636719
Epoch 790, val loss: 2.1405560970306396
Epoch 800, training loss: 86.58055877685547 = 2.1398773193359375 + 10.0 * 8.44406795501709
Epoch 800, val loss: 2.140303134918213
Epoch 810, training loss: 86.56224060058594 = 2.1396374702453613 + 10.0 * 8.442259788513184
Epoch 810, val loss: 2.1400585174560547
Epoch 820, training loss: 86.54833984375 = 2.1393468379974365 + 10.0 * 8.440899848937988
Epoch 820, val loss: 2.139812469482422
Epoch 830, training loss: 86.5252685546875 = 2.139100432395935 + 10.0 * 8.438616752624512
Epoch 830, val loss: 2.13956618309021
Epoch 840, training loss: 86.50468444824219 = 2.1388697624206543 + 10.0 * 8.4365816116333
Epoch 840, val loss: 2.139336585998535
Epoch 850, training loss: 86.48103332519531 = 2.1386436223983765 + 10.0 * 8.434239387512207
Epoch 850, val loss: 2.1391043663024902
Epoch 860, training loss: 86.45964813232422 = 2.1384294033050537 + 10.0 * 8.432122230529785
Epoch 860, val loss: 2.1388866901397705
Epoch 870, training loss: 86.45134735107422 = 2.1382142305374146 + 10.0 * 8.431313514709473
Epoch 870, val loss: 2.138662815093994
Epoch 880, training loss: 86.44542694091797 = 2.137933373451233 + 10.0 * 8.430749893188477
Epoch 880, val loss: 2.1383893489837646
Epoch 890, training loss: 86.41046142578125 = 2.1376991271972656 + 10.0 * 8.427275657653809
Epoch 890, val loss: 2.138160228729248
Epoch 900, training loss: 86.38707733154297 = 2.137469172477722 + 10.0 * 8.42496109008789
Epoch 900, val loss: 2.137924909591675
Epoch 910, training loss: 86.36917877197266 = 2.1372270584106445 + 10.0 * 8.423194885253906
Epoch 910, val loss: 2.137688159942627
Epoch 920, training loss: 86.3579330444336 = 2.1369656324386597 + 10.0 * 8.422097206115723
Epoch 920, val loss: 2.1374447345733643
Epoch 930, training loss: 86.35615539550781 = 2.136702299118042 + 10.0 * 8.421945571899414
Epoch 930, val loss: 2.1371350288391113
Epoch 940, training loss: 86.33049774169922 = 2.136430263519287 + 10.0 * 8.41940689086914
Epoch 940, val loss: 2.1368770599365234
Epoch 950, training loss: 86.31356048583984 = 2.1361700296401978 + 10.0 * 8.417738914489746
Epoch 950, val loss: 2.136626720428467
Epoch 960, training loss: 86.29852294921875 = 2.1358935832977295 + 10.0 * 8.41626262664795
Epoch 960, val loss: 2.136359691619873
Epoch 970, training loss: 86.29964447021484 = 2.1356106996536255 + 10.0 * 8.416402816772461
Epoch 970, val loss: 2.136073589324951
Epoch 980, training loss: 86.28202819824219 = 2.1353012323379517 + 10.0 * 8.4146728515625
Epoch 980, val loss: 2.135775566101074
Epoch 990, training loss: 86.2637710571289 = 2.1350632905960083 + 10.0 * 8.412870407104492
Epoch 990, val loss: 2.1355342864990234
Epoch 1000, training loss: 86.24773406982422 = 2.134787082672119 + 10.0 * 8.411294937133789
Epoch 1000, val loss: 2.1352834701538086
Epoch 1010, training loss: 86.23436737060547 = 2.1345479488372803 + 10.0 * 8.409982681274414
Epoch 1010, val loss: 2.1350293159484863
Epoch 1020, training loss: 86.22148895263672 = 2.134278655052185 + 10.0 * 8.408720970153809
Epoch 1020, val loss: 2.134780168533325
Epoch 1030, training loss: 86.21018981933594 = 2.1340187788009644 + 10.0 * 8.407617568969727
Epoch 1030, val loss: 2.1345326900482178
Epoch 1040, training loss: 86.23355865478516 = 2.1337332725524902 + 10.0 * 8.409982681274414
Epoch 1040, val loss: 2.1342859268188477
Epoch 1050, training loss: 86.19925689697266 = 2.133476734161377 + 10.0 * 8.406578063964844
Epoch 1050, val loss: 2.13399076461792
Epoch 1060, training loss: 86.18069458007812 = 2.1332157850265503 + 10.0 * 8.40474796295166
Epoch 1060, val loss: 2.133715867996216
Epoch 1070, training loss: 86.16410064697266 = 2.132968544960022 + 10.0 * 8.40311336517334
Epoch 1070, val loss: 2.1334776878356934
Epoch 1080, training loss: 86.15193176269531 = 2.132701277732849 + 10.0 * 8.401923179626465
Epoch 1080, val loss: 2.133246898651123
Epoch 1090, training loss: 86.14266204833984 = 2.132458806037903 + 10.0 * 8.401020050048828
Epoch 1090, val loss: 2.133004665374756
Epoch 1100, training loss: 86.14169311523438 = 2.132184147834778 + 10.0 * 8.40095043182373
Epoch 1100, val loss: 2.1327497959136963
Epoch 1110, training loss: 86.1404800415039 = 2.1319637298583984 + 10.0 * 8.400851249694824
Epoch 1110, val loss: 2.132493495941162
Epoch 1120, training loss: 86.11932373046875 = 2.131672739982605 + 10.0 * 8.398764610290527
Epoch 1120, val loss: 2.1322288513183594
Epoch 1130, training loss: 86.09827423095703 = 2.1314350366592407 + 10.0 * 8.396684646606445
Epoch 1130, val loss: 2.1319918632507324
Epoch 1140, training loss: 86.08928680419922 = 2.1311943531036377 + 10.0 * 8.395809173583984
Epoch 1140, val loss: 2.1317410469055176
Epoch 1150, training loss: 86.08499145507812 = 2.1309434175491333 + 10.0 * 8.395404815673828
Epoch 1150, val loss: 2.1315035820007324
Epoch 1160, training loss: 86.0777359008789 = 2.1306700706481934 + 10.0 * 8.394706726074219
Epoch 1160, val loss: 2.131230592727661
Epoch 1170, training loss: 86.06511688232422 = 2.130415916442871 + 10.0 * 8.39346981048584
Epoch 1170, val loss: 2.130979537963867
Epoch 1180, training loss: 86.0573501586914 = 2.130152940750122 + 10.0 * 8.392720222473145
Epoch 1180, val loss: 2.1307315826416016
Epoch 1190, training loss: 86.04729461669922 = 2.1299067735671997 + 10.0 * 8.391738891601562
Epoch 1190, val loss: 2.1304755210876465
Epoch 1200, training loss: 86.04395294189453 = 2.1296513080596924 + 10.0 * 8.391429901123047
Epoch 1200, val loss: 2.130216121673584
Epoch 1210, training loss: 86.04025268554688 = 2.129339337348938 + 10.0 * 8.391091346740723
Epoch 1210, val loss: 2.129934549331665
Epoch 1220, training loss: 86.03693389892578 = 2.129081606864929 + 10.0 * 8.390785217285156
Epoch 1220, val loss: 2.1296510696411133
Epoch 1230, training loss: 86.02095031738281 = 2.128822445869446 + 10.0 * 8.389212608337402
Epoch 1230, val loss: 2.1293976306915283
Epoch 1240, training loss: 86.01237487792969 = 2.128544807434082 + 10.0 * 8.388382911682129
Epoch 1240, val loss: 2.129145622253418
Epoch 1250, training loss: 86.00599670410156 = 2.1282832622528076 + 10.0 * 8.387771606445312
Epoch 1250, val loss: 2.1288938522338867
Epoch 1260, training loss: 86.00116729736328 = 2.1280105113983154 + 10.0 * 8.38731575012207
Epoch 1260, val loss: 2.1286330223083496
Epoch 1270, training loss: 86.03776550292969 = 2.127692937850952 + 10.0 * 8.391007423400879
Epoch 1270, val loss: 2.128329038619995
Epoch 1280, training loss: 86.00061798095703 = 2.127405285835266 + 10.0 * 8.387321472167969
Epoch 1280, val loss: 2.1280689239501953
Epoch 1290, training loss: 85.98271942138672 = 2.1271679401397705 + 10.0 * 8.385555267333984
Epoch 1290, val loss: 2.1278114318847656
Epoch 1300, training loss: 85.97506713867188 = 2.1269129514694214 + 10.0 * 8.384815216064453
Epoch 1300, val loss: 2.1275475025177
Epoch 1310, training loss: 85.96980285644531 = 2.1266592741012573 + 10.0 * 8.384313583374023
Epoch 1310, val loss: 2.1272897720336914
Epoch 1320, training loss: 85.96405029296875 = 2.126396417617798 + 10.0 * 8.38376522064209
Epoch 1320, val loss: 2.127035140991211
Epoch 1330, training loss: 85.97464752197266 = 2.1261212825775146 + 10.0 * 8.384852409362793
Epoch 1330, val loss: 2.1267833709716797
Epoch 1340, training loss: 85.95520782470703 = 2.125844717025757 + 10.0 * 8.382936477661133
Epoch 1340, val loss: 2.1265006065368652
Epoch 1350, training loss: 85.95626831054688 = 2.1255838871002197 + 10.0 * 8.383068084716797
Epoch 1350, val loss: 2.126256227493286
Epoch 1360, training loss: 85.94684600830078 = 2.1253201961517334 + 10.0 * 8.382152557373047
Epoch 1360, val loss: 2.125983715057373
Epoch 1370, training loss: 85.93754577636719 = 2.1250603199005127 + 10.0 * 8.381248474121094
Epoch 1370, val loss: 2.125735282897949
Epoch 1380, training loss: 85.92874908447266 = 2.1248077154159546 + 10.0 * 8.380393981933594
Epoch 1380, val loss: 2.12548828125
Epoch 1390, training loss: 85.92366027832031 = 2.1245545148849487 + 10.0 * 8.379910469055176
Epoch 1390, val loss: 2.1252450942993164
Epoch 1400, training loss: 85.93022155761719 = 2.124304175376892 + 10.0 * 8.38059139251709
Epoch 1400, val loss: 2.125004529953003
Epoch 1410, training loss: 85.91546630859375 = 2.124022603034973 + 10.0 * 8.379144668579102
Epoch 1410, val loss: 2.124720335006714
Epoch 1420, training loss: 85.90943145751953 = 2.1237730979919434 + 10.0 * 8.378565788269043
Epoch 1420, val loss: 2.124481678009033
Epoch 1430, training loss: 85.90491485595703 = 2.123528838157654 + 10.0 * 8.378138542175293
Epoch 1430, val loss: 2.1242289543151855
Epoch 1440, training loss: 85.9039306640625 = 2.1232725381851196 + 10.0 * 8.37806510925293
Epoch 1440, val loss: 2.1239852905273438
Epoch 1450, training loss: 85.89203643798828 = 2.123023509979248 + 10.0 * 8.376901626586914
Epoch 1450, val loss: 2.1237480640411377
Epoch 1460, training loss: 85.91072082519531 = 2.122766852378845 + 10.0 * 8.378795623779297
Epoch 1460, val loss: 2.1234917640686035
Epoch 1470, training loss: 85.88926696777344 = 2.122499704360962 + 10.0 * 8.376676559448242
Epoch 1470, val loss: 2.1232383251190186
Epoch 1480, training loss: 85.87661743164062 = 2.1222517490386963 + 10.0 * 8.375436782836914
Epoch 1480, val loss: 2.1230034828186035
Epoch 1490, training loss: 85.86965942382812 = 2.1220091581344604 + 10.0 * 8.374765396118164
Epoch 1490, val loss: 2.1227636337280273
Epoch 1500, training loss: 85.86843872070312 = 2.121764898300171 + 10.0 * 8.374667167663574
Epoch 1500, val loss: 2.1225311756134033
Epoch 1510, training loss: 85.8821792602539 = 2.121495246887207 + 10.0 * 8.376068115234375
Epoch 1510, val loss: 2.122269630432129
Epoch 1520, training loss: 85.85576629638672 = 2.1212538480758667 + 10.0 * 8.373451232910156
Epoch 1520, val loss: 2.1220202445983887
Epoch 1530, training loss: 85.84964752197266 = 2.121004343032837 + 10.0 * 8.372864723205566
Epoch 1530, val loss: 2.1217896938323975
Epoch 1540, training loss: 85.84510040283203 = 2.1207644939422607 + 10.0 * 8.37243366241455
Epoch 1540, val loss: 2.1215572357177734
Epoch 1550, training loss: 85.85515594482422 = 2.1204992532730103 + 10.0 * 8.373465538024902
Epoch 1550, val loss: 2.121335506439209
Epoch 1560, training loss: 85.84069061279297 = 2.1202645301818848 + 10.0 * 8.372042655944824
Epoch 1560, val loss: 2.1210265159606934
Epoch 1570, training loss: 85.83755493164062 = 2.1199982166290283 + 10.0 * 8.371755599975586
Epoch 1570, val loss: 2.120826482772827
Epoch 1580, training loss: 85.825439453125 = 2.11977219581604 + 10.0 * 8.370566368103027
Epoch 1580, val loss: 2.120588779449463
Epoch 1590, training loss: 85.82242584228516 = 2.119544267654419 + 10.0 * 8.370287895202637
Epoch 1590, val loss: 2.120347023010254
Epoch 1600, training loss: 85.84526824951172 = 2.119292974472046 + 10.0 * 8.372597694396973
Epoch 1600, val loss: 2.1200995445251465
Epoch 1610, training loss: 85.82133483886719 = 2.1190249919891357 + 10.0 * 8.370230674743652
Epoch 1610, val loss: 2.119873046875
Epoch 1620, training loss: 85.8119888305664 = 2.1187965869903564 + 10.0 * 8.369318962097168
Epoch 1620, val loss: 2.1196274757385254
Epoch 1630, training loss: 85.80455017089844 = 2.118554949760437 + 10.0 * 8.368599891662598
Epoch 1630, val loss: 2.1194005012512207
Epoch 1640, training loss: 85.8000717163086 = 2.118322253227234 + 10.0 * 8.36817455291748
Epoch 1640, val loss: 2.1191728115081787
Epoch 1650, training loss: 85.82315063476562 = 2.118082046508789 + 10.0 * 8.37050724029541
Epoch 1650, val loss: 2.1189377307891846
Epoch 1660, training loss: 85.8084945678711 = 2.1178025007247925 + 10.0 * 8.36906909942627
Epoch 1660, val loss: 2.118666648864746
Epoch 1670, training loss: 85.78921508789062 = 2.1175715923309326 + 10.0 * 8.367164611816406
Epoch 1670, val loss: 2.118447780609131
Epoch 1680, training loss: 85.78422546386719 = 2.117338538169861 + 10.0 * 8.36668872833252
Epoch 1680, val loss: 2.11820912361145
Epoch 1690, training loss: 85.78036499023438 = 2.117100238800049 + 10.0 * 8.366326332092285
Epoch 1690, val loss: 2.117980718612671
Epoch 1700, training loss: 85.78851318359375 = 2.116863250732422 + 10.0 * 8.367164611816406
Epoch 1700, val loss: 2.1177475452423096
Epoch 1710, training loss: 85.77259826660156 = 2.116599202156067 + 10.0 * 8.365599632263184
Epoch 1710, val loss: 2.117488145828247
Epoch 1720, training loss: 85.76888275146484 = 2.1163679361343384 + 10.0 * 8.365251541137695
Epoch 1720, val loss: 2.1172738075256348
Epoch 1730, training loss: 85.76443481445312 = 2.116129755973816 + 10.0 * 8.36483097076416
Epoch 1730, val loss: 2.1170358657836914
Epoch 1740, training loss: 85.76260375976562 = 2.1159013509750366 + 10.0 * 8.364670753479004
Epoch 1740, val loss: 2.116809368133545
Epoch 1750, training loss: 85.7874755859375 = 2.1156601905822754 + 10.0 * 8.367181777954102
Epoch 1750, val loss: 2.116562843322754
Epoch 1760, training loss: 85.76070404052734 = 2.1153945922851562 + 10.0 * 8.364530563354492
Epoch 1760, val loss: 2.116331100463867
Epoch 1770, training loss: 85.75017547607422 = 2.1151610612869263 + 10.0 * 8.36350154876709
Epoch 1770, val loss: 2.116098642349243
Epoch 1780, training loss: 85.74878692626953 = 2.11493456363678 + 10.0 * 8.363385200500488
Epoch 1780, val loss: 2.1158735752105713
Epoch 1790, training loss: 85.76002502441406 = 2.1146918535232544 + 10.0 * 8.364533424377441
Epoch 1790, val loss: 2.1156461238861084
Epoch 1800, training loss: 85.74144744873047 = 2.11443555355072 + 10.0 * 8.362701416015625
Epoch 1800, val loss: 2.1153900623321533
Epoch 1810, training loss: 85.7367935180664 = 2.114202857017517 + 10.0 * 8.362258911132812
Epoch 1810, val loss: 2.1151647567749023
Epoch 1820, training loss: 85.7348861694336 = 2.1139742136001587 + 10.0 * 8.362091064453125
Epoch 1820, val loss: 2.1149444580078125
Epoch 1830, training loss: 85.7313003540039 = 2.1137375831604004 + 10.0 * 8.361756324768066
Epoch 1830, val loss: 2.114717483520508
Epoch 1840, training loss: 85.74298858642578 = 2.1134883165359497 + 10.0 * 8.362950325012207
Epoch 1840, val loss: 2.1144919395446777
Epoch 1850, training loss: 85.73523712158203 = 2.1132466793060303 + 10.0 * 8.362199783325195
Epoch 1850, val loss: 2.1142477989196777
Epoch 1860, training loss: 85.72159576416016 = 2.112998366355896 + 10.0 * 8.360859870910645
Epoch 1860, val loss: 2.1140031814575195
Epoch 1870, training loss: 85.71814727783203 = 2.1127861738204956 + 10.0 * 8.360536575317383
Epoch 1870, val loss: 2.1137707233428955
Epoch 1880, training loss: 85.71430206298828 = 2.112544059753418 + 10.0 * 8.360176086425781
Epoch 1880, val loss: 2.1135575771331787
Epoch 1890, training loss: 85.7107162475586 = 2.112326741218567 + 10.0 * 8.359838485717773
Epoch 1890, val loss: 2.1133294105529785
Epoch 1900, training loss: 85.71221923828125 = 2.1120946407318115 + 10.0 * 8.360013008117676
Epoch 1900, val loss: 2.113107204437256
Epoch 1910, training loss: 85.72832489013672 = 2.111830949783325 + 10.0 * 8.361649513244629
Epoch 1910, val loss: 2.112849473953247
Epoch 1920, training loss: 85.70368957519531 = 2.111607789993286 + 10.0 * 8.359208106994629
Epoch 1920, val loss: 2.112633228302002
Epoch 1930, training loss: 85.69860076904297 = 2.1113749742507935 + 10.0 * 8.358722686767578
Epoch 1930, val loss: 2.1124095916748047
Epoch 1940, training loss: 85.69637298583984 = 2.111151337623596 + 10.0 * 8.358522415161133
Epoch 1940, val loss: 2.112184524536133
Epoch 1950, training loss: 85.69583129882812 = 2.110934257507324 + 10.0 * 8.358489990234375
Epoch 1950, val loss: 2.111969470977783
Epoch 1960, training loss: 85.72770690917969 = 2.110693097114563 + 10.0 * 8.361701011657715
Epoch 1960, val loss: 2.1117138862609863
Epoch 1970, training loss: 85.69026184082031 = 2.110430598258972 + 10.0 * 8.357983589172363
Epoch 1970, val loss: 2.1114938259124756
Epoch 1980, training loss: 85.68714904785156 = 2.1102012395858765 + 10.0 * 8.357694625854492
Epoch 1980, val loss: 2.1112723350524902
Epoch 1990, training loss: 85.68757629394531 = 2.1099870204925537 + 10.0 * 8.357759475708008
Epoch 1990, val loss: 2.111055374145508
Epoch 2000, training loss: 85.68934631347656 = 2.1097500324249268 + 10.0 * 8.357959747314453
Epoch 2000, val loss: 2.1108312606811523
Epoch 2010, training loss: 85.67938232421875 = 2.109539031982422 + 10.0 * 8.35698413848877
Epoch 2010, val loss: 2.110602855682373
Epoch 2020, training loss: 85.6766357421875 = 2.109315514564514 + 10.0 * 8.356732368469238
Epoch 2020, val loss: 2.110384464263916
Epoch 2030, training loss: 85.68074035644531 = 2.1090877056121826 + 10.0 * 8.357165336608887
Epoch 2030, val loss: 2.1101739406585693
Epoch 2040, training loss: 85.6714096069336 = 2.1088621616363525 + 10.0 * 8.356254577636719
Epoch 2040, val loss: 2.10994815826416
Epoch 2050, training loss: 85.6667251586914 = 2.108641266822815 + 10.0 * 8.35580825805664
Epoch 2050, val loss: 2.109736680984497
Epoch 2060, training loss: 85.66386413574219 = 2.108419418334961 + 10.0 * 8.355544090270996
Epoch 2060, val loss: 2.1095261573791504
Epoch 2070, training loss: 85.67032623291016 = 2.108190894126892 + 10.0 * 8.356213569641113
Epoch 2070, val loss: 2.1093201637268066
Epoch 2080, training loss: 85.66058349609375 = 2.10796058177948 + 10.0 * 8.35526180267334
Epoch 2080, val loss: 2.109084129333496
Epoch 2090, training loss: 85.662109375 = 2.107760429382324 + 10.0 * 8.355435371398926
Epoch 2090, val loss: 2.108881950378418
Epoch 2100, training loss: 85.67162322998047 = 2.10751473903656 + 10.0 * 8.35641098022461
Epoch 2100, val loss: 2.1086583137512207
Epoch 2110, training loss: 85.654052734375 = 2.1073111295700073 + 10.0 * 8.354674339294434
Epoch 2110, val loss: 2.108429431915283
Epoch 2120, training loss: 85.64762115478516 = 2.1070836782455444 + 10.0 * 8.354053497314453
Epoch 2120, val loss: 2.1082258224487305
Epoch 2130, training loss: 85.6446762084961 = 2.106889247894287 + 10.0 * 8.353778839111328
Epoch 2130, val loss: 2.1080284118652344
Epoch 2140, training loss: 85.64594268798828 = 2.1066731214523315 + 10.0 * 8.353926658630371
Epoch 2140, val loss: 2.1078286170959473
Epoch 2150, training loss: 85.6575698852539 = 2.106436252593994 + 10.0 * 8.35511302947998
Epoch 2150, val loss: 2.1075901985168457
Epoch 2160, training loss: 85.63750457763672 = 2.10622501373291 + 10.0 * 8.353128433227539
Epoch 2160, val loss: 2.107382297515869
Epoch 2170, training loss: 85.632568359375 = 2.1060153245925903 + 10.0 * 8.352655410766602
Epoch 2170, val loss: 2.107174873352051
Epoch 2180, training loss: 85.63200378417969 = 2.1058167219161987 + 10.0 * 8.352618217468262
Epoch 2180, val loss: 2.1069841384887695
Epoch 2190, training loss: 85.64652252197266 = 2.1056041717529297 + 10.0 * 8.35409164428711
Epoch 2190, val loss: 2.1067686080932617
Epoch 2200, training loss: 85.62870788574219 = 2.1053847074508667 + 10.0 * 8.35233211517334
Epoch 2200, val loss: 2.106566905975342
Epoch 2210, training loss: 85.62260437011719 = 2.10517954826355 + 10.0 * 8.3517427444458
Epoch 2210, val loss: 2.10636043548584
Epoch 2220, training loss: 85.62066650390625 = 2.104981541633606 + 10.0 * 8.351568222045898
Epoch 2220, val loss: 2.106171131134033
Epoch 2230, training loss: 85.6288833618164 = 2.104775309562683 + 10.0 * 8.352411270141602
Epoch 2230, val loss: 2.105966567993164
Epoch 2240, training loss: 85.61522674560547 = 2.104559898376465 + 10.0 * 8.351066589355469
Epoch 2240, val loss: 2.1057567596435547
Epoch 2250, training loss: 85.61286163330078 = 2.1043694019317627 + 10.0 * 8.350849151611328
Epoch 2250, val loss: 2.1055760383605957
Epoch 2260, training loss: 85.61563873291016 = 2.1041765213012695 + 10.0 * 8.35114574432373
Epoch 2260, val loss: 2.105379581451416
Epoch 2270, training loss: 85.61115264892578 = 2.1039057970046997 + 10.0 * 8.350725173950195
Epoch 2270, val loss: 2.105123519897461
Epoch 2280, training loss: 85.6142578125 = 2.103731155395508 + 10.0 * 8.351053237915039
Epoch 2280, val loss: 2.1049447059631348
Epoch 2290, training loss: 85.59831237792969 = 2.1035165786743164 + 10.0 * 8.349479675292969
Epoch 2290, val loss: 2.1047425270080566
Epoch 2300, training loss: 85.59736633300781 = 2.103337287902832 + 10.0 * 8.34940242767334
Epoch 2300, val loss: 2.1045737266540527
Epoch 2310, training loss: 85.59259796142578 = 2.1031566858291626 + 10.0 * 8.348943710327148
Epoch 2310, val loss: 2.104393720626831
Epoch 2320, training loss: 85.58992004394531 = 2.102971076965332 + 10.0 * 8.348694801330566
Epoch 2320, val loss: 2.104214668273926
Epoch 2330, training loss: 85.59257507324219 = 2.102774977684021 + 10.0 * 8.348979949951172
Epoch 2330, val loss: 2.104041814804077
Epoch 2340, training loss: 85.59618377685547 = 2.102531313896179 + 10.0 * 8.349365234375
Epoch 2340, val loss: 2.103795289993286
Epoch 2350, training loss: 85.58876037597656 = 2.1023507118225098 + 10.0 * 8.348641395568848
Epoch 2350, val loss: 2.103630304336548
Epoch 2360, training loss: 85.5870132446289 = 2.1021701097488403 + 10.0 * 8.34848403930664
Epoch 2360, val loss: 2.1034250259399414
Epoch 2370, training loss: 85.58755493164062 = 2.1019738912582397 + 10.0 * 8.34855842590332
Epoch 2370, val loss: 2.1032323837280273
Epoch 2380, training loss: 85.57373809814453 = 2.1017826795578003 + 10.0 * 8.347195625305176
Epoch 2380, val loss: 2.103060483932495
Epoch 2390, training loss: 85.57405853271484 = 2.1015976667404175 + 10.0 * 8.347246170043945
Epoch 2390, val loss: 2.1028800010681152
Epoch 2400, training loss: 85.57367706298828 = 2.1014182567596436 + 10.0 * 8.3472261428833
Epoch 2400, val loss: 2.1026992797851562
Epoch 2410, training loss: 85.57107543945312 = 2.101228356361389 + 10.0 * 8.34698486328125
Epoch 2410, val loss: 2.1025161743164062
Epoch 2420, training loss: 85.59379577636719 = 2.1010323762893677 + 10.0 * 8.349276542663574
Epoch 2420, val loss: 2.1023263931274414
Epoch 2430, training loss: 85.56879425048828 = 2.100820779800415 + 10.0 * 8.346796989440918
Epoch 2430, val loss: 2.102125883102417
Epoch 2440, training loss: 85.56228637695312 = 2.100634455680847 + 10.0 * 8.346165657043457
Epoch 2440, val loss: 2.1019411087036133
Epoch 2450, training loss: 85.55622863769531 = 2.100459933280945 + 10.0 * 8.345577239990234
Epoch 2450, val loss: 2.101775646209717
Epoch 2460, training loss: 85.55215454101562 = 2.100284695625305 + 10.0 * 8.345187187194824
Epoch 2460, val loss: 2.1016054153442383
Epoch 2470, training loss: 85.54888916015625 = 2.100109100341797 + 10.0 * 8.344878196716309
Epoch 2470, val loss: 2.101436138153076
Epoch 2480, training loss: 85.55209350585938 = 2.099937081336975 + 10.0 * 8.345215797424316
Epoch 2480, val loss: 2.1012697219848633
Epoch 2490, training loss: 85.57573699951172 = 2.0997194051742554 + 10.0 * 8.347601890563965
Epoch 2490, val loss: 2.1010594367980957
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.3972463768115942
0.8645222053176846
=== training gcn model ===
Epoch 0, training loss: 108.01685333251953 = 2.1948803663253784 + 10.0 * 10.582197189331055
Epoch 0, val loss: 2.1923155784606934
Epoch 10, training loss: 108.00621032714844 = 2.1897757053375244 + 10.0 * 10.581644058227539
Epoch 10, val loss: 2.1878552436828613
Epoch 20, training loss: 107.98407745361328 = 2.188097834587097 + 10.0 * 10.579598426818848
Epoch 20, val loss: 2.1858112812042236
Epoch 30, training loss: 107.89353942871094 = 2.187627077102661 + 10.0 * 10.57059097290039
Epoch 30, val loss: 2.1853928565979004
Epoch 40, training loss: 107.51278686523438 = 2.1880760192871094 + 10.0 * 10.532471656799316
Epoch 40, val loss: 2.186206817626953
Epoch 50, training loss: 106.1822738647461 = 2.189318537712097 + 10.0 * 10.399295806884766
Epoch 50, val loss: 2.187983512878418
Epoch 60, training loss: 103.01248168945312 = 2.190683126449585 + 10.0 * 10.08218002319336
Epoch 60, val loss: 2.1897411346435547
Epoch 70, training loss: 99.26663208007812 = 2.190288543701172 + 10.0 * 9.707634925842285
Epoch 70, val loss: 2.189453125
Epoch 80, training loss: 97.37386322021484 = 2.1884262561798096 + 10.0 * 9.51854419708252
Epoch 80, val loss: 2.1875510215759277
Epoch 90, training loss: 95.62496948242188 = 2.1869927644729614 + 10.0 * 9.34379768371582
Epoch 90, val loss: 2.186116933822632
Epoch 100, training loss: 94.22123718261719 = 2.187463641166687 + 10.0 * 9.203377723693848
Epoch 100, val loss: 2.1866791248321533
Epoch 110, training loss: 93.27020263671875 = 2.1876319646835327 + 10.0 * 9.108257293701172
Epoch 110, val loss: 2.186882972717285
Epoch 120, training loss: 92.25177001953125 = 2.187061309814453 + 10.0 * 9.006470680236816
Epoch 120, val loss: 2.18631911277771
Epoch 130, training loss: 91.43106079101562 = 2.186604380607605 + 10.0 * 8.924445152282715
Epoch 130, val loss: 2.185856580734253
Epoch 140, training loss: 90.77875518798828 = 2.1867690086364746 + 10.0 * 8.859198570251465
Epoch 140, val loss: 2.1859703063964844
Epoch 150, training loss: 90.24491882324219 = 2.186572790145874 + 10.0 * 8.805834770202637
Epoch 150, val loss: 2.185734272003174
Epoch 160, training loss: 89.92896270751953 = 2.186003565788269 + 10.0 * 8.774295806884766
Epoch 160, val loss: 2.1851508617401123
Epoch 170, training loss: 89.71012115478516 = 2.1857247352600098 + 10.0 * 8.752439498901367
Epoch 170, val loss: 2.184861898422241
Epoch 180, training loss: 89.5321273803711 = 2.1857502460479736 + 10.0 * 8.734637260437012
Epoch 180, val loss: 2.1848254203796387
Epoch 190, training loss: 89.34308624267578 = 2.185736298561096 + 10.0 * 8.715734481811523
Epoch 190, val loss: 2.184779405593872
Epoch 200, training loss: 89.1213607788086 = 2.185722827911377 + 10.0 * 8.693563461303711
Epoch 200, val loss: 2.184734344482422
Epoch 210, training loss: 88.87979125976562 = 2.1858363151550293 + 10.0 * 8.669395446777344
Epoch 210, val loss: 2.1847939491271973
Epoch 220, training loss: 88.66239929199219 = 2.1859508752822876 + 10.0 * 8.647644996643066
Epoch 220, val loss: 2.1848690509796143
Epoch 230, training loss: 88.48722076416016 = 2.185991883277893 + 10.0 * 8.630123138427734
Epoch 230, val loss: 2.1848549842834473
Epoch 240, training loss: 88.36225128173828 = 2.1859503984451294 + 10.0 * 8.617630004882812
Epoch 240, val loss: 2.1847681999206543
Epoch 250, training loss: 88.24342346191406 = 2.185864210128784 + 10.0 * 8.605755805969238
Epoch 250, val loss: 2.1846394538879395
Epoch 260, training loss: 88.13180541992188 = 2.185801148414612 + 10.0 * 8.594600677490234
Epoch 260, val loss: 2.1845409870147705
Epoch 270, training loss: 88.03617095947266 = 2.1857820749282837 + 10.0 * 8.585039138793945
Epoch 270, val loss: 2.1845004558563232
Epoch 280, training loss: 87.90612030029297 = 2.1857861280441284 + 10.0 * 8.572033882141113
Epoch 280, val loss: 2.1844606399536133
Epoch 290, training loss: 87.77738189697266 = 2.185859203338623 + 10.0 * 8.559152603149414
Epoch 290, val loss: 2.1844658851623535
Epoch 300, training loss: 87.68064880371094 = 2.185887932777405 + 10.0 * 8.54947566986084
Epoch 300, val loss: 2.1844606399536133
Epoch 310, training loss: 87.56863403320312 = 2.1858770847320557 + 10.0 * 8.538275718688965
Epoch 310, val loss: 2.184415817260742
Epoch 320, training loss: 87.4776382446289 = 2.1858742237091064 + 10.0 * 8.529176712036133
Epoch 320, val loss: 2.184372663497925
Epoch 330, training loss: 87.39575958251953 = 2.18586802482605 + 10.0 * 8.520989418029785
Epoch 330, val loss: 2.18432879447937
Epoch 340, training loss: 87.33561706542969 = 2.1858577728271484 + 10.0 * 8.514975547790527
Epoch 340, val loss: 2.184267044067383
Epoch 350, training loss: 87.27179718017578 = 2.1857950687408447 + 10.0 * 8.508600234985352
Epoch 350, val loss: 2.1842000484466553
Epoch 360, training loss: 87.2107925415039 = 2.1857587099075317 + 10.0 * 8.502503395080566
Epoch 360, val loss: 2.184107542037964
Epoch 370, training loss: 87.15917205810547 = 2.1857229471206665 + 10.0 * 8.497344970703125
Epoch 370, val loss: 2.1840271949768066
Epoch 380, training loss: 87.1185073852539 = 2.185648202896118 + 10.0 * 8.4932861328125
Epoch 380, val loss: 2.18394136428833
Epoch 390, training loss: 87.06443786621094 = 2.185580253601074 + 10.0 * 8.487886428833008
Epoch 390, val loss: 2.1838393211364746
Epoch 400, training loss: 87.01560974121094 = 2.1855279207229614 + 10.0 * 8.48300838470459
Epoch 400, val loss: 2.1837477684020996
Epoch 410, training loss: 87.01282501220703 = 2.185468554496765 + 10.0 * 8.482735633850098
Epoch 410, val loss: 2.183659076690674
Epoch 420, training loss: 86.94979858398438 = 2.185396909713745 + 10.0 * 8.4764404296875
Epoch 420, val loss: 2.18355131149292
Epoch 430, training loss: 86.90019226074219 = 2.1853333711624146 + 10.0 * 8.47148609161377
Epoch 430, val loss: 2.18344783782959
Epoch 440, training loss: 86.85821533203125 = 2.1852755546569824 + 10.0 * 8.467294692993164
Epoch 440, val loss: 2.183353900909424
Epoch 450, training loss: 86.82331848144531 = 2.1852149963378906 + 10.0 * 8.463810920715332
Epoch 450, val loss: 2.1832637786865234
Epoch 460, training loss: 86.82466125488281 = 2.1851505041122437 + 10.0 * 8.463951110839844
Epoch 460, val loss: 2.1831719875335693
Epoch 470, training loss: 86.76211547851562 = 2.1850883960723877 + 10.0 * 8.45770263671875
Epoch 470, val loss: 2.1830739974975586
Epoch 480, training loss: 86.72694396972656 = 2.185031294822693 + 10.0 * 8.454191207885742
Epoch 480, val loss: 2.182981014251709
Epoch 490, training loss: 86.71173858642578 = 2.1849721670150757 + 10.0 * 8.452676773071289
Epoch 490, val loss: 2.1828904151916504
Epoch 500, training loss: 86.68672943115234 = 2.1848889589309692 + 10.0 * 8.450183868408203
Epoch 500, val loss: 2.1827988624572754
Epoch 510, training loss: 86.65120697021484 = 2.1848337650299072 + 10.0 * 8.446637153625488
Epoch 510, val loss: 2.182706832885742
Epoch 520, training loss: 86.6277847290039 = 2.1847751140594482 + 10.0 * 8.444300651550293
Epoch 520, val loss: 2.1826181411743164
Epoch 530, training loss: 86.62773132324219 = 2.1847041845321655 + 10.0 * 8.444302558898926
Epoch 530, val loss: 2.182527780532837
Epoch 540, training loss: 86.59139251708984 = 2.184646487236023 + 10.0 * 8.440674781799316
Epoch 540, val loss: 2.1824285984039307
Epoch 550, training loss: 86.56782531738281 = 2.1845850944519043 + 10.0 * 8.438323974609375
Epoch 550, val loss: 2.182338237762451
Epoch 560, training loss: 86.55903625488281 = 2.1845239400863647 + 10.0 * 8.437451362609863
Epoch 560, val loss: 2.182250499725342
Epoch 570, training loss: 86.53466796875 = 2.184456467628479 + 10.0 * 8.43502140045166
Epoch 570, val loss: 2.1821608543395996
Epoch 580, training loss: 86.52369689941406 = 2.1843968629837036 + 10.0 * 8.433930397033691
Epoch 580, val loss: 2.1820716857910156
Epoch 590, training loss: 86.50211334228516 = 2.184338688850403 + 10.0 * 8.431777000427246
Epoch 590, val loss: 2.181987762451172
Epoch 600, training loss: 86.48505401611328 = 2.1842833757400513 + 10.0 * 8.43007755279541
Epoch 600, val loss: 2.1819071769714355
Epoch 610, training loss: 86.48165130615234 = 2.1842256784439087 + 10.0 * 8.429742813110352
Epoch 610, val loss: 2.1818313598632812
Epoch 620, training loss: 86.49885559082031 = 2.184162974357605 + 10.0 * 8.431468963623047
Epoch 620, val loss: 2.181734085083008
Epoch 630, training loss: 86.45777130126953 = 2.1840951442718506 + 10.0 * 8.427367210388184
Epoch 630, val loss: 2.181643009185791
Epoch 640, training loss: 86.42611694335938 = 2.18405544757843 + 10.0 * 8.424205780029297
Epoch 640, val loss: 2.1815695762634277
Epoch 650, training loss: 86.40904235839844 = 2.1840083599090576 + 10.0 * 8.422503471374512
Epoch 650, val loss: 2.18149995803833
Epoch 660, training loss: 86.39386749267578 = 2.183962106704712 + 10.0 * 8.420990943908691
Epoch 660, val loss: 2.181428909301758
Epoch 670, training loss: 86.37781524658203 = 2.183919668197632 + 10.0 * 8.419389724731445
Epoch 670, val loss: 2.181359052658081
Epoch 680, training loss: 86.36490631103516 = 2.183875799179077 + 10.0 * 8.418103218078613
Epoch 680, val loss: 2.1812918186187744
Epoch 690, training loss: 86.35416412353516 = 2.183825969696045 + 10.0 * 8.417034149169922
Epoch 690, val loss: 2.181215524673462
Epoch 700, training loss: 86.3432388305664 = 2.183773994445801 + 10.0 * 8.415946006774902
Epoch 700, val loss: 2.1811423301696777
Epoch 710, training loss: 86.3268051147461 = 2.183733344078064 + 10.0 * 8.414307594299316
Epoch 710, val loss: 2.181076765060425
Epoch 720, training loss: 86.31062316894531 = 2.1836944818496704 + 10.0 * 8.41269302368164
Epoch 720, val loss: 2.18101167678833
Epoch 730, training loss: 86.2977294921875 = 2.1836541891098022 + 10.0 * 8.411407470703125
Epoch 730, val loss: 2.1809468269348145
Epoch 740, training loss: 86.2889175415039 = 2.1836005449295044 + 10.0 * 8.410531997680664
Epoch 740, val loss: 2.180875778198242
Epoch 750, training loss: 86.28113555908203 = 2.183541774749756 + 10.0 * 8.409759521484375
Epoch 750, val loss: 2.1807968616485596
Epoch 760, training loss: 86.27041625976562 = 2.183503031730652 + 10.0 * 8.40869140625
Epoch 760, val loss: 2.1807303428649902
Epoch 770, training loss: 86.25223541259766 = 2.1834654808044434 + 10.0 * 8.406877517700195
Epoch 770, val loss: 2.1806678771972656
Epoch 780, training loss: 86.241455078125 = 2.183423161506653 + 10.0 * 8.405802726745605
Epoch 780, val loss: 2.180605173110962
Epoch 790, training loss: 86.25675964355469 = 2.183382272720337 + 10.0 * 8.40733814239502
Epoch 790, val loss: 2.180541515350342
Epoch 800, training loss: 86.25239562988281 = 2.1833243370056152 + 10.0 * 8.406907081604004
Epoch 800, val loss: 2.1804709434509277
Epoch 810, training loss: 86.21388244628906 = 2.183284044265747 + 10.0 * 8.403059959411621
Epoch 810, val loss: 2.180403709411621
Epoch 820, training loss: 86.20378112792969 = 2.183242917060852 + 10.0 * 8.402053833007812
Epoch 820, val loss: 2.1803436279296875
Epoch 830, training loss: 86.19245910644531 = 2.1832008361816406 + 10.0 * 8.400925636291504
Epoch 830, val loss: 2.180288314819336
Epoch 840, training loss: 86.18133544921875 = 2.1831668615341187 + 10.0 * 8.399816513061523
Epoch 840, val loss: 2.180232524871826
Epoch 850, training loss: 86.17147064208984 = 2.1831289529800415 + 10.0 * 8.398834228515625
Epoch 850, val loss: 2.1801774501800537
Epoch 860, training loss: 86.164306640625 = 2.1830981969833374 + 10.0 * 8.398120880126953
Epoch 860, val loss: 2.1801223754882812
Epoch 870, training loss: 86.17164611816406 = 2.183057188987732 + 10.0 * 8.398859024047852
Epoch 870, val loss: 2.1800708770751953
Epoch 880, training loss: 86.15697479248047 = 2.1830185651779175 + 10.0 * 8.397395133972168
Epoch 880, val loss: 2.180006504058838
Epoch 890, training loss: 86.1358871459961 = 2.1829833984375 + 10.0 * 8.39529037475586
Epoch 890, val loss: 2.1799545288085938
Epoch 900, training loss: 86.15003204345703 = 2.1829479932785034 + 10.0 * 8.396708488464355
Epoch 900, val loss: 2.179903507232666
Epoch 910, training loss: 86.12371826171875 = 2.1828997135162354 + 10.0 * 8.394082069396973
Epoch 910, val loss: 2.1798510551452637
Epoch 920, training loss: 86.11143493652344 = 2.1828733682632446 + 10.0 * 8.392855644226074
Epoch 920, val loss: 2.1798043251037598
Epoch 930, training loss: 86.0977554321289 = 2.1828439235687256 + 10.0 * 8.391490936279297
Epoch 930, val loss: 2.179759979248047
Epoch 940, training loss: 86.08650970458984 = 2.1828187704086304 + 10.0 * 8.390369415283203
Epoch 940, val loss: 2.179718494415283
Epoch 950, training loss: 86.07987976074219 = 2.182791829109192 + 10.0 * 8.389708518981934
Epoch 950, val loss: 2.1796770095825195
Epoch 960, training loss: 86.1454086303711 = 2.182761073112488 + 10.0 * 8.396265029907227
Epoch 960, val loss: 2.1796302795410156
Epoch 970, training loss: 86.06387329101562 = 2.1827203035354614 + 10.0 * 8.388115882873535
Epoch 970, val loss: 2.1795778274536133
Epoch 980, training loss: 86.05690002441406 = 2.1826980113983154 + 10.0 * 8.387419700622559
Epoch 980, val loss: 2.179536819458008
Epoch 990, training loss: 86.04731750488281 = 2.182674765586853 + 10.0 * 8.38646411895752
Epoch 990, val loss: 2.1794991493225098
Epoch 1000, training loss: 86.05778503417969 = 2.1826508045196533 + 10.0 * 8.387514114379883
Epoch 1000, val loss: 2.1794610023498535
Epoch 1010, training loss: 86.0426254272461 = 2.1826138496398926 + 10.0 * 8.386000633239746
Epoch 1010, val loss: 2.179408073425293
Epoch 1020, training loss: 86.02941131591797 = 2.182584285736084 + 10.0 * 8.384682655334473
Epoch 1020, val loss: 2.1793675422668457
Epoch 1030, training loss: 86.01725769042969 = 2.182563304901123 + 10.0 * 8.383469581604004
Epoch 1030, val loss: 2.179332733154297
Epoch 1040, training loss: 86.0084457397461 = 2.1825417280197144 + 10.0 * 8.382590293884277
Epoch 1040, val loss: 2.1792969703674316
Epoch 1050, training loss: 86.00431823730469 = 2.182521939277649 + 10.0 * 8.382180213928223
Epoch 1050, val loss: 2.179263114929199
Epoch 1060, training loss: 86.014892578125 = 2.182494282722473 + 10.0 * 8.38323974609375
Epoch 1060, val loss: 2.179218292236328
Epoch 1070, training loss: 85.99797821044922 = 2.182455897331238 + 10.0 * 8.381551742553711
Epoch 1070, val loss: 2.179175853729248
Epoch 1080, training loss: 85.98639678955078 = 2.18244206905365 + 10.0 * 8.380395889282227
Epoch 1080, val loss: 2.1791396141052246
Epoch 1090, training loss: 85.97637939453125 = 2.1824159622192383 + 10.0 * 8.379396438598633
Epoch 1090, val loss: 2.1791086196899414
Epoch 1100, training loss: 85.97010040283203 = 2.1823967695236206 + 10.0 * 8.37877082824707
Epoch 1100, val loss: 2.179075241088867
Epoch 1110, training loss: 86.00688171386719 = 2.1823697090148926 + 10.0 * 8.382451057434082
Epoch 1110, val loss: 2.179041862487793
Epoch 1120, training loss: 85.9749526977539 = 2.1823498010635376 + 10.0 * 8.379260063171387
Epoch 1120, val loss: 2.1789984703063965
Epoch 1130, training loss: 85.95323181152344 = 2.1823235750198364 + 10.0 * 8.377090454101562
Epoch 1130, val loss: 2.178964614868164
Epoch 1140, training loss: 85.94686126708984 = 2.1823055744171143 + 10.0 * 8.376455307006836
Epoch 1140, val loss: 2.1789326667785645
Epoch 1150, training loss: 86.00714111328125 = 2.1822915077209473 + 10.0 * 8.382485389709473
Epoch 1150, val loss: 2.178896903991699
Epoch 1160, training loss: 85.94194030761719 = 2.1822391748428345 + 10.0 * 8.375970840454102
Epoch 1160, val loss: 2.1788454055786133
Epoch 1170, training loss: 85.9368896484375 = 2.182217836380005 + 10.0 * 8.375467300415039
Epoch 1170, val loss: 2.1788201332092285
Epoch 1180, training loss: 85.92337036132812 = 2.18220317363739 + 10.0 * 8.374116897583008
Epoch 1180, val loss: 2.1787919998168945
Epoch 1190, training loss: 85.91947937011719 = 2.182188630104065 + 10.0 * 8.37372875213623
Epoch 1190, val loss: 2.178762912750244
Epoch 1200, training loss: 85.93585968017578 = 2.1821643114089966 + 10.0 * 8.37536907196045
Epoch 1200, val loss: 2.1787338256835938
Epoch 1210, training loss: 85.90917205810547 = 2.1821383237838745 + 10.0 * 8.372703552246094
Epoch 1210, val loss: 2.1786928176879883
Epoch 1220, training loss: 85.90776824951172 = 2.1821142435073853 + 10.0 * 8.372565269470215
Epoch 1220, val loss: 2.178661584854126
Epoch 1230, training loss: 85.90113067626953 = 2.182098388671875 + 10.0 * 8.371903419494629
Epoch 1230, val loss: 2.178633213043213
Epoch 1240, training loss: 85.89421081542969 = 2.182079315185547 + 10.0 * 8.37121295928955
Epoch 1240, val loss: 2.1786069869995117
Epoch 1250, training loss: 85.89131164550781 = 2.1820610761642456 + 10.0 * 8.370924949645996
Epoch 1250, val loss: 2.178579330444336
Epoch 1260, training loss: 85.9364013671875 = 2.182034492492676 + 10.0 * 8.375436782836914
Epoch 1260, val loss: 2.1785478591918945
Epoch 1270, training loss: 85.88052368164062 = 2.1820108890533447 + 10.0 * 8.369851112365723
Epoch 1270, val loss: 2.1785101890563965
Epoch 1280, training loss: 85.87995910644531 = 2.181994915008545 + 10.0 * 8.369796752929688
Epoch 1280, val loss: 2.1784825325012207
Epoch 1290, training loss: 85.87041473388672 = 2.1818805932998657 + 10.0 * 8.368853569030762
Epoch 1290, val loss: 2.178257942199707
Epoch 1300, training loss: 85.87674713134766 = 2.179689884185791 + 10.0 * 8.369706153869629
Epoch 1300, val loss: 2.176128387451172
Epoch 1310, training loss: 85.86988830566406 = 2.1771820783615112 + 10.0 * 8.369270324707031
Epoch 1310, val loss: 2.1738405227661133
Epoch 1320, training loss: 85.8554458618164 = 2.1749407052993774 + 10.0 * 8.368050575256348
Epoch 1320, val loss: 2.1718223094940186
Epoch 1330, training loss: 85.84857940673828 = 2.1729955673217773 + 10.0 * 8.367558479309082
Epoch 1330, val loss: 2.1700549125671387
Epoch 1340, training loss: 85.84104919433594 = 2.1712732315063477 + 10.0 * 8.36697769165039
Epoch 1340, val loss: 2.168494462966919
Epoch 1350, training loss: 85.839111328125 = 2.169736862182617 + 10.0 * 8.366937637329102
Epoch 1350, val loss: 2.167086124420166
Epoch 1360, training loss: 85.84419250488281 = 2.1683199405670166 + 10.0 * 8.367587089538574
Epoch 1360, val loss: 2.165799617767334
Epoch 1370, training loss: 85.82420349121094 = 2.167007327079773 + 10.0 * 8.36571979522705
Epoch 1370, val loss: 2.164607048034668
Epoch 1380, training loss: 85.83399963378906 = 2.1657780408859253 + 10.0 * 8.366822242736816
Epoch 1380, val loss: 2.1635007858276367
Epoch 1390, training loss: 85.81641387939453 = 2.164653182029724 + 10.0 * 8.3651762008667
Epoch 1390, val loss: 2.162437677383423
Epoch 1400, training loss: 85.80795288085938 = 2.1635838747024536 + 10.0 * 8.364437103271484
Epoch 1400, val loss: 2.161444664001465
Epoch 1410, training loss: 85.81344604492188 = 2.1625559329986572 + 10.0 * 8.36508846282959
Epoch 1410, val loss: 2.1605021953582764
Epoch 1420, training loss: 85.81063842773438 = 2.1615697145462036 + 10.0 * 8.364907264709473
Epoch 1420, val loss: 2.1595892906188965
Epoch 1430, training loss: 85.80413055419922 = 2.1606321334838867 + 10.0 * 8.364350318908691
Epoch 1430, val loss: 2.1587188243865967
Epoch 1440, training loss: 85.7918930053711 = 2.1597187519073486 + 10.0 * 8.3632173538208
Epoch 1440, val loss: 2.1578831672668457
Epoch 1450, training loss: 85.78793334960938 = 2.1588308811187744 + 10.0 * 8.362910270690918
Epoch 1450, val loss: 2.1570863723754883
Epoch 1460, training loss: 85.7936782836914 = 2.1579957008361816 + 10.0 * 8.363568305969238
Epoch 1460, val loss: 2.1563000679016113
Epoch 1470, training loss: 85.78030395507812 = 2.157192587852478 + 10.0 * 8.362310409545898
Epoch 1470, val loss: 2.155522346496582
Epoch 1480, training loss: 85.78802490234375 = 2.1564186811447144 + 10.0 * 8.363161087036133
Epoch 1480, val loss: 2.1547775268554688
Epoch 1490, training loss: 85.77351379394531 = 2.155616521835327 + 10.0 * 8.36178970336914
Epoch 1490, val loss: 2.1540753841400146
Epoch 1500, training loss: 85.76190948486328 = 2.1548874378204346 + 10.0 * 8.360702514648438
Epoch 1500, val loss: 2.153367519378662
Epoch 1510, training loss: 85.76544189453125 = 2.154187321662903 + 10.0 * 8.361124992370605
Epoch 1510, val loss: 2.152681350708008
Epoch 1520, training loss: 85.77066802978516 = 2.1534464359283447 + 10.0 * 8.361721992492676
Epoch 1520, val loss: 2.1520261764526367
Epoch 1530, training loss: 85.75714874267578 = 2.152735471725464 + 10.0 * 8.360441207885742
Epoch 1530, val loss: 2.151366710662842
Epoch 1540, training loss: 85.7488021850586 = 2.152074694633484 + 10.0 * 8.359672546386719
Epoch 1540, val loss: 2.150726795196533
Epoch 1550, training loss: 85.74940490722656 = 2.1514008045196533 + 10.0 * 8.359800338745117
Epoch 1550, val loss: 2.1501107215881348
Epoch 1560, training loss: 85.75131225585938 = 2.1507792472839355 + 10.0 * 8.360053062438965
Epoch 1560, val loss: 2.149470806121826
Epoch 1570, training loss: 85.73567962646484 = 2.1501084566116333 + 10.0 * 8.358556747436523
Epoch 1570, val loss: 2.148890733718872
Epoch 1580, training loss: 85.72833251953125 = 2.149493932723999 + 10.0 * 8.357884407043457
Epoch 1580, val loss: 2.148293972015381
Epoch 1590, training loss: 85.73838806152344 = 2.1488689184188843 + 10.0 * 8.358951568603516
Epoch 1590, val loss: 2.1477341651916504
Epoch 1600, training loss: 85.72969818115234 = 2.148245692253113 + 10.0 * 8.358144760131836
Epoch 1600, val loss: 2.14711856842041
Epoch 1610, training loss: 85.72233581542969 = 2.147631883621216 + 10.0 * 8.357470512390137
Epoch 1610, val loss: 2.1465699672698975
Epoch 1620, training loss: 85.7191162109375 = 2.1470972299575806 + 10.0 * 8.357202529907227
Epoch 1620, val loss: 2.1460037231445312
Epoch 1630, training loss: 85.71009826660156 = 2.146503448486328 + 10.0 * 8.356359481811523
Epoch 1630, val loss: 2.145481824874878
Epoch 1640, training loss: 85.70545959472656 = 2.14595890045166 + 10.0 * 8.355950355529785
Epoch 1640, val loss: 2.14495587348938
Epoch 1650, training loss: 85.7090835571289 = 2.145392894744873 + 10.0 * 8.356369018554688
Epoch 1650, val loss: 2.144446849822998
Epoch 1660, training loss: 85.71378326416016 = 2.1448185443878174 + 10.0 * 8.35689640045166
Epoch 1660, val loss: 2.1438980102539062
Epoch 1670, training loss: 85.70828247070312 = 2.144289016723633 + 10.0 * 8.356399536132812
Epoch 1670, val loss: 2.143376111984253
Epoch 1680, training loss: 85.69170379638672 = 2.1437665224075317 + 10.0 * 8.354793548583984
Epoch 1680, val loss: 2.1428730487823486
Epoch 1690, training loss: 85.68915557861328 = 2.1432554721832275 + 10.0 * 8.354589462280273
Epoch 1690, val loss: 2.142383337020874
Epoch 1700, training loss: 85.68614959716797 = 2.142746686935425 + 10.0 * 8.354340553283691
Epoch 1700, val loss: 2.141904354095459
Epoch 1710, training loss: 85.75556945800781 = 2.142240047454834 + 10.0 * 8.361332893371582
Epoch 1710, val loss: 2.1414194107055664
Epoch 1720, training loss: 85.69263458251953 = 2.1416996717453003 + 10.0 * 8.355093002319336
Epoch 1720, val loss: 2.140901565551758
Epoch 1730, training loss: 85.67491912841797 = 2.1412168741226196 + 10.0 * 8.35336971282959
Epoch 1730, val loss: 2.1404290199279785
Epoch 1740, training loss: 85.67298889160156 = 2.14072048664093 + 10.0 * 8.353226661682129
Epoch 1740, val loss: 2.1399850845336914
Epoch 1750, training loss: 85.66752624511719 = 2.140261173248291 + 10.0 * 8.352726936340332
Epoch 1750, val loss: 2.1395227909088135
Epoch 1760, training loss: 85.67101287841797 = 2.1397955417633057 + 10.0 * 8.353121757507324
Epoch 1760, val loss: 2.139068365097046
Epoch 1770, training loss: 85.694580078125 = 2.1393195390701294 + 10.0 * 8.355525970458984
Epoch 1770, val loss: 2.1386003494262695
Epoch 1780, training loss: 85.66842651367188 = 2.1387932300567627 + 10.0 * 8.3529634475708
Epoch 1780, val loss: 2.1381609439849854
Epoch 1790, training loss: 85.66304779052734 = 2.138345956802368 + 10.0 * 8.352470397949219
Epoch 1790, val loss: 2.1377205848693848
Epoch 1800, training loss: 85.6561050415039 = 2.137885808944702 + 10.0 * 8.351821899414062
Epoch 1800, val loss: 2.13728666305542
Epoch 1810, training loss: 85.64845275878906 = 2.1374666690826416 + 10.0 * 8.351099014282227
Epoch 1810, val loss: 2.1368515491485596
Epoch 1820, training loss: 85.6499252319336 = 2.137038826942444 + 10.0 * 8.351288795471191
Epoch 1820, val loss: 2.1364245414733887
Epoch 1830, training loss: 85.65403747558594 = 2.1365954875946045 + 10.0 * 8.351743698120117
Epoch 1830, val loss: 2.1360013484954834
Epoch 1840, training loss: 85.66572570800781 = 2.136124014854431 + 10.0 * 8.352960586547852
Epoch 1840, val loss: 2.1355905532836914
Epoch 1850, training loss: 85.63627624511719 = 2.135690689086914 + 10.0 * 8.350058555603027
Epoch 1850, val loss: 2.135159969329834
Epoch 1860, training loss: 85.63168334960938 = 2.1352574825286865 + 10.0 * 8.349642753601074
Epoch 1860, val loss: 2.134765148162842
Epoch 1870, training loss: 85.63109588623047 = 2.1348319053649902 + 10.0 * 8.349626541137695
Epoch 1870, val loss: 2.134369134902954
Epoch 1880, training loss: 85.657470703125 = 2.1343653202056885 + 10.0 * 8.352310180664062
Epoch 1880, val loss: 2.1339879035949707
Epoch 1890, training loss: 85.63211059570312 = 2.134017586708069 + 10.0 * 8.349809646606445
Epoch 1890, val loss: 2.133526086807251
Epoch 1900, training loss: 85.62310791015625 = 2.1335748434066772 + 10.0 * 8.348953247070312
Epoch 1900, val loss: 2.1331539154052734
Epoch 1910, training loss: 85.61774444580078 = 2.133199453353882 + 10.0 * 8.348454475402832
Epoch 1910, val loss: 2.132758617401123
Epoch 1920, training loss: 85.62596130371094 = 2.132784128189087 + 10.0 * 8.34931755065918
Epoch 1920, val loss: 2.1323866844177246
Epoch 1930, training loss: 85.62181091308594 = 2.132373571395874 + 10.0 * 8.348943710327148
Epoch 1930, val loss: 2.131971836090088
Epoch 1940, training loss: 85.61087799072266 = 2.1319583654403687 + 10.0 * 8.347891807556152
Epoch 1940, val loss: 2.1315956115722656
Epoch 1950, training loss: 85.61140441894531 = 2.1315724849700928 + 10.0 * 8.347983360290527
Epoch 1950, val loss: 2.1312131881713867
Epoch 1960, training loss: 85.62421417236328 = 2.131186366081238 + 10.0 * 8.349302291870117
Epoch 1960, val loss: 2.130828857421875
Epoch 1970, training loss: 85.6048355102539 = 2.1307746171951294 + 10.0 * 8.347406387329102
Epoch 1970, val loss: 2.130467414855957
Epoch 1980, training loss: 85.5957260131836 = 2.130412220954895 + 10.0 * 8.346531867980957
Epoch 1980, val loss: 2.1300954818725586
Epoch 1990, training loss: 85.59371185302734 = 2.1300463676452637 + 10.0 * 8.346366882324219
Epoch 1990, val loss: 2.1297385692596436
Epoch 2000, training loss: 85.59468078613281 = 2.1296746730804443 + 10.0 * 8.346500396728516
Epoch 2000, val loss: 2.1293811798095703
Epoch 2010, training loss: 85.6299057006836 = 2.129317879676819 + 10.0 * 8.350058555603027
Epoch 2010, val loss: 2.128998279571533
Epoch 2020, training loss: 85.60832214355469 = 2.128861904144287 + 10.0 * 8.347946166992188
Epoch 2020, val loss: 2.1286392211914062
Epoch 2030, training loss: 85.58694458007812 = 2.1285085678100586 + 10.0 * 8.345843315124512
Epoch 2030, val loss: 2.128281593322754
Epoch 2040, training loss: 85.580322265625 = 2.1281648874282837 + 10.0 * 8.345215797424316
Epoch 2040, val loss: 2.1279306411743164
Epoch 2050, training loss: 85.57881927490234 = 2.127814531326294 + 10.0 * 8.345100402832031
Epoch 2050, val loss: 2.127586841583252
Epoch 2060, training loss: 85.59829711914062 = 2.1274871826171875 + 10.0 * 8.347081184387207
Epoch 2060, val loss: 2.12722110748291
Epoch 2070, training loss: 85.58155059814453 = 2.127041816711426 + 10.0 * 8.345450401306152
Epoch 2070, val loss: 2.126899242401123
Epoch 2080, training loss: 85.56830596923828 = 2.126691460609436 + 10.0 * 8.344161033630371
Epoch 2080, val loss: 2.1265392303466797
Epoch 2090, training loss: 85.56787109375 = 2.1263710260391235 + 10.0 * 8.344149589538574
Epoch 2090, val loss: 2.126204013824463
Epoch 2100, training loss: 85.59400939941406 = 2.1260323524475098 + 10.0 * 8.346797943115234
Epoch 2100, val loss: 2.1258764266967773
Epoch 2110, training loss: 85.5770492553711 = 2.125633955001831 + 10.0 * 8.345141410827637
Epoch 2110, val loss: 2.1255221366882324
Epoch 2120, training loss: 85.56389617919922 = 2.1252923011779785 + 10.0 * 8.343860626220703
Epoch 2120, val loss: 2.125185966491699
Epoch 2130, training loss: 85.55931091308594 = 2.124942660331726 + 10.0 * 8.343436241149902
Epoch 2130, val loss: 2.1248621940612793
Epoch 2140, training loss: 85.55845642089844 = 2.1246211528778076 + 10.0 * 8.3433837890625
Epoch 2140, val loss: 2.124547004699707
Epoch 2150, training loss: 85.57367706298828 = 2.124269723892212 + 10.0 * 8.344941139221191
Epoch 2150, val loss: 2.1242196559906006
Epoch 2160, training loss: 85.55203247070312 = 2.1239272356033325 + 10.0 * 8.34281063079834
Epoch 2160, val loss: 2.1238901615142822
Epoch 2170, training loss: 85.5566177368164 = 2.1235755681991577 + 10.0 * 8.343304634094238
Epoch 2170, val loss: 2.1235849857330322
Epoch 2180, training loss: 85.56411743164062 = 2.12321937084198 + 10.0 * 8.34408950805664
Epoch 2180, val loss: 2.1232683658599854
Epoch 2190, training loss: 85.54396057128906 = 2.1229277849197388 + 10.0 * 8.342103958129883
Epoch 2190, val loss: 2.122924327850342
Epoch 2200, training loss: 85.54049682617188 = 2.1226130723953247 + 10.0 * 8.341788291931152
Epoch 2200, val loss: 2.122615337371826
Epoch 2210, training loss: 85.54473114013672 = 2.1222898960113525 + 10.0 * 8.342244148254395
Epoch 2210, val loss: 2.122310161590576
Epoch 2220, training loss: 85.56314849853516 = 2.121952533721924 + 10.0 * 8.34411907196045
Epoch 2220, val loss: 2.1219959259033203
Epoch 2230, training loss: 85.54195404052734 = 2.1215893030166626 + 10.0 * 8.342036247253418
Epoch 2230, val loss: 2.121685028076172
Epoch 2240, training loss: 85.53044891357422 = 2.1213154792785645 + 10.0 * 8.340913772583008
Epoch 2240, val loss: 2.1213626861572266
Epoch 2250, training loss: 85.5280532836914 = 2.1210083961486816 + 10.0 * 8.340703964233398
Epoch 2250, val loss: 2.121067762374878
Epoch 2260, training loss: 85.5334701538086 = 2.1207181215286255 + 10.0 * 8.341275215148926
Epoch 2260, val loss: 2.1207618713378906
Epoch 2270, training loss: 85.55364990234375 = 2.120407223701477 + 10.0 * 8.343324661254883
Epoch 2270, val loss: 2.1204423904418945
Epoch 2280, training loss: 85.54601287841797 = 2.119967222213745 + 10.0 * 8.342604637145996
Epoch 2280, val loss: 2.1201722621917725
Epoch 2290, training loss: 85.52581024169922 = 2.1197292804718018 + 10.0 * 8.340608596801758
Epoch 2290, val loss: 2.11983060836792
Epoch 2300, training loss: 85.52019500732422 = 2.1194040775299072 + 10.0 * 8.340078353881836
Epoch 2300, val loss: 2.1195578575134277
Epoch 2310, training loss: 85.52888488769531 = 2.119097590446472 + 10.0 * 8.340978622436523
Epoch 2310, val loss: 2.1192877292633057
Epoch 2320, training loss: 85.51439666748047 = 2.1187973022460938 + 10.0 * 8.339559555053711
Epoch 2320, val loss: 2.1189730167388916
Epoch 2330, training loss: 85.51847076416016 = 2.118492603302002 + 10.0 * 8.339998245239258
Epoch 2330, val loss: 2.118687152862549
Epoch 2340, training loss: 85.52364349365234 = 2.1181763410568237 + 10.0 * 8.340546607971191
Epoch 2340, val loss: 2.1184144020080566
Epoch 2350, training loss: 85.50773620605469 = 2.1178873777389526 + 10.0 * 8.338984489440918
Epoch 2350, val loss: 2.1181092262268066
Epoch 2360, training loss: 85.5047378540039 = 2.117605447769165 + 10.0 * 8.338712692260742
Epoch 2360, val loss: 2.117830276489258
Epoch 2370, training loss: 85.50568389892578 = 2.1173300743103027 + 10.0 * 8.338834762573242
Epoch 2370, val loss: 2.117556571960449
Epoch 2380, training loss: 85.53691864013672 = 2.1170506477355957 + 10.0 * 8.341986656188965
Epoch 2380, val loss: 2.1172664165496826
Epoch 2390, training loss: 85.50761413574219 = 2.1166869401931763 + 10.0 * 8.339093208312988
Epoch 2390, val loss: 2.116964817047119
Epoch 2400, training loss: 85.50003051757812 = 2.1164097785949707 + 10.0 * 8.338361740112305
Epoch 2400, val loss: 2.1166861057281494
Epoch 2410, training loss: 85.49246215820312 = 2.1161136627197266 + 10.0 * 8.337635040283203
Epoch 2410, val loss: 2.1164257526397705
Epoch 2420, training loss: 85.49172973632812 = 2.1158387660980225 + 10.0 * 8.337589263916016
Epoch 2420, val loss: 2.11616587638855
Epoch 2430, training loss: 85.50029754638672 = 2.1155784130096436 + 10.0 * 8.338472366333008
Epoch 2430, val loss: 2.1158971786499023
Epoch 2440, training loss: 85.50434112548828 = 2.115288257598877 + 10.0 * 8.338905334472656
Epoch 2440, val loss: 2.115597724914551
Epoch 2450, training loss: 85.49264526367188 = 2.1149775981903076 + 10.0 * 8.337766647338867
Epoch 2450, val loss: 2.1153528690338135
Epoch 2460, training loss: 85.48617553710938 = 2.1147350072860718 + 10.0 * 8.337143898010254
Epoch 2460, val loss: 2.11507511138916
Epoch 2470, training loss: 85.50283813476562 = 2.1144587993621826 + 10.0 * 8.338838577270508
Epoch 2470, val loss: 2.114806890487671
Epoch 2480, training loss: 85.490234375 = 2.1141027212142944 + 10.0 * 8.337613105773926
Epoch 2480, val loss: 2.114561080932617
Epoch 2490, training loss: 85.49897766113281 = 2.113842248916626 + 10.0 * 8.338513374328613
Epoch 2490, val loss: 2.1142659187316895
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.39666666666666667
0.865391581540245
=== training gcn model ===
Epoch 0, training loss: 108.02142333984375 = 2.1986026763916016 + 10.0 * 10.582282066345215
Epoch 0, val loss: 2.198096752166748
Epoch 10, training loss: 108.00463104248047 = 2.184081196784973 + 10.0 * 10.58205509185791
Epoch 10, val loss: 2.1855251789093018
Epoch 20, training loss: 107.99382019042969 = 2.179656147956848 + 10.0 * 10.581416130065918
Epoch 20, val loss: 2.1818246841430664
Epoch 30, training loss: 107.96914672851562 = 2.175681233406067 + 10.0 * 10.579346656799316
Epoch 30, val loss: 2.178691864013672
Epoch 40, training loss: 107.89077758789062 = 2.173142194747925 + 10.0 * 10.57176399230957
Epoch 40, val loss: 2.1767678260803223
Epoch 50, training loss: 107.62911224365234 = 2.1720250844955444 + 10.0 * 10.545708656311035
Epoch 50, val loss: 2.175841808319092
Epoch 60, training loss: 106.96246337890625 = 2.1730995178222656 + 10.0 * 10.478936195373535
Epoch 60, val loss: 2.176969051361084
Epoch 70, training loss: 105.51434326171875 = 2.1757315397262573 + 10.0 * 10.333860397338867
Epoch 70, val loss: 2.179260730743408
Epoch 80, training loss: 102.75486755371094 = 2.1803265810012817 + 10.0 * 10.057454109191895
Epoch 80, val loss: 2.1835596561431885
Epoch 90, training loss: 98.30650329589844 = 2.1861878633499146 + 10.0 * 9.612031936645508
Epoch 90, val loss: 2.1887550354003906
Epoch 100, training loss: 95.90567779541016 = 2.1816002130508423 + 10.0 * 9.372407913208008
Epoch 100, val loss: 2.1832029819488525
Epoch 110, training loss: 94.9410400390625 = 2.1705411672592163 + 10.0 * 9.277050018310547
Epoch 110, val loss: 2.1720786094665527
Epoch 120, training loss: 94.53041076660156 = 2.1604913473129272 + 10.0 * 9.236991882324219
Epoch 120, val loss: 2.1624011993408203
Epoch 130, training loss: 94.19525909423828 = 2.1540967226028442 + 10.0 * 9.204115867614746
Epoch 130, val loss: 2.156261682510376
Epoch 140, training loss: 93.74794006347656 = 2.149970054626465 + 10.0 * 9.159796714782715
Epoch 140, val loss: 2.1522560119628906
Epoch 150, training loss: 93.2367935180664 = 2.1467233896255493 + 10.0 * 9.109006881713867
Epoch 150, val loss: 2.1490793228149414
Epoch 160, training loss: 92.71446228027344 = 2.1433932781219482 + 10.0 * 9.057106971740723
Epoch 160, val loss: 2.1458468437194824
Epoch 170, training loss: 92.23866271972656 = 2.140947699546814 + 10.0 * 9.009771347045898
Epoch 170, val loss: 2.1434946060180664
Epoch 180, training loss: 91.87215423583984 = 2.1399441957473755 + 10.0 * 8.973220825195312
Epoch 180, val loss: 2.1425342559814453
Epoch 190, training loss: 91.53594970703125 = 2.140047311782837 + 10.0 * 8.939590454101562
Epoch 190, val loss: 2.142585039138794
Epoch 200, training loss: 91.12818145751953 = 2.14063036441803 + 10.0 * 8.898755073547363
Epoch 200, val loss: 2.143125057220459
Epoch 210, training loss: 90.791748046875 = 2.141437530517578 + 10.0 * 8.865031242370605
Epoch 210, val loss: 2.14387583732605
Epoch 220, training loss: 90.65320587158203 = 2.141087532043457 + 10.0 * 8.851211547851562
Epoch 220, val loss: 2.143448829650879
Epoch 230, training loss: 90.57428741455078 = 2.1388550996780396 + 10.0 * 8.84354305267334
Epoch 230, val loss: 2.141176223754883
Epoch 240, training loss: 90.50588989257812 = 2.136070966720581 + 10.0 * 8.836981773376465
Epoch 240, val loss: 2.1384122371673584
Epoch 250, training loss: 90.44003295898438 = 2.134067177772522 + 10.0 * 8.830596923828125
Epoch 250, val loss: 2.136444091796875
Epoch 260, training loss: 90.36431121826172 = 2.1328691244125366 + 10.0 * 8.823144912719727
Epoch 260, val loss: 2.135265350341797
Epoch 270, training loss: 90.26878356933594 = 2.13203763961792 + 10.0 * 8.813674926757812
Epoch 270, val loss: 2.13444447517395
Epoch 280, training loss: 90.1345443725586 = 2.131625533103943 + 10.0 * 8.800292015075684
Epoch 280, val loss: 2.1340408325195312
Epoch 290, training loss: 89.9410171508789 = 2.1319427490234375 + 10.0 * 8.78090763092041
Epoch 290, val loss: 2.1343648433685303
Epoch 300, training loss: 89.70191192626953 = 2.133076548576355 + 10.0 * 8.75688362121582
Epoch 300, val loss: 2.1354925632476807
Epoch 310, training loss: 89.51425170898438 = 2.1344528198242188 + 10.0 * 8.737979888916016
Epoch 310, val loss: 2.1368281841278076
Epoch 320, training loss: 89.3982925415039 = 2.1349923610687256 + 10.0 * 8.726329803466797
Epoch 320, val loss: 2.1372854709625244
Epoch 330, training loss: 89.26506042480469 = 2.1348507404327393 + 10.0 * 8.713021278381348
Epoch 330, val loss: 2.137075424194336
Epoch 340, training loss: 89.10399627685547 = 2.135249137878418 + 10.0 * 8.696874618530273
Epoch 340, val loss: 2.1373655796051025
Epoch 350, training loss: 88.95806121826172 = 2.1362677812576294 + 10.0 * 8.68217945098877
Epoch 350, val loss: 2.1382410526275635
Epoch 360, training loss: 88.87632751464844 = 2.136413335800171 + 10.0 * 8.673991203308105
Epoch 360, val loss: 2.138352155685425
Epoch 370, training loss: 88.82759857177734 = 2.135566830635071 + 10.0 * 8.66920280456543
Epoch 370, val loss: 2.137507677078247
Epoch 380, training loss: 88.77757263183594 = 2.134358048439026 + 10.0 * 8.664320945739746
Epoch 380, val loss: 2.1363558769226074
Epoch 390, training loss: 88.72331237792969 = 2.133515238761902 + 10.0 * 8.658979415893555
Epoch 390, val loss: 2.1355738639831543
Epoch 400, training loss: 88.65629577636719 = 2.1331138610839844 + 10.0 * 8.652318000793457
Epoch 400, val loss: 2.1352128982543945
Epoch 410, training loss: 88.56907653808594 = 2.133028745651245 + 10.0 * 8.64360523223877
Epoch 410, val loss: 2.135162353515625
Epoch 420, training loss: 88.4576187133789 = 2.133246898651123 + 10.0 * 8.632436752319336
Epoch 420, val loss: 2.135436773300171
Epoch 430, training loss: 88.3469009399414 = 2.1336816549301147 + 10.0 * 8.621321678161621
Epoch 430, val loss: 2.135885715484619
Epoch 440, training loss: 88.24604797363281 = 2.134187936782837 + 10.0 * 8.611186027526855
Epoch 440, val loss: 2.1363813877105713
Epoch 450, training loss: 88.17115783691406 = 2.1344146728515625 + 10.0 * 8.603673934936523
Epoch 450, val loss: 2.1366066932678223
Epoch 460, training loss: 88.10202026367188 = 2.1345468759536743 + 10.0 * 8.596747398376465
Epoch 460, val loss: 2.136732578277588
Epoch 470, training loss: 88.0219955444336 = 2.1347676515579224 + 10.0 * 8.588723182678223
Epoch 470, val loss: 2.136975049972534
Epoch 480, training loss: 87.94009399414062 = 2.135223627090454 + 10.0 * 8.580487251281738
Epoch 480, val loss: 2.1374423503875732
Epoch 490, training loss: 87.85997009277344 = 2.135629892349243 + 10.0 * 8.572434425354004
Epoch 490, val loss: 2.1378493309020996
Epoch 500, training loss: 87.79863739013672 = 2.135870933532715 + 10.0 * 8.566276550292969
Epoch 500, val loss: 2.1380844116210938
Epoch 510, training loss: 87.7443618774414 = 2.1358866691589355 + 10.0 * 8.560847282409668
Epoch 510, val loss: 2.1380677223205566
Epoch 520, training loss: 87.69200134277344 = 2.135769248008728 + 10.0 * 8.555623054504395
Epoch 520, val loss: 2.1379308700561523
Epoch 530, training loss: 87.63971710205078 = 2.1356879472732544 + 10.0 * 8.550402641296387
Epoch 530, val loss: 2.137848138809204
Epoch 540, training loss: 87.57868194580078 = 2.1356500387191772 + 10.0 * 8.544302940368652
Epoch 540, val loss: 2.1378095149993896
Epoch 550, training loss: 87.5198745727539 = 2.135703682899475 + 10.0 * 8.538416862487793
Epoch 550, val loss: 2.137852191925049
Epoch 560, training loss: 87.46513366699219 = 2.135804057121277 + 10.0 * 8.532933235168457
Epoch 560, val loss: 2.1379356384277344
Epoch 570, training loss: 87.42082977294922 = 2.1358377933502197 + 10.0 * 8.528498649597168
Epoch 570, val loss: 2.1379384994506836
Epoch 580, training loss: 87.365234375 = 2.135861873626709 + 10.0 * 8.522936820983887
Epoch 580, val loss: 2.1379663944244385
Epoch 590, training loss: 87.31941223144531 = 2.1358951330184937 + 10.0 * 8.518351554870605
Epoch 590, val loss: 2.137969732284546
Epoch 600, training loss: 87.27410125732422 = 2.1358925104141235 + 10.0 * 8.51382064819336
Epoch 600, val loss: 2.13796329498291
Epoch 610, training loss: 87.23280334472656 = 2.1358988285064697 + 10.0 * 8.509690284729004
Epoch 610, val loss: 2.137928009033203
Epoch 620, training loss: 87.19807434082031 = 2.135781407356262 + 10.0 * 8.506229400634766
Epoch 620, val loss: 2.1378231048583984
Epoch 630, training loss: 87.16276550292969 = 2.1356457471847534 + 10.0 * 8.50271224975586
Epoch 630, val loss: 2.1376638412475586
Epoch 640, training loss: 87.1346664428711 = 2.1354469060897827 + 10.0 * 8.499921798706055
Epoch 640, val loss: 2.1374402046203613
Epoch 650, training loss: 87.12025451660156 = 2.1352145671844482 + 10.0 * 8.498503684997559
Epoch 650, val loss: 2.137183427810669
Epoch 660, training loss: 87.09074401855469 = 2.134873151779175 + 10.0 * 8.495587348937988
Epoch 660, val loss: 2.1368370056152344
Epoch 670, training loss: 87.06076049804688 = 2.1346479654312134 + 10.0 * 8.492610931396484
Epoch 670, val loss: 2.136601686477661
Epoch 680, training loss: 87.02597045898438 = 2.134425163269043 + 10.0 * 8.489154815673828
Epoch 680, val loss: 2.1364002227783203
Epoch 690, training loss: 86.99404907226562 = 2.1342722177505493 + 10.0 * 8.485978126525879
Epoch 690, val loss: 2.1362476348876953
Epoch 700, training loss: 86.95854949951172 = 2.1341806650161743 + 10.0 * 8.482437133789062
Epoch 700, val loss: 2.136148452758789
Epoch 710, training loss: 86.91971588134766 = 2.1341421604156494 + 10.0 * 8.478557586669922
Epoch 710, val loss: 2.136094570159912
Epoch 720, training loss: 86.9433822631836 = 2.1341958045959473 + 10.0 * 8.480918884277344
Epoch 720, val loss: 2.1360630989074707
Epoch 730, training loss: 86.84375762939453 = 2.1340525150299072 + 10.0 * 8.470970153808594
Epoch 730, val loss: 2.1359925270080566
Epoch 740, training loss: 86.80216979980469 = 2.1340208053588867 + 10.0 * 8.466814994812012
Epoch 740, val loss: 2.1359376907348633
Epoch 750, training loss: 86.76243591308594 = 2.1340644359588623 + 10.0 * 8.462837219238281
Epoch 750, val loss: 2.135958194732666
Epoch 760, training loss: 86.72440338134766 = 2.134055733680725 + 10.0 * 8.45903491973877
Epoch 760, val loss: 2.135958194732666
Epoch 770, training loss: 86.74226379394531 = 2.1340492963790894 + 10.0 * 8.460821151733398
Epoch 770, val loss: 2.1359171867370605
Epoch 780, training loss: 86.67249298095703 = 2.1339040994644165 + 10.0 * 8.453859329223633
Epoch 780, val loss: 2.135770797729492
Epoch 790, training loss: 86.63797760009766 = 2.1337947845458984 + 10.0 * 8.450418472290039
Epoch 790, val loss: 2.1356544494628906
Epoch 800, training loss: 86.61090087890625 = 2.1336848735809326 + 10.0 * 8.447721481323242
Epoch 800, val loss: 2.1355321407318115
Epoch 810, training loss: 86.58531188964844 = 2.1335458755493164 + 10.0 * 8.44517707824707
Epoch 810, val loss: 2.135395050048828
Epoch 820, training loss: 86.56109619140625 = 2.133395791053772 + 10.0 * 8.442770004272461
Epoch 820, val loss: 2.1352462768554688
Epoch 830, training loss: 86.53742218017578 = 2.1332411766052246 + 10.0 * 8.440418243408203
Epoch 830, val loss: 2.1350865364074707
Epoch 840, training loss: 86.51534271240234 = 2.1330621242523193 + 10.0 * 8.438227653503418
Epoch 840, val loss: 2.1349124908447266
Epoch 850, training loss: 86.55156707763672 = 2.1328426599502563 + 10.0 * 8.441872596740723
Epoch 850, val loss: 2.1346895694732666
Epoch 860, training loss: 86.49285888671875 = 2.1325687170028687 + 10.0 * 8.436029434204102
Epoch 860, val loss: 2.134427547454834
Epoch 870, training loss: 86.46453857421875 = 2.132351040840149 + 10.0 * 8.433218002319336
Epoch 870, val loss: 2.1342082023620605
Epoch 880, training loss: 86.4444351196289 = 2.1321187019348145 + 10.0 * 8.431231498718262
Epoch 880, val loss: 2.1339809894561768
Epoch 890, training loss: 86.42852020263672 = 2.1318771839141846 + 10.0 * 8.429664611816406
Epoch 890, val loss: 2.1337437629699707
Epoch 900, training loss: 86.41380310058594 = 2.1316250562667847 + 10.0 * 8.428217887878418
Epoch 900, val loss: 2.1334989070892334
Epoch 910, training loss: 86.40054321289062 = 2.1313596963882446 + 10.0 * 8.426918029785156
Epoch 910, val loss: 2.133246898651123
Epoch 920, training loss: 86.39849090576172 = 2.1310638189315796 + 10.0 * 8.426742553710938
Epoch 920, val loss: 2.1329498291015625
Epoch 930, training loss: 86.3766098022461 = 2.130743384361267 + 10.0 * 8.424586296081543
Epoch 930, val loss: 2.132645845413208
Epoch 940, training loss: 86.36318969726562 = 2.130489468574524 + 10.0 * 8.423269271850586
Epoch 940, val loss: 2.132385730743408
Epoch 950, training loss: 86.34996795654297 = 2.1302380561828613 + 10.0 * 8.421972274780273
Epoch 950, val loss: 2.1321401596069336
Epoch 960, training loss: 86.33621215820312 = 2.129987120628357 + 10.0 * 8.420621871948242
Epoch 960, val loss: 2.131901741027832
Epoch 970, training loss: 86.33013153076172 = 2.1297287940979004 + 10.0 * 8.420040130615234
Epoch 970, val loss: 2.1316683292388916
Epoch 980, training loss: 86.31783294677734 = 2.129488229751587 + 10.0 * 8.418834686279297
Epoch 980, val loss: 2.131394386291504
Epoch 990, training loss: 86.29725646972656 = 2.129232406616211 + 10.0 * 8.416802406311035
Epoch 990, val loss: 2.131165027618408
Epoch 1000, training loss: 86.28021240234375 = 2.129030466079712 + 10.0 * 8.415118217468262
Epoch 1000, val loss: 2.130970001220703
Epoch 1010, training loss: 86.26506042480469 = 2.1288281679153442 + 10.0 * 8.413622856140137
Epoch 1010, val loss: 2.130784511566162
Epoch 1020, training loss: 86.25226593017578 = 2.1286362409591675 + 10.0 * 8.412363052368164
Epoch 1020, val loss: 2.130610466003418
Epoch 1030, training loss: 86.24254608154297 = 2.1284444332122803 + 10.0 * 8.411410331726074
Epoch 1030, val loss: 2.1304166316986084
Epoch 1040, training loss: 86.22535705566406 = 2.128228783607483 + 10.0 * 8.409712791442871
Epoch 1040, val loss: 2.1302170753479004
Epoch 1050, training loss: 86.20693969726562 = 2.128078579902649 + 10.0 * 8.407885551452637
Epoch 1050, val loss: 2.130049705505371
Epoch 1060, training loss: 86.19351196289062 = 2.1279025077819824 + 10.0 * 8.406560897827148
Epoch 1060, val loss: 2.1298763751983643
Epoch 1070, training loss: 86.17942810058594 = 2.1277133226394653 + 10.0 * 8.405171394348145
Epoch 1070, val loss: 2.1296935081481934
Epoch 1080, training loss: 86.17510986328125 = 2.127519726753235 + 10.0 * 8.404759407043457
Epoch 1080, val loss: 2.129502773284912
Epoch 1090, training loss: 86.16648864746094 = 2.1272647380828857 + 10.0 * 8.403922080993652
Epoch 1090, val loss: 2.1292614936828613
Epoch 1100, training loss: 86.14533233642578 = 2.127038836479187 + 10.0 * 8.401829719543457
Epoch 1100, val loss: 2.129019021987915
Epoch 1110, training loss: 86.13414764404297 = 2.126798391342163 + 10.0 * 8.400734901428223
Epoch 1110, val loss: 2.128791332244873
Epoch 1120, training loss: 86.1402359008789 = 2.126557230949402 + 10.0 * 8.401368141174316
Epoch 1120, val loss: 2.12855863571167
Epoch 1130, training loss: 86.119384765625 = 2.126279354095459 + 10.0 * 8.399310111999512
Epoch 1130, val loss: 2.1282958984375
Epoch 1140, training loss: 86.1068115234375 = 2.1260379552841187 + 10.0 * 8.398077011108398
Epoch 1140, val loss: 2.128058910369873
Epoch 1150, training loss: 86.09066009521484 = 2.125809669494629 + 10.0 * 8.396485328674316
Epoch 1150, val loss: 2.127830982208252
Epoch 1160, training loss: 86.07911682128906 = 2.1255775690078735 + 10.0 * 8.395353317260742
Epoch 1160, val loss: 2.1276049613952637
Epoch 1170, training loss: 86.068115234375 = 2.125356435775757 + 10.0 * 8.394275665283203
Epoch 1170, val loss: 2.127392292022705
Epoch 1180, training loss: 86.0571517944336 = 2.1251436471939087 + 10.0 * 8.393200874328613
Epoch 1180, val loss: 2.1271779537200928
Epoch 1190, training loss: 86.0848617553711 = 2.124946713447571 + 10.0 * 8.395991325378418
Epoch 1190, val loss: 2.126960277557373
Epoch 1200, training loss: 86.03504180908203 = 2.124641537666321 + 10.0 * 8.391039848327637
Epoch 1200, val loss: 2.1266837120056152
Epoch 1210, training loss: 86.0244140625 = 2.1244308948516846 + 10.0 * 8.389998435974121
Epoch 1210, val loss: 2.1264870166778564
Epoch 1220, training loss: 86.01390838623047 = 2.124224901199341 + 10.0 * 8.388968467712402
Epoch 1220, val loss: 2.126291275024414
Epoch 1230, training loss: 86.02198791503906 = 2.1240257024765015 + 10.0 * 8.389796257019043
Epoch 1230, val loss: 2.1260838508605957
Epoch 1240, training loss: 85.99463653564453 = 2.1237869262695312 + 10.0 * 8.3870849609375
Epoch 1240, val loss: 2.1258673667907715
Epoch 1250, training loss: 85.97884368896484 = 2.12358820438385 + 10.0 * 8.385525703430176
Epoch 1250, val loss: 2.1256752014160156
Epoch 1260, training loss: 85.96775817871094 = 2.12339448928833 + 10.0 * 8.384435653686523
Epoch 1260, val loss: 2.1254801750183105
Epoch 1270, training loss: 85.95695495605469 = 2.12319278717041 + 10.0 * 8.383376121520996
Epoch 1270, val loss: 2.1252894401550293
Epoch 1280, training loss: 85.9709701538086 = 2.1229721307754517 + 10.0 * 8.38479995727539
Epoch 1280, val loss: 2.125082015991211
Epoch 1290, training loss: 85.95092010498047 = 2.12274968624115 + 10.0 * 8.382817268371582
Epoch 1290, val loss: 2.1248412132263184
Epoch 1300, training loss: 85.93525695800781 = 2.122495174407959 + 10.0 * 8.38127613067627
Epoch 1300, val loss: 2.1246047019958496
Epoch 1310, training loss: 85.92105102539062 = 2.1222602128982544 + 10.0 * 8.379878997802734
Epoch 1310, val loss: 2.124384880065918
Epoch 1320, training loss: 85.9134750366211 = 2.122034192085266 + 10.0 * 8.379144668579102
Epoch 1320, val loss: 2.124159812927246
Epoch 1330, training loss: 85.90557861328125 = 2.121790289878845 + 10.0 * 8.378378868103027
Epoch 1330, val loss: 2.1239242553710938
Epoch 1340, training loss: 85.90953063964844 = 2.12153697013855 + 10.0 * 8.378799438476562
Epoch 1340, val loss: 2.1236796379089355
Epoch 1350, training loss: 85.88716888427734 = 2.1212680339813232 + 10.0 * 8.37658977508545
Epoch 1350, val loss: 2.123411178588867
Epoch 1360, training loss: 85.88099670410156 = 2.121009349822998 + 10.0 * 8.375998497009277
Epoch 1360, val loss: 2.1231613159179688
Epoch 1370, training loss: 85.8725814819336 = 2.120754837989807 + 10.0 * 8.375182151794434
Epoch 1370, val loss: 2.1229147911071777
Epoch 1380, training loss: 85.87146759033203 = 2.12049400806427 + 10.0 * 8.375097274780273
Epoch 1380, val loss: 2.1226601600646973
Epoch 1390, training loss: 85.8641357421875 = 2.1202118396759033 + 10.0 * 8.37439250946045
Epoch 1390, val loss: 2.122385025024414
Epoch 1400, training loss: 85.86441040039062 = 2.1199305057525635 + 10.0 * 8.3744478225708
Epoch 1400, val loss: 2.1220977306365967
Epoch 1410, training loss: 85.84730529785156 = 2.119626522064209 + 10.0 * 8.372767448425293
Epoch 1410, val loss: 2.1218202114105225
Epoch 1420, training loss: 85.84033203125 = 2.1193660497665405 + 10.0 * 8.372096061706543
Epoch 1420, val loss: 2.1215577125549316
Epoch 1430, training loss: 85.83313751220703 = 2.1190991401672363 + 10.0 * 8.371403694152832
Epoch 1430, val loss: 2.121293783187866
Epoch 1440, training loss: 85.8295669555664 = 2.1188220977783203 + 10.0 * 8.371074676513672
Epoch 1440, val loss: 2.1210286617279053
Epoch 1450, training loss: 85.84332275390625 = 2.1185277700424194 + 10.0 * 8.372479438781738
Epoch 1450, val loss: 2.120745897293091
Epoch 1460, training loss: 85.81990051269531 = 2.1182243824005127 + 10.0 * 8.37016773223877
Epoch 1460, val loss: 2.120443344116211
Epoch 1470, training loss: 85.80972290039062 = 2.117948889732361 + 10.0 * 8.369176864624023
Epoch 1470, val loss: 2.120171546936035
Epoch 1480, training loss: 85.80403137207031 = 2.117668867111206 + 10.0 * 8.368636131286621
Epoch 1480, val loss: 2.119905948638916
Epoch 1490, training loss: 85.8038330078125 = 2.117403268814087 + 10.0 * 8.368642807006836
Epoch 1490, val loss: 2.119629383087158
Epoch 1500, training loss: 85.79803466796875 = 2.117063522338867 + 10.0 * 8.368097305297852
Epoch 1500, val loss: 2.119340419769287
Epoch 1510, training loss: 85.79336547851562 = 2.116788387298584 + 10.0 * 8.367657661437988
Epoch 1510, val loss: 2.1190285682678223
Epoch 1520, training loss: 85.78604125976562 = 2.1164923906326294 + 10.0 * 8.366954803466797
Epoch 1520, val loss: 2.1187496185302734
Epoch 1530, training loss: 85.7787857055664 = 2.1162109375 + 10.0 * 8.366257667541504
Epoch 1530, val loss: 2.1184847354888916
Epoch 1540, training loss: 85.77425384521484 = 2.1159316301345825 + 10.0 * 8.365832328796387
Epoch 1540, val loss: 2.1182193756103516
Epoch 1550, training loss: 85.78719329833984 = 2.1156316995620728 + 10.0 * 8.367156028747559
Epoch 1550, val loss: 2.1179513931274414
Epoch 1560, training loss: 85.76895904541016 = 2.1153368949890137 + 10.0 * 8.365362167358398
Epoch 1560, val loss: 2.117619514465332
Epoch 1570, training loss: 85.76309967041016 = 2.115031361579895 + 10.0 * 8.36480712890625
Epoch 1570, val loss: 2.117347240447998
Epoch 1580, training loss: 85.75695037841797 = 2.1147541999816895 + 10.0 * 8.364219665527344
Epoch 1580, val loss: 2.1170735359191895
Epoch 1590, training loss: 85.75172424316406 = 2.1144853830337524 + 10.0 * 8.363723754882812
Epoch 1590, val loss: 2.1168088912963867
Epoch 1600, training loss: 85.74686431884766 = 2.114203929901123 + 10.0 * 8.363265991210938
Epoch 1600, val loss: 2.116539478302002
Epoch 1610, training loss: 85.74230194091797 = 2.113931179046631 + 10.0 * 8.362836837768555
Epoch 1610, val loss: 2.116272211074829
Epoch 1620, training loss: 85.74446105957031 = 2.113648295402527 + 10.0 * 8.363080978393555
Epoch 1620, val loss: 2.1160085201263428
Epoch 1630, training loss: 85.73705291748047 = 2.1133357286453247 + 10.0 * 8.362371444702148
Epoch 1630, val loss: 2.11568021774292
Epoch 1640, training loss: 85.7358627319336 = 2.113037943840027 + 10.0 * 8.362282752990723
Epoch 1640, val loss: 2.115408420562744
Epoch 1650, training loss: 85.7264404296875 = 2.1127594709396362 + 10.0 * 8.361368179321289
Epoch 1650, val loss: 2.1151418685913086
Epoch 1660, training loss: 85.72248077392578 = 2.1124935150146484 + 10.0 * 8.360998153686523
Epoch 1660, val loss: 2.1148757934570312
Epoch 1670, training loss: 85.73497009277344 = 2.1122177839279175 + 10.0 * 8.362275123596191
Epoch 1670, val loss: 2.1146068572998047
Epoch 1680, training loss: 85.71426391601562 = 2.111897587776184 + 10.0 * 8.360236167907715
Epoch 1680, val loss: 2.114299774169922
Epoch 1690, training loss: 85.71051788330078 = 2.1116228103637695 + 10.0 * 8.359889030456543
Epoch 1690, val loss: 2.1140341758728027
Epoch 1700, training loss: 85.70547485351562 = 2.1113548278808594 + 10.0 * 8.359411239624023
Epoch 1700, val loss: 2.1137733459472656
Epoch 1710, training loss: 85.70087432861328 = 2.111084818840027 + 10.0 * 8.358979225158691
Epoch 1710, val loss: 2.1135120391845703
Epoch 1720, training loss: 85.69791412353516 = 2.110812187194824 + 10.0 * 8.358710289001465
Epoch 1720, val loss: 2.1132521629333496
Epoch 1730, training loss: 85.71047973632812 = 2.110538959503174 + 10.0 * 8.359993934631348
Epoch 1730, val loss: 2.1129846572875977
Epoch 1740, training loss: 85.69779205322266 = 2.1102511882781982 + 10.0 * 8.35875415802002
Epoch 1740, val loss: 2.1126885414123535
Epoch 1750, training loss: 85.68757629394531 = 2.1099681854248047 + 10.0 * 8.357760429382324
Epoch 1750, val loss: 2.1124267578125
Epoch 1760, training loss: 85.68246459960938 = 2.109700083732605 + 10.0 * 8.35727596282959
Epoch 1760, val loss: 2.1121671199798584
Epoch 1770, training loss: 85.6806640625 = 2.10943341255188 + 10.0 * 8.357122421264648
Epoch 1770, val loss: 2.1119117736816406
Epoch 1780, training loss: 85.6905288696289 = 2.1091562509536743 + 10.0 * 8.358137130737305
Epoch 1780, val loss: 2.111633777618408
Epoch 1790, training loss: 85.67391204833984 = 2.1088865995407104 + 10.0 * 8.356502532958984
Epoch 1790, val loss: 2.111358880996704
Epoch 1800, training loss: 85.6664810180664 = 2.1086068153381348 + 10.0 * 8.35578727722168
Epoch 1800, val loss: 2.1111032962799072
Epoch 1810, training loss: 85.66118621826172 = 2.10834801197052 + 10.0 * 8.355283737182617
Epoch 1810, val loss: 2.110849380493164
Epoch 1820, training loss: 85.6574935913086 = 2.1081044673919678 + 10.0 * 8.354939460754395
Epoch 1820, val loss: 2.1106021404266357
Epoch 1830, training loss: 85.65447998046875 = 2.1078484058380127 + 10.0 * 8.354662895202637
Epoch 1830, val loss: 2.110351085662842
Epoch 1840, training loss: 85.68489837646484 = 2.107588768005371 + 10.0 * 8.357730865478516
Epoch 1840, val loss: 2.1100823879241943
Epoch 1850, training loss: 85.66027069091797 = 2.1072317361831665 + 10.0 * 8.355303764343262
Epoch 1850, val loss: 2.109774112701416
Epoch 1860, training loss: 85.64292907714844 = 2.1070057153701782 + 10.0 * 8.353592872619629
Epoch 1860, val loss: 2.109532356262207
Epoch 1870, training loss: 85.63861846923828 = 2.1067521572113037 + 10.0 * 8.35318660736084
Epoch 1870, val loss: 2.1092772483825684
Epoch 1880, training loss: 85.63501739501953 = 2.1064943075180054 + 10.0 * 8.352852821350098
Epoch 1880, val loss: 2.1090424060821533
Epoch 1890, training loss: 85.63130950927734 = 2.1062532663345337 + 10.0 * 8.352505683898926
Epoch 1890, val loss: 2.108798027038574
Epoch 1900, training loss: 85.6396484375 = 2.10597562789917 + 10.0 * 8.353367805480957
Epoch 1900, val loss: 2.108546018600464
Epoch 1910, training loss: 85.63184356689453 = 2.1056982278823853 + 10.0 * 8.352614402770996
Epoch 1910, val loss: 2.108253002166748
Epoch 1920, training loss: 85.62408447265625 = 2.105439066886902 + 10.0 * 8.3518648147583
Epoch 1920, val loss: 2.107999801635742
Epoch 1930, training loss: 85.61738586425781 = 2.1051840782165527 + 10.0 * 8.35122013092041
Epoch 1930, val loss: 2.107760190963745
Epoch 1940, training loss: 85.613525390625 = 2.104938268661499 + 10.0 * 8.350858688354492
Epoch 1940, val loss: 2.1075072288513184
Epoch 1950, training loss: 85.61467742919922 = 2.1046905517578125 + 10.0 * 8.350998878479004
Epoch 1950, val loss: 2.1072702407836914
Epoch 1960, training loss: 85.61105346679688 = 2.1044033765792847 + 10.0 * 8.350665092468262
Epoch 1960, val loss: 2.1069841384887695
Epoch 1970, training loss: 85.60401916503906 = 2.104126214981079 + 10.0 * 8.34998893737793
Epoch 1970, val loss: 2.1067252159118652
Epoch 1980, training loss: 85.60166931152344 = 2.1038780212402344 + 10.0 * 8.34977912902832
Epoch 1980, val loss: 2.106472969055176
Epoch 1990, training loss: 85.59735107421875 = 2.1036250591278076 + 10.0 * 8.349372863769531
Epoch 1990, val loss: 2.106231212615967
Epoch 2000, training loss: 85.5972900390625 = 2.103379011154175 + 10.0 * 8.349390983581543
Epoch 2000, val loss: 2.105985164642334
Epoch 2010, training loss: 85.62535858154297 = 2.103082537651062 + 10.0 * 8.352228164672852
Epoch 2010, val loss: 2.105696439743042
Epoch 2020, training loss: 85.59169006347656 = 2.102764844894409 + 10.0 * 8.348892211914062
Epoch 2020, val loss: 2.105386257171631
Epoch 2030, training loss: 85.5873031616211 = 2.102515459060669 + 10.0 * 8.348478317260742
Epoch 2030, val loss: 2.105154514312744
Epoch 2040, training loss: 85.58317565917969 = 2.1022783517837524 + 10.0 * 8.348089218139648
Epoch 2040, val loss: 2.104910373687744
Epoch 2050, training loss: 85.5790023803711 = 2.1020299196243286 + 10.0 * 8.347697257995605
Epoch 2050, val loss: 2.1046762466430664
Epoch 2060, training loss: 85.57608795166016 = 2.101785898208618 + 10.0 * 8.347430229187012
Epoch 2060, val loss: 2.1044349670410156
Epoch 2070, training loss: 85.57302856445312 = 2.1015377044677734 + 10.0 * 8.347148895263672
Epoch 2070, val loss: 2.104191780090332
Epoch 2080, training loss: 85.57056427001953 = 2.101288676261902 + 10.0 * 8.346927642822266
Epoch 2080, val loss: 2.103947639465332
Epoch 2090, training loss: 85.5989990234375 = 2.1010375022888184 + 10.0 * 8.349796295166016
Epoch 2090, val loss: 2.103698253631592
Epoch 2100, training loss: 85.58140563964844 = 2.100683808326721 + 10.0 * 8.348072052001953
Epoch 2100, val loss: 2.1033711433410645
Epoch 2110, training loss: 85.56433868408203 = 2.1004515886306763 + 10.0 * 8.346388816833496
Epoch 2110, val loss: 2.103120803833008
Epoch 2120, training loss: 85.56011962890625 = 2.1001957654953003 + 10.0 * 8.345992088317871
Epoch 2120, val loss: 2.102883815765381
Epoch 2130, training loss: 85.55635070800781 = 2.099963903427124 + 10.0 * 8.3456392288208
Epoch 2130, val loss: 2.102658987045288
Epoch 2140, training loss: 85.5531005859375 = 2.099726438522339 + 10.0 * 8.345337867736816
Epoch 2140, val loss: 2.1024208068847656
Epoch 2150, training loss: 85.55010223388672 = 2.0994845628738403 + 10.0 * 8.345061302185059
Epoch 2150, val loss: 2.102187156677246
Epoch 2160, training loss: 85.54731750488281 = 2.0992408990859985 + 10.0 * 8.344807624816895
Epoch 2160, val loss: 2.101949691772461
Epoch 2170, training loss: 85.54566955566406 = 2.0989967584609985 + 10.0 * 8.344667434692383
Epoch 2170, val loss: 2.101707935333252
Epoch 2180, training loss: 85.57398223876953 = 2.098726749420166 + 10.0 * 8.347525596618652
Epoch 2180, val loss: 2.101445436477661
Epoch 2190, training loss: 85.563232421875 = 2.0984336137771606 + 10.0 * 8.346479415893555
Epoch 2190, val loss: 2.101163864135742
Epoch 2200, training loss: 85.54092407226562 = 2.098164439201355 + 10.0 * 8.34427547454834
Epoch 2200, val loss: 2.1008963584899902
Epoch 2210, training loss: 85.53335571289062 = 2.0979212522506714 + 10.0 * 8.34354305267334
Epoch 2210, val loss: 2.100658893585205
Epoch 2220, training loss: 85.53175354003906 = 2.0976935625076294 + 10.0 * 8.343405723571777
Epoch 2220, val loss: 2.1004371643066406
Epoch 2230, training loss: 85.52751922607422 = 2.097468376159668 + 10.0 * 8.343005180358887
Epoch 2230, val loss: 2.100217580795288
Epoch 2240, training loss: 85.5253677368164 = 2.0972388982772827 + 10.0 * 8.342813491821289
Epoch 2240, val loss: 2.0999927520751953
Epoch 2250, training loss: 85.5322036743164 = 2.0969988107681274 + 10.0 * 8.343520164489746
Epoch 2250, val loss: 2.099760055541992
Epoch 2260, training loss: 85.52597045898438 = 2.0967098474502563 + 10.0 * 8.342926025390625
Epoch 2260, val loss: 2.0994741916656494
Epoch 2270, training loss: 85.51911926269531 = 2.0964648723602295 + 10.0 * 8.342265129089355
Epoch 2270, val loss: 2.099238872528076
Epoch 2280, training loss: 85.51492309570312 = 2.0962339639663696 + 10.0 * 8.34186840057373
Epoch 2280, val loss: 2.0990090370178223
Epoch 2290, training loss: 85.51154327392578 = 2.0960118770599365 + 10.0 * 8.341553688049316
Epoch 2290, val loss: 2.098794460296631
Epoch 2300, training loss: 85.50841522216797 = 2.0957926511764526 + 10.0 * 8.341261863708496
Epoch 2300, val loss: 2.0985727310180664
Epoch 2310, training loss: 85.53046417236328 = 2.0955852270126343 + 10.0 * 8.343487739562988
Epoch 2310, val loss: 2.098336696624756
Epoch 2320, training loss: 85.50900268554688 = 2.095274329185486 + 10.0 * 8.3413724899292
Epoch 2320, val loss: 2.0980782508850098
Epoch 2330, training loss: 85.5000228881836 = 2.0950511693954468 + 10.0 * 8.340497016906738
Epoch 2330, val loss: 2.0978646278381348
Epoch 2340, training loss: 85.49748992919922 = 2.0948410034179688 + 10.0 * 8.340265274047852
Epoch 2340, val loss: 2.0976462364196777
Epoch 2350, training loss: 85.49459838867188 = 2.0946189165115356 + 10.0 * 8.339998245239258
Epoch 2350, val loss: 2.0974366664886475
Epoch 2360, training loss: 85.51861572265625 = 2.094376564025879 + 10.0 * 8.342424392700195
Epoch 2360, val loss: 2.097212791442871
Epoch 2370, training loss: 85.4963150024414 = 2.0941022634506226 + 10.0 * 8.340221405029297
Epoch 2370, val loss: 2.0969183444976807
Epoch 2380, training loss: 85.49079132080078 = 2.0938808917999268 + 10.0 * 8.339691162109375
Epoch 2380, val loss: 2.096714973449707
Epoch 2390, training loss: 85.4847640991211 = 2.0936644077301025 + 10.0 * 8.339109420776367
Epoch 2390, val loss: 2.096498966217041
Epoch 2400, training loss: 85.48944091796875 = 2.0934627056121826 + 10.0 * 8.339597702026367
Epoch 2400, val loss: 2.096287250518799
Epoch 2410, training loss: 85.47634887695312 = 2.0932066440582275 + 10.0 * 8.338314056396484
Epoch 2410, val loss: 2.0960659980773926
Epoch 2420, training loss: 85.47469329833984 = 2.092995047569275 + 10.0 * 8.338170051574707
Epoch 2420, val loss: 2.095853328704834
Epoch 2430, training loss: 85.47112274169922 = 2.092788815498352 + 10.0 * 8.337833404541016
Epoch 2430, val loss: 2.095648765563965
Epoch 2440, training loss: 85.46824645996094 = 2.0925744771957397 + 10.0 * 8.337567329406738
Epoch 2440, val loss: 2.095447301864624
Epoch 2450, training loss: 85.47119903564453 = 2.092363715171814 + 10.0 * 8.337883949279785
Epoch 2450, val loss: 2.095242500305176
Epoch 2460, training loss: 85.48147583007812 = 2.092118501663208 + 10.0 * 8.338935852050781
Epoch 2460, val loss: 2.094991683959961
Epoch 2470, training loss: 85.4596176147461 = 2.0918421745300293 + 10.0 * 8.336777687072754
Epoch 2470, val loss: 2.094735622406006
Epoch 2480, training loss: 85.4578857421875 = 2.09162700176239 + 10.0 * 8.336626052856445
Epoch 2480, val loss: 2.094520092010498
Epoch 2490, training loss: 85.45498657226562 = 2.0914324522018433 + 10.0 * 8.336355209350586
Epoch 2490, val loss: 2.094336986541748
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.39666666666666667
0.8630007969282041
The final CL Acc:0.39686, 0.00027, The final GNN Acc:0.86430, 0.00099
Begin epxeriment: noisy_level: 0.2 cont_weight: 10 epoch: der1:0.4 der2:0 dfr1:0.4 dfr2:0.4
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=2500, cl_num_layers=2, cl_num_proj_hidden=64, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=10.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.0, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=32, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, noisy_level=0.2, num_hidden=64, num_proj_hidden=64, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=40, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 88648])
add edge: torch.Size([2, 106352])
remove edge: torch.Size([2, 70806])
updated graph: torch.Size([2, 88510])
=== Noisy graph ===
/home/project-robust-contrastive/Robust-Contrastive-Learning/model.py:227: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 106.9241714477539 = 1.1011502742767334 + 10.0 * 10.58230209350586
Epoch 0, val loss: 1.1014995574951172
Epoch 10, training loss: 106.9186019897461 = 1.0980678796768188 + 10.0 * 10.582053184509277
Epoch 10, val loss: 1.0984500646591187
Epoch 20, training loss: 106.90381622314453 = 1.0949199199676514 + 10.0 * 10.580889701843262
Epoch 20, val loss: 1.0953164100646973
Epoch 30, training loss: 106.84638214111328 = 1.091515064239502 + 10.0 * 10.57548713684082
Epoch 30, val loss: 1.0919249057769775
Epoch 40, training loss: 106.59223175048828 = 1.0877162218093872 + 10.0 * 10.550451278686523
Epoch 40, val loss: 1.0881502628326416
Epoch 50, training loss: 105.52247619628906 = 1.0834484100341797 + 10.0 * 10.443902969360352
Epoch 50, val loss: 1.0839208364486694
Epoch 60, training loss: 101.65489196777344 = 1.0790300369262695 + 10.0 * 10.057585716247559
Epoch 60, val loss: 1.0795689821243286
Epoch 70, training loss: 97.82270812988281 = 1.0746686458587646 + 10.0 * 9.674803733825684
Epoch 70, val loss: 1.0753328800201416
Epoch 80, training loss: 97.29924011230469 = 1.0706427097320557 + 10.0 * 9.622859954833984
Epoch 80, val loss: 1.071379542350769
Epoch 90, training loss: 96.32530212402344 = 1.0671137571334839 + 10.0 * 9.525818824768066
Epoch 90, val loss: 1.067970871925354
Epoch 100, training loss: 94.75593566894531 = 1.0641754865646362 + 10.0 * 9.369175910949707
Epoch 100, val loss: 1.0651592016220093
Epoch 110, training loss: 93.59773254394531 = 1.0617506504058838 + 10.0 * 9.2535982131958
Epoch 110, val loss: 1.0627797842025757
Epoch 120, training loss: 92.89458465576172 = 1.0589838027954102 + 10.0 * 9.183560371398926
Epoch 120, val loss: 1.0600390434265137
Epoch 130, training loss: 91.79844665527344 = 1.0564247369766235 + 10.0 * 9.074201583862305
Epoch 130, val loss: 1.0575848817825317
Epoch 140, training loss: 90.9339828491211 = 1.0544085502624512 + 10.0 * 8.987957954406738
Epoch 140, val loss: 1.0556585788726807
Epoch 150, training loss: 90.41326904296875 = 1.0520408153533936 + 10.0 * 8.93612289428711
Epoch 150, val loss: 1.0532621145248413
Epoch 160, training loss: 89.89208984375 = 1.0494410991668701 + 10.0 * 8.884264945983887
Epoch 160, val loss: 1.0507432222366333
Epoch 170, training loss: 89.51956176757812 = 1.0469541549682617 + 10.0 * 8.847261428833008
Epoch 170, val loss: 1.0483059883117676
Epoch 180, training loss: 89.22345733642578 = 1.0445175170898438 + 10.0 * 8.817893981933594
Epoch 180, val loss: 1.0459061861038208
Epoch 190, training loss: 88.98956298828125 = 1.0419902801513672 + 10.0 * 8.794756889343262
Epoch 190, val loss: 1.0434291362762451
Epoch 200, training loss: 88.76809692382812 = 1.0394256114959717 + 10.0 * 8.772867202758789
Epoch 200, val loss: 1.0408889055252075
Epoch 210, training loss: 88.58573913574219 = 1.0368198156356812 + 10.0 * 8.754892349243164
Epoch 210, val loss: 1.0383319854736328
Epoch 220, training loss: 88.4214859008789 = 1.0341709852218628 + 10.0 * 8.738731384277344
Epoch 220, val loss: 1.0357086658477783
Epoch 230, training loss: 88.27127838134766 = 1.0314338207244873 + 10.0 * 8.723984718322754
Epoch 230, val loss: 1.0330243110656738
Epoch 240, training loss: 88.12557220458984 = 1.0286171436309814 + 10.0 * 8.709695816040039
Epoch 240, val loss: 1.030235767364502
Epoch 250, training loss: 88.00618743896484 = 1.0256456136703491 + 10.0 * 8.698054313659668
Epoch 250, val loss: 1.0273083448410034
Epoch 260, training loss: 87.90878295898438 = 1.0224844217300415 + 10.0 * 8.688630104064941
Epoch 260, val loss: 1.0241942405700684
Epoch 270, training loss: 87.80970001220703 = 1.0191129446029663 + 10.0 * 8.679059028625488
Epoch 270, val loss: 1.0208675861358643
Epoch 280, training loss: 87.72268676757812 = 1.0155366659164429 + 10.0 * 8.67071533203125
Epoch 280, val loss: 1.017345666885376
Epoch 290, training loss: 87.65535736083984 = 1.0117433071136475 + 10.0 * 8.664361953735352
Epoch 290, val loss: 1.0136059522628784
Epoch 300, training loss: 87.58228302001953 = 1.0076972246170044 + 10.0 * 8.657458305358887
Epoch 300, val loss: 1.0096335411071777
Epoch 310, training loss: 87.50816345214844 = 1.0034058094024658 + 10.0 * 8.650476455688477
Epoch 310, val loss: 1.0054190158843994
Epoch 320, training loss: 87.45687103271484 = 0.9988182783126831 + 10.0 * 8.645805358886719
Epoch 320, val loss: 1.0009024143218994
Epoch 330, training loss: 87.38797760009766 = 0.9939337372779846 + 10.0 * 8.639404296875
Epoch 330, val loss: 0.9961017370223999
Epoch 340, training loss: 87.30921173095703 = 0.9887891411781311 + 10.0 * 8.632041931152344
Epoch 340, val loss: 0.99106365442276
Epoch 350, training loss: 87.23995971679688 = 0.9833121299743652 + 10.0 * 8.625664710998535
Epoch 350, val loss: 0.9856923818588257
Epoch 360, training loss: 87.17243194580078 = 0.9774868488311768 + 10.0 * 8.619494438171387
Epoch 360, val loss: 0.979981005191803
Epoch 370, training loss: 87.10795593261719 = 0.9713649749755859 + 10.0 * 8.613658905029297
Epoch 370, val loss: 0.9740113019943237
Epoch 380, training loss: 87.03868103027344 = 0.9649321436882019 + 10.0 * 8.60737419128418
Epoch 380, val loss: 0.9677355885505676
Epoch 390, training loss: 86.97486877441406 = 0.958161473274231 + 10.0 * 8.60167121887207
Epoch 390, val loss: 0.9611337184906006
Epoch 400, training loss: 86.97116088867188 = 0.9510278105735779 + 10.0 * 8.60201358795166
Epoch 400, val loss: 0.9541754126548767
Epoch 410, training loss: 86.85087585449219 = 0.9435248970985413 + 10.0 * 8.590734481811523
Epoch 410, val loss: 0.9468871355056763
Epoch 420, training loss: 86.79456329345703 = 0.9357913136482239 + 10.0 * 8.585877418518066
Epoch 420, val loss: 0.9393816590309143
Epoch 430, training loss: 86.72975158691406 = 0.9277432560920715 + 10.0 * 8.580201148986816
Epoch 430, val loss: 0.9315776824951172
Epoch 440, training loss: 86.67324829101562 = 0.9193845391273499 + 10.0 * 8.575386047363281
Epoch 440, val loss: 0.9234923124313354
Epoch 450, training loss: 86.66397094726562 = 0.9107246994972229 + 10.0 * 8.575324058532715
Epoch 450, val loss: 0.9151225686073303
Epoch 460, training loss: 86.578369140625 = 0.9017544984817505 + 10.0 * 8.56766128540039
Epoch 460, val loss: 0.906506359577179
Epoch 470, training loss: 86.52342987060547 = 0.892594575881958 + 10.0 * 8.56308364868164
Epoch 470, val loss: 0.8976837396621704
Epoch 480, training loss: 86.47891235351562 = 0.8831977844238281 + 10.0 * 8.559571266174316
Epoch 480, val loss: 0.8886624574661255
Epoch 490, training loss: 86.44605255126953 = 0.8735707998275757 + 10.0 * 8.55724811553955
Epoch 490, val loss: 0.8794592618942261
Epoch 500, training loss: 86.44561004638672 = 0.8636698722839355 + 10.0 * 8.558194160461426
Epoch 500, val loss: 0.8700382113456726
Epoch 510, training loss: 86.36392211914062 = 0.8536836504936218 + 10.0 * 8.551023483276367
Epoch 510, val loss: 0.8605303168296814
Epoch 520, training loss: 86.3318099975586 = 0.8436050415039062 + 10.0 * 8.548820495605469
Epoch 520, val loss: 0.8509812355041504
Epoch 530, training loss: 86.29220581054688 = 0.8334329128265381 + 10.0 * 8.545877456665039
Epoch 530, val loss: 0.8413861989974976
Epoch 540, training loss: 86.27671813964844 = 0.8232179880142212 + 10.0 * 8.545350074768066
Epoch 540, val loss: 0.8317787051200867
Epoch 550, training loss: 86.22494506835938 = 0.812972903251648 + 10.0 * 8.541196823120117
Epoch 550, val loss: 0.8221870064735413
Epoch 560, training loss: 86.19425964355469 = 0.8027589917182922 + 10.0 * 8.53915023803711
Epoch 560, val loss: 0.8126647472381592
Epoch 570, training loss: 86.23446655273438 = 0.7925729155540466 + 10.0 * 8.544189453125
Epoch 570, val loss: 0.8031876683235168
Epoch 580, training loss: 86.13980865478516 = 0.782432496547699 + 10.0 * 8.535737991333008
Epoch 580, val loss: 0.7938210368156433
Epoch 590, training loss: 86.10557556152344 = 0.7724716067314148 + 10.0 * 8.533310890197754
Epoch 590, val loss: 0.7846394777297974
Epoch 600, training loss: 86.07437133789062 = 0.7626363039016724 + 10.0 * 8.531173706054688
Epoch 600, val loss: 0.7756043076515198
Epoch 610, training loss: 86.04598236083984 = 0.7529503107070923 + 10.0 * 8.529302597045898
Epoch 610, val loss: 0.7667489647865295
Epoch 620, training loss: 86.02076721191406 = 0.7434245944023132 + 10.0 * 8.527734756469727
Epoch 620, val loss: 0.7580795288085938
Epoch 630, training loss: 86.03060913085938 = 0.7340847253799438 + 10.0 * 8.52965259552002
Epoch 630, val loss: 0.7495982646942139
Epoch 640, training loss: 85.9804458618164 = 0.7249224781990051 + 10.0 * 8.525552749633789
Epoch 640, val loss: 0.7413451671600342
Epoch 650, training loss: 85.95860290527344 = 0.7160578966140747 + 10.0 * 8.52425479888916
Epoch 650, val loss: 0.7333853244781494
Epoch 660, training loss: 85.9326400756836 = 0.707436740398407 + 10.0 * 8.522520065307617
Epoch 660, val loss: 0.725672721862793
Epoch 670, training loss: 85.92237091064453 = 0.6990230083465576 + 10.0 * 8.522335052490234
Epoch 670, val loss: 0.7181900143623352
Epoch 680, training loss: 85.91880798339844 = 0.6908072233200073 + 10.0 * 8.52280044555664
Epoch 680, val loss: 0.7109315991401672
Epoch 690, training loss: 85.88545989990234 = 0.6828140616416931 + 10.0 * 8.520264625549316
Epoch 690, val loss: 0.7038698196411133
Epoch 700, training loss: 85.86103820800781 = 0.675123929977417 + 10.0 * 8.518590927124023
Epoch 700, val loss: 0.6971622705459595
Epoch 710, training loss: 85.83839416503906 = 0.6677229404449463 + 10.0 * 8.517066955566406
Epoch 710, val loss: 0.6907234191894531
Epoch 720, training loss: 85.82102966308594 = 0.6605949997901917 + 10.0 * 8.516042709350586
Epoch 720, val loss: 0.6845781803131104
Epoch 730, training loss: 85.8039321899414 = 0.6537531614303589 + 10.0 * 8.51501750946045
Epoch 730, val loss: 0.678713858127594
Epoch 740, training loss: 85.82317352294922 = 0.6471977829933167 + 10.0 * 8.517597198486328
Epoch 740, val loss: 0.6731341481208801
Epoch 750, training loss: 85.78229522705078 = 0.6408741474151611 + 10.0 * 8.514142036437988
Epoch 750, val loss: 0.6678017973899841
Epoch 760, training loss: 85.76380157470703 = 0.6348996758460999 + 10.0 * 8.512889862060547
Epoch 760, val loss: 0.6627989411354065
Epoch 770, training loss: 85.73938751220703 = 0.6292193531990051 + 10.0 * 8.511016845703125
Epoch 770, val loss: 0.6581132411956787
Epoch 780, training loss: 85.72817993164062 = 0.6238207221031189 + 10.0 * 8.510436058044434
Epoch 780, val loss: 0.6537318229675293
Epoch 790, training loss: 85.71849060058594 = 0.6186627745628357 + 10.0 * 8.50998306274414
Epoch 790, val loss: 0.6495580673217773
Epoch 800, training loss: 85.70339965820312 = 0.6137605309486389 + 10.0 * 8.508963584899902
Epoch 800, val loss: 0.6456597447395325
Epoch 810, training loss: 85.67969512939453 = 0.6091288328170776 + 10.0 * 8.50705623626709
Epoch 810, val loss: 0.6419917941093445
Epoch 820, training loss: 85.66716003417969 = 0.6047163009643555 + 10.0 * 8.506244659423828
Epoch 820, val loss: 0.6385666131973267
Epoch 830, training loss: 85.68212890625 = 0.6004828810691833 + 10.0 * 8.50816535949707
Epoch 830, val loss: 0.6353338360786438
Epoch 840, training loss: 85.63966369628906 = 0.5964534282684326 + 10.0 * 8.504321098327637
Epoch 840, val loss: 0.632304847240448
Epoch 850, training loss: 85.61795806884766 = 0.5926265716552734 + 10.0 * 8.502532958984375
Epoch 850, val loss: 0.629493236541748
Epoch 860, training loss: 85.60153198242188 = 0.5889925956726074 + 10.0 * 8.501254081726074
Epoch 860, val loss: 0.6268694400787354
Epoch 870, training loss: 85.62181854248047 = 0.5855368971824646 + 10.0 * 8.503628730773926
Epoch 870, val loss: 0.6244001984596252
Epoch 880, training loss: 85.59847259521484 = 0.5822250843048096 + 10.0 * 8.501625061035156
Epoch 880, val loss: 0.6221267580986023
Epoch 890, training loss: 85.57429504394531 = 0.5790939927101135 + 10.0 * 8.499520301818848
Epoch 890, val loss: 0.6200225353240967
Epoch 900, training loss: 85.54524993896484 = 0.576145350933075 + 10.0 * 8.496910095214844
Epoch 900, val loss: 0.6180225014686584
Epoch 910, training loss: 85.5348892211914 = 0.5733570456504822 + 10.0 * 8.496152877807617
Epoch 910, val loss: 0.6162286996841431
Epoch 920, training loss: 85.58899688720703 = 0.5706828236579895 + 10.0 * 8.5018310546875
Epoch 920, val loss: 0.6145771145820618
Epoch 930, training loss: 85.52041625976562 = 0.5681074261665344 + 10.0 * 8.495230674743652
Epoch 930, val loss: 0.6129425764083862
Epoch 940, training loss: 85.49131774902344 = 0.5656876564025879 + 10.0 * 8.492563247680664
Epoch 940, val loss: 0.6114993691444397
Epoch 950, training loss: 85.47955322265625 = 0.5633866190910339 + 10.0 * 8.491617202758789
Epoch 950, val loss: 0.6101748943328857
Epoch 960, training loss: 85.50100708007812 = 0.5611881017684937 + 10.0 * 8.493982315063477
Epoch 960, val loss: 0.6088971495628357
Epoch 970, training loss: 85.47648620605469 = 0.5590416193008423 + 10.0 * 8.491744041442871
Epoch 970, val loss: 0.6077154874801636
Epoch 980, training loss: 85.44912719726562 = 0.557018518447876 + 10.0 * 8.48921012878418
Epoch 980, val loss: 0.606647789478302
Epoch 990, training loss: 85.4326400756836 = 0.5550894737243652 + 10.0 * 8.487754821777344
Epoch 990, val loss: 0.6056445240974426
Epoch 1000, training loss: 85.4325180053711 = 0.5532456040382385 + 10.0 * 8.487927436828613
Epoch 1000, val loss: 0.6046928763389587
Epoch 1010, training loss: 85.43142700195312 = 0.5514342784881592 + 10.0 * 8.487998962402344
Epoch 1010, val loss: 0.6037936806678772
Epoch 1020, training loss: 85.40506744384766 = 0.5496905446052551 + 10.0 * 8.4855375289917
Epoch 1020, val loss: 0.6029530167579651
Epoch 1030, training loss: 85.39006042480469 = 0.5480260848999023 + 10.0 * 8.484203338623047
Epoch 1030, val loss: 0.6021693348884583
Epoch 1040, training loss: 85.38018798828125 = 0.5464242696762085 + 10.0 * 8.483376502990723
Epoch 1040, val loss: 0.6014453172683716
Epoch 1050, training loss: 85.38581085205078 = 0.5448710322380066 + 10.0 * 8.484094619750977
Epoch 1050, val loss: 0.6007629632949829
Epoch 1060, training loss: 85.3963851928711 = 0.5433281064033508 + 10.0 * 8.485305786132812
Epoch 1060, val loss: 0.6000546216964722
Epoch 1070, training loss: 85.37183380126953 = 0.5418299436569214 + 10.0 * 8.482999801635742
Epoch 1070, val loss: 0.599400520324707
Epoch 1080, training loss: 85.34539794921875 = 0.5403907895088196 + 10.0 * 8.480501174926758
Epoch 1080, val loss: 0.5987575650215149
Epoch 1090, training loss: 85.33622741699219 = 0.5390048027038574 + 10.0 * 8.47972297668457
Epoch 1090, val loss: 0.5982458591461182
Epoch 1100, training loss: 85.36795043945312 = 0.5376456379890442 + 10.0 * 8.483030319213867
Epoch 1100, val loss: 0.5977157354354858
Epoch 1110, training loss: 85.32189178466797 = 0.5362863540649414 + 10.0 * 8.478560447692871
Epoch 1110, val loss: 0.5970873832702637
Epoch 1120, training loss: 85.30815124511719 = 0.5349725484848022 + 10.0 * 8.477317810058594
Epoch 1120, val loss: 0.596609354019165
Epoch 1130, training loss: 85.29847717285156 = 0.5337015986442566 + 10.0 * 8.47647762298584
Epoch 1130, val loss: 0.5961074233055115
Epoch 1140, training loss: 85.2946548461914 = 0.5324596762657166 + 10.0 * 8.476219177246094
Epoch 1140, val loss: 0.5956379175186157
Epoch 1150, training loss: 85.32691192626953 = 0.5312291383743286 + 10.0 * 8.479568481445312
Epoch 1150, val loss: 0.5951656699180603
Epoch 1160, training loss: 85.28972625732422 = 0.5299940705299377 + 10.0 * 8.475973129272461
Epoch 1160, val loss: 0.594671905040741
Epoch 1170, training loss: 85.26924133300781 = 0.5288028717041016 + 10.0 * 8.474043846130371
Epoch 1170, val loss: 0.5942062735557556
Epoch 1180, training loss: 85.26150512695312 = 0.527647852897644 + 10.0 * 8.47338581085205
Epoch 1180, val loss: 0.5937833786010742
Epoch 1190, training loss: 85.35458374023438 = 0.5265017151832581 + 10.0 * 8.482808113098145
Epoch 1190, val loss: 0.5933173298835754
Epoch 1200, training loss: 85.2793960571289 = 0.5253300070762634 + 10.0 * 8.475406646728516
Epoch 1200, val loss: 0.5928652286529541
Epoch 1210, training loss: 85.24732208251953 = 0.5242071747779846 + 10.0 * 8.472311019897461
Epoch 1210, val loss: 0.5924371480941772
Epoch 1220, training loss: 85.236328125 = 0.5231240391731262 + 10.0 * 8.471320152282715
Epoch 1220, val loss: 0.5920407176017761
Epoch 1230, training loss: 85.22779083251953 = 0.5220646262168884 + 10.0 * 8.470572471618652
Epoch 1230, val loss: 0.5915944576263428
Epoch 1240, training loss: 85.22091674804688 = 0.52101069688797 + 10.0 * 8.469990730285645
Epoch 1240, val loss: 0.5911802649497986
Epoch 1250, training loss: 85.23429870605469 = 0.5199579000473022 + 10.0 * 8.471433639526367
Epoch 1250, val loss: 0.590773344039917
Epoch 1260, training loss: 85.22161102294922 = 0.5188789963722229 + 10.0 * 8.4702730178833
Epoch 1260, val loss: 0.5903762578964233
Epoch 1270, training loss: 85.2132568359375 = 0.5178309082984924 + 10.0 * 8.469542503356934
Epoch 1270, val loss: 0.5898773670196533
Epoch 1280, training loss: 85.19721221923828 = 0.5168076157569885 + 10.0 * 8.468040466308594
Epoch 1280, val loss: 0.5895215272903442
Epoch 1290, training loss: 85.19168090820312 = 0.5158045887947083 + 10.0 * 8.4675874710083
Epoch 1290, val loss: 0.5890816450119019
Epoch 1300, training loss: 85.1978759765625 = 0.5148071646690369 + 10.0 * 8.468306541442871
Epoch 1300, val loss: 0.5886562466621399
Epoch 1310, training loss: 85.20137786865234 = 0.5137847065925598 + 10.0 * 8.468759536743164
Epoch 1310, val loss: 0.5882737636566162
Epoch 1320, training loss: 85.1796875 = 0.5127781629562378 + 10.0 * 8.466691017150879
Epoch 1320, val loss: 0.5877843499183655
Epoch 1330, training loss: 85.17013549804688 = 0.511796236038208 + 10.0 * 8.46583366394043
Epoch 1330, val loss: 0.5874499678611755
Epoch 1340, training loss: 85.16171264648438 = 0.5108379125595093 + 10.0 * 8.465086936950684
Epoch 1340, val loss: 0.586993396282196
Epoch 1350, training loss: 85.15995788574219 = 0.5098782777786255 + 10.0 * 8.465007781982422
Epoch 1350, val loss: 0.5866156220436096
Epoch 1360, training loss: 85.21162414550781 = 0.5089042782783508 + 10.0 * 8.470272064208984
Epoch 1360, val loss: 0.586177408695221
Epoch 1370, training loss: 85.14611053466797 = 0.5079132914543152 + 10.0 * 8.46381950378418
Epoch 1370, val loss: 0.5857522487640381
